mmlu

{'acc,none': 0.5022076627261074, 'acc_stderr,none': np.float64(0.0040290684166938645), 'alias': 'mmlu'}


mmlu_humanities

{'acc,none': 0.46439957492029754, 'acc_stderr,none': np.float64(0.006916782249765903), 'alias': ' - humanities'}


mmlu_formal_logic

{'alias': '  - formal_logic', 'acc,none': 0.24603174603174602, 'acc_stderr,none': 0.03852273364924315}


mmlu_high_school_european_history

{'alias': '  - high_school_european_history', 'acc,none': 0.6, 'acc_stderr,none': 0.03825460278380026}


mmlu_high_school_us_history

{'alias': '  - high_school_us_history', 'acc,none': 0.6421568627450981, 'acc_stderr,none': 0.03364487286088299}


mmlu_high_school_world_history

{'alias': '  - high_school_world_history', 'acc,none': 0.6413502109704642, 'acc_stderr,none': 0.03121956944530185}


mmlu_international_law

{'alias': '  - international_law', 'acc,none': 0.71900826446281, 'acc_stderr,none': 0.04103203830514512}


mmlu_jurisprudence

{'alias': '  - jurisprudence', 'acc,none': 0.6018518518518519, 'acc_stderr,none': 0.04732332615978815}


mmlu_logical_fallacies

{'alias': '  - logical_fallacies', 'acc,none': 0.6441717791411042, 'acc_stderr,none': 0.03761521380046734}


mmlu_moral_disputes

{'alias': '  - moral_disputes', 'acc,none': 0.5115606936416185, 'acc_stderr,none': 0.02691189868637793}


mmlu_moral_scenarios

{'alias': '  - moral_scenarios', 'acc,none': 0.24916201117318434, 'acc_stderr,none': 0.01446589382985993}


mmlu_philosophy

{'alias': '  - philosophy', 'acc,none': 0.6109324758842444, 'acc_stderr,none': 0.027690337536485372}


mmlu_prehistory

{'alias': '  - prehistory', 'acc,none': 0.6018518518518519, 'acc_stderr,none': 0.027237415094592467}


mmlu_professional_law

{'alias': '  - professional_law', 'acc,none': 0.39308996088657105, 'acc_stderr,none': 0.012474899613873956}


mmlu_world_religions

{'alias': '  - world_religions', 'acc,none': 0.7426900584795322, 'acc_stderr,none': 0.03352799844161865}


mmlu_other

{'acc,none': 0.566784679755391, 'acc_stderr,none': np.float64(0.008588649266340943), 'alias': ' - other'}


mmlu_business_ethics

{'alias': '  - business_ethics', 'acc,none': 0.48, 'acc_stderr,none': 0.050211673156867795}


mmlu_clinical_knowledge

{'alias': '  - clinical_knowledge', 'acc,none': 0.5886792452830188, 'acc_stderr,none': 0.030285009259009794}


mmlu_college_medicine

{'alias': '  - college_medicine', 'acc,none': 0.41040462427745666, 'acc_stderr,none': 0.03750757044895537}


mmlu_global_facts

{'alias': '  - global_facts', 'acc,none': 0.37, 'acc_stderr,none': 0.04852365870939099}


mmlu_human_aging

{'alias': '  - human_aging', 'acc,none': 0.5246636771300448, 'acc_stderr,none': 0.033516951676526276}


mmlu_management

{'alias': '  - management', 'acc,none': 0.7087378640776699, 'acc_stderr,none': 0.044986763205729224}


mmlu_marketing

{'alias': '  - marketing', 'acc,none': 0.7649572649572649, 'acc_stderr,none': 0.027778835904935434}


mmlu_medical_genetics

{'alias': '  - medical_genetics', 'acc,none': 0.53, 'acc_stderr,none': 0.05016135580465919}


mmlu_miscellaneous

{'alias': '  - miscellaneous', 'acc,none': 0.7037037037037037, 'acc_stderr,none': 0.016328814422102052}


mmlu_nutrition

{'alias': '  - nutrition', 'acc,none': 0.5784313725490197, 'acc_stderr,none': 0.02827549015679146}


mmlu_professional_accounting

{'alias': '  - professional_accounting', 'acc,none': 0.35815602836879434, 'acc_stderr,none': 0.028602085862759415}


mmlu_professional_medicine

{'alias': '  - professional_medicine', 'acc,none': 0.47058823529411764, 'acc_stderr,none': 0.030320243265004137}


mmlu_virology

{'alias': '  - virology', 'acc,none': 0.42168674698795183, 'acc_stderr,none': 0.038444531817709175}


mmlu_social_sciences

{'acc,none': 0.5914852128696783, 'acc_stderr,none': np.float64(0.008658397345865867), 'alias': ' - social sciences'}


mmlu_econometrics

{'alias': '  - econometrics', 'acc,none': 0.2894736842105263, 'acc_stderr,none': 0.04266339443159394}


mmlu_high_school_geography

{'alias': '  - high_school_geography', 'acc,none': 0.6464646464646465, 'acc_stderr,none': 0.03406086723547153}


mmlu_high_school_government_and_politics

{'alias': '  - high_school_government_and_politics', 'acc,none': 0.7253886010362695, 'acc_stderr,none': 0.032210245080411544}


mmlu_high_school_macroeconomics

{'alias': '  - high_school_macroeconomics', 'acc,none': 0.46923076923076923, 'acc_stderr,none': 0.025302958890850154}


mmlu_high_school_microeconomics

{'alias': '  - high_school_microeconomics', 'acc,none': 0.4957983193277311, 'acc_stderr,none': 0.03247734334448111}


mmlu_high_school_psychology

{'alias': '  - high_school_psychology', 'acc,none': 0.6587155963302752, 'acc_stderr,none': 0.020328612816592442}


mmlu_human_sexuality

{'alias': '  - human_sexuality', 'acc,none': 0.6412213740458015, 'acc_stderr,none': 0.04206739313864908}


mmlu_professional_psychology

{'alias': '  - professional_psychology', 'acc,none': 0.5310457516339869, 'acc_stderr,none': 0.02018880445636189}


mmlu_public_relations

{'alias': '  - public_relations', 'acc,none': 0.6181818181818182, 'acc_stderr,none': 0.04653429807913509}


mmlu_security_studies

{'alias': '  - security_studies', 'acc,none': 0.6244897959183674, 'acc_stderr,none': 0.03100120903989484}


mmlu_sociology

{'alias': '  - sociology', 'acc,none': 0.7562189054726368, 'acc_stderr,none': 0.03036049015401466}


mmlu_us_foreign_policy

{'alias': '  - us_foreign_policy', 'acc,none': 0.77, 'acc_stderr,none': 0.04229525846816506}


mmlu_stem

{'acc,none': 0.40786552489692357, 'acc_stderr,none': np.float64(0.008509837136586184), 'alias': ' - stem'}


mmlu_abstract_algebra

{'alias': '  - abstract_algebra', 'acc,none': 0.32, 'acc_stderr,none': 0.046882617226215034}


mmlu_anatomy

{'alias': '  - anatomy', 'acc,none': 0.48148148148148145, 'acc_stderr,none': 0.043163785995113245}


mmlu_astronomy

{'alias': '  - astronomy', 'acc,none': 0.5263157894736842, 'acc_stderr,none': 0.04063302731486671}


mmlu_college_biology

{'alias': '  - college_biology', 'acc,none': 0.4791666666666667, 'acc_stderr,none': 0.041775789507399935}


mmlu_college_chemistry

{'alias': '  - college_chemistry', 'acc,none': 0.37, 'acc_stderr,none': 0.048523658709391}


mmlu_college_computer_science

{'alias': '  - college_computer_science', 'acc,none': 0.35, 'acc_stderr,none': 0.047937248544110196}


mmlu_college_mathematics

{'alias': '  - college_mathematics', 'acc,none': 0.3, 'acc_stderr,none': 0.046056618647183814}


mmlu_college_physics

{'alias': '  - college_physics', 'acc,none': 0.28431372549019607, 'acc_stderr,none': 0.04488482852329017}


mmlu_computer_security

{'alias': '  - computer_security', 'acc,none': 0.65, 'acc_stderr,none': 0.047937248544110196}


mmlu_conceptual_physics

{'alias': '  - conceptual_physics', 'acc,none': 0.4127659574468085, 'acc_stderr,none': 0.03218471141400351}


mmlu_electrical_engineering

{'alias': '  - electrical_engineering', 'acc,none': 0.4689655172413793, 'acc_stderr,none': 0.04158632762097828}


mmlu_elementary_mathematics

{'alias': '  - elementary_mathematics', 'acc,none': 0.30687830687830686, 'acc_stderr,none': 0.023752928712112133}


mmlu_high_school_biology

{'alias': '  - high_school_biology', 'acc,none': 0.635483870967742, 'acc_stderr,none': 0.027379871229943252}


mmlu_high_school_chemistry

{'alias': '  - high_school_chemistry', 'acc,none': 0.4482758620689655, 'acc_stderr,none': 0.034991131376767445}


mmlu_high_school_computer_science

{'alias': '  - high_school_computer_science', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}


mmlu_high_school_mathematics

{'alias': '  - high_school_mathematics', 'acc,none': 0.24074074074074073, 'acc_stderr,none': 0.026067159222275794}


mmlu_high_school_physics

{'alias': '  - high_school_physics', 'acc,none': 0.304635761589404, 'acc_stderr,none': 0.03757949922943343}


mmlu_high_school_statistics

{'alias': '  - high_school_statistics', 'acc,none': 0.375, 'acc_stderr,none': 0.033016908987210894}


mmlu_machine_learning

{'alias': '  - machine_learning', 'acc,none': 0.29464285714285715, 'acc_stderr,none': 0.0432704093257873}


