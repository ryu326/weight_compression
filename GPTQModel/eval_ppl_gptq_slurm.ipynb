{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glog\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "\n",
    "def get_wikitext2(nsamples, seed, seqlen, model):\n",
    "    from datasets import load_dataset\n",
    "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "    trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
    "\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader, testenc\n",
    "\n",
    "\n",
    "def get_ptb(nsamples, seed, seqlen, model):\n",
    "    from datasets import load_dataset\n",
    "    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n",
    "    valdata = load_dataset('ptb_text_only',\n",
    "                           'penn_treebank',\n",
    "                           split='validation')\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "    trainenc = tokenizer(\"\\n\\n\".join(traindata['sentence']),\n",
    "                         return_tensors='pt')\n",
    "    testenc = tokenizer(\"\\n\\n\".join(valdata['sentence']), return_tensors='pt')\n",
    "\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader, testenc\n",
    "\n",
    "\n",
    "def get_c4(nsamples, seed, seqlen, model):\n",
    "    from datasets import load_dataset\n",
    "    traindata = load_dataset(\n",
    "        'allenai/c4',\n",
    "        data_files={'train': 'en/c4-train.00000-of-01024.json.gz'},\n",
    "        split='train')\n",
    "    valdata = load_dataset(\n",
    "        'allenai/c4',\n",
    "        data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'},\n",
    "        split='validation')\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        while True:\n",
    "            i = random.randint(0, len(traindata) - 1)\n",
    "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
    "            if trainenc.input_ids.shape[1] >= seqlen:\n",
    "                break\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    valenc = []\n",
    "    for _ in range(256):\n",
    "        while True:\n",
    "            i = random.randint(0, len(valdata) - 1)\n",
    "            tmp = tokenizer(valdata[i]['text'], return_tensors='pt')\n",
    "            if tmp.input_ids.shape[1] >= seqlen:\n",
    "                break\n",
    "        i = random.randint(0, tmp.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        valenc.append(tmp.input_ids[:, i:j])\n",
    "    valenc = torch.hstack(valenc)\n",
    "\n",
    "    class TokenizerWrapper:\n",
    "\n",
    "        def __init__(self, input_ids):\n",
    "            self.input_ids = input_ids\n",
    "\n",
    "    valenc = TokenizerWrapper(valenc)\n",
    "\n",
    "    return trainloader, valenc\n",
    "\n",
    "\n",
    "def get_ptb_new(nsamples, seed, seqlen, model):\n",
    "    from datasets import load_dataset\n",
    "    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n",
    "    testdata = load_dataset('ptb_text_only', 'penn_treebank', split='test')\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "    trainenc = tokenizer(\" \".join(traindata['sentence']), return_tensors='pt')\n",
    "    testenc = tokenizer(\" \".join(testdata['sentence']), return_tensors='pt')\n",
    "\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "    return trainloader, testenc\n",
    "\n",
    "\n",
    "def get_c4_new(nsamples, seed, seqlen, model):\n",
    "    from datasets import load_dataset\n",
    "    traindata = load_dataset(\n",
    "        'allenai/c4',\n",
    "        data_files={'train': 'en/c4-train.00000-of-01024.json.gz'},\n",
    "        split='train')\n",
    "    valdata = load_dataset(\n",
    "        'allenai/c4',\n",
    "        data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'},\n",
    "        split='validation')\n",
    "\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        while True:\n",
    "            i = random.randint(0, len(traindata) - 1)\n",
    "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
    "            if trainenc.input_ids.shape[1] >= seqlen:\n",
    "                break\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "\n",
    "    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n",
    "    valenc = valenc.input_ids[:, :(256 * seqlen)]\n",
    "\n",
    "    class TokenizerWrapper:\n",
    "\n",
    "        def __init__(self, input_ids):\n",
    "            self.input_ids = input_ids\n",
    "\n",
    "    valenc = TokenizerWrapper(valenc)\n",
    "\n",
    "    return trainloader, valenc\n",
    "\n",
    "\n",
    "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, model=''):\n",
    "    if 'wikitext2' in name:\n",
    "        return get_wikitext2(nsamples, seed, seqlen, model)\n",
    "    if 'ptb' in name:\n",
    "        if 'new' in name:\n",
    "            return get_ptb_new(nsamples, seed, seqlen, model)\n",
    "        return get_ptb(nsamples, seed, seqlen, model)\n",
    "    if 'c4' in name:\n",
    "        if 'new' in name:\n",
    "            return get_c4_new(nsamples, seed, seqlen, model)\n",
    "        return get_c4(nsamples, seed, seqlen, model)\n",
    "\n",
    "\n",
    "def get_test_tokens(name, seed=0, seqlen=2048, model=''):\n",
    "    train_samples = 0\n",
    "    if name == 'wikitext2':\n",
    "        return get_wikitext2(train_samples, seed, seqlen,\n",
    "                             model)[1]['input_ids']\n",
    "    elif name == 'c4':\n",
    "        return get_c4(train_samples, seed, seqlen, model)[1].input_ids\n",
    "    elif name == 'c4_new':\n",
    "        return get_c4_new(train_samples, seed, seqlen, model)[1].input_ids\n",
    "    else:\n",
    "        raise Exception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    # \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "]\n",
    "for model_id in model_ids:\n",
    "    datasets = ['wikitext2']\n",
    "    seqlen = 2048\n",
    "    seed = 0\n",
    "\n",
    "    for bit in [2, 3, 4, 8]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        gptq_config = GPTQConfig(bits=bit, dataset=\"c4\", tokenizer=tokenizer)\n",
    "        quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", token=\"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\", quantization_config=gptq_config)\n",
    "        \n",
    "        if \"2-7b\" in model_id.lower():\n",
    "            hf_path = f\"./hf/meta-llama--Llama-2-7b-hf/{bit}bit\"\n",
    "        elif \"2-13b\" in model_id.lower():\n",
    "            hf_path = f\"./hf/meta-llama--Llama-2-13b-hf/{bit}bit\"\n",
    "        elif \"3-8b\" in model_id.lower():\n",
    "            hf_path = f\"./hf/meta-llama--Meta-Llama-3-8B/{bit}bit\"\n",
    "        \n",
    "        quantized_model.save_pretrained(hf_path)\n",
    "        \n",
    "        # for dataset in datasets:\n",
    "        #     input_tok = get_test_tokens(dataset,\n",
    "        #                                                 seed=seed,\n",
    "        #                                                 seqlen=seqlen,\n",
    "        #                                                 model=model_str)\n",
    "        #     nsamples = input_tok.numel() // seqlen\n",
    "        #     input_tok = input_tok[0, :(seqlen * nsamples)].view(\n",
    "        #         nsamples, seqlen)\n",
    "\n",
    "        #     loss_fct = torch.nn.CrossEntropyLoss().cuda()\n",
    "        #     acc_loss = 0.0\n",
    "        #     progress = tqdm(range(nsamples))\n",
    "        #     for ii in progress:\n",
    "        #         input = input_tok[ii, :].cuda().view(1, -1)\n",
    "        #         output = quantized_model(input,\n",
    "        #                         use_cache=False,\n",
    "        #                         output_hidden_states=False,\n",
    "        #                         output_attentions=False)[0]\n",
    "        #         shift_logits = output[:, :-1, :].contiguous()\n",
    "        #         shift_labels = input[:, 1:]\n",
    "        #         loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        #                         shift_labels.view(-1))\n",
    "        #         acc_loss += loss.item()\n",
    "        #         progress.set_description(f\"avg_loss = {acc_loss/(ii+1)}\")\n",
    "\n",
    "        #     avg_loss = acc_loss / nsamples\n",
    "\n",
    "        #     ppl = torch.exp(torch.tensor(avg_loss)).item()\n",
    "        #     glog.info(f'{dataset} perplexity: {ppl}')\n",
    "        #     print(f'{dataset} perplexity: {ppl:.3f}')\n",
    "            \n",
    "        #     try:\n",
    "        #         with open(f'{hf_path}_result.json', 'r') as f:\n",
    "        #             comp_result= json.load(f)\n",
    "        #     except:\n",
    "        #         comp_result = {}\n",
    "        #     comp_result['ppl'] = {dataset: ppl}\n",
    "        #     with open(f'./{hf_path}_result.json', 'w') as f:\n",
    "        #         json.dump(comp_result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    # \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "]\n",
    "for model_id in model_ids:\n",
    "    datasets = ['wikitext2']\n",
    "    seqlen = 2048\n",
    "    seed = 0\n",
    "\n",
    "    for bit in [2, 3, 4, 8]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        if \"2-7b\" in model_id.lower():\n",
    "            hf_path = f\"./hf/meta-llama--Llama-2-7b-hf/{bit}bit\"\n",
    "        elif \"2-13b\" in model_id.lower():\n",
    "            hf_path = f\"./hf/meta-llama--Llama-2-13b-hf/{bit}bit\"\n",
    "        elif \"3-8b\" in model_id.lower():\n",
    "            hf_path = f\"./hf/meta-llama--Meta-Llama-3-8B/{bit}bit\"\n",
    "            \n",
    "        quantized_model = AutoModelForCausalLM.from_pretrained(hf_path, device_map=\"auto\")\n",
    "        for dataset in datasets:\n",
    "            input_tok = get_test_tokens(dataset,seed=seed,seqlen=seqlen,model=model_id)\n",
    "            nsamples = input_tok.numel() // seqlen\n",
    "            input_tok = input_tok[0, :(seqlen * nsamples)].view(\n",
    "                nsamples, seqlen)\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss().cuda()\n",
    "            acc_loss = 0.0\n",
    "            progress = tqdm(range(nsamples))\n",
    "            for ii in progress:\n",
    "                input = input_tok[ii, :].cuda().view(1, -1)\n",
    "                output = quantized_model(input,\n",
    "                                use_cache=False,\n",
    "                                output_hidden_states=False,\n",
    "                                output_attentions=False)[0]\n",
    "                shift_logits = output[:, :-1, :].contiguous()\n",
    "                shift_labels = input[:, 1:]\n",
    "                loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                                shift_labels.view(-1))\n",
    "                acc_loss += loss.item()\n",
    "                progress.set_description(f\"avg_loss = {acc_loss/(ii+1)}\")\n",
    "\n",
    "            avg_loss = acc_loss / nsamples\n",
    "\n",
    "            ppl = torch.exp(torch.tensor(avg_loss)).item()\n",
    "            glog.info(f'{dataset} perplexity: {ppl}')\n",
    "            print(f'{dataset} perplexity: {ppl:.3f}')\n",
    "            \n",
    "            try:\n",
    "                with open(f'{hf_path}_result.json', 'r') as f:\n",
    "                    comp_result= json.load(f)\n",
    "            except:\n",
    "                comp_result = {}\n",
    "            comp_result['ppl'] = {dataset: ppl}\n",
    "            with open(f'./{hf_path}_result.json', 'w') as f:\n",
    "                json.dump(comp_result, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
