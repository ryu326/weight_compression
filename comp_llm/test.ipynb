{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_root \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath:\n\u001b[1;32m     26\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(project_root)\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mConfig\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    OPTForCausalLM,\n",
    "    BloomForCausalLM,    \n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "\n",
    "std = 0.012528747320175171\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import models\n",
    "from models import get_model\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "device = torch.device('cuda:5')\n",
    "\n",
    "comp_model_path = '/home/jgryu/Weight_compression/VQVAE_v2/checkpoint/nwc_ql/block_seq_ql_random__llama-3-8b-hf/block_seq_ql_random_col_16/lmbda1000_rdloss_ql_encdim512_M16_batch_size2048_total_iter1500000_lr0.0001_seed100/recent_model_loss_7.16303_bpp_5.97462_MSE_0.00115_total_iter_170000.pth.tar'\n",
    "comp_model_path = '/home/jgryu/Weight_compression/VQVAE_v2/checkpoint/nwc_ql/block_seq_ql_random__meta-llama--Llama-2-7b-hf__adapt_4096.pt/lmbda300_rdloss_ql_size16_encdim512_M16_batch_size1024_total_iter200000_lr0.0001_seed100/recent_model_loss_5.70953_bpp_5.64828_MSE_0.00301_total_iter_200000.pth.tar'\n",
    "config = os.path.join(os.path.dirname(comp_model_path), 'config.json')\n",
    "with open(config, 'r', encoding='utf-8') as file:\n",
    "    config = json.load(file)\n",
    "config = Config(**config)\n",
    "\n",
    "if not hasattr(config, \"Q\"):\n",
    "        config.Q = 4\n",
    "\n",
    "comp_model = get_model(config.architecture, config, scale=torch.zeros(1), shift=torch.zeros(1))      \n",
    "\n",
    "ckpt = torch.load(comp_model_path)\n",
    "\n",
    "if 'scale' in ckpt[\"state_dict\"]:\n",
    "    del ckpt[\"state_dict\"]['scale']\n",
    "if 'shift' in ckpt[\"state_dict\"]:\n",
    "    del ckpt[\"state_dict\"]['shift']\n",
    "\n",
    "comp_model.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 564208\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in comp_model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality_embedding.weight: 2048\n",
      "g_s.weight_in.weight: 8192\n",
      "g_s.weight_in.bias: 512\n",
      "g_s.weight_stack.0.lin_1.0.weight: 262144\n",
      "g_s.weight_stack.0.lin_1.0.bias: 512\n",
      "g_s.weight_stack.0.lin_1.1.weight: 512\n",
      "g_s.weight_stack.0.lin_1.1.bias: 512\n",
      "g_s.out.weight: 8192\n",
      "g_s.out.bias: 16\n",
      "entropy_bottleneck._matrix0: 48\n",
      "entropy_bottleneck._bias0: 48\n",
      "entropy_bottleneck._factor0: 48\n",
      "entropy_bottleneck._matrix1: 144\n",
      "entropy_bottleneck._bias1: 48\n",
      "entropy_bottleneck._factor1: 48\n",
      "entropy_bottleneck._matrix2: 144\n",
      "entropy_bottleneck._bias2: 48\n",
      "entropy_bottleneck._factor2: 48\n",
      "entropy_bottleneck._matrix3: 144\n",
      "entropy_bottleneck._bias3: 48\n",
      "entropy_bottleneck._factor3: 48\n",
      "entropy_bottleneck._matrix4: 48\n",
      "entropy_bottleneck._bias4: 16\n",
      "entropy_bottleneck.quantiles: 48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "283616"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for name, param in comp_model.named_parameters():\n",
    "    if 'g_a' in name: continue\n",
    "    print(f\"{name}: {param.numel()}\")\n",
    "    count +=  param.numel()\n",
    "    \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/Weight_compression/VQVAE_v2/recon_lm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"quip_lib::hadamard\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matmul_had import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.randn(4096, 4096)\n",
    "hw = matmul_hadU(w)\n",
    "w_hat = matmul_hadU(hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3052,  1.2386, -0.7914,  ..., -0.5430,  0.1173, -1.2914],\n",
       "        [-0.1574, -0.2344,  0.0640,  ..., -0.5559, -2.1109, -0.3195],\n",
       "        [-0.0472, -0.0049,  0.5701,  ...,  1.6446,  0.1710, -0.3300],\n",
       "        ...,\n",
       "        [-0.0803, -0.7596,  0.2771,  ...,  0.9104,  0.7887, -0.3427],\n",
       "        [ 3.2827, -0.7055, -0.2649,  ..., -0.0517,  1.6282,  0.9684],\n",
       "        [ 1.3440, -1.3535, -0.8028,  ...,  1.2691, -0.0721, -1.1403]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3052,  1.2386, -0.7914,  ..., -0.5430,  0.1173, -1.2914],\n",
       "        [-0.1574, -0.2344,  0.0640,  ..., -0.5559, -2.1109, -0.3195],\n",
       "        [-0.0472, -0.0049,  0.5701,  ...,  1.6446,  0.1710, -0.3300],\n",
       "        ...,\n",
       "        [-0.0803, -0.7596,  0.2771,  ...,  0.9104,  0.7887, -0.3427],\n",
       "        [ 3.2827, -0.7055, -0.2649,  ..., -0.0517,  1.6282,  0.9684],\n",
       "        [ 1.3440, -1.3535, -0.8028,  ...,  1.2691, -0.0721, -1.1403]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0895, -0.0894,  0.8875,  ..., -1.4697,  1.1138, -1.0637],\n",
       "        [-0.7803,  0.7130,  0.4208,  ...,  0.8103, -0.4406,  0.3364],\n",
       "        [ 0.3479,  0.0641, -1.3461,  ..., -0.4162, -2.1352,  1.6657],\n",
       "        ...,\n",
       "        [ 0.4868,  0.6456,  0.1941,  ...,  1.6534, -0.8680,  1.3202],\n",
       "        [-0.3154, -0.5766,  1.4982,  ..., -1.4923,  0.3776, -1.1650],\n",
       "        [ 1.2925,  0.4151,  1.9718,  ...,  0.4050,  0.3518,  0.1145]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
