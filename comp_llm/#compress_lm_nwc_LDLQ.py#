import argparse
import json
import os
import sys
import torch
import torch.nn as nn
import re
import math
from tqdm import tqdm
from transformers import (
    AutoModel,
    AutoModelForCausalLM,
    AutoTokenizer,
    OPTForCausalLM,
    BloomForCausalLM,    
)
from torch.utils.data import DataLoader
import logging
from matmul_had import *


notebook_dir = os.path.dirname(os.path.abspath("__file__"))
project_root = os.path.abspath(os.path.join(notebook_dir, ".."))

std = 0.012528747320175171
# std = 1.5953409671783447

if project_root not in sys.path:
    sys.path.append(project_root)

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from VQVAE_v2.models import get_model
# from models import get_model?

def parse_args():
    parser = argparse.ArgumentParser(description="Reconstruct model using specified configuration.")
    parser.add_argument("--lm_model_path", type=str)
    parser.add_argument("--comp_model_path", type=str)
    parser.add_argument("--direction", type=str, default='col')
    parser.add_argument("--batch_size", type=int, default=32768)
    parser.add_argument("--save_path", type=str)
    parser.add_argument("--no_save", action='store_true', default = False)
    parser.add_argument("--bundle", action='store_true', default = True)
    parser.add_argument("--ql", type=str, default = None)
    parser.add_argument("--hess", type=str, default = None)
    parser.add_argument("--quip_hess", type=str, default = None)
    parser.add_argument('--seed', default=0, type=int)
    parser.add_argument('--hf_path', default='hfized/quantized_hada_70b', type=str)
    parser.add_argument('--seqlen', default=2048, type=int)
    parser.add_argument('--no_use_cuda_graph', action='store_true')
    parser.add_argument('--no_use_flash_attn', action='store_true')
    parser.add_argument('--sigma_reg', default=1e-2, type=float)
    
    return parser.parse_args()

args = parse_args()

class Config:
    def __init__(self, **entries):
        self.__dict__.update(entries)

def setup_logging(log_file):
    # Remove any pre-existing handlers
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    # Configure logging settings
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[
            logging.FileHandler(log_file),  # Log to file
            logging.StreamHandler(sys.stdout)  # Log to console
        ]
    )

def get_named_linears(module):
    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}

def get_blocks(model):
    if model.__class__.__name__ in ("LlamaForCausalLM", "Qwen2ForCausalLM"):
        layers = model.model.layers
    elif model.__class__.__name__ == "LlavaLlamaForCausalLM":
        # layers = [model.model.layers, model.model.vision_tower.vision_tower.vision_model.encoder.layers]
        layers = model.model.layers
    elif isinstance(model, OPTForCausalLM):
        layers = model.model.decoder.layers
    elif isinstance(model, BloomForCausalLM):
        layers = model.transformer.h
    elif "mpt" in str(model.__class__).lower():
        layers = model.transformer.blocks
    elif "falcon" in str(model.__class__).lower():
        layers = model.transformer.h
    elif "bigcode" in str(model.__class__).lower():
        layers = model.transformer.h
    elif "neox" in str(model.__class__).lower():
        layers = model.gpt_neox.layers
    elif model.__class__.__name__ == "LlavaLlamaModel":
        layers = model.llm.model.layers
    else:
        raise NotImplementedError(type(model))
    return layers

def flat_to_sym(V, N):
    A = torch.zeros(N, N, dtype=V.dtype, device=V.device)
    idxs = torch.tril_indices(N, N, device=V.device)
    A[idxs.unbind()] = V
    A[idxs[1, :], idxs[0, :]] = V
    return A

def regularize_H(H, n, sigma_reg):
    H.div_(torch.diag(H).mean())
    idx = torch.arange(n)
    H[idx, idx] += sigma_reg
    return H

def pseudo_compress_tensor_gptq(W, model, direction, bs, ch_rank, hess_weight):

    W = W.to(device)
    Losses = torch.zeros_like(W, device=W.device)
    Q = torch.zeros_like(W, device=W.device)

    H = flat_to_sym(hess_weight['flatH'], hess_weight['n']).to(W.device)
    mu = hess_weight['mu'].to(device)
    H.add_(mu[None, :] * mu[:, None])
    n = hess_weight['n']
    H = regularize_H(H, n, args.sigma_reg)
    assert torch.isfinite(H).all()
    
    H = torch.linalg.cholesky(H)
    H = torch.cholesky_inverse(H)
    H = torch.linalg.cholesky(H, upper=True)
    Hinv = H
    assert torch.isfinite(H).all()
    
    rows = W.shape[0]
    columns = W.shape[1]

    assert direction == 'col'
    
    num_pixels = 0
    bpp_loss = 0
    bpp = 0

    for i1 in range(0, columns, bs):
        i2 = min(i1 + bs, columns)
        count = i2 - i1

        W1 = W[:, i1:i2].clone()
        Q1 = torch.zeros_like(W1, device=W1.device)
        Err1 = torch.zeros_like(W1, device=W1.device)
        Losses1 = torch.zeros_like(W1, device=W1.device)
        Hinv1 = Hinv[i1:i2, i1:i2]

        for i in range(count):
            w = W1[:, i]
            d = Hinv1[i, i]
            
            assert w.size(-1) == rows
            if args.bundle:
                w_reshape = w.reshape(1, -1, model.input_size)  # (row, col) --> (row, -1, inputsize)
            else :
                w_reshape = w.reshape(-1, model.input_size)
                
            assert torch.isfinite(w_reshape).all()
            data = {}
            data['weight_block'] = w_reshape.cuda()
            out = model(data)
            q = out["x_hat"].flatten()
            assert torch.isfinite(q).all()
            
            num_pixels += w.numel()
            if isinstance(out["likelihoods"], dict):
                bpp_loss += sum(
                    (torch.log(likelihoods).sum() / -math.log(2))
                    for likelihoods in out["likelihoods"].values()
                ).item()
            else :
                bpp_loss += (torch.log(out["likelihoods"]).sum() / -math.log(2)).item()

            Q1[:, i] = q
            Losses1[:, i] = (w - q)**2 / d**2

            err1 = (w - q) / d
            assert torch.isfinite(err1).all()
            W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))
            Err1[:, i] = err1

        Q[:, i1:i2] = Q1
        Losses[:, i1:i2] = Losses1 / 2

        W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])

    return {'w_hat': Q.cpu(),
            'bpp_loss_sum': bpp_loss,
            'num_pixels': num_pixels,
            'bpp': bpp}

def pseudo_compress_model(model, comp_model, direction, bs, in_ch_rank, hess, args):
    
    mse_total = 0
    n_total = 0
    total_bpp_loss = 0
    total_num_pixels = 0
    bpp = 0
    
    comp_model.to(device)
    comp_model.update()
    
    with torch.no_grad():
        # layers = model.model.layers
        layers = get_blocks(model)
        for i in tqdm(range(len(layers)), desc="pseudo compress quantization..."):
            named_linears = get_named_linears(layers[i])
            
            hess = {}
            hess['qkv'] = torch.load(f'{args.quip_hess}/{i}_qkv.pt')
            hess['o'] = torch.load(f'{args.quip_hess}/{i}_o.pt')
            hess['up'] = torch.load(f'{args.quip_hess}/{i}_up.pt')
            hess['down'] = torch.load(f'{args.quip_hess}/{i}_down.pt')
            
            for n, m in named_linears.items():
                
                in_ch_rank_weight = None
                if in_ch_rank is not None:
                    in_ch_rank_weight = in_ch_rank[i][n]
                
                # hess_weight = None
                # if hess is not None:
                #     hess_weight = hess[i][n].to(device)

                hess_weight = None
                if 'q_proj' in n or 'k_proj' in n or 'v_proj' in n:
                    hess_weight = hess['qkv']
                elif 'o_proj' in n:
                    hess_weight = hess['o']
                elif 'up_proj' in n or 'gate' in n:
                    hess_weight = hess['up']
                elif 'down_proj' in n:
                    hess_weight = hess['down']


                
                out = pseudo_compress_tensor(m.weight.data.to(device), comp_model, direction, bs, in_ch_rank_weight, hess_weight)                
                mse = ((m.weight.data - out['w_hat'])**2).sum().item()
                
                m.weight.data = out['w_hat']
                
                num = m.weight.data.numel()
                mse_total += mse
                n_total += num
                
                total_bpp_loss += out['bpp_loss_sum']
                total_num_pixels += out['num_pixels']
                # bpp += out['bpp']

                # print(f"layer{i}_{n} | mse: {mse/num/std**2}, bpp_loss: {out['bpp_loss']}, bpp: {out['bpp']}")
                logging.info(f"layer{i}_{n} | mse: {mse/num/std**2}, bpp_loss: {out['bpp_loss_sum']/out['num_pixels']}, bpp: {out['bpp']}")
                
        mse_total = mse_total / n_total / std **2 
        total_bpp_loss /= total_num_pixels
        # bpp /= total_num_pixels
    
    # print(f'#### Total | mse: {mse_total}, bpp_loss: {bpp_loss}, bpp: {bpp} ####')
    logging.info(f'#### Total | mse: {mse_total}, bpp_loss: {total_bpp_loss}, bpp: {bpp} ####')
    
    return {'mse': mse_total,
            'bpp_loss': total_bpp_loss,
            'bpp': bpp}

def get_model_weight_stats(model, args, size):
    dataset_stats = {}
    weights = []
    layers = get_blocks(model)
    for i in tqdm(range(len(layers)), desc="calculating model weight mean & std"):
        named_linears = get_named_linears(layers[i])
        for n, m in named_linears.items():
            w = m.weight.data.detach()
            if args.direction == 'col':
                w = w.T    
            w = w.reshape(-1, size)
            weights.append(w)
    
    weights = torch.cat(weights, dim = 0)
    
    mean = weights.mean(0)
    std = weights.std(0)
    
    return mean, std

def get_model_weight_stats(model, args, size):
    dataset_stats = {}
    weights = []
    layers = get_blocks(model)
    for i in tqdm(range(len(layers)), desc="calculating model weight mean & std"):
        named_linears = get_named_linears(layers[i])
        for n, m in named_linears.items():
            w = m.weight.data.detach()
            if args.direction == 'col':
                w = w.T    
            w = w.reshape(-1, size)
            weights.append(w)
    
    weights = torch.cat(weights, dim = 0)
    
    mean = weights.mean(0)
    std = weights.std(0)
    
    # mean = torch.zeros(1)
    # std = torch.zeros(1)
    return mean, std

if __name__ == "__main__":

    device = torch.device(f"cuda" if torch.cuda.is_available() else "cpu")

    in_ch_rank = None
    if args.ql is not None:
        assert args.direction == 'col'
        in_ch_rank = torch.load(args.ql)
    
    hess = None
    if args.hess is not None:
        # assert args.direction == 'row'
        hess = torch.load(args.hess)
    
    net = AutoModelForCausalLM.from_pretrained(args.lm_model_path, local_files_only=True)
    tokenizer = AutoTokenizer.from_pretrained(args.lm_model_path, local_files_only=True)

    config = os.path.join(os.path.dirname(args.comp_model_path), 'config.json')
    with open(config, 'r', encoding='utf-8') as file:
        config = json.load(file)
    config = Config(**config)
    
    shift, scale = get_model_weight_stats(net, args, config.input_size)    
    
    save_directory = (
        f"{args.save_path}/{args.lm_model_path.split('/')[-1]}/{os.path.join(*args.comp_model_path.split('/')[-3:])}"
    )
    log_file = save_directory.replace('_reconstructed',  '_eval')
    os.makedirs(log_file, mode=0o777 ,exist_ok=True)
    setup_logging(log_file + f'/{(args.direction).upper()}_compression_log.txt')

    if config.architecture == 'nwc_ql' and not hasattr(config, "Q"):
        config.Q = 4
        
    comp_model = get_model(config.architecture, config, scale=scale.to(device), shift=shift.to(device))      

    ckpt = torch.load(args.comp_model_path)
    
    if 'scale' in ckpt["state_dict"]:
        del ckpt["state_dict"]['scale']
    if 'shift' in ckpt["state_dict"]:
        del ckpt["state_dict"]['shift']
    
    comp_model.load_state_dict(ckpt["state_dict"])
   
    result = pseudo_compress_model(net, comp_model, args.direction, args.batch_size, in_ch_rank, hess, args)

    save_directory += f"/{(args.direction).upper()}_MSE{round(result['mse'], 5)}_bpploss{round(result['bpp_loss'], 4)}_bpp{round(result['bpp'], 4)}"

    net = net.to(dtype=torch.bfloat16)
    
    if args.no_save == False:
        print(f'## Strart saving {save_directory}')
        net.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print('## End saving')
        
    # from eval_ppl import main
    # import random
    # args.hf_path = save_directory
    # args.no_use_cuda_graph = True
    
    # torch.set_grad_enabled(False)
    # random.seed(args.seed)
    # torch.random.manual_seed(args.seed)
    # main(args)

    
    # python ../../model_lm_eval/eval_ppl.py \
    #     --hf_path $pretrain_path \
    #     --seqlen 2048 \
    #     --no_use_cuda_graph | tee -a "$log_path"