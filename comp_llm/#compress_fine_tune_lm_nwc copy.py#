import argparse
import json
import os
import sys
import torch
import torch.nn as nn
import re
import math
from tqdm import tqdm
from transformers import (
    AutoModel,
    AutoModelForCausalLM,
    AutoTokenizer,
    OPTForCausalLM,
    BloomForCausalLM,    
)
from torch.utils.data import DataLoader
import logging

notebook_dir = os.path.dirname(os.path.abspath("__file__"))
project_root = os.path.abspath(os.path.join(notebook_dir, ".."))

std = 0.012528747320175171

if project_root not in sys.path:
    sys.path.append(project_root)

import models
from models import get_model

def parse_args():
    parser = argparse.ArgumentParser(description="Reconstruct model using specified configuration.")
    parser.add_argument("--lm_model_path", type=str)
    parser.add_argument("--comp_model_path", type=str)
    parser.add_argument("--direction", type=str, default='col')
    parser.add_argument("--batch_size", type=int, default=32768)
    parser.add_argument("--save_path", type=str)
    parser.add_argument("--no_save", action='store_true', default = False)
    parser.add_argument("--bundle", action='store_true', default = True)
    parser.add_argument("--ql", type=str, default = None)
    parser.add_argument("--hesseigen", type=str, default = None)
    parser.add_argument('--seed', default=0, type=int)
    parser.add_argument('--hf_path', default='hfized/quantized_hada_70b', type=str)
    parser.add_argument('--seqlen', default=2048, type=int)
    parser.add_argument('--no_use_cuda_graph', action='store_true')
    parser.add_argument('--no_use_flash_attn', action='store_true')
    
    return parser.parse_args()

args = parse_args()

class Config:
    def __init__(self, **entries):
        self.__dict__.update(entries)

def setup_logging(log_file):
    # Remove any pre-existing handlers
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    # Configure logging settings
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[
            logging.FileHandler(log_file),  # Log to file
            logging.StreamHandler(sys.stdout)  # Log to console
        ]
    )

def get_named_linears(module):
    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}

def get_blocks(model):
    if model.__class__.__name__ in ("LlamaForCausalLM", "Qwen2ForCausalLM"):
        layers = model.model.layers
    elif model.__class__.__name__ == "LlavaLlamaForCausalLM":
        # layers = [model.model.layers, model.model.vision_tower.vision_tower.vision_model.encoder.layers]
        layers = model.model.layers
    elif isinstance(model, OPTForCausalLM):
        layers = model.model.decoder.layers
    elif isinstance(model, BloomForCausalLM):
        layers = model.transformer.h
    elif "mpt" in str(model.__class__).lower():
        layers = model.transformer.blocks
    elif "falcon" in str(model.__class__).lower():
        layers = model.transformer.h
    elif "bigcode" in str(model.__class__).lower():
        layers = model.transformer.h
    elif "neox" in str(model.__class__).lower():
        layers = model.gpt_neox.layers
    elif model.__class__.__name__ == "LlavaLlamaModel":
        layers = model.llm.model.layers
    else:
        raise NotImplementedError(type(model))
    return layers

def finetune_decoder_layer(layer, name, device, train_dl, valid_dl, args):
    layer = layer.to(device)

    susv_params, params = utils.extract_susv_params(layer)
    optim = utils.get_susv_adam(susv_params, params, args)

    best_loss = utils.calculate_mse_loss(layer, valid_dl, device)
    best_sd = copy.deepcopy(layer.state_dict())
    glog.info(f'layer {name} initial loss {best_loss}')
    scaler = torch.cuda.amp.GradScaler(enabled=True)
    worse_ct = 0
    position_ids = None

    for epoch in range(args.ft_epochs):
        for bidx, (source, targets) in enumerate(train_dl):
            if position_ids is None:
                position_ids = torch.arange(source.shape[1], device=device).unsqueeze(0)
            with torch.autocast(device_type='cuda',
                                dtype=torch.float16,
                                enabled=True):
                output = layer(source.to(device), position_ids=position_ids)[0]
                loss = nn.MSELoss()(output, targets.to(device))
            scaler.scale(loss).backward()
            if bidx % args.ft_update_freq == args.ft_update_freq - 1 or bidx == len(
                    train_dl) - 1:
                scaler.step(optim)
                scaler.update()
                optim.zero_grad()

        if epoch % args.ft_valid_freq == (args.ft_valid_freq - 1):
            test_loss = utils.calculate_mse_loss(layer, valid_dl, device)
            if test_loss < best_loss:
                glog.info(
                    f'layer {name} @ epoch {epoch} new loss {test_loss} old loss {best_loss} BETTER'
                )
                best_loss = test_loss
                best_sd = copy.deepcopy(layer.state_dict())
                worse_ct = 0
            else:
                glog.info(
                    f'layer {name} @ epoch {epoch} new loss {test_loss} old loss {best_loss} WORSE'
                )
                worse_ct += 1
                if worse_ct >= args.ft_early_stop:
                    break

    del optim, train_dl, valid_dl

    layer.load_state_dict(best_sd)
    utils.clean()
    layer = layer.cpu()

def pseudo_compress_tensor(w, model, direction, bs, ch_rank, hess_eigen):
    if direction == 'col':
        w = w.T
    
    ori_shape = w.shape
    
    if args.bundle:
        w = w.reshape(ori_shape[0], -1, model.input_size)  # (row, col) --> (row, -1, inputsize)
    else :
        w = w.reshape(-1, model.input_size)
        
    w_hat = torch.zeros(w.shape, dtype=w.dtype, device=w.device)

    num_pixels = 0
    bpp_loss = 0
    bpp = 0

    if ch_rank is not None:
        # if direction == 'col':
        #     ch_rank= ch_rank.T
        ch_rank = ch_rank.reshape(-1, )
        # print(ch_rank.shape)

    if hess_eigen is not None:
        R = hess_eigen['eigenvectors'].size(0)
        hess_eigen = hess_eigen['eigenvectors'].to(device)
        hess_eigen = hess_eigen.reshape(1, R, -1, model.input_size).repeat(bs, 1, 1, 1)
        

    for start_idx in range(0, w.shape[0], bs):
        end_idx = min(start_idx + bs, w.shape[0])
        batch = w[start_idx:end_idx]

        data = {}
        data['weight_block'] = batch.cuda()
        
        if ch_rank is not None:
            ql = ch_rank[start_idx:end_idx]
            data['q_level'] = ql.to(device)
        if hess_eigen is not None:
            data['hesseigen'] = hess_eigen
        
        out = model(data)
        x_hat = out["x_hat"].cpu()

        w_hat[start_idx:end_idx] = x_hat
        
        num_pixels += batch.numel()
        if isinstance(out["likelihoods"], dict):
            bpp_loss += sum(
                (torch.log(likelihoods).sum() / -math.log(2))
                for likelihoods in out["likelihoods"].values()
            ).item()
        else :
            bpp_loss += (torch.log(out["likelihoods"]).sum() / -math.log(2)).item()

        # out_enc = model.compress(data)
        # try:
        #     out_dec = model.decompress(out_enc["strings"][0], out_enc["shape"], data["q_level"])
        # except:
        #     out_dec = model.decompress(out_enc["strings"][0], out_enc["shape"])
        
        # for s in out_enc["strings"]:
        #     bpp += len(s[0]) * 8.0

    w_hat = w_hat.reshape(ori_shape).cpu()

    if direction == 'col':
        w_hat = w_hat.T
        
    # bpp_loss /= num_pixels
    # bpp /= num_pixels
    
    return {'w_hat': w_hat,
            'bpp_loss_sum': bpp_loss,
            'num_pixels': num_pixels,
            'bpp': bpp}

def pseudo_compress_model(model, comp_model, direction, bs, in_ch_rank, hesseigen):
    
    mse_total = 0
    n_total = 0
    total_bpp_loss = 0
    total_num_pixels = 0
    bpp = 0
    
    comp_model.to(device)
    comp_model.update()
    
    with torch.no_grad():
        # layers = model.model.layers
        layers = get_blocks(model)
        for i in tqdm(range(len(layers)), desc="pseudo compress quantization..."):
            named_linears = get_named_linears(layers[i])
            for n, m in named_linears.items():
                
                in_ch_rank_weight = None
                if in_ch_rank is not None:
                    in_ch_rank_weight = in_ch_rank[i][n]
                
                hesseigen_weight = None
                if hesseigen is not None:
                    hesseigen_weight = hesseigen[i][n]
                
                out = pseudo_compress_tensor(m.weight.data, comp_model, direction, bs, in_ch_rank_weight, hesseigen_weight)                
                mse = ((m.weight.data - out['w_hat'])**2).sum().item()
                
                m.weight.data = out['w_hat']
                
                num = m.weight.data.numel()
                mse_total += mse
                n_total += num
                
                total_bpp_loss += out['bpp_loss_sum']
                total_num_pixels += out['num_pixels']
                # bpp += out['bpp']

                # print(f"layer{i}_{n} | mse: {mse/num/std**2}, bpp_loss: {out['bpp_loss']}, bpp: {out['bpp']}")
                logging.info(f"layer{i}_{n} | mse: {mse/num/std**2}, bpp_loss: {out['bpp_loss_sum']/out['num_pixels']}, bpp: {out['bpp']}")
                
        mse_total = mse_total / n_total / std **2 
        total_bpp_loss /= total_num_pixels
        # bpp /= total_num_pixels
    
    # print(f'#### Total | mse: {mse_total}, bpp_loss: {bpp_loss}, bpp: {bpp} ####')
    logging.info(f'#### Total | mse: {mse_total}, bpp_loss: {total_bpp_loss}, bpp: {bpp} ####')
    
    return {'mse': mse_total,
            'bpp_loss': total_bpp_loss,
            'bpp': bpp}

def get_model_weight_stats(model, args, size):
    dataset_stats = {}
    weights = []
    layers = get_blocks(model)
    for i in tqdm(range(len(layers)), desc="calculating model weight mean & std"):
        named_linears = get_named_linears(layers[i])
        for n, m in named_linears.items():
            w = m.weight.data.detach()
            if args.direction == 'col':
                w = w.T    
            w = w.reshape(-1, size)
            weights.append(w)
    
    weights = torch.cat(weights, dim = 0)
    
    mean = weights.mean(0)
    std = weights.std(0)
    
    return mean, std

if __name__ == "__main__":

    device = torch.device(f"cuda" if torch.cuda.is_available() else "cpu")

    in_ch_rank = None
    if args.ql is not None:
        assert args.direction == 'col'
        in_ch_rank = torch.load(args.ql)
    
    hesseigen = None
    if args.hesseigen is not None:
        assert args.direction == 'row'
        hesseigen = torch.load(args.hesseigen)
    
    net = AutoModelForCausalLM.from_pretrained(args.lm_model_path, local_files_only=True)
    tokenizer = AutoTokenizer.from_pretrained(args.lm_model_path, local_files_only=True)

    import models
    config = os.path.join(os.path.dirname(args.comp_model_path), 'config.json')
    with open(config, 'r', encoding='utf-8') as file:
        config = json.load(file)
    config = Config(**config)
    
    shift, scale = get_model_weight_stats(net, args, config.input_size)    
    
    save_directory = (
        f"{args.save_path}/{args.lm_model_path.split('/')[-1]}/{os.path.join(*args.comp_model_path.split('/')[-3:])}"
    )
    log_file = save_directory.replace('_reconstructed',  '_eval')
    os.makedirs(log_file, mode=0o777 ,exist_ok=True)
    setup_logging(log_file + f'/{(args.direction).upper()}_compression_log.txt')

    # if args.dataset == 'seq':
    #     scale=torch.zeros(128, config.input_size)
    #     shift=torch.zeros(128, config.input_size)
    # else:
    #     scale=torch.zeros(config.input_size)
    #     shift=torch.zeros(config.input_size)
    if config.architecture == 'nwc_ql' and not hasattr(config, "Q"):
        config.Q = 4
        
    comp_model = get_model(config.architecture, config, scale=scale.to(device), shift=shift.to(device))      

    ckpt = torch.load(args.comp_model_path)
    
    if 'scale' in ckpt["state_dict"]:
        del ckpt["state_dict"]['scale']
    if 'shift' in ckpt["state_dict"]:
        del ckpt["state_dict"]['shift']
    
    comp_model.load_state_dict(ckpt["state_dict"])
   
    result = pseudo_compress_model(net, comp_model, args.direction, args.batch_size, in_ch_rank, hesseigen)

    save_directory += f"/{(args.direction).upper()}_MSE{round(result['mse'], 5)}_bpploss{round(result['bpp_loss'], 4)}_bpp{round(result['bpp'], 4)}"

    net = net.to(dtype=torch.bfloat16)
    
    if args.no_save == False:
        print(f'## Strart saving {save_directory}')
        net.save_pretrained(save_directory)
        tokenizer.save_pretrained(save_directory)
        print('## End saving')
        
    # from eval_ppl import main
    # import random
    # args.hf_path = save_directory
    # args.no_use_cuda_graph = True
    
    # torch.set_grad_enabled(False)
    # random.seed(args.seed)
    # torch.random.manual_seed(args.seed)
    # main(args)

    
    # python ../../model_lm_eval/eval_ppl.py \
    #     --hf_path $pretrain_path \
    #     --seqlen 2048 \
    #     --no_use_cuda_graph | tee -a "$log_path"