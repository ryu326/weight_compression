Running with lmbda=50
/home/jgryu/Weight_compression/comp_llm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")
/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.73it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  2.86it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:01<00:01,  2.28it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:01,  2.69it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.17it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  3.56it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  3.45it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  3.14it/s]
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:29,  1.04it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:02<00:32,  1.07s/it]calculating model weight mean & std:   9%|▉         | 3/32 [00:03<00:36,  1.27s/it]calculating model weight mean & std:  12%|█▎        | 4/32 [00:05<00:38,  1.38s/it]calculating model weight mean & std:  16%|█▌        | 5/32 [00:06<00:37,  1.38s/it]calculating model weight mean & std:  19%|█▉        | 6/32 [00:07<00:35,  1.36s/it]calculating model weight mean & std:  22%|██▏       | 7/32 [00:08<00:28,  1.15s/it]calculating model weight mean & std:  25%|██▌       | 8/32 [00:09<00:24,  1.00s/it]calculating model weight mean & std:  28%|██▊       | 9/32 [00:09<00:19,  1.15it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:10<00:16,  1.31it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:10<00:14,  1.44it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:11<00:12,  1.60it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:11<00:10,  1.79it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:12<00:09,  1.95it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:12<00:08,  2.09it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:12<00:07,  2.22it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:13<00:07,  2.06it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:14<00:07,  1.93it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:14<00:07,  1.85it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:15<00:06,  1.79it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:15<00:05,  1.84it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:16<00:05,  1.89it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:04,  1.94it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:04,  1.95it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:03,  1.98it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:02,  2.02it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:18<00:02,  2.05it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:19<00:01,  2.06it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:19<00:01,  2.07it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:20<00:00,  2.03it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:20<00:00,  2.07it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:21<00:00,  2.08it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:21<00:00,  1.51it/s]
pseudo compress quantization...:   0%|          | 0/32 [00:00<?, ?it/s]2025-03-02 20:40:40 - INFO - layer0_self_attn.q_proj | mse: 0.03822934477703775, bpp_loss: 2.863302767276764, bpp: 0
2025-03-02 20:40:40 - INFO - layer0_self_attn.k_proj | mse: 0.04163288309307898, bpp_loss: 3.37219101190567, bpp: 0
2025-03-02 20:40:41 - INFO - layer0_self_attn.v_proj | mse: 0.034546117280206796, bpp_loss: 2.0789365768432617, bpp: 0
2025-03-02 20:40:42 - INFO - layer0_self_attn.o_proj | mse: 0.03518167917023709, bpp_loss: 2.1752718091011047, bpp: 0
2025-03-02 20:40:47 - INFO - layer0_mlp.gate_proj | mse: 0.03855828243991976, bpp_loss: 2.5674123082842146, bpp: 0
2025-03-02 20:40:52 - INFO - layer0_mlp.up_proj | mse: 0.03780512421177049, bpp_loss: 2.46112973349435, bpp: 0
2025-03-02 20:40:57 - INFO - layer0_mlp.down_proj | mse: 0.037771051680247074, bpp_loss: 2.4546360799244473, bpp: 0
pseudo compress quantization...:   3%|▎         | 1/32 [00:18<09:42, 18.80s/it]2025-03-02 20:40:58 - INFO - layer1_self_attn.q_proj | mse: 0.04248312452863885, bpp_loss: 3.107744872570038, bpp: 0
2025-03-02 20:40:59 - INFO - layer1_self_attn.k_proj | mse: 0.04702375205321898, bpp_loss: 3.729934513568878, bpp: 0
2025-03-02 20:41:00 - INFO - layer1_self_attn.v_proj | mse: 0.03571024458704709, bpp_loss: 2.1858800649642944, bpp: 0
2025-03-02 20:41:01 - INFO - layer1_self_attn.o_proj | mse: 0.03602689423693883, bpp_loss: 2.2528128027915955, bpp: 0
scripts/recon0.sh: line 23: 2563070 Killed                  taskset -c 0-31 python compress_lm_nwc.py --lm_model_path "$lm_model_path" --comp_model_path $comp_model --direction col --save_path "$save_path" --batch_size "$batch_size" --ql "$ql"
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda50_rdloss_ql_encdim512_M16_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_3.84823_bpp_4.61283_MSE_0.01614_total_iter_95000.pth.tar/COL_MSE0.03903_bpploss2.6134_bpp0
I0302 20:41:09.042036 2564787 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.68s/it]
W0302 20:41:15.984529 2564787 big_modeling.py:414] Some parameters are on the meta device device because they were offloaded to the cpu.
I0302 20:41:15.999872 2564787 config.py:54] PyTorch version 2.4.1 available.
Running with lmbda=100
/home/jgryu/Weight_compression/comp_llm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")
/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.14it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  3.18it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.45it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.57it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.60it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  3.82it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.03it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.72it/s]
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:17,  1.76it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:18,  1.64it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.71it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.81it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:13,  1.96it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.97it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:12,  2.02it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:11,  2.04it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:10,  2.10it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:10,  2.11it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:09,  2.18it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:05<00:08,  2.23it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:08,  2.24it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:06<00:07,  2.27it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:07,  2.34it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:07<00:06,  2.38it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:07<00:06,  2.43it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:08<00:05,  2.42it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:08<00:05,  2.39it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:09<00:07,  1.69it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:10<00:07,  1.41it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:07,  1.27it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:07,  1.24it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:13<00:05,  1.35it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:04,  1.47it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:14<00:03,  1.59it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.67it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:15<00:02,  1.72it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.79it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:16<00:01,  1.85it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.89it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:17<00:00,  1.84it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:17<00:00,  1.84it/s]
pseudo compress quantization...:   0%|          | 0/32 [00:00<?, ?it/s]2025-03-02 20:42:05 - INFO - layer0_self_attn.q_proj | mse: 0.018977459761298422, bpp_loss: 3.3660439252853394, bpp: 0
2025-03-02 20:42:05 - INFO - layer0_self_attn.k_proj | mse: 0.02041236784302716, bpp_loss: 3.8803369998931885, bpp: 0
2025-03-02 20:42:05 - INFO - layer0_self_attn.v_proj | mse: 0.01724444165360162, bpp_loss: 2.5797988772392273, bpp: 0
2025-03-02 20:42:06 - INFO - layer0_self_attn.o_proj | mse: 0.017603511249317652, bpp_loss: 2.67601215839386, bpp: 0
2025-03-02 20:42:09 - INFO - layer0_mlp.gate_proj | mse: 0.019286128886637575, bpp_loss: 3.0658580916268483, bpp: 0
2025-03-02 20:42:11 - INFO - layer0_mlp.up_proj | mse: 0.018909415680878297, bpp_loss: 2.95988130569458, bpp: 0
scripts/recon0.sh: line 23: 2565075 Killed                  taskset -c 0-31 python compress_lm_nwc.py --lm_model_path "$lm_model_path" --comp_model_path $comp_model --direction col --save_path "$save_path" --batch_size "$batch_size" --ql "$ql"
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda100_rdloss_ql_encdim512_M16_batch_size2048_total_iter1500000_lr0.0001_seed100/best_loss_model_loss_4.39201_bpp_5.10767_MSE_0.0081_total_iter_190000.pth.tar/COL_MSE0.0195_bpploss3.1127_bpp0
I0302 20:42:16.387557 2565909 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.72s/it]
W0302 20:42:23.407399 2565909 big_modeling.py:414] Some parameters are on the meta device device because they were offloaded to the cpu.
I0302 20:42:23.421217 2565909 config.py:54] PyTorch version 2.4.1 available.
Running with lmbda=200
/home/jgryu/Weight_compression/comp_llm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")
/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  9.00it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  9.98it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00, 10.06it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.90it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.67it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.06it/s]
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:14,  2.13it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:14,  2.14it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:13,  2.12it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:12,  2.16it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:12,  2.22it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:11,  2.23it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:10,  2.28it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:10,  2.29it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:09,  2.30it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:04<00:09,  2.32it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:04<00:08,  2.34it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:05<00:08,  2.38it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:05<00:07,  2.42it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:06<00:07,  2.41it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:06<00:07,  2.38it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:06<00:06,  2.29it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:07<00:06,  2.23it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:07<00:06,  2.21it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:08<00:05,  2.17it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:08<00:05,  2.14it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:09<00:05,  2.07it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:09<00:04,  2.05it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:10<00:04,  2.02it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:10<00:03,  2.04it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:11<00:03,  2.06it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:11<00:02,  2.08it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:12<00:02,  2.11it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:12<00:01,  2.12it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:13<00:01,  2.13it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:13<00:00,  2.12it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:14<00:00,  2.15it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:14<00:00,  2.19it/s]
pseudo compress quantization...:   0%|          | 0/32 [00:00<?, ?it/s]2025-03-02 20:43:06 - INFO - layer0_self_attn.q_proj | mse: 0.009871408924785594, bpp_loss: 3.916435480117798, bpp: 0
2025-03-02 20:43:07 - INFO - layer0_self_attn.k_proj | mse: 0.010488095133954142, bpp_loss: 4.4975186586380005, bpp: 0
2025-03-02 20:43:07 - INFO - layer0_self_attn.v_proj | mse: 0.00941104423562609, bpp_loss: 3.066488027572632, bpp: 0
2025-03-02 20:43:09 - INFO - layer0_self_attn.o_proj | mse: 0.009451694088440392, bpp_loss: 3.161941111087799, bpp: 0
pseudo compress quantization...:   0%|          | 0/32 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/home/jgryu/Weight_compression/comp_llm/compress_lm_nwc.py", line 534, in <module>
    result = pseudo_compress_model(net, comp_model, args.direction, args.batch_size, args)
  File "/home/jgryu/Weight_compression/comp_llm/compress_lm_nwc.py", line 434, in pseudo_compress_model
    out = pseudo_compress_tensor(m.weight.data, comp_model, direction, bs, in_ch_rank_weight, hesseigen_weight)         
  File "/home/jgryu/Weight_compression/comp_llm/compress_lm_nwc.py", line 158, in pseudo_compress_tensor
    out = model(data)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/Weight_compression/VQVAE_v2/models/nwc_ql.py", line 142, in forward
    y = self.g_a(x_shift, q_embed)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/Weight_compression/VQVAE_v2/models/nwc_ql.py", line 86, in forward
    x = layer(x)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/Weight_compression/VQVAE_v2/models/nwc_ql.py", line 47, in forward
    res = self.lin_1(x)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/container.py", line 219, in forward
    input = module(input)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 202, in forward
    return F.layer_norm(
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/functional.py", line 2576, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacity of 23.58 GiB of which 894.19 MiB is free. Process 2566348 has 16.61 GiB memory in use. Including non-PyTorch memory, this process has 6.08 GiB memory in use. Of the allocated memory 4.12 GiB is allocated by PyTorch, and 1.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda200_rdloss_ql_encdim512_M16_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_4.97679_bpp_5.524_MSE_0.00426_total_iter_95000.pth.tar/COL_MSE0.00981_bpploss3.6177_bpp0
I0302 20:43:16.145381 2567381 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.20s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.58it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.49it/s]
W0302 20:43:19.025370 2567381 big_modeling.py:414] Some parameters are on the meta device device because they were offloaded to the cpu.
I0302 20:43:19.039114 2567381 config.py:54] PyTorch version 2.4.1 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.65625:   0%|          | 0/141 [00:05<?, ?it/s]avg_loss = 1.65625:   1%|          | 1/141 [00:05<12:57,  5.55s/it]avg_loss = 1.9609375:   1%|          | 1/141 [00:07<12:57,  5.55s/it]avg_loss = 1.9609375:   1%|▏         | 2/141 [00:07<07:17,  3.15s/it]avg_loss = 2.0833333333333335:   1%|▏         | 2/141 [00:08<07:17,  3.15s/it]avg_loss = 2.0833333333333335:   2%|▏         | 3/141 [00:08<05:02,  2.19s/it]avg_loss = 2.033203125:   2%|▏         | 3/141 [00:09<05:02,  2.19s/it]       avg_loss = 2.033203125:   3%|▎         | 4/141 [00:09<03:59,  1.75s/it]avg_loss = 1.9796875:   3%|▎         | 4/141 [00:10<03:59,  1.75s/it]  avg_loss = 1.9796875:   4%|▎         | 5/141 [00:10<03:23,  1.50s/it]avg_loss = 1.8815104166666667:   4%|▎         | 5/141 [00:11<03:23,  1.50s/it]avg_loss = 1.8815104166666667:   4%|▍         | 6/141 [00:11<03:00,  1.34s/it]avg_loss = 1.8180803571428572:   4%|▍         | 6/141 [00:12<03:00,  1.34s/it]avg_loss = 1.8180803571428572:   5%|▍         | 7/141 [00:12<02:46,  1.24s/it]avg_loss = 1.8134765625:   5%|▍         | 7/141 [00:13<02:46,  1.24s/it]      avg_loss = 1.8134765625:   6%|▌         | 8/141 [00:13<02:37,  1.18s/it]avg_loss = 1.8463541666666667:   6%|▌         | 8/141 [00:14<02:37,  1.18s/it]avg_loss = 1.8463541666666667:   6%|▋         | 9/141 [00:14<02:30,  1.14s/it]avg_loss = 1.846875:   6%|▋         | 9/141 [00:15<02:30,  1.14s/it]          avg_loss = 1.846875:   7%|▋         | 10/141 [00:15<02:24,  1.11s/it]avg_loss = 1.84375:   7%|▋         | 10/141 [00:16<02:24,  1.11s/it] avg_loss = 1.84375:   8%|▊         | 11/141 [00:16<02:21,  1.09s/it]avg_loss = 1.8671875:   8%|▊         | 11/141 [00:17<02:21,  1.09s/it]avg_loss = 1.8671875:   9%|▊         | 12/141 [00:17<02:19,  1.08s/it]avg_loss = 1.8798076923076923:   9%|▊         | 12/141 [00:18<02:19,  1.08s/it]avg_loss = 1.8798076923076923:   9%|▉         | 13/141 [00:18<02:16,  1.07s/it]avg_loss = 1.8973214285714286:   9%|▉         | 13/141 [00:19<02:16,  1.07s/it]avg_loss = 1.8973214285714286:  10%|▉         | 14/141 [00:19<02:14,  1.06s/it]avg_loss = 1.90625:  10%|▉         | 14/141 [00:20<02:14,  1.06s/it]           avg_loss = 1.90625:  11%|█         | 15/141 [00:20<02:12,  1.05s/it]avg_loss = 1.9296875:  11%|█         | 15/141 [00:21<02:12,  1.05s/it]avg_loss = 1.9296875:  11%|█▏        | 16/141 [00:21<02:11,  1.05s/it]avg_loss = 1.9324448529411764:  11%|█▏        | 16/141 [00:22<02:11,  1.05s/it]avg_loss = 1.9324448529411764:  12%|█▏        | 17/141 [00:22<02:09,  1.05s/it]avg_loss = 1.9357638888888888:  12%|█▏        | 17/141 [00:23<02:09,  1.05s/it]avg_loss = 1.9357638888888888:  13%|█▎        | 18/141 [00:23<02:08,  1.04s/it]avg_loss = 1.921875:  13%|█▎        | 18/141 [00:24<02:08,  1.04s/it]          avg_loss = 1.921875:  13%|█▎        | 19/141 [00:24<02:07,  1.04s/it]avg_loss = 1.9203125:  13%|█▎        | 19/141 [00:25<02:07,  1.04s/it]avg_loss = 1.9203125:  14%|█▍        | 20/141 [00:25<02:07,  1.05s/it]avg_loss = 1.9255952380952381:  14%|█▍        | 20/141 [00:26<02:07,  1.05s/it]avg_loss = 1.9255952380952381:  15%|█▍        | 21/141 [00:26<02:06,  1.05s/it]avg_loss = 1.9282670454545454:  15%|█▍        | 21/141 [00:27<02:06,  1.05s/it]avg_loss = 1.9282670454545454:  16%|█▌        | 22/141 [00:27<02:04,  1.05s/it]avg_loss = 1.9300271739130435:  16%|█▌        | 22/141 [00:29<02:04,  1.05s/it]avg_loss = 1.9300271739130435:  16%|█▋        | 23/141 [00:29<02:05,  1.06s/it]avg_loss = 1.9348958333333333:  16%|█▋        | 23/141 [00:30<02:05,  1.06s/it]avg_loss = 1.9348958333333333:  17%|█▋        | 24/141 [00:30<02:04,  1.06s/it]avg_loss = 1.940625:  17%|█▋        | 24/141 [00:31<02:04,  1.06s/it]          avg_loss = 1.940625:  18%|█▊        | 25/141 [00:31<02:03,  1.07s/it]avg_loss = 1.9513221153846154:  18%|█▊        | 25/141 [00:32<02:03,  1.07s/it]avg_loss = 1.9513221153846154:  18%|█▊        | 26/141 [00:32<02:02,  1.06s/it]avg_loss = 1.9641203703703705:  18%|█▊        | 26/141 [00:33<02:02,  1.06s/it]avg_loss = 1.9641203703703705:  19%|█▉        | 27/141 [00:33<02:01,  1.07s/it]avg_loss = 1.9698660714285714:  19%|█▉        | 27/141 [00:34<02:01,  1.07s/it]avg_loss = 1.9698660714285714:  20%|█▉        | 28/141 [00:34<01:59,  1.06s/it]avg_loss = 1.9665948275862069:  20%|█▉        | 28/141 [00:35<01:59,  1.06s/it]avg_loss = 1.9665948275862069:  21%|██        | 29/141 [00:35<01:57,  1.05s/it]avg_loss = 1.9572916666666667:  21%|██        | 29/141 [00:36<01:57,  1.05s/it]avg_loss = 1.9572916666666667:  21%|██▏       | 30/141 [00:36<01:55,  1.04s/it]avg_loss = 1.9438004032258065:  21%|██▏       | 30/141 [00:37<01:55,  1.04s/it]avg_loss = 1.9438004032258065:  22%|██▏       | 31/141 [00:37<01:53,  1.03s/it]avg_loss = 1.932373046875:  22%|██▏       | 31/141 [00:38<01:53,  1.03s/it]    avg_loss = 1.932373046875:  23%|██▎       | 32/141 [00:38<01:54,  1.05s/it]avg_loss = 1.9308712121212122:  23%|██▎       | 32/141 [00:39<01:54,  1.05s/it]avg_loss = 1.9308712121212122:  23%|██▎       | 33/141 [00:39<01:55,  1.07s/it]avg_loss = 1.9287683823529411:  23%|██▎       | 33/141 [00:40<01:55,  1.07s/it]avg_loss = 1.9287683823529411:  24%|██▍       | 34/141 [00:40<01:54,  1.07s/it]avg_loss = 1.93125:  24%|██▍       | 34/141 [00:41<01:54,  1.07s/it]           avg_loss = 1.93125:  25%|██▍       | 35/141 [00:41<01:55,  1.09s/it]avg_loss = 1.9144965277777777:  25%|██▍       | 35/141 [00:42<01:55,  1.09s/it]avg_loss = 1.9144965277777777:  26%|██▌       | 36/141 [00:42<01:54,  1.09s/it]avg_loss = 1.8984375:  26%|██▌       | 36/141 [00:43<01:54,  1.09s/it]         avg_loss = 1.8984375:  26%|██▌       | 37/141 [00:43<01:50,  1.06s/it]avg_loss = 1.8838404605263157:  26%|██▌       | 37/141 [00:44<01:50,  1.06s/it]avg_loss = 1.8838404605263157:  27%|██▋       | 38/141 [00:44<01:47,  1.04s/it]avg_loss = 1.8697916666666667:  27%|██▋       | 38/141 [00:46<01:47,  1.04s/it]avg_loss = 1.8697916666666667:  28%|██▊       | 39/141 [00:46<01:49,  1.07s/it]avg_loss = 1.861328125:  28%|██▊       | 39/141 [00:47<01:49,  1.07s/it]       avg_loss = 1.861328125:  28%|██▊       | 40/141 [00:47<01:57,  1.16s/it]avg_loss = 1.8669969512195121:  28%|██▊       | 40/141 [00:48<01:57,  1.16s/it]avg_loss = 1.8669969512195121:  29%|██▉       | 41/141 [00:48<02:06,  1.27s/it]avg_loss = 1.8835565476190477:  29%|██▉       | 41/141 [00:50<02:06,  1.27s/it]avg_loss = 1.8835565476190477:  30%|██▉       | 42/141 [00:50<02:14,  1.36s/it]avg_loss = 1.8997093023255813:  30%|██▉       | 42/141 [00:52<02:14,  1.36s/it]avg_loss = 1.8997093023255813:  30%|███       | 43/141 [00:52<02:23,  1.47s/it]avg_loss = 1.9037642045454546:  30%|███       | 43/141 [00:53<02:23,  1.47s/it]avg_loss = 1.9037642045454546:  31%|███       | 44/141 [00:53<02:27,  1.52s/it]avg_loss = 1.9083333333333334:  31%|███       | 44/141 [00:55<02:27,  1.52s/it]avg_loss = 1.9083333333333334:  32%|███▏      | 45/141 [00:55<02:31,  1.58s/it]avg_loss = 1.913383152173913:  32%|███▏      | 45/141 [00:57<02:31,  1.58s/it] avg_loss = 1.913383152173913:  33%|███▎      | 46/141 [00:57<02:29,  1.58s/it]avg_loss = 1.9195478723404256:  33%|███▎      | 46/141 [00:58<02:29,  1.58s/it]avg_loss = 1.9195478723404256:  33%|███▎      | 47/141 [00:58<02:28,  1.58s/it]avg_loss = 1.9225260416666667:  33%|███▎      | 47/141 [01:00<02:28,  1.58s/it]avg_loss = 1.9225260416666667:  34%|███▍      | 48/141 [01:00<02:25,  1.56s/it]avg_loss = 1.921875:  34%|███▍      | 48/141 [01:01<02:25,  1.56s/it]          avg_loss = 1.921875:  35%|███▍      | 49/141 [01:01<02:21,  1.54s/it]avg_loss = 1.92203125:  35%|███▍      | 49/141 [01:03<02:21,  1.54s/it]avg_loss = 1.92203125:  35%|███▌      | 50/141 [01:03<02:15,  1.48s/it]avg_loss = 1.9160539215686274:  35%|███▌      | 50/141 [01:04<02:15,  1.48s/it]avg_loss = 1.9160539215686274:  36%|███▌      | 51/141 [01:04<02:18,  1.54s/it]avg_loss = 1.9116586538461537:  36%|███▌      | 51/141 [01:06<02:18,  1.54s/it]avg_loss = 1.9116586538461537:  37%|███▋      | 52/141 [01:06<02:20,  1.58s/it]avg_loss = 1.9049233490566038:  37%|███▋      | 52/141 [01:08<02:20,  1.58s/it]avg_loss = 1.9049233490566038:  38%|███▊      | 53/141 [01:08<02:20,  1.60s/it]avg_loss = 1.9020543981481481:  38%|███▊      | 53/141 [01:09<02:20,  1.60s/it]avg_loss = 1.9020543981481481:  38%|███▊      | 54/141 [01:09<02:21,  1.63s/it]avg_loss = 1.894034090909091:  38%|███▊      | 54/141 [01:11<02:21,  1.63s/it] avg_loss = 1.894034090909091:  39%|███▉      | 55/141 [01:11<02:19,  1.62s/it]avg_loss = 1.8861607142857142:  39%|███▉      | 55/141 [01:12<02:19,  1.62s/it]avg_loss = 1.8861607142857142:  40%|███▉      | 56/141 [01:12<02:15,  1.59s/it]avg_loss = 1.8811677631578947:  40%|███▉      | 56/141 [01:14<02:15,  1.59s/it]avg_loss = 1.8811677631578947:  40%|████      | 57/141 [01:14<02:12,  1.57s/it]avg_loss = 1.8780980603448276:  40%|████      | 57/141 [01:15<02:12,  1.57s/it]avg_loss = 1.8780980603448276:  41%|████      | 58/141 [01:15<02:05,  1.51s/it]avg_loss = 1.8804290254237288:  41%|████      | 58/141 [01:17<02:05,  1.51s/it]avg_loss = 1.8804290254237288:  42%|████▏     | 59/141 [01:17<02:00,  1.48s/it]avg_loss = 1.8858072916666666:  42%|████▏     | 59/141 [01:18<02:00,  1.48s/it]avg_loss = 1.8858072916666666:  43%|████▎     | 60/141 [01:18<01:57,  1.45s/it]avg_loss = 1.891265368852459:  43%|████▎     | 60/141 [01:20<01:57,  1.45s/it] avg_loss = 1.891265368852459:  43%|████▎     | 61/141 [01:20<01:57,  1.47s/it]avg_loss = 1.8983114919354838:  43%|████▎     | 61/141 [01:21<01:57,  1.47s/it]avg_loss = 1.8983114919354838:  44%|████▍     | 62/141 [01:21<02:02,  1.55s/it]avg_loss = 1.8893849206349207:  44%|████▍     | 62/141 [01:23<02:02,  1.55s/it]avg_loss = 1.8893849206349207:  45%|████▍     | 63/141 [01:23<01:55,  1.48s/it]avg_loss = 1.8870849609375:  45%|████▍     | 63/141 [01:24<01:55,  1.48s/it]   avg_loss = 1.8870849609375:  45%|████▌     | 64/141 [01:24<01:59,  1.55s/it]avg_loss = 1.8844951923076922:  45%|████▌     | 64/141 [01:26<01:59,  1.55s/it]avg_loss = 1.8844951923076922:  46%|████▌     | 65/141 [01:26<01:58,  1.57s/it]avg_loss = 1.8786695075757576:  46%|████▌     | 65/141 [01:27<01:58,  1.57s/it]avg_loss = 1.8786695075757576:  47%|████▋     | 66/141 [01:27<01:55,  1.54s/it]avg_loss = 1.8758162313432836:  47%|████▋     | 66/141 [01:29<01:55,  1.54s/it]avg_loss = 1.8758162313432836:  48%|████▊     | 67/141 [01:29<01:56,  1.58s/it]avg_loss = 1.8728170955882353:  48%|████▊     | 67/141 [01:31<01:56,  1.58s/it]avg_loss = 1.8728170955882353:  48%|████▊     | 68/141 [01:31<01:52,  1.55s/it]avg_loss = 1.8703577898550725:  48%|████▊     | 68/141 [01:32<01:52,  1.55s/it]avg_loss = 1.8703577898550725:  49%|████▉     | 69/141 [01:32<01:50,  1.53s/it]avg_loss = 1.8714285714285714:  49%|████▉     | 69/141 [01:34<01:50,  1.53s/it]avg_loss = 1.8714285714285714:  50%|████▉     | 70/141 [01:34<01:46,  1.50s/it]avg_loss = 1.8752200704225352:  50%|████▉     | 70/141 [01:35<01:46,  1.50s/it]avg_loss = 1.8752200704225352:  50%|█████     | 71/141 [01:35<01:48,  1.55s/it]avg_loss = 1.8778211805555556:  50%|█████     | 71/141 [01:37<01:48,  1.55s/it]avg_loss = 1.8778211805555556:  51%|█████     | 72/141 [01:37<01:49,  1.58s/it]avg_loss = 1.8763912671232876:  51%|█████     | 72/141 [01:38<01:49,  1.58s/it]avg_loss = 1.8763912671232876:  52%|█████▏    | 73/141 [01:38<01:46,  1.56s/it]avg_loss = 1.8779560810810811:  52%|█████▏    | 73/141 [01:40<01:46,  1.56s/it]avg_loss = 1.8779560810810811:  52%|█████▏    | 74/141 [01:40<01:48,  1.61s/it]avg_loss = 1.8780208333333333:  52%|█████▏    | 74/141 [01:42<01:48,  1.61s/it]avg_loss = 1.8780208333333333:  53%|█████▎    | 75/141 [01:42<01:46,  1.61s/it]avg_loss = 1.8768503289473684:  53%|█████▎    | 75/141 [01:43<01:46,  1.61s/it]avg_loss = 1.8768503289473684:  54%|█████▍    | 76/141 [01:43<01:44,  1.61s/it]avg_loss = 1.8781452922077921:  54%|█████▍    | 76/141 [01:45<01:44,  1.61s/it]avg_loss = 1.8781452922077921:  55%|█████▍    | 77/141 [01:45<01:42,  1.60s/it]avg_loss = 1.880508814102564:  55%|█████▍    | 77/141 [01:46<01:42,  1.60s/it] avg_loss = 1.880508814102564:  55%|█████▌    | 78/141 [01:46<01:39,  1.57s/it]avg_loss = 1.8843947784810127:  55%|█████▌    | 78/141 [01:48<01:39,  1.57s/it]avg_loss = 1.8843947784810127:  56%|█████▌    | 79/141 [01:48<01:37,  1.57s/it]avg_loss = 1.88125:  56%|█████▌    | 79/141 [01:50<01:37,  1.57s/it]           avg_loss = 1.88125:  57%|█████▋    | 80/141 [01:50<01:38,  1.62s/it]avg_loss = 1.8802083333333333:  57%|█████▋    | 80/141 [01:51<01:38,  1.62s/it]avg_loss = 1.8802083333333333:  57%|█████▋    | 81/141 [01:51<01:37,  1.63s/it]avg_loss = 1.8793826219512195:  57%|█████▋    | 81/141 [01:53<01:37,  1.63s/it]avg_loss = 1.8793826219512195:  58%|█████▊    | 82/141 [01:53<01:37,  1.65s/it]avg_loss = 1.8774472891566265:  58%|█████▊    | 82/141 [01:55<01:37,  1.65s/it]avg_loss = 1.8774472891566265:  59%|█████▉    | 83/141 [01:55<01:36,  1.66s/it]avg_loss = 1.8751860119047619:  59%|█████▉    | 83/141 [01:56<01:36,  1.66s/it]avg_loss = 1.8751860119047619:  60%|█████▉    | 84/141 [01:56<01:30,  1.59s/it]avg_loss = 1.872610294117647:  60%|█████▉    | 84/141 [01:58<01:30,  1.59s/it] avg_loss = 1.872610294117647:  60%|██████    | 85/141 [01:58<01:29,  1.60s/it]avg_loss = 1.8740915697674418:  60%|██████    | 85/141 [01:59<01:29,  1.60s/it]avg_loss = 1.8740915697674418:  61%|██████    | 86/141 [01:59<01:27,  1.59s/it]avg_loss = 1.8758979885057472:  61%|██████    | 86/141 [02:01<01:27,  1.59s/it]avg_loss = 1.8758979885057472:  62%|██████▏   | 87/141 [02:01<01:23,  1.55s/it]avg_loss = 1.8767755681818181:  62%|██████▏   | 87/141 [02:02<01:23,  1.55s/it]avg_loss = 1.8767755681818181:  62%|██████▏   | 88/141 [02:02<01:23,  1.58s/it]avg_loss = 1.8857092696629214:  62%|██████▏   | 88/141 [02:04<01:23,  1.58s/it]avg_loss = 1.8857092696629214:  63%|██████▎   | 89/141 [02:04<01:24,  1.63s/it]avg_loss = 1.8932291666666667:  63%|██████▎   | 89/141 [02:06<01:24,  1.63s/it]avg_loss = 1.8932291666666667:  64%|██████▍   | 90/141 [02:06<01:22,  1.61s/it]avg_loss = 1.8964629120879122:  64%|██████▍   | 90/141 [02:07<01:22,  1.61s/it]avg_loss = 1.8964629120879122:  65%|██████▍   | 91/141 [02:07<01:21,  1.62s/it]avg_loss = 1.9014945652173914:  65%|██████▍   | 91/141 [02:09<01:21,  1.62s/it]avg_loss = 1.9014945652173914:  65%|██████▌   | 92/141 [02:09<01:19,  1.62s/it]avg_loss = 1.9065860215053763:  65%|██████▌   | 92/141 [02:11<01:19,  1.62s/it]avg_loss = 1.9065860215053763:  66%|██████▌   | 93/141 [02:11<01:16,  1.60s/it]avg_loss = 1.907496675531915:  66%|██████▌   | 93/141 [02:12<01:16,  1.60s/it] avg_loss = 1.907496675531915:  67%|██████▋   | 94/141 [02:12<01:15,  1.60s/it]avg_loss = 1.911266447368421:  67%|██████▋   | 94/141 [02:14<01:15,  1.60s/it]avg_loss = 1.911266447368421:  67%|██████▋   | 95/141 [02:14<01:13,  1.60s/it]avg_loss = 1.9119466145833333:  67%|██████▋   | 95/141 [02:15<01:13,  1.60s/it]avg_loss = 1.9119466145833333:  68%|██████▊   | 96/141 [02:15<01:09,  1.55s/it]avg_loss = 1.9139819587628866:  68%|██████▊   | 96/141 [02:17<01:09,  1.55s/it]avg_loss = 1.9139819587628866:  69%|██████▉   | 97/141 [02:17<01:09,  1.58s/it]avg_loss = 1.9106345663265305:  69%|██████▉   | 97/141 [02:19<01:09,  1.58s/it]avg_loss = 1.9106345663265305:  70%|██████▉   | 98/141 [02:19<01:09,  1.61s/it]avg_loss = 1.9116950757575757:  70%|██████▉   | 98/141 [02:20<01:09,  1.61s/it]avg_loss = 1.9116950757575757:  70%|███████   | 99/141 [02:20<01:07,  1.61s/it]avg_loss = 1.913828125:  70%|███████   | 99/141 [02:22<01:07,  1.61s/it]       avg_loss = 1.913828125:  71%|███████   | 100/141 [02:22<01:06,  1.63s/it]avg_loss = 1.9127475247524752:  71%|███████   | 100/141 [02:23<01:06,  1.63s/it]avg_loss = 1.9127475247524752:  72%|███████▏  | 101/141 [02:23<01:05,  1.64s/it]avg_loss = 1.9128370098039216:  72%|███████▏  | 101/141 [02:25<01:05,  1.64s/it]avg_loss = 1.9128370098039216:  72%|███████▏  | 102/141 [02:25<01:03,  1.62s/it]avg_loss = 1.9114077669902914:  72%|███████▏  | 102/141 [02:27<01:03,  1.62s/it]avg_loss = 1.9114077669902914:  73%|███████▎  | 103/141 [02:27<01:01,  1.61s/it]avg_loss = 1.9140625:  73%|███████▎  | 103/141 [02:28<01:01,  1.61s/it]         avg_loss = 1.9140625:  74%|███████▍  | 104/141 [02:28<00:57,  1.56s/it]avg_loss = 1.9130208333333334:  74%|███████▍  | 104/141 [02:29<00:57,  1.56s/it]avg_loss = 1.9130208333333334:  74%|███████▍  | 105/141 [02:29<00:54,  1.51s/it]avg_loss = 1.9121462264150944:  74%|███████▍  | 105/141 [02:31<00:54,  1.51s/it]avg_loss = 1.9121462264150944:  75%|███████▌  | 106/141 [02:31<00:53,  1.52s/it]avg_loss = 1.909827686915888:  75%|███████▌  | 106/141 [02:33<00:53,  1.52s/it] avg_loss = 1.909827686915888:  76%|███████▌  | 107/141 [02:33<00:52,  1.56s/it]avg_loss = 1.9076244212962963:  76%|███████▌  | 107/141 [02:34<00:52,  1.56s/it]avg_loss = 1.9076244212962963:  77%|███████▋  | 108/141 [02:34<00:51,  1.56s/it]avg_loss = 1.9052465596330275:  77%|███████▋  | 108/141 [02:36<00:51,  1.56s/it]avg_loss = 1.9052465596330275:  77%|███████▋  | 109/141 [02:36<00:50,  1.57s/it]avg_loss = 1.902840909090909:  77%|███████▋  | 109/141 [02:38<00:50,  1.57s/it] avg_loss = 1.902840909090909:  78%|███████▊  | 110/141 [02:38<00:50,  1.61s/it]avg_loss = 1.9049831081081081:  78%|███████▊  | 110/141 [02:39<00:50,  1.61s/it]avg_loss = 1.9049831081081081:  79%|███████▊  | 111/141 [02:39<00:47,  1.59s/it]avg_loss = 1.9047154017857142:  79%|███████▊  | 111/141 [02:41<00:47,  1.59s/it]avg_loss = 1.9047154017857142:  79%|███████▉  | 112/141 [02:41<00:47,  1.63s/it]avg_loss = 1.9058351769911503:  79%|███████▉  | 112/141 [02:42<00:47,  1.63s/it]avg_loss = 1.9058351769911503:  80%|████████  | 113/141 [02:42<00:45,  1.62s/it]avg_loss = 1.9069353070175439:  80%|████████  | 113/141 [02:44<00:45,  1.62s/it]avg_loss = 1.9069353070175439:  81%|████████  | 114/141 [02:44<00:42,  1.57s/it]avg_loss = 1.9061820652173913:  81%|████████  | 114/141 [02:46<00:42,  1.57s/it]avg_loss = 1.9061820652173913:  82%|████████▏ | 115/141 [02:46<00:41,  1.60s/it]avg_loss = 1.9048356681034482:  82%|████████▏ | 115/141 [02:47<00:41,  1.60s/it]avg_loss = 1.9048356681034482:  82%|████████▏ | 116/141 [02:47<00:40,  1.64s/it]avg_loss = 1.9069845085470085:  82%|████████▏ | 116/141 [02:49<00:40,  1.64s/it]avg_loss = 1.9069845085470085:  83%|████████▎ | 117/141 [02:49<00:39,  1.66s/it]avg_loss = 1.906448622881356:  83%|████████▎ | 117/141 [02:51<00:39,  1.66s/it] avg_loss = 1.906448622881356:  84%|████████▎ | 118/141 [02:51<00:38,  1.67s/it]avg_loss = 1.9048713235294117:  84%|████████▎ | 118/141 [02:52<00:38,  1.67s/it]avg_loss = 1.9048713235294117:  84%|████████▍ | 119/141 [02:52<00:36,  1.67s/it]avg_loss = 1.9029296875:  84%|████████▍ | 119/141 [02:54<00:36,  1.67s/it]      avg_loss = 1.9029296875:  85%|████████▌ | 120/141 [02:54<00:34,  1.65s/it]avg_loss = 1.9026988636363635:  85%|████████▌ | 120/141 [02:55<00:34,  1.65s/it]avg_loss = 1.9026988636363635:  86%|████████▌ | 121/141 [02:55<00:32,  1.63s/it]avg_loss = 1.903048155737705:  86%|████████▌ | 121/141 [02:57<00:32,  1.63s/it] avg_loss = 1.903048155737705:  87%|████████▋ | 122/141 [02:57<00:30,  1.60s/it]avg_loss = 1.902756605691057:  87%|████████▋ | 122/141 [02:59<00:30,  1.60s/it]avg_loss = 1.902756605691057:  87%|████████▋ | 123/141 [02:59<00:28,  1.57s/it]avg_loss = 1.9029107862903225:  87%|████████▋ | 123/141 [03:00<00:28,  1.57s/it]avg_loss = 1.9029107862903225:  88%|████████▊ | 124/141 [03:00<00:26,  1.58s/it]avg_loss = 1.90175:  88%|████████▊ | 124/141 [03:02<00:26,  1.58s/it]           avg_loss = 1.90175:  89%|████████▊ | 125/141 [03:02<00:25,  1.61s/it]avg_loss = 1.9020957341269842:  89%|████████▊ | 125/141 [03:03<00:25,  1.61s/it]avg_loss = 1.9020957341269842:  89%|████████▉ | 126/141 [03:03<00:23,  1.59s/it]avg_loss = 1.9018208661417322:  89%|████████▉ | 126/141 [03:05<00:23,  1.59s/it]avg_loss = 1.9018208661417322:  90%|█████████ | 127/141 [03:05<00:22,  1.63s/it]avg_loss = 1.9007568359375:  90%|█████████ | 127/141 [03:07<00:22,  1.63s/it]   avg_loss = 1.9007568359375:  91%|█████████ | 128/141 [03:07<00:21,  1.63s/it]avg_loss = 1.900859980620155:  91%|█████████ | 128/141 [03:08<00:21,  1.63s/it]avg_loss = 1.900859980620155:  91%|█████████▏| 129/141 [03:08<00:19,  1.65s/it]avg_loss = 1.9017427884615385:  91%|█████████▏| 129/141 [03:10<00:19,  1.65s/it]avg_loss = 1.9017427884615385:  92%|█████████▏| 130/141 [03:10<00:18,  1.64s/it]avg_loss = 1.9026121183206106:  92%|█████████▏| 130/141 [03:12<00:18,  1.64s/it]avg_loss = 1.9026121183206106:  93%|█████████▎| 131/141 [03:12<00:16,  1.63s/it]avg_loss = 1.9031723484848484:  93%|█████████▎| 131/141 [03:13<00:16,  1.63s/it]avg_loss = 1.9031723484848484:  94%|█████████▎| 132/141 [03:13<00:14,  1.60s/it]avg_loss = 1.900375939849624:  94%|█████████▎| 132/141 [03:15<00:14,  1.60s/it] avg_loss = 1.900375939849624:  94%|█████████▍| 133/141 [03:15<00:12,  1.60s/it]avg_loss = 1.8958722014925373:  94%|█████████▍| 133/141 [03:16<00:12,  1.60s/it]avg_loss = 1.8958722014925373:  95%|█████████▌| 134/141 [03:16<00:11,  1.63s/it]avg_loss = 1.898263888888889:  95%|█████████▌| 134/141 [03:18<00:11,  1.63s/it] avg_loss = 1.898263888888889:  96%|█████████▌| 135/141 [03:18<00:09,  1.62s/it]avg_loss = 1.9017693014705883:  96%|█████████▌| 135/141 [03:20<00:09,  1.62s/it]avg_loss = 1.9017693014705883:  96%|█████████▋| 136/141 [03:20<00:08,  1.67s/it]avg_loss = 1.9031706204379562:  96%|█████████▋| 136/141 [03:21<00:08,  1.67s/it]avg_loss = 1.9031706204379562:  97%|█████████▋| 137/141 [03:21<00:06,  1.66s/it]avg_loss = 1.9021173007246377:  97%|█████████▋| 137/141 [03:23<00:06,  1.66s/it]avg_loss = 1.9021173007246377:  98%|█████████▊| 138/141 [03:23<00:04,  1.63s/it]avg_loss = 1.902428057553957:  98%|█████████▊| 138/141 [03:25<00:04,  1.63s/it] avg_loss = 1.902428057553957:  99%|█████████▊| 139/141 [03:25<00:03,  1.61s/it]avg_loss = 1.9030133928571429:  99%|█████████▊| 139/141 [03:26<00:03,  1.61s/it]avg_loss = 1.9030133928571429:  99%|█████████▉| 140/141 [03:26<00:01,  1.56s/it]avg_loss = 1.9042553191489362:  99%|█████████▉| 140/141 [03:28<00:01,  1.56s/it]avg_loss = 1.9042553191489362: 100%|██████████| 141/141 [03:28<00:00,  1.53s/it]avg_loss = 1.9042553191489362: 100%|██████████| 141/141 [03:28<00:00,  1.48s/it]
I0302 20:47:18.407119 2567381 eval_ppl.py:105] wikitext2 perplexity: 6.714405536651611
wikitext2 perplexity: 6.714
Running with lmbda=300
/home/jgryu/Weight_compression/comp_llm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")
/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.50it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  9.34it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  9.09it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.62it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.22it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.51it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.59it/s]
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:12,  2.58it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:11,  2.58it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:11,  2.55it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:10,  2.57it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:01<00:10,  2.58it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:10,  2.49it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:02<00:09,  2.54it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:09,  2.55it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:03<00:10,  2.27it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:04<00:10,  2.06it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:04<00:10,  1.92it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:05<00:11,  1.79it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:10,  1.76it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:06<00:10,  1.74it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:10,  1.69it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:07<00:09,  1.71it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:08<00:08,  1.69it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:08,  1.67it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:09<00:07,  1.67it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:07,  1.64it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:10<00:06,  1.62it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:06,  1.62it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:05,  1.65it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.70it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:04,  1.71it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.71it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.75it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:15<00:02,  1.75it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.78it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:16<00:01,  1.87it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.94it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.88it/s]
pseudo compress quantization...:   0%|          | 0/32 [00:00<?, ?it/s]2025-03-02 20:48:05 - INFO - layer0_self_attn.q_proj | mse: 0.006776120283705778, bpp_loss: 4.222776412963867, bpp: 0
2025-03-02 20:48:06 - INFO - layer0_self_attn.k_proj | mse: 0.00728827395435941, bpp_loss: 4.833387970924377, bpp: 0
2025-03-02 20:48:06 - INFO - layer0_self_attn.v_proj | mse: 0.006424120196120033, bpp_loss: 3.357487678527832, bpp: 0
2025-03-02 20:48:08 - INFO - layer0_self_attn.o_proj | mse: 0.00644283798816174, bpp_loss: 3.4526625871658325, bpp: 0
2025-03-02 20:48:13 - INFO - layer0_mlp.gate_proj | mse: 0.006589228135845033, bpp_loss: 3.8542799949645996, bpp: 0
2025-03-02 20:48:17 - INFO - layer0_mlp.up_proj | mse: 0.0065500261835206575, bpp_loss: 3.739786148071289, bpp: 0
2025-03-02 20:48:22 - INFO - layer0_mlp.down_proj | mse: 0.006548018119156752, bpp_loss: 3.7336818831307546, bpp: 0
pseudo compress quantization...:   3%|▎         | 1/32 [00:18<09:37, 18.62s/it]2025-03-02 20:48:24 - INFO - layer1_self_attn.q_proj | mse: 0.006864725606434084, bpp_loss: 4.494054913520813, bpp: 0
2025-03-02 20:48:24 - INFO - layer1_self_attn.k_proj | mse: 0.007288912027985347, bpp_loss: 5.276386618614197, bpp: 0
2025-03-02 20:48:25 - INFO - layer1_self_attn.v_proj | mse: 0.006460220967839537, bpp_loss: 3.4596117734909058, bpp: 0
2025-03-02 20:48:26 - INFO - layer1_self_attn.o_proj | mse: 0.006474325219589273, bpp_loss: 3.5332435369491577, bpp: 0
2025-03-02 20:48:31 - INFO - layer1_mlp.gate_proj | mse: 0.006595166659249337, bpp_loss: 3.8648599897112166, bpp: 0
2025-03-02 20:48:36 - INFO - layer1_mlp.up_proj | mse: 0.006555392334085868, bpp_loss: 3.754194532121931, bpp: 0
2025-03-02 20:48:41 - INFO - layer1_mlp.down_proj | mse: 0.006552865699102017, bpp_loss: 3.7488328899656023, bpp: 0
pseudo compress quantization...:   6%|▋         | 2/32 [00:37<09:16, 18.54s/it]2025-03-02 20:48:42 - INFO - layer2_self_attn.q_proj | mse: 0.006830858454535684, bpp_loss: 4.419785499572754, bpp: 0
2025-03-02 20:48:43 - INFO - layer2_self_attn.k_proj | mse: 0.007296869479663193, bpp_loss: 5.340187072753906, bpp: 0
2025-03-02 20:48:43 - INFO - layer2_self_attn.v_proj | mse: 0.006418397088523934, bpp_loss: 3.3706202507019043, bpp: 0
2025-03-02 20:48:45 - INFO - layer2_self_attn.o_proj | mse: 0.006451988297117587, bpp_loss: 3.485027551651001, bpp: 0
2025-03-02 20:48:49 - INFO - layer2_mlp.gate_proj | mse: 0.006610009036059467, bpp_loss: 3.8990086146763394, bpp: 0
2025-03-02 20:48:54 - INFO - layer2_mlp.up_proj | mse: 0.006554417686192835, bpp_loss: 3.7460481098720004, bpp: 0
2025-03-02 20:48:59 - INFO - layer2_mlp.down_proj | mse: 0.00655393056917793, bpp_loss: 3.7509957211358205, bpp: 0
pseudo compress quantization...:   9%|▉         | 3/32 [00:55<08:51, 18.33s/it]2025-03-02 20:49:00 - INFO - layer3_self_attn.q_proj | mse: 0.006839833492416075, bpp_loss: 4.460689663887024, bpp: 0
2025-03-02 20:49:01 - INFO - layer3_self_attn.k_proj | mse: 0.0073062327212479, bpp_loss: 5.4237635135650635, bpp: 0
2025-03-02 20:49:01 - INFO - layer3_self_attn.v_proj | mse: 0.006459248285796818, bpp_loss: 3.468001127243042, bpp: 0
2025-03-02 20:49:03 - INFO - layer3_self_attn.o_proj | mse: 0.00649208191775854, bpp_loss: 3.5817941427230835, bpp: 0
2025-03-02 20:49:08 - INFO - layer3_mlp.gate_proj | mse: 0.006638988152882434, bpp_loss: 3.9772169249398366, bpp: 0
2025-03-02 20:49:13 - INFO - layer3_mlp.up_proj | mse: 0.0065473311062045495, bpp_loss: 3.727827617100307, bpp: 0
2025-03-02 20:49:18 - INFO - layer3_mlp.down_proj | mse: 0.00654339361148934, bpp_loss: 3.7269759689058577, bpp: 0
pseudo compress quantization...:  12%|█▎        | 4/32 [01:14<08:39, 18.56s/it]2025-03-02 20:49:19 - INFO - layer4_self_attn.q_proj | mse: 0.006827279158441035, bpp_loss: 4.424898743629456, bpp: 0
2025-03-02 20:49:20 - INFO - layer4_self_attn.k_proj | mse: 0.007286882649665395, bpp_loss: 5.398918032646179, bpp: 0
2025-03-02 20:49:20 - INFO - layer4_self_attn.v_proj | mse: 0.006476654441815172, bpp_loss: 3.5136829614639282, bpp: 0
2025-03-02 20:49:22 - INFO - layer4_self_attn.o_proj | mse: 0.006496047244775579, bpp_loss: 3.592482089996338, bpp: 0
2025-03-02 20:49:26 - INFO - layer4_mlp.gate_proj | mse: 0.006669668247556938, bpp_loss: 4.052600996834891, bpp: 0
2025-03-02 20:49:31 - INFO - layer4_mlp.up_proj | mse: 0.006536646813208138, bpp_loss: 3.7008980342320035, bpp: 0
2025-03-02 20:49:36 - INFO - layer4_mlp.down_proj | mse: 0.0065338921395877445, bpp_loss: 3.7018615688596452, bpp: 0
pseudo compress quantization...:  16%|█▌        | 5/32 [01:32<08:19, 18.49s/it]2025-03-02 20:49:38 - INFO - layer5_self_attn.q_proj | mse: 0.006823329040897486, bpp_loss: 4.398013114929199, bpp: 0
2025-03-02 20:49:38 - INFO - layer5_self_attn.k_proj | mse: 0.00731365639283231, bpp_loss: 5.366533160209656, bpp: 0
2025-03-02 20:49:39 - INFO - layer5_self_attn.v_proj | mse: 0.006429119767334392, bpp_loss: 3.40103417634964, bpp: 0
2025-03-02 20:49:40 - INFO - layer5_self_attn.o_proj | mse: 0.006473179439253026, bpp_loss: 3.53877055644989, bpp: 0
2025-03-02 20:49:45 - INFO - layer5_mlp.gate_proj | mse: 0.0066678050353215085, bpp_loss: 4.0545003073556085, bpp: 0
2025-03-02 20:49:50 - INFO - layer5_mlp.up_proj | mse: 0.006539703606982214, bpp_loss: 3.706060137067522, bpp: 0
2025-03-02 20:49:55 - INFO - layer5_mlp.down_proj | mse: 0.006535787633154662, bpp_loss: 3.707068221909659, bpp: 0
pseudo compress quantization...:  19%|█▉        | 6/32 [01:51<08:02, 18.56s/it]2025-03-02 20:49:56 - INFO - layer6_self_attn.q_proj | mse: 0.006834581878498168, bpp_loss: 4.44633162021637, bpp: 0
2025-03-02 20:49:57 - INFO - layer6_self_attn.k_proj | mse: 0.00729720915790447, bpp_loss: 5.4364975690841675, bpp: 0
2025-03-02 20:49:57 - INFO - layer6_self_attn.v_proj | mse: 0.006448654525380705, bpp_loss: 3.4461315870285034, bpp: 0
2025-03-02 20:49:59 - INFO - layer6_self_attn.o_proj | mse: 0.006486474691865223, bpp_loss: 3.5713290572166443, bpp: 0
2025-03-02 20:50:04 - INFO - layer6_mlp.gate_proj | mse: 0.00667189483170323, bpp_loss: 4.0604617936270575, bpp: 0
2025-03-02 20:50:09 - INFO - layer6_mlp.up_proj | mse: 0.006539431698843901, bpp_loss: 3.705094609941755, bpp: 0
2025-03-02 20:50:13 - INFO - layer6_mlp.down_proj | mse: 0.006534308485991307, bpp_loss: 3.7072129590170726, bpp: 0
pseudo compress quantization...:  22%|██▏       | 7/32 [02:09<07:42, 18.49s/it]2025-03-02 20:50:15 - INFO - layer7_self_attn.q_proj | mse: 0.0068039912817459015, bpp_loss: 4.367117404937744, bpp: 0
2025-03-02 20:50:15 - INFO - layer7_self_attn.k_proj | mse: 0.0073003510005711205, bpp_loss: 5.465544104576111, bpp: 0
2025-03-02 20:50:16 - INFO - layer7_self_attn.v_proj | mse: 0.006445207768983612, bpp_loss: 3.4443153142929077, bpp: 0
2025-03-02 20:50:17 - INFO - layer7_self_attn.o_proj | mse: 0.006491539446537397, bpp_loss: 3.5830092430114746, bpp: 0
2025-03-02 20:50:22 - INFO - layer7_mlp.gate_proj | mse: 0.006659708629066131, bpp_loss: 4.030967167445591, bpp: 0
2025-03-02 20:50:27 - INFO - layer7_mlp.up_proj | mse: 0.00654400281815539, bpp_loss: 3.7188036101205006, bpp: 0
2025-03-02 20:50:32 - INFO - layer7_mlp.down_proj | mse: 0.006538884571661485, bpp_loss: 3.722070404461452, bpp: 0
pseudo compress quantization...:  25%|██▌       | 8/32 [02:28<07:25, 18.55s/it]2025-03-02 20:50:33 - INFO - layer8_self_attn.q_proj | mse: 0.006803827598840723, bpp_loss: 4.352226257324219, bpp: 0
2025-03-02 20:50:34 - INFO - layer8_self_attn.k_proj | mse: 0.007297185257303271, bpp_loss: 5.368265151977539, bpp: 0
2025-03-02 20:50:34 - INFO - layer8_self_attn.v_proj | mse: 0.006455774731755957, bpp_loss: 3.466909348964691, bpp: 0
2025-03-02 20:50:36 - INFO - layer8_self_attn.o_proj | mse: 0.0064985213191299535, bpp_loss: 3.5946831703186035, bpp: 0
2025-03-02 20:50:41 - INFO - layer8_mlp.gate_proj | mse: 0.00666344581398082, bpp_loss: 4.036606516156878, bpp: 0
2025-03-02 20:50:45 - INFO - layer8_mlp.up_proj | mse: 0.006543829823327666, bpp_loss: 3.7142809459141324, bpp: 0
2025-03-02 20:50:50 - INFO - layer8_mlp.down_proj | mse: 0.006538522855203518, bpp_loss: 3.720350078174046, bpp: 0
pseudo compress quantization...:  28%|██▊       | 9/32 [02:46<07:05, 18.48s/it]2025-03-02 20:50:52 - INFO - layer9_self_attn.q_proj | mse: 0.006804208559938615, bpp_loss: 4.3649598360061646, bpp: 0
2025-03-02 20:50:52 - INFO - layer9_self_attn.k_proj | mse: 0.007288789627936785, bpp_loss: 5.390282511711121, bpp: 0
2025-03-02 20:50:53 - INFO - layer9_self_attn.v_proj | mse: 0.006497633375582394, bpp_loss: 3.5687814950942993, bpp: 0
2025-03-02 20:50:54 - INFO - layer9_self_attn.o_proj | mse: 0.006517106571474105, bpp_loss: 3.6516350507736206, bpp: 0
2025-03-02 20:50:59 - INFO - layer9_mlp.gate_proj | mse: 0.0066701508120763755, bpp_loss: 4.052177633558001, bpp: 0
2025-03-02 20:51:04 - INFO - layer9_mlp.up_proj | mse: 0.006545327594336111, bpp_loss: 3.724099636077881, bpp: 0
2025-03-02 20:51:08 - INFO - layer9_mlp.down_proj | mse: 0.006538672673690685, bpp_loss: 3.7220191104071483, bpp: 0
pseudo compress quantization...:  31%|███▏      | 10/32 [03:04<06:45, 18.43s/it]2025-03-02 20:51:10 - INFO - layer10_self_attn.q_proj | mse: 0.006813079304286504, bpp_loss: 4.367587924003601, bpp: 0
2025-03-02 20:51:11 - INFO - layer10_self_attn.k_proj | mse: 0.007308798052443216, bpp_loss: 5.392351031303406, bpp: 0
2025-03-02 20:51:11 - INFO - layer10_self_attn.v_proj | mse: 0.006449773508073184, bpp_loss: 3.457229495048523, bpp: 0
2025-03-02 20:51:13 - INFO - layer10_self_attn.o_proj | mse: 0.006491792213501588, bpp_loss: 3.5874322652816772, bpp: 0
2025-03-02 20:51:18 - INFO - layer10_mlp.gate_proj | mse: 0.0066565893419452005, bpp_loss: 4.019632952553885, bpp: 0
2025-03-02 20:51:22 - INFO - layer10_mlp.up_proj | mse: 0.006552265597426901, bpp_loss: 3.740826061793736, bpp: 0
2025-03-02 20:51:27 - INFO - layer10_mlp.down_proj | mse: 0.006546054752021062, bpp_loss: 3.738754391670227, bpp: 0
pseudo compress quantization...:  34%|███▍      | 11/32 [03:23<06:29, 18.55s/it]2025-03-02 20:51:29 - INFO - layer11_self_attn.q_proj | mse: 0.00677755649255962, bpp_loss: 4.301084995269775, bpp: 0
2025-03-02 20:51:29 - INFO - layer11_self_attn.k_proj | mse: 0.007281888872536175, bpp_loss: 5.3908504247665405, bpp: 0
2025-03-02 20:51:30 - INFO - layer11_self_attn.v_proj | mse: 0.00645745429218564, bpp_loss: 3.4616958498954773, bpp: 0
2025-03-02 20:51:31 - INFO - layer11_self_attn.o_proj | mse: 0.00649900367671778, bpp_loss: 3.6068873405456543, bpp: 0
2025-03-02 20:51:36 - INFO - layer11_mlp.gate_proj | mse: 0.006648762774512015, bpp_loss: 3.9987904003688266, bpp: 0
2025-03-02 20:51:41 - INFO - layer11_mlp.up_proj | mse: 0.0065562813122914875, bpp_loss: 3.745962824140276, bpp: 0
2025-03-02 20:51:46 - INFO - layer11_mlp.down_proj | mse: 0.006547767318042875, bpp_loss: 3.746199829237802, bpp: 0
pseudo compress quantization...:  38%|███▊      | 12/32 [03:42<06:11, 18.56s/it]2025-03-02 20:51:47 - INFO - layer12_self_attn.q_proj | mse: 0.006795603619246481, bpp_loss: 4.3608397245407104, bpp: 0
2025-03-02 20:51:48 - INFO - layer12_self_attn.k_proj | mse: 0.0072564021405307705, bpp_loss: 5.385471940040588, bpp: 0
2025-03-02 20:51:49 - INFO - layer12_self_attn.v_proj | mse: 0.00649816136159069, bpp_loss: 3.5799861550331116, bpp: 0
2025-03-02 20:51:50 - INFO - layer12_self_attn.o_proj | mse: 0.0065211971955822755, bpp_loss: 3.652704954147339, bpp: 0
2025-03-02 20:51:55 - INFO - layer12_mlp.gate_proj | mse: 0.006643070085862897, bpp_loss: 3.980346611567906, bpp: 0
2025-03-02 20:52:00 - INFO - layer12_mlp.up_proj | mse: 0.006562812901696096, bpp_loss: 3.766302926199777, bpp: 0
2025-03-02 20:52:05 - INFO - layer12_mlp.down_proj | mse: 0.00655418964755629, bpp_loss: 3.759148972375052, bpp: 0
pseudo compress quantization...:  41%|████      | 13/32 [04:00<05:53, 18.62s/it]2025-03-02 20:52:06 - INFO - layer13_self_attn.q_proj | mse: 0.006795036523163497, bpp_loss: 4.336499214172363, bpp: 0
2025-03-02 20:52:07 - INFO - layer13_self_attn.k_proj | mse: 0.007306129876236682, bpp_loss: 5.405180215835571, bpp: 0
2025-03-02 20:52:07 - INFO - layer13_self_attn.v_proj | mse: 0.006472955642714531, bpp_loss: 3.5202398896217346, bpp: 0
2025-03-02 20:52:09 - INFO - layer13_self_attn.o_proj | mse: 0.006511873788332899, bpp_loss: 3.6349398493766785, bpp: 0
2025-03-02 20:52:14 - INFO - layer13_mlp.gate_proj | mse: 0.006643495951120616, bpp_loss: 3.9848267691476003, bpp: 0
2025-03-02 20:52:18 - INFO - layer13_mlp.up_proj | mse: 0.006564786201549168, bpp_loss: 3.7697230066571916, bpp: 0
2025-03-02 20:52:23 - INFO - layer13_mlp.down_proj | mse: 0.006555538427804017, bpp_loss: 3.758672220366342, bpp: 0
pseudo compress quantization...:  44%|████▍     | 14/32 [04:19<05:35, 18.66s/it]2025-03-02 20:52:25 - INFO - layer14_self_attn.q_proj | mse: 0.006782833455600011, bpp_loss: 4.3024210929870605, bpp: 0
2025-03-02 20:52:25 - INFO - layer14_self_attn.k_proj | mse: 0.0072952427902604035, bpp_loss: 5.349801778793335, bpp: 0
2025-03-02 20:52:26 - INFO - layer14_self_attn.v_proj | mse: 0.006472008309794296, bpp_loss: 3.513006806373596, bpp: 0
2025-03-02 20:52:27 - INFO - layer14_self_attn.o_proj | mse: 0.006510474516771818, bpp_loss: 3.6268357038497925, bpp: 0
2025-03-02 20:52:32 - INFO - layer14_mlp.gate_proj | mse: 0.006656537195178949, bpp_loss: 4.018544810158866, bpp: 0
2025-03-02 20:52:37 - INFO - layer14_mlp.up_proj | mse: 0.006563733333506757, bpp_loss: 3.764061859675816, bpp: 0
2025-03-02 20:52:42 - INFO - layer14_mlp.down_proj | mse: 0.006553569266583188, bpp_loss: 3.7543374810900008, bpp: 0
pseudo compress quantization...:  47%|████▋     | 15/32 [04:38<05:17, 18.65s/it]2025-03-02 20:52:44 - INFO - layer15_self_attn.q_proj | mse: 0.006849033775356247, bpp_loss: 4.450774669647217, bpp: 0
2025-03-02 20:52:44 - INFO - layer15_self_attn.k_proj | mse: 0.007279992033913777, bpp_loss: 5.396917104721069, bpp: 0
2025-03-02 20:52:45 - INFO - layer15_self_attn.v_proj | mse: 0.006495976991493267, bpp_loss: 3.576978385448456, bpp: 0
2025-03-02 20:52:46 - INFO - layer15_self_attn.o_proj | mse: 0.0065201325324379744, bpp_loss: 3.657300353050232, bpp: 0
2025-03-02 20:52:51 - INFO - layer15_mlp.gate_proj | mse: 0.006672301245389412, bpp_loss: 4.0562935556684225, bpp: 0
2025-03-02 20:52:56 - INFO - layer15_mlp.up_proj | mse: 0.006559202358928019, bpp_loss: 3.7560135977608815, bpp: 0
2025-03-02 20:53:01 - INFO - layer15_mlp.down_proj | mse: 0.006551361306281985, bpp_loss: 3.748531869479588, bpp: 0
pseudo compress quantization...:  50%|█████     | 16/32 [04:57<04:59, 18.69s/it]2025-03-02 20:53:02 - INFO - layer16_self_attn.q_proj | mse: 0.006830541228374321, bpp_loss: 4.4216591119766235, bpp: 0
2025-03-02 20:53:03 - INFO - layer16_self_attn.k_proj | mse: 0.007286362630524164, bpp_loss: 5.388777732849121, bpp: 0
2025-03-02 20:53:03 - INFO - layer16_self_attn.v_proj | mse: 0.006476168462924135, bpp_loss: 3.5337514877319336, bpp: 0
2025-03-02 20:53:05 - INFO - layer16_self_attn.o_proj | mse: 0.006514045121738758, bpp_loss: 3.6378178000450134, bpp: 0
2025-03-02 20:53:10 - INFO - layer16_mlp.gate_proj | mse: 0.006686205394270239, bpp_loss: 4.090757029397147, bpp: 0
2025-03-02 20:53:14 - INFO - layer16_mlp.up_proj | mse: 0.006554669728896384, bpp_loss: 3.7426908356802806, bpp: 0
2025-03-02 20:53:17 - INFO - layer16_mlp.down_proj | mse: 0.006545403331306142, bpp_loss: 3.734083345958165, bpp: 0
pseudo compress quantization...:  53%|█████▎    | 17/32 [05:13<04:30, 18.05s/it]2025-03-02 20:53:18 - INFO - layer17_self_attn.q_proj | mse: 0.006835273547411641, bpp_loss: 4.438994407653809, bpp: 0
2025-03-02 20:53:19 - INFO - layer17_self_attn.k_proj | mse: 0.007289425528780796, bpp_loss: 5.4192527532577515, bpp: 0
2025-03-02 20:53:19 - INFO - layer17_self_attn.v_proj | mse: 0.00650412492372006, bpp_loss: 3.60322767496109, bpp: 0
2025-03-02 20:53:20 - INFO - layer17_self_attn.o_proj | mse: 0.006519963055447657, bpp_loss: 3.667127847671509, bpp: 0
2025-03-02 20:53:23 - INFO - layer17_mlp.gate_proj | mse: 0.006690700776611695, bpp_loss: 4.105377469744001, bpp: 0
2025-03-02 20:53:26 - INFO - layer17_mlp.up_proj | mse: 0.006551838076716284, bpp_loss: 3.7401222501482283, bpp: 0
2025-03-02 20:53:28 - INFO - layer17_mlp.down_proj | mse: 0.006547015742427696, bpp_loss: 3.730523552213396, bpp: 0
pseudo compress quantization...:  56%|█████▋    | 18/32 [05:24<03:42, 15.91s/it]2025-03-02 20:53:29 - INFO - layer18_self_attn.q_proj | mse: 0.0068319788857494484, bpp_loss: 4.433554649353027, bpp: 0
2025-03-02 20:53:29 - INFO - layer18_self_attn.k_proj | mse: 0.00732869204376815, bpp_loss: 5.503702640533447, bpp: 0
2025-03-02 20:53:30 - INFO - layer18_self_attn.v_proj | mse: 0.006472140125231209, bpp_loss: 3.528018891811371, bpp: 0
2025-03-02 20:53:31 - INFO - layer18_self_attn.o_proj | mse: 0.006517548370465957, bpp_loss: 3.645887792110443, bpp: 0
2025-03-02 20:53:33 - INFO - layer18_mlp.gate_proj | mse: 0.006693565537849732, bpp_loss: 4.1122260093688965, bpp: 0
2025-03-02 20:53:36 - INFO - layer18_mlp.up_proj | mse: 0.0065500592925785955, bpp_loss: 3.736075128827776, bpp: 0
2025-03-02 20:53:38 - INFO - layer18_mlp.down_proj | mse: 0.006546046060893353, bpp_loss: 3.7293809822627475, bpp: 0
pseudo compress quantization...:  59%|█████▉    | 19/32 [05:34<03:03, 14.15s/it]2025-03-02 20:53:39 - INFO - layer19_self_attn.q_proj | mse: 0.006830688977545367, bpp_loss: 4.4361701011657715, bpp: 0
2025-03-02 20:53:39 - INFO - layer19_self_attn.k_proj | mse: 0.007281959850079128, bpp_loss: 5.398898363113403, bpp: 0
2025-03-02 20:53:40 - INFO - layer19_self_attn.v_proj | mse: 0.006483863007988796, bpp_loss: 3.5737228989601135, bpp: 0
2025-03-02 20:53:41 - INFO - layer19_self_attn.o_proj | mse: 0.006522362530955867, bpp_loss: 3.6591458916664124, bpp: 0
2025-03-02 20:53:43 - INFO - layer19_mlp.gate_proj | mse: 0.006696758906487798, bpp_loss: 4.122783184051514, bpp: 0
2025-03-02 20:53:46 - INFO - layer19_mlp.up_proj | mse: 0.006548236225075914, bpp_loss: 3.7324203082493375, bpp: 0
2025-03-02 20:53:48 - INFO - layer19_mlp.down_proj | mse: 0.006543994127027681, bpp_loss: 3.7276914630617415, bpp: 0
pseudo compress quantization...:  62%|██████▎   | 20/32 [05:44<02:34, 12.92s/it]2025-03-02 20:53:49 - INFO - layer20_self_attn.q_proj | mse: 0.006821258379720918, bpp_loss: 4.407014012336731, bpp: 0
2025-03-02 20:53:50 - INFO - layer20_self_attn.k_proj | mse: 0.00726051376819757, bpp_loss: 5.34692645072937, bpp: 0
2025-03-02 20:53:50 - INFO - layer20_self_attn.v_proj | mse: 0.0065072262077907365, bpp_loss: 3.6080442667007446, bpp: 0
2025-03-02 20:53:51 - INFO - layer20_self_attn.o_proj | mse: 0.0065182414879007165, bpp_loss: 3.6465184092521667, bpp: 0
2025-03-02 20:53:53 - INFO - layer20_mlp.gate_proj | mse: 0.006695531802027991, bpp_loss: 4.124522754124233, bpp: 0
2025-03-02 20:53:56 - INFO - layer20_mlp.up_proj | mse: 0.006547989562594281, bpp_loss: 3.736179828643799, bpp: 0
2025-03-02 20:53:59 - INFO - layer20_mlp.down_proj | mse: 0.0065475252080567075, bpp_loss: 3.7318947315216064, bpp: 0
pseudo compress quantization...:  66%|██████▌   | 21/32 [05:55<02:14, 12.21s/it]2025-03-02 20:54:00 - INFO - layer21_self_attn.q_proj | mse: 0.006811342527266073, bpp_loss: 4.403081178665161, bpp: 0
2025-03-02 20:54:00 - INFO - layer21_self_attn.k_proj | mse: 0.007270513634886931, bpp_loss: 5.392202377319336, bpp: 0
2025-03-02 20:54:00 - INFO - layer21_self_attn.v_proj | mse: 0.0065169957595958205, bpp_loss: 3.633606195449829, bpp: 0
2025-03-02 20:54:01 - INFO - layer21_self_attn.o_proj | mse: 0.0065263459644889655, bpp_loss: 3.6657726764678955, bpp: 0
2025-03-02 20:54:04 - INFO - layer21_mlp.gate_proj | mse: 0.006700934786420157, bpp_loss: 4.1365565572466165, bpp: 0
2025-03-02 20:54:06 - INFO - layer21_mlp.up_proj | mse: 0.006550982621431825, bpp_loss: 3.7391128540039062, bpp: 0
2025-03-02 20:54:09 - INFO - layer21_mlp.down_proj | mse: 0.006546896549819121, bpp_loss: 3.7329379660742625, bpp: 0
pseudo compress quantization...:  69%|██████▉   | 22/32 [06:05<01:56, 11.61s/it]2025-03-02 20:54:10 - INFO - layer22_self_attn.q_proj | mse: 0.006797527979773289, bpp_loss: 4.361530542373657, bpp: 0
2025-03-02 20:54:11 - INFO - layer22_self_attn.k_proj | mse: 0.007235268214986079, bpp_loss: 5.323513746261597, bpp: 0
2025-03-02 20:54:11 - INFO - layer22_self_attn.v_proj | mse: 0.006531988679153759, bpp_loss: 3.68385773897171, bpp: 0
2025-03-02 20:54:12 - INFO - layer22_self_attn.o_proj | mse: 0.006533731250259329, bpp_loss: 3.693459212779999, bpp: 0
2025-03-02 20:54:15 - INFO - layer22_mlp.gate_proj | mse: 0.006702446628778226, bpp_loss: 4.140300137656076, bpp: 0
2025-03-02 20:54:18 - INFO - layer22_mlp.up_proj | mse: 0.006551905536421831, bpp_loss: 3.7433115414210727, bpp: 0
2025-03-02 20:54:21 - INFO - layer22_mlp.down_proj | mse: 0.006547769801222221, bpp_loss: 3.7385394743510654, bpp: 0
pseudo compress quantization...:  72%|███████▏  | 23/32 [06:17<01:44, 11.65s/it]2025-03-02 20:54:22 - INFO - layer23_self_attn.q_proj | mse: 0.006797973400068354, bpp_loss: 4.373885154724121, bpp: 0
2025-03-02 20:54:22 - INFO - layer23_self_attn.k_proj | mse: 0.00724797464369602, bpp_loss: 5.326449751853943, bpp: 0
2025-03-02 20:54:23 - INFO - layer23_self_attn.v_proj | mse: 0.006553355092364655, bpp_loss: 3.736473023891449, bpp: 0
2025-03-02 20:54:24 - INFO - layer23_self_attn.o_proj | mse: 0.006540397345211808, bpp_loss: 3.7165786027908325, bpp: 0
2025-03-02 20:54:26 - INFO - layer23_mlp.gate_proj | mse: 0.006703187030086352, bpp_loss: 4.142692429678781, bpp: 0
2025-03-02 20:54:29 - INFO - layer23_mlp.up_proj | mse: 0.006553669421483449, bpp_loss: 3.747326510293143, bpp: 0
2025-03-02 20:54:32 - INFO - layer23_mlp.down_proj | mse: 0.006549854844145832, bpp_loss: 3.743962253843035, bpp: 0
pseudo compress quantization...:  75%|███████▌  | 24/32 [06:28<01:32, 11.61s/it]2025-03-02 20:54:33 - INFO - layer24_self_attn.q_proj | mse: 0.006784842554621978, bpp_loss: 4.339503884315491, bpp: 0
2025-03-02 20:54:34 - INFO - layer24_self_attn.k_proj | mse: 0.007159740866677255, bpp_loss: 5.148183703422546, bpp: 0
2025-03-02 20:54:34 - INFO - layer24_self_attn.v_proj | mse: 0.006590600195899122, bpp_loss: 3.836285412311554, bpp: 0
2025-03-02 20:54:35 - INFO - layer24_self_attn.o_proj | mse: 0.0065554482056211375, bpp_loss: 3.7587374448776245, bpp: 0
2025-03-02 20:54:37 - INFO - layer24_mlp.gate_proj | mse: 0.006708222090072187, bpp_loss: 4.148943969181606, bpp: 0
2025-03-02 20:54:40 - INFO - layer24_mlp.up_proj | mse: 0.006555382815231711, bpp_loss: 3.7521112305777415, bpp: 0
2025-03-02 20:54:43 - INFO - layer24_mlp.down_proj | mse: 0.0065531901678698035, bpp_loss: 3.7495153972080777, bpp: 0
pseudo compress quantization...:  78%|███████▊  | 25/32 [06:38<01:18, 11.18s/it]2025-03-02 20:54:43 - INFO - layer25_self_attn.q_proj | mse: 0.006774403061722691, bpp_loss: 4.316907644271851, bpp: 0
2025-03-02 20:54:44 - INFO - layer25_self_attn.k_proj | mse: 0.007156574399148763, bpp_loss: 5.129524111747742, bpp: 0
2025-03-02 20:54:44 - INFO - layer25_self_attn.v_proj | mse: 0.006593261853759874, bpp_loss: 3.8467968702316284, bpp: 0
2025-03-02 20:54:45 - INFO - layer25_self_attn.o_proj | mse: 0.006559774938698725, bpp_loss: 3.7586745619773865, bpp: 0
2025-03-02 20:54:47 - INFO - layer25_mlp.gate_proj | mse: 0.006711225495490337, bpp_loss: 4.158696583339146, bpp: 0
2025-03-02 20:54:50 - INFO - layer25_mlp.up_proj | mse: 0.006559451090725773, bpp_loss: 3.7615688187735423, bpp: 0
2025-03-02 20:54:53 - INFO - layer25_mlp.down_proj | mse: 0.006554955708384318, bpp_loss: 3.7586227314812795, bpp: 0
pseudo compress quantization...:  81%|████████▏ | 26/32 [06:49<01:05, 10.86s/it]2025-03-02 20:54:54 - INFO - layer26_self_attn.q_proj | mse: 0.006778773974699464, bpp_loss: 4.327561259269714, bpp: 0
2025-03-02 20:54:54 - INFO - layer26_self_attn.k_proj | mse: 0.00719267299808634, bpp_loss: 5.214334964752197, bpp: 0
2025-03-02 20:54:54 - INFO - layer26_self_attn.v_proj | mse: 0.006625325596658744, bpp_loss: 3.8989970684051514, bpp: 0
2025-03-02 20:54:55 - INFO - layer26_self_attn.o_proj | mse: 0.006568726800238559, bpp_loss: 3.775977849960327, bpp: 0
2025-03-02 20:54:58 - INFO - layer26_mlp.gate_proj | mse: 0.0067145359874208575, bpp_loss: 4.16623170035226, bpp: 0
2025-03-02 20:55:00 - INFO - layer26_mlp.up_proj | mse: 0.00656241393754795, bpp_loss: 3.770907061440604, bpp: 0
2025-03-02 20:55:03 - INFO - layer26_mlp.down_proj | mse: 0.006558965215300542, bpp_loss: 3.7665369851248607, bpp: 0
pseudo compress quantization...:  84%|████████▍ | 27/32 [06:59<00:54, 10.81s/it]2025-03-02 20:55:04 - INFO - layer27_self_attn.q_proj | mse: 0.00676724664231532, bpp_loss: 4.2962586879730225, bpp: 0
2025-03-02 20:55:04 - INFO - layer27_self_attn.k_proj | mse: 0.007179059070791496, bpp_loss: 5.1667680740356445, bpp: 0
2025-03-02 20:55:05 - INFO - layer27_self_attn.v_proj | mse: 0.006659041378082886, bpp_loss: 3.992181360721588, bpp: 0
2025-03-02 20:55:06 - INFO - layer27_self_attn.o_proj | mse: 0.006576561851867842, bpp_loss: 3.8171902298927307, bpp: 0
2025-03-02 20:55:08 - INFO - layer27_mlp.gate_proj | mse: 0.006719501104521799, bpp_loss: 4.1780460221426825, bpp: 0
2025-03-02 20:55:12 - INFO - layer27_mlp.up_proj | mse: 0.006569351112912292, bpp_loss: 3.786723954336984, bpp: 0
2025-03-02 20:55:15 - INFO - layer27_mlp.down_proj | mse: 0.006565419826145446, bpp_loss: 3.7778116294315884, bpp: 0
pseudo compress quantization...:  88%|████████▊ | 28/32 [07:11<00:44, 11.10s/it]2025-03-02 20:55:16 - INFO - layer28_self_attn.q_proj | mse: 0.006771841351830588, bpp_loss: 4.3056535720825195, bpp: 0
2025-03-02 20:55:17 - INFO - layer28_self_attn.k_proj | mse: 0.0071325122878269235, bpp_loss: 5.1170654296875, bpp: 0
2025-03-02 20:55:17 - INFO - layer28_self_attn.v_proj | mse: 0.0066886209069783875, bpp_loss: 4.046416759490967, bpp: 0
2025-03-02 20:55:19 - INFO - layer28_self_attn.o_proj | mse: 0.006589542051100602, bpp_loss: 3.8471834659576416, bpp: 0
2025-03-02 20:55:24 - INFO - layer28_mlp.gate_proj | mse: 0.006714274011999927, bpp_loss: 4.164454051426479, bpp: 0
2025-03-02 20:55:29 - INFO - layer28_mlp.up_proj | mse: 0.006575916225238061, bpp_loss: 3.809438501085554, bpp: 0
2025-03-02 20:55:34 - INFO - layer28_mlp.down_proj | mse: 0.006571135277271896, bpp_loss: 3.7933257818222046, bpp: 0
pseudo compress quantization...:  91%|█████████ | 29/32 [07:30<00:40, 13.48s/it]2025-03-02 20:55:36 - INFO - layer29_self_attn.q_proj | mse: 0.006763036515201156, bpp_loss: 4.293528079986572, bpp: 0
2025-03-02 20:55:36 - INFO - layer29_self_attn.k_proj | mse: 0.007184529411423403, bpp_loss: 5.211215138435364, bpp: 0
2025-03-02 20:55:37 - INFO - layer29_self_attn.v_proj | mse: 0.006706408024094634, bpp_loss: 4.1164470911026, bpp: 0
2025-03-02 20:55:38 - INFO - layer29_self_attn.o_proj | mse: 0.006607468226260186, bpp_loss: 3.883187413215637, bpp: 0
2025-03-02 20:55:43 - INFO - layer29_mlp.gate_proj | mse: 0.00671171344023169, bpp_loss: 4.1562089920043945, bpp: 0
2025-03-02 20:55:48 - INFO - layer29_mlp.up_proj | mse: 0.006588351159672916, bpp_loss: 3.8432837213788713, bpp: 0
2025-03-02 20:55:53 - INFO - layer29_mlp.down_proj | mse: 0.006580115281510977, bpp_loss: 3.808284946850368, bpp: 0
pseudo compress quantization...:  94%|█████████▍| 30/32 [07:49<00:30, 15.05s/it]2025-03-02 20:55:54 - INFO - layer30_self_attn.q_proj | mse: 0.006724625352032455, bpp_loss: 4.196936368942261, bpp: 0
2025-03-02 20:55:55 - INFO - layer30_self_attn.k_proj | mse: 0.00703670926283463, bpp_loss: 4.855374574661255, bpp: 0
2025-03-02 20:55:55 - INFO - layer30_self_attn.v_proj | mse: 0.00682993574647729, bpp_loss: 4.408815979957581, bpp: 0
2025-03-02 20:55:57 - INFO - layer30_self_attn.o_proj | mse: 0.006638068962661446, bpp_loss: 3.9714202880859375, bpp: 0
2025-03-02 20:56:02 - INFO - layer30_mlp.gate_proj | mse: 0.006732981871324252, bpp_loss: 4.2090935707092285, bpp: 0
2025-03-02 20:56:07 - INFO - layer30_mlp.up_proj | mse: 0.0065985384029370415, bpp_loss: 3.8762705666678294, bpp: 0
2025-03-02 20:56:12 - INFO - layer30_mlp.down_proj | mse: 0.0065844169758634994, bpp_loss: 3.809278096471514, bpp: 0
pseudo compress quantization...:  97%|█████████▋| 31/32 [08:08<00:16, 16.24s/it]2025-03-02 20:56:13 - INFO - layer31_self_attn.q_proj | mse: 0.006794322402170108, bpp_loss: 4.352590799331665, bpp: 0
2025-03-02 20:56:14 - INFO - layer31_self_attn.k_proj | mse: 0.007137025880150245, bpp_loss: 5.10060441493988, bpp: 0
2025-03-02 20:56:14 - INFO - layer31_self_attn.v_proj | mse: 0.006743058509641706, bpp_loss: 4.221346974372864, bpp: 0
2025-03-02 20:56:15 - INFO - layer31_self_attn.o_proj | mse: 0.006632967994957153, bpp_loss: 3.9315865635871887, bpp: 0
2025-03-02 20:56:20 - INFO - layer31_mlp.gate_proj | mse: 0.006828742371870257, bpp_loss: 4.432864461626325, bpp: 0
2025-03-02 20:56:25 - INFO - layer31_mlp.up_proj | mse: 0.006673919864459328, bpp_loss: 4.075457368578229, bpp: 0
2025-03-02 20:56:30 - INFO - layer31_mlp.down_proj | mse: 0.00659631595742299, bpp_loss: 3.8353018930980136, bpp: 0
pseudo compress quantization...: 100%|██████████| 32/32 [08:26<00:00, 16.85s/it]pseudo compress quantization...: 100%|██████████| 32/32 [08:26<00:00, 15.83s/it]
2025-03-02 20:56:30 - INFO - #### Total | mse: 0.006618893694257647, bpp_loss: 3.9103365596025608, bpp: 0 ####
## Strart saving /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda300_rdloss_ql_encdim512_M16_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_5.34295_bpp_5.7068_MSE_0.00302_total_iter_95000.pth.tar/COL_MSE0.00662_bpploss3.9103_bpp0
## End saving
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda300_rdloss_ql_encdim512_M16_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_5.34295_bpp_5.7068_MSE_0.00302_total_iter_95000.pth.tar/COL_MSE0.00662_bpploss3.9103_bpp0
I0302 20:57:16.142435 2579477 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.11it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.53it/s]
W0302 20:57:18.955203 2579477 big_modeling.py:414] Some parameters are on the meta device device because they were offloaded to the cpu.
I0302 20:57:18.968833 2579477 config.py:54] PyTorch version 2.4.1 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.6328125:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.6328125:   1%|          | 1/141 [00:01<02:26,  1.04s/it]avg_loss = 1.6328125:   1%|          | 1/141 [00:01<04:31,  1.94s/it]
Traceback (most recent call last):
  File "/home/jgryu/Weight_compression/comp_llm/eval_ppl.py", line 115, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/comp_llm/eval_ppl.py", line 91, in main
    output = model(input,
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1179, in forward
    logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/hooks.py", line 160, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/hooks.py", line 293, in pre_forward
    set_module_tensor_to_device(
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 347, in set_module_tensor_to_device
    new_value = value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 934.19 MiB is free. Process 2577142 has 7.63 GiB memory in use. Including non-PyTorch memory, this process has 15.02 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running with lmbda=1000
/home/jgryu/Weight_compression/comp_llm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")
/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  9.32it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  9.51it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.92it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.16it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.28it/s]
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:12,  2.48it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:12,  2.38it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:12,  2.30it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:12,  2.30it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:11,  2.33it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:10,  2.37it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:02<00:10,  2.44it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:09,  2.47it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:03<00:09,  2.49it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:04<00:08,  2.45it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:04<00:08,  2.49it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:04<00:07,  2.52it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:05<00:07,  2.39it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:05<00:07,  2.35it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:06<00:08,  1.90it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:07<00:08,  1.95it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:07<00:07,  2.07it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:08<00:08,  1.58it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:09<00:09,  1.33it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:09,  1.29it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:10<00:07,  1.38it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:06,  1.44it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:06,  1.49it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:05,  1.59it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:04,  1.69it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.76it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.76it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.74it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.83it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.89it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.93it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
pseudo compress quantization...:   0%|          | 0/32 [00:00<?, ?it/s]2025-03-02 20:58:27 - INFO - layer0_self_attn.q_proj | mse: 0.002119437233790442, bpp_loss: 5.094824552536011, bpp: 0
2025-03-02 20:58:28 - INFO - layer0_self_attn.k_proj | mse: 0.002292103686911593, bpp_loss: 5.76834774017334, bpp: 0
2025-03-02 20:58:28 - INFO - layer0_self_attn.v_proj | mse: 0.0019996025164232887, bpp_loss: 4.199488878250122, bpp: 0
2025-03-02 20:58:30 - INFO - layer0_self_attn.o_proj | mse: 0.0020080524653379533, bpp_loss: 4.2925344705581665, bpp: 0
2025-03-02 20:58:34 - INFO - layer0_mlp.gate_proj | mse: 0.0020553085064030294, bpp_loss: 4.689701761518206, bpp: 0
2025-03-02 20:58:39 - INFO - layer0_mlp.up_proj | mse: 0.0020424044580035247, bpp_loss: 4.574499130249023, bpp: 0
2025-03-02 20:58:45 - INFO - layer0_mlp.down_proj | mse: 0.0020413435196282416, bpp_loss: 4.56966621535165, bpp: 0
pseudo compress quantization...:   3%|▎         | 1/32 [00:18<09:45, 18.89s/it]2025-03-02 20:58:46 - INFO - layer1_self_attn.q_proj | mse: 0.002138106319303948, bpp_loss: 5.387323260307312, bpp: 0
2025-03-02 20:58:46 - INFO - layer1_self_attn.k_proj | mse: 0.0022663352175163027, bpp_loss: 6.272781491279602, bpp: 0
2025-03-02 20:58:47 - INFO - layer1_self_attn.v_proj | mse: 0.0020123071344816234, bpp_loss: 4.3010090589523315, bpp: 0
2025-03-02 20:58:48 - INFO - layer1_self_attn.o_proj | mse: 0.0020176938230093353, bpp_loss: 4.3747878074646, bpp: 0
2025-03-02 20:58:53 - INFO - layer1_mlp.gate_proj | mse: 0.0020573993434117783, bpp_loss: 4.699557168143136, bpp: 0
2025-03-02 20:58:58 - INFO - layer1_mlp.up_proj | mse: 0.002044300158502055, bpp_loss: 4.58854729788644, bpp: 0
2025-03-02 20:59:04 - INFO - layer1_mlp.down_proj | mse: 0.0020433321324206093, bpp_loss: 4.583758933203561, bpp: 0
pseudo compress quantization...:   6%|▋         | 2/32 [00:37<09:29, 18.99s/it]2025-03-02 20:59:05 - INFO - layer2_self_attn.q_proj | mse: 0.0021248746205631205, bpp_loss: 5.281952738761902, bpp: 0
2025-03-02 20:59:06 - INFO - layer2_self_attn.k_proj | mse: 0.0022678241163318785, bpp_loss: 6.285651922225952, bpp: 0
2025-03-02 20:59:06 - INFO - layer2_self_attn.v_proj | mse: 0.0019998893236376717, bpp_loss: 4.212159514427185, bpp: 0
2025-03-02 20:59:08 - INFO - layer2_self_attn.o_proj | mse: 0.002012140373468715, bpp_loss: 4.323513627052307, bpp: 0
2025-03-02 20:59:12 - INFO - layer2_mlp.gate_proj | mse: 0.002061110454943341, bpp_loss: 4.733896664210728, bpp: 0
2025-03-02 20:59:17 - INFO - layer2_mlp.up_proj | mse: 0.0020434960222573996, bpp_loss: 4.58087294442313, bpp: 0
2025-03-02 20:59:22 - INFO - layer2_mlp.down_proj | mse: 0.002043874293244335, bpp_loss: 4.585523707526071, bpp: 0
pseudo compress quantization...:   9%|▉         | 3/32 [00:56<09:05, 18.80s/it]2025-03-02 20:59:24 - INFO - layer3_self_attn.q_proj | mse: 0.002126263028214566, bpp_loss: 5.3197104930877686, bpp: 0
2025-03-02 20:59:24 - INFO - layer3_self_attn.k_proj | mse: 0.0022691873559260007, bpp_loss: 6.365141868591309, bpp: 0
2025-03-02 20:59:25 - INFO - layer3_self_attn.v_proj | mse: 0.0020115392371355383, bpp_loss: 4.3082627058029175, bpp: 0
2025-03-02 20:59:26 - INFO - layer3_self_attn.o_proj | mse: 0.002024479239902647, bpp_loss: 4.418449759483337, bpp: 0
2025-03-02 20:59:31 - INFO - layer3_mlp.gate_proj | mse: 0.0020704304478211177, bpp_loss: 4.814120428902762, bpp: 0
2025-03-02 20:59:36 - INFO - layer3_mlp.up_proj | mse: 0.002040773216105269, bpp_loss: 4.562651361737933, bpp: 0
2025-03-02 20:59:41 - INFO - layer3_mlp.down_proj | mse: 0.00204048185639542, bpp_loss: 4.561384848185948, bpp: 0
pseudo compress quantization...:  12%|█▎        | 4/32 [01:15<08:51, 18.99s/it]2025-03-02 20:59:43 - INFO - layer4_self_attn.q_proj | mse: 0.002124255196648724, bpp_loss: 5.284260272979736, bpp: 0
2025-03-02 20:59:44 - INFO - layer4_self_attn.k_proj | mse: 0.002265325598180823, bpp_loss: 6.331292629241943, bpp: 0
2025-03-02 20:59:44 - INFO - layer4_self_attn.v_proj | mse: 0.0020161828342441673, bpp_loss: 4.353525996208191, bpp: 0
2025-03-02 20:59:45 - INFO - layer4_self_attn.o_proj | mse: 0.0020257796498860427, bpp_loss: 4.430657148361206, bpp: 0
2025-03-02 20:59:50 - INFO - layer4_mlp.gate_proj | mse: 0.0020800502846048405, bpp_loss: 4.891960144042969, bpp: 0
2025-03-02 20:59:55 - INFO - layer4_mlp.up_proj | mse: 0.0020386865177287623, bpp_loss: 4.53638219833374, bpp: 0
2025-03-02 21:00:00 - INFO - layer4_mlp.down_proj | mse: 0.0020377292520911464, bpp_loss: 4.536625896181379, bpp: 0
pseudo compress quantization...:  16%|█▌        | 5/32 [01:34<08:29, 18.88s/it]2025-03-02 21:00:02 - INFO - layer5_self_attn.q_proj | mse: 0.0021227997948878585, bpp_loss: 5.25641655921936, bpp: 0
2025-03-02 21:00:02 - INFO - layer5_self_attn.k_proj | mse: 0.002272569290995601, bpp_loss: 6.296302914619446, bpp: 0
2025-03-02 21:00:03 - INFO - layer5_self_attn.v_proj | mse: 0.002005293937616283, bpp_loss: 4.242075085639954, bpp: 0
2025-03-02 21:00:04 - INFO - layer5_self_attn.o_proj | mse: 0.0020184530292277117, bpp_loss: 4.375704050064087, bpp: 0
2025-03-02 21:00:09 - INFO - layer5_mlp.gate_proj | mse: 0.0020795629606583242, bpp_loss: 4.892638887677874, bpp: 0
2025-03-02 21:00:14 - INFO - layer5_mlp.up_proj | mse: 0.0020389288346465417, bpp_loss: 4.541427135467529, bpp: 0
2025-03-02 21:00:19 - INFO - layer5_mlp.down_proj | mse: 0.0020382213354647413, bpp_loss: 4.541748114994594, bpp: 0
pseudo compress quantization...:  19%|█▉        | 6/32 [01:53<08:12, 18.94s/it]2025-03-02 21:00:21 - INFO - layer6_self_attn.q_proj | mse: 0.0021264976886626974, bpp_loss: 5.3046486377716064, bpp: 0
2025-03-02 21:00:21 - INFO - layer6_self_attn.k_proj | mse: 0.0022668179372344497, bpp_loss: 6.358419418334961, bpp: 0
2025-03-02 21:00:22 - INFO - layer6_self_attn.v_proj | mse: 0.0020092494871146492, bpp_loss: 4.287115931510925, bpp: 0
2025-03-02 21:00:23 - INFO - layer6_self_attn.o_proj | mse: 0.002022748800162837, bpp_loss: 4.407845854759216, bpp: 0
2025-03-02 21:00:28 - INFO - layer6_mlp.gate_proj | mse: 0.0020806874270385242, bpp_loss: 4.899926866803851, bpp: 0
2025-03-02 21:00:33 - INFO - layer6_mlp.up_proj | mse: 0.0020392164695873733, bpp_loss: 4.540552003043039, bpp: 0
2025-03-02 21:00:38 - INFO - layer6_mlp.down_proj | mse: 0.0020382422355575643, bpp_loss: 4.541880062648228, bpp: 0
pseudo compress quantization...:  22%|██▏       | 7/32 [02:12<07:50, 18.84s/it]2025-03-02 21:00:39 - INFO - layer7_self_attn.q_proj | mse: 0.0021182027315255025, bpp_loss: 5.220611333847046, bpp: 0
2025-03-02 21:00:40 - INFO - layer7_self_attn.k_proj | mse: 0.002271352170986079, bpp_loss: 6.387667536735535, bpp: 0
2025-03-02 21:00:40 - INFO - layer7_self_attn.v_proj | mse: 0.0020099636081080375, bpp_loss: 4.2850000858306885, bpp: 0
2025-03-02 21:00:42 - INFO - layer7_self_attn.o_proj | mse: 0.002023736691679045, bpp_loss: 4.419977903366089, bpp: 0
2025-03-02 21:00:46 - INFO - layer7_mlp.gate_proj | mse: 0.002077586763762683, bpp_loss: 4.870955467224121, bpp: 0
2025-03-02 21:00:52 - INFO - layer7_mlp.up_proj | mse: 0.0020406159480800667, bpp_loss: 4.554443563733782, bpp: 0
2025-03-02 21:00:57 - INFO - layer7_mlp.down_proj | mse: 0.0020395483878931963, bpp_loss: 4.557101147515433, bpp: 0
pseudo compress quantization...:  25%|██▌       | 8/32 [02:30<07:30, 18.78s/it]2025-03-02 21:00:58 - INFO - layer8_self_attn.q_proj | mse: 0.0021186246133496896, bpp_loss: 5.206719160079956, bpp: 0
2025-03-02 21:00:58 - INFO - layer8_self_attn.k_proj | mse: 0.0022676991813710677, bpp_loss: 6.28777289390564, bpp: 0
2025-03-02 21:00:59 - INFO - layer8_self_attn.v_proj | mse: 0.0020117896502526416, bpp_loss: 4.307764172554016, bpp: 0
2025-03-02 21:01:00 - INFO - layer8_self_attn.o_proj | mse: 0.0020255781243623, bpp_loss: 4.432040333747864, bpp: 0
2025-03-02 21:01:05 - INFO - layer8_mlp.gate_proj | mse: 0.0020781423751411957, bpp_loss: 4.877561705453055, bpp: 0
2025-03-02 21:01:10 - INFO - layer8_mlp.up_proj | mse: 0.0020403881163751345, bpp_loss: 4.549878869737897, bpp: 0
2025-03-02 21:01:15 - INFO - layer8_mlp.down_proj | mse: 0.002038723351555718, bpp_loss: 4.555212804249355, bpp: 0
pseudo compress quantization...:  28%|██▊       | 9/32 [02:49<07:12, 18.79s/it]2025-03-02 21:01:17 - INFO - layer9_self_attn.q_proj | mse: 0.002116020715275168, bpp_loss: 5.2186702489852905, bpp: 0
2025-03-02 21:01:17 - INFO - layer9_self_attn.k_proj | mse: 0.002265305862078318, bpp_loss: 6.309414982795715, bpp: 0
2025-03-02 21:01:18 - INFO - layer9_self_attn.v_proj | mse: 0.0020253608461695857, bpp_loss: 4.409639835357666, bpp: 0
2025-03-02 21:01:19 - INFO - layer9_self_attn.o_proj | mse: 0.0020319402109101395, bpp_loss: 4.4886497259140015, bpp: 0
2025-03-02 21:01:24 - INFO - layer9_mlp.gate_proj | mse: 0.002080065597544137, bpp_loss: 4.89493111201695, bpp: 0
2025-03-02 21:01:29 - INFO - layer9_mlp.up_proj | mse: 0.0020410169815443338, bpp_loss: 4.559728690556118, bpp: 0
2025-03-02 21:01:34 - INFO - layer9_mlp.down_proj | mse: 0.00203927792827617, bpp_loss: 4.557270697184971, bpp: 0
pseudo compress quantization...:  31%|███▏      | 10/32 [03:08<06:52, 18.76s/it]2025-03-02 21:01:35 - INFO - layer10_self_attn.q_proj | mse: 0.0021202217891313014, bpp_loss: 5.224997282028198, bpp: 0
2025-03-02 21:01:36 - INFO - layer10_self_attn.k_proj | mse: 0.0022725006672997356, bpp_loss: 6.314556121826172, bpp: 0
2025-03-02 21:01:36 - INFO - layer10_self_attn.v_proj | mse: 0.0020121979521897843, bpp_loss: 4.298290848731995, bpp: 0
2025-03-02 21:01:38 - INFO - layer10_self_attn.o_proj | mse: 0.0020244040978609997, bpp_loss: 4.424322962760925, bpp: 0
2025-03-02 21:01:43 - INFO - layer10_mlp.gate_proj | mse: 0.0020761601772288038, bpp_loss: 4.860466139657157, bpp: 0
2025-03-02 21:01:48 - INFO - layer10_mlp.up_proj | mse: 0.002043203007094653, bpp_loss: 4.576058523995536, bpp: 0
2025-03-02 21:01:53 - INFO - layer10_mlp.down_proj | mse: 0.002040687132554632, bpp_loss: 4.573565653392246, bpp: 0
pseudo compress quantization...:  34%|███▍      | 11/32 [03:27<06:34, 18.80s/it]2025-03-02 21:01:54 - INFO - layer11_self_attn.q_proj | mse: 0.002108328886187916, bpp_loss: 5.151569604873657, bpp: 0
2025-03-02 21:01:55 - INFO - layer11_self_attn.k_proj | mse: 0.0022608226887019765, bpp_loss: 6.307459831237793, bpp: 0
2025-03-02 21:01:55 - INFO - layer11_self_attn.v_proj | mse: 0.0020105645633760535, bpp_loss: 4.302067279815674, bpp: 0
2025-03-02 21:01:57 - INFO - layer11_self_attn.o_proj | mse: 0.0020271772918606782, bpp_loss: 4.443593621253967, bpp: 0
2025-03-02 21:02:01 - INFO - layer11_mlp.gate_proj | mse: 0.0020738951038026584, bpp_loss: 4.840528215680804, bpp: 0
2025-03-02 21:02:07 - INFO - layer11_mlp.up_proj | mse: 0.002043582312739649, bpp_loss: 4.581678526742118, bpp: 0
2025-03-02 21:02:12 - INFO - layer11_mlp.down_proj | mse: 0.0020417869740729914, bpp_loss: 4.581337077277047, bpp: 0
pseudo compress quantization...:  38%|███▊      | 12/32 [03:46<06:16, 18.81s/it]2025-03-02 21:02:13 - INFO - layer12_self_attn.q_proj | mse: 0.00211539187597242, bpp_loss: 5.210153579711914, bpp: 0
2025-03-02 21:02:14 - INFO - layer12_self_attn.k_proj | mse: 0.0022545373737822296, bpp_loss: 6.29413902759552, bpp: 0
2025-03-02 21:02:14 - INFO - layer12_self_attn.v_proj | mse: 0.0020238131011768162, bpp_loss: 4.419206023216248, bpp: 0
2025-03-02 21:02:16 - INFO - layer12_self_attn.o_proj | mse: 0.002032996545057053, bpp_loss: 4.489535212516785, bpp: 0
2025-03-02 21:02:20 - INFO - layer12_mlp.gate_proj | mse: 0.002071913940548327, bpp_loss: 4.823076111929757, bpp: 0
2025-03-02 21:02:26 - INFO - layer12_mlp.up_proj | mse: 0.002046407136176549, bpp_loss: 4.602205548967634, bpp: 0
2025-03-02 21:02:31 - INFO - layer12_mlp.down_proj | mse: 0.0020437110242023807, bpp_loss: 4.59462833404541, bpp: 0
pseudo compress quantization...:  41%|████      | 13/32 [04:05<05:59, 18.92s/it]2025-03-02 21:02:32 - INFO - layer13_self_attn.q_proj | mse: 0.0021137269818207456, bpp_loss: 5.188850045204163, bpp: 0
2025-03-02 21:02:33 - INFO - layer13_self_attn.k_proj | mse: 0.0022693834494949255, bpp_loss: 6.3285151720047, bpp: 0
2025-03-02 21:02:33 - INFO - layer13_self_attn.v_proj | mse: 0.0020173796749557026, bpp_loss: 4.360058307647705, bpp: 0
2025-03-02 21:02:35 - INFO - layer13_self_attn.o_proj | mse: 0.0020301525545795814, bpp_loss: 4.471097230911255, bpp: 0
2025-03-02 21:02:40 - INFO - layer13_mlp.gate_proj | mse: 0.0020719236663340962, bpp_loss: 4.827086312430246, bpp: 0
2025-03-02 21:02:45 - INFO - layer13_mlp.up_proj | mse: 0.002046836105408451, bpp_loss: 4.605423927307129, bpp: 0
2025-03-02 21:02:50 - INFO - layer13_mlp.down_proj | mse: 0.0020436541180090507, bpp_loss: 4.594102246420724, bpp: 0
pseudo compress quantization...:  44%|████▍     | 14/32 [04:23<05:38, 18.83s/it]2025-03-02 21:02:51 - INFO - layer14_self_attn.q_proj | mse: 0.002110307566262902, bpp_loss: 5.152938723564148, bpp: 0
2025-03-02 21:02:52 - INFO - layer14_self_attn.k_proj | mse: 0.0022674234191314806, bpp_loss: 6.26961076259613, bpp: 0
2025-03-02 21:02:52 - INFO - layer14_self_attn.v_proj | mse: 0.0020165779184245862, bpp_loss: 4.3527761697769165, bpp: 0
2025-03-02 21:02:54 - INFO - layer14_self_attn.o_proj | mse: 0.0020294657744254433, bpp_loss: 4.463836669921875, bpp: 0
2025-03-02 21:02:58 - INFO - layer14_mlp.gate_proj | mse: 0.0020762210151227638, bpp_loss: 4.862515040806362, bpp: 0
2025-03-02 21:03:03 - INFO - layer14_mlp.up_proj | mse: 0.0020460828743403743, bpp_loss: 4.599917139325823, bpp: 0
2025-03-02 21:03:08 - INFO - layer14_mlp.down_proj | mse: 0.0020436957112630848, bpp_loss: 4.589890275682722, bpp: 0
pseudo compress quantization...:  47%|████▋     | 15/32 [04:42<05:18, 18.72s/it]2025-03-02 21:03:09 - INFO - layer15_self_attn.q_proj | mse: 0.0021310082028782886, bpp_loss: 5.3130176067352295, bpp: 0
2025-03-02 21:03:10 - INFO - layer15_self_attn.k_proj | mse: 0.002260546021136587, bpp_loss: 6.312330603599548, bpp: 0
2025-03-02 21:03:11 - INFO - layer15_self_attn.v_proj | mse: 0.0020244877499651947, bpp_loss: 4.4165791273117065, bpp: 0
2025-03-02 21:03:12 - INFO - layer15_self_attn.o_proj | mse: 0.002032783974558514, bpp_loss: 4.493864417076111, bpp: 0
2025-03-02 21:03:16 - INFO - layer15_mlp.gate_proj | mse: 0.002080777649221404, bpp_loss: 4.900191579546247, bpp: 0
2025-03-02 21:03:21 - INFO - layer15_mlp.up_proj | mse: 0.002045065391603634, bpp_loss: 4.59149169921875, bpp: 0
2025-03-02 21:03:26 - INFO - layer15_mlp.down_proj | mse: 0.0020428864017281263, bpp_loss: 4.58391056741987, bpp: 0
pseudo compress quantization...:  50%|█████     | 16/32 [05:00<04:56, 18.56s/it]2025-03-02 21:03:28 - INFO - layer16_self_attn.q_proj | mse: 0.0021245564890759547, bpp_loss: 5.277305722236633, bpp: 0
2025-03-02 21:03:29 - INFO - layer16_self_attn.k_proj | mse: 0.0022650429554651333, bpp_loss: 6.306827425956726, bpp: 0
2025-03-02 21:03:29 - INFO - layer16_self_attn.v_proj | mse: 0.002018126206612837, bpp_loss: 4.372990727424622, bpp: 0
2025-03-02 21:03:31 - INFO - layer16_self_attn.o_proj | mse: 0.0020306874210639803, bpp_loss: 4.474428653717041, bpp: 0
2025-03-02 21:03:35 - INFO - layer16_mlp.gate_proj | mse: 0.0020845961582596512, bpp_loss: 4.9353147234235495, bpp: 0
2025-03-02 21:03:40 - INFO - layer16_mlp.up_proj | mse: 0.0020438415980496213, bpp_loss: 4.577877589634487, bpp: 0
2025-03-02 21:03:45 - INFO - layer16_mlp.down_proj | mse: 0.0020415667988377072, bpp_loss: 4.569388287408011, bpp: 0
pseudo compress quantization...:  53%|█████▎    | 17/32 [05:19<04:39, 18.63s/it]2025-03-02 21:03:46 - INFO - layer17_self_attn.q_proj | mse: 0.002126029816287719, bpp_loss: 5.296669244766235, bpp: 0
2025-03-02 21:03:47 - INFO - layer17_self_attn.k_proj | mse: 0.0022710794868542224, bpp_loss: 6.340497374534607, bpp: 0
2025-03-02 21:03:47 - INFO - layer17_self_attn.v_proj | mse: 0.002027775169020964, bpp_loss: 4.442517638206482, bpp: 0
2025-03-02 21:03:49 - INFO - layer17_self_attn.o_proj | mse: 0.00203403278097114, bpp_loss: 4.50321626663208, bpp: 0
2025-03-02 21:03:52 - INFO - layer17_mlp.gate_proj | mse: 0.002086794392775085, bpp_loss: 4.950778688703265, bpp: 0
2025-03-02 21:03:55 - INFO - layer17_mlp.up_proj | mse: 0.0020429337890672996, bpp_loss: 4.574952466147287, bpp: 0
2025-03-02 21:03:58 - INFO - layer17_mlp.down_proj | mse: 0.002041477404381276, bpp_loss: 4.566013370241437, bpp: 0
pseudo compress quantization...:  56%|█████▋    | 18/32 [05:32<03:57, 16.95s/it]2025-03-02 21:03:59 - INFO - layer18_self_attn.q_proj | mse: 0.0021246309068569595, bpp_loss: 5.286189317703247, bpp: 0
2025-03-02 21:03:59 - INFO - layer18_self_attn.k_proj | mse: 0.002276862889148799, bpp_loss: 6.431848406791687, bpp: 0
2025-03-02 21:03:59 - INFO - layer18_self_attn.v_proj | mse: 0.002016191525371876, bpp_loss: 4.367993235588074, bpp: 0
2025-03-02 21:04:00 - INFO - layer18_self_attn.o_proj | mse: 0.002031837909094403, bpp_loss: 4.482360482215881, bpp: 0
2025-03-02 21:04:03 - INFO - layer18_mlp.gate_proj | mse: 0.0020872800612687045, bpp_loss: 4.955394199916294, bpp: 0
2025-03-02 21:04:06 - INFO - layer18_mlp.up_proj | mse: 0.002042393490628083, bpp_loss: 4.570554392678397, bpp: 0
2025-03-02 21:04:08 - INFO - layer18_mlp.down_proj | mse: 0.0020405112406843393, bpp_loss: 4.564256020954677, bpp: 0
pseudo compress quantization...:  59%|█████▉    | 19/32 [05:42<03:14, 14.99s/it]2025-03-02 21:04:09 - INFO - layer19_self_attn.q_proj | mse: 0.0021247696027699753, bpp_loss: 5.2900389432907104, bpp: 0
2025-03-02 21:04:10 - INFO - layer19_self_attn.k_proj | mse: 0.002259729236197141, bpp_loss: 6.315402984619141, bpp: 0
2025-03-02 21:04:10 - INFO - layer19_self_attn.v_proj | mse: 0.0020220844720886125, bpp_loss: 4.412238597869873, bpp: 0
2025-03-02 21:04:11 - INFO - layer19_self_attn.o_proj | mse: 0.0020320198795808013, bpp_loss: 4.495166301727295, bpp: 0
2025-03-02 21:04:13 - INFO - layer19_mlp.gate_proj | mse: 0.002088258226999144, bpp_loss: 4.965449060712542, bpp: 0
2025-03-02 21:04:16 - INFO - layer19_mlp.up_proj | mse: 0.002041645432850309, bpp_loss: 4.566730976104736, bpp: 0
2025-03-02 21:04:19 - INFO - layer19_mlp.down_proj | mse: 0.002040924483113721, bpp_loss: 4.562402384621756, bpp: 0
pseudo compress quantization...:  62%|██████▎   | 20/32 [05:53<02:42, 13.58s/it]2025-03-02 21:04:20 - INFO - layer20_self_attn.q_proj | mse: 0.002122085854959631, bpp_loss: 5.260354518890381, bpp: 0
2025-03-02 21:04:20 - INFO - layer20_self_attn.k_proj | mse: 0.0022576734223637416, bpp_loss: 6.257520794868469, bpp: 0
2025-03-02 21:04:20 - INFO - layer20_self_attn.v_proj | mse: 0.0020274592103157255, bpp_loss: 4.4469640254974365, bpp: 0
2025-03-02 21:04:21 - INFO - layer20_self_attn.o_proj | mse: 0.002032387985052292, bpp_loss: 4.483432650566101, bpp: 0
2025-03-02 21:04:24 - INFO - layer20_mlp.gate_proj | mse: 0.002088135309621551, bpp_loss: 4.966340065002441, bpp: 0
2025-03-02 21:04:26 - INFO - layer20_mlp.up_proj | mse: 0.0020422918872065373, bpp_loss: 4.570367336273193, bpp: 0
2025-03-02 21:04:29 - INFO - layer20_mlp.down_proj | mse: 0.002041334414637309, bpp_loss: 4.56639518056597, bpp: 0
pseudo compress quantization...:  66%|██████▌   | 21/32 [06:03<02:18, 12.55s/it]2025-03-02 21:04:30 - INFO - layer21_self_attn.q_proj | mse: 0.0021211490238187106, bpp_loss: 5.254759192466736, bpp: 0
2025-03-02 21:04:30 - INFO - layer21_self_attn.k_proj | mse: 0.002260894028375251, bpp_loss: 6.305242538452148, bpp: 0
2025-03-02 21:04:30 - INFO - layer21_self_attn.v_proj | mse: 0.0020314256237237275, bpp_loss: 4.472708582878113, bpp: 0
2025-03-02 21:04:31 - INFO - layer21_self_attn.o_proj | mse: 0.0020349190949322544, bpp_loss: 4.503135681152344, bpp: 0
2025-03-02 21:04:34 - INFO - layer21_mlp.gate_proj | mse: 0.002089748548469553, bpp_loss: 4.978444371904645, bpp: 0
2025-03-02 21:04:36 - INFO - layer21_mlp.up_proj | mse: 0.002042564830002909, bpp_loss: 4.573340347834995, bpp: 0
2025-03-02 21:04:39 - INFO - layer21_mlp.down_proj | mse: 0.0020416675745328046, bpp_loss: 4.567831311907087, bpp: 0
pseudo compress quantization...:  69%|██████▉   | 22/32 [06:13<01:58, 11.84s/it]2025-03-02 21:04:40 - INFO - layer22_self_attn.q_proj | mse: 0.002116544174654449, bpp_loss: 5.210860729217529, bpp: 0
2025-03-02 21:04:40 - INFO - layer22_self_attn.k_proj | mse: 0.0022527412073891236, bpp_loss: 6.231993794441223, bpp: 0
2025-03-02 21:04:40 - INFO - layer22_self_attn.v_proj | mse: 0.002035819531975895, bpp_loss: 4.523005962371826, bpp: 0
2025-03-02 21:04:41 - INFO - layer22_self_attn.o_proj | mse: 0.0020371190366334883, bpp_loss: 4.529124140739441, bpp: 0
2025-03-02 21:04:44 - INFO - layer22_mlp.gate_proj | mse: 0.0020900440468116445, bpp_loss: 4.980461665562221, bpp: 0
2025-03-02 21:04:47 - INFO - layer22_mlp.up_proj | mse: 0.002043097885835702, bpp_loss: 4.577251025608608, bpp: 0
2025-03-02 21:04:49 - INFO - layer22_mlp.down_proj | mse: 0.002042002389881197, bpp_loss: 4.572938135692051, bpp: 0
pseudo compress quantization...:  72%|███████▏  | 23/32 [06:23<01:42, 11.34s/it]2025-03-02 21:04:50 - INFO - layer23_self_attn.q_proj | mse: 0.0021164607036154143, bpp_loss: 5.22284197807312, bpp: 0
2025-03-02 21:04:50 - INFO - layer23_self_attn.k_proj | mse: 0.002254366629335788, bpp_loss: 6.23278272151947, bpp: 0
2025-03-02 21:04:51 - INFO - layer23_self_attn.v_proj | mse: 0.0020421128137627083, bpp_loss: 4.575304985046387, bpp: 0
2025-03-02 21:04:51 - INFO - layer23_self_attn.o_proj | mse: 0.002038741561537583, bpp_loss: 4.5510014295578, bpp: 0
2025-03-02 21:04:55 - INFO - layer23_mlp.gate_proj | mse: 0.002089761171297891, bpp_loss: 4.98276560647147, bpp: 0
2025-03-02 21:04:58 - INFO - layer23_mlp.up_proj | mse: 0.0020433890386139394, bpp_loss: 4.581254005432129, bpp: 0
2025-03-02 21:05:01 - INFO - layer23_mlp.down_proj | mse: 0.0020425958697447254, bpp_loss: 4.578108583177839, bpp: 0
pseudo compress quantization...:  75%|███████▌  | 24/32 [06:35<01:31, 11.38s/it]2025-03-02 21:05:01 - INFO - layer24_self_attn.q_proj | mse: 0.0021125810204193378, bpp_loss: 5.188285708427429, bpp: 0
2025-03-02 21:05:02 - INFO - layer24_self_attn.k_proj | mse: 0.002231449030894072, bpp_loss: 6.0436131954193115, bpp: 0
2025-03-02 21:05:02 - INFO - layer24_self_attn.v_proj | mse: 0.0020551737680570945, bpp_loss: 4.67528223991394, bpp: 0
2025-03-02 21:05:03 - INFO - layer24_self_attn.o_proj | mse: 0.002044788594705987, bpp_loss: 4.593521595001221, bpp: 0
2025-03-02 21:05:05 - INFO - layer24_mlp.gate_proj | mse: 0.002091112227793351, bpp_loss: 4.989077840532575, bpp: 0
2025-03-02 21:05:08 - INFO - layer24_mlp.up_proj | mse: 0.0020443618241224636, bpp_loss: 4.585970197405134, bpp: 0
2025-03-02 21:05:11 - INFO - layer24_mlp.down_proj | mse: 0.002043207973453344, bpp_loss: 4.583559785570417, bpp: 0
pseudo compress quantization...:  78%|███████▊  | 25/32 [06:45<01:16, 10.98s/it]2025-03-02 21:05:12 - INFO - layer25_self_attn.q_proj | mse: 0.0021093625871897548, bpp_loss: 5.164109706878662, bpp: 0
2025-03-02 21:05:12 - INFO - layer25_self_attn.k_proj | mse: 0.00222660680530427, bpp_loss: 6.024110317230225, bpp: 0
2025-03-02 21:05:12 - INFO - layer25_self_attn.v_proj | mse: 0.002055452427339251, bpp_loss: 4.6853721141815186, bpp: 0
2025-03-02 21:05:13 - INFO - layer25_self_attn.o_proj | mse: 0.002046722913816627, bpp_loss: 4.594152808189392, bpp: 0
2025-03-02 21:05:16 - INFO - layer25_mlp.gate_proj | mse: 0.0020919600266081613, bpp_loss: 4.999738693237305, bpp: 0
2025-03-02 21:05:18 - INFO - layer25_mlp.up_proj | mse: 0.002045141128573666, bpp_loss: 4.595420837402344, bpp: 0
2025-03-02 21:05:21 - INFO - layer25_mlp.down_proj | mse: 0.002044336164602562, bpp_loss: 4.592635325023106, bpp: 0
pseudo compress quantization...:  81%|████████▏ | 26/32 [06:55<01:03, 10.67s/it]2025-03-02 21:05:21 - INFO - layer26_self_attn.q_proj | mse: 0.002112175796589925, bpp_loss: 5.174656391143799, bpp: 0
2025-03-02 21:05:22 - INFO - layer26_self_attn.k_proj | mse: 0.0022413188927981253, bpp_loss: 6.1162227392196655, bpp: 0
2025-03-02 21:05:22 - INFO - layer26_self_attn.v_proj | mse: 0.0020642585314248067, bpp_loss: 4.738374829292297, bpp: 0
2025-03-02 21:05:23 - INFO - layer26_self_attn.o_proj | mse: 0.00204763005027121, bpp_loss: 4.611484050750732, bpp: 0
2025-03-02 21:05:25 - INFO - layer26_mlp.gate_proj | mse: 0.002092957023115303, bpp_loss: 5.009598731994629, bpp: 0
2025-03-02 21:05:28 - INFO - layer26_mlp.up_proj | mse: 0.002046067147537854, bpp_loss: 4.604663031441825, bpp: 0
2025-03-02 21:05:31 - INFO - layer26_mlp.down_proj | mse: 0.002045426487266764, bpp_loss: 4.6006041935512, bpp: 0
pseudo compress quantization...:  84%|████████▍ | 27/32 [07:05<00:52, 10.48s/it]2025-03-02 21:05:32 - INFO - layer27_self_attn.q_proj | mse: 0.002108337396250464, bpp_loss: 5.142436742782593, bpp: 0
2025-03-02 21:05:32 - INFO - layer27_self_attn.k_proj | mse: 0.00224016460239933, bpp_loss: 6.068073630332947, bpp: 0
2025-03-02 21:05:32 - INFO - layer27_self_attn.v_proj | mse: 0.00207346533271076, bpp_loss: 4.833148837089539, bpp: 0
2025-03-02 21:05:33 - INFO - layer27_self_attn.o_proj | mse: 0.00205162326132298, bpp_loss: 4.652828097343445, bpp: 0
2025-03-02 21:05:35 - INFO - layer27_mlp.gate_proj | mse: 0.002095235547096235, bpp_loss: 5.0237835475376675, bpp: 0
2025-03-02 21:05:38 - INFO - layer27_mlp.up_proj | mse: 0.002047601752373254, bpp_loss: 4.6202690941946845, bpp: 0
2025-03-02 21:05:41 - INFO - layer27_mlp.down_proj | mse: 0.00204650439403424, bpp_loss: 4.611804178782871, bpp: 0
pseudo compress quantization...:  88%|████████▊ | 28/32 [07:15<00:41, 10.39s/it]2025-03-02 21:05:42 - INFO - layer28_self_attn.q_proj | mse: 0.002108489853115685, bpp_loss: 5.151460886001587, bpp: 0
2025-03-02 21:05:42 - INFO - layer28_self_attn.k_proj | mse: 0.0022185832648428075, bpp_loss: 6.005569100379944, bpp: 0
2025-03-02 21:05:42 - INFO - layer28_self_attn.v_proj | mse: 0.0020826688748238225, bpp_loss: 4.887310147285461, bpp: 0
2025-03-02 21:05:43 - INFO - layer28_self_attn.o_proj | mse: 0.002054808921758495, bpp_loss: 4.683029890060425, bpp: 0
2025-03-02 21:05:46 - INFO - layer28_mlp.gate_proj | mse: 0.0020942211683336763, bpp_loss: 5.0117499487740655, bpp: 0
2025-03-02 21:05:48 - INFO - layer28_mlp.up_proj | mse: 0.0020505550803412735, bpp_loss: 4.643125397818429, bpp: 0
2025-03-02 21:05:51 - INFO - layer28_mlp.down_proj | mse: 0.0020492456170998477, bpp_loss: 4.627546651022775, bpp: 0
pseudo compress quantization...:  91%|█████████ | 29/32 [07:25<00:30, 10.25s/it]2025-03-02 21:05:52 - INFO - layer29_self_attn.q_proj | mse: 0.002106401628618539, bpp_loss: 5.1378291845321655, bpp: 0
2025-03-02 21:05:52 - INFO - layer29_self_attn.k_proj | mse: 0.0022366059477329885, bpp_loss: 6.1073795557022095, bpp: 0
2025-03-02 21:05:52 - INFO - layer29_self_attn.v_proj | mse: 0.002089484529598953, bpp_loss: 4.957388997077942, bpp: 0
2025-03-02 21:05:53 - INFO - layer29_self_attn.o_proj | mse: 0.002060212268280982, bpp_loss: 4.718843340873718, bpp: 0
2025-03-02 21:05:56 - INFO - layer29_mlp.gate_proj | mse: 0.002092975853892005, bpp_loss: 5.005994524274554, bpp: 0
2025-03-02 21:05:58 - INFO - layer29_mlp.up_proj | mse: 0.002054260190856085, bpp_loss: 4.676926067897251, bpp: 0
2025-03-02 21:06:01 - INFO - layer29_mlp.down_proj | mse: 0.0020520418839742767, bpp_loss: 4.642790590013776, bpp: 0
pseudo compress quantization...:  94%|█████████▍| 30/32 [07:35<00:20, 10.13s/it]2025-03-02 21:06:01 - INFO - layer30_self_attn.q_proj | mse: 0.002095386348508559, bpp_loss: 5.042828559875488, bpp: 0
2025-03-02 21:06:02 - INFO - layer30_self_attn.k_proj | mse: 0.0022004226103002527, bpp_loss: 5.731076121330261, bpp: 0
2025-03-02 21:06:02 - INFO - layer30_self_attn.v_proj | mse: 0.002123323978527782, bpp_loss: 5.261212587356567, bpp: 0
2025-03-02 21:06:03 - INFO - layer30_self_attn.o_proj | mse: 0.0020696850542878498, bpp_loss: 4.8077744245529175, bpp: 0
2025-03-02 21:06:05 - INFO - layer30_mlp.gate_proj | mse: 0.002099068748278942, bpp_loss: 5.063662937709263, bpp: 0
2025-03-02 21:06:08 - INFO - layer30_mlp.up_proj | mse: 0.002057772234176799, bpp_loss: 4.710258211408343, bpp: 0
2025-03-02 21:06:11 - INFO - layer30_mlp.down_proj | mse: 0.0020527309662425994, bpp_loss: 4.64414736202785, bpp: 0
pseudo compress quantization...:  97%|█████████▋| 31/32 [07:44<00:10, 10.08s/it]2025-03-02 21:06:11 - INFO - layer31_self_attn.q_proj | mse: 0.002117541119428687, bpp_loss: 5.204469919204712, bpp: 0
2025-03-02 21:06:12 - INFO - layer31_self_attn.k_proj | mse: 0.00222953245616917, bpp_loss: 5.9919353723526, bpp: 0
2025-03-02 21:06:12 - INFO - layer31_self_attn.v_proj | mse: 0.0021011876762540346, bpp_loss: 5.0655295848846436, bpp: 0
2025-03-02 21:06:13 - INFO - layer31_self_attn.o_proj | mse: 0.0020678262393491777, bpp_loss: 4.772657752037048, bpp: 0
2025-03-02 21:06:15 - INFO - layer31_mlp.gate_proj | mse: 0.0021274804586215064, bpp_loss: 5.2984724044799805, bpp: 0
2025-03-02 21:06:18 - INFO - layer31_mlp.up_proj | mse: 0.0020813676112475263, bpp_loss: 4.913950511387417, bpp: 0
2025-03-02 21:06:20 - INFO - layer31_mlp.down_proj | mse: 0.002056189207344165, bpp_loss: 4.672363315309797, bpp: 0
pseudo compress quantization...: 100%|██████████| 32/32 [07:54<00:00,  9.99s/it]pseudo compress quantization...: 100%|██████████| 32/32 [07:54<00:00, 14.84s/it]
2025-03-02 21:06:20 - INFO - #### Total | mse: 0.002063509231342302, bpp_loss: 4.750659859882524, bpp: 0 ####
## Strart saving /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda1000_rdloss_ql_encdim512_M16_batch_size2048_total_iter1500000_lr0.0001_seed100/best_loss_model_loss_6.59649_bpp_6.05166_MSE_0.00106_total_iter_140000.pth.tar/COL_MSE0.00206_bpploss4.7507_bpp0
## End saving
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda1000_rdloss_ql_encdim512_M16_batch_size2048_total_iter1500000_lr0.0001_seed100/best_loss_model_loss_6.59649_bpp_6.05166_MSE_0.00106_total_iter_140000.pth.tar/COL_MSE0.00206_bpploss4.7507_bpp0
I0302 21:07:07.723114 2587812 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.08it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.05it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.50it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
I0302 21:07:10.892520 2587812 config.py:54] PyTorch version 2.4.1 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.5546875:   0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.5546875:   1%|          | 1/141 [00:00<01:15,  1.84it/s]avg_loss = 1.86328125:   1%|          | 1/141 [00:00<01:15,  1.84it/s]avg_loss = 1.86328125:   1%|▏         | 2/141 [00:00<01:04,  2.14it/s]avg_loss = 1.9973958333333333:   1%|▏         | 2/141 [00:01<01:04,  2.14it/s]avg_loss = 1.9973958333333333:   2%|▏         | 3/141 [00:01<01:01,  2.26it/s]avg_loss = 1.953125:   2%|▏         | 3/141 [00:01<01:01,  2.26it/s]          avg_loss = 1.953125:   3%|▎         | 4/141 [00:01<00:59,  2.31it/s]avg_loss = 1.903125:   3%|▎         | 4/141 [00:02<00:59,  2.31it/s]avg_loss = 1.903125:   4%|▎         | 5/141 [00:02<00:58,  2.34it/s]avg_loss = 1.8072916666666667:   4%|▎         | 5/141 [00:02<00:58,  2.34it/s]avg_loss = 1.8072916666666667:   4%|▍         | 6/141 [00:02<00:57,  2.36it/s]avg_loss = 1.7433035714285714:   4%|▍         | 6/141 [00:03<00:57,  2.36it/s]avg_loss = 1.7433035714285714:   5%|▍         | 7/141 [00:03<00:56,  2.37it/s]avg_loss = 1.740234375:   5%|▍         | 7/141 [00:03<00:56,  2.37it/s]       avg_loss = 1.740234375:   6%|▌         | 8/141 [00:03<00:55,  2.38it/s]avg_loss = 1.7743055555555556:   6%|▌         | 8/141 [00:03<00:55,  2.38it/s]avg_loss = 1.7743055555555556:   6%|▋         | 9/141 [00:03<00:55,  2.39it/s]avg_loss = 1.7796875:   6%|▋         | 9/141 [00:04<00:55,  2.39it/s]         avg_loss = 1.7796875:   7%|▋         | 10/141 [00:04<00:54,  2.39it/s]avg_loss = 1.7769886363636365:   7%|▋         | 10/141 [00:04<00:54,  2.39it/s]avg_loss = 1.7769886363636365:   8%|▊         | 11/141 [00:04<00:54,  2.39it/s]avg_loss = 1.7994791666666667:   8%|▊         | 11/141 [00:05<00:54,  2.39it/s]avg_loss = 1.7994791666666667:   9%|▊         | 12/141 [00:05<00:54,  2.39it/s]avg_loss = 1.8118990384615385:   9%|▊         | 12/141 [00:05<00:54,  2.39it/s]avg_loss = 1.8118990384615385:   9%|▉         | 13/141 [00:05<00:53,  2.39it/s]avg_loss = 1.8309151785714286:   9%|▉         | 13/141 [00:05<00:53,  2.39it/s]avg_loss = 1.8309151785714286:  10%|▉         | 14/141 [00:05<00:53,  2.39it/s]avg_loss = 1.8411458333333333:  10%|▉         | 14/141 [00:06<00:53,  2.39it/s]avg_loss = 1.8411458333333333:  11%|█         | 15/141 [00:06<00:52,  2.38it/s]avg_loss = 1.86572265625:  11%|█         | 15/141 [00:06<00:52,  2.38it/s]     avg_loss = 1.86572265625:  11%|█▏        | 16/141 [00:06<00:52,  2.38it/s]avg_loss = 1.869485294117647:  11%|█▏        | 16/141 [00:07<00:52,  2.38it/s]avg_loss = 1.869485294117647:  12%|█▏        | 17/141 [00:07<00:52,  2.38it/s]avg_loss = 1.87109375:  12%|█▏        | 17/141 [00:07<00:52,  2.38it/s]       avg_loss = 1.87109375:  13%|█▎        | 18/141 [00:07<00:51,  2.38it/s]avg_loss = 1.8515625:  13%|█▎        | 18/141 [00:08<00:51,  2.38it/s] avg_loss = 1.8515625:  13%|█▎        | 19/141 [00:08<00:51,  2.38it/s]avg_loss = 1.85:  13%|█▎        | 19/141 [00:08<00:51,  2.38it/s]     avg_loss = 1.85:  14%|█▍        | 20/141 [00:08<00:50,  2.38it/s]avg_loss = 1.8549107142857142:  14%|█▍        | 20/141 [00:08<00:50,  2.38it/s]avg_loss = 1.8549107142857142:  15%|█▍        | 21/141 [00:08<00:50,  2.39it/s]avg_loss = 1.8572443181818181:  15%|█▍        | 21/141 [00:09<00:50,  2.39it/s]avg_loss = 1.8572443181818181:  16%|█▌        | 22/141 [00:09<00:49,  2.40it/s]avg_loss = 1.8590353260869565:  16%|█▌        | 22/141 [00:09<00:49,  2.40it/s]avg_loss = 1.8590353260869565:  16%|█▋        | 23/141 [00:09<00:49,  2.40it/s]avg_loss = 1.8642578125:  16%|█▋        | 23/141 [00:10<00:49,  2.40it/s]      avg_loss = 1.8642578125:  17%|█▋        | 24/141 [00:10<00:48,  2.40it/s]avg_loss = 1.8703125:  17%|█▋        | 24/141 [00:10<00:48,  2.40it/s]   avg_loss = 1.8703125:  18%|█▊        | 25/141 [00:10<00:48,  2.40it/s]avg_loss = 1.8819110576923077:  18%|█▊        | 25/141 [00:10<00:48,  2.40it/s]avg_loss = 1.8819110576923077:  18%|█▊        | 26/141 [00:10<00:47,  2.40it/s]avg_loss = 1.8949652777777777:  18%|█▊        | 26/141 [00:11<00:47,  2.40it/s]avg_loss = 1.8949652777777777:  19%|█▉        | 27/141 [00:11<00:47,  2.40it/s]avg_loss = 1.9020647321428572:  19%|█▉        | 27/141 [00:11<00:47,  2.40it/s]avg_loss = 1.9020647321428572:  20%|█▉        | 28/141 [00:11<00:47,  2.40it/s]avg_loss = 1.8987068965517242:  20%|█▉        | 28/141 [00:12<00:47,  2.40it/s]avg_loss = 1.8987068965517242:  21%|██        | 29/141 [00:12<00:46,  2.40it/s]avg_loss = 1.8893229166666667:  21%|██        | 29/141 [00:12<00:46,  2.40it/s]avg_loss = 1.8893229166666667:  21%|██▏       | 30/141 [00:12<00:46,  2.40it/s]avg_loss = 1.8747479838709677:  21%|██▏       | 30/141 [00:13<00:46,  2.40it/s]avg_loss = 1.8747479838709677:  22%|██▏       | 31/141 [00:13<00:45,  2.40it/s]avg_loss = 1.862548828125:  22%|██▏       | 31/141 [00:13<00:45,  2.40it/s]    avg_loss = 1.862548828125:  23%|██▎       | 32/141 [00:13<00:45,  2.40it/s]avg_loss = 1.8615056818181819:  23%|██▎       | 32/141 [00:13<00:45,  2.40it/s]avg_loss = 1.8615056818181819:  23%|██▎       | 33/141 [00:13<00:45,  2.40it/s]avg_loss = 1.8602941176470589:  23%|██▎       | 33/141 [00:14<00:45,  2.40it/s]avg_loss = 1.8602941176470589:  24%|██▍       | 34/141 [00:14<00:44,  2.40it/s]avg_loss = 1.863169642857143:  24%|██▍       | 34/141 [00:14<00:44,  2.40it/s] avg_loss = 1.863169642857143:  25%|██▍       | 35/141 [00:14<00:44,  2.40it/s]avg_loss = 1.8472222222222223:  25%|██▍       | 35/141 [00:15<00:44,  2.40it/s]avg_loss = 1.8472222222222223:  26%|██▌       | 36/141 [00:15<00:43,  2.40it/s]avg_loss = 1.8319256756756757:  26%|██▌       | 36/141 [00:15<00:43,  2.40it/s]avg_loss = 1.8319256756756757:  26%|██▌       | 37/141 [00:15<00:43,  2.40it/s]avg_loss = 1.8172286184210527:  26%|██▌       | 37/141 [00:15<00:43,  2.40it/s]avg_loss = 1.8172286184210527:  27%|██▋       | 38/141 [00:15<00:42,  2.40it/s]avg_loss = 1.803084935897436:  27%|██▋       | 38/141 [00:16<00:42,  2.40it/s] avg_loss = 1.803084935897436:  28%|██▊       | 39/141 [00:16<00:42,  2.40it/s]avg_loss = 1.794921875:  28%|██▊       | 39/141 [00:16<00:42,  2.40it/s]      avg_loss = 1.794921875:  28%|██▊       | 40/141 [00:16<00:42,  2.39it/s]avg_loss = 1.7997332317073171:  28%|██▊       | 40/141 [00:17<00:42,  2.39it/s]avg_loss = 1.7997332317073171:  29%|██▉       | 41/141 [00:17<00:41,  2.39it/s]avg_loss = 1.8167782738095237:  29%|██▉       | 41/141 [00:17<00:41,  2.39it/s]avg_loss = 1.8167782738095237:  30%|██▉       | 42/141 [00:17<00:41,  2.39it/s]avg_loss = 1.8333938953488371:  30%|██▉       | 42/141 [00:18<00:41,  2.39it/s]avg_loss = 1.8333938953488371:  30%|███       | 43/141 [00:18<00:40,  2.39it/s]avg_loss = 1.8361150568181819:  30%|███       | 43/141 [00:18<00:40,  2.39it/s]avg_loss = 1.8361150568181819:  31%|███       | 44/141 [00:18<00:40,  2.39it/s]avg_loss = 1.840451388888889:  31%|███       | 44/141 [00:18<00:40,  2.39it/s] avg_loss = 1.840451388888889:  32%|███▏      | 45/141 [00:18<00:40,  2.39it/s]avg_loss = 1.8459578804347827:  32%|███▏      | 45/141 [00:19<00:40,  2.39it/s]avg_loss = 1.8459578804347827:  33%|███▎      | 46/141 [00:19<00:39,  2.39it/s]avg_loss = 1.852559840425532:  33%|███▎      | 46/141 [00:19<00:39,  2.39it/s] avg_loss = 1.852559840425532:  33%|███▎      | 47/141 [00:19<00:39,  2.39it/s]avg_loss = 1.85595703125:  33%|███▎      | 47/141 [00:20<00:39,  2.39it/s]    avg_loss = 1.85595703125:  34%|███▍      | 48/141 [00:20<00:38,  2.39it/s]avg_loss = 1.854751275510204:  34%|███▍      | 48/141 [00:20<00:38,  2.39it/s]avg_loss = 1.854751275510204:  35%|███▍      | 49/141 [00:20<00:38,  2.39it/s]avg_loss = 1.8546875:  35%|███▍      | 49/141 [00:21<00:38,  2.39it/s]        avg_loss = 1.8546875:  35%|███▌      | 50/141 [00:21<00:38,  2.39it/s]avg_loss = 1.8481924019607843:  35%|███▌      | 50/141 [00:21<00:38,  2.39it/s]avg_loss = 1.8481924019607843:  36%|███▌      | 51/141 [00:21<00:37,  2.39it/s]avg_loss = 1.8443509615384615:  36%|███▌      | 51/141 [00:21<00:37,  2.39it/s]avg_loss = 1.8443509615384615:  37%|███▋      | 52/141 [00:21<00:37,  2.39it/s]avg_loss = 1.838001179245283:  37%|███▋      | 52/141 [00:22<00:37,  2.39it/s] avg_loss = 1.838001179245283:  38%|███▊      | 53/141 [00:22<00:36,  2.38it/s]avg_loss = 1.8350694444444444:  38%|███▊      | 53/141 [00:22<00:36,  2.38it/s]avg_loss = 1.8350694444444444:  38%|███▊      | 54/141 [00:22<00:36,  2.39it/s]avg_loss = 1.8275568181818183:  38%|███▊      | 54/141 [00:23<00:36,  2.39it/s]avg_loss = 1.8275568181818183:  39%|███▉      | 55/141 [00:23<00:36,  2.38it/s]avg_loss = 1.8198939732142858:  39%|███▉      | 55/141 [00:23<00:36,  2.38it/s]avg_loss = 1.8198939732142858:  40%|███▉      | 56/141 [00:23<00:35,  2.38it/s]avg_loss = 1.8133223684210527:  40%|███▉      | 56/141 [00:23<00:35,  2.38it/s]avg_loss = 1.8133223684210527:  40%|████      | 57/141 [00:23<00:35,  2.38it/s]avg_loss = 1.810479525862069:  40%|████      | 57/141 [00:24<00:35,  2.38it/s] avg_loss = 1.810479525862069:  41%|████      | 58/141 [00:24<00:34,  2.38it/s]avg_loss = 1.8127648305084745:  41%|████      | 58/141 [00:24<00:34,  2.38it/s]avg_loss = 1.8127648305084745:  42%|████▏     | 59/141 [00:24<00:34,  2.38it/s]avg_loss = 1.8184895833333334:  42%|████▏     | 59/141 [00:25<00:34,  2.38it/s]avg_loss = 1.8184895833333334:  43%|████▎     | 60/141 [00:25<00:34,  2.38it/s]avg_loss = 1.8245389344262295:  43%|████▎     | 60/141 [00:25<00:34,  2.38it/s]avg_loss = 1.8245389344262295:  43%|████▎     | 61/141 [00:25<00:33,  2.38it/s]avg_loss = 1.8319052419354838:  43%|████▎     | 61/141 [00:26<00:33,  2.38it/s]avg_loss = 1.8319052419354838:  44%|████▍     | 62/141 [00:26<00:33,  2.38it/s]avg_loss = 1.8226686507936507:  44%|████▍     | 62/141 [00:26<00:33,  2.38it/s]avg_loss = 1.8226686507936507:  45%|████▍     | 63/141 [00:26<00:32,  2.38it/s]avg_loss = 1.820556640625:  45%|████▍     | 63/141 [00:26<00:32,  2.38it/s]    avg_loss = 1.820556640625:  45%|████▌     | 64/141 [00:26<00:32,  2.38it/s]avg_loss = 1.8179086538461537:  45%|████▌     | 64/141 [00:27<00:32,  2.38it/s]avg_loss = 1.8179086538461537:  46%|████▌     | 65/141 [00:27<00:31,  2.38it/s]avg_loss = 1.811908143939394:  46%|████▌     | 65/141 [00:27<00:31,  2.38it/s] avg_loss = 1.811908143939394:  47%|████▋     | 66/141 [00:27<00:31,  2.38it/s]avg_loss = 1.8094682835820894:  47%|████▋     | 66/141 [00:28<00:31,  2.38it/s]avg_loss = 1.8094682835820894:  48%|████▊     | 67/141 [00:28<00:31,  2.38it/s]avg_loss = 1.8060661764705883:  48%|████▊     | 67/141 [00:28<00:31,  2.38it/s]avg_loss = 1.8060661764705883:  48%|████▊     | 68/141 [00:28<00:30,  2.38it/s]avg_loss = 1.8031023550724639:  48%|████▊     | 68/141 [00:28<00:30,  2.38it/s]avg_loss = 1.8031023550724639:  49%|████▉     | 69/141 [00:28<00:30,  2.38it/s]avg_loss = 1.8041294642857142:  49%|████▉     | 69/141 [00:29<00:30,  2.38it/s]avg_loss = 1.8041294642857142:  50%|████▉     | 70/141 [00:29<00:29,  2.38it/s]avg_loss = 1.8079885563380282:  50%|████▉     | 70/141 [00:29<00:29,  2.38it/s]avg_loss = 1.8079885563380282:  50%|█████     | 71/141 [00:29<00:29,  2.38it/s]avg_loss = 1.810546875:  50%|█████     | 71/141 [00:30<00:29,  2.38it/s]       avg_loss = 1.810546875:  51%|█████     | 72/141 [00:30<00:29,  2.38it/s]avg_loss = 1.8091823630136987:  51%|█████     | 72/141 [00:30<00:29,  2.38it/s]avg_loss = 1.8091823630136987:  52%|█████▏    | 73/141 [00:30<00:28,  2.38it/s]avg_loss = 1.810916385135135:  52%|█████▏    | 73/141 [00:31<00:28,  2.38it/s] avg_loss = 1.810916385135135:  52%|█████▏    | 74/141 [00:31<00:28,  2.38it/s]avg_loss = 1.81125:  52%|█████▏    | 74/141 [00:31<00:28,  2.38it/s]          avg_loss = 1.81125:  53%|█████▎    | 75/141 [00:31<00:27,  2.38it/s]avg_loss = 1.8102384868421053:  53%|█████▎    | 75/141 [00:31<00:27,  2.38it/s]avg_loss = 1.8102384868421053:  54%|█████▍    | 76/141 [00:31<00:27,  2.38it/s]avg_loss = 1.8115868506493507:  54%|█████▍    | 76/141 [00:32<00:27,  2.38it/s]avg_loss = 1.8115868506493507:  55%|█████▍    | 77/141 [00:32<00:26,  2.38it/s]avg_loss = 1.8140024038461537:  55%|█████▍    | 77/141 [00:32<00:26,  2.38it/s]avg_loss = 1.8140024038461537:  55%|█████▌    | 78/141 [00:32<00:26,  2.38it/s]avg_loss = 1.8183346518987342:  55%|█████▌    | 78/141 [00:33<00:26,  2.38it/s]avg_loss = 1.8183346518987342:  56%|█████▌    | 79/141 [00:33<00:26,  2.37it/s]avg_loss = 1.815625:  56%|█████▌    | 79/141 [00:33<00:26,  2.37it/s]          avg_loss = 1.815625:  57%|█████▋    | 80/141 [00:33<00:25,  2.37it/s]avg_loss = 1.8147183641975309:  57%|█████▋    | 80/141 [00:34<00:25,  2.37it/s]avg_loss = 1.8147183641975309:  57%|█████▋    | 81/141 [00:34<00:25,  2.37it/s]avg_loss = 1.8141196646341464:  57%|█████▋    | 81/141 [00:34<00:25,  2.37it/s]avg_loss = 1.8141196646341464:  58%|█████▊    | 82/141 [00:34<00:24,  2.37it/s]avg_loss = 1.8124058734939759:  58%|█████▊    | 82/141 [00:34<00:24,  2.37it/s]avg_loss = 1.8124058734939759:  59%|█████▉    | 83/141 [00:34<00:24,  2.37it/s]avg_loss = 1.8103608630952381:  59%|█████▉    | 83/141 [00:35<00:24,  2.37it/s]avg_loss = 1.8103608630952381:  60%|█████▉    | 84/141 [00:35<00:24,  2.37it/s]avg_loss = 1.8081801470588235:  60%|█████▉    | 84/141 [00:35<00:24,  2.37it/s]avg_loss = 1.8081801470588235:  60%|██████    | 85/141 [00:35<00:23,  2.37it/s]avg_loss = 1.8099563953488371:  60%|██████    | 85/141 [00:36<00:23,  2.37it/s]avg_loss = 1.8099563953488371:  61%|██████    | 86/141 [00:36<00:23,  2.37it/s]avg_loss = 1.8120510057471264:  61%|██████    | 86/141 [00:36<00:23,  2.37it/s]avg_loss = 1.8120510057471264:  62%|██████▏   | 87/141 [00:36<00:22,  2.37it/s]avg_loss = 1.8124112215909092:  62%|██████▏   | 87/141 [00:36<00:22,  2.37it/s]avg_loss = 1.8124112215909092:  62%|██████▏   | 88/141 [00:36<00:22,  2.37it/s]avg_loss = 1.821190308988764:  62%|██████▏   | 88/141 [00:37<00:22,  2.37it/s] avg_loss = 1.821190308988764:  63%|██████▎   | 89/141 [00:37<00:21,  2.37it/s]avg_loss = 1.82890625:  63%|██████▎   | 89/141 [00:37<00:21,  2.37it/s]       avg_loss = 1.82890625:  64%|██████▍   | 90/141 [00:37<00:21,  2.37it/s]avg_loss = 1.8321600274725274:  64%|██████▍   | 90/141 [00:38<00:21,  2.37it/s]avg_loss = 1.8321600274725274:  65%|██████▍   | 91/141 [00:38<00:21,  2.37it/s]avg_loss = 1.837211277173913:  65%|██████▍   | 91/141 [00:38<00:21,  2.37it/s] avg_loss = 1.837211277173913:  65%|██████▌   | 92/141 [00:38<00:20,  2.37it/s]avg_loss = 1.8423219086021505:  65%|██████▌   | 92/141 [00:39<00:20,  2.37it/s]avg_loss = 1.8423219086021505:  66%|██████▌   | 93/141 [00:39<00:20,  2.37it/s]avg_loss = 1.843500664893617:  66%|██████▌   | 93/141 [00:39<00:20,  2.37it/s] avg_loss = 1.843500664893617:  67%|██████▋   | 94/141 [00:39<00:19,  2.37it/s]avg_loss = 1.8472861842105264:  67%|██████▋   | 94/141 [00:39<00:19,  2.37it/s]avg_loss = 1.8472861842105264:  67%|██████▋   | 95/141 [00:39<00:19,  2.37it/s]avg_loss = 1.84814453125:  67%|██████▋   | 95/141 [00:40<00:19,  2.37it/s]     avg_loss = 1.84814453125:  68%|██████▊   | 96/141 [00:40<00:18,  2.37it/s]avg_loss = 1.8501932989690721:  68%|██████▊   | 96/141 [00:40<00:18,  2.37it/s]avg_loss = 1.8501932989690721:  69%|██████▉   | 97/141 [00:40<00:18,  2.37it/s]avg_loss = 1.8451849489795917:  69%|██████▉   | 97/141 [00:41<00:18,  2.37it/s]avg_loss = 1.8451849489795917:  70%|██████▉   | 98/141 [00:41<00:18,  2.37it/s]avg_loss = 1.8458806818181819:  70%|██████▉   | 98/141 [00:41<00:18,  2.37it/s]avg_loss = 1.8458806818181819:  70%|███████   | 99/141 [00:41<00:17,  2.36it/s]avg_loss = 1.847734375:  70%|███████   | 99/141 [00:42<00:17,  2.36it/s]       avg_loss = 1.847734375:  71%|███████   | 100/141 [00:42<00:17,  2.37it/s]avg_loss = 1.8462252475247525:  71%|███████   | 100/141 [00:42<00:17,  2.37it/s]avg_loss = 1.8462252475247525:  72%|███████▏  | 101/141 [00:42<00:16,  2.36it/s]avg_loss = 1.8462775735294117:  72%|███████▏  | 101/141 [00:42<00:16,  2.36it/s]avg_loss = 1.8462775735294117:  72%|███████▏  | 102/141 [00:42<00:16,  2.37it/s]avg_loss = 1.8439775485436893:  72%|███████▏  | 102/141 [00:43<00:16,  2.37it/s]avg_loss = 1.8439775485436893:  73%|███████▎  | 103/141 [00:43<00:16,  2.36it/s]avg_loss = 1.8462289663461537:  73%|███████▎  | 103/141 [00:43<00:16,  2.36it/s]avg_loss = 1.8462289663461537:  74%|███████▍  | 104/141 [00:43<00:15,  2.36it/s]avg_loss = 1.8441220238095237:  74%|███████▍  | 104/141 [00:44<00:15,  2.36it/s]avg_loss = 1.8441220238095237:  74%|███████▍  | 105/141 [00:44<00:15,  2.36it/s]avg_loss = 1.8427181603773586:  74%|███████▍  | 105/141 [00:44<00:15,  2.36it/s]avg_loss = 1.8427181603773586:  75%|███████▌  | 106/141 [00:44<00:14,  2.36it/s]avg_loss = 1.8402453271028036:  75%|███████▌  | 106/141 [00:45<00:14,  2.36it/s]avg_loss = 1.8402453271028036:  76%|███████▌  | 107/141 [00:45<00:14,  2.36it/s]avg_loss = 1.8377459490740742:  76%|███████▌  | 107/141 [00:45<00:14,  2.36it/s]avg_loss = 1.8377459490740742:  77%|███████▋  | 108/141 [00:45<00:13,  2.36it/s]avg_loss = 1.8351490825688073:  77%|███████▋  | 108/141 [00:45<00:13,  2.36it/s]avg_loss = 1.8351490825688073:  77%|███████▋  | 109/141 [00:45<00:13,  2.37it/s]avg_loss = 1.8327414772727273:  77%|███████▋  | 109/141 [00:46<00:13,  2.37it/s]avg_loss = 1.8327414772727273:  78%|███████▊  | 110/141 [00:46<00:13,  2.37it/s]avg_loss = 1.8350929054054055:  78%|███████▊  | 110/141 [00:46<00:13,  2.37it/s]avg_loss = 1.8350929054054055:  79%|███████▊  | 111/141 [00:46<00:12,  2.37it/s]avg_loss = 1.8348214285714286:  79%|███████▊  | 111/141 [00:47<00:12,  2.37it/s]avg_loss = 1.8348214285714286:  79%|███████▉  | 112/141 [00:47<00:12,  2.37it/s]avg_loss = 1.8360066371681416:  79%|███████▉  | 112/141 [00:47<00:12,  2.37it/s]avg_loss = 1.8360066371681416:  80%|████████  | 113/141 [00:47<00:11,  2.38it/s]avg_loss = 1.8369654605263157:  80%|████████  | 113/141 [00:47<00:11,  2.38it/s]avg_loss = 1.8369654605263157:  81%|████████  | 114/141 [00:47<00:11,  2.38it/s]avg_loss = 1.8363451086956522:  81%|████████  | 114/141 [00:48<00:11,  2.38it/s]avg_loss = 1.8363451086956522:  82%|████████▏ | 115/141 [00:48<00:10,  2.38it/s]avg_loss = 1.8348599137931034:  82%|████████▏ | 115/141 [00:48<00:10,  2.38it/s]avg_loss = 1.8348599137931034:  82%|████████▏ | 116/141 [00:48<00:10,  2.38it/s]avg_loss = 1.8370726495726495:  82%|████████▏ | 116/141 [00:49<00:10,  2.38it/s]avg_loss = 1.8370726495726495:  83%|████████▎ | 117/141 [00:49<00:10,  2.37it/s]avg_loss = 1.8367319915254237:  83%|████████▎ | 117/141 [00:49<00:10,  2.37it/s]avg_loss = 1.8367319915254237:  84%|████████▎ | 118/141 [00:49<00:09,  2.37it/s]avg_loss = 1.8354122899159664:  84%|████████▎ | 118/141 [00:50<00:09,  2.37it/s]avg_loss = 1.8354122899159664:  84%|████████▍ | 119/141 [00:50<00:09,  2.37it/s]avg_loss = 1.8337239583333333:  84%|████████▍ | 119/141 [00:50<00:09,  2.37it/s]avg_loss = 1.8337239583333333:  85%|████████▌ | 120/141 [00:50<00:08,  2.37it/s]avg_loss = 1.8336131198347108:  85%|████████▌ | 120/141 [00:50<00:08,  2.37it/s]avg_loss = 1.8336131198347108:  86%|████████▌ | 121/141 [00:50<00:08,  2.37it/s]avg_loss = 1.834016393442623:  86%|████████▌ | 121/141 [00:51<00:08,  2.37it/s] avg_loss = 1.834016393442623:  87%|████████▋ | 122/141 [00:51<00:08,  2.37it/s]avg_loss = 1.8339049796747968:  87%|████████▋ | 122/141 [00:51<00:08,  2.37it/s]avg_loss = 1.8339049796747968:  87%|████████▋ | 123/141 [00:51<00:07,  2.37it/s]avg_loss = 1.8342363911290323:  87%|████████▋ | 123/141 [00:52<00:07,  2.37it/s]avg_loss = 1.8342363911290323:  88%|████████▊ | 124/141 [00:52<00:07,  2.37it/s]avg_loss = 1.833125:  88%|████████▊ | 124/141 [00:52<00:07,  2.37it/s]          avg_loss = 1.833125:  89%|████████▊ | 125/141 [00:52<00:06,  2.37it/s]avg_loss = 1.8335813492063493:  89%|████████▊ | 125/141 [00:53<00:06,  2.37it/s]avg_loss = 1.8335813492063493:  89%|████████▉ | 126/141 [00:53<00:06,  2.37it/s]avg_loss = 1.833353838582677:  89%|████████▉ | 126/141 [00:53<00:06,  2.37it/s] avg_loss = 1.833353838582677:  90%|█████████ | 127/141 [00:53<00:05,  2.37it/s]avg_loss = 1.83209228515625:  90%|█████████ | 127/141 [00:53<00:05,  2.37it/s] avg_loss = 1.83209228515625:  91%|█████████ | 128/141 [00:53<00:05,  2.37it/s]avg_loss = 1.8323037790697674:  91%|█████████ | 128/141 [00:54<00:05,  2.37it/s]avg_loss = 1.8323037790697674:  91%|█████████▏| 129/141 [00:54<00:05,  2.37it/s]avg_loss = 1.8329927884615385:  91%|█████████▏| 129/141 [00:54<00:05,  2.37it/s]avg_loss = 1.8329927884615385:  92%|█████████▏| 130/141 [00:54<00:04,  2.37it/s]avg_loss = 1.8338501908396947:  92%|█████████▏| 130/141 [00:55<00:04,  2.37it/s]avg_loss = 1.8338501908396947:  93%|█████████▎| 131/141 [00:55<00:04,  2.37it/s]avg_loss = 1.8344578598484849:  93%|█████████▎| 131/141 [00:55<00:04,  2.37it/s]avg_loss = 1.8344578598484849:  94%|█████████▎| 132/141 [00:55<00:03,  2.37it/s]avg_loss = 1.8317081766917294:  94%|█████████▎| 132/141 [00:55<00:03,  2.37it/s]avg_loss = 1.8317081766917294:  94%|█████████▍| 133/141 [00:55<00:03,  2.37it/s]avg_loss = 1.8273670708955223:  94%|█████████▍| 133/141 [00:56<00:03,  2.37it/s]avg_loss = 1.8273670708955223:  95%|█████████▌| 134/141 [00:56<00:02,  2.37it/s]avg_loss = 1.8299189814814816:  95%|█████████▌| 134/141 [00:56<00:02,  2.37it/s]avg_loss = 1.8299189814814816:  96%|█████████▌| 135/141 [00:56<00:02,  2.37it/s]avg_loss = 1.8334673713235294:  96%|█████████▌| 135/141 [00:57<00:02,  2.37it/s]avg_loss = 1.8334673713235294:  96%|█████████▋| 136/141 [00:57<00:02,  2.37it/s]avg_loss = 1.834340784671533:  96%|█████████▋| 136/141 [00:57<00:02,  2.37it/s] avg_loss = 1.834340784671533:  97%|█████████▋| 137/141 [00:57<00:01,  2.37it/s]avg_loss = 1.8329936594202898:  97%|█████████▋| 137/141 [00:58<00:01,  2.37it/s]avg_loss = 1.8329936594202898:  98%|█████████▊| 138/141 [00:58<00:01,  2.37it/s]avg_loss = 1.83318345323741:  98%|█████████▊| 138/141 [00:58<00:01,  2.37it/s]  avg_loss = 1.83318345323741:  99%|█████████▊| 139/141 [00:58<00:00,  2.37it/s]avg_loss = 1.8337053571428572:  99%|█████████▊| 139/141 [00:58<00:00,  2.37it/s]avg_loss = 1.8337053571428572:  99%|█████████▉| 140/141 [00:58<00:00,  2.37it/s]avg_loss = 1.8349955673758864:  99%|█████████▉| 140/141 [00:59<00:00,  2.37it/s]avg_loss = 1.8349955673758864: 100%|██████████| 141/141 [00:59<00:00,  2.37it/s]avg_loss = 1.8349955673758864: 100%|██████████| 141/141 [00:59<00:00,  2.38it/s]
I0302 21:08:31.642207 2587812 eval_ppl.py:105] wikitext2 perplexity: 6.265106201171875
wikitext2 perplexity: 6.265
Running with lmbda=10000
/home/jgryu/Weight_compression/comp_llm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")
/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.58it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  9.67it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  9.37it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  9.46it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.62it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.69it/s]
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:12,  2.44it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:14,  2.01it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:13,  2.20it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:11,  2.34it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:11,  2.41it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:10,  2.46it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:02<00:10,  2.50it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:09,  2.53it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:03<00:09,  2.55it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:04<00:08,  2.60it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:04<00:08,  2.62it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:04<00:08,  2.35it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:05<00:09,  2.06it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:06<00:09,  1.91it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:06<00:09,  1.85it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:07<00:09,  1.77it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:07<00:08,  1.78it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:08<00:07,  1.77it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:09<00:07,  1.77it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:09<00:06,  1.76it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:10<00:06,  1.69it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:10<00:05,  1.71it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:11<00:05,  1.71it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.66it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:12<00:04,  1.67it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.75it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:13<00:02,  1.82it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.84it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:14<00:01,  1.88it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.86it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:15<00:00,  1.83it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.81it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s]
pseudo compress quantization...:   0%|          | 0/32 [00:00<?, ?it/s]2025-03-02 21:09:13 - INFO - layer0_self_attn.q_proj | mse: 0.0006055117727824062, bpp_loss: 6.3190529346466064, bpp: 0
2025-03-02 21:09:13 - INFO - layer0_self_attn.k_proj | mse: 0.0009167654998232513, bpp_loss: 7.1024826765060425, bpp: 0
2025-03-02 21:09:13 - INFO - layer0_self_attn.v_proj | mse: 0.00040924270645071643, bpp_loss: 5.404982924461365, bpp: 0
2025-03-02 21:09:14 - INFO - layer0_self_attn.o_proj | mse: 0.0004212883378570641, bpp_loss: 5.4861332178115845, bpp: 0
2025-03-02 21:09:16 - INFO - layer0_mlp.gate_proj | mse: 0.0004906036056369894, bpp_loss: 5.8798554284232, bpp: 0
2025-03-02 21:09:19 - INFO - layer0_mlp.up_proj | mse: 0.00047122916785715784, bpp_loss: 5.767600059509277, bpp: 0
2025-03-02 21:09:21 - INFO - layer0_mlp.down_proj | mse: 0.00047052627290372666, bpp_loss: 5.763381004333496, bpp: 0
pseudo compress quantization...:   3%|▎         | 1/32 [00:09<05:01,  9.72s/it]2025-03-02 21:09:22 - INFO - layer1_self_attn.q_proj | mse: 0.0006288283483054943, bpp_loss: 6.626645565032959, bpp: 0
2025-03-02 21:09:22 - INFO - layer1_self_attn.k_proj | mse: 0.0008635570580026346, bpp_loss: 7.6460301876068115, bpp: 0
2025-03-02 21:09:23 - INFO - layer1_self_attn.v_proj | mse: 0.0004273125568152326, bpp_loss: 5.50684928894043, bpp: 0
2025-03-02 21:09:24 - INFO - layer1_self_attn.o_proj | mse: 0.0004358263311990071, bpp_loss: 5.569278359413147, bpp: 0
2025-03-02 21:09:26 - INFO - layer1_mlp.gate_proj | mse: 0.0004933248080691257, bpp_loss: 5.892867633274624, bpp: 0
2025-03-02 21:09:28 - INFO - layer1_mlp.up_proj | mse: 0.00047414695532079767, bpp_loss: 5.7822786058698386, bpp: 0
2025-03-02 21:09:31 - INFO - layer1_mlp.down_proj | mse: 0.0004732669786403042, bpp_loss: 5.778383391244071, bpp: 0
pseudo compress quantization...:   6%|▋         | 2/32 [00:19<04:49,  9.66s/it]2025-03-02 21:09:32 - INFO - layer2_self_attn.q_proj | mse: 0.000613348680329585, bpp_loss: 6.515021443367004, bpp: 0
2025-03-02 21:09:32 - INFO - layer2_self_attn.k_proj | mse: 0.0009042477410106512, bpp_loss: 7.6709208488464355, bpp: 0
2025-03-02 21:09:32 - INFO - layer2_self_attn.v_proj | mse: 0.0004100991899266227, bpp_loss: 5.414950847625732, bpp: 0
2025-03-02 21:09:33 - INFO - layer2_self_attn.o_proj | mse: 0.00042597385155037075, bpp_loss: 5.515326142311096, bpp: 0
2025-03-02 21:09:36 - INFO - layer2_mlp.gate_proj | mse: 0.0004999178044296273, bpp_loss: 5.928293636866978, bpp: 0
2025-03-02 21:09:38 - INFO - layer2_mlp.up_proj | mse: 0.0004730879310629269, bpp_loss: 5.776717322213309, bpp: 0
2025-03-02 21:09:41 - INFO - layer2_mlp.down_proj | mse: 0.0004735750998107345, bpp_loss: 5.780177218573434, bpp: 0
pseudo compress quantization...:   9%|▉         | 3/32 [00:28<04:39,  9.63s/it]2025-03-02 21:09:41 - INFO - layer3_self_attn.q_proj | mse: 0.0006175596675032613, bpp_loss: 6.5467822551727295, bpp: 0
2025-03-02 21:09:42 - INFO - layer3_self_attn.k_proj | mse: 0.00090305823343812, bpp_loss: 7.744325757026672, bpp: 0
2025-03-02 21:09:42 - INFO - layer3_self_attn.v_proj | mse: 0.00042644552629372167, bpp_loss: 5.5095438957214355, bpp: 0
2025-03-02 21:09:43 - INFO - layer3_self_attn.o_proj | mse: 0.00044333039571472143, bpp_loss: 5.61147928237915, bpp: 0
2025-03-02 21:09:45 - INFO - layer3_mlp.gate_proj | mse: 0.0005141082431958073, bpp_loss: 6.009733745029995, bpp: 0
2025-03-02 21:09:48 - INFO - layer3_mlp.up_proj | mse: 0.0004697189292190835, bpp_loss: 5.757477896554129, bpp: 0
2025-03-02 21:09:51 - INFO - layer3_mlp.down_proj | mse: 0.0004691971511591506, bpp_loss: 5.755275998796735, bpp: 0
pseudo compress quantization...:  12%|█▎        | 4/32 [00:39<04:35,  9.83s/it]2025-03-02 21:09:52 - INFO - layer4_self_attn.q_proj | mse: 0.0006115454976615059, bpp_loss: 6.5106014013290405, bpp: 0
2025-03-02 21:09:52 - INFO - layer4_self_attn.k_proj | mse: 0.0008887509171107756, bpp_loss: 7.69355046749115, bpp: 0
2025-03-02 21:09:52 - INFO - layer4_self_attn.v_proj | mse: 0.0004354686822405411, bpp_loss: 5.5568965673446655, bpp: 0
2025-03-02 21:09:53 - INFO - layer4_self_attn.o_proj | mse: 0.0004465722768826012, bpp_loss: 5.626068830490112, bpp: 0
2025-03-02 21:09:56 - INFO - layer4_mlp.gate_proj | mse: 0.0005290811421202746, bpp_loss: 6.089667592729841, bpp: 0
2025-03-02 21:09:58 - INFO - layer4_mlp.up_proj | mse: 0.0004652211154311854, bpp_loss: 5.731193133762905, bpp: 0
2025-03-02 21:10:01 - INFO - layer4_mlp.down_proj | mse: 0.0004645561924285762, bpp_loss: 5.729699066707066, bpp: 0
pseudo compress quantization...:  16%|█▌        | 5/32 [00:49<04:30, 10.01s/it]2025-03-02 21:10:02 - INFO - layer5_self_attn.q_proj | mse: 0.0006105313516970112, bpp_loss: 6.48577094078064, bpp: 0
2025-03-02 21:10:02 - INFO - layer5_self_attn.k_proj | mse: 0.0009063021063227662, bpp_loss: 7.661797881126404, bpp: 0
2025-03-02 21:10:02 - INFO - layer5_self_attn.v_proj | mse: 0.00041572646878647594, bpp_loss: 5.4446940422058105, bpp: 0
2025-03-02 21:10:03 - INFO - layer5_self_attn.o_proj | mse: 0.0004353598168127331, bpp_loss: 5.568520188331604, bpp: 0
2025-03-02 21:10:06 - INFO - layer5_mlp.gate_proj | mse: 0.0005286998188920608, bpp_loss: 6.089936937604632, bpp: 0
2025-03-02 21:10:09 - INFO - layer5_mlp.up_proj | mse: 0.0004662460477059613, bpp_loss: 5.736887386866978, bpp: 0
2025-03-02 21:10:11 - INFO - layer5_mlp.down_proj | mse: 0.0004652622430890921, bpp_loss: 5.734656640461513, bpp: 0
pseudo compress quantization...:  19%|█▉        | 6/32 [00:59<04:22, 10.10s/it]2025-03-02 21:10:12 - INFO - layer6_self_attn.q_proj | mse: 0.0006188115520236177, bpp_loss: 6.5377033948898315, bpp: 0
2025-03-02 21:10:12 - INFO - layer6_self_attn.k_proj | mse: 0.0008902899709758361, bpp_loss: 7.720619320869446, bpp: 0
2025-03-02 21:10:13 - INFO - layer6_self_attn.v_proj | mse: 0.0004246581415609045, bpp_loss: 5.491144895553589, bpp: 0
2025-03-02 21:10:14 - INFO - layer6_self_attn.o_proj | mse: 0.00044165029208955705, bpp_loss: 5.601120352745056, bpp: 0
2025-03-02 21:10:16 - INFO - layer6_mlp.gate_proj | mse: 0.0005305124363483294, bpp_loss: 6.0978303636823386, bpp: 0
2025-03-02 21:10:19 - INFO - layer6_mlp.up_proj | mse: 0.0004661166119825871, bpp_loss: 5.736459868294852, bpp: 0
2025-03-02 21:10:22 - INFO - layer6_mlp.down_proj | mse: 0.00046498836910046585, bpp_loss: 5.734255961009434, bpp: 0
pseudo compress quantization...:  22%|██▏       | 7/32 [01:09<04:13, 10.13s/it]2025-03-02 21:10:22 - INFO - layer7_self_attn.q_proj | mse: 0.0006003315438040628, bpp_loss: 6.44616174697876, bpp: 0
2025-03-02 21:10:23 - INFO - layer7_self_attn.k_proj | mse: 0.0008932060253872246, bpp_loss: 7.755807161331177, bpp: 0
2025-03-02 21:10:23 - INFO - layer7_self_attn.v_proj | mse: 0.00042354513401872477, bpp_loss: 5.487396597862244, bpp: 0
2025-03-02 21:10:24 - INFO - layer7_self_attn.o_proj | mse: 0.00044388631102403937, bpp_loss: 5.613853573799133, bpp: 0
2025-03-02 21:10:26 - INFO - layer7_mlp.gate_proj | mse: 0.0005256079502097343, bpp_loss: 6.068413325718471, bpp: 0
2025-03-02 21:10:29 - INFO - layer7_mlp.up_proj | mse: 0.00046859658388790726, bpp_loss: 5.750455583844866, bpp: 0
2025-03-02 21:10:32 - INFO - layer7_mlp.down_proj | mse: 0.0004671867070817057, bpp_loss: 5.748951775687082, bpp: 0
pseudo compress quantization...:  25%|██▌       | 8/32 [01:19<04:02, 10.11s/it]2025-03-02 21:10:32 - INFO - layer8_self_attn.q_proj | mse: 0.0006025912370082931, bpp_loss: 6.435274004936218, bpp: 0
2025-03-02 21:10:33 - INFO - layer8_self_attn.k_proj | mse: 0.0008957643855738568, bpp_loss: 7.653181076049805, bpp: 0
2025-03-02 21:10:33 - INFO - layer8_self_attn.v_proj | mse: 0.0004283300977514884, bpp_loss: 5.512456297874451, bpp: 0
2025-03-02 21:10:34 - INFO - layer8_self_attn.o_proj | mse: 0.000446268992738604, bpp_loss: 5.6271071434021, bpp: 0
2025-03-02 21:10:36 - INFO - layer8_mlp.gate_proj | mse: 0.0005263541456029993, bpp_loss: 6.07555171421596, bpp: 0
2025-03-02 21:10:39 - INFO - layer8_mlp.up_proj | mse: 0.0004679871702902463, bpp_loss: 5.745833396911621, bpp: 0
2025-03-02 21:10:42 - INFO - layer8_mlp.down_proj | mse: 0.0004665561347267066, bpp_loss: 5.74616551399231, bpp: 0
pseudo compress quantization...:  28%|██▊       | 9/32 [01:30<03:52, 10.12s/it]2025-03-02 21:10:43 - INFO - layer9_self_attn.q_proj | mse: 0.0005993554215232933, bpp_loss: 6.445547699928284, bpp: 0
2025-03-02 21:10:43 - INFO - layer9_self_attn.k_proj | mse: 0.0008825286128669157, bpp_loss: 7.675116419792175, bpp: 0
2025-03-02 21:10:43 - INFO - layer9_self_attn.v_proj | mse: 0.0004460341964916019, bpp_loss: 5.6159584522247314, bpp: 0
2025-03-02 21:10:44 - INFO - layer9_self_attn.o_proj | mse: 0.00045638659704864206, bpp_loss: 5.684253573417664, bpp: 0
scripts/recon0.sh: line 23: 2589560 Killed                  taskset -c 0-31 python compress_lm_nwc.py --lm_model_path "$lm_model_path" --comp_model_path $comp_model --direction col --save_path "$save_path" --batch_size "$batch_size" --ql "$ql"
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda10000_rdloss_ql_encdim512_M16_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_10.96029_bpp_6.2788_MSE_0.0004_total_iter_140000.pth.tar/COL_MSE0.00051_bpploss5.9521_bpp0
I0302 21:10:49.558783 2591755 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.26s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.81s/it]
I0302 21:10:56.926836 2591755 config.py:54] PyTorch version 2.4.1 available.
  0%|          | 0/141 [00:00<?, ?it/s]Running with lmbda=100000
/home/jgryu/Weight_compression/comp_llm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")
/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.71it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.00it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  4.17it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.15it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  4.04it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.13it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.24it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.14it/s]
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:28,  1.09it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:22,  1.33it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:19,  1.51it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:20,  1.33it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:19,  1.36it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:17,  1.49it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:15,  1.64it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:14,  1.71it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:05<00:13,  1.67it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:06<00:12,  1.78it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:11,  1.85it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:07<00:10,  1.88it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:09,  1.91it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:08<00:09,  1.93it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  1.92it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:09<00:08,  1.94it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.95it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:10<00:07,  2.00it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.98it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:11<00:06,  1.98it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.97it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:12<00:05,  1.99it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.96it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:13<00:03,  2.00it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  2.04it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:14<00:02,  2.04it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  2.08it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:15<00:01,  2.10it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  2.10it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:16<00:00,  2.05it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  2.06it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:17<00:00,  1.94it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:17<00:00,  1.84it/s]
pseudo compress quantization...:   0%|          | 0/32 [00:00<?, ?it/s]2025-03-02 21:12:07 - INFO - layer0_self_attn.q_proj | mse: 0.000570501330193717, bpp_loss: 6.537891745567322, bpp: 0
2025-03-02 21:12:07 - INFO - layer0_self_attn.k_proj | mse: 0.0009835161671351168, bpp_loss: 7.331157207489014, bpp: 0
2025-03-02 21:12:08 - INFO - layer0_self_attn.v_proj | mse: 0.00033558591968285776, bpp_loss: 5.650701642036438, bpp: 0
2025-03-02 21:12:08 - INFO - layer0_self_attn.o_proj | mse: 0.0003501140223404032, bpp_loss: 5.724863052368164, bpp: 0
2025-03-02 21:12:11 - INFO - layer0_mlp.gate_proj | mse: 0.0004280525248602957, bpp_loss: 6.11570794241769, bpp: 0
scripts/recon0.sh: line 23: 2592520 Killed                  taskset -c 0-31 python compress_lm_nwc.py --lm_model_path "$lm_model_path" --comp_model_path $comp_model --direction col --save_path "$save_path" --batch_size "$batch_size" --ql "$ql"
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda100000_rdloss_ql_encdim512_M16_batch_size1024_total_iter200000_lr0.0001_seed100/best_loss_model_loss_49.72731_bpp_6.30559_MSE_0.00042_total_iter_140000.pth.tar/COL_MSE0.00045_bpploss6.1921_bpp0
I0302 21:12:15.877075 2593442 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.22s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.83s/it]
I0302 21:12:23.311500 2593442 config.py:54] PyTorch version 2.4.1 available.
scripts/recon0.sh: line 53: e: command not found
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            2025-03-02 20:39:21 - INFO - layer20_mlp.up_proj | mse: 0.0020422709871137142, bpp_loss: 4.5703646796090265, bpp: 0
2025-03-02 20:39:24 - INFO - layer20_mlp.down_proj | mse: 0.002041639017970333, bpp_loss: 4.566398007529123, bpp: 0
pseudo compress quantization...:  66%|██████▌   | 21/32 [03:26<01:49, 10.00s/it]2025-03-02 20:39:25 - INFO - layer21_self_attn.q_proj | mse: 0.0021210474462636164, bpp_loss: 5.2547523975372314, bpp: 0
2025-03-02 20:39:26 - INFO - layer21_self_attn.k_proj | mse: 0.0022583500628688864, bpp_loss: 6.305259823799133, bpp: 0
2025-03-02 20:39:26 - INFO - layer21_self_attn.v_proj | mse: 0.0020298958041818573, bpp_loss: 4.472691535949707, bpp: 0
2025-03-02 20:39:27 - INFO - layer21_self_attn.o_proj | mse: 0.002034767181262515, bpp_loss: 4.503105878829956, bpp: 0
2025-03-02 20:39:31 - INFO - layer21_mlp.gate_proj | mse: 0.002090309333138368, bpp_loss: 4.978454862322126, bpp: 0
2025-03-02 20:39:35 - INFO - layer21_mlp.up_proj | mse: 0.0020426608462709274, bpp_loss: 4.573337691170829, bpp: 0
2025-03-02 20:39:38 - INFO - layer21_mlp.down_proj | mse: 0.002041892716126779, bpp_loss: 4.567811386925833, bpp: 0
pseudo compress quantization...:  69%|██████▉   | 22/32 [03:39<01:51, 11.16s/it]2025-03-02 20:39:39 - INFO - layer22_self_attn.q_proj | mse: 0.002115956618208317, bpp_loss: 5.210858583450317, bpp: 0
2025-03-02 20:39:39 - INFO - layer22_self_attn.k_proj | mse: 0.0022547157229654163, bpp_loss: 6.232005953788757, bpp: 0
2025-03-02 20:39:39 - INFO - layer22_self_attn.v_proj | mse: 0.002037216630755049, bpp_loss: 4.522947907447815, bpp: 0
2025-03-02 20:39:40 - INFO - layer22_self_attn.o_proj | mse: 0.0020374150781710615, bpp_loss: 4.529092192649841, bpp: 0
2025-03-02 20:39:43 - INFO - layer22_mlp.gate_proj | mse: 0.002090278500328164, bpp_loss: 4.980467115129743, bpp: 0
2025-03-02 20:39:45 - INFO - layer22_mlp.up_proj | mse: 0.0020431723812160607, bpp_loss: 4.5772616522652765, bpp: 0
2025-03-02 20:39:48 - INFO - layer22_mlp.down_proj | mse: 0.0020417453808189573, bpp_loss: 4.57292195728847, bpp: 0
pseudo compress quantization...:  72%|███████▏  | 23/32 [03:50<01:37, 10.86s/it]2025-03-02 20:39:49 - INFO - layer23_self_attn.q_proj | mse: 0.0021164616089412174, bpp_loss: 5.222835659980774, bpp: 0
2025-03-02 20:39:49 - INFO - layer23_self_attn.k_proj | mse: 0.0022532516300768418, bpp_loss: 6.232781171798706, bpp: 0
2025-03-02 20:39:50 - INFO - layer23_self_attn.v_proj | mse: 0.0020430054650044435, bpp_loss: 4.575336933135986, bpp: 0
2025-03-02 20:39:50 - INFO - layer23_self_attn.o_proj | mse: 0.002039356096692644, bpp_loss: 4.551046967506409, bpp: 0
2025-03-02 20:39:53 - INFO - layer23_mlp.gate_proj | mse: 0.002090428939610167, bpp_loss: 4.982790265764509, bpp: 0
2025-03-02 20:39:56 - INFO - layer23_mlp.up_proj | mse: 0.0020436404605226515, bpp_loss: 4.581262588500977, bpp: 0
2025-03-02 20:39:59 - INFO - layer23_mlp.down_proj | mse: 0.002042579729078981, bpp_loss: 4.578102724892752, bpp: 0
pseudo compress quantization...:  75%|███████▌  | 24/32 [04:00<01:26, 10.84s/it]2025-03-02 20:40:00 - INFO - layer24_self_attn.q_proj | mse: 0.002113000548396437, bpp_loss: 5.18828284740448, bpp: 0
2025-03-02 20:40:00 - INFO - layer24_self_attn.k_proj | mse: 0.0022287138605781185, bpp_loss: 6.043656706809998, bpp: 0
2025-03-02 20:40:00 - INFO - layer24_self_attn.v_proj | mse: 0.0020547464542780896, bpp_loss: 4.675195813179016, bpp: 0
2025-03-02 20:40:01 - INFO - layer24_self_attn.o_proj | mse: 0.0020445664277539363, bpp_loss: 4.593493819236755, bpp: 0
2025-03-02 20:40:04 - INFO - layer24_mlp.gate_proj | mse: 0.002091313158388709, bpp_loss: 4.989094189235142, bpp: 0
2025-03-02 20:40:07 - INFO - layer24_mlp.up_proj | mse: 0.0020439343034118463, bpp_loss: 4.585977826799665, bpp: 0
2025-03-02 20:40:10 - INFO - layer24_mlp.down_proj | mse: 0.0020431984545991864, bpp_loss: 4.583556345530918, bpp: 0
pseudo compress quantization...:  78%|███████▊  | 25/32 [04:12<01:17, 11.01s/it]2025-03-02 20:40:11 - INFO - layer25_self_attn.q_proj | mse: 0.002109862327032998, bpp_loss: 5.164105296134949, bpp: 0
2025-03-02 20:40:12 - INFO - layer25_self_attn.k_proj | mse: 0.0022273915417102903, bpp_loss: 6.024067044258118, bpp: 0
2025-03-02 20:40:12 - INFO - layer25_self_attn.v_proj | mse: 0.002054233858808444, bpp_loss: 4.68536376953125, bpp: 0
2025-03-02 20:40:14 - INFO - layer25_self_attn.o_proj | mse: 0.0020455528707488597, bpp_loss: 4.594151735305786, bpp: 0
2025-03-02 20:40:17 - INFO - layer25_mlp.gate_proj | mse: 0.0020921102589585524, bpp_loss: 4.99973269871303, bpp: 0
2025-03-02 20:40:20 - INFO - layer25_mlp.up_proj | mse: 0.002044833628198072, bpp_loss: 4.595416341509138, bpp: 0
2025-03-02 20:40:23 - INFO - layer25_mlp.down_proj | mse: 0.002044193381790207, bpp_loss: 4.592645985739572, bpp: 0
pseudo compress quantization...:  81%|████████▏ | 26/32 [04:24<01:08, 11.41s/it]2025-03-02 20:40:23 - INFO - layer26_self_attn.q_proj | mse: 0.002110422723705041, bpp_loss: 5.17462432384491, bpp: 0
2025-03-02 20:40:24 - INFO - layer26_self_attn.k_proj | mse: 0.0022422991795775886, bpp_loss: 6.116231799125671, bpp: 0
2025-03-02 20:40:24 - INFO - layer26_self_attn.v_proj | mse: 0.0020644341646305843, bpp_loss: 4.738359332084656, bpp: 0
2025-03-02 20:40:25 - INFO - layer26_self_attn.o_proj | mse: 0.002048483772503417, bpp_loss: 4.61145544052124, bpp: 0
2025-03-02 20:40:28 - INFO - layer26_mlp.gate_proj | mse: 0.002093365713049218, bpp_loss: 5.009596824645996, bpp: 0
2025-03-02 20:40:30 - INFO - layer26_mlp.up_proj | mse: 0.0020459694758169383, bpp_loss: 4.604662077767508, bpp: 0
2025-03-02 20:40:35 - INFO - layer26_mlp.down_proj | mse: 0.0020457822027079796, bpp_loss: 4.600605998720441, bpp: 0
pseudo compress quantization...:  84%|████████▍ | 27/32 [04:37<00:58, 11.79s/it]2025-03-02 20:40:37 - INFO - layer27_self_attn.q_proj | mse: 0.0021073935035682803, bpp_loss: 5.14240026473999, bpp: 0
2025-03-02 20:40:37 - INFO - layer27_self_attn.k_proj | mse: 0.002238648905939986, bpp_loss: 6.06805419921875, bpp: 0
2025-03-02 20:40:37 - INFO - layer27_self_attn.v_proj | mse: 0.0020722359002703175, bpp_loss: 4.833151340484619, bpp: 0
2025-03-02 20:40:39 - INFO - layer27_self_attn.o_proj | mse: 0.0020513657866646135, bpp_loss: 4.652824282646179, bpp: 0
2025-03-02 20:40:43 - INFO - layer27_mlp.gate_proj | mse: 0.002095157326946858, bpp_loss: 5.023788860866001, bpp: 0
2025-03-02 20:40:48 - INFO - layer27_mlp.up_proj | mse: 0.0020480799713288376, bpp_loss: 4.6202609198434015, bpp: 0
2025-03-02 20:40:53 - INFO - layer27_mlp.down_proj | mse: 0.0020471100828628828, bpp_loss: 4.611800125667027, bpp: 0
pseudo compress quantization...:  88%|████████▊ | 28/32 [04:55<00:54, 13.64s/it]2025-03-02 20:40:55 - INFO - layer28_self_attn.q_proj | mse: 0.0021083437335310848, bpp_loss: 5.151453495025635, bpp: 0
2025-03-02 20:40:56 - INFO - layer28_self_attn.k_proj | mse: 0.00221968178717214, bpp_loss: 6.005625128746033, bpp: 0
2025-03-02 20:40:56 - INFO - layer28_self_attn.v_proj | mse: 0.0020807961178677843, bpp_loss: 4.8873302936553955, bpp: 0
2025-03-02 20:40:58 - INFO - layer28_self_attn.o_proj | mse: 0.002054087558158683, bpp_loss: 4.68305230140686, bpp: 0
2025-03-02 20:41:02 - INFO - layer28_mlp.gate_proj | mse: 0.0020933102553771727, bpp_loss: 5.011770112173898, bpp: 0
2025-03-02 20:41:06 - INFO - layer28_mlp.up_proj | mse: 0.0020507700822862546, bpp_loss: 4.6431267602103095, bpp: 0
2025-03-02 20:41:09 - INFO - layer28_mlp.down_proj | mse: 0.0020491922287439236, bpp_loss: 4.627553735460554, bpp: 0
pseudo compress quantization...:  91%|█████████ | 29/32 [05:11<00:42, 14.33s/it]2025-03-02 20:41:10 - INFO - layer29_self_attn.q_proj | mse: 0.0021070641460411575, bpp_loss: 5.137818932533264, bpp: 0
2025-03-02 20:41:11 - INFO - layer29_self_attn.k_proj | mse: 0.0022363240292779417, bpp_loss: 6.107442259788513, bpp: 0
2025-03-02 20:41:11 - INFO - layer29_self_attn.v_proj | mse: 0.002090946087575279, bpp_loss: 4.957422852516174, bpp: 0
2025-03-02 20:41:12 - INFO - layer29_self_attn.o_proj | mse: 0.0020596489745663697, bpp_loss: 4.718822956085205, bpp: 0
2025-03-02 20:41:15 - INFO - layer29_mlp.gate_proj | mse: 0.0020922238644136004, bpp_loss: 5.006000791277204, bpp: 0
2025-03-02 20:41:18 - INFO - layer29_mlp.up_proj | mse: 0.002054451809528898, bpp_loss: 4.676928520202637, bpp: 0
2025-03-02 20:41:21 - INFO - layer29_mlp.down_proj | mse: 0.002051977942106135, bpp_loss: 4.642795664923532, bpp: 0
pseudo compress quantization...:  94%|█████████▍| 30/32 [05:23<00:27, 13.62s/it]2025-03-02 20:41:22 - INFO - layer30_self_attn.q_proj | mse: 0.002094298509023702, bpp_loss: 5.0428314208984375, bpp: 0
2025-03-02 20:41:23 - INFO - layer30_self_attn.k_proj | mse: 0.0021992857021568744, bpp_loss: 5.731124520301819, bpp: 0
2025-03-02 20:41:23 - INFO - layer30_self_attn.v_proj | mse: 0.0021246229399898933, bpp_loss: 5.261219620704651, bpp: 0
2025-03-02 20:41:24 - INFO - layer30_self_attn.o_proj | mse: 0.0020707142286606737, bpp_loss: 4.807782053947449, bpp: 0
2025-03-02 20:41:27 - INFO - layer30_mlp.gate_proj | mse: 0.00209948261150316, bpp_loss: 5.063664027622768, bpp: 0
2025-03-02 20:41:30 - INFO - layer30_mlp.up_proj | mse: 0.002057918948689784, bpp_loss: 4.710241590227399, bpp: 0
2025-03-02 20:41:33 - INFO - layer30_mlp.down_proj | mse: 0.0020530099100557224, bpp_loss: 4.644140652247837, bpp: 0
pseudo compress quantization...:  97%|█████████▋| 31/32 [05:34<00:12, 13.00s/it]2025-03-02 20:41:33 - INFO - layer31_self_attn.q_proj | mse: 0.002116515204228754, bpp_loss: 5.204448461532593, bpp: 0
2025-03-02 20:41:34 - INFO - layer31_self_attn.k_proj | mse: 0.002225638106695085, bpp_loss: 5.991970062255859, bpp: 0
2025-03-02 20:41:34 - INFO - layer31_self_attn.v_proj | mse: 0.0021020397688997967, bpp_loss: 5.065556526184082, bpp: 0
2025-03-02 20:41:35 - INFO - layer31_self_attn.o_proj | mse: 0.0020675439587638095, bpp_loss: 4.772690534591675, bpp: 0
2025-03-02 20:41:38 - INFO - layer31_mlp.gate_proj | mse: 0.00212787921583804, bpp_loss: 5.298458099365234, bpp: 0
2025-03-02 20:41:40 - INFO - layer31_mlp.up_proj | mse: 0.0020818176875038634, bpp_loss: 4.913954734802246, bpp: 0
2025-03-02 20:41:43 - INFO - layer31_mlp.down_proj | mse: 0.0020561788607635595, bpp_loss: 4.672347375324795, bpp: 0
pseudo compress quantization...: 100%|██████████| 32/32 [05:45<00:00, 12.23s/it]pseudo compress quantization...: 100%|██████████| 32/32 [05:45<00:00, 10.79s/it]
2025-03-02 20:41:43 - INFO - #### Total | mse: 0.002063526899014736, bpp_loss: 4.750660865782545, bpp: 0 ####
## Strart saving /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda1000_rdloss_ql_encdim512_M16_batch_size2048_total_iter1500000_lr0.0001_seed100/best_loss_model_loss_6.59649_bpp_6.05166_MSE_0.00106_total_iter_140000.pth.tar/COL_MSE0.00206_bpploss4.7507_bpp0
## End saving
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda1000_rdloss_ql_encdim512_M16_batch_size2048_total_iter1500000_lr0.0001_seed100/best_loss_model_loss_6.59649_bpp_6.05166_MSE_0.00106_total_iter_140000.pth.tar/COL_MSE0.00206_bpploss4.7507_bpp0
I0302 20:42:28.273322 2566348 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.42it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
I0302 20:42:31.793427 2566348 config.py:54] PyTorch version 2.4.1 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.546875:   0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.546875:   1%|          | 1/141 [00:00<01:14,  1.89it/s]avg_loss = 1.8671875:   1%|          | 1/141 [00:00<01:14,  1.89it/s]avg_loss = 1.8671875:   1%|▏         | 2/141 [00:00<01:04,  2.16it/s]avg_loss = 2.0052083333333335:   1%|▏         | 2/141 [00:01<01:04,  2.16it/s]avg_loss = 2.0052083333333335:   2%|▏         | 3/141 [00:01<01:01,  2.25it/s]avg_loss = 1.958984375:   2%|▏         | 3/141 [00:01<01:01,  2.25it/s]       avg_loss = 1.958984375:   3%|▎         | 4/141 [00:01<00:59,  2.30it/s]avg_loss = 1.909375:   3%|▎         | 4/141 [00:02<00:59,  2.30it/s]   avg_loss = 1.909375:   4%|▎         | 5/141 [00:02<00:58,  2.33it/s]avg_loss = 1.8125:   4%|▎         | 5/141 [00:02<00:58,  2.33it/s]  avg_loss = 1.8125:   4%|▍         | 6/141 [00:02<00:57,  2.35it/s]avg_loss = 1.7488839285714286:   4%|▍         | 6/141 [00:03<00:57,  2.35it/s]avg_loss = 1.7488839285714286:   5%|▍         | 7/141 [00:03<00:56,  2.36it/s]avg_loss = 1.7451171875:   5%|▍         | 7/141 [00:03<00:56,  2.36it/s]      avg_loss = 1.7451171875:   6%|▌         | 8/141 [00:03<00:56,  2.37it/s]avg_loss = 1.7803819444444444:   6%|▌         | 8/141 [00:03<00:56,  2.37it/s]avg_loss = 1.7803819444444444:   6%|▋         | 9/141 [00:03<00:55,  2.37it/s]avg_loss = 1.7859375:   6%|▋         | 9/141 [00:04<00:55,  2.37it/s]         avg_loss = 1.7859375:   7%|▋         | 10/141 [00:04<00:55,  2.37it/s]avg_loss = 1.7826704545454546:   7%|▋         | 10/141 [00:04<00:55,  2.37it/s]avg_loss = 1.7826704545454546:   8%|▊         | 11/141 [00:04<00:54,  2.37it/s]avg_loss = 1.8059895833333333:   8%|▊         | 11/141 [00:05<00:54,  2.37it/s]avg_loss = 1.8059895833333333:   9%|▊         | 12/141 [00:05<00:54,  2.38it/s]avg_loss = 1.8185096153846154:   9%|▊         | 12/141 [00:05<00:54,  2.38it/s]avg_loss = 1.8185096153846154:   9%|▉         | 13/141 [00:05<00:53,  2.38it/s]avg_loss = 1.8370535714285714:   9%|▉         | 13/141 [00:05<00:53,  2.38it/s]avg_loss = 1.8370535714285714:  10%|▉         | 14/141 [00:05<00:53,  2.37it/s]avg_loss = 1.8463541666666667:  10%|▉         | 14/141 [00:06<00:53,  2.37it/s]avg_loss = 1.8463541666666667:  11%|█         | 15/141 [00:06<00:53,  2.38it/s]avg_loss = 1.87060546875:  11%|█         | 15/141 [00:06<00:53,  2.38it/s]     avg_loss = 1.87060546875:  11%|█▏        | 16/141 [00:06<00:52,  2.39it/s]avg_loss = 1.8740808823529411:  11%|█▏        | 16/141 [00:07<00:52,  2.39it/s]avg_loss = 1.8740808823529411:  12%|█▏        | 17/141 [00:07<00:51,  2.39it/s]avg_loss = 1.8758680555555556:  12%|█▏        | 17/141 [00:07<00:51,  2.39it/s]avg_loss = 1.8758680555555556:  13%|█▎        | 18/141 [00:07<00:51,  2.39it/s]avg_loss = 1.8560855263157894:  13%|█▎        | 18/141 [00:08<00:51,  2.39it/s]avg_loss = 1.8560855263157894:  13%|█▎        | 19/141 [00:08<00:50,  2.40it/s]avg_loss = 1.8546875:  13%|█▎        | 19/141 [00:08<00:50,  2.40it/s]         avg_loss = 1.8546875:  14%|█▍        | 20/141 [00:08<00:50,  2.39it/s]avg_loss = 1.8597470238095237:  14%|█▍        | 20/141 [00:08<00:50,  2.39it/s]avg_loss = 1.8597470238095237:  15%|█▍        | 21/141 [00:08<00:50,  2.39it/s]avg_loss = 1.8618607954545454:  15%|█▍        | 21/141 [00:09<00:50,  2.39it/s]avg_loss = 1.8618607954545454:  16%|█▌        | 22/141 [00:09<00:49,  2.40it/s]avg_loss = 1.8634510869565217:  16%|█▌        | 22/141 [00:09<00:49,  2.40it/s]avg_loss = 1.8634510869565217:  16%|█▋        | 23/141 [00:09<00:49,  2.40it/s]avg_loss = 1.8684895833333333:  16%|█▋        | 23/141 [00:10<00:49,  2.40it/s]avg_loss = 1.8684895833333333:  17%|█▋        | 24/141 [00:10<00:48,  2.40it/s]avg_loss = 1.874375:  17%|█▋        | 24/141 [00:10<00:48,  2.40it/s]          avg_loss = 1.874375:  18%|█▊        | 25/141 [00:10<00:48,  2.40it/s]avg_loss = 1.8858173076923077:  18%|█▊        | 25/141 [00:10<00:48,  2.40it/s]avg_loss = 1.8858173076923077:  18%|█▊        | 26/141 [00:10<00:47,  2.40it/s]avg_loss = 1.8993055555555556:  18%|█▊        | 26/141 [00:11<00:47,  2.40it/s]avg_loss = 1.8993055555555556:  19%|█▉        | 27/141 [00:11<00:47,  2.40it/s]avg_loss = 1.90625:  19%|█▉        | 27/141 [00:11<00:47,  2.40it/s]           avg_loss = 1.90625:  20%|█▉        | 28/141 [00:11<00:48,  2.35it/s]avg_loss = 1.9027478448275863:  20%|█▉        | 28/141 [00:12<00:48,  2.35it/s]avg_loss = 1.9027478448275863:  21%|██        | 29/141 [00:12<00:52,  2.13it/s]avg_loss = 1.8932291666666667:  21%|██        | 29/141 [00:13<00:52,  2.13it/s]avg_loss = 1.8932291666666667:  21%|██▏       | 30/141 [00:13<01:03,  1.75it/s]avg_loss = 1.8787802419354838:  21%|██▏       | 30/141 [00:13<01:03,  1.75it/s]avg_loss = 1.8787802419354838:  22%|██▏       | 31/141 [00:13<01:04,  1.69it/s]avg_loss = 1.86669921875:  22%|██▏       | 31/141 [00:14<01:04,  1.69it/s]     avg_loss = 1.86669921875:  23%|██▎       | 32/141 [00:14<01:04,  1.69it/s]avg_loss = 1.8652935606060606:  23%|██▎       | 32/141 [00:15<01:04,  1.69it/s]avg_loss = 1.8652935606060606:  23%|██▎       | 33/141 [00:15<01:05,  1.66it/s]avg_loss = 1.8639705882352942:  23%|██▎       | 33/141 [00:15<01:05,  1.66it/s]avg_loss = 1.8639705882352942:  24%|██▍       | 34/141 [00:15<01:11,  1.49it/s]avg_loss = 1.8669642857142856:  24%|██▍       | 34/141 [00:16<01:11,  1.49it/s]avg_loss = 1.8669642857142856:  25%|██▍       | 35/141 [00:16<01:09,  1.52it/s]avg_loss = 1.8509114583333333:  25%|██▍       | 35/141 [00:17<01:09,  1.52it/s]avg_loss = 1.8509114583333333:  26%|██▌       | 36/141 [00:17<01:12,  1.45it/s]avg_loss = 1.835304054054054:  26%|██▌       | 36/141 [00:18<01:12,  1.45it/s] avg_loss = 1.835304054054054:  26%|██▌       | 37/141 [00:18<01:13,  1.42it/s]avg_loss = 1.820518092105263:  26%|██▌       | 37/141 [00:18<01:13,  1.42it/s]avg_loss = 1.820518092105263:  27%|██▋       | 38/141 [00:18<01:03,  1.61it/s]avg_loss = 1.806290064102564:  27%|██▋       | 38/141 [00:18<01:03,  1.61it/s]avg_loss = 1.806290064102564:  28%|██▊       | 39/141 [00:18<00:57,  1.79it/s]avg_loss = 1.798046875:  28%|██▊       | 39/141 [00:19<00:57,  1.79it/s]      avg_loss = 1.798046875:  28%|██▊       | 40/141 [00:19<00:52,  1.93it/s]avg_loss = 1.8029725609756098:  28%|██▊       | 40/141 [00:19<00:52,  1.93it/s]avg_loss = 1.8029725609756098:  29%|██▉       | 41/141 [00:19<00:48,  2.05it/s]avg_loss = 1.8199404761904763:  29%|██▉       | 41/141 [00:20<00:48,  2.05it/s]avg_loss = 1.8199404761904763:  30%|██▉       | 42/141 [00:20<00:46,  2.14it/s]avg_loss = 1.8361191860465116:  30%|██▉       | 42/141 [00:20<00:46,  2.14it/s]avg_loss = 1.8361191860465116:  30%|███       | 43/141 [00:20<00:44,  2.20it/s]avg_loss = 1.8391335227272727:  30%|███       | 43/141 [00:21<00:44,  2.20it/s]avg_loss = 1.8391335227272727:  31%|███       | 44/141 [00:21<00:43,  2.25it/s]avg_loss = 1.8434027777777777:  31%|███       | 44/141 [00:21<00:43,  2.25it/s]avg_loss = 1.8434027777777777:  32%|███▏      | 45/141 [00:21<00:41,  2.29it/s]avg_loss = 1.848845108695652:  32%|███▏      | 45/141 [00:21<00:41,  2.29it/s] avg_loss = 1.848845108695652:  33%|███▎      | 46/141 [00:21<00:41,  2.32it/s]avg_loss = 1.855718085106383:  33%|███▎      | 46/141 [00:22<00:41,  2.32it/s]avg_loss = 1.855718085106383:  33%|███▎      | 47/141 [00:22<00:40,  2.34it/s]avg_loss = 1.8590494791666667:  33%|███▎      | 47/141 [00:22<00:40,  2.34it/s]avg_loss = 1.8590494791666667:  34%|███▍      | 48/141 [00:22<00:39,  2.35it/s]avg_loss = 1.8577806122448979:  34%|███▍      | 48/141 [00:23<00:39,  2.35it/s]avg_loss = 1.8577806122448979:  35%|███▍      | 49/141 [00:23<00:39,  2.35it/s]avg_loss = 1.85734375:  35%|███▍      | 49/141 [00:23<00:39,  2.35it/s]        avg_loss = 1.85734375:  35%|███▌      | 50/141 [00:23<00:38,  2.36it/s]avg_loss = 1.850796568627451:  35%|███▌      | 50/141 [00:23<00:38,  2.36it/s]avg_loss = 1.850796568627451:  36%|███▌      | 51/141 [00:23<00:38,  2.36it/s]avg_loss = 1.8469050480769231:  36%|███▌      | 51/141 [00:24<00:38,  2.36it/s]avg_loss = 1.8469050480769231:  37%|███▋      | 52/141 [00:24<00:37,  2.36it/s]avg_loss = 1.8405070754716981:  37%|███▋      | 52/141 [00:24<00:37,  2.36it/s]avg_loss = 1.8405070754716981:  38%|███▊      | 53/141 [00:24<00:37,  2.36it/s]avg_loss = 1.8375289351851851:  38%|███▊      | 53/141 [00:25<00:37,  2.36it/s]avg_loss = 1.8375289351851851:  38%|███▊      | 54/141 [00:25<00:36,  2.37it/s]avg_loss = 1.829971590909091:  38%|███▊      | 54/141 [00:25<00:36,  2.37it/s] avg_loss = 1.829971590909091:  39%|███▉      | 55/141 [00:25<00:36,  2.37it/s]avg_loss = 1.822265625:  39%|███▉      | 55/141 [00:26<00:36,  2.37it/s]      avg_loss = 1.822265625:  40%|███▉      | 56/141 [00:26<00:36,  2.36it/s]avg_loss = 1.8159265350877194:  40%|███▉      | 56/141 [00:26<00:36,  2.36it/s]avg_loss = 1.8159265350877194:  40%|████      | 57/141 [00:26<00:35,  2.37it/s]avg_loss = 1.8131734913793103:  40%|████      | 57/141 [00:26<00:35,  2.37it/s]avg_loss = 1.8131734913793103:  41%|████      | 58/141 [00:26<00:35,  2.37it/s]avg_loss = 1.8154131355932204:  41%|████      | 58/141 [00:27<00:35,  2.37it/s]avg_loss = 1.8154131355932204:  42%|████▏     | 59/141 [00:27<00:34,  2.36it/s]avg_loss = 1.82109375:  42%|████▏     | 59/141 [00:27<00:34,  2.36it/s]        avg_loss = 1.82109375:  43%|████▎     | 60/141 [00:27<00:34,  2.37it/s]avg_loss = 1.8271004098360655:  43%|████▎     | 60/141 [00:28<00:34,  2.37it/s]avg_loss = 1.8271004098360655:  43%|████▎     | 61/141 [00:28<00:33,  2.37it/s]avg_loss = 1.8344254032258065:  43%|████▎     | 61/141 [00:28<00:33,  2.37it/s]avg_loss = 1.8344254032258065:  44%|████▍     | 62/141 [00:28<00:33,  2.37it/s]avg_loss = 1.8253968253968254:  44%|████▍     | 62/141 [00:29<00:33,  2.37it/s]avg_loss = 1.8253968253968254:  45%|████▍     | 63/141 [00:29<00:32,  2.37it/s]avg_loss = 1.8232421875:  45%|████▍     | 63/141 [00:29<00:32,  2.37it/s]      avg_loss = 1.8232421875:  45%|████▌     | 64/141 [00:29<00:32,  2.37it/s]avg_loss = 1.820673076923077:  45%|████▌     | 64/141 [00:29<00:32,  2.37it/s]avg_loss = 1.820673076923077:  46%|████▌     | 65/141 [00:29<00:32,  2.37it/s]avg_loss = 1.8146306818181819:  46%|████▌     | 65/141 [00:30<00:32,  2.37it/s]avg_loss = 1.8146306818181819:  47%|████▋     | 66/141 [00:30<00:31,  2.37it/s]avg_loss = 1.812150186567164:  47%|████▋     | 66/141 [00:30<00:31,  2.37it/s] avg_loss = 1.812150186567164:  48%|████▊     | 67/141 [00:30<00:31,  2.37it/s]avg_loss = 1.8087086397058822:  48%|████▊     | 67/141 [00:31<00:31,  2.37it/s]avg_loss = 1.8087086397058822:  48%|████▊     | 68/141 [00:31<00:30,  2.37it/s]avg_loss = 1.8059329710144927:  48%|████▊     | 68/141 [00:31<00:30,  2.37it/s]avg_loss = 1.8059329710144927:  49%|████▉     | 69/141 [00:31<00:30,  2.37it/s]avg_loss = 1.8068080357142857:  49%|████▉     | 69/141 [00:31<00:30,  2.37it/s]avg_loss = 1.8068080357142857:  50%|████▉     | 70/141 [00:31<00:29,  2.37it/s]avg_loss = 1.8106294014084507:  50%|████▉     | 70/141 [00:32<00:29,  2.37it/s]avg_loss = 1.8106294014084507:  50%|█████     | 71/141 [00:32<00:29,  2.37it/s]avg_loss = 1.8130425347222223:  50%|█████     | 71/141 [00:32<00:29,  2.37it/s]avg_loss = 1.8130425347222223:  51%|█████     | 72/141 [00:32<00:29,  2.37it/s]avg_loss = 1.8116438356164384:  51%|█████     | 72/141 [00:33<00:29,  2.37it/s]avg_loss = 1.8116438356164384:  52%|█████▏    | 73/141 [00:33<00:28,  2.37it/s]avg_loss = 1.8133445945945945:  52%|█████▏    | 73/141 [00:33<00:28,  2.37it/s]avg_loss = 1.8133445945945945:  52%|█████▏    | 74/141 [00:33<00:28,  2.37it/s]avg_loss = 1.8136458333333334:  52%|█████▏    | 74/141 [00:34<00:28,  2.37it/s]avg_loss = 1.8136458333333334:  53%|█████▎    | 75/141 [00:34<00:27,  2.37it/s]avg_loss = 1.8126027960526316:  53%|█████▎    | 75/141 [00:34<00:27,  2.37it/s]avg_loss = 1.8126027960526316:  54%|█████▍    | 76/141 [00:34<00:27,  2.37it/s]avg_loss = 1.8139204545454546:  54%|█████▍    | 76/141 [00:34<00:27,  2.37it/s]avg_loss = 1.8139204545454546:  55%|█████▍    | 77/141 [00:34<00:27,  2.37it/s]avg_loss = 1.8165064102564104:  55%|█████▍    | 77/141 [00:35<00:27,  2.37it/s]avg_loss = 1.8165064102564104:  55%|█████▌    | 78/141 [00:35<00:26,  2.37it/s]avg_loss = 1.8208069620253164:  55%|█████▌    | 78/141 [00:35<00:26,  2.37it/s]avg_loss = 1.8208069620253164:  56%|█████▌    | 79/141 [00:35<00:26,  2.37it/s]avg_loss = 1.81806640625:  56%|█████▌    | 79/141 [00:36<00:26,  2.37it/s]     avg_loss = 1.81806640625:  57%|█████▋    | 80/141 [00:36<00:25,  2.36it/s]avg_loss = 1.8170331790123457:  57%|█████▋    | 80/141 [00:36<00:25,  2.36it/s]avg_loss = 1.8170331790123457:  57%|█████▋    | 81/141 [00:36<00:25,  2.37it/s]avg_loss = 1.81640625:  57%|█████▋    | 81/141 [00:37<00:25,  2.37it/s]        avg_loss = 1.81640625:  58%|█████▊    | 82/141 [00:37<00:24,  2.36it/s]avg_loss = 1.8146649096385543:  58%|█████▊    | 82/141 [00:37<00:24,  2.36it/s]avg_loss = 1.8146649096385543:  59%|█████▉    | 83/141 [00:37<00:24,  2.36it/s]avg_loss = 1.812593005952381:  59%|█████▉    | 83/141 [00:37<00:24,  2.36it/s] avg_loss = 1.812593005952381:  60%|█████▉    | 84/141 [00:37<00:24,  2.37it/s]avg_loss = 1.8102941176470588:  60%|█████▉    | 84/141 [00:38<00:24,  2.37it/s]avg_loss = 1.8102941176470588:  60%|██████    | 85/141 [00:38<00:23,  2.37it/s]avg_loss = 1.812045784883721:  60%|██████    | 85/141 [00:38<00:23,  2.37it/s] avg_loss = 1.812045784883721:  61%|██████    | 86/141 [00:38<00:23,  2.36it/s]avg_loss = 1.8140265804597702:  61%|██████    | 86/141 [00:39<00:23,  2.36it/s]avg_loss = 1.8140265804597702:  62%|██████▏   | 87/141 [00:39<00:22,  2.36it/s]avg_loss = 1.8141867897727273:  62%|██████▏   | 87/141 [00:39<00:22,  2.36it/s]avg_loss = 1.8141867897727273:  62%|██████▏   | 88/141 [00:39<00:22,  2.36it/s]avg_loss = 1.8229459269662922:  62%|██████▏   | 88/141 [00:39<00:22,  2.36it/s]avg_loss = 1.8229459269662922:  63%|██████▎   | 89/141 [00:39<00:22,  2.36it/s]avg_loss = 1.8306423611111111:  63%|██████▎   | 89/141 [00:40<00:22,  2.36it/s]avg_loss = 1.8306423611111111:  64%|██████▍   | 90/141 [00:40<00:21,  2.36it/s]avg_loss = 1.8338770604395604:  64%|██████▍   | 90/141 [00:40<00:21,  2.36it/s]avg_loss = 1.8338770604395604:  65%|██████▍   | 91/141 [00:40<00:21,  2.36it/s]avg_loss = 1.8389096467391304:  65%|██████▍   | 91/141 [00:41<00:21,  2.36it/s]avg_loss = 1.8389096467391304:  65%|██████▌   | 92/141 [00:41<00:20,  2.36it/s]avg_loss = 1.8440020161290323:  65%|██████▌   | 92/141 [00:41<00:20,  2.36it/s]avg_loss = 1.8440020161290323:  66%|██████▌   | 93/141 [00:41<00:20,  2.36it/s]avg_loss = 1.8450797872340425:  66%|██████▌   | 93/141 [00:42<00:20,  2.36it/s]avg_loss = 1.8450797872340425:  67%|██████▋   | 94/141 [00:42<00:19,  2.36it/s]avg_loss = 1.8488486842105263:  67%|██████▋   | 94/141 [00:42<00:19,  2.36it/s]avg_loss = 1.8488486842105263:  67%|██████▋   | 95/141 [00:42<00:19,  2.36it/s]avg_loss = 1.8497721354166667:  67%|██████▋   | 95/141 [00:42<00:19,  2.36it/s]avg_loss = 1.8497721354166667:  68%|██████▊   | 96/141 [00:42<00:19,  2.36it/s]avg_loss = 1.8518041237113403:  68%|██████▊   | 96/141 [00:43<00:19,  2.36it/s]avg_loss = 1.8518041237113403:  69%|██████▉   | 97/141 [00:43<00:18,  2.36it/s]avg_loss = 1.8466198979591837:  69%|██████▉   | 97/141 [00:43<00:18,  2.36it/s]avg_loss = 1.8466198979591837:  70%|██████▉   | 98/141 [00:43<00:18,  2.36it/s]avg_loss = 1.8473800505050506:  70%|██████▉   | 98/141 [00:44<00:18,  2.36it/s]avg_loss = 1.8473800505050506:  70%|███████   | 99/141 [00:44<00:17,  2.36it/s]avg_loss = 1.8490625:  70%|███████   | 99/141 [00:44<00:17,  2.36it/s]         avg_loss = 1.8490625:  71%|███████   | 100/141 [00:44<00:17,  2.36it/s]avg_loss = 1.8474628712871286:  71%|███████   | 100/141 [00:45<00:17,  2.36it/s]avg_loss = 1.8474628712871286:  72%|███████▏  | 101/141 [00:45<00:16,  2.36it/s]avg_loss = 1.8475030637254901:  72%|███████▏  | 101/141 [00:45<00:16,  2.36it/s]avg_loss = 1.8475030637254901:  72%|███████▏  | 102/141 [00:45<00:16,  2.36it/s]avg_loss = 1.8453428398058251:  72%|███████▏  | 102/141 [00:45<00:16,  2.36it/s]avg_loss = 1.8453428398058251:  73%|███████▎  | 103/141 [00:45<00:16,  2.36it/s]avg_loss = 1.8475811298076923:  73%|███████▎  | 103/141 [00:46<00:16,  2.36it/s]avg_loss = 1.8475811298076923:  74%|███████▍  | 104/141 [00:46<00:15,  2.36it/s]avg_loss = 1.8453869047619047:  74%|███████▍  | 104/141 [00:46<00:15,  2.36it/s]avg_loss = 1.8453869047619047:  74%|███████▍  | 105/141 [00:46<00:15,  2.36it/s]avg_loss = 1.8439711084905661:  74%|███████▍  | 105/141 [00:47<00:15,  2.36it/s]avg_loss = 1.8439711084905661:  75%|███████▌  | 106/141 [00:47<00:14,  2.36it/s]avg_loss = 1.8415595794392523:  75%|███████▌  | 106/141 [00:47<00:14,  2.36it/s]avg_loss = 1.8415595794392523:  76%|███████▌  | 107/141 [00:47<00:14,  2.36it/s]avg_loss = 1.8390480324074074:  76%|███████▌  | 107/141 [00:48<00:14,  2.36it/s]avg_loss = 1.8390480324074074:  77%|███████▋  | 108/141 [00:48<00:13,  2.36it/s]avg_loss = 1.836295871559633:  77%|███████▋  | 108/141 [00:48<00:13,  2.36it/s] avg_loss = 1.836295871559633:  77%|███████▋  | 109/141 [00:48<00:13,  2.36it/s]avg_loss = 1.8337357954545455:  77%|███████▋  | 109/141 [00:48<00:13,  2.36it/s]avg_loss = 1.8337357954545455:  78%|███████▊  | 110/141 [00:48<00:13,  2.36it/s]avg_loss = 1.8360782657657657:  78%|███████▊  | 110/141 [00:49<00:13,  2.36it/s]avg_loss = 1.8360782657657657:  79%|███████▊  | 111/141 [00:49<00:12,  2.36it/s]avg_loss = 1.8358677455357142:  79%|███████▊  | 111/141 [00:49<00:12,  2.36it/s]avg_loss = 1.8358677455357142:  79%|███████▉  | 112/141 [00:49<00:12,  2.36it/s]avg_loss = 1.8370436946902655:  79%|███████▉  | 112/141 [00:50<00:12,  2.36it/s]avg_loss = 1.8370436946902655:  80%|████████  | 113/141 [00:50<00:11,  2.36it/s]avg_loss = 1.838061951754386:  80%|████████  | 113/141 [00:50<00:11,  2.36it/s] avg_loss = 1.838061951754386:  81%|████████  | 114/141 [00:50<00:11,  2.36it/s]avg_loss = 1.8374320652173912:  81%|████████  | 114/141 [00:51<00:11,  2.36it/s]avg_loss = 1.8374320652173912:  82%|████████▏ | 115/141 [00:51<00:10,  2.37it/s]avg_loss = 1.836004849137931:  82%|████████▏ | 115/141 [00:51<00:10,  2.37it/s] avg_loss = 1.836004849137931:  82%|████████▏ | 116/141 [00:51<00:10,  2.37it/s]avg_loss = 1.8382077991452992:  82%|████████▏ | 116/141 [00:51<00:10,  2.37it/s]avg_loss = 1.8382077991452992:  83%|████████▎ | 117/141 [00:51<00:10,  2.37it/s]avg_loss = 1.837989936440678:  83%|████████▎ | 117/141 [00:52<00:10,  2.37it/s] avg_loss = 1.837989936440678:  84%|████████▎ | 118/141 [00:52<00:09,  2.36it/s]avg_loss = 1.8366596638655461:  84%|████████▎ | 118/141 [00:52<00:09,  2.36it/s]avg_loss = 1.8366596638655461:  84%|████████▍ | 119/141 [00:52<00:09,  2.36it/s]avg_loss = 1.8349609375:  84%|████████▍ | 119/141 [00:53<00:09,  2.36it/s]      avg_loss = 1.8349609375:  85%|████████▌ | 120/141 [00:53<00:08,  2.37it/s]avg_loss = 1.8348398760330578:  85%|████████▌ | 120/141 [00:53<00:08,  2.37it/s]avg_loss = 1.8348398760330578:  86%|████████▌ | 121/141 [00:53<00:08,  2.37it/s]avg_loss = 1.835233094262295:  86%|████████▌ | 121/141 [00:53<00:08,  2.37it/s] avg_loss = 1.835233094262295:  87%|████████▋ | 122/141 [00:53<00:08,  2.37it/s]avg_loss = 1.8351753048780488:  87%|████████▋ | 122/141 [00:54<00:08,  2.37it/s]avg_loss = 1.8351753048780488:  87%|████████▋ | 123/141 [00:54<00:07,  2.37it/s]avg_loss = 1.8354964717741935:  87%|████████▋ | 123/141 [00:54<00:07,  2.37it/s]avg_loss = 1.8354964717741935:  88%|████████▊ | 124/141 [00:54<00:07,  2.36it/s]avg_loss = 1.8344375:  88%|████████▊ | 124/141 [00:55<00:07,  2.36it/s]         avg_loss = 1.8344375:  89%|████████▊ | 125/141 [00:55<00:06,  2.36it/s]avg_loss = 1.8348834325396826:  89%|████████▊ | 125/141 [00:55<00:06,  2.36it/s]avg_loss = 1.8348834325396826:  89%|████████▉ | 126/141 [00:55<00:06,  2.36it/s]avg_loss = 1.8347071850393701:  89%|████████▉ | 126/141 [00:56<00:06,  2.36it/s]avg_loss = 1.8347071850393701:  90%|█████████ | 127/141 [00:56<00:05,  2.36it/s]avg_loss = 1.8333740234375:  90%|█████████ | 127/141 [00:56<00:05,  2.36it/s]   avg_loss = 1.8333740234375:  91%|█████████ | 128/141 [00:56<00:05,  2.36it/s]avg_loss = 1.833575581395349:  91%|█████████ | 128/141 [00:56<00:05,  2.36it/s]avg_loss = 1.833575581395349:  91%|█████████▏| 129/141 [00:56<00:05,  2.37it/s]avg_loss = 1.834314903846154:  91%|█████████▏| 129/141 [00:57<00:05,  2.37it/s]avg_loss = 1.834314903846154:  92%|█████████▏| 130/141 [00:57<00:04,  2.36it/s]avg_loss = 1.8352814885496183:  92%|█████████▏| 130/141 [00:57<00:04,  2.36it/s]avg_loss = 1.8352814885496183:  93%|█████████▎| 131/141 [00:57<00:04,  2.12it/s]avg_loss = 1.8359966856060606:  93%|█████████▎| 131/141 [00:58<00:04,  2.12it/s]avg_loss = 1.8359966856060606:  94%|█████████▎| 132/141 [00:58<00:04,  2.14it/s]avg_loss = 1.8331766917293233:  94%|█████████▎| 132/141 [00:58<00:04,  2.14it/s]avg_loss = 1.8331766917293233:  94%|█████████▍| 133/141 [00:58<00:03,  2.15it/s]avg_loss = 1.8288829291044777:  94%|█████████▍| 133/141 [00:59<00:03,  2.15it/s]avg_loss = 1.8288829291044777:  95%|█████████▌| 134/141 [00:59<00:03,  2.16it/s]avg_loss = 1.831423611111111:  95%|█████████▌| 134/141 [00:59<00:03,  2.16it/s] avg_loss = 1.831423611111111:  96%|█████████▌| 135/141 [00:59<00:02,  2.17it/s]avg_loss = 1.8349609375:  96%|█████████▌| 135/141 [01:00<00:02,  2.17it/s]     avg_loss = 1.8349609375:  96%|█████████▋| 136/141 [01:00<00:02,  2.18it/s]avg_loss = 1.8358234489051095:  96%|█████████▋| 136/141 [01:00<00:02,  2.18it/s]avg_loss = 1.8358234489051095:  97%|█████████▋| 137/141 [01:00<00:01,  2.18it/s]avg_loss = 1.834465579710145:  97%|█████████▋| 137/141 [01:01<00:01,  2.18it/s] avg_loss = 1.834465579710145:  98%|█████████▊| 138/141 [01:01<00:01,  2.19it/s]avg_loss = 1.8346447841726619:  98%|█████████▊| 138/141 [01:01<00:01,  2.19it/s]avg_loss = 1.8346447841726619:  99%|█████████▊| 139/141 [01:01<00:00,  2.19it/s]avg_loss = 1.8352678571428571:  99%|█████████▊| 139/141 [01:02<00:00,  2.19it/s]avg_loss = 1.8352678571428571:  99%|█████████▉| 140/141 [01:02<00:00,  2.18it/s]avg_loss = 1.836546985815603:  99%|█████████▉| 140/141 [01:02<00:00,  2.18it/s] avg_loss = 1.836546985815603: 100%|██████████| 141/141 [01:02<00:00,  2.19it/s]avg_loss = 1.836546985815603: 100%|██████████| 141/141 [01:02<00:00,  2.26it/s]
I0302 20:43:55.533963 2566348 eval_ppl.py:105] wikitext2 perplexity: 6.274833679199219
wikitext2 perplexity: 6.275
Running with lmbda=10000
/home/jgryu/Weight_compression/comp_llm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")
/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.80it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.22it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.50it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.37it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.05it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.94it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.22it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.18it/s]
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:12,  2.42it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:12,  2.38it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:12,  2.30it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:12,  2.31it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:12,  2.09it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:12,  2.01it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:12,  2.01it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:11,  2.01it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:11,  2.01it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:04<00:10,  2.03it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:10,  2.04it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:05<00:09,  2.01it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:09,  2.02it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:06<00:08,  2.02it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:08,  1.98it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:07<00:07,  2.00it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:08<00:07,  2.01it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:08<00:06,  2.02it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:09<00:06,  2.03it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:09<00:06,  1.99it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:10<00:05,  1.96it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:10<00:05,  1.97it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:11<00:04,  1.95it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:11<00:04,  1.95it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:12<00:03,  1.95it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:12<00:03,  1.98it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:13<00:02,  2.00it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:13<00:02,  1.99it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:14<00:01,  2.00it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:14<00:00,  2.04it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:15<00:00,  2.07it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:15<00:00,  2.06it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:15<00:00,  2.03it/s]
pseudo compress quantization...:   0%|          | 0/32 [00:00<?, ?it/s]2025-03-02 20:44:37 - INFO - layer0_self_attn.q_proj | mse: 0.0006056285145447, bpp_loss: 6.319071650505066, bpp: 0
2025-03-02 20:44:37 - INFO - layer0_self_attn.k_proj | mse: 0.0009169282774026266, bpp_loss: 7.10249125957489, bpp: 0
2025-03-02 20:44:38 - INFO - layer0_self_attn.v_proj | mse: 0.00040931807482381423, bpp_loss: 5.404968857765198, bpp: 0
2025-03-02 20:44:39 - INFO - layer0_self_attn.o_proj | mse: 0.00042112162211044594, bpp_loss: 5.48613703250885, bpp: 0
2025-03-02 20:44:43 - INFO - layer0_mlp.gate_proj | mse: 0.0004906296790201151, bpp_loss: 5.87984916142055, bpp: 0
2025-03-02 20:44:46 - INFO - layer0_mlp.up_proj | mse: 0.00047119186843407523, bpp_loss: 5.767597198486328, bpp: 0
2025-03-02 20:44:50 - INFO - layer0_mlp.down_proj | mse: 0.00047054086158238034, bpp_loss: 5.7633746692112515, bpp: 0
pseudo compress quantization...:   3%|▎         | 1/32 [00:14<07:35, 14.69s/it]2025-03-02 20:44:51 - INFO - layer1_self_attn.q_proj | mse: 0.0006286135144924479, bpp_loss: 6.626657724380493, bpp: 0
2025-03-02 20:44:52 - INFO - layer1_self_attn.k_proj | mse: 0.0008636436071493992, bpp_loss: 7.646036505699158, bpp: 0
2025-03-02 20:44:52 - INFO - layer1_self_attn.v_proj | mse: 0.0004278880271618948, bpp_loss: 5.5068700313568115, bpp: 0
2025-03-02 20:44:54 - INFO - layer1_self_attn.o_proj | mse: 0.00043584285339491136, bpp_loss: 5.5692819356918335, bpp: 0
2025-03-02 20:44:57 - INFO - layer1_mlp.gate_proj | mse: 0.0004931192215124954, bpp_loss: 5.8928695406232565, bpp: 0
2025-03-02 20:45:01 - INFO - layer1_mlp.up_proj | mse: 0.000474138057261477, bpp_loss: 5.782279287065778, bpp: 0
2025-03-02 20:45:05 - INFO - layer1_mlp.down_proj | mse: 0.0004733475785032207, bpp_loss: 5.778380972998483, bpp: 0
pseudo compress quantization...:   6%|▋         | 2/32 [00:29<07:27, 14.92s/it]2025-03-02 20:45:06 - INFO - layer2_self_attn.q_proj | mse: 0.0006129024452412977, bpp_loss: 6.515012145042419, bpp: 0
2025-03-02 20:45:07 - INFO - layer2_self_attn.k_proj | mse: 0.0009032742441747102, bpp_loss: 7.670872211456299, bpp: 0
2025-03-02 20:45:08 - INFO - layer2_self_attn.v_proj | mse: 0.00040996850614696297, bpp_loss: 5.414968252182007, bpp: 0
2025-03-02 20:45:09 - INFO - layer2_self_attn.o_proj | mse: 0.00042599666576060576, bpp_loss: 5.515327453613281, bpp: 0
2025-03-02 20:45:13 - INFO - layer2_mlp.gate_proj | mse: 0.0005000295992330691, bpp_loss: 5.928293228149414, bpp: 0
2025-03-02 20:45:17 - INFO - layer2_mlp.up_proj | mse: 0.000473040595456657, bpp_loss: 5.7767069680350165, bpp: 0
2025-03-02 20:45:21 - INFO - layer2_mlp.down_proj | mse: 0.00047354892296180276, bpp_loss: 5.780181952885219, bpp: 0
pseudo compress quantization...:   9%|▉         | 3/32 [00:45<07:19, 15.17s/it]2025-03-02 20:45:22 - INFO - layer3_self_attn.q_proj | mse: 0.0006175722062656325, bpp_loss: 6.546782732009888, bpp: 0
2025-03-02 20:45:22 - INFO - layer3_self_attn.k_proj | mse: 0.0009036882496644116, bpp_loss: 7.744290113449097, bpp: 0
2025-03-02 20:45:23 - INFO - layer3_self_attn.v_proj | mse: 0.0004266344225225128, bpp_loss: 5.509547829627991, bpp: 0
2025-03-02 20:45:24 - INFO - layer3_self_attn.o_proj | mse: 0.0004433468273780455, bpp_loss: 5.611493468284607, bpp: 0
2025-03-02 20:45:28 - INFO - layer3_mlp.gate_proj | mse: 0.0005140463189108838, bpp_loss: 6.009732382638114, bpp: 0
2025-03-02 20:45:31 - INFO - layer3_mlp.up_proj | mse: 0.000469699167250127, bpp_loss: 5.757469858442034, bpp: 0
2025-03-02 20:45:35 - INFO - layer3_mlp.down_proj | mse: 0.00046913243329746354, bpp_loss: 5.755270344870431, bpp: 0
pseudo compress quantization...:  12%|█▎        | 4/32 [00:59<06:55, 14.84s/it]2025-03-02 20:45:36 - INFO - layer4_self_attn.q_proj | mse: 0.0006118028817872922, bpp_loss: 6.510618567466736, bpp: 0
2025-03-02 20:45:36 - INFO - layer4_self_attn.k_proj | mse: 0.0008885510211734782, bpp_loss: 7.693511128425598, bpp: 0
2025-03-02 20:45:37 - INFO - layer4_self_attn.v_proj | mse: 0.0004356295133694399, bpp_loss: 5.5569069385528564, bpp: 0
2025-03-02 20:45:38 - INFO - layer4_self_attn.o_proj | mse: 0.0004466719985197991, bpp_loss: 5.626067519187927, bpp: 0
2025-03-02 20:45:42 - INFO - layer4_mlp.gate_proj | mse: 0.0005292874012046442, bpp_loss: 6.089661734444754, bpp: 0
2025-03-02 20:45:45 - INFO - layer4_mlp.up_proj | mse: 0.0004650844888342905, bpp_loss: 5.731195722307477, bpp: 0
2025-03-02 20:45:49 - INFO - layer4_mlp.down_proj | mse: 0.0004645984582103495, bpp_loss: 5.729692765644619, bpp: 0
pseudo compress quantization...:  16%|█▌        | 5/32 [01:13<06:34, 14.60s/it]2025-03-02 20:45:50 - INFO - layer5_self_attn.q_proj | mse: 0.0006105645318876903, bpp_loss: 6.485771656036377, bpp: 0
2025-03-02 20:45:51 - INFO - layer5_self_attn.k_proj | mse: 0.0009076747613052397, bpp_loss: 7.661806225776672, bpp: 0
2025-03-02 20:45:51 - INFO - layer5_self_attn.v_proj | mse: 0.00041578323271432256, bpp_loss: 5.444671988487244, bpp: 0
2025-03-02 20:45:52 - INFO - layer5_self_attn.o_proj | mse: 0.0004356741006652365, bpp_loss: 5.56851589679718, bpp: 0
2025-03-02 20:45:56 - INFO - layer5_mlp.gate_proj | mse: 0.0005286650026483234, bpp_loss: 6.089939662388393, bpp: 0
2025-03-02 20:46:00 - INFO - layer5_mlp.up_proj | mse: 0.0004661586708327483, bpp_loss: 5.736885343279157, bpp: 0
2025-03-02 20:46:03 - INFO - layer5_mlp.down_proj | mse: 0.0004652652953303707, bpp_loss: 5.734655788966587, bpp: 0
pseudo compress quantization...:  19%|█▉        | 6/32 [01:28<06:17, 14.51s/it]2025-03-02 20:46:05 - INFO - layer6_self_attn.q_proj | mse: 0.0006188848834136588, bpp_loss: 6.537695050239563, bpp: 0
2025-03-02 20:46:05 - INFO - layer6_self_attn.k_proj | mse: 0.0008902976662451614, bpp_loss: 7.720701098442078, bpp: 0
2025-03-02 20:46:05 - INFO - layer6_self_attn.v_proj | mse: 0.00042405225226726227, bpp_loss: 5.491141676902771, bpp: 0
2025-03-02 20:46:06 - INFO - layer6_self_attn.o_proj | mse: 0.0004416710693167354, bpp_loss: 5.6011152267456055, bpp: 0
2025-03-02 20:46:10 - INFO - layer6_mlp.gate_proj | mse: 0.0005304297671692919, bpp_loss: 6.09782954624721, bpp: 0
2025-03-02 20:46:14 - INFO - layer6_mlp.up_proj | mse: 0.0004661355979579981, bpp_loss: 5.736462729317801, bpp: 0
2025-03-02 20:46:18 - INFO - layer6_mlp.down_proj | mse: 0.0004649854720578963, bpp_loss: 5.734259128570557, bpp: 0
pseudo compress quantization...:  22%|██▏       | 7/32 [01:42<06:01, 14.45s/it]2025-03-02 20:46:19 - INFO - layer7_self_attn.q_proj | mse: 0.0006004005748965398, bpp_loss: 6.446156144142151, bpp: 0
2025-03-02 20:46:19 - INFO - layer7_self_attn.k_proj | mse: 0.0008939967369435445, bpp_loss: 7.755698800086975, bpp: 0
2025-03-02 20:46:20 - INFO - layer7_self_attn.v_proj | mse: 0.00042313515722884674, bpp_loss: 5.487397909164429, bpp: 0
2025-03-02 20:46:21 - INFO - layer7_self_attn.o_proj | mse: 0.0004440761578449236, bpp_loss: 5.613850831985474, bpp: 0
2025-03-02 20:46:25 - INFO - layer7_mlp.gate_proj | mse: 0.0005256502159915075, bpp_loss: 6.068416459219796, bpp: 0
2025-03-02 20:46:28 - INFO - layer7_mlp.up_proj | mse: 0.00046866425052506693, bpp_loss: 5.750458717346191, bpp: 0
2025-03-02 20:46:32 - INFO - layer7_mlp.down_proj | mse: 0.00046720393413841383, bpp_loss: 5.748949425561087, bpp: 0
pseudo compress quantization...:  25%|██▌       | 8/32 [01:56<05:46, 14.43s/it]2025-03-02 20:46:33 - INFO - layer8_self_attn.q_proj | mse: 0.000602735998604189, bpp_loss: 6.435277938842773, bpp: 0
2025-03-02 20:46:34 - INFO - layer8_self_attn.k_proj | mse: 0.0008958918554469159, bpp_loss: 7.653282880783081, bpp: 0
2025-03-02 20:46:34 - INFO - layer8_self_attn.v_proj | mse: 0.00042804265680904325, bpp_loss: 5.512471079826355, bpp: 0
2025-03-02 20:46:35 - INFO - layer8_self_attn.o_proj | mse: 0.00044636835224548066, bpp_loss: 5.627125978469849, bpp: 0
2025-03-02 20:46:39 - INFO - layer8_mlp.gate_proj | mse: 0.0005265347969003704, bpp_loss: 6.075543948582241, bpp: 0
2025-03-02 20:46:43 - INFO - layer8_mlp.up_proj | mse: 0.0004679359547162493, bpp_loss: 5.745824541364398, bpp: 0
2025-03-02 20:46:47 - INFO - layer8_mlp.down_proj | mse: 0.00046653611409323506, bpp_loss: 5.746167489460537, bpp: 0
pseudo compress quantization...:  28%|██▊       | 9/32 [02:11<05:31, 14.41s/it]2025-03-02 20:46:48 - INFO - layer9_self_attn.q_proj | mse: 0.0005995883618523991, bpp_loss: 6.445547938346863, bpp: 0
2025-03-02 20:46:48 - INFO - layer9_self_attn.k_proj | mse: 0.0008825344974846351, bpp_loss: 7.675172924995422, bpp: 0
2025-03-02 20:46:49 - INFO - layer9_self_attn.v_proj | mse: 0.0004467049976453176, bpp_loss: 5.615944743156433, bpp: 0
2025-03-02 20:46:50 - INFO - layer9_self_attn.o_proj | mse: 0.00045678756584678046, bpp_loss: 5.6842323541641235, bpp: 0
2025-03-02 20:46:53 - INFO - layer9_mlp.gate_proj | mse: 0.0005296943322198566, bpp_loss: 6.0936416898454935, bpp: 0
2025-03-02 20:46:57 - INFO - layer9_mlp.up_proj | mse: 0.000469420068238295, bpp_loss: 5.755361829485212, bpp: 0
2025-03-02 20:47:01 - INFO - layer9_mlp.down_proj | mse: 0.00046709405345238394, bpp_loss: 5.7492789540972025, bpp: 0
pseudo compress quantization...:  31%|███▏      | 10/32 [02:25<05:15, 14.36s/it]2025-03-02 20:47:02 - INFO - layer10_self_attn.q_proj | mse: 0.0006095736980626223, bpp_loss: 6.461548566818237, bpp: 0
2025-03-02 20:47:02 - INFO - layer10_self_attn.k_proj | mse: 0.0009041581137561565, bpp_loss: 7.686950087547302, bpp: 0
2025-03-02 20:47:03 - INFO - layer10_self_attn.v_proj | mse: 0.0004262196022395888, bpp_loss: 5.503589034080505, bpp: 0
2025-03-02 20:47:04 - INFO - layer10_self_attn.o_proj | mse: 0.0004445488284466578, bpp_loss: 5.6183366775512695, bpp: 0
2025-03-02 20:47:07 - INFO - layer10_mlp.gate_proj | mse: 0.000523672725773291, bpp_loss: 6.057361194065639, bpp: 0
2025-03-02 20:47:11 - INFO - layer10_mlp.up_proj | mse: 0.00047224846124550374, bpp_loss: 5.771875108991351, bpp: 0
2025-03-02 20:47:15 - INFO - layer10_mlp.down_proj | mse: 0.0004697126695378171, bpp_loss: 5.764663662229266, bpp: 0
pseudo compress quantization...:  34%|███▍      | 11/32 [02:39<04:59, 14.26s/it]2025-03-02 20:47:16 - INFO - layer11_self_attn.q_proj | mse: 0.0005849306391730126, bpp_loss: 6.376288056373596, bpp: 0
2025-03-02 20:47:16 - INFO - layer11_self_attn.k_proj | mse: 0.0008864392582054545, bpp_loss: 7.672305107116699, bpp: 0
2025-03-02 20:47:17 - INFO - layer11_self_attn.v_proj | mse: 0.0004265273224800206, bpp_loss: 5.5038957595825195, bpp: 0
2025-03-02 20:47:18 - INFO - layer11_self_attn.o_proj | mse: 0.0004480150040822251, bpp_loss: 5.637141704559326, bpp: 0
2025-03-02 20:47:21 - INFO - layer11_mlp.gate_proj | mse: 0.0005202940498765814, bpp_loss: 6.037706920078823, bpp: 0
2025-03-02 20:47:23 - INFO - layer11_mlp.up_proj | mse: 0.0004736562170026812, bpp_loss: 5.778383936200823, bpp: 0
2025-03-02 20:47:26 - INFO - layer11_mlp.down_proj | mse: 0.00047090713053581323, bpp_loss: 5.772511039461408, bpp: 0
pseudo compress quantization...:  38%|███▊      | 12/32 [02:50<04:25, 13.27s/it]2025-03-02 20:47:27 - INFO - layer12_self_attn.q_proj | mse: 0.0005919303466838886, bpp_loss: 6.429394483566284, bpp: 0
2025-03-02 20:47:27 - INFO - layer12_self_attn.k_proj | mse: 0.0008594332084374949, bpp_loss: 7.638084650039673, bpp: 0
2025-03-02 20:47:27 - INFO - layer12_self_attn.v_proj | mse: 0.0004462345450918007, bpp_loss: 5.621660113334656, bpp: 0
2025-03-02 20:47:28 - INFO - layer12_self_attn.o_proj | mse: 0.00045725955745416243, bpp_loss: 5.685441970825195, bpp: 0
2025-03-02 20:47:31 - INFO - layer12_mlp.gate_proj | mse: 0.0005169938010608612, bpp_loss: 6.019883973257882, bpp: 0
2025-03-02 20:47:34 - INFO - layer12_mlp.up_proj | mse: 0.0004776225269448773, bpp_loss: 5.798991884504046, bpp: 0
2025-03-02 20:47:36 - INFO - layer12_mlp.down_proj | mse: 0.00047371296799730215, bpp_loss: 5.78754414830889, bpp: 0
pseudo compress quantization...:  41%|████      | 13/32 [03:00<03:55, 12.39s/it]2025-03-02 20:47:37 - INFO - layer13_self_attn.q_proj | mse: 0.0005993363644151405, bpp_loss: 6.416510343551636, bpp: 0
2025-03-02 20:47:37 - INFO - layer13_self_attn.k_proj | mse: 0.0009037362319319693, bpp_loss: 7.699846863746643, bpp: 0
2025-03-02 20:47:38 - INFO - layer13_self_attn.v_proj | mse: 0.00043590695046176215, bpp_loss: 5.561992168426514, bpp: 0
2025-03-02 20:47:39 - INFO - layer13_self_attn.o_proj | mse: 0.00045298230043170826, bpp_loss: 5.664443135261536, bpp: 0
2025-03-02 20:47:41 - INFO - layer13_mlp.gate_proj | mse: 0.000518180036527276, bpp_loss: 6.023244857788086, bpp: 0
2025-03-02 20:47:44 - INFO - layer13_mlp.up_proj | mse: 0.0004780262505201019, bpp_loss: 5.802248818533761, bpp: 0
2025-03-02 20:47:47 - INFO - layer13_mlp.down_proj | mse: 0.0004739580784918452, bpp_loss: 5.78761761529105, bpp: 0
pseudo compress quantization...:  44%|████▍     | 14/32 [03:11<03:34, 11.93s/it]2025-03-02 20:47:48 - INFO - layer14_self_attn.q_proj | mse: 0.0005896579789184168, bpp_loss: 6.378239870071411, bpp: 0
2025-03-02 20:47:48 - INFO - layer14_self_attn.k_proj | mse: 0.0008982773889377599, bpp_loss: 7.635838270187378, bpp: 0
2025-03-02 20:47:49 - INFO - layer14_self_attn.v_proj | mse: 0.0004346577819188147, bpp_loss: 5.553811550140381, bpp: 0
2025-03-02 20:47:49 - INFO - layer14_self_attn.o_proj | mse: 0.00045250062183823444, bpp_loss: 5.658936619758606, bpp: 0
2025-03-02 20:47:52 - INFO - layer14_mlp.gate_proj | mse: 0.0005243715338273829, bpp_loss: 6.059584208897182, bpp: 0
2025-03-02 20:47:55 - INFO - layer14_mlp.up_proj | mse: 0.00047723375417862746, bpp_loss: 5.797333308628628, bpp: 0
2025-03-02 20:47:59 - INFO - layer14_mlp.down_proj | mse: 0.00047358265281457644, bpp_loss: 5.7845496109553745, bpp: 0
pseudo compress quantization...:  47%|████▋     | 15/32 [03:23<03:20, 11.79s/it]2025-03-02 20:48:00 - INFO - layer15_self_attn.q_proj | mse: 0.0006300546121056264, bpp_loss: 6.556238293647766, bpp: 0
2025-03-02 20:48:00 - INFO - layer15_self_attn.k_proj | mse: 0.0008743902770936358, bpp_loss: 7.661945581436157, bpp: 0
2025-03-02 20:48:01 - INFO - layer15_self_attn.v_proj | mse: 0.0004443839233516456, bpp_loss: 5.617037534713745, bpp: 0
2025-03-02 20:48:02 - INFO - layer15_self_attn.o_proj | mse: 0.00045752812235361554, bpp_loss: 5.688822031021118, bpp: 0
2025-03-02 20:48:05 - INFO - layer15_mlp.gate_proj | mse: 0.0005313000180640163, bpp_loss: 6.09808213370187, bpp: 0
2025-03-02 20:48:09 - INFO - layer15_mlp.up_proj | mse: 0.00047568104282716774, bpp_loss: 5.788658823285784, bpp: 0
2025-03-02 20:48:14 - INFO - layer15_mlp.down_proj | mse: 0.000472708418486319, bpp_loss: 5.7785976614270895, bpp: 0
pseudo compress quantization...:  50%|█████     | 16/32 [03:38<03:27, 12.95s/it]2025-03-02 20:48:16 - INFO - layer16_self_attn.q_proj | mse: 0.000615604073236251, bpp_loss: 6.510157823562622, bpp: 0
2025-03-02 20:48:16 - INFO - layer16_self_attn.k_proj | mse: 0.0008808498672304558, bpp_loss: 7.664705872535706, bpp: 0
2025-03-02 20:48:17 - INFO - layer16_self_attn.v_proj | mse: 0.00043612947954413384, bpp_loss: 5.570767760276794, bpp: 0
2025-03-02 20:48:18 - INFO - layer16_self_attn.o_proj | mse: 0.0004546457007958077, bpp_loss: 5.669236183166504, bpp: 0
2025-03-02 20:48:23 - INFO - layer16_mlp.gate_proj | mse: 0.000537936987392091, bpp_loss: 6.134886469159808, bpp: 0
2025-03-02 20:48:28 - INFO - layer16_mlp.up_proj | mse: 0.00047296206490986156, bpp_loss: 5.774292400905064, bpp: 0
2025-03-02 20:48:33 - INFO - layer16_mlp.down_proj | mse: 0.0004706007682840859, bpp_loss: 5.764283316476004, bpp: 0
pseudo compress quantization...:  53%|█████▎    | 17/32 [03:57<03:39, 14.64s/it]2025-03-02 20:48:34 - INFO - layer17_self_attn.q_proj | mse: 0.0006187032297912915, bpp_loss: 6.530200362205505, bpp: 0
2025-03-02 20:48:35 - INFO - layer17_self_attn.k_proj | mse: 0.000882362032919168, bpp_loss: 7.6991389989852905, bpp: 0
2025-03-02 20:48:35 - INFO - layer17_self_attn.v_proj | mse: 0.0004501991478481969, bpp_loss: 5.644440412521362, bpp: 0
2025-03-02 20:48:37 - INFO - layer17_self_attn.o_proj | mse: 0.0004591767659071265, bpp_loss: 5.698547005653381, bpp: 0
2025-03-02 20:48:41 - INFO - layer17_mlp.gate_proj | mse: 0.0005404924376029279, bpp_loss: 6.151030949183872, bpp: 0
2025-03-02 20:48:46 - INFO - layer17_mlp.up_proj | mse: 0.00047223304484040165, bpp_loss: 5.7703929628644675, bpp: 0
2025-03-02 20:48:51 - INFO - layer17_mlp.down_proj | mse: 0.00047023020549970176, bpp_loss: 5.76182542528425, bpp: 0
pseudo compress quantization...:  56%|█████▋    | 18/32 [04:15<03:40, 15.77s/it]2025-03-02 20:48:53 - INFO - layer18_self_attn.q_proj | mse: 0.0006114610760303783, bpp_loss: 6.51383638381958, bpp: 0
2025-03-02 20:48:53 - INFO - layer18_self_attn.k_proj | mse: 0.0009034926992909685, bpp_loss: 7.804102540016174, bpp: 0
2025-03-02 20:48:54 - INFO - layer18_self_attn.v_proj | mse: 0.0004344933747529941, bpp_loss: 5.565708041191101, bpp: 0
2025-03-02 20:48:55 - INFO - layer18_self_attn.o_proj | mse: 0.0004556544600717747, bpp_loss: 5.677343726158142, bpp: 0
2025-03-02 20:49:00 - INFO - layer18_mlp.gate_proj | mse: 0.0005415072819616134, bpp_loss: 6.155330521719796, bpp: 0
2025-03-02 20:49:05 - INFO - layer18_mlp.up_proj | mse: 0.0004711902129811784, bpp_loss: 5.765385900224958, bpp: 0
2025-03-02 20:49:10 - INFO - layer18_mlp.down_proj | mse: 0.00046971634257393207, bpp_loss: 5.759210280009678, bpp: 0
pseudo compress quantization...:  59%|█████▉    | 19/32 [04:34<03:36, 16.68s/it]2025-03-02 20:49:12 - INFO - layer19_self_attn.q_proj | mse: 0.0006132400412332278, bpp_loss: 6.516611099243164, bpp: 0
2025-03-02 20:49:12 - INFO - layer19_self_attn.k_proj | mse: 0.0008752146667698264, bpp_loss: 7.661573529243469, bpp: 0
2025-03-02 20:49:13 - INFO - layer19_self_attn.v_proj | mse: 0.00044214319672298777, bpp_loss: 5.608813047409058, bpp: 0
2025-03-02 20:49:14 - INFO - layer19_self_attn.o_proj | mse: 0.00045793352724818857, bpp_loss: 5.690187096595764, bpp: 0
2025-03-02 20:49:19 - INFO - layer19_mlp.gate_proj | mse: 0.0005431295740676449, bpp_loss: 6.166020938328335, bpp: 0
2025-03-02 20:49:24 - INFO - layer19_mlp.up_proj | mse: 0.0004705886110518745, bpp_loss: 5.761355400085449, bpp: 0
2025-03-02 20:49:29 - INFO - layer19_mlp.down_proj | mse: 0.0004695475898442572, bpp_loss: 5.756997721535819, bpp: 0
pseudo compress quantization...:  62%|██████▎   | 20/32 [04:53<03:27, 17.31s/it]2025-03-02 20:49:30 - INFO - layer20_self_attn.q_proj | mse: 0.0006070163790006636, bpp_loss: 6.486273407936096, bpp: 0
2025-03-02 20:49:31 - INFO - layer20_self_attn.k_proj | mse: 0.0008551568114745535, bpp_loss: 7.596030235290527, bpp: 0
2025-03-02 20:49:31 - INFO - layer20_self_attn.v_proj | mse: 0.0004501346886510249, bpp_loss: 5.646947979927063, bpp: 0
2025-03-02 20:49:33 - INFO - layer20_self_attn.o_proj | mse: 0.00045639578610554227, bpp_loss: 5.679326176643372, bpp: 0
2025-03-02 20:49:38 - INFO - layer20_mlp.gate_proj | mse: 0.000543093671432944, bpp_loss: 6.166372162955148, bpp: 0
2025-03-02 20:49:42 - INFO - layer20_mlp.up_proj | mse: 0.00047127976263631853, bpp_loss: 5.7648758207048685, bpp: 0
2025-03-02 20:49:48 - INFO - layer20_mlp.down_proj | mse: 0.0004702531231757428, bpp_loss: 5.7609356471470425, bpp: 0
pseudo compress quantization...:  66%|██████▌   | 21/32 [05:12<03:15, 17.81s/it]2025-03-02 20:49:49 - INFO - layer21_self_attn.q_proj | mse: 0.0006034589465241562, bpp_loss: 6.475778579711914, bpp: 0
2025-03-02 20:49:50 - INFO - layer21_self_attn.k_proj | mse: 0.0008687755369961537, bpp_loss: 7.647090435028076, bpp: 0
2025-03-02 20:49:50 - INFO - layer21_self_attn.v_proj | mse: 0.0004551654936055869, bpp_loss: 5.673931121826172, bpp: 0
2025-03-02 20:49:52 - INFO - layer21_self_attn.o_proj | mse: 0.00046049098210901784, bpp_loss: 5.700215220451355, bpp: 0
2025-03-02 20:49:57 - INFO - layer21_mlp.gate_proj | mse: 0.0005455967162130143, bpp_loss: 6.179300853184292, bpp: 0
2025-03-02 20:50:02 - INFO - layer21_mlp.up_proj | mse: 0.00047177986460988295, bpp_loss: 5.767890793936593, bpp: 0
2025-03-02 20:50:07 - INFO - layer21_mlp.down_proj | mse: 0.0004707893864485233, bpp_loss: 5.763366460800171, bpp: 0
pseudo compress quantization...:  69%|██████▉   | 22/32 [05:31<03:02, 18.23s/it]2025-03-02 20:50:09 - INFO - layer22_self_attn.q_proj | mse: 0.0005932715868609987, bpp_loss: 6.428506135940552, bpp: 0
2025-03-02 20:50:09 - INFO - layer22_self_attn.k_proj | mse: 0.0008513949111654439, bpp_loss: 7.562965035438538, bpp: 0
2025-03-02 20:50:10 - INFO - layer22_self_attn.v_proj | mse: 0.00046284301854515163, bpp_loss: 5.724933624267578, bpp: 0
2025-03-02 20:50:11 - INFO - layer22_self_attn.o_proj | mse: 0.0004633494577993369, bpp_loss: 5.722862005233765, bpp: 0
2025-03-02 20:50:16 - INFO - layer22_mlp.gate_proj | mse: 0.000545993041983106, bpp_loss: 6.1803387233189175, bpp: 0
2025-03-02 20:50:21 - INFO - layer22_mlp.up_proj | mse: 0.0004722961072491918, bpp_loss: 5.7715364183698386, bpp: 0
2025-03-02 20:50:26 - INFO - layer22_mlp.down_proj | mse: 0.00047119543800438416, bpp_loss: 5.767480577741351, bpp: 0
pseudo compress quantization...:  72%|███████▏  | 23/32 [05:50<02:45, 18.44s/it]2025-03-02 20:50:27 - INFO - layer23_self_attn.q_proj | mse: 0.0005933691809825596, bpp_loss: 6.4382665157318115, bpp: 0
2025-03-02 20:50:28 - INFO - layer23_self_attn.k_proj | mse: 0.0008414459237862102, bpp_loss: 7.558411598205566, bpp: 0
2025-03-02 20:50:28 - INFO - layer23_self_attn.v_proj | mse: 0.0004735727588968725, bpp_loss: 5.778478503227234, bpp: 0
2025-03-02 20:50:30 - INFO - layer23_self_attn.o_proj | mse: 0.0004668056748510714, bpp_loss: 5.743096590042114, bpp: 0
2025-03-02 20:50:34 - INFO - layer23_mlp.gate_proj | mse: 0.0005467438416047405, bpp_loss: 6.183022226606097, bpp: 0
2025-03-02 20:50:40 - INFO - layer23_mlp.up_proj | mse: 0.0004730038650955076, bpp_loss: 5.7756354468209405, bpp: 0
2025-03-02 20:50:45 - INFO - layer23_mlp.down_proj | mse: 0.0004720176807650992, bpp_loss: 5.772191796983991, bpp: 0
pseudo compress quantization...:  75%|███████▌  | 24/32 [06:09<02:29, 18.63s/it]2025-03-02 20:50:46 - INFO - layer24_self_attn.q_proj | mse: 0.0005861253071026208, bpp_loss: 6.402236819267273, bpp: 0
2025-03-02 20:50:47 - INFO - layer24_self_attn.k_proj | mse: 0.0008030758624089641, bpp_loss: 7.352193593978882, bpp: 0
2025-03-02 20:50:47 - INFO - layer24_self_attn.v_proj | mse: 0.0004906279201014123, bpp_loss: 5.880210638046265, bpp: 0
2025-03-02 20:50:49 - INFO - layer24_self_attn.o_proj | mse: 0.0004750366254539959, bpp_loss: 5.788207530975342, bpp: 0
2025-03-02 20:50:53 - INFO - layer24_mlp.gate_proj | mse: 0.0005473514962836986, bpp_loss: 6.190090724400112, bpp: 0
2025-03-02 20:50:58 - INFO - layer24_mlp.up_proj | mse: 0.0004738864801540555, bpp_loss: 5.78037657056536, bpp: 0
2025-03-02 20:51:03 - INFO - layer24_mlp.down_proj | mse: 0.0004729622201085707, bpp_loss: 5.777523687907627, bpp: 0
pseudo compress quantization...:  78%|███████▊  | 25/32 [06:28<02:10, 18.59s/it]2025-03-02 20:51:05 - INFO - layer25_self_attn.q_proj | mse: 0.0005810151503414279, bpp_loss: 6.374264597892761, bpp: 0
2025-03-02 20:51:05 - INFO - layer25_self_attn.k_proj | mse: 0.00079528598653725, bpp_loss: 7.3319785594940186, bpp: 0
2025-03-02 20:51:06 - INFO - layer25_self_attn.v_proj | mse: 0.0004928011546914582, bpp_loss: 5.890134692192078, bpp: 0
2025-03-02 20:51:07 - INFO - layer25_self_attn.o_proj | mse: 0.0004762019608275877, bpp_loss: 5.7909393310546875, bpp: 0
2025-03-02 20:51:12 - INFO - layer25_mlp.gate_proj | mse: 0.0005491423859206959, bpp_loss: 6.201796804155622, bpp: 0
2025-03-02 20:51:17 - INFO - layer25_mlp.up_proj | mse: 0.000475567747769538, bpp_loss: 5.7897330692836215, bpp: 0
2025-03-02 20:51:22 - INFO - layer25_mlp.down_proj | mse: 0.00047456635396563956, bpp_loss: 5.786712510245187, bpp: 0
pseudo compress quantization...:  81%|████████▏ | 26/32 [06:46<01:51, 18.51s/it]2025-03-02 20:51:23 - INFO - layer26_self_attn.q_proj | mse: 0.0005838122449423051, bpp_loss: 6.385236978530884, bpp: 0
2025-03-02 20:51:24 - INFO - layer26_self_attn.k_proj | mse: 0.000824726366856834, bpp_loss: 7.434001684188843, bpp: 0
2025-03-02 20:51:24 - INFO - layer26_self_attn.v_proj | mse: 0.000505723005676217, bpp_loss: 5.950302720069885, bpp: 0
2025-03-02 20:51:26 - INFO - layer26_self_attn.o_proj | mse: 0.0004804504284895072, bpp_loss: 5.8106794357299805, bpp: 0
2025-03-02 20:51:30 - INFO - layer26_mlp.gate_proj | mse: 0.0005508016153194888, bpp_loss: 6.212818554469517, bpp: 0
2025-03-02 20:51:35 - INFO - layer26_mlp.up_proj | mse: 0.000476997334811793, bpp_loss: 5.798839841570173, bpp: 0
2025-03-02 20:51:40 - INFO - layer26_mlp.down_proj | mse: 0.0004762497943630493, bpp_loss: 5.795280660901751, bpp: 0
pseudo compress quantization...:  84%|████████▍ | 27/32 [07:05<01:32, 18.57s/it]2025-03-02 20:51:42 - INFO - layer27_self_attn.q_proj | mse: 0.0005767329592933475, bpp_loss: 6.351723551750183, bpp: 0
2025-03-02 20:51:42 - INFO - layer27_self_attn.k_proj | mse: 0.0008164939677326247, bpp_loss: 7.38261091709137, bpp: 0
2025-03-02 20:51:43 - INFO - layer27_self_attn.v_proj | mse: 0.000521284696169876, bpp_loss: 6.046483516693115, bpp: 0
2025-03-02 20:51:44 - INFO - layer27_self_attn.o_proj | mse: 0.00048517138568800037, bpp_loss: 5.846941232681274, bpp: 0
2025-03-02 20:51:49 - INFO - layer27_mlp.gate_proj | mse: 0.0005532281471359818, bpp_loss: 6.227093015398298, bpp: 0
2025-03-02 20:51:54 - INFO - layer27_mlp.up_proj | mse: 0.0004793175555125651, bpp_loss: 5.813609395708356, bpp: 0
2025-03-02 20:51:59 - INFO - layer27_mlp.down_proj | mse: 0.00047838574246323825, bpp_loss: 5.807011842727661, bpp: 0
pseudo compress quantization...:  88%|████████▊ | 28/32 [07:23<01:14, 18.65s/it]2025-03-02 20:52:01 - INFO - layer28_self_attn.q_proj | mse: 0.0005781724725863708, bpp_loss: 6.359267711639404, bpp: 0
2025-03-02 20:52:01 - INFO - layer28_self_attn.k_proj | mse: 0.0007761104617695581, bpp_loss: 7.292735934257507, bpp: 0
2025-03-02 20:52:02 - INFO - layer28_self_attn.v_proj | mse: 0.0005333363027265232, bpp_loss: 6.106019735336304, bpp: 0
2025-03-02 20:52:03 - INFO - layer28_self_attn.o_proj | mse: 0.0004897561818857258, bpp_loss: 5.877429723739624, bpp: 0
2025-03-02 20:52:08 - INFO - layer28_mlp.gate_proj | mse: 0.0005514987161877809, bpp_loss: 6.215586934770856, bpp: 0
2025-03-02 20:52:13 - INFO - layer28_mlp.up_proj | mse: 0.00048350243697005125, bpp_loss: 5.83702768598284, bpp: 0
2025-03-02 20:52:18 - INFO - layer28_mlp.down_proj | mse: 0.00048181946216876883, bpp_loss: 5.824474505015782, bpp: 0
pseudo compress quantization...:  91%|█████████ | 29/32 [07:42<00:55, 18.59s/it]2025-03-02 20:52:19 - INFO - layer29_self_attn.q_proj | mse: 0.0005759300711049775, bpp_loss: 6.345190763473511, bpp: 0
2025-03-02 20:52:20 - INFO - layer29_self_attn.k_proj | mse: 0.00081396828980748, bpp_loss: 7.419207692146301, bpp: 0
2025-03-02 20:52:20 - INFO - layer29_self_attn.v_proj | mse: 0.0005466383452822423, bpp_loss: 6.175405144691467, bpp: 0
2025-03-02 20:52:22 - INFO - layer29_self_attn.o_proj | mse: 0.0004985019460065131, bpp_loss: 5.917465806007385, bpp: 0
2025-03-02 20:52:26 - INFO - layer29_mlp.gate_proj | mse: 0.0005510586761146312, bpp_loss: 6.210243361336844, bpp: 0
2025-03-02 20:52:31 - INFO - layer29_mlp.up_proj | mse: 0.0004891659482618622, bpp_loss: 5.870229312351772, bpp: 0
2025-03-02 20:52:37 - INFO - layer29_mlp.down_proj | mse: 0.0004860181563113633, bpp_loss: 5.842300278799875, bpp: 0
pseudo compress quantization...:  94%|█████████▍| 30/32 [08:01<00:37, 18.69s/it]2025-03-02 20:52:38 - INFO - layer30_self_attn.q_proj | mse: 0.0005580756882815698, bpp_loss: 6.245165944099426, bpp: 0
2025-03-02 20:52:38 - INFO - layer30_self_attn.k_proj | mse: 0.0007273868229117281, bpp_loss: 6.985673546791077, bpp: 0
2025-03-02 20:52:39 - INFO - layer30_self_attn.v_proj | mse: 0.0006121877810524279, bpp_loss: 6.506207823753357, bpp: 0
2025-03-02 20:52:40 - INFO - layer30_self_attn.o_proj | mse: 0.0005141024749771198, bpp_loss: 6.007678031921387, bpp: 0
2025-03-02 20:52:45 - INFO - layer30_mlp.gate_proj | mse: 0.0005624264124929355, bpp_loss: 6.27025386265346, bpp: 0
2025-03-02 20:52:50 - INFO - layer30_mlp.up_proj | mse: 0.00049432971971043, bpp_loss: 5.902088574000767, bpp: 0
2025-03-02 20:52:55 - INFO - layer30_mlp.down_proj | mse: 0.00048765845146764823, bpp_loss: 5.8461315631866455, bpp: 0
pseudo compress quantization...:  97%|█████████▋| 31/32 [08:19<00:18, 18.68s/it]2025-03-02 20:52:57 - INFO - layer31_self_attn.q_proj | mse: 0.0005947488975062964, bpp_loss: 6.419811129570007, bpp: 0
2025-03-02 20:52:57 - INFO - layer31_self_attn.k_proj | mse: 0.0007881286618040757, bpp_loss: 7.2771360874176025, bpp: 0
2025-03-02 20:52:58 - INFO - layer31_self_attn.v_proj | mse: 0.000565301410379159, bpp_loss: 6.2827324867248535, bpp: 0
2025-03-02 20:52:59 - INFO - layer31_self_attn.o_proj | mse: 0.0005116253677813047, bpp_loss: 5.979685425758362, bpp: 0
2025-03-02 20:53:04 - INFO - layer31_mlp.gate_proj | mse: 0.0006122972931415236, bpp_loss: 6.522943224225726, bpp: 0
2025-03-02 20:53:09 - INFO - layer31_mlp.up_proj | mse: 0.0005311388700710864, bpp_loss: 6.109066826956613, bpp: 0
2025-03-02 20:53:14 - INFO - layer31_mlp.down_proj | mse: 0.0004941064922338674, bpp_loss: 5.87838796206883, bpp: 0
pseudo compress quantization...: 100%|██████████| 32/32 [08:38<00:00, 18.62s/it]pseudo compress quantization...: 100%|██████████| 32/32 [08:38<00:00, 16.20s/it]
2025-03-02 20:53:14 - INFO - #### Total | mse: 0.0005064660053657749, bpp_loss: 5.952133220548813, bpp: 0 ####
## Strart saving /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda10000_rdloss_ql_encdim512_M16_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_10.96029_bpp_6.2788_MSE_0.0004_total_iter_140000.pth.tar/COL_MSE0.00051_bpploss5.9521_bpp0
## End saving
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda10000_rdloss_ql_encdim512_M16_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_10.96029_bpp_6.2788_MSE_0.0004_total_iter_140000.pth.tar/COL_MSE0.00051_bpploss5.9521_bpp0
I0302 20:54:00.216214 2576431 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]
W0302 20:54:03.597922 2576431 big_modeling.py:414] Some parameters are on the meta device device because they were offloaded to the cpu.
I0302 20:54:03.611886 2576431 config.py:54] PyTorch version 2.4.1 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.5234375:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.5234375:   1%|          | 1/141 [00:01<02:59,  1.28s/it]avg_loss = 1.5234375:   1%|          | 1/141 [00:02<04:46,  2.05s/it]
Traceback (most recent call last):
  File "/home/jgryu/Weight_compression/comp_llm/eval_ppl.py", line 115, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/comp_llm/eval_ppl.py", line 91, in main
    output = model(input,
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1179, in forward
    logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/hooks.py", line 160, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/hooks.py", line 293, in pre_forward
    set_module_tensor_to_device(
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 347, in set_module_tensor_to_device
    new_value = value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 934.19 MiB is free. Process 2571270 has 7.63 GiB memory in use. Including non-PyTorch memory, this process has 15.02 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running with lmbda=100000
/home/jgryu/Weight_compression/comp_llm/matmul_had.py:96: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")
/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  5.30it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  6.42it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  6.63it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  6.46it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  6.71it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.31it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.83it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.08it/s]
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:12,  2.48it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:12,  2.32it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:14,  1.94it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.84it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:15,  1.73it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:14,  1.74it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.80it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.89it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:11,  1.94it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.99it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:11,  1.82it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:13,  1.50it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:13,  1.37it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:08<00:14,  1.28it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:09<00:11,  1.45it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:09<00:09,  1.61it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:08,  1.76it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:10<00:07,  1.78it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.88it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:11<00:06,  1.90it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.94it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:12<00:05,  1.93it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:13<00:04,  1.82it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:13<00:04,  1.84it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:14<00:03,  1.89it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:14<00:03,  1.95it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:15<00:02,  2.02it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:15<00:01,  2.06it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  2.15it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:16<00:01,  1.97it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:17<00:00,  1.77it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:18<00:00,  1.49it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:18<00:00,  1.76it/s]
pseudo compress quantization...:   0%|          | 0/32 [00:00<?, ?it/s]2025-03-02 20:55:17 - INFO - layer0_self_attn.q_proj | mse: 0.0005704047771968295, bpp_loss: 6.537895321846008, bpp: 0
2025-03-02 20:55:18 - INFO - layer0_self_attn.k_proj | mse: 0.0009835221422854164, bpp_loss: 7.331218004226685, bpp: 0
2025-03-02 20:55:18 - INFO - layer0_self_attn.v_proj | mse: 0.0003357223296482213, bpp_loss: 5.650698304176331, bpp: 0
2025-03-02 20:55:20 - INFO - layer0_self_attn.o_proj | mse: 0.00035003519109610894, bpp_loss: 5.7248536348342896, bpp: 0
2025-03-02 20:55:25 - INFO - layer0_mlp.gate_proj | mse: 0.00042793028001044233, bpp_loss: 6.115706716264997, bpp: 0
2025-03-02 20:55:30 - INFO - layer0_mlp.up_proj | mse: 0.00040613006054121163, bpp_loss: 6.006252697535923, bpp: 0
2025-03-02 20:55:36 - INFO - layer0_mlp.down_proj | mse: 0.00040594806418836176, bpp_loss: 6.002389669418335, bpp: 0
pseudo compress quantization...:   3%|▎         | 1/32 [00:19<10:09, 19.67s/it]2025-03-02 20:55:37 - INFO - layer1_self_attn.q_proj | mse: 0.0005968408791055251, bpp_loss: 6.865119099617004, bpp: 0
2025-03-02 20:55:37 - INFO - layer1_self_attn.k_proj | mse: 0.0008813275171241063, bpp_loss: 7.901660084724426, bpp: 0
2025-03-02 20:55:38 - INFO - layer1_self_attn.v_proj | mse: 0.00035569370369616495, bpp_loss: 5.7515647411346436, bpp: 0
2025-03-02 20:55:39 - INFO - layer1_self_attn.o_proj | mse: 0.00036672919520469615, bpp_loss: 5.808393359184265, bpp: 0
2025-03-02 20:55:44 - INFO - layer1_mlp.gate_proj | mse: 0.0004314446771782439, bpp_loss: 6.13105228969029, bpp: 0
2025-03-02 20:55:49 - INFO - layer1_mlp.up_proj | mse: 0.0004095517782132398, bpp_loss: 6.021517208644322, bpp: 0
2025-03-02 20:55:54 - INFO - layer1_mlp.down_proj | mse: 0.0004089322766994886, bpp_loss: 6.01781347819737, bpp: 0
pseudo compress quantization...:   6%|▋         | 2/32 [00:38<09:30, 19.01s/it]2025-03-02 20:55:55 - INFO - layer2_self_attn.q_proj | mse: 0.0005727102346204002, bpp_loss: 6.760465145111084, bpp: 0
2025-03-02 20:55:56 - INFO - layer2_self_attn.k_proj | mse: 0.0009158940332053058, bpp_loss: 7.9436376094818115, bpp: 0
2025-03-02 20:55:56 - INFO - layer2_self_attn.v_proj | mse: 0.0003370014871083923, bpp_loss: 5.658459782600403, bpp: 0
2025-03-02 20:55:58 - INFO - layer2_self_attn.o_proj | mse: 0.0003555260373574536, bpp_loss: 5.7535353899002075, bpp: 0
2025-03-02 20:56:03 - INFO - layer2_mlp.gate_proj | mse: 0.00043850650297242984, bpp_loss: 6.166543006896973, bpp: 0
2025-03-02 20:56:08 - INFO - layer2_mlp.up_proj | mse: 0.00040843121766721813, bpp_loss: 6.016619682312012, bpp: 0
2025-03-02 20:56:13 - INFO - layer2_mlp.down_proj | mse: 0.0004091221364535986, bpp_loss: 6.0198544434138705, bpp: 0
pseudo compress quantization...:   9%|▉         | 3/32 [00:57<09:09, 18.94s/it]2025-03-02 20:56:14 - INFO - layer3_self_attn.q_proj | mse: 0.0005758333370429294, bpp_loss: 6.7907490730285645, bpp: 0
2025-03-02 20:56:15 - INFO - layer3_self_attn.k_proj | mse: 0.0009020937898602087, bpp_loss: 8.01991331577301, bpp: 0
2025-03-02 20:56:15 - INFO - layer3_self_attn.v_proj | mse: 0.0003554472061131594, bpp_loss: 5.75187361240387, bpp: 0
2025-03-02 20:56:17 - INFO - layer3_self_attn.o_proj | mse: 0.0003748050861307158, bpp_loss: 5.850863575935364, bpp: 0
2025-03-02 20:56:22 - INFO - layer3_mlp.gate_proj | mse: 0.00045541271221592836, bpp_loss: 6.24775505065918, bpp: 0
2025-03-02 20:56:27 - INFO - layer3_mlp.up_proj | mse: 0.00040456369170335256, bpp_loss: 5.9969940185546875, bpp: 0
2025-03-02 20:56:31 - INFO - layer3_mlp.down_proj | mse: 0.00040406296893495187, bpp_loss: 5.994899238858904, bpp: 0
pseudo compress quantization...:  12%|█▎        | 4/32 [01:15<08:42, 18.65s/it]2025-03-02 20:56:32 - INFO - layer4_self_attn.q_proj | mse: 0.0005673823922698815, bpp_loss: 6.754468083381653, bpp: 0
2025-03-02 20:56:33 - INFO - layer4_self_attn.k_proj | mse: 0.0008912222754877417, bpp_loss: 7.966318249702454, bpp: 0
2025-03-02 20:56:33 - INFO - layer4_self_attn.v_proj | mse: 0.00036536656670549064, bpp_loss: 5.800578474998474, bpp: 0
2025-03-02 20:56:34 - INFO - layer4_self_attn.o_proj | mse: 0.00037849856644226544, bpp_loss: 5.8668540716171265, bpp: 0
2025-03-02 20:56:37 - INFO - layer4_mlp.gate_proj | mse: 0.0004717148880836807, bpp_loss: 6.327863829476493, bpp: 0
2025-03-02 20:56:39 - INFO - layer4_mlp.up_proj | mse: 0.000399222657462562, bpp_loss: 5.970759800502232, bpp: 0
2025-03-02 20:56:42 - INFO - layer4_mlp.down_proj | mse: 0.0003988323327092214, bpp_loss: 5.96898753302438, bpp: 0
pseudo compress quantization...:  16%|█▌        | 5/32 [01:26<07:09, 15.92s/it]2025-03-02 20:56:43 - INFO - layer5_self_attn.q_proj | mse: 0.0005644241044097842, bpp_loss: 6.731091499328613, bpp: 0
2025-03-02 20:56:43 - INFO - layer5_self_attn.k_proj | mse: 0.0009078416128507283, bpp_loss: 7.934002041816711, bpp: 0
2025-03-02 20:56:44 - INFO - layer5_self_attn.v_proj | mse: 0.00034248273691622543, bpp_loss: 5.68835973739624, bpp: 0
2025-03-02 20:56:44 - INFO - layer5_self_attn.o_proj | mse: 0.0003661936723590903, bpp_loss: 5.807414174079895, bpp: 0
2025-03-02 20:56:47 - INFO - layer5_mlp.gate_proj | mse: 0.00047199305590325824, bpp_loss: 6.328234127589634, bpp: 0
2025-03-02 20:56:49 - INFO - layer5_mlp.up_proj | mse: 0.00040041799791990955, bpp_loss: 5.976695878165109, bpp: 0
2025-03-02 20:56:52 - INFO - layer5_mlp.down_proj | mse: 0.00039973708945026496, bpp_loss: 5.973683765956333, bpp: 0
pseudo compress quantization...:  19%|█▉        | 6/32 [01:36<06:00, 13.85s/it]2025-03-02 20:56:53 - INFO - layer6_self_attn.q_proj | mse: 0.0005758202098187863, bpp_loss: 6.78472626209259, bpp: 0
2025-03-02 20:56:53 - INFO - layer6_self_attn.k_proj | mse: 0.0008857838928566796, bpp_loss: 7.9927356243133545, bpp: 0
2025-03-02 20:56:53 - INFO - layer6_self_attn.v_proj | mse: 0.00035224730942939273, bpp_loss: 5.7351155281066895, bpp: 0
2025-03-02 20:56:54 - INFO - layer6_self_attn.o_proj | mse: 0.00037304834667632936, bpp_loss: 5.840500473976135, bpp: 0
2025-03-02 20:56:57 - INFO - layer6_mlp.gate_proj | mse: 0.00047296796246080677, bpp_loss: 6.335976464407785, bpp: 0
2025-03-02 20:56:59 - INFO - layer6_mlp.up_proj | mse: 0.0004003719039033123, bpp_loss: 5.976438386099679, bpp: 0
2025-03-02 20:57:02 - INFO - layer6_mlp.down_proj | mse: 0.00039955211845549103, bpp_loss: 5.9729602336883545, bpp: 0
pseudo compress quantization...:  22%|██▏       | 7/32 [01:45<05:12, 12.52s/it]2025-03-02 20:57:03 - INFO - layer7_self_attn.q_proj | mse: 0.0005523868020008239, bpp_loss: 6.691587448120117, bpp: 0
2025-03-02 20:57:03 - INFO - layer7_self_attn.k_proj | mse: 0.000890101391611076, bpp_loss: 8.029474973678589, bpp: 0
2025-03-02 20:57:03 - INFO - layer7_self_attn.v_proj | mse: 0.00035122528713041215, bpp_loss: 5.730509400367737, bpp: 0
2025-03-02 20:57:04 - INFO - layer7_self_attn.o_proj | mse: 0.0003753733138709542, bpp_loss: 5.853430390357971, bpp: 0
2025-03-02 20:57:07 - INFO - layer7_mlp.gate_proj | mse: 0.00046756166716284723, bpp_loss: 6.30665602002825, bpp: 0
2025-03-02 20:57:09 - INFO - layer7_mlp.up_proj | mse: 0.0004030581866259052, bpp_loss: 5.990350042070661, bpp: 0
2025-03-02 20:57:12 - INFO - layer7_mlp.down_proj | mse: 0.00040225503330640713, bpp_loss: 5.987076112202236, bpp: 0
pseudo compress quantization...:  25%|██▌       | 8/32 [01:55<04:41, 11.72s/it]2025-03-02 20:57:13 - INFO - layer8_self_attn.q_proj | mse: 0.0005583837706523229, bpp_loss: 6.682127833366394, bpp: 0
2025-03-02 20:57:13 - INFO - layer8_self_attn.k_proj | mse: 0.0008968504144071078, bpp_loss: 7.927953124046326, bpp: 0
2025-03-02 20:57:13 - INFO - layer8_self_attn.v_proj | mse: 0.0003568945052080883, bpp_loss: 5.756963491439819, bpp: 0
2025-03-02 20:57:14 - INFO - layer8_self_attn.o_proj | mse: 0.00037823871530366605, bpp_loss: 5.8671547174453735, bpp: 0
2025-03-02 20:57:17 - INFO - layer8_mlp.gate_proj | mse: 0.0004692524018995837, bpp_loss: 6.314070292881557, bpp: 0
2025-03-02 20:57:19 - INFO - layer8_mlp.up_proj | mse: 0.0004023572833892405, bpp_loss: 5.985742160252163, bpp: 0
2025-03-02 20:57:22 - INFO - layer8_mlp.down_proj | mse: 0.0004015250561782411, bpp_loss: 5.983890669686454, bpp: 0
pseudo compress quantization...:  28%|██▊       | 9/32 [02:06<04:17, 11.19s/it]2025-03-02 20:57:23 - INFO - layer9_self_attn.q_proj | mse: 0.0005552833466411582, bpp_loss: 6.690201163291931, bpp: 0
2025-03-02 20:57:23 - INFO - layer9_self_attn.k_proj | mse: 0.0008827328543680673, bpp_loss: 7.949598670005798, bpp: 0
2025-03-02 20:57:23 - INFO - layer9_self_attn.v_proj | mse: 0.0003775771711062858, bpp_loss: 5.859814286231995, bpp: 0
2025-03-02 20:57:24 - INFO - layer9_self_attn.o_proj | mse: 0.0003900205121378812, bpp_loss: 5.924340605735779, bpp: 0
2025-03-02 20:57:27 - INFO - layer9_mlp.gate_proj | mse: 0.00047286765236183694, bpp_loss: 6.331881250653948, bpp: 0
2025-03-02 20:57:29 - INFO - layer9_mlp.up_proj | mse: 0.0004040920686929047, bpp_loss: 5.9950874873570035, bpp: 0
2025-03-02 20:57:32 - INFO - layer9_mlp.down_proj | mse: 0.00040227919257212086, bpp_loss: 5.987189599445888, bpp: 0
pseudo compress quantization...:  31%|███▏      | 10/32 [02:15<03:57, 10.81s/it]2025-03-02 20:57:33 - INFO - layer10_self_attn.q_proj | mse: 0.0005639552814427127, bpp_loss: 6.710004806518555, bpp: 0
2025-03-02 20:57:33 - INFO - layer10_self_attn.k_proj | mse: 0.0009048412726070828, bpp_loss: 7.964692831039429, bpp: 0
2025-03-02 20:57:33 - INFO - layer10_self_attn.v_proj | mse: 0.00035520206651885834, bpp_loss: 5.7483508586883545, bpp: 0
2025-03-02 20:57:34 - INFO - layer10_self_attn.o_proj | mse: 0.00037655231966417094, bpp_loss: 5.857797384262085, bpp: 0
2025-03-02 20:57:37 - INFO - layer10_mlp.gate_proj | mse: 0.0004651054406600165, bpp_loss: 6.295466831752232, bpp: 0
2025-03-02 20:57:39 - INFO - layer10_mlp.up_proj | mse: 0.0004075559745473516, bpp_loss: 6.011620657784598, bpp: 0
2025-03-02 20:57:43 - INFO - layer10_mlp.down_proj | mse: 0.0004052366591723826, bpp_loss: 6.002208369118827, bpp: 0
pseudo compress quantization...:  34%|███▍      | 11/32 [02:26<03:46, 10.80s/it]2025-03-02 20:57:44 - INFO - layer11_self_attn.q_proj | mse: 0.0005380797120049303, bpp_loss: 6.622583031654358, bpp: 0
2025-03-02 20:57:44 - INFO - layer11_self_attn.k_proj | mse: 0.0008797250904528373, bpp_loss: 7.949828028678894, bpp: 0
2025-03-02 20:57:44 - INFO - layer11_self_attn.v_proj | mse: 0.000354082178500576, bpp_loss: 5.7465115785598755, bpp: 0
2025-03-02 20:57:45 - INFO - layer11_self_attn.o_proj | mse: 0.0003800304682334826, bpp_loss: 5.876406192779541, bpp: 0
2025-03-02 20:57:48 - INFO - layer11_mlp.gate_proj | mse: 0.0004611991408853321, bpp_loss: 6.275691168648856, bpp: 0
2025-03-02 20:57:50 - INFO - layer11_mlp.up_proj | mse: 0.00040890607398410525, bpp_loss: 6.01846626826695, bpp: 0
2025-03-02 20:57:53 - INFO - layer11_mlp.down_proj | mse: 0.00040671927244024045, bpp_loss: 6.00986065183367, bpp: 0
pseudo compress quantization...:  38%|███▊      | 12/32 [02:37<03:34, 10.70s/it]2025-03-02 20:57:54 - INFO - layer12_self_attn.q_proj | mse: 0.000547004459036966, bpp_loss: 6.672748327255249, bpp: 0
2025-03-02 20:57:54 - INFO - layer12_self_attn.k_proj | mse: 0.0008502332876276443, bpp_loss: 7.909248471260071, bpp: 0
2025-03-02 20:57:55 - INFO - layer12_self_attn.v_proj | mse: 0.0003783353588331338, bpp_loss: 5.863956332206726, bpp: 0
2025-03-02 20:57:56 - INFO - layer12_self_attn.o_proj | mse: 0.00039029220041135454, bpp_loss: 5.925922632217407, bpp: 0
2025-03-02 20:57:58 - INFO - layer12_mlp.gate_proj | mse: 0.00045773593342507597, bpp_loss: 6.257945333208356, bpp: 0
2025-03-02 20:58:01 - INFO - layer12_mlp.up_proj | mse: 0.0004129700556472168, bpp_loss: 6.039080211094448, bpp: 0
2025-03-02 20:58:04 - INFO - layer12_mlp.down_proj | mse: 0.00040995027676525873, bpp_loss: 6.025431530816214, bpp: 0
pseudo compress quantization...:  41%|████      | 13/32 [02:47<03:23, 10.70s/it]2025-03-02 20:58:05 - INFO - layer13_self_attn.q_proj | mse: 0.000547690831794493, bpp_loss: 6.662181496620178, bpp: 0
2025-03-02 20:58:05 - INFO - layer13_self_attn.k_proj | mse: 0.0008948693804850338, bpp_loss: 7.976004004478455, bpp: 0
2025-03-02 20:58:06 - INFO - layer13_self_attn.v_proj | mse: 0.0003657946500114282, bpp_loss: 5.80441677570343, bpp: 0
2025-03-02 20:58:07 - INFO - layer13_self_attn.o_proj | mse: 0.0003857505882544311, bpp_loss: 5.903615474700928, bpp: 0
2025-03-02 20:58:10 - INFO - layer13_mlp.gate_proj | mse: 0.00045837261026263233, bpp_loss: 6.26077447618757, bpp: 0
2025-03-02 20:58:12 - INFO - layer13_mlp.up_proj | mse: 0.0004137779425273418, bpp_loss: 6.04244191305978, bpp: 0
2025-03-02 20:58:15 - INFO - layer13_mlp.down_proj | mse: 0.0004099013633054465, bpp_loss: 6.025763920375279, bpp: 0
pseudo compress quantization...:  44%|████▍     | 14/32 [02:59<03:16, 10.93s/it]2025-03-02 20:58:16 - INFO - layer14_self_attn.q_proj | mse: 0.0005424715832740414, bpp_loss: 6.625025629997253, bpp: 0
2025-03-02 20:58:17 - INFO - layer14_self_attn.k_proj | mse: 0.000899219289903177, bpp_loss: 7.9119555950164795, bpp: 0
2025-03-02 20:58:18 - INFO - layer14_self_attn.v_proj | mse: 0.00036414770394379783, bpp_loss: 5.795316219329834, bpp: 0
2025-03-02 20:58:19 - INFO - layer14_self_attn.o_proj | mse: 0.000384917869580853, bpp_loss: 5.899123191833496, bpp: 0
2025-03-02 20:58:22 - INFO - layer14_mlp.gate_proj | mse: 0.00046490797616916154, bpp_loss: 6.296719414847238, bpp: 0
2025-03-02 20:58:25 - INFO - layer14_mlp.up_proj | mse: 0.0004129455342511818, bpp_loss: 6.037975038800921, bpp: 0
2025-03-02 20:58:29 - INFO - layer14_mlp.down_proj | mse: 0.00040956481490480267, bpp_loss: 6.023240872791836, bpp: 0
pseudo compress quantization...:  47%|████▋     | 15/32 [03:12<03:18, 11.70s/it]2025-03-02 20:58:30 - INFO - layer15_self_attn.q_proj | mse: 0.000587027102134966, bpp_loss: 6.805227518081665, bpp: 0
2025-03-02 20:58:31 - INFO - layer15_self_attn.k_proj | mse: 0.0008691851969220007, bpp_loss: 7.931321024894714, bpp: 0
2025-03-02 20:58:31 - INFO - layer15_self_attn.v_proj | mse: 0.0003763222990107796, bpp_loss: 5.856878161430359, bpp: 0
2025-03-02 20:58:33 - INFO - layer15_self_attn.o_proj | mse: 0.0003907029919944553, bpp_loss: 5.928553223609924, bpp: 0
2025-03-02 20:58:38 - INFO - layer15_mlp.gate_proj | mse: 0.0004733489235586994, bpp_loss: 6.335273197719029, bpp: 0
2025-03-02 20:58:43 - INFO - layer15_mlp.up_proj | mse: 0.0004111393316748885, bpp_loss: 6.029209954398019, bpp: 0
2025-03-02 20:58:48 - INFO - layer15_mlp.down_proj | mse: 0.0004084953406005204, bpp_loss: 6.017545870372227, bpp: 0
pseudo compress quantization...:  50%|█████     | 16/32 [03:31<03:41, 13.84s/it]2025-03-02 20:58:49 - INFO - layer16_self_attn.q_proj | mse: 0.0005720831607029683, bpp_loss: 6.7577197551727295, bpp: 0
2025-03-02 20:58:50 - INFO - layer16_self_attn.k_proj | mse: 0.0008848360167409627, bpp_loss: 7.9360997676849365, bpp: 0
2025-03-02 20:58:50 - INFO - layer16_self_attn.v_proj | mse: 0.0003667187160585267, bpp_loss: 5.8100340366363525, bpp: 0
2025-03-02 20:58:52 - INFO - layer16_self_attn.o_proj | mse: 0.0003868831508339551, bpp_loss: 5.908999443054199, bpp: 0
2025-03-02 20:58:57 - INFO - layer16_mlp.gate_proj | mse: 0.00048078423504629053, bpp_loss: 6.372546332223075, bpp: 0
2025-03-02 20:59:02 - INFO - layer16_mlp.up_proj | mse: 0.00040824376349309885, bpp_loss: 6.014631952558245, bpp: 0
2025-03-02 20:59:07 - INFO - layer16_mlp.down_proj | mse: 0.00040570820458347593, bpp_loss: 6.003577675138201, bpp: 0
pseudo compress quantization...:  53%|█████▎    | 17/32 [03:50<03:51, 15.41s/it]2025-03-02 20:59:08 - INFO - layer17_self_attn.q_proj | mse: 0.0005732318833480755, bpp_loss: 6.777130842208862, bpp: 0
2025-03-02 20:59:09 - INFO - layer17_self_attn.k_proj | mse: 0.0008841164637927568, bpp_loss: 7.9673497676849365, bpp: 0
2025-03-02 20:59:09 - INFO - layer17_self_attn.v_proj | mse: 0.00038232628393725153, bpp_loss: 5.88566792011261, bpp: 0
2025-03-02 20:59:11 - INFO - layer17_self_attn.o_proj | mse: 0.00039299305887937534, bpp_loss: 5.9387383460998535, bpp: 0
2025-03-02 20:59:16 - INFO - layer17_mlp.gate_proj | mse: 0.0004842578925529581, bpp_loss: 6.389199393136161, bpp: 0
2025-03-02 20:59:21 - INFO - layer17_mlp.up_proj | mse: 0.0004073473357494428, bpp_loss: 6.010408673967634, bpp: 0
2025-03-02 20:59:26 - INFO - layer17_mlp.down_proj | mse: 0.0004054066276252786, bpp_loss: 6.001666852406093, bpp: 0
pseudo compress quantization...:  56%|█████▋    | 18/32 [04:09<03:51, 16.52s/it]2025-03-02 20:59:27 - INFO - layer18_self_attn.q_proj | mse: 0.0005691563329145245, bpp_loss: 6.760573863983154, bpp: 0
2025-03-02 20:59:28 - INFO - layer18_self_attn.k_proj | mse: 0.0009142075922995206, bpp_loss: 8.075459957122803, bpp: 0
2025-03-02 20:59:28 - INFO - layer18_self_attn.v_proj | mse: 0.00036526523811499244, bpp_loss: 5.804106950759888, bpp: 0
2025-03-02 20:59:30 - INFO - layer18_self_attn.o_proj | mse: 0.000388394909126056, bpp_loss: 5.917173743247986, bpp: 0
2025-03-02 20:59:35 - INFO - layer18_mlp.gate_proj | mse: 0.00048533000523529485, bpp_loss: 6.393498693193708, bpp: 0
2025-03-02 20:59:40 - INFO - layer18_mlp.up_proj | mse: 0.00040610546154582215, bpp_loss: 6.005176544189453, bpp: 0
2025-03-02 20:59:45 - INFO - layer18_mlp.down_proj | mse: 0.00040480618968629284, bpp_loss: 5.99891471862793, bpp: 0
pseudo compress quantization...:  59%|█████▉    | 19/32 [04:28<03:43, 17.21s/it]2025-03-02 20:59:46 - INFO - layer19_self_attn.q_proj | mse: 0.000569163484988368, bpp_loss: 6.761992692947388, bpp: 0
2025-03-02 20:59:47 - INFO - layer19_self_attn.k_proj | mse: 0.0008702023304616452, bpp_loss: 7.9274057149887085, bpp: 0
2025-03-02 20:59:47 - INFO - layer19_self_attn.v_proj | mse: 0.00037367711807964187, bpp_loss: 5.846960067749023, bpp: 0
2025-03-02 20:59:49 - INFO - layer19_self_attn.o_proj | mse: 0.0003910321684564177, bpp_loss: 5.930129766464233, bpp: 0
2025-03-02 20:59:54 - INFO - layer19_mlp.gate_proj | mse: 0.0004870604191086532, bpp_loss: 6.404379980904715, bpp: 0
2025-03-02 20:59:58 - INFO - layer19_mlp.up_proj | mse: 0.0004054339684645285, bpp_loss: 6.001043047223773, bpp: 0
2025-03-02 21:00:03 - INFO - layer19_mlp.down_proj | mse: 0.0004043167446907521, bpp_loss: 5.996616704123361, bpp: 0
pseudo compress quantization...:  62%|██████▎   | 20/32 [04:47<03:31, 17.61s/it]2025-03-02 21:00:05 - INFO - layer20_self_attn.q_proj | mse: 0.0005624722219785662, bpp_loss: 6.730763077735901, bpp: 0
2025-03-02 21:00:05 - INFO - layer20_self_attn.k_proj | mse: 0.0008565023972155179, bpp_loss: 7.858149886131287, bpp: 0
2025-03-02 21:00:06 - INFO - layer20_self_attn.v_proj | mse: 0.00038198465924549823, bpp_loss: 5.886940240859985, bpp: 0
2025-03-02 21:00:07 - INFO - layer20_self_attn.o_proj | mse: 0.0003891238321963228, bpp_loss: 5.919836044311523, bpp: 0
2025-03-02 21:00:13 - INFO - layer20_mlp.gate_proj | mse: 0.0004875589690951268, bpp_loss: 6.404699870518276, bpp: 0
2025-03-02 21:00:17 - INFO - layer20_mlp.up_proj | mse: 0.0004060925283200653, bpp_loss: 6.004554748535156, bpp: 0
2025-03-02 21:00:22 - INFO - layer20_mlp.down_proj | mse: 0.00040520093760284225, bpp_loss: 6.000584159578596, bpp: 0
pseudo compress quantization...:  66%|██████▌   | 21/32 [05:06<03:18, 18.05s/it]2025-03-02 21:00:24 - INFO - layer21_self_attn.q_proj | mse: 0.0005583682443148017, bpp_loss: 6.718688130378723, bpp: 0
2025-03-02 21:00:24 - INFO - layer21_self_attn.k_proj | mse: 0.0008627423553125358, bpp_loss: 7.910720705986023, bpp: 0
2025-03-02 21:00:25 - INFO - layer21_self_attn.v_proj | mse: 0.0003881016740984718, bpp_loss: 5.914263486862183, bpp: 0
2025-03-02 21:00:27 - INFO - layer21_self_attn.o_proj | mse: 0.0003937602319648179, bpp_loss: 5.941314101219177, bpp: 0
2025-03-02 21:00:32 - INFO - layer21_mlp.gate_proj | mse: 0.0004903508386727983, bpp_loss: 6.417675154549735, bpp: 0
2025-03-02 21:00:37 - INFO - layer21_mlp.up_proj | mse: 0.00040676629764909225, bpp_loss: 6.0075885227748325, bpp: 0
2025-03-02 21:00:41 - INFO - layer21_mlp.down_proj | mse: 0.000405810946128888, bpp_loss: 6.003296852111816, bpp: 0
pseudo compress quantization...:  69%|██████▉   | 22/32 [05:25<03:03, 18.37s/it]2025-03-02 21:00:43 - INFO - layer22_self_attn.q_proj | mse: 0.0005466007289951285, bpp_loss: 6.670865178108215, bpp: 0
2025-03-02 21:00:43 - INFO - layer22_self_attn.k_proj | mse: 0.0008442083444088334, bpp_loss: 7.822420239448547, bpp: 0
2025-03-02 21:00:44 - INFO - layer22_self_attn.v_proj | mse: 0.0003975179677752339, bpp_loss: 5.964767694473267, bpp: 0
2025-03-02 21:00:45 - INFO - layer22_self_attn.o_proj | mse: 0.000396948698910322, bpp_loss: 5.961663842201233, bpp: 0
2025-03-02 21:00:50 - INFO - layer22_mlp.gate_proj | mse: 0.0004905235231031032, bpp_loss: 6.418720517839704, bpp: 0
2025-03-02 21:00:55 - INFO - layer22_mlp.up_proj | mse: 0.0004073436885797793, bpp_loss: 6.0110931396484375, bpp: 0
2025-03-02 21:01:00 - INFO - layer22_mlp.down_proj | mse: 0.0004065981915807052, bpp_loss: 6.007131201880319, bpp: 0
pseudo compress quantization...:  72%|███████▏  | 23/32 [05:43<02:45, 18.41s/it]2025-03-02 21:01:01 - INFO - layer23_self_attn.q_proj | mse: 0.0005473685810749233, bpp_loss: 6.678952097892761, bpp: 0
2025-03-02 21:01:02 - INFO - layer23_self_attn.k_proj | mse: 0.0008353337976925726, bpp_loss: 7.817593693733215, bpp: 0
2025-03-02 21:01:02 - INFO - layer23_self_attn.v_proj | mse: 0.00040864066479173684, bpp_loss: 6.018612742424011, bpp: 0
2025-03-02 21:01:04 - INFO - layer23_self_attn.o_proj | mse: 0.0004011409911061669, bpp_loss: 5.98131537437439, bpp: 0
2025-03-02 21:01:09 - INFO - layer23_mlp.gate_proj | mse: 0.0004909692020626831, bpp_loss: 6.421406064714704, bpp: 0
2025-03-02 21:01:14 - INFO - layer23_mlp.up_proj | mse: 0.00040825540339628003, bpp_loss: 6.015189443315778, bpp: 0
2025-03-02 21:01:18 - INFO - layer23_mlp.down_proj | mse: 0.0004074606825399755, bpp_loss: 6.011696508952549, bpp: 0
pseudo compress quantization...:  75%|███████▌  | 24/32 [06:02<02:27, 18.45s/it]2025-03-02 21:01:20 - INFO - layer24_self_attn.q_proj | mse: 0.0005404136419250046, bpp_loss: 6.641959309577942, bpp: 0
2025-03-02 21:01:20 - INFO - layer24_self_attn.k_proj | mse: 0.0007950265201621168, bpp_loss: 7.6068501472473145, bpp: 0
2025-03-02 21:01:21 - INFO - layer24_self_attn.v_proj | mse: 0.0004292116587521371, bpp_loss: 6.120034337043762, bpp: 0
2025-03-02 21:01:22 - INFO - layer24_self_attn.o_proj | mse: 0.00041053560222994767, bpp_loss: 6.027387738227844, bpp: 0
2025-03-02 21:01:27 - INFO - layer24_mlp.gate_proj | mse: 0.0004922523332564679, bpp_loss: 6.428794997079032, bpp: 0
2025-03-02 21:01:32 - INFO - layer24_mlp.up_proj | mse: 0.00040921215170486595, bpp_loss: 6.019913809640067, bpp: 0
2025-03-02 21:01:37 - INFO - layer24_mlp.down_proj | mse: 0.0004085509276048232, bpp_loss: 6.01692499433245, bpp: 0
pseudo compress quantization...:  78%|███████▊  | 25/32 [06:21<02:09, 18.54s/it]2025-03-02 21:01:39 - INFO - layer25_self_attn.q_proj | mse: 0.0005329646212181112, bpp_loss: 6.612588405609131, bpp: 0
2025-03-02 21:01:39 - INFO - layer25_self_attn.k_proj | mse: 0.0007862937474666023, bpp_loss: 7.590622782707214, bpp: 0
2025-03-02 21:01:40 - INFO - layer25_self_attn.v_proj | mse: 0.0004310109485192634, bpp_loss: 6.129838347434998, bpp: 0
2025-03-02 21:01:41 - INFO - layer25_self_attn.o_proj | mse: 0.0004117349325874412, bpp_loss: 6.031739234924316, bpp: 0
2025-03-02 21:01:46 - INFO - layer25_mlp.gate_proj | mse: 0.0004947015240844868, bpp_loss: 6.440681457519531, bpp: 0
2025-03-02 21:01:51 - INFO - layer25_mlp.up_proj | mse: 0.00041112350140656216, bpp_loss: 6.0291748046875, bpp: 0
2025-03-02 21:01:56 - INFO - layer25_mlp.down_proj | mse: 0.00041031882196642484, bpp_loss: 6.026156323296683, bpp: 0
pseudo compress quantization...:  81%|████████▏ | 26/32 [06:40<01:52, 18.68s/it]2025-03-02 21:01:58 - INFO - layer26_self_attn.q_proj | mse: 0.0005347160645165502, bpp_loss: 6.6235257387161255, bpp: 0
2025-03-02 21:01:58 - INFO - layer26_self_attn.k_proj | mse: 0.0008173492290186969, bpp_loss: 7.6896408796310425, bpp: 0
2025-03-02 21:01:59 - INFO - layer26_self_attn.v_proj | mse: 0.00044567016498622494, bpp_loss: 6.194265604019165, bpp: 0
2025-03-02 21:02:00 - INFO - layer26_self_attn.o_proj | mse: 0.0004161781358295811, bpp_loss: 6.052600383758545, bpp: 0
2025-03-02 21:02:06 - INFO - layer26_mlp.gate_proj | mse: 0.0004970706841115226, bpp_loss: 6.451846122741699, bpp: 0
2025-03-02 21:02:10 - INFO - layer26_mlp.up_proj | mse: 0.00041267241038964945, bpp_loss: 6.038178988865444, bpp: 0
2025-03-02 21:02:15 - INFO - layer26_mlp.down_proj | mse: 0.0004122056502720861, bpp_loss: 6.035007885524204, bpp: 0
pseudo compress quantization...:  84%|████████▍ | 27/32 [06:59<01:33, 18.79s/it]2025-03-02 21:02:17 - INFO - layer27_self_attn.q_proj | mse: 0.0005283711791589673, bpp_loss: 6.5903273820877075, bpp: 0
2025-03-02 21:02:17 - INFO - layer27_self_attn.k_proj | mse: 0.0008123379791014793, bpp_loss: 7.638235330581665, bpp: 0
2025-03-02 21:02:18 - INFO - layer27_self_attn.v_proj | mse: 0.00046466725003814995, bpp_loss: 6.287766814231873, bpp: 0
2025-03-02 21:02:19 - INFO - layer27_self_attn.o_proj | mse: 0.0004214020015116279, bpp_loss: 6.08429229259491, bpp: 0
2025-03-02 21:02:24 - INFO - layer27_mlp.gate_proj | mse: 0.0005006784850357398, bpp_loss: 6.465643201555524, bpp: 0
2025-03-02 21:02:29 - INFO - layer27_mlp.up_proj | mse: 0.0004156891370303289, bpp_loss: 6.052490506853376, bpp: 0
2025-03-02 21:02:34 - INFO - layer27_mlp.down_proj | mse: 0.00041466110078137145, bpp_loss: 6.047131334032331, bpp: 0
pseudo compress quantization...:  88%|████████▊ | 28/32 [07:18<01:15, 18.80s/it]2025-03-02 21:02:35 - INFO - layer28_self_attn.q_proj | mse: 0.0005306555424913285, bpp_loss: 6.596796274185181, bpp: 0
2025-03-02 21:02:36 - INFO - layer28_self_attn.k_proj | mse: 0.0007622994450474056, bpp_loss: 7.541727900505066, bpp: 0
2025-03-02 21:02:36 - INFO - layer28_self_attn.v_proj | mse: 0.0004778547236135022, bpp_loss: 6.351853251457214, bpp: 0
2025-03-02 21:02:38 - INFO - layer28_self_attn.o_proj | mse: 0.000427541694775966, bpp_loss: 6.1140289306640625, bpp: 0
2025-03-02 21:02:43 - INFO - layer28_mlp.gate_proj | mse: 0.0004987036331955776, bpp_loss: 6.454204831804548, bpp: 0
2025-03-02 21:02:48 - INFO - layer28_mlp.up_proj | mse: 0.0004203236291480249, bpp_loss: 6.075965063912528, bpp: 0
2025-03-02 21:02:53 - INFO - layer28_mlp.down_proj | mse: 0.0004185025274956597, bpp_loss: 6.065505845206125, bpp: 0
pseudo compress quantization...:  91%|█████████ | 29/32 [07:36<00:56, 18.75s/it]2025-03-02 21:02:54 - INFO - layer29_self_attn.q_proj | mse: 0.0005270582304132001, bpp_loss: 6.581472754478455, bpp: 0
2025-03-02 21:02:54 - INFO - layer29_self_attn.k_proj | mse: 0.0008052652117983031, bpp_loss: 7.67617928981781, bpp: 0
2025-03-02 21:02:55 - INFO - layer29_self_attn.v_proj | mse: 0.0004920760339895638, bpp_loss: 6.4199090003967285, bpp: 0
2025-03-02 21:02:56 - INFO - layer29_self_attn.o_proj | mse: 0.00043692390293624604, bpp_loss: 6.157227039337158, bpp: 0
2025-03-02 21:03:01 - INFO - layer29_mlp.gate_proj | mse: 0.000497016364563344, bpp_loss: 6.448534148080008, bpp: 0
2025-03-02 21:03:06 - INFO - layer29_mlp.up_proj | mse: 0.0004269711778547687, bpp_loss: 6.108619144984654, bpp: 0
2025-03-02 21:03:11 - INFO - layer29_mlp.down_proj | mse: 0.0004228286915113133, bpp_loss: 6.084931407655988, bpp: 0
pseudo compress quantization...:  94%|█████████▍| 30/32 [07:54<00:37, 18.57s/it]2025-03-02 21:03:12 - INFO - layer30_self_attn.q_proj | mse: 0.0005053807925226917, bpp_loss: 6.480248212814331, bpp: 0
2025-03-02 21:03:13 - INFO - layer30_self_attn.k_proj | mse: 0.0007074762641245994, bpp_loss: 7.222796320915222, bpp: 0
2025-03-02 21:03:14 - INFO - layer30_self_attn.v_proj | mse: 0.0005665435626471333, bpp_loss: 6.75625741481781, bpp: 0
2025-03-02 21:03:15 - INFO - layer30_self_attn.o_proj | mse: 0.0004555745198033718, bpp_loss: 6.247478246688843, bpp: 0
2025-03-02 21:03:20 - INFO - layer30_mlp.gate_proj | mse: 0.0005091074303917755, bpp_loss: 6.508305549621582, bpp: 0
2025-03-02 21:03:25 - INFO - layer30_mlp.up_proj | mse: 0.0004330433790804949, bpp_loss: 6.139559609549386, bpp: 0
2025-03-02 21:03:30 - INFO - layer30_mlp.down_proj | mse: 0.00042442910059936427, bpp_loss: 6.090523277010236, bpp: 0
pseudo compress quantization...:  97%|█████████▋| 31/32 [08:13<00:18, 18.66s/it]2025-03-02 21:03:31 - INFO - layer31_self_attn.q_proj | mse: 0.0005487764532311326, bpp_loss: 6.6587148904800415, bpp: 0
2025-03-02 21:03:32 - INFO - layer31_self_attn.k_proj | mse: 0.0007766567353590744, bpp_loss: 7.5256065130233765, bpp: 0
2025-03-02 21:03:33 - INFO - layer31_self_attn.v_proj | mse: 0.000515392609512652, bpp_loss: 6.523502707481384, bpp: 0
2025-03-02 21:03:34 - INFO - layer31_self_attn.o_proj | mse: 0.00045173625526278107, bpp_loss: 6.222994804382324, bpp: 0
2025-03-02 21:03:39 - INFO - layer31_mlp.gate_proj | mse: 0.000565996500130846, bpp_loss: 6.7652497972760886, bpp: 0
2025-03-02 21:03:44 - INFO - layer31_mlp.up_proj | mse: 0.0004752674900003679, bpp_loss: 6.3457328251429965, bpp: 0
2025-03-02 21:03:49 - INFO - layer31_mlp.down_proj | mse: 0.00043147641531425106, bpp_loss: 6.123612097331455, bpp: 0
pseudo compress quantization...: 100%|██████████| 32/32 [08:33<00:00, 18.94s/it]pseudo compress quantization...: 100%|██████████| 32/32 [08:33<00:00, 16.04s/it]
2025-03-02 21:03:49 - INFO - #### Total | mse: 0.0004466606815800174, bpp_loss: 6.192096360385991, bpp: 0 ####
## Strart saving /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda100000_rdloss_ql_encdim512_M16_batch_size1024_total_iter200000_lr0.0001_seed100/best_loss_model_loss_49.72731_bpp_6.30559_MSE_0.00042_total_iter_140000.pth.tar/COL_MSE0.00045_bpploss6.1921_bpp0
## End saving
Running evaluation for directory: /home/jgryu/Weight_compression/comp_llm/model_lm_reconstructed/nwc_ql_n6144/meta-llama--Meta-Llama-3-8B/block_seq_ql_random_col_16/lmbda100000_rdloss_ql_encdim512_M16_batch_size1024_total_iter200000_lr0.0001_seed100/best_loss_model_loss_49.72731_bpp_6.30559_MSE_0.00042_total_iter_140000.pth.tar/COL_MSE0.00045_bpploss6.1921_bpp0
I0302 21:04:26.514625 2585538 modeling.py:879] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.00it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.46it/s]
W0302 21:04:29.394023 2585538 big_modeling.py:414] Some parameters are on the meta device device because they were offloaded to the cpu.
I0302 21:04:29.411666 2585538 config.py:54] PyTorch version 2.4.1 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.5078125:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.5078125:   1%|          | 1/141 [00:01<03:07,  1.34s/it]avg_loss = 1.5078125:   1%|          | 1/141 [00:02<04:56,  2.12s/it]
Traceback (most recent call last):
  File "/home/jgryu/Weight_compression/comp_llm/eval_ppl.py", line 115, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/comp_llm/eval_ppl.py", line 91, in main
    output = model(input,
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1179, in forward
    logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/hooks.py", line 160, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/hooks.py", line 293, in pre_forward
    set_module_tensor_to_device(
  File "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 347, in set_module_tensor_to_device
    new_value = value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1002.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 934.19 MiB is free. Process 2580051 has 7.63 GiB memory in use. Including non-PyTorch memory, this process has 15.02 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
