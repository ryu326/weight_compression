{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, AutoModelForCausalLM, LlavaForConditionalGeneration\n",
    "from transformers import AutoModel, AutoTokenizer, OPTForCausalLM, BloomForCausalLM\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "def get_named_linears(module):\n",
    "    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vision_tower.vision_model.encoder.layers.0.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.0.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.0.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.0.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.0.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.0.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.1.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.1.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.1.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.1.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.1.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.1.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.2.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.2.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.2.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.2.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.2.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.2.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.3.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.3.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.3.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.3.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.3.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.3.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.4.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.4.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.4.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.4.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.4.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.4.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.5.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.5.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.5.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.5.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.5.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.5.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.6.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.6.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.6.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.6.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.6.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.6.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.7.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.7.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.7.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.7.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.7.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.7.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.8.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.8.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.8.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.8.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.8.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.8.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.9.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.9.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.9.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.9.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.9.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.9.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.10.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.10.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.10.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.10.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.10.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.10.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.11.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.11.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.11.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.11.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.11.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.11.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.12.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.12.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.12.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.12.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.12.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.12.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.13.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.13.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.13.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.13.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.13.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.13.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.14.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.14.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.14.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.14.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.14.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.14.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.15.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.15.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.15.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.15.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.15.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.15.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.16.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.16.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.16.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.16.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.16.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.16.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.17.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.17.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.17.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.17.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.17.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.17.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.18.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.18.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.18.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.18.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.18.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.18.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.19.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.19.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.19.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.19.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.19.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.19.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.20.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.20.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.20.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.20.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.20.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.20.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.21.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.21.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.21.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.21.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.21.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.21.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.22.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.22.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.22.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.22.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.22.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.22.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.23.self_attn.k_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.23.self_attn.v_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.23.self_attn.q_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.23.self_attn.out_proj': Linear(in_features=1024, out_features=1024, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.23.mlp.fc1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'vision_tower.vision_model.encoder.layers.23.mlp.fc2': Linear(in_features=4096, out_features=1024, bias=True),\n",
       " 'multi_modal_projector.linear_1': Linear(in_features=1024, out_features=4096, bias=True),\n",
       " 'multi_modal_projector.linear_2': Linear(in_features=4096, out_features=4096, bias=True),\n",
       " 'language_model.model.layers.0.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.0.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.0.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.0.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.0.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.0.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.0.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.1.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.1.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.1.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.1.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.1.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.1.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.1.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.2.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.2.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.2.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.2.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.2.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.2.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.2.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.3.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.3.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.3.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.3.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.3.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.3.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.3.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.4.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.4.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.4.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.4.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.4.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.4.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.4.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.5.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.5.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.5.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.5.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.5.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.5.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.5.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.6.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.6.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.6.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.6.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.6.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.6.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.6.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.7.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.7.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.7.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.7.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.7.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.7.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.7.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.8.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.8.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.8.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.8.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.8.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.8.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.8.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.9.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.9.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.9.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.9.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.9.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.9.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.9.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.10.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.10.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.10.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.10.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.10.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.10.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.10.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.11.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.11.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.11.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.11.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.11.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.11.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.11.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.12.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.12.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.12.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.12.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.12.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.12.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.12.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.13.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.13.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.13.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.13.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.13.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.13.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.13.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.14.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.14.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.14.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.14.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.14.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.14.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.14.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.15.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.15.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.15.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.15.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.15.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.15.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.15.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.16.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.16.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.16.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.16.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.16.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.16.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.16.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.17.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.17.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.17.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.17.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.17.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.17.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.17.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.18.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.18.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.18.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.18.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.18.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.18.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.18.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.19.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.19.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.19.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.19.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.19.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.19.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.19.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.20.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.20.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.20.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.20.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.20.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.20.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.20.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.21.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.21.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.21.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.21.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.21.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.21.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.21.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.22.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.22.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.22.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.22.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.22.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.22.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.22.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.23.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.23.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.23.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.23.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.23.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.23.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.23.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.24.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.24.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.24.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.24.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.24.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.24.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.24.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.25.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.25.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.25.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.25.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.25.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.25.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.25.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.26.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.26.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.26.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.26.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.26.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.26.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.26.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.27.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.27.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.27.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.27.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.27.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.27.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.27.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.28.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.28.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.28.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.28.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.28.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.28.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.28.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.29.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.29.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.29.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.29.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.29.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.29.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.29.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.30.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.30.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.30.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.30.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.30.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.30.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.30.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.31.self_attn.q_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.31.self_attn.k_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.31.self_attn.v_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.31.self_attn.o_proj': Linear(in_features=4096, out_features=4096, bias=False),\n",
       " 'language_model.model.layers.31.mlp.gate_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.31.mlp.up_proj': Linear(in_features=4096, out_features=11008, bias=False),\n",
       " 'language_model.model.layers.31.mlp.down_proj': Linear(in_features=11008, out_features=4096, bias=False),\n",
       " 'language_model.lm_head': Linear(in_features=4096, out_features=32064, bias=False)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_named_linears(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  llava-hf--llava-1.5-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dataset shape:  torch.Size([6767872, 1024])\n",
      "train:  torch.Size([6766872, 1024]) val:  torch.Size([1000, 1024])\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    # '/workspace/Weight_compression/Wparam_dataset/hf_model/llava-hf--llava-1.5-7b-hf',\n",
    "    'llava-hf--llava-1.5-7b-hf',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "]\n",
    "\n",
    "direction = 'col'\n",
    "shuffle = False\n",
    "drop_last = False\n",
    "modelwise_norm = False\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    model_id = model_name\n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "    datas = []\n",
    "    named_linears = get_named_linears(model)\n",
    "    for n, m in named_linears.items():\n",
    "        w = m.weight.data.detach()\n",
    "        \n",
    "        if direction == 'col':\n",
    "            w = w.T\n",
    "        w = w.reshape(-1, size)\n",
    "\n",
    "        datas.append(w)\n",
    "        \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "\n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': data.mean().item(),\n",
    "            'std': data.std().item(),\n",
    "            # 'mean_channel': data.mean(dim=0).tolist(),\n",
    "            # 'std_channel': data.std(dim=0).tolist(),\n",
    "        }\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gaussian block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load('/workspace/Weight_compression/Wparam_dataset/block_pt/meta-llama--Meta-Llama-3-8B/col_1024_gaussian_padding.pt')\n",
    "print(d['train'].shape)\n",
    "print(d['val'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataset['train'] = torch.normal(mean=0.0, std=1.0, size=d['train'].shape)\n",
    "dataset['val'] = torch.normal(mean=0.0, std=1.0, size=d['val'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "dataset_stats = {}\n",
    "for split in ['train', 'val']:\n",
    "    data = dataset[split]\n",
    "    \n",
    "    mean_all = data.mean()\n",
    "    std_all = data.std()\n",
    "    \n",
    "    dataset_stats[split] = {\n",
    "        'mean': mean_all.item(),\n",
    "        'std': std_all.item(),\n",
    "    }\n",
    "\n",
    "sub = 'llama8b_'\n",
    "direction = 'col'\n",
    "size = 1024\n",
    "os.makedirs(f'./block_pt/gaussian', exist_ok = True)\n",
    "torch.save(dataset, f'./block_pt/gaussian/{sub}{direction}_{size}.pt')\n",
    "json_path = f'./block_pt/gaussian/{sub}{direction}_{size}_dataset_stats.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layerwise, channelwise normalized Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai--clip-vit-large-patch14',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    # 4096,\n",
    "    # 4096,\n",
    "    # 256,\n",
    "]\n",
    "\n",
    "# direction = 'adapt'\n",
    "direction = 'col'\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    # model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "        \n",
    "    datas = []\n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.detach().cuda()\n",
    "\n",
    "            W = (W - W.mean(dim=0, keepdim=True)) / W.std(dim=0, keepdim=True)\n",
    "            # W = (W - W.mean()) / W.std()\n",
    "            \n",
    "            if direction == 'col':\n",
    "                W = W.T    \n",
    "            W = W.reshape(-1, size).cpu()\n",
    "                \n",
    "            datas.append(W)\n",
    "        \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        # mean_dim0 = data.mean(dim=0)\n",
    "        # std_dim0 = data.std(dim=0)        \n",
    "        # mean_all = data.mean()\n",
    "        # std_all = data.std()\n",
    "        \n",
    "        # dataset_stats[split] = {\n",
    "        #     'mean': mean_all.item(),\n",
    "        #     'std': std_all.item(),\n",
    "        #     'mean_channel': mean_dim0.tolist(),\n",
    "        #     'std_channel': std_dim0.tolist(),\n",
    "        # }\n",
    "        dataset_stats[split] = {\n",
    "            'mean': 0,\n",
    "            'std': 1,\n",
    "            'mean_channel': None,\n",
    "            'std_channel': None,\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_colwise_normed.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_colwise_normed_dataset_stats.json'\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_layerwise_normed.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/{direction}_{size}_layerwise_normed_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gaussian padding\n",
    "\n",
    "size가 안 맞으면 같은 row 나 col의 mean, std를 갖는 가우시안으로 padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    # 'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    # 1024,\n",
    "    1024,\n",
    "    1280,\n",
    "]\n",
    "\n",
    "# direction = 'adapt'\n",
    "direction = 'col'\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    datas = []\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.detach()\n",
    "\n",
    "            r, c = W.shape\n",
    "\n",
    "            if direction == 'col':\n",
    "                if r % size != 0:\n",
    "                    padding_size = size - r % size\n",
    "                    mean_c = W.mean(0)\n",
    "                    std_c = W.std(0)\n",
    "\n",
    "                    g = torch.normal(mean_c.expand(padding_size, c), std_c.expand(padding_size, c))\n",
    "                    W = torch.cat([W, g], dim=0)\n",
    "            elif direction =='row':\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                raise KeyError\n",
    "            \n",
    "            if direction == 'col':\n",
    "                W = W.T\n",
    "            \n",
    "            assert W.shape[1] % size == 0\n",
    "            assert W.shape[1] >= size\n",
    "\n",
    "            W = W.reshape(-1, size)\n",
    "                \n",
    "            datas.append(W)\n",
    "    \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        mean_dim0 = data.mean(dim=0)\n",
    "        std_dim0 = data.std(dim=0)\n",
    "        \n",
    "        mean_all = data.mean()\n",
    "        std_all = data.std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': mean_dim0.tolist(),\n",
    "            'std_channel': std_dim0.tolist(),\n",
    "        }\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_gaussian_padding.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_gaussian_padding_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RHT smoothed Weight Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/Weight_compression/Wparam_dataset')\n",
    "from utils import *\n",
    "\n",
    "def RHT_H(H, SU):\n",
    "    return matmul_hadUt(matmul_hadUt(H * SU).T * SU)\n",
    "\n",
    "\n",
    "def RHT_W(W, SU, SV):\n",
    "    return matmul_hadUt(matmul_hadUt(W.T * SV).T * SU)\n",
    "\n",
    "\n",
    "def incoherence_preprocess(H, W, args):\n",
    "    # dtype_ = torch.float64 if args.use_fp64 else torch.float32\n",
    "    dtype_ = torch.float32\n",
    "    device = W.device\n",
    "    # device = torch.device('cpu')\n",
    "    (m, n) = W.shape\n",
    "\n",
    "    def _dump(Hr, Lhr, msg=''):\n",
    "        torch.save(Hr, f\"{args.save_pfx}/Hr_debug_fft.pt\")\n",
    "        torch.save(Lhr, f\"{args.save_pfx}/Lhr_debug_fft.pt\")\n",
    "        raise Exception(msg)\n",
    "\n",
    "    # diagonally rescale W,H to minimize proxy loss\n",
    "    scaleWH = None\n",
    "    Wr = W\n",
    "    Hr = H\n",
    "    # if args.rescale_WH:\n",
    "    if False:\n",
    "        Hr = H / H.abs().max()\n",
    "        diagH = torch.diag(Hr)\n",
    "        diagW2 = torch.diag(W.T @ W)\n",
    "        diagH = torch.clamp(diagH, min=1e-8)\n",
    "        diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "        scaleWH = (diagH / diagW2).sqrt().sqrt().to(torch.float32)\n",
    "        scaleWH = scaleWH.clamp(min=1e-8)\n",
    "        Wr = Wr * scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[:, None]\n",
    "        scaleWH = scaleWH.cpu()\n",
    "\n",
    "    # randomized hadamard transformation on H, W\n",
    "    if True:\n",
    "        SU = (torch.randn(n, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        SV = (torch.randn(m, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        # Hr = RHT_H(Hr, SU)\n",
    "        Wr = RHT_W(Wr, SU, SV)\n",
    "    # randomized kronecker product on H, W\n",
    "    elif args.incoh_mode == \"kron\":\n",
    "        SU = utils.rand_ortho_butterfly_noblock(n).to(dtype_).to(device)\n",
    "        SV = utils.rand_ortho_butterfly_noblock(m).to(dtype_).to(device)\n",
    "        Hr = SU @ Hr @ SU.T\n",
    "        Wr = SV @ Wr @ SU.T\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    SV = SV.cpu()\n",
    "    SU = SU.cpu()\n",
    "\n",
    "    # Lhr = torch.linalg.cholesky(Hr)\n",
    "    Lhr = None\n",
    "    # if not torch.all(torch.isfinite(Lhr)):\n",
    "    #     return None\n",
    "\n",
    "    Wr = Wr.to(device)\n",
    "\n",
    "    return Lhr, Hr, Wr, SU, SV, scaleWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    4096,\n",
    "    # 4096,\n",
    "    # 4096,\n",
    "]\n",
    "\n",
    "# direction = 'adapt'\n",
    "direction = 'col'\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    datas = []\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            w = m.weight.data.detach()\n",
    "            \n",
    "            Lhr, H, w, SU, SV, scaleWH = incoherence_preprocess(None, w, None) \n",
    "            \n",
    "            if direction == 'col':\n",
    "                w = w.T\n",
    "            \n",
    "            # if w.size(0) % size == 0:\n",
    "            #     w = w.T    \n",
    "            #     w = w.reshape(-1, size)\n",
    "            # else:\n",
    "            #     w = w.reshape(-1, size)\n",
    "                \n",
    "            w = w.reshape(-1, size)\n",
    "            \n",
    "            datas.append(w)\n",
    "    \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        mean_dim0 = data.mean(dim=0)\n",
    "        std_dim0 = data.std(dim=0)\n",
    "        \n",
    "        mean_all = data.mean()\n",
    "        std_all = data.std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': mean_dim0.tolist(),\n",
    "            'std_channel': std_dim0.tolist(),\n",
    "        }\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_RHT.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_RHT_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
