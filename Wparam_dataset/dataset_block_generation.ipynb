{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, OPTForCausalLM, BloomForCausalLM\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "def get_named_linears(module):\n",
    "    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}\n",
    "\n",
    "def get_blocks(model):\n",
    "    if model.__class__.__name__ in (\"LlamaForCausalLM\", \"Qwen2ForCausalLM\"):\n",
    "        layers = model.model.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaForCausalLM\":\n",
    "        layers = model.model.layers\n",
    "    elif isinstance(model, OPTForCausalLM):\n",
    "        layers = model.model.decoder.layers\n",
    "    elif isinstance(model, BloomForCausalLM):\n",
    "        layers = model.transformer.h\n",
    "    elif \"mpt\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.blocks\n",
    "    elif \"falcon\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"bigcode\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"neox\" in str(model.__class__).lower():\n",
    "        layers = model.gpt_neox.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaModel\":\n",
    "        layers = model.llm.model.layers\n",
    "    elif model.__class__.__name__ in (\"CLIPModel\"):\n",
    "        vision_layers = model.vision_model.encoder.layers\n",
    "        text_layers = model.text_model.encoder.layers\n",
    "        layers = {'vision': vision_layers,\n",
    "                  'text': text_layers}\n",
    "    else:\n",
    "        raise NotImplementedError(type(model))\n",
    "    # if not isinstance(layers, dict):\n",
    "    #     layers = {'': layers}\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dignal Scaled RHT Weight Block\n",
    "\n",
    "많이 수정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_to_sym(V, N):\n",
    "    A = torch.zeros(N, N, dtype=V.dtype, device=V.device)\n",
    "    idxs = torch.tril_indices(N, N, device=V.device)\n",
    "    A[idxs.unbind()] = V\n",
    "    A[idxs[1, :], idxs[0, :]] = V\n",
    "    return A\n",
    "\n",
    "def regularize_H(H, n, sigma_reg):\n",
    "    H.div_(torch.diag(H).mean())\n",
    "    idx = torch.arange(n)\n",
    "    H[idx, idx] += sigma_reg\n",
    "    return H\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/Weight_compression/Wparam_dataset')\n",
    "from utils import *\n",
    "\n",
    "def RHT_H(H, SU):\n",
    "    return matmul_hadUt(matmul_hadUt(H * SU).T * SU)\n",
    "\n",
    "def RHT_W(W, SU, SV):\n",
    "    return matmul_hadUt(matmul_hadUt(W.T * SV).T * SU)\n",
    "\n",
    "def incoherence_preprocess(H, W, args):\n",
    "    # dtype_ = torch.float64 if args.use_fp64 else torch.float32\n",
    "    dtype_ = torch.float32\n",
    "    device = H.device\n",
    "    # device = torch.device('cpu')\n",
    "    (m, n) = H.shape\n",
    "\n",
    "    def _dump(Hr, Lhr, msg=''):\n",
    "        torch.save(Hr, f\"{args.save_pfx}/Hr_debug_fft.pt\")\n",
    "        torch.save(Lhr, f\"{args.save_pfx}/Lhr_debug_fft.pt\")\n",
    "        raise Exception(msg)\n",
    "\n",
    "    # diagonally rescale W,H to minimize proxy loss\n",
    "    scaleWH = None\n",
    "    Wr = W\n",
    "    Hr = H\n",
    "    # if args.rescale_WH:\n",
    "    if False:\n",
    "        Hr = H / H.abs().max()\n",
    "        diagH = torch.diag(Hr)\n",
    "        diagW2 = torch.diag(W.T @ W)\n",
    "        diagH = torch.clamp(diagH, min=1e-8)\n",
    "        diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "        scaleWH = (diagH / diagW2).sqrt().sqrt().to(torch.float32)\n",
    "        scaleWH = scaleWH.clamp(min=1e-8)\n",
    "        Wr = Wr * scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[:, None]\n",
    "        scaleWH = scaleWH.cpu()\n",
    "\n",
    "    # randomized hadamard transformation on H, W\n",
    "    if True:\n",
    "        SU = (torch.randn(n, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        SV = (torch.randn(m, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        Hr = RHT_H(Hr, SU)\n",
    "        # Wr = RHT_W(Wr, SU, SV)\n",
    "    # randomized kronecker product on H, W\n",
    "    elif args.incoh_mode == \"kron\":\n",
    "        SU = utils.rand_ortho_butterfly_noblock(n).to(dtype_).to(device)\n",
    "        SV = utils.rand_ortho_butterfly_noblock(m).to(dtype_).to(device)\n",
    "        Hr = SU @ Hr @ SU.T\n",
    "        Wr = SV @ Wr @ SU.T\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    SV = SV.cpu()\n",
    "    SU = SU.cpu()\n",
    "\n",
    "    # Lhr = torch.linalg.cholesky(Hr)\n",
    "    Lhr = None\n",
    "    # if not torch.all(torch.isfinite(Lhr)):\n",
    "    #     return None\n",
    "\n",
    "    # Wr = Wr.to(device)\n",
    "\n",
    "    return Lhr, Hr, Wr, SU, SV, scaleWH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.27it/s]\n",
      "100%|██████████| 32/32 [00:44<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight total shape:  torch.Size([6815744, 1024])\n",
      "idx total shape:  torch.Size([6815744])\n",
      "layer_type total shape:  torch.Size([6815744])\n",
      "scale total shape:  torch.Size([6815744, 1024])\n",
      "train Weight:  torch.Size([6814744, 1024]) val:  torch.Size([1000, 1024])\n",
      "train Scale:  torch.Size([6814744, 1024]) val:  torch.Size([1000, 1024])\n",
      "Scale stats: tensor(0.9961) tensor(0.0058) tensor(7.3696)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf'\n",
    "]\n",
    "quip_hess_path = [\n",
    "    './quip_hess/llama3_8b_6144',\n",
    "    # './quip_hess/Hessians-Llama-2-7b-6144',\n",
    "]\n",
    "quip_hess_eig_path = [\n",
    "    './quip_hess_eig_reg0.0001/llama3_8b_6144'\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    # 4096\n",
    "]\n",
    "\n",
    "wtype_mapping = {'self_attn.q_proj': 0, \n",
    "                 'self_attn.k_proj': 1, \n",
    "                 'self_attn.v_proj': 2, \n",
    "                 'self_attn.o_proj': 3, \n",
    "                 'mlp.gate_proj': 4, \n",
    "                 'mlp.up_proj': 5, \n",
    "                 'mlp.down_proj': 6}\n",
    "sigma_reg = 1e-4\n",
    "# direction = 'col'\n",
    "direction = 'row'\n",
    "\n",
    "global_std = 0.012529\n",
    "\n",
    "for model_name, size, quip_hess, eig_path in zip(model_list, size_list, quip_hess_path, quip_hess_eig_path):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    raw_data = {}\n",
    "    raw_data['weight'] = []\n",
    "    raw_data['idx'] = []\n",
    "    raw_data['layer_type'] = []\n",
    "    raw_data['scale'] = []\n",
    "        \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        \n",
    "        # hess_dict = {}\n",
    "        # hess_dict['qkv'] = torch.load(f'{quip_hess}/{i}_qkv.pt', weights_only=False)\n",
    "        # hess_dict['o'] = torch.load(f'{quip_hess}/{i}_o.pt', weights_only=False)\n",
    "        # hess_dict['up'] = torch.load(f'{quip_hess}/{i}_up.pt', weights_only=False)\n",
    "        # hess_dict['down'] = torch.load(f'{quip_hess}/{i}_down.pt', weights_only=False)\n",
    "        \n",
    "        hess_eig_dict = {}\n",
    "        hess_eig_dict['qkv'] = torch.load(f'{eig_path}/{i}_qkv_eig.pt', weights_only=False)\n",
    "        hess_eig_dict['o'] = torch.load(f'{eig_path}/{i}_o_eig.pt', weights_only=False)\n",
    "        hess_eig_dict['up'] = torch.load(f'{eig_path}/{i}_up_eig.pt', weights_only=False)\n",
    "        hess_eig_dict['down'] = torch.load(f'{eig_path}/{i}_down_eig.pt', weights_only=False)\n",
    "        \n",
    "        for n, m in named_linears.items():\n",
    "            \n",
    "            W = m.weight.data.detach().to(device)\n",
    "            \n",
    "            if 'q_proj' in n or 'k_proj' in n or 'v_proj' in n:\n",
    "                # H_flat = hess_dict['qkv']\n",
    "                H_eig = hess_eig_dict['qkv']\n",
    "            elif 'o_proj' in n:\n",
    "                # H_flat = hess_dict['o']\n",
    "                H_eig = hess_eig_dict['o']\n",
    "            elif 'up_proj' in n or 'gate_proj' in n:\n",
    "                # H_flat = hess_dict['up']\n",
    "                H_eig = hess_eig_dict['up']\n",
    "            elif 'down_proj' in n:\n",
    "                # H_flat = hess_dict['down']\n",
    "                H_eig = hess_eig_dict['down']\n",
    "            else:\n",
    "                raise NotImplementedError(n)\n",
    "            \n",
    "            # H = flat_to_sym(H_flat['flatH'], H_flat['n']).to(device)\n",
    "            # mu = H_flat['mu'].to(device)\n",
    "            # H.add_(mu[None, :] * mu[:, None])\n",
    "            # n_h = H_flat['n']\n",
    "\n",
    "            # # print('before',torch.diag(H).mean())\n",
    "            # H = regularize_H(H, n_h, sigma_reg)\n",
    "            # # print('after',torch.diag(H).mean())\n",
    "            \n",
    "            #############################################\n",
    "            ## _rnormed_scale_cond(scaleWH)\n",
    "            # row normalize\n",
    "            # Wr = W / W.std(dim=1, keepdim=True)\n",
    "            # Wr = W / global_std\n",
    "            \n",
    "            # compute scaleH\n",
    "            # diagH = torch.diag(H)\n",
    "            # diagH = torch.clamp(diagH, min=1e-8)\n",
    "            # scaleH = diagH.sqrt()\n",
    "            \n",
    "            # compute scale\n",
    "            # col_std = Wr.std(dim=0, keepdim=False)\n",
    "            # scale_cond = scaleH / col_std\n",
    "            # scale_cond = scale_cond[None, :]\n",
    "            # scale_cond = scale_cond.repeat(Wr.shape[0], 1)\n",
    "            # assert scale_cond.shape == Wr.shape            \n",
    "            \n",
    "            #############################################\n",
    "            ## _whiten_scale_cond(col_std)\n",
    "            \n",
    "            # U = H_eig['eigenvectors'].to(W.device).to(W.dtype)       # [n, k] (k=n이면 full)\n",
    "            # Lam = H_eig['eigenvalues'].to(W.device).to(W.dtype).flatten()  # [k]\n",
    "\n",
    "            # # sqrt(Λ): η = 1/2\n",
    "            # eps = 1e-12\n",
    "            # sqrtLam = Lam.clamp_min(eps).pow(0.5)                    # [k]\n",
    "            # inv_sqrtLam = Lam.clamp_min(eps).pow(-0.5)               # 복원 시 필요\n",
    "\n",
    "            # # Z = (W @ U) * sqrtLam  (열별 스케일링)\n",
    "            # WU = W @ U                                               # [out, k]\n",
    "            # Z  = WU * sqrtLam                                       \n",
    "\n",
    "            # Wr = Z\n",
    "            # scale_cond = Wr.std(dim = 0, keepdim=True).expand(Wr.shape[0], -1).contiguous()\n",
    "            # # scale_cond = sqrtLam.unsqueeze(0).expand(Wr.shape[0], -1).contiguous()\n",
    "            \n",
    "            # assert scale_cond.shape == Wr.shape\n",
    "\n",
    "            #############################################\n",
    "            ## _rnormed_scale_cond(col_std)\n",
    "            Wr = W / W.std(dim=1, keepdim=True)\n",
    "            \n",
    "            col_std = Wr.std(dim=0, keepdim=True)\n",
    "            scale_cond = col_std\n",
    "            scale_cond = scale_cond.repeat(Wr.shape[0], 1)\n",
    "            assert scale_cond.shape == Wr.shape   \n",
    "            #############################################\n",
    "            \n",
    "            if direction == 'col':\n",
    "                assert Wr.shape[0] % size == 0\n",
    "                w = Wr.T.to('cpu')\n",
    "                s = scale_cond.T.to('cpu')\n",
    "            else:\n",
    "                assert Wr.shape[1] % size == 0\n",
    "                w = Wr.to('cpu')\n",
    "                s = scale_cond.to('cpu')\n",
    "            \n",
    "            if w.shape[-1] % size == 0:\n",
    "                w = w.reshape(-1, size)\n",
    "                s = s.reshape(-1, size)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "            raw_data['weight'].append(w)\n",
    "            raw_data['scale'].append(s)\n",
    "            \n",
    "            idx = torch.tensor([i], dtype = torch.int8)\n",
    "            raw_data['idx'].extend([idx] * w.shape[0])\n",
    "            \n",
    "            layer_type = torch.tensor([wtype_mapping[n]], dtype = torch.int8)\n",
    "            raw_data['layer_type'].extend([layer_type] * w.shape[0])\n",
    "    \n",
    "    for k in raw_data.keys():\n",
    "        # if datas[k][0].shape == torch.Size([]):\n",
    "        #     datas[k] = torch.tensor(datas[k])\n",
    "        # else:\n",
    "        raw_data[k] = torch.cat(raw_data[k], dim = 0)\n",
    "        print(f'{k} total shape: ', raw_data[k].shape)\n",
    "    \n",
    "    ###################### split, compute stats\n",
    "    \n",
    "    indices = torch.randperm(len(raw_data['weight']))\n",
    "    split_index = int(len(raw_data['weight']) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = {}\n",
    "    dataset['val'] = {}\n",
    "    for k in raw_data.keys():\n",
    "        dataset['train'][k] = raw_data[k][train_indices]\n",
    "        dataset['val'][k] = raw_data[k][val_indices]\n",
    "        \n",
    "    print('train Weight: ', dataset['train']['weight'].shape, 'val: ', dataset['val']['weight'].shape)\n",
    "    print('train Scale: ', dataset['train']['scale'].shape, 'val: ', dataset['val']['scale'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        # mean_dim0 = data['weight'].mean(dim=0)\n",
    "        # std_dim0 = data['weight'].std(dim=0)\n",
    "        \n",
    "        mean_all = data['weight'].mean()\n",
    "        std_all = data['weight'].std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': None,\n",
    "            'std_channel': None\n",
    "        }\n",
    "        \n",
    "    ###################### save\n",
    "    \n",
    "    print('Scale stats:', dataset['train']['scale'].mean(), dataset['train']['scale'].min(), dataset['train']['scale'].max())\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_rnormed_scale_cond(scaleWH).pt')\n",
    "    # json_path = f'./block_pt/{model_name}/{direction}_{size}_rnormed_scale_cond(scaleWH)_dataset_stats.json'\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_scale_cond(scaleWH).pt')\n",
    "    # json_path = f'./block_pt/{model_name}/{direction}_{size}_scale_cond(scaleWH)_dataset_stats.json'\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_whiten_scale_cond(col_std).pt')\n",
    "    # json_path = f'./block_pt/{model_name}/{direction}_{size}_whiten_scale_cond(col_std)_dataset_stats.json'\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_rnormed_scale_cond(col_std).pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_rnormed_scale_cond(col_std)_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf'\n",
    "]\n",
    "\n",
    "quip_hess_path = [\n",
    "    './quip_hess/llama3_8b_6144',\n",
    "    # './quip_hess/Hessians-Llama-2-7b-6144',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    # 4096\n",
    "]\n",
    "\n",
    "wtype_mapping = {'self_attn.q_proj': 0, \n",
    "                 'self_attn.k_proj': 1, \n",
    "                 'self_attn.v_proj': 2, \n",
    "                 'self_attn.o_proj': 3, \n",
    "                 'mlp.gate_proj': 4, \n",
    "                 'mlp.up_proj': 5, \n",
    "                 'mlp.down_proj': 6}\n",
    "sigma_reg = 1e-4\n",
    "# direction = 'col'\n",
    "direction = 'row'\n",
    "\n",
    "for model_name, size, quip_hess in zip(model_list, size_list, quip_hess_path):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    raw_data = {}\n",
    "    raw_data['weight'] = []\n",
    "    raw_data['idx'] = []\n",
    "    raw_data['layer_type'] = []\n",
    "    # raw_data['scale'] = []\n",
    "    \n",
    "    scale_cond_dict = {}\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        \n",
    "        hess_dict = {}\n",
    "        hess_dict['qkv'] = torch.load(f'{quip_hess}/{i}_qkv.pt', weights_only=False)\n",
    "        hess_dict['o'] = torch.load(f'{quip_hess}/{i}_o.pt', weights_only=False)\n",
    "        hess_dict['up'] = torch.load(f'{quip_hess}/{i}_up.pt', weights_only=False)\n",
    "        hess_dict['down'] = torch.load(f'{quip_hess}/{i}_down.pt', weights_only=False)\n",
    "        \n",
    "        for n, m in named_linears.items():\n",
    "            \n",
    "            W = m.weight.data.detach().to(device)\n",
    "            \n",
    "            if 'q_proj' in n or 'k_proj' in n or 'v_proj' in n:\n",
    "                H_flat = hess_dict['qkv']\n",
    "            elif 'o_proj' in n:\n",
    "                H_flat = hess_dict['o']\n",
    "            elif 'up_proj' in n or 'gate_proj' in n:\n",
    "                H_flat = hess_dict['up']\n",
    "            elif 'down_proj' in n:\n",
    "                H_flat = hess_dict['down']\n",
    "            else:\n",
    "                raise NotImplementedError(n)\n",
    "            \n",
    "            H = flat_to_sym(H_flat['flatH'], H_flat['n']).to(device)\n",
    "            mu = H_flat['mu'].to(device)\n",
    "            H.add_(mu[None, :] * mu[:, None])\n",
    "            n_h = H_flat['n']\n",
    "\n",
    "            # print('before',torch.diag(H).mean())\n",
    "            H = regularize_H(H, n_h, sigma_reg)\n",
    "            # print('after',torch.diag(H).mean())\n",
    "            \n",
    "            #############################################\n",
    "            \n",
    "            ## row normalize\n",
    "            Wr = W / W.std(dim=1, keepdim=True)\n",
    "            \n",
    "            ## compute scaleH\n",
    "            diagH = torch.diag(H)\n",
    "            diagH = torch.clamp(diagH, min=1e-8)\n",
    "            scaleH = diagH.sqrt()\n",
    "            \n",
    "            ## compute scale\n",
    "            col_std = Wr.std(dim=0, keepdim=False)\n",
    "            scale_cond = scaleH / col_std\n",
    "            scale_cond = scale_cond[None, :]\n",
    "            \n",
    "            scale_cond_dict[f'{}']\n",
    "            \n",
    "            #############################################\n",
    "            \n",
    "            ## 1\n",
    "            # diagH = torch.diag(H)\n",
    "            # diagW2 = torch.diag(W.T @ W)\n",
    "            # diagH = torch.clamp(diagH, min=1e-8)\n",
    "            # diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "            # scaleWH = (diagH / diagW2).sqrt()\n",
    "            # scaleWH = diagH.sqrt()\n",
    "            # scaleWH = scaleWH.clamp(min=1)\n",
    "            # print(scaleWH.numel())\n",
    "            # print((scaleWH>=1).sum() / scaleWH.numel())\n",
    "            # fig = plt.figure()\n",
    "            # ax = fig.add_subplot(111)\n",
    "            # ax.plot(scaleWH.cpu().numpy())\n",
    "            # plt.show\n",
    "            \n",
    "            # (M, N) = W.shape\n",
    "            # SU = (torch.randn(N).sign() + 1e-5).sign().to(device)\n",
    "            # SV = (torch.randn(M).sign() + 1e-5).sign().to(device)\n",
    "            # Wr = RHT_W(Wr, SU, SV)\n",
    "\n",
    "            ## 2\n",
    "            # diagH = torch.diag(H)\n",
    "            # diagH = torch.clamp(diagH, min=1e-8)\n",
    "            # scaleWH = diagH.sqrt()\n",
    "            # Wr = W * scaleWH[None, :]\n",
    "            # # W_normalized = Wr / Wr.norm(p=2, dim=1, keepdim=True)\n",
    "            # W_normalized = Wr / Wr.std(dim=1, keepdim=True)\n",
    "            # Wr = W_normalized\n",
    "            \n",
    "            ## 4 invH\n",
    "            # Lhr = torch.linalg.cholesky(H)\n",
    "            # H_inv = torch.cholesky_inverse(Lhr)\n",
    "            # diagH_inv = torch.diag(H_inv)\n",
    "            # scaleWH = 1/diagH_inv\n",
    "            # scaleWH = torch.clamp(scaleWH, min=1e-8)\n",
    "            # scaleWH = scaleWH.sqrt()\n",
    "            # Wr = W * scaleWH[None, :]\n",
    "            # # W_normalized = Wr / Wr.norm(p=2, dim=1, keepdim=True)\n",
    "            # W_normalized = Wr / Wr.std(dim=1, keepdim=True)\n",
    "            # Wr = W_normalized\n",
    "            \n",
    "            ## 3\n",
    "            # W_normalized = W / W.norm(p=2, dim=1, keepdim=True)\n",
    "            # Wr = W_normalized            \n",
    "            \n",
    "            # print(Wr.norm(p=2, dim=1)[:10])\n",
    "            \n",
    "            ## 6 \n",
    "            # diagH = torch.diag(H)\n",
    "            # diagH = torch.clamp(diagH, min=1e-8)\n",
    "            # scaleWH = diagH.sqrt()\n",
    "            # Wr = W * scaleWH[None, :]\n",
    "            \n",
    "            # ## 5 with col std\n",
    "            # col_std = Wr.std(dim=0, keepdim=True)\n",
    "            # col_std = col_std.expand(Wr.shape[0], Wr.shape[1])\n",
    "\n",
    "            ## 7 with col-row scale\n",
    "            # col_std = Wr.std(dim=0, keepdim=True)\n",
    "            # row_std = (Wr/col_std).std(dim=1, keepdim=True)\n",
    "            # col_std = row_std @ col_std\n",
    "            # assert col_std.shape == Wr.shape            \n",
    "                        \n",
    "            ## 8 관찰용\n",
    "            # diagH = torch.diag(H)\n",
    "            # diagH = torch.clamp(diagH, min=1e-8)\n",
    "            # scaleWH = diagH.sqrt()\n",
    "            # print('scaleWH', scaleWH.mean(), scaleWH.max(), scaleWH.min())\n",
    "\n",
    "            # W_normalized = W / W.std(dim=1, keepdim=True)\n",
    "            # col_std_ = W_normalized.std(dim=0, keepdim=False)\n",
    "            # print('col_std_', col_std_.mean(), col_std_.max(), col_std_.min())\n",
    "            \n",
    "            # Wr = W * scaleWH[None, :]\n",
    "            # row_std = Wr.std(dim=1, keepdim=True)\n",
    "\n",
    "            # W_normalized = W / row_std\n",
    "            # Wr = W_normalized\n",
    "\n",
    "            # col_std_ = Wr.std(dim=0, keepdim=False)\n",
    "            # col_std = scaleWH / col_std_\n",
    "            # # print(col_std.shape)\n",
    "            # col_std = col_std[None, :]\n",
    "            # # print(col_std.shape)      \n",
    "            \n",
    "            if direction == 'col':\n",
    "                w = Wr.T.to('cpu')\n",
    "                s = scale_cond.T.to('cpu')\n",
    "            else:\n",
    "                w = Wr.to('cpu')\n",
    "                s = scale_cond.to('cpu')\n",
    "            \n",
    "            if w.shape[-1] % size == 0:\n",
    "                w = w.reshape(-1, size)\n",
    "                s = s.reshape(-1, size)\n",
    "            else:\n",
    "                raise\n",
    "                D = w.shape[1]\n",
    "                span = size * (D //size)\n",
    "                if D < span:\n",
    "                    raise ValueError(\"Tensor's second dimension is too small for the requested slice.\")\n",
    "                max_start = D - span\n",
    "                start = random.randint(0, max_start)\n",
    "                sliced = w[:, start : start + span]\n",
    "                w = sliced.reshape(-1, size)\n",
    "\n",
    "            # datas.append(w)\n",
    "\n",
    "            raw_data['weight'].append(w)\n",
    "            raw_data['scale'].append(s)\n",
    "            \n",
    "            idx = torch.tensor([i], dtype = torch.int8)\n",
    "            raw_data['idx'].extend([idx] * w.shape[0])\n",
    "            \n",
    "            layer_type = torch.tensor([wtype_mapping[n]], dtype = torch.int8)\n",
    "            raw_data['layer_type'].extend([layer_type] * w.shape[0])\n",
    "    \n",
    "    for k in raw_data.keys():\n",
    "        # if datas[k][0].shape == torch.Size([]):\n",
    "        #     datas[k] = torch.tensor(datas[k])\n",
    "        # else:\n",
    "        raw_data[k] = torch.cat(raw_data[k], dim = 0)\n",
    "        print(f'{k} total shape: ', raw_data[k].shape)\n",
    "    \n",
    "    ###################### split, compute stats\n",
    "    \n",
    "    indices = torch.randperm(len(raw_data['weight']))\n",
    "    split_index = int(len(raw_data['weight']) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = {}\n",
    "    dataset['val'] = {}\n",
    "    for k in raw_data.keys():\n",
    "        dataset['train'][k] = raw_data[k][train_indices]\n",
    "        dataset['val'][k] = raw_data[k][val_indices]\n",
    "        \n",
    "    print('train Weight: ', dataset['train']['weight'].shape, 'val: ', dataset['val']['weight'].shape)\n",
    "    print('train Scale: ', dataset['train']['scale'].shape, 'val: ', dataset['val']['scale'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        # mean_dim0 = data['weight'].mean(dim=0)\n",
    "        # std_dim0 = data['weight'].std(dim=0)\n",
    "        \n",
    "        mean_all = data['weight'].mean()\n",
    "        std_all = data['weight'].std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': None,\n",
    "            'std_channel': None\n",
    "        }\n",
    "    \n",
    "    ###################### split, compute stats V0\n",
    "    \n",
    "    # datas = torch.cat(datas, dim = 0)\n",
    "    # print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    # indices = torch.randperm(len(datas))\n",
    "    # split_index = int(len(datas) - 1000)\n",
    "    # train_indices = indices[:split_index]\n",
    "    # val_indices = indices[split_index:]\n",
    "\n",
    "    # dataset = {}\n",
    "    # dataset['train'] = datas[train_indices]\n",
    "    # dataset['val'] = datas[val_indices]\n",
    "    # print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    # dataset_stats = {}\n",
    "    # for split in ['train', 'val']:\n",
    "    #     data = dataset[split]\n",
    "        \n",
    "    #     # mean_dim0 = data.mean(dim=0)\n",
    "    #     # std_dim0 = data.std(dim=0)\n",
    "        \n",
    "    #     mean_all = data.mean()\n",
    "    #     std_all = data.std()\n",
    "        \n",
    "    #     dataset_stats[split] = {\n",
    "    #         'mean': mean_all.item(),\n",
    "    #         'std': std_all.item(),\n",
    "    #         # 'mean_channel': mean_dim0.tolist(),\n",
    "    #         # 'std_channel': std_dim0.tolist(),\n",
    "    #         'mean_channel': None,\n",
    "    #         'std_channel': None,\n",
    "    #     }\n",
    "    \n",
    "    ###################### save\n",
    "    \n",
    "    print('Scale stats:', dataset['train']['scale'].mean(), dataset['train']['scale'].min(), dataset['train']['scale'].max())\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_rnormed_scale_cond(scaleWH).pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_rnormed_scale_cond(scaleWH)_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)\n",
    "    \n",
    "    ## 1\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaled3_RHT_sig{sigma_reg}_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaled3_RHT_sig{sigma_reg}_{direction}_{size}_dataset_stats.json'\n",
    "    ## 2\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_std_rnormed_lidx_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_std_rnormed_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    ## 3\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/rnormed_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/rnormed_{direction}_{size}_dataset_stats.json'\n",
    "    ## 4\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaleHinv_sig{sigma_reg}_std_rnormed_lidx_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaleHinv_sig{sigma_reg}_std_rnormed_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    ## 5\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_std_rnormed_with_col_std_lidx_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_std_rnormed_with_col_std_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    ## 6\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_with_col_std_lidx_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_with_col_std_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    ## 7\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_with_colrow_std_lidx_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_with_colrow_std_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    # with open(json_path, 'w') as f:\n",
    "    #     json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = datas['scale']\n",
    "print(f\"Statistics for 'scale' column:\")\n",
    "print(f\"  - Minimum value: {scale.min():.6f}\")\n",
    "print(f\"  - Maximum value: {scale.max():.6f}\")\n",
    "print(f\"  - Mean value: {scale.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics for 'scale' column:\n",
    "  - Minimum value: 1.758009\n",
    "  - Maximum value: 29573.744141\n",
    "  - Mean value: 78.177910\n",
    "\n",
    "Statistics for 'scale' column:\n",
    "  - Minimum value: 0.012938\n",
    "  - Maximum value: 141.257156\n",
    "  - Mean value: 0.908103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "# path = '/workspace/Weight_compression/Wparam_dataset/block_pt/meta-llama--Meta-Llama-3-8B/scaleH_sig0.0001_std_rnormed_with_col_std_lidx_row_1024.pt'\n",
    "# # path = '/workspace/Weight_compression/Wparam_dataset/block_pt/meta-llama--Meta-Llama-3-8B/scaleH_sig0.0001_with_colrow_std_lidx_row_1024.pt'\n",
    "# dataset = torch.load(path)\n",
    "\n",
    "scale_data = np.array(dataset['train']['scale'].flatten())\n",
    "# 2. Calculate min and max values\n",
    "min_val = scale_data.min()\n",
    "max_val = scale_data.max()\n",
    "\n",
    "# 3. Print the results\n",
    "print(f\"Statistics for 'scale' column:\")\n",
    "print(f\"  - Minimum value: {min_val:.6f}\")\n",
    "print(f\"  - Maximum value: {max_val:.6f}\")\n",
    "print(f\"  - Mean value: {scale_data.mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics for 'scale' column:\n",
    "  - Minimum value: 0.000049\n",
    "  - Maximum value: 34.011650\n",
    "  - Mean value: 0.938580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_data = np.array(dataset['train']['scale'].flatten())\n",
    "\n",
    "WEIGHT_SAMPLE_SIZE = 10000 # Number of weights to sample from each matrix\n",
    "scale_data = np.random.choice(scale_data, WEIGHT_SAMPLE_SIZE, replace=False)\n",
    "\n",
    "# 2. Calculate min and max values\n",
    "min_val = scale_data.min()\n",
    "max_val = scale_data.max()\n",
    "\n",
    "# 3. Print the results\n",
    "print(f\"Statistics for 'scale' column:\")\n",
    "print(f\"  - Minimum value: {min_val:.6f}\")\n",
    "print(f\"  - Maximum value: {max_val:.6f}\")\n",
    "\n",
    "# 4. Plot the distribution\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Using seaborn's histplot which is great for distributions\n",
    "sns.histplot(scale_data, kde=True, ax=ax, bins=50)\n",
    "\n",
    "ax.set_title(\"Distribution of 'scale' values\", fontsize=16)\n",
    "ax.set_xlabel(\"Scale Value\", fontsize=12)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "# Add vertical lines for min and max for better visualization\n",
    "ax.axvline(min_val, color='red', linestyle='--', label=f'Min: {min_val:.4f}')\n",
    "ax.axvline(max_val, color='green', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print std, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def describe_distribution(x):\n",
    "    assert isinstance(x, torch.Tensor), \"Input must be a PyTorch tensor\"\n",
    "    x = x.flatten().float()\n",
    "    n = x.numel()\n",
    "    \n",
    "    # 중심 경향\n",
    "    mean = x.mean()\n",
    "    median = x.median()\n",
    "\n",
    "    # 산포도\n",
    "    std_dev = x.std(unbiased=False)\n",
    "    value_range = x.max() - x.min()\n",
    "    q1 = x.kthvalue(int(0.25 * n + 1)).values\n",
    "    q3 = x.kthvalue(int(0.75 * n + 1)).values\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # 모양\n",
    "    skewness = ((x - mean)**3).mean() / (std_dev**3)\n",
    "    kurtosis = ((x - mean)**4).mean() / (std_dev**4) - 3  # Fisher's definition\n",
    "\n",
    "    del x\n",
    "    return {\n",
    "        \"mean\": mean.item(),\n",
    "        \"median\": median.item(),\n",
    "        \"std\": std_dev.item(),\n",
    "        \"range\": value_range.item(),\n",
    "        \"iqr\": iqr.item(),\n",
    "        \"skewness\": skewness.item(),\n",
    "        \"kurtosis\": kurtosis.item()\n",
    "    }\n",
    "\n",
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "\n",
    "for model_name in model_list:\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "    # std = defaultdict(list)\n",
    "    # mean = defaultdict(list)\n",
    "    stats = {}\n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.to('cuda:0')\n",
    "            r = describe_distribution(W)\n",
    "            for k,v in r.items():\n",
    "                stats[f'{i}_{n}_{k}'] = v\n",
    "            \n",
    "    fig, ax = plt.subplots(1, len(r), figsize=(15, 3))\n",
    "    \n",
    "    for idx, k in enumerate(r.keys()):\n",
    "        ax[idx].set_title(k)\n",
    "        for n, m in named_linears.items():\n",
    "            list = []\n",
    "            for i in range(len(layers)):\n",
    "                list.append(stats[f'{i}_{n}_{k}'])\n",
    "            ax[idx].plot(list, label=n)\n",
    "        # ax[idx].legend()\n",
    "        ax[idx].set_xlabel('Layer')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block with idx ltype stats\n",
    "droplast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "#     'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024\n",
    "]\n",
    "\n",
    "wtype_mapping = {'self_attn.q_proj': 0, \n",
    "                 'self_attn.k_proj': 1, \n",
    "                 'self_attn.v_proj': 2, \n",
    "                 'self_attn.o_proj': 3, \n",
    "                 'mlp.gate_proj': 4, \n",
    "                 'mlp.up_proj': 5, \n",
    "                 'mlp.down_proj': 6}\n",
    "\n",
    "direction = 'col'\n",
    "shuffle = False\n",
    "drop_last = True\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    datas = {}\n",
    "    datas['weight'] = []\n",
    "    datas['idx'] = []\n",
    "    datas['layer_type'] = []\n",
    "    datas['mean'] = []\n",
    "    datas['median'] = []\n",
    "    datas['std'] = []\n",
    "    datas['range'] = []\n",
    "    datas['iqr'] = []\n",
    "    datas['skewness'] = []\n",
    "    datas['kurtosis'] = []\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            w = m.weight.data.detach()\n",
    "            \n",
    "            stats = describe_distribution(w)\n",
    "            \n",
    "            if direction == 'col':\n",
    "                w = w.T\n",
    "\n",
    "            if shuffle:\n",
    "                w = w.reshape(-1, 16)  # (num_rows, 16)\n",
    "                indices = torch.randperm(w.size(0))  # row 인덱스 섞기\n",
    "                w = w[indices]  # 섞인 순서대로 row 재배열\n",
    "                \n",
    "            if drop_last:\n",
    "                (r, c) = w.shape\n",
    "                d = (c // size) * size\n",
    "                w = w[:, :d]\n",
    "\n",
    "            w = w.reshape(-1, size)\n",
    "            \n",
    "            if drop_last:\n",
    "                assert w.shape[0] % r == 0\n",
    "                \n",
    "            # if w.size(0) % size == 0:\n",
    "            #     w = w.T    \n",
    "            #     w = w.reshape(-1, size)\n",
    "            # else:\n",
    "            #     w = w.reshape(-1, size)\n",
    "                \n",
    "            datas['weight'].append(w)\n",
    "            \n",
    "            idx = torch.tensor([i], dtype = torch.int8)\n",
    "            datas['idx'].extend([idx] * w.shape[0])\n",
    "            \n",
    "            layer_type = torch.tensor([wtype_mapping[n]], dtype = torch.int8)\n",
    "            datas['layer_type'].extend([layer_type] * w.shape[0])\n",
    "            \n",
    "            for k, v in stats.items():\n",
    "                val_tensor = torch.tensor([v], dtype=torch.float32)\n",
    "                datas[k].extend([val_tensor] * w.shape[0])\n",
    "    \n",
    "    for k in datas.keys():\n",
    "        # if datas[k][0].shape == torch.Size([]):\n",
    "        #     datas[k] = torch.tensor(datas[k])\n",
    "        # else:\n",
    "        datas[k] = torch.cat(datas[k], dim = 0)\n",
    "    \n",
    "    print('total weight shape: ', datas['weight'].shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas['weight']))\n",
    "    split_index = int(len(datas['weight']) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = {}\n",
    "    dataset['val'] = {}\n",
    "    for k in datas.keys():\n",
    "        dataset['train'][k] = datas[k][train_indices]\n",
    "        dataset['val'][k] = datas[k][val_indices]\n",
    "        \n",
    "    print('train: ', dataset['train']['weight'].shape, 'val: ', dataset['val']['weight'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        mean_dim0 = data['weight'].mean(dim=0)\n",
    "        std_dim0 = data['weight'].std(dim=0)\n",
    "        \n",
    "        mean_all = data['weight'].mean()\n",
    "        std_all = data['weight'].std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': mean_dim0.tolist(),\n",
    "            'std_channel': std_dim0.tolist(),\n",
    "        }\n",
    "\n",
    "    # os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_layerwise_stats.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/{direction}_{size}_layerwise_stats_dataset_stats.json'\n",
    "    # with open(json_path, 'w') as f:\n",
    "    #     json.dump(dataset_stats, f)\n",
    "        \n",
    "    sub = ''\n",
    "    if shuffle:\n",
    "        sub += 'shuffled_'\n",
    "    if drop_last:\n",
    "        sub += 'droplast_'\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{sub}{direction}_{size}_idx_ltype_stats.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{sub}{direction}_{size}_idx_ltype_stats_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬 크기 설정 (예: 5x3 행렬)\n",
    "rows, cols = 1000, 3\n",
    "\n",
    "# 열별 평균과 표준편차 정의\n",
    "mean = torch.tensor([0.0, 2.0, -1.0])  # 열별 평균값\n",
    "std = torch.tensor([1.0, 0.5, 2.0])    # 열별 표준편차\n",
    "\n",
    "# 가우시안 랜덤 행렬 생성 (브로드캐스팅 활용)\n",
    "random_matrix = torch.normal(mean.expand(rows, cols), std.expand(rows, cols))\n",
    "\n",
    "print(random_matrix)\n",
    "print(random_matrix.mean(0))\n",
    "print(random_matrix.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai/clip-vit-large-patch14',\n",
    "    \n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    4096,\n",
    "    # 5120//4,\n",
    "]\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    # model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)[0:1]\n",
    "\n",
    "    if not isinstance(layers, dict):\n",
    "        layers = {'': layers}\n",
    "\n",
    "    for k, v in layers.items():\n",
    "        print(k)\n",
    "        named_linears = get_named_linears(v)\n",
    "        for n, m in named_linears.items():\n",
    "            print(n, m.weight.data.shape)\n",
    "            W = m.weight.data\n",
    "            r, c = W.shape\n",
    "\n",
    "        # mean_c = W.mean(0)\n",
    "        # std_c = W.std(0)\n",
    "        # if r % size != 0:\n",
    "        #     padding_size = size - r % size\n",
    "        #     g = torch.normal(mean_c.expand(padding_size, c), std_c.expand(padding_size, c))\n",
    "        #     W = torch.cat([W, g], dim=0)\n",
    "        #     print('padding')\n",
    "        #     print(n, W.shape)\n",
    "        #     print(mean_c[:5], std_c[:5])\n",
    "        #     print(W.mean(0)[:5], W.std(0)[:5])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block\n",
    "### layerwise suffle\n",
    "레이어 안에서 16 block으로 섞음\n",
    "### drop last\n",
    "### 8b + 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai--clip-vit-large-patch14',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    1024,\n",
    "    # 4096,\n",
    "    # 256,\n",
    "]\n",
    "\n",
    "direction = 'col'\n",
    "shuffle = False\n",
    "drop_last = True\n",
    "modelwise_norm = True\n",
    "\n",
    "model_datas = []\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    # model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    # layers_ = get_blocks(model)    \n",
    "    layers = get_blocks(model)    \n",
    "\n",
    "    # datas = []    \n",
    "    # for sub in layers_:\n",
    "        # layers = layers_[sub]\n",
    "    # if sub == 'vision':\n",
    "    #     size = 1024\n",
    "    # elif sub == 'text':\n",
    "    #     size = 768\n",
    "    # else:\n",
    "    #     raise\n",
    "    \n",
    "    datas = []\n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            w = m.weight.data.detach()\n",
    "            \n",
    "            # if w.size(0) % size == 0:\n",
    "            #     w = w.T    \n",
    "            #     w = w.reshape(-1, size)\n",
    "            # else:\n",
    "            #     w = w.reshape(-1, size)\n",
    "            \n",
    "            if direction == 'col':\n",
    "                w = w.T\n",
    "\n",
    "            if shuffle:\n",
    "                w = w.reshape(-1, 16)  # (num_rows, 16)\n",
    "                indices = torch.randperm(w.size(0))  # row 인덱스 섞기\n",
    "                w = w[indices]  # 섞인 순서대로 row 재배열\n",
    "                \n",
    "            if drop_last:\n",
    "                (r, c) = w.shape\n",
    "                d = (c // size) * size\n",
    "                w = w[:, :d]\n",
    "\n",
    "            w = w.reshape(-1, size)\n",
    "            \n",
    "            if drop_last:\n",
    "                assert w.shape[0] % r == 0\n",
    "\n",
    "            datas.append(w)\n",
    "        \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    if modelwise_norm:\n",
    "        mu = datas.mean()\n",
    "        std = datas.std()\n",
    "        datas = (datas - mu) / std\n",
    "        \n",
    "    model_datas.append(datas)\n",
    "    \n",
    "model_name = 'llama8b+7b'\n",
    "model_datas = torch.cat(model_datas, dim = 0)\n",
    "print('total dataset shape: ', model_datas.shape)\n",
    "datas = model_datas\n",
    "\n",
    "indices = torch.randperm(len(datas))\n",
    "split_index = int(len(datas) - 1000)\n",
    "train_indices = indices[:split_index]\n",
    "val_indices = indices[split_index:]\n",
    "\n",
    "dataset = {}\n",
    "dataset['train'] = datas[train_indices]\n",
    "dataset['val'] = datas[val_indices]\n",
    "print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "dataset_stats = {}\n",
    "for split in ['train', 'val']:\n",
    "    data = dataset[split]\n",
    "    \n",
    "    dataset_stats[split] = {\n",
    "        'mean': data.mean().item(),\n",
    "        'std': data.std().item(),\n",
    "        # 'mean_channel': data.mean(dim=0).tolist(),\n",
    "        # 'std_channel': data.std(dim=0).tolist(),\n",
    "    }\n",
    "\n",
    "# if sub != '':\n",
    "#     sub = sub+ '_'\n",
    "# sub = 'vision_text_'\n",
    "sub = ''\n",
    "if shuffle:\n",
    "    sub += 'shuffled_'\n",
    "if drop_last:\n",
    "    sub += 'droplast_'\n",
    "if modelwise_norm:\n",
    "    sub += 'modelwise_norm2_'\n",
    "os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "torch.save(dataset, f'./block_pt/{model_name}/{sub}{direction}_{size}.pt')\n",
    "json_path = f'./block_pt/{model_name}/{sub}{direction}_{size}_dataset_stats.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gaussian block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.load('/workspace/Weight_compression/Wparam_dataset/block_pt/meta-llama--Meta-Llama-3-8B/col_1024_gaussian_padding.pt')\n",
    "print(d['train'].shape)\n",
    "print(d['val'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataset['train'] = torch.normal(mean=0.0, std=1.0, size=d['train'].shape)\n",
    "dataset['val'] = torch.normal(mean=0.0, std=1.0, size=d['val'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "dataset_stats = {}\n",
    "for split in ['train', 'val']:\n",
    "    data = dataset[split]\n",
    "    \n",
    "    mean_all = data.mean()\n",
    "    std_all = data.std()\n",
    "    \n",
    "    dataset_stats[split] = {\n",
    "        'mean': mean_all.item(),\n",
    "        'std': std_all.item(),\n",
    "    }\n",
    "\n",
    "sub = 'llama8b_'\n",
    "direction = 'col'\n",
    "size = 1024\n",
    "os.makedirs(f'./block_pt/gaussian', exist_ok = True)\n",
    "torch.save(dataset, f'./block_pt/gaussian/{sub}{direction}_{size}.pt')\n",
    "json_path = f'./block_pt/gaussian/{sub}{direction}_{size}_dataset_stats.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layerwise, channelwise normalized Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai--clip-vit-large-patch14',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    # 4096,\n",
    "    # 4096,\n",
    "    # 256,\n",
    "]\n",
    "\n",
    "# direction = 'adapt'\n",
    "direction = 'col'\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    # model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "        \n",
    "    datas = []\n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.detach().cuda()\n",
    "\n",
    "            W = (W - W.mean(dim=0, keepdim=True)) / W.std(dim=0, keepdim=True)\n",
    "            # W = (W - W.mean()) / W.std()\n",
    "            \n",
    "            if direction == 'col':\n",
    "                W = W.T    \n",
    "            W = W.reshape(-1, size).cpu()\n",
    "                \n",
    "            datas.append(W)\n",
    "        \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        # mean_dim0 = data.mean(dim=0)\n",
    "        # std_dim0 = data.std(dim=0)        \n",
    "        # mean_all = data.mean()\n",
    "        # std_all = data.std()\n",
    "        \n",
    "        # dataset_stats[split] = {\n",
    "        #     'mean': mean_all.item(),\n",
    "        #     'std': std_all.item(),\n",
    "        #     'mean_channel': mean_dim0.tolist(),\n",
    "        #     'std_channel': std_dim0.tolist(),\n",
    "        # }\n",
    "        dataset_stats[split] = {\n",
    "            'mean': 0,\n",
    "            'std': 1,\n",
    "            'mean_channel': None,\n",
    "            'std_channel': None,\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_colwise_normed.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_colwise_normed_dataset_stats.json'\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_layerwise_normed.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/{direction}_{size}_layerwise_normed_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gaussian padding\n",
    "\n",
    "size가 안 맞으면 같은 row 나 col의 mean, std를 갖는 가우시안으로 padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    # 'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    # 1024,\n",
    "    1024,\n",
    "    1280,\n",
    "]\n",
    "\n",
    "# direction = 'adapt'\n",
    "direction = 'col'\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    datas = []\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.detach()\n",
    "\n",
    "            r, c = W.shape\n",
    "\n",
    "            if direction == 'col':\n",
    "                if r % size != 0:\n",
    "                    padding_size = size - r % size\n",
    "                    mean_c = W.mean(0)\n",
    "                    std_c = W.std(0)\n",
    "\n",
    "                    g = torch.normal(mean_c.expand(padding_size, c), std_c.expand(padding_size, c))\n",
    "                    W = torch.cat([W, g], dim=0)\n",
    "            elif direction =='row':\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                raise KeyError\n",
    "            \n",
    "            if direction == 'col':\n",
    "                W = W.T\n",
    "            \n",
    "            assert W.shape[1] % size == 0\n",
    "            assert W.shape[1] >= size\n",
    "\n",
    "            W = W.reshape(-1, size)\n",
    "                \n",
    "            datas.append(W)\n",
    "    \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        mean_dim0 = data.mean(dim=0)\n",
    "        std_dim0 = data.std(dim=0)\n",
    "        \n",
    "        mean_all = data.mean()\n",
    "        std_all = data.std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': mean_dim0.tolist(),\n",
    "            'std_channel': std_dim0.tolist(),\n",
    "        }\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_gaussian_padding.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_gaussian_padding_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RHT smoothed Weight Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/Weight_compression/Wparam_dataset')\n",
    "from utils import *\n",
    "\n",
    "def RHT_H(H, SU):\n",
    "    return matmul_hadUt(matmul_hadUt(H * SU).T * SU)\n",
    "\n",
    "\n",
    "def RHT_W(W, SU, SV):\n",
    "    return matmul_hadUt(matmul_hadUt(W.T * SV).T * SU)\n",
    "\n",
    "\n",
    "def incoherence_preprocess(H, W, args):\n",
    "    # dtype_ = torch.float64 if args.use_fp64 else torch.float32\n",
    "    dtype_ = torch.float32\n",
    "    device = W.device\n",
    "    # device = torch.device('cpu')\n",
    "    (m, n) = W.shape\n",
    "\n",
    "    def _dump(Hr, Lhr, msg=''):\n",
    "        torch.save(Hr, f\"{args.save_pfx}/Hr_debug_fft.pt\")\n",
    "        torch.save(Lhr, f\"{args.save_pfx}/Lhr_debug_fft.pt\")\n",
    "        raise Exception(msg)\n",
    "\n",
    "    # diagonally rescale W,H to minimize proxy loss\n",
    "    scaleWH = None\n",
    "    Wr = W\n",
    "    Hr = H\n",
    "    # if args.rescale_WH:\n",
    "    if False:\n",
    "        Hr = H / H.abs().max()\n",
    "        diagH = torch.diag(Hr)\n",
    "        diagW2 = torch.diag(W.T @ W)\n",
    "        diagH = torch.clamp(diagH, min=1e-8)\n",
    "        diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "        scaleWH = (diagH / diagW2).sqrt().sqrt().to(torch.float32)\n",
    "        scaleWH = scaleWH.clamp(min=1e-8)\n",
    "        Wr = Wr * scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[:, None]\n",
    "        scaleWH = scaleWH.cpu()\n",
    "\n",
    "    # randomized hadamard transformation on H, W\n",
    "    if True:\n",
    "        SU = (torch.randn(n, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        SV = (torch.randn(m, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        # Hr = RHT_H(Hr, SU)\n",
    "        Wr = RHT_W(Wr, SU, SV)\n",
    "    # randomized kronecker product on H, W\n",
    "    elif args.incoh_mode == \"kron\":\n",
    "        SU = utils.rand_ortho_butterfly_noblock(n).to(dtype_).to(device)\n",
    "        SV = utils.rand_ortho_butterfly_noblock(m).to(dtype_).to(device)\n",
    "        Hr = SU @ Hr @ SU.T\n",
    "        Wr = SV @ Wr @ SU.T\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    SV = SV.cpu()\n",
    "    SU = SU.cpu()\n",
    "\n",
    "    # Lhr = torch.linalg.cholesky(Hr)\n",
    "    Lhr = None\n",
    "    # if not torch.all(torch.isfinite(Lhr)):\n",
    "    #     return None\n",
    "\n",
    "    Wr = Wr.to(device)\n",
    "\n",
    "    return Lhr, Hr, Wr, SU, SV, scaleWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    4096,\n",
    "    # 4096,\n",
    "    # 4096,\n",
    "]\n",
    "\n",
    "# direction = 'adapt'\n",
    "direction = 'col'\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    datas = []\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            w = m.weight.data.detach()\n",
    "            \n",
    "            Lhr, H, w, SU, SV, scaleWH = incoherence_preprocess(None, w, None) \n",
    "            \n",
    "            if direction == 'col':\n",
    "                w = w.T\n",
    "            \n",
    "            # if w.size(0) % size == 0:\n",
    "            #     w = w.T    \n",
    "            #     w = w.reshape(-1, size)\n",
    "            # else:\n",
    "            #     w = w.reshape(-1, size)\n",
    "                \n",
    "            w = w.reshape(-1, size)\n",
    "            \n",
    "            datas.append(w)\n",
    "    \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        mean_dim0 = data.mean(dim=0)\n",
    "        std_dim0 = data.std(dim=0)\n",
    "        \n",
    "        mean_all = data.mean()\n",
    "        std_all = data.std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': mean_dim0.tolist(),\n",
    "            'std_channel': std_dim0.tolist(),\n",
    "        }\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_RHT.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_RHT_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
