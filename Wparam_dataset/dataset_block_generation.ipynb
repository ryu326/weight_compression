{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, OPTForCausalLM, BloomForCausalLM\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_named_linears(module):\n",
    "    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}\n",
    "\n",
    "def get_blocks(model):\n",
    "    if model.__class__.__name__ in (\"LlamaForCausalLM\", \"Qwen2ForCausalLM\"):\n",
    "        layers = model.model.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaForCausalLM\":\n",
    "        layers = model.model.layers\n",
    "    elif isinstance(model, OPTForCausalLM):\n",
    "        layers = model.model.decoder.layers\n",
    "    elif isinstance(model, BloomForCausalLM):\n",
    "        layers = model.transformer.h\n",
    "    elif \"mpt\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.blocks\n",
    "    elif \"falcon\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"bigcode\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"neox\" in str(model.__class__).lower():\n",
    "        layers = model.gpt_neox.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaModel\":\n",
    "        layers = model.llm.model.layers\n",
    "    elif model.__class__.__name__ in (\"CLIPModel\"):\n",
    "        vision_layers = model.vision_model.encoder.layers\n",
    "        text_layers = model.text_model.encoder.layers\n",
    "        layers = {'vision': vision_layers,\n",
    "                  'text': text_layers}\n",
    "    else:\n",
    "        raise NotImplementedError(type(model))\n",
    "    # if not isinstance(layers, dict):\n",
    "    #     layers = {'': layers}\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dignal Scaled RHT Weight Block\n",
    "\n",
    "많이 수정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_to_sym(V, N):\n",
    "    A = torch.zeros(N, N, dtype=V.dtype, device=V.device)\n",
    "    idxs = torch.tril_indices(N, N, device=V.device)\n",
    "    A[idxs.unbind()] = V\n",
    "    A[idxs[1, :], idxs[0, :]] = V\n",
    "    return A\n",
    "\n",
    "def regularize_H(H, n, sigma_reg):\n",
    "    H.div_(torch.diag(H).mean())\n",
    "    idx = torch.arange(n)\n",
    "    H[idx, idx] += sigma_reg\n",
    "    return H\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/Weight_compression/Wparam_dataset')\n",
    "from utils import *\n",
    "\n",
    "def RHT_H(H, SU):\n",
    "    return matmul_hadUt(matmul_hadUt(H * SU).T * SU)\n",
    "\n",
    "\n",
    "def RHT_W(W, SU, SV):\n",
    "    return matmul_hadUt(matmul_hadUt(W.T * SV).T * SU)\n",
    "\n",
    "\n",
    "def incoherence_preprocess(H, W, args):\n",
    "    # dtype_ = torch.float64 if args.use_fp64 else torch.float32\n",
    "    dtype_ = torch.float32\n",
    "    device = H.device\n",
    "    # device = torch.device('cpu')\n",
    "    (m, n) = H.shape\n",
    "\n",
    "    def _dump(Hr, Lhr, msg=''):\n",
    "        torch.save(Hr, f\"{args.save_pfx}/Hr_debug_fft.pt\")\n",
    "        torch.save(Lhr, f\"{args.save_pfx}/Lhr_debug_fft.pt\")\n",
    "        raise Exception(msg)\n",
    "\n",
    "    # diagonally rescale W,H to minimize proxy loss\n",
    "    scaleWH = None\n",
    "    Wr = W\n",
    "    Hr = H\n",
    "    # if args.rescale_WH:\n",
    "    if False:\n",
    "        Hr = H / H.abs().max()\n",
    "        diagH = torch.diag(Hr)\n",
    "        diagW2 = torch.diag(W.T @ W)\n",
    "        diagH = torch.clamp(diagH, min=1e-8)\n",
    "        diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "        scaleWH = (diagH / diagW2).sqrt().sqrt().to(torch.float32)\n",
    "        scaleWH = scaleWH.clamp(min=1e-8)\n",
    "        Wr = Wr * scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[:, None]\n",
    "        scaleWH = scaleWH.cpu()\n",
    "\n",
    "    # randomized hadamard transformation on H, W\n",
    "    if True:\n",
    "        SU = (torch.randn(n, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        SV = (torch.randn(m, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        Hr = RHT_H(Hr, SU)\n",
    "        # Wr = RHT_W(Wr, SU, SV)\n",
    "    # randomized kronecker product on H, W\n",
    "    elif args.incoh_mode == \"kron\":\n",
    "        SU = utils.rand_ortho_butterfly_noblock(n).to(dtype_).to(device)\n",
    "        SV = utils.rand_ortho_butterfly_noblock(m).to(dtype_).to(device)\n",
    "        Hr = SU @ Hr @ SU.T\n",
    "        Wr = SV @ Wr @ SU.T\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    SV = SV.cpu()\n",
    "    SU = SU.cpu()\n",
    "\n",
    "    # Lhr = torch.linalg.cholesky(Hr)\n",
    "    Lhr = None\n",
    "    # if not torch.all(torch.isfinite(Lhr)):\n",
    "    #     return None\n",
    "\n",
    "    # Wr = Wr.to(device)\n",
    "\n",
    "    return Lhr, Hr, Wr, SU, SV, scaleWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.72it/s]\n",
      "100%|██████████| 32/32 [01:37<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total weight shape:  torch.Size([6815744, 1024])\n",
      "train:  torch.Size([6814744, 1024]) val:  torch.Size([1000, 1024])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'scale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 243\u001b[0m\n\u001b[1;32m    205\u001b[0m     dataset_stats[split] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_all\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m: std_all\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_channel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd_channel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     }\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# datas = torch.cat(datas, dim = 0)\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# print('total dataset shape: ', datas.shape)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m#         'std_channel': None,\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmean(), dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(), dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m    244\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./block_pt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, exist_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m## 1\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# torch.save(dataset, f'./block_pt/{model_name}/scaled3_RHT_sig{sigma_reg}_{direction}_{size}.pt')\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# json_path = f'./block_pt/{model_name}/scaled3_RHT_sig{sigma_reg}_{direction}_{size}_dataset_stats.json'\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# json_path = f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_with_col_std_lidx_{direction}_{size}_dataset_stats.json'\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m## 7\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'scale'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf'\n",
    "]\n",
    "\n",
    "quip_hess_path = [\n",
    "    './quip_hess/llama3_8b_6144',\n",
    "    # './quip_hess/Hessians-Llama-2-7b-6144',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    # 4096\n",
    "]\n",
    "\n",
    "wtype_mapping = {'self_attn.q_proj': 0, \n",
    "                 'self_attn.k_proj': 1, \n",
    "                 'self_attn.v_proj': 2, \n",
    "                 'self_attn.o_proj': 3, \n",
    "                 'mlp.gate_proj': 4, \n",
    "                 'mlp.up_proj': 5, \n",
    "                 'mlp.down_proj': 6}\n",
    "sigma_reg = 1e-4\n",
    "# direction = 'col'\n",
    "direction = 'row'\n",
    "\n",
    "for model_name, size, quip_hess in zip(model_list, size_list, quip_hess_path):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    # datas = []\n",
    "    datas = {}\n",
    "    datas['weight'] = []\n",
    "    datas['idx'] = []\n",
    "    datas['layer_type'] = []\n",
    "    datas['scale'] = []\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        \n",
    "        hess_dict = {}\n",
    "        hess_dict['qkv'] = torch.load(f'{quip_hess}/{i}_qkv.pt', weights_only=False)\n",
    "        hess_dict['o'] = torch.load(f'{quip_hess}/{i}_o.pt', weights_only=False)\n",
    "        hess_dict['up'] = torch.load(f'{quip_hess}/{i}_up.pt', weights_only=False)\n",
    "        hess_dict['down'] = torch.load(f'{quip_hess}/{i}_down.pt', weights_only=False)\n",
    "        \n",
    "        for n, m in named_linears.items():\n",
    "            \n",
    "            W = m.weight.data.detach().to(device)\n",
    "            \n",
    "            if 'q_proj' in n or 'k_proj' in n or 'v_proj' in n:\n",
    "                H_flat = hess_dict['qkv']\n",
    "            elif 'o_proj' in n:\n",
    "                H_flat = hess_dict['o']\n",
    "            elif 'up_proj' in n or 'gate_proj' in n:\n",
    "                H_flat = hess_dict['up']\n",
    "            elif 'down_proj' in n:\n",
    "                H_flat = hess_dict['down']\n",
    "            else:\n",
    "                raise NotImplementedError(n)\n",
    "            \n",
    "            H = flat_to_sym(H_flat['flatH'], H_flat['n']).to(device)\n",
    "            mu = H_flat['mu'].to(device)\n",
    "            H.add_(mu[None, :] * mu[:, None])\n",
    "            n_h = H_flat['n']\n",
    "\n",
    "            # print('before',torch.diag(H).mean())\n",
    "            H = regularize_H(H, n_h, sigma_reg)\n",
    "            # print('after',torch.diag(H).mean())\n",
    "\n",
    "            ## 1\n",
    "            # diagH = torch.diag(H)\n",
    "            # diagW2 = torch.diag(W.T @ W)\n",
    "            # diagH = torch.clamp(diagH, min=1e-8)\n",
    "            # diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "            # scaleWH = (diagH / diagW2).sqrt()\n",
    "            # scaleWH = diagH.sqrt()\n",
    "            # scaleWH = scaleWH.clamp(min=1)\n",
    "            # print(scaleWH.numel())\n",
    "            # print((scaleWH>=1).sum() / scaleWH.numel())\n",
    "            # fig = plt.figure()\n",
    "            # ax = fig.add_subplot(111)\n",
    "            # ax.plot(scaleWH.cpu().numpy())\n",
    "            # plt.show\n",
    "            \n",
    "            # (M, N) = W.shape\n",
    "            # SU = (torch.randn(N).sign() + 1e-5).sign().to(device)\n",
    "            # SV = (torch.randn(M).sign() + 1e-5).sign().to(device)\n",
    "            # Wr = RHT_W(Wr, SU, SV)\n",
    "\n",
    "            ## 2\n",
    "            # diagH = torch.diag(H)\n",
    "            # diagH = torch.clamp(diagH, min=1e-8)\n",
    "            # scaleWH = diagH.sqrt()\n",
    "            # Wr = W * scaleWH[None, :]\n",
    "            # # W_normalized = Wr / Wr.norm(p=2, dim=1, keepdim=True)\n",
    "            # W_normalized = Wr / Wr.std(dim=1, keepdim=True)\n",
    "            # Wr = W_normalized\n",
    "            \n",
    "            ## 4 invH\n",
    "            # Lhr = torch.linalg.cholesky(H)\n",
    "            # H_inv = torch.cholesky_inverse(Lhr)\n",
    "            # diagH_inv = torch.diag(H_inv)\n",
    "            # scaleWH = 1/diagH_inv\n",
    "            # scaleWH = torch.clamp(scaleWH, min=1e-8)\n",
    "            # scaleWH = scaleWH.sqrt()\n",
    "            # Wr = W * scaleWH[None, :]\n",
    "            # # W_normalized = Wr / Wr.norm(p=2, dim=1, keepdim=True)\n",
    "            # W_normalized = Wr / Wr.std(dim=1, keepdim=True)\n",
    "            # Wr = W_normalized\n",
    "            \n",
    "            ## 3\n",
    "            # W_normalized = W / W.norm(p=2, dim=1, keepdim=True)\n",
    "            # Wr = W_normalized            \n",
    "            \n",
    "            # print(Wr.norm(p=2, dim=1)[:10])\n",
    "            \n",
    "            ## 6 \n",
    "            diagH = torch.diag(H)\n",
    "            diagH = torch.clamp(diagH, min=1e-8)\n",
    "            scaleWH = diagH.sqrt()\n",
    "            Wr = W * scaleWH[None, :]\n",
    "            \n",
    "            # ## 5 with col std\n",
    "            # col_std = Wr.std(dim=0, keepdim=True)\n",
    "            # col_std = col_std.expand(Wr.shape[0], Wr.shape[1])\n",
    "\n",
    "            ## 7 with col-row scale\n",
    "            col_std = Wr.std(dim=0, keepdim=True)\n",
    "            row_std = (Wr/col_std).std(dim=1, keepdim=True)\n",
    "            col_std = row_std @ col_std\n",
    "            assert col_std.shape == Wr.shape\n",
    "            \n",
    "            if direction == 'col':\n",
    "                w = Wr.T.to('cpu')\n",
    "                s = col_std.T.to('cpu')\n",
    "            else:\n",
    "                w = Wr.to('cpu')\n",
    "                s = col_std.to('cpu')\n",
    "            \n",
    "            if w.shape[-1] % size == 0:\n",
    "                w = w.reshape(-1, size)\n",
    "                s = s.reshape(-1, size)\n",
    "            else:\n",
    "                raise\n",
    "                D = w.shape[1]\n",
    "                span = size * (D //size)\n",
    "                if D < span:\n",
    "                    raise ValueError(\"Tensor's second dimension is too small for the requested slice.\")\n",
    "                max_start = D - span\n",
    "                start = random.randint(0, max_start)\n",
    "                sliced = w[:, start : start + span]\n",
    "                w = sliced.reshape(-1, size)\n",
    "\n",
    "            # datas.append(w)\n",
    "\n",
    "            datas['weight'].append(w)\n",
    "            datas['scale'].append(s)\n",
    "            \n",
    "            idx = torch.tensor([i], dtype = torch.int8)\n",
    "            datas['idx'].extend([idx] * w.shape[0])\n",
    "            \n",
    "            layer_type = torch.tensor([wtype_mapping[n]], dtype = torch.int8)\n",
    "            datas['layer_type'].extend([layer_type] * w.shape[0])\n",
    "    \n",
    "    for k in datas.keys():\n",
    "        # if datas[k][0].shape == torch.Size([]):\n",
    "        #     datas[k] = torch.tensor(datas[k])\n",
    "        # else:\n",
    "        datas[k] = torch.cat(datas[k], dim = 0)\n",
    "    \n",
    "    print('total weight shape: ', datas['weight'].shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas['weight']))\n",
    "    split_index = int(len(datas['weight']) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = {}\n",
    "    dataset['val'] = {}\n",
    "    for k in datas.keys():\n",
    "        dataset['train'][k] = datas[k][train_indices]\n",
    "        dataset['val'][k] = datas[k][val_indices]\n",
    "        \n",
    "    print('train: ', dataset['train']['weight'].shape, 'val: ', dataset['val']['weight'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        # mean_dim0 = data['weight'].mean(dim=0)\n",
    "        # std_dim0 = data['weight'].std(dim=0)\n",
    "        \n",
    "        mean_all = data['weight'].mean()\n",
    "        std_all = data['weight'].std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': None,\n",
    "            'std_channel': None\n",
    "        }\n",
    "    \n",
    "    # datas = torch.cat(datas, dim = 0)\n",
    "    # print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    # indices = torch.randperm(len(datas))\n",
    "    # split_index = int(len(datas) - 1000)\n",
    "    # train_indices = indices[:split_index]\n",
    "    # val_indices = indices[split_index:]\n",
    "\n",
    "    # dataset = {}\n",
    "    # dataset['train'] = datas[train_indices]\n",
    "    # dataset['val'] = datas[val_indices]\n",
    "    # print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    # dataset_stats = {}\n",
    "    # for split in ['train', 'val']:\n",
    "    #     data = dataset[split]\n",
    "        \n",
    "    #     # mean_dim0 = data.mean(dim=0)\n",
    "    #     # std_dim0 = data.std(dim=0)\n",
    "        \n",
    "    #     mean_all = data.mean()\n",
    "    #     std_all = data.std()\n",
    "        \n",
    "    #     dataset_stats[split] = {\n",
    "    #         'mean': mean_all.item(),\n",
    "    #         'std': std_all.item(),\n",
    "    #         # 'mean_channel': mean_dim0.tolist(),\n",
    "    #         # 'std_channel': std_dim0.tolist(),\n",
    "    #         'mean_channel': None,\n",
    "    #         'std_channel': None,\n",
    "    #     }\n",
    "    print(dataset['scale'].mean(), dataset['scale'].min(), dataset['scale'].max())\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    ## 1\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaled3_RHT_sig{sigma_reg}_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaled3_RHT_sig{sigma_reg}_{direction}_{size}_dataset_stats.json'\n",
    "    ## 2\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_std_rnormed_lidx_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_std_rnormed_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    ## 3\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/rnormed_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/rnormed_{direction}_{size}_dataset_stats.json'\n",
    "    ## 4\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaleHinv_sig{sigma_reg}_std_rnormed_lidx_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaleHinv_sig{sigma_reg}_std_rnormed_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    ## 5\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_std_rnormed_with_col_std_lidx_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_std_rnormed_with_col_std_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    ## 6\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_with_col_std_lidx_{direction}_{size}.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_with_col_std_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    ## 7\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_with_colrow_std_lidx_{direction}_{size}.pt')\n",
    "    json_path = f'./block_pt/{model_name}/scaleH_sig{sigma_reg}_with_colrow_std_lidx_{direction}_{size}_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for 'scale' column:\n",
      "  - Minimum value: 0.000000\n",
      "  - Maximum value: 7.301373\n",
      "  - Mean value: 0.011055\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "path = '/workspace/Weight_compression/Wparam_dataset/block_pt/meta-llama--Meta-Llama-3-8B/scaleH_sig0.0001_with_colrow_std_lidx_row_1024.pt'\n",
    "dataset = torch.load(path)\n",
    "\n",
    "scale_data = np.array(dataset['train']['scale'].flatten())\n",
    "# 2. Calculate min and max values\n",
    "min_val = scale_data.min()\n",
    "max_val = scale_data.max()\n",
    "\n",
    "# 3. Print the results\n",
    "print(f\"Statistics for 'scale' column:\")\n",
    "print(f\"  - Minimum value: {min_val:.6f}\")\n",
    "print(f\"  - Maximum value: {max_val:.6f}\")\n",
    "print(f\"  - Mean value: {scale_data.mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_data = np.array(dataset['train']['scale'].flatten())\n",
    "\n",
    "WEIGHT_SAMPLE_SIZE = 10000 # Number of weights to sample from each matrix\n",
    "scale_data = np.random.choice(scale_data, WEIGHT_SAMPLE_SIZE, replace=False)\n",
    "\n",
    "# 2. Calculate min and max values\n",
    "min_val = scale_data.min()\n",
    "max_val = scale_data.max()\n",
    "\n",
    "# 3. Print the results\n",
    "print(f\"Statistics for 'scale' column:\")\n",
    "print(f\"  - Minimum value: {min_val:.6f}\")\n",
    "print(f\"  - Maximum value: {max_val:.6f}\")\n",
    "\n",
    "# 4. Plot the distribution\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Using seaborn's histplot which is great for distributions\n",
    "sns.histplot(scale_data, kde=True, ax=ax, bins=50)\n",
    "\n",
    "ax.set_title(\"Distribution of 'scale' values\", fontsize=16)\n",
    "ax.set_xlabel(\"Scale Value\", fontsize=12)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "# Add vertical lines for min and max for better visualization\n",
    "ax.axvline(min_val, color='red', linestyle='--', label=f'Min: {min_val:.4f}')\n",
    "ax.axvline(max_val, color='green', linestyle='--', label=f'Max: {max_val:.4f}')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print std, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.39it/s]\n",
      "  0%|          | 0/32 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, m \u001b[38;5;129;01min\u001b[39;00m named_linears\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     58\u001b[0m     W \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mdescribe_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     61\u001b[0m         stats[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m v\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mdescribe_distribution\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     22\u001b[0m kurtosis \u001b[38;5;241m=\u001b[39m ((x \u001b[38;5;241m-\u001b[39m mean)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m/\u001b[39m (std_dev\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Fisher's definition\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m x\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m: median\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m: std_dev\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrange\u001b[39m\u001b[38;5;124m\"\u001b[39m: value_range\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miqr\u001b[39m\u001b[38;5;124m\"\u001b[39m: iqr\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskewness\u001b[39m\u001b[38;5;124m\"\u001b[39m: skewness\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkurtosis\u001b[39m\u001b[38;5;124m\"\u001b[39m: kurtosis\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     33\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def describe_distribution(x):\n",
    "    assert isinstance(x, torch.Tensor), \"Input must be a PyTorch tensor\"\n",
    "    x = x.flatten().float()\n",
    "    n = x.numel()\n",
    "    \n",
    "    # 중심 경향\n",
    "    mean = x.mean()\n",
    "    median = x.median()\n",
    "\n",
    "    # 산포도\n",
    "    std_dev = x.std(unbiased=False)\n",
    "    value_range = x.max() - x.min()\n",
    "    q1 = x.kthvalue(int(0.25 * n + 1)).values\n",
    "    q3 = x.kthvalue(int(0.75 * n + 1)).values\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    # 모양\n",
    "    skewness = ((x - mean)**3).mean() / (std_dev**3)\n",
    "    kurtosis = ((x - mean)**4).mean() / (std_dev**4) - 3  # Fisher's definition\n",
    "\n",
    "    del x\n",
    "    return {\n",
    "        \"mean\": mean.item(),\n",
    "        \"median\": median.item(),\n",
    "        \"std\": std_dev.item(),\n",
    "        \"range\": value_range.item(),\n",
    "        \"iqr\": iqr.item(),\n",
    "        \"skewness\": skewness.item(),\n",
    "        \"kurtosis\": kurtosis.item()\n",
    "    }\n",
    "\n",
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "\n",
    "for model_name in model_list:\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "    # std = defaultdict(list)\n",
    "    # mean = defaultdict(list)\n",
    "    stats = {}\n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.to('cuda:0')\n",
    "            r = describe_distribution(W)\n",
    "            for k,v in r.items():\n",
    "                stats[f'{i}_{n}_{k}'] = v\n",
    "            \n",
    "    fig, ax = plt.subplots(1, len(r), figsize=(15, 3))\n",
    "    \n",
    "    for idx, k in enumerate(r.keys()):\n",
    "        ax[idx].set_title(k)\n",
    "        for n, m in named_linears.items():\n",
    "            list = []\n",
    "            for i in range(len(layers)):\n",
    "                list.append(stats[f'{i}_{n}_{k}'])\n",
    "            ax[idx].plot(list, label=n)\n",
    "        # ax[idx].legend()\n",
    "        ax[idx].set_xlabel('Layer')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block with idx ltype stats\n",
    "droplast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Llama-2-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.61it/s]\n",
      "100%|██████████| 32/32 [06:46<00:00, 12.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total weight shape:  torch.Size([6127616, 1024])\n",
      "train:  torch.Size([6126616, 1024]) val:  torch.Size([1000, 1024])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    # 'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "#     'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024\n",
    "]\n",
    "\n",
    "wtype_mapping = {'self_attn.q_proj': 0, \n",
    "                 'self_attn.k_proj': 1, \n",
    "                 'self_attn.v_proj': 2, \n",
    "                 'self_attn.o_proj': 3, \n",
    "                 'mlp.gate_proj': 4, \n",
    "                 'mlp.up_proj': 5, \n",
    "                 'mlp.down_proj': 6}\n",
    "\n",
    "direction = 'col'\n",
    "shuffle = False\n",
    "drop_last = True\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    datas = {}\n",
    "    datas['weight'] = []\n",
    "    datas['idx'] = []\n",
    "    datas['layer_type'] = []\n",
    "    datas['mean'] = []\n",
    "    datas['median'] = []\n",
    "    datas['std'] = []\n",
    "    datas['range'] = []\n",
    "    datas['iqr'] = []\n",
    "    datas['skewness'] = []\n",
    "    datas['kurtosis'] = []\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            w = m.weight.data.detach()\n",
    "            \n",
    "            stats = describe_distribution(w)\n",
    "            \n",
    "            if direction == 'col':\n",
    "                w = w.T\n",
    "\n",
    "            if shuffle:\n",
    "                w = w.reshape(-1, 16)  # (num_rows, 16)\n",
    "                indices = torch.randperm(w.size(0))  # row 인덱스 섞기\n",
    "                w = w[indices]  # 섞인 순서대로 row 재배열\n",
    "                \n",
    "            if drop_last:\n",
    "                (r, c) = w.shape\n",
    "                d = (c // size) * size\n",
    "                w = w[:, :d]\n",
    "\n",
    "            w = w.reshape(-1, size)\n",
    "            \n",
    "            if drop_last:\n",
    "                assert w.shape[0] % r == 0\n",
    "                \n",
    "            # if w.size(0) % size == 0:\n",
    "            #     w = w.T    \n",
    "            #     w = w.reshape(-1, size)\n",
    "            # else:\n",
    "            #     w = w.reshape(-1, size)\n",
    "                \n",
    "            datas['weight'].append(w)\n",
    "            \n",
    "            idx = torch.tensor([i], dtype = torch.int8)\n",
    "            datas['idx'].extend([idx] * w.shape[0])\n",
    "            \n",
    "            layer_type = torch.tensor([wtype_mapping[n]], dtype = torch.int8)\n",
    "            datas['layer_type'].extend([layer_type] * w.shape[0])\n",
    "            \n",
    "            for k, v in stats.items():\n",
    "                val_tensor = torch.tensor([v], dtype=torch.float32)\n",
    "                datas[k].extend([val_tensor] * w.shape[0])\n",
    "    \n",
    "    for k in datas.keys():\n",
    "        # if datas[k][0].shape == torch.Size([]):\n",
    "        #     datas[k] = torch.tensor(datas[k])\n",
    "        # else:\n",
    "        datas[k] = torch.cat(datas[k], dim = 0)\n",
    "    \n",
    "    print('total weight shape: ', datas['weight'].shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas['weight']))\n",
    "    split_index = int(len(datas['weight']) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = {}\n",
    "    dataset['val'] = {}\n",
    "    for k in datas.keys():\n",
    "        dataset['train'][k] = datas[k][train_indices]\n",
    "        dataset['val'][k] = datas[k][val_indices]\n",
    "        \n",
    "    print('train: ', dataset['train']['weight'].shape, 'val: ', dataset['val']['weight'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        mean_dim0 = data['weight'].mean(dim=0)\n",
    "        std_dim0 = data['weight'].std(dim=0)\n",
    "        \n",
    "        mean_all = data['weight'].mean()\n",
    "        std_all = data['weight'].std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': mean_dim0.tolist(),\n",
    "            'std_channel': std_dim0.tolist(),\n",
    "        }\n",
    "\n",
    "    # os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_layerwise_stats.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/{direction}_{size}_layerwise_stats_dataset_stats.json'\n",
    "    # with open(json_path, 'w') as f:\n",
    "    #     json.dump(dataset_stats, f)\n",
    "        \n",
    "    sub = ''\n",
    "    if shuffle:\n",
    "        sub += 'shuffled_'\n",
    "    if drop_last:\n",
    "        sub += 'droplast_'\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{sub}{direction}_{size}_idx_ltype_stats.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{sub}{direction}_{size}_idx_ltype_stats_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5469,  2.6361, -0.1016],\n",
      "        [ 1.1328,  2.3082,  2.6023],\n",
      "        [ 0.4617,  1.7054, -1.7808],\n",
      "        ...,\n",
      "        [-0.0045,  1.9968,  1.9926],\n",
      "        [ 0.7723,  2.4261, -2.2338],\n",
      "        [ 1.5540,  3.0081, -0.6574]])\n",
      "tensor([-0.0077,  2.0123, -0.9304])\n",
      "tensor([1.0347, 0.5134, 2.0589])\n"
     ]
    }
   ],
   "source": [
    "# 행렬 크기 설정 (예: 5x3 행렬)\n",
    "rows, cols = 1000, 3\n",
    "\n",
    "# 열별 평균과 표준편차 정의\n",
    "mean = torch.tensor([0.0, 2.0, -1.0])  # 열별 평균값\n",
    "std = torch.tensor([1.0, 0.5, 2.0])    # 열별 표준편차\n",
    "\n",
    "# 가우시안 랜덤 행렬 생성 (브로드캐스팅 활용)\n",
    "random_matrix = torch.normal(mean.expand(rows, cols), std.expand(rows, cols))\n",
    "\n",
    "print(random_matrix)\n",
    "print(random_matrix.mean(0))\n",
    "print(random_matrix.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.self_attn.q_proj torch.Size([4096, 4096])\n",
      "0.self_attn.k_proj torch.Size([1024, 4096])\n",
      "0.self_attn.v_proj torch.Size([1024, 4096])\n",
      "0.self_attn.o_proj torch.Size([4096, 4096])\n",
      "0.mlp.gate_proj torch.Size([14336, 4096])\n",
      "0.mlp.up_proj torch.Size([14336, 4096])\n",
      "0.mlp.down_proj torch.Size([4096, 14336])\n",
      "model_name:  meta-llama--Llama-2-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.self_attn.q_proj torch.Size([4096, 4096])\n",
      "0.self_attn.k_proj torch.Size([4096, 4096])\n",
      "0.self_attn.v_proj torch.Size([4096, 4096])\n",
      "0.self_attn.o_proj torch.Size([4096, 4096])\n",
      "0.mlp.gate_proj torch.Size([11008, 4096])\n",
      "0.mlp.up_proj torch.Size([11008, 4096])\n",
      "0.mlp.down_proj torch.Size([4096, 11008])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai/clip-vit-large-patch14',\n",
    "    \n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    4096,\n",
    "    # 5120//4,\n",
    "]\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    # model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)[0:1]\n",
    "\n",
    "    if not isinstance(layers, dict):\n",
    "        layers = {'': layers}\n",
    "\n",
    "    for k, v in layers.items():\n",
    "        print(k)\n",
    "        named_linears = get_named_linears(v)\n",
    "        for n, m in named_linears.items():\n",
    "            print(n, m.weight.data.shape)\n",
    "            W = m.weight.data\n",
    "            r, c = W.shape\n",
    "\n",
    "        # mean_c = W.mean(0)\n",
    "        # std_c = W.std(0)\n",
    "        # if r % size != 0:\n",
    "        #     padding_size = size - r % size\n",
    "        #     g = torch.normal(mean_c.expand(padding_size, c), std_c.expand(padding_size, c))\n",
    "        #     W = torch.cat([W, g], dim=0)\n",
    "        #     print('padding')\n",
    "        #     print(n, W.shape)\n",
    "        #     print(mean_c[:5], std_c[:5])\n",
    "        #     print(W.mean(0)[:5], W.std(0)[:5])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block\n",
    "### layerwise suffle\n",
    "레이어 안에서 16 block으로 섞음\n",
    "### drop last\n",
    "### 8b + 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 144.67it/s]\n",
      "100%|██████████| 32/32 [00:17<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Llama-2-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 128.87it/s]\n",
      "100%|██████████| 32/32 [00:15<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dataset shape:  torch.Size([12943360, 1024])\n",
      "train:  torch.Size([12942360, 1024]) val:  torch.Size([1000, 1024])\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai--clip-vit-large-patch14',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    1024,\n",
    "    # 4096,\n",
    "    # 256,\n",
    "]\n",
    "\n",
    "direction = 'col'\n",
    "shuffle = False\n",
    "drop_last = True\n",
    "modelwise_norm = True\n",
    "\n",
    "model_datas = []\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    # model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    # layers_ = get_blocks(model)    \n",
    "    layers = get_blocks(model)    \n",
    "\n",
    "    # datas = []    \n",
    "    # for sub in layers_:\n",
    "        # layers = layers_[sub]\n",
    "    # if sub == 'vision':\n",
    "    #     size = 1024\n",
    "    # elif sub == 'text':\n",
    "    #     size = 768\n",
    "    # else:\n",
    "    #     raise\n",
    "    \n",
    "    datas = []\n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            w = m.weight.data.detach()\n",
    "            \n",
    "            # if w.size(0) % size == 0:\n",
    "            #     w = w.T    \n",
    "            #     w = w.reshape(-1, size)\n",
    "            # else:\n",
    "            #     w = w.reshape(-1, size)\n",
    "            \n",
    "            if direction == 'col':\n",
    "                w = w.T\n",
    "\n",
    "            if shuffle:\n",
    "                w = w.reshape(-1, 16)  # (num_rows, 16)\n",
    "                indices = torch.randperm(w.size(0))  # row 인덱스 섞기\n",
    "                w = w[indices]  # 섞인 순서대로 row 재배열\n",
    "                \n",
    "            if drop_last:\n",
    "                (r, c) = w.shape\n",
    "                d = (c // size) * size\n",
    "                w = w[:, :d]\n",
    "\n",
    "            w = w.reshape(-1, size)\n",
    "            \n",
    "            if drop_last:\n",
    "                assert w.shape[0] % r == 0\n",
    "\n",
    "            datas.append(w)\n",
    "        \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    if modelwise_norm:\n",
    "        mu = datas.mean()\n",
    "        std = datas.std()\n",
    "        datas = (datas - mu) / std\n",
    "        \n",
    "    model_datas.append(datas)\n",
    "    \n",
    "model_name = 'llama8b+7b'\n",
    "model_datas = torch.cat(model_datas, dim = 0)\n",
    "print('total dataset shape: ', model_datas.shape)\n",
    "datas = model_datas\n",
    "\n",
    "indices = torch.randperm(len(datas))\n",
    "split_index = int(len(datas) - 1000)\n",
    "train_indices = indices[:split_index]\n",
    "val_indices = indices[split_index:]\n",
    "\n",
    "dataset = {}\n",
    "dataset['train'] = datas[train_indices]\n",
    "dataset['val'] = datas[val_indices]\n",
    "print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "dataset_stats = {}\n",
    "for split in ['train', 'val']:\n",
    "    data = dataset[split]\n",
    "    \n",
    "    dataset_stats[split] = {\n",
    "        'mean': data.mean().item(),\n",
    "        'std': data.std().item(),\n",
    "        # 'mean_channel': data.mean(dim=0).tolist(),\n",
    "        # 'std_channel': data.std(dim=0).tolist(),\n",
    "    }\n",
    "\n",
    "# if sub != '':\n",
    "#     sub = sub+ '_'\n",
    "# sub = 'vision_text_'\n",
    "sub = ''\n",
    "if shuffle:\n",
    "    sub += 'shuffled_'\n",
    "if drop_last:\n",
    "    sub += 'droplast_'\n",
    "if modelwise_norm:\n",
    "    sub += 'modelwise_norm2_'\n",
    "os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "torch.save(dataset, f'./block_pt/{model_name}/{sub}{direction}_{size}.pt')\n",
    "json_path = f'./block_pt/{model_name}/{sub}{direction}_{size}_dataset_stats.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gaussian block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6814744, 1024])\n",
      "torch.Size([1000, 1024])\n"
     ]
    }
   ],
   "source": [
    "d = torch.load('/workspace/Weight_compression/Wparam_dataset/block_pt/meta-llama--Meta-Llama-3-8B/col_1024_gaussian_padding.pt')\n",
    "print(d['train'].shape)\n",
    "print(d['val'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "dataset['train'] = torch.normal(mean=0.0, std=1.0, size=d['train'].shape)\n",
    "dataset['val'] = torch.normal(mean=0.0, std=1.0, size=d['val'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  torch.Size([6814744, 1024]) val:  torch.Size([1000, 1024])\n"
     ]
    }
   ],
   "source": [
    "print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "dataset_stats = {}\n",
    "for split in ['train', 'val']:\n",
    "    data = dataset[split]\n",
    "    \n",
    "    mean_all = data.mean()\n",
    "    std_all = data.std()\n",
    "    \n",
    "    dataset_stats[split] = {\n",
    "        'mean': mean_all.item(),\n",
    "        'std': std_all.item(),\n",
    "    }\n",
    "\n",
    "sub = 'llama8b_'\n",
    "direction = 'col'\n",
    "size = 1024\n",
    "os.makedirs(f'./block_pt/gaussian', exist_ok = True)\n",
    "torch.save(dataset, f'./block_pt/gaussian/{sub}{direction}_{size}.pt')\n",
    "json_path = f'./block_pt/gaussian/{sub}{direction}_{size}_dataset_stats.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layerwise, channelwise normalized Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 128.67it/s]\n",
      "100%|██████████| 32/32 [00:14<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dataset shape:  torch.Size([6815744, 1024])\n",
      "train:  torch.Size([6814744, 1024]) val:  torch.Size([1000, 1024])\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai--clip-vit-large-patch14',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    # 4096,\n",
    "    # 4096,\n",
    "    # 256,\n",
    "]\n",
    "\n",
    "# direction = 'adapt'\n",
    "direction = 'col'\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    # model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "        \n",
    "    datas = []\n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.detach().cuda()\n",
    "\n",
    "            W = (W - W.mean(dim=0, keepdim=True)) / W.std(dim=0, keepdim=True)\n",
    "            # W = (W - W.mean()) / W.std()\n",
    "            \n",
    "            if direction == 'col':\n",
    "                W = W.T    \n",
    "            W = W.reshape(-1, size).cpu()\n",
    "                \n",
    "            datas.append(W)\n",
    "        \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        # mean_dim0 = data.mean(dim=0)\n",
    "        # std_dim0 = data.std(dim=0)        \n",
    "        # mean_all = data.mean()\n",
    "        # std_all = data.std()\n",
    "        \n",
    "        # dataset_stats[split] = {\n",
    "        #     'mean': mean_all.item(),\n",
    "        #     'std': std_all.item(),\n",
    "        #     'mean_channel': mean_dim0.tolist(),\n",
    "        #     'std_channel': std_dim0.tolist(),\n",
    "        # }\n",
    "        dataset_stats[split] = {\n",
    "            'mean': 0,\n",
    "            'std': 1,\n",
    "            'mean_channel': None,\n",
    "            'std_channel': None,\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_colwise_normed.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_colwise_normed_dataset_stats.json'\n",
    "    # torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_layerwise_normed.pt')\n",
    "    # json_path = f'./block_pt/{model_name}/{direction}_{size}_layerwise_normed_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': ModuleList(\n",
       "   (0-31): 32 x LlamaDecoderLayer(\n",
       "     (self_attn): LlamaAttention(\n",
       "       (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "       (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "       (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "       (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "     )\n",
       "     (mlp): LlamaMLP(\n",
       "       (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "       (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "       (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "       (act_fn): SiLU()\n",
       "     )\n",
       "     (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "     (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "   )\n",
       " )}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gaussian padding\n",
    "\n",
    "size가 안 맞으면 같은 row 나 col의 mean, std를 갖는 가우시안으로 padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    # 'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    # 1024,\n",
    "    1024,\n",
    "    1280,\n",
    "]\n",
    "\n",
    "# direction = 'adapt'\n",
    "direction = 'col'\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    datas = []\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.detach()\n",
    "\n",
    "            r, c = W.shape\n",
    "\n",
    "            if direction == 'col':\n",
    "                if r % size != 0:\n",
    "                    padding_size = size - r % size\n",
    "                    mean_c = W.mean(0)\n",
    "                    std_c = W.std(0)\n",
    "\n",
    "                    g = torch.normal(mean_c.expand(padding_size, c), std_c.expand(padding_size, c))\n",
    "                    W = torch.cat([W, g], dim=0)\n",
    "            elif direction =='row':\n",
    "                raise NotImplementedError\n",
    "            else:\n",
    "                raise KeyError\n",
    "            \n",
    "            if direction == 'col':\n",
    "                W = W.T\n",
    "            \n",
    "            assert W.shape[1] % size == 0\n",
    "            assert W.shape[1] >= size\n",
    "\n",
    "            W = W.reshape(-1, size)\n",
    "                \n",
    "            datas.append(W)\n",
    "    \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        mean_dim0 = data.mean(dim=0)\n",
    "        std_dim0 = data.std(dim=0)\n",
    "        \n",
    "        mean_all = data.mean()\n",
    "        std_all = data.std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': mean_dim0.tolist(),\n",
    "            'std_channel': std_dim0.tolist(),\n",
    "        }\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_gaussian_padding.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_gaussian_padding_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RHT smoothed Weight Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/Weight_compression/Wparam_dataset')\n",
    "from utils import *\n",
    "\n",
    "def RHT_H(H, SU):\n",
    "    return matmul_hadUt(matmul_hadUt(H * SU).T * SU)\n",
    "\n",
    "\n",
    "def RHT_W(W, SU, SV):\n",
    "    return matmul_hadUt(matmul_hadUt(W.T * SV).T * SU)\n",
    "\n",
    "\n",
    "def incoherence_preprocess(H, W, args):\n",
    "    # dtype_ = torch.float64 if args.use_fp64 else torch.float32\n",
    "    dtype_ = torch.float32\n",
    "    device = W.device\n",
    "    # device = torch.device('cpu')\n",
    "    (m, n) = W.shape\n",
    "\n",
    "    def _dump(Hr, Lhr, msg=''):\n",
    "        torch.save(Hr, f\"{args.save_pfx}/Hr_debug_fft.pt\")\n",
    "        torch.save(Lhr, f\"{args.save_pfx}/Lhr_debug_fft.pt\")\n",
    "        raise Exception(msg)\n",
    "\n",
    "    # diagonally rescale W,H to minimize proxy loss\n",
    "    scaleWH = None\n",
    "    Wr = W\n",
    "    Hr = H\n",
    "    # if args.rescale_WH:\n",
    "    if False:\n",
    "        Hr = H / H.abs().max()\n",
    "        diagH = torch.diag(Hr)\n",
    "        diagW2 = torch.diag(W.T @ W)\n",
    "        diagH = torch.clamp(diagH, min=1e-8)\n",
    "        diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "        scaleWH = (diagH / diagW2).sqrt().sqrt().to(torch.float32)\n",
    "        scaleWH = scaleWH.clamp(min=1e-8)\n",
    "        Wr = Wr * scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[:, None]\n",
    "        scaleWH = scaleWH.cpu()\n",
    "\n",
    "    # randomized hadamard transformation on H, W\n",
    "    if True:\n",
    "        SU = (torch.randn(n, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        SV = (torch.randn(m, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        # Hr = RHT_H(Hr, SU)\n",
    "        Wr = RHT_W(Wr, SU, SV)\n",
    "    # randomized kronecker product on H, W\n",
    "    elif args.incoh_mode == \"kron\":\n",
    "        SU = utils.rand_ortho_butterfly_noblock(n).to(dtype_).to(device)\n",
    "        SV = utils.rand_ortho_butterfly_noblock(m).to(dtype_).to(device)\n",
    "        Hr = SU @ Hr @ SU.T\n",
    "        Wr = SV @ Wr @ SU.T\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    SV = SV.cpu()\n",
    "    SU = SU.cpu()\n",
    "\n",
    "    # Lhr = torch.linalg.cholesky(Hr)\n",
    "    Lhr = None\n",
    "    # if not torch.all(torch.isfinite(Lhr)):\n",
    "    #     return None\n",
    "\n",
    "    Wr = Wr.to(device)\n",
    "\n",
    "    return Lhr, Hr, Wr, SU, SV, scaleWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    # 'meta-llama--Llama-2-13b-hf',\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    4096,\n",
    "    # 4096,\n",
    "    # 4096,\n",
    "]\n",
    "\n",
    "# direction = 'adapt'\n",
    "direction = 'col'\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    \n",
    "    datas = []\n",
    "    \n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        for n, m in named_linears.items():\n",
    "            w = m.weight.data.detach()\n",
    "            \n",
    "            Lhr, H, w, SU, SV, scaleWH = incoherence_preprocess(None, w, None) \n",
    "            \n",
    "            if direction == 'col':\n",
    "                w = w.T\n",
    "            \n",
    "            # if w.size(0) % size == 0:\n",
    "            #     w = w.T    \n",
    "            #     w = w.reshape(-1, size)\n",
    "            # else:\n",
    "            #     w = w.reshape(-1, size)\n",
    "                \n",
    "            w = w.reshape(-1, size)\n",
    "            \n",
    "            datas.append(w)\n",
    "    \n",
    "    datas = torch.cat(datas, dim = 0)\n",
    "    print('total dataset shape: ', datas.shape)\n",
    "    \n",
    "    indices = torch.randperm(len(datas))\n",
    "    split_index = int(len(datas) - 1000)\n",
    "    train_indices = indices[:split_index]\n",
    "    val_indices = indices[split_index:]\n",
    "\n",
    "    dataset = {}\n",
    "    dataset['train'] = datas[train_indices]\n",
    "    dataset['val'] = datas[val_indices]\n",
    "    print('train: ', dataset['train'].shape, 'val: ', dataset['val'].shape)\n",
    "\n",
    "    dataset_stats = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = dataset[split]\n",
    "        \n",
    "        mean_dim0 = data.mean(dim=0)\n",
    "        std_dim0 = data.std(dim=0)\n",
    "        \n",
    "        mean_all = data.mean()\n",
    "        std_all = data.std()\n",
    "        \n",
    "        dataset_stats[split] = {\n",
    "            'mean': mean_all.item(),\n",
    "            'std': std_all.item(),\n",
    "            'mean_channel': mean_dim0.tolist(),\n",
    "            'std_channel': std_dim0.tolist(),\n",
    "        }\n",
    "\n",
    "    os.makedirs(f'./block_pt/{model_name}', exist_ok = True)\n",
    "    torch.save(dataset, f'./block_pt/{model_name}/{direction}_{size}_RHT.pt')\n",
    "    json_path = f'./block_pt/{model_name}/{direction}_{size}_RHT_dataset_stats.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(dataset_stats, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
