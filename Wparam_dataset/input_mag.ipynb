{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, AutoModelForCausalLM, LlamaForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, OPTForCausalLM, BloomForCausalLM\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import functools\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "# from tinychat.models import LlavaLlamaForCausalLM\n",
    "from transformers.models.bloom.modeling_bloom import BloomForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "from transformers.models.opt.modeling_opt import OPTForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_linears(module):\n",
    "    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}\n",
    "\n",
    "def get_blocks(model):\n",
    "    if model.__class__.__name__ in (\"LlamaForCausalLM\", \"Qwen2ForCausalLM\"):\n",
    "        layers = model.model.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaForCausalLM\":\n",
    "        # layers = [model.model.layers, model.model.vision_tower.vision_tower.vision_model.encoder.layers]\n",
    "        layers = model.model.layers\n",
    "    elif isinstance(model, OPTForCausalLM):\n",
    "        layers = model.model.decoder.layers\n",
    "    elif isinstance(model, BloomForCausalLM):\n",
    "        layers = model.transformer.h\n",
    "    elif \"mpt\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.blocks\n",
    "    elif \"falcon\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"bigcode\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"neox\" in str(model.__class__).lower():\n",
    "        layers = model.gpt_neox.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaModel\":\n",
    "        layers = model.llm.model.layers\n",
    "    else:\n",
    "        raise NotImplementedError(type(model))\n",
    "    return layers\n",
    "\n",
    "def get_calib_dataset(data=\"pileval\", tokenizer=None, n_samples=512, block_size=512):\n",
    "    if data == \"pileval\":\n",
    "        # dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "        dataset = load_dataset(\"NeelNanda/pile-10k\", split=\"train\")        \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    samples = []\n",
    "    n_run = 0\n",
    "    for data in dataset:\n",
    "        line = data[\"text\"]\n",
    "        line = line.strip()\n",
    "        line_encoded = tokenizer.encode(line)\n",
    "        if len(line_encoded) > 512:\n",
    "            continue\n",
    "        sample = torch.tensor([line_encoded])\n",
    "        if sample.numel() == 0:\n",
    "            continue\n",
    "        samples.append(sample)\n",
    "        n_run += 1\n",
    "        if n_run == n_samples:\n",
    "            break\n",
    "    # now concatenate all samples and split according to block size\n",
    "    cat_samples = torch.cat(samples, dim=1)\n",
    "    n_split = cat_samples.shape[1] // block_size\n",
    "    print(f\" * Split into {n_split} blocks\")\n",
    "    return [\n",
    "        cat_samples[:, i * block_size : (i + 1) * block_size] for i in range(n_split)\n",
    "    ]\n",
    "    \n",
    "def move_embed(model, device):\n",
    "    if isinstance(model, LlamaForCausalLM):\n",
    "        model.model.embed_tokens = model.model.embed_tokens.to(device)\n",
    "        ## add the following line to move rotary_emb to GPU as well\n",
    "        model.model.rotary_emb = model.model.rotary_emb.to(device)\n",
    "    elif isinstance(model, LlavaLlamaForCausalLM):\n",
    "        model.model.embed_tokens = model.model.embed_tokens.to(device)\n",
    "        model.model.vision_tower.vision_tower.vision_model.embeddings.to(device)\n",
    "    elif isinstance(model, OPTForCausalLM):\n",
    "        model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(device)\n",
    "        model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(device)\n",
    "    elif isinstance(model, BloomForCausalLM):\n",
    "        model.transformer.word_embeddings = model.transformer.word_embeddings.to(device)\n",
    "        model.transformer.word_embeddings_layernorm = model.transformer.word_embeddings_layernorm.to(device)\n",
    "    elif \"mpt\" in str(model.__class__).lower():\n",
    "        model.transformer.wte = model.transformer.wte.to(device)\n",
    "        model.transformer.emb_drop = model.transformer.emb_drop.to(device)\n",
    "    elif \"falcon\" in str(model.__class__).lower():\n",
    "        model.transformer.word_embeddings = model.transformer.word_embeddings.to(device)\n",
    "    elif \"bigcode\" in str(model.__class__).lower():\n",
    "        model.transformer.wte = model.transformer.wte.to(device)\n",
    "        model.transformer.wpe = model.transformer.wpe.to(device)\n",
    "        model.transformer.drop = model.transformer.drop.to(device)\n",
    "    elif \"neox\" in str(model.__class__).lower():\n",
    "        model.gpt_neox.embed_in = model.gpt_neox.embed_in.to(device)\n",
    "        model.gpt_neox.emb_dropout = model.gpt_neox.emb_dropout.to(device)\n",
    "        model.embed_out = model.embed_out.to(device)\n",
    "    else:\n",
    "        raise NotImplementedError(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_awq(\n",
    "    model,\n",
    "    enc,\n",
    "    n_samples=512,\n",
    "    seqlen=512,\n",
    "    calib_data=\"pileval\",\n",
    "):\n",
    "    # from ..utils.calib_data import get_calib_dataset\n",
    "    # from ..utils.module import append_str_prefix, get_op_name\n",
    "    \n",
    "    input_feat_list = []\n",
    "\n",
    "    if \"bigcode\" in str(model.__class__).lower():\n",
    "        # otherwise attention_mask will always be on cpu.\n",
    "        model.transformer.bias = model.transformer.bias.to(\"cuda\")\n",
    "\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "    samples = get_calib_dataset(data=calib_data, tokenizer=enc, n_samples=n_samples, block_size=seqlen)\n",
    "    samples = torch.cat(samples, dim=0)\n",
    "\n",
    "    inps = []\n",
    "    layer_kwargs = {}\n",
    "\n",
    "    layers[0] = layers[0].cuda()\n",
    "    move_embed(model, \"cuda\")\n",
    "\n",
    "    # get input and kwargs to layer 0\n",
    "    # with_kwargs is only supported in PyTorch 2.0\n",
    "    # use this Catcher hack for now\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps.append(inp)\n",
    "            layer_kwargs.update(kwargs)\n",
    "            raise ValueError  # early exit to break later inference\n",
    "\n",
    "    # patch layer 0 to catch input and kwargs\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    try:\n",
    "        model(samples.to(next(model.parameters()).device))\n",
    "    except ValueError:  # work with early exit\n",
    "        pass\n",
    "    del samples\n",
    "    layers[0] = layers[0].module  # restore\n",
    "    inps = inps[0]\n",
    "\n",
    "    layers[0] = layers[0].cpu()\n",
    "    move_embed(model, \"cpu\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # solve layer by layer\n",
    "    for i in tqdm.tqdm(range(len(layers)), desc=\"Running AWQ...\"):\n",
    "        layer = layers[i]\n",
    "        layer = layer.cuda()\n",
    "        named_linears = get_named_linears(layer)\n",
    "\n",
    "        # firstly, get input features of all linear layers\n",
    "        def cache_input_hook(m, x, y, name, feat_dict):\n",
    "            x = x[0]\n",
    "            x = x.detach().cpu()\n",
    "            feat_dict[name].append(x)\n",
    "\n",
    "        input_feat = defaultdict(list)\n",
    "        handles = []\n",
    "        for name in named_linears:\n",
    "            handles.append(\n",
    "                named_linears[name].register_forward_hook(\n",
    "                    functools.partial(cache_input_hook, name=name, feat_dict=input_feat)\n",
    "                )\n",
    "            )\n",
    "        inps = inps.to(next(layer.parameters()).device)  # in case multi-gpu\n",
    "        # get output as next layer's input\n",
    "        inps = layer(inps, **layer_kwargs)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "        # now solve for scaling and clipping\n",
    "        input_feat = {k: torch.cat(v, dim=0) for k, v in input_feat.items()}\n",
    "\n",
    "        input_feat_list.appnend(input_feat)\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return input_feat_list\n",
    "\n",
    "    #     if auto_scale:  # if it applies, we should also modify the input_feat with scales\n",
    "    #         scales_list = auto_scale_block(\n",
    "    #             layer,\n",
    "    #             layer_kwargs,\n",
    "    #             w_bit=w_bit,\n",
    "    #             q_config=q_config,\n",
    "    #             input_feat=input_feat,\n",
    "    #         )\n",
    "    #         # apply_scale(layer, scales_list, input_feat_dict=input_feat)\n",
    "    #         apply_scale(layers[i], scales_list, input_feat_dict=input_feat)\n",
    "    #         # append prefix to make names global\n",
    "    #         awq_results[\"scale\"] += append_str_prefix(scales_list, get_op_name(model, layer) + \".\")\n",
    "\n",
    "    #     # Clear GPU memory\n",
    "    #     torch.cuda.empty_cache()\n",
    "\n",
    "    #     if mse_range:\n",
    "    #         clip_list = auto_clip_block(\n",
    "    #             layer,\n",
    "    #             w_bit=w_bit,\n",
    "    #             q_config=q_config,\n",
    "    #             input_feat=input_feat,\n",
    "    #         )\n",
    "    #         apply_clip(layer, clip_list)\n",
    "    #         # append prefix to make names global\n",
    "    #         awq_results[\"clip\"] += append_str_prefix(clip_list, get_op_name(model, layer) + \".\")\n",
    "\n",
    "    #     layer = layer.cpu()\n",
    "    #     # Haotian: check activation replacement\n",
    "    #     del input_feat\n",
    "    #     gc.collect()\n",
    "    #     torch.cuda.empty_cache()\n",
    "\n",
    "    # return awq_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Llama-2-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Split into 269 blocks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running AWQ...:   0%|          | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.10 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.44 GiB is free. Process 433494 has 19.01 GiB memory in use. Including non-PyTorch memory, this process has 3.12 GiB memory in use. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, local_files_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_awq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mrun_awq\u001b[0;34m(model, enc, n_samples, seqlen, calib_data)\u001b[0m\n\u001b[1;32m     78\u001b[0m inps \u001b[38;5;241m=\u001b[39m inps\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mnext\u001b[39m(layer\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# in case multi-gpu\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# get output as next layer's input\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m inps \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlayer_kwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handles:\n\u001b[1;32m     82\u001b[0m     h\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:637\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m        into the model\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    635\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 637\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    640\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    641\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    642\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    650\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:73\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     71\u001b[0m input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     72\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 73\u001b[0m variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.10 GiB. GPU 0 has a total capacity of 23.58 GiB of which 1.44 GiB is free. Process 433494 has 19.01 GiB memory in use. Including non-PyTorch memory, this process has 3.12 GiB memory in use. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 13.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Llama-2-7b-hf',\n",
    "    # 'meta-llama/Llama-2-13b-hf',\n",
    "    # 'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'lmsys/vicuna-7b-v1.5',\n",
    "    # 'lmsys/vicuna-13b-v1.5',\n",
    "    # 'facebook/opt-6.7b',\n",
    "]\n",
    "\n",
    "\n",
    "direction = 'adapt'\n",
    "\n",
    "for model_name in model_list:\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only = True)\n",
    "   \n",
    "    result = run_awq(model, tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
