{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a434a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import torch, torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, BloomForCausalLM\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:3\")\n",
    "\n",
    "def get_named_linears(module):\n",
    "    return {n:m for n,m in module.named_modules() if isinstance(m, nn.Linear)}\n",
    "\n",
    "def get_blocks(model):\n",
    "    if model.__class__.__name__ in (\"LlamaForCausalLM\",\"Qwen2ForCausalLM\",\"LlavaLlamaForCausalLM\"):\n",
    "        return model.model.layers\n",
    "    if isinstance(model, OPTForCausalLM): return model.model.decoder.layers\n",
    "    if isinstance(model, BloomForCausalLM): return model.transformer.h\n",
    "    if \"mpt\" in str(model.__class__).lower(): return model.transformer.blocks\n",
    "    if \"falcon\" in str(model.__class__).lower() or \"bigcode\" in str(model.__class__).lower():\n",
    "        return model.transformer.h\n",
    "    if \"neox\" in str(model.__class__).lower(): return model.gpt_neox.layers\n",
    "    raise NotImplementedError(type(model))\n",
    "\n",
    "def make_blocks(W, axis, bsz):\n",
    "    flat = (W if axis==0 else W.T).ravel()     # axis=0이면 행우선(flat by rows), axis=1이면 열우선\n",
    "    n_full = flat.size // bsz\n",
    "    if n_full == 0:\n",
    "        blk = np.pad(flat, (0, bsz-flat.size)) if flat.size < bsz else flat[:bsz]\n",
    "        return blk[None, :]\n",
    "    flat = flat[:n_full*bsz]\n",
    "    return flat.reshape(n_full, bsz)\n",
    "\n",
    "def flat_to_sym(V, N):\n",
    "    A = torch.zeros(N, N, dtype=V.dtype, device=V.device)\n",
    "    idxs = torch.tril_indices(N, N, device=V.device)\n",
    "    A[idxs.unbind()] = V\n",
    "    A[idxs[1, :], idxs[0, :]] = V\n",
    "    return A\n",
    "\n",
    "def regularize_H(H, n, sigma_reg):\n",
    "    H.div_(torch.diag(H).mean())\n",
    "    idx = torch.arange(n)\n",
    "    H[idx, idx] += sigma_reg\n",
    "    return H\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/Weight_compression/Wparam_dataset')\n",
    "from utils import *\n",
    "\n",
    "def RHT_H(H, SU):\n",
    "    return matmul_hadUt(matmul_hadUt(H * SU).T * SU)\n",
    "\n",
    "\n",
    "def RHT_W(W, SU, SV):\n",
    "    return matmul_hadUt(matmul_hadUt(W.T * SV).T * SU)\n",
    "\n",
    "\n",
    "def incoherence_preprocess(H, W, args):\n",
    "    # dtype_ = torch.float64 if args.use_fp64 else torch.float32\n",
    "    dtype_ = torch.float32\n",
    "    device = H.device\n",
    "    # device = torch.device('cpu')\n",
    "    (m, n) = H.shape\n",
    "\n",
    "    def _dump(Hr, Lhr, msg=''):\n",
    "        torch.save(Hr, f\"{args.save_pfx}/Hr_debug_fft.pt\")\n",
    "        torch.save(Lhr, f\"{args.save_pfx}/Lhr_debug_fft.pt\")\n",
    "        raise Exception(msg)\n",
    "\n",
    "    # diagonally rescale W,H to minimize proxy loss\n",
    "    scaleWH = None\n",
    "    Wr = W\n",
    "    Hr = H\n",
    "    # if args.rescale_WH:\n",
    "    if False:\n",
    "        Hr = H / H.abs().max()\n",
    "        diagH = torch.diag(Hr)\n",
    "        diagW2 = torch.diag(W.T @ W)\n",
    "        diagH = torch.clamp(diagH, min=1e-8)\n",
    "        diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "        scaleWH = (diagH / diagW2).sqrt().sqrt().to(torch.float32)\n",
    "        scaleWH = scaleWH.clamp(min=1e-8)\n",
    "        Wr = Wr * scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[:, None]\n",
    "        scaleWH = scaleWH.cpu()\n",
    "\n",
    "    # randomized hadamard transformation on H, W\n",
    "    if True:\n",
    "        SU = (torch.randn(n, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        SV = (torch.randn(m, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        Hr = RHT_H(Hr, SU)\n",
    "        # Wr = RHT_W(Wr, SU, SV)\n",
    "    # randomized kronecker product on H, W\n",
    "    elif args.incoh_mode == \"kron\":\n",
    "        SU = utils.rand_ortho_butterfly_noblock(n).to(dtype_).to(device)\n",
    "        SV = utils.rand_ortho_butterfly_noblock(m).to(dtype_).to(device)\n",
    "        Hr = SU @ Hr @ SU.T\n",
    "        Wr = SV @ Wr @ SU.T\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    SV = SV.cpu()\n",
    "    SU = SU.cpu()\n",
    "\n",
    "    # Lhr = torch.linalg.cholesky(Hr)\n",
    "    Lhr = None\n",
    "    # if not torch.all(torch.isfinite(Lhr)):\n",
    "    #     return None\n",
    "\n",
    "    # Wr = Wr.to(device)\n",
    "\n",
    "    return Lhr, Hr, Wr, SU, SV, scaleWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60656d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.self_attn.q_proj torch.Size([4096, 4096])\n",
      "0.self_attn.k_proj torch.Size([1024, 4096])\n",
      "0.self_attn.v_proj torch.Size([1024, 4096])\n",
      "0.self_attn.o_proj torch.Size([4096, 4096])\n",
      "0.mlp.gate_proj torch.Size([14336, 4096])\n",
      "0.mlp.up_proj torch.Size([14336, 4096])\n",
      "0.mlp.down_proj torch.Size([4096, 14336])\n",
      "model_name:  meta-llama--Llama-2-7b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.self_attn.q_proj torch.Size([4096, 4096])\n",
      "0.self_attn.k_proj torch.Size([4096, 4096])\n",
      "0.self_attn.v_proj torch.Size([4096, 4096])\n",
      "0.self_attn.o_proj torch.Size([4096, 4096])\n",
      "0.mlp.gate_proj torch.Size([11008, 4096])\n",
      "0.mlp.up_proj torch.Size([11008, 4096])\n",
      "0.mlp.down_proj torch.Size([4096, 11008])\n",
      "model_name:  meta-llama--Llama-2-13b-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:02<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.self_attn.q_proj torch.Size([5120, 5120])\n",
      "0.self_attn.k_proj torch.Size([5120, 5120])\n",
      "0.self_attn.v_proj torch.Size([5120, 5120])\n",
      "0.self_attn.o_proj torch.Size([5120, 5120])\n",
      "0.mlp.gate_proj torch.Size([13824, 5120])\n",
      "0.mlp.up_proj torch.Size([13824, 5120])\n",
      "0.mlp.down_proj torch.Size([5120, 13824])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai/clip-vit-large-patch14'\n",
    "]\n",
    "\n",
    "size_list = [\n",
    "    1024,\n",
    "    4096,\n",
    "    5120//4,\n",
    "]\n",
    "\n",
    "for model_name, size in zip(model_list, size_list):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    \n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    # model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)[0:1]\n",
    "\n",
    "    if not isinstance(layers, dict):\n",
    "        layers = {'': layers}\n",
    "\n",
    "    for k, v in layers.items():\n",
    "        print(k)\n",
    "        named_linears = get_named_linears(v)\n",
    "        for n, m in named_linears.items():\n",
    "            print(n, m.weight.data.shape)\n",
    "            W = m.weight.data\n",
    "            r, c = W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24825c5a",
   "metadata": {},
   "source": [
    "# Weight block pca per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef39fa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:02<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    # 'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai/clip-vit-large-patch14'\n",
    "]\n",
    "target_layers = [0,1,10,31]\n",
    "block_sizes = [512]\n",
    "max_blocks_per_weight = 500\n",
    "save_dir = \"./plot/pca_per_trblock\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "random.seed(0); np.random.seed(0)\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "norm_funcs = {\n",
    "    \"orig\":        lambda W: W,\n",
    "    \"lnormed\":     lambda W: W / W.std(),                                  # global l2 norm\n",
    "    \"rnormed\":     lambda W: W / W.std(dim=1, keepdims=True),             # row-wise\n",
    "    \"cnormed\":     lambda W: W / W.std(dim=0, keepdims=True),             # col-wise\n",
    "    \"r_c_normed\":  lambda W: (\n",
    "                        lambda X: X / X.std(dim=0, keepdims=True)\n",
    "                      )(W / W.std(dim=1, keepdims=True)),                 # row then col\n",
    "    \"c_r_normed\":  lambda W: (\n",
    "                        lambda X: X / X.std(dim=1, keepdims=True)\n",
    "                      )(W / W.std(dim=0, keepdims=True)),                 # col then row\n",
    "}\n",
    "\n",
    "for model_name in model_list:\n",
    "    mn = model_name.replace('/', '--')\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"./hf_model/{mn}\", local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "    for block_size in block_sizes:\n",
    "        for lidx in target_layers:\n",
    "            # data[(axis_name, norm_name)] = (X2, labels)\n",
    "            data = {}\n",
    "\n",
    "            named_linears = get_named_linears(layers[lidx])\n",
    "            if not named_linears:\n",
    "                print(f\"[skip] layer {lidx}\")\n",
    "                continue\n",
    "\n",
    "            # 블록 생성 & PCA 수행\n",
    "            for axis_name, axis in [(\"row\",0), (\"col\",1)]:\n",
    "                for norm_name, norm_fn in norm_funcs.items():\n",
    "                    feats, labels = [], []\n",
    "                    for wname, mod in named_linears.items():\n",
    "                        W = mod.weight.data.detach().cpu().numpy()\n",
    "                        W = norm_fn(torch.from_numpy(W)).numpy()  # numpy ← torch 변환\n",
    "                        blocks = make_blocks(W, axis=axis, bsz=block_size)\n",
    "                        if max_blocks_per_weight and blocks.shape[0] > max_blocks_per_weight:\n",
    "                            idx = rng.choice(blocks.shape[0], max_blocks_per_weight, replace=False)\n",
    "                            blocks = blocks[idx]\n",
    "                        feats.append(blocks)\n",
    "                        labels.extend([wname.split('.')[-1]] * blocks.shape[0])\n",
    "                    X = np.vstack(feats)\n",
    "                    X2 = PCA(n_components=2, svd_solver='randomized', random_state=0).fit_transform(X)\n",
    "                    data[(axis_name, norm_name)] = (X2, labels)\n",
    "\n",
    "            # 2행×4열 서브플롯\n",
    "            fig, axs = plt.subplots(2, 6, figsize=(18, 8))\n",
    "            cmap = plt.get_cmap('tab10')\n",
    "            axes = [\"row\", \"col\"]\n",
    "            norms = list(norm_funcs.keys())\n",
    "\n",
    "            for i, axis_name in enumerate(axes):\n",
    "                for j, norm_name in enumerate(norms):\n",
    "                    ax = axs[i, j]\n",
    "                    X2, labels = data[(axis_name, norm_name)]\n",
    "                    uniq = sorted(set(labels))\n",
    "                    color_map = {u: cmap(k % 10) for k, u in enumerate(uniq)}\n",
    "                    for u in uniq:\n",
    "                        idxs = [k for k, l in enumerate(labels) if l == u]\n",
    "                        ax.scatter(X2[idxs, 0], X2[idxs, 1], s=6, alpha=0.6, label=u, color=color_map[u])\n",
    "                    ax.set_title(f\"{axis_name} / {norm_name}\")\n",
    "                    ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\"); ax.grid(True, alpha=0.2)\n",
    "                    if i == 0 and j == 0:\n",
    "                        ax.legend(markerscale=3, fontsize=6, ncol=2)\n",
    "\n",
    "            fig.suptitle(f\"{mn} / layer {lidx}  (block_size={block_size})\", fontsize=12)\n",
    "            fig.tight_layout(rect=[0,0,1,0.95])\n",
    "            out_dir = os.path.join(save_dir, mn)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            fig.savefig(os.path.join(out_dir, f\"layer{lidx}_bs{block_size}_pca_all_norms.png\"), dpi=200)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a249db",
   "metadata": {},
   "source": [
    "# Hessian scaled weight pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d10935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  18%|█▊        | 2/11 [00:00<00:02,  4.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:01<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    # 'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai/clip-vit-large-patch14'\n",
    "]\n",
    "quip_hess_path = [\n",
    "    # './quip_hess/llama3_8b_6144',\n",
    "    # './quip_hess/Hessians-Llama-2-7b-6144',\n",
    "    './quip_hess/Hessians-Llama-2-13b-6144',\n",
    "]\n",
    "\n",
    "target_layers = [0,1,10,31]\n",
    "block_sizes = [512, 16, 128]\n",
    "max_blocks_per_weight = 500\n",
    "save_dir = \"./plot/pca_per_trblock\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "random.seed(0); np.random.seed(0)\n",
    "rng = np.random.default_rng(0)\n",
    "sigma_reg = 1e-4\n",
    "\n",
    "\n",
    "norm_funcs = {\n",
    "    \"orig\":        lambda W: W,\n",
    "    \"lnormed\":     lambda W: W / W.std(),                                  # global l2 norm\n",
    "    \"rnormed\":     lambda W: W / W.std(dim=1, keepdims=True),             # row-wise\n",
    "    \"cnormed\":     lambda W: W / W.std(dim=0, keepdims=True),             # col-wise\n",
    "    \"r_c_normed\":  lambda W: (\n",
    "                        lambda X: X / X.std(dim=0, keepdims=True)\n",
    "                      )(W / W.std(dim=1, keepdims=True)),                 # row then col\n",
    "    \"c_r_normed\":  lambda W: (\n",
    "                        lambda X: X / X.std(dim=1, keepdims=True)\n",
    "                      )(W / W.std(dim=0, keepdims=True)),                 # col then row\n",
    "}\n",
    "\n",
    "for model_name, quip_hess in zip(model_list, quip_hess_path):\n",
    "    mn = model_name.replace('/', '--')\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"./hf_model/{mn}\", local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    for block_size in block_sizes:\n",
    "        for lidx in target_layers:\n",
    "            # data[(axis_name, norm_name)] = (X2, labels)\n",
    "            data = {}\n",
    "\n",
    "            named_linears = get_named_linears(layers[lidx])\n",
    "            if not named_linears:\n",
    "                print(f\"[skip] layer {lidx}\")\n",
    "                continue\n",
    "\n",
    "            hess_dict = {}\n",
    "            hess_dict['qkv'] = torch.load(f'{quip_hess}/{lidx}_qkv.pt', weights_only=False)\n",
    "            hess_dict['o'] = torch.load(f'{quip_hess}/{lidx}_o.pt', weights_only=False)\n",
    "            hess_dict['up'] = torch.load(f'{quip_hess}/{lidx}_up.pt', weights_only=False)\n",
    "            hess_dict['down'] = torch.load(f'{quip_hess}/{lidx}_down.pt', weights_only=False)\n",
    "\n",
    "            # 블록 생성 & PCA 수행\n",
    "            for axis_name, axis in [(\"row\",0), (\"col\",1)]:\n",
    "                for norm_name, norm_fn in norm_funcs.items():\n",
    "                    feats, labels = [], []\n",
    "                    for wname, mod in named_linears.items():\n",
    "                        # W = mod.weight.data.detach().cpu().numpy()\n",
    "                        W = mod.weight.data.detach().to(device)                    \n",
    "                        \n",
    "                        if 'q_proj' in wname or 'k_proj' in wname or 'v_proj' in wname:\n",
    "                            H_flat = hess_dict['qkv']\n",
    "                        elif 'o_proj' in wname:\n",
    "                            H_flat = hess_dict['o']\n",
    "                        elif 'up_proj' in wname or 'gate_proj' in wname:\n",
    "                            H_flat = hess_dict['up']\n",
    "                        elif 'down_proj' in wname:\n",
    "                            H_flat = hess_dict['down']\n",
    "                        else:\n",
    "                            raise NotImplementedError(wname)\n",
    "                \n",
    "                        H = flat_to_sym(H_flat['flatH'], H_flat['n']).to(device)\n",
    "                        mu = H_flat['mu'].to(device)\n",
    "                        H.add_(mu[None, :] * mu[:, None])\n",
    "                        n_h = H_flat['n']                   \n",
    "                        H = regularize_H(H, n_h, sigma_reg)\n",
    "                        \n",
    "                        # scaleh\n",
    "                        diagH = torch.diag(H)\n",
    "                        diagH = torch.clamp(diagH, min=1e-8)\n",
    "                        scaleWH = diagH.sqrt()\n",
    "                        W = W * scaleWH[None, :]\n",
    "                        \n",
    "                        ## cholesky\n",
    "                        # L = torch.linalg.cholesky(H)\n",
    "                        # W = W @ L\n",
    "                        \n",
    "                        W = norm_fn(W).cpu().numpy()  # numpy ← torch 변환\n",
    "                        blocks = make_blocks(W, axis=axis, bsz=block_size)\n",
    "                        if max_blocks_per_weight and blocks.shape[0] > max_blocks_per_weight:\n",
    "                            idx = rng.choice(blocks.shape[0], max_blocks_per_weight, replace=False)\n",
    "                            blocks = blocks[idx]\n",
    "                        feats.append(blocks)\n",
    "                        labels.extend([wname.split('.')[-1]] * blocks.shape[0])\n",
    "                    X = np.vstack(feats)\n",
    "                    X2 = PCA(n_components=2, svd_solver='randomized', random_state=0).fit_transform(X)\n",
    "                    data[(axis_name, norm_name)] = (X2, labels)\n",
    "                    del W, H\n",
    "                    \n",
    "            # 2행×4열 서브플롯\n",
    "            fig, axs = plt.subplots(2, 6, figsize=(18, 8))\n",
    "            cmap = plt.get_cmap('tab10')\n",
    "            axes = [\"row\", \"col\"]\n",
    "            norms = list(norm_funcs.keys())\n",
    "\n",
    "            for i, axis_name in enumerate(axes):\n",
    "                for j, norm_name in enumerate(norms):\n",
    "                    ax = axs[i, j]\n",
    "                    X2, labels = data[(axis_name, norm_name)]\n",
    "                    uniq = sorted(set(labels))\n",
    "                    color_map = {u: cmap(k % 10) for k, u in enumerate(uniq)}\n",
    "                    for u in uniq:\n",
    "                        idxs = [k for k, l in enumerate(labels) if l == u]\n",
    "                        ax.scatter(X2[idxs, 0], X2[idxs, 1], s=6, alpha=0.6, label=u, color=color_map[u])\n",
    "                    ax.set_title(f\"{axis_name} / {norm_name}\")\n",
    "                    ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\"); ax.grid(True, alpha=0.2)\n",
    "                    if i == 0 and j == 0:\n",
    "                        ax.legend(markerscale=3, fontsize=6, ncol=2)\n",
    "\n",
    "            # fig.suptitle(f\"{mn} / cholesky layer {lidx}  (block_size={block_size})\", fontsize=12)\n",
    "            fig.suptitle(f\"{mn} / scaleh layer {lidx}  (block_size={block_size})\", fontsize=12)\n",
    "            fig.tight_layout(rect=[0,0,1,0.95])\n",
    "            out_dir = os.path.join(save_dir, mn)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            # fig.savefig(os.path.join(out_dir, f\"cholesky_layer{lidx}_bs{block_size}_pca_all_norms.png\"), dpi=200)\n",
    "            fig.savefig(os.path.join(out_dir, f\"scaleh_layer{lidx}_bs{block_size}_pca_all_norms.png\"), dpi=200)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbcd0c",
   "metadata": {},
   "source": [
    "# Cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "951f3853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:02<00:00,  4.79it/s]\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1136 is not positive-definite).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 88\u001b[0m\n\u001b[1;32m     79\u001b[0m H \u001b[38;5;241m=\u001b[39m regularize_H(H, n_h, sigma_reg)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# # scaleh\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# diagH = torch.diag(H)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# diagH = torch.clamp(diagH, min=1e-8)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# cholesky\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m W \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m@\u001b[39m L\n\u001b[1;32m     91\u001b[0m W \u001b[38;5;241m=\u001b[39m norm_fn(W)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# numpy ← torch 변환\u001b[39;00m\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: linalg.cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1136 is not positive-definite)."
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    # 'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama--Llama-2-13b-hf',\n",
    "    # 'openai/clip-vit-large-patch14'\n",
    "]\n",
    "quip_hess_path = [\n",
    "    # './quip_hess/llama3_8b_6144',\n",
    "    # './quip_hess/Hessians-Llama-2-7b-6144',\n",
    "    './quip_hess/Hessians-Llama-2-13b-6144',\n",
    "]\n",
    "\n",
    "target_layers = [0,1,10,31]\n",
    "block_sizes = [512, 16, 128]\n",
    "max_blocks_per_weight = 500\n",
    "save_dir = \"./plot/pca_per_trblock\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "random.seed(0); np.random.seed(0)\n",
    "rng = np.random.default_rng(0)\n",
    "sigma_reg = 1e-4\n",
    "\n",
    "\n",
    "norm_funcs = {\n",
    "    \"orig\":        lambda W: W,\n",
    "    \"lnormed\":     lambda W: W / W.std(),                                  # global l2 norm\n",
    "    \"rnormed\":     lambda W: W / W.std(dim=1, keepdims=True),             # row-wise\n",
    "    \"cnormed\":     lambda W: W / W.std(dim=0, keepdims=True),             # col-wise\n",
    "    \"r_c_normed\":  lambda W: (\n",
    "                        lambda X: X / X.std(dim=0, keepdims=True)\n",
    "                      )(W / W.std(dim=1, keepdims=True)),                 # row then col\n",
    "    \"c_r_normed\":  lambda W: (\n",
    "                        lambda X: X / X.std(dim=1, keepdims=True)\n",
    "                      )(W / W.std(dim=0, keepdims=True)),                 # col then row\n",
    "}\n",
    "\n",
    "for model_name, quip_hess in zip(model_list, quip_hess_path):\n",
    "    mn = model_name.replace('/', '--')\n",
    "    model = AutoModelForCausalLM.from_pretrained(f\"./hf_model/{mn}\", local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "    for block_size in block_sizes:\n",
    "        for lidx in target_layers:\n",
    "            # data[(axis_name, norm_name)] = (X2, labels)\n",
    "            data = {}\n",
    "\n",
    "            named_linears = get_named_linears(layers[lidx])\n",
    "            if not named_linears:\n",
    "                print(f\"[skip] layer {lidx}\")\n",
    "                continue\n",
    "\n",
    "            hess_dict = {}\n",
    "            hess_dict['qkv'] = torch.load(f'{quip_hess}/{lidx}_qkv.pt', weights_only=False)\n",
    "            hess_dict['o'] = torch.load(f'{quip_hess}/{lidx}_o.pt', weights_only=False)\n",
    "            hess_dict['up'] = torch.load(f'{quip_hess}/{lidx}_up.pt', weights_only=False)\n",
    "            hess_dict['down'] = torch.load(f'{quip_hess}/{lidx}_down.pt', weights_only=False)\n",
    "\n",
    "            # 블록 생성 & PCA 수행\n",
    "            for axis_name, axis in [(\"row\",0), (\"col\",1)]:\n",
    "                for norm_name, norm_fn in norm_funcs.items():\n",
    "                    feats, labels = [], []\n",
    "                    for wname, mod in named_linears.items():\n",
    "                        # W = mod.weight.data.detach().cpu().numpy()\n",
    "                        W = mod.weight.data.detach().to(device)                    \n",
    "                        \n",
    "                        if 'q_proj' in wname or 'k_proj' in wname or 'v_proj' in wname:\n",
    "                            H_flat = hess_dict['qkv']\n",
    "                        elif 'o_proj' in wname:\n",
    "                            H_flat = hess_dict['o']\n",
    "                        elif 'up_proj' in wname or 'gate_proj' in wname:\n",
    "                            H_flat = hess_dict['up']\n",
    "                        elif 'down_proj' in wname:\n",
    "                            H_flat = hess_dict['down']\n",
    "                        else:\n",
    "                            raise NotImplementedError(wname)\n",
    "                \n",
    "                        H = flat_to_sym(H_flat['flatH'], H_flat['n']).to(device)\n",
    "                        mu = H_flat['mu'].to(device)\n",
    "                        H.add_(mu[None, :] * mu[:, None])\n",
    "                        n_h = H_flat['n']                   \n",
    "                        H = regularize_H(H, n_h, sigma_reg)\n",
    "                        \n",
    "                        # # scaleh\n",
    "                        # diagH = torch.diag(H)\n",
    "                        # diagH = torch.clamp(diagH, min=1e-8)\n",
    "                        # scaleWH = diagH.sqrt()\n",
    "                        # W = W * scaleWH[None, :]\n",
    "                        \n",
    "                        # cholesky\n",
    "                        L = torch.linalg.cholesky(H)\n",
    "                        W = W @ L\n",
    "                        \n",
    "                        W = norm_fn(W).cpu().numpy()  # numpy ← torch 변환\n",
    "                        blocks = make_blocks(W, axis=axis, bsz=block_size)\n",
    "                        if max_blocks_per_weight and blocks.shape[0] > max_blocks_per_weight:\n",
    "                            idx = rng.choice(blocks.shape[0], max_blocks_per_weight, replace=False)\n",
    "                            blocks = blocks[idx]\n",
    "                        feats.append(blocks)\n",
    "                        labels.extend([wname.split('.')[-1]] * blocks.shape[0])\n",
    "                    X = np.vstack(feats)\n",
    "                    X2 = PCA(n_components=2, svd_solver='randomized', random_state=0).fit_transform(X)\n",
    "                    data[(axis_name, norm_name)] = (X2, labels)\n",
    "                    del W, H\n",
    "                    \n",
    "            # 2행×4열 서브플롯\n",
    "            fig, axs = plt.subplots(2, 6, figsize=(18, 8))\n",
    "            cmap = plt.get_cmap('tab10')\n",
    "            axes = [\"row\", \"col\"]\n",
    "            norms = list(norm_funcs.keys())\n",
    "\n",
    "            for i, axis_name in enumerate(axes):\n",
    "                for j, norm_name in enumerate(norms):\n",
    "                    ax = axs[i, j]\n",
    "                    X2, labels = data[(axis_name, norm_name)]\n",
    "                    uniq = sorted(set(labels))\n",
    "                    color_map = {u: cmap(k % 10) for k, u in enumerate(uniq)}\n",
    "                    for u in uniq:\n",
    "                        idxs = [k for k, l in enumerate(labels) if l == u]\n",
    "                        ax.scatter(X2[idxs, 0], X2[idxs, 1], s=6, alpha=0.6, label=u, color=color_map[u])\n",
    "                    ax.set_title(f\"{axis_name} / {norm_name}\")\n",
    "                    ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\"); ax.grid(True, alpha=0.2)\n",
    "                    if i == 0 and j == 0:\n",
    "                        ax.legend(markerscale=3, fontsize=6, ncol=2)\n",
    "\n",
    "            fig.suptitle(f\"{mn} / cholesky layer {lidx}  (block_size={block_size})\", fontsize=12)\n",
    "            # fig.suptitle(f\"{mn} / scaleh layer {lidx}  (block_size={block_size})\", fontsize=12)\n",
    "            fig.tight_layout(rect=[0,0,1,0.95])\n",
    "            out_dir = os.path.join(save_dir, mn)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            fig.savefig(os.path.join(out_dir, f\"cholesky_layer{lidx}_bs{block_size}_pca_all_norms.png\"), dpi=200)\n",
    "            # fig.savefig(os.path.join(out_dir, f\"scaleh_layer{lidx}_bs{block_size}_pca_all_norms.png\"), dpi=200)\n",
    "            plt.close(fig)\n",
    "\n",
    "    print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
