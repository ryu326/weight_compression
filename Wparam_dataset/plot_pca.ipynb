{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104561a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, OPTForCausalLM, BloomForCausalLM\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_named_linears(module):\n",
    "    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}\n",
    "\n",
    "def get_blocks(model):\n",
    "    if model.__class__.__name__ in (\"LlamaForCausalLM\", \"Qwen2ForCausalLM\"):\n",
    "        layers = model.model.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaForCausalLM\":\n",
    "        layers = model.model.layers\n",
    "    elif isinstance(model, OPTForCausalLM):\n",
    "        layers = model.model.decoder.layers\n",
    "    elif isinstance(model, BloomForCausalLM):\n",
    "        layers = model.transformer.h\n",
    "    elif \"mpt\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.blocks\n",
    "    elif \"falcon\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"bigcode\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"neox\" in str(model.__class__).lower():\n",
    "        layers = model.gpt_neox.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaModel\":\n",
    "        layers = model.llm.model.layers\n",
    "    elif model.__class__.__name__ in (\"CLIPModel\"):\n",
    "        vision_layers = model.vision_model.encoder.layers\n",
    "        text_layers = model.text_model.encoder.layers\n",
    "        layers = {'vision': vision_layers,\n",
    "                  'text': text_layers}\n",
    "    else:\n",
    "        raise NotImplementedError(type(model))\n",
    "    # if not isinstance(layers, dict):\n",
    "    #     layers = {'': layers}\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3632d",
   "metadata": {},
   "source": [
    "### Weight block 별 PCA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "model_list = [\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "]\n",
    "block_size = 16\n",
    "\n",
    "for model_name in model_list:\n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "    for idx in [0, 1, 10, 31]:\n",
    "        named_linears = get_named_linears(layers[idx])\n",
    "\n",
    "        for n, m in named_linears.items():\n",
    "\n",
    "            W = m.weight.data.detach().cpu()\n",
    "            shape = W.shape\n",
    "\n",
    "            variants = {\n",
    "                'original': W,\n",
    "                'row_norm': W / (W.std(dim=1, keepdim=True) + 1e-6),\n",
    "                'col_norm': W / (W.std(dim=0, keepdim=True) + 1e-6),\n",
    "            }\n",
    "\n",
    "            block_sizes= [8, 16, 64, 128, -1]\n",
    "            fig, axes = plt.subplots(len(block_sizes), 6, figsize=(24, 4 * len(block_sizes)))\n",
    "            \n",
    "            for i, block_size in enumerate(block_sizes):\n",
    "                pca_results = []\n",
    "                for name, W_var in variants.items():\n",
    "                    # reshape for row and col PCA\n",
    "                    try:\n",
    "                        bs = block_size if block_size > 0 else shape[1]\n",
    "                        row_data = W_var.reshape(-1, bs).numpy()\n",
    "                        row_pca = PCA(n_components=2).fit_transform(row_data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[{name}] Row PCA failed: {e}\")\n",
    "                        row_pca = None\n",
    "\n",
    "                    try:\n",
    "                        bs = block_size if block_size > 0 else shape[0]\n",
    "                        col_data = W_var.T.reshape(-1, bs).numpy()\n",
    "                        col_pca = PCA(n_components=2).fit_transform(col_data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"[{name}] Col PCA failed: {e}\")\n",
    "                        col_pca = None\n",
    "\n",
    "                    # 축 범위 맞추기\n",
    "                    all_valid = [p for p in [row_pca, col_pca] if p is not None]\n",
    "                    if all_valid:\n",
    "                        all_concat = np.concatenate(all_valid, axis=0)\n",
    "                        max_abs = np.abs(all_concat).max()\n",
    "                    else:\n",
    "                        max_abs = 1.0\n",
    "\n",
    "                    pca_results.extend([\n",
    "                        (f\"{name} row PCA\", row_pca, max_abs, '#6baed6'),\n",
    "                        (f\"{name} col PCA\", col_pca, max_abs, 'orange')\n",
    "                    ])\n",
    "                \n",
    "                # plot\n",
    "                # axes = axes.flatten()\n",
    "                axs = axes[i]\n",
    "                for ax, (title, data, max_abs, color) in zip(axs, pca_results):\n",
    "                    if data is not None:\n",
    "                        ax.scatter(data[:, 0], data[:, 1], s=5, alpha=0.6, color=color)\n",
    "                        ax.set_xlim(-max_abs, max_abs)\n",
    "                        ax.set_ylim(-max_abs, max_abs)\n",
    "                    ax.set_title(f\"{idx}_{n.split('.')[-1]} ({title}) block_size {block_size}\")\n",
    "                    ax.set_xlabel(\"PC1\")\n",
    "                    ax.set_ylabel(\"PC2\")\n",
    "                    ax.grid(True)\n",
    "\n",
    "            plt.suptitle(f\"PCA of {model_name} Layer {idx} / {n.split('.')[-1]}\", fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            os.makedirs(f\"./plot/pca_compare/{model_name}/{n.split('.')[-1]}\", exist_ok=True)\n",
    "            plt.savefig(f\"./plot/pca_compare/{model_name}/{n.split('.')[-1]}/{idx}_{n.split('.')[-1]}.png\")\n",
    "            # plt.show()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890763a",
   "metadata": {},
   "source": [
    "## PCA with MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f97ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "model_list = [\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "]\n",
    "\n",
    "comp_path = '/workspace/Weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_8b/lmbda300'\n",
    "comp_path2 = '/workspace/Weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Llama-2-7b-hf/ql_8b_7b_droplast/lmbda300'\n",
    "n_map = {'q_proj':'q', 'k_proj':'k', 'v_proj':'v', 'o_proj':'o', 'gate_proj':'gate', 'up_proj':'up', 'down_proj':'down'}\n",
    "\n",
    "for model_name in model_list:\n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "    for idx in [1, 10, 31]:\n",
    "        named_linears = get_named_linears(layers[idx])\n",
    "\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.detach().cuda()\n",
    "            \n",
    "            ckpt_path = os.path.join(comp_path, f\"{idx}_{n_map[n.split('.')[-1]]}.pt\")\n",
    "            W_hat = torch.load(ckpt_path, map_location='cuda', weights_only=False)['W_hat']\n",
    "            \n",
    "            mse = (W_hat - W)**2\n",
    "            W = W.cpu()\n",
    "            W_hat = W_hat.cpu()\n",
    "                        \n",
    "            shape = W.shape\n",
    "\n",
    "            variants = {\n",
    "                'original': W,\n",
    "                'row_norm': W / (W.std(dim=1, keepdim=True) + 1e-6),\n",
    "                'col_norm': W / (W.std(dim=0, keepdim=True) + 1e-6),\n",
    "            }\n",
    "\n",
    "            block_sizes= [8, 16, 64, 128, -1]\n",
    "            # block_sizes= [8, 16]\n",
    "            fig, axes = plt.subplots(len(block_sizes), 6, figsize=(24, 4 * len(block_sizes)))\n",
    "            \n",
    "            for i, block_size in enumerate(block_sizes):\n",
    "                pca_results = []\n",
    "                for name, W_var in variants.items():\n",
    "                    \n",
    "                    # reshape for row and col PCA\n",
    "                    bs = block_size if block_size > 0 else shape[1]\n",
    "                    row_data = W_var.reshape(-1, bs).numpy()\n",
    "                    row_pca = PCA(n_components=2).fit_transform(row_data)\n",
    "                    row_mse_block = mse.reshape(-1, bs).mean(dim=1).cpu().numpy()  \n",
    "\n",
    "                    bs = block_size if block_size > 0 else shape[0]\n",
    "                    col_data = W_var.T.reshape(-1, bs).numpy()\n",
    "                    col_pca = PCA(n_components=2).fit_transform(col_data)\n",
    "                    col_mse_block = mse.T.reshape(-1, bs).mean(dim=1).cpu().numpy()\n",
    "                    \n",
    "                    # 축 범위 맞추기\n",
    "                    all_valid = [p for p in [row_pca, col_pca] if p is not None]\n",
    "                    if all_valid:\n",
    "                        all_concat = np.concatenate(all_valid, axis=0)\n",
    "                        max_abs = np.abs(all_concat).max()\n",
    "                    else:\n",
    "                        max_abs = 1.0\n",
    "\n",
    "                    pca_results.extend([\n",
    "                        (f\"{name} row PCA\", row_pca, max_abs, row_mse_block),\n",
    "                        (f\"{name} col PCA\", col_pca, max_abs, col_mse_block)\n",
    "                    ])\n",
    "                \n",
    "                # plot\n",
    "                # axes = axes.flatten()\n",
    "                axs = axes[i]\n",
    "                for ax, (title, data, max_abs, errors) in zip(axs, pca_results):\n",
    "                    if data is not None:\n",
    "                        sc = ax.scatter(data[:, 0], data[:, 1], s=5, alpha=0.7, c=errors, cmap='viridis')\n",
    "                        ax.set_xlim(-max_abs, max_abs)\n",
    "                        ax.set_ylim(-max_abs, max_abs)\n",
    "                        plt.colorbar(sc, ax=ax)  # 컬러바 추가\n",
    "                    ax.set_title(f\"{idx}_{n.split('.')[-1]} ({title}) block_size {block_size}\")\n",
    "                    ax.set_xlabel(\"PC1\")\n",
    "                    ax.set_ylabel(\"PC2\")\n",
    "                    ax.grid(True)\n",
    "\n",
    "            plt.suptitle(f\"PCA of {model_name} Layer {idx} / {n.split('.')[-1]}\", fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            os.makedirs(f\"./plot/pca_mse/{model_name}/{n.split('.')[-1]}\", exist_ok=True)\n",
    "            plt.savefig(f\"./plot/pca_mse/{model_name}/{n.split('.')[-1]}/{idx}_{n.split('.')[-1]}.png\")\n",
    "            plt.show()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec86ec",
   "metadata": {},
   "source": [
    "# PCA scaleH\n",
    "original rnorm, cnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c85de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtype_mapping = {'self_attn.q_proj': 0, \n",
    "                 'self_attn.k_proj': 1, \n",
    "                 'self_attn.v_proj': 2, \n",
    "                 'self_attn.o_proj': 3, \n",
    "                 'mlp.gate_proj': 4, \n",
    "                 'mlp.up_proj': 5, \n",
    "                 'mlp.down_proj': 6}\n",
    "sigma_reg = 1e-4\n",
    "\n",
    "def flat_to_sym(V, N):\n",
    "    A = torch.zeros(N, N, dtype=V.dtype, device=V.device)\n",
    "    idxs = torch.tril_indices(N, N, device=V.device)\n",
    "    A[idxs.unbind()] = V\n",
    "    A[idxs[1, :], idxs[0, :]] = V\n",
    "    return A\n",
    "\n",
    "def regularize_H(H, n, sigma_reg):\n",
    "    H.div_(torch.diag(H).mean())\n",
    "    idx = torch.arange(n)\n",
    "    H[idx, idx] += sigma_reg\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "model_list = [\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "]\n",
    "\n",
    "# comp_path = '/workspace/Weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_8b/lmbda300'\n",
    "# comp_path2 = '/workspace/Weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Llama-2-7b-hf/ql_8b_7b_droplast/lmbda300'\n",
    "n_map = {'q_proj':'q', 'k_proj':'k', 'v_proj':'v', 'o_proj':'o', 'gate_proj':'gate', 'up_proj':'up', 'down_proj':'down'}\n",
    "\n",
    "quip_hess_path = [\n",
    "    # './quip_hess/Hessians-Llama-2-7b-6144',\n",
    "    './quip_hess/llama3_8b_6144',\n",
    "]\n",
    "\n",
    "# device = torch.device(\"cuda:3\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "for model_name, quip_hess in zip(model_list, quip_hess_path):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "    for idx in [0, 1, 10, 31]:\n",
    "        named_linears = get_named_linears(layers[idx])\n",
    "        \n",
    "        hess_dict = {}\n",
    "        hess_dict['qkv'] = torch.load(f'{quip_hess}/{idx}_qkv.pt', weights_only=False)\n",
    "        hess_dict['o'] = torch.load(f'{quip_hess}/{idx}_o.pt', weights_only=False)\n",
    "        hess_dict['up'] = torch.load(f'{quip_hess}/{idx}_up.pt', weights_only=False)\n",
    "        hess_dict['down'] = torch.load(f'{quip_hess}/{idx}_down.pt', weights_only=False)\n",
    "\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.detach().cuda()\n",
    "            \n",
    "            if 'q_proj' in n or 'k_proj' in n or 'v_proj' in n:\n",
    "                H_flat = hess_dict['qkv']\n",
    "            elif 'o_proj' in n:\n",
    "                H_flat = hess_dict['o']\n",
    "            elif 'up_proj' in n or 'gate_proj' in n:\n",
    "                H_flat = hess_dict['up']\n",
    "            elif 'down_proj' in n:\n",
    "                H_flat = hess_dict['down']\n",
    "            else:\n",
    "                raise NotImplementedError(n)\n",
    "            \n",
    "            H = flat_to_sym(H_flat['flatH'], H_flat['n']).cuda()\n",
    "            mu = H_flat['mu'].cuda()\n",
    "            H.add_(mu[None, :] * mu[:, None])\n",
    "            n_h = H_flat['n']\n",
    "\n",
    "            # print('before',torch.diag(H).mean())\n",
    "            H = regularize_H(H, n_h, sigma_reg)\n",
    "            \n",
    "            diagH = torch.diag(H)\n",
    "            diagH = torch.clamp(diagH, min=1e-8)\n",
    "            scaleWH = diagH.sqrt()\n",
    "            Wr = W * scaleWH[None, :]\n",
    "            # W_normalized = Wr / Wr.norm(p=2, dim=1, keepdim=True)\n",
    "            # W_normalized = Wr / Wr.std(dim=1, keepdim=True)\n",
    "            # W = W_normalized            \n",
    "            \n",
    "            # print('after',torch.diag(H).mean())\n",
    "            \n",
    "            # ckpt_path = os.path.join(comp_path, f\"{idx}_{n_map[n.split('.')[-1]]}.pt\")\n",
    "            # W_hat = torch.load(ckpt_path, map_location='cuda', weights_only=False)['W_hat']\n",
    "            \n",
    "            # mse = (W_hat - W)**2\n",
    "            # W_hat = W_hat.cpu()\n",
    "            # r_std = Wr.std(dim=1, keepdim=True)\n",
    "            W = Wr\n",
    "                        \n",
    "            shape = W.shape\n",
    "\n",
    "            # variants = {\n",
    "            #     'original': W,\n",
    "            #     # 'row_norm': W / (W.std(dim=1, keepdim=True) + 1e-6),\n",
    "            #     # 'col_norm': W / (W.std(dim=0, keepdim=True) + 1e-6),\n",
    "            # }\n",
    "            \n",
    "            variants = {\n",
    "                'original': W / (W.std() + 1e-6),\n",
    "                'row_norm': W / (W.std(dim=1, keepdim=True) + 1e-6),\n",
    "                'col_norm': W / (W.std(dim=0, keepdim=True) + 1e-6),\n",
    "            }\n",
    "            \n",
    "            norms = {}\n",
    "            for name, W_var in variants.items():\n",
    "                norms[name] = {'rnorm': W_var.norm(p=2, dim=1, keepdim=True), \n",
    "                               'cnrom': W_var.norm(p=2, dim=0, keepdim=True)}\n",
    "            \n",
    "            \n",
    "            block_sizes= [8, 16, 64, 128, -1]\n",
    "            # block_sizes= [8, 16]\n",
    "            fig, axes = plt.subplots(len(block_sizes), 6, figsize=(24, 4 * len(block_sizes)))\n",
    "            \n",
    "            for i, block_size in enumerate(block_sizes):\n",
    "                pca_results = []\n",
    "                for name, W_var in variants.items():\n",
    "                    \n",
    "                    # reshape for row and col PCA\n",
    "                    bs = block_size if block_size > 0 else shape[1]\n",
    "                    row_data = W_var.reshape(-1, bs).cpu().numpy()\n",
    "                    row_pca = PCA(n_components=2).fit_transform(row_data)\n",
    "                    # row_std_block = (W_var.reshape(-1, bs).norm(p=2, dim=1)*torch.sqrt(bs)).cpu().numpy()  \n",
    "                    # row_mse_block = mse.reshape(-1, bs).mean(dim=1).cpu().numpy()\n",
    "                    row_norm = norms[name]['rnorm'].repeat(1, W_var.shape[1]//bs).reshape(-1 ).cpu().numpy()\n",
    "                    \n",
    "                    bs = block_size if block_size > 0 else shape[0]\n",
    "                    col_data = W_var.T.reshape(-1, bs).cpu().numpy()\n",
    "                    col_pca = PCA(n_components=2).fit_transform(col_data)\n",
    "                    # col_mse_block = mse.T.reshape(-1, bs).mean(dim=1).cpu().numpy()\n",
    "                    # col_std_block = (W_var.T.reshape(-1, bs).norm(p=2, dim=1)*torch.sqrt(bs)).cpu().numpy()  \n",
    "                    col_norm = norms[name]['cnrom'].repeat(W_var.shape[0]//bs, 1).T.reshape(-1 ).cpu().numpy()            \n",
    "                    \n",
    "                    # 축 범위 맞추기\n",
    "                    all_valid = [p for p in [row_pca, col_pca] if p is not None]\n",
    "                    if all_valid:\n",
    "                        all_concat = np.concatenate(all_valid, axis=0)\n",
    "                        max_abs = np.abs(all_concat).max()\n",
    "                    else:\n",
    "                        max_abs = 1.0\n",
    "\n",
    "                    pca_results.extend([\n",
    "                        (f\"{name} row PCA\", row_pca, max_abs, row_norm),\n",
    "                        (f\"{name} col PCA\", col_pca, max_abs, col_norm)\n",
    "                    ])\n",
    "                \n",
    "                # plot\n",
    "                # axes = axes.flatten()\n",
    "                axs = axes[i]\n",
    "                for ax, (title, data, max_abs, errors) in zip(axs, pca_results):\n",
    "                    if data is not None:\n",
    "                        sc = ax.scatter(data[:, 0], data[:, 1], s=5, alpha=0.7, c=errors, cmap='viridis')\n",
    "                        # sc = ax.scatter(data[:, 0], data[:, 1], s=5, alpha=0.7)\n",
    "                        ax.set_xlim(-max_abs, max_abs)\n",
    "                        ax.set_ylim(-max_abs, max_abs)\n",
    "                        plt.colorbar(sc, ax=ax)  # 컬러바 추가\n",
    "                    ax.set_title(f\"{idx}_{n.split('.')[-1]} ({title}) block_size {block_size}\")\n",
    "                    ax.set_xlabel(\"PC1\")\n",
    "                    ax.set_ylabel(\"PC2\")\n",
    "                    ax.grid(True)\n",
    "\n",
    "            plt.suptitle(f\"PCA of {model_name} Layer {idx} / {n.split('.')[-1]}\", fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            os.makedirs(f\"./plot/pca_scaleh_vector_norm_colarbar/{model_name}/{n.split('.')[-1]}\", exist_ok=True)\n",
    "            plt.savefig(f\"./plot/pca_scaleh_vector_norm_colarbar/{model_name}/{n.split('.')[-1]}/{idx}_{n.split('.')[-1]}.png\")\n",
    "            # plt.show()\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_linears.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a69f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "model_list = [\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "]\n",
    "\n",
    "block_size = 16\n",
    "\n",
    "for model_name in model_list:\n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)    \n",
    "\n",
    "    for idx in [0, 1, 10, 31]:\n",
    "        named_linears = get_named_linears(layers[idx])\n",
    "        \n",
    "        for (n, m) in zip(named_linears.items()):\n",
    "            W = m.weight.data.detach()\n",
    "            shape = W.shape\n",
    "\n",
    "            # # 정규분포 기반 residual 생성\n",
    "            # W_flat = W.flatten()\n",
    "            # mu = W_flat.mean().item()\n",
    "            # sigma = W_flat.std(unbiased=False).item()\n",
    "            # fitted = torch.tensor(norm(loc=mu, scale=sigma).rvs(size=W_flat.numel()))\n",
    "            # residual = (W_flat - fitted).reshape(shape)\n",
    "\n",
    "            # PCA 계산\n",
    "            row_pca, col_pca = None, None\n",
    "            try:\n",
    "                bs = block_size if block_size > 0 else shape[1]\n",
    "                pca_row = PCA(n_components=2)\n",
    "                row_pca = pca_row.fit_transform(W.reshape(-1, bs).cpu())\n",
    "            except Exception as e:\n",
    "                print(f\"[Row PCA] {n} failed: {e}\")\n",
    "\n",
    "            try:\n",
    "                bs = block_size if block_size > 0 else shape[0]\n",
    "                pca_col = PCA(n_components=2)\n",
    "                col_pca = pca_col.fit_transform(W.T.reshape(-1, block_size).cpu())\n",
    "            except Exception as e:\n",
    "                print(f\"[Col PCA] {n} failed: {e}\")\n",
    "\n",
    "            # x, y 동일한 범위로 설정\n",
    "            all_pca = []\n",
    "            if row_pca is not None:                all_pca.append(row_pca)\n",
    "            if col_pca is not None:                all_pca.append(col_pca)\n",
    "            all_concat = np.concatenate(all_pca, axis=0)\n",
    "            max_abs = np.abs(all_concat).max()\n",
    "\n",
    "            fig, axes = plt.subplots(4, 2, figsize=(10, 5 * 4))\n",
    "\n",
    "            # Row PCA plot\n",
    "            if row_pca is not None:\n",
    "                ax_row.scatter(row_pca[:, 0], row_pca[:, 1], s=5, alpha=0.6)\n",
    "                ax_row.set_title(f\"{idx}_{n.split('.')[-1]} (Row PCA block_size={block_size})\")\n",
    "                ax_row.set_xlim(-max_abs, max_abs)\n",
    "                ax_row.set_ylim(-max_abs, max_abs)\n",
    "                ax_row.set_xlabel(\"PC1\")\n",
    "                ax_row.set_ylabel(\"PC2\")\n",
    "                ax_row.grid(True)\n",
    "            else:\n",
    "                ax_row.set_title(f\"{idx}_{n} (Row PCA failed)\")\n",
    "\n",
    "            # Column PCA plot\n",
    "            if col_pca is not None:\n",
    "                ax_col.scatter(col_pca[:, 0], col_pca[:, 1], s=5, alpha=0.6, color='orange')\n",
    "                ax_col.set_title(f\"{idx}_{n.split('.')[-1]} (Column PCA block_size={block_size})\")\n",
    "                ax_col.set_xlim(-max_abs, max_abs)\n",
    "                ax_col.set_ylim(-max_abs, max_abs)\n",
    "                ax_col.set_xlabel(\"PC1\")\n",
    "                ax_col.set_ylabel(\"PC2\")\n",
    "                ax_col.grid(True)\n",
    "            else:\n",
    "                ax_col.set_title(f\"{idx}_{n} (Col PCA failed)\")\n",
    "\n",
    "            plt.suptitle(f\"PCA of Residual Vectors (Layer {idx})\", fontsize=18)\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "            os.makedirs(f\"./plot/pca_residual/{model_name}\", exist_ok=True)\n",
    "            plt.savefig(f\"./plot/pca_residual/{model_name}/{idx}_{n.split('.')[-1]}_block_size{block_size}.png\")\n",
    "            plt.show()\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95392869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_list = [\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "]\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "for model_name in model_list:\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)    \n",
    "\n",
    "    for idx in [0, 1, 10, 31]:  # 하나의 레이어만 예시\n",
    "        named_linears = get_named_linears(layers[idx])\n",
    "        num_layers = len(named_linears)\n",
    "        fig, axes = plt.subplots(num_layers, 2, figsize=(10, 10 * num_layers))\n",
    "\n",
    "        for (n, m), (ax_row, ax_col) in zip(named_linears.items(), axes):\n",
    "            W = m.weight.data.detach()\n",
    "            shape = W.shape\n",
    "\n",
    "            # 정규분포 기반 residual 생성\n",
    "            W_flat = W.flatten()\n",
    "            mu = W_flat.mean().item()\n",
    "            sigma = W_flat.std(unbiased=False).item()\n",
    "            fitted = torch.tensor(norm(loc=mu, scale=sigma).rvs(size=W_flat.numel()))\n",
    "            residual = (W_flat - fitted).reshape(shape)\n",
    "            residual_blockwise = residual.reshape(-1, block_size)\n",
    "\n",
    "            # Row-wise PCA\n",
    "            try:\n",
    "                pca_row = PCA(n_components=2)\n",
    "                # row_pca = pca_row.fit_transform(residual_blockwise.cpu())\n",
    "                row_pca = pca_row.fit_transform(W.cpu())\n",
    "                ax_row.scatter(row_pca[:, 0], row_pca[:, 1], s=5, alpha=0.6)\n",
    "                ax_row.set_title(f\"{idx}_{n} (Row PCA)\")\n",
    "                ax_row.set_xlabel(\"PC1\")\n",
    "                ax_row.set_ylabel(\"PC2\")\n",
    "                ax_row.grid(True)\n",
    "            except Exception as e:\n",
    "                ax_row.set_title(f\"{idx}_{n} (Row PCA failed)\")\n",
    "                print(f\"[Row PCA] {n} failed: {e}\")\n",
    "\n",
    "            # Column-wise PCA\n",
    "            residual_blockwise = residual.T.reshape(-1, block_size)\n",
    "            try:\n",
    "                pca_col = PCA(n_components=2)\n",
    "                # col_pca = pca_col.fit_transform(residual_blockwise.cpu())  # transpose\n",
    "                col_pca = pca_col.fit_transform(W.T.cpu())  # transpose\n",
    "                ax_col.scatter(col_pca[:, 0], col_pca[:, 1], s=5, alpha=0.6, color='orange')\n",
    "                ax_col.set_title(f\"{idx}_{n} (Column PCA)\")\n",
    "                ax_col.set_xlabel(\"PC1\")\n",
    "                ax_col.set_ylabel(\"PC2\")\n",
    "                ax_col.grid(True)\n",
    "            except Exception as e:\n",
    "                ax_col.set_title(f\"{idx}_{n} (Col PCA failed)\")\n",
    "                print(f\"[Col PCA] {n} failed: {e}\")\n",
    "\n",
    "        plt.suptitle(f\"PCA of Residual Vectors (Layer {idx})\", fontsize=18)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "        os.makedirs(f\"./plot/pca_residual/{model_name}\", exist_ok=True)\n",
    "        # plt.savefig(f\"./plot/pca_residual/{model_name}/{idx}.png\")\n",
    "        plt.show()\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_list = [\n",
    "    # 'meta-llama--Llama-2-7b-hf',\n",
    "    'meta-llama--Llama-2-7b-hf',\n",
    "]\n",
    "\n",
    "for model_name in model_list:\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)    \n",
    "\n",
    "    for idx in [0]:  # 하나의 레이어만 예시\n",
    "        named_linears = get_named_linears(layers[idx])\n",
    "        num_plots = len(named_linears)\n",
    "        cols = 3\n",
    "        rows = (num_plots + cols - 1) // cols\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5 * rows))\n",
    "\n",
    "        for ax, (n, m) in zip(axes.flat, named_linears.items()):\n",
    "            W = m.weight.data.detach()\n",
    "            shape = W.shape\n",
    "\n",
    "            W_flat = W.flatten()\n",
    "            mu = W_flat.mean().item()\n",
    "            sigma = W_flat.std(unbiased=False).item()\n",
    "            fitted = torch.tensor(norm(loc=mu, scale=sigma).rvs(size=W_flat.numel()))\n",
    "            residual = W_flat - fitted\n",
    "            residual = residual.reshape(shape)\n",
    "\n",
    "            # PCA: row 기준\n",
    "            try:\n",
    "                pca = PCA(n_components=2)\n",
    "                W_pca = pca.fit_transform(residual.cpu())\n",
    "                ax.scatter(W_pca[:, 0], W_pca[:, 1], s=5, alpha=0.6)\n",
    "                ax.set_title(f\"{idx}_{n}\")\n",
    "                ax.set_xlabel(\"PC1\")\n",
    "                ax.set_ylabel(\"PC2\")\n",
    "                ax.grid(True)\n",
    "            except Exception as e:\n",
    "                ax.set_title(f\"{idx}_{n} (PCA failed)\")\n",
    "                print(f\"Warning: PCA failed for {n}, shape={shape}, error={e}\")\n",
    "\n",
    "        # 빈 subplot 처리\n",
    "        for ax in axes.flat[num_plots:]:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"PCA of Residual Row Vectors (Layer {idx})\", fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "        plt.show()\n",
    "            \n",
    "            # group_size = 256  # 또는 128\n",
    "            # num_groups = W.shape[0] * (W.shape[1] // group_size)\n",
    "            # residuals_all = []\n",
    "            # for row in W:\n",
    "            #     for i in range(0, W.shape[1], group_size):\n",
    "            #         group = row[i:i+group_size]\n",
    "            #         if group.numel() != group_size:\n",
    "            #             continue  # 마지막 잘리는 경우 skip\n",
    "            #         # 정규분포 피팅 (MLE)\n",
    "            #         mu = group.mean().item()\n",
    "            #         sigma = group.std(unbiased=False).item()\n",
    "            #         # Gaussian 값 생성\n",
    "            #         fitted = torch.tensor(norm(loc=mu, scale=sigma).rvs(size=group_size))\n",
    "            #         # 잔차 계산\n",
    "            #         residual = group - fitted\n",
    "            #         residuals_all.append(residual.numpy())\n",
    "            # # 모든 residual flatten\n",
    "            # residuals_all = np.concatenate(residuals_all)\n",
    "\n",
    "            # 시각화\n",
    "            # plt.figure(figsize=(8, 4))\n",
    "            # sns.histplot(residual, kde=False, bins=1000, color='skyblue')\n",
    "            # plt.title(f\"{idx}_{n}, Residual Distribution after Gaussian Fit\")\n",
    "            # plt.xlabel(\"Residual\")\n",
    "            # plt.ylabel(\"Frequency\")\n",
    "            # plt.grid(True)\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09968c9",
   "metadata": {},
   "source": [
    "# PCA SVD scaleH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd11a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.36it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './quip_hess/llama3_8b_6144/2_qkv.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m named_linears \u001b[38;5;241m=\u001b[39m get_named_linears(layers[idx])\n\u001b[1;32m     31\u001b[0m hess_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 32\u001b[0m hess_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqkv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mquip_hess\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_qkv.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m hess_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquip_hess\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_o.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m hess_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mup\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquip_hess\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_up.pt\u001b[39m\u001b[38;5;124m'\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './quip_hess/llama3_8b_6144/2_qkv.pt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "]\n",
    "\n",
    "n_map = {'q_proj':'q', 'k_proj':'k', 'v_proj':'v', 'o_proj':'o', 'gate_proj':'gate', 'up_proj':'up', 'down_proj':'down'}\n",
    "\n",
    "quip_hess_path = [\n",
    "    './quip_hess/llama3_8b_6144',\n",
    "]\n",
    "\n",
    "# sigma_reg 값 (필요시 조정)\n",
    "sigma_reg = 1e-4\n",
    "for model_name, quip_hess in zip(model_list, quip_hess_path):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "    for idx in [2]:\n",
    "        named_linears = get_named_linears(layers[idx])\n",
    "        \n",
    "        hess_dict = {}\n",
    "        hess_dict['qkv'] = torch.load(f'{quip_hess}/{idx}_qkv.pt', weights_only=False)\n",
    "        hess_dict['o'] = torch.load(f'{quip_hess}/{idx}_o.pt', weights_only=False)\n",
    "        hess_dict['up'] = torch.load(f'{quip_hess}/{idx}_up.pt', weights_only=False)\n",
    "        hess_dict['down'] = torch.load(f'{quip_hess}/{idx}_down.pt', weights_only=False)\n",
    "\n",
    "        for n, m in named_linears.items():\n",
    "            W = m.weight.data.detach().cuda()\n",
    "            \n",
    "            if 'q_proj' in n or 'k_proj' in n or 'v_proj' in n:\n",
    "                H_flat = hess_dict['qkv']\n",
    "            elif 'o_proj' in n:\n",
    "                H_flat = hess_dict['o']\n",
    "            elif 'up_proj' in n or 'gate_proj' in n:\n",
    "                H_flat = hess_dict['up']\n",
    "            elif 'down_proj' in n:\n",
    "                H_flat = hess_dict['down']\n",
    "            else:\n",
    "                raise NotImplementedError(n)\n",
    "            \n",
    "            H = flat_to_sym(H_flat['flatH'], H_flat['n']).cuda()\n",
    "            mu = H_flat['mu'].cuda()\n",
    "            H.add_(mu[None, :] * mu[:, None])\n",
    "            n_h = H_flat['n']\n",
    "\n",
    "            # print('before',torch.diag(H).mean())\n",
    "            H = regularize_H(H, n_h, sigma_reg)\n",
    "            \n",
    "            diagH = torch.diag(H)\n",
    "            diagH = torch.clamp(diagH, min=1e-8)\n",
    "            scaleWH = diagH.sqrt()\n",
    "            Wr = W * scaleWH[None, :]\n",
    "            \n",
    "            W = Wr\n",
    "                        \n",
    "            shape = W.shape\n",
    "            \n",
    "            variants = {\n",
    "                'original': W / (W.std() + 1e-6),\n",
    "                'row_norm': W / (W.std(dim=1, keepdim=True) + 1e-6),\n",
    "                'col_norm': W / (W.std(dim=0, keepdim=True) + 1e-6),\n",
    "            }\n",
    "            \n",
    "            # ### SVD 기반 분석을 위해 플롯 설정 변경 ###\n",
    "            # block_size 루프가 없으므로 1행 6열로 변경\n",
    "            fig, axes = plt.subplots(1, 6, figsize=(24, 5))\n",
    "            \n",
    "            pca_results = []\n",
    "            \n",
    "            for name, W_var in variants.items():\n",
    "                \n",
    "                # ### 핵심 변경 사항: SVD 수행 ###\n",
    "                # full_matrices=False for efficiency (thin SVD)\n",
    "                U, S, Vh = torch.linalg.svd(W_var, full_matrices=False)\n",
    "                V = Vh.T # We need V, not Vh\n",
    "                \n",
    "                # 특이 벡터들(U와 V의 열벡터)을 PCA의 데이터 포인트로 사용\n",
    "                # U.T: 각 행이 좌측 특이 벡터가 됨\n",
    "                # V.T: 각 행이 우측 특이 벡터가 됨\n",
    "                u_data = U.T.cpu().numpy()\n",
    "                v_data = V.T.cpu().numpy()\n",
    "                \n",
    "                # PCA 수행\n",
    "                u_pca = PCA(n_components=2).fit_transform(u_data)\n",
    "                v_pca = PCA(n_components=2).fit_transform(v_data)\n",
    "                \n",
    "                # 컬러바에 사용할 특이값\n",
    "                singular_values = S.cpu().numpy()\n",
    "                \n",
    "                # 축 범위를 맞추기 위한 계산\n",
    "                all_concat = np.concatenate([u_pca, v_pca], axis=0)\n",
    "                max_abs = np.abs(all_concat).max() * 1.1 # 약간의 여백 추가\n",
    "\n",
    "                # 결과 저장\n",
    "                pca_results.extend([\n",
    "                    (f\"{name} Left Singular Vectors (U)\", u_pca, max_abs, singular_values),\n",
    "                    (f\"{name} Right Singular Vectors (V)\", v_pca, max_abs, singular_values)\n",
    "                ])\n",
    "            \n",
    "            # ### 플로팅 ###\n",
    "            for ax, (title, data, max_abs, colors) in zip(axes.flatten(), pca_results):\n",
    "                sc = ax.scatter(data[:, 0], data[:, 1], s=10, alpha=0.8, c=colors, cmap='viridis', \n",
    "                                norm=plt.Normalize(vmin=np.min(singular_values), vmax=np.max(singular_values)))\n",
    "                ax.set_xlim(-max_abs, max_abs)\n",
    "                ax.set_ylim(-max_abs, max_abs)\n",
    "                ax.set_aspect('equal', adjustable='box') # 종횡비 1:1\n",
    "                fig.colorbar(sc, ax=ax, label=\"Singular Value\")\n",
    "                ax.set_title(f\"{title}\")\n",
    "                ax.set_xlabel(\"PC1\")\n",
    "                ax.set_ylabel(\"PC2\")\n",
    "                ax.grid(True)\n",
    "\n",
    "            layer_name = n.split('.')[-1]\n",
    "            plt.suptitle(f\"PCA of Singular Vectors - {model_name_fs} Layer {idx} / {layer_name}\", fontsize=16)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            \n",
    "            save_dir = f\"./plot/pca_singular_vectors/{model_name_fs}/{layer_name}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            plt.savefig(f\"{save_dir}/{idx}_{layer_name}.png\")\n",
    "            plt.show()\n",
    "            # plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29585d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
