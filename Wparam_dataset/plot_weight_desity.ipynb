{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2961b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, AutoModelForCausalLM, LlamaForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, OPTForCausalLM, BloomForCausalLM\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import functools\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from tinychat.models import LlavaLlamaForCausalLM\n",
    "from transformers.models.bloom.modeling_bloom import BloomForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "from transformers.models.opt.modeling_opt import OPTForCausalLM\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92173db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_named_linears(module):\n",
    "    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}\n",
    "\n",
    "def get_blocks(model):\n",
    "    if model.__class__.__name__ in (\"LlamaForCausalLM\", \"Qwen2ForCausalLM\"):\n",
    "        layers = model.model.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaForCausalLM\":\n",
    "        # layers = [model.model.layers, model.model.vision_tower.vision_tower.vision_model.encoder.layers]\n",
    "        layers = model.model.layers\n",
    "    elif isinstance(model, OPTForCausalLM):\n",
    "        layers = model.model.decoder.layers\n",
    "    elif isinstance(model, BloomForCausalLM):\n",
    "        layers = model.transformer.h\n",
    "    elif \"mpt\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.blocks\n",
    "    elif \"falcon\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"bigcode\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"neox\" in str(model.__class__).lower():\n",
    "        layers = model.gpt_neox.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaModel\":\n",
    "        layers = model.llm.model.layers\n",
    "    else:\n",
    "        raise NotImplementedError(type(model))\n",
    "    return layers\n",
    "\n",
    "def get_calib_dataset(data=\"pileval\", tokenizer=None, n_samples=512, block_size=512):\n",
    "    if data == \"pileval\":\n",
    "        # dataset = load_dataset(\"mit-han-lab/pile-val-backup\", split=\"validation\")\n",
    "        dataset = load_dataset(\"NeelNanda/pile-10k\", split=\"train\")        \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    samples = []\n",
    "    n_run = 0\n",
    "    for data in dataset:\n",
    "        line = data[\"text\"]\n",
    "        line = line.strip()\n",
    "        line_encoded = tokenizer.encode(line)\n",
    "        if len(line_encoded) > 512:\n",
    "            continue\n",
    "        sample = torch.tensor([line_encoded])\n",
    "        if sample.numel() == 0:\n",
    "            continue\n",
    "        samples.append(sample)\n",
    "        n_run += 1\n",
    "        if n_run == n_samples:\n",
    "            break\n",
    "    # now concatenate all samples and split according to block size\n",
    "    cat_samples = torch.cat(samples, dim=1)\n",
    "    n_split = cat_samples.shape[1] // block_size\n",
    "    print(f\" * Split into {n_split} blocks\")\n",
    "    return [\n",
    "        cat_samples[:, i * block_size : (i + 1) * block_size] for i in range(n_split)\n",
    "    ]\n",
    "    \n",
    "def move_embed(model, device):\n",
    "    if isinstance(model, LlamaForCausalLM):\n",
    "        model.model.embed_tokens = model.model.embed_tokens.to(device)\n",
    "        ## add the following line to move rotary_emb to GPU as well\n",
    "        model.model.rotary_emb = model.model.rotary_emb.to(device)\n",
    "    # elif isinstance(model, LlavaLlamaForCausalLM):\n",
    "    #     model.model.embed_tokens = model.model.embed_tokens.to(device)\n",
    "    #     model.model.vision_tower.vision_tower.vision_model.embeddings.to(device)\n",
    "    elif isinstance(model, OPTForCausalLM):\n",
    "        model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(device)\n",
    "        model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(device)\n",
    "    elif isinstance(model, BloomForCausalLM):\n",
    "        model.transformer.word_embeddings = model.transformer.word_embeddings.to(device)\n",
    "        model.transformer.word_embeddings_layernorm = model.transformer.word_embeddings_layernorm.to(device)\n",
    "    elif \"mpt\" in str(model.__class__).lower():\n",
    "        model.transformer.wte = model.transformer.wte.to(device)\n",
    "        model.transformer.emb_drop = model.transformer.emb_drop.to(device)\n",
    "    elif \"falcon\" in str(model.__class__).lower():\n",
    "        model.transformer.word_embeddings = model.transformer.word_embeddings.to(device)\n",
    "    elif \"bigcode\" in str(model.__class__).lower():\n",
    "        model.transformer.wte = model.transformer.wte.to(device)\n",
    "        model.transformer.wpe = model.transformer.wpe.to(device)\n",
    "        model.transformer.drop = model.transformer.drop.to(device)\n",
    "    elif \"neox\" in str(model.__class__).lower():\n",
    "        model.gpt_neox.embed_in = model.gpt_neox.embed_in.to(device)\n",
    "        model.gpt_neox.emb_dropout = model.gpt_neox.emb_dropout.to(device)\n",
    "        model.embed_out = model.embed_out.to(device)\n",
    "    else:\n",
    "        raise NotImplementedError(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842536f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_to_sym(V, N):\n",
    "    A = torch.zeros(N, N, dtype=V.dtype, device=V.device)\n",
    "    idxs = torch.tril_indices(N, N, device=V.device)\n",
    "    A[idxs.unbind()] = V\n",
    "    A[idxs[1, :], idxs[0, :]] = V\n",
    "    return A\n",
    "\n",
    "def regularize_H(H, n, sigma_reg):\n",
    "    H.div_(torch.diag(H).mean())\n",
    "    idx = torch.arange(n)\n",
    "    H[idx, idx] += sigma_reg\n",
    "    return H\n",
    "\n",
    "import sys\n",
    "sys.path.append('/workspace/Weight_compression/Wparam_dataset')\n",
    "from utils import *\n",
    "\n",
    "def RHT_H(H, SU):\n",
    "    return matmul_hadUt(matmul_hadUt(H * SU).T * SU)\n",
    "\n",
    "\n",
    "def RHT_W(W, SU, SV):\n",
    "    return matmul_hadUt(matmul_hadUt(W.T * SV).T * SU)\n",
    "\n",
    "\n",
    "def incoherence_preprocess(H, W, args):\n",
    "    # dtype_ = torch.float64 if args.use_fp64 else torch.float32\n",
    "    dtype_ = torch.float32\n",
    "    device = H.device\n",
    "    # device = torch.device('cpu')\n",
    "    (m, n) = H.shape\n",
    "\n",
    "    def _dump(Hr, Lhr, msg=''):\n",
    "        torch.save(Hr, f\"{args.save_pfx}/Hr_debug_fft.pt\")\n",
    "        torch.save(Lhr, f\"{args.save_pfx}/Lhr_debug_fft.pt\")\n",
    "        raise Exception(msg)\n",
    "\n",
    "    # diagonally rescale W,H to minimize proxy loss\n",
    "    scaleWH = None\n",
    "    Wr = W\n",
    "    Hr = H\n",
    "    # if args.rescale_WH:\n",
    "    if False:\n",
    "        Hr = H / H.abs().max()\n",
    "        diagH = torch.diag(Hr)\n",
    "        diagW2 = torch.diag(W.T @ W)\n",
    "        diagH = torch.clamp(diagH, min=1e-8)\n",
    "        diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "        scaleWH = (diagH / diagW2).sqrt().sqrt().to(torch.float32)\n",
    "        scaleWH = scaleWH.clamp(min=1e-8)\n",
    "        Wr = Wr * scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[None, :]\n",
    "        Hr = Hr / scaleWH[:, None]\n",
    "        scaleWH = scaleWH.cpu()\n",
    "\n",
    "    # randomized hadamard transformation on H, W\n",
    "    if True:\n",
    "        SU = (torch.randn(n, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        SV = (torch.randn(m, device=device).sign() + 1e-5).sign().to(dtype_)\n",
    "        Hr = RHT_H(Hr, SU)\n",
    "        # Wr = RHT_W(Wr, SU, SV)\n",
    "    # randomized kronecker product on H, W\n",
    "    elif args.incoh_mode == \"kron\":\n",
    "        SU = utils.rand_ortho_butterfly_noblock(n).to(dtype_).to(device)\n",
    "        SV = utils.rand_ortho_butterfly_noblock(m).to(dtype_).to(device)\n",
    "        Hr = SU @ Hr @ SU.T\n",
    "        Wr = SV @ Wr @ SU.T\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    SV = SV.cpu()\n",
    "    SU = SU.cpu()\n",
    "\n",
    "    # Lhr = torch.linalg.cholesky(Hr)\n",
    "    Lhr = None\n",
    "    # if not torch.all(torch.isfinite(Lhr)):\n",
    "    #     return None\n",
    "\n",
    "    # Wr = Wr.to(device)\n",
    "\n",
    "    return Lhr, Hr, Wr, SU, SV, scaleWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54040949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama/Llama-2-7b-hf',\n",
    "    # 'meta-llama/Llama-2-13b-hf',\n",
    "    # 'lmsys/vicuna-7b-v1.5',\n",
    "    # 'lmsys/vicuna-13b-v1.5',\n",
    "    # 'facebook/opt-6.7b',\n",
    "]\n",
    "\n",
    "quip_hess_path = [\n",
    "    './quip_hess/llama3_8b_6144',\n",
    "    # '/home/jgryu/Weight_compression/Wparam_dataset/quip_hess/Hessians-Llama-2-7b-6144',\n",
    "    # '/home/jgryu/Weight_compression/Wparam_dataset/quip_hess/Hessians-Llama-2-13b-6144'\n",
    "]\n",
    "\n",
    "n_samples=6144\n",
    "\n",
    "top = np.array([0.1, 1.0, 10.0])\n",
    "qlevel = [3, 2, 1]\n",
    "\n",
    "device='cpu'\n",
    "for model_name, quip_hess in zip(model_list, quip_hess_path):\n",
    "    \n",
    "    model_name = model_name.replace('/', '--')\n",
    "    print('model_name: ', model_name)\n",
    "\n",
    "    model_path = f\"./hf_model/{model_name}\"\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "    layers = get_blocks(model)\n",
    "\n",
    "    save_path = f'./hessian/{model_name}'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    input_ch_qlevel = []\n",
    "    for i in tqdm(range(len(layers))):\n",
    "        named_linears = get_named_linears(layers[i])\n",
    "        ql_one_layer = defaultdict(list)\n",
    "        \n",
    "        hess_dict = {}\n",
    "        hess_dict['qkv'] = torch.load(f'{quip_hess}/{i}_qkv.pt', weights_only=False)\n",
    "        hess_dict['o'] = torch.load(f'{quip_hess}/{i}_o.pt', weights_only=False)\n",
    "        hess_dict['up'] = torch.load(f'{quip_hess}/{i}_up.pt', weights_only=False)\n",
    "        hess_dict['down'] = torch.load(f'{quip_hess}/{i}_down.pt', weights_only=False)\n",
    "        \n",
    "        for n, m in named_linears.items():            \n",
    "            if 'q_proj' in n or 'k_proj' in n or 'v_proj' in n:\n",
    "                H_flat = hess_dict['qkv']\n",
    "            elif 'o_proj' in n:\n",
    "                H_flat = hess_dict['o']\n",
    "            elif 'up_proj' in n or 'gate_proj' in n:\n",
    "                H_flat = hess_dict['up']\n",
    "            elif 'down_proj' in n:\n",
    "                H_flat = hess_dict['down']\n",
    "            else:\n",
    "                raise NotImplementedError(n)\n",
    "            \n",
    "            H = flat_to_sym(H_flat['flatH'].to(device), H_flat['n'])\n",
    "            mu = H_flat['mu'].to(device)\n",
    "            H.add_(mu[None, :] * mu[:, None])\n",
    "            n_h = H_flat['n']\n",
    "            sigma_reg = 1e-6\n",
    "            H = regularize_H(H, n_h, sigma_reg)\n",
    "\n",
    "            data = []\n",
    "            W = m.weight.data.to(device=device)\n",
    "            data.append(W.flatten().cpu().numpy())\n",
    "\n",
    "            (M, N) = W.shape\n",
    "            SU = (torch.randn(N).sign() + 1e-5).sign().to(device)\n",
    "            SV = (torch.randn(M).sign() + 1e-5).sign().to(device)\n",
    "            W_rht = RHT_W(W, SU, SV)\n",
    "            data.append(W_rht.flatten().cpu().numpy())\n",
    "            \n",
    "            diagH = torch.diag(H)\n",
    "            diagW2 = torch.diag(W.T @ W)\n",
    "            diagH = torch.clamp(diagH, min=1e-8)\n",
    "            diagW2 = torch.clamp(diagW2, min=1e-8)\n",
    "            scaleWH = (diagH / diagW2).sqrt()\n",
    "            scaleWH = scaleWH.clamp(min=1e-8)\n",
    "            Wr = W * scaleWH[None, :]\n",
    "            \n",
    "            Wr = W * scaleWH[None, :]\n",
    "            data.append(Wr.flatten().cpu().numpy())\n",
    "\n",
    "            SU = (torch.randn(N).sign() + 1e-5).sign().to(device)\n",
    "            SV = (torch.randn(M).sign() + 1e-5).sign().to(device)\n",
    "            Wr_rht = RHT_W(Wr, SU, SV)\n",
    "            data.append(Wr_rht.flatten().cpu().numpy())\n",
    "\n",
    "            print(\"Start plot\")\n",
    "            labels = ['W', 'W_rht', 'Wr', 'Wr_rht']\n",
    "            fig, axs = plt.subplots(3, 1, figsize=(12, 10))\n",
    "\n",
    "            sns.boxplot(data=data, ax=axs[0])\n",
    "            axs[0].set_title('Box Plot for Multiple Datasets')\n",
    "            axs[0].set_xticklabels(labels)  # x축 라벨 설정\n",
    "\n",
    "            # 바이올린 플롯\n",
    "            sns.violinplot(data=data, palette='pastel', ax=axs[1])\n",
    "            axs[1].set_title('Violin Plot for Multiple Datasets')\n",
    "            axs[1].set_xticklabels(labels)  # x축 라벨 설정\n",
    "\n",
    "            # KDE 플롯\n",
    "            colors = ['blue', 'green', 'red', 'purple']  # 색상 리스트\n",
    "            for ii, dataset in enumerate(data):\n",
    "                sns.kdeplot(data=dataset, fill=True, color=colors[ii], label=labels[ii], common_norm=False, ax=axs[2])\n",
    "            axs[2].set_title('KDE Plot for Multiple Datasets')\n",
    "            axs[2].legend()  # 범례 표시\n",
    "\n",
    "            plt.suptitle(f'{i}_{n}')\n",
    "            plt.tight_layout()\n",
    "            filename = f'./plot/weight_rht_box/{model_name}/{i}_{n}.png'\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            plt.savefig(filename)\n",
    "            plt.close(fig)\n",
    "            print(\"End plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c716721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
