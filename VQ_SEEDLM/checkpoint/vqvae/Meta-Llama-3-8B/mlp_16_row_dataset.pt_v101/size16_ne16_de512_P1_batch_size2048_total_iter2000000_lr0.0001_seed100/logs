22:59:58 INFO - logger_setup: /home/jgryu/Weight_compression/VQ_SEEDLM/utils/util.py
22:59:58 INFO - ddp_or_single_process: Create new exp folder!
22:59:58 INFO - ddp_or_single_process: seed : 100
22:59:58 INFO - ddp_or_single_process: exp name : Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100
23:00:07 INFO - main: Create experiment save folder
23:00:20 INFO - main: Training mode : scratch!
23:00:20 INFO - main: batch_size : 2048
23:00:20 INFO - main: num of gpus: 1
23:00:20 INFO - main: Namespace(dist_port=6044, iter=2000000, dataset_path='../Wparam_dataset/dataset_per_row/meta-llama/Meta-Llama-3-8B/mlp_16_row_dataset.pt', learning_rate=0.0001, num_workers=2, batch_size=2048, seed=100, input_size=16, dim_encoder=512, P=1, K=16, n_embeddings=16, dim_embeddings=512, n_resblock=4, vq_beta=0.25, clip_max_norm=1.0, save_dir='vqvae', architecture='vqvae', checkpoint='None', save_path='./checkpoint/vqvae/Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100', logger=<Logger utils.util (INFO)>, **{'dev.num_gpus': 1})
23:00:51 INFO - logger_setup: /home/jgryu/Weight_compression/VQ_SEEDLM/utils/util.py
23:00:51 INFO - ddp_or_single_process: find checkpoint...
23:00:51 INFO - ddp_or_single_process: no checkpoint is here
23:00:51 INFO - ddp_or_single_process: seed : 100
23:00:51 INFO - ddp_or_single_process: exp name : Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100
23:00:52 INFO - main: Create experiment save folder
23:01:02 INFO - main: Training mode : scratch!
23:01:02 INFO - main: batch_size : 2048
23:01:02 INFO - main: num of gpus: 1
23:01:02 INFO - main: Namespace(dist_port=6044, iter=2000000, dataset_path='../Wparam_dataset/dataset_per_row/meta-llama/Meta-Llama-3-8B/mlp_16_row_dataset.pt', learning_rate=0.0001, num_workers=2, batch_size=2048, seed=100, input_size=16, dim_encoder=512, P=1, K=16, n_embeddings=16, dim_embeddings=512, n_resblock=4, vq_beta=0.25, clip_max_norm=1.0, save_dir='vqvae', architecture='vqvae', checkpoint='None', save_path='./checkpoint/vqvae/Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100', logger=<Logger utils.util (INFO)>, **{'dev.num_gpus': 1})
23:01:20 INFO - main: Train iter. 1/2000000 (5e-05%): 	Loss: 4.827638149261475	recon_loss: 1.9991724491119385	embedding_loss: 2.828465700149536
23:03:03 INFO - logger_setup: /home/jgryu/Weight_compression/VQ_SEEDLM/utils/util.py
23:03:03 INFO - ddp_or_single_process: find checkpoint...
23:03:03 INFO - ddp_or_single_process: no checkpoint is here
23:03:03 INFO - ddp_or_single_process: seed : 100
23:03:03 INFO - ddp_or_single_process: exp name : Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100
23:03:04 INFO - main: Create experiment save folder
23:03:14 INFO - main: Training mode : scratch!
23:03:14 INFO - main: batch_size : 2048
23:03:14 INFO - main: num of gpus: 1
23:03:14 INFO - main: Namespace(dist_port=6044, iter=2000000, dataset_path='../Wparam_dataset/dataset_per_row/meta-llama/Meta-Llama-3-8B/mlp_16_row_dataset.pt', learning_rate=0.0001, num_workers=2, batch_size=2048, seed=100, input_size=16, dim_encoder=512, P=1, K=16, n_embeddings=16, dim_embeddings=512, n_resblock=4, vq_beta=0.25, clip_max_norm=1.0, save_dir='vqvae', architecture='vqvae', checkpoint='None', save_path='./checkpoint/vqvae/Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100', logger=<Logger utils.util (INFO)>, **{'dev.num_gpus': 1})
23:03:32 INFO - main: Train iter. 1/2000000 (5e-05%): 	Loss: 4.827638149261475	recon_loss: 1.9991724491119385	embedding_loss: 2.828465700149536
23:03:37 INFO - main: Average_MSE: 1.3682419197076126
23:03:37 INFO - main: can not find prev_mse_best_model!
23:03:38 INFO - main: can not find recent_saved_model!
23:05:32 INFO - logger_setup: /home/jgryu/Weight_compression/VQ_SEEDLM/utils/util.py
23:05:32 INFO - ddp_or_single_process: find checkpoint...
23:05:32 INFO - ddp_or_single_process: checkpoint exist, name: recent_model_MSE_1.36824_total_iter_1.pth.tar
23:05:32 INFO - ddp_or_single_process: seed : 100
23:05:32 INFO - ddp_or_single_process: exp name : Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100
23:05:34 INFO - main: Create experiment save folder
23:05:43 INFO - main: Training mode : scratch!
23:05:43 INFO - main: batch_size : 2048
23:05:43 INFO - main: num of gpus: 1
23:05:43 INFO - main: Namespace(dist_port=6044, iter=2000000, dataset_path='../Wparam_dataset/dataset_per_row/meta-llama/Meta-Llama-3-8B/mlp_16_row_dataset.pt', learning_rate=0.0001, num_workers=2, batch_size=2048, seed=100, input_size=16, dim_encoder=512, P=1, K=16, n_embeddings=16, dim_embeddings=512, n_resblock=4, vq_beta=0.25, clip_max_norm=1.0, save_dir='vqvae', architecture='vqvae', checkpoint='./checkpoint/vqvae/Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100/recent_model_MSE_1.36824_total_iter_1.pth.tar', save_path='./checkpoint/vqvae/Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100', logger=<Logger utils.util (INFO)>, **{'dev.num_gpus': 1})
23:05:43 INFO - main: Loading ./checkpoint/vqvae/Meta-Llama-3-8B/mlp_16_row_dataset.pt_v101/size16_ne16_de512_P1_batch_size2048_total_iter2000000_lr0.0001_seed100/recent_model_MSE_1.36824_total_iter_1.pth.tar
23:08:15 INFO - main: Train iter. 1000/2000000 (0.05%): 	Loss: 16.827714920043945	recon_loss: 0.8527882695198059	embedding_loss: 15.974926948547363
23:10:33 INFO - main: Train iter. 2000/2000000 (0.1%): 	Loss: 4.847074508666992	recon_loss: 0.8660564422607422	embedding_loss: 3.98101806640625
23:12:51 INFO - main: Train iter. 3000/2000000 (0.15%): 	Loss: 3.1108384132385254	recon_loss: 0.8661961555480957	embedding_loss: 2.2446422576904297
23:15:10 INFO - main: Train iter. 4000/2000000 (0.2%): 	Loss: 2.861959218978882	recon_loss: 0.8484280109405518	embedding_loss: 2.01353120803833
23:17:28 INFO - main: Train iter. 5000/2000000 (0.25%): 	Loss: 2.948195695877075	recon_loss: 0.8390777111053467	embedding_loss: 2.1091179847717285
23:19:47 INFO - main: Train iter. 6000/2000000 (0.3%): 	Loss: 2.326073408126831	recon_loss: 0.8109999299049377	embedding_loss: 1.515073537826538
23:22:06 INFO - main: Train iter. 7000/2000000 (0.35%): 	Loss: 2.649726152420044	recon_loss: 0.8147901296615601	embedding_loss: 1.8349360227584839
23:24:26 INFO - main: Train iter. 8000/2000000 (0.4%): 	Loss: 2.3507468700408936	recon_loss: 0.8026943802833557	embedding_loss: 1.548052430152893
23:26:46 INFO - main: Train iter. 9000/2000000 (0.45%): 	Loss: 2.294046640396118	recon_loss: 0.7809147834777832	embedding_loss: 1.513131856918335
23:29:06 INFO - main: Train iter. 10000/2000000 (0.5%): 	Loss: 2.100437879562378	recon_loss: 0.7873024344444275	embedding_loss: 1.3131353855133057
23:31:26 INFO - main: Train iter. 11000/2000000 (0.55%): 	Loss: 1.954785704612732	recon_loss: 0.7641962766647339	embedding_loss: 1.190589427947998
23:33:46 INFO - main: Train iter. 12000/2000000 (0.6%): 	Loss: 1.9233272075653076	recon_loss: 0.7552376389503479	embedding_loss: 1.168089509010315
23:36:06 INFO - main: Train iter. 13000/2000000 (0.65%): 	Loss: 1.8804423809051514	recon_loss: 0.7499787211418152	embedding_loss: 1.1304636001586914
23:38:25 INFO - main: Train iter. 14000/2000000 (0.7%): 	Loss: 1.9490199089050293	recon_loss: 0.7374619841575623	embedding_loss: 1.2115578651428223
23:40:46 INFO - main: Train iter. 15000/2000000 (0.75%): 	Loss: 1.8190892934799194	recon_loss: 0.720589280128479	embedding_loss: 1.0985000133514404
23:43:05 INFO - main: Train iter. 16000/2000000 (0.8%): 	Loss: 1.8422049283981323	recon_loss: 0.7266043424606323	embedding_loss: 1.1156005859375
23:45:25 INFO - main: Train iter. 17000/2000000 (0.85%): 	Loss: 1.9279499053955078	recon_loss: 0.7367245554924011	embedding_loss: 1.191225290298462
23:47:44 INFO - main: Train iter. 18000/2000000 (0.9%): 	Loss: 1.8474996089935303	recon_loss: 0.7295757532119751	embedding_loss: 1.1179238557815552
23:50:03 INFO - main: Train iter. 19000/2000000 (0.95%): 	Loss: 1.8571586608886719	recon_loss: 0.7461413741111755	embedding_loss: 1.1110173463821411
23:52:22 INFO - main: Train iter. 20000/2000000 (1.0%): 	Loss: 1.8295660018920898	recon_loss: 0.7332457900047302	embedding_loss: 1.0963202714920044
23:54:38 INFO - main: Train iter. 21000/2000000 (1.05%): 	Loss: 1.803957223892212	recon_loss: 0.7246578335762024	embedding_loss: 1.0792994499206543
23:56:54 INFO - main: Train iter. 22000/2000000 (1.1%): 	Loss: 1.944969892501831	recon_loss: 0.709944486618042	embedding_loss: 1.235025405883789
23:59:09 INFO - main: Train iter. 23000/2000000 (1.15%): 	Loss: 1.9279439449310303	recon_loss: 0.7446804642677307	embedding_loss: 1.1832635402679443
00:01:24 INFO - main: Train iter. 24000/2000000 (1.2%): 	Loss: 1.79649019241333	recon_loss: 0.7063679695129395	embedding_loss: 1.0901222229003906
00:03:40 INFO - main: Train iter. 25000/2000000 (1.25%): 	Loss: 1.771580457687378	recon_loss: 0.6987934708595276	embedding_loss: 1.0727870464324951
00:05:56 INFO - main: Train iter. 26000/2000000 (1.3%): 	Loss: 1.7613009214401245	recon_loss: 0.7047871351242065	embedding_loss: 1.056513786315918
00:08:11 INFO - main: Train iter. 27000/2000000 (1.35%): 	Loss: 1.7547647953033447	recon_loss: 0.7044801712036133	embedding_loss: 1.0502846240997314
00:10:23 INFO - main: Train iter. 28000/2000000 (1.4%): 	Loss: 1.7285090684890747	recon_loss: 0.6951917409896851	embedding_loss: 1.0333173274993896
00:12:33 INFO - main: Train iter. 29000/2000000 (1.45%): 	Loss: 1.7246313095092773	recon_loss: 0.7020915150642395	embedding_loss: 1.0225398540496826
00:14:42 INFO - main: Train iter. 30000/2000000 (1.5%): 	Loss: 1.6865363121032715	recon_loss: 0.6820108294487	embedding_loss: 1.0045254230499268
00:16:57 INFO - main: Train iter. 31000/2000000 (1.55%): 	Loss: 1.711641550064087	recon_loss: 0.6932759284973145	embedding_loss: 1.0183656215667725
00:19:12 INFO - main: Train iter. 32000/2000000 (1.6%): 	Loss: 1.746953010559082	recon_loss: 0.7210981845855713	embedding_loss: 1.0258548259735107
00:21:31 INFO - main: Train iter. 33000/2000000 (1.65%): 	Loss: 1.687638282775879	recon_loss: 0.6908238530158997	embedding_loss: 0.9968143701553345
00:23:52 INFO - main: Train iter. 34000/2000000 (1.7%): 	Loss: 1.6651442050933838	recon_loss: 0.6861529350280762	embedding_loss: 0.9789912700653076
00:26:13 INFO - main: Train iter. 35000/2000000 (1.75%): 	Loss: 1.6371924877166748	recon_loss: 0.6824158430099487	embedding_loss: 0.9547767043113708
00:28:34 INFO - main: Train iter. 36000/2000000 (1.8%): 	Loss: 1.6504822969436646	recon_loss: 0.6876729130744934	embedding_loss: 0.9628093838691711
00:30:54 INFO - main: Train iter. 37000/2000000 (1.85%): 	Loss: 1.6115777492523193	recon_loss: 0.6790657043457031	embedding_loss: 0.9325119853019714
00:33:25 INFO - main: Train iter. 38000/2000000 (1.9%): 	Loss: 1.5670475959777832	recon_loss: 0.6567570567131042	embedding_loss: 0.9102905988693237
00:36:02 INFO - main: Train iter. 39000/2000000 (1.95%): 	Loss: 1.6383988857269287	recon_loss: 0.6841829419136047	embedding_loss: 0.9542160034179688
00:38:34 INFO - main: Train iter. 40000/2000000 (2.0%): 	Loss: 1.638200283050537	recon_loss: 0.6847459673881531	embedding_loss: 0.9534542560577393
00:40:53 INFO - main: Train iter. 41000/2000000 (2.05%): 	Loss: 1.5765211582183838	recon_loss: 0.6591188907623291	embedding_loss: 0.9174022078514099
00:43:13 INFO - main: Train iter. 42000/2000000 (2.1%): 	Loss: 1.5737279653549194	recon_loss: 0.658686637878418	embedding_loss: 0.9150413274765015
00:45:31 INFO - main: Train iter. 43000/2000000 (2.15%): 	Loss: 1.6075948476791382	recon_loss: 0.6774829626083374	embedding_loss: 0.9301118850708008
00:47:50 INFO - main: Train iter. 44000/2000000 (2.2%): 	Loss: 1.586733102798462	recon_loss: 0.6708334684371948	embedding_loss: 0.9158996343612671
00:50:09 INFO - main: Train iter. 45000/2000000 (2.25%): 	Loss: 1.569326400756836	recon_loss: 0.6657831072807312	embedding_loss: 0.90354323387146
00:52:27 INFO - main: Train iter. 46000/2000000 (2.3%): 	Loss: 1.617780327796936	recon_loss: 0.693091630935669	embedding_loss: 0.9246886968612671
00:54:45 INFO - main: Train iter. 47000/2000000 (2.35%): 	Loss: 1.5339847803115845	recon_loss: 0.6555038094520569	embedding_loss: 0.8784809708595276
00:57:02 INFO - main: Train iter. 48000/2000000 (2.4%): 	Loss: 1.5882759094238281	recon_loss: 0.6823556423187256	embedding_loss: 0.9059203267097473
00:59:20 INFO - main: Train iter. 49000/2000000 (2.45%): 	Loss: 1.552644968032837	recon_loss: 0.6649354100227356	embedding_loss: 0.8877095580101013
01:01:37 INFO - main: Train iter. 50000/2000000 (2.5%): 	Loss: 1.5561299324035645	recon_loss: 0.667755126953125	embedding_loss: 0.8883747458457947
01:01:41 INFO - main: Average_MSE: 0.6735914335201566
01:03:53 INFO - main: Train iter. 51000/2000000 (2.55%): 	Loss: 1.5122543573379517	recon_loss: 0.6493583917617798	embedding_loss: 0.8628959655761719
01:06:04 INFO - main: Train iter. 52000/2000000 (2.6%): 	Loss: 1.5801408290863037	recon_loss: 0.6774179935455322	embedding_loss: 0.9027227759361267
01:08:15 INFO - main: Train iter. 53000/2000000 (2.65%): 	Loss: 1.5397895574569702	recon_loss: 0.6666918992996216	embedding_loss: 0.8730976581573486
01:10:27 INFO - main: Train iter. 54000/2000000 (2.7%): 	Loss: 1.5211248397827148	recon_loss: 0.6609579920768738	embedding_loss: 0.8601668477058411
01:12:40 INFO - main: Train iter. 55000/2000000 (2.75%): 	Loss: 1.5022456645965576	recon_loss: 0.6517395377159119	embedding_loss: 0.8505061268806458
01:15:00 INFO - main: Train iter. 56000/2000000 (2.8%): 	Loss: 1.4594855308532715	recon_loss: 0.6374921202659607	embedding_loss: 0.8219934105873108
01:17:20 INFO - main: Train iter. 57000/2000000 (2.85%): 	Loss: 1.525308609008789	recon_loss: 0.6703771352767944	embedding_loss: 0.8549314141273499
01:19:38 INFO - main: Train iter. 58000/2000000 (2.9%): 	Loss: 1.4757721424102783	recon_loss: 0.644932210445404	embedding_loss: 0.8308398723602295
01:21:57 INFO - main: Train iter. 59000/2000000 (2.95%): 	Loss: 1.4745795726776123	recon_loss: 0.6434649229049683	embedding_loss: 0.8311145901679993
01:24:17 INFO - main: Train iter. 60000/2000000 (3.0%): 	Loss: 1.479473352432251	recon_loss: 0.6467833518981934	embedding_loss: 0.8326899409294128
01:26:37 INFO - main: Train iter. 61000/2000000 (3.05%): 	Loss: 1.463263988494873	recon_loss: 0.640949010848999	embedding_loss: 0.8223149180412292
01:28:56 INFO - main: Train iter. 62000/2000000 (3.1%): 	Loss: 1.4798113107681274	recon_loss: 0.6483563184738159	embedding_loss: 0.8314549922943115
01:31:16 INFO - main: Train iter. 63000/2000000 (3.15%): 	Loss: 1.4761979579925537	recon_loss: 0.6510884165763855	embedding_loss: 0.8251095414161682
01:33:35 INFO - main: Train iter. 64000/2000000 (3.2%): 	Loss: 1.4667003154754639	recon_loss: 0.641417384147644	embedding_loss: 0.825282871723175
01:35:54 INFO - main: Train iter. 65000/2000000 (3.25%): 	Loss: 1.4611430168151855	recon_loss: 0.6407702565193176	embedding_loss: 0.8203727602958679
01:38:10 INFO - main: Train iter. 66000/2000000 (3.3%): 	Loss: 1.448298692703247	recon_loss: 0.6341331601142883	embedding_loss: 0.8141655325889587
01:40:24 INFO - main: Train iter. 67000/2000000 (3.35%): 	Loss: 1.4889166355133057	recon_loss: 0.6541425585746765	embedding_loss: 0.8347740173339844
