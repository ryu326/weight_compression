{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import sys, os, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from VQ_SEEDLM import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_model(state_dict, model, weight_condition, batch_size=32768):\n",
    "    with torch.no_grad():\n",
    "        mean_MSE = 0\n",
    "        count = 0\n",
    "        mse_func = nn.MSELoss()\n",
    "        device = next(model.parameters()).device\n",
    "        recon_state_dict = {}\n",
    "        \n",
    "        for k, W in tqdm(state_dict.items()):\n",
    "            if not weight_condition in k: continue\n",
    "            # print(f'### Reconstructing {k} ####')\n",
    "            \n",
    "            W_reshaped = W.reshape(-1, model.input_size) # ( -1, -1) --> (-1, size, size)\n",
    "            W_recon = torch.zeros(W_reshaped.shape, dtype=W_reshaped.dtype, device=W_reshaped.device)\n",
    "            \n",
    "            for start_idx in range(0, W_reshaped.shape[0], batch_size):\n",
    "                end_idx = min(start_idx + batch_size, W_reshaped.shape[0])  # 마지막 배치를 처리할 때 범위 조정\n",
    "                batch = W_reshaped[start_idx:end_idx]  # batch_size 크기로 슬라이싱\n",
    "                batch = batch.to(device)  # 배치를 GPU로 이동\n",
    "\n",
    "                out = model(batch)\n",
    "                x_hat = out['x_hat']\n",
    "                W_recon[start_idx:end_idx] = x_hat\n",
    "\n",
    "                # print(mse_func(out[\"x\"], out[\"x_hat\"]).item())\n",
    "                mean_MSE += mse_func(out[\"x\"], out[\"x_hat\"]).item()\n",
    "                count += 1\n",
    "\n",
    "            W_recon = W_recon.reshape(W.shape).cpu()\n",
    "            recon_state_dict[k] = W_recon\n",
    "            \n",
    "        mean_MSE /= count  \n",
    "\n",
    "    return recon_state_dict, mean_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2100137/2666077041.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path)\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "# model_path = '/home/jgryu/Weight_compression/VQ_SEEDLM/checkpoint/Meta-Llama-3-8B/mlp_16_row_dataset.pt/size16_ne512_P4_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.11122_total_iter_2000000.pth.tar'\n",
    "# model_path = '/home/jgryu/Weight_compression/VQ_SEEDLM/checkpoint/Meta-Llama-3-8B/mlp_16_row_dataset.pt/size16_ne256_P4_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.28962_total_iter_1250000.pth.tar'\n",
    "model_path = '/home/jgryu/Weight_compression/VQ_SEEDLM/checkpoint/Meta-Llama-3-8B/mlp_16_row_dataset.pt/size16_ne512_P32_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.0_total_iter_1750000.pth.tar'\n",
    "ckpt = torch.load(model_path)\n",
    "\n",
    "with open('/home/jgryu/Weight_compression/Wparam_dataset/dataset_per_row/meta-llama/Meta-Llama-3-8B/mlp_16_row_dataset_stats.json', 'r', encoding='utf-8') as file:\n",
    "        dataset_stats = json.load(file)  # JSON 파일을 Python 객체로 변환\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_size = 16\n",
    "dim_encoder = 64\n",
    "P = 32\n",
    "ne = 512\n",
    "n_resblock = 4\n",
    "\n",
    "model = models.VQ_SEEDLM(input_size = input_size, \n",
    "                    dim_encoder = dim_encoder, \n",
    "                    P = P, n_embeddings = ne, n_resblock = n_resblock, \n",
    "                    beta = 0.25,\n",
    "                    scale = torch.Tensor(dataset_stats['train']['mean_channel']).to(device), \n",
    "                    shift = torch.Tensor(dataset_stats['train']['mean_channel']).to(device)\n",
    "                    )\n",
    "\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "\n",
    "ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 61/291 [13:53<54:44, 14.28s/it]  "
     ]
    }
   ],
   "source": [
    "recon_state_dict, mean_MSE = reconstruct_model(\n",
    "        state_dict, model, weight_condition = 'mlp')\n",
    "\n",
    "print(mean_MSE / dataset_stats['train']['std']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000669\n",
      "            0.000552\n",
      "            0.000550\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000688\n",
      "            0.000568\n",
      "            0.000563\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000728\n",
      "            0.000562\n",
      "            0.000566\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000820\n",
      "            0.000542\n",
      "            0.000538\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000912\n",
      "            0.000515\n",
      "            0.000512\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000912\n",
      "            0.000520\n",
      "            0.000517\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000921\n",
      "            0.000520\n",
      "            0.000518\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000887\n",
      "            0.000534\n",
      "            0.000532\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000895\n",
      "            0.000530\n",
      "            0.000528\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000917\n",
      "            0.000540\n",
      "            0.000530\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000873\n",
      "            0.000557\n",
      "            0.000547\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000853\n",
      "            0.000564\n",
      "            0.000555\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000834\n",
      "            0.000587\n",
      "            0.000573\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000840\n",
      "            0.000590\n",
      "            0.000573\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000882\n",
      "            0.000585\n",
      "            0.000570\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000926\n",
      "            0.000576\n",
      "            0.000564\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000968\n",
      "            0.000560\n",
      "            0.000548\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000988\n",
      "            0.000555\n",
      "            0.000547\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.000991\n",
      "            0.000549\n",
      "            0.000544\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001004\n",
      "            0.000545\n",
      "            0.000541\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001002\n",
      "            0.000549\n",
      "            0.000545\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001020\n",
      "            0.000552\n",
      "            0.000547\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001019\n",
      "            0.000555\n",
      "            0.000552\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001023\n",
      "            0.000559\n",
      "            0.000556\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001031\n",
      "            0.000565\n",
      "            0.000562\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001046\n",
      "            0.000574\n",
      "            0.000572\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001061\n",
      "            0.000584\n",
      "            0.000581\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001079\n",
      "            0.000600\n",
      "            0.000593\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001065\n",
      "            0.000625\n",
      "            0.000612\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001059\n",
      "            0.000659\n",
      "            0.000639\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001135\n",
      "            0.000691\n",
      "            0.000648\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "            0.001483\n",
      "            0.000937\n",
      "            0.000692\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([128256, 4096])\n"
     ]
    }
   ],
   "source": [
    "for k, v in state_dict.items():\n",
    "    if k not in recon_state_dict.keys():\n",
    "        recon_state_dict[k] = v\n",
    "        print(k, v.shape)\n",
    "    else:\n",
    "        mse = ((recon_state_dict[k] - state_dict[k])**2).mean()\n",
    "        print(f'{mse.item():-20f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/jgryu/Weight_compression/model_cache_reconstructed/vq_seedlm_/mlp_16_row_dataset.pt/size16_ne512_P4_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.11122_total_iter_2000000.pth.tar/tokenizer_config.json',\n",
       " '/home/jgryu/Weight_compression/model_cache_reconstructed/vq_seedlm_/mlp_16_row_dataset.pt/size16_ne512_P4_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.11122_total_iter_2000000.pth.tar/special_tokens_map.json',\n",
       " '/home/jgryu/Weight_compression/model_cache_reconstructed/vq_seedlm_/mlp_16_row_dataset.pt/size16_ne512_P4_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.11122_total_iter_2000000.pth.tar/tokenizer.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(recon_state_dict)\n",
    "save_directory = f\"/home/jgryu/Weight_compression/model_cache_reconstructed/vq_seedlm_/{os.path.join(*model_path.split('/')[-3:])}\"\n",
    "net.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n",
      "            0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# print(k)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m-20f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k, v in state_dict.items():\n",
    "    # print(k)\n",
    "    mean = ((recon_state_dict[k] - state_dict[k])**2).mean()\n",
    "    print(f'{mean.item():-20f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
