{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "import sys, os, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from VQ_SEEDLM import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_model(state_dict, model, weight_condition, batch_size=32768):\n",
    "    with torch.no_grad():\n",
    "        mean_MSE = 0\n",
    "        count = 0\n",
    "        mse_func = nn.MSELoss()\n",
    "        device = next(model.parameters()).device\n",
    "        recon_state_dict = {}\n",
    "        \n",
    "        for k, W in tqdm(state_dict.items()):\n",
    "            if not weight_condition in k: continue\n",
    "            # print(f'### Reconstructing {k} ####')\n",
    "            \n",
    "            W_reshaped = W.reshape(-1, model.input_size) # ( -1, -1) --> (-1, size, size)\n",
    "            W_recon = torch.zeros(W_reshaped.shape, dtype=W_reshaped.dtype, device=W_reshaped.device)\n",
    "            \n",
    "            for start_idx in range(0, W_reshaped.shape[0], batch_size):\n",
    "                end_idx = min(start_idx + batch_size, W_reshaped.shape[0])  # 마지막 배치를 처리할 때 범위 조정\n",
    "                batch = W_reshaped[start_idx:end_idx]  # batch_size 크기로 슬라이싱\n",
    "                # batch = batch.to(device)  # 배치를 GPU로 이동\n",
    "\n",
    "                # out = model(batch)\n",
    "                # x_hat = out['x_hat']\n",
    "                # W_recon[start_idx:end_idx] = x_hat\n",
    "                W_recon[start_idx:end_idx] = batch\n",
    "\n",
    "                # print(mse_func(out[\"x\"], out[\"x_hat\"]).item())\n",
    "                # mean_MSE += mse_func(out[\"x\"], out[\"x_hat\"]).item()\n",
    "                count += 1\n",
    "\n",
    "            W_recon = W_recon.reshape(W.shape).cpu()\n",
    "            recon_state_dict[k] = W_recon\n",
    "            \n",
    "        mean_MSE /= count  \n",
    "\n",
    "    return recon_state_dict, mean_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3081610/3884113478.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path)\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# model_path = '/home/jgryu/Weight_compression/VQ_SEEDLM/checkpoint/Meta-Llama-3-8B/mlp_16_row_dataset.pt/size16_ne512_P4_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.11122_total_iter_2000000.pth.tar'\n",
    "# model_path = '/home/jgryu/Weight_compression/VQ_SEEDLM/checkpoint/Meta-Llama-3-8B/mlp_16_row_dataset.pt/size16_ne256_P4_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.28962_total_iter_1250000.pth.tar'\n",
    "model_path = '/home/jgryu/Weight_compression/VQ_SEEDLM/checkpoint/Meta-Llama-3-8B/mlp_16_row_dataset.pt/size16_ne512_P32_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.0_total_iter_1750000.pth.tar'\n",
    "ckpt = torch.load(model_path)\n",
    "\n",
    "with open('/home/jgryu/Weight_compression/Wparam_dataset/dataset_per_row/meta-llama/Meta-Llama-3-8B/mlp_16_row_dataset_stats.json', 'r', encoding='utf-8') as file:\n",
    "        dataset_stats = json.load(file)  # JSON 파일을 Python 객체로 변환\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_size = 16\n",
    "dim_encoder = 64\n",
    "P = 32\n",
    "ne = 512\n",
    "n_resblock = 4\n",
    "\n",
    "model = models.VQ_SEEDLM(input_size = input_size, \n",
    "                    dim_encoder = dim_encoder, \n",
    "                    P = P, n_embeddings = ne, n_resblock = n_resblock, \n",
    "                    beta = 0.25,\n",
    "                    scale = torch.Tensor(dataset_stats['train']['mean_channel']).to(device), \n",
    "                    shift = torch.Tensor(dataset_stats['train']['mean_channel']).to(device)\n",
    "                    )\n",
    "\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "\n",
    "ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291/291 [02:25<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "recon_state_dict, mean_MSE = reconstruct_model(\n",
    "        state_dict, model, weight_condition = 'mlp')\n",
    "\n",
    "print(mean_MSE / dataset_stats['train']['std']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in state_dict.items():\n",
    "    if k not in recon_state_dict.keys():\n",
    "        recon_state_dict[k] = v\n",
    "        # print(k, v.shape)\n",
    "    else:\n",
    "        mse = ((recon_state_dict[k] - state_dict[k])**2).mean()\n",
    "        # print(f'{mse.item():-20f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/jgryu/Weight_compression/model_cache_reconstructed/test/original_llama/tokenizer_config.json',\n",
       " '/home/jgryu/Weight_compression/model_cache_reconstructed/test/original_llama/special_tokens_map.json',\n",
       " '/home/jgryu/Weight_compression/model_cache_reconstructed/test/original_llama/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(recon_state_dict)\n",
    "# save_directory = f\"/home/jgryu/Weight_compression/model_cache_reconstructed/vq_seedlm_/{os.path.join(*model_path.split('/')[-3:])}\"\n",
    "save_directory = f\"/home/jgryu/Weight_compression/model_cache_reconstructed/test/original_llama\"\n",
    "net.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
