{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, socket, lpips, shutil, operator\n",
    "\n",
    "# 시간 측정해보기\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets_Imagenet_best_worst import Imagenet_best_worst\n",
    "from datasets_ImageNet import ImageNet_dataset\n",
    "from datasets_WeightParam import WParam_dataset\n",
    "# from datasets_openimages_v6 import Openimages_v6_dataset\n",
    "\n",
    "from pytorch_msssim import ms_ssim as ms_ssim_func\n",
    "\n",
    "from models.TCM import TCM\n",
    "from models.FTIC import FrequencyAwareTransFormer\n",
    "from models.ELIC import ELIC, model_config\n",
    "\n",
    "from utils.optimizers import *\n",
    "from utils.util import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, p):\n",
    "    h, w = x.size(2), x.size(3)\n",
    "    new_h = (h + p - 1) // p * p\n",
    "    new_w = (w + p - 1) // p * p\n",
    "    padding_left = (new_w - w) // 2\n",
    "    padding_right = new_w - w - padding_left\n",
    "    padding_top = (new_h - h) // 2\n",
    "    padding_bottom = new_h - h - padding_top\n",
    "    x_padded = F.pad(\n",
    "        x,\n",
    "        (padding_left, padding_right, padding_top, padding_bottom),\n",
    "        mode=\"constant\",\n",
    "        value=0,\n",
    "    )\n",
    "    return x_padded, (padding_left, padding_right, padding_top, padding_bottom)\n",
    "\n",
    "def crop(x, padding):\n",
    "    return F.pad(\n",
    "        x,\n",
    "        (-padding[0], -padding[1], -padding[2], -padding[3]),\n",
    "    )\n",
    "    \n",
    "def make_image_format(W, wp_mean, wp_std, normalize):\n",
    "    if normalize:\n",
    "        W = (W - wp_mean) / wp_std\n",
    "    W = W.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "    return W\n",
    "\n",
    "def reverse_image_format(W, wp_mean, wp_std, normalize):\n",
    "    # 이미지를 채널 축에서 3 -> 1로 줄이기\n",
    "    # W = W[:, 0, :, :]  # 첫 번째 채널만 유지\n",
    "    W = W.mean(1)  # 첫 번째 채널만 유지\n",
    "    # Normalize를 반대로 적용\n",
    "    if normalize:\n",
    "        W = W * wp_std + wp_mean\n",
    "    return W\n",
    "\n",
    "def reconstruct_model(state_dict, model, save_path, logger, size, weight_condition, mean, std, batch=4, normalize = True):\n",
    "    avg_bpp = 0.0\n",
    "    mean_MSE = 0\n",
    "    count = 0\n",
    "    mse_func = nn.MSELoss()\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    recon_state_dict = {}\n",
    "    \n",
    "    for k, W in state_dict.items():\n",
    "        if not weight_condition in k: continue\n",
    "        print(f'### Reconstructing {k} ####')\n",
    "        \n",
    "        W_reshaped = W.reshape(-1, size, size) # ( -1, -1) --> (-1, size, size)\n",
    "        W_reshaped = W_reshaped.to(device)\n",
    "        W_reshaped = make_image_format(W_reshaped, mean, std, normalize)  # (-1, size, size) --> (-1, 3, size, size)\n",
    "        \n",
    "        # try : \n",
    "        #     W_reshaped = W_reshaped.reshape(-1, batch, 3, size, size)  # (-1, 3, size, size) --> (-1, batch, 3, size, size)\n",
    "        # except:\n",
    "        #     W_reshaped = W_reshaped.reshape(-1, 1, 3, size, size)  # (-1, 3, size, size) --> (-1, 1, 3, size, size)\n",
    "            \n",
    "        W_reshaped = W_reshaped.reshape(-1, 1, 3, size, size)  # (-1, 3, size, size) --> (-1, 1, 3, size, size)\n",
    "        W_recon = torch.zeros(W_reshaped.shape, dtype=W_reshaped.dtype, device=W_reshaped.device)\n",
    "        \n",
    "        for idx, W_slice in tqdm(enumerate(W_reshaped)): # (bath, 3, size, size) in (-1, bath, 3, size, size)\n",
    "            # print(W_slice.shape)\n",
    "            count += 1\n",
    "            x = W_slice.to(device)  # (bach3, size, size) --> (1, 3, size, size)\n",
    "\n",
    "            try:\n",
    "                x_paddeimg, padding = pad(x, p = 128)\n",
    "                out_enc = model.compress(x_paddeimg.to(device))\n",
    "            except:\n",
    "                x_paddeimg, padding = pad(x, p = 256)\n",
    "                out_enc = model.compress(x_paddeimg.to(device))\n",
    "            \n",
    "            out_dec = model.decompress(out_enc[\"strings\"], out_enc[\"shape\"])\n",
    "            \n",
    "            num_pixels = x.size(0) * x.size(2) * x.size(3)\n",
    "            bpp = 0\n",
    "            for s in out_enc[\"strings\"]:\n",
    "                if s != [0]: #  \n",
    "                    bpp += len(s[0]) * 8.0 / num_pixels \n",
    "\n",
    "            x_hat = crop(out_dec[\"x_hat\"], padding).clone().detach() # (1, 3, size, size)\n",
    "            mse = mse_func(x, x_hat).item()\n",
    "            avg_bpp += bpp\n",
    "            mean_MSE += mse\n",
    "            \n",
    "            W_recon_slice = x_hat\n",
    "            W_recon[idx] = W_recon_slice\n",
    "            # logger.info(f\"File name: {idx}, MSE: {mse}, BPP: {bpp}\")\n",
    "\n",
    "        W_recon = W_recon.reshape(-1, 3, size, size)  # (-1, batch, 3, size, size) --> (-1, 3, size, size)\n",
    "        W_recon = reverse_image_format(W_recon, mean, std, normalize)  #  (-1, 3, size, size) --> (-1, size, size)\n",
    "        W_recon = reshape(W.shape)\n",
    "        recon_state_dict[k] = W_recon\n",
    "        \n",
    "        \n",
    "    avg_bpp /= count\n",
    "    mean_MSE /= count  \n",
    "    # logger.info(f'Average_MSE: {mean_MSE}, Average_Bit-rate: {avg_bpp} bpp')\n",
    "\n",
    "    return recon_state_dict, avg_bpp, mean_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "ckpt_path = '/home/jgryu/Weight_compression/llm-awq/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "mean = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/attn/d16/attn_d16_train_mean.npy')\n",
    "std = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/attn/d16/attn_d16_train_std.npy')\n",
    "mean = torch.from_numpy(mean)\n",
    "std = torch.from_numpy(std)\n",
    "\n",
    "size = 256\n",
    "weight_condition = 'attn'\n",
    "\n",
    "path = 'checkpoints_image_pretrained'\n",
    "pt_list = os.listdir(path)\n",
    "lmbdas = []\n",
    "for pt in pt_list:\n",
    "    lm = pt.replace('.pth', '')\n",
    "    lmbdas.append(float(lm))\n",
    "lmbdas = sorted(lmbdas)[-2:-1]\n",
    "print(lmbdas)\n",
    "\n",
    "for lm in lmbdas:\n",
    "    print(f'##### lambda: {lm} #####')\n",
    "    pt = f'{lm}.pth'\n",
    "    ck_path = f'checkpoints_image_pretrained/{lm}.pth'\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(ck_path, map_location=device)\n",
    "        assert isinstance(checkpoint, dict), \"Checkpoint is not a dictionary\"\n",
    "        assert \"state_dict\" in checkpoint, \"Missing 'state_dict' in checkpoint\"\n",
    "        print(f\"Checkpoint for {lm} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load checkpoint for {lm}: {e}\")\n",
    "\n",
    "\n",
    "    model = TCM(N=64)\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        print(f\"Model state_dict loaded successfully for {lm}.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to load model state_dict for {lm}: {e}\")\n",
    "        \n",
    "    model = model.eval().to(device)\n",
    "    model.requires_grad_(False)\n",
    "    model.update()\n",
    "        \n",
    "    recon_state_dict, avg_bpp, mean_MSE = reconstruct_model(\n",
    "        net.state_dict(), model, save_path = None, logger= None, size = size, \n",
    "        weight_condition = weight_condition, mean = mean, std = std)\n",
    "\n",
    "print(avg_bpp, mean_MSE)\n",
    "torch.save(recon_state_dict, \"reconstruncted_state_dict/meta-llama--Meta-Llama-3-8B_attn_d256_256.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "ckpt_path = '/home/jgryu/Weight_compression/llm-awq/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3170895/2745218304.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  reconstruncted_state_dict = torch.load(\"reconstruncted_state_dict/meta-llama--Meta-Llama-3-8B_attn_d256_256.pth\", map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "reconstruncted_state_dict = torch.load(\"reconstruncted_state_dict/meta-llama--Meta-Llama-3-8B_attn_d256_256.pth\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([128256, 4096])\n"
     ]
    }
   ],
   "source": [
    "for k, v in state_dict.items():\n",
    "    if k not in reconstruncted_state_dict.keys():\n",
    "    # v = v.reshape(state_dict[k].shape)\n",
    "        reconstruncted_state_dict[k] = v\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(reconstruncted_state_dict, \"reconstruncted_state_dict/meta-llama--Meta-Llama-3-8B_attn_d256_256.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
