{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701bc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, OPTForCausalLM, BloomForCausalLM\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "def get_named_linears(module):\n",
    "    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}\n",
    "\n",
    "def get_blocks(model):\n",
    "    if model.__class__.__name__ in (\"LlamaForCausalLM\", \"Qwen2ForCausalLM\"):\n",
    "        layers = model.model.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaForCausalLM\":\n",
    "        layers = model.model.layers\n",
    "    elif isinstance(model, OPTForCausalLM):\n",
    "        layers = model.model.decoder.layers\n",
    "    elif isinstance(model, BloomForCausalLM):\n",
    "        layers = model.transformer.h\n",
    "    elif \"mpt\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.blocks\n",
    "    elif \"falcon\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"bigcode\" in str(model.__class__).lower():\n",
    "        layers = model.transformer.h\n",
    "    elif \"neox\" in str(model.__class__).lower():\n",
    "        layers = model.gpt_neox.layers\n",
    "    elif model.__class__.__name__ == \"LlavaLlamaModel\":\n",
    "        layers = model.llm.model.layers\n",
    "    elif model.__class__.__name__ in (\"CLIPModel\"):\n",
    "        vision_layers = model.vision_model.encoder.layers\n",
    "        text_layers = model.text_model.encoder.layers\n",
    "        layers = {'vision': vision_layers,\n",
    "                  'text': text_layers}\n",
    "    else:\n",
    "        raise NotImplementedError(type(model))\n",
    "    # if not isinstance(layers, dict):\n",
    "    #     layers = {'': layers}\n",
    "    return layers\n",
    "\n",
    "def flat_to_sym(V, N):\n",
    "    A = torch.zeros(N, N, dtype=V.dtype, device=V.device)\n",
    "    idxs = torch.tril_indices(N, N, device=V.device)\n",
    "    A[idxs.unbind()] = V\n",
    "    A[idxs[1, :], idxs[0, :]] = V\n",
    "    return A\n",
    "\n",
    "def regularize_H(H, n, sigma_reg):\n",
    "    H.div_(torch.diag(H).mean())\n",
    "    idx = torch.arange(n)\n",
    "    H[idx, idx] += sigma_reg\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441cc27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    'meta-llama/Meta-Llama-3-8B',\n",
    "    # 'meta-llama--Llama-2-7b-hf'\n",
    "]\n",
    "quip_hess_path = [\n",
    "    './quip_hess/llama3_8b_6144',\n",
    "    # './quip_hess/Hessians-Llama-2-7b-6144',\n",
    "]\n",
    "wtype_mapping = {'self_attn.q_proj': 0, \n",
    "                 'self_attn.k_proj': 1, \n",
    "                 'self_attn.v_proj': 2, \n",
    "                 'self_attn.o_proj': 3, \n",
    "                 'mlp.gate_proj': 4, \n",
    "                 'mlp.up_proj': 5, \n",
    "                 'mlp.down_proj': 6}\n",
    "sigma_reg = 1e-4\n",
    "# direction = 'col'\n",
    "direction = 'row'\n",
    "\n",
    "global_std = 0.012529\n",
    "\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "quip_hess = '../Wparam_dataset/quip_hess/llama3_8b_6144'\n",
    "\n",
    "\n",
    "model_name = model_name.replace('/', '--')\n",
    "print('model_name: ', model_name)\n",
    "\n",
    "model_path = f\"../Wparam_dataset/hf_model/{model_name}\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "layers = get_blocks(model)\n",
    "\n",
    "\n",
    "H = torch.load(f\"{quip_hess}/1_down.pt\", weights_only=False)\n",
    "hatW = torch.load('/workspace/Weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_rnorm/lmbda1000/1_down.pt', weights_only=False)\n",
    "hatW_c = torch.load('/workspace/Weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_rnorm_cnorm_trained/lmbda1000/1_down.pt', weights_only=False)\n",
    "hatW_l = torch.load('/workspace/Weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_rnorm_lnorm_trained/lmbda1000/1_down.pt', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = layers[1].mlp.down_proj.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22136ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "플롯이 ./plots 폴더에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "def to_tensor_2d(save):\n",
    "    if isinstance(save, dict):\n",
    "        x = save['metadata']['row_std'] * save['hatWr']\n",
    "    else:\n",
    "        x = save\n",
    "    return x\n",
    "\n",
    "def squared_error_map(W, W_hat):\n",
    "    if W.shape != W_hat.shape:\n",
    "        raise ValueError(f\"Shape mismatch: W {tuple(W.shape)} vs W_hat {tuple(W_hat.shape)}\")\n",
    "    return (W - W_hat) ** 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_heatmap(mat: torch.Tensor, title: str, fname: str, pool_size: int = 32):\n",
    "    \"\"\"\n",
    "    mat       : 2D torch.Tensor\n",
    "    pool_size : 다운샘플링 크기 (예: 4 → 4x4 블록에서 max pooling)\n",
    "    \"\"\"\n",
    "    os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "    # --- 텐서 다운샘플링 ---\n",
    "    x = mat.detach().float().unsqueeze(0).unsqueeze(0)  # (1,1,H,W) 형태로\n",
    "    H, W = mat.shape\n",
    "    newH, newW = H // pool_size, W // pool_size\n",
    "    x_pooled = F.max_pool2d(x, kernel_size=pool_size, stride=pool_size)\n",
    "    arr = x_pooled.squeeze().cpu().numpy()\n",
    "\n",
    "    # --- 로그 스케일 ---\n",
    "    arr_log = np.log1p(arr)\n",
    "\n",
    "    # --- 플롯 ---\n",
    "    plt.figure(figsize=(24, 18))\n",
    "    plt.imshow(arr_log, aspect='auto', cmap='gray_r')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label=\"log(1+x), pooled\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(\"plots\", fname), dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "hatW   = to_tensor_2d(hatW)\n",
    "hatW_c = to_tensor_2d(hatW_c)\n",
    "hatW_l = to_tensor_2d(hatW_l)\n",
    "\n",
    "\n",
    "se_base = squared_error_map(W, hatW)\n",
    "se_c    = squared_error_map(W, hatW_c)\n",
    "se_l    = squared_error_map(W, hatW_l)\n",
    "\n",
    "# 플롯\n",
    "plot_heatmap(W,      \"W (down_proj weight)\",              \"W_down_heatmap.png\")\n",
    "plot_heatmap(se_base,\"(W - hatW)^2 [ql_rnorm]\",           \"SE_W_minus_hatW.png\")\n",
    "plot_heatmap(se_c,   \"(W - hatW_c)^2 [ql_rnorm_cnorm]\",   \"SE_W_minus_hatW_c.png\")\n",
    "plot_heatmap(se_l,   \"(W - hatW_l)^2 [ql_rnorm_lnorm]\",   \"SE_W_minus_hatW_l.png\")\n",
    "\n",
    "print(\"플롯이 ./plots 폴더에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "317bd4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: plots/W_and_SE_quad_shared.png\n",
      "Saved: plots/W_and_SE_quad_individual.png\n"
     ]
    }
   ],
   "source": [
    "def to_tensor_2d(save):\n",
    "    if isinstance(save, dict):\n",
    "        x = save['metadata']['row_std'] * save['hatWr']\n",
    "    else:\n",
    "        x = save\n",
    "    return x\n",
    "\n",
    "def squared_error_map(W, W_hat):\n",
    "    if W.shape != W_hat.shape:\n",
    "        raise ValueError(f\"Shape mismatch: W {tuple(W.shape)} vs W_hat {tuple(W_hat.shape)}\")\n",
    "    return (W - W_hat) ** 2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def _pool2d_max(mat: torch.Tensor, pool_size: int) -> torch.Tensor:\n",
    "    x = mat.detach().float().unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    x_pooled = F.max_pool2d(x, kernel_size=pool_size, stride=pool_size)\n",
    "    return x_pooled.squeeze(0).squeeze(0)\n",
    "def plot_four(\n",
    "    W: torch.Tensor,\n",
    "    se_base: torch.Tensor,\n",
    "    se_c: torch.Tensor,\n",
    "    se_l: torch.Tensor,\n",
    "    pool_size: int = 32,\n",
    "    share_error_scale: bool = True,\n",
    "    out_path: str = \"plots/W_and_SE_quad.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    2x2 플롯:\n",
    "      [0,0] W (|W|에 log1p, 개별 스케일, 컬러바)\n",
    "      [0,1] (W - hatW)^2 (log1p, 컬러바)\n",
    "      [1,0] (W - hatW_c)^2 (log1p, 컬러바)\n",
    "      [1,1] (W - hatW_l)^2 (log1p, 컬러바)\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "\n",
    "    # 1) 풀링 (텐서 상태)\n",
    "    def _pool2d_max(mat: torch.Tensor, pool_size: int) -> torch.Tensor:\n",
    "        x = mat.detach().float().unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "        x_pooled = F.max_pool2d(x, kernel_size=pool_size, stride=pool_size)\n",
    "        return x_pooled.squeeze(0).squeeze(0)\n",
    "\n",
    "    Wp  = _pool2d_max(W,       pool_size)\n",
    "    Ebp = _pool2d_max(se_base, pool_size)\n",
    "    Ecp = _pool2d_max(se_c,    pool_size)\n",
    "    Elp = _pool2d_max(se_l,    pool_size)\n",
    "\n",
    "    # 2) NumPy 변환 후 log1p\n",
    "    Wn  = Wp.cpu().numpy()\n",
    "    Ebn = Ebp.cpu().numpy()\n",
    "    Ecn = Ecp.cpu().numpy()\n",
    "    Eln = Elp.cpu().numpy()\n",
    "\n",
    "    W_vis  = np.log1p(np.abs(Wn))\n",
    "    Eb_vis = np.log1p(Ebn)\n",
    "    Ec_vis = np.log1p(Ecn)\n",
    "    El_vis = np.log1p(Eln)\n",
    "\n",
    "    # 에러맵 스케일 (공유 옵션)\n",
    "    if share_error_scale:\n",
    "        vmin_err = min(Eb_vis.min(), Ec_vis.min(), El_vis.min())\n",
    "        vmax_err = max(Eb_vis.max(), Ec_vis.max(), El_vis.max())\n",
    "        err_norm = dict(vmin=vmin_err, vmax=vmax_err)\n",
    "    else:\n",
    "        err_norm = {}\n",
    "\n",
    "    # 3) 플롯\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(32, 24))\n",
    "    (axW, axE1), (axE2, axE3) = axs\n",
    "\n",
    "    # W\n",
    "    imW  = axW.imshow(W_vis,  aspect='auto', cmap='gray_r')\n",
    "    axW.set_title(\"W (pooled, log1p(|W|))\")\n",
    "    cbarW = fig.colorbar(imW, ax=axW)\n",
    "    cbarW.set_label(\"log(1+|W|)\")\n",
    "\n",
    "    # Error maps (각자 컬러바 추가)\n",
    "    imE1 = axE1.imshow(Eb_vis, aspect='auto', cmap='gray_r', **err_norm)\n",
    "    axE1.set_title(\"(W - hatW)^2 (pooled, log1p)\")\n",
    "    cbarE1 = fig.colorbar(imE1, ax=axE1)\n",
    "    cbarE1.set_label(\"log(1 + SE)\")\n",
    "\n",
    "    imE2 = axE2.imshow(Ec_vis, aspect='auto', cmap='gray_r', **err_norm)\n",
    "    axE2.set_title(\"(W - hatW_c)^2 (pooled, log1p)\")\n",
    "    cbarE2 = fig.colorbar(imE2, ax=axE2)\n",
    "    cbarE2.set_label(\"log(1 + SE)\")\n",
    "\n",
    "    imE3 = axE3.imshow(El_vis, aspect='auto', cmap='gray_r', **err_norm)\n",
    "    axE3.set_title(\"(W - hatW_l)^2 (pooled, log1p)\")\n",
    "    cbarE3 = fig.colorbar(imE3, ax=axE3)\n",
    "    cbarE3.set_label(\"log(1 + SE)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "\n",
    "# ===== 예시 사용 =====\n",
    "hatW   = to_tensor_2d(hatW)\n",
    "hatW_c = to_tensor_2d(hatW_c)\n",
    "hatW_l = to_tensor_2d(hatW_l)\n",
    "\n",
    "se_base = squared_error_map(W, hatW)\n",
    "se_c    = squared_error_map(W, hatW_c)\n",
    "se_l    = squared_error_map(W, hatW_l)\n",
    "\n",
    "# 4개를 한 번에, 에러맵 스케일 공유\n",
    "plot_four(W, se_base, se_c, se_l, pool_size=32, share_error_scale=True,\n",
    "          out_path=\"plots/W_and_SE_quad_shared.png\")\n",
    "\n",
    "# 4개를 한 번에, 에러맵 스케일 개별\n",
    "plot_four(W, se_base, se_c, se_l, pool_size=32, share_error_scale=False,\n",
    "          out_path=\"plots/W_and_SE_quad_individual.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c841fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
