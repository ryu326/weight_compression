{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruction error: tensor(0.0003)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_U_from_H(H: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Given a symmetric positive–definite matrix H (n×n),\n",
    "    compute U and D such that\n",
    "        H = (U + I) @ D @ (U + I).T\n",
    "    where U is strictly upper–triangular and D is diagonal.\n",
    "    \"\"\"\n",
    "    n = H.shape[-1]\n",
    "    # Method A: use PyTorch's LDL decomposition (requires PyTorch ≥2.0)\n",
    "    if hasattr(torch.linalg, \"ldl\"):\n",
    "        L, D, _ = torch.linalg.ldl(H)          # H = L @ D @ L.T\n",
    "        # L is unit lower–triangular, so U = L.T – I is strictly upper\n",
    "        U = L.transpose(-2, -1) - torch.eye(n, device=H.device, dtype=H.dtype)\n",
    "        return U, D\n",
    "\n",
    "    # Method B: fallback via Cholesky\n",
    "    # 1) H = L_chol @ L_chol.T,  L_chol lower–triangular with diag d\n",
    "    L_chol = torch.linalg.cholesky(H)            # (n, n)\n",
    "    d = torch.diagonal(L_chol, 0)                # (n,)\n",
    "    # 2) form D = diag(d^2)\n",
    "    D = torch.diag(d * d)                        # (n, n)\n",
    "    # 3) build a unit–lower triangular L_unit = L_chol @ diag(1/d)\n",
    "    inv_d = (1.0 / d).to(H.dtype)\n",
    "    L_unit = L_chol @ torch.diag(inv_d)           # now L_unit has 1's on diag\n",
    "    # 4) extract strictly upper U\n",
    "    U = L_unit.transpose(-2, -1) - torch.eye(n, device=H.device, dtype=H.dtype)\n",
    "    return U, D\n",
    "\n",
    "\n",
    "# 예시 사용법\n",
    "H = torch.randn(128, 128)\n",
    "H = H @ H.T + 1e-3 * torch.eye(128)  # SPD 보장\n",
    "U, D = compute_U_from_H(H)\n",
    "# 검증: H 재구성\n",
    "H_recon = (U + torch.eye(128)).T @ D @ (U + torch.eye(128))\n",
    "print(\"reconstruction error:\", torch.norm(H - H_recon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Qwen2ForCausalLM, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:  50%|█████     | 2/4 [00:43<00:43, 21.91s/it]"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2.5-7B\" # 필요에 따라 1.5B, 14B, 72B 등으로 변경\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\", # BF16·FP16 자동 선택\n",
    "    device_map=\"cpu\" # 여러 GPU가 있으면 자동 분할\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.60it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"/workspace/Weight_compression/Wparam_dataset/hf_model/lmsys--vicuna-7b-v1.5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\", # BF16·FP16 자동 선택\n",
    "    device_map=\"cpu\" # 여러 GPU가 있으면 자동 분할\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "d = torch.load('/workspace/Weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/optim_code_frdelta/lmbda50/0_v.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, MllamaForConditionalGeneration\n",
    "\n",
    "# model_name=\"openai--clip-vit-large-patch14\"\n",
    "# lm_model_path=f\"../Wparam_dataset/hf_model/{model_name}\"\n",
    "# ori = AutoModel.from_pretrained(lm_model_path)\n",
    "\n",
    "q = AutoModel.from_pretrained(\"/workspace/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-3.2-3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, MllamaForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"/workspace/Weight_compression/Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B\"  # 예시 모델\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 파라미터 개수 계산\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 4084433813.00 / 8030261248 * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_directory_size(directory_path):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "    return total_size\n",
    "\n",
    "# 디렉토리 경로\n",
    "directory_path = '/workspace/Weight_compression/comp_lm_qtip/aqlm_cache/models--ISTA-DASLab--Meta-Llama-3-8B-AQLM-2Bit-1x16/snapshots/812d023a2163f2c04f7f1016e8b1810e877c5aea'\n",
    "\n",
    "# 디렉토리 크기 계산 (바이트 단위)\n",
    "directory_size_bytes = get_directory_size(directory_path)\n",
    "\n",
    "# 바이트를 메가바이트로 변환\n",
    "directory_size_mb = directory_size_bytes\n",
    "\n",
    "print(f\"Directory size: {directory_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './path_to_model_directory/pytorch_model.bin'\n",
    "\n",
    "# 파일 크기 확인 (바이트 단위)\n",
    "file_size_bytes = os.path.getsize(model_path)\n",
    "\n",
    "# 바이트를 메가바이트로 변환\n",
    "file_size_mb = file_size_bytes / (1024 ** 2)\n",
    "\n",
    "print(f\"Model file size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\",\n",
    "    cache_dir='/workspace/Weight_compression/Wparam_dataset/hf_model/cache'\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "model.save_pretrained('/workspace/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-3.2-11B-Vision')\n",
    "processor.save_pretrained('/workspace/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-3.2-11B-Vision')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    '/workspace/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-3.2-11B-Vision_',\n",
    "    device_map=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir='/workspace/Weight_compression/Wparam_dataset/hf_model/cache'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori = MllamaForConditionalGeneration.from_pretrained(\"/workspace/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-3.2-11B-Vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_s = ori.state_dict()\n",
    "q_s = q.state_dict()\n",
    "\n",
    "for k, v in ori_s.items():\n",
    "    # if 'bias' in k:\n",
    "        print(k,v.mean(), v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../hf_model_comp/qtip/hf/clip-vit-large-patch14_4bit'\n",
    "\n",
    "qtip = AutoModel.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in qtip.state_dict().items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path='/workspace/Weight_compression/hf_model_comp/comp_qtip/hf/clip_test/openai--clip-vit-large-patch14/lmbda100000_result.json'\n",
    "with open(path, 'r') as f:\n",
    "    r = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r['bpp_loss']/ r['num_pixels'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ori_s:\n",
    "    if 'bias' in k: \n",
    "        print(ori_s[k])\n",
    "    # print(k)\n",
    "    # # print(f'{((ori_s[k] - q_s[k])**2).mean().item():.4f}')\n",
    "    # print(f'{((ori_s[k] - q_s[k])**2).mean().item() / (ori_s[k]**2).mean().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'quip_params' in c['model_config'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf\",\n",
    "    trust_remote_code=True, torch_dtype=\"auto\"\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
