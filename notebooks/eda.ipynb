{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/Wcomp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/jgryu/Weight_compression/Wparam_dataset/Wparam_npy/llama_7b_self_attn_d=2048_val.npy'\n",
    "os.path.isfile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gparams = np.load('/home/jgryu/Weight_compression/RD-sandwich/data/gaussian_params-dim=1000.npz')\n",
    "loc = gparams['loc']\n",
    "scale = gparams['scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"/home/jgryu/Weight_compression/Wparam_dataset/Wparam_npy/llama_7b_self_attn_d=2048_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2936012, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_mean = 8.708306e-07\n",
    "wp_std = 0.023440132\n",
    "nor_d = (d - wp_mean) / wp_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.453125 -1.4765625\n",
      "61.993004 -62.99296\n"
     ]
    }
   ],
   "source": [
    "print(d.max(), d.min())\n",
    "print(nor_d.max(), nor_d.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = d.mean(axis=0)\n",
    "# s = d.var(axis=0)\n",
    "m = d.mean()\n",
    "s = d.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = d.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023440132"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_d = (d-m)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0009615, 1.0072731, 1.1371735, ..., 0.9774631, 0.9887671,\n",
       "       0.987939 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nor_d.var(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023440132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.708306e-07"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005519013\n",
      "0.00055592053\n",
      "0.0006244048\n",
      "0.0005502901\n",
      "0.00054507813\n",
      "0.000547752\n",
      "0.00054456946\n",
      "0.0005488622\n",
      "0.00055248634\n",
      "0.0005476372\n",
      "0.000549594\n",
      "0.0005429699\n",
      "0.0005469753\n",
      "0.0005620375\n",
      "0.0005419004\n",
      "0.0005519321\n",
      "0.00054698763\n",
      "0.00054371735\n",
      "0.000549649\n",
      "0.0005512757\n",
      "0.0005441512\n",
      "0.0005516877\n",
      "0.0006109052\n",
      "0.0005499016\n",
      "0.0005367981\n",
      "0.00054645847\n",
      "0.00056516216\n",
      "0.0005391629\n",
      "0.0005643185\n",
      "0.000583725\n",
      "0.00056811195\n",
      "0.0005474951\n",
      "0.0005441823\n",
      "0.00054927083\n",
      "0.00054851687\n",
      "0.000545287\n",
      "0.00054395455\n",
      "0.00054915145\n",
      "0.0005391971\n",
      "0.00054115546\n",
      "0.0005386838\n",
      "0.0005464965\n",
      "0.0005479124\n",
      "0.0005638776\n",
      "0.00055515656\n",
      "0.0005476286\n",
      "0.0005493475\n",
      "0.0005439585\n",
      "0.0005352645\n",
      "0.0005484602\n",
      "0.0005593901\n",
      "0.0005435107\n",
      "0.00054504856\n",
      "0.0005528241\n",
      "0.00054054696\n",
      "0.0005467469\n",
      "0.00054280367\n",
      "0.00055291894\n",
      "0.0005438386\n",
      "0.0005444267\n",
      "0.0005405132\n",
      "0.0005475916\n",
      "0.0005508495\n",
      "0.0005472913\n",
      "0.00053791975\n",
      "0.00054547255\n",
      "0.00055312295\n",
      "0.0005538455\n",
      "0.00054435764\n",
      "0.00054462935\n",
      "0.00054843625\n",
      "0.00054445997\n",
      "0.0005436076\n",
      "0.0005515073\n",
      "0.0005502792\n",
      "0.0005487697\n",
      "0.0005412088\n",
      "0.00054916216\n",
      "0.00054357346\n",
      "0.00054680556\n",
      "0.0005485215\n",
      "0.00055849\n",
      "0.0005979184\n",
      "0.00054924586\n",
      "0.00054546737\n",
      "0.0005483934\n",
      "0.0005515984\n",
      "0.000565907\n",
      "0.0005696226\n",
      "0.00054679945\n",
      "0.00054908515\n",
      "0.00055430946\n",
      "0.00055438513\n",
      "0.0005433284\n",
      "0.0006592242\n",
      "0.0005481704\n",
      "0.0005512947\n",
      "0.00054892746\n",
      "0.00054078095\n",
      "0.0006518882\n",
      "0.0005511107\n",
      "0.0005412502\n",
      "0.00057102187\n",
      "0.00054320676\n",
      "0.00055108\n",
      "0.00055363704\n",
      "0.00054412655\n",
      "0.0005546547\n",
      "0.00054122985\n",
      "0.0005550651\n",
      "0.00067940075\n",
      "0.00054819224\n",
      "0.000551816\n",
      "0.00055604515\n",
      "0.0005476375\n",
      "0.00054489187\n",
      "0.00054342067\n",
      "0.00054792437\n",
      "0.00055108685\n",
      "0.00055271335\n",
      "0.0005733612\n",
      "0.000555322\n",
      "0.0005486346\n",
      "0.0005416474\n",
      "0.00055832585\n",
      "0.0005717245\n",
      "0.0005445365\n",
      "0.0005548472\n",
      "0.0005435558\n",
      "0.00055514544\n",
      "0.00054992066\n",
      "0.00053420913\n",
      "0.0005359713\n",
      "0.00054946647\n",
      "0.0005382828\n",
      "0.0005346547\n",
      "0.0005435742\n",
      "0.0005419839\n",
      "0.000538577\n",
      "0.0005473035\n",
      "0.00054626964\n",
      "0.00055592053\n",
      "0.0005610532\n",
      "0.00053906353\n",
      "0.0005464742\n",
      "0.0005478726\n",
      "0.0005457358\n",
      "0.0005416949\n",
      "0.00056854327\n",
      "0.00053916464\n",
      "0.0005432612\n",
      "0.0005424266\n",
      "0.0005541041\n",
      "0.0005440243\n",
      "0.00054376933\n",
      "0.0005555724\n",
      "0.00054022996\n",
      "0.00054239854\n",
      "0.0005443779\n",
      "0.00054518785\n",
      "0.00054103776\n",
      "0.0005511023\n",
      "0.00054603774\n",
      "0.0005455751\n",
      "0.00055115565\n",
      "0.00054464256\n",
      "0.000565954\n",
      "0.000551945\n",
      "0.00053745415\n",
      "0.0005431884\n",
      "0.0005480429\n",
      "0.00055071147\n",
      "0.000546574\n",
      "0.00054144394\n",
      "0.000539613\n",
      "0.0005445402\n",
      "0.0005338884\n",
      "0.0005456375\n",
      "0.0005419872\n",
      "0.0005418899\n",
      "0.000545542\n",
      "0.0005441746\n",
      "0.0005497119\n",
      "0.00054247334\n",
      "0.00056756276\n",
      "0.0005423981\n",
      "0.0005415966\n",
      "0.00069024705\n",
      "0.00054721424\n",
      "0.00053955783\n",
      "0.0005433452\n",
      "0.0005493827\n",
      "0.0005570674\n",
      "0.0005439361\n",
      "0.0005488272\n",
      "0.0005440717\n",
      "0.0005488732\n",
      "0.0005423179\n",
      "0.00053895445\n",
      "0.0005608268\n",
      "0.0005436385\n",
      "0.0005506174\n",
      "0.00055227755\n",
      "0.00054405024\n",
      "0.0005449366\n",
      "0.0005504647\n",
      "0.0005458557\n",
      "0.000566675\n",
      "0.0005457013\n",
      "0.0005384934\n",
      "0.0005707396\n",
      "0.0005449624\n",
      "0.00055014994\n",
      "0.00054717896\n",
      "0.0005434663\n",
      "0.00054173265\n",
      "0.00055804953\n",
      "0.00054014544\n",
      "0.00054358086\n",
      "0.00055022305\n",
      "0.0005402252\n",
      "0.0005419573\n",
      "0.00054476096\n",
      "0.0005471737\n",
      "0.00054274907\n",
      "0.0005470958\n",
      "0.0005388353\n",
      "0.000547072\n",
      "0.0005427312\n",
      "0.00054204464\n",
      "0.0005501535\n",
      "0.0005486738\n",
      "0.0005411606\n",
      "0.0005412173\n",
      "0.0005463975\n",
      "0.00054280687\n",
      "0.000542993\n",
      "0.00054012815\n",
      "0.0005509781\n",
      "0.0005563784\n",
      "0.0005543708\n",
      "0.0005432278\n",
      "0.0005433246\n",
      "0.00053786766\n",
      "0.0005519128\n",
      "0.0005465386\n",
      "0.00054033834\n",
      "0.00054014416\n",
      "0.00054052315\n",
      "0.0005608756\n",
      "0.00063837325\n",
      "0.00053876813\n",
      "0.00054078276\n",
      "0.0005399547\n",
      "0.00054739235\n",
      "0.00054673053\n",
      "0.0005419804\n",
      "0.0006603859\n",
      "0.0005401483\n",
      "0.0005489226\n",
      "0.00054310967\n",
      "0.0005493876\n",
      "0.0005569993\n",
      "0.0005485991\n",
      "0.0005385232\n",
      "0.00053219585\n",
      "0.00057324336\n",
      "0.0005366119\n",
      "0.0005524317\n",
      "0.0005819659\n",
      "0.0005435362\n",
      "0.00053864176\n",
      "0.0005478021\n",
      "0.000539413\n",
      "0.00053831306\n",
      "0.0005420136\n",
      "0.0005440257\n",
      "0.0005350721\n",
      "0.0005457712\n",
      "0.0005409147\n",
      "0.00054139644\n",
      "0.00054157217\n",
      "0.00054478884\n",
      "0.00055063213\n",
      "0.0005349822\n",
      "0.00054031005\n",
      "0.00054647203\n",
      "0.0005489815\n",
      "0.00057944725\n",
      "0.0005698545\n",
      "0.000534024\n",
      "0.00054579746\n",
      "0.00053835934\n",
      "0.00054545014\n",
      "0.0005443305\n",
      "0.0005559369\n",
      "0.0005467386\n",
      "0.00054972136\n",
      "0.0005477037\n",
      "0.0005345481\n",
      "0.00053696794\n",
      "0.00053727574\n",
      "0.0005793039\n",
      "0.00053293747\n",
      "0.00054827385\n",
      "0.0005379574\n",
      "0.00053516513\n",
      "0.00054110016\n",
      "0.0005468463\n",
      "0.00054997095\n",
      "0.0006649615\n",
      "0.0005496187\n",
      "0.00054250297\n",
      "0.0005402597\n",
      "0.00054069265\n",
      "0.00054367067\n",
      "0.0005385996\n",
      "0.0005625415\n",
      "0.00053661846\n",
      "0.0005561862\n",
      "0.00055575394\n",
      "0.0005480967\n",
      "0.0005366211\n",
      "0.0005345148\n",
      "0.0005539589\n",
      "0.0005392789\n",
      "0.0005396846\n",
      "0.0005476876\n",
      "0.000551511\n",
      "0.00055105396\n",
      "0.00054403767\n",
      "0.0005437808\n",
      "0.0005393363\n",
      "0.00054127397\n",
      "0.0005336224\n",
      "0.0005386199\n",
      "0.0005507699\n",
      "0.00054111116\n",
      "0.00054650544\n",
      "0.00059722475\n",
      "0.00054624164\n",
      "0.0005443432\n",
      "0.0005442937\n",
      "0.0005349785\n",
      "0.00053497247\n",
      "0.00067751115\n",
      "0.00056365633\n",
      "0.0005330127\n",
      "0.0005455213\n",
      "0.0005465921\n",
      "0.00053541764\n",
      "0.00054293725\n",
      "0.00053505896\n",
      "0.0005383347\n",
      "0.0005483638\n",
      "0.0005935965\n",
      "0.0005418189\n",
      "0.00055147376\n",
      "0.00053589453\n",
      "0.0005403774\n",
      "0.00054662256\n",
      "0.00053705164\n",
      "0.00054314395\n",
      "0.00064451294\n",
      "0.0005392112\n",
      "0.00053743325\n",
      "0.00053724385\n",
      "0.0005418051\n",
      "0.0005392168\n",
      "0.0005478891\n",
      "0.0005353293\n",
      "0.0005372595\n",
      "0.00054197805\n",
      "0.000548902\n",
      "0.0005291924\n",
      "0.0005833903\n",
      "0.00053909875\n",
      "0.0005397476\n",
      "0.00054213783\n",
      "0.00053677417\n",
      "0.00054158445\n",
      "0.0005465187\n",
      "0.00053501606\n",
      "0.0005415537\n",
      "0.00055672834\n",
      "0.00054576277\n",
      "0.0005511756\n",
      "0.0005498617\n",
      "0.0005416927\n",
      "0.0005465189\n",
      "0.0005486028\n",
      "0.0005629047\n",
      "0.00054266484\n",
      "0.0005473621\n",
      "0.00055619923\n",
      "0.0005428821\n",
      "0.00054598437\n",
      "0.00054526824\n",
      "0.000549971\n",
      "0.0005437013\n",
      "0.0005451508\n",
      "0.00054165226\n",
      "0.00054962083\n",
      "0.0005443917\n",
      "0.0005488883\n",
      "0.00055312953\n",
      "0.000556336\n",
      "0.0005424565\n",
      "0.00055862387\n",
      "0.00055166864\n",
      "0.00054801436\n",
      "0.00058511033\n",
      "0.0005504497\n",
      "0.0005511821\n",
      "0.0005396566\n",
      "0.0005473805\n",
      "0.00054724806\n",
      "0.00054689083\n",
      "0.00053791667\n",
      "0.00054835004\n",
      "0.0005462243\n",
      "0.00061693386\n",
      "0.0005468636\n",
      "0.000556935\n",
      "0.00056588365\n",
      "0.00054295594\n",
      "0.00054273306\n",
      "0.0005474381\n",
      "0.00053852075\n",
      "0.00055202574\n",
      "0.00055251486\n",
      "0.0005515227\n",
      "0.00055339973\n",
      "0.0005425003\n",
      "0.000548374\n",
      "0.00054108875\n",
      "0.0005508373\n",
      "0.00054593105\n",
      "0.0005441638\n",
      "0.00055228244\n",
      "0.00055744057\n",
      "0.00054480263\n",
      "0.00054108584\n",
      "0.00054792664\n",
      "0.000558101\n",
      "0.0005457196\n",
      "0.0005423177\n",
      "0.0005726983\n",
      "0.00055161276\n",
      "0.0005483382\n",
      "0.00054329046\n",
      "0.00053786335\n",
      "0.0005506228\n",
      "0.0005445029\n",
      "0.00054016686\n",
      "0.0005544955\n",
      "0.00054473954\n",
      "0.0005471012\n",
      "0.0005392531\n",
      "0.0005503585\n",
      "0.0005523769\n",
      "0.0005555771\n",
      "0.00062650436\n",
      "0.0005451782\n",
      "0.00054388994\n",
      "0.00054897205\n",
      "0.00053593784\n",
      "0.00055551744\n",
      "0.00054665306\n",
      "0.00055099145\n",
      "0.00054737745\n",
      "0.00054893835\n",
      "0.00055478315\n",
      "0.0005845402\n",
      "0.0005813295\n",
      "0.0005413535\n",
      "0.0005476347\n",
      "0.00055013935\n",
      "0.0005592799\n",
      "0.00054103095\n",
      "0.0005611275\n",
      "0.00054476206\n",
      "0.00054433633\n",
      "0.0005445688\n",
      "0.0005522327\n",
      "0.0016615904\n",
      "0.0005443687\n",
      "0.00054933695\n",
      "0.00053891086\n",
      "0.00054368057\n",
      "0.0006597125\n",
      "0.00055088644\n",
      "0.00054959545\n",
      "0.0005506872\n",
      "0.00055117276\n",
      "0.00064972753\n",
      "0.00057860895\n",
      "0.000556243\n",
      "0.0005480613\n",
      "0.00055303273\n",
      "0.0005503357\n",
      "0.00054443884\n",
      "0.0005427365\n",
      "0.0005449588\n",
      "0.00054802716\n",
      "0.00055652694\n",
      "0.0005445351\n",
      "0.00055604626\n",
      "0.00054668856\n",
      "0.0005421283\n",
      "0.00055408233\n",
      "0.0005506491\n",
      "0.0005511081\n",
      "0.0005431089\n",
      "0.0005409451\n",
      "0.00055040815\n",
      "0.0005453447\n",
      "0.0005541893\n",
      "0.0005409102\n",
      "0.0005459432\n",
      "0.000536431\n",
      "0.00054570875\n",
      "0.0005394606\n",
      "0.00054925255\n",
      "0.00054466963\n",
      "0.000538108\n",
      "0.0005509556\n",
      "0.0005364616\n",
      "0.000552134\n",
      "0.0005365286\n",
      "0.00054305076\n",
      "0.0005392457\n",
      "0.00055316766\n",
      "0.0005412339\n",
      "0.0005524769\n",
      "0.0005384051\n",
      "0.00055770244\n",
      "0.00054331706\n",
      "0.00055178767\n",
      "0.0005485497\n",
      "0.0005554648\n",
      "0.00053917686\n",
      "0.000542989\n",
      "0.00053799554\n",
      "0.0005400289\n",
      "0.0005615234\n",
      "0.0005381879\n",
      "0.0005519019\n",
      "0.00054334133\n",
      "0.00054077804\n",
      "0.00054669875\n",
      "0.00053563615\n",
      "0.00055654574\n",
      "0.00055083574\n",
      "0.000541109\n",
      "0.0005467986\n",
      "0.00053698797\n",
      "0.0005448265\n",
      "0.00054390234\n",
      "0.0005525665\n",
      "0.00069859135\n",
      "0.00054995547\n",
      "0.0005418513\n",
      "0.0005497266\n",
      "0.00054517115\n",
      "0.00054764154\n",
      "0.00054152345\n",
      "0.0005559967\n",
      "0.00055778574\n",
      "0.0005447909\n",
      "0.0005370001\n",
      "0.0005375221\n",
      "0.0005346139\n",
      "0.00053905364\n",
      "0.0005820607\n",
      "0.00060031575\n",
      "0.00054639083\n",
      "0.0005734359\n",
      "0.000553995\n",
      "0.00054572785\n",
      "0.00060088147\n",
      "0.0005436641\n",
      "0.0005348598\n",
      "0.00054682663\n",
      "0.000548649\n",
      "0.0005540734\n",
      "0.0005397334\n",
      "0.0005499275\n",
      "0.00064328837\n",
      "0.0005477531\n",
      "0.00053633505\n",
      "0.0005399548\n",
      "0.0005341278\n",
      "0.0005423449\n",
      "0.000546479\n",
      "0.0005413355\n",
      "0.0005477013\n",
      "0.00057274505\n",
      "0.0005448847\n",
      "0.0005468884\n",
      "0.0005356328\n",
      "0.0005574473\n",
      "0.0005397083\n",
      "0.0005535612\n",
      "0.0005448351\n",
      "0.0005411461\n",
      "0.0005474977\n",
      "0.0005443069\n",
      "0.00054974505\n",
      "0.0005485258\n",
      "0.0005469733\n",
      "0.0005515325\n",
      "0.00053792424\n",
      "0.0005562488\n",
      "0.0005453696\n",
      "0.000537371\n",
      "0.0005456543\n",
      "0.00054505235\n",
      "0.0005436341\n",
      "0.00053915783\n",
      "0.0005560524\n",
      "0.0005396527\n",
      "0.00053485733\n",
      "0.0005348299\n",
      "0.0005387965\n",
      "0.0005492096\n",
      "0.0005471313\n",
      "0.00054748816\n",
      "0.00054919935\n",
      "0.0005499126\n",
      "0.00054309657\n",
      "0.00054066424\n",
      "0.0005388019\n",
      "0.0005449061\n",
      "0.00054669566\n",
      "0.00053944165\n",
      "0.0005392401\n",
      "0.00054844597\n",
      "0.00053937297\n",
      "0.0005397438\n",
      "0.0005521201\n",
      "0.0005479509\n",
      "0.00055308803\n",
      "0.0005539667\n",
      "0.0005505758\n",
      "0.00054586184\n",
      "0.00054872734\n",
      "0.0005453262\n",
      "0.00053987274\n",
      "0.0005512578\n",
      "0.00054574054\n",
      "0.00053871516\n",
      "0.0005496014\n",
      "0.00053671957\n",
      "0.0005483387\n",
      "0.000549329\n",
      "0.00058019086\n",
      "0.00054459705\n",
      "0.00054521684\n",
      "0.0005435576\n",
      "0.00054665253\n",
      "0.00054049166\n",
      "0.0005518504\n",
      "0.0005414382\n",
      "0.0005514824\n",
      "0.0005483255\n",
      "0.0005473531\n",
      "0.0005398934\n",
      "0.0005514318\n",
      "0.00054465054\n",
      "0.0005458858\n",
      "0.00054927834\n",
      "0.00055048673\n",
      "0.00053593115\n",
      "0.0005385737\n",
      "0.0005408268\n",
      "0.00054486806\n",
      "0.000556381\n",
      "0.0005452205\n",
      "0.0005549307\n",
      "0.0005423918\n",
      "0.00053133344\n",
      "0.0005359017\n",
      "0.0005447541\n",
      "0.00054034323\n",
      "0.0005539722\n",
      "0.00054227293\n",
      "0.00054058986\n",
      "0.0005447029\n",
      "0.0005429368\n",
      "0.00054309645\n",
      "0.0005423163\n",
      "0.0005461737\n",
      "0.000550977\n",
      "0.00054226466\n",
      "0.000540209\n",
      "0.00055346184\n",
      "0.00054569147\n",
      "0.0005401517\n",
      "0.0005412258\n",
      "0.0005395775\n",
      "0.00054673717\n",
      "0.0005524871\n",
      "0.0005366952\n",
      "0.0005412453\n",
      "0.0005404523\n",
      "0.00053621194\n",
      "0.0005404922\n",
      "0.00053992966\n",
      "0.0005411599\n",
      "0.00053794164\n",
      "0.0005464249\n",
      "0.0005505952\n",
      "0.00055629626\n",
      "0.00055184716\n",
      "0.0005517896\n",
      "0.0005420424\n",
      "0.0005491741\n",
      "0.000545846\n",
      "0.00054110604\n",
      "0.000538256\n",
      "0.0005400529\n",
      "0.0005386458\n",
      "0.00054772734\n",
      "0.00054527924\n",
      "0.000552917\n",
      "0.00054795895\n",
      "0.0005405293\n",
      "0.0005380288\n",
      "0.00055407383\n",
      "0.0005753043\n",
      "0.000542729\n",
      "0.0005429725\n",
      "0.0005466919\n",
      "0.000552064\n",
      "0.0005415948\n",
      "0.000550139\n",
      "0.0005407592\n",
      "0.00054327014\n",
      "0.00054782757\n",
      "0.00054246525\n",
      "0.0006509991\n",
      "0.0005383084\n",
      "0.0005432908\n",
      "0.0005497703\n",
      "0.0005533805\n",
      "0.00054676796\n",
      "0.00054288574\n",
      "0.00054216484\n",
      "0.0005539083\n",
      "0.0005452388\n",
      "0.00054292387\n",
      "0.0005457926\n",
      "0.0005505937\n",
      "0.0005422008\n",
      "0.00053621427\n",
      "0.00054445036\n",
      "0.0005456289\n",
      "0.0005446432\n",
      "0.00056556886\n",
      "0.00056829577\n",
      "0.00054490793\n",
      "0.00053406757\n",
      "0.00054903916\n",
      "0.0005381828\n",
      "0.0005383145\n",
      "0.0005364998\n",
      "0.00053668953\n",
      "0.00054620777\n",
      "0.00055124296\n",
      "0.0005532149\n",
      "0.00053670886\n",
      "0.0005510405\n",
      "0.0005508177\n",
      "0.0005436698\n",
      "0.0005368525\n",
      "0.00053897436\n",
      "0.0005578981\n",
      "0.0005512271\n",
      "0.0005493481\n",
      "0.00056227756\n",
      "0.0005370717\n",
      "0.0005538559\n",
      "0.00054210634\n",
      "0.00055018987\n",
      "0.0005449278\n",
      "0.00054362614\n",
      "0.00054730097\n",
      "0.00057253643\n",
      "0.0005462591\n",
      "0.0005469289\n",
      "0.0005436666\n",
      "0.00055419235\n",
      "0.00054948713\n",
      "0.0005531458\n",
      "0.00054773485\n",
      "0.0005709316\n",
      "0.00054263376\n",
      "0.00054141745\n",
      "0.0005449876\n",
      "0.0005616809\n",
      "0.0005502736\n",
      "0.0005581774\n",
      "0.00054804544\n",
      "0.000536775\n",
      "0.0005763956\n",
      "0.0005501729\n",
      "0.0005522178\n",
      "0.0005542189\n",
      "0.00055888086\n",
      "0.0005370238\n",
      "0.00054338644\n",
      "0.0005588462\n",
      "0.00053839094\n",
      "0.0005440496\n",
      "0.0005817568\n",
      "0.00054734905\n",
      "0.00055193494\n",
      "0.0005811533\n",
      "0.00053668\n",
      "0.0005453283\n",
      "0.0005407314\n",
      "0.0005487627\n",
      "0.00055635156\n",
      "0.0005495436\n",
      "0.0005451146\n",
      "0.00054460386\n",
      "0.00055011397\n",
      "0.00055307045\n",
      "0.0005485184\n",
      "0.00054373307\n",
      "0.0005439094\n",
      "0.0005581877\n",
      "0.0005437728\n",
      "0.00054035836\n",
      "0.0005496779\n",
      "0.0005569394\n",
      "0.00053989707\n",
      "0.0005437385\n",
      "0.000537486\n",
      "0.0005478883\n",
      "0.0005448802\n",
      "0.0005551778\n",
      "0.00054389593\n",
      "0.0005425255\n",
      "0.00054606184\n",
      "0.0005424106\n",
      "0.0005414644\n",
      "0.00054324686\n",
      "0.0005454308\n",
      "0.0005469304\n",
      "0.00054649153\n",
      "0.0005397766\n",
      "0.00055000116\n",
      "0.000550555\n",
      "0.000540189\n",
      "0.0005530829\n",
      "0.00053714315\n",
      "0.0005351031\n",
      "0.000548101\n",
      "0.00054689235\n",
      "0.000547055\n",
      "0.0005451078\n",
      "0.0005462038\n",
      "0.0005517844\n",
      "0.0005479569\n",
      "0.00059282104\n",
      "0.00054544775\n",
      "0.00054582604\n",
      "0.00054799387\n",
      "0.00054317544\n",
      "0.00054215593\n",
      "0.00055435987\n",
      "0.00054513325\n",
      "0.0005379588\n",
      "0.0005487546\n",
      "0.0005370145\n",
      "0.0005510761\n",
      "0.0005503215\n",
      "0.000664139\n",
      "0.0005483639\n",
      "0.0005432997\n",
      "0.00054520625\n",
      "0.00054878026\n",
      "0.0005468356\n",
      "0.0005481759\n",
      "0.0005615544\n",
      "0.0005506624\n",
      "0.0005682525\n",
      "0.0005428888\n",
      "0.00054658373\n",
      "0.00054911384\n",
      "0.00055330165\n",
      "0.00053630065\n",
      "0.00054899737\n",
      "0.00055394444\n",
      "0.00063464465\n",
      "0.0005460392\n",
      "0.0005492939\n",
      "0.0005626812\n",
      "0.00054674293\n",
      "0.0005460311\n",
      "0.0005460257\n",
      "0.0005389155\n",
      "0.0005442388\n",
      "0.0005590581\n",
      "0.00054636446\n",
      "0.0005506551\n",
      "0.0005446864\n",
      "0.0005462887\n",
      "0.00057948974\n",
      "0.0005460241\n",
      "0.0005482574\n",
      "0.0005648052\n",
      "0.0005667295\n",
      "0.000544955\n",
      "0.00054472947\n",
      "0.0005443987\n",
      "0.0005399684\n",
      "0.00055517285\n",
      "0.000549533\n",
      "0.0005488356\n",
      "0.000543665\n",
      "0.00055591005\n",
      "0.0005478822\n",
      "0.0005395455\n",
      "0.0005444546\n",
      "0.0005434164\n",
      "0.0005449813\n",
      "0.00054362335\n",
      "0.00054538785\n",
      "0.0005464874\n",
      "0.0005419215\n",
      "0.000542499\n",
      "0.0005483571\n",
      "0.0005516283\n",
      "0.000550642\n",
      "0.0005415726\n",
      "0.00055666774\n",
      "0.00053977844\n",
      "0.0005478274\n",
      "0.0005474218\n",
      "0.00054585125\n",
      "0.0005425834\n",
      "0.0005607603\n",
      "0.0005610241\n",
      "0.0005599659\n",
      "0.0005599529\n",
      "0.0005562394\n",
      "0.000549985\n",
      "0.00055621815\n",
      "0.00054303155\n",
      "0.00055573677\n",
      "0.000544619\n",
      "0.0005430003\n",
      "0.0005421107\n",
      "0.00055192714\n",
      "0.00055023504\n",
      "0.00054361933\n",
      "0.00058691244\n",
      "0.0005420075\n",
      "0.0005463527\n",
      "0.00054041814\n",
      "0.0005467911\n",
      "0.00054566615\n",
      "0.0005440003\n",
      "0.0005521357\n",
      "0.00054517225\n",
      "0.00054535794\n",
      "0.0005606265\n",
      "0.00053787685\n",
      "0.000548221\n",
      "0.0005721795\n",
      "0.00054659706\n",
      "0.0005581758\n",
      "0.00055503985\n",
      "0.0005502273\n",
      "0.00054950657\n",
      "0.0005428659\n",
      "0.0005507097\n",
      "0.00055392546\n",
      "0.0005534597\n",
      "0.00055910554\n",
      "0.00055411225\n",
      "0.0005492469\n",
      "0.0005512636\n",
      "0.00053723465\n",
      "0.00054431014\n",
      "0.00054356636\n",
      "0.00055190455\n",
      "0.00053782255\n",
      "0.0005518465\n",
      "0.0005429246\n",
      "0.0005493143\n",
      "0.00054896274\n",
      "0.00054992695\n",
      "0.0005422421\n",
      "0.0005781961\n",
      "0.0005473955\n",
      "0.0005513834\n",
      "0.0005414551\n",
      "0.0005442553\n",
      "0.00055298547\n",
      "0.0005488761\n",
      "0.0005463921\n",
      "0.0005464635\n",
      "0.00054715626\n",
      "0.00054974767\n",
      "0.0005463451\n",
      "0.00054617703\n",
      "0.0005497664\n",
      "0.00055242213\n",
      "0.0005499224\n",
      "0.00054802065\n",
      "0.00055150676\n",
      "0.00054399314\n",
      "0.0005380653\n",
      "0.00054836826\n",
      "0.000554632\n",
      "0.00055208657\n",
      "0.00054392114\n",
      "0.0005445374\n",
      "0.000538643\n",
      "0.00054851576\n",
      "0.00054155977\n",
      "0.0005488598\n",
      "0.0005401033\n",
      "0.0005418306\n",
      "0.00054668786\n",
      "0.00054374273\n",
      "0.0005872723\n",
      "0.0005445272\n",
      "0.00054554665\n",
      "0.00054015935\n",
      "0.0005460395\n",
      "0.00054164097\n",
      "0.00055994384\n",
      "0.0005375687\n",
      "0.00054679695\n",
      "0.00055995554\n",
      "0.00054366305\n",
      "0.00055144314\n",
      "0.0005520251\n",
      "0.0005401819\n",
      "0.000546208\n",
      "0.0005544582\n",
      "0.00054036157\n",
      "0.00055773574\n",
      "0.00054550887\n",
      "0.00056075625\n",
      "0.0005460038\n",
      "0.00054215477\n",
      "0.00053786166\n",
      "0.0005368466\n",
      "0.0005581298\n",
      "0.00056023826\n",
      "0.00054888363\n",
      "0.0005539489\n",
      "0.00054882467\n",
      "0.0005657847\n",
      "0.0005502299\n",
      "0.0005465509\n",
      "0.00055006845\n",
      "0.0005408794\n",
      "0.0005422149\n",
      "0.0005473671\n",
      "0.00055632414\n",
      "0.0005536602\n",
      "0.00056167576\n",
      "0.0005575172\n",
      "0.00055004994\n",
      "0.00054361\n",
      "0.00054155645\n",
      "0.00054963323\n",
      "0.00055673067\n",
      "0.0005453392\n",
      "0.0006406646\n",
      "0.00054143346\n",
      "0.0005519009\n",
      "0.0005459175\n",
      "0.00054539513\n",
      "0.0005581848\n",
      "0.0005417246\n",
      "0.0005398503\n",
      "0.0005467981\n",
      "0.0005422656\n",
      "0.00055063545\n",
      "0.00061352877\n",
      "0.0005510863\n",
      "0.00054900127\n",
      "0.00055125554\n",
      "0.0005439592\n",
      "0.0005448191\n",
      "0.00055159937\n",
      "0.00054127845\n",
      "0.00054645515\n",
      "0.0005422747\n",
      "0.0005461984\n",
      "0.0005498674\n",
      "0.0005376059\n",
      "0.00055362\n",
      "0.0005556685\n",
      "0.000545365\n",
      "0.000554316\n",
      "0.0005426064\n",
      "0.0005409396\n",
      "0.00054131396\n",
      "0.0005466982\n",
      "0.0005422801\n",
      "0.0005480281\n",
      "0.0005873259\n",
      "0.00054841756\n",
      "0.00055107224\n",
      "0.00054295134\n",
      "0.0005464883\n",
      "0.00054307666\n",
      "0.0005920042\n",
      "0.00054162287\n",
      "0.0005593426\n",
      "0.00054398493\n",
      "0.0005667891\n",
      "0.0005459601\n",
      "0.0005389028\n",
      "0.00055529753\n",
      "0.0005418466\n",
      "0.0005469114\n",
      "0.00054650236\n",
      "0.0005559727\n",
      "0.0005425213\n",
      "0.0005464598\n",
      "0.00057387707\n",
      "0.0005345026\n",
      "0.0005422292\n",
      "0.0005362063\n",
      "0.0005425538\n",
      "0.00054600067\n",
      "0.0005440234\n",
      "0.0005359785\n",
      "0.00054666435\n",
      "0.0005456358\n",
      "0.00054300675\n",
      "0.00055556395\n",
      "0.0005550811\n",
      "0.0005442283\n",
      "0.00053853274\n",
      "0.00054053165\n",
      "0.0005410838\n",
      "0.0005418469\n",
      "0.0005412333\n",
      "0.000544492\n",
      "0.00054276554\n",
      "0.0005473265\n",
      "0.0005468584\n",
      "0.0005392541\n",
      "0.000553151\n",
      "0.0005447731\n",
      "0.0005427675\n",
      "0.00054417213\n",
      "0.0005412754\n",
      "0.000563885\n",
      "0.0005432379\n",
      "0.00066974066\n",
      "0.00054247695\n",
      "0.0005539821\n",
      "0.0005539853\n",
      "0.0005498236\n",
      "0.00053933315\n",
      "0.00055834156\n",
      "0.0005450142\n",
      "0.00054728024\n",
      "0.00055389095\n",
      "0.0005411942\n",
      "0.0005356024\n",
      "0.00055374485\n",
      "0.00057464716\n",
      "0.0005381621\n",
      "0.000548698\n",
      "0.00054496125\n",
      "0.00054518844\n",
      "0.00054319605\n",
      "0.00053495116\n",
      "0.00055094046\n",
      "0.00055368914\n",
      "0.0005565257\n",
      "0.0005523173\n",
      "0.00054974953\n",
      "0.0005389969\n",
      "0.00055526895\n",
      "0.0005514255\n",
      "0.000536801\n",
      "0.0005867926\n",
      "0.0005427153\n",
      "0.0005497667\n",
      "0.00053808605\n",
      "0.0005456526\n",
      "0.0005444552\n",
      "0.00054176606\n",
      "0.0005458201\n",
      "0.00054448965\n",
      "0.00053811865\n",
      "0.00054449926\n",
      "0.00054122927\n",
      "0.0005363971\n",
      "0.00054428086\n",
      "0.00053717627\n",
      "0.00054752745\n",
      "0.0005527864\n",
      "0.00054907444\n",
      "0.00054258393\n",
      "0.000542542\n",
      "0.0005565715\n",
      "0.0005438122\n",
      "0.0005604162\n",
      "0.00055258855\n",
      "0.00054332777\n",
      "0.00054738537\n",
      "0.0005439953\n",
      "0.00055079156\n",
      "0.00068400323\n",
      "0.00053295866\n",
      "0.00055984233\n",
      "0.0005459788\n",
      "0.0005430985\n",
      "0.0005392408\n",
      "0.00054649456\n",
      "0.0005430142\n",
      "0.00054244045\n",
      "0.0005430081\n",
      "0.00054402446\n",
      "0.00054568506\n",
      "0.00054390036\n",
      "0.00055313244\n",
      "0.000548526\n",
      "0.0005429363\n",
      "0.000544531\n",
      "0.00054889754\n",
      "0.00055301347\n",
      "0.0005437377\n",
      "0.0005438741\n",
      "0.0005460372\n",
      "0.00053780887\n",
      "0.0005427072\n",
      "0.000543844\n",
      "0.00054349296\n",
      "0.0005479636\n",
      "0.0005409986\n",
      "0.00054684497\n",
      "0.00055729016\n",
      "0.00053547806\n",
      "0.00055457593\n",
      "0.00053846295\n",
      "0.0005467713\n",
      "0.0005502567\n",
      "0.00054063654\n",
      "0.0005524086\n",
      "0.0005634341\n",
      "0.0005408593\n",
      "0.0005463002\n",
      "0.0005672434\n",
      "0.00055492943\n",
      "0.00054202345\n",
      "0.00057908375\n",
      "0.00055788347\n",
      "0.0005571455\n",
      "0.0005566931\n",
      "0.000551364\n",
      "0.00055080565\n",
      "0.0005449258\n",
      "0.00054889667\n",
      "0.00055490545\n",
      "0.0005598905\n",
      "0.0005429109\n",
      "0.00056060834\n",
      "0.00054477877\n",
      "0.00054995215\n",
      "0.0005419597\n",
      "0.00055919203\n",
      "0.00053959736\n",
      "0.00054038916\n",
      "0.00055783853\n",
      "0.0005357755\n",
      "0.00053723407\n",
      "0.00054092007\n",
      "0.0005512736\n",
      "0.0005512573\n",
      "0.0005388905\n",
      "0.00054213835\n",
      "0.0005380973\n",
      "0.0005342669\n",
      "0.00054150843\n",
      "0.0005422576\n",
      "0.00054685544\n",
      "0.0005409618\n",
      "0.00054097275\n",
      "0.0005495684\n",
      "0.00054773776\n",
      "0.00054203573\n",
      "0.0005367355\n",
      "0.00054291\n",
      "0.00054616155\n",
      "0.0005475548\n",
      "0.00053489994\n",
      "0.0005495023\n",
      "0.00055483234\n",
      "0.00054846314\n",
      "0.0005412934\n",
      "0.0005337954\n",
      "0.0005698937\n",
      "0.0005390248\n",
      "0.0005282131\n",
      "0.0005423004\n",
      "0.00055953336\n",
      "0.00053349003\n",
      "0.0005504905\n",
      "0.00055133866\n",
      "0.00055397704\n",
      "0.00054457\n",
      "0.00053886446\n",
      "0.0005361352\n",
      "0.000543619\n",
      "0.0005399649\n",
      "0.00054656295\n",
      "0.00054784276\n",
      "0.00055067765\n",
      "0.0005439038\n",
      "0.0005373956\n",
      "0.0005459203\n",
      "0.00053358846\n",
      "0.00054233667\n",
      "0.0005475003\n",
      "0.0005482788\n",
      "0.0005423066\n",
      "0.00054452574\n",
      "0.00054691336\n",
      "0.0005439164\n",
      "0.00053293695\n",
      "0.0005401416\n",
      "0.0005435851\n",
      "0.00055196363\n",
      "0.00054574973\n",
      "0.000547272\n",
      "0.00052958203\n",
      "0.00054593495\n",
      "0.00055435614\n",
      "0.00054245937\n",
      "0.0005510524\n",
      "0.0005442494\n",
      "0.00054876157\n",
      "0.0005467529\n",
      "0.0005405225\n",
      "0.0005443061\n",
      "0.00054133125\n",
      "0.00053512404\n",
      "0.0005469888\n",
      "0.00054071285\n",
      "0.0005342057\n",
      "0.0005451586\n",
      "0.0005450506\n",
      "0.00054842076\n",
      "0.0005502757\n",
      "0.0005451726\n",
      "0.0005396164\n",
      "0.0005448276\n",
      "0.0005457326\n",
      "0.0005333686\n",
      "0.0005418194\n",
      "0.0005410143\n",
      "0.00054217986\n",
      "0.0005446134\n",
      "0.0005450751\n",
      "0.0005415695\n",
      "0.0005555953\n",
      "0.00054018083\n",
      "0.0005414149\n",
      "0.000551671\n",
      "0.0005546193\n",
      "0.0005426148\n",
      "0.0005387141\n",
      "0.0005419176\n",
      "0.0005460059\n",
      "0.0005428796\n",
      "0.0005374463\n",
      "0.00054528145\n",
      "0.0006724596\n",
      "0.00054373464\n",
      "0.00054758653\n",
      "0.00054435304\n",
      "0.00054735556\n",
      "0.000549542\n",
      "0.00053715374\n",
      "0.0005388318\n",
      "0.00055364857\n",
      "0.000550175\n",
      "0.00056466454\n",
      "0.0005630422\n",
      "0.0006149836\n",
      "0.0005403754\n",
      "0.0005404114\n",
      "0.00053297024\n",
      "0.0005531134\n",
      "0.0005519031\n",
      "0.000532917\n",
      "0.00054906675\n",
      "0.0005367661\n",
      "0.0006940402\n",
      "0.00054882653\n",
      "0.00054774835\n",
      "0.0005499777\n",
      "0.0005386555\n",
      "0.0005476035\n",
      "0.00053525995\n",
      "0.0006432039\n",
      "0.000540328\n",
      "0.00054616685\n",
      "0.0005414803\n",
      "0.00043930986\n",
      "0.0005514416\n",
      "0.0005407152\n",
      "0.0005474859\n",
      "0.0005440092\n",
      "0.000552805\n",
      "0.00053854473\n",
      "0.0005448789\n",
      "0.0005628949\n",
      "0.0005510438\n",
      "0.0005387233\n",
      "0.0005395437\n",
      "0.00055002794\n",
      "0.0005399936\n",
      "0.000542283\n",
      "0.000538568\n",
      "0.0005409596\n",
      "0.0005441838\n",
      "0.0005551848\n",
      "0.00054559123\n",
      "0.0005487609\n",
      "0.0005518146\n",
      "0.00054651935\n",
      "0.00055947993\n",
      "0.00054356735\n",
      "0.0005532082\n",
      "0.00054441974\n",
      "0.0005572315\n",
      "0.000556733\n",
      "0.00055415713\n",
      "0.00054605736\n",
      "0.00054116134\n",
      "0.000541867\n",
      "0.00055297365\n",
      "0.0005473551\n",
      "0.000544173\n",
      "0.0005453933\n",
      "0.00054971693\n",
      "0.0005426216\n",
      "0.00054175017\n",
      "0.0005492388\n",
      "0.00056169485\n",
      "0.0005402259\n",
      "0.00054723694\n",
      "0.00054586085\n",
      "0.0005496379\n",
      "0.0005424625\n",
      "0.00054769905\n",
      "0.0005425724\n",
      "0.00055008964\n",
      "0.000550079\n",
      "0.0005449493\n",
      "0.00055266364\n",
      "0.0005469024\n",
      "0.0005416104\n",
      "0.000543066\n",
      "0.0005361875\n",
      "0.0005325193\n",
      "0.00054435123\n",
      "0.0005457546\n",
      "0.0005404928\n",
      "0.0005379304\n",
      "0.00054203876\n",
      "0.00055066735\n",
      "0.0005442146\n",
      "0.000542987\n",
      "0.00053332886\n",
      "0.00053515285\n",
      "0.00054918666\n",
      "0.0005454006\n",
      "0.00054506154\n",
      "0.0005444505\n",
      "0.0005575712\n",
      "0.0005500986\n",
      "0.0005460478\n",
      "0.00054255076\n",
      "0.00054278807\n",
      "0.0005474772\n",
      "0.00054437824\n",
      "0.0005377724\n",
      "0.0005431072\n",
      "0.0005381619\n",
      "0.00056492025\n",
      "0.0005528619\n",
      "0.0005460912\n",
      "0.0005433547\n",
      "0.0005532913\n",
      "0.00057471765\n",
      "0.0005477179\n",
      "0.00054781284\n",
      "0.00053913466\n",
      "0.00054103526\n",
      "0.00055450614\n",
      "0.0005433973\n",
      "0.00054829015\n",
      "0.0005407249\n",
      "0.00054409716\n",
      "0.00055449706\n",
      "0.0005404046\n",
      "0.000540632\n",
      "0.0005404243\n",
      "0.000546363\n",
      "0.0005348396\n",
      "0.000539381\n",
      "0.000550559\n",
      "0.00053907564\n",
      "0.0005450393\n",
      "0.0005475655\n",
      "0.000537041\n",
      "0.00057313807\n",
      "0.0005416496\n",
      "0.0005414282\n",
      "0.0005573568\n",
      "0.00054492423\n",
      "0.0005353774\n",
      "0.00054452196\n",
      "0.0005420158\n",
      "0.0005423206\n",
      "0.00054271735\n",
      "0.0005418287\n",
      "0.0005469774\n",
      "0.0005445397\n",
      "0.0005466552\n",
      "0.00054705417\n",
      "0.00053828815\n",
      "0.0005433187\n",
      "0.0005427878\n",
      "0.000565773\n",
      "0.0005438428\n",
      "0.00057068287\n",
      "0.00053409726\n",
      "0.0005411689\n",
      "0.0005386456\n",
      "0.0005428346\n",
      "0.00054634083\n",
      "0.00055786414\n",
      "0.0005442526\n",
      "0.0005416938\n",
      "0.00057284493\n",
      "0.0005409687\n",
      "0.00054349797\n",
      "0.0005408581\n",
      "0.0005481258\n",
      "0.0005441435\n",
      "0.00053987815\n",
      "0.00054134347\n",
      "0.00054915494\n",
      "0.00054977974\n",
      "0.0005442168\n",
      "0.0005602539\n",
      "0.00055261166\n",
      "0.00054114696\n",
      "0.00054061506\n",
      "0.0005568103\n",
      "0.00054096634\n",
      "0.0005499897\n",
      "0.0006416262\n",
      "0.000556648\n",
      "0.00054997567\n",
      "0.0005328904\n",
      "0.0005434723\n",
      "0.0005376292\n",
      "0.0005393671\n",
      "0.00054888014\n",
      "0.0005416132\n",
      "0.00054693245\n",
      "0.00055934157\n",
      "0.0005469718\n",
      "0.00055666093\n",
      "0.0005500717\n",
      "0.00054459204\n",
      "0.00054373575\n",
      "0.00054350105\n",
      "0.0005387624\n",
      "0.0005372766\n",
      "0.00054225855\n",
      "0.0005619396\n",
      "0.00053578574\n",
      "0.0005395359\n",
      "0.0005457968\n",
      "0.0005395631\n",
      "0.00053813815\n",
      "0.00054246077\n",
      "0.0005454129\n",
      "0.00053766684\n",
      "0.0005429065\n",
      "0.00054323307\n",
      "0.00053941086\n",
      "0.00053855835\n",
      "0.0005480846\n",
      "0.00053972314\n",
      "0.00054480176\n",
      "0.0005399602\n",
      "0.00061951095\n",
      "0.0005444077\n",
      "0.00053509587\n",
      "0.0005412924\n",
      "0.00054745254\n",
      "0.0005398531\n",
      "0.0005427782\n",
      "0.0005520792\n",
      "0.00054744084\n",
      "0.00053813745\n",
      "0.00054147077\n",
      "0.00053742895\n",
      "0.0005587334\n",
      "0.00054811727\n",
      "0.00060109264\n",
      "0.00054378406\n",
      "0.0005402127\n",
      "0.00053659757\n",
      "0.0005703713\n",
      "0.00054514786\n",
      "0.0005491832\n",
      "0.00054128584\n",
      "0.0005452379\n",
      "0.0005401137\n",
      "0.0005357259\n",
      "0.00053065276\n",
      "0.0005474352\n",
      "0.0005369643\n",
      "0.0005535777\n",
      "0.0005427063\n",
      "0.00054470194\n",
      "0.0005482268\n",
      "0.0005348211\n",
      "0.00054707704\n",
      "0.00054329593\n",
      "0.00054229953\n",
      "0.0005534012\n",
      "0.00054281746\n",
      "0.0005421776\n",
      "0.000550066\n",
      "0.0005630074\n",
      "0.000542412\n",
      "0.0005422994\n",
      "0.0005559444\n",
      "0.0005390159\n",
      "0.0005552448\n",
      "0.00054725574\n",
      "0.0005387313\n",
      "0.00053683476\n",
      "0.00054844643\n",
      "0.0005464892\n",
      "0.0005520675\n",
      "0.00054372713\n",
      "0.0005375352\n",
      "0.0005428454\n",
      "0.0005411102\n",
      "0.0005363423\n",
      "0.0005451579\n",
      "0.00054071314\n",
      "0.0005448054\n",
      "0.0005499183\n",
      "0.00054601335\n",
      "0.00053687446\n",
      "0.0005366554\n",
      "0.0005356402\n",
      "0.0005393182\n",
      "0.00054420345\n",
      "0.00054959464\n",
      "0.00055492576\n",
      "0.00054494035\n",
      "0.00055357127\n",
      "0.0005418632\n",
      "0.0005413279\n",
      "0.00057008595\n",
      "0.0005407124\n",
      "0.0005463882\n",
      "0.00054579636\n",
      "0.00054003083\n",
      "0.00054411776\n",
      "0.0005426005\n",
      "0.0005456686\n",
      "0.000537848\n",
      "0.0005351101\n",
      "0.0005427303\n",
      "0.00054490595\n",
      "0.0005363134\n",
      "0.0005465944\n",
      "0.0005475276\n",
      "0.0005458991\n",
      "0.0005410088\n",
      "0.000530906\n",
      "0.0005413323\n",
      "0.00053675316\n",
      "0.00053667283\n",
      "0.00054046256\n",
      "0.00053971476\n",
      "0.00055661716\n",
      "0.00054343\n",
      "0.0005399695\n",
      "0.0005446929\n",
      "0.0005444318\n",
      "0.0005473992\n",
      "0.0005668694\n",
      "0.0005344857\n",
      "0.0005522312\n",
      "0.00054314686\n",
      "0.00055359874\n",
      "0.00053853734\n",
      "0.00054210564\n",
      "0.0005545991\n",
      "0.00055167155\n",
      "0.0005463668\n",
      "0.0005519978\n",
      "0.00054379855\n",
      "0.00053634134\n",
      "0.0005509754\n",
      "0.0005503622\n",
      "0.0005491423\n",
      "0.0005404716\n",
      "0.00055356545\n",
      "0.00054647465\n",
      "0.00054086547\n",
      "0.0005368016\n",
      "0.0005546159\n",
      "0.0005428072\n",
      "0.00054361735\n",
      "0.00054553594\n",
      "0.00054580555\n",
      "0.0005433713\n",
      "0.0005436042\n",
      "0.00055101374\n",
      "0.0005478652\n",
      "0.0005458111\n",
      "0.0005584481\n",
      "0.0005411121\n",
      "0.00054249616\n",
      "0.0006451758\n",
      "0.0005445333\n",
      "0.0005417986\n",
      "0.00055208284\n",
      "0.00053952925\n",
      "0.0005798482\n",
      "0.0005397504\n",
      "0.00054216833\n",
      "0.00053838716\n",
      "0.0005413525\n",
      "0.00054061017\n",
      "0.0005441447\n",
      "0.00054046453\n",
      "0.00054844393\n",
      "0.00053607055\n",
      "0.00059900095\n",
      "0.0005470049\n",
      "0.0005390588\n",
      "0.0005384953\n",
      "0.0005431889\n",
      "0.00055102294\n",
      "0.00054485985\n",
      "0.0005509029\n",
      "0.0005457236\n",
      "0.00053871184\n",
      "0.0005393949\n",
      "0.00053358334\n",
      "0.0005407897\n",
      "0.0005584509\n",
      "0.0005335\n",
      "0.0005405661\n",
      "0.00053914817\n",
      "0.0005376494\n",
      "0.0005448173\n",
      "0.00054129056\n",
      "0.00054172624\n",
      "0.00053741224\n",
      "0.00055043714\n",
      "0.0005495644\n",
      "0.0005384411\n",
      "0.0005487986\n",
      "0.00053585577\n",
      "0.00054986426\n",
      "0.00054199673\n",
      "0.0005762671\n",
      "0.00054047274\n",
      "0.0005450595\n",
      "0.0005569506\n",
      "0.000555516\n",
      "0.00061716\n",
      "0.0005515308\n",
      "0.0005441841\n",
      "0.0005312305\n",
      "0.00054161437\n",
      "0.00054859027\n",
      "0.00053608126\n",
      "0.0005508714\n",
      "0.00054200983\n",
      "0.00054152816\n",
      "0.00054365344\n",
      "0.00057927746\n",
      "0.0005488311\n",
      "0.0005679306\n",
      "0.00054103835\n",
      "0.0005599656\n",
      "0.00054901786\n",
      "0.00053646346\n",
      "0.0005383032\n",
      "0.0005451758\n",
      "0.00059276604\n",
      "0.00055334513\n",
      "0.00055700185\n",
      "0.0005415448\n",
      "0.0005481392\n",
      "0.00054496183\n",
      "0.00054015126\n",
      "0.00054543087\n",
      "0.00055077486\n",
      "0.0005475413\n",
      "0.0005445207\n",
      "0.00055799\n",
      "0.000533474\n",
      "0.0005452422\n",
      "0.00054005586\n",
      "0.0005449271\n",
      "0.0005851318\n",
      "0.0005525345\n",
      "0.00054190145\n",
      "0.00054343673\n",
      "0.0005382974\n",
      "0.0005412955\n",
      "0.0005432179\n",
      "0.00053799315\n",
      "0.00053936365\n",
      "0.0005549208\n",
      "0.00054341747\n",
      "0.00055097195\n",
      "0.0005402778\n",
      "0.0005395046\n",
      "0.0005372872\n",
      "0.0005396075\n",
      "0.0006503688\n",
      "0.0005418972\n",
      "0.00054273015\n",
      "0.00054568145\n",
      "0.0005495966\n",
      "0.0005546021\n",
      "0.00054539926\n",
      "0.0005449885\n",
      "0.00054986536\n",
      "0.00054476364\n",
      "0.00054072856\n",
      "0.0005483147\n",
      "0.00054125587\n",
      "0.000549383\n",
      "0.00054278073\n",
      "0.0005389702\n",
      "0.00053670077\n",
      "0.0005463865\n",
      "0.00054938736\n",
      "0.0005464475\n",
      "0.00054413517\n",
      "0.000545346\n",
      "0.0005825301\n",
      "0.00054871273\n",
      "0.0005431926\n",
      "0.0005432698\n",
      "0.0005428384\n",
      "0.0005785178\n",
      "0.0005483924\n",
      "0.00054500846\n",
      "0.00055449776\n",
      "0.0005400076\n",
      "0.0005401533\n",
      "0.0005469079\n",
      "0.0005411253\n",
      "0.0005376456\n",
      "0.0005495189\n",
      "0.00053750724\n",
      "0.00054031715\n",
      "0.0005446414\n",
      "0.0005764493\n",
      "0.00055290083\n",
      "0.0005584519\n",
      "0.00054653455\n",
      "0.00054700737\n",
      "0.00055830576\n",
      "0.0005596476\n",
      "0.00053951435\n",
      "0.0005360701\n",
      "0.00053720205\n",
      "0.000540231\n",
      "0.00055312575\n",
      "0.00054943375\n",
      "0.0005785037\n",
      "0.00054037047\n",
      "0.00053276366\n",
      "0.0005389892\n",
      "0.0005504081\n",
      "0.0005418335\n",
      "0.0005511203\n",
      "0.00055705645\n",
      "0.00054441724\n",
      "0.0005367306\n",
      "0.00054979144\n",
      "0.00053778687\n",
      "0.0005343468\n",
      "0.0005327942\n",
      "0.0005452599\n",
      "0.00054223055\n",
      "0.0005429646\n",
      "0.00054570654\n",
      "0.00065264426\n",
      "0.00054759154\n",
      "0.00054168515\n",
      "0.00054983626\n",
      "0.0005407038\n",
      "0.0005435461\n",
      "0.0005448294\n",
      "0.0005504934\n",
      "0.0005447892\n",
      "0.0005461627\n",
      "0.0005442988\n",
      "0.0005468565\n",
      "0.0005456753\n",
      "0.0005599811\n",
      "0.0005389784\n",
      "0.0005455084\n",
      "0.0005358066\n",
      "0.0005491044\n",
      "0.00054237\n",
      "0.00054567005\n",
      "0.0005612149\n",
      "0.0005408713\n",
      "0.0005505537\n",
      "0.0005451732\n",
      "0.0005557935\n",
      "0.00054514786\n",
      "0.0005453024\n",
      "0.00054080377\n",
      "0.00054488826\n",
      "0.00055040506\n",
      "0.0005787141\n",
      "0.0005390123\n",
      "0.00054872583\n",
      "0.0005576169\n",
      "0.0005463676\n",
      "0.0005652609\n",
      "0.0005494681\n",
      "0.0005432746\n",
      "0.00054580957\n",
      "0.0005454809\n",
      "0.0005609887\n",
      "0.00054218556\n",
      "0.0005418183\n",
      "0.0005485822\n",
      "0.0005386015\n",
      "0.00055409595\n",
      "0.0005518511\n",
      "0.00055289216\n",
      "0.0005464208\n",
      "0.0005488109\n",
      "0.0005555435\n",
      "0.0005508765\n",
      "0.00055518857\n",
      "0.0005424861\n",
      "0.00054052804\n",
      "0.0005424173\n",
      "0.00054812664\n",
      "0.00055297976\n",
      "0.00054606487\n",
      "0.00054294814\n",
      "0.0005393486\n",
      "0.0005736697\n",
      "0.00054237584\n",
      "0.0005489452\n",
      "0.0005413758\n",
      "0.0005466225\n",
      "0.00061712775\n",
      "0.00056161365\n",
      "0.00054660294\n",
      "0.00053633895\n",
      "0.00055569626\n",
      "0.0005419966\n",
      "0.0005557002\n",
      "0.00054056564\n",
      "0.00053622987\n",
      "0.0005592196\n",
      "0.00055096776\n",
      "0.0005480393\n",
      "0.0005467126\n",
      "0.0005795945\n",
      "0.0005746832\n",
      "0.00054833817\n",
      "0.00053818646\n",
      "0.0005404852\n",
      "0.00055315404\n",
      "0.0005377845\n",
      "0.00054587715\n",
      "0.0006111484\n",
      "0.0005481538\n",
      "0.0005366819\n",
      "0.00053828093\n",
      "0.00053693727\n",
      "0.0005573991\n",
      "0.00054844207\n",
      "0.0005412453\n",
      "0.00054577936\n",
      "0.00054729084\n",
      "0.0005382123\n",
      "0.00055036525\n",
      "0.00056988467\n",
      "0.00055445824\n",
      "0.00053822243\n",
      "0.0005574448\n",
      "0.00055056467\n",
      "0.0005364059\n",
      "0.0005449893\n",
      "0.0005611624\n",
      "0.00067216787\n",
      "0.0005376657\n",
      "0.0005589261\n",
      "0.00054441585\n",
      "0.0005498653\n",
      "0.0005800665\n",
      "0.0005497745\n",
      "0.00055404264\n",
      "0.0005464429\n",
      "0.0005393285\n",
      "0.00054162944\n",
      "0.00056288525\n",
      "0.0005359735\n",
      "0.0005404096\n",
      "0.0005401592\n",
      "0.00054527225\n",
      "0.00055732456\n",
      "0.000545641\n",
      "0.000536215\n",
      "0.000540414\n",
      "0.00054467714\n",
      "0.0005383304\n",
      "0.0005381535\n",
      "0.00054465537\n",
      "0.0005432368\n"
     ]
    }
   ],
   "source": [
    "scale_s = s\n",
    "for i in scale_s:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/effl_project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.229, 0.224, 0.225)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET_DEFAULT_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.004078466"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048813503"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.004078466, 0.08444769, -0.49945402, 0.49980858)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc.mean(), loc.var(), loc.min(), loc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(1,1, 1)\n",
    "\n",
    "ax.hist(loc, bins=100, density=True)\n",
    "# plt.title(p)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.018677436"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    \n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "\n",
    "    return os.path.join(path, 'snapshots', revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"/home/jgryu/Weight_compression/model_zoo/huggingface/\"\n",
    "# model_name =  \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model_name =  \"google--codegemma-1.1-7b-it\"\n",
    "\n",
    "\n",
    "filepath = latest_version_path(cache_path, model_name)\n",
    "# print(filepath)\n",
    "if filepath is not None:\n",
    "    model = AutoModelForCausalLM.from_pretrained(filepath, local_files_only=True)\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        print(k)\n",
    "        print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 3/3 [00:06<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "torch.Size([32000, 5120])\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.0.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.1.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.2.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.3.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.4.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.5.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.6.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.7.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.8.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.9.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.10.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.11.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.12.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.13.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.14.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.15.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.16.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.17.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.18.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.19.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.20.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.21.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.22.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.23.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.24.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.25.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.26.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.27.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.28.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.28.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.29.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.29.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.30.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.30.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.31.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.32.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.32.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.32.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.32.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.33.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.33.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.33.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.33.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.34.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.34.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.34.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.34.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.35.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.35.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.35.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.35.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.36.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.36.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.36.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.36.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.37.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.37.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.37.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.37.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.38.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.38.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.38.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.38.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.39.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.39.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.39.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.39.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.norm.weight\n",
      "torch.Size([5120])\n",
      "lm_head.weight\n",
      "torch.Size([32000, 5120])\n"
     ]
    }
   ],
   "source": [
    "cache_path = \"/home/jgryu/Weight_compression/model_zoo/huggingface/\"\n",
    "model_name =  \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# model_name =  \"google--codegemma-1.1-7b-it\"\n",
    "\n",
    "\n",
    "filepath = latest_version_path(cache_path, model_name)\n",
    "# print(filepath)\n",
    "if filepath is not None:\n",
    "    model = AutoModelForCausalLM.from_pretrained(filepath, local_files_only=True)\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        print(k)\n",
    "        print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 'model.safetensors.index.json' files with 'phi' in the path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "def count_files_with_name(directory, target_filename, keyword):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == target_filename and keyword in os.path.join(root, file).lower():\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "#  \n",
    "directory_path = \"/home/jgryu/Weight_compression/Wparam_dataset/model_zoo/huggingface\"  #    .\n",
    "target_file = \"model.safetensors.index.json\"\n",
    "keyword = \"phi\"\n",
    "\n",
    "file_count = count_files_with_name(directory_path, target_file, keyword)\n",
    "print(f\"Found {file_count} '{target_file}' files with '{keyword}' in the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|| 4/4 [17:42<00:00, 265.67s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/jgryu/miniconda3/envs/Wcomp/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|| 4/4 [00:12<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AwqConfig, AutoModelForCausalLM\n",
    "\n",
    "# model_id = \"meta-llama/Llama-Guard-3-8B\"\n",
    "\n",
    "# quantization_config = AwqConfig(\n",
    "#     bits=4,\n",
    "#     fuse_max_seq_len=512,\n",
    "#     do_fuse=True,\n",
    "# )\n",
    "\n",
    "model_id = 'Efficient-ML/LLaMA-3-8B-AWQ-4bit-b128'\n",
    "# model = AutoModelForCausalLM.from_pretrained('Efficient-ML/LLaMA-3-8B-AWQ-4bit-b128', trust_remote)\n",
    "model_awq = AutoModelForCausalLM.from_pretrained(model_id, token=\"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\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = model.state_dict()\n",
    "for k, v in sd.items():\n",
    "    print(k)\n",
    "    print(v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 4/4 [00:09<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_ckpt_path(path, branch = 'main'):\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "model_list = [\n",
    "              '/home/jgryu/Weight_compression/Wparam_dataset/model_zoo/huggingface/models--meta-llama--Meta-Llama-3-8B',\n",
    "              ]\n",
    "for model_path in model_list:\n",
    "    ckpt_path = get_ckpt_path(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_weights(state_dict, bits=4):\n",
    "    \"\"\"\n",
    "    Perform simple weight quantization on a model's state_dict.\n",
    "    \n",
    "    Args:\n",
    "        state_dict (dict): The state_dict of the model.\n",
    "        bits (int): Number of bits for quantization (default: 8).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Quantized state_dict.\n",
    "    \"\"\"\n",
    "    quantized_state_dict = {}\n",
    "    scale = 2 ** (bits - 1) - 1  # Scale factor for quantization\n",
    "    \n",
    "    for name, param in state_dict.items():\n",
    "        if param.dtype in [torch.float32, torch.float64]:  # Quantize only float weights\n",
    "            max_val = param.abs().max()\n",
    "            scale_factor = scale / max_val\n",
    "            quantized = (param * scale_factor).round().clamp(-scale, scale)  # Quantization\n",
    "            quantized_state_dict[name] = quantized / scale_factor  # Dequantization for storage\n",
    "        else:\n",
    "            quantized_state_dict[name] = param  # Keep non-float parameters as is\n",
    "    \n",
    "    return quantized_state_dict\n",
    "\n",
    "model_q = quantize_weights(fp_model.state_dict())\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Assume state_dict is available\n",
    "#     model_state_dict = {\n",
    "#         \"linear.weight\": torch.randn(4, 4),\n",
    "#         \"linear.bias\": torch.randn(4),\n",
    "#     }\n",
    "    \n",
    "#     quantized_state_dict = quantize_weights(model_state_dict)\n",
    "#     for k, v in quantized_state_dict.items():\n",
    "#         print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def quantize_weights_with_zero_point(state_dict, bits=4):\n",
    "    \"\"\"\n",
    "    Perform asymmetric weight quantization on a model's state_dict.\n",
    "\n",
    "    Args:\n",
    "        state_dict (dict): The state_dict of the model.\n",
    "        bits (int): Number of bits for quantization (default: 4).\n",
    "\n",
    "    Returns:\n",
    "        dict: Quantized state_dict with scale and zero-point.\n",
    "    \"\"\"\n",
    "    quantized_state_dict = {}\n",
    "    scale_and_zero_point = {}  # To store scale and zero-point for each parameter\n",
    "\n",
    "    qmin = 0\n",
    "    qmax = 2 ** bits - 1  # Range for quantized values\n",
    "\n",
    "    for name, param in state_dict.items():\n",
    "        if param.dtype in [torch.float32, torch.float64]:  # Quantize only float weights\n",
    "            min_val = param.min()\n",
    "            max_val = param.max()\n",
    "\n",
    "            # Calculate scale and zero-point\n",
    "            scale = (max_val - min_val) / (qmax - qmin)\n",
    "            zero_point = torch.round(qmin - min_val / scale)\n",
    "\n",
    "            # Quantize the parameter\n",
    "            quantized = torch.round(param / scale + zero_point).clamp(qmin, qmax)\n",
    "\n",
    "            # Dequantization for storage\n",
    "            dequantized = scale * (quantized - zero_point)\n",
    "            quantized_state_dict[name] = dequantized\n",
    "\n",
    "            # Save scale and zero-point for reference\n",
    "            scale_and_zero_point[name] = {\n",
    "                \"scale\": scale,\n",
    "                \"zero_point\": zero_point\n",
    "            }\n",
    "        else:\n",
    "            quantized_state_dict[name] = param  # Keep non-float parameters as is\n",
    "\n",
    "    return quantized_state_dict, scale_and_zero_point\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example state_dict with random weights\n",
    "#     model_state_dict = {\n",
    "#         \"linear.weight\": torch.randn(4, 4),\n",
    "#         \"linear.bias\": torch.randn(4),\n",
    "#     }\n",
    "\n",
    "#     quantized_state_dict, scale_and_zero_point = quantize_weights_with_zero_point(model_state_dict)\n",
    "\n",
    "#     # Print results\n",
    "#     for name, quantized_param in quantized_state_dict.items():\n",
    "#         print(f\"{name}: {quantized_param}\")\n",
    "\n",
    "#     print(\"\\nScale and Zero-Point:\")\n",
    "#     for name, values in scale_and_zero_point.items():\n",
    "#         print(f\"{name}: Scale={values['scale']}, Zero-Point={values['zero_point']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.6436466818640247\n",
      "2 1.6236762742152016\n",
      "3 1.2921279783070763\n",
      "4 0.6426828348204263\n",
      "5 0.22821856959557643\n",
      "6 0.0611583548993061\n",
      "7 0.01520839248038775\n",
      "8 0.003791649803076995\n",
      "mse:  [1.6436466818640247, 1.6236762742152016, 1.2921279783070763, 0.6426828348204263, 0.22821856959557643, 0.0611583548993061, 0.01520839248038775, 0.003791649803076995]\n",
      "bits:  [1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#   state_dict \n",
    "# model1_state_dict = model.state_dict()\n",
    "model1_state_dict = model.state_dict()\n",
    "\n",
    "bits_list = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "mse_list = []\n",
    "for bits in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "    quantized_state_dict, scale_and_zero_point = quantize_weights_with_zero_point(model.state_dict(), bits = bits)\n",
    "    model2_state_dict = quantized_state_dict\n",
    "\n",
    "\n",
    "    total_squared_error = 0.0\n",
    "    total_elements = 0\n",
    "\n",
    "    for key in model1_state_dict.keys():\n",
    "        if 'attn' not in key: continue\n",
    "        # print(key)\n",
    "        if key not in model2_state_dict:\n",
    "            print(f\"Key '{key}' is missing in model2.\")\n",
    "        else:\n",
    "            # Shape \n",
    "            if model1_state_dict[key].shape != model2_state_dict[key].shape:\n",
    "                print(f\"Shape mismatch for key '{key}': model1={model1_state_dict[key].shape}, model2={model2_state_dict[key].shape}\")\n",
    "            else:\n",
    "                #  MSE    \n",
    "                diff = model1_state_dict[key] - model2_state_dict[key]\n",
    "                squared_error = torch.sum(diff ** 2).item()  #     \n",
    "                total_squared_error += squared_error\n",
    "                total_elements += diff.numel()  #    \n",
    "\n",
    "    #   MSE   \n",
    "    if total_elements > 0:\n",
    "        overall_mse = total_squared_error / total_elements\n",
    "        # print(f\"Total MSE over all elements: {overall_mse:.6f}\")\n",
    "    else:\n",
    "        pass\n",
    "        # print(\"No elements to compare. Models may be incompatible.\")\n",
    "\n",
    "    for d in [16]:\n",
    "        std = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_train_std.npy')\n",
    "        # print(std)\n",
    "    overall_mse = overall_mse / std**2\n",
    "    mse_list.append(overall_mse)\n",
    "    print(bits, overall_mse)\n",
    "print('mse: ', mse_list)\n",
    "print('bits: ', bits_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### per tensor quantization MLP\n",
    "* asymetric (4, 0.7952778297410023)\n",
    "mse:  [1.0000164869236037, 0.9993737145706393, 0.9887189679460173, 0.7952778297410023, 0.37116162976546385, 0.10550744880290548, 0.02616760658136518, 0.006492661767538381]\n",
    "bits:  [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "### per tensor quantization Attn\n",
    "* asymetric (4, 0.6426828348204263) (3, 1.2921279783070763)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  !\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "path = '/home/jgryu/Weight_compression/awq_cache/llama3-8b-w4-g128.pt'\n",
    "try:\n",
    "    model_weights = torch.load(path, map_location='cpu')  # CPU  ( GPU )\n",
    "    print(\"  !\")\n",
    "    print(type(model_weights))  #    (: dict, Tensor )\n",
    "except Exception as e:\n",
    "    print(f\"  : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_weights, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# state_dict \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(value,\u001b[38;5;250m \u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "if isinstance(model_weights, dict):\n",
    "    # state_dict \n",
    "    for key, value in model_weights['clip'].items():\n",
    "        print(f\"{key}: {value.shape if isinstance(value, torch.Tensor) else type(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1338],\n",
       "         [0.1393],\n",
       "         [0.1440],\n",
       "         ...,\n",
       "         [0.2927],\n",
       "         [0.2462],\n",
       "         [0.2717]],\n",
       "\n",
       "        [[0.2030],\n",
       "         [0.1896],\n",
       "         [0.1676],\n",
       "         ...,\n",
       "         [0.2498],\n",
       "         [0.1958],\n",
       "         [0.2107]],\n",
       "\n",
       "        [[0.1211],\n",
       "         [0.1073],\n",
       "         [0.1519],\n",
       "         ...,\n",
       "         [0.1411],\n",
       "         [0.1211],\n",
       "         [0.1555]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.2306],\n",
       "         [0.2010],\n",
       "         [0.1537],\n",
       "         ...,\n",
       "         [0.2822],\n",
       "         [0.1294],\n",
       "         [0.1471]],\n",
       "\n",
       "        [[0.2267],\n",
       "         [0.1895],\n",
       "         [0.1153],\n",
       "         ...,\n",
       "         [0.2335],\n",
       "         [0.2355],\n",
       "         [0.2172]],\n",
       "\n",
       "        [[0.1915],\n",
       "         [0.1089],\n",
       "         [0.1740],\n",
       "         ...,\n",
       "         [0.1744],\n",
       "         [0.1779],\n",
       "         [0.2307]]], dtype=torch.float16)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights['clip'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/Wcomp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|| 4/4 [00:06<00:00,  1.53s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jgryu/Weight_compression/llm-awq/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m net \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(ckpt_path, local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m meam \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_mean.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_std.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# cache_directory = \"/home/jgryu/Weight_compression/llm-awq/model_cache\" \n",
    "# ver = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# net = AutoModelForCausalLM.from_pretrained(ver, cache_dir = cache_directory, token=\"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\", trust_remote_code=True)\n",
    "# tok = AutoTokenizer.from_pretrained(ver, cache_dir = cache_directory, token=\"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\", trust_remote_code=True)\n",
    "\n",
    "ckpt_path = '/home/jgryu/Weight_compression/llm-awq/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "meam = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_mean.npy')\n",
    "std = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_std.npy')\n",
    "size = 256\n",
    "weight_condition = 'mlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers              4.44.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 16 ##\n",
      "0.013787343 0.013866982 0.013697659 4.6187477e-05\n",
      "## 32 ##\n",
      "0.014501593 0.014622698 0.014366101 6.392919e-05\n",
      "## 64 ##\n",
      "0.014888803 0.015070886 0.014723518 8.179265e-05\n",
      "## 128 ##\n",
      "0.015046192 0.015470855 0.014769139 0.00013238109\n",
      "## 256 ##\n",
      "0.015112722 0.015859565 0.014786505 0.00019158793\n",
      "## 1024 ##\n",
      "0.015144215 0.01755336 0.014522944 0.00037536578\n",
      "## 4096 ##\n",
      "0.015128609 0.021753341 0.013272276 0.0007092323\n"
     ]
    }
   ],
   "source": [
    "for d in [16, 32, 64, 128, 256, 1024, 4096]:\n",
    "    path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/attn/d{d}/attn_d{d}_train_std_vector.npy'\n",
    "    std = np.load(path)\n",
    "    print(f'## {d} ##')\n",
    "    print(std.mean(), std.max(), std.min(), std.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 64 ##\n",
      "0.010743368 0.010794666 0.010696257 1.7599377e-05\n",
      "## 128 ##\n",
      "0.011308035 0.011375209 0.0112355985 2.1920741e-05\n",
      "## 256 ##\n",
      "0.011604545 0.011706344 0.011459176 3.3088007e-05\n",
      "## 1024 ##\n",
      "0.011814561 0.012050536 0.011308623 7.070596e-05\n",
      "## 4096 ##\n",
      "0.011811152 0.012583601 0.0095969755 0.00014643215\n"
     ]
    }
   ],
   "source": [
    "for d in [16, 32, 64, 128, 256, 1024, 4096]:\n",
    "    try:\n",
    "        try:\n",
    "            path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_train_std_channel.npy'\n",
    "            std = np.load(path)\n",
    "        except:\n",
    "            try:\n",
    "                path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_train_std_vector.npy'\n",
    "                std = np.load(path)\n",
    "            except:\n",
    "                try:\n",
    "                    path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_val_std_channel.npy'\n",
    "                    std = np.load(path)\n",
    "                except:\n",
    "                    path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_val_std_vector.npy'\n",
    "                    std = np.load(path) \n",
    "        print(f'## {d} ##')\n",
    "    except:\n",
    "        continue\n",
    "    print(std.mean(), std.max(), std.min(), std.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|| 4/4 [00:04<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([1024, 4096])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([14336, 4096])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([4096, 14336])\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096])\n",
      "model.norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([128256, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "ckpt_path = '/home/jgryu/Weight_compression/llm-awq/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "mean = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_mean.npy')\n",
    "std = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_std.npy')\n",
    "mean = torch.from_numpy(mean)\n",
    "std = torch.from_numpy(std)\n",
    "\n",
    "state_dict = net.state_dict()\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.mlp.gate_proj.weight torch.Size([14336, 4096])\n",
      "[ 0.00902044 -0.00585394  0.00988577  0.01487079  0.00742491 -0.01090256\n",
      "  0.01327876 -0.00535423  0.01126308 -0.01660915 -0.01462678  0.00425439\n",
      " -0.01227447 -0.00779258  0.016295    0.01026298 -0.00222772  0.00686085\n",
      "  0.00115176 -0.00429258 -0.00010531  0.01710752  0.00663035  0.02561843\n",
      " -0.01038772  0.012054    0.01254301  0.01325585 -0.01065041  0.00811421\n",
      "  0.01174714 -0.00253346 -0.00501138 -0.01422127  0.01290093  0.00784237\n",
      " -0.00546274 -0.01318091 -0.01777732  0.01776122  0.00296346 -0.00889001\n",
      "  0.00225166 -0.02175815 -0.00558524  0.0100997  -0.00324057 -0.00486698\n",
      " -0.01052314 -0.00034835 -0.00767504  0.00726538  0.0144815   0.01168484\n",
      "  0.00085301  0.0154621   0.01432316 -0.00309921 -0.01591488  0.0020155\n",
      " -0.01713781  0.02448303  0.00473965  0.00167099]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.fft import dct, idct\n",
    "\n",
    "# #  \n",
    "# data = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# # DCT  (-II  )\n",
    "# dct_result = dct(data, type=2, norm='ortho')\n",
    "\n",
    "# # IDCT  ()\n",
    "# idct_result = idct(dct_result, type=2, norm='ortho')\n",
    "\n",
    "# print(\"Original Data:\", data)\n",
    "# print(\"DCT Result:\", dct_result)\n",
    "# print(\"Reconstructed Data (IDCT):\", idct_result)\n",
    "\n",
    "\n",
    "# for k, v in state_dict.items():\n",
    "#     print(k, v.shape)\n",
    "#     v =(v - mean) / std\n",
    "#     data = v.reshape(-1, 64)[45].numpy()\n",
    "#     dct_result = dct(data, type=2, norm='ortho')\n",
    "#     idct_result = idct(dct_result, type=2, norm='ortho')\n",
    "#     print(\"Original Data:\", data)\n",
    "#     print(\"DCT Result:\", dct_result)\n",
    "#     print(\"Reconstructed Data (IDCT):\", idct_result)\n",
    "#     break\n",
    "\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    if 'layers.0' in k and 'mlp' in k:\n",
    "        break\n",
    "print(k, v.shape)\n",
    "v =(v - mean) / std\n",
    "data = v.reshape(-1, 64).numpy()\n",
    "dct_all = []\n",
    "for i in data[:10000]:\n",
    "    # print(i)\n",
    "    dct_result = dct(i, type=2, norm='ortho')\n",
    "    dct_all.append(dct_result)\n",
    "    \n",
    "dct_all = np.vstack(dct_all).mean(0)\n",
    "print(dct_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
