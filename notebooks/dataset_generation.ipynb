{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575d12c-2319-4417-aeba-f1a33d1ce740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a367b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeed46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "path = \"/home/jgryu/Weight_compression/Wparam_dataset/path_json\"\n",
    "list = os.listdir(path)\n",
    "\n",
    "for l in list :\n",
    "    file_path = os.path.join(path, l)\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].replace(\"/home/jgryu/Weight_compression\", \"\")\n",
    "        \n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f7410c",
   "metadata": {},
   "source": [
    "# V4\n",
    "weight tensor splice를 각각 npy 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, os\n",
    "# import numpy as np\n",
    "# import tqdm\n",
    "# dataset_folder = \"/home/jgryu/Weight_compression/Wparam_dataset\"\n",
    "# param_type = 'embed'\n",
    "# split = ['train', 'val']\n",
    "\n",
    "# for s in split:\n",
    "#     print(f'Start {s}')\n",
    "    \n",
    "#     path = os.path.join(dataset_folder, 'path_json', f'{param_type}_tensor_path_{s}.json')\n",
    "#     with open(path, 'r') as f:\n",
    "#         tensor_path_list = json.load(f)\n",
    "    \n",
    "#     count = 0\n",
    "#     for tensor_path in tqdm.tqdm(tensor_path_list):\n",
    "        \n",
    "     \n",
    "#         directory = dataset_folder + tensor_path\n",
    "\n",
    "#         # 디렉토리 삭제\n",
    "#         if os.path.exists(directory):\n",
    "#             os.remove(directory)\n",
    "#             print(\"디렉토리가 삭제되었습니다.\")\n",
    "#         else:\n",
    "#             print(\"디렉토리가 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38541e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3212/3212 [1:12:05<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 326/804 [13:57<20:27,  2.57s/it]  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device: '/home/jgryu/Weight_compression/Wparam_dataset/tensor_slices/val/meta-llama/CodeLlama-7b-Python-hf/model-layers-30-self_attn-q_proj-weight/d=128/d=128_46253.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(l):\n\u001b[1;32m     55\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43md=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdim\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/RD/lib/python3.8/site-packages/numpy/lib/npyio.py:524\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    523\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 524\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    527\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device: '/home/jgryu/Weight_compression/Wparam_dataset/tensor_slices/val/meta-llama/CodeLlama-7b-Python-hf/model-layers-30-self_attn-q_proj-weight/d=128/d=128_46253.npy'"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "dataset_folder = \"/home/jgryu/Weight_compression/Wparam_dataset\"\n",
    "param_type = 'mlp'\n",
    "split = ['train', 'val']\n",
    "dim = 128\n",
    "param_type = 'attn'\n",
    "\n",
    "def check_contains_any_substrings(string, substrings):\n",
    "    return any(substring in string for substring in substrings)\n",
    "\n",
    "def contains_all_substrings(string, substrings):\n",
    "    return all(substring in string for substring in substrings)\n",
    "\n",
    "# model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b', '13b', 'mini', 'small']\n",
    "model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b','mini']\n",
    "# model_filter2 = ['llama', '7b']\n",
    "\n",
    "for s in split:\n",
    "    print(f'Start {s}')\n",
    "    \n",
    "    path = os.path.join(dataset_folder, 'path_json', f'{param_type}_tensor_path_{s}.json')\n",
    "    with open(path, 'r') as f:\n",
    "        tensor_path_list = json.load(f)\n",
    "        \n",
    "    save_path = os.path.join(dataset_folder, 'tensor_slices', s)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    filtered_tensor_paths = [\n",
    "        tensor_path for tensor_path in tqdm.tqdm(tensor_path_list)\n",
    "        if check_contains_any_substrings(tensor_path.lower(), model_filter)\n",
    "        and contains_all_substrings(tensor_path.lower(), model_filter2)\n",
    "    ]\n",
    "    \n",
    "    for tensor_path in tqdm.tqdm(filtered_tensor_paths):\n",
    "    # for tensor_path in tqdm.tqdm(tensor_path_list):\n",
    "    #     if not check_contains_any_substrings(tensor_path.lower(), model_filter):\n",
    "    #         continue\n",
    "    #     if not contains_all_substrings(tensor_path.lower(), model_filter2):\n",
    "    #         continue\n",
    "        \n",
    "        path = os.path.join(save_path,  tensor_path.replace(\"/model_param_tensor/\" ,\"\").replace(\".npy\", \"\"), f'd={dim}')\n",
    "        if os.path.isdir(path):\n",
    "            # print('continue')\n",
    "            continue\n",
    "        try:\n",
    "            t = np.load(dataset_folder + tensor_path)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(f\"Fail load {path}\")\n",
    "            continue\n",
    "\n",
    "        if t.size % (dim) != 0:\n",
    "            continue \n",
    "            # print(f'나누어 떨어지지 않습니다.')\n",
    "        t = t.reshape(-1, dim)\n",
    "        l = t.shape[0]\n",
    "\n",
    "        for i in range(l):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            np.save(os.path.join(path, f'd={dim}_{i}.npy'), t[i])\n",
    "        count += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9593240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# from pathlib import Path\n",
    "\n",
    "# def reorganize_npy_files(base_path: str):\n",
    "#     \"\"\"\n",
    "#     Wparam_dataset/tensor_slices/train/ 경로 아래의 모든 d=1024_{n}.npy 파일들을\n",
    "#     해당하는 d=1024 디렉토리로 이동시키는 함수\n",
    "    \n",
    "#     Args:\n",
    "#         base_path (str): 기본 경로 (Wparam_dataset/tensor_slices/train/)\n",
    "#     \"\"\"\n",
    "#     # 기본 경로를 Path 객체로 변환\n",
    "#     base = Path(base_path)\n",
    "    \n",
    "#     # 모든 .npy 파일 찾기\n",
    "#     for npy_file in base.rglob(\"*.npy\"):\n",
    "#         # 파일명이 'd=1024_'로 시작하는지 확인\n",
    "#         if npy_file.name.startswith(\"d=1024_\"):\n",
    "#             # 현재 파일의 디렉토리\n",
    "#             current_dir = npy_file.parent\n",
    "            \n",
    "#             # 새로운 디렉토리 경로 생성 (d=1024 서브디렉토리)\n",
    "#             new_dir = current_dir / \"d=1024\"\n",
    "            \n",
    "#             # 새 디렉토리가 없으면 생성\n",
    "#             if not new_dir.exists():\n",
    "#                 new_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "#             # 새로운 파일 경로\n",
    "#             new_file_path = new_dir / npy_file.name\n",
    "            \n",
    "#             try:\n",
    "#                 # 파일 이동\n",
    "#                 shutil.move(str(npy_file), str(new_file_path))\n",
    "#                 print(f\"Moved: {npy_file} -> {new_file_path}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error moving {npy_file}: {e}\")\n",
    "\n",
    "\n",
    "# base_path = \"Wparam_dataset/tensor_slices/train/\"\n",
    "\n",
    "# # 경로가 존재하는지 확인\n",
    "# if not os.path.exists(base_path):\n",
    "#     print(f\"Error: Path {base_path} does not exist\")\n",
    "#     return\n",
    "\n",
    "# # 파일 재구성 실행\n",
    "# reorganize_npy_files(base_path)\n",
    "# print(\"File reorganization completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664cfaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import json\n",
    "def contains_all_substrings(string, substrings):\n",
    "    return all(substring in string for substring in substrings)\n",
    "\n",
    "save_path = '/home/jgryu/Weight_compression/model_param_dataset/'\n",
    "dataset_path = '/home/jgryu/Weight_compression/model_param_tensor/'\n",
    "\n",
    "tenosr_path_list = glob.glob(f'{dataset_path}/**/*.npy', recursive=True)\n",
    "\n",
    "print(f'all : {len(tenosr_path_list)}')\n",
    "\n",
    "data = {'attn':[], 'mlp':[], 'embed':[], 'layernorm':[]}   \n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "for weight_type in ['attn', 'mlp', 'embed', 'layernorm']: \n",
    "    for path in tenosr_path_list:\n",
    "        if contains_all_substrings(path.lower(), [weight_type]):\n",
    "            data[weight_type].append(path)\n",
    "    print(f'{weight_type} : {len(data[weight_type])}')\n",
    "\n",
    "\n",
    "    random.shuffle(data[weight_type])\n",
    "\n",
    "    split_index = int(0.8 * len(data[weight_type]))\n",
    "    train_data = data[weight_type][:split_index]\n",
    "    validation_data = data[weight_type][split_index:]\n",
    "\n",
    "    file_path_train = os.path.join(save_path, f'{weight_type}_tensor_path_train.json')\n",
    "    file_path_validation = file_path_train.replace('train', 'val')\n",
    "\n",
    "    with open(file_path_train, 'w') as json_file:\n",
    "        json.dump(train_data, json_file, indent=4)\n",
    "    with open(file_path_validation, 'w') as json_file:\n",
    "        json.dump(validation_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4adaa9",
   "metadata": {},
   "source": [
    "# V3\n",
    "각 tensor의 path를 train val split해서 json 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import json\n",
    "def contains_all_substrings(string, substrings):\n",
    "    return all(substring in string for substring in substrings)\n",
    "\n",
    "save_path = '/home/jgryu/Weight_compression/model_param_dataset/'\n",
    "dataset_path = '/home/jgryu/Weight_compression/model_param_tensor/'\n",
    "\n",
    "tenosr_path_list = glob.glob(f'{dataset_path}/**/*.npy', recursive=True)\n",
    "\n",
    "print(f'all : {len(tenosr_path_list)}')\n",
    "\n",
    "data = {'attn':[], 'mlp':[], 'embed':[], 'layernorm':[]}   \n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "for weight_type in ['attn', 'mlp', 'embed', 'layernorm']: \n",
    "    for path in tenosr_path_list:\n",
    "        if contains_all_substrings(path.lower(), [weight_type]):\n",
    "            data[weight_type].append(path)\n",
    "    print(f'{weight_type} : {len(data[weight_type])}')\n",
    "\n",
    "\n",
    "    random.shuffle(data[weight_type])\n",
    "\n",
    "    split_index = int(0.8 * len(data[weight_type]))\n",
    "    train_data = data[weight_type][:split_index]\n",
    "    validation_data = data[weight_type][split_index:]\n",
    "\n",
    "    file_path_train = os.path.join(save_path, f'{weight_type}_tensor_path_train.json')\n",
    "    file_path_validation = file_path_train.replace('train', 'val')\n",
    "\n",
    "    with open(file_path_train, 'w') as json_file:\n",
    "        json.dump(train_data, json_file, indent=4)\n",
    "    with open(file_path_validation, 'w') as json_file:\n",
    "        json.dump(validation_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e99bd3",
   "metadata": {},
   "source": [
    "# V2\n",
    "state_dict tensor별로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "814e8449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4\n",
      "models--meta-llama--Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--meta-llama--Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--meta-llama--Meta-Llama-3.1-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "# import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "def get_ckpt_path(path, branch = 'main'):\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "def check_contains_any_substrings(string, substrings):\n",
    "    return any(substring in string for substring in substrings)\n",
    "\n",
    "model_zoo_path = '/home/jgryu/Weight_compression/Wparam_dataset/model_zoo/huggingface'\n",
    "# model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b', '13b', 'mini', 'small']\n",
    "# model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b','mini']\n",
    "# model_filter = ['-13b', '-small']\n",
    "model_filter = ['meta-llama-3-8b', 'meta-llama-3.1-8b']\n",
    "\n",
    "model_list = os.listdir(model_zoo_path)\n",
    "print()\n",
    "ckpt_path_list = []\n",
    "for ck in model_list:\n",
    "    if check_contains_any_substrings(ck.lower(), model_filter):\n",
    "        ckpt_path = get_ckpt_path(os.path.join(model_zoo_path, ck))\n",
    "        if ckpt_path is not None:\n",
    "            ckpt_path_list.append(ckpt_path)\n",
    "\n",
    "print(len(ckpt_path_list))\n",
    "\n",
    "save_dir = '/home/jgryu/Weight_compression/Wparam_dataset/model_param_tensor'\n",
    "\n",
    "for ckpt_path in ckpt_path_list:\n",
    "    model_name = ckpt_path.split('/')[-3]\n",
    "    print(model_name)\n",
    "    model_name = model_name.split('--')\n",
    "    \n",
    "    save_path = os.path.join(save_dir, model_name[1], model_name[2])\n",
    "\n",
    "    if os.path.isdir(save_path) and bool(os.listdir(save_path)):\n",
    "        print('### Skip ###')\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True, trust_remote_code=True)\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "        for k, v in state_dict.items():\n",
    "            np.save(os.path.join(save_path, k.replace(\".\", \"-\")), v)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Fail load model from {ckpt_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf83332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_path(path, branch = 'main'):\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "def check_contains_any_substrings(string, substrings):\n",
    "    return any(substring in string for substring in substrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3adfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zoo_path = '/home/jgryu/Weight_compression/model_zoo/huggingface'\n",
    "# model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b', '13b', 'mini', 'small']\n",
    "model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b','mini']\n",
    "model_list = os.listdir(model_zoo_path)\n",
    "ckpt_path_list = []\n",
    "for ck in model_list:\n",
    "    if check_contains_any_substrings(ck.lower(), model_filter):\n",
    "        ckpt_path = get_ckpt_path(os.path.join(model_zoo_path, ck))\n",
    "        if ckpt_path is not None:\n",
    "            ckpt_path_list.append(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79728f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/jgryu/Weight_compression/model_param_tensor'\n",
    "\n",
    "for ckpt_path in ckpt_path_list:\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Fail load model from {ckpt_path}\")\n",
    "        continue\n",
    "    \n",
    "    model_name = ckpt_path.split('/')[-3]\n",
    "    print(model_name)\n",
    "    model_name = model_name.split('--')\n",
    "    \n",
    "    save_path = os.path.join(save_dir, model_name[1], model_name[2])\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    if os.path.isdir(save_path) and not os.listdir(save_path):\n",
    "        continue\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    for k, v in state_dict.items():\n",
    "        np.save(os.path.join(save_path, k.replace(\".\", \"-\")), v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e5f761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8662cdd",
   "metadata": {},
   "source": [
    "# Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e265808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model_filter = ['llama-2-7b-hf', 'attn']\n",
    "dim = [128, 256, 512]\n",
    "for d in dim:\n",
    "    print(\"##### Start generating WP dataset #####\")\n",
    "    path_save = '/home/jgryu/Weight_compression/Wparam_dataset/Wparam_npy/'\n",
    "    dataset_path = '/home/jgryu/Weight_compression/Wparam_dataset/model_param_tensor/meta-llama'\n",
    "\n",
    "    import glob\n",
    "    def contains_all_substrings(string, substrings):\n",
    "        return all(substring in string for substring in substrings)\n",
    "\n",
    "    np_list = []\n",
    "    tenosr_path_list = glob.glob(f'{dataset_path}/**/*.npy', recursive=True)\n",
    "    for path in tenosr_path_list:\n",
    "        if contains_all_substrings(path.lower(), model_filter):\n",
    "            np_tensor = np.load(path).astype('float32')\n",
    "            if np_tensor.size % d != 0:\n",
    "                continue\n",
    "            np_list.append(np_tensor.reshape(-1,d))\n",
    "            \n",
    "    np_list = np.vstack(np_list)\n",
    "    indices = np.random.permutation(np_list.shape[0])\n",
    "    np_list = np_list[indices]\n",
    "    split_index = int(0.8 * np_list.shape[0])\n",
    "    train_data = np_list[:split_index]\n",
    "    validation_data = np_list[split_index:]\n",
    "\n",
    "    np.save(path_save + '_'.join(model_filter) + f'_d={d}_train', train_data)\n",
    "    np.save(path_save + '_'.join(model_filter) + f'_d={d}_val', validation_data)\n",
    "    \n",
    "    print(\"##### End generating WP dataset #####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "import torch\n",
    "import torchvision\n",
    "# import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "import argparse\n",
    "\n",
    "def ckpt_path(path, branch = 'main'):\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "def contains_all_substrings(string, substrings):\n",
    "    return all(substring in string for substring in substrings)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model_filter', nargs='+', type=str, default=['gemma', '2b'], \n",
    "                        help=\"Model filter list\")\n",
    "    parser.add_argument('--d', type=int, default=128, \n",
    "                        help=\"Dimension size\")\n",
    "    parser.add_argument('--path_model_zoo', type=str, default='/home/jgryu/Weight_compression/model_zoo/huggingface',\n",
    "                        help=\"Path to the model zoo\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "ckpt_list = os.listdir(args.path_model_zoo)\n",
    "path_list_ckpts = []\n",
    "for ck in ckpt_list :\n",
    "    if contains_all_substrings(ck.lower(), args.model_filter):\n",
    "        path_ckpt = ckpt_path(os.path.join(args.path_model_zoo, ck))\n",
    "        if path_ckpt is not None:\n",
    "            path_list_ckpts.append(path_ckpt)\n",
    "\n",
    "for p in path_list_ckpts:\n",
    "    print(p.split('/')[-3])\n",
    "    \n",
    "data = {'attn':[], 'mlp':[], 'embed':[], 'layernorm':[]}   \n",
    "\n",
    "for path_model in path_list_ckpts:\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(path_model, local_files_only=True)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Fail load model from {path_model}\")\n",
    "        continue\n",
    "    \n",
    "    state_dict = model.state_dict()\n",
    "    for k, v in state_dict.items():\n",
    "        # print(k)\n",
    "        # print(v.shape)\n",
    "        if v.numel() % args.d != 0:\n",
    "            print(f'나누어 떨어지지 않습니다.')\n",
    "            continue\n",
    "        if 'attn' in k.lower():\n",
    "            v_split = v.view(-1, args.d).numpy()\n",
    "            data['attn'].append(v_split)\n",
    "        elif 'mlp' in k.lower():\n",
    "            v_split = v.view(-1, args.d).numpy()\n",
    "            data['mlp'].append(v_split)\n",
    "        elif 'embed' in k.lower():\n",
    "            v_split = v.view(-1, args.d).numpy()\n",
    "            data['embed'].append(v_split)\n",
    "        elif 'layernorm' in k.lower():\n",
    "            v_split = v.view(-1, args.d).numpy()\n",
    "            data['layernorm'].append(v_split)\n",
    "\n",
    "path_save = '/home/jgryu/Weight_compression/model_parm_dataset/'\n",
    "\n",
    "print('## Shape ##')\n",
    "for k, v in data.items():\n",
    "    data[k] = np.vstack(v)\n",
    "    print(k, ': ', data[k].shape)\n",
    "\n",
    "# attn = np.vstack(attn)\n",
    "# mlp = np.vstack(mlp)\n",
    "# embed = np.vstack(embed)\n",
    "# layernorm = np.vstack(layernorm)\n",
    "\n",
    "# print('attn :', attn.shape)\n",
    "# print('mlp :', mlp.shape)\n",
    "# print('embed :', embed.shape)\n",
    "# print('layernorm :', layernorm.shape)\n",
    "\n",
    "# 데이터를 랜덤하게 섞기 위한 인덱스 생성\n",
    "    indices = np.random.permutation(data[k].shape[0])\n",
    "    shuffled_data = data[k][indices]\n",
    "    # shuffled_data = data[k]\n",
    "    split_index = int(0.8 * shuffled_data.shape[0])\n",
    "    train_data = shuffled_data[:split_index]\n",
    "    validation_data = shuffled_data[split_index:]\n",
    "\n",
    "    np.save(path_save + '_'.join(args.model_filter) + '_' + k + f'_d={args.d}_train', train_data)\n",
    "    np.save(path_save + '_'.join(args.model_filter) + '_' + k + f'_d={args.d}_validation', validation_data)\n",
    "\n",
    "\n",
    "# path_save = '/home/jgryu/Weight_compression/model_parm_dataset/'\n",
    "# np.save(path_save + '_'.join(model_filter) + '_attn' + f'_d={d}', attn)\n",
    "# np.save(path_save + '_'.join(model_filter) + '_mlp' + f'_d={d}', mlp)\n",
    "# np.save(path_save + '_'.join(model_filter) + '_embed' + f'_d={d}', embed)\n",
    "# np.save(path_save + '_'.join(model_filter) + '_layernorm' + f'_d={d}', layernorm)\n",
    "# print(\"데이터가 '.npy' 파일로 저장되었습니다.\")\n",
    "\n",
    "\n",
    "# print(\"Train set shape:\", train_data.shape)\n",
    "# print(\"Validation set shape:\", validation_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96ecdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length131072=======\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "file_glob = '/home/jgryu/Weight_compression/Wparam_dataset/tensor_slices/train/meta-llama/CodeLlama-7b-hf/model-layers-0-self_attn-k_proj-weight'\n",
    "files = sorted(glob.glob(file_glob + '/**/*.npy', recursive=True))\n",
    "# print(files)\n",
    "\n",
    "print(f'length{len(files)}=======')\n",
    "if not files:\n",
    "    raise RuntimeError(f\"No images found with glob '{file_glob}'.\")\n",
    "\n",
    "l = []\n",
    "for f in files:\n",
    "    l.append(np.load(f))\n",
    "n = np.vstack(l)\n",
    "np.save('/home/jgryu/Weight_compression/Wparam_dataset/npy/test', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44a1884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131072, 128)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f4805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
