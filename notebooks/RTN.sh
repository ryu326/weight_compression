#!/bin/bash

# ==============================================================================
# Llama-2-7b-hf λ¨λΈμ— λ€ν•΄ λ‹¤μ–‘ν• λΉ„νΈ μλ΅ RTN μ–‘μν™”λ¥Ό μ‹¤ν–‰ν•λ” μ¤ν¬λ¦½νΈ
# ==============================================================================

# --- μ„¤μ • (Configuration) ---

export CUDA_VISIBLE_DEVICES=3

# λ¨λΈ λ° κ²½λ΅ μ„¤μ •
MODEL_TYPE="llama"
MODEL_NAME="meta-llama--Llama-2-7b-hf"
MODEL_PATH="/workspace/Weight_compression/Wparam_dataset/hf_model/${MODEL_NAME}"
BASE_OUTPUT_DIR="/workspace/Weight_compression/hf_model_comp/RTN"

# μ–‘μν™” νλΌλ―Έν„° μ„¤μ •
QUANT_TYPE="group"
GROUP_SIZE=128

# μ–‘μν™”λ¥Ό μ‹¤ν–‰ν•  λΉ„νΈ λ¦¬μ¤νΈ
BITS_TO_RUN=(3 4 5 6 7 8)


# μ¤ν¬λ¦½νΈ μ‹μ‘ λ©”μ‹μ§€
echo "π€ Starting RTN quantization process for ${MODEL_NAME}"
echo "=========================================================="

# μ§€μ •λ λΉ„νΈ λ¦¬μ¤νΈλ¥Ό μνν•λ©° μ–‘μν™” μ‹¤ν–‰
for bit in "${BITS_TO_RUN[@]}"; do
    
    # κ²°κ³Όλ¬Όμ„ μ €μ¥ν•  μµμΆ… κ²½λ΅ μƒμ„±
    OUTPUT_PATH="${BASE_OUTPUT_DIR}/${MODEL_NAME}_W${bit}g${GROUP_SIZE}"
    
    echo "π”„ Running quantization for ${bit}-bit..."
    echo "   - Model: ${MODEL_PATH}"
    echo "   - Output: ${OUTPUT_PATH}"
    
    # νμ΄μ¬ μ¤ν¬λ¦½νΈ μ‹¤ν–‰
    python ${PYTHON_SCRIPT} \
        --model_type ${MODEL_TYPE} \
        --model_path ${MODEL_PATH} \
        --output_path ${OUTPUT_PATH} \
        --num_bits ${bit} \
        --quant_type ${QUANT_TYPE} \
        --group_size ${GROUP_SIZE}
        
    # μ‹¤ν–‰ κ²°κ³Ό ν™•μΈ
    if [ $? -eq 0 ]; then
        echo "β… Successfully completed ${bit}-bit quantization."
    else
        echo "β Error during ${bit}-bit quantization. Aborting."
        exit 1
    fi
    
    echo "----------------------------------------------------------"

done

echo "π‰ All quantization tasks are complete."
echo "=========================================================="