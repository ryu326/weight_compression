{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "import sys, os, json, math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "std = 0.012528747320175171"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpps = [2, 3, 4, 5, 6, 7, 8, 9, 10, 12]\n",
    "# bpps = [4]\n",
    "mses = []\n",
    "mse_fn = nn.MSELoss()\n",
    "for bpp in bpps:\n",
    "    ckpt_path = f'../model_cache_reconstructed/awq/llama3-8b-my-w{bpp}-g128-fake-quantized'\n",
    "    recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "    n = 0\n",
    "    mse = 0\n",
    "    mse_layer = []\n",
    "    for k, v in state_dict.items():\n",
    "        if 'mlp' not in k and 'self_attn' not in k: continue\n",
    "        \n",
    "        mse += ((recon_state_dict[k] - v)**2).sum()\n",
    "        n += v.numel()\n",
    "        print(k, mse_fn(recon_state_dict[k], v)/std**2)\n",
    "        mse_layer.append(mse_fn(recon_state_dict[k], v)/std**2)\n",
    "    mse = mse / n / std **2 \n",
    "    mses.append(mse.item())\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mse_layer, label='awq')\n",
    "plt.plot(mse_, label='vqvae')\n",
    "# plt.plot(mse_2, label='vqvae_idx')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ\n",
    "bpp:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 12]\n",
    "\n",
    "mse:  [23.015626907348633, 21.956884384155273, 18.508888244628906, 189.52224731445312, 21.462759017944336, 18.203344345092773, 23.026723861694336, 12.113859176635742, 13.774173736572266, 57.454349517822266]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bpp: ', bpps)\n",
    "print('mse: ', mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/jgryu/Weight_compression/model_cache_reconstructed/vqvae_idx_v1/per_row_16_calib'\n",
    "root_dir = '../model_cache_reconstructed/vqvae_corrected_scale/mlp_attn_16_row_dataset.pt'\n",
    "root_dir = '../model_cache_reconstructed/vqvae_idx_v2/per_row_16_calib'\n",
    "root_dir = '/home/jgryu/Weight_compression/model_cache_reconstructed/vqvae_idx_v2_random/per_row_16_calib'\n",
    "root_dir = '/home/jgryu/Weight_compression/model_cache_reconstructed/vqvae_idx_v2_random_all/per_row_16_calib'\n",
    "import glob\n",
    "ckpt_paths = glob.glob(os.path.join(root_dir, \"**/*.pth.tar\"), recursive=True)\n",
    "ckpt_path_list = []\n",
    "\n",
    "for ck in ckpt_paths:\n",
    "    # if 'nmse' in ck: continue////// \n",
    "    ckpt_path_list.append(ck)\n",
    "\n",
    "ckpt_path_list = ['/home/jgryu/Weight_compression/model_cache_reconstructed/vqvae_calib__/per_row_16_calib/bpp4.0_size16_nmse_neNone_de16_K8_P8_encdim512_batch_size2048_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.01304_total_iter_1100000.pth.tar']\n",
    "ckpt_path_list = ['/home/jgryu/Weight_compression/model_cache_reconstructed/vqvae_idx_v2/per_row_16_calib/bpp4.0_size16_smse_neNone_de16_K8_P8_encdim512_batch_size2048_total_iter1500000_lr0.0001_seed100/best_mse_model_MSE_0.00994_total_iter_900000.pth.tar']\n",
    "mses = []\n",
    "mse_fn = nn.MSELoss()\n",
    "for ckpt_path in ckpt_path_list:\n",
    "    recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "    n = 0\n",
    "    mse = 0\n",
    "    mse_2 = []\n",
    "    for k, v in state_dict.items():\n",
    "        if 'mlp' not in k and 'attn' not in k: continue\n",
    "        # if 'mlp' not in k: continue\n",
    "        \n",
    "        \n",
    "        mse += ((recon_state_dict[k] - v)**2).sum()\n",
    "        n += v.numel()\n",
    "        mse_2.append(mse_fn(recon_state_dict[k], v)/std**2)\n",
    "    mse = mse / n / std **2 \n",
    "    mses.append(mse.item())\n",
    "    print(ckpt_path.split('/')[-2])\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()\n",
    "\n",
    "ckpt_path = f'../model_cache_reconstructed/seedlm/bpp4.0_C8_P3_K16'\n",
    "recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "n = 0\n",
    "mse = 0\n",
    "for k, v in state_dict.items():\n",
    "    if 'mlp' not in k and 'attn' not in k: continue\n",
    "    \n",
    "    mse += ((recon_state_dict[k] - v)**2).sum()\n",
    "    n += v.numel()\n",
    "    \n",
    "mse = mse / n / std **2 \n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b5_g128\n",
      "tensor(0.0025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7_g-1\n",
      "tensor(0.0004)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b5_g-1\n",
      "tensor(0.0060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7_g128\n",
      "tensor(0.0002)\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/home/jgryu/Weight_compression/model_reconstructed/rtn'\n",
    "import glob\n",
    "ckpt_paths = glob.glob(os.path.join(root_dir, \"**/*\"), recursive=True)\n",
    "ckpt_path_list = []\n",
    "\n",
    "for ck in ckpt_paths:\n",
    "    if 'result' in ck: continue\n",
    "    ckpt_path_list.append(ck)\n",
    "\n",
    "mses = []\n",
    "mse_fn = nn.MSELoss()\n",
    "for ckpt_path in ckpt_path_list:\n",
    "    try:\n",
    "        recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "        recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "        n = 0\n",
    "        mse = 0\n",
    "        mse_2 = []\n",
    "        for k, v in state_dict.items():\n",
    "            if 'mlp' not in k and 'attn' not in k: continue\n",
    "            # if 'mlp' not in k: continue\n",
    "            \n",
    "            \n",
    "            mse += ((recon_state_dict[k] - v)**2).sum()\n",
    "            n += v.numel()\n",
    "            mse_2.append(mse_fn(recon_state_dict[k], v)/std**2)\n",
    "        mse = mse / n / std **2 \n",
    "        mses.append(mse.item())\n",
    "        print(ckpt_path.split('/')[-1])\n",
    "        print(mse)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpps = [3, 4]\n",
    "mses = []\n",
    "mse_fn = nn.MSELoss()\n",
    "for bpp in bpps:\n",
    "    ckpt_path = f'../model_reconstructed/rtn/bpp{bpp}'\n",
    "    recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "    n = 0\n",
    "    mse = 0\n",
    "    mse_layer = []\n",
    "    for k, v in state_dict.items():\n",
    "        # if 'mlp' not in k and 'attn' not in k: continue\n",
    "        \n",
    "        mse += ((recon_state_dict[k] - v)**2).sum()\n",
    "        n += v.numel()\n",
    "        print(k, mse_fn(recon_state_dict[k], v)/std**2)\n",
    "        mse_layer.append(mse_fn(recon_state_dict[k], v)/std**2)\n",
    "    mse = mse / n / std **2 \n",
    "    mses.append(mse.item())\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
