{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, OPTForCausalLM, BloomForCausalLM\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- 유틸리티 함수 ---\n",
    "\n",
    "def get_magnitude_spectrum(matrix):\n",
    "    \"\"\"2D 행렬을 입력받아 주파수 스펙트럼을 반환하는 함수\"\"\"\n",
    "    f = np.fft.fft2(matrix)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    spectrum = 20 * np.log(np.abs(fshift) + 1)\n",
    "    return spectrum\n",
    "\n",
    "# --- 2. 압축 수행 ---\n",
    "\n",
    "# (A) 양자화 (INT4 예시)\n",
    "def quantize_dequantize(weights, bits=4):\n",
    "    \"\"\"가중치를 양자화 및 역양자화하여 복원\"\"\"\n",
    "    # 스케일과 제로포인트 계산\n",
    "    scale = (weights.max() - weights.min()) / (2**bits - 1)\n",
    "    zeropoint = np.round(-weights.min() / scale)\n",
    "    \n",
    "    # 양자화 및 역양자화\n",
    "    quantized = np.round(weights / scale + zeropoint)\n",
    "    dequantized = (quantized - zeropoint) * scale\n",
    "    return dequantized\n",
    "\n",
    "# (B) JPEG 압축\n",
    "def jpeg_compress_decompress(weights, quality=10):\n",
    "    \"\"\"가중치를 이미지처럼 취급하여 JPEG 압축 및 복원\"\"\"\n",
    "    # 0-255 범위로 정규화\n",
    "    min_val, max_val = weights.min(), weights.max()\n",
    "    normalized = 255 * (weights - min_val) / (max_val - min_val)\n",
    "    normalized = normalized.astype(np.uint8)\n",
    "    \n",
    "    # JPEG 압축/복원\n",
    "    _, encoded_img = cv2.imencode('.jpg', normalized, [cv2.IMWRITE_JPEG_QUALITY, quality])\n",
    "    decoded_img = cv2.imdecode(encoded_img, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # 원래 스케일로 역정규화\n",
    "    denormalized = decoded_img.astype(np.float32) / 255 * (max_val - min_val) + min_val\n",
    "    return denormalized\n",
    "\n",
    "def webp_compression_decompression(weights, quality=10):\n",
    "    \"\"\"가중치를 이미지처럼 취급하여 WEBP 압축 및 복원\"\"\"\n",
    "    # 0-255 범위로 정규화\n",
    "    min_val, max_val = weights.min(), weights.max()\n",
    "    normalized = 255 * (weights - min_val) / (max_val - min_val)\n",
    "    normalized = normalized.astype(np.uint8)\n",
    "    \n",
    "    # WEBP 압축/복원\n",
    "    _, encoded_img = cv2.imencode('.webp', normalized, [cv2.IMWRITE_WEBP_QUALITY, quality])\n",
    "    decoded_img = cv2.imdecode(encoded_img, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # 원래 스케일로 역정규화\n",
    "    denormalized = decoded_img.astype(np.float32) / 255 * (max_val - min_val) + min_val\n",
    "    return denormalized\n",
    "\n",
    "# def nic_compress_decompress(weights, comp_model, quality=10):\n",
    "#     # from ..comp_lm_qtip.nic_models.TCM.models import TCM\n",
    "    \n",
    "#     \"\"\"가중치를 이미지처럼 취급하여 NIC 압축 및 복원\"\"\"\n",
    "#     # 0-255 범위로 정규화\n",
    "#     min_val, max_val = weights.min(), weights.max()\n",
    "#     normalized = 2 * (weights - min_val) / (max_val - min_val)\n",
    "\n",
    "#     normalized = torch.tensor(normalized, dtype=torch.float32).to('cuda')\n",
    "#     p = normalized.unsqueeze(0).unsqueeze(0).float()  # [1,1,h_p,w_p]\n",
    "#     # p_pad, padding = pad(p, patch_size)\n",
    "#     p3 = p.repeat(1, 3, 1, 1)  # [1,3,patch_size,patch_size]\n",
    "\n",
    "#     # Compress and decompress\n",
    "#     out_enc = comp_model.compress(p3)\n",
    "#     out_dec = comp_model.decompress(out_enc[\"strings\"], out_enc[\"shape\"])\n",
    "    \n",
    "#     rec1 = out_dec[\"x_hat\"][:, 0:1, :, :]\n",
    "#     # rec_crop = crop(rec1, padding)  # [1,1,h_p,w_p]\n",
    "#     denormalized = rec1.squeeze(0).squeeze(0)\n",
    "    \n",
    "#     denormalized = denormalized / 2 * (max_val - min_val) + min_val\n",
    "    \n",
    "#     return denormalized.detach().cpu().numpy()\n",
    "\n",
    "def nic_compress_decompress(weights, comp_model, quality=10):\n",
    "    # from ..comp_lm_qtip.nic_models.TCM.models import TCM\n",
    "    \n",
    "    \"\"\"가중치를 이미지처럼 취급하여 NIC 압축 및 복원\"\"\"\n",
    "    # 0-255 범위로 정규화\n",
    "    m, s = weights.mean(), weights.std()\n",
    "    normalized = (weights - m) / s\n",
    "\n",
    "    normalized = torch.tensor(normalized, dtype=torch.float32).to('cuda')\n",
    "    p = normalized.unsqueeze(0).unsqueeze(0).float()  # [1,1,h_p,w_p]\n",
    "    # p_pad, padding = pad(p, patch_size)\n",
    "    p3 = p.repeat(1, 3, 1, 1)  # [1,3,patch_size,patch_size]\n",
    "\n",
    "    # Compress and decompress\n",
    "    # out_enc = comp_model.compress(p3)\n",
    "    # out_dec = comp_model.decompress(out_enc[\"strings\"], out_enc[\"shape\"])\n",
    "    out_dec = comp_model(p3)\n",
    "    \n",
    "    rec1 = out_dec[\"x_hat\"][:, 0:1, :, :]\n",
    "    # rec_crop = crop(rec1, padding)  # [1,1,h_p,w_p]\n",
    "    denormalized = rec1.squeeze(0).squeeze(0)\n",
    "    \n",
    "    denormalized = denormalized * s + m\n",
    "    \n",
    "    return denormalized.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def pad(x, p):\n",
    "    h, w = x.size(2), x.size(3)\n",
    "    new_h = (h + p - 1) // p * p\n",
    "    new_w = (w + p - 1) // p * p\n",
    "    padding_left = (new_w - w) // 2\n",
    "    padding_right = new_w - w - padding_left\n",
    "    padding_top = (new_h - h) // 2\n",
    "    padding_bottom = new_h - h - padding_top\n",
    "    x_padded = F.pad(\n",
    "        x,\n",
    "        (padding_left, padding_right, padding_top, padding_bottom),\n",
    "        mode=\"constant\",\n",
    "        value=0,\n",
    "    )\n",
    "    return x_padded, (padding_left, padding_right, padding_top, padding_bottom)\n",
    "\n",
    "def crop(x, padding):\n",
    "    return F.pad(\n",
    "        x,\n",
    "        (-padding[0], -padding[1], -padding[2], -padding[3]),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7da2c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_list = [\n",
    "    # 'meta-llama/Meta-Llama-3-8B',\n",
    "    'meta-llama--Llama-2-7b-hf'\n",
    "]\n",
    "model_name = model_list[0]\n",
    "model_name = model_name.replace('/', '--')\n",
    "print('model_name: ', model_name)\n",
    "\n",
    "model_path = f\"../Wparam_dataset/hf_model/{model_name}\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "\n",
    "def get_named_linears(module):\n",
    "    return {name: m for name, m in module.named_modules() if isinstance(m, nn.Linear)}\n",
    "# original_weights = torch.randn(4096, 11008, dtype=torch.float32)\n",
    "\n",
    "# comp_model = TCM(config=[2,2,2,2,2,2], head_dim=[8, 16, 32, 32, 16, 8], drop_path_rate=0.0, N=64, M=320).to('cuda')\n",
    "# dictory = {}\n",
    "# checkpoint = torch.load('/workspace/Weight_compression/comp_lm_qtip/nic_models/TCM/checkpoints/0.05.pth.tar')\n",
    "# for k, v in checkpoint[\"state_dict\"].items():\n",
    "#     dictory[k.replace(\"module.\", \"\")] = v\n",
    "# comp_model.load_state_dict(dictory)\n",
    "# comp_model.eval()\n",
    "# comp_model.update()\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('/workspace/Weight_compression/comp_lm_qtip')\n",
    "\n",
    "\n",
    "from nic_models.FTIC.models import FrequencyAwareTransFormer\n",
    "comp_model = FrequencyAwareTransFormer()\n",
    "dictory = {}\n",
    "print(\"Loading FTIC\")\n",
    "checkpoint = torch.load('/workspace/Weight_compression/comp_lm_qtip/nic_models/FTIC/checkpoints/ckpt_0483.pth')\n",
    "for k, v in checkpoint.items():\n",
    "    dictory[k.replace(\"module.\", \"\")] = v\n",
    "comp_model.load_state_dict(dictory,strict=True)\n",
    "comp_model.eval()\n",
    "comp_model.update()\n",
    "comp_model.to('cuda')\n",
    "# for i in range(len(model.model.layers)):\n",
    "for i in [0, 1, 2, 10, 20, 31]:\n",
    "    linear = get_named_linears(model.model.layers[i])\n",
    "    for n,m in linear.items():\n",
    "        original_weights = m.weight.data\n",
    "\n",
    "        # 가중치 텐서의 일부(512x512)를 잘라내어 분석 (시각화를 위해)\n",
    "        weight_slice = original_weights[:512, :512].numpy()\n",
    "\n",
    "        quantized_weights = quantize_dequantize(weight_slice, bits=4)\n",
    "        jpeg_weights = jpeg_compress_decompress(weight_slice, quality=10)\n",
    "        webp_weights = jpeg_compress_decompress(weight_slice, quality=10)\n",
    "\n",
    "        # webp = torch.load('/workspace/Weight_compression/hf_model_comp/handcraft/ckpt/meta-llama--Llama-2-7b-hf/webp/qm_group_gs128_q97/0_k.pt')\n",
    "        # webp_weights = webp['W_hat'][:512, :512].numpy()\n",
    "\n",
    "        # nic_weights = torch.load('/workspace/Weight_compression/hf_model_comp/nic/ckpt/meta-llama--Meta-Llama-3-8B/tcm/patch256_norm_patch256_lmbda0.05/0_q.pt')\n",
    "        # nic_weights = torch.load('/workspace/Weight_compression/hf_model_comp/nic/ckpt/meta-llama--Meta-Llama-3-8B/tcm/group64_lmbda0.05/10_k.pt')\n",
    "        # nic_weights = nic_weights['W_hat'][:512, :512].numpy()\n",
    "        nic_weights = nic_compress_decompress(weight_slice, comp_model, quality=10)\n",
    "        # --- 3. 주파수 스펙트럼 분석 및 오차 계산 ---\n",
    "\n",
    "        # 각 가중치의 스펙트럼 계산\n",
    "        original_spec = get_magnitude_spectrum(weight_slice)\n",
    "        quantized_spec = get_magnitude_spectrum(quantized_weights)\n",
    "        jpeg_spec = get_magnitude_spectrum(jpeg_weights)\n",
    "        webp_spec = get_magnitude_spectrum(webp_weights)\n",
    "        nic_spec = get_magnitude_spectrum(nic_weights)\n",
    "\n",
    "        # 오차 스펙트럼 계산\n",
    "        quant_error_spec = np.abs(original_spec - quantized_spec)\n",
    "        jpeg_error_spec = np.abs(original_spec - jpeg_spec)\n",
    "        webp_error_spec = np.abs(original_spec - webp_spec)\n",
    "        nic_error_spec = np.abs(original_spec - nic_spec)\n",
    "\n",
    "\n",
    "        quant_error_norm = (quant_error_spec - np.min(quant_error_spec)) / (np.max(quant_error_spec) - np.min(quant_error_spec))\n",
    "        jpeg_error_norm = (jpeg_error_spec - np.min(jpeg_error_spec)) / (np.max(jpeg_error_spec) - np.min(jpeg_error_spec))\n",
    "        webp_error_norm = (webp_error_spec - np.min(webp_error_spec)) / (np.max(webp_error_spec) - np.min(webp_error_spec))\n",
    "        nic_error_norm = (nic_error_spec - np.min(nic_error_spec)) / (np.max(nic_error_spec) - np.min(nic_error_spec))\n",
    "\n",
    "\n",
    "        # --- 4. 시각화 (컬러바 수정) ---\n",
    "        # plt.style.use('dark_background')\n",
    "        # plt.style.use('background')\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(21, 7))\n",
    "\n",
    "        # 원본 가중치 스펙트럼\n",
    "        axes[0].imshow(original_spec, cmap='viridis')\n",
    "        axes[0].set_title('Original Weight Spectrum', fontsize=16)\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # 정규화된 양자화 오차 스펙트럼 + 개별 컬러바\n",
    "        im1 = axes[1].imshow(quant_error_norm, cmap='inferno')\n",
    "        axes[1].set_title('Normalized Quantization Error', fontsize=16)\n",
    "        axes[1].axis('off')\n",
    "        fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04) # 양자화 오차 플롯의 컬러바\n",
    "\n",
    "        # 정규화된 JPEG 오차 스펙트럼 + 개별 컬러바\n",
    "        im2 = axes[2].imshow(jpeg_error_norm, cmap='inferno')\n",
    "        axes[2].set_title('Normalized JPEG Error', fontsize=16)\n",
    "        axes[2].axis('off')\n",
    "        fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04) # JPEG 오차 플롯의 컬러바\n",
    "\n",
    "        # im3 = axes[3].imshow(webp_error_norm, cmap='inferno')\n",
    "        # axes[3].set_title('Normalized WebP Error', fontsize=16)\n",
    "        # axes[3].axis('off')\n",
    "        # fig.colorbar(im3, ax=axes[3], fraction=0.046, pad=0.04) # JPEG 오차 플롯의 컬러\n",
    "\n",
    "        im3 = axes[3].imshow(nic_error_norm, cmap='inferno')\n",
    "        axes[3].set_title('Normalized FTIC Error', fontsize=16)\n",
    "        axes[3].axis('off')\n",
    "        fig.colorbar(im3, ax=axes[3], fraction=0.046, pad=0.04) # JPEG 오차 플롯의 컬러\n",
    "\n",
    "        plt.suptitle(f'{i}_{n} Normalized Frequency Error Comparison: Quantization vs. JPEG', fontsize=20)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
