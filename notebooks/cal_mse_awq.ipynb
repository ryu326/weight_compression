{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/Wcomp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "import sys, os, json, math\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "# state_dict = net.state_dict()\n",
    "\n",
    "std = 0.012528747320175171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.0156)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.9569)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.5089)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(189.5222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21.4628)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.2033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.0267)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.1139)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.7742)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(57.4543)\n"
     ]
    }
   ],
   "source": [
    "bpps = [2, 3, 4, 5, 6, 7, 8, 9, 10, 12]\n",
    "mses = []\n",
    "for bpp in bpps:\n",
    "    ckpt_path = f'../model_cache_reconstructed/awq/llama3-8b-my-w{bpp}-g128-fake-quantized'\n",
    "    recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "    n = 0\n",
    "    mse = 0\n",
    "    for k, v in state_dict.items():\n",
    "        if 'mlp' not in k and 'attn' not in k: continue\n",
    "        \n",
    "        mse += ((recon_state_dict[k] - v)**2).sum()\n",
    "        n += v.numel()\n",
    "        \n",
    "    mse = mse / n / std **2 \n",
    "    mses.append(mse.item())\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ\n",
    "bpp:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 12]\n",
    "\n",
    "mse:  [23.015626907348633, 21.956884384155273, 18.508888244628906, 189.52224731445312, 21.462759017944336, 18.203344345092773, 23.026723861694336, 12.113859176635742, 13.774173736572266, 57.454349517822266]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpp:  [2, 3, 4, 5, 6, 7, 8, 9, 10, 12]\n",
      "mse:  [23.015626907348633, 21.956884384155273, 18.508888244628906, 189.52224731445312, 21.462759017944336, 18.203344345092773, 23.026723861694336, 12.113859176635742, 13.774173736572266, 57.454349517822266]\n"
     ]
    }
   ],
   "source": [
    "print('bpp: ', bpps)\n",
    "print('mse: ', mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.86s/it]\n"
     ]
    }
   ],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()\n",
    "\n",
    "std = 0.012528747320175171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0116)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0004)\n"
     ]
    }
   ],
   "source": [
    "ckpt_paths = [\n",
    "    # '../model_cache_reconstructed/vq_seedlm_/mlp_16_row_dataset.pt_v101/size16_ne256_de256_P4_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.09639_total_iter_2000000.pth.tar',\n",
    "    # '../model_cache_reconstructed/vq_seedlm_/mlp_16_row_dataset.pt_v101/size16_ne512_de256_P4_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.0864_total_iter_2000000.pth.tar',\n",
    "    # '../model_cache_reconstructed/vq_seedlm_/mlp_16_row_dataset.pt_v101/size16_ne512_de256_P8_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.00338_total_iter_1950000.pth.tar',\n",
    "    # '../model_cache_reconstructed/vq_seedlm_/mlp_16_row_dataset.pt_v101/size16_ne512_de256_P16_batch_size512_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_2e-05_total_iter_1850000.pth.tar'\n",
    "    '../model_cache_reconstructed/vq_seedlm_/mlp_attn_16_row_dataset.pt/size16_ne256_denc512_P8_KNone_de16_batch_size1024_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.01045_total_iter_1850000.pth.tar',\n",
    "    '../model_cache_reconstructed/vq_seedlm_/mlp_attn_16_row_dataset.pt/size16_ne256_denc512_P16_KNone_de16_batch_size1024_total_iter2000000_lr0.0001_seed100/best_mse_model_MSE_0.00029_total_iter_1850000.pth.tar'\n",
    "]\n",
    "\n",
    "mses = []\n",
    "for ckpt_path in ckpt_paths:\n",
    "    recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "    n = 0\n",
    "    mse = 0\n",
    "    for k, v in state_dict.items():\n",
    "        if 'mlp' not in k and 'attn' not in k: continue\n",
    "        # if 'mlp' not in k: continue\n",
    "        \n",
    "        mse += ((recon_state_dict[k] - v)**2).sum()\n",
    "        n += v.numel()\n",
    "        \n",
    "    mse = mse / n / std **2 \n",
    "    mses.append(mse.item())\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:  [0.011624915525317192, 0.00042339859646745026]\n"
     ]
    }
   ],
   "source": [
    "# print('bpp: ', bpps)\n",
    "print('mse: ', mses[-4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQseedlm v101 mlp\n",
    "bpp:  [6, 6.25, 12.25, 24.25]\n",
    "\n",
    "<!-- mse:  [0.08586853742599487, 0.07806961983442307, 0.003792528761550784, 3.880444273818284e-05] -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQseedlm v101 mlp attn\n",
    "bpp:  [12, 24]\n",
    "\n",
    "mse:  [0.011624915525317192, 0.00042339859646745026]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
