{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## huggingface model download\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델과 토크나이저 로드 (캐시 디렉토리 지정)\n",
    "# model = AutoModel.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "cache_directory = \"/home/jgryu/Weight_compression/model_zoo/huggingface\" \n",
    "\n",
    "for model_name in ['gemma']:\n",
    "    if model_name == 'VIT':\n",
    "        model_cls = ViTForImageClassification\n",
    "        ver_list = ['google/vit-base-patch16-224', \n",
    "                    'google/vit-base-patch16-224-in21k', \n",
    "                    'google/vit-base-patch16-384',\n",
    "                    'google/vit-base-patch32-224-in21k',\n",
    "                    'google/vit-base-patch32-384',\n",
    "                    'google/vit-large-patch16-384',\n",
    "                    'google/vit-large-patch32-224-in21k',\n",
    "                    'google/vit-large-patch16-224-in21k',\n",
    "                    'google/vit-large-patch16-224',\n",
    "                    'google/vit-huge-patch14-224-in21k',]\n",
    "\n",
    "    # elif model_name == 'CLIP':\n",
    "    #     ver_list = ['openai/clip-vit-base-patch14', 'openai/clip-vit-large-patch14']\n",
    "    #     model_cls = CLIPVisionModelWithProjection\n",
    "\n",
    "    elif model_name == 'llama':\n",
    "        model_cls = AutoModelForCausalLM\n",
    "        ver_list = [\n",
    "            \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "            \"meta-llama/Llama-Guard-3-8B\",\n",
    "            \"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\",\n",
    "            \"meta-llama/Meta-Llama-3.1-405B-Instruct\",\n",
    "            \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "            \"meta-llama/Meta-Llama-3.1-405B-FP8\",\n",
    "            \"meta-llama/Meta-Llama-3.1-405B\",\n",
    "            \"meta-llama/Llama-Guard-3-8B-INT8\",\n",
    "            \"meta-llama/Prompt-Guard-86M\",\n",
    "            \"meta-llama/Meta-Llama-3.1-70B\",\n",
    "            \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            \"meta-llama/Meta-Llama-Guard-2-8B\",\n",
    "            \"meta-llama/Meta-Llama-3-8B\",\n",
    "            \"meta-llama/Meta-Llama-3-70B\",\n",
    "            \"meta-llama/LlamaGuard-7b\",\n",
    "            \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "            \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "            \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            \"meta-llama/Llama-2-70b-hf\",\n",
    "            \"meta-llama/Llama-2-13b-hf\",\n",
    "            \"meta-llama/Llama-2-7b-hf\",\n",
    "            \"meta-llama/Llama-2-70b-chat\",\n",
    "            \"meta-llama/Llama-2-13b-chat\",\n",
    "            \"meta-llama/Llama-2-7b-chat\",\n",
    "            \"meta-llama/Llama-2-70b\",\n",
    "            \"meta-llama/Llama-2-13b\",\n",
    "            \"meta-llama/Llama-2-7b\",\n",
    "            \"meta-llama/CodeLlama-70b-Instruct-hf\",\n",
    "            \"meta-llama/CodeLlama-70b-Python-hf\",\n",
    "            \"meta-llama/CodeLlama-70b-hf\",\n",
    "            \"meta-llama/CodeLlama-34b-Instruct-hf\",\n",
    "            \"meta-llama/CodeLlama-34b-Python-hf\",\n",
    "            \"meta-llama/CodeLlama-34b-hf\",\n",
    "            \"meta-llama/CodeLlama-13b-Instruct-hf\",\n",
    "            \"meta-llama/CodeLlama-13b-Python-hf\",\n",
    "            \"meta-llama/CodeLlama-13b-hf\",\n",
    "            \"meta-llama/CodeLlama-7b-Instruct-hf\",\n",
    "            \"meta-llama/CodeLlama-7b-Python-hf\",\n",
    "            \"meta-llama/CodeLlama-7b-hf\"\n",
    "        ]\n",
    "    elif model_name == \"qwen\":\n",
    "        model_cls = AutoModelForCausalLM\n",
    "        ver_list = [\n",
    "            \"Qwen/Qwen2.5-Coder-1.5B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-Math-RM-72B\",\n",
    "            \"Qwen/Qwen2-Math-RM-72B\",\n",
    "            \"Qwen/Qwen2.5-Math-1.5B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-Math-72B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-Math-7B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-Coder-7B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2.5-0.5B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2.5-1.5B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2.5-3B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2.5-14B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2.5-7B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2.5-32B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2.5-72B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-Coder-7B\",\n",
    "            \"Qwen/Qwen2.5-Coder-1.5B\",\n",
    "            \"Qwen/Qwen2.5-7B\",\n",
    "            \"Qwen/Qwen2.5-72B\",\n",
    "            \"Qwen/Qwen2.5-3B\",\n",
    "            \"Qwen/Qwen2.5-32B\",\n",
    "            \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2.5-14B\",\n",
    "            \"Qwen/Qwen2.5-1.5B\",\n",
    "            \"Qwen/Qwen2.5-0.5B\",\n",
    "            \"Qwen/Qwen2.5-Math-7B\",\n",
    "            \"Qwen/Qwen2.5-Math-1.5B\",\n",
    "            \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2.5-3B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2.5-72B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2.5-Math-72B\",\n",
    "            \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2.5-7B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2.5-3B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2.5-3B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-32B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2.5-32B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2.5-14B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2.5-1.5B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-0.5B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2.5-0.5B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "            \"Qwen/Qwen2-VL-72B-Instruct\",\n",
    "            \"Qwen/Qwen2-VL-72B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "            \"Qwen/Qwen2.5-72B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2-7B-Instruct-MLX\",\n",
    "            \"Qwen/Qwen2-VL-7B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2-Math-72B-Instruct\",\n",
    "            \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2-VL-2B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "            \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "            \"Qwen/Qwen2-0.5B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2-0.5B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2-0.5B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2-1.5B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2-1.5B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2-1.5B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2-57B-A14B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2-7B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2-7B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2-7B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2-0.5B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2-7B-Instruct\",\n",
    "            \"Qwen/Qwen2-7B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2-72B-Instruct-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2-72B-Instruct-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen2-57B-A14B-Instruct\",\n",
    "            \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "            \"Qwen/Qwen2-72B-Instruct-AWQ\",\n",
    "            \"Qwen/Qwen2-72B-Instruct\",\n",
    "            \"Qwen/Qwen2-Math-7B-Instruct\",\n",
    "            \"Qwen/Qwen2-Math-1.5B-Instruct\",\n",
    "            \"Qwen/Qwen2-Audio-7B\",\n",
    "            \"Qwen/Qwen2-Audio-7B-Instruct\",\n",
    "            \"Qwen/Qwen2-Math-1.5B\",\n",
    "            \"Qwen/Qwen2-Math-72B\",\n",
    "            \"Qwen/Qwen2-Math-7B\",\n",
    "            \"Qwen/Qwen2-1.5B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2-72B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2-57B-A14B-Instruct-GGUF\",\n",
    "            \"Qwen/Qwen2-57B-A14B\",\n",
    "            \"Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen2-7B\",\n",
    "            \"Qwen/Qwen2-72B\",\n",
    "            \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "            \"Qwen/Qwen2-1.5B\",\n",
    "            \"Qwen/Qwen2-0.5B\",\n",
    "            \"Qwen/Qwen2-1.5B-Instruct-MLX\",\n",
    "            \"Qwen/Qwen2-0.5B-Instruct-MLX\",\n",
    "            \"Qwen/CodeQwen1.5-7B\",\n",
    "            \"Qwen/Qwen1.5-32B-Chat-AWQ\",\n",
    "            \"Qwen/Qwen1.5-MoE-A2.7B-Chat\",\n",
    "            \"Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen1.5-7B-Chat-GPTQ-Int4\",\n",
    "            \"Qwen/Qwen1.5-7B-Chat-AWQ\",\n",
    "            \"Qwen/Qwen1.5-72B-Chat-GPTQ-Int8\",\n",
    "            \"Qwen/Qwen1.5-72B-Chat-GPTQ-Int4\",\n",
    "            ]\n",
    "    \n",
    "    elif model_name == 'gemma':\n",
    "        model_cls = AutoModelForCausalLM\n",
    "        ver_list = [\"google/gemma-2-2b\",\n",
    "            \"google/gemma-2-2b-it\",\n",
    "            \"google/gemma-2-9b\",\n",
    "            \"google/gemma-2-9b-it\",\n",
    "            \"google/gemma-2-27b\",\n",
    "            \"google/gemma-2-27b-it\",\n",
    "            \"google/gemma-2-2b-pytorch\",\n",
    "            \"google/gemma-2-2b-it-pytorch\",\n",
    "            \"google/gemma-2-9b-pytorch\",\n",
    "            \"google/gemma-2-9b-it-pytorch\",\n",
    "            \"google/gemma-2-27b-pytorch\",\n",
    "            \"google/gemma-2-27b-it-pytorch\",\n",
    "            \"google/gemma-2-9b-keras\",\n",
    "            \"google/gemma-2-instruct-9b-keras\",\n",
    "            \"google/gemma-2-2b-it-GGUF\",\n",
    "            \"google/gemma-1.1-2b-it\",\n",
    "            \"google/gemma-1.1-7b-it\",\n",
    "            \"google/gemma-1.1-7b-it-GGUF\",\n",
    "            \"google/gemma-1.1-2b-it-GGUF\",\n",
    "            \"google/gemma-1.1-2b-it-pytorch\",\n",
    "            \"google/gemma-1.1-7b-it-pytorch\",\n",
    "            \"google/gemma-7b\",\n",
    "            \"google/gemma-7b-it\",\n",
    "            \"google/gemma-2b\",\n",
    "            \"google/gemma-2b-it\",\n",
    "            \"google/gemma-7b-it-GGUF\",\n",
    "            \"google/gemma-7b-GGUF\",\n",
    "            \"google/gemma-2b-GGUF\",\n",
    "            \"google/gemma-2b-it-GGUF\",\n",
    "            \"google/gemma-7b-it-pytorch\",\n",
    "            \"google/gemma-7b-pytorch\",\n",
    "            \"google/gemma-2b-pytorch\",\n",
    "            \"google/gemma-7b-quant-pytorch\",\n",
    "            \"google/gemma-7b-it-quant-pytorch\",\n",
    "            \"google/gemma-2b-it-pytorch\",\n",
    "            \"google/gemma-7b-flax\",\n",
    "            \"google/gemma-7b-it-flax\",\n",
    "            \"google/gemma-2b-flax\",\n",
    "            \"google/gemma-2b-it-flax\",\n",
    "            \"google/gemma-2b-it-keras\",\n",
    "            \"google/gemma-7b-keras\",\n",
    "            \"google/gemma-2b-keras\",\n",
    "            \"google/gemma-7b-it-keras\",\n",
    "            \"google/gemma-2b-sfp-cpp\",\n",
    "            \"google/gemma-2b-it-cpp\",\n",
    "            \"google/gemma-7b-sfp-cpp\",\n",
    "            \"google/gemma-7b-cpp\",\n",
    "            \"google/gemma-7b-it-cpp\",\n",
    "            \"google/gemma-7b-it-sfp-cpp\",\n",
    "            \"google/gemma-2b-it-sfp-cpp\",\n",
    "            \"google/gemma-2b-cpp\",\n",
    "            \"google/gemma-1.1-2b-it-keras\",\n",
    "            \"google/gemma-1.1-7b-it-keras\",\n",
    "            \"google/gemma-1.1-2b-it-tflite\",\n",
    "            \"google/gemma-2b-it-tflite\",\n",
    "            \"google/codegemma-2b\",\n",
    "            \"google/codegemma-7b\",\n",
    "            \"google/codegemma-7b-it\",\n",
    "            \"google/codegemma-7b-it-GGUF\",\n",
    "            \"google/codegemma-2b-GGUF\",\n",
    "            \"google/codegemma-7b-GGUF\",\n",
    "            \"google/codegemma-7b-pytorch\",\n",
    "            \"google/codegemma-2b-pytorch\",\n",
    "            \"google/codegemma-7b-it-pytorch\",\n",
    "            \"google/codegemma-2b-keras\",\n",
    "            \"google/codegemma-7b-keras\",\n",
    "            \"google/codegemma-7b-it-keras\",\n",
    "            \"google/codegemma-1.1-7b-it\",\n",
    "            \"google/codegemma-1.1-2b\",\n",
    "            \"google/codegemma-1.1-2b-pytorch\",\n",
    "            \"google/codegemma-1.1-7b-it-pytorch\",\n",
    "            \"google/codegemma-1.1-2b-GGUF\",\n",
    "            \"google/codegemma-1.1-7b-it-GGUF\",]\n",
    "\n",
    "    for ver in tqdm(ver_list):\n",
    "        if \"70b\" in ver.lower() or \"405b\" in ver.lower() or \"72b\" in ver.lower():\n",
    "            continue\n",
    "        try:\n",
    "            print(ver)\n",
    "            net = model_cls.from_pretrained(ver, cache_dir = cache_directory, token=\"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\", trust_remote_code=True)\n",
    "        except:\n",
    "            print('continue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"google/gemma-1.1-2b-it\",\n",
      "\"google/gemma-1.1-7b-it\",\n",
      "\"google/gemma-1.1-7b-it-GGUF\",\n",
      "\"google/gemma-1.1-2b-it-GGUF\",\n",
      "\"google/gemma-1.1-2b-it-pytorch\",\n",
      "\"google/gemma-1.1-7b-it-pytorch\",\n",
      "\"google/gemma-7b\",\n",
      "\"google/gemma-7b-it\",\n",
      "\"google/gemma-2b\",\n",
      "\"google/gemma-2b-it\",\n",
      "\"google/gemma-7b-it-GGUF\",\n",
      "\"google/gemma-7b-GGUF\",\n",
      "\"google/gemma-2b-GGUF\",\n",
      "\"google/gemma-2b-it-GGUF\",\n",
      "\"google/gemma-7b-it-pytorch\",\n",
      "\"google/gemma-7b-pytorch\",\n",
      "\"google/gemma-2b-pytorch\",\n",
      "\"google/gemma-7b-quant-pytorch\",\n",
      "\"google/gemma-7b-it-quant-pytorch\",\n",
      "\"google/gemma-2b-it-pytorch\",\n",
      "\"google/gemma-7b-flax\",\n",
      "\"google/gemma-7b-it-flax\",\n",
      "\"google/gemma-2b-flax\",\n",
      "\"google/gemma-2b-it-flax\",\n",
      "\"google/gemma-2b-it-keras\",\n",
      "\"google/gemma-7b-keras\",\n",
      "\"google/gemma-2b-keras\",\n",
      "\"google/gemma-7b-it-keras\",\n",
      "\"google/gemma-2b-sfp-cpp\",\n",
      "\"google/gemma-2b-it-cpp\",\n",
      "\"google/gemma-7b-sfp-cpp\",\n",
      "\"google/gemma-7b-cpp\",\n",
      "\"google/gemma-7b-it-cpp\",\n",
      "\"google/gemma-7b-it-sfp-cpp\",\n",
      "\"google/gemma-2b-it-sfp-cpp\",\n",
      "\"google/gemma-2b-cpp\",\n",
      "\"google/gemma-1.1-2b-it-keras\",\n",
      "\"google/gemma-1.1-7b-it-keras\",\n",
      "\"google/gemma-1.1-2b-it-tflite\",\n",
      "\"google/gemma-2b-it-tflite\",\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b'\n",
    "\n",
    "# Send an HTTP request to get the page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the elements based on the class and href attribute\n",
    "elements = soup.find_all('a', class_='flex items-center justify-between gap-4 p-2')\n",
    "\n",
    "# Initialize a list to store the extracted data\n",
    "gemma_models = []\n",
    "\n",
    "# Loop through the extracted elements to get the necessary details\n",
    "for element in elements:\n",
    "    # Extract the model name from the h4 tag\n",
    "    model_name = element.find('h4').text.strip()\n",
    "    \n",
    "    # Extract the href attribute (link to the model)\n",
    "    model_link = element['href']\n",
    "    \n",
    "    # Optionally, you can construct the full URL for the model\n",
    "    full_link = f'https://huggingface.co{model_link}'\n",
    "    \n",
    "    # Append the data to the list\n",
    "    gemma_models.append({'name': model_name, 'link': full_link})\n",
    "\n",
    "# Output the extracted models and their links\n",
    "for model in gemma_models:\n",
    "    print(f\"\\\"{model['name']}\\\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = \"/home/jgryu/Weight_compression/model_zoo/huggingface/models--google--vit-base-patch16-224-in21k/snapshots/b4569560a39a0f1af58e3ddaf17facf20ab919b0\"\n",
    "model =  ViTForImageClassification.from_pretrained(path, local_files_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## huggingface model download\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델과 토크나이저 로드 (캐시 디렉토리 지정)\n",
    "# model = AutoModel.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "cache_directory = \"/home/jgryu/Weight_compression/model_zoo/huggingface\" \n",
    "\n",
    "for model_name in ['VIT']:\n",
    "    if model_name == 'VIT':\n",
    "        ver_list = ['google/vit-base-patch16-224', \n",
    "                    'google/vit-base-patch16-224-in21k', \n",
    "                    'google/vit-base-patch16-384',\n",
    "                    'google/vit-base-patch32-224-in21k',\n",
    "                    'google/vit-base-patch32-384',\n",
    "                    'google/vit-large-patch16-384',\n",
    "                    'google/vit-large-patch32-224-in21k',\n",
    "                    'google/vit-large-patch16-224-in21k',\n",
    "                    'google/vit-large-patch16-224',\n",
    "                    'google/vit-huge-patch14-224-in21k',]\n",
    "        model_cls = ViTForImageClassification\n",
    "\n",
    "    # elif model_name == 'CLIP':\n",
    "    #     ver_list = ['openai/clip-vit-base-patch14', 'openai/clip-vit-large-patch14']\n",
    "    #     model_cls = CLIPVisionModelWithProjection\n",
    "\n",
    "\n",
    "    for ver in tqdm(ver_list):\n",
    "        print(ver)\n",
    "        value_list = []\n",
    "\n",
    "        print(model_cls.from_pretrained)\n",
    "\n",
    "        net = model_cls.from_pretrained(ver, cache_dir = cache_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## timm vision model download 거진 완료\n",
    "\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "# torch.hub.set_dir(\"/home/jgryu/Weight_compression/timm\")\n",
    "\n",
    "# import os\n",
    "# os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/home/jgryu/Weight_compression/model_dict/timm\" \n",
    "\n",
    "save_path = \"/home/jgryu/Weight_compression/model_dict/timm\" \n",
    "files_and_folders = os.listdir(save_path)\n",
    "\n",
    "for model_name in tqdm(timm.list_models()):\n",
    "\n",
    "    if model_name + '.pt' in files_and_folders:\n",
    "        print('continue')\n",
    "        continue\n",
    "\n",
    "    print(model_name)\n",
    "    model = timm.create_model(model_name, pretrained=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, model_name + '.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## torch.hub 모델 다운\n",
    "repos = [\n",
    "    'pytorch/vision',\n",
    "    'huggingface/pytorch-transformers',\n",
    "    'facebookresearch/detectron2',\n",
    "    'ultralytics/yolov5',\n",
    "    'NVIDIA/DeepLearningExamples:torchhub',\n",
    "    'rwightman/gen-efficientnet-pytorch',\n",
    "    'pytorch/fairseq',\n",
    "    'deepmind/deepmind-research',\n",
    "    'philipperemy/deep-learning-v2-pytorch',\n",
    "    'rwightman/pytorch-image-models',\n",
    "    'facebookresearch/pytext',\n",
    "    'yunjey/mnist-svhn-transfer',\n",
    "    'moein-shariatnia/PyTorch_GANs',\n",
    "    'Helsinki-NLP/Opus-MT',\n",
    "    'open-mmlab/mmdetection'\n",
    "]\n",
    "\n",
    "dataset_path = \"/home/jgryu/Weight_compression/dataset\"\n",
    "\n",
    "for repo in repos:\n",
    "\n",
    "    try: \n",
    "        entrypoints = torch.hub.list(repo, force_reload=True)\n",
    "        print(entrypoints)\n",
    "        for m in tqdm.tqdm(entrypoints):\n",
    "\n",
    "            model = torch.hub.load(repo, m, pretrained = True)\n",
    "\n",
    "            path = os.path.join(dataset_path, repo)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            torch.save(model, os.path.join(path, m + '.pth'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not list {repo}: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
