{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/Wcomp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "import sys, os, json, math\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "\n",
    "ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (topk): tensor(0.0301)\n",
      "attn_r0.1_top0.3_topk_layer_all\n",
      "MSE (bottomk): tensor(0.0301)\n",
      "attn_r0.1_top0.3_bottomk_layer_all\n",
      "MSE (random): tensor(0.0300)\n",
      "attn_r0.1_top0.3_random_layer_all\n"
     ]
    }
   ],
   "source": [
    "for wtype in ['attn']:\n",
    "    # for top in [0.001, 0.01, 0.03, 0.1, 0.3]:\n",
    "    for top in [0.3]:\n",
    "        r = 0.1\n",
    "        std = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset_v0/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_std.npy')\n",
    "        a = std * math.sqrt(r) * math.sqrt(3)\n",
    "\n",
    "        for method in ['topk', 'bottomk', 'random']:\n",
    "            noisy_state_dict = {}\n",
    "            mse_fn = nn.MSELoss()\n",
    "            mse = 0\n",
    "            count = 0\n",
    "\n",
    "            for k, v in state_dict.items():\n",
    "                if wtype in k:\n",
    "                    count += 1\n",
    "\n",
    "                    # Flatten the tensor to work with absolute values\n",
    "                    v_flat = v.view(-1)\n",
    "                    abs_v_flat = torch.abs(v_flat).to(dtype=torch.float32)\n",
    "                    k_value = int(len(abs_v_flat) * top)\n",
    "\n",
    "                    if method == 'topk' and k_value > 0:\n",
    "                        # Top k method\n",
    "                        top_values, _ = torch.topk(abs_v_flat, k=k_value)\n",
    "                        threshold = top_values[-1]  # Smallest value in top k\n",
    "                        mask = abs_v_flat >= threshold\n",
    "\n",
    "                    elif method == 'bottomk' and k_value > 0:\n",
    "                        # Bottom k method\n",
    "                        bottom_values, _ = torch.topk(-abs_v_flat, k=k_value)\n",
    "                        threshold = -bottom_values[-1]  # Largest negative value in bottom k\n",
    "                        mask = abs_v_flat <= threshold\n",
    "\n",
    "                    elif method == 'random' and k_value > 0:\n",
    "                        # Random k method\n",
    "                        indices = torch.randperm(len(abs_v_flat))[:k_value]\n",
    "                        mask = torch.zeros_like(abs_v_flat, dtype=torch.bool)\n",
    "                        mask[indices] = True\n",
    "\n",
    "                    else:\n",
    "                        # No weights are modified if k_value is 0\n",
    "                        mask = torch.zeros_like(v_flat, dtype=torch.bool)\n",
    "\n",
    "                    mask = mask.view(v.shape)  # Reshape to original shape\n",
    "\n",
    "                    # Generate noise and apply to selected weights\n",
    "                    noise = torch.empty(v.shape).uniform_(-a, a).to(dtype=v.dtype)\n",
    "                    noise = noise * mask  # Apply noise only to selected weights\n",
    "\n",
    "                    noisy_state_dict[k] = v + noise\n",
    "                    mse += mse_fn(noisy_state_dict[k].to(dtype=torch.float32), v.to(dtype=torch.float32))\n",
    "                #     print(k, v.shape, v.dtype)\n",
    "                else:\n",
    "                    noisy_state_dict[k] = v\n",
    "\n",
    "            mse /= count\n",
    "            print(f\"MSE ({method}):\", mse / std**2)\n",
    "\n",
    "            # Save the modified model\n",
    "            recon_net = AutoModelForCausalLM.from_config(net.config)\n",
    "            recon_net.load_state_dict(noisy_state_dict)\n",
    "            recon_net = recon_net.to(dtype=torch.bfloat16)\n",
    "            save_directory = f\"/home/jgryu/Weight_compression/model_cache_reconstructed/uniform_noise/exp_magnitude/r{r}/{wtype}_r{r}_top{top}_{method}_layer_all\"\n",
    "            recon_net.save_pretrained(save_directory)\n",
    "            tokenizer.save_pretrained(save_directory)\n",
    "            print(save_directory.split(\"/\")[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Wcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
