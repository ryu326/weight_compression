{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c575d12c-2319-4417-aeba-f1a33d1ce740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a367b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/RD/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "def contains_all_substrings(string, substrings):\n",
    "    return all(substring in string for substring in substrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c6ee9",
   "metadata": {},
   "source": [
    "### train val 나눠서 path 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf95a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all : 290\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# dataset_path = '/home/jgryu/Weight_compression/Wparam_dataset/model_param_tensor/meta-llama/Meta-Llama-3-8B'\n",
    "dataset_path = '/home/jgryu/Weight_compression/Wparam_dataset/model_param_tensor/meta-llama/Llama-2-7b-hf'\n",
    "save_path = '/home/jgryu/Weight_compression/Wparam_dataset/path_json/'\n",
    "\n",
    "tenosr_path_list = glob.glob(f'{dataset_path}/**/*.npy', recursive=True)\n",
    "\n",
    "print(f'all : {len(tenosr_path_list)}')\n",
    "# filters = ['meta-llama-3-8b', 'attn']\n",
    "filters = ['llama-2-7b', 'attn']\n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "data = {'filters': filters, 'path_list': []}\n",
    "\n",
    "for path in tenosr_path_list:\n",
    "    if contains_all_substrings(path.lower(), filters):\n",
    "        data['path_list'].append(path)\n",
    "        \n",
    "print(len(data['path_list']))\n",
    "\n",
    "\n",
    "random.shuffle(data['path_list'])\n",
    "\n",
    "split_index = int(0.8 * len(data['path_list']))\n",
    "train_data = data['path_list'][:split_index]\n",
    "validation_data = data['path_list'][split_index:]\n",
    "\n",
    "file_path_train = os.path.join(save_path, '_'.join(filters) + '_train.json')\n",
    "file_path_validation = file_path_train.replace('train', 'val')\n",
    "\n",
    "with open(file_path_train, 'w') as json_file:\n",
    "    json.dump(train_data, json_file, indent=4)\n",
    "with open(file_path_validation, 'w') as json_file:\n",
    "    json.dump(validation_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51579110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import json\n",
    "def contains_all_substrings(string, substrings):\n",
    "    return all(substring in string for substring in substrings)\n",
    "\n",
    "save_path = '/home/jgryu/Weight_compression/model_param_dataset/'\n",
    "dataset_path = '/home/jgryu/Weight_compression/model_param_tensor/'\n",
    "\n",
    "tenosr_path_list = glob.glob(f'{dataset_path}/**/*.npy', recursive=True)\n",
    "\n",
    "print(f'all : {len(tenosr_path_list)}')\n",
    "\n",
    "data = {'attn':[], 'mlp':[], 'embed':[], 'layernorm':[]}   \n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "for weight_type in ['attn', 'mlp', 'embed', 'layernorm']: \n",
    "    for path in tenosr_path_list:\n",
    "        if contains_all_substrings(path.lower(), [weight_type]):\n",
    "            data[weight_type].append(path)\n",
    "    print(f'{weight_type} : {len(data[weight_type])}')\n",
    "\n",
    "\n",
    "    random.shuffle(data[weight_type])\n",
    "\n",
    "    split_index = int(0.8 * len(data[weight_type]))\n",
    "    train_data = data[weight_type][:split_index]\n",
    "    validation_data = data[weight_type][split_index:]\n",
    "\n",
    "    file_path_train = os.path.join(save_path, f'{weight_type}_tensor_path_train.json')\n",
    "    file_path_validation = file_path_train.replace('train', 'val')\n",
    "\n",
    "    with open(file_path_train, 'w') as json_file:\n",
    "        json.dump(train_data, json_file, indent=4)\n",
    "    with open(file_path_validation, 'w') as json_file:\n",
    "        json.dump(validation_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50001fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8960, 128, 1024)\n"
     ]
    }
   ],
   "source": [
    "# ## \n",
    "# import numpy as np\n",
    "# json_file = '/home/jgryu/Weight_compression/Wparam_dataset/path_json/meta-llama-3-8b_mlp_val.json'\n",
    "# save_path = '/home/jgryu/Weight_compression/Wparam_dataset/transformer_dataset'\n",
    "# dim = 1024\n",
    "# length = 128\n",
    "\n",
    "# with open(json_file, 'r') as f:\n",
    "#     tensor_path_list = json.load(f)\n",
    "\n",
    "# data = []\n",
    "# for tensor_path in tensor_path_list:\n",
    "#     t = np.load(tensor_path)\n",
    "#     data.append(t.reshape(-1, length, dim))\n",
    "    \n",
    "# data = np.concatenate(data, axis = 0)\n",
    "# print(data.shape)\n",
    "# path = os.path.join(save_path, json_file.split('/')[-1].replace('.json', ''))\n",
    "# np.save(path, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05aea0d",
   "metadata": {},
   "source": [
    "### 1. Model tensor를 따로 npy로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91754626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "# import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "def get_ckpt_path(path, branch = 'main'):\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "def check_contains_any_substrings(string, substrings):\n",
    "    return any(substring in string for substring in substrings)\n",
    "\n",
    "model_zoo_path = '/home/jgryu/Weight_compression/Wparam_dataset/model_zoo/huggingface'\n",
    "# model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b', '13b', 'mini', 'small']\n",
    "# model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b','mini']\n",
    "# model_filter = ['-13b', '-small']\n",
    "model_filter = ['meta-llama-3-8b', 'meta-llama-3.1-8b']\n",
    "\n",
    "model_list = os.listdir(model_zoo_path)\n",
    "print()\n",
    "ckpt_path_list = []\n",
    "for ck in model_list:\n",
    "    if check_contains_any_substrings(ck.lower(), model_filter):\n",
    "        ckpt_path = get_ckpt_path(os.path.join(model_zoo_path, ck))\n",
    "        if ckpt_path is not None:\n",
    "            ckpt_path_list.append(ckpt_path)\n",
    "\n",
    "print(len(ckpt_path_list))\n",
    "\n",
    "save_dir = '/home/jgryu/Weight_compression/Wparam_dataset/model_param_tensor'\n",
    "\n",
    "for ckpt_path in ckpt_path_list:\n",
    "    model_name = ckpt_path.split('/')[-3]\n",
    "    print(model_name)\n",
    "    model_name = model_name.split('--')\n",
    "    \n",
    "    # save_path = os.path.join(save_dir, model_name[1], model_name[2])\n",
    "\n",
    "    # if os.path.isdir(save_path) and bool(os.listdir(save_path)):\n",
    "    #     print('### Skip ###')\n",
    "    #     continue\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True, trust_remote_code=True)\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "        for k, v in state_dict.items():\n",
    "            np.save(os.path.join(save_path, k.replace(\".\", \"-\")), v)\n",
    "            print(k, v.shape)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Fail load model from {ckpt_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
