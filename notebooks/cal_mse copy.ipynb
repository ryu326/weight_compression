{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "import sys, os, json, math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import re\n",
    "\n",
    "std = 0.012528747320175171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1942763/1966618227.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  layer_inputs = torch.load('../Wparam_dataset/calib_data/layer_inputs_channelwise_mag.pt')\n"
     ]
    }
   ],
   "source": [
    "class LayerInputs:\n",
    "    def __init__(self, num_layers):\n",
    "        self.layers = [\n",
    "            {\n",
    "                \"self_attn\": {\n",
    "                    \"q_proj\": None,\n",
    "                    \"k_proj\": None,\n",
    "                    \"v_proj\": None,\n",
    "                    \"o_proj\": None,\n",
    "                },\n",
    "                \"mlp\": {\n",
    "                    \"gate_proj\": None,\n",
    "                    \"up_proj\": None,\n",
    "                    \"down_proj\": None,\n",
    "                },\n",
    "            }\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "layer_inputs = torch.load('../Wparam_dataset/calib_data/layer_inputs_channelwise_mag.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_corr(state_dict, recon_state_dict):\n",
    "    \n",
    "    pearson = []\n",
    "    spearman = []\n",
    "    \n",
    "    for k, v in state_dict.items():\n",
    "        if 'mlp' not in k and 'self_attn' not in k: continue\n",
    "        match = re.search(r\"layers\\.(\\d+).\", k)\n",
    "        if match:\n",
    "            layer_index = int(match.group(1))  # 찾은 숫자를 정수형으로 변환\n",
    "        \n",
    "        # if layer_index not in [0, 10, 20, 31]: continue\n",
    "        if 'self_attn' in k:\n",
    "            ltype_str = 'self_attn'\n",
    "        elif 'mlp' in k:\n",
    "            ltype_str = 'mlp' \n",
    "        if 'q_proj' in k:\n",
    "            wtype = 'q_proj'\n",
    "        elif 'k_proj' in k:\n",
    "            wtype = 'k_proj'\n",
    "        elif 'v_proj' in k:\n",
    "            wtype = 'v_proj'\n",
    "        elif 'o_proj' in k:\n",
    "            wtype = 'o_proj'\n",
    "        elif 'gate_proj' in k:\n",
    "            wtype = 'gate_proj'\n",
    "        elif 'up_proj' in k:\n",
    "            wtype = 'up_proj'\n",
    "        elif 'down_proj' in k:\n",
    "            wtype = 'down_proj'\n",
    "        \n",
    "        input_scale =  layer_inputs.layers[layer_index][ltype_str][wtype]\n",
    "        \n",
    "        mse = ((recon_state_dict[k] - v)**2).mean(0)\n",
    "\n",
    "        pearson_corr, _ = pearsonr(input_scale, mse)\n",
    "        spearman_corr, _ = spearmanr(input_scale, mse)\n",
    "        \n",
    "        # print(k, pearson_corr)\n",
    "        pearson.append(pearson_corr)\n",
    "        spearman.append(spearman_corr)\n",
    "        # mutual_info = mutual_info_score(None, np.histogram2d(input_scale.numpy(), mse.numpy(), bins=1000)[0].flatten())\n",
    "        # print(f\"{k}, {mutual_info:.3f}\")\n",
    "\n",
    "        # plt.figure(figsize=(4, 3))\n",
    "        # plt.scatter(input_scale, mse, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "        # plt.title(k)\n",
    "        # plt.xlabel('input_scale')\n",
    "        # plt.ylabel('mse')\n",
    "        # plt.xscale('log')\n",
    "        # plt.yscale('log')\n",
    "        # plt.grid(True)\n",
    "        # plt.show()\n",
    "    print(np.array(pearson).mean())\n",
    "    print(np.array(spearman).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpps = [2, 3, 4, 5, 6, 7, 8, 9, 10, 12]\n",
    "# bpps = [4]\n",
    "mses = []\n",
    "mse_fn = nn.MSELoss()\n",
    "for bpp in bpps:\n",
    "    ckpt_path = f'../model_cache_reconstructed/awq/llama3-8b-my-w{bpp}-g128-fake-quantized'\n",
    "    recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    recon_state_dict = recon_net.state_dict()\n",
    "    \n",
    "    print(bpp)\n",
    "    cal_corr(state_dict, recon_state_dict)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\n",
    "-0.029420970685099492\n",
    "-0.09500712973978678\n",
    "3\n",
    "0.14973438032635197\n",
    "-0.2311222337514005\n",
    "4\n",
    "0.21332432331943263\n",
    "-0.3178309782803166\n",
    "5\n",
    "0.23209445225211087\n",
    "-0.33430709057249974\n",
    "6\n",
    "0.24620023228092477\n",
    "-0.36772141005864556\n",
    "7\n",
    "0.23775588807491824\n",
    "-0.3499329517380149\n",
    "8\n",
    "0.255313750082977\n",
    "-0.33740311855527744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jgryu/Weight_compression/model_reconstructed/vqvae_qlike/row_16_calib/bpp6.0_size16_nmse_ne64_de1_K6_P16_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100_MSE_0.00101', '/home/jgryu/Weight_compression/model_reconstructed/vqvae_qlike/row_16_calib/bpp8.0_size16_nmse_ne256_de1_K8_P16_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100_MSE_9e-05', '/home/jgryu/Weight_compression/model_reconstructed/vqvae_qlike/row_16_calib/bpp4.0_size16_nmse_ne16_de1_K4_P16_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100_MSE_0.01228']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jgryu/Weight_compression/model_reconstructed/vqvae_qlike/row_16_calib/bpp6.0_size16_nmse_ne64_de1_K6_P16_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100_MSE_0.00101\n",
      "0.07648009591966119\n",
      "0.004637024256094685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jgryu/Weight_compression/model_reconstructed/vqvae_qlike/row_16_calib/bpp8.0_size16_nmse_ne256_de1_K8_P16_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100_MSE_9e-05\n",
      "0.06675589994613879\n",
      "0.013480944218589292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jgryu/Weight_compression/model_reconstructed/vqvae_qlike/row_16_calib/bpp4.0_size16_nmse_ne16_de1_K4_P16_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100_MSE_0.01228\n",
      "0.06871469711724709\n",
      "-0.01805949488144771\n"
     ]
    }
   ],
   "source": [
    "root_dir = '/home/jgryu/Weight_compression/model_cache_reconstructed/vqvae_idx/row_v2/per_row_16_calib'\n",
    "root_dir = '/home/jgryu/Weight_compression/model_cache_reconstructed/vqvae_idx/col/per_col_16_calib'\n",
    "root_dir = '/home/jgryu/Weight_compression/model_cache_reconstructed/vqvae_idx/col_random_idx/per_col_16_calib'\n",
    "root_dir = '/home/jgryu/Weight_compression/model_reconstructed/vqvae_qlike/row_16_calib'\n",
    "import glob\n",
    "# ckpt_paths = glob.glob(os.path.join(root_dir, \"**/*th.tar\"), recursive=True)\n",
    "ckpt_paths = glob.glob(os.path.join(root_dir, \"**/bpp*\"), recursive=True)\n",
    "ckpt_path_list = []\n",
    "\n",
    "for ck in ckpt_paths:\n",
    "    if 'bpp3.' in ck: continue\n",
    "    if 'bpp5.' in ck: continue\n",
    "    if 'result.' in ck: continue\n",
    "    ckpt_path_list.append(ck)\n",
    "\n",
    "print(ckpt_path_list)\n",
    "mse_fn = nn.MSELoss()\n",
    "for ckpt_path in ckpt_path_list:\n",
    "    recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "    print(ckpt_path)\n",
    "    cal_corr(state_dict, recon_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## row idx\n",
    "3 smse\n",
    "0.012930385150270552\n",
    "-0.002914573436993903\n",
    "\n",
    "3 nmse\n",
    "0.0996818714499644\n",
    "0.014991141838406658\n",
    "\n",
    "4 smse\n",
    "0.0489207773622126\n",
    "-0.0018312189708702376\n",
    "\n",
    "4 nmse\n",
    "0.05967029839169057\n",
    "-0.008844155775352877\n",
    "\n",
    "6 smse\n",
    "0.017747773141605385\n",
    "-0.0035077291401593483\n",
    "\n",
    "6 nmse\n",
    "\n",
    "8 smse\n",
    "0.02382473622823794\n",
    "-0.009273916552225536\n",
    "\n",
    "8 nmse\n",
    "0.06857445852456583\n",
    "-0.00032242275318888756"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# col idx\n",
    "3 smse\n",
    "0.08260417737577232\n",
    "-0.023754363656675832\n",
    "\n",
    "4 smse\n",
    "0.0927952513919582\n",
    "-0.020731512135732315\n",
    "\n",
    "6 smse\n",
    "0.11646683411423621\n",
    "-0.008781474896219648\n",
    "\n",
    "8 smse\n",
    "0.10627975621443728\n",
    "-0.014214981441471876"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# col random idx\n",
    "3 smse\n",
    "0.08264684718664947\n",
    "-0.0238141508414145\n",
    "\n",
    "4 smse\n",
    "0.09280146198547408\n",
    "-0.02074162417035556\n",
    "\n",
    "6 smse\n",
    "0.11753815427956073\n",
    "-0.011298514783288735\n",
    "\n",
    "8 smse\n",
    "0.10688536223638802\n",
    "-0.013171601392293137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQVAE qlike\n",
    "3\n",
    "0.07324685930719574\n",
    "-0.002356598737998506\n",
    "\n",
    "4\n",
    "0.06871469711724709\n",
    "-0.01805949488144771\n",
    "\n",
    "5\n",
    "0.08376904820523023\n",
    "0.0017911386220212917\n",
    "\n",
    "6\n",
    "0.07648009591966119\n",
    "0.004637024256094685\n",
    "\n",
    "8\n",
    "0.06675589994613879\n",
    "0.013480944218589292"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()\n",
    "\n",
    "ckpt_path = f'../model_cache_reconstructed/seedlm/bpp4.0_C8_P3_K16'\n",
    "recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "n = 0\n",
    "mse = 0\n",
    "for k, v in state_dict.items():\n",
    "    if 'mlp' not in k and 'attn' not in k: continue\n",
    "    \n",
    "    mse += ((recon_state_dict[k] - v)**2).sum()\n",
    "    n += v.numel()\n",
    "    \n",
    "mse = mse / n / std **2 \n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "cache_directory = \"../Wparam_dataset_v0/model_zoo/huggingface\" \n",
    "ckpt_path = latest_version_path(cache_directory, 'meta-llama/Meta-Llama-3-8B')\n",
    "net = LlamaForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "# ckpt_path = '/home/jgryu/Weight_compression/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "# net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "state_dict = net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0021007181346949905\n",
      "-0.009032166794796246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m코드를 실행할 수 없습니다. 세션이 삭제되었습니다. 커널을 다시 시작해 보세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "root_dir = '/home/jgryu/Weight_compression/model_reconstructed/rtn'\n",
    "import glob\n",
    "ckpt_paths = glob.glob(os.path.join(root_dir, \"**/*\"), recursive=True)\n",
    "ckpt_path_list = []\n",
    "\n",
    "for ck in ckpt_paths:\n",
    "    if 'result' in ck: continue\n",
    "    ckpt_path_list.append(ck)\n",
    "\n",
    "mses = []\n",
    "mse_fn = nn.MSELoss()\n",
    "for ckpt_path in ckpt_path_list:\n",
    "    try:\n",
    "        recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "        recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "        print(ckpt_path)\n",
    "        cal_corr(state_dict, recon_state_dict)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpps = [3, 4]\n",
    "mses = []\n",
    "mse_fn = nn.MSELoss()\n",
    "for bpp in bpps:\n",
    "    ckpt_path = f'../model_reconstructed/rtn/bpp{bpp}'\n",
    "    recon_net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    recon_state_dict = recon_net.state_dict()\n",
    "\n",
    "    n = 0\n",
    "    mse = 0\n",
    "    mse_layer = []\n",
    "    for k, v in state_dict.items():\n",
    "        # if 'mlp' not in k and 'attn' not in k: continue\n",
    "        \n",
    "        mse += ((recon_state_dict[k] - v)**2).sum()\n",
    "        n += v.numel()\n",
    "        print(k, mse_fn(recon_state_dict[k], v)/std**2)\n",
    "        mse_layer.append(mse_fn(recon_state_dict[k], v)/std**2)\n",
    "    mse = mse / n / std **2 \n",
    "    mses.append(mse.item())\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
