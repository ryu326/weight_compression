{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/nic/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    \n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "\n",
    "    return os.path.join(path, 'snapshots', revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 'model.safetensors.index.json' files with 'phi' in the path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "def count_files_with_name(directory, target_filename, keyword):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == target_filename and keyword in os.path.join(root, file).lower():\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "# 사용 예시\n",
    "directory_path = \"/home/jgryu/Weight_compression/Wparam_dataset/model_zoo/huggingface\"  # 대상 디렉토리 경로로 변경하세요.\n",
    "target_file = \"model.safetensors.index.json\"\n",
    "keyword = \"phi\"\n",
    "\n",
    "file_count = count_files_with_name(directory_path, target_file, keyword)\n",
    "print(f\"Found {file_count} '{target_file}' files with '{keyword}' in the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [17:42<00:00, 265.67s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/jgryu/miniconda3/envs/Wcomp/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AwqConfig, AutoModelForCausalLM\n",
    "\n",
    "# model_id = \"meta-llama/Llama-Guard-3-8B\"\n",
    "\n",
    "# quantization_config = AwqConfig(\n",
    "#     bits=4,\n",
    "#     fuse_max_seq_len=512,\n",
    "#     do_fuse=True,\n",
    "# )\n",
    "\n",
    "model_id = 'Efficient-ML/LLaMA-3-8B-AWQ-4bit-b128'\n",
    "# model = AutoModelForCausalLM.from_pretrained('Efficient-ML/LLaMA-3-8B-AWQ-4bit-b128', trust_remote)\n",
    "model_awq = AutoModelForCausalLM.from_pretrained(model_id, token=\"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\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = model.state_dict()\n",
    "for k, v in sd.items():\n",
    "    print(k)\n",
    "    print(v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_ckpt_path(path, branch = 'main'):\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "model_list = [\n",
    "              '/home/jgryu/Weight_compression/Wparam_dataset/model_zoo/huggingface/models--meta-llama--Meta-Llama-3-8B',\n",
    "              ]\n",
    "for model_path in model_list:\n",
    "    ckpt_path = get_ckpt_path(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_weights(state_dict, bits=4):\n",
    "    \"\"\"\n",
    "    Perform simple weight quantization on a model's state_dict.\n",
    "    \n",
    "    Args:\n",
    "        state_dict (dict): The state_dict of the model.\n",
    "        bits (int): Number of bits for quantization (default: 8).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Quantized state_dict.\n",
    "    \"\"\"\n",
    "    quantized_state_dict = {}\n",
    "    scale = 2 ** (bits - 1) - 1  # Scale factor for quantization\n",
    "    \n",
    "    for name, param in state_dict.items():\n",
    "        if param.dtype in [torch.float32, torch.float64]:  # Quantize only float weights\n",
    "            max_val = param.abs().max()\n",
    "            scale_factor = scale / max_val\n",
    "            quantized = (param * scale_factor).round().clamp(-scale, scale)  # Quantization\n",
    "            quantized_state_dict[name] = quantized / scale_factor  # Dequantization for storage\n",
    "        else:\n",
    "            quantized_state_dict[name] = param  # Keep non-float parameters as is\n",
    "    \n",
    "    return quantized_state_dict\n",
    "\n",
    "model_q = quantize_weights(fp_model.state_dict())\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Assume state_dict is available\n",
    "#     model_state_dict = {\n",
    "#         \"linear.weight\": torch.randn(4, 4),\n",
    "#         \"linear.bias\": torch.randn(4),\n",
    "#     }\n",
    "    \n",
    "#     quantized_state_dict = quantize_weights(model_state_dict)\n",
    "#     for k, v in quantized_state_dict.items():\n",
    "#         print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def quantize_weights_with_zero_point(state_dict, bits=4):\n",
    "    \"\"\"\n",
    "    Perform asymmetric weight quantization on a model's state_dict.\n",
    "\n",
    "    Args:\n",
    "        state_dict (dict): The state_dict of the model.\n",
    "        bits (int): Number of bits for quantization (default: 4).\n",
    "\n",
    "    Returns:\n",
    "        dict: Quantized state_dict with scale and zero-point.\n",
    "    \"\"\"\n",
    "    quantized_state_dict = {}\n",
    "    scale_and_zero_point = {}  # To store scale and zero-point for each parameter\n",
    "\n",
    "    qmin = 0\n",
    "    qmax = 2 ** bits - 1  # Range for quantized values\n",
    "\n",
    "    for name, param in state_dict.items():\n",
    "        if param.dtype in [torch.float32, torch.float64]:  # Quantize only float weights\n",
    "            min_val = param.min()\n",
    "            max_val = param.max()\n",
    "\n",
    "            # Calculate scale and zero-point\n",
    "            scale = (max_val - min_val) / (qmax - qmin)\n",
    "            zero_point = torch.round(qmin - min_val / scale)\n",
    "\n",
    "            # Quantize the parameter\n",
    "            quantized = torch.round(param / scale + zero_point).clamp(qmin, qmax)\n",
    "\n",
    "            # Dequantization for storage\n",
    "            dequantized = scale * (quantized - zero_point)\n",
    "            quantized_state_dict[name] = dequantized\n",
    "\n",
    "            # Save scale and zero-point for reference\n",
    "            scale_and_zero_point[name] = {\n",
    "                \"scale\": scale,\n",
    "                \"zero_point\": zero_point\n",
    "            }\n",
    "        else:\n",
    "            quantized_state_dict[name] = param  # Keep non-float parameters as is\n",
    "\n",
    "    return quantized_state_dict, scale_and_zero_point\n",
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example state_dict with random weights\n",
    "#     model_state_dict = {\n",
    "#         \"linear.weight\": torch.randn(4, 4),\n",
    "#         \"linear.bias\": torch.randn(4),\n",
    "#     }\n",
    "\n",
    "#     quantized_state_dict, scale_and_zero_point = quantize_weights_with_zero_point(model_state_dict)\n",
    "\n",
    "#     # Print results\n",
    "#     for name, quantized_param in quantized_state_dict.items():\n",
    "#         print(f\"{name}: {quantized_param}\")\n",
    "\n",
    "#     print(\"\\nScale and Zero-Point:\")\n",
    "#     for name, values in scale_and_zero_point.items():\n",
    "#         print(f\"{name}: Scale={values['scale']}, Zero-Point={values['zero_point']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.6436466818640247\n",
      "2 1.6236762742152016\n",
      "3 1.2921279783070763\n",
      "4 0.6426828348204263\n",
      "5 0.22821856959557643\n",
      "6 0.0611583548993061\n",
      "7 0.01520839248038775\n",
      "8 0.003791649803076995\n",
      "mse:  [1.6436466818640247, 1.6236762742152016, 1.2921279783070763, 0.6426828348204263, 0.22821856959557643, 0.0611583548993061, 0.01520839248038775, 0.003791649803076995]\n",
      "bits:  [1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 두 모델의 state_dict 가져오기\n",
    "# model1_state_dict = model.state_dict()\n",
    "model1_state_dict = model.state_dict()\n",
    "\n",
    "bits_list = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "mse_list = []\n",
    "for bits in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "    quantized_state_dict, scale_and_zero_point = quantize_weights_with_zero_point(model.state_dict(), bits = bits)\n",
    "    model2_state_dict = quantized_state_dict\n",
    "\n",
    "\n",
    "    total_squared_error = 0.0\n",
    "    total_elements = 0\n",
    "\n",
    "    for key in model1_state_dict.keys():\n",
    "        if 'attn' not in key: continue\n",
    "        # print(key)\n",
    "        if key not in model2_state_dict:\n",
    "            print(f\"Key '{key}' is missing in model2.\")\n",
    "        else:\n",
    "            # Shape 비교\n",
    "            if model1_state_dict[key].shape != model2_state_dict[key].shape:\n",
    "                print(f\"Shape mismatch for key '{key}': model1={model1_state_dict[key].shape}, model2={model2_state_dict[key].shape}\")\n",
    "            else:\n",
    "                # 전체 MSE를 위해 값 차이 누적\n",
    "                diff = model1_state_dict[key] - model2_state_dict[key]\n",
    "                squared_error = torch.sum(diff ** 2).item()  # 각 요소의 제곱 오차 합\n",
    "                total_squared_error += squared_error\n",
    "                total_elements += diff.numel()  # 전체 요소 수 추가\n",
    "\n",
    "    # 전체 요소 MSE 계산 및 출력\n",
    "    if total_elements > 0:\n",
    "        overall_mse = total_squared_error / total_elements\n",
    "        # print(f\"Total MSE over all elements: {overall_mse:.6f}\")\n",
    "    else:\n",
    "        pass\n",
    "        # print(\"No elements to compare. Models may be incompatible.\")\n",
    "\n",
    "    for d in [16]:\n",
    "        std = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_train_std.npy')\n",
    "        # print(std)\n",
    "    overall_mse = overall_mse / std**2\n",
    "    mse_list.append(overall_mse)\n",
    "    print(bits, overall_mse)\n",
    "print('mse: ', mse_list)\n",
    "print('bits: ', bits_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### per tensor quantization MLP\n",
    "* asymetric (4, 0.7952778297410023)\n",
    "mse:  [1.0000164869236037, 0.9993737145706393, 0.9887189679460173, 0.7952778297410023, 0.37116162976546385, 0.10550744880290548, 0.02616760658136518, 0.006492661767538381]\n",
    "bits:  [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "### per tensor quantization Attn\n",
    "* asymetric (4, 0.6426828348204263) (3, 1.2921279783070763)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 로드 성공!\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "path = '/home/jgryu/Weight_compression/awq_cache/llama3-8b-w4-g128.pt'\n",
    "try:\n",
    "    model_weights = torch.load(path, map_location='cpu')  # CPU에 로드 (필요시 GPU로 변경)\n",
    "    print(\"파일 로드 성공!\")\n",
    "    print(type(model_weights))  # 데이터 구조 확인 (예: dict, Tensor 등)\n",
    "except Exception as e:\n",
    "    print(f\"파일 로드 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_weights, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# state_dict 확인\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(value,\u001b[38;5;250m \u001b[39mtorch\u001b[38;5;241m.\u001b[39mTensor)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "if isinstance(model_weights, dict):\n",
    "    # state_dict 확인\n",
    "    for key, value in model_weights['clip'].items():\n",
    "        print(f\"{key}: {value.shape if isinstance(value, torch.Tensor) else type(value)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1338],\n",
       "         [0.1393],\n",
       "         [0.1440],\n",
       "         ...,\n",
       "         [0.2927],\n",
       "         [0.2462],\n",
       "         [0.2717]],\n",
       "\n",
       "        [[0.2030],\n",
       "         [0.1896],\n",
       "         [0.1676],\n",
       "         ...,\n",
       "         [0.2498],\n",
       "         [0.1958],\n",
       "         [0.2107]],\n",
       "\n",
       "        [[0.1211],\n",
       "         [0.1073],\n",
       "         [0.1519],\n",
       "         ...,\n",
       "         [0.1411],\n",
       "         [0.1211],\n",
       "         [0.1555]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.2306],\n",
       "         [0.2010],\n",
       "         [0.1537],\n",
       "         ...,\n",
       "         [0.2822],\n",
       "         [0.1294],\n",
       "         [0.1471]],\n",
       "\n",
       "        [[0.2267],\n",
       "         [0.1895],\n",
       "         [0.1153],\n",
       "         ...,\n",
       "         [0.2335],\n",
       "         [0.2355],\n",
       "         [0.2172]],\n",
       "\n",
       "        [[0.1915],\n",
       "         [0.1089],\n",
       "         [0.1740],\n",
       "         ...,\n",
       "         [0.1744],\n",
       "         [0.1779],\n",
       "         [0.2307]]], dtype=torch.float16)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights['clip'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/Wcomp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.53s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jgryu/Weight_compression/llm-awq/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m net \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(ckpt_path, local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m meam \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_mean.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_std.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# cache_directory = \"/home/jgryu/Weight_compression/llm-awq/model_cache\" \n",
    "# ver = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# net = AutoModelForCausalLM.from_pretrained(ver, cache_dir = cache_directory, token=\"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\", trust_remote_code=True)\n",
    "# tok = AutoTokenizer.from_pretrained(ver, cache_dir = cache_directory, token=\"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\", trust_remote_code=True)\n",
    "\n",
    "ckpt_path = '/home/jgryu/Weight_compression/llm-awq/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "meam = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_mean.npy')\n",
    "std = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_std.npy')\n",
    "size = 256\n",
    "weight_condition = 'mlp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers              4.44.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 16 ##\n",
      "0.013787343 0.013866982 0.013697659 4.6187477e-05\n",
      "## 32 ##\n",
      "0.014501593 0.014622698 0.014366101 6.392919e-05\n",
      "## 64 ##\n",
      "0.014888803 0.015070886 0.014723518 8.179265e-05\n",
      "## 128 ##\n",
      "0.015046192 0.015470855 0.014769139 0.00013238109\n",
      "## 256 ##\n",
      "0.015112722 0.015859565 0.014786505 0.00019158793\n",
      "## 1024 ##\n",
      "0.015144215 0.01755336 0.014522944 0.00037536578\n",
      "## 4096 ##\n",
      "0.015128609 0.021753341 0.013272276 0.0007092323\n"
     ]
    }
   ],
   "source": [
    "for d in [16, 32, 64, 128, 256, 1024, 4096]:\n",
    "    path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/attn/d{d}/attn_d{d}_train_std_vector.npy'\n",
    "    std = np.load(path)\n",
    "    print(f'## {d} ##')\n",
    "    print(std.mean(), std.max(), std.min(), std.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 64 ##\n",
      "0.010743368 0.010794666 0.010696257 1.7599377e-05\n",
      "## 128 ##\n",
      "0.011308035 0.011375209 0.0112355985 2.1920741e-05\n",
      "## 256 ##\n",
      "0.011604545 0.011706344 0.011459176 3.3088007e-05\n",
      "## 1024 ##\n",
      "0.011814561 0.012050536 0.011308623 7.070596e-05\n",
      "## 4096 ##\n",
      "0.011811152 0.012583601 0.0095969755 0.00014643215\n"
     ]
    }
   ],
   "source": [
    "for d in [16, 32, 64, 128, 256, 1024, 4096]:\n",
    "    try:\n",
    "        try:\n",
    "            path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_train_std_channel.npy'\n",
    "            std = np.load(path)\n",
    "        except:\n",
    "            try:\n",
    "                path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_train_std_vector.npy'\n",
    "                std = np.load(path)\n",
    "            except:\n",
    "                try:\n",
    "                    path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_val_std_channel.npy'\n",
    "                    std = np.load(path)\n",
    "                except:\n",
    "                    path  = f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d{d}/mlp_d{d}_val_std_vector.npy'\n",
    "                    std = np.load(path) \n",
    "        print(f'## {d} ##')\n",
    "    except:\n",
    "        continue\n",
    "    print(std.mean(), std.max(), std.min(), std.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
