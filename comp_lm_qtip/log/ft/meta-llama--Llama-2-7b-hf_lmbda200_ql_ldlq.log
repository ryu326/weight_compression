I0314 02:52:46.659843 2343506 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.50it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 10.05it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.90it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.31it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.10it/s]
I0314 02:52:48.450250 2343506 quantize_finetune_llama.py:142] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:19,  1.57it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:19,  1.56it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:18,  1.56it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:17,  1.56it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:17,  1.56it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:16,  1.55it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:16,  1.55it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:15,  1.55it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:05<00:14,  1.55it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:06<00:14,  1.55it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:07<00:13,  1.55it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:07<00:12,  1.55it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:08<00:12,  1.55it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:09<00:11,  1.54it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:09<00:11,  1.54it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:10<00:10,  1.54it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:10<00:09,  1.55it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:11<00:08,  1.56it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:12<00:08,  1.58it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:12<00:07,  1.59it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:13<00:06,  1.59it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:14<00:06,  1.59it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:14<00:05,  1.59it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:15<00:05,  1.60it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:15<00:04,  1.60it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:16<00:03,  1.59it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:17<00:03,  1.60it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:17<00:02,  1.60it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:18<00:01,  1.60it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:19<00:01,  1.61it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:19<00:00,  1.61it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:20<00:00,  1.61it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:20<00:00,  1.58it/s]
I0314 02:53:17.394043 2343506 quantize_finetune_llama.py:167] loaded compression model
I0314 02:53:30.859477 2343506 quantize_finetune_llama.py:171] loaded dataset and devset
I0314 02:53:35.990474 2343506 quantize_finetune_llama.py:191] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 02:54:52.222228 2343506 quantize_finetune_llama.py:218] computed original embedding for layer 0 in 76.1167643070221s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 02:55:16.330756 2345163 config.py:54] PyTorch version 2.1.1 available.
I0314 02:55:17.258202 2343506 quantize_finetune_llama.py:191] layer 1 gpu 1
I0314 02:56:25.791825 2343506 quantize_finetune_llama.py:218] computed original embedding for layer 1 in 68.37329912185669s
I0314 02:56:37.595866 2345956 config.py:54] PyTorch version 2.1.1 available.
I0314 02:56:38.538674 2343506 quantize_finetune_llama.py:191] layer 2 gpu 2
I0314 02:57:50.254447 2343506 quantize_finetune_llama.py:218] computed original embedding for layer 2 in 71.53886413574219s
I0314 02:57:57.544622 2346741 config.py:54] PyTorch version 2.1.1 available.
I0314 02:57:58.494483 2343506 quantize_finetune_llama.py:191] layer 3 gpu 3
