I0313 06:34:18.884844 2090717 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.72it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 10.14it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.77it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.26it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.08it/s]
I0313 06:34:20.700519 2090717 quantize_finetune_llama.py:135] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.44it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:20,  1.44it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:20,  1.44it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:19,  1.44it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:18,  1.44it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:18,  1.44it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:17,  1.44it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:16,  1.44it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:16,  1.43it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:06<00:15,  1.43it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:07<00:14,  1.43it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:13,  1.43it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:13,  1.43it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:09<00:12,  1.43it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:10<00:11,  1.43it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:11,  1.43it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:11<00:10,  1.43it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:12<00:09,  1.44it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:08,  1.47it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:13<00:08,  1.48it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:14<00:07,  1.50it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:06,  1.46it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:15<00:06,  1.42it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:16<00:05,  1.42it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:04,  1.42it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.43it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:18<00:03,  1.43it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:19<00:02,  1.44it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:02,  1.42it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:20<00:01,  1.43it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:21<00:00,  1.43it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.44it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.44it/s]
I0313 06:34:52.810873 2090717 quantize_finetune_llama.py:160] loaded compression model
I0313 06:35:07.208379 2090717 quantize_finetune_llama.py:164] loaded dataset and devset
I0313 06:35:12.952648 2090717 quantize_finetune_llama.py:184] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 06:36:22.729644 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 0 in 69.66232204437256s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0313 06:36:42.827664 2092127 config.py:54] PyTorch version 2.1.1 available.
I0313 06:36:43.906794 2090717 quantize_finetune_llama.py:184] layer 1 gpu 1
I0313 06:36:43.972888 2092127 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 06:37:01.185993 2092127 finetune.py:45] layer 0_v initial loss 1.4812351878390473e-07
I0313 06:37:32.964684 2092127 finetune.py:68] layer 0_v @ epoch 0 new loss 6.127645235665113e-08 old loss 1.4812351878390473e-07 BETTER
I0313 06:37:53.840874 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 1 in 69.80683922767639s
I0313 06:38:02.274563 2092954 config.py:54] PyTorch version 2.1.1 available.
I0313 06:38:03.323241 2090717 quantize_finetune_llama.py:184] layer 2 gpu 2
I0313 06:38:03.396567 2092954 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 06:38:05.959244 2092127 finetune.py:68] layer 0_v @ epoch 1 new loss 3.860156994051067e-08 old loss 6.127645235665113e-08 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 06:38:20.194635 2092954 finetune.py:45] layer 1_v initial loss 8.21965113573242e-06
I0313 06:38:39.531379 2092127 finetune.py:68] layer 0_v @ epoch 2 new loss 3.169339279907035e-08 old loss 3.860156994051067e-08 BETTER
I0313 06:38:51.135802 2092954 finetune.py:68] layer 1_v @ epoch 0 new loss 4.1779894672799855e-06 old loss 8.21965113573242e-06 BETTER
I0313 06:39:13.418991 2092127 finetune.py:68] layer 0_v @ epoch 3 new loss 2.8626319092950325e-08 old loss 3.169339279907035e-08 BETTER
I0313 06:39:15.545560 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 2 in 72.07946109771729s
I0313 06:39:24.760426 2092954 finetune.py:76] layer 1_v @ epoch 1 new loss 4.219215043121949e-06 old loss 4.1779894672799855e-06 WORSE
I0313 06:39:26.129567 2093823 config.py:54] PyTorch version 2.1.1 available.
I0313 06:39:27.148927 2090717 quantize_finetune_llama.py:184] layer 3 gpu 3
I0313 06:39:27.213042 2093823 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 06:39:43.992357 2093823 finetune.py:45] layer 2_v initial loss 1.111453639168758e-05
I0313 06:39:47.570554 2092127 finetune.py:68] layer 0_v @ epoch 4 new loss 2.69512376860348e-08 old loss 2.8626319092950325e-08 BETTER
I0313 06:39:56.490961 2092954 finetune.py:76] layer 1_v @ epoch 2 new loss 4.4030407480022404e-06 old loss 4.1779894672799855e-06 WORSE
I0313 06:40:05.403398 2092127 finetune.py:45] layer 0_q initial loss 2.7420794523891345e-08
I0313 06:40:15.434981 2093823 finetune.py:68] layer 2_v @ epoch 0 new loss 6.2559970501752105e-06 old loss 1.111453639168758e-05 BETTER
I0313 06:40:28.425783 2092954 finetune.py:68] layer 1_v @ epoch 3 new loss 3.088857738475781e-06 old loss 4.1779894672799855e-06 BETTER
I0313 06:40:37.995624 2092127 finetune.py:68] layer 0_q @ epoch 0 new loss 2.592153158786914e-08 old loss 2.7420794523891345e-08 BETTER
I0313 06:40:42.468147 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 3 in 75.1467604637146s
I0313 06:40:51.257170 2093823 finetune.py:68] layer 2_v @ epoch 1 new loss 4.3339809963072184e-06 old loss 6.2559970501752105e-06 BETTER
I0313 06:40:53.473232 2094706 config.py:54] PyTorch version 2.1.1 available.
I0313 06:40:54.528655 2090717 quantize_finetune_llama.py:184] layer 4 gpu 0
I0313 06:40:54.608334 2094706 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 06:41:01.175254 2092954 finetune.py:76] layer 1_v @ epoch 4 new loss 3.2959567306534154e-06 old loss 3.088857738475781e-06 WORSE
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 06:41:11.444773 2094706 finetune.py:45] layer 3_v initial loss 2.4698681954760104e-05
I0313 06:41:11.547018 2092127 finetune.py:68] layer 0_q @ epoch 1 new loss 2.498568107967003e-08 old loss 2.592153158786914e-08 BETTER
I0313 06:41:18.442202 2092954 finetune.py:45] layer 1_q initial loss 5.65026311960537e-06
I0313 06:41:23.735226 2093823 finetune.py:68] layer 2_v @ epoch 2 new loss 3.508490863168845e-06 old loss 4.3339809963072184e-06 BETTER
I0313 06:41:42.029623 2094706 finetune.py:68] layer 3_v @ epoch 0 new loss 1.1161237125634216e-05 old loss 2.4698681954760104e-05 BETTER
I0313 06:41:45.150368 2092127 finetune.py:68] layer 0_q @ epoch 2 new loss 2.42560389551727e-08 old loss 2.498568107967003e-08 BETTER
I0313 06:41:49.819335 2092954 finetune.py:68] layer 1_q @ epoch 0 new loss 3.708974190885783e-06 old loss 5.65026311960537e-06 BETTER
I0313 06:41:56.246413 2093823 finetune.py:68] layer 2_v @ epoch 3 new loss 3.130065806544735e-06 old loss 3.508490863168845e-06 BETTER
I0313 06:42:13.998331 2094706 finetune.py:68] layer 3_v @ epoch 1 new loss 7.68008339946391e-06 old loss 1.1161237125634216e-05 BETTER
I0313 06:42:18.796105 2092127 finetune.py:68] layer 0_q @ epoch 3 new loss 2.3685696959319102e-08 old loss 2.42560389551727e-08 BETTER
I0313 06:42:21.851735 2092954 finetune.py:76] layer 1_q @ epoch 1 new loss 3.807740085903788e-06 old loss 3.708974190885783e-06 WORSE
I0313 06:42:29.038829 2093823 finetune.py:68] layer 2_v @ epoch 4 new loss 2.9458349217748037e-06 old loss 3.130065806544735e-06 BETTER
I0313 06:42:45.913465 2094706 finetune.py:68] layer 3_v @ epoch 2 new loss 6.651583589700749e-06 old loss 7.68008339946391e-06 BETTER
I0313 06:42:46.730041 2093823 finetune.py:45] layer 2_q initial loss 3.1631375350116286e-06
I0313 06:42:52.735945 2092127 finetune.py:68] layer 0_q @ epoch 4 new loss 2.3191132569877482e-08 old loss 2.3685696959319102e-08 BETTER
I0313 06:42:53.262119 2092954 finetune.py:68] layer 1_q @ epoch 2 new loss 2.9788166102662217e-06 old loss 3.708974190885783e-06 BETTER
I0313 06:43:10.226754 2092127 finetune.py:45] layer 0_k initial loss 2.4593227010427654e-08
I0313 06:43:18.160444 2093823 finetune.py:68] layer 2_q @ epoch 0 new loss 3.0293097097455757e-06 old loss 3.1631375350116286e-06 BETTER
I0313 06:43:18.244333 2094706 finetune.py:68] layer 3_v @ epoch 3 new loss 6.263363047764869e-06 old loss 6.651583589700749e-06 BETTER
I0313 06:43:25.531869 2092954 finetune.py:76] layer 1_q @ epoch 3 new loss 5.417520696937572e-06 old loss 2.9788166102662217e-06 WORSE
I0313 06:43:42.593754 2092127 finetune.py:68] layer 0_k @ epoch 0 new loss 2.3349230104940943e-08 old loss 2.4593227010427654e-08 BETTER
I0313 06:43:50.214488 2093823 finetune.py:68] layer 2_q @ epoch 1 new loss 2.9673028620891273e-06 old loss 3.0293097097455757e-06 BETTER
I0313 06:43:50.432495 2094706 finetune.py:68] layer 3_v @ epoch 4 new loss 6.070470135455253e-06 old loss 6.263363047764869e-06 BETTER
I0313 06:43:56.943089 2092954 finetune.py:76] layer 1_q @ epoch 4 new loss 3.431075583648635e-06 old loss 2.9788166102662217e-06 WORSE
I0313 06:44:07.827670 2094706 finetune.py:45] layer 3_q initial loss 7.125865977286594e-06
I0313 06:44:14.134997 2092954 finetune.py:45] layer 1_k initial loss 3.944164291169727e-06
I0313 06:44:16.010827 2092127 finetune.py:68] layer 0_k @ epoch 1 new loss 2.2916450959087342e-08 old loss 2.3349230104940943e-08 BETTER
I0313 06:44:22.722612 2093823 finetune.py:68] layer 2_q @ epoch 2 new loss 2.926840579675627e-06 old loss 2.9673028620891273e-06 BETTER
I0313 06:44:38.969701 2094706 finetune.py:68] layer 3_q @ epoch 0 new loss 6.9056477514095604e-06 old loss 7.125865977286594e-06 BETTER
I0313 06:44:45.300419 2092954 finetune.py:76] layer 1_k @ epoch 0 new loss 4.2349133764219005e-06 old loss 3.944164291169727e-06 WORSE
I0313 06:44:49.460095 2092127 finetune.py:68] layer 0_k @ epoch 2 new loss 2.2540017852179517e-08 old loss 2.2916450959087342e-08 BETTER
I0313 06:44:55.134026 2093823 finetune.py:68] layer 2_q @ epoch 3 new loss 2.8958286293345736e-06 old loss 2.926840579675627e-06 BETTER
I0313 06:45:10.925905 2094706 finetune.py:68] layer 3_q @ epoch 1 new loss 6.786853191442788e-06 old loss 6.9056477514095604e-06 BETTER
I0313 06:45:16.717461 2092954 finetune.py:68] layer 1_k @ epoch 1 new loss 3.6691205878014443e-06 old loss 3.944164291169727e-06 BETTER
I0313 06:45:22.972132 2092127 finetune.py:68] layer 0_k @ epoch 3 new loss 2.225395689947618e-08 old loss 2.2540017852179517e-08 BETTER
I0313 06:45:27.784087 2093823 finetune.py:68] layer 2_q @ epoch 4 new loss 2.870136768251541e-06 old loss 2.8958286293345736e-06 BETTER
I0313 06:45:43.182247 2094706 finetune.py:68] layer 3_q @ epoch 2 new loss 6.698465767840389e-06 old loss 6.786853191442788e-06 BETTER
I0313 06:45:45.669232 2093823 finetune.py:45] layer 2_k initial loss 3.0806152153672883e-06
I0313 06:45:49.062434 2092954 finetune.py:68] layer 1_k @ epoch 2 new loss 3.400351033633342e-06 old loss 3.6691205878014443e-06 BETTER
I0313 06:45:56.533591 2092127 finetune.py:68] layer 0_k @ epoch 4 new loss 2.203439386505579e-08 old loss 2.225395689947618e-08 BETTER
I0313 06:46:14.507762 2092127 finetune.py:45] layer 0_o initial loss 1.4191265051977098e-07
I0313 06:46:15.491122 2094706 finetune.py:68] layer 3_q @ epoch 3 new loss 6.628845767409075e-06 old loss 6.698465767840389e-06 BETTER
I0313 06:46:17.166099 2093823 finetune.py:68] layer 2_k @ epoch 0 new loss 3.055448814848205e-06 old loss 3.0806152153672883e-06 BETTER
I0313 06:46:21.116863 2092954 finetune.py:68] layer 1_k @ epoch 3 new loss 3.2717518934077816e-06 old loss 3.400351033633342e-06 BETTER
I0313 06:46:46.265479 2092127 finetune.py:68] layer 0_o @ epoch 0 new loss 1.3640297424899472e-07 old loss 1.4191265051977098e-07 BETTER
I0313 06:46:47.648021 2094706 finetune.py:68] layer 3_q @ epoch 4 new loss 6.570598998223431e-06 old loss 6.628845767409075e-06 BETTER
I0313 06:46:49.475843 2093823 finetune.py:68] layer 2_k @ epoch 1 new loss 3.0351250188687118e-06 old loss 3.055448814848205e-06 BETTER
I0313 06:46:53.146915 2092954 finetune.py:76] layer 1_k @ epoch 4 new loss 4.415465809870511e-06 old loss 3.2717518934077816e-06 WORSE
I0313 06:47:05.531165 2094706 finetune.py:45] layer 3_k initial loss 7.402188202831894e-06
I0313 06:47:10.711731 2092954 finetune.py:45] layer 1_o initial loss 8.41593646327965e-06
I0313 06:47:19.157090 2092127 finetune.py:68] layer 0_o @ epoch 1 new loss 1.3264026677006768e-07 old loss 1.3640297424899472e-07 BETTER
I0313 06:47:21.607934 2093823 finetune.py:68] layer 2_k @ epoch 2 new loss 3.017022663698299e-06 old loss 3.0351250188687118e-06 BETTER
I0313 06:47:36.002927 2094706 finetune.py:68] layer 3_k @ epoch 0 new loss 7.324324087676359e-06 old loss 7.402188202831894e-06 BETTER
I0313 06:47:40.947895 2092954 finetune.py:68] layer 1_o @ epoch 0 new loss 7.136917702155188e-06 old loss 8.41593646327965e-06 BETTER
I0313 06:47:51.864846 2092127 finetune.py:68] layer 0_o @ epoch 2 new loss 1.297785701126486e-07 old loss 1.3264026677006768e-07 BETTER
I0313 06:47:53.740500 2093823 finetune.py:68] layer 2_k @ epoch 3 new loss 3.000978495037998e-06 old loss 3.017022663698299e-06 BETTER
I0313 06:48:07.335396 2094706 finetune.py:68] layer 3_k @ epoch 1 new loss 7.278011253220029e-06 old loss 7.324324087676359e-06 BETTER
I0313 06:48:12.163180 2092954 finetune.py:68] layer 1_o @ epoch 1 new loss 7.131103757274104e-06 old loss 7.136917702155188e-06 BETTER
I0313 06:48:24.640607 2092127 finetune.py:68] layer 0_o @ epoch 3 new loss 1.2757055856127408e-07 old loss 1.297785701126486e-07 BETTER
I0313 06:48:25.827691 2093823 finetune.py:68] layer 2_k @ epoch 4 new loss 2.9862185328966007e-06 old loss 3.000978495037998e-06 BETTER
I0313 06:48:38.822040 2094706 finetune.py:68] layer 3_k @ epoch 2 new loss 7.238820671773283e-06 old loss 7.278011253220029e-06 BETTER
I0313 06:48:43.332505 2092954 finetune.py:76] layer 1_o @ epoch 2 new loss 7.176744929893175e-06 old loss 7.131103757274104e-06 WORSE
I0313 06:48:43.878067 2093823 finetune.py:45] layer 2_o initial loss 9.954042980098166e-06
I0313 06:48:57.367640 2092127 finetune.py:68] layer 0_o @ epoch 4 new loss 1.2585840636347712e-07 old loss 1.2757055856127408e-07 BETTER
I0313 06:49:10.336486 2094706 finetune.py:68] layer 3_k @ epoch 3 new loss 7.203841505543096e-06 old loss 7.238820671773283e-06 BETTER
I0313 06:49:14.003446 2092954 finetune.py:76] layer 1_o @ epoch 3 new loss 7.147528322093422e-06 old loss 7.131103757274104e-06 WORSE
I0313 06:49:14.271830 2093823 finetune.py:68] layer 2_o @ epoch 0 new loss 9.816492820391431e-06 old loss 9.954042980098166e-06 BETTER
I0313 06:49:20.430886 2092127 finetune.py:45] layer 0_up initial loss 2.251078541348761e-07
I0313 06:49:42.006461 2094706 finetune.py:68] layer 3_k @ epoch 4 new loss 7.1723375185683835e-06 old loss 7.203841505543096e-06 BETTER
I0313 06:49:44.792872 2092954 finetune.py:76] layer 1_o @ epoch 4 new loss 7.1320800998364575e-06 old loss 7.131103757274104e-06 WORSE
I0313 06:49:45.622967 2093823 finetune.py:68] layer 2_o @ epoch 1 new loss 9.724679330247454e-06 old loss 9.816492820391431e-06 BETTER
I0313 06:49:50.547178 2092127 finetune.py:68] layer 0_up @ epoch 0 new loss 2.161410463941138e-07 old loss 2.251078541348761e-07 BETTER
I0313 06:50:00.069763 2094706 finetune.py:45] layer 3_o initial loss 1.9865152353304438e-05
I0313 06:50:07.703885 2092954 finetune.py:45] layer 1_up initial loss 8.679352504259441e-06
I0313 06:50:17.093611 2093823 finetune.py:68] layer 2_o @ epoch 2 new loss 9.652416338212788e-06 old loss 9.724679330247454e-06 BETTER
I0313 06:50:21.893343 2092127 finetune.py:68] layer 0_up @ epoch 1 new loss 2.13099752954804e-07 old loss 2.161410463941138e-07 BETTER
I0313 06:50:30.040279 2094706 finetune.py:68] layer 3_o @ epoch 0 new loss 1.9480023183859885e-05 old loss 1.9865152353304438e-05 BETTER
I0313 06:50:36.659547 2092954 finetune.py:68] layer 1_up @ epoch 0 new loss 8.383446584048215e-06 old loss 8.679352504259441e-06 BETTER
I0313 06:50:48.744552 2093823 finetune.py:68] layer 2_o @ epoch 3 new loss 9.593321919965092e-06 old loss 9.652416338212788e-06 BETTER
I0313 06:50:53.254061 2092127 finetune.py:68] layer 0_up @ epoch 2 new loss 2.114215220672122e-07 old loss 2.13099752954804e-07 BETTER
I0313 06:51:00.882824 2094706 finetune.py:68] layer 3_o @ epoch 1 new loss 1.925454671436455e-05 old loss 1.9480023183859885e-05 BETTER
I0313 06:51:06.208925 2092954 finetune.py:68] layer 1_up @ epoch 1 new loss 8.314839760714676e-06 old loss 8.383446584048215e-06 BETTER
I0313 06:51:20.555160 2093823 finetune.py:68] layer 2_o @ epoch 4 new loss 9.543887244944926e-06 old loss 9.593321919965092e-06 BETTER
I0313 06:51:24.859038 2092127 finetune.py:68] layer 0_up @ epoch 3 new loss 2.10279665679991e-07 old loss 2.114215220672122e-07 BETTER
I0313 06:51:31.889168 2094706 finetune.py:68] layer 3_o @ epoch 2 new loss 1.9105318642687052e-05 old loss 1.925454671436455e-05 BETTER
I0313 06:51:35.800048 2092954 finetune.py:68] layer 1_up @ epoch 2 new loss 8.307360985781997e-06 old loss 8.314839760714676e-06 BETTER
I0313 06:51:43.956425 2093823 finetune.py:45] layer 2_up initial loss 1.4010547602083534e-05
I0313 06:51:56.390044 2092127 finetune.py:68] layer 0_up @ epoch 4 new loss 2.094160862498029e-07 old loss 2.10279665679991e-07 BETTER
I0313 06:52:02.745413 2094706 finetune.py:68] layer 3_o @ epoch 3 new loss 1.89937672985252e-05 old loss 1.9105318642687052e-05 BETTER
I0313 06:52:05.529920 2092954 finetune.py:68] layer 1_up @ epoch 3 new loss 8.30390490591526e-06 old loss 8.307360985781997e-06 BETTER
I0313 06:52:12.857390 2093823 finetune.py:68] layer 2_up @ epoch 0 new loss 1.39477915581665e-05 old loss 1.4010547602083534e-05 BETTER
I0313 06:52:19.438194 2092127 finetune.py:45] layer 0_gate initial loss 3.057928950056521e-07
I0313 06:52:33.694225 2094706 finetune.py:68] layer 3_o @ epoch 4 new loss 1.8904811440734193e-05 old loss 1.89937672985252e-05 BETTER
I0313 06:52:35.261879 2092954 finetune.py:68] layer 1_up @ epoch 4 new loss 8.266531949630007e-06 old loss 8.30390490591526e-06 BETTER
I0313 06:52:42.629013 2093823 finetune.py:68] layer 2_up @ epoch 1 new loss 1.3905626474297605e-05 old loss 1.39477915581665e-05 BETTER
I0313 06:52:48.271775 2092127 finetune.py:68] layer 0_gate @ epoch 0 new loss 2.9537355317188485e-07 old loss 3.057928950056521e-07 BETTER
I0313 06:52:56.699116 2094706 finetune.py:45] layer 3_up initial loss 3.018779170815833e-05
I0313 06:52:58.349842 2092954 finetune.py:45] layer 1_gate initial loss 9.299133125750814e-06
I0313 06:53:12.319739 2093823 finetune.py:68] layer 2_up @ epoch 2 new loss 1.3871346709493082e-05 old loss 1.3905626474297605e-05 BETTER
I0313 06:53:18.014030 2092127 finetune.py:68] layer 0_gate @ epoch 1 new loss 2.917661277024308e-07 old loss 2.9537355317188485e-07 BETTER
I0313 06:53:25.180446 2094706 finetune.py:68] layer 3_up @ epoch 0 new loss 3.0043578590266407e-05 old loss 3.018779170815833e-05 BETTER
I0313 06:53:25.804399 2092954 finetune.py:68] layer 1_gate @ epoch 0 new loss 9.097323527385015e-06 old loss 9.299133125750814e-06 BETTER
I0313 06:53:42.170852 2093823 finetune.py:68] layer 2_up @ epoch 3 new loss 1.3841000509273726e-05 old loss 1.3871346709493082e-05 BETTER
I0313 06:53:47.710421 2092127 finetune.py:68] layer 0_gate @ epoch 2 new loss 2.898694049235928e-07 old loss 2.917661277024308e-07 BETTER
I0313 06:53:53.996386 2092954 finetune.py:68] layer 1_gate @ epoch 1 new loss 9.055073860508855e-06 old loss 9.097323527385015e-06 BETTER
I0313 06:53:54.494346 2094706 finetune.py:68] layer 3_up @ epoch 1 new loss 2.9951788746984676e-05 old loss 3.0043578590266407e-05 BETTER
I0313 06:54:12.222078 2093823 finetune.py:68] layer 2_up @ epoch 4 new loss 1.3813641999149695e-05 old loss 1.3841000509273726e-05 BETTER
I0313 06:54:17.445088 2092127 finetune.py:68] layer 0_gate @ epoch 3 new loss 2.886447134642367e-07 old loss 2.898694049235928e-07 BETTER
I0313 06:54:22.307907 2092954 finetune.py:68] layer 1_gate @ epoch 2 new loss 9.010786016006023e-06 old loss 9.055073860508855e-06 BETTER
I0313 06:54:23.900243 2094706 finetune.py:68] layer 3_up @ epoch 2 new loss 2.9876669941586442e-05 old loss 2.9951788746984676e-05 BETTER
I0313 06:54:35.224648 2093823 finetune.py:45] layer 2_gate initial loss 1.7007760106935166e-05
I0313 06:54:47.282414 2092127 finetune.py:68] layer 0_gate @ epoch 4 new loss 2.8770500648533925e-07 old loss 2.886447134642367e-07 BETTER
I0313 06:54:50.519243 2092954 finetune.py:68] layer 1_gate @ epoch 3 new loss 9.001093530969229e-06 old loss 9.010786016006023e-06 BETTER
I0313 06:54:53.291419 2094706 finetune.py:68] layer 3_up @ epoch 3 new loss 2.980879071401432e-05 old loss 2.9876669941586442e-05 BETTER
I0313 06:55:02.763750 2093823 finetune.py:68] layer 2_gate @ epoch 0 new loss 1.695459650363773e-05 old loss 1.7007760106935166e-05 BETTER
I0313 06:55:18.689931 2092954 finetune.py:76] layer 1_gate @ epoch 4 new loss 9.00168379303068e-06 old loss 9.001093530969229e-06 WORSE
I0313 06:55:22.723068 2094706 finetune.py:68] layer 3_up @ epoch 4 new loss 2.9746526706730947e-05 old loss 2.980879071401432e-05 BETTER
I0313 06:55:27.937011 2092127 finetune.py:45] layer 0_down initial loss 7.434167628161958e-07
I0313 06:55:30.835926 2093823 finetune.py:68] layer 2_gate @ epoch 1 new loss 1.6923833754844964e-05 old loss 1.695459650363773e-05 BETTER
I0313 06:55:45.409099 2094706 finetune.py:45] layer 3_gate initial loss 3.793219366343692e-05
I0313 06:55:54.391182 2092127 finetune.py:68] layer 0_down @ epoch 0 new loss 7.425529133797681e-07 old loss 7.434167628161958e-07 BETTER
I0313 06:55:59.168019 2093823 finetune.py:68] layer 2_gate @ epoch 2 new loss 1.689975215413142e-05 old loss 1.6923833754844964e-05 BETTER
I0313 06:56:00.369667 2092954 finetune.py:45] layer 1_down initial loss 1.4522610399581026e-05
I0313 06:56:12.484435 2094706 finetune.py:68] layer 3_gate @ epoch 0 new loss 3.78122967958916e-05 old loss 3.793219366343692e-05 BETTER
I0313 06:56:22.246345 2092127 finetune.py:68] layer 0_down @ epoch 1 new loss 7.421813847940939e-07 old loss 7.425529133797681e-07 BETTER
I0313 06:56:25.539131 2092954 finetune.py:68] layer 1_down @ epoch 0 new loss 1.4514637769025285e-05 old loss 1.4522610399581026e-05 BETTER
I0313 06:56:27.339352 2093823 finetune.py:68] layer 2_gate @ epoch 3 new loss 1.687900657998398e-05 old loss 1.689975215413142e-05 BETTER
I0313 06:56:40.198855 2094706 finetune.py:68] layer 3_gate @ epoch 1 new loss 3.77353499061428e-05 old loss 3.78122967958916e-05 BETTER
I0313 06:56:50.582696 2092127 finetune.py:68] layer 0_down @ epoch 2 new loss 7.418954055538052e-07 old loss 7.421813847940939e-07 BETTER
I0313 06:56:51.817253 2092954 finetune.py:68] layer 1_down @ epoch 1 new loss 1.4373246813192964e-05 old loss 1.4514637769025285e-05 BETTER
I0313 06:56:55.501023 2093823 finetune.py:68] layer 2_gate @ epoch 4 new loss 1.6860245523275807e-05 old loss 1.687900657998398e-05 BETTER
I0313 06:57:08.171040 2094706 finetune.py:68] layer 3_gate @ epoch 2 new loss 3.767563248402439e-05 old loss 3.77353499061428e-05 BETTER
I0313 06:57:18.612412 2092954 finetune.py:68] layer 1_down @ epoch 2 new loss 1.4342685062729288e-05 old loss 1.4373246813192964e-05 BETTER
I0313 06:57:19.000247 2092127 finetune.py:68] layer 0_down @ epoch 3 new loss 7.416857670250465e-07 old loss 7.418954055538052e-07 BETTER
I0313 06:57:36.141202 2094706 finetune.py:68] layer 3_gate @ epoch 3 new loss 3.762429696507752e-05 old loss 3.767563248402439e-05 BETTER
I0313 06:57:36.273050 2093823 finetune.py:45] layer 2_down initial loss 2.6814130251295865e-05
I0313 06:57:45.224164 2092954 finetune.py:76] layer 1_down @ epoch 3 new loss 1.437395258108154e-05 old loss 1.4342685062729288e-05 WORSE
I0313 06:57:47.366242 2092127 finetune.py:68] layer 0_down @ epoch 4 new loss 7.41549570193456e-07 old loss 7.416857670250465e-07 BETTER
0_v proxy err 0.0049193985760211945 tr(WHW.T) 4.225186347961426
0_q proxy err 1.0331964404031169e-05 tr(WHW.T) 2710.42529296875
0_k proxy err 1.5422630895045586e-05 tr(WHW.T) 1698.7462158203125
0_o proxy err 0.0005083662108518183 tr(WHW.T) 0.9680441617965698
0_up proxy err 0.005082752555608749 tr(WHW.T) 43.272708892822266
0_gate proxy err 0.003476179903373122 tr(WHW.T) 63.46289825439453
0_down proxy err 0.003050530329346657 tr(WHW.T) 0.6570964455604553
I0313 06:58:01.994666 2093823 finetune.py:68] layer 2_down @ epoch 0 new loss 2.681019395822659e-05 old loss 2.6814130251295865e-05 BETTER
I0313 06:58:04.501717 2094706 finetune.py:68] layer 3_gate @ epoch 4 new loss 3.757722879527137e-05 old loss 3.762429696507752e-05 BETTER
I0313 06:58:11.695674 2092954 finetune.py:76] layer 1_down @ epoch 4 new loss 1.4345376257551834e-05 old loss 1.4342685062729288e-05 WORSE
1_v proxy err 0.013161279261112213 tr(WHW.T) 16.465883255004883
1_q proxy err 5.3227453463478014e-05 tr(WHW.T) 4778.29833984375
1_k proxy err 5.201610110816546e-05 tr(WHW.T) 4994.8623046875
1_o proxy err 0.009414760395884514 tr(WHW.T) 1.1120301485061646
1_up proxy err 0.007194385398179293 tr(WHW.T) 109.70372772216797
1_gate proxy err 0.003614411922171712 tr(WHW.T) 221.32684326171875
1_down proxy err 9.109471284318715e-05 tr(WHW.T) 2041.6270751953125
I0313 06:58:28.998819 2093823 finetune.py:68] layer 2_down @ epoch 1 new loss 2.6808429538505152e-05 old loss 2.681019395822659e-05 BETTER
I0313 06:58:45.195427 2094706 finetune.py:45] layer 3_down initial loss 5.8474866818869486e-05
I0313 06:58:55.633097 2093823 finetune.py:68] layer 2_down @ epoch 2 new loss 2.6807338144863024e-05 old loss 2.6808429538505152e-05 BETTER
I0313 06:59:09.990684 2094706 finetune.py:68] layer 3_down @ epoch 0 new loss 5.8466339396545663e-05 old loss 5.8474866818869486e-05 BETTER
I0313 06:59:22.244488 2093823 finetune.py:68] layer 2_down @ epoch 3 new loss 2.6806546884472482e-05 old loss 2.6807338144863024e-05 BETTER
I0313 06:59:28.556771 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 4 in 72.37934398651123s
I0313 06:59:31.745970 2104946 config.py:54] PyTorch version 2.1.1 available.
I0313 06:59:32.921940 2090717 quantize_finetune_llama.py:184] layer 5 gpu 1
I0313 06:59:33.001554 2104946 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 06:59:36.077667 2094706 finetune.py:68] layer 3_down @ epoch 1 new loss 5.846190470037982e-05 old loss 5.8466339396545663e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 06:59:48.976479 2093823 finetune.py:68] layer 2_down @ epoch 4 new loss 2.6805944798979908e-05 old loss 2.6806546884472482e-05 BETTER
I0313 06:59:49.763326 2104946 finetune.py:45] layer 4_v initial loss 3.311934051453136e-05
2_v proxy err 0.008486867882311344 tr(WHW.T) 136.67332458496094
2_q proxy err 0.00017021424719132483 tr(WHW.T) 7750.087890625
2_k proxy err 0.00013036229938734323 tr(WHW.T) 10203.947265625
2_o proxy err 0.014235914684832096 tr(WHW.T) 1.461812973022461
2_up proxy err 0.010333461686968803 tr(WHW.T) 193.42652893066406
2_gate proxy err 0.006629396695643663 tr(WHW.T) 306.5737609863281
2_down proxy err 0.01370998565107584 tr(WHW.T) 3.0139577388763428
I0313 07:00:02.127936 2094706 finetune.py:68] layer 3_down @ epoch 2 new loss 5.845933628734201e-05 old loss 5.846190470037982e-05 BETTER
I0313 07:00:22.486441 2104946 finetune.py:68] layer 4_v @ epoch 0 new loss 1.4519504475174472e-05 old loss 3.311934051453136e-05 BETTER
I0313 07:00:28.165619 2094706 finetune.py:68] layer 3_down @ epoch 3 new loss 5.845748819410801e-05 old loss 5.845933628734201e-05 BETTER
I0313 07:00:54.378973 2094706 finetune.py:68] layer 3_down @ epoch 4 new loss 5.845623309141956e-05 old loss 5.845748819410801e-05 BETTER
3_v proxy err 0.014254020527005196 tr(WHW.T) 284.77557373046875
3_q proxy err 0.0006405014428310096 tr(WHW.T) 7215.8994140625
3_k proxy err 0.0004627686575986445 tr(WHW.T) 10073.9169921875
3_o proxy err 0.01229079719632864 tr(WHW.T) 3.3588037490844727
3_up proxy err 0.01290204282850027 tr(WHW.T) 284.7816467285156
3_gate proxy err 0.007826696150004864 tr(WHW.T) 478.0036926269531
3_down proxy err 0.014315620064735413 tr(WHW.T) 6.144412994384766
I0313 07:00:56.402616 2104946 finetune.py:68] layer 4_v @ epoch 1 new loss 1.1128824553452432e-05 old loss 1.4519504475174472e-05 BETTER
I0313 07:01:03.604011 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 5 in 69.3573784828186s
I0313 07:01:06.703812 2105897 config.py:54] PyTorch version 2.1.1 available.
I0313 07:01:07.707985 2090717 quantize_finetune_llama.py:184] layer 6 gpu 2
I0313 07:01:07.772398 2105897 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:01:24.660830 2105897 finetune.py:45] layer 5_v initial loss 4.858460306422785e-05
I0313 07:01:30.909198 2104946 finetune.py:68] layer 4_v @ epoch 2 new loss 1.0198570635111537e-05 old loss 1.1128824553452432e-05 BETTER
I0313 07:01:55.760994 2105897 finetune.py:68] layer 5_v @ epoch 0 new loss 2.4074348402791657e-05 old loss 4.858460306422785e-05 BETTER
I0313 07:02:05.623676 2104946 finetune.py:68] layer 4_v @ epoch 3 new loss 9.778497769730166e-06 old loss 1.0198570635111537e-05 BETTER
I0313 07:02:17.648160 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 6 in 69.54529905319214s
I0313 07:02:20.765086 2106700 config.py:54] PyTorch version 2.1.1 available.
I0313 07:02:21.787710 2090717 quantize_finetune_llama.py:184] layer 7 gpu 3
I0313 07:02:21.856731 2106700 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 07:02:27.883712 2105897 finetune.py:68] layer 5_v @ epoch 1 new loss 2.044379107246641e-05 old loss 2.4074348402791657e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:02:38.709414 2106700 finetune.py:45] layer 6_v initial loss 7.160089444369078e-05
I0313 07:02:40.414910 2104946 finetune.py:68] layer 4_v @ epoch 4 new loss 9.532190233585425e-06 old loss 9.778497769730166e-06 BETTER
I0313 07:02:57.898969 2104946 finetune.py:45] layer 4_q initial loss 1.1130235179734882e-05
I0313 07:03:00.243830 2105897 finetune.py:68] layer 5_v @ epoch 2 new loss 1.9329403585288674e-05 old loss 2.044379107246641e-05 BETTER
I0313 07:03:10.089659 2106700 finetune.py:68] layer 6_v @ epoch 0 new loss 3.167392060277052e-05 old loss 7.160089444369078e-05 BETTER
I0313 07:03:30.767714 2104946 finetune.py:68] layer 4_q @ epoch 0 new loss 1.0817538168339524e-05 old loss 1.1130235179734882e-05 BETTER
I0313 07:03:31.548215 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 7 in 69.31210446357727s
I0313 07:03:32.735843 2105897 finetune.py:68] layer 5_v @ epoch 3 new loss 1.8740005543804727e-05 old loss 1.9329403585288674e-05 BETTER
I0313 07:03:34.738292 2107465 config.py:54] PyTorch version 2.1.1 available.
I0313 07:03:35.781825 2090717 quantize_finetune_llama.py:184] layer 8 gpu 0
I0313 07:03:35.849216 2107465 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 07:03:42.400823 2106700 finetune.py:68] layer 6_v @ epoch 1 new loss 2.699353535717819e-05 old loss 3.167392060277052e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:03:52.512167 2107465 finetune.py:45] layer 7_v initial loss 8.63952882355079e-05
I0313 07:04:04.682112 2104946 finetune.py:68] layer 4_q @ epoch 1 new loss 1.0636863407853525e-05 old loss 1.0817538168339524e-05 BETTER
I0313 07:04:05.464754 2105897 finetune.py:68] layer 5_v @ epoch 4 new loss 1.835017610574141e-05 old loss 1.8740005543804727e-05 BETTER
I0313 07:04:14.971138 2106700 finetune.py:68] layer 6_v @ epoch 2 new loss 2.5414776246179827e-05 old loss 2.699353535717819e-05 BETTER
I0313 07:04:22.866784 2105897 finetune.py:45] layer 5_q initial loss 2.0770936316694133e-05
I0313 07:04:23.254189 2107465 finetune.py:68] layer 7_v @ epoch 0 new loss 4.289781645638868e-05 old loss 8.63952882355079e-05 BETTER
I0313 07:04:38.847324 2104946 finetune.py:68] layer 4_q @ epoch 2 new loss 1.0497879884496797e-05 old loss 1.0636863407853525e-05 BETTER
I0313 07:04:47.441540 2106700 finetune.py:68] layer 6_v @ epoch 3 new loss 2.4580596800660715e-05 old loss 2.5414776246179827e-05 BETTER
I0313 07:04:54.308063 2105897 finetune.py:68] layer 5_q @ epoch 0 new loss 2.0282201148802415e-05 old loss 2.0770936316694133e-05 BETTER
I0313 07:04:55.173539 2107465 finetune.py:68] layer 7_v @ epoch 1 new loss 3.812598151853308e-05 old loss 4.289781645638868e-05 BETTER
I0313 07:05:12.958334 2104946 finetune.py:68] layer 4_q @ epoch 3 new loss 1.038447953760624e-05 old loss 1.0497879884496797e-05 BETTER
I0313 07:05:20.272222 2106700 finetune.py:68] layer 6_v @ epoch 4 new loss 2.4025192033150233e-05 old loss 2.4580596800660715e-05 BETTER
I0313 07:05:26.461170 2105897 finetune.py:68] layer 5_q @ epoch 1 new loss 1.99679871002445e-05 old loss 2.0282201148802415e-05 BETTER
I0313 07:05:27.012291 2107465 finetune.py:68] layer 7_v @ epoch 2 new loss 3.639504939201288e-05 old loss 3.812598151853308e-05 BETTER
I0313 07:05:37.764238 2106700 finetune.py:45] layer 6_q initial loss 2.929418405983597e-05
I0313 07:05:47.273410 2104946 finetune.py:68] layer 4_q @ epoch 4 new loss 1.0288913472322747e-05 old loss 1.038447953760624e-05 BETTER
I0313 07:05:58.649417 2105897 finetune.py:68] layer 5_q @ epoch 2 new loss 1.972631253011059e-05 old loss 1.99679871002445e-05 BETTER
I0313 07:05:59.196039 2107465 finetune.py:68] layer 7_v @ epoch 3 new loss 3.539323734003119e-05 old loss 3.639504939201288e-05 BETTER
I0313 07:06:04.839325 2104946 finetune.py:45] layer 4_k initial loss 1.1566805369511712e-05
I0313 07:06:09.177692 2106700 finetune.py:68] layer 6_q @ epoch 0 new loss 2.846845382009633e-05 old loss 2.929418405983597e-05 BETTER
I0313 07:06:30.954248 2105897 finetune.py:68] layer 5_q @ epoch 3 new loss 1.9523500668583438e-05 old loss 1.972631253011059e-05 BETTER
I0313 07:06:31.228876 2107465 finetune.py:68] layer 7_v @ epoch 4 new loss 3.471907621133141e-05 old loss 3.539323734003119e-05 BETTER
I0313 07:06:37.322082 2104946 finetune.py:68] layer 4_k @ epoch 0 new loss 1.142486962635303e-05 old loss 1.1566805369511712e-05 BETTER
I0313 07:06:41.428295 2106700 finetune.py:68] layer 6_q @ epoch 1 new loss 2.799499452521559e-05 old loss 2.846845382009633e-05 BETTER
I0313 07:06:48.781253 2107465 finetune.py:45] layer 7_q initial loss 4.2472282075323164e-05
I0313 07:07:03.269657 2105897 finetune.py:68] layer 5_q @ epoch 4 new loss 1.9344892280059867e-05 old loss 1.9523500668583438e-05 BETTER
I0313 07:07:10.343722 2104946 finetune.py:68] layer 4_k @ epoch 1 new loss 1.1342400284775067e-05 old loss 1.142486962635303e-05 BETTER
I0313 07:07:13.477820 2106700 finetune.py:68] layer 6_q @ epoch 2 new loss 2.7629552278085612e-05 old loss 2.799499452521559e-05 BETTER
I0313 07:07:19.778048 2107465 finetune.py:68] layer 7_q @ epoch 0 new loss 4.126613202970475e-05 old loss 4.2472282075323164e-05 BETTER
I0313 07:07:20.978009 2105897 finetune.py:45] layer 5_k initial loss 2.0811899958061986e-05
I0313 07:07:43.700735 2104946 finetune.py:68] layer 4_k @ epoch 2 new loss 1.1272740266576875e-05 old loss 1.1342400284775067e-05 BETTER
I0313 07:07:45.767556 2106700 finetune.py:68] layer 6_q @ epoch 3 new loss 2.7334119295119308e-05 old loss 2.7629552278085612e-05 BETTER
I0313 07:07:51.709182 2107465 finetune.py:68] layer 7_q @ epoch 1 new loss 4.059319326188415e-05 old loss 4.126613202970475e-05 BETTER
I0313 07:07:51.988735 2105897 finetune.py:68] layer 5_k @ epoch 0 new loss 2.0610468709492125e-05 old loss 2.0811899958061986e-05 BETTER
I0313 07:08:17.232017 2104946 finetune.py:68] layer 4_k @ epoch 3 new loss 1.1209849617443979e-05 old loss 1.1272740266576875e-05 BETTER
I0313 07:08:18.098286 2106700 finetune.py:68] layer 6_q @ epoch 4 new loss 2.7074607714894228e-05 old loss 2.7334119295119308e-05 BETTER
I0313 07:08:23.625437 2107465 finetune.py:68] layer 7_q @ epoch 2 new loss 4.0062706830212846e-05 old loss 4.059319326188415e-05 BETTER
I0313 07:08:23.843967 2105897 finetune.py:68] layer 5_k @ epoch 1 new loss 2.0464245608309284e-05 old loss 2.0610468709492125e-05 BETTER
I0313 07:08:36.068995 2106700 finetune.py:45] layer 6_k initial loss 3.056721106986515e-05
I0313 07:08:51.047562 2104946 finetune.py:68] layer 4_k @ epoch 4 new loss 1.1154196727147792e-05 old loss 1.1209849617443979e-05 BETTER
I0313 07:08:55.571298 2107465 finetune.py:68] layer 7_q @ epoch 3 new loss 3.9617701986571774e-05 old loss 4.0062706830212846e-05 BETTER
I0313 07:08:55.794993 2105897 finetune.py:68] layer 5_k @ epoch 2 new loss 2.0336517991381697e-05 old loss 2.0464245608309284e-05 BETTER
I0313 07:09:07.380309 2106700 finetune.py:68] layer 6_k @ epoch 0 new loss 3.0175277061061934e-05 old loss 3.056721106986515e-05 BETTER
I0313 07:09:09.284988 2104946 finetune.py:45] layer 4_o initial loss 3.1334184313891456e-05
I0313 07:09:27.582218 2107465 finetune.py:68] layer 7_q @ epoch 4 new loss 3.9244820072781295e-05 old loss 3.9617701986571774e-05 BETTER
I0313 07:09:27.786627 2105897 finetune.py:68] layer 5_k @ epoch 3 new loss 2.0223182218614966e-05 old loss 2.0336517991381697e-05 BETTER
I0313 07:09:39.592439 2106700 finetune.py:68] layer 6_k @ epoch 1 new loss 2.9970440664328635e-05 old loss 3.0175277061061934e-05 BETTER
I0313 07:09:41.152248 2104946 finetune.py:68] layer 4_o @ epoch 0 new loss 3.0497902116621844e-05 old loss 3.1334184313891456e-05 BETTER
I0313 07:09:45.610699 2107465 finetune.py:45] layer 7_k initial loss 4.462477591005154e-05
I0313 07:09:59.776387 2105897 finetune.py:68] layer 5_k @ epoch 4 new loss 2.01243474293733e-05 old loss 2.0223182218614966e-05 BETTER
I0313 07:10:11.641654 2106700 finetune.py:68] layer 6_k @ epoch 2 new loss 2.979478449560702e-05 old loss 2.9970440664328635e-05 BETTER
I0313 07:10:14.518169 2104946 finetune.py:68] layer 4_o @ epoch 1 new loss 3.0190887628123164e-05 old loss 3.0497902116621844e-05 BETTER
I0313 07:10:16.359783 2107465 finetune.py:68] layer 7_k @ epoch 0 new loss 4.400723264552653e-05 old loss 4.462477591005154e-05 BETTER
I0313 07:10:17.601639 2105897 finetune.py:45] layer 5_o initial loss 5.1055361836915836e-05
I0313 07:10:43.713223 2106700 finetune.py:68] layer 6_k @ epoch 3 new loss 2.9641496439580806e-05 old loss 2.979478449560702e-05 BETTER
I0313 07:10:48.100379 2105897 finetune.py:68] layer 5_o @ epoch 0 new loss 4.960090518579818e-05 old loss 5.1055361836915836e-05 BETTER
I0313 07:10:48.114248 2107465 finetune.py:68] layer 7_k @ epoch 1 new loss 4.368305599200539e-05 old loss 4.400723264552653e-05 BETTER
I0313 07:10:48.146805 2104946 finetune.py:68] layer 4_o @ epoch 2 new loss 2.999334355990868e-05 old loss 3.0190887628123164e-05 BETTER
I0313 07:11:16.023407 2106700 finetune.py:68] layer 6_k @ epoch 4 new loss 2.950897214759607e-05 old loss 2.9641496439580806e-05 BETTER
I0313 07:11:19.271727 2105897 finetune.py:68] layer 5_o @ epoch 1 new loss 4.905028617940843e-05 old loss 4.960090518579818e-05 BETTER
I0313 07:11:19.626835 2107465 finetune.py:68] layer 7_k @ epoch 2 new loss 4.341052772360854e-05 old loss 4.368305599200539e-05 BETTER
I0313 07:11:21.629553 2104946 finetune.py:68] layer 4_o @ epoch 3 new loss 2.983635749842506e-05 old loss 2.999334355990868e-05 BETTER
I0313 07:11:33.871269 2106700 finetune.py:45] layer 6_o initial loss 7.578005897812545e-05
I0313 07:11:52.211260 2105897 finetune.py:68] layer 5_o @ epoch 2 new loss 4.865611845161766e-05 old loss 4.905028617940843e-05 BETTER
I0313 07:11:53.010171 2107465 finetune.py:68] layer 7_k @ epoch 3 new loss 4.3183266825508326e-05 old loss 4.341052772360854e-05 BETTER
I0313 07:11:55.780832 2104946 finetune.py:68] layer 4_o @ epoch 4 new loss 2.969947126985062e-05 old loss 2.983635749842506e-05 BETTER
I0313 07:12:05.012885 2106700 finetune.py:68] layer 6_o @ epoch 0 new loss 7.342344906646758e-05 old loss 7.578005897812545e-05 BETTER
I0313 07:12:20.616073 2104946 finetune.py:45] layer 4_up initial loss 5.06279720866587e-05
I0313 07:12:25.184903 2105897 finetune.py:68] layer 5_o @ epoch 3 new loss 4.832330523640849e-05 old loss 4.865611845161766e-05 BETTER
I0313 07:12:25.947551 2107465 finetune.py:68] layer 7_k @ epoch 4 new loss 4.296952101867646e-05 old loss 4.3183266825508326e-05 BETTER
I0313 07:12:36.915462 2106700 finetune.py:68] layer 6_o @ epoch 1 new loss 7.269802881637588e-05 old loss 7.342344906646758e-05 BETTER
I0313 07:12:45.593348 2107465 finetune.py:45] layer 7_o initial loss 0.00010468484106240794
I0313 07:12:51.305727 2104946 finetune.py:68] layer 4_up @ epoch 0 new loss 5.0335507694398984e-05 old loss 5.06279720866587e-05 BETTER
I0313 07:12:57.070376 2105897 finetune.py:68] layer 5_o @ epoch 4 new loss 4.803636693395674e-05 old loss 4.832330523640849e-05 BETTER
I0313 07:13:08.951749 2106700 finetune.py:68] layer 6_o @ epoch 2 new loss 7.216525409603491e-05 old loss 7.269802881637588e-05 BETTER
I0313 07:13:16.690526 2107465 finetune.py:68] layer 7_o @ epoch 0 new loss 0.0001015798989101313 old loss 0.00010468484106240794 BETTER
I0313 07:13:23.739583 2104946 finetune.py:68] layer 4_up @ epoch 1 new loss 5.014294947613962e-05 old loss 5.0335507694398984e-05 BETTER
I0313 07:13:24.305813 2105897 finetune.py:45] layer 5_up initial loss 8.142376464093104e-05
I0313 07:13:40.878952 2106700 finetune.py:68] layer 6_o @ epoch 3 new loss 7.1719303377904e-05 old loss 7.216525409603491e-05 BETTER
I0313 07:13:47.674644 2107465 finetune.py:68] layer 7_o @ epoch 1 new loss 0.00010043672227766365 old loss 0.0001015798989101313 BETTER
I0313 07:13:53.218156 2105897 finetune.py:68] layer 5_up @ epoch 0 new loss 8.091210474958643e-05 old loss 8.142376464093104e-05 BETTER
I0313 07:13:55.579686 2104946 finetune.py:68] layer 4_up @ epoch 2 new loss 4.9984955694526434e-05 old loss 5.014294947613962e-05 BETTER
I0313 07:14:12.569222 2106700 finetune.py:68] layer 6_o @ epoch 4 new loss 7.132949394872412e-05 old loss 7.1719303377904e-05 BETTER
I0313 07:14:18.551186 2107465 finetune.py:68] layer 7_o @ epoch 2 new loss 9.957022120943293e-05 old loss 0.00010043672227766365 BETTER
I0313 07:14:22.793090 2105897 finetune.py:68] layer 5_up @ epoch 1 new loss 8.054451609496027e-05 old loss 8.091210474958643e-05 BETTER
I0313 07:14:27.378574 2104946 finetune.py:68] layer 4_up @ epoch 3 new loss 4.9841815780382603e-05 old loss 4.9984955694526434e-05 BETTER
I0313 07:14:35.852936 2106700 finetune.py:45] layer 6_up initial loss 0.00012338923988863826
I0313 07:14:49.374607 2107465 finetune.py:68] layer 7_o @ epoch 3 new loss 9.885335748549551e-05 old loss 9.957022120943293e-05 BETTER
I0313 07:14:52.473912 2105897 finetune.py:68] layer 5_up @ epoch 2 new loss 8.022367546800524e-05 old loss 8.054451609496027e-05 BETTER
I0313 07:14:59.121708 2104946 finetune.py:68] layer 4_up @ epoch 4 new loss 4.970779991708696e-05 old loss 4.9841815780382603e-05 BETTER
I0313 07:15:04.711037 2106700 finetune.py:68] layer 6_up @ epoch 0 new loss 0.00012250919826328754 old loss 0.00012338923988863826 BETTER
I0313 07:15:20.649097 2107465 finetune.py:68] layer 7_o @ epoch 4 new loss 9.821902494877577e-05 old loss 9.885335748549551e-05 BETTER
I0313 07:15:22.335966 2105897 finetune.py:68] layer 5_up @ epoch 3 new loss 7.992963219294325e-05 old loss 8.022367546800524e-05 BETTER
I0313 07:15:22.472614 2104946 finetune.py:45] layer 4_gate initial loss 6.33744421065785e-05
I0313 07:15:34.603789 2106700 finetune.py:68] layer 6_up @ epoch 1 new loss 0.00012190152483526617 old loss 0.00012250919826328754 BETTER
I0313 07:15:43.983528 2107465 finetune.py:45] layer 7_up initial loss 0.00016904853691812605
I0313 07:15:51.288600 2104946 finetune.py:68] layer 4_gate @ epoch 0 new loss 6.314494385151193e-05 old loss 6.33744421065785e-05 BETTER
I0313 07:15:52.167398 2105897 finetune.py:68] layer 5_up @ epoch 4 new loss 7.965976692503318e-05 old loss 7.992963219294325e-05 BETTER
I0313 07:16:04.476009 2106700 finetune.py:68] layer 6_up @ epoch 2 new loss 0.00012137051089666784 old loss 0.00012190152483526617 BETTER
I0313 07:16:12.296176 2107465 finetune.py:68] layer 7_up @ epoch 0 new loss 0.00016762512677814811 old loss 0.00016904853691812605 BETTER
I0313 07:16:15.270459 2105897 finetune.py:45] layer 5_gate initial loss 0.00010030044359154999
I0313 07:16:21.083164 2104946 finetune.py:68] layer 4_gate @ epoch 1 new loss 6.298339576460421e-05 old loss 6.314494385151193e-05 BETTER
I0313 07:16:34.324671 2106700 finetune.py:68] layer 6_up @ epoch 3 new loss 0.00012088543735444546 old loss 0.00012137051089666784 BETTER
I0313 07:16:41.587637 2107465 finetune.py:68] layer 7_up @ epoch 1 new loss 0.00016665252042002976 old loss 0.00016762512677814811 BETTER
I0313 07:16:42.629127 2105897 finetune.py:68] layer 5_gate @ epoch 0 new loss 9.990981925511733e-05 old loss 0.00010030044359154999 BETTER
I0313 07:16:51.158979 2104946 finetune.py:68] layer 4_gate @ epoch 2 new loss 6.285257404670119e-05 old loss 6.298339576460421e-05 BETTER
I0313 07:17:04.355497 2106700 finetune.py:68] layer 6_up @ epoch 4 new loss 0.00012043836613884196 old loss 0.00012088543735444546 BETTER
I0313 07:17:10.860541 2105897 finetune.py:68] layer 5_gate @ epoch 1 new loss 9.962564217858016e-05 old loss 9.990981925511733e-05 BETTER
I0313 07:17:10.998201 2107465 finetune.py:68] layer 7_up @ epoch 2 new loss 0.00016578884969931096 old loss 0.00016665252042002976 BETTER
I0313 07:17:21.040207 2104946 finetune.py:68] layer 4_gate @ epoch 3 new loss 6.273880717344582e-05 old loss 6.285257404670119e-05 BETTER
I0313 07:17:27.416453 2106700 finetune.py:45] layer 6_gate initial loss 0.0001498216442996636
I0313 07:17:39.012434 2105897 finetune.py:68] layer 5_gate @ epoch 2 new loss 9.938208677340299e-05 old loss 9.962564217858016e-05 BETTER
I0313 07:17:40.382607 2107465 finetune.py:68] layer 7_up @ epoch 3 new loss 0.00016500921628903598 old loss 0.00016578884969931096 BETTER
I0313 07:17:50.915801 2104946 finetune.py:68] layer 4_gate @ epoch 4 new loss 6.263500836212188e-05 old loss 6.273880717344582e-05 BETTER
I0313 07:17:54.898886 2106700 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.00014919154637027532 old loss 0.0001498216442996636 BETTER
I0313 07:18:07.264708 2105897 finetune.py:68] layer 5_gate @ epoch 3 new loss 9.916025737766176e-05 old loss 9.938208677340299e-05 BETTER
I0313 07:18:09.875942 2107465 finetune.py:68] layer 7_up @ epoch 4 new loss 0.00016429094830527902 old loss 0.00016500921628903598 BETTER
I0313 07:18:23.011715 2106700 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.00014872786414343864 old loss 0.00014919154637027532 BETTER
I0313 07:18:32.163318 2104946 finetune.py:45] layer 4_down initial loss 0.00010141215898329392
I0313 07:18:32.825105 2107465 finetune.py:45] layer 7_gate initial loss 0.0002037353697232902
I0313 07:18:35.514918 2105897 finetune.py:68] layer 5_gate @ epoch 4 new loss 9.895986295305192e-05 old loss 9.916025737766176e-05 BETTER
I0313 07:18:51.222126 2106700 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.00014832454326096922 old loss 0.00014872786414343864 BETTER
I0313 07:18:58.500089 2104946 finetune.py:68] layer 4_down @ epoch 0 new loss 0.00010140196536667645 old loss 0.00010141215898329392 BETTER
I0313 07:18:59.684041 2107465 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.00020278547890484333 old loss 0.0002037353697232902 BETTER
I0313 07:19:16.234320 2105897 finetune.py:45] layer 5_down initial loss 0.00015705898113083094
I0313 07:19:19.413775 2106700 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.00014796546020079404 old loss 0.00014832454326096922 BETTER
I0313 07:19:26.434051 2104946 finetune.py:68] layer 4_down @ epoch 1 new loss 0.00010139730875380337 old loss 0.00010140196536667645 BETTER
I0313 07:19:27.327908 2107465 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.00020209149806760252 old loss 0.00020278547890484333 BETTER
I0313 07:19:41.371950 2105897 finetune.py:68] layer 5_down @ epoch 0 new loss 0.0001570498279761523 old loss 0.00015705898113083094 BETTER
I0313 07:19:47.563189 2106700 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.0001476369216106832 old loss 0.00014796546020079404 BETTER
I0313 07:19:54.706657 2104946 finetune.py:68] layer 4_down @ epoch 2 new loss 0.00010139454388990998 old loss 0.00010139730875380337 BETTER
I0313 07:19:55.288149 2107465 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.00020147193572483957 old loss 0.00020209149806760252 BETTER
I0313 07:20:07.644135 2105897 finetune.py:68] layer 5_down @ epoch 1 new loss 0.00015704543329775333 old loss 0.0001570498279761523 BETTER
I0313 07:20:23.059332 2104946 finetune.py:68] layer 4_down @ epoch 3 new loss 0.00010139263031305745 old loss 0.00010139454388990998 BETTER
I0313 07:20:23.158201 2107465 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.0002009189483942464 old loss 0.00020147193572483957 BETTER
I0313 07:20:29.375044 2106700 finetune.py:45] layer 6_down initial loss 0.0002335721073905006
I0313 07:20:34.256515 2105897 finetune.py:68] layer 5_down @ epoch 2 new loss 0.00015704130055382848 old loss 0.00015704543329775333 BETTER
I0313 07:20:52.075322 2107465 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.00020041594689246267 old loss 0.0002009189483942464 BETTER
I0313 07:20:52.096326 2104946 finetune.py:68] layer 4_down @ epoch 4 new loss 0.00010139145160792395 old loss 0.00010139263031305745 BETTER
4_v proxy err 0.01366881188005209 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0006204353412613273 tr(WHW.T) 6915.90771484375
4_k proxy err 0.00041344313649460673 tr(WHW.T) 10415.826171875
4_o proxy err 0.014003020711243153 tr(WHW.T) 5.147854804992676
4_up proxy err 0.012642435729503632 tr(WHW.T) 397.5287780761719
4_gate proxy err 0.006276520900428295 tr(WHW.T) 820.599609375
4_down proxy err 0.014195534400641918 tr(WHW.T) 11.605305671691895
I0313 07:20:55.099653 2106700 finetune.py:68] layer 6_down @ epoch 0 new loss 0.0002335560420760885 old loss 0.0002335721073905006 BETTER
I0313 07:21:01.348766 2105897 finetune.py:68] layer 5_down @ epoch 3 new loss 0.0001570394670125097 old loss 0.00015704130055382848 BETTER
I0313 07:21:21.873878 2106700 finetune.py:68] layer 6_down @ epoch 1 new loss 0.00023354726727120578 old loss 0.0002335560420760885 BETTER
I0313 07:21:27.801382 2105897 finetune.py:68] layer 5_down @ epoch 4 new loss 0.0001570377207826823 old loss 0.0001570394670125097 BETTER
5_v proxy err 0.014689801260828972 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0007396740838885307 tr(WHW.T) 6772.81591796875
5_k proxy err 0.00046562650823034346 tr(WHW.T) 10841.9375
5_o proxy err 0.015171696431934834 tr(WHW.T) 7.9515061378479
5_up proxy err 0.012460821308195591 tr(WHW.T) 506.3958740234375
5_gate proxy err 0.005877407267689705 tr(WHW.T) 1101.950927734375
5_down proxy err 0.015547974035143852 tr(WHW.T) 15.712620735168457
I0313 07:21:33.768853 2107465 finetune.py:45] layer 7_down initial loss 0.0003144242218695581
I0313 07:21:48.486964 2106700 finetune.py:68] layer 6_down @ epoch 2 new loss 0.00023354213044513017 old loss 0.00023354726727120578 BETTER
I0313 07:21:58.666598 2107465 finetune.py:68] layer 7_down @ epoch 0 new loss 0.00031440018210560083 old loss 0.0003144242218695581 BETTER
I0313 07:22:15.219278 2106700 finetune.py:68] layer 6_down @ epoch 3 new loss 0.00023353764845523983 old loss 0.00023354213044513017 BETTER
I0313 07:22:24.315783 2107465 finetune.py:68] layer 7_down @ epoch 1 new loss 0.00031438819132745266 old loss 0.00031440018210560083 BETTER
I0313 07:22:42.504709 2106700 finetune.py:68] layer 6_down @ epoch 4 new loss 0.00023353500000666827 old loss 0.00023353764845523983 BETTER
I0313 07:22:44.014388 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 8 in 71.83328700065613s
6_v proxy err 0.01609533652663231 tr(WHW.T) 443.5464782714844
6_q proxy err 0.0010688415495678782 tr(WHW.T) 7578.064453125
6_k proxy err 0.0007802438922226429 tr(WHW.T) 10409.4833984375
6_o proxy err 0.017051000148057938 tr(WHW.T) 11.601139068603516
6_up proxy err 0.012666611932218075 tr(WHW.T) 617.0028686523438
6_gate proxy err 0.0051878271624445915 tr(WHW.T) 1553.6385498046875
6_down proxy err 0.015704788267612457 tr(WHW.T) 23.09180450439453
I0313 07:22:47.461031 2117858 config.py:54] PyTorch version 2.1.1 available.
I0313 07:22:48.614778 2090717 quantize_finetune_llama.py:184] layer 9 gpu 1
I0313 07:22:48.681576 2117858 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 07:22:50.408120 2107465 finetune.py:68] layer 7_down @ epoch 2 new loss 0.00031438120640814304 old loss 0.00031438819132745266 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:23:06.785936 2117858 finetune.py:45] layer 8_v initial loss 0.00011048027954529971
I0313 07:23:16.605943 2107465 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00031437669531442225 old loss 0.00031438120640814304 BETTER
I0313 07:23:40.522207 2117858 finetune.py:68] layer 8_v @ epoch 0 new loss 6.0621463489951566e-05 old loss 0.00011048027954529971 BETTER
I0313 07:23:43.158096 2107465 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0003143732901662588 old loss 0.00031437669531442225 BETTER
7_v proxy err 0.016091929748654366 tr(WHW.T) 489.9357604980469
7_q proxy err 0.0011609436478465796 tr(WHW.T) 7673.9716796875
7_k proxy err 0.000874500663485378 tr(WHW.T) 10200.0546875
7_o proxy err 0.018465610221028328 tr(WHW.T) 15.153830528259277
7_up proxy err 0.012294188141822815 tr(WHW.T) 735.8994750976562
7_gate proxy err 0.004969184752553701 tr(WHW.T) 1875.3121337890625
7_down proxy err 0.015690961852669716 tr(WHW.T) 30.718103408813477
I0313 07:24:01.361873 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 9 in 72.31500363349915s
I0313 07:24:04.424913 2118654 config.py:54] PyTorch version 2.1.1 available.
I0313 07:24:05.399568 2090717 quantize_finetune_llama.py:184] layer 10 gpu 2
I0313 07:24:05.468266 2118654 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 07:24:15.159244 2117858 finetune.py:68] layer 8_v @ epoch 1 new loss 5.490624243975617e-05 old loss 6.0621463489951566e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:24:23.154376 2118654 finetune.py:45] layer 9_v initial loss 9.955288987839594e-05
I0313 07:24:49.921702 2117858 finetune.py:68] layer 8_v @ epoch 2 new loss 5.266308653517626e-05 old loss 5.490624243975617e-05 BETTER
I0313 07:24:54.567242 2118654 finetune.py:68] layer 9_v @ epoch 0 new loss 7.225872104754671e-05 old loss 9.955288987839594e-05 BETTER
I0313 07:25:16.083857 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 10 in 70.26811695098877s
I0313 07:25:19.091445 2119442 config.py:54] PyTorch version 2.1.1 available.
I0313 07:25:20.066457 2090717 quantize_finetune_llama.py:184] layer 11 gpu 3
I0313 07:25:20.140586 2119442 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 07:25:25.088431 2117858 finetune.py:68] layer 8_v @ epoch 3 new loss 5.132170190336183e-05 old loss 5.266308653517626e-05 BETTER
I0313 07:25:27.100121 2118654 finetune.py:68] layer 9_v @ epoch 1 new loss 6.866482726763934e-05 old loss 7.225872104754671e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:25:36.864184 2119442 finetune.py:45] layer 10_v initial loss 0.00013054317969363183
I0313 07:25:59.855108 2118654 finetune.py:68] layer 9_v @ epoch 2 new loss 6.681613740511239e-05 old loss 6.866482726763934e-05 BETTER
I0313 07:26:00.266545 2117858 finetune.py:68] layer 8_v @ epoch 4 new loss 5.0393562560202554e-05 old loss 5.132170190336183e-05 BETTER
I0313 07:26:08.199548 2119442 finetune.py:68] layer 10_v @ epoch 0 new loss 0.00010367295908508822 old loss 0.00013054317969363183 BETTER
I0313 07:26:18.095681 2117858 finetune.py:45] layer 8_q initial loss 6.0272261180216447e-05
I0313 07:26:30.347284 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 11 in 69.8535463809967s
I0313 07:26:32.605372 2118654 finetune.py:68] layer 9_v @ epoch 3 new loss 6.554940773639828e-05 old loss 6.681613740511239e-05 BETTER
I0313 07:26:33.653962 2120229 config.py:54] PyTorch version 2.1.1 available.
I0313 07:26:34.728599 2090717 quantize_finetune_llama.py:184] layer 12 gpu 0
I0313 07:26:34.804997 2120229 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 07:26:40.471433 2119442 finetune.py:68] layer 10_v @ epoch 1 new loss 9.912133100442588e-05 old loss 0.00010367295908508822 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:26:51.552240 2117858 finetune.py:68] layer 8_q @ epoch 0 new loss 5.873573900316842e-05 old loss 6.0272261180216447e-05 BETTER
I0313 07:26:51.647009 2120229 finetune.py:45] layer 11_v initial loss 0.00014017036301083863
I0313 07:27:05.653670 2118654 finetune.py:68] layer 9_v @ epoch 4 new loss 6.456804112531245e-05 old loss 6.554940773639828e-05 BETTER
I0313 07:27:13.326029 2119442 finetune.py:68] layer 10_v @ epoch 2 new loss 9.638049232307822e-05 old loss 9.912133100442588e-05 BETTER
I0313 07:27:22.482425 2120229 finetune.py:68] layer 11_v @ epoch 0 new loss 0.00010622185800457373 old loss 0.00014017036301083863 BETTER
I0313 07:27:23.048560 2118654 finetune.py:45] layer 9_q initial loss 7.68934260122478e-05
I0313 07:27:26.060897 2117858 finetune.py:68] layer 8_q @ epoch 1 new loss 5.779488128609955e-05 old loss 5.873573900316842e-05 BETTER
I0313 07:27:46.435308 2119442 finetune.py:68] layer 10_v @ epoch 3 new loss 9.435854008188471e-05 old loss 9.638049232307822e-05 BETTER
I0313 07:27:54.711602 2120229 finetune.py:68] layer 11_v @ epoch 1 new loss 0.00010107832349603996 old loss 0.00010622185800457373 BETTER
I0313 07:27:54.712131 2118654 finetune.py:68] layer 9_q @ epoch 0 new loss 7.520311191910878e-05 old loss 7.68934260122478e-05 BETTER
I0313 07:28:00.797606 2117858 finetune.py:68] layer 8_q @ epoch 2 new loss 5.7051605836022645e-05 old loss 5.779488128609955e-05 BETTER
I0313 07:28:19.544581 2119442 finetune.py:68] layer 10_v @ epoch 4 new loss 9.274962940253317e-05 old loss 9.435854008188471e-05 BETTER
I0313 07:28:26.686776 2120229 finetune.py:68] layer 11_v @ epoch 2 new loss 9.825588495004922e-05 old loss 0.00010107832349603996 BETTER
I0313 07:28:26.911545 2118654 finetune.py:68] layer 9_q @ epoch 1 new loss 7.413185085169971e-05 old loss 7.520311191910878e-05 BETTER
I0313 07:28:35.444825 2117858 finetune.py:68] layer 8_q @ epoch 3 new loss 5.641666939482093e-05 old loss 5.7051605836022645e-05 BETTER
I0313 07:28:37.234298 2119442 finetune.py:45] layer 10_q initial loss 0.00010724119056249037
I0313 07:28:58.856422 2120229 finetune.py:68] layer 11_v @ epoch 3 new loss 9.63054335443303e-05 old loss 9.825588495004922e-05 BETTER
I0313 07:28:59.361493 2118654 finetune.py:68] layer 9_q @ epoch 2 new loss 7.325795741053298e-05 old loss 7.413185085169971e-05 BETTER
I0313 07:29:08.668145 2119442 finetune.py:68] layer 10_q @ epoch 0 new loss 0.00010479334741830826 old loss 0.00010724119056249037 BETTER
I0313 07:29:10.054262 2117858 finetune.py:68] layer 8_q @ epoch 4 new loss 5.587739360635169e-05 old loss 5.641666939482093e-05 BETTER
I0313 07:29:27.848923 2117858 finetune.py:45] layer 8_k initial loss 6.289106386248022e-05
I0313 07:29:31.214293 2120229 finetune.py:68] layer 11_v @ epoch 4 new loss 9.478828724240884e-05 old loss 9.63054335443303e-05 BETTER
I0313 07:29:31.720533 2118654 finetune.py:68] layer 9_q @ epoch 3 new loss 7.252750219777226e-05 old loss 7.325795741053298e-05 BETTER
I0313 07:29:40.981251 2119442 finetune.py:68] layer 10_q @ epoch 1 new loss 0.00010321264562662691 old loss 0.00010479334741830826 BETTER
I0313 07:29:48.654706 2120229 finetune.py:45] layer 11_q initial loss 0.00011132618237752467
I0313 07:30:00.954178 2117858 finetune.py:68] layer 8_k @ epoch 0 new loss 6.216494512045756e-05 old loss 6.289106386248022e-05 BETTER
I0313 07:30:04.308769 2118654 finetune.py:68] layer 9_q @ epoch 4 new loss 7.18532464816235e-05 old loss 7.252750219777226e-05 BETTER
I0313 07:30:13.290937 2119442 finetune.py:68] layer 10_q @ epoch 2 new loss 0.00010184744314756244 old loss 0.00010321264562662691 BETTER
I0313 07:30:19.736167 2120229 finetune.py:68] layer 11_q @ epoch 0 new loss 0.00010895154991885647 old loss 0.00011132618237752467 BETTER
I0313 07:30:22.113099 2118654 finetune.py:45] layer 9_k initial loss 8.09544580988586e-05
I0313 07:30:34.851136 2117858 finetune.py:68] layer 8_k @ epoch 1 new loss 6.171653512865305e-05 old loss 6.216494512045756e-05 BETTER
I0313 07:30:45.800211 2119442 finetune.py:68] layer 10_q @ epoch 3 new loss 0.0001006765232887119 old loss 0.00010184744314756244 BETTER
I0313 07:30:51.790425 2120229 finetune.py:68] layer 11_q @ epoch 1 new loss 0.00010745001054601744 old loss 0.00010895154991885647 BETTER
I0313 07:30:53.563303 2118654 finetune.py:68] layer 9_k @ epoch 0 new loss 7.99201152403839e-05 old loss 8.09544580988586e-05 BETTER
I0313 07:31:09.155461 2117858 finetune.py:68] layer 8_k @ epoch 2 new loss 6.133929855423048e-05 old loss 6.171653512865305e-05 BETTER
I0313 07:31:18.470721 2119442 finetune.py:68] layer 10_q @ epoch 4 new loss 9.965928620658815e-05 old loss 0.0001006765232887119 BETTER
I0313 07:31:23.949394 2120229 finetune.py:68] layer 11_q @ epoch 2 new loss 0.00010619116073939949 old loss 0.00010745001054601744 BETTER
I0313 07:31:25.696032 2118654 finetune.py:68] layer 9_k @ epoch 1 new loss 7.935788744362071e-05 old loss 7.99201152403839e-05 BETTER
I0313 07:31:36.821934 2119442 finetune.py:45] layer 10_k initial loss 0.00011061158875236288
I0313 07:31:43.638675 2117858 finetune.py:68] layer 8_k @ epoch 3 new loss 6.0994032537564635e-05 old loss 6.133929855423048e-05 BETTER
I0313 07:31:56.366215 2120229 finetune.py:68] layer 11_q @ epoch 3 new loss 0.00010512726294109598 old loss 0.00010619116073939949 BETTER
I0313 07:31:57.804717 2118654 finetune.py:68] layer 9_k @ epoch 2 new loss 7.887778338044882e-05 old loss 7.935788744362071e-05 BETTER
I0313 07:32:08.354624 2119442 finetune.py:68] layer 10_k @ epoch 0 new loss 0.00010899231710936874 old loss 0.00011061158875236288 BETTER
I0313 07:32:17.849049 2117858 finetune.py:68] layer 8_k @ epoch 4 new loss 6.070585368433967e-05 old loss 6.0994032537564635e-05 BETTER
I0313 07:32:28.657528 2120229 finetune.py:68] layer 11_q @ epoch 4 new loss 0.00010421247861813754 old loss 0.00010512726294109598 BETTER
I0313 07:32:29.936852 2118654 finetune.py:68] layer 9_k @ epoch 3 new loss 7.844689389457926e-05 old loss 7.887778338044882e-05 BETTER
I0313 07:32:35.794304 2117858 finetune.py:45] layer 8_o initial loss 0.00014819052012171596
I0313 07:32:40.385395 2119442 finetune.py:68] layer 10_k @ epoch 1 new loss 0.00010804800695041195 old loss 0.00010899231710936874 BETTER
I0313 07:32:46.302482 2120229 finetune.py:45] layer 11_k initial loss 0.00011681357864290476
I0313 07:33:02.200339 2118654 finetune.py:68] layer 9_k @ epoch 4 new loss 7.80724294600077e-05 old loss 7.844689389457926e-05 BETTER
I0313 07:33:08.414096 2117858 finetune.py:68] layer 8_o @ epoch 0 new loss 0.0001440300839021802 old loss 0.00014819052012171596 BETTER
I0313 07:33:12.469300 2119442 finetune.py:68] layer 10_k @ epoch 2 new loss 0.00010725500760599971 old loss 0.00010804800695041195 BETTER
I0313 07:33:17.099908 2120229 finetune.py:68] layer 11_k @ epoch 0 new loss 0.00011533878569025546 old loss 0.00011681357864290476 BETTER
I0313 07:33:20.236167 2118654 finetune.py:45] layer 9_o initial loss 0.0001901148061733693
I0313 07:33:41.996260 2117858 finetune.py:68] layer 8_o @ epoch 1 new loss 0.00014238989388104528 old loss 0.0001440300839021802 BETTER
I0313 07:33:44.585445 2119442 finetune.py:68] layer 10_k @ epoch 3 new loss 0.00010656423546606675 old loss 0.00010725500760599971 BETTER
I0313 07:33:48.650993 2120229 finetune.py:68] layer 11_k @ epoch 1 new loss 0.00011453316983534023 old loss 0.00011533878569025546 BETTER
I0313 07:33:51.018569 2118654 finetune.py:68] layer 9_o @ epoch 0 new loss 0.00018635996093507856 old loss 0.0001901148061733693 BETTER
I0313 07:34:17.977456 2117858 finetune.py:68] layer 8_o @ epoch 2 new loss 0.00014111914788372815 old loss 0.00014238989388104528 BETTER
I0313 07:34:18.775082 2119442 finetune.py:68] layer 10_k @ epoch 4 new loss 0.00010590488091111183 old loss 0.00010656423546606675 BETTER
I0313 07:34:21.598891 2120229 finetune.py:68] layer 11_k @ epoch 2 new loss 0.00011381952936062589 old loss 0.00011453316983534023 BETTER
I0313 07:34:23.160174 2118654 finetune.py:68] layer 9_o @ epoch 1 new loss 0.000184413802344352 old loss 0.00018635996093507856 BETTER
I0313 07:34:38.989491 2119442 finetune.py:45] layer 10_o initial loss 0.00025640896637924016
I0313 07:34:53.778015 2117858 finetune.py:68] layer 8_o @ epoch 3 new loss 0.0001400491310050711 old loss 0.00014111914788372815 BETTER
I0313 07:34:54.900485 2120229 finetune.py:68] layer 11_k @ epoch 3 new loss 0.00011322819045744836 old loss 0.00011381952936062589 BETTER
I0313 07:34:55.948148 2118654 finetune.py:68] layer 9_o @ epoch 2 new loss 0.00018284132238477468 old loss 0.000184413802344352 BETTER
I0313 07:35:09.883311 2119442 finetune.py:68] layer 10_o @ epoch 0 new loss 0.00025139047647826374 old loss 0.00025640896637924016 BETTER
I0313 07:35:29.843857 2120229 finetune.py:68] layer 11_k @ epoch 4 new loss 0.00011267523223068565 old loss 0.00011322819045744836 BETTER
I0313 07:35:30.542110 2117858 finetune.py:68] layer 8_o @ epoch 4 new loss 0.00013911565474700183 old loss 0.0001400491310050711 BETTER
I0313 07:35:30.794600 2118654 finetune.py:68] layer 9_o @ epoch 3 new loss 0.00018147987429983914 old loss 0.00018284132238477468 BETTER
I0313 07:35:41.848866 2119442 finetune.py:68] layer 10_o @ epoch 1 new loss 0.00024844863219186664 old loss 0.00025139047647826374 BETTER
I0313 07:35:48.746853 2120229 finetune.py:45] layer 11_o initial loss 0.0002682176127564162
I0313 07:35:54.453119 2117858 finetune.py:45] layer 8_up initial loss 0.00022305389575194567
I0313 07:36:02.447365 2118654 finetune.py:68] layer 9_o @ epoch 4 new loss 0.00018027839541900903 old loss 0.00018147987429983914 BETTER
I0313 07:36:13.580328 2119442 finetune.py:68] layer 10_o @ epoch 2 new loss 0.00024605891667306423 old loss 0.00024844863219186664 BETTER
I0313 07:36:18.480817 2120229 finetune.py:68] layer 11_o @ epoch 0 new loss 0.0002632064279168844 old loss 0.0002682176127564162 BETTER
I0313 07:36:25.313679 2117858 finetune.py:68] layer 8_up @ epoch 0 new loss 0.00022111224825493991 old loss 0.00022305389575194567 BETTER
I0313 07:36:26.275677 2118654 finetune.py:45] layer 9_up initial loss 0.0002776773471850902
I0313 07:36:45.423312 2119442 finetune.py:68] layer 10_o @ epoch 3 new loss 0.00024400913389399648 old loss 0.00024605891667306423 BETTER
I0313 07:36:49.495513 2120229 finetune.py:68] layer 11_o @ epoch 1 new loss 0.00026040460215881467 old loss 0.0002632064279168844 BETTER
I0313 07:36:55.305912 2118654 finetune.py:68] layer 9_up @ epoch 0 new loss 0.0002753452572505921 old loss 0.0002776773471850902 BETTER
I0313 07:36:57.349481 2117858 finetune.py:68] layer 8_up @ epoch 1 new loss 0.00021976418793201447 old loss 0.00022111224825493991 BETTER
I0313 07:37:17.240211 2119442 finetune.py:68] layer 10_o @ epoch 4 new loss 0.00024221652711275965 old loss 0.00024400913389399648 BETTER
I0313 07:37:20.527429 2120229 finetune.py:68] layer 11_o @ epoch 2 new loss 0.00025816773995757103 old loss 0.00026040460215881467 BETTER
I0313 07:37:25.443170 2118654 finetune.py:68] layer 9_up @ epoch 1 new loss 0.000273692246992141 old loss 0.0002753452572505921 BETTER
I0313 07:37:29.405417 2117858 finetune.py:68] layer 8_up @ epoch 2 new loss 0.00021857341926079243 old loss 0.00021976418793201447 BETTER
I0313 07:37:40.869958 2119442 finetune.py:45] layer 10_up initial loss 0.0003526050422806293
I0313 07:37:51.640968 2120229 finetune.py:68] layer 11_o @ epoch 3 new loss 0.0002562385343480855 old loss 0.00025816773995757103 BETTER
I0313 07:37:55.486142 2118654 finetune.py:68] layer 9_up @ epoch 2 new loss 0.00027220710762776434 old loss 0.000273692246992141 BETTER
I0313 07:38:01.509800 2117858 finetune.py:68] layer 8_up @ epoch 3 new loss 0.00021750769519712776 old loss 0.00021857341926079243 BETTER
I0313 07:38:09.728350 2119442 finetune.py:68] layer 10_up @ epoch 0 new loss 0.0003495212004054338 old loss 0.0003526050422806293 BETTER
I0313 07:38:22.762625 2120229 finetune.py:68] layer 11_o @ epoch 4 new loss 0.0002545876195654273 old loss 0.0002562385343480855 BETTER
I0313 07:38:25.407234 2118654 finetune.py:68] layer 9_up @ epoch 3 new loss 0.0002708693500608206 old loss 0.00027220710762776434 BETTER
I0313 07:38:33.579745 2117858 finetune.py:68] layer 8_up @ epoch 4 new loss 0.0002165355981560424 old loss 0.00021750769519712776 BETTER
I0313 07:38:39.674209 2119442 finetune.py:68] layer 10_up @ epoch 1 new loss 0.00034729414619505405 old loss 0.0003495212004054338 BETTER
I0313 07:38:45.754702 2120229 finetune.py:45] layer 11_up initial loss 0.00038226033211685717
I0313 07:38:55.294383 2118654 finetune.py:68] layer 9_up @ epoch 4 new loss 0.00026965769939124584 old loss 0.0002708693500608206 BETTER
I0313 07:38:57.286270 2117858 finetune.py:45] layer 8_gate initial loss 0.0002676451695151627
I0313 07:39:09.545039 2119442 finetune.py:68] layer 10_up @ epoch 2 new loss 0.00034531712299212813 old loss 0.00034729414619505405 BETTER
I0313 07:39:14.289275 2120229 finetune.py:68] layer 11_up @ epoch 0 new loss 0.0003790023329202086 old loss 0.00038226033211685717 BETTER
I0313 07:39:18.659238 2118654 finetune.py:45] layer 9_gate initial loss 0.00033136436832137406
I0313 07:39:26.591494 2117858 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.00026636640541255474 old loss 0.0002676451695151627 BETTER
I0313 07:39:39.735584 2119442 finetune.py:68] layer 10_up @ epoch 3 new loss 0.00034355526440776885 old loss 0.00034531712299212813 BETTER
I0313 07:39:43.880359 2120229 finetune.py:68] layer 11_up @ epoch 1 new loss 0.00037663482362404466 old loss 0.0003790023329202086 BETTER
I0313 07:39:45.861031 2118654 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.00032990460749715567 old loss 0.00033136436832137406 BETTER
I0313 07:39:56.855887 2117858 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.0002654427080415189 old loss 0.00026636640541255474 BETTER
I0313 07:40:09.906188 2119442 finetune.py:68] layer 10_up @ epoch 4 new loss 0.0003419306012801826 old loss 0.00034355526440776885 BETTER
I0313 07:40:13.572740 2120229 finetune.py:68] layer 11_up @ epoch 2 new loss 0.00037460841122083366 old loss 0.00037663482362404466 BETTER
I0313 07:40:14.292357 2118654 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.00032878408092074096 old loss 0.00032990460749715567 BETTER
I0313 07:40:27.178903 2117858 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.00026464619440957904 old loss 0.0002654427080415189 BETTER
I0313 07:40:33.686594 2119442 finetune.py:45] layer 10_gate initial loss 0.00041535391937941313
I0313 07:40:42.683741 2118654 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.0003277855576016009 old loss 0.00032878408092074096 BETTER
I0313 07:40:43.144520 2120229 finetune.py:68] layer 11_up @ epoch 3 new loss 0.0003727780422195792 old loss 0.00037460841122083366 BETTER
I0313 07:40:57.505144 2117858 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.00026392017025500536 old loss 0.00026464619440957904 BETTER
I0313 07:41:01.208180 2119442 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.0004134911287110299 old loss 0.00041535391937941313 BETTER
I0313 07:41:11.052629 2118654 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.00032688514329493046 old loss 0.0003277855576016009 BETTER
I0313 07:41:12.713867 2120229 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00037109400727786124 old loss 0.0003727780422195792 BETTER
I0313 07:41:27.788980 2117858 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.0002632557589095086 old loss 0.00026392017025500536 BETTER
I0313 07:41:29.260821 2119442 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.00041204073932021856 old loss 0.0004134911287110299 BETTER
I0313 07:41:36.275657 2120229 finetune.py:45] layer 11_gate initial loss 0.00045630443491972983
I0313 07:41:39.484993 2118654 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0003260614175815135 old loss 0.00032688514329493046 BETTER
I0313 07:41:57.392944 2119442 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.0004107646818738431 old loss 0.00041204073932021856 BETTER
I0313 07:42:03.219788 2120229 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.0004542616952676326 old loss 0.00045630443491972983 BETTER
I0313 07:42:08.493812 2117858 finetune.py:45] layer 8_down initial loss 0.0003999498439952731
I0313 07:42:20.999960 2118654 finetune.py:45] layer 9_down initial loss 0.0004857168532907963
I0313 07:42:25.624496 2119442 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.0004095920012332499 old loss 0.0004107646818738431 BETTER
I0313 07:42:30.971190 2120229 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.0004527218115981668 old loss 0.0004542616952676326 BETTER
I0313 07:42:36.034790 2117858 finetune.py:68] layer 8_down @ epoch 0 new loss 0.0003999241453129798 old loss 0.0003999498439952731 BETTER
I0313 07:42:46.769968 2118654 finetune.py:68] layer 9_down @ epoch 0 new loss 0.0004856835585087538 old loss 0.0004857168532907963 BETTER
I0313 07:42:53.674279 2119442 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.0004085183027200401 old loss 0.0004095920012332499 BETTER
I0313 07:42:58.980639 2120229 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.00045134651008993387 old loss 0.0004527218115981668 BETTER
I0313 07:43:04.367417 2117858 finetune.py:68] layer 8_down @ epoch 1 new loss 0.0003999074106104672 old loss 0.0003999241453129798 BETTER
I0313 07:43:13.075325 2118654 finetune.py:68] layer 9_down @ epoch 1 new loss 0.00048566528130322695 old loss 0.0004856835585087538 BETTER
I0313 07:43:27.071006 2120229 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.00045009967288933694 old loss 0.00045134651008993387 BETTER
I0313 07:43:32.914502 2117858 finetune.py:68] layer 8_down @ epoch 2 new loss 0.0003998981846962124 old loss 0.0003999074106104672 BETTER
I0313 07:43:36.246518 2119442 finetune.py:45] layer 10_down initial loss 0.0005934143555350602
I0313 07:43:39.787704 2118654 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00048565439647063613 old loss 0.00048566528130322695 BETTER
I0313 07:43:55.185110 2120229 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.0004489694256335497 old loss 0.00045009967288933694 BETTER
I0313 07:44:01.718932 2117858 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0003998934116680175 old loss 0.0003998981846962124 BETTER
I0313 07:44:01.870672 2119442 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0005933683132752776 old loss 0.0005934143555350602 BETTER
I0313 07:44:06.559290 2118654 finetune.py:68] layer 9_down @ epoch 3 new loss 0.0004856482846662402 old loss 0.00048565439647063613 BETTER
I0313 07:44:28.901756 2119442 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0005933447973802686 old loss 0.0005933683132752776 BETTER
I0313 07:44:30.545626 2117858 finetune.py:68] layer 8_down @ epoch 4 new loss 0.0003998875035904348 old loss 0.0003998934116680175 BETTER
8_v proxy err 0.014558561146259308 tr(WHW.T) 530.9967041015625
8_q proxy err 0.0012057434068992734 tr(WHW.T) 7231.333984375
8_k proxy err 0.0008212194079533219 tr(WHW.T) 10640.12890625
8_o proxy err 0.01998969167470932 tr(WHW.T) 20.170928955078125
8_up proxy err 0.011243243701756 tr(WHW.T) 866.0274658203125
8_gate proxy err 0.005059167742729187 tr(WHW.T) 1968.7017822265625
8_down proxy err 0.015515176579356194 tr(WHW.T) 37.32933807373047
I0313 07:44:33.511863 2118654 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00048564362805336714 old loss 0.0004856482846662402 BETTER
9_v proxy err 0.01464515458792448 tr(WHW.T) 565.0663452148438
9_q proxy err 0.0013408982194960117 tr(WHW.T) 6974.10009765625
9_k proxy err 0.000854634097777307 tr(WHW.T) 10990.19140625
9_o proxy err 0.020432962104678154 tr(WHW.T) 25.737247467041016
9_up proxy err 0.010875101201236248 tr(WHW.T) 970.4112548828125
9_gate proxy err 0.0050458949990570545 tr(WHW.T) 2130.6171875
9_down proxy err 0.015714867040514946 tr(WHW.T) 43.13645553588867
I0313 07:44:37.307926 2120229 finetune.py:45] layer 11_down initial loss 0.0006535084103234112
I0313 07:44:56.983195 2119442 finetune.py:68] layer 10_down @ epoch 2 new loss 0.0005933296051807702 old loss 0.0005933447973802686 BETTER
I0313 07:45:02.790457 2120229 finetune.py:68] layer 11_down @ epoch 0 new loss 0.0006534542189911008 old loss 0.0006535084103234112 BETTER
I0313 07:45:24.200119 2119442 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0005933185457251966 old loss 0.0005933296051807702 BETTER
I0313 07:45:29.217284 2120229 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0006534234853461385 old loss 0.0006534542189911008 BETTER
I0313 07:45:51.477398 2119442 finetune.py:68] layer 10_down @ epoch 4 new loss 0.0005933103966526687 old loss 0.0005933185457251966 BETTER
10_v proxy err 0.014848072081804276 tr(WHW.T) 578.807373046875
10_q proxy err 0.001401775167323649 tr(WHW.T) 6920.26318359375
10_k proxy err 0.0008863466791808605 tr(WHW.T) 11000.6865234375
10_o proxy err 0.020478250458836555 tr(WHW.T) 35.3456916809082
10_up proxy err 0.010246426798403263 tr(WHW.T) 1079.30615234375
10_gate proxy err 0.004967306274920702 tr(WHW.T) 2259.851806640625
10_down proxy err 0.014922657050192356 tr(WHW.T) 52.496665954589844
I0313 07:45:55.463257 2120229 finetune.py:68] layer 11_down @ epoch 2 new loss 0.000653404975309968 old loss 0.0006534234853461385 BETTER
I0313 07:45:56.760766 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 12 in 78.16750812530518s
I0313 07:45:59.957341 2130708 config.py:54] PyTorch version 2.1.1 available.
I0313 07:46:00.945698 2090717 quantize_finetune_llama.py:184] layer 13 gpu 1
I0313 07:46:01.014630 2130708 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:46:19.504437 2130708 finetune.py:45] layer 12_v initial loss 0.00014461636601481587
I0313 07:46:22.595072 2120229 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0006533924024552107 old loss 0.000653404975309968 BETTER
I0313 07:46:49.004159 2120229 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0006533843115903437 old loss 0.0006533924024552107 BETTER
11_v proxy err 0.01487690769135952 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0016803444596007466 tr(WHW.T) 7031.06982421875
11_k proxy err 0.001122617512010038 tr(WHW.T) 10518.033203125
11_o proxy err 0.020726507529616356 tr(WHW.T) 36.82060241699219
11_up proxy err 0.010699515230953693 tr(WHW.T) 1139.6077880859375
11_gate proxy err 0.005153475794941187 tr(WHW.T) 2395.80810546875
11_down proxy err 0.015362903475761414 tr(WHW.T) 56.33756637573242
I0313 07:46:52.807587 2130708 finetune.py:68] layer 12_v @ epoch 0 new loss 0.00011421968520153314 old loss 0.00014461636601481587 BETTER
I0313 07:47:13.511557 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 13 in 72.1388156414032s
I0313 07:47:16.499641 2131512 config.py:54] PyTorch version 2.1.1 available.
I0313 07:47:17.449980 2090717 quantize_finetune_llama.py:184] layer 14 gpu 2
I0313 07:47:17.513921 2131512 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 07:47:27.830340 2130708 finetune.py:68] layer 12_v @ epoch 1 new loss 0.00010942000517388806 old loss 0.00011421968520153314 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:47:34.319884 2131512 finetune.py:45] layer 13_v initial loss 0.00016008553211577237
I0313 07:48:02.780628 2130708 finetune.py:68] layer 12_v @ epoch 2 new loss 0.00010665883746696636 old loss 0.00010942000517388806 BETTER
I0313 07:48:05.383760 2131512 finetune.py:68] layer 13_v @ epoch 0 new loss 0.00012534830602817237 old loss 0.00016008553211577237 BETTER
I0313 07:48:27.009583 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 14 in 69.17196536064148s
I0313 07:48:30.123470 2132274 config.py:54] PyTorch version 2.1.1 available.
I0313 07:48:31.130043 2090717 quantize_finetune_llama.py:184] layer 15 gpu 3
I0313 07:48:31.197424 2132274 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 07:48:37.468576 2131512 finetune.py:68] layer 13_v @ epoch 1 new loss 0.00012015565880574286 old loss 0.00012534830602817237 BETTER
I0313 07:48:37.982451 2130708 finetune.py:68] layer 12_v @ epoch 3 new loss 0.00010463982471264899 old loss 0.00010665883746696636 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:48:48.411364 2132274 finetune.py:45] layer 14_v initial loss 0.00019449627143330872
I0313 07:49:09.562719 2131512 finetune.py:68] layer 13_v @ epoch 2 new loss 0.00011717422603396699 old loss 0.00012015565880574286 BETTER
I0313 07:49:13.080232 2130708 finetune.py:68] layer 12_v @ epoch 4 new loss 0.00010310960351489484 old loss 0.00010463982471264899 BETTER
I0313 07:49:19.686378 2132274 finetune.py:68] layer 14_v @ epoch 0 new loss 0.00016027738456614316 old loss 0.00019449627143330872 BETTER
I0313 07:49:31.248832 2130708 finetune.py:45] layer 12_q initial loss 0.00012300621892791241
I0313 07:49:40.296604 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 15 in 68.78727889060974s
I0313 07:49:42.093922 2131512 finetune.py:68] layer 13_v @ epoch 3 new loss 0.00011499472020659596 old loss 0.00011717422603396699 BETTER
I0313 07:49:43.585781 2133047 config.py:54] PyTorch version 2.1.1 available.
I0313 07:49:44.597040 2090717 quantize_finetune_llama.py:184] layer 16 gpu 0
I0313 07:49:44.681031 2133047 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 07:49:51.999453 2132274 finetune.py:68] layer 14_v @ epoch 1 new loss 0.0001536229974590242 old loss 0.00016027738456614316 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 07:50:01.710001 2133047 finetune.py:45] layer 15_v initial loss 0.00020195222168695182
I0313 07:50:04.880941 2130708 finetune.py:68] layer 12_q @ epoch 0 new loss 0.00012033597886329517 old loss 0.00012300621892791241 BETTER
I0313 07:50:14.515847 2131512 finetune.py:68] layer 13_v @ epoch 4 new loss 0.00011328704567858949 old loss 0.00011499472020659596 BETTER
I0313 07:50:24.393026 2132274 finetune.py:68] layer 14_v @ epoch 2 new loss 0.00014946558803785592 old loss 0.0001536229974590242 BETTER
I0313 07:50:32.577316 2131512 finetune.py:45] layer 13_q initial loss 0.0001336711284238845
I0313 07:50:32.875696 2133047 finetune.py:68] layer 15_v @ epoch 0 new loss 0.00015972935943864286 old loss 0.00020195222168695182 BETTER
I0313 07:50:39.234310 2130708 finetune.py:68] layer 12_q @ epoch 1 new loss 0.00011858906509587541 old loss 0.00012033597886329517 BETTER
I0313 07:50:57.456636 2132274 finetune.py:68] layer 14_v @ epoch 3 new loss 0.00014635181287303567 old loss 0.00014946558803785592 BETTER
I0313 07:51:04.229250 2131512 finetune.py:68] layer 13_q @ epoch 0 new loss 0.00013069408305455 old loss 0.0001336711284238845 BETTER
I0313 07:51:05.209228 2133047 finetune.py:68] layer 15_v @ epoch 1 new loss 0.00015311197785194963 old loss 0.00015972935943864286 BETTER
I0313 07:51:13.663512 2130708 finetune.py:68] layer 12_q @ epoch 2 new loss 0.0001171785406768322 old loss 0.00011858906509587541 BETTER
I0313 07:51:30.314054 2132274 finetune.py:68] layer 14_v @ epoch 4 new loss 0.00014379917411133647 old loss 0.00014635181287303567 BETTER
I0313 07:51:36.458532 2131512 finetune.py:68] layer 13_q @ epoch 1 new loss 0.00012872614024672657 old loss 0.00013069408305455 BETTER
I0313 07:51:37.328736 2133047 finetune.py:68] layer 15_v @ epoch 2 new loss 0.00014907123113516718 old loss 0.00015311197785194963 BETTER
I0313 07:51:48.605758 2130708 finetune.py:68] layer 12_q @ epoch 3 new loss 0.00011598831770243123 old loss 0.0001171785406768322 BETTER
I0313 07:51:49.064740 2132274 finetune.py:45] layer 14_q initial loss 0.00016760836297180504
I0313 07:52:08.744602 2131512 finetune.py:68] layer 13_q @ epoch 2 new loss 0.00012713257456198335 old loss 0.00012872614024672657 BETTER
I0313 07:52:09.791679 2133047 finetune.py:68] layer 15_v @ epoch 3 new loss 0.00014592985098715872 old loss 0.00014907123113516718 BETTER
I0313 07:52:20.532480 2132274 finetune.py:68] layer 14_q @ epoch 0 new loss 0.00016369616787414998 old loss 0.00016760836297180504 BETTER
I0313 07:52:23.312465 2130708 finetune.py:68] layer 12_q @ epoch 4 new loss 0.00011492383055156097 old loss 0.00011598831770243123 BETTER
I0313 07:52:42.797631 2131512 finetune.py:68] layer 13_q @ epoch 3 new loss 0.00012576562585309148 old loss 0.00012713257456198335 BETTER
I0313 07:52:43.040500 2130708 finetune.py:45] layer 12_k initial loss 0.00012893788516521454
I0313 07:52:43.800715 2133047 finetune.py:68] layer 15_v @ epoch 4 new loss 0.00014344777446240187 old loss 0.00014592985098715872 BETTER
I0313 07:52:52.848775 2132274 finetune.py:68] layer 14_q @ epoch 1 new loss 0.00016106630209833384 old loss 0.00016369616787414998 BETTER
I0313 07:53:02.227530 2133047 finetune.py:45] layer 15_q initial loss 0.00017112538625951856
I0313 07:53:15.812018 2131512 finetune.py:68] layer 13_q @ epoch 4 new loss 0.00012457865523174405 old loss 0.00012576562585309148 BETTER
I0313 07:53:17.136362 2130708 finetune.py:68] layer 12_k @ epoch 0 new loss 0.0001273468224098906 old loss 0.00012893788516521454 BETTER
I0313 07:53:25.540038 2132274 finetune.py:68] layer 14_q @ epoch 2 new loss 0.0001588171871844679 old loss 0.00016106630209833384 BETTER
I0313 07:53:34.317227 2133047 finetune.py:68] layer 15_q @ epoch 0 new loss 0.00016649365716148168 old loss 0.00017112538625951856 BETTER
I0313 07:53:35.386566 2131512 finetune.py:45] layer 13_k initial loss 0.00013972025772091
I0313 07:53:51.290962 2130708 finetune.py:68] layer 12_k @ epoch 1 new loss 0.00012640577915590256 old loss 0.0001273468224098906 BETTER
I0313 07:53:58.478940 2132274 finetune.py:68] layer 14_q @ epoch 3 new loss 0.00015690256259404123 old loss 0.0001588171871844679 BETTER
I0313 07:54:06.546422 2131512 finetune.py:68] layer 13_k @ epoch 0 new loss 0.00013793994730804116 old loss 0.00013972025772091 BETTER
I0313 07:54:07.015102 2133047 finetune.py:68] layer 15_q @ epoch 1 new loss 0.00016357250569853932 old loss 0.00016649365716148168 BETTER
I0313 07:54:25.777467 2130708 finetune.py:68] layer 12_k @ epoch 2 new loss 0.0001256291288882494 old loss 0.00012640577915590256 BETTER
I0313 07:54:31.615992 2132274 finetune.py:68] layer 14_q @ epoch 4 new loss 0.0001552323519717902 old loss 0.00015690256259404123 BETTER
I0313 07:54:38.924447 2131512 finetune.py:68] layer 13_k @ epoch 1 new loss 0.00013686031161341816 old loss 0.00013793994730804116 BETTER
I0313 07:54:39.346841 2133047 finetune.py:68] layer 15_q @ epoch 2 new loss 0.00016116155893541873 old loss 0.00016357250569853932 BETTER
I0313 07:54:52.119091 2132274 finetune.py:45] layer 14_k initial loss 0.00017290656978730112
I0313 07:55:01.327847 2130708 finetune.py:68] layer 12_k @ epoch 3 new loss 0.00012492339010350406 old loss 0.0001256291288882494 BETTER
I0313 07:55:11.654325 2131512 finetune.py:68] layer 13_k @ epoch 2 new loss 0.0001359727611998096 old loss 0.00013686031161341816 BETTER
I0313 07:55:12.029899 2133047 finetune.py:68] layer 15_q @ epoch 3 new loss 0.00015913083916530013 old loss 0.00016116155893541873 BETTER
I0313 07:55:23.727393 2132274 finetune.py:68] layer 14_k @ epoch 0 new loss 0.0001708612107904628 old loss 0.00017290656978730112 BETTER
I0313 07:55:35.591961 2130708 finetune.py:68] layer 12_k @ epoch 4 new loss 0.00012430734932422638 old loss 0.00012492339010350406 BETTER
I0313 07:55:43.844883 2131512 finetune.py:68] layer 13_k @ epoch 3 new loss 0.0001351680839434266 old loss 0.0001359727611998096 BETTER
I0313 07:55:44.196572 2133047 finetune.py:68] layer 15_q @ epoch 4 new loss 0.00015734907356090844 old loss 0.00015913083916530013 BETTER
I0313 07:55:57.097317 2130708 finetune.py:45] layer 12_o initial loss 0.00029949125018902123
I0313 07:55:57.497351 2132274 finetune.py:68] layer 14_k @ epoch 1 new loss 0.00016941552166827023 old loss 0.0001708612107904628 BETTER
I0313 07:56:04.512867 2133047 finetune.py:45] layer 15_k initial loss 0.0001775548153091222
I0313 07:56:16.649251 2131512 finetune.py:68] layer 13_k @ epoch 4 new loss 0.00013446710363496095 old loss 0.0001351680839434266 BETTER
I0313 07:56:30.996836 2130708 finetune.py:68] layer 12_o @ epoch 0 new loss 0.00029403867665678263 old loss 0.00029949125018902123 BETTER
I0313 07:56:31.321126 2132274 finetune.py:68] layer 14_k @ epoch 2 new loss 0.0001682049041846767 old loss 0.00016941552166827023 BETTER
I0313 07:56:35.778657 2133047 finetune.py:68] layer 15_k @ epoch 0 new loss 0.00017506309086456895 old loss 0.0001775548153091222 BETTER
I0313 07:56:37.122535 2131512 finetune.py:45] layer 13_o initial loss 0.00032143565476872027
I0313 07:57:06.541656 2132274 finetune.py:68] layer 14_k @ epoch 3 new loss 0.000167123565915972 old loss 0.0001682049041846767 BETTER
I0313 07:57:07.417315 2130708 finetune.py:68] layer 12_o @ epoch 1 new loss 0.00029091149917803705 old loss 0.00029403867665678263 BETTER
I0313 07:57:10.059580 2131512 finetune.py:68] layer 13_o @ epoch 0 new loss 0.00031472474802285433 old loss 0.00032143565476872027 BETTER
I0313 07:57:10.200463 2133047 finetune.py:68] layer 15_k @ epoch 1 new loss 0.0001735411788104102 old loss 0.00017506309086456895 BETTER
I0313 07:57:40.983127 2132274 finetune.py:68] layer 14_k @ epoch 4 new loss 0.00016613263869658113 old loss 0.000167123565915972 BETTER
I0313 07:57:43.157724 2131512 finetune.py:68] layer 13_o @ epoch 1 new loss 0.00031099558691494167 old loss 0.00031472474802285433 BETTER
I0313 07:57:43.467771 2130708 finetune.py:68] layer 12_o @ epoch 2 new loss 0.00028837937861680984 old loss 0.00029091149917803705 BETTER
I0313 07:57:43.670648 2133047 finetune.py:68] layer 15_k @ epoch 2 new loss 0.00017228026990778744 old loss 0.0001735411788104102 BETTER
I0313 07:58:00.326911 2132274 finetune.py:45] layer 14_o initial loss 0.00039856319199316204
I0313 07:58:14.565298 2131512 finetune.py:68] layer 13_o @ epoch 2 new loss 0.0003079485904891044 old loss 0.00031099558691494167 BETTER
I0313 07:58:15.540982 2133047 finetune.py:68] layer 15_k @ epoch 3 new loss 0.00017115395166911185 old loss 0.00017228026990778744 BETTER
I0313 07:58:17.031477 2130708 finetune.py:68] layer 12_o @ epoch 3 new loss 0.0002862026449292898 old loss 0.00028837937861680984 BETTER
I0313 07:58:30.623603 2132274 finetune.py:68] layer 14_o @ epoch 0 new loss 0.0003908929065801203 old loss 0.00039856319199316204 BETTER
I0313 07:58:46.102712 2131512 finetune.py:68] layer 13_o @ epoch 3 new loss 0.00030541932210326195 old loss 0.0003079485904891044 BETTER
I0313 07:58:47.512631 2133047 finetune.py:68] layer 15_k @ epoch 4 new loss 0.00017018010839819908 old loss 0.00017115395166911185 BETTER
I0313 07:58:50.447955 2130708 finetune.py:68] layer 12_o @ epoch 4 new loss 0.0002843080146703869 old loss 0.0002862026449292898 BETTER
I0313 07:59:01.915533 2132274 finetune.py:68] layer 14_o @ epoch 1 new loss 0.00038640224374830723 old loss 0.0003908929065801203 BETTER
I0313 07:59:05.439007 2133047 finetune.py:45] layer 15_o initial loss 0.0004073581949342042
I0313 07:59:13.972727 2130708 finetune.py:45] layer 12_up initial loss 0.00042825285345315933
I0313 07:59:17.557670 2131512 finetune.py:68] layer 13_o @ epoch 4 new loss 0.00030319986399263144 old loss 0.00030541932210326195 BETTER
I0313 07:59:33.527518 2132274 finetune.py:68] layer 14_o @ epoch 2 new loss 0.0003828568442258984 old loss 0.00038640224374830723 BETTER
I0313 07:59:35.413277 2133047 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0003988454700447619 old loss 0.0004073581949342042 BETTER
I0313 07:59:40.908631 2131512 finetune.py:45] layer 13_up initial loss 0.00047298287972807884
I0313 07:59:44.860509 2130708 finetune.py:68] layer 12_up @ epoch 0 new loss 0.0004245603340677917 old loss 0.00042825285345315933 BETTER
I0313 08:00:05.192201 2132274 finetune.py:68] layer 14_o @ epoch 3 new loss 0.00037985615199431777 old loss 0.0003828568442258984 BETTER
I0313 08:00:06.468546 2133047 finetune.py:68] layer 15_o @ epoch 1 new loss 0.0003937861474696547 old loss 0.0003988454700447619 BETTER
I0313 08:00:09.819998 2131512 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00046814876259304583 old loss 0.00047298287972807884 BETTER
I0313 08:00:16.818045 2130708 finetune.py:68] layer 12_up @ epoch 1 new loss 0.00042186665814369917 old loss 0.0004245603340677917 BETTER
I0313 08:00:37.007319 2132274 finetune.py:68] layer 14_o @ epoch 4 new loss 0.00037723095738328993 old loss 0.00037985615199431777 BETTER
I0313 08:00:37.833757 2133047 finetune.py:68] layer 15_o @ epoch 2 new loss 0.0003897550341207534 old loss 0.0003937861474696547 BETTER
I0313 08:00:39.352401 2131512 finetune.py:68] layer 13_up @ epoch 1 new loss 0.0004646804591175169 old loss 0.00046814876259304583 BETTER
I0313 08:00:48.825042 2130708 finetune.py:68] layer 12_up @ epoch 2 new loss 0.00041951207094825804 old loss 0.00042186665814369917 BETTER
I0313 08:01:00.457523 2132274 finetune.py:45] layer 14_up initial loss 0.0005702186608687043
I0313 08:01:08.907634 2133047 finetune.py:68] layer 15_o @ epoch 3 new loss 0.0003863690362777561 old loss 0.0003897550341207534 BETTER
I0313 08:01:08.945411 2131512 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0004617077065631747 old loss 0.0004646804591175169 BETTER
I0313 08:01:20.730715 2130708 finetune.py:68] layer 12_up @ epoch 3 new loss 0.0004174000641796738 old loss 0.00041951207094825804 BETTER
I0313 08:01:29.409899 2132274 finetune.py:68] layer 14_up @ epoch 0 new loss 0.0005646008066833019 old loss 0.0005702186608687043 BETTER
I0313 08:01:38.731021 2131512 finetune.py:68] layer 13_up @ epoch 3 new loss 0.00045907191815786064 old loss 0.0004617077065631747 BETTER
I0313 08:01:40.120175 2133047 finetune.py:68] layer 15_o @ epoch 4 new loss 0.00038347949157468975 old loss 0.0003863690362777561 BETTER
I0313 08:01:52.634806 2130708 finetune.py:68] layer 12_up @ epoch 4 new loss 0.0004154830821789801 old loss 0.0004174000641796738 BETTER
I0313 08:01:59.102402 2132274 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0005606911145150661 old loss 0.0005646008066833019 BETTER
I0313 08:02:02.808764 2133047 finetune.py:45] layer 15_up initial loss 0.0006149679538793862
I0313 08:02:08.446238 2131512 finetune.py:68] layer 13_up @ epoch 4 new loss 0.00045670472900383174 old loss 0.00045907191815786064 BETTER
I0313 08:02:15.890081 2130708 finetune.py:45] layer 12_gate initial loss 0.0005158586427569389
I0313 08:02:28.893644 2132274 finetune.py:68] layer 14_up @ epoch 2 new loss 0.0005573707749135792 old loss 0.0005606911145150661 BETTER
I0313 08:02:31.454747 2133047 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0006078632432036102 old loss 0.0006149679538793862 BETTER
I0313 08:02:31.554610 2131512 finetune.py:45] layer 13_gate initial loss 0.0005770554416812956
I0313 08:02:45.094902 2130708 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.0005134816747158766 old loss 0.0005158586427569389 BETTER
I0313 08:02:59.005766 2131512 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.0005740795168094337 old loss 0.0005770554416812956 BETTER
I0313 08:02:59.166370 2132274 finetune.py:68] layer 14_up @ epoch 3 new loss 0.0005544363521039486 old loss 0.0005573707749135792 BETTER
I0313 08:03:01.232316 2133047 finetune.py:68] layer 15_up @ epoch 1 new loss 0.000603055173996836 old loss 0.0006078632432036102 BETTER
I0313 08:03:15.107257 2130708 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0005116604152135551 old loss 0.0005134816747158766 BETTER
I0313 08:03:27.353669 2131512 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0005717540625482798 old loss 0.0005740795168094337 BETTER
I0313 08:03:29.253975 2132274 finetune.py:68] layer 14_up @ epoch 4 new loss 0.000551795877981931 old loss 0.0005544363521039486 BETTER
I0313 08:03:30.758354 2133047 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0005989985074847937 old loss 0.000603055173996836 BETTER
I0313 08:03:45.312735 2130708 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.0005100575508549809 old loss 0.0005116604152135551 BETTER
I0313 08:03:53.302889 2132274 finetune.py:45] layer 14_gate initial loss 0.0006959565216675401
I0313 08:03:55.963585 2131512 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0005697474698536098 old loss 0.0005717540625482798 BETTER
I0313 08:04:00.351349 2133047 finetune.py:68] layer 15_up @ epoch 3 new loss 0.000595408899243921 old loss 0.0005989985074847937 BETTER
I0313 08:04:15.796924 2130708 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.0005086218588985503 old loss 0.0005100575508549809 BETTER
I0313 08:04:20.826866 2132274 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.000692501082085073 old loss 0.0006959565216675401 BETTER
I0313 08:04:24.771827 2131512 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.0005679285968653858 old loss 0.0005697474698536098 BETTER
I0313 08:04:30.004473 2133047 finetune.py:68] layer 15_up @ epoch 4 new loss 0.0005922501441091299 old loss 0.000595408899243921 BETTER
I0313 08:04:46.575790 2130708 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.0005073203938081861 old loss 0.0005086218588985503 BETTER
I0313 08:04:49.614907 2132274 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0006898024585098028 old loss 0.000692501082085073 BETTER
I0313 08:04:53.220513 2131512 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.000566310656722635 old loss 0.0005679285968653858 BETTER
I0313 08:04:54.671610 2133047 finetune.py:45] layer 15_gate initial loss 0.0007677255780436099
I0313 08:05:18.072857 2132274 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.0006874838145449758 old loss 0.0006898024585098028 BETTER
I0313 08:05:22.112539 2133047 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.0007634219364263117 old loss 0.0007677255780436099 BETTER
I0313 08:05:30.630391 2130708 finetune.py:45] layer 12_down initial loss 0.0007411636179313064
I0313 08:05:38.611114 2131512 finetune.py:45] layer 13_down initial loss 0.0008497528033331037
I0313 08:05:47.660979 2132274 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.0006854209932498634 old loss 0.0006874838145449758 BETTER
I0313 08:05:51.232698 2133047 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.0007601338438689709 old loss 0.0007634219364263117 BETTER
I0313 08:05:58.536651 2130708 finetune.py:68] layer 12_down @ epoch 0 new loss 0.0007410959806293249 old loss 0.0007411636179313064 BETTER
I0313 08:06:03.904278 2131512 finetune.py:68] layer 13_down @ epoch 0 new loss 0.0008496555383317173 old loss 0.0008497528033331037 BETTER
I0313 08:06:16.421036 2132274 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0006835534004494548 old loss 0.0006854209932498634 BETTER
I0313 08:06:19.776580 2133047 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.0007573234033770859 old loss 0.0007601338438689709 BETTER
I0313 08:06:27.804130 2130708 finetune.py:68] layer 12_down @ epoch 1 new loss 0.0007410527323372662 old loss 0.0007410959806293249 BETTER
I0313 08:06:30.710179 2131512 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0008495932561345398 old loss 0.0008496555383317173 BETTER
I0313 08:06:48.023320 2133047 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0007548141293227673 old loss 0.0007573234033770859 BETTER
I0313 08:06:58.135449 2130708 finetune.py:68] layer 12_down @ epoch 2 new loss 0.0007410279358737171 old loss 0.0007410527323372662 BETTER
I0313 08:06:58.781448 2131512 finetune.py:68] layer 13_down @ epoch 2 new loss 0.0008495550719089806 old loss 0.0008495932561345398 BETTER
I0313 08:07:03.870311 2132274 finetune.py:45] layer 14_down initial loss 0.0010165509302169085
I0313 08:07:17.598572 2133047 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0007525738910771906 old loss 0.0007548141293227673 BETTER
I0313 08:07:27.265693 2131512 finetune.py:68] layer 13_down @ epoch 3 new loss 0.0008495327783748507 old loss 0.0008495550719089806 BETTER
I0313 08:07:28.671171 2130708 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0007410114048980176 old loss 0.0007410279358737171 BETTER
I0313 08:07:30.525680 2132274 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0010163899278268218 old loss 0.0010165509302169085 BETTER
I0313 08:07:56.289509 2131512 finetune.py:68] layer 13_down @ epoch 4 new loss 0.000849515781737864 old loss 0.0008495327783748507 BETTER
I0313 08:07:59.223308 2132274 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0010162924882024527 old loss 0.0010163899278268218 BETTER
13_v proxy err 0.016268199309706688 tr(WHW.T) 714.5677490234375
13_q proxy err 0.0018446705071255565 tr(WHW.T) 6960.38232421875
13_k proxy err 0.001235291943885386 tr(WHW.T) 10430.3388671875
13_o proxy err 0.019747303798794746 tr(WHW.T) 46.05501174926758
13_up proxy err 0.010186702013015747 tr(WHW.T) 1366.56396484375
13_gate proxy err 0.005384831689298153 tr(WHW.T) 2599.244384765625
13_down proxy err 0.01507359929382801 tr(WHW.T) 79.67666625976562
I0313 08:07:59.650743 2130708 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0007409994723275304 old loss 0.0007410114048980176 BETTER
12_v proxy err 0.015598466619849205 tr(WHW.T) 703.318603515625
12_q proxy err 0.001735162572003901 tr(WHW.T) 7050.86279296875
12_k proxy err 0.0011289557442069054 tr(WHW.T) 10897.9697265625
12_o proxy err 0.021729152649641037 tr(WHW.T) 39.482303619384766
12_up proxy err 0.010708527639508247 tr(WHW.T) 1227.5115966796875
12_gate proxy err 0.005563299171626568 tr(WHW.T) 2382.4697265625
12_down proxy err 0.01536633726209402 tr(WHW.T) 64.42455291748047
I0313 08:08:04.139358 2133047 finetune.py:45] layer 15_down initial loss 0.001164875691756606
I0313 08:08:26.481989 2132274 finetune.py:68] layer 14_down @ epoch 2 new loss 0.0010162325343117118 old loss 0.0010162924882024527 BETTER
I0313 08:08:29.368225 2133047 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0011646865168586373 old loss 0.001164875691756606 BETTER
I0313 08:08:53.433472 2132274 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0010161952814087272 old loss 0.0010162325343117118 BETTER
I0313 08:08:55.472867 2133047 finetune.py:68] layer 15_down @ epoch 1 new loss 0.0011645732447504997 old loss 0.0011646865168586373 BETTER
I0313 08:09:19.148335 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 16 in 73.0448477268219s
I0313 08:09:20.267117 2132274 finetune.py:68] layer 14_down @ epoch 4 new loss 0.001016169902868569 old loss 0.0010161952814087272 BETTER
14_v proxy err 0.016913844272494316 tr(WHW.T) 706.1612548828125
14_q proxy err 0.0018681212095543742 tr(WHW.T) 7081.1669921875
14_k proxy err 0.0011739458423107862 tr(WHW.T) 11300.1318359375
14_o proxy err 0.02170192264020443 tr(WHW.T) 51.184165954589844
14_up proxy err 0.01053262036293745 tr(WHW.T) 1464.55810546875
14_gate proxy err 0.0057720765471458435 tr(WHW.T) 2684.765869140625
14_down proxy err 0.015511148609220982 tr(WHW.T) 90.64920806884766
I0313 08:09:21.773286 2133047 finetune.py:68] layer 15_down @ epoch 2 new loss 0.001164505840279162 old loss 0.0011645732447504997 BETTER
I0313 08:09:22.466439 2143638 config.py:54] PyTorch version 2.1.1 available.
I0313 08:09:23.479372 2090717 quantize_finetune_llama.py:184] layer 17 gpu 1
I0313 08:09:23.547898 2143638 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:09:40.264126 2143638 finetune.py:45] layer 16_v initial loss 0.0002589044743217528
I0313 08:09:48.070944 2133047 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0011644675396382809 old loss 0.001164505840279162 BETTER
I0313 08:10:13.091849 2143638 finetune.py:68] layer 16_v @ epoch 0 new loss 0.00020144754671491683 old loss 0.0002589044743217528 BETTER
I0313 08:10:14.303603 2133047 finetune.py:68] layer 15_down @ epoch 4 new loss 0.001164440531283617 old loss 0.0011644675396382809 BETTER
15_v proxy err 0.015360572375357151 tr(WHW.T) 762.7275390625
15_q proxy err 0.0017709623789414763 tr(WHW.T) 7256.3037109375
15_k proxy err 0.0011681533651426435 tr(WHW.T) 11076.6416015625
15_o proxy err 0.01848532259464264 tr(WHW.T) 59.893348693847656
15_up proxy err 0.010095970705151558 tr(WHW.T) 1640.842529296875
15_gate proxy err 0.005723586305975914 tr(WHW.T) 2906.962158203125
15_down proxy err 0.015190916135907173 tr(WHW.T) 114.59236145019531
I0313 08:10:34.341773 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 17 in 69.72615909576416s
I0313 08:10:37.303131 2144417 config.py:54] PyTorch version 2.1.1 available.
I0313 08:10:38.520563 2090717 quantize_finetune_llama.py:184] layer 18 gpu 2
I0313 08:10:38.588412 2144417 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:10:47.301936 2143638 finetune.py:68] layer 16_v @ epoch 1 new loss 0.00019324665481690317 old loss 0.00020144754671491683 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:10:55.068420 2144417 finetune.py:45] layer 17_v initial loss 0.00022564601385965943
I0313 08:11:21.728715 2143638 finetune.py:68] layer 16_v @ epoch 2 new loss 0.00018804913270287216 old loss 0.00019324665481690317 BETTER
I0313 08:11:26.179308 2144417 finetune.py:68] layer 17_v @ epoch 0 new loss 0.00016603224503342062 old loss 0.00022564601385965943 BETTER
I0313 08:11:47.542043 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 18 in 68.62113738059998s
I0313 08:11:50.684780 2145190 config.py:54] PyTorch version 2.1.1 available.
I0313 08:11:51.761799 2090717 quantize_finetune_llama.py:184] layer 19 gpu 3
I0313 08:11:51.832408 2145190 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:11:56.336268 2143638 finetune.py:68] layer 16_v @ epoch 3 new loss 0.00018416382954455912 old loss 0.00018804913270287216 BETTER
I0313 08:11:58.132455 2144417 finetune.py:68] layer 17_v @ epoch 1 new loss 0.0001588966988492757 old loss 0.00016603224503342062 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:12:08.723648 2145190 finetune.py:45] layer 18_v initial loss 0.00025819215807132423
I0313 08:12:30.371211 2144417 finetune.py:68] layer 17_v @ epoch 2 new loss 0.0001545628474559635 old loss 0.0001588966988492757 BETTER
I0313 08:12:31.071248 2143638 finetune.py:68] layer 16_v @ epoch 4 new loss 0.00018101533351000398 old loss 0.00018416382954455912 BETTER
I0313 08:12:40.041364 2145190 finetune.py:68] layer 18_v @ epoch 0 new loss 0.00017237990687135607 old loss 0.00025819215807132423 BETTER
I0313 08:12:48.781742 2143638 finetune.py:45] layer 16_q initial loss 0.00021305953850969672
I0313 08:13:01.128871 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 19 in 68.87660360336304s
I0313 08:13:02.982200 2144417 finetune.py:68] layer 17_v @ epoch 3 new loss 0.0001513112656539306 old loss 0.0001545628474559635 BETTER
I0313 08:13:04.521945 2145952 config.py:54] PyTorch version 2.1.1 available.
I0313 08:13:05.683615 2090717 quantize_finetune_llama.py:184] layer 20 gpu 0
I0313 08:13:05.760533 2145952 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:13:12.509566 2145190 finetune.py:68] layer 18_v @ epoch 1 new loss 0.00016499239427503198 old loss 0.00017237990687135607 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:13:21.293540 2143638 finetune.py:68] layer 16_q @ epoch 0 new loss 0.00020759092876687646 old loss 0.00021305953850969672 BETTER
I0313 08:13:23.167954 2145952 finetune.py:45] layer 19_v initial loss 0.00025889542303048074
I0313 08:13:35.332592 2144417 finetune.py:68] layer 17_v @ epoch 4 new loss 0.00014882606046739966 old loss 0.0001513112656539306 BETTER
I0313 08:13:44.963340 2145190 finetune.py:68] layer 18_v @ epoch 2 new loss 0.00016086135292425752 old loss 0.00016499239427503198 BETTER
I0313 08:13:52.978058 2144417 finetune.py:45] layer 17_q initial loss 0.00018067227210849524
I0313 08:13:54.172112 2145952 finetune.py:68] layer 19_v @ epoch 0 new loss 0.00017033900076057762 old loss 0.00025889542303048074 BETTER
I0313 08:13:55.109048 2143638 finetune.py:68] layer 16_q @ epoch 1 new loss 0.00020403973758220673 old loss 0.00020759092876687646 BETTER
I0313 08:14:17.712751 2145190 finetune.py:68] layer 18_v @ epoch 3 new loss 0.00015776223153807223 old loss 0.00016086135292425752 BETTER
I0313 08:14:24.791522 2144417 finetune.py:68] layer 17_q @ epoch 0 new loss 0.0001749820658005774 old loss 0.00018067227210849524 BETTER
I0313 08:14:26.578278 2145952 finetune.py:68] layer 19_v @ epoch 1 new loss 0.00016290851635858417 old loss 0.00017033900076057762 BETTER
I0313 08:14:29.198633 2143638 finetune.py:68] layer 16_q @ epoch 2 new loss 0.0002011414326261729 old loss 0.00020403973758220673 BETTER
I0313 08:14:50.764196 2145190 finetune.py:68] layer 18_v @ epoch 4 new loss 0.00015543030167464167 old loss 0.00015776223153807223 BETTER
I0313 08:14:57.059127 2144417 finetune.py:68] layer 17_q @ epoch 1 new loss 0.00017175177345052361 old loss 0.0001749820658005774 BETTER
I0313 08:14:58.790957 2145952 finetune.py:68] layer 19_v @ epoch 2 new loss 0.00015884973981883377 old loss 0.00016290851635858417 BETTER
I0313 08:15:03.583028 2143638 finetune.py:68] layer 16_q @ epoch 3 new loss 0.00019867396622430533 old loss 0.0002011414326261729 BETTER
I0313 08:15:08.814656 2145190 finetune.py:45] layer 18_q initial loss 0.00019841949688270688
I0313 08:15:29.348221 2144417 finetune.py:68] layer 17_q @ epoch 2 new loss 0.00016910105478018522 old loss 0.00017175177345052361 BETTER
I0313 08:15:31.202883 2145952 finetune.py:68] layer 19_v @ epoch 3 new loss 0.0001560359523864463 old loss 0.00015884973981883377 BETTER
I0313 08:15:38.083848 2143638 finetune.py:68] layer 16_q @ epoch 4 new loss 0.00019656353106256574 old loss 0.00019867396622430533 BETTER
I0313 08:15:40.388160 2145190 finetune.py:68] layer 18_q @ epoch 0 new loss 0.0001917760237120092 old loss 0.00019841949688270688 BETTER
I0313 08:15:56.457859 2143638 finetune.py:45] layer 16_k initial loss 0.00021965711493976414
I0313 08:16:01.660448 2144417 finetune.py:68] layer 17_q @ epoch 3 new loss 0.00016691854398231953 old loss 0.00016910105478018522 BETTER
I0313 08:16:03.737988 2145952 finetune.py:68] layer 19_v @ epoch 4 new loss 0.00015383161371573806 old loss 0.0001560359523864463 BETTER
I0313 08:16:12.798663 2145190 finetune.py:68] layer 18_q @ epoch 1 new loss 0.00018825134611688554 old loss 0.0001917760237120092 BETTER
I0313 08:16:21.956345 2145952 finetune.py:45] layer 19_q initial loss 0.00019258307293057442
I0313 08:16:29.546816 2143638 finetune.py:68] layer 16_k @ epoch 0 new loss 0.00021711940644308925 old loss 0.00021965711493976414 BETTER
I0313 08:16:34.109471 2144417 finetune.py:68] layer 17_q @ epoch 4 new loss 0.00016505275561939925 old loss 0.00016691854398231953 BETTER
I0313 08:16:45.325996 2145190 finetune.py:68] layer 18_q @ epoch 2 new loss 0.0001855148875620216 old loss 0.00018825134611688554 BETTER
I0313 08:16:53.291632 2144417 finetune.py:45] layer 17_k initial loss 0.00018835127411875874
I0313 08:16:54.112934 2145952 finetune.py:68] layer 19_q @ epoch 0 new loss 0.00018614962755236775 old loss 0.00019258307293057442 BETTER
I0313 08:17:03.435148 2143638 finetune.py:68] layer 16_k @ epoch 1 new loss 0.0002154016401618719 old loss 0.00021711940644308925 BETTER
I0313 08:17:17.864435 2145190 finetune.py:68] layer 18_q @ epoch 3 new loss 0.0001832463312894106 old loss 0.0001855148875620216 BETTER
I0313 08:17:24.439579 2144417 finetune.py:68] layer 17_k @ epoch 0 new loss 0.00018567542429082096 old loss 0.00018835127411875874 BETTER
I0313 08:17:26.506671 2145952 finetune.py:68] layer 19_q @ epoch 1 new loss 0.00018308425205759704 old loss 0.00018614962755236775 BETTER
I0313 08:17:37.017460 2143638 finetune.py:68] layer 16_k @ epoch 2 new loss 0.00021394404757302254 old loss 0.0002154016401618719 BETTER
I0313 08:17:50.365984 2145190 finetune.py:68] layer 18_q @ epoch 4 new loss 0.00018134049605578184 old loss 0.0001832463312894106 BETTER
I0313 08:17:58.962229 2144417 finetune.py:68] layer 17_k @ epoch 1 new loss 0.00018407679453957826 old loss 0.00018567542429082096 BETTER
I0313 08:18:00.935843 2145952 finetune.py:68] layer 19_q @ epoch 2 new loss 0.00018070753139909357 old loss 0.00018308425205759704 BETTER
I0313 08:18:10.403196 2145190 finetune.py:45] layer 18_k initial loss 0.0002123474405379966
I0313 08:18:11.905711 2143638 finetune.py:68] layer 16_k @ epoch 3 new loss 0.00021267912234179676 old loss 0.00021394404757302254 BETTER
I0313 08:18:31.835104 2144417 finetune.py:68] layer 17_k @ epoch 2 new loss 0.00018274392641615123 old loss 0.00018407679453957826 BETTER
I0313 08:18:33.681825 2145952 finetune.py:68] layer 19_q @ epoch 3 new loss 0.0001787392102414742 old loss 0.00018070753139909357 BETTER
I0313 08:18:43.099898 2145190 finetune.py:68] layer 18_k @ epoch 0 new loss 0.0002097061660606414 old loss 0.0002123474405379966 BETTER
I0313 08:18:46.834454 2143638 finetune.py:68] layer 16_k @ epoch 4 new loss 0.0002115357347065583 old loss 0.00021267912234179676 BETTER
I0313 08:19:05.447707 2144417 finetune.py:68] layer 17_k @ epoch 3 new loss 0.00018163186905439943 old loss 0.00018274392641615123 BETTER
I0313 08:19:07.565139 2145952 finetune.py:68] layer 19_q @ epoch 4 new loss 0.00017705425852909684 old loss 0.0001787392102414742 BETTER
I0313 08:19:08.273611 2143638 finetune.py:45] layer 16_o initial loss 0.0005378163186833262
I0313 08:19:16.190191 2145190 finetune.py:68] layer 18_k @ epoch 1 new loss 0.00020811620925087482 old loss 0.0002097061660606414 BETTER
I0313 08:19:27.926903 2145952 finetune.py:45] layer 19_k initial loss 0.0002057855890598148
I0313 08:19:38.892640 2144417 finetune.py:68] layer 17_k @ epoch 4 new loss 0.00018062329036183655 old loss 0.00018163186905439943 BETTER
I0313 08:19:40.901733 2143638 finetune.py:68] layer 16_o @ epoch 0 new loss 0.000525272567756474 old loss 0.0005378163186833262 BETTER
I0313 08:19:48.532857 2145190 finetune.py:68] layer 18_k @ epoch 2 new loss 0.00020684153423644602 old loss 0.00020811620925087482 BETTER
I0313 08:19:58.231162 2145952 finetune.py:68] layer 19_k @ epoch 0 new loss 0.00020319873874541372 old loss 0.0002057855890598148 BETTER
I0313 08:19:58.231238 2144417 finetune.py:45] layer 17_o initial loss 0.0004305485635995865
I0313 08:20:13.955897 2143638 finetune.py:68] layer 16_o @ epoch 1 new loss 0.0005181669839657843 old loss 0.000525272567756474 BETTER
I0313 08:20:20.642494 2145190 finetune.py:68] layer 18_k @ epoch 3 new loss 0.00020577805116772652 old loss 0.00020684153423644602 BETTER
I0313 08:20:28.322792 2144417 finetune.py:68] layer 17_o @ epoch 0 new loss 0.0004214711661916226 old loss 0.0004305485635995865 BETTER
I0313 08:20:29.712966 2145952 finetune.py:68] layer 19_k @ epoch 1 new loss 0.00020161048450972885 old loss 0.00020319873874541372 BETTER
I0313 08:20:47.192002 2143638 finetune.py:68] layer 16_o @ epoch 2 new loss 0.0005125071038492024 old loss 0.0005181669839657843 BETTER
I0313 08:20:52.613347 2145190 finetune.py:68] layer 18_k @ epoch 4 new loss 0.00020486499124672264 old loss 0.00020577805116772652 BETTER
I0313 08:20:59.395852 2144417 finetune.py:68] layer 17_o @ epoch 1 new loss 0.0004161742690484971 old loss 0.0004214711661916226 BETTER
I0313 08:21:01.178976 2145952 finetune.py:68] layer 19_k @ epoch 2 new loss 0.0002003999543376267 old loss 0.00020161048450972885 BETTER
I0313 08:21:10.124320 2145190 finetune.py:45] layer 18_o initial loss 0.0004876565944869071
I0313 08:21:20.546657 2143638 finetune.py:68] layer 16_o @ epoch 3 new loss 0.0005077063106000423 old loss 0.0005125071038492024 BETTER
I0313 08:21:30.699031 2144417 finetune.py:68] layer 17_o @ epoch 2 new loss 0.00041205438901670277 old loss 0.0004161742690484971 BETTER
I0313 08:21:32.834221 2145952 finetune.py:68] layer 19_k @ epoch 3 new loss 0.00019937288016080856 old loss 0.0002003999543376267 BETTER
I0313 08:21:40.598321 2145190 finetune.py:68] layer 18_o @ epoch 0 new loss 0.00047693023225292563 old loss 0.0004876565944869071 BETTER
I0313 08:21:53.805067 2143638 finetune.py:68] layer 16_o @ epoch 4 new loss 0.000503556861076504 old loss 0.0005077063106000423 BETTER
I0313 08:22:02.067079 2144417 finetune.py:68] layer 17_o @ epoch 3 new loss 0.00040855896077118814 old loss 0.00041205438901670277 BETTER
I0313 08:22:04.504445 2145952 finetune.py:68] layer 19_k @ epoch 4 new loss 0.00019857072038576007 old loss 0.00019937288016080856 BETTER
I0313 08:22:12.108390 2145190 finetune.py:68] layer 18_o @ epoch 1 new loss 0.0004711422952823341 old loss 0.00047693023225292563 BETTER
I0313 08:22:17.407572 2143638 finetune.py:45] layer 16_up initial loss 0.0007989253499545157
I0313 08:22:22.932929 2145952 finetune.py:45] layer 19_o initial loss 0.00046712023322470486
I0313 08:22:34.392815 2144417 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0004056205216329545 old loss 0.00040855896077118814 BETTER
I0313 08:22:43.952366 2145190 finetune.py:68] layer 18_o @ epoch 2 new loss 0.00046657101484015584 old loss 0.0004711422952823341 BETTER
I0313 08:22:48.169890 2143638 finetune.py:68] layer 16_up @ epoch 0 new loss 0.0007898393087089062 old loss 0.0007989253499545157 BETTER
I0313 08:22:53.124942 2145952 finetune.py:68] layer 19_o @ epoch 0 new loss 0.0004572389298118651 old loss 0.00046712023322470486 BETTER
I0313 08:22:58.314754 2144417 finetune.py:45] layer 17_up initial loss 0.0007478034240193665
I0313 08:23:15.689779 2145190 finetune.py:68] layer 18_o @ epoch 3 new loss 0.00046279942034743726 old loss 0.00046657101484015584 BETTER
I0313 08:23:19.899238 2143638 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0007836520089767873 old loss 0.0007898393087089062 BETTER
I0313 08:23:24.614478 2145952 finetune.py:68] layer 19_o @ epoch 1 new loss 0.00045226176735013723 old loss 0.0004572389298118651 BETTER
I0313 08:23:27.369273 2144417 finetune.py:68] layer 17_up @ epoch 0 new loss 0.0007393212872557342 old loss 0.0007478034240193665 BETTER
I0313 08:23:47.500711 2145190 finetune.py:68] layer 18_o @ epoch 4 new loss 0.0004596190992742777 old loss 0.00046279942034743726 BETTER
I0313 08:23:51.627843 2143638 finetune.py:68] layer 16_up @ epoch 2 new loss 0.0007785257766954601 old loss 0.0007836520089767873 BETTER
I0313 08:23:55.766427 2145952 finetune.py:68] layer 19_o @ epoch 2 new loss 0.00044839701149612665 old loss 0.00045226176735013723 BETTER
I0313 08:23:56.872769 2144417 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0007336079142987728 old loss 0.0007393212872557342 BETTER
I0313 08:24:11.796461 2145190 finetune.py:45] layer 18_up initial loss 0.0008738227770663798
I0313 08:24:23.782621 2143638 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0007740447181276977 old loss 0.0007785257766954601 BETTER
I0313 08:24:27.483873 2144417 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0007289553177542984 old loss 0.0007336079142987728 BETTER
I0313 08:24:27.679878 2145952 finetune.py:68] layer 19_o @ epoch 3 new loss 0.000445353303803131 old loss 0.00044839701149612665 BETTER
I0313 08:24:40.879594 2145190 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0008636852144263685 old loss 0.0008738227770663798 BETTER
I0313 08:24:55.678723 2143638 finetune.py:68] layer 16_up @ epoch 4 new loss 0.0007700882852077484 old loss 0.0007740447181276977 BETTER
I0313 08:24:57.380733 2144417 finetune.py:68] layer 17_up @ epoch 3 new loss 0.0007248607580550015 old loss 0.0007289553177542984 BETTER
I0313 08:24:58.991494 2145952 finetune.py:68] layer 19_o @ epoch 4 new loss 0.0004427866078913212 old loss 0.000445353303803131 BETTER
I0313 08:25:11.108359 2145190 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0008571558282710612 old loss 0.0008636852144263685 BETTER
I0313 08:25:20.369934 2143638 finetune.py:45] layer 16_gate initial loss 0.000996837974525988
I0313 08:25:23.956106 2145952 finetune.py:45] layer 19_up initial loss 0.0009130509570240974
I0313 08:25:27.032721 2144417 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0007212732452899218 old loss 0.0007248607580550015 BETTER
I0313 08:25:41.276400 2145190 finetune.py:68] layer 18_up @ epoch 2 new loss 0.0008518233080394566 old loss 0.0008571558282710612 BETTER
I0313 08:25:49.509540 2143638 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.000991361914202571 old loss 0.000996837974525988 BETTER
I0313 08:25:51.095279 2144417 finetune.py:45] layer 17_gate initial loss 0.0009838264668360353
I0313 08:25:53.371341 2145952 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0009024699684232473 old loss 0.0009130509570240974 BETTER
I0313 08:26:12.210118 2145190 finetune.py:68] layer 18_up @ epoch 3 new loss 0.0008471679175272584 old loss 0.0008518233080394566 BETTER
I0313 08:26:19.526467 2144417 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0009785513393580914 old loss 0.0009838264668360353 BETTER
I0313 08:26:20.498854 2143638 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0009869809728115797 old loss 0.000991361914202571 BETTER
I0313 08:26:23.191998 2145952 finetune.py:68] layer 19_up @ epoch 1 new loss 0.000895876728463918 old loss 0.0009024699684232473 BETTER
I0313 08:26:43.418025 2145190 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0008431206806562841 old loss 0.0008471679175272584 BETTER
I0313 08:26:48.346564 2144417 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0009743288392201066 old loss 0.0009785513393580914 BETTER
I0313 08:26:50.724891 2143638 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.0009832477662712336 old loss 0.0009869809728115797 BETTER
I0313 08:26:53.045220 2145952 finetune.py:68] layer 19_up @ epoch 2 new loss 0.0008905345457606018 old loss 0.000895876728463918 BETTER
I0313 08:27:09.854671 2145190 finetune.py:45] layer 18_gate initial loss 0.0011547190370038152
I0313 08:27:18.049158 2144417 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.0009707919089123607 old loss 0.0009743288392201066 BETTER
I0313 08:27:21.872426 2143638 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0009799982653930783 old loss 0.0009832477662712336 BETTER
I0313 08:27:23.388268 2145952 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0008860061061568558 old loss 0.0008905345457606018 BETTER
I0313 08:27:37.678133 2145190 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0011490191100165248 old loss 0.0011547190370038152 BETTER
I0313 08:27:47.285743 2144417 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.00096769945230335 old loss 0.0009707919089123607 BETTER
I0313 08:27:52.919591 2143638 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.0009770954493433237 old loss 0.0009799982653930783 BETTER
I0313 08:27:54.572710 2145952 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0008821673691272736 old loss 0.0008860061061568558 BETTER
I0313 08:28:06.219456 2145190 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.0011443526018410921 old loss 0.0011490191100165248 BETTER
I0313 08:28:16.211439 2144417 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.0009649800485931337 old loss 0.00096769945230335 BETTER
I0313 08:28:20.206334 2145952 finetune.py:45] layer 19_gate initial loss 0.001251875888556242
I0313 08:28:34.957026 2145190 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0011404013494029641 old loss 0.0011443526018410921 BETTER
I0313 08:28:36.452134 2143638 finetune.py:45] layer 16_down initial loss 0.0015421032439917326
I0313 08:28:47.932049 2145952 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.001245805062353611 old loss 0.001251875888556242 BETTER
I0313 08:29:03.905997 2144417 finetune.py:45] layer 17_down initial loss 0.0016056847525760531
I0313 08:29:05.280189 2143638 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0015417564427480102 old loss 0.0015421032439917326 BETTER
I0313 08:29:05.953105 2145190 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0011369955027475953 old loss 0.0011404013494029641 BETTER
I0313 08:29:17.358707 2145952 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.001241078949533403 old loss 0.001245805062353611 BETTER
I0313 08:29:29.982936 2144417 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0016052918508648872 old loss 0.0016056847525760531 BETTER
I0313 08:29:34.347736 2143638 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0015415247762575746 old loss 0.0015417564427480102 BETTER
I0313 08:29:35.466253 2145190 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.0011340139899402857 old loss 0.0011369955027475953 BETTER
I0313 08:29:46.316379 2145952 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0012371200136840343 old loss 0.001241078949533403 BETTER
I0313 08:29:57.004458 2144417 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0016050315462052822 old loss 0.0016052918508648872 BETTER
I0313 08:30:03.046827 2143638 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0015413732035085559 old loss 0.0015415247762575746 BETTER
I0313 08:30:15.508494 2145952 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0012337027583271265 old loss 0.0012371200136840343 BETTER
I0313 08:30:20.586205 2145190 finetune.py:45] layer 18_down initial loss 0.0018956235144287348
I0313 08:30:24.588390 2144417 finetune.py:68] layer 17_down @ epoch 2 new loss 0.001604852732270956 old loss 0.0016050315462052822 BETTER
I0313 08:30:32.466423 2143638 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0015412698267027736 old loss 0.0015413732035085559 BETTER
I0313 08:30:45.061463 2145952 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.0012307577999308705 old loss 0.0012337027583271265 BETTER
I0313 08:30:47.104737 2145190 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0018951199017465115 old loss 0.0018956235144287348 BETTER
I0313 08:30:51.377448 2144417 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0016047325916588306 old loss 0.001604852732270956 BETTER
I0313 08:31:01.028313 2143638 finetune.py:68] layer 16_down @ epoch 4 new loss 0.001541203004308045 old loss 0.0015412698267027736 BETTER
16_v proxy err 0.015978697687387466 tr(WHW.T) 780.7407836914062
16_q proxy err 0.0018788984743878245 tr(WHW.T) 7196.1787109375
16_k proxy err 0.0011675432324409485 tr(WHW.T) 11634.5791015625
16_o proxy err 0.016570521518588066 tr(WHW.T) 88.56233215332031
16_up proxy err 0.009906265884637833 tr(WHW.T) 1888.2274169921875
16_gate proxy err 0.005596870556473732 tr(WHW.T) 3363.895263671875
16_down proxy err 0.015502866357564926 tr(WHW.T) 152.79708862304688
I0313 08:31:14.238742 2145190 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0018947552889585495 old loss 0.0018951199017465115 BETTER
I0313 08:31:18.643231 2144417 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0016046523815020919 old loss 0.0016047325916588306 BETTER
17_v proxy err 0.015828043222427368 tr(WHW.T) 845.7654418945312
17_q proxy err 0.002026145113632083 tr(WHW.T) 7168.30029296875
17_k proxy err 0.0013617182848975062 tr(WHW.T) 10702.8291015625
17_o proxy err 0.018793649971485138 tr(WHW.T) 58.385894775390625
17_up proxy err 0.011246156878769398 tr(WHW.T) 1921.4407958984375
17_gate proxy err 0.006114508491009474 tr(WHW.T) 3569.40966796875
17_down proxy err 0.016183556988835335 tr(WHW.T) 166.2469940185547
I0313 08:31:28.268440 2145952 finetune.py:45] layer 19_down initial loss 0.002073390409350395
I0313 08:31:40.971927 2145190 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0018944975454360247 old loss 0.0018947552889585495 BETTER
I0313 08:31:53.120136 2145952 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0020728332456201315 old loss 0.002073390409350395 BETTER
I0313 08:32:07.685174 2145190 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0018943196628242731 old loss 0.0018944975454360247 BETTER
I0313 08:32:19.053935 2145952 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0020724439527839422 old loss 0.0020728332456201315 BETTER
I0313 08:32:34.497506 2145190 finetune.py:68] layer 18_down @ epoch 4 new loss 0.001894187182188034 old loss 0.0018943196628242731 BETTER
I0313 08:32:35.194803 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 20 in 72.1051254272461s
18_v proxy err 0.015446105040609837 tr(WHW.T) 1003.7705078125
18_q proxy err 0.002205604687333107 tr(WHW.T) 7514.0517578125
18_k proxy err 0.0015896958066150546 tr(WHW.T) 10465.02734375
18_o proxy err 0.017075780779123306 tr(WHW.T) 70.23438262939453
18_up proxy err 0.012044625356793404 tr(WHW.T) 2023.1668701171875
18_gate proxy err 0.006535088177770376 tr(WHW.T) 3778.393798828125
18_down proxy err 0.015968406572937965 tr(WHW.T) 199.60128784179688
I0313 08:32:38.337105 2156521 config.py:54] PyTorch version 2.1.1 available.
I0313 08:32:39.402869 2090717 quantize_finetune_llama.py:184] layer 21 gpu 1
I0313 08:32:39.480855 2156521 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:32:45.128834 2145952 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0020721687469631433 old loss 0.0020724439527839422 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:32:56.516396 2156521 finetune.py:45] layer 20_v initial loss 0.00029296951834112406
I0313 08:33:11.481247 2145952 finetune.py:68] layer 19_down @ epoch 3 new loss 0.002071977127343416 old loss 0.0020721687469631433 BETTER
I0313 08:33:29.360471 2156521 finetune.py:68] layer 20_v @ epoch 0 new loss 0.00019909636466763914 old loss 0.00029296951834112406 BETTER
I0313 08:33:37.853172 2145952 finetune.py:68] layer 19_down @ epoch 4 new loss 0.002071849536150694 old loss 0.002071977127343416 BETTER
19_v proxy err 0.015157570131123066 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.0023613348603248596 tr(WHW.T) 6950.0556640625
19_k proxy err 0.0015594476135447621 tr(WHW.T) 10554.046875
19_o proxy err 0.017511874437332153 tr(WHW.T) 62.54766845703125
19_up proxy err 0.01223843079060316 tr(WHW.T) 2149.2587890625
19_gate proxy err 0.007237662095576525 tr(WHW.T) 3687.162353515625
19_down proxy err 0.01576552726328373 tr(WHW.T) 224.03245544433594
I0313 08:33:49.086373 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 21 in 69.25884485244751s
I0313 08:33:52.160485 2157290 config.py:54] PyTorch version 2.1.1 available.
I0313 08:33:53.134874 2090717 quantize_finetune_llama.py:184] layer 22 gpu 2
I0313 08:33:53.219285 2157290 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:34:03.613297 2156521 finetune.py:68] layer 20_v @ epoch 1 new loss 0.0001895956083899364 old loss 0.00019909636466763914 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:34:09.916583 2157290 finetune.py:45] layer 21_v initial loss 0.0002872913028113544
I0313 08:34:38.204572 2156521 finetune.py:68] layer 20_v @ epoch 2 new loss 0.00018478040874470025 old loss 0.0001895956083899364 BETTER
I0313 08:34:40.953937 2157290 finetune.py:68] layer 21_v @ epoch 0 new loss 0.00018187744717579335 old loss 0.0002872913028113544 BETTER
I0313 08:35:02.887493 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 22 in 69.35446882247925s
I0313 08:35:06.000088 2158071 config.py:54] PyTorch version 2.1.1 available.
I0313 08:35:06.999924 2090717 quantize_finetune_llama.py:184] layer 23 gpu 3
I0313 08:35:07.079054 2158071 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:35:12.920934 2157290 finetune.py:68] layer 21_v @ epoch 1 new loss 0.0001735013647703454 old loss 0.00018187744717579335 BETTER
I0313 08:35:12.992679 2156521 finetune.py:68] layer 20_v @ epoch 3 new loss 0.00018150308460462838 old loss 0.00018478040874470025 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:35:24.267348 2158071 finetune.py:45] layer 22_v initial loss 0.00034998025512322783
I0313 08:35:45.061105 2157290 finetune.py:68] layer 21_v @ epoch 2 new loss 0.00016951806901488453 old loss 0.0001735013647703454 BETTER
I0313 08:35:47.757967 2156521 finetune.py:68] layer 20_v @ epoch 4 new loss 0.00017902300169225782 old loss 0.00018150308460462838 BETTER
I0313 08:35:55.687855 2158071 finetune.py:68] layer 22_v @ epoch 0 new loss 0.00022967683617025614 old loss 0.00034998025512322783 BETTER
I0313 08:36:05.271703 2156521 finetune.py:45] layer 20_q initial loss 0.0002241198963019997
I0313 08:36:17.196723 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 23 in 69.80715560913086s
I0313 08:36:17.544322 2157290 finetune.py:68] layer 21_v @ epoch 3 new loss 0.00016698840772733092 old loss 0.00016951806901488453 BETTER
I0313 08:36:20.931695 2158858 config.py:54] PyTorch version 2.1.1 available.
I0313 08:36:22.139200 2090717 quantize_finetune_llama.py:184] layer 24 gpu 0
I0313 08:36:22.208799 2158858 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:36:28.063926 2158071 finetune.py:68] layer 22_v @ epoch 1 new loss 0.00021864697919227183 old loss 0.00022967683617025614 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:36:38.203720 2156521 finetune.py:68] layer 20_q @ epoch 0 new loss 0.00021763761469628662 old loss 0.0002241198963019997 BETTER
I0313 08:36:39.435969 2158858 finetune.py:45] layer 23_v initial loss 0.0003865543694701046
I0313 08:36:50.361945 2157290 finetune.py:68] layer 21_v @ epoch 4 new loss 0.00016509713896084577 old loss 0.00016698840772733092 BETTER
I0313 08:37:00.554080 2158071 finetune.py:68] layer 22_v @ epoch 2 new loss 0.00021346897119656205 old loss 0.00021864697919227183 BETTER
I0313 08:37:07.831773 2157290 finetune.py:45] layer 21_q initial loss 0.00020473542099352926
I0313 08:37:10.463586 2158858 finetune.py:68] layer 23_v @ epoch 0 new loss 0.0002389375731581822 old loss 0.0003865543694701046 BETTER
I0313 08:37:12.299507 2156521 finetune.py:68] layer 20_q @ epoch 1 new loss 0.00021411565830931067 old loss 0.00021763761469628662 BETTER
I0313 08:37:33.605324 2158071 finetune.py:68] layer 22_v @ epoch 3 new loss 0.00020994179067201912 old loss 0.00021346897119656205 BETTER
I0313 08:37:39.265430 2157290 finetune.py:68] layer 21_q @ epoch 0 new loss 0.000198803681996651 old loss 0.00020473542099352926 BETTER
I0313 08:37:42.651733 2158858 finetune.py:68] layer 23_v @ epoch 1 new loss 0.00022628044825978577 old loss 0.0002389375731581822 BETTER
I0313 08:37:46.546911 2156521 finetune.py:68] layer 20_q @ epoch 2 new loss 0.0002113555237883702 old loss 0.00021411565830931067 BETTER
I0313 08:38:06.955690 2158071 finetune.py:68] layer 22_v @ epoch 4 new loss 0.00020729113020934165 old loss 0.00020994179067201912 BETTER
I0313 08:38:11.779930 2157290 finetune.py:68] layer 21_q @ epoch 1 new loss 0.0001962641254067421 old loss 0.000198803681996651 BETTER
I0313 08:38:15.125683 2158858 finetune.py:68] layer 23_v @ epoch 2 new loss 0.00022085773525759578 old loss 0.00022628044825978577 BETTER
I0313 08:38:21.057153 2156521 finetune.py:68] layer 20_q @ epoch 3 new loss 0.00020908069564029574 old loss 0.0002113555237883702 BETTER
I0313 08:38:26.165785 2158071 finetune.py:45] layer 22_q initial loss 0.00027564686024561524
I0313 08:38:44.918605 2157290 finetune.py:68] layer 21_q @ epoch 2 new loss 0.00019430492829997092 old loss 0.0001962641254067421 BETTER
I0313 08:38:48.182609 2158858 finetune.py:68] layer 23_v @ epoch 3 new loss 0.00021748975268565118 old loss 0.00022085773525759578 BETTER
I0313 08:38:55.836445 2156521 finetune.py:68] layer 20_q @ epoch 4 new loss 0.00020719721214845777 old loss 0.00020908069564029574 BETTER
I0313 08:38:58.037518 2158071 finetune.py:68] layer 22_q @ epoch 0 new loss 0.0002661190228536725 old loss 0.00027564686024561524 BETTER
I0313 08:39:15.194261 2156521 finetune.py:45] layer 20_k initial loss 0.0002400302910245955
I0313 08:39:17.977575 2157290 finetune.py:68] layer 21_q @ epoch 3 new loss 0.00019267428433522582 old loss 0.00019430492829997092 BETTER
I0313 08:39:21.426270 2158858 finetune.py:68] layer 23_v @ epoch 4 new loss 0.00021493241365533322 old loss 0.00021748975268565118 BETTER
I0313 08:39:30.382189 2158071 finetune.py:68] layer 22_q @ epoch 1 new loss 0.0002617263817228377 old loss 0.0002661190228536725 BETTER
I0313 08:39:41.154706 2158858 finetune.py:45] layer 23_q initial loss 0.00027081830194219947
I0313 08:39:48.445571 2156521 finetune.py:68] layer 20_k @ epoch 0 new loss 0.0002366973931202665 old loss 0.0002400302910245955 BETTER
I0313 08:39:51.167484 2157290 finetune.py:68] layer 21_q @ epoch 4 new loss 0.00019131640146952122 old loss 0.00019267428433522582 BETTER
I0313 08:40:03.421745 2158071 finetune.py:68] layer 22_q @ epoch 2 new loss 0.00025819565053097904 old loss 0.0002617263817228377 BETTER
I0313 08:40:11.579782 2157290 finetune.py:45] layer 21_k initial loss 0.00022629603336099535
I0313 08:40:13.762207 2158858 finetune.py:68] layer 23_q @ epoch 0 new loss 0.0002627616049721837 old loss 0.00027081830194219947 BETTER
I0313 08:40:22.583705 2156521 finetune.py:68] layer 20_k @ epoch 1 new loss 0.00023491094179917127 old loss 0.0002366973931202665 BETTER
I0313 08:40:36.518063 2158071 finetune.py:68] layer 22_q @ epoch 3 new loss 0.00025528084370307624 old loss 0.00025819565053097904 BETTER
I0313 08:40:43.315304 2157290 finetune.py:68] layer 21_k @ epoch 0 new loss 0.00022351178631652147 old loss 0.00022629603336099535 BETTER
I0313 08:40:47.211354 2158858 finetune.py:68] layer 23_q @ epoch 1 new loss 0.0002592133532743901 old loss 0.0002627616049721837 BETTER
I0313 08:40:57.320672 2156521 finetune.py:68] layer 20_k @ epoch 2 new loss 0.0002335056778974831 old loss 0.00023491094179917127 BETTER
I0313 08:41:09.610859 2158071 finetune.py:68] layer 22_q @ epoch 4 new loss 0.00025279412511736155 old loss 0.00025528084370307624 BETTER
I0313 08:41:15.885206 2157290 finetune.py:68] layer 21_k @ epoch 1 new loss 0.0002220685564680025 old loss 0.00022351178631652147 BETTER
I0313 08:41:20.083941 2158858 finetune.py:68] layer 23_q @ epoch 2 new loss 0.00025654296041466296 old loss 0.0002592133532743901 BETTER
I0313 08:41:30.196714 2158071 finetune.py:45] layer 22_k initial loss 0.0003051691164728254
I0313 08:41:32.286557 2156521 finetune.py:68] layer 20_k @ epoch 3 new loss 0.00023239459551405162 old loss 0.0002335056778974831 BETTER
I0313 08:41:48.783747 2157290 finetune.py:68] layer 21_k @ epoch 2 new loss 0.0002210260136052966 old loss 0.0002220685564680025 BETTER
I0313 08:41:52.560310 2158858 finetune.py:68] layer 23_q @ epoch 3 new loss 0.00025429693050682545 old loss 0.00025654296041466296 BETTER
I0313 08:42:01.581948 2158071 finetune.py:68] layer 22_k @ epoch 0 new loss 0.00030189621611498296 old loss 0.0003051691164728254 BETTER
I0313 08:42:06.283989 2156521 finetune.py:68] layer 20_k @ epoch 4 new loss 0.00023147962929215282 old loss 0.00023239459551405162 BETTER
I0313 08:42:21.107892 2157290 finetune.py:68] layer 21_k @ epoch 3 new loss 0.00022011512191966176 old loss 0.0002210260136052966 BETTER
I0313 08:42:24.054764 2156521 finetune.py:45] layer 20_o initial loss 0.0005460266256704926
I0313 08:42:24.672920 2158858 finetune.py:68] layer 23_q @ epoch 4 new loss 0.0002524782612454146 old loss 0.00025429693050682545 BETTER
I0313 08:42:33.732855 2158071 finetune.py:68] layer 22_k @ epoch 1 new loss 0.00029972026823088527 old loss 0.00030189621611498296 BETTER
I0313 08:42:42.282904 2158858 finetune.py:45] layer 23_k initial loss 0.0003033423563465476
I0313 08:42:53.188492 2157290 finetune.py:68] layer 21_k @ epoch 4 new loss 0.0002194824774051085 old loss 0.00022011512191966176 BETTER
I0313 08:42:55.929588 2156521 finetune.py:68] layer 20_o @ epoch 0 new loss 0.0005349422572180629 old loss 0.0005460266256704926 BETTER
I0313 08:43:05.704199 2158071 finetune.py:68] layer 22_k @ epoch 2 new loss 0.0002980358258355409 old loss 0.00029972026823088527 BETTER
I0313 08:43:11.023539 2157290 finetune.py:45] layer 21_o initial loss 0.0005111556965857744
I0313 08:43:12.888534 2158858 finetune.py:68] layer 23_k @ epoch 0 new loss 0.00030066451290622354 old loss 0.0003033423563465476 BETTER
I0313 08:43:29.144987 2156521 finetune.py:68] layer 20_o @ epoch 1 new loss 0.0005291540292091668 old loss 0.0005349422572180629 BETTER
I0313 08:43:37.796725 2158071 finetune.py:68] layer 22_k @ epoch 3 new loss 0.0002966158208437264 old loss 0.0002980358258355409 BETTER
I0313 08:43:41.442249 2157290 finetune.py:68] layer 21_o @ epoch 0 new loss 0.0005022997502237558 old loss 0.0005111556965857744 BETTER
I0313 08:43:44.407187 2158858 finetune.py:68] layer 23_k @ epoch 1 new loss 0.0002989749482367188 old loss 0.00030066451290622354 BETTER
I0313 08:44:02.442126 2156521 finetune.py:68] layer 20_o @ epoch 2 new loss 0.000524690025486052 old loss 0.0005291540292091668 BETTER
I0313 08:44:10.024581 2158071 finetune.py:68] layer 22_k @ epoch 4 new loss 0.0002954345545731485 old loss 0.0002966158208437264 BETTER
I0313 08:44:12.705470 2157290 finetune.py:68] layer 21_o @ epoch 1 new loss 0.0004983789985999465 old loss 0.0005022997502237558 BETTER
I0313 08:44:16.078551 2158858 finetune.py:68] layer 23_k @ epoch 2 new loss 0.0002976288378704339 old loss 0.0002989749482367188 BETTER
I0313 08:44:28.512303 2158071 finetune.py:45] layer 22_o initial loss 0.0006477927672676742
I0313 08:44:35.696315 2156521 finetune.py:68] layer 20_o @ epoch 3 new loss 0.0005211798124946654 old loss 0.000524690025486052 BETTER
I0313 08:44:44.050647 2157290 finetune.py:68] layer 21_o @ epoch 2 new loss 0.0004954368341714144 old loss 0.0004983789985999465 BETTER
I0313 08:44:47.849144 2158858 finetune.py:68] layer 23_k @ epoch 3 new loss 0.0002966253086924553 old loss 0.0002976288378704339 BETTER
I0313 08:44:59.348011 2158071 finetune.py:68] layer 22_o @ epoch 0 new loss 0.0006366014131344855 old loss 0.0006477927672676742 BETTER
I0313 08:45:09.267946 2156521 finetune.py:68] layer 20_o @ epoch 4 new loss 0.0005181470769457519 old loss 0.0005211798124946654 BETTER
I0313 08:45:15.439308 2157290 finetune.py:68] layer 21_o @ epoch 3 new loss 0.0004931791918352246 old loss 0.0004954368341714144 BETTER
I0313 08:45:19.798298 2158858 finetune.py:68] layer 23_k @ epoch 4 new loss 0.00029575848020613194 old loss 0.0002966253086924553 BETTER
I0313 08:45:30.836947 2158071 finetune.py:68] layer 22_o @ epoch 1 new loss 0.0006309480522759259 old loss 0.0006366014131344855 BETTER
I0313 08:45:32.371688 2156521 finetune.py:45] layer 20_up initial loss 0.0010695349192246795
I0313 08:45:38.743900 2158858 finetune.py:45] layer 23_o initial loss 0.0006274220068007708
I0313 08:45:47.195677 2157290 finetune.py:68] layer 21_o @ epoch 4 new loss 0.0004913691082037985 old loss 0.0004931791918352246 BETTER
I0313 08:46:03.258160 2158071 finetune.py:68] layer 22_o @ epoch 2 new loss 0.0006265665870159864 old loss 0.0006309480522759259 BETTER
I0313 08:46:03.532470 2156521 finetune.py:68] layer 20_up @ epoch 0 new loss 0.0010578497312963009 old loss 0.0010695349192246795 BETTER
I0313 08:46:08.820415 2158858 finetune.py:68] layer 23_o @ epoch 0 new loss 0.0006173084839247167 old loss 0.0006274220068007708 BETTER
I0313 08:46:12.263355 2157290 finetune.py:45] layer 21_up initial loss 0.001105668256059289
I0313 08:46:35.080527 2156521 finetune.py:68] layer 20_up @ epoch 1 new loss 0.0010506322141736746 old loss 0.0010578497312963009 BETTER
I0313 08:46:35.333705 2158071 finetune.py:68] layer 22_o @ epoch 3 new loss 0.0006230648141354322 old loss 0.0006265665870159864 BETTER
I0313 08:46:40.590522 2158858 finetune.py:68] layer 23_o @ epoch 1 new loss 0.0006125698564574122 old loss 0.0006173084839247167 BETTER
I0313 08:46:41.762217 2157290 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0010955474572256207 old loss 0.001105668256059289 BETTER
I0313 08:47:06.871200 2156521 finetune.py:68] layer 20_up @ epoch 2 new loss 0.0010448418324813247 old loss 0.0010506322141736746 BETTER
I0313 08:47:07.395892 2158071 finetune.py:68] layer 22_o @ epoch 4 new loss 0.0006202518125064671 old loss 0.0006230648141354322 BETTER
I0313 08:47:11.728799 2157290 finetune.py:68] layer 21_up @ epoch 1 new loss 0.0010893065482378006 old loss 0.0010955474572256207 BETTER
I0313 08:47:11.944602 2158858 finetune.py:68] layer 23_o @ epoch 2 new loss 0.0006092331605032086 old loss 0.0006125698564574122 BETTER
I0313 08:47:32.178518 2158071 finetune.py:45] layer 22_up initial loss 0.0013249757466837764
I0313 08:47:38.915190 2156521 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0010398408630862832 old loss 0.0010448418324813247 BETTER
I0313 08:47:41.715599 2157290 finetune.py:68] layer 21_up @ epoch 2 new loss 0.0010842641349881887 old loss 0.0010893065482378006 BETTER
I0313 08:47:43.231899 2158858 finetune.py:68] layer 23_o @ epoch 3 new loss 0.0006064518238417804 old loss 0.0006092331605032086 BETTER
I0313 08:48:01.087664 2158071 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0013141125673428178 old loss 0.0013249757466837764 BETTER
I0313 08:48:10.671109 2156521 finetune.py:68] layer 20_up @ epoch 4 new loss 0.0010355691192671657 old loss 0.0010398408630862832 BETTER
I0313 08:48:11.653037 2157290 finetune.py:68] layer 21_up @ epoch 3 new loss 0.0010800698073580861 old loss 0.0010842641349881887 BETTER
I0313 08:48:14.356469 2158858 finetune.py:68] layer 23_o @ epoch 4 new loss 0.000604401808232069 old loss 0.0006064518238417804 BETTER
I0313 08:48:30.902894 2158071 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0013072735164314508 old loss 0.0013141125673428178 BETTER
I0313 08:48:34.489955 2156521 finetune.py:45] layer 20_gate initial loss 0.0014666138449683785
I0313 08:48:38.426494 2158858 finetune.py:45] layer 23_up initial loss 0.0013957973569631577
I0313 08:48:41.661853 2157290 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0010764104081317782 old loss 0.0010800698073580861 BETTER
I0313 08:49:00.774821 2158071 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0013018501922488213 old loss 0.0013072735164314508 BETTER
I0313 08:49:03.417498 2156521 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.0014601234579458833 old loss 0.0014666138449683785 BETTER
I0313 08:49:05.063128 2157290 finetune.py:45] layer 21_gate initial loss 0.001555734546855092
I0313 08:49:07.004013 2158858 finetune.py:68] layer 23_up @ epoch 0 new loss 0.0013853331329301 old loss 0.0013957973569631577 BETTER
I0313 08:49:31.078484 2158071 finetune.py:68] layer 22_up @ epoch 3 new loss 0.0012972003314644098 old loss 0.0013018501922488213 BETTER
I0313 08:49:32.800638 2157290 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0015499505680054426 old loss 0.001555734546855092 BETTER
I0313 08:49:33.431510 2156521 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0014548569452017546 old loss 0.0014601234579458833 BETTER
I0313 08:49:36.436353 2158858 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0013784572947770357 old loss 0.0013853331329301 BETTER
I0313 08:50:01.460424 2158071 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0012933678226545453 old loss 0.0012972003314644098 BETTER
I0313 08:50:01.472133 2157290 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0015454716049134731 old loss 0.0015499505680054426 BETTER
I0313 08:50:03.568928 2156521 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.001450379379093647 old loss 0.0014548569452017546 BETTER
I0313 08:50:05.886910 2158858 finetune.py:68] layer 23_up @ epoch 2 new loss 0.0013732091756537557 old loss 0.0013784572947770357 BETTER
I0313 08:50:25.360948 2158071 finetune.py:45] layer 22_gate initial loss 0.0018420994747430086
I0313 08:50:29.810345 2157290 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0015415903180837631 old loss 0.0015454716049134731 BETTER
I0313 08:50:33.492962 2156521 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0014466081047430634 old loss 0.001450379379093647 BETTER
I0313 08:50:35.287055 2158858 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0013686619931831956 old loss 0.0013732091756537557 BETTER
I0313 08:50:52.878750 2158071 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0018358428496867418 old loss 0.0018420994747430086 BETTER
I0313 08:50:58.121167 2157290 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.001538436277769506 old loss 0.0015415903180837631 BETTER
I0313 08:51:03.442315 2156521 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.0014432608149945736 old loss 0.0014466081047430634 BETTER
I0313 08:51:04.762554 2158858 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0013648546300828457 old loss 0.0013686619931831956 BETTER
I0313 08:51:20.970778 2158071 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.0018311046296730638 old loss 0.0018358428496867418 BETTER
I0313 08:51:28.226613 2157290 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.001535671530291438 old loss 0.001538436277769506 BETTER
I0313 08:51:30.119570 2158858 finetune.py:45] layer 23_gate initial loss 0.001996884122490883
I0313 08:51:44.777038 2156521 finetune.py:45] layer 20_down initial loss 0.002477353671565652
I0313 08:51:49.480392 2158071 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0018271416192874312 old loss 0.0018311046296730638 BETTER
I0313 08:51:57.333921 2158858 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0019911096896976233 old loss 0.001996884122490883 BETTER
I0313 08:52:12.322025 2156521 finetune.py:68] layer 20_down @ epoch 0 new loss 0.0024765757843852043 old loss 0.002477353671565652 BETTER
I0313 08:52:13.157566 2157290 finetune.py:45] layer 21_down initial loss 0.002625717781484127
I0313 08:52:18.837660 2158071 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.0018237207550555468 old loss 0.0018271416192874312 BETTER
I0313 08:52:26.184102 2158858 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.0019864647183567286 old loss 0.0019911096896976233 BETTER
I0313 08:52:39.720835 2157290 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0026250623632222414 old loss 0.002625717781484127 BETTER
I0313 08:52:41.608429 2156521 finetune.py:68] layer 20_down @ epoch 1 new loss 0.002475993474945426 old loss 0.0024765757843852043 BETTER
I0313 08:52:48.104925 2158071 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0018210244597867131 old loss 0.0018237207550555468 BETTER
I0313 08:52:55.060050 2158858 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.001982594607397914 old loss 0.0019864647183567286 BETTER
I0313 08:53:06.602592 2157290 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0026245592162013054 old loss 0.0026250623632222414 BETTER
I0313 08:53:10.356545 2156521 finetune.py:68] layer 20_down @ epoch 2 new loss 0.002475565066561103 old loss 0.002475993474945426 BETTER
I0313 08:53:23.359886 2158858 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.0019792348612099886 old loss 0.001982594607397914 BETTER
I0313 08:53:32.502268 2158071 finetune.py:45] layer 22_down initial loss 0.003062434960156679
I0313 08:53:33.690303 2157290 finetune.py:68] layer 21_down @ epoch 2 new loss 0.0026241790037602186 old loss 0.0026245592162013054 BETTER
I0313 08:53:39.040032 2156521 finetune.py:68] layer 20_down @ epoch 3 new loss 0.002475244924426079 old loss 0.002475565066561103 BETTER
I0313 08:53:53.384720 2158858 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0019765393808484077 old loss 0.0019792348612099886 BETTER
I0313 08:53:58.853369 2158071 finetune.py:68] layer 22_down @ epoch 0 new loss 0.003061736701056361 old loss 0.003062434960156679 BETTER
I0313 08:54:01.277000 2157290 finetune.py:68] layer 21_down @ epoch 3 new loss 0.0026238993741571903 old loss 0.0026241790037602186 BETTER
I0313 08:54:07.601996 2156521 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0024750162847340107 old loss 0.002475244924426079 BETTER
20_v proxy err 0.015991350635886192 tr(WHW.T) 990.5983276367188
20_q proxy err 0.002343161264434457 tr(WHW.T) 7154.6328125
20_k proxy err 0.0016178408404812217 tr(WHW.T) 10394.21484375
20_o proxy err 0.012364908121526241 tr(WHW.T) 100.58531951904297
20_up proxy err 0.011951878666877747 tr(WHW.T) 2340.0048828125
20_gate proxy err 0.007058441173285246 tr(WHW.T) 4024.190673828125
20_down proxy err 0.015526816248893738 tr(WHW.T) 276.1948547363281
I0313 08:54:25.933089 2158071 finetune.py:68] layer 22_down @ epoch 1 new loss 0.003061218187212944 old loss 0.003061736701056361 BETTER
I0313 08:54:28.396085 2157290 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0026236893609166145 old loss 0.0026238993741571903 BETTER
21_v proxy err 0.01569516398012638 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.0026546407025307417 tr(WHW.T) 7070.6416015625
21_k proxy err 0.0018823068821802735 tr(WHW.T) 9984.0322265625
21_o proxy err 0.015231531113386154 tr(WHW.T) 75.794189453125
21_up proxy err 0.012805606238543987 tr(WHW.T) 2361.5986328125
21_gate proxy err 0.007690070196986198 tr(WHW.T) 4005.20703125
21_down proxy err 0.01622045785188675 tr(WHW.T) 277.8256530761719
I0313 08:54:37.781279 2158858 finetune.py:45] layer 23_down initial loss 0.0032748773228377104
I0313 08:54:52.745050 2158071 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0030608298256993294 old loss 0.003061218187212944 BETTER
I0313 08:55:02.591609 2158858 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0032742335461080074 old loss 0.0032748773228377104 BETTER
I0313 08:55:19.757303 2158071 finetune.py:68] layer 22_down @ epoch 3 new loss 0.003060524584725499 old loss 0.0030608298256993294 BETTER
I0313 08:55:28.624899 2158858 finetune.py:68] layer 23_down @ epoch 1 new loss 0.003273750888183713 old loss 0.0032742335461080074 BETTER
I0313 08:55:45.449488 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 24 in 72.45451664924622s
I0313 08:55:46.789918 2158071 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0030602903570979834 old loss 0.003060524584725499 BETTER
22_v proxy err 0.015029271133244038 tr(WHW.T) 1243.2529296875
22_q proxy err 0.002532860729843378 tr(WHW.T) 7751.3505859375
22_k proxy err 0.001855243113823235 tr(WHW.T) 10608.791015625
22_o proxy err 0.012064789421856403 tr(WHW.T) 114.6304931640625
22_up proxy err 0.012995879165828228 tr(WHW.T) 2475.05322265625
22_gate proxy err 0.007890943437814713 tr(WHW.T) 4156.79931640625
22_down proxy err 0.01626439020037651 tr(WHW.T) 313.1726989746094
I0313 08:55:48.683283 2169357 config.py:54] PyTorch version 2.1.1 available.
I0313 08:55:49.730113 2090717 quantize_finetune_llama.py:184] layer 25 gpu 1
I0313 08:55:49.809042 2169357 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:55:54.797767 2158858 finetune.py:68] layer 23_down @ epoch 2 new loss 0.003273378126323223 old loss 0.003273750888183713 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:56:07.052921 2169357 finetune.py:45] layer 24_v initial loss 0.00040047758375294507
I0313 08:56:21.112406 2158858 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0032731005921959877 old loss 0.003273378126323223 BETTER
I0313 08:56:39.772786 2169357 finetune.py:68] layer 24_v @ epoch 0 new loss 0.0002681300393305719 old loss 0.00040047758375294507 BETTER
I0313 08:56:47.500896 2158858 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0032728801015764475 old loss 0.0032731005921959877 BETTER
23_v proxy err 0.014414460398256779 tr(WHW.T) 1486.037353515625
23_q proxy err 0.003029906190931797 tr(WHW.T) 7349.91845703125
23_k proxy err 0.002233269391581416 tr(WHW.T) 9987.435546875
23_o proxy err 0.015180910937488079 tr(WHW.T) 85.43501281738281
23_up proxy err 0.013682099990546703 tr(WHW.T) 2533.811767578125
23_gate proxy err 0.008619023486971855 tr(WHW.T) 4097.26416015625
23_down proxy err 0.01656605862081051 tr(WHW.T) 322.71820068359375
I0313 08:57:00.417155 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 25 in 69.35657906532288s
I0313 08:57:03.489553 2170151 config.py:54] PyTorch version 2.1.1 available.
I0313 08:57:04.456172 2090717 quantize_finetune_llama.py:184] layer 26 gpu 2
I0313 08:57:04.529014 2170151 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:57:13.957575 2169357 finetune.py:68] layer 24_v @ epoch 1 new loss 0.00025501259369775653 old loss 0.0002681300393305719 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:57:21.147732 2170151 finetune.py:45] layer 25_v initial loss 0.00044114916818216443
I0313 08:57:48.525675 2169357 finetune.py:68] layer 24_v @ epoch 2 new loss 0.00024915067479014397 old loss 0.00025501259369775653 BETTER
I0313 08:57:52.326655 2170151 finetune.py:68] layer 25_v @ epoch 0 new loss 0.000249627250013873 old loss 0.00044114916818216443 BETTER
I0313 08:58:14.184285 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 26 in 69.26720929145813s
I0313 08:58:17.439420 2170931 config.py:54] PyTorch version 2.1.1 available.
I0313 08:58:18.497288 2090717 quantize_finetune_llama.py:184] layer 27 gpu 3
I0313 08:58:18.563464 2170931 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:58:23.308400 2169357 finetune.py:68] layer 24_v @ epoch 3 new loss 0.0002454217174090445 old loss 0.00024915067479014397 BETTER
I0313 08:58:24.397294 2170151 finetune.py:68] layer 25_v @ epoch 1 new loss 0.0002346828841837123 old loss 0.000249627250013873 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:58:35.724422 2170931 finetune.py:45] layer 26_v initial loss 0.0005466158618219197
I0313 08:58:56.711131 2170151 finetune.py:68] layer 25_v @ epoch 2 new loss 0.000228947596042417 old loss 0.0002346828841837123 BETTER
I0313 08:58:58.207859 2169357 finetune.py:68] layer 24_v @ epoch 4 new loss 0.00024283416860271245 old loss 0.0002454217174090445 BETTER
I0313 08:59:07.070502 2170931 finetune.py:68] layer 26_v @ epoch 0 new loss 0.0003727101720869541 old loss 0.0005466158618219197 BETTER
I0313 08:59:16.443593 2169357 finetune.py:45] layer 24_q initial loss 0.0003095954016316682
I0313 08:59:29.317190 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 27 in 70.36542892456055s
I0313 08:59:29.462803 2170151 finetune.py:68] layer 25_v @ epoch 3 new loss 0.00022565785911865532 old loss 0.000228947596042417 BETTER
I0313 08:59:32.523794 2171710 config.py:54] PyTorch version 2.1.1 available.
I0313 08:59:33.618561 2090717 quantize_finetune_llama.py:184] layer 28 gpu 0
I0313 08:59:33.688557 2171710 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 08:59:39.254849 2170931 finetune.py:68] layer 26_v @ epoch 1 new loss 0.00035968050360679626 old loss 0.0003727101720869541 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 08:59:49.546556 2169357 finetune.py:68] layer 24_q @ epoch 0 new loss 0.0003016149566974491 old loss 0.0003095954016316682 BETTER
I0313 08:59:51.036739 2171710 finetune.py:45] layer 27_v initial loss 0.0005185031332075596
I0313 09:00:02.389378 2170151 finetune.py:68] layer 25_v @ epoch 4 new loss 0.00022335242829285562 old loss 0.00022565785911865532 BETTER
I0313 09:00:12.226898 2170931 finetune.py:68] layer 26_v @ epoch 2 new loss 0.0003529178793542087 old loss 0.00035968050360679626 BETTER
I0313 09:00:20.441799 2170151 finetune.py:45] layer 25_q initial loss 0.00028861951432190835
I0313 09:00:22.395590 2171710 finetune.py:68] layer 27_v @ epoch 0 new loss 0.0003365427546668798 old loss 0.0005185031332075596 BETTER
I0313 09:00:24.002704 2169357 finetune.py:68] layer 24_q @ epoch 1 new loss 0.00029800445190630853 old loss 0.0003016149566974491 BETTER
I0313 09:00:45.675791 2170931 finetune.py:68] layer 26_v @ epoch 3 new loss 0.00034852646058425307 old loss 0.0003529178793542087 BETTER
I0313 09:00:52.448641 2170151 finetune.py:68] layer 25_q @ epoch 0 new loss 0.0002791762526612729 old loss 0.00028861951432190835 BETTER
I0313 09:00:54.899371 2171710 finetune.py:68] layer 27_v @ epoch 1 new loss 0.0003275936469435692 old loss 0.0003365427546668798 BETTER
I0313 09:00:58.456922 2169357 finetune.py:68] layer 24_q @ epoch 2 new loss 0.00029519875533878803 old loss 0.00029800445190630853 BETTER
I0313 09:01:19.685013 2170931 finetune.py:68] layer 26_v @ epoch 4 new loss 0.00034514477010816336 old loss 0.00034852646058425307 BETTER
I0313 09:01:25.326278 2170151 finetune.py:68] layer 25_q @ epoch 1 new loss 0.0002758577174972743 old loss 0.0002791762526612729 BETTER
I0313 09:01:27.776971 2171710 finetune.py:68] layer 27_v @ epoch 2 new loss 0.0003229362773708999 old loss 0.0003275936469435692 BETTER
I0313 09:01:33.043967 2169357 finetune.py:68] layer 24_q @ epoch 3 new loss 0.0002929454203695059 old loss 0.00029519875533878803 BETTER
I0313 09:01:39.204338 2170931 finetune.py:45] layer 26_q initial loss 0.00043966504745185375
I0313 09:01:58.360253 2170151 finetune.py:68] layer 25_q @ epoch 2 new loss 0.0002734244626481086 old loss 0.0002758577174972743 BETTER
I0313 09:02:01.202664 2171710 finetune.py:68] layer 27_v @ epoch 3 new loss 0.0003195964964106679 old loss 0.0003229362773708999 BETTER
I0313 09:02:07.887831 2169357 finetune.py:68] layer 24_q @ epoch 4 new loss 0.00029123236890882254 old loss 0.0002929454203695059 BETTER
I0313 09:02:11.089324 2170931 finetune.py:68] layer 26_q @ epoch 0 new loss 0.0004297566774766892 old loss 0.00043966504745185375 BETTER
I0313 09:02:26.760259 2169357 finetune.py:45] layer 24_k initial loss 0.0003492627292871475
I0313 09:02:32.038863 2170151 finetune.py:68] layer 25_q @ epoch 3 new loss 0.00027153498376719654 old loss 0.0002734244626481086 BETTER
I0313 09:02:35.153395 2171710 finetune.py:68] layer 27_v @ epoch 4 new loss 0.0003169895790051669 old loss 0.0003195964964106679 BETTER
I0313 09:02:44.217631 2170931 finetune.py:68] layer 26_q @ epoch 1 new loss 0.000424554746132344 old loss 0.0004297566774766892 BETTER
I0313 09:02:55.935759 2171710 finetune.py:45] layer 27_q initial loss 0.00041890417924150825
I0313 09:03:00.403264 2169357 finetune.py:68] layer 24_k @ epoch 0 new loss 0.00034631628659553826 old loss 0.0003492627292871475 BETTER
I0313 09:03:05.151709 2170151 finetune.py:68] layer 25_q @ epoch 4 new loss 0.0002698799653444439 old loss 0.00027153498376719654 BETTER
I0313 09:03:16.840303 2170931 finetune.py:68] layer 26_q @ epoch 2 new loss 0.0004207375750411302 old loss 0.000424554746132344 BETTER
I0313 09:03:26.000365 2170151 finetune.py:45] layer 25_k initial loss 0.0003299957897979766
I0313 09:03:28.510453 2171710 finetune.py:68] layer 27_q @ epoch 0 new loss 0.00040518620517104864 old loss 0.00041890417924150825 BETTER
I0313 09:03:35.437625 2169357 finetune.py:68] layer 24_k @ epoch 1 new loss 0.0003445062611717731 old loss 0.00034631628659553826 BETTER
I0313 09:03:50.067043 2170931 finetune.py:68] layer 26_q @ epoch 3 new loss 0.0004173599008936435 old loss 0.0004207375750411302 BETTER
I0313 09:03:58.035518 2170151 finetune.py:68] layer 25_k @ epoch 0 new loss 0.0003270209417678416 old loss 0.0003299957897979766 BETTER
I0313 09:04:02.109971 2171710 finetune.py:68] layer 27_q @ epoch 1 new loss 0.0003997870662715286 old loss 0.00040518620517104864 BETTER
I0313 09:04:10.345258 2169357 finetune.py:68] layer 24_k @ epoch 2 new loss 0.0003431281365919858 old loss 0.0003445062611717731 BETTER
I0313 09:04:22.750144 2170931 finetune.py:68] layer 26_q @ epoch 4 new loss 0.0004146576684433967 old loss 0.0004173599008936435 BETTER
I0313 09:04:30.448347 2170151 finetune.py:68] layer 25_k @ epoch 1 new loss 0.00032519627711735666 old loss 0.0003270209417678416 BETTER
I0313 09:04:34.460363 2171710 finetune.py:68] layer 27_q @ epoch 2 new loss 0.0003955451538786292 old loss 0.0003997870662715286 BETTER
I0313 09:04:40.723226 2170931 finetune.py:45] layer 26_k initial loss 0.0004967727581970394
I0313 09:04:44.490565 2169357 finetune.py:68] layer 24_k @ epoch 3 new loss 0.000342051062034443 old loss 0.0003431281365919858 BETTER
I0313 09:05:02.500560 2170151 finetune.py:68] layer 25_k @ epoch 2 new loss 0.0003238946374040097 old loss 0.00032519627711735666 BETTER
I0313 09:05:06.574654 2171710 finetune.py:68] layer 27_q @ epoch 3 new loss 0.00039223188650794327 old loss 0.0003955451538786292 BETTER
I0313 09:05:12.373418 2170931 finetune.py:68] layer 26_k @ epoch 0 new loss 0.0004927592817693949 old loss 0.0004967727581970394 BETTER
I0313 09:05:18.246777 2169357 finetune.py:68] layer 24_k @ epoch 4 new loss 0.0003411895304452628 old loss 0.000342051062034443 BETTER
I0313 09:05:34.781122 2170151 finetune.py:68] layer 25_k @ epoch 3 new loss 0.00032285822089761496 old loss 0.0003238946374040097 BETTER
I0313 09:05:36.204482 2169357 finetune.py:45] layer 24_o initial loss 0.0007344152545556426
I0313 09:05:38.744913 2171710 finetune.py:68] layer 27_q @ epoch 4 new loss 0.00038939667865633965 old loss 0.00039223188650794327 BETTER
I0313 09:05:44.401677 2170931 finetune.py:68] layer 26_k @ epoch 1 new loss 0.0004903602530248463 old loss 0.0004927592817693949 BETTER
I0313 09:05:56.534348 2171710 finetune.py:45] layer 27_k initial loss 0.0004711420915555209
I0313 09:06:06.899278 2170151 finetune.py:68] layer 25_k @ epoch 4 new loss 0.00032207160256803036 old loss 0.00032285822089761496 BETTER
I0313 09:06:08.190701 2169357 finetune.py:68] layer 24_o @ epoch 0 new loss 0.0007255889941006899 old loss 0.0007344152545556426 BETTER
I0313 09:06:16.453700 2170931 finetune.py:68] layer 26_k @ epoch 2 new loss 0.0004883657093159854 old loss 0.0004903602530248463 BETTER
I0313 09:06:24.536034 2170151 finetune.py:45] layer 25_o initial loss 0.0006537565495818853
I0313 09:06:27.207963 2171710 finetune.py:68] layer 27_k @ epoch 0 new loss 0.0004675531527027488 old loss 0.0004711420915555209 BETTER
I0313 09:06:41.251492 2169357 finetune.py:68] layer 24_o @ epoch 1 new loss 0.0007214643410407007 old loss 0.0007255889941006899 BETTER
I0313 09:06:48.483782 2170931 finetune.py:68] layer 26_k @ epoch 3 new loss 0.0004868225660175085 old loss 0.0004883657093159854 BETTER
I0313 09:06:54.943121 2170151 finetune.py:68] layer 25_o @ epoch 0 new loss 0.0006437987904064357 old loss 0.0006537565495818853 BETTER
I0313 09:06:58.837128 2171710 finetune.py:68] layer 27_k @ epoch 1 new loss 0.0004653597716242075 old loss 0.0004675531527027488 BETTER
I0313 09:07:14.611763 2169357 finetune.py:68] layer 24_o @ epoch 2 new loss 0.0007185095455497503 old loss 0.0007214643410407007 BETTER
I0313 09:07:20.561310 2170931 finetune.py:68] layer 26_k @ epoch 4 new loss 0.000485487311379984 old loss 0.0004868225660175085 BETTER
I0313 09:07:26.154058 2170151 finetune.py:68] layer 25_o @ epoch 1 new loss 0.0006398818804882467 old loss 0.0006437987904064357 BETTER
I0313 09:07:30.548956 2171710 finetune.py:68] layer 27_k @ epoch 2 new loss 0.00046364383888430893 old loss 0.0004653597716242075 BETTER
I0313 09:07:38.377302 2170931 finetune.py:45] layer 26_o initial loss 0.0009627657709643245
I0313 09:07:47.827237 2169357 finetune.py:68] layer 24_o @ epoch 3 new loss 0.000716291950084269 old loss 0.0007185095455497503 BETTER
I0313 09:07:57.504658 2170151 finetune.py:68] layer 25_o @ epoch 2 new loss 0.0006372923962771893 old loss 0.0006398818804882467 BETTER
I0313 09:08:02.235302 2171710 finetune.py:68] layer 27_k @ epoch 3 new loss 0.00046237933565862477 old loss 0.00046364383888430893 BETTER
I0313 09:08:08.957948 2170931 finetune.py:68] layer 26_o @ epoch 0 new loss 0.0009493549005128443 old loss 0.0009627657709643245 BETTER
I0313 09:08:21.227790 2169357 finetune.py:68] layer 24_o @ epoch 4 new loss 0.0007145953713916242 old loss 0.000716291950084269 BETTER
I0313 09:08:28.773712 2170151 finetune.py:68] layer 25_o @ epoch 3 new loss 0.0006353147327899933 old loss 0.0006372923962771893 BETTER
I0313 09:08:33.929919 2171710 finetune.py:68] layer 27_k @ epoch 4 new loss 0.00046120447223074734 old loss 0.00046237933565862477 BETTER
I0313 09:08:40.388408 2170931 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0009429882047697902 old loss 0.0009493549005128443 BETTER
I0313 09:08:44.146575 2169357 finetune.py:45] layer 24_up initial loss 0.0015821987763047218
I0313 09:08:51.811497 2171710 finetune.py:45] layer 27_o initial loss 0.0008802460506558418
I0313 09:09:00.064591 2170151 finetune.py:68] layer 25_o @ epoch 4 new loss 0.0006337777012959123 old loss 0.0006353147327899933 BETTER
I0313 09:09:11.998957 2170931 finetune.py:68] layer 26_o @ epoch 2 new loss 0.000938209006562829 old loss 0.0009429882047697902 BETTER
I0313 09:09:14.388016 2169357 finetune.py:68] layer 24_up @ epoch 0 new loss 0.0015718805370852351 old loss 0.0015821987763047218 BETTER
I0313 09:09:21.736894 2171710 finetune.py:68] layer 27_o @ epoch 0 new loss 0.0008656788268126547 old loss 0.0008802460506558418 BETTER
I0313 09:09:23.090679 2170151 finetune.py:45] layer 25_up initial loss 0.001615325454622507
I0313 09:09:43.828331 2170931 finetune.py:68] layer 26_o @ epoch 3 new loss 0.0009345618891529739 old loss 0.000938209006562829 BETTER
I0313 09:09:45.925343 2169357 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0015655237948521972 old loss 0.0015718805370852351 BETTER
I0313 09:09:52.046775 2170151 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0016041618073359132 old loss 0.001615325454622507 BETTER
I0313 09:09:52.699566 2171710 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0008584201568737626 old loss 0.0008656788268126547 BETTER
I0313 09:10:15.527896 2170931 finetune.py:68] layer 26_o @ epoch 4 new loss 0.0009317347430624068 old loss 0.0009345618891529739 BETTER
I0313 09:10:17.283455 2169357 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0015605221269652247 old loss 0.0015655237948521972 BETTER
I0313 09:10:21.779144 2170151 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0015973877161741257 old loss 0.0016041618073359132 BETTER
I0313 09:10:23.612346 2171710 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0008531452622264624 old loss 0.0008584201568737626 BETTER
I0313 09:10:38.882595 2170931 finetune.py:45] layer 26_up initial loss 0.0020084844436496496
I0313 09:10:48.806763 2169357 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0015564014902338386 old loss 0.0015605221269652247 BETTER
I0313 09:10:51.270302 2170151 finetune.py:68] layer 25_up @ epoch 2 new loss 0.0015919231809675694 old loss 0.0015973877161741257 BETTER
I0313 09:10:54.545636 2171710 finetune.py:68] layer 27_o @ epoch 3 new loss 0.0008491401094943285 old loss 0.0008531452622264624 BETTER
I0313 09:11:07.711069 2170931 finetune.py:68] layer 26_up @ epoch 0 new loss 0.001995854778215289 old loss 0.0020084844436496496 BETTER
I0313 09:11:20.341942 2169357 finetune.py:68] layer 24_up @ epoch 4 new loss 0.001552900648675859 old loss 0.0015564014902338386 BETTER
I0313 09:11:21.141070 2170151 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0015873750671744347 old loss 0.0015919231809675694 BETTER
I0313 09:11:25.576609 2171710 finetune.py:68] layer 27_o @ epoch 4 new loss 0.0008459921227768064 old loss 0.0008491401094943285 BETTER
I0313 09:11:37.432935 2170931 finetune.py:68] layer 26_up @ epoch 1 new loss 0.001988108502700925 old loss 0.001995854778215289 BETTER
I0313 09:11:43.889107 2169357 finetune.py:45] layer 24_gate initial loss 0.002263362053781748
I0313 09:11:49.261242 2171710 finetune.py:45] layer 27_up initial loss 0.002060134895145893
I0313 09:11:50.912592 2170151 finetune.py:68] layer 25_up @ epoch 4 new loss 0.0015834481455385685 old loss 0.0015873750671744347 BETTER
I0313 09:12:07.820199 2170931 finetune.py:68] layer 26_up @ epoch 2 new loss 0.001981879584491253 old loss 0.001988108502700925 BETTER
I0313 09:12:12.684702 2169357 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.002257906598970294 old loss 0.002263362053781748 BETTER
I0313 09:12:14.427989 2170151 finetune.py:45] layer 25_gate initial loss 0.0023857716005295515
I0313 09:12:17.944721 2171710 finetune.py:68] layer 27_up @ epoch 0 new loss 0.002045165514573455 old loss 0.002060134895145893 BETTER
I0313 09:12:38.013994 2170931 finetune.py:68] layer 26_up @ epoch 3 new loss 0.001976733561605215 old loss 0.001981879584491253 BETTER
I0313 09:12:42.413602 2170151 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.0023799578193575144 old loss 0.0023857716005295515 BETTER
I0313 09:12:42.834343 2169357 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0022533261217176914 old loss 0.002257906598970294 BETTER
I0313 09:12:47.669484 2171710 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0020356434397399426 old loss 0.002045165514573455 BETTER
I0313 09:13:08.205922 2170931 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0019722632132470608 old loss 0.001976733561605215 BETTER
I0313 09:13:11.066910 2170151 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.002375320764258504 old loss 0.0023799578193575144 BETTER
I0313 09:13:12.788942 2169357 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.0022495584562420845 old loss 0.0022533261217176914 BETTER
I0313 09:13:17.154467 2171710 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0020281218457967043 old loss 0.0020356434397399426 BETTER
I0313 09:13:33.230387 2170931 finetune.py:45] layer 26_gate initial loss 0.0028766251634806395
I0313 09:13:40.505581 2170151 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.0023714611306786537 old loss 0.002375320764258504 BETTER
I0313 09:13:43.179770 2169357 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.002246519550681114 old loss 0.0022495584562420845 BETTER
I0313 09:13:46.753435 2171710 finetune.py:68] layer 27_up @ epoch 3 new loss 0.002021850785240531 old loss 0.0020281218457967043 BETTER
I0313 09:14:00.742913 2170931 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0028701010160148144 old loss 0.0028766251634806395 BETTER
I0313 09:14:09.184910 2170151 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.00236823339946568 old loss 0.0023714611306786537 BETTER
I0313 09:14:13.617358 2169357 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.0022437807638198137 old loss 0.002246519550681114 BETTER
I0313 09:14:16.678167 2171710 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0020166190806776285 old loss 0.002021850785240531 BETTER
I0313 09:14:29.245871 2170931 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.002864963375031948 old loss 0.0028701010160148144 BETTER
I0313 09:14:37.859323 2170151 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.002365439897403121 old loss 0.00236823339946568 BETTER
I0313 09:14:40.635144 2171710 finetune.py:45] layer 27_gate initial loss 0.003056222340092063
I0313 09:14:56.504302 2169357 finetune.py:45] layer 24_down initial loss 0.0036228627432137728
I0313 09:14:58.613703 2170931 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0028606641571968794 old loss 0.002864963375031948 BETTER
I0313 09:15:08.567566 2171710 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.003048278857022524 old loss 0.003056222340092063 BETTER
I0313 09:15:21.903167 2170151 finetune.py:45] layer 25_down initial loss 0.003814897034317255
I0313 09:15:23.965527 2169357 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0036221640184521675 old loss 0.0036228627432137728 BETTER
I0313 09:15:27.583685 2170931 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0028569523710757494 old loss 0.0028606641571968794 BETTER
I0313 09:15:36.813843 2171710 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.00304233911447227 old loss 0.003048278857022524 BETTER
I0313 09:15:47.304702 2170151 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0038142285775393248 old loss 0.003814897034317255 BETTER
I0313 09:15:51.942631 2169357 finetune.py:68] layer 24_down @ epoch 1 new loss 0.003621630137786269 old loss 0.0036221640184521675 BETTER
I0313 09:15:55.754090 2170931 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0028538652695715427 old loss 0.0028569523710757494 BETTER
I0313 09:16:05.370032 2171710 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.003037389600649476 old loss 0.00304233911447227 BETTER
I0313 09:16:13.787713 2170151 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0038137007504701614 old loss 0.0038142285775393248 BETTER
I0313 09:16:20.368117 2169357 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0036212243139743805 old loss 0.003621630137786269 BETTER
I0313 09:16:33.591271 2171710 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0030330463778227568 old loss 0.003037389600649476 BETTER
I0313 09:16:38.468866 2170931 finetune.py:45] layer 26_down initial loss 0.004430626984685659
I0313 09:16:40.601140 2170151 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0038132918998599052 old loss 0.0038137007504701614 BETTER
I0313 09:16:48.827444 2169357 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0036209109239280224 old loss 0.0036212243139743805 BETTER
I0313 09:17:02.142819 2171710 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.0030294808093458414 old loss 0.0030330463778227568 BETTER
I0313 09:17:04.248502 2170931 finetune.py:68] layer 26_down @ epoch 0 new loss 0.004429917316883802 old loss 0.004430626984685659 BETTER
I0313 09:17:07.533903 2170151 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0038129633758217096 old loss 0.0038132918998599052 BETTER
I0313 09:17:17.311698 2169357 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0036206673830747604 old loss 0.0036209109239280224 BETTER
24_v proxy err 0.014946429058909416 tr(WHW.T) 1394.900634765625
24_q proxy err 0.0030546661000698805 tr(WHW.T) 7025.46337890625
24_k proxy err 0.002077633049339056 tr(WHW.T) 10329.9912109375
24_o proxy err 0.011396735906600952 tr(WHW.T) 134.35403442382812
24_up proxy err 0.01402055099606514 tr(WHW.T) 2622.44970703125
24_gate proxy err 0.008787554688751698 tr(WHW.T) 4262.490234375
24_down proxy err 0.01656263880431652 tr(WHW.T) 341.6495666503906
I0313 09:17:31.182308 2170931 finetune.py:68] layer 26_down @ epoch 1 new loss 0.004429392050951719 old loss 0.004429917316883802 BETTER
I0313 09:17:34.625867 2170151 finetune.py:68] layer 25_down @ epoch 4 new loss 0.003812705632299185 old loss 0.0038129633758217096 BETTER
25_v proxy err 0.014472809620201588 tr(WHW.T) 1707.664794921875
25_q proxy err 0.0035372504498809576 tr(WHW.T) 7165.6337890625
25_k proxy err 0.0026382359210401773 tr(WHW.T) 9616.0361328125
25_o proxy err 0.014972597360610962 tr(WHW.T) 83.93362426757812
25_up proxy err 0.014018085785210133 tr(WHW.T) 2806.682861328125
25_gate proxy err 0.008584322407841682 tr(WHW.T) 4665.63232421875
25_down proxy err 0.015902766957879066 tr(WHW.T) 375.1463623046875
I0313 09:17:44.270096 2171710 finetune.py:45] layer 27_down initial loss 0.004779655486345291
I0313 09:17:57.915910 2170931 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0044289748184382915 old loss 0.004429392050951719 BETTER
I0313 09:18:09.164014 2171710 finetune.py:68] layer 27_down @ epoch 0 new loss 0.004778955597430468 old loss 0.004779655486345291 BETTER
I0313 09:18:24.836857 2170931 finetune.py:68] layer 26_down @ epoch 3 new loss 0.004428651183843613 old loss 0.0044289748184382915 BETTER
I0313 09:18:35.276416 2171710 finetune.py:68] layer 27_down @ epoch 1 new loss 0.004778417758643627 old loss 0.004778955597430468 BETTER
I0313 09:18:51.483020 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 28 in 72.37848401069641s
I0313 09:18:51.520151 2170931 finetune.py:68] layer 26_down @ epoch 4 new loss 0.004428397864103317 old loss 0.004428651183843613 BETTER
26_v proxy err 0.014197018928825855 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.003224012441933155 tr(WHW.T) 7474.87744140625
26_k proxy err 0.0022985294926911592 tr(WHW.T) 10494.9453125
26_o proxy err 0.008991466835141182 tr(WHW.T) 203.5280303955078
26_up proxy err 0.013149318285286427 tr(WHW.T) 3156.091796875
26_gate proxy err 0.007963129319250584 tr(WHW.T) 5303.94921875
26_down proxy err 0.01613764651119709 tr(WHW.T) 402.9529113769531
I0313 09:18:54.628614 2182160 config.py:54] PyTorch version 2.1.1 available.
I0313 09:18:55.623211 2090717 quantize_finetune_llama.py:184] layer 29 gpu 1
I0313 09:18:55.697124 2182160 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 09:19:01.357033 2171710 finetune.py:68] layer 27_down @ epoch 2 new loss 0.004777981899678707 old loss 0.004778417758643627 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 09:19:12.719900 2182160 finetune.py:45] layer 28_v initial loss 0.0007550164009444416
I0313 09:19:27.756335 2171710 finetune.py:68] layer 27_down @ epoch 3 new loss 0.004777655005455017 old loss 0.004777981899678707 BETTER
I0313 09:19:45.499425 2182160 finetune.py:68] layer 28_v @ epoch 0 new loss 0.00043743199785239995 old loss 0.0007550164009444416 BETTER
I0313 09:19:53.960584 2171710 finetune.py:68] layer 27_down @ epoch 4 new loss 0.004777383990585804 old loss 0.004777655005455017 BETTER
27_v proxy err 0.013706684112548828 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.0033015513326972723 tr(WHW.T) 7693.61279296875
27_k proxy err 0.0023977416567504406 tr(WHW.T) 10621.439453125
27_o proxy err 0.012351001612842083 tr(WHW.T) 126.69752502441406
27_up proxy err 0.011884390376508236 tr(WHW.T) 3690.556396484375
27_gate proxy err 0.0074462248012423515 tr(WHW.T) 5987.24853515625
27_down proxy err 0.01545807532966137 tr(WHW.T) 469.01593017578125
I0313 09:20:05.819408 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 29 in 69.78736901283264s
I0313 09:20:08.863866 2182932 config.py:54] PyTorch version 2.1.1 available.
I0313 09:20:09.853128 2090717 quantize_finetune_llama.py:184] layer 30 gpu 2
I0313 09:20:09.918265 2182932 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 09:20:19.650880 2182160 finetune.py:68] layer 28_v @ epoch 1 new loss 0.0004232632927596569 old loss 0.00043743199785239995 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 09:20:26.578494 2182932 finetune.py:45] layer 29_v initial loss 0.0006689223810099065
I0313 09:20:54.044320 2182160 finetune.py:68] layer 28_v @ epoch 2 new loss 0.0004160639364272356 old loss 0.0004232632927596569 BETTER
I0313 09:20:57.754166 2182932 finetune.py:68] layer 29_v @ epoch 0 new loss 0.00046735830255784094 old loss 0.0006689223810099065 BETTER
I0313 09:21:20.670888 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 30 in 70.4475953578949s
I0313 09:21:23.880823 2183733 config.py:54] PyTorch version 2.1.1 available.
I0313 09:21:24.923233 2090717 quantize_finetune_llama.py:184] layer 31 gpu 3
I0313 09:21:24.992558 2183733 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 09:21:28.931114 2182160 finetune.py:68] layer 28_v @ epoch 3 new loss 0.00041098837391473353 old loss 0.0004160639364272356 BETTER
I0313 09:21:30.021784 2182932 finetune.py:68] layer 29_v @ epoch 1 new loss 0.0004562392714433372 old loss 0.00046735830255784094 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 09:21:42.440176 2183733 finetune.py:45] layer 30_v initial loss 0.0007493274169974029
I0313 09:22:02.540508 2182932 finetune.py:68] layer 29_v @ epoch 2 new loss 0.00044966195127926767 old loss 0.0004562392714433372 BETTER
I0313 09:22:03.918568 2182160 finetune.py:68] layer 28_v @ epoch 4 new loss 0.0004070552240591496 old loss 0.00041098837391473353 BETTER
I0313 09:22:13.854532 2183733 finetune.py:68] layer 30_v @ epoch 0 new loss 0.0004554568149615079 old loss 0.0007493274169974029 BETTER
I0313 09:22:22.329338 2182160 finetune.py:45] layer 28_q initial loss 0.0005246977088972926
I0313 09:22:35.362704 2182932 finetune.py:68] layer 29_v @ epoch 3 new loss 0.0004454474546946585 old loss 0.00044966195127926767 BETTER
I0313 09:22:37.873773 2090717 quantize_finetune_llama.py:211] computed original embedding for layer 31 in 72.47237658500671s
I0313 09:22:41.126384 2184529 config.py:54] PyTorch version 2.1.1 available.
I0313 09:22:42.237086 2184529 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 09:22:46.645073 2183733 finetune.py:68] layer 30_v @ epoch 1 new loss 0.00044361717300489545 old loss 0.0004554568149615079 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 09:22:55.400599 2182160 finetune.py:68] layer 28_q @ epoch 0 new loss 0.0005103334551677108 old loss 0.0005246977088972926 BETTER
I0313 09:23:00.043545 2184529 finetune.py:45] layer 31_v initial loss 0.0008813624735921621
I0313 09:23:08.141219 2182932 finetune.py:68] layer 29_v @ epoch 4 new loss 0.0004415719013195485 old loss 0.0004454474546946585 BETTER
I0313 09:23:19.174252 2183733 finetune.py:68] layer 30_v @ epoch 2 new loss 0.0004373033298179507 old loss 0.00044361717300489545 BETTER
I0313 09:23:25.990995 2182932 finetune.py:45] layer 29_q initial loss 0.0005479718092828989
I0313 09:23:29.390298 2182160 finetune.py:68] layer 28_q @ epoch 1 new loss 0.0005043106502853334 old loss 0.0005103334551677108 BETTER
I0313 09:23:31.106958 2184529 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0006734670023433864 old loss 0.0008813624735921621 BETTER
I0313 09:23:51.945337 2183733 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0004324802721384913 old loss 0.0004373033298179507 BETTER
I0313 09:23:57.405284 2182932 finetune.py:68] layer 29_q @ epoch 0 new loss 0.0005358383641578257 old loss 0.0005479718092828989 BETTER
I0313 09:24:03.339994 2184529 finetune.py:68] layer 31_v @ epoch 1 new loss 0.00064689019927755 old loss 0.0006734670023433864 BETTER
I0313 09:24:03.583282 2182160 finetune.py:68] layer 28_q @ epoch 2 new loss 0.0004997063660994172 old loss 0.0005043106502853334 BETTER
I0313 09:24:24.817319 2183733 finetune.py:68] layer 30_v @ epoch 4 new loss 0.0004292297817301005 old loss 0.0004324802721384913 BETTER
I0313 09:24:29.536157 2182932 finetune.py:68] layer 29_q @ epoch 1 new loss 0.0005305884405970573 old loss 0.0005358383641578257 BETTER
I0313 09:24:35.653264 2184529 finetune.py:68] layer 31_v @ epoch 2 new loss 0.0006368319736793637 old loss 0.00064689019927755 BETTER
I0313 09:24:38.061204 2182160 finetune.py:68] layer 28_q @ epoch 3 new loss 0.0004957057535648346 old loss 0.0004997063660994172 BETTER
I0313 09:24:42.676431 2183733 finetune.py:45] layer 30_q initial loss 0.0005760875064879656
I0313 09:25:02.288063 2182932 finetune.py:68] layer 29_q @ epoch 2 new loss 0.0005264347419142723 old loss 0.0005305884405970573 BETTER
I0313 09:25:09.037014 2184529 finetune.py:76] layer 31_v @ epoch 3 new loss 0.0006467591738328338 old loss 0.0006368319736793637 WORSE
I0313 09:25:13.791089 2182160 finetune.py:68] layer 28_q @ epoch 4 new loss 0.0004924413515254855 old loss 0.0004957057535648346 BETTER
I0313 09:25:15.198010 2183733 finetune.py:68] layer 30_q @ epoch 0 new loss 0.0005565550527535379 old loss 0.0005760875064879656 BETTER
I0313 09:25:34.130398 2182160 finetune.py:45] layer 28_k initial loss 0.0006014231476001441
I0313 09:25:35.834419 2182932 finetune.py:68] layer 29_q @ epoch 3 new loss 0.0005231765098869801 old loss 0.0005264347419142723 BETTER
I0313 09:25:42.009768 2184529 finetune.py:68] layer 31_v @ epoch 4 new loss 0.0006247672718018293 old loss 0.0006368319736793637 BETTER
I0313 09:25:48.669581 2183733 finetune.py:68] layer 30_q @ epoch 1 new loss 0.0005490806070156395 old loss 0.0005565550527535379 BETTER
I0313 09:26:02.376467 2184529 finetune.py:45] layer 31_q initial loss 0.0011301402701064944
I0313 09:26:07.938667 2182160 finetune.py:68] layer 28_k @ epoch 0 new loss 0.0005960828857496381 old loss 0.0006014231476001441 BETTER
I0313 09:26:09.335765 2182932 finetune.py:68] layer 29_q @ epoch 4 new loss 0.0005203620530664921 old loss 0.0005231765098869801 BETTER
I0313 09:26:21.810864 2183733 finetune.py:68] layer 30_q @ epoch 2 new loss 0.000543582602404058 old loss 0.0005490806070156395 BETTER
I0313 09:26:29.378345 2182932 finetune.py:45] layer 29_k initial loss 0.0006213823799043894
I0313 09:26:34.446343 2184529 finetune.py:68] layer 31_q @ epoch 0 new loss 0.0010627590818330646 old loss 0.0011301402701064944 BETTER
I0313 09:26:41.819058 2182160 finetune.py:68] layer 28_k @ epoch 1 new loss 0.0005930295446887612 old loss 0.0005960828857496381 BETTER
I0313 09:26:54.350861 2183733 finetune.py:68] layer 30_q @ epoch 3 new loss 0.0005389688303694129 old loss 0.000543582602404058 BETTER
I0313 09:27:00.528919 2182932 finetune.py:68] layer 29_k @ epoch 0 new loss 0.0006173463771119714 old loss 0.0006213823799043894 BETTER
I0313 09:27:06.526393 2184529 finetune.py:68] layer 31_q @ epoch 1 new loss 0.0010351818054914474 old loss 0.0010627590818330646 BETTER
I0313 09:27:15.763423 2182160 finetune.py:68] layer 28_k @ epoch 2 new loss 0.0005907992017455399 old loss 0.0005930295446887612 BETTER
I0313 09:27:27.022010 2183733 finetune.py:68] layer 30_q @ epoch 4 new loss 0.0005347764817997813 old loss 0.0005389688303694129 BETTER
I0313 09:27:32.600240 2182932 finetune.py:68] layer 29_k @ epoch 1 new loss 0.0006145608494989574 old loss 0.0006173463771119714 BETTER
I0313 09:27:38.509657 2184529 finetune.py:68] layer 31_q @ epoch 2 new loss 0.0010177899384871125 old loss 0.0010351818054914474 BETTER
I0313 09:27:44.728400 2183733 finetune.py:45] layer 30_k initial loss 0.0006577902822755277
I0313 09:27:49.356929 2182160 finetune.py:68] layer 28_k @ epoch 3 new loss 0.0005889440071769059 old loss 0.0005907992017455399 BETTER
I0313 09:28:04.591653 2182932 finetune.py:68] layer 29_k @ epoch 2 new loss 0.0006126552470959723 old loss 0.0006145608494989574 BETTER
I0313 09:28:10.546431 2184529 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0009977588197216392 old loss 0.0010177899384871125 BETTER
I0313 09:28:16.145216 2183733 finetune.py:68] layer 30_k @ epoch 0 new loss 0.0006521681789308786 old loss 0.0006577902822755277 BETTER
I0313 09:28:23.091802 2182160 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0005875475471839309 old loss 0.0005889440071769059 BETTER
I0313 09:28:36.620505 2182932 finetune.py:68] layer 29_k @ epoch 3 new loss 0.0006109820678830147 old loss 0.0006126552470959723 BETTER
I0313 09:28:40.914452 2182160 finetune.py:45] layer 28_o initial loss 0.0011029205052182078
I0313 09:28:42.559220 2184529 finetune.py:68] layer 31_q @ epoch 4 new loss 0.0009895359398797154 old loss 0.0009977588197216392 BETTER
I0313 09:28:48.155308 2183733 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0006490017403848469 old loss 0.0006521681789308786 BETTER
I0313 09:29:00.266902 2184529 finetune.py:45] layer 31_k initial loss 0.0011344323866069317
I0313 09:29:08.671378 2182932 finetune.py:68] layer 29_k @ epoch 4 new loss 0.0006097237346693873 old loss 0.0006109820678830147 BETTER
I0313 09:29:12.971531 2182160 finetune.py:68] layer 28_o @ epoch 0 new loss 0.0010836338624358177 old loss 0.0011029205052182078 BETTER
I0313 09:29:20.227190 2183733 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0006464176112785935 old loss 0.0006490017403848469 BETTER
I0313 09:29:26.379886 2182932 finetune.py:45] layer 29_o initial loss 0.0011218821164220572
I0313 09:29:30.806101 2184529 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0011110325576737523 old loss 0.0011344323866069317 BETTER
I0313 09:29:46.136713 2182160 finetune.py:68] layer 28_o @ epoch 1 new loss 0.0010750979417935014 old loss 0.0010836338624358177 BETTER
I0313 09:29:52.255979 2183733 finetune.py:68] layer 30_k @ epoch 3 new loss 0.0006445720209740102 old loss 0.0006464176112785935 BETTER
I0313 09:29:56.794527 2182932 finetune.py:68] layer 29_o @ epoch 0 new loss 0.0011066342703998089 old loss 0.0011218821164220572 BETTER
I0313 09:30:02.453777 2184529 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0010980990482494235 old loss 0.0011110325576737523 BETTER
I0313 09:30:19.589823 2182160 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0010686112800613046 old loss 0.0010750979417935014 BETTER
I0313 09:30:24.366815 2183733 finetune.py:68] layer 30_k @ epoch 4 new loss 0.000642600585706532 old loss 0.0006445720209740102 BETTER
I0313 09:30:28.025936 2182932 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0010994757758453488 old loss 0.0011066342703998089 BETTER
I0313 09:30:34.144183 2184529 finetune.py:68] layer 31_k @ epoch 2 new loss 0.001088721095584333 old loss 0.0010980990482494235 BETTER
I0313 09:30:42.055784 2183733 finetune.py:45] layer 30_o initial loss 0.001246185740455985
I0313 09:30:52.857368 2182160 finetune.py:68] layer 28_o @ epoch 3 new loss 0.001063911127857864 old loss 0.0010686112800613046 BETTER
I0313 09:30:59.296467 2182932 finetune.py:68] layer 29_o @ epoch 2 new loss 0.0010944295208901167 old loss 0.0010994757758453488 BETTER
I0313 09:31:05.871494 2184529 finetune.py:68] layer 31_k @ epoch 3 new loss 0.001082026050426066 old loss 0.001088721095584333 BETTER
I0313 09:31:12.669341 2183733 finetune.py:68] layer 30_o @ epoch 0 new loss 0.001214090152643621 old loss 0.001246185740455985 BETTER
I0313 09:31:26.336347 2182160 finetune.py:68] layer 28_o @ epoch 4 new loss 0.00105990597512573 old loss 0.001063911127857864 BETTER
I0313 09:31:30.645186 2182932 finetune.py:68] layer 29_o @ epoch 3 new loss 0.001090792240574956 old loss 0.0010944295208901167 BETTER
I0313 09:31:37.685408 2184529 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0010780426673591137 old loss 0.001082026050426066 BETTER
I0313 09:31:44.280557 2183733 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0011985708260908723 old loss 0.001214090152643621 BETTER
I0313 09:31:49.557425 2182160 finetune.py:45] layer 28_up initial loss 0.002467992017045617
I0313 09:31:55.623703 2184529 finetune.py:45] layer 31_o initial loss 0.0018212918657809496
I0313 09:32:02.016641 2182932 finetune.py:68] layer 29_o @ epoch 4 new loss 0.001087966375052929 old loss 0.001090792240574956 BETTER
I0313 09:32:16.131292 2183733 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0011882897233590484 old loss 0.0011985708260908723 BETTER
I0313 09:32:20.029578 2182160 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0024488249327987432 old loss 0.002467992017045617 BETTER
I0313 09:32:25.272675 2182932 finetune.py:45] layer 29_up initial loss 0.0027323057875037193
I0313 09:32:25.715800 2184529 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0017295406432822347 old loss 0.0018212918657809496 BETTER
I0313 09:32:47.822700 2183733 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0011802306398749352 old loss 0.0011882897233590484 BETTER
I0313 09:32:51.447861 2182160 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0024367838632315397 old loss 0.0024488249327987432 BETTER
I0313 09:32:54.257933 2182932 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0027071742806583643 old loss 0.0027323057875037193 BETTER
I0313 09:32:56.609691 2184529 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0017007557908073068 old loss 0.0017295406432822347 BETTER
I0313 09:33:19.746212 2183733 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0011741267517209053 old loss 0.0011802306398749352 BETTER
I0313 09:33:23.010807 2182160 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0024273693561553955 old loss 0.0024367838632315397 BETTER
I0313 09:33:23.810168 2182932 finetune.py:68] layer 29_up @ epoch 1 new loss 0.002692443085834384 old loss 0.0027071742806583643 BETTER
I0313 09:33:27.491475 2184529 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0016820457531139255 old loss 0.0017007557908073068 BETTER
I0313 09:33:43.101384 2183733 finetune.py:45] layer 30_up initial loss 0.003674114588648081
I0313 09:33:53.935299 2182932 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0026806173846125603 old loss 0.002692443085834384 BETTER
I0313 09:33:54.966585 2182160 finetune.py:68] layer 28_up @ epoch 3 new loss 0.0024192684795707464 old loss 0.0024273693561553955 BETTER
I0313 09:33:58.581326 2184529 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0016666542505845428 old loss 0.0016820457531139255 BETTER
I0313 09:34:12.094260 2183733 finetune.py:68] layer 30_up @ epoch 0 new loss 0.00361724104732275 old loss 0.003674114588648081 BETTER
I0313 09:34:23.988839 2182932 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0026711157988756895 old loss 0.0026806173846125603 BETTER
I0313 09:34:26.894528 2182160 finetune.py:68] layer 28_up @ epoch 4 new loss 0.0024123911280184984 old loss 0.0024192684795707464 BETTER
I0313 09:34:29.988358 2184529 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0016559876967221498 old loss 0.0016666542505845428 BETTER
I0313 09:34:42.457257 2183733 finetune.py:68] layer 30_up @ epoch 1 new loss 0.003580619813874364 old loss 0.00361724104732275 BETTER
I0313 09:34:51.608905 2182160 finetune.py:45] layer 28_gate initial loss 0.003640842391178012
I0313 09:34:54.595725 2182932 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0026625883765518665 old loss 0.0026711157988756895 BETTER
I0313 09:34:55.097453 2184529 finetune.py:45] layer 31_up initial loss 0.007463639602065086
I0313 09:35:12.689666 2183733 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0035503567196428776 old loss 0.003580619813874364 BETTER
I0313 09:35:20.337790 2182932 finetune.py:45] layer 29_gate initial loss 0.004108565393835306
I0313 09:35:21.901880 2182160 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.003631254890933633 old loss 0.003640842391178012 BETTER
I0313 09:35:24.468175 2184529 finetune.py:68] layer 31_up @ epoch 0 new loss 0.007171221077442169 old loss 0.007463639602065086 BETTER
I0313 09:35:42.841233 2183733 finetune.py:68] layer 30_up @ epoch 3 new loss 0.003524632193148136 old loss 0.0035503567196428776 BETTER
I0313 09:35:47.848001 2182932 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.004096636548638344 old loss 0.004108565393835306 BETTER
I0313 09:35:52.981445 2182160 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0036238166503608227 old loss 0.003631254890933633 BETTER
I0313 09:35:55.457442 2184529 finetune.py:68] layer 31_up @ epoch 1 new loss 0.007000528741627932 old loss 0.007171221077442169 BETTER
I0313 09:36:14.267580 2183733 finetune.py:68] layer 30_up @ epoch 4 new loss 0.00350282178260386 old loss 0.003524632193148136 BETTER
I0313 09:36:17.230609 2182932 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.0040873875841498375 old loss 0.004096636548638344 BETTER
I0313 09:36:23.537875 2182160 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.003617567475885153 old loss 0.0036238166503608227 BETTER
I0313 09:36:25.586553 2184529 finetune.py:68] layer 31_up @ epoch 2 new loss 0.006859408225864172 old loss 0.007000528741627932 BETTER
I0313 09:36:41.157454 2183733 finetune.py:45] layer 30_gate initial loss 0.0052140261977910995
I0313 09:36:46.338021 2182932 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.004079952836036682 old loss 0.0040873875841498375 BETTER
I0313 09:36:54.663316 2182160 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.003612479194998741 old loss 0.003617567475885153 BETTER
I0313 09:36:56.595221 2184529 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0067335469648242 old loss 0.006859408225864172 BETTER
I0313 09:37:09.636764 2183733 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.005187790375202894 old loss 0.0052140261977910995 BETTER
I0313 09:37:15.831419 2182932 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.004073543939739466 old loss 0.004079952836036682 BETTER
I0313 09:37:26.344126 2182160 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.003607792779803276 old loss 0.003612479194998741 BETTER
I0313 09:37:27.446481 2184529 finetune.py:68] layer 31_up @ epoch 4 new loss 0.006623127032071352 old loss 0.0067335469648242 BETTER
I0313 09:37:38.428531 2183733 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.005170064512640238 old loss 0.005187790375202894 BETTER
I0313 09:37:44.365242 2182932 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.004068200010806322 old loss 0.004073543939739466 BETTER
I0313 09:37:51.802765 2184529 finetune.py:45] layer 31_gate initial loss 0.009223483502864838
I0313 09:38:06.701375 2183733 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.00515431584790349 old loss 0.005170064512640238 BETTER
I0313 09:38:09.673004 2182160 finetune.py:45] layer 28_down initial loss 0.005648863967508078
I0313 09:38:18.654597 2184529 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.009115495719015598 old loss 0.009223483502864838 BETTER
I0313 09:38:25.250243 2182932 finetune.py:45] layer 29_down initial loss 0.00645443657413125
I0313 09:38:35.068636 2183733 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.00514084380120039 old loss 0.00515431584790349 BETTER
I0313 09:38:36.082773 2182160 finetune.py:68] layer 28_down @ epoch 0 new loss 0.005648094229400158 old loss 0.005648863967508078 BETTER
I0313 09:38:46.725830 2184529 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.00904965028166771 old loss 0.009115495719015598 BETTER
I0313 09:38:50.583384 2182932 finetune.py:68] layer 29_down @ epoch 0 new loss 0.00645369291305542 old loss 0.00645443657413125 BETTER
I0313 09:39:03.517686 2183733 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.005128729157149792 old loss 0.00514084380120039 BETTER
I0313 09:39:03.841508 2182160 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0056475079618394375 old loss 0.005648094229400158 BETTER
I0313 09:39:14.874983 2184529 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.008994737640023232 old loss 0.00904965028166771 BETTER
I0313 09:39:17.116905 2182932 finetune.py:68] layer 29_down @ epoch 1 new loss 0.006453122477978468 old loss 0.00645369291305542 BETTER
I0313 09:39:31.854169 2182160 finetune.py:68] layer 28_down @ epoch 2 new loss 0.005647044163197279 old loss 0.0056475079618394375 BETTER
I0313 09:39:43.117875 2184529 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.008948011323809624 old loss 0.008994737640023232 BETTER
I0313 09:39:43.948803 2182932 finetune.py:68] layer 29_down @ epoch 2 new loss 0.006452659144997597 old loss 0.006453122477978468 BETTER
I0313 09:39:44.656266 2183733 finetune.py:45] layer 30_down initial loss 0.008389226160943508
I0313 09:40:00.182743 2182160 finetune.py:68] layer 28_down @ epoch 3 new loss 0.00564666697755456 old loss 0.005647044163197279 BETTER
I0313 09:40:10.570907 2183733 finetune.py:68] layer 30_down @ epoch 0 new loss 0.008388547226786613 old loss 0.008389226160943508 BETTER
I0313 09:40:10.991778 2182932 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0064522819593548775 old loss 0.006452659144997597 BETTER
I0313 09:40:11.665497 2184529 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.008904607966542244 old loss 0.008948011323809624 BETTER
I0313 09:40:28.549381 2182160 finetune.py:68] layer 28_down @ epoch 4 new loss 0.005646353587508202 old loss 0.00564666697755456 BETTER
28_v proxy err 0.012633349746465683 tr(WHW.T) 2018.944091796875
28_q proxy err 0.00337525038048625 tr(WHW.T) 7652.25341796875
28_k proxy err 0.0024566613137722015 tr(WHW.T) 10549.0361328125
28_o proxy err 0.010047250427305698 tr(WHW.T) 195.6096649169922
28_up proxy err 0.009815721772611141 tr(WHW.T) 4662.8193359375
28_gate proxy err 0.007083835080265999 tr(WHW.T) 6544.50439453125
28_down proxy err 0.014086312614381313 tr(WHW.T) 606.4635009765625
I0313 09:40:37.172649 2183733 finetune.py:68] layer 30_down @ epoch 1 new loss 0.008387957699596882 old loss 0.008388547226786613 BETTER
I0313 09:40:37.933515 2182932 finetune.py:68] layer 29_down @ epoch 4 new loss 0.006451963447034359 old loss 0.0064522819593548775 BETTER
29_v proxy err 0.01312949601560831 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.003276279428973794 tr(WHW.T) 7232.45361328125
29_k proxy err 0.0022468380630016327 tr(WHW.T) 10567.1357421875
29_o proxy err 0.008967124857008457 tr(WHW.T) 208.52352905273438
29_up proxy err 0.007728086784482002 tr(WHW.T) 6069.96728515625
29_gate proxy err 0.0064376178197562695 tr(WHW.T) 7368.02880859375
29_down proxy err 0.01270447950810194 tr(WHW.T) 785.4636840820312
I0313 09:40:52.398829 2184529 finetune.py:45] layer 31_down initial loss 0.014497831463813782
I0313 09:41:03.824923 2183733 finetune.py:68] layer 30_down @ epoch 2 new loss 0.00838744081556797 old loss 0.008387957699596882 BETTER
I0313 09:41:17.237132 2184529 finetune.py:68] layer 31_down @ epoch 0 new loss 0.014495274052023888 old loss 0.014497831463813782 BETTER
I0313 09:41:30.618095 2183733 finetune.py:68] layer 30_down @ epoch 3 new loss 0.008387009613215923 old loss 0.00838744081556797 BETTER
I0313 09:41:43.192377 2184529 finetune.py:68] layer 31_down @ epoch 1 new loss 0.014493424445390701 old loss 0.014495274052023888 BETTER
I0313 09:41:57.289821 2183733 finetune.py:68] layer 30_down @ epoch 4 new loss 0.00838658306747675 old loss 0.008387009613215923 BETTER
30_v proxy err 0.01112433522939682 tr(WHW.T) 2261.489501953125
30_q proxy err 0.003214968601241708 tr(WHW.T) 7819.24658203125
30_k proxy err 0.00239746761508286 tr(WHW.T) 10529.068359375
30_o proxy err 0.008542194031178951 tr(WHW.T) 252.76760864257812
30_up proxy err 0.004574203863739967 tr(WHW.T) 10017.7685546875
30_gate proxy err 0.004223445896059275 tr(WHW.T) 10991.7490234375
30_down proxy err 0.0038898831699043512 tr(WHW.T) 3589.1708984375
I0313 09:42:09.304488 2184529 finetune.py:68] layer 31_down @ epoch 2 new loss 0.014491710811853409 old loss 0.014493424445390701 BETTER
I0313 09:42:35.657001 2184529 finetune.py:68] layer 31_down @ epoch 3 new loss 0.014490189030766487 old loss 0.014491710811853409 BETTER
I0313 09:43:02.011052 2184529 finetune.py:68] layer 31_down @ epoch 4 new loss 0.014488709159195423 old loss 0.014490189030766487 BETTER
31_v proxy err 0.01255490817129612 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.0023810206912457943 tr(WHW.T) 6860.87744140625
31_k proxy err 0.0016074018785730004 tr(WHW.T) 10246.43359375
31_o proxy err 0.005110593978315592 tr(WHW.T) 458.5888366699219
31_up proxy err 0.002423119731247425 tr(WHW.T) 14565.4951171875
31_gate proxy err 0.002417941577732563 tr(WHW.T) 14813.0283203125
31_down proxy err 0.0013972899178043008 tr(WHW.T) 17909.478515625
