I0312 04:22:28.286538 1799993 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.94it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.73it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.20it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.44it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.62it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.70it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.40it/s]
I0312 04:22:30.260636 1799993 quantize_finetune_llama.py:134] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:22,  1.39it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:22,  1.35it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:21,  1.34it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:20,  1.35it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:20,  1.34it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:19,  1.35it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:05<00:18,  1.36it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:17,  1.35it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:17,  1.34it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:07<00:16,  1.34it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:08<00:15,  1.36it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:14,  1.40it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:13,  1.43it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:10<00:12,  1.45it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:10<00:11,  1.44it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:11,  1.45it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:12<00:10,  1.46it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:12<00:09,  1.44it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:08,  1.46it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:14<00:08,  1.45it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:14<00:07,  1.45it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:06,  1.43it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:06,  1.43it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:05,  1.45it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:04,  1.43it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.44it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:19<00:03,  1.43it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:19<00:02,  1.43it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:02,  1.45it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.45it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:21<00:00,  1.46it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.44it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.41it/s]
I0312 04:23:02.931659 1799993 quantize_finetune_llama.py:159] loaded compression model
I0312 04:23:17.315198 1799993 quantize_finetune_llama.py:163] loaded dataset and devset
I0312 04:23:22.338604 1799993 quantize_finetune_llama.py:183] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 04:24:28.454423 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 0 in 65.98742818832397s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0312 04:24:48.945868 1801421 config.py:54] PyTorch version 2.1.1 available.
I0312 04:24:49.936143 1799993 quantize_finetune_llama.py:183] layer 1 gpu 1
I0312 04:25:57.628019 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 1 in 67.56188988685608s
I0312 04:26:08.197982 1802166 config.py:54] PyTorch version 2.1.1 available.
I0312 04:26:09.121806 1799993 quantize_finetune_llama.py:183] layer 2 gpu 2
I0312 04:27:17.314713 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 2 in 68.016841173172s
I0312 04:27:28.746771 1802928 config.py:54] PyTorch version 2.1.1 available.
I0312 04:27:29.789432 1799993 quantize_finetune_llama.py:183] layer 3 gpu 3
I0312 04:28:37.136873 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 3 in 67.20856285095215s
I0312 04:28:48.292820 1803671 config.py:54] PyTorch version 2.1.1 available.
I0312 04:28:49.380620 1799993 quantize_finetune_llama.py:183] layer 4 gpu 0
I0312 04:28:49.444585 1803671 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 04:28:57.603440 1803671 finetune.py:45] layer 3_v initial loss 2.8370429845381295e-06
I0312 04:29:28.190467 1803671 finetune.py:68] layer 3_v @ epoch 0 new loss 1.5485850326513173e-06 old loss 2.8370429845381295e-06 BETTER
I0312 04:29:59.012099 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 4 in 68.66592264175415s
I0312 04:30:00.046435 1803671 finetune.py:68] layer 3_v @ epoch 1 new loss 1.2396204738251981e-06 old loss 1.5485850326513173e-06 BETTER
I0312 04:30:02.129749 1804445 config.py:54] PyTorch version 2.1.1 available.
I0312 04:30:03.096721 1799993 quantize_finetune_llama.py:183] layer 5 gpu 1
I0312 04:30:03.168699 1804445 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 04:30:11.182596 1804445 finetune.py:45] layer 4_v initial loss 3.9540323086839635e-06
I0312 04:30:32.006370 1803671 finetune.py:68] layer 3_v @ epoch 2 new loss 1.1289765780020389e-06 old loss 1.2396204738251981e-06 BETTER
I0312 04:30:43.362737 1804445 finetune.py:68] layer 4_v @ epoch 0 new loss 2.0007325929327635e-06 old loss 3.9540323086839635e-06 BETTER
I0312 04:31:04.037467 1803671 finetune.py:68] layer 3_v @ epoch 3 new loss 1.0668654795153998e-06 old loss 1.1289765780020389e-06 BETTER
I0312 04:31:11.009420 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 5 in 67.54531359672546s
I0312 04:31:14.185408 1805192 config.py:54] PyTorch version 2.1.1 available.
I0312 04:31:15.197001 1799993 quantize_finetune_llama.py:183] layer 6 gpu 2
I0312 04:31:15.264441 1805192 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 04:31:16.795141 1804445 finetune.py:68] layer 4_v @ epoch 1 new loss 1.6875355868251063e-06 old loss 2.0007325929327635e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 04:31:23.105591 1805192 finetune.py:45] layer 5_v initial loss 5.833312116010347e-06
I0312 04:31:36.262672 1803671 finetune.py:68] layer 3_v @ epoch 4 new loss 1.0239614312013146e-06 old loss 1.0668654795153998e-06 BETTER
I0312 04:31:44.903038 1803671 finetune.py:45] layer 3_q initial loss 1.265608716494171e-06
I0312 04:31:50.941652 1804445 finetune.py:68] layer 4_v @ epoch 2 new loss 1.5594353044434683e-06 old loss 1.6875355868251063e-06 BETTER
I0312 04:31:54.363441 1805192 finetune.py:68] layer 5_v @ epoch 0 new loss 3.360017217346467e-06 old loss 5.833312116010347e-06 BETTER
I0312 04:32:16.269994 1803671 finetune.py:68] layer 3_q @ epoch 0 new loss 1.1436890190452687e-06 old loss 1.265608716494171e-06 BETTER
I0312 04:32:25.354702 1804445 finetune.py:68] layer 4_v @ epoch 3 new loss 1.4829373640168342e-06 old loss 1.5594353044434683e-06 BETTER
I0312 04:32:26.414882 1805192 finetune.py:68] layer 5_v @ epoch 1 new loss 2.9957554943393916e-06 old loss 3.360017217346467e-06 BETTER
I0312 04:32:48.177889 1803671 finetune.py:68] layer 3_q @ epoch 1 new loss 1.1037241165468004e-06 old loss 1.1436890190452687e-06 BETTER
I0312 04:32:58.384032 1805192 finetune.py:68] layer 5_v @ epoch 2 new loss 2.8206236493133474e-06 old loss 2.9957554943393916e-06 BETTER
I0312 04:32:59.964838 1804445 finetune.py:68] layer 4_v @ epoch 4 new loss 1.4305718423202052e-06 old loss 1.4829373640168342e-06 BETTER
I0312 04:33:08.821567 1804445 finetune.py:45] layer 4_q initial loss 1.8300454485142836e-06
I0312 04:33:20.095892 1803671 finetune.py:68] layer 3_q @ epoch 2 new loss 1.07449886854738e-06 old loss 1.1037241165468004e-06 BETTER
I0312 04:33:31.038963 1805192 finetune.py:68] layer 5_v @ epoch 3 new loss 2.7090525236417307e-06 old loss 2.8206236493133474e-06 BETTER
I0312 04:33:42.149439 1804445 finetune.py:68] layer 4_q @ epoch 0 new loss 1.6123196928674588e-06 old loss 1.8300454485142836e-06 BETTER
I0312 04:33:52.040451 1803671 finetune.py:68] layer 3_q @ epoch 3 new loss 1.0515831263546715e-06 old loss 1.07449886854738e-06 BETTER
I0312 04:34:03.729220 1805192 finetune.py:68] layer 5_v @ epoch 4 new loss 2.629725486258394e-06 old loss 2.7090525236417307e-06 BETTER
I0312 04:34:12.605762 1805192 finetune.py:45] layer 5_q initial loss 3.381012902536895e-06
I0312 04:34:16.182471 1804445 finetune.py:68] layer 4_q @ epoch 1 new loss 1.5582753576381947e-06 old loss 1.6123196928674588e-06 BETTER
I0312 04:34:23.935631 1803671 finetune.py:68] layer 3_q @ epoch 4 new loss 1.0325917401132756e-06 old loss 1.0515831263546715e-06 BETTER
I0312 04:34:32.877112 1803671 finetune.py:45] layer 3_k initial loss 1.2347286428848747e-06
I0312 04:34:44.037720 1805192 finetune.py:68] layer 5_q @ epoch 0 new loss 2.8947088139830157e-06 old loss 3.381012902536895e-06 BETTER
I0312 04:34:50.321557 1804445 finetune.py:68] layer 4_q @ epoch 2 new loss 1.5189300484053092e-06 old loss 1.5582753576381947e-06 BETTER
I0312 04:35:03.578525 1803671 finetune.py:68] layer 3_k @ epoch 0 new loss 1.1964106079176418e-06 old loss 1.2347286428848747e-06 BETTER
I0312 04:35:16.228205 1805192 finetune.py:68] layer 5_q @ epoch 1 new loss 2.8113961434428347e-06 old loss 2.8947088139830157e-06 BETTER
I0312 04:35:24.438544 1804445 finetune.py:68] layer 4_q @ epoch 3 new loss 1.4873041891405592e-06 old loss 1.5189300484053092e-06 BETTER
I0312 04:35:35.011400 1803671 finetune.py:68] layer 3_k @ epoch 1 new loss 1.1777149211411597e-06 old loss 1.1964106079176418e-06 BETTER
I0312 04:35:48.297677 1805192 finetune.py:68] layer 5_q @ epoch 2 new loss 2.751756483121426e-06 old loss 2.8113961434428347e-06 BETTER
I0312 04:35:58.730581 1804445 finetune.py:68] layer 4_q @ epoch 4 new loss 1.4610182006435934e-06 old loss 1.4873041891405592e-06 BETTER
I0312 04:36:06.606632 1803671 finetune.py:68] layer 3_k @ epoch 2 new loss 1.1624478020166862e-06 old loss 1.1777149211411597e-06 BETTER
I0312 04:36:08.405963 1804445 finetune.py:45] layer 4_k initial loss 1.8187683963333257e-06
I0312 04:36:20.647800 1805192 finetune.py:68] layer 5_q @ epoch 3 new loss 2.703501195355784e-06 old loss 2.751756483121426e-06 BETTER
I0312 04:36:38.134522 1803671 finetune.py:68] layer 3_k @ epoch 3 new loss 1.1494176987980609e-06 old loss 1.1624478020166862e-06 BETTER
I0312 04:36:40.967796 1804445 finetune.py:68] layer 4_k @ epoch 0 new loss 1.714973222988192e-06 old loss 1.8187683963333257e-06 BETTER
I0312 04:36:52.655778 1805192 finetune.py:68] layer 5_q @ epoch 4 new loss 2.6624350084603066e-06 old loss 2.703501195355784e-06 BETTER
I0312 04:37:01.613839 1805192 finetune.py:45] layer 5_k initial loss 3.080286660406273e-06
I0312 04:37:09.725565 1803671 finetune.py:68] layer 3_k @ epoch 4 new loss 1.1380329851817805e-06 old loss 1.1494176987980609e-06 BETTER
I0312 04:37:14.268341 1804445 finetune.py:68] layer 4_k @ epoch 1 new loss 1.6829701507958816e-06 old loss 1.714973222988192e-06 BETTER
I0312 04:37:18.717225 1803671 finetune.py:45] layer 3_o initial loss 2.882141643567593e-06
I0312 04:37:33.045692 1805192 finetune.py:68] layer 5_k @ epoch 0 new loss 2.968381977552781e-06 old loss 3.080286660406273e-06 BETTER
I0312 04:37:48.059462 1804445 finetune.py:68] layer 4_k @ epoch 2 new loss 1.6582893067607074e-06 old loss 1.6829701507958816e-06 BETTER
I0312 04:37:48.833001 1803671 finetune.py:68] layer 3_o @ epoch 0 new loss 2.8221104457770707e-06 old loss 2.882141643567593e-06 BETTER
I0312 04:38:05.036649 1805192 finetune.py:68] layer 5_k @ epoch 1 new loss 2.921977966252598e-06 old loss 2.968381977552781e-06 BETTER
I0312 04:38:19.760678 1803671 finetune.py:68] layer 3_o @ epoch 1 new loss 2.7828000384033658e-06 old loss 2.8221104457770707e-06 BETTER
I0312 04:38:21.762511 1804445 finetune.py:68] layer 4_k @ epoch 3 new loss 1.6376549183405587e-06 old loss 1.6582893067607074e-06 BETTER
I0312 04:38:37.066813 1805192 finetune.py:68] layer 5_k @ epoch 2 new loss 2.8858985388069414e-06 old loss 2.921977966252598e-06 BETTER
I0312 04:38:50.672071 1803671 finetune.py:68] layer 3_o @ epoch 2 new loss 2.7537514597497648e-06 old loss 2.7828000384033658e-06 BETTER
I0312 04:38:55.274090 1804445 finetune.py:68] layer 4_k @ epoch 4 new loss 1.6197992636080016e-06 old loss 1.6376549183405587e-06 BETTER
I0312 04:39:04.373216 1804445 finetune.py:45] layer 4_o initial loss 4.452002940524835e-06
I0312 04:39:09.036663 1805192 finetune.py:68] layer 5_k @ epoch 3 new loss 2.855171715054894e-06 old loss 2.8858985388069414e-06 BETTER
I0312 04:39:21.684486 1803671 finetune.py:68] layer 3_o @ epoch 3 new loss 2.7295659492665436e-06 old loss 2.7537514597497648e-06 BETTER
I0312 04:39:36.887557 1804445 finetune.py:68] layer 4_o @ epoch 0 new loss 4.323021585150855e-06 old loss 4.452002940524835e-06 BETTER
I0312 04:39:41.026648 1805192 finetune.py:68] layer 5_k @ epoch 4 new loss 2.8296549317019526e-06 old loss 2.855171715054894e-06 BETTER
I0312 04:39:49.981759 1805192 finetune.py:45] layer 5_o initial loss 7.478297447960358e-06
I0312 04:39:52.494389 1803671 finetune.py:68] layer 3_o @ epoch 4 new loss 2.708815145524568e-06 old loss 2.7295659492665436e-06 BETTER
I0312 04:40:07.008014 1803671 finetune.py:45] layer 3_up initial loss 4.294089194445405e-06
I0312 04:40:10.132051 1804445 finetune.py:68] layer 4_o @ epoch 1 new loss 4.256399279256584e-06 old loss 4.323021585150855e-06 BETTER
I0312 04:40:20.632409 1805192 finetune.py:68] layer 5_o @ epoch 0 new loss 7.168384854594478e-06 old loss 7.478297447960358e-06 BETTER
I0312 04:40:35.418899 1803671 finetune.py:68] layer 3_up @ epoch 0 new loss 4.266019004717236e-06 old loss 4.294089194445405e-06 BETTER
I0312 04:40:43.433782 1804445 finetune.py:68] layer 4_o @ epoch 2 new loss 4.204601737001212e-06 old loss 4.256399279256584e-06 BETTER
I0312 04:40:52.100355 1805192 finetune.py:68] layer 5_o @ epoch 1 new loss 6.997911896178266e-06 old loss 7.168384854594478e-06 BETTER
I0312 04:41:04.461038 1803671 finetune.py:68] layer 3_up @ epoch 1 new loss 4.2435203795321286e-06 old loss 4.266019004717236e-06 BETTER
I0312 04:41:16.526797 1804445 finetune.py:68] layer 4_o @ epoch 3 new loss 4.161484866926912e-06 old loss 4.204601737001212e-06 BETTER
I0312 04:41:23.514262 1805192 finetune.py:68] layer 5_o @ epoch 2 new loss 6.868071977805812e-06 old loss 6.997911896178266e-06 BETTER
I0312 04:41:33.803287 1803671 finetune.py:68] layer 3_up @ epoch 2 new loss 4.223616542731179e-06 old loss 4.2435203795321286e-06 BETTER
I0312 04:41:49.408716 1804445 finetune.py:68] layer 4_o @ epoch 4 new loss 4.12339431932196e-06 old loss 4.161484866926912e-06 BETTER
I0312 04:41:54.925940 1805192 finetune.py:68] layer 5_o @ epoch 3 new loss 6.761221811757423e-06 old loss 6.868071977805812e-06 BETTER
I0312 04:42:03.171702 1803671 finetune.py:68] layer 3_up @ epoch 3 new loss 4.205107870802749e-06 old loss 4.223616542731179e-06 BETTER
I0312 04:42:04.872821 1804445 finetune.py:45] layer 4_up initial loss 7.078160251694499e-06
I0312 04:42:26.513322 1805192 finetune.py:68] layer 5_o @ epoch 4 new loss 6.672235485893907e-06 old loss 6.761221811757423e-06 BETTER
I0312 04:42:32.784167 1803671 finetune.py:68] layer 3_up @ epoch 4 new loss 4.1880389289872255e-06 old loss 4.205107870802749e-06 BETTER
I0312 04:42:35.307098 1804445 finetune.py:68] layer 4_up @ epoch 0 new loss 7.003702648944454e-06 old loss 7.078160251694499e-06 BETTER
I0312 04:42:41.362281 1805192 finetune.py:45] layer 5_up initial loss 1.1447918041085359e-05
I0312 04:42:47.561898 1803671 finetune.py:45] layer 3_gate initial loss 5.3117591960472055e-06
I0312 04:43:06.499454 1804445 finetune.py:68] layer 4_up @ epoch 1 new loss 6.95153448759811e-06 old loss 7.003702648944454e-06 BETTER
I0312 04:43:10.238923 1805192 finetune.py:68] layer 5_up @ epoch 0 new loss 1.128802978200838e-05 old loss 1.1447918041085359e-05 BETTER
I0312 04:43:14.511601 1803671 finetune.py:68] layer 3_gate @ epoch 0 new loss 5.293000867823139e-06 old loss 5.3117591960472055e-06 BETTER
I0312 04:43:37.885977 1804445 finetune.py:68] layer 4_up @ epoch 2 new loss 6.906431735842489e-06 old loss 6.95153448759811e-06 BETTER
I0312 04:43:40.049926 1805192 finetune.py:68] layer 5_up @ epoch 1 new loss 1.1173465281899553e-05 old loss 1.128802978200838e-05 BETTER
I0312 04:43:42.338936 1803671 finetune.py:68] layer 3_gate @ epoch 1 new loss 5.278179742163047e-06 old loss 5.293000867823139e-06 BETTER
I0312 04:44:09.317087 1804445 finetune.py:68] layer 4_up @ epoch 3 new loss 6.8653862399514765e-06 old loss 6.906431735842489e-06 BETTER
I0312 04:44:09.879336 1805192 finetune.py:68] layer 5_up @ epoch 2 new loss 1.1074883332184982e-05 old loss 1.1173465281899553e-05 BETTER
I0312 04:44:10.272435 1803671 finetune.py:68] layer 3_gate @ epoch 2 new loss 5.2644481911556795e-06 old loss 5.278179742163047e-06 BETTER
I0312 04:44:38.500276 1803671 finetune.py:68] layer 3_gate @ epoch 3 new loss 5.251732090982841e-06 old loss 5.2644481911556795e-06 BETTER
I0312 04:44:39.791521 1805192 finetune.py:68] layer 5_up @ epoch 3 new loss 1.0986579582095146e-05 old loss 1.1074883332184982e-05 BETTER
I0312 04:44:41.009261 1804445 finetune.py:68] layer 4_up @ epoch 4 new loss 6.827292963862419e-06 old loss 6.8653862399514765e-06 BETTER
I0312 04:44:56.119270 1804445 finetune.py:45] layer 4_gate initial loss 8.718695426068734e-06
I0312 04:45:06.376128 1803671 finetune.py:68] layer 3_gate @ epoch 4 new loss 5.239568963588681e-06 old loss 5.251732090982841e-06 BETTER
I0312 04:45:09.763799 1805192 finetune.py:68] layer 5_up @ epoch 4 new loss 1.0907744581345469e-05 old loss 1.0986579582095146e-05 BETTER
I0312 04:45:22.069985 1803671 finetune.py:45] layer 3_down initial loss 7.960983566590585e-06
I0312 04:45:25.033166 1805192 finetune.py:45] layer 5_gate initial loss 1.3809791198582388e-05
I0312 04:45:25.132057 1804445 finetune.py:68] layer 4_gate @ epoch 0 new loss 8.67274502525106e-06 old loss 8.718695426068734e-06 BETTER
I0312 04:45:47.281114 1803671 finetune.py:68] layer 3_down @ epoch 0 new loss 7.96051426732447e-06 old loss 7.960983566590585e-06 BETTER
I0312 04:45:52.544455 1805192 finetune.py:68] layer 5_gate @ epoch 0 new loss 1.3722061339649372e-05 old loss 1.3809791198582388e-05 BETTER
I0312 04:45:54.806322 1804445 finetune.py:68] layer 4_gate @ epoch 1 new loss 8.638611689093523e-06 old loss 8.67274502525106e-06 BETTER
I0312 04:46:13.405240 1803671 finetune.py:68] layer 3_down @ epoch 1 new loss 7.960315997479483e-06 old loss 7.96051426732447e-06 BETTER
I0312 04:46:20.937274 1805192 finetune.py:68] layer 5_gate @ epoch 1 new loss 1.365192929370096e-05 old loss 1.3722061339649372e-05 BETTER
I0312 04:46:24.504345 1804445 finetune.py:68] layer 4_gate @ epoch 2 new loss 8.607388735981658e-06 old loss 8.638611689093523e-06 BETTER
I0312 04:46:39.544066 1803671 finetune.py:68] layer 3_down @ epoch 2 new loss 7.960114999150392e-06 old loss 7.960315997479483e-06 BETTER
I0312 04:46:49.222655 1805192 finetune.py:68] layer 5_gate @ epoch 2 new loss 1.3589249647338875e-05 old loss 1.365192929370096e-05 BETTER
I0312 04:46:54.227786 1804445 finetune.py:68] layer 4_gate @ epoch 3 new loss 8.578423148719594e-06 old loss 8.607388735981658e-06 BETTER
I0312 04:47:05.899182 1803671 finetune.py:68] layer 3_down @ epoch 3 new loss 7.959961294545792e-06 old loss 7.960114999150392e-06 BETTER
I0312 04:47:17.521182 1805192 finetune.py:68] layer 5_gate @ epoch 3 new loss 1.3530616342904978e-05 old loss 1.3589249647338875e-05 BETTER
I0312 04:47:24.103727 1804445 finetune.py:68] layer 4_gate @ epoch 4 new loss 8.550803613616154e-06 old loss 8.578423148719594e-06 BETTER
I0312 04:47:32.064669 1803671 finetune.py:68] layer 3_down @ epoch 4 new loss 7.95985761214979e-06 old loss 7.959961294545792e-06 BETTER
3_v proxy err 0.002572977216914296 tr(WHW.T) 284.77557373046875
3_q proxy err 0.00014744956570211798 tr(WHW.T) 7218.2138671875
3_k proxy err 0.00012062508176313713 tr(WHW.T) 10076.1923828125
3_o proxy err 0.0017928620800375938 tr(WHW.T) 3.3552680015563965
3_up proxy err 0.00190050492528826 tr(WHW.T) 284.7780456542969
3_gate proxy err 0.0011487890733405948 tr(WHW.T) 478.1308288574219
3_down proxy err 0.0018377240048721433 tr(WHW.T) 6.1347336769104
I0312 04:47:39.727781 1804445 finetune.py:45] layer 4_down initial loss 1.3685713383893017e-05
I0312 04:47:45.674099 1805192 finetune.py:68] layer 5_gate @ epoch 4 new loss 1.3476773347065318e-05 old loss 1.3530616342904978e-05 BETTER
I0312 04:48:00.944194 1805192 finetune.py:45] layer 5_down initial loss 2.1089210349600762e-05
I0312 04:48:06.860855 1804445 finetune.py:68] layer 4_down @ epoch 0 new loss 1.3683978068002034e-05 old loss 1.3685713383893017e-05 BETTER
I0312 04:48:26.979857 1805192 finetune.py:68] layer 5_down @ epoch 0 new loss 2.1088109861011617e-05 old loss 2.1089210349600762e-05 BETTER
I0312 04:48:34.735482 1804445 finetune.py:68] layer 4_down @ epoch 1 new loss 1.3683511497220024e-05 old loss 1.3683978068002034e-05 BETTER
I0312 04:48:45.616932 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 6 in 69.67942714691162s
I0312 04:48:48.854490 1814747 config.py:54] PyTorch version 2.1.1 available.
I0312 04:48:49.857743 1799993 quantize_finetune_llama.py:183] layer 7 gpu 3
I0312 04:48:49.932729 1814747 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 04:48:53.550311 1805192 finetune.py:68] layer 5_down @ epoch 1 new loss 2.108791704813484e-05 old loss 2.1088109861011617e-05 BETTER
I0312 04:48:58.552751 1814747 finetune.py:45] layer 6_v initial loss 7.697327419009525e-06
I0312 04:49:02.926556 1804445 finetune.py:68] layer 4_down @ epoch 2 new loss 1.3683215001947246e-05 old loss 1.3683511497220024e-05 BETTER
I0312 04:49:20.524544 1805192 finetune.py:68] layer 5_down @ epoch 2 new loss 2.1087307686684653e-05 old loss 2.108791704813484e-05 BETTER
I0312 04:49:30.315432 1814747 finetune.py:68] layer 6_v @ epoch 0 new loss 4.162221557635348e-06 old loss 7.697327419009525e-06 BETTER
I0312 04:49:31.319103 1804445 finetune.py:68] layer 4_down @ epoch 3 new loss 1.3683036740985699e-05 old loss 1.3683215001947246e-05 BETTER
I0312 04:49:47.635517 1805192 finetune.py:68] layer 5_down @ epoch 3 new loss 2.1087154891574755e-05 old loss 2.1087307686684653e-05 BETTER
I0312 04:49:59.556987 1804445 finetune.py:68] layer 4_down @ epoch 4 new loss 1.3682853932550643e-05 old loss 1.3683036740985699e-05 BETTER
4_v proxy err 0.002386063802987337 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0001453381555620581 tr(WHW.T) 6916.8291015625
4_k proxy err 0.00011317156167933717 tr(WHW.T) 10418.00390625
4_o proxy err 0.0020476130302995443 tr(WHW.T) 5.14010763168335
4_up proxy err 0.0018238595221191645 tr(WHW.T) 397.666259765625
4_gate proxy err 0.0009085935307666659 tr(WHW.T) 820.8688354492188
4_down proxy err 0.001853287685662508 tr(WHW.T) 11.572380065917969
I0312 04:50:02.186596 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 7 in 71.91930842399597s
I0312 04:50:02.815124 1814747 finetune.py:68] layer 6_v @ epoch 1 new loss 3.7330489703890635e-06 old loss 4.162221557635348e-06 BETTER
I0312 04:50:05.387407 1815538 config.py:54] PyTorch version 2.1.1 available.
I0312 04:50:06.404537 1799993 quantize_finetune_llama.py:183] layer 8 gpu 0
I0312 04:50:06.476346 1815538 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 04:50:14.807507 1805192 finetune.py:68] layer 5_down @ epoch 4 new loss 2.1086896595079452e-05 old loss 2.1087154891574755e-05 BETTER
I0312 04:50:15.108816 1815538 finetune.py:45] layer 7_v initial loss 1.0172330803470686e-05
5_v proxy err 0.002444664714857936 tr(WHW.T) 298.47540283203125
5_q proxy err 0.00015598071331623942 tr(WHW.T) 6771.92578125
5_k proxy err 0.00011579041165532544 tr(WHW.T) 10842.8134765625
5_o proxy err 0.0023591092322021723 tr(WHW.T) 7.949172496795654
5_up proxy err 0.0018008238403126597 tr(WHW.T) 506.6869812011719
5_gate proxy err 0.0008496795198880136 tr(WHW.T) 1103.944580078125
5_down proxy err 0.002006822032853961 tr(WHW.T) 15.662373542785645
I0312 04:50:35.518164 1814747 finetune.py:68] layer 6_v @ epoch 2 new loss 3.515947128107655e-06 old loss 3.7330489703890635e-06 BETTER
I0312 04:50:46.069970 1815538 finetune.py:68] layer 7_v @ epoch 0 new loss 5.701820555259474e-06 old loss 1.0172330803470686e-05 BETTER
I0312 04:51:08.507581 1814747 finetune.py:68] layer 6_v @ epoch 3 new loss 3.3984538276854437e-06 old loss 3.515947128107655e-06 BETTER
I0312 04:51:18.020934 1815538 finetune.py:68] layer 7_v @ epoch 1 new loss 5.168996722204611e-06 old loss 5.701820555259474e-06 BETTER
I0312 04:51:31.690071 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 8 in 71.9211118221283s
I0312 04:51:35.052422 1816459 config.py:54] PyTorch version 2.1.1 available.
I0312 04:51:36.216667 1799993 quantize_finetune_llama.py:183] layer 9 gpu 1
I0312 04:51:36.289191 1816459 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 04:51:41.500865 1814747 finetune.py:68] layer 6_v @ epoch 4 new loss 3.301546030343161e-06 old loss 3.3984538276854437e-06 BETTER
I0312 04:51:44.639514 1816459 finetune.py:45] layer 8_v initial loss 1.4574401575373486e-05
I0312 04:51:50.145073 1815538 finetune.py:68] layer 7_v @ epoch 2 new loss 4.902365617454052e-06 old loss 5.168996722204611e-06 BETTER
I0312 04:51:50.697628 1814747 finetune.py:45] layer 6_q initial loss 4.361654646345414e-06
I0312 04:52:17.622512 1816459 finetune.py:68] layer 8_v @ epoch 0 new loss 8.09159337222809e-06 old loss 1.4574401575373486e-05 BETTER
I0312 04:52:22.340390 1814747 finetune.py:68] layer 6_q @ epoch 0 new loss 3.905846369889332e-06 old loss 4.361654646345414e-06 BETTER
I0312 04:52:22.417807 1815538 finetune.py:68] layer 7_v @ epoch 3 new loss 4.724634891317692e-06 old loss 4.902365617454052e-06 BETTER
I0312 04:52:51.838055 1816459 finetune.py:68] layer 8_v @ epoch 1 new loss 7.327144885493908e-06 old loss 8.09159337222809e-06 BETTER
I0312 04:52:54.817739 1814747 finetune.py:68] layer 6_q @ epoch 1 new loss 3.786270326600061e-06 old loss 3.905846369889332e-06 BETTER
I0312 04:52:54.920424 1815538 finetune.py:68] layer 7_v @ epoch 4 new loss 4.606649326888146e-06 old loss 4.724634891317692e-06 BETTER
I0312 04:53:03.947888 1815538 finetune.py:45] layer 7_q initial loss 6.379313617799198e-06
I0312 04:53:26.610263 1816459 finetune.py:68] layer 8_v @ epoch 2 new loss 6.952236162760528e-06 old loss 7.327144885493908e-06 BETTER
I0312 04:53:27.124090 1814747 finetune.py:68] layer 6_q @ epoch 2 new loss 3.7063448417029576e-06 old loss 3.786270326600061e-06 BETTER
I0312 04:53:35.394800 1815538 finetune.py:68] layer 7_q @ epoch 0 new loss 5.481902917381376e-06 old loss 6.379313617799198e-06 BETTER
I0312 04:53:59.678367 1814747 finetune.py:68] layer 6_q @ epoch 3 new loss 3.6409717267815722e-06 old loss 3.7063448417029576e-06 BETTER
I0312 04:54:01.406300 1816459 finetune.py:68] layer 8_v @ epoch 3 new loss 6.715467861795332e-06 old loss 6.952236162760528e-06 BETTER
I0312 04:54:07.450901 1815538 finetune.py:68] layer 7_q @ epoch 1 new loss 5.316288479662035e-06 old loss 5.481902917381376e-06 BETTER
I0312 04:54:32.388122 1814747 finetune.py:68] layer 6_q @ epoch 4 new loss 3.5874727473128587e-06 old loss 3.6409717267815722e-06 BETTER
I0312 04:54:36.182146 1816459 finetune.py:68] layer 8_v @ epoch 4 new loss 6.574327017005999e-06 old loss 6.715467861795332e-06 BETTER
I0312 04:54:39.441136 1815538 finetune.py:68] layer 7_q @ epoch 2 new loss 5.199944553169189e-06 old loss 5.316288479662035e-06 BETTER
I0312 04:54:41.484551 1814747 finetune.py:45] layer 6_k initial loss 4.365564564068336e-06
I0312 04:54:45.780389 1816459 finetune.py:45] layer 8_q initial loss 8.518848517269362e-06
I0312 04:55:11.471529 1815538 finetune.py:68] layer 7_q @ epoch 3 new loss 5.109782250656281e-06 old loss 5.199944553169189e-06 BETTER
I0312 04:55:12.936310 1814747 finetune.py:68] layer 6_k @ epoch 0 new loss 4.184735189483035e-06 old loss 4.365564564068336e-06 BETTER
I0312 04:55:19.450333 1816459 finetune.py:68] layer 8_q @ epoch 0 new loss 7.627920695085777e-06 old loss 8.518848517269362e-06 BETTER
I0312 04:55:43.462158 1815538 finetune.py:68] layer 7_q @ epoch 4 new loss 5.035828053223668e-06 old loss 5.109782250656281e-06 BETTER
I0312 04:55:44.925398 1814747 finetune.py:68] layer 6_k @ epoch 1 new loss 4.126588464714587e-06 old loss 4.184735189483035e-06 BETTER
I0312 04:55:52.411892 1815538 finetune.py:45] layer 7_k initial loss 6.4332725742133334e-06
I0312 04:55:53.943749 1816459 finetune.py:68] layer 8_q @ epoch 1 new loss 7.402428764180513e-06 old loss 7.627920695085777e-06 BETTER
I0312 04:56:16.895755 1814747 finetune.py:68] layer 6_k @ epoch 2 new loss 4.0821464608598035e-06 old loss 4.126588464714587e-06 BETTER
I0312 04:56:23.426600 1815538 finetune.py:68] layer 7_k @ epoch 0 new loss 5.957174380455399e-06 old loss 6.4332725742133334e-06 BETTER
I0312 04:56:28.371283 1816459 finetune.py:68] layer 8_q @ epoch 2 new loss 7.2474131229682826e-06 old loss 7.402428764180513e-06 BETTER
I0312 04:56:48.945605 1814747 finetune.py:68] layer 6_k @ epoch 3 new loss 4.04781530960463e-06 old loss 4.0821464608598035e-06 BETTER
I0312 04:56:55.140241 1815538 finetune.py:68] layer 7_k @ epoch 1 new loss 5.867128038516967e-06 old loss 5.957174380455399e-06 BETTER
I0312 04:57:02.736500 1816459 finetune.py:68] layer 8_q @ epoch 3 new loss 7.114570507837925e-06 old loss 7.2474131229682826e-06 BETTER
I0312 04:57:20.941247 1814747 finetune.py:68] layer 6_k @ epoch 4 new loss 4.015681042801589e-06 old loss 4.04781530960463e-06 BETTER
I0312 04:57:26.809860 1815538 finetune.py:68] layer 7_k @ epoch 2 new loss 5.7986899264506064e-06 old loss 5.867128038516967e-06 BETTER
I0312 04:57:30.176896 1814747 finetune.py:45] layer 6_o initial loss 1.0935878890450113e-05
I0312 04:57:37.234471 1816459 finetune.py:68] layer 8_q @ epoch 4 new loss 7.017821189947426e-06 old loss 7.114570507837925e-06 BETTER
I0312 04:57:46.625068 1816459 finetune.py:45] layer 8_k initial loss 8.604883078078274e-06
I0312 04:57:58.620557 1815538 finetune.py:68] layer 7_k @ epoch 3 new loss 5.74400701225386e-06 old loss 5.7986899264506064e-06 BETTER
I0312 04:58:01.143232 1814747 finetune.py:68] layer 6_o @ epoch 0 new loss 1.0417666999273933e-05 old loss 1.0935878890450113e-05 BETTER
I0312 04:58:19.377135 1816459 finetune.py:68] layer 8_k @ epoch 0 new loss 8.116148819681257e-06 old loss 8.604883078078274e-06 BETTER
I0312 04:58:30.170992 1815538 finetune.py:68] layer 7_k @ epoch 4 new loss 5.697385859093629e-06 old loss 5.74400701225386e-06 BETTER
I0312 04:58:32.713724 1814747 finetune.py:68] layer 6_o @ epoch 1 new loss 1.01591031125281e-05 old loss 1.0417666999273933e-05 BETTER
I0312 04:58:39.285275 1815538 finetune.py:45] layer 7_o initial loss 1.528964094177354e-05
I0312 04:58:53.035259 1816459 finetune.py:68] layer 8_k @ epoch 1 new loss 7.998826731636655e-06 old loss 8.116148819681257e-06 BETTER
I0312 04:59:04.444056 1814747 finetune.py:68] layer 6_o @ epoch 2 new loss 9.971116014639847e-06 old loss 1.01591031125281e-05 BETTER
I0312 04:59:09.477022 1815538 finetune.py:68] layer 7_o @ epoch 0 new loss 1.434501518815523e-05 old loss 1.528964094177354e-05 BETTER
I0312 04:59:26.821435 1816459 finetune.py:68] layer 8_k @ epoch 2 new loss 7.91181082604453e-06 old loss 7.998826731636655e-06 BETTER
I0312 04:59:36.034626 1814747 finetune.py:68] layer 6_o @ epoch 3 new loss 9.822130778047722e-06 old loss 9.971116014639847e-06 BETTER
I0312 04:59:40.378535 1815538 finetune.py:68] layer 7_o @ epoch 1 new loss 1.3906923413742334e-05 old loss 1.434501518815523e-05 BETTER
I0312 05:00:00.386607 1816459 finetune.py:68] layer 8_k @ epoch 3 new loss 7.836554686946329e-06 old loss 7.91181082604453e-06 BETTER
I0312 05:00:07.883863 1814747 finetune.py:68] layer 6_o @ epoch 4 new loss 9.69871507550124e-06 old loss 9.822130778047722e-06 BETTER
I0312 05:00:11.155883 1815538 finetune.py:68] layer 7_o @ epoch 2 new loss 1.3602288163383491e-05 old loss 1.3906923413742334e-05 BETTER
I0312 05:00:22.716394 1814747 finetune.py:45] layer 6_up initial loss 1.7018937796819955e-05
I0312 05:00:33.803293 1816459 finetune.py:68] layer 8_k @ epoch 4 new loss 7.780222404107917e-06 old loss 7.836554686946329e-06 BETTER
I0312 05:00:42.108057 1815538 finetune.py:68] layer 7_o @ epoch 3 new loss 1.3367321116675157e-05 old loss 1.3602288163383491e-05 BETTER
I0312 05:00:42.942937 1816459 finetune.py:45] layer 8_o initial loss 2.2044898287276737e-05
I0312 05:00:51.749626 1814747 finetune.py:68] layer 6_up @ epoch 0 new loss 1.6714282537577674e-05 old loss 1.7018937796819955e-05 BETTER
I0312 05:01:13.160951 1815538 finetune.py:68] layer 7_o @ epoch 4 new loss 1.317479200224625e-05 old loss 1.3367321116675157e-05 BETTER
I0312 05:01:15.206082 1816459 finetune.py:68] layer 8_o @ epoch 0 new loss 2.0532857888611034e-05 old loss 2.2044898287276737e-05 BETTER
I0312 05:01:21.505430 1814747 finetune.py:68] layer 6_up @ epoch 1 new loss 1.6515959941898473e-05 old loss 1.6714282537577674e-05 BETTER
I0312 05:01:27.763584 1815538 finetune.py:45] layer 7_up initial loss 2.3108535970095545e-05
I0312 05:01:48.450620 1816459 finetune.py:68] layer 8_o @ epoch 1 new loss 1.9811748643405735e-05 old loss 2.0532857888611034e-05 BETTER
I0312 05:01:51.232303 1814747 finetune.py:68] layer 6_up @ epoch 2 new loss 1.6350906662410125e-05 old loss 1.6515959941898473e-05 BETTER
I0312 05:01:56.266361 1815538 finetune.py:68] layer 7_up @ epoch 0 new loss 2.2582300516660325e-05 old loss 2.3108535970095545e-05 BETTER
I0312 05:02:21.024515 1814747 finetune.py:68] layer 6_up @ epoch 3 new loss 1.620672264834866e-05 old loss 1.6350906662410125e-05 BETTER
I0312 05:02:21.707144 1816459 finetune.py:68] layer 8_o @ epoch 2 new loss 1.9315513782203197e-05 old loss 1.9811748643405735e-05 BETTER
I0312 05:02:25.658680 1815538 finetune.py:68] layer 7_up @ epoch 1 new loss 2.2265174266067334e-05 old loss 2.2582300516660325e-05 BETTER
I0312 05:02:51.008366 1814747 finetune.py:68] layer 6_up @ epoch 4 new loss 1.6079668057500385e-05 old loss 1.620672264834866e-05 BETTER
I0312 05:02:54.968047 1816459 finetune.py:68] layer 8_o @ epoch 3 new loss 1.8938757420983166e-05 old loss 1.9315513782203197e-05 BETTER
I0312 05:02:55.070193 1815538 finetune.py:68] layer 7_up @ epoch 2 new loss 2.200587186962366e-05 old loss 2.2265174266067334e-05 BETTER
I0312 05:03:05.968134 1814747 finetune.py:45] layer 6_gate initial loss 2.0149809643044136e-05
I0312 05:03:24.488721 1815538 finetune.py:68] layer 7_up @ epoch 3 new loss 2.1786052457173355e-05 old loss 2.200587186962366e-05 BETTER
I0312 05:03:28.077490 1816459 finetune.py:68] layer 8_o @ epoch 4 new loss 1.863723446149379e-05 old loss 1.8938757420983166e-05 BETTER
I0312 05:03:33.619971 1814747 finetune.py:68] layer 6_gate @ epoch 0 new loss 1.9994971808046103e-05 old loss 2.0149809643044136e-05 BETTER
I0312 05:03:43.250818 1816459 finetune.py:45] layer 8_up initial loss 3.0420056646107696e-05
I0312 05:03:54.071083 1815538 finetune.py:68] layer 7_up @ epoch 4 new loss 2.1593958081211895e-05 old loss 2.1786052457173355e-05 BETTER
I0312 05:04:01.911054 1814747 finetune.py:68] layer 6_gate @ epoch 1 new loss 1.9880995751009323e-05 old loss 1.9994971808046103e-05 BETTER
I0312 05:04:08.724027 1815538 finetune.py:45] layer 7_gate initial loss 2.709457476157695e-05
I0312 05:04:13.668079 1816459 finetune.py:68] layer 8_up @ epoch 0 new loss 2.9678898499696516e-05 old loss 3.0420056646107696e-05 BETTER
I0312 05:04:30.024642 1814747 finetune.py:68] layer 6_gate @ epoch 2 new loss 1.978010914172046e-05 old loss 1.9880995751009323e-05 BETTER
I0312 05:04:35.641910 1815538 finetune.py:68] layer 7_gate @ epoch 0 new loss 2.68488784058718e-05 old loss 2.709457476157695e-05 BETTER
I0312 05:04:44.982312 1816459 finetune.py:68] layer 8_up @ epoch 1 new loss 2.9226677725091577e-05 old loss 2.9678898499696516e-05 BETTER
I0312 05:04:58.314242 1814747 finetune.py:68] layer 6_gate @ epoch 3 new loss 1.9688528482220136e-05 old loss 1.978010914172046e-05 BETTER
I0312 05:05:03.489989 1815538 finetune.py:68] layer 7_gate @ epoch 1 new loss 2.667372245923616e-05 old loss 2.68488784058718e-05 BETTER
I0312 05:05:16.445641 1816459 finetune.py:68] layer 8_up @ epoch 2 new loss 2.8864202249678783e-05 old loss 2.9226677725091577e-05 BETTER
I0312 05:05:26.366450 1814747 finetune.py:68] layer 6_gate @ epoch 4 new loss 1.9605526176746935e-05 old loss 1.9688528482220136e-05 BETTER
I0312 05:05:31.361651 1815538 finetune.py:68] layer 7_gate @ epoch 2 new loss 2.652052899065893e-05 old loss 2.667372245923616e-05 BETTER
I0312 05:05:42.743061 1814747 finetune.py:45] layer 6_down initial loss 3.1012528779683635e-05
I0312 05:05:47.750656 1816459 finetune.py:68] layer 8_up @ epoch 3 new loss 2.8559770726133138e-05 old loss 2.8864202249678783e-05 BETTER
I0312 05:05:59.223537 1815538 finetune.py:68] layer 7_gate @ epoch 3 new loss 2.6385505407233723e-05 old loss 2.652052899065893e-05 BETTER
I0312 05:06:08.770672 1814747 finetune.py:68] layer 6_down @ epoch 0 new loss 3.101028414675966e-05 old loss 3.1012528779683635e-05 BETTER
I0312 05:06:19.086095 1816459 finetune.py:68] layer 8_up @ epoch 4 new loss 2.8298527467995882e-05 old loss 2.8559770726133138e-05 BETTER
I0312 05:06:27.023909 1815538 finetune.py:68] layer 7_gate @ epoch 4 new loss 2.6264297048328444e-05 old loss 2.6385505407233723e-05 BETTER
I0312 05:06:34.136426 1816459 finetune.py:45] layer 8_gate initial loss 3.5487599234329537e-05
I0312 05:06:35.522468 1814747 finetune.py:68] layer 6_down @ epoch 1 new loss 3.100952380918898e-05 old loss 3.101028414675966e-05 BETTER
I0312 05:06:42.410933 1815538 finetune.py:45] layer 7_down initial loss 4.158367664786056e-05
I0312 05:07:02.413942 1814747 finetune.py:68] layer 6_down @ epoch 2 new loss 3.100882895523682e-05 old loss 3.100952380918898e-05 BETTER
I0312 05:07:02.771611 1816459 finetune.py:68] layer 8_gate @ epoch 0 new loss 3.5113258491037413e-05 old loss 3.5487599234329537e-05 BETTER
I0312 05:07:07.650561 1815538 finetune.py:68] layer 7_down @ epoch 0 new loss 4.158090450800955e-05 old loss 4.158367664786056e-05 BETTER
I0312 05:07:29.248006 1814747 finetune.py:68] layer 6_down @ epoch 3 new loss 3.100846515735611e-05 old loss 3.100882895523682e-05 BETTER
I0312 05:07:32.394003 1816459 finetune.py:68] layer 8_gate @ epoch 1 new loss 3.48628600477241e-05 old loss 3.5113258491037413e-05 BETTER
I0312 05:07:33.903570 1815538 finetune.py:68] layer 7_down @ epoch 1 new loss 4.157988587394357e-05 old loss 4.158090450800955e-05 BETTER
I0312 05:07:56.138637 1814747 finetune.py:68] layer 6_down @ epoch 4 new loss 3.1008123187348247e-05 old loss 3.100846515735611e-05 BETTER
6_v proxy err 0.002599157392978668 tr(WHW.T) 443.5464782714844
6_q proxy err 0.0001957695058081299 tr(WHW.T) 7577.115234375
6_k proxy err 0.00015367992455139756 tr(WHW.T) 10412.419921875
6_o proxy err 0.0026403393130749464 tr(WHW.T) 11.58066463470459
6_up proxy err 0.0017977873794734478 tr(WHW.T) 617.2507934570312
6_gate proxy err 0.00073873350629583 tr(WHW.T) 1554.6519775390625
6_down proxy err 0.0020556291565299034 tr(WHW.T) 22.99789047241211
I0312 05:08:00.409489 1815538 finetune.py:68] layer 7_down @ epoch 2 new loss 4.1579274693503976e-05 old loss 4.157988587394357e-05 BETTER
I0312 05:08:02.185647 1816459 finetune.py:68] layer 8_gate @ epoch 2 new loss 3.4654865885386243e-05 old loss 3.48628600477241e-05 BETTER
I0312 05:08:26.968610 1815538 finetune.py:68] layer 7_down @ epoch 3 new loss 4.157873991061933e-05 old loss 4.1579274693503976e-05 BETTER
I0312 05:08:32.258168 1816459 finetune.py:68] layer 8_gate @ epoch 3 new loss 3.4471129765734076e-05 old loss 3.4654865885386243e-05 BETTER
I0312 05:08:53.519763 1815538 finetune.py:68] layer 7_down @ epoch 4 new loss 4.1578336094971746e-05 old loss 4.157873991061933e-05 BETTER
7_v proxy err 0.002579286927357316 tr(WHW.T) 489.9357604980469
7_q proxy err 0.00020735498401336372 tr(WHW.T) 7673.25341796875
7_k proxy err 0.00016534941096324474 tr(WHW.T) 10200.4150390625
7_o proxy err 0.0029399036429822445 tr(WHW.T) 15.1396484375
7_up proxy err 0.0017432122258469462 tr(WHW.T) 735.8589477539062
7_gate proxy err 0.0007111682207323611 tr(WHW.T) 1874.4349365234375
7_down proxy err 0.002080050064250827 tr(WHW.T) 30.615455627441406
I0312 05:09:02.196817 1816459 finetune.py:68] layer 8_gate @ epoch 4 new loss 3.4307900932617486e-05 old loss 3.4471129765734076e-05 BETTER
I0312 05:09:13.849548 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 9 in 70.79320621490479s
I0312 05:09:17.221287 1826059 config.py:54] PyTorch version 2.1.1 available.
I0312 05:09:17.887750 1816459 finetune.py:45] layer 8_down initial loss 5.277578384266235e-05
I0312 05:09:18.270291 1799993 quantize_finetune_llama.py:183] layer 10 gpu 2
I0312 05:09:18.340508 1826059 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:09:26.677448 1826059 finetune.py:45] layer 9_v initial loss 1.3812816177960485e-05
I0312 05:09:45.057299 1816459 finetune.py:68] layer 8_down @ epoch 0 new loss 5.2772375056520104e-05 old loss 5.277578384266235e-05 BETTER
I0312 05:09:57.881493 1826059 finetune.py:68] layer 9_v @ epoch 0 new loss 9.531713658361696e-06 old loss 1.3812816177960485e-05 BETTER
I0312 05:10:13.353951 1816459 finetune.py:68] layer 8_down @ epoch 1 new loss 5.27709889865946e-05 old loss 5.2772375056520104e-05 BETTER
I0312 05:10:28.480906 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 10 in 69.80764555931091s
I0312 05:10:30.101887 1826059 finetune.py:68] layer 9_v @ epoch 1 new loss 8.840026566758752e-06 old loss 9.531713658361696e-06 BETTER
I0312 05:10:31.731997 1826846 config.py:54] PyTorch version 2.1.1 available.
I0312 05:10:32.739688 1799993 quantize_finetune_llama.py:183] layer 11 gpu 3
I0312 05:10:32.808736 1826846 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:10:41.113652 1826846 finetune.py:45] layer 10_v initial loss 2.0061368559254333e-05
I0312 05:10:41.604099 1816459 finetune.py:68] layer 8_down @ epoch 2 new loss 5.2770446927752346e-05 old loss 5.27709889865946e-05 BETTER
I0312 05:11:02.474910 1826059 finetune.py:68] layer 9_v @ epoch 2 new loss 8.443679689662531e-06 old loss 8.840026566758752e-06 BETTER
I0312 05:11:10.001093 1816459 finetune.py:68] layer 8_down @ epoch 3 new loss 5.276952651911415e-05 old loss 5.2770446927752346e-05 BETTER
I0312 05:11:12.519326 1826846 finetune.py:68] layer 10_v @ epoch 0 new loss 1.3688124454347417e-05 old loss 2.0061368559254333e-05 BETTER
I0312 05:11:35.146052 1826059 finetune.py:68] layer 9_v @ epoch 3 new loss 8.183977115550078e-06 old loss 8.443679689662531e-06 BETTER
I0312 05:11:38.417038 1816459 finetune.py:68] layer 8_down @ epoch 4 new loss 5.276916272123344e-05 old loss 5.276952651911415e-05 BETTER
8_v proxy err 0.0023414480965584517 tr(WHW.T) 530.9967041015625
8_q proxy err 0.00021533819381147623 tr(WHW.T) 7229.0537109375
8_k proxy err 0.00016043272626120597 tr(WHW.T) 10642.4580078125
8_o proxy err 0.003316078567877412 tr(WHW.T) 20.13175392150879
8_up proxy err 0.0015902561135590076 tr(WHW.T) 866.2029418945312
8_gate proxy err 0.00072452612221241 tr(WHW.T) 1969.683837890625
8_down proxy err 0.0020681696478277445 tr(WHW.T) 37.20112991333008
I0312 05:11:43.989727 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 11 in 70.82764267921448s
I0312 05:11:45.007122 1826846 finetune.py:68] layer 10_v @ epoch 1 new loss 1.2612315913429484e-05 old loss 1.3688124454347417e-05 BETTER
I0312 05:11:47.185070 1827627 config.py:54] PyTorch version 2.1.1 available.
I0312 05:11:48.190204 1799993 quantize_finetune_llama.py:183] layer 12 gpu 0
I0312 05:11:48.257409 1827627 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:11:56.864636 1827627 finetune.py:45] layer 11_v initial loss 2.0851313820458017e-05
I0312 05:12:08.355856 1826059 finetune.py:68] layer 9_v @ epoch 4 new loss 7.981564522197004e-06 old loss 8.183977115550078e-06 BETTER
I0312 05:12:17.491603 1826059 finetune.py:45] layer 9_q initial loss 1.0116168596141506e-05
I0312 05:12:17.918837 1826846 finetune.py:68] layer 10_v @ epoch 2 new loss 1.2016230357403401e-05 old loss 1.2612315913429484e-05 BETTER
I0312 05:12:27.927861 1827627 finetune.py:68] layer 11_v @ epoch 0 new loss 1.395641447743401e-05 old loss 2.0851313820458017e-05 BETTER
I0312 05:12:48.906747 1826059 finetune.py:68] layer 9_q @ epoch 0 new loss 9.305202183895744e-06 old loss 1.0116168596141506e-05 BETTER
I0312 05:12:51.146103 1826846 finetune.py:68] layer 10_v @ epoch 3 new loss 1.1640461707429495e-05 old loss 1.2016230357403401e-05 BETTER
I0312 05:12:59.880161 1827627 finetune.py:68] layer 11_v @ epoch 1 new loss 1.288739986193832e-05 old loss 1.395641447743401e-05 BETTER
I0312 05:13:21.340594 1826059 finetune.py:68] layer 9_q @ epoch 1 new loss 9.065000995178707e-06 old loss 9.305202183895744e-06 BETTER
I0312 05:13:24.319482 1826846 finetune.py:68] layer 10_v @ epoch 4 new loss 1.1342979632900096e-05 old loss 1.1640461707429495e-05 BETTER
I0312 05:13:32.049095 1827627 finetune.py:68] layer 11_v @ epoch 2 new loss 1.2307866199989803e-05 old loss 1.288739986193832e-05 BETTER
I0312 05:13:33.623229 1826846 finetune.py:45] layer 10_q initial loss 1.4022789400769398e-05
I0312 05:13:53.583303 1826059 finetune.py:68] layer 9_q @ epoch 2 new loss 8.916844308259897e-06 old loss 9.065000995178707e-06 BETTER
I0312 05:14:04.524352 1827627 finetune.py:68] layer 11_v @ epoch 3 new loss 1.1932789675483946e-05 old loss 1.2307866199989803e-05 BETTER
I0312 05:14:05.147306 1826846 finetune.py:68] layer 10_q @ epoch 0 new loss 1.2862223229603842e-05 old loss 1.4022789400769398e-05 BETTER
I0312 05:14:25.689880 1826059 finetune.py:68] layer 9_q @ epoch 3 new loss 8.80567586136749e-06 old loss 8.916844308259897e-06 BETTER
I0312 05:14:37.133705 1827627 finetune.py:68] layer 11_v @ epoch 4 new loss 1.173727105197031e-05 old loss 1.1932789675483946e-05 BETTER
I0312 05:14:37.576552 1826846 finetune.py:68] layer 10_q @ epoch 1 new loss 1.252703987120185e-05 old loss 1.2862223229603842e-05 BETTER
I0312 05:14:46.107565 1827627 finetune.py:45] layer 11_q initial loss 1.4976568309066352e-05
I0312 05:14:57.744114 1826059 finetune.py:68] layer 9_q @ epoch 4 new loss 8.72111922944896e-06 old loss 8.80567586136749e-06 BETTER
I0312 05:15:06.760397 1826059 finetune.py:45] layer 9_k initial loss 1.042548319674097e-05
I0312 05:15:10.066750 1826846 finetune.py:68] layer 10_q @ epoch 2 new loss 1.2246167898410931e-05 old loss 1.252703987120185e-05 BETTER
I0312 05:15:17.497610 1827627 finetune.py:68] layer 11_q @ epoch 0 new loss 1.3528644558391534e-05 old loss 1.4976568309066352e-05 BETTER
I0312 05:15:38.170986 1826059 finetune.py:68] layer 9_k @ epoch 0 new loss 9.942432370735332e-06 old loss 1.042548319674097e-05 BETTER
I0312 05:15:42.457137 1826846 finetune.py:68] layer 10_q @ epoch 3 new loss 1.2030304787913337e-05 old loss 1.2246167898410931e-05 BETTER
I0312 05:15:49.473616 1827627 finetune.py:68] layer 11_q @ epoch 1 new loss 1.3161173228581902e-05 old loss 1.3528644558391534e-05 BETTER
I0312 05:16:10.348008 1826059 finetune.py:68] layer 9_k @ epoch 1 new loss 9.828921065491159e-06 old loss 9.942432370735332e-06 BETTER
I0312 05:16:14.806866 1826846 finetune.py:68] layer 10_q @ epoch 4 new loss 1.1847196219605394e-05 old loss 1.2030304787913337e-05 BETTER
I0312 05:16:21.514856 1827627 finetune.py:68] layer 11_q @ epoch 2 new loss 1.2910584700875916e-05 old loss 1.3161173228581902e-05 BETTER
I0312 05:16:24.102506 1826846 finetune.py:45] layer 10_k initial loss 1.4342276699608192e-05
I0312 05:16:42.491335 1826059 finetune.py:68] layer 9_k @ epoch 2 new loss 9.72631369222654e-06 old loss 9.828921065491159e-06 BETTER
I0312 05:16:53.569430 1827627 finetune.py:68] layer 11_q @ epoch 3 new loss 1.2707226233032998e-05 old loss 1.2910584700875916e-05 BETTER
I0312 05:16:55.514103 1826846 finetune.py:68] layer 10_k @ epoch 0 new loss 1.3514804777514655e-05 old loss 1.4342276699608192e-05 BETTER
I0312 05:17:14.500933 1826059 finetune.py:68] layer 9_k @ epoch 3 new loss 9.649531421018764e-06 old loss 9.72631369222654e-06 BETTER
I0312 05:17:25.622668 1827627 finetune.py:68] layer 11_q @ epoch 4 new loss 1.2552090993267484e-05 old loss 1.2707226233032998e-05 BETTER
I0312 05:17:27.563131 1826846 finetune.py:68] layer 10_k @ epoch 1 new loss 1.3302279512572568e-05 old loss 1.3514804777514655e-05 BETTER
I0312 05:17:34.601507 1827627 finetune.py:45] layer 11_k initial loss 1.5170433471212164e-05
I0312 05:17:46.530971 1826059 finetune.py:68] layer 9_k @ epoch 4 new loss 9.586090527591296e-06 old loss 9.649531421018764e-06 BETTER
I0312 05:17:55.466799 1826059 finetune.py:45] layer 9_o initial loss 2.8160064175608568e-05
I0312 05:17:59.613961 1826846 finetune.py:68] layer 10_k @ epoch 2 new loss 1.3152458450349513e-05 old loss 1.3302279512572568e-05 BETTER
I0312 05:18:05.508027 1827627 finetune.py:68] layer 11_k @ epoch 0 new loss 1.4542360077030025e-05 old loss 1.5170433471212164e-05 BETTER
I0312 05:18:25.818977 1826059 finetune.py:68] layer 9_o @ epoch 0 new loss 2.609820876386948e-05 old loss 2.8160064175608568e-05 BETTER
I0312 05:18:31.640504 1826846 finetune.py:68] layer 10_k @ epoch 3 new loss 1.3088463674648665e-05 old loss 1.3152458450349513e-05 BETTER
I0312 05:18:37.140743 1827627 finetune.py:68] layer 11_k @ epoch 1 new loss 1.4375328646565322e-05 old loss 1.4542360077030025e-05 BETTER
I0312 05:18:56.968679 1826059 finetune.py:68] layer 9_o @ epoch 1 new loss 2.512060382287018e-05 old loss 2.609820876386948e-05 BETTER
I0312 05:19:03.654291 1826846 finetune.py:68] layer 10_k @ epoch 4 new loss 1.2990899449505378e-05 old loss 1.3088463674648665e-05 BETTER
I0312 05:19:08.710146 1827627 finetune.py:68] layer 11_k @ epoch 2 new loss 1.4231817658583168e-05 old loss 1.4375328646565322e-05 BETTER
I0312 05:19:13.010226 1826846 finetune.py:45] layer 10_o initial loss 3.904202822013758e-05
I0312 05:19:28.119609 1826059 finetune.py:68] layer 9_o @ epoch 2 new loss 2.4461885914206505e-05 old loss 2.512060382287018e-05 BETTER
I0312 05:19:40.401167 1827627 finetune.py:68] layer 11_k @ epoch 3 new loss 1.4133590411802288e-05 old loss 1.4231817658583168e-05 BETTER
I0312 05:19:43.742137 1826846 finetune.py:68] layer 10_o @ epoch 0 new loss 3.596193346311338e-05 old loss 3.904202822013758e-05 BETTER
I0312 05:19:59.169079 1826059 finetune.py:68] layer 9_o @ epoch 3 new loss 2.396036870777607e-05 old loss 2.4461885914206505e-05 BETTER
I0312 05:20:12.067244 1827627 finetune.py:68] layer 11_k @ epoch 4 new loss 1.4093986465013586e-05 old loss 1.4133590411802288e-05 BETTER
I0312 05:20:15.071807 1826846 finetune.py:68] layer 10_o @ epoch 1 new loss 3.447880226303823e-05 old loss 3.596193346311338e-05 BETTER
I0312 05:20:21.085827 1827627 finetune.py:45] layer 11_o initial loss 4.061596337123774e-05
I0312 05:20:30.460345 1826059 finetune.py:68] layer 9_o @ epoch 4 new loss 2.3559638066217303e-05 old loss 2.396036870777607e-05 BETTER
I0312 05:20:45.098407 1826059 finetune.py:45] layer 9_up initial loss 3.719048618222587e-05
I0312 05:20:46.462441 1826846 finetune.py:68] layer 10_o @ epoch 2 new loss 3.3467935281805694e-05 old loss 3.447880226303823e-05 BETTER
I0312 05:20:51.180678 1827627 finetune.py:68] layer 11_o @ epoch 0 new loss 3.750755058717914e-05 old loss 4.061596337123774e-05 BETTER
I0312 05:21:13.906413 1826059 finetune.py:68] layer 9_up @ epoch 0 new loss 3.62498831236735e-05 old loss 3.719048618222587e-05 BETTER
I0312 05:21:17.853218 1826846 finetune.py:68] layer 10_o @ epoch 3 new loss 3.269942317274399e-05 old loss 3.3467935281805694e-05 BETTER
I0312 05:21:22.078402 1827627 finetune.py:68] layer 11_o @ epoch 1 new loss 3.608113911468536e-05 old loss 3.750755058717914e-05 BETTER
I0312 05:21:43.397346 1826059 finetune.py:68] layer 9_up @ epoch 1 new loss 3.5687982745002955e-05 old loss 3.62498831236735e-05 BETTER
I0312 05:21:49.298728 1826846 finetune.py:68] layer 10_o @ epoch 4 new loss 3.2093004847411066e-05 old loss 3.269942317274399e-05 BETTER
I0312 05:21:53.042540 1827627 finetune.py:68] layer 11_o @ epoch 2 new loss 3.513385308906436e-05 old loss 3.608113911468536e-05 BETTER
I0312 05:22:04.354399 1826846 finetune.py:45] layer 10_up initial loss 4.754473047796637e-05
I0312 05:22:12.828095 1826059 finetune.py:68] layer 9_up @ epoch 2 new loss 3.5233842936577275e-05 old loss 3.5687982745002955e-05 BETTER
I0312 05:22:23.928240 1827627 finetune.py:68] layer 11_o @ epoch 3 new loss 3.441432272666134e-05 old loss 3.513385308906436e-05 BETTER
I0312 05:22:33.250039 1826846 finetune.py:68] layer 10_up @ epoch 0 new loss 4.629884278983809e-05 old loss 4.754473047796637e-05 BETTER
I0312 05:22:42.459727 1826059 finetune.py:68] layer 9_up @ epoch 3 new loss 3.4855234844144434e-05 old loss 3.5233842936577275e-05 BETTER
I0312 05:22:54.819195 1827627 finetune.py:68] layer 11_o @ epoch 4 new loss 3.3852462365757674e-05 old loss 3.441432272666134e-05 BETTER
I0312 05:23:03.580627 1826846 finetune.py:68] layer 10_up @ epoch 1 new loss 4.554542465484701e-05 old loss 4.629884278983809e-05 BETTER
I0312 05:23:10.055904 1827627 finetune.py:45] layer 11_up initial loss 5.133359445608221e-05
I0312 05:23:12.050500 1826059 finetune.py:68] layer 9_up @ epoch 4 new loss 3.45295193255879e-05 old loss 3.4855234844144434e-05 BETTER
I0312 05:23:26.692920 1826059 finetune.py:45] layer 9_gate initial loss 4.3212588934693485e-05
I0312 05:23:33.330792 1826846 finetune.py:68] layer 10_up @ epoch 2 new loss 4.494422682910226e-05 old loss 4.554542465484701e-05 BETTER
I0312 05:23:38.582271 1827627 finetune.py:68] layer 11_up @ epoch 0 new loss 5.00440364703536e-05 old loss 5.133359445608221e-05 BETTER
I0312 05:23:54.155099 1826059 finetune.py:68] layer 9_gate @ epoch 0 new loss 4.2753785237437114e-05 old loss 4.3212588934693485e-05 BETTER
I0312 05:24:02.942036 1826846 finetune.py:68] layer 10_up @ epoch 3 new loss 4.4445503590395674e-05 old loss 4.494422682910226e-05 BETTER
I0312 05:24:07.909389 1827627 finetune.py:68] layer 11_up @ epoch 1 new loss 4.926499605062418e-05 old loss 5.00440364703536e-05 BETTER
I0312 05:24:22.286881 1826059 finetune.py:68] layer 9_gate @ epoch 1 new loss 4.2442996345926076e-05 old loss 4.2753785237437114e-05 BETTER
I0312 05:24:32.613995 1826846 finetune.py:68] layer 10_up @ epoch 4 new loss 4.4012627768097445e-05 old loss 4.4445503590395674e-05 BETTER
I0312 05:24:37.286162 1827627 finetune.py:68] layer 11_up @ epoch 2 new loss 4.8651945689925924e-05 old loss 4.926499605062418e-05 BETTER
I0312 05:24:47.525744 1826846 finetune.py:45] layer 10_gate initial loss 5.4391603043768555e-05
I0312 05:24:50.459987 1826059 finetune.py:68] layer 9_gate @ epoch 2 new loss 4.218200410832651e-05 old loss 4.2442996345926076e-05 BETTER
I0312 05:25:06.756885 1827627 finetune.py:68] layer 11_up @ epoch 3 new loss 4.814625935978256e-05 old loss 4.8651945689925924e-05 BETTER
I0312 05:25:15.060398 1826846 finetune.py:68] layer 10_gate @ epoch 0 new loss 5.378714558901265e-05 old loss 5.4391603043768555e-05 BETTER
I0312 05:25:18.652441 1826059 finetune.py:68] layer 9_gate @ epoch 3 new loss 4.195431756670587e-05 old loss 4.218200410832651e-05 BETTER
I0312 05:25:36.219218 1827627 finetune.py:68] layer 11_up @ epoch 4 new loss 4.7709891077829525e-05 old loss 4.814625935978256e-05 BETTER
I0312 05:25:43.331355 1826846 finetune.py:68] layer 10_gate @ epoch 1 new loss 5.338114351616241e-05 old loss 5.378714558901265e-05 BETTER
I0312 05:25:46.843523 1826059 finetune.py:68] layer 9_gate @ epoch 4 new loss 4.175466165179387e-05 old loss 4.195431756670587e-05 BETTER
I0312 05:25:51.041382 1827627 finetune.py:45] layer 11_gate initial loss 5.950063496129587e-05
I0312 05:26:02.133566 1826059 finetune.py:45] layer 9_down initial loss 6.331503391265869e-05
I0312 05:26:11.402935 1826846 finetune.py:68] layer 10_gate @ epoch 2 new loss 5.3043633670313284e-05 old loss 5.338114351616241e-05 BETTER
I0312 05:26:18.130136 1827627 finetune.py:68] layer 11_gate @ epoch 0 new loss 5.887194492970593e-05 old loss 5.950063496129587e-05 BETTER
I0312 05:26:27.788355 1826059 finetune.py:68] layer 9_down @ epoch 0 new loss 6.331232725642622e-05 old loss 6.331503391265869e-05 BETTER
I0312 05:26:39.555126 1826846 finetune.py:68] layer 10_gate @ epoch 3 new loss 5.2750281611224636e-05 old loss 5.3043633670313284e-05 BETTER
I0312 05:26:46.162076 1827627 finetune.py:68] layer 11_gate @ epoch 1 new loss 5.84562003496103e-05 old loss 5.887194492970593e-05 BETTER
I0312 05:26:54.356548 1826059 finetune.py:68] layer 9_down @ epoch 1 new loss 6.331063195830211e-05 old loss 6.331232725642622e-05 BETTER
I0312 05:27:07.706533 1826846 finetune.py:68] layer 10_gate @ epoch 4 new loss 5.249225068837404e-05 old loss 5.2750281611224636e-05 BETTER
I0312 05:27:14.800891 1827627 finetune.py:68] layer 11_gate @ epoch 2 new loss 5.810820221086033e-05 old loss 5.84562003496103e-05 BETTER
I0312 05:27:21.110386 1826059 finetune.py:68] layer 9_down @ epoch 2 new loss 6.330945325316861e-05 old loss 6.331063195830211e-05 BETTER
I0312 05:27:23.818774 1826846 finetune.py:45] layer 10_down initial loss 7.761369488434866e-05
I0312 05:27:42.955639 1827627 finetune.py:68] layer 11_gate @ epoch 3 new loss 5.7809076679404825e-05 old loss 5.810820221086033e-05 BETTER
I0312 05:27:47.824389 1826059 finetune.py:68] layer 9_down @ epoch 3 new loss 6.330873293336481e-05 old loss 6.330945325316861e-05 BETTER
I0312 05:27:50.092298 1826846 finetune.py:68] layer 10_down @ epoch 0 new loss 7.760657172184438e-05 old loss 7.761369488434866e-05 BETTER
I0312 05:28:11.106099 1827627 finetune.py:68] layer 11_gate @ epoch 4 new loss 5.754740413976833e-05 old loss 5.7809076679404825e-05 BETTER
I0312 05:28:14.636051 1826059 finetune.py:68] layer 9_down @ epoch 4 new loss 6.330843461910263e-05 old loss 6.330873293336481e-05 BETTER
9_v proxy err 0.0023007309064269066 tr(WHW.T) 565.0663452148438
9_q proxy err 0.00022673416242469102 tr(WHW.T) 6971.1416015625
9_k proxy err 0.0001599123061168939 tr(WHW.T) 10991.1298828125
9_o proxy err 0.003383897477760911 tr(WHW.T) 25.655405044555664
9_up proxy err 0.001529225381091237 tr(WHW.T) 970.8236083984375
9_gate proxy err 0.0007191934273578227 tr(WHW.T) 2132.6748046875
9_down proxy err 0.0020916631910949945 tr(WHW.T) 43.025962829589844
I0312 05:28:17.023651 1826846 finetune.py:68] layer 10_down @ epoch 1 new loss 7.760420703561977e-05 old loss 7.760657172184438e-05 BETTER
I0312 05:28:26.557152 1827627 finetune.py:45] layer 11_down initial loss 8.504393917974085e-05
I0312 05:28:43.836012 1826846 finetune.py:68] layer 10_down @ epoch 2 new loss 7.760277367196977e-05 old loss 7.760420703561977e-05 BETTER
I0312 05:28:51.846130 1827627 finetune.py:68] layer 11_down @ epoch 0 new loss 8.503950812155381e-05 old loss 8.504393917974085e-05 BETTER
I0312 05:29:10.468332 1826846 finetune.py:68] layer 10_down @ epoch 3 new loss 7.760176958981901e-05 old loss 7.760277367196977e-05 BETTER
I0312 05:29:17.985756 1827627 finetune.py:68] layer 11_down @ epoch 1 new loss 8.503741992171854e-05 old loss 8.503950812155381e-05 BETTER
I0312 05:29:29.574866 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 12 in 70.57263612747192s
I0312 05:29:32.730388 1837271 config.py:54] PyTorch version 2.1.1 available.
I0312 05:29:33.805536 1799993 quantize_finetune_llama.py:183] layer 13 gpu 1
I0312 05:29:33.876885 1837271 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:29:37.240877 1826846 finetune.py:68] layer 10_down @ epoch 4 new loss 7.760112202959135e-05 old loss 7.760176958981901e-05 BETTER
10_v proxy err 0.0023117803502827883 tr(WHW.T) 578.807373046875
10_q proxy err 0.0002377779019298032 tr(WHW.T) 6916.80908203125
10_k proxy err 0.00016794564726296812 tr(WHW.T) 11002.255859375
10_o proxy err 0.00350291864015162 tr(WHW.T) 35.24748992919922
10_up proxy err 0.0014402263332158327 tr(WHW.T) 1080.11767578125
10_gate proxy err 0.0007101880037225783 tr(WHW.T) 2261.83447265625
10_down proxy err 0.0019975025206804276 tr(WHW.T) 52.38410949707031
I0312 05:29:42.248696 1837271 finetune.py:45] layer 12_v initial loss 2.147675877495203e-05
I0312 05:29:44.194120 1827627 finetune.py:68] layer 11_down @ epoch 2 new loss 8.50359137984924e-05 old loss 8.503741992171854e-05 BETTER
I0312 05:30:10.569782 1827627 finetune.py:68] layer 11_down @ epoch 3 new loss 8.50347860250622e-05 old loss 8.50359137984924e-05 BETTER
I0312 05:30:14.751921 1837271 finetune.py:68] layer 12_v @ epoch 0 new loss 1.4921311958460137e-05 old loss 2.147675877495203e-05 BETTER
I0312 05:30:36.882367 1827627 finetune.py:68] layer 11_down @ epoch 4 new loss 8.503420394845307e-05 old loss 8.50347860250622e-05 BETTER
11_v proxy err 0.0022731104400008917 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0002748857077676803 tr(WHW.T) 7028.029296875
11_k proxy err 0.00019990491273347288 tr(WHW.T) 10515.63671875
11_o proxy err 0.0035042844247072935 tr(WHW.T) 36.734127044677734
11_up proxy err 0.0014757789904251695 tr(WHW.T) 1139.8297119140625
11_gate proxy err 0.0007234279764816165 tr(WHW.T) 2394.093505859375
11_down proxy err 0.002035620156675577 tr(WHW.T) 56.16365432739258
I0312 05:30:48.683165 1837271 finetune.py:68] layer 12_v @ epoch 1 new loss 1.3847333320882171e-05 old loss 1.4921311958460137e-05 BETTER
I0312 05:30:51.791077 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 13 in 70.32064390182495s
I0312 05:30:54.893405 1838113 config.py:54] PyTorch version 2.1.1 available.
I0312 05:30:55.918286 1799993 quantize_finetune_llama.py:183] layer 14 gpu 2
I0312 05:30:55.992276 1838113 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:31:04.130888 1838113 finetune.py:45] layer 13_v initial loss 2.2697249733028002e-05
I0312 05:31:23.214170 1837271 finetune.py:68] layer 12_v @ epoch 2 new loss 1.3273717740958091e-05 old loss 1.3847333320882171e-05 BETTER
I0312 05:31:35.299525 1838113 finetune.py:68] layer 13_v @ epoch 0 new loss 1.5938436263240874e-05 old loss 2.2697249733028002e-05 BETTER
I0312 05:31:57.805783 1837271 finetune.py:68] layer 12_v @ epoch 3 new loss 1.2873119885625783e-05 old loss 1.3273717740958091e-05 BETTER
I0312 05:32:05.540614 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 14 in 69.2214126586914s
I0312 05:32:07.521039 1838113 finetune.py:68] layer 13_v @ epoch 1 new loss 1.4847866623313166e-05 old loss 1.5938436263240874e-05 BETTER
I0312 05:32:08.809767 1838903 config.py:54] PyTorch version 2.1.1 available.
I0312 05:32:09.809930 1799993 quantize_finetune_llama.py:183] layer 15 gpu 3
I0312 05:32:09.880051 1838903 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:32:17.941378 1838903 finetune.py:45] layer 14_v initial loss 2.950058114947751e-05
I0312 05:32:32.563919 1837271 finetune.py:68] layer 12_v @ epoch 4 new loss 1.2664325367950369e-05 old loss 1.2873119885625783e-05 BETTER
I0312 05:32:39.994038 1838113 finetune.py:68] layer 13_v @ epoch 2 new loss 1.4306949196907226e-05 old loss 1.4847866623313166e-05 BETTER
I0312 05:32:41.809137 1837271 finetune.py:45] layer 12_q initial loss 1.59613809955772e-05
I0312 05:32:49.424365 1838903 finetune.py:68] layer 14_v @ epoch 0 new loss 2.081428283418063e-05 old loss 2.950058114947751e-05 BETTER
I0312 05:33:12.523604 1838113 finetune.py:68] layer 13_v @ epoch 3 new loss 1.385156519972952e-05 old loss 1.4306949196907226e-05 BETTER
I0312 05:33:15.428532 1837271 finetune.py:68] layer 12_q @ epoch 0 new loss 1.4769680092285853e-05 old loss 1.59613809955772e-05 BETTER
I0312 05:33:19.214817 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 15 in 68.966228723526s
I0312 05:33:21.803857 1838903 finetune.py:68] layer 14_v @ epoch 1 new loss 1.919918395287823e-05 old loss 2.081428283418063e-05 BETTER
I0312 05:33:22.619253 1839672 config.py:54] PyTorch version 2.1.1 available.
I0312 05:33:23.672570 1799993 quantize_finetune_llama.py:183] layer 16 gpu 0
I0312 05:33:23.746729 1839672 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:33:31.697713 1839672 finetune.py:45] layer 15_v initial loss 2.9400513085420243e-05
I0312 05:33:45.374586 1838113 finetune.py:68] layer 13_v @ epoch 4 new loss 1.3657277122547384e-05 old loss 1.385156519972952e-05 BETTER
I0312 05:33:49.690195 1837271 finetune.py:68] layer 12_q @ epoch 1 new loss 1.4361989087774418e-05 old loss 1.4769680092285853e-05 BETTER
I0312 05:33:54.398501 1838903 finetune.py:68] layer 14_v @ epoch 2 new loss 1.828479435062036e-05 old loss 1.919918395287823e-05 BETTER
I0312 05:33:54.670399 1838113 finetune.py:45] layer 13_q initial loss 1.6989111827570014e-05
I0312 05:34:02.711678 1839672 finetune.py:68] layer 15_v @ epoch 0 new loss 2.047253292403184e-05 old loss 2.9400513085420243e-05 BETTER
I0312 05:34:24.020094 1837271 finetune.py:68] layer 12_q @ epoch 2 new loss 1.4088241186982486e-05 old loss 1.4361989087774418e-05 BETTER
I0312 05:34:26.340672 1838113 finetune.py:68] layer 13_q @ epoch 0 new loss 1.5545769201708026e-05 old loss 1.6989111827570014e-05 BETTER
I0312 05:34:27.209480 1838903 finetune.py:68] layer 14_v @ epoch 3 new loss 1.7655031115282327e-05 old loss 1.828479435062036e-05 BETTER
I0312 05:34:34.584140 1839672 finetune.py:68] layer 15_v @ epoch 1 new loss 1.8936130800284445e-05 old loss 2.047253292403184e-05 BETTER
I0312 05:34:58.603550 1837271 finetune.py:68] layer 12_q @ epoch 3 new loss 1.3864453649148345e-05 old loss 1.4088241186982486e-05 BETTER
I0312 05:34:58.631829 1838113 finetune.py:68] layer 13_q @ epoch 1 new loss 1.5130050087464042e-05 old loss 1.5545769201708026e-05 BETTER
I0312 05:35:00.321383 1838903 finetune.py:68] layer 14_v @ epoch 4 new loss 1.7220090740011074e-05 old loss 1.7655031115282327e-05 BETTER
I0312 05:35:06.736466 1839672 finetune.py:68] layer 15_v @ epoch 2 new loss 1.8046252080239356e-05 old loss 1.8936130800284445e-05 BETTER
I0312 05:35:09.554941 1838903 finetune.py:45] layer 14_q initial loss 2.144771315215621e-05
I0312 05:35:30.904240 1838113 finetune.py:68] layer 13_q @ epoch 2 new loss 1.4851242667646147e-05 old loss 1.5130050087464042e-05 BETTER
I0312 05:35:33.218204 1837271 finetune.py:68] layer 12_q @ epoch 4 new loss 1.3676311027666088e-05 old loss 1.3864453649148345e-05 BETTER
I0312 05:35:39.134660 1839672 finetune.py:68] layer 15_v @ epoch 3 new loss 1.745568442856893e-05 old loss 1.8046252080239356e-05 BETTER
I0312 05:35:41.527295 1838903 finetune.py:68] layer 14_q @ epoch 0 new loss 1.9751369109144434e-05 old loss 2.144771315215621e-05 BETTER
I0312 05:35:42.620716 1837271 finetune.py:45] layer 12_k initial loss 1.638753201405052e-05
I0312 05:36:03.193812 1838113 finetune.py:68] layer 13_q @ epoch 3 new loss 1.4640200788562652e-05 old loss 1.4851242667646147e-05 BETTER
I0312 05:36:11.795930 1839672 finetune.py:68] layer 15_v @ epoch 4 new loss 1.7068658053176478e-05 old loss 1.745568442856893e-05 BETTER
I0312 05:36:13.957008 1838903 finetune.py:68] layer 14_q @ epoch 1 new loss 1.9197426809114404e-05 old loss 1.9751369109144434e-05 BETTER
I0312 05:36:15.646542 1837271 finetune.py:68] layer 12_k @ epoch 0 new loss 1.5728577636764385e-05 old loss 1.638753201405052e-05 BETTER
I0312 05:36:20.893444 1839672 finetune.py:45] layer 15_q initial loss 2.192505053244531e-05
I0312 05:36:35.633648 1838113 finetune.py:68] layer 13_q @ epoch 4 new loss 1.4469824236584827e-05 old loss 1.4640200788562652e-05 BETTER
I0312 05:36:44.648591 1838113 finetune.py:45] layer 13_k initial loss 1.7004029359668493e-05
I0312 05:36:46.627546 1838903 finetune.py:68] layer 14_q @ epoch 2 new loss 1.8791117327054963e-05 old loss 1.9197426809114404e-05 BETTER
I0312 05:36:49.302862 1837271 finetune.py:68] layer 12_k @ epoch 1 new loss 1.554233858769294e-05 old loss 1.5728577636764385e-05 BETTER
I0312 05:36:52.266391 1839672 finetune.py:68] layer 15_q @ epoch 0 new loss 1.9707786123035476e-05 old loss 2.192505053244531e-05 BETTER
I0312 05:37:16.068326 1838113 finetune.py:68] layer 13_k @ epoch 0 new loss 1.6474265066790394e-05 old loss 1.7004029359668493e-05 BETTER
I0312 05:37:19.307735 1838903 finetune.py:68] layer 14_q @ epoch 3 new loss 1.8466282199369743e-05 old loss 1.8791117327054963e-05 BETTER
I0312 05:37:23.074290 1837271 finetune.py:68] layer 12_k @ epoch 2 new loss 1.542152494948823e-05 old loss 1.554233858769294e-05 BETTER
I0312 05:37:24.255572 1839672 finetune.py:68] layer 15_q @ epoch 1 new loss 1.9142900782753713e-05 old loss 1.9707786123035476e-05 BETTER
I0312 05:37:48.172053 1838113 finetune.py:68] layer 13_k @ epoch 1 new loss 1.6274312656605616e-05 old loss 1.6474265066790394e-05 BETTER
I0312 05:37:51.707216 1838903 finetune.py:68] layer 14_q @ epoch 4 new loss 1.8204458683612756e-05 old loss 1.8466282199369743e-05 BETTER
I0312 05:37:56.464710 1839672 finetune.py:68] layer 15_q @ epoch 2 new loss 1.872479151643347e-05 old loss 1.9142900782753713e-05 BETTER
I0312 05:37:56.836404 1837271 finetune.py:68] layer 12_k @ epoch 3 new loss 1.5288611393771134e-05 old loss 1.542152494948823e-05 BETTER
I0312 05:38:02.193675 1838903 finetune.py:45] layer 14_k initial loss 2.1489835489774123e-05
I0312 05:38:20.628871 1838113 finetune.py:68] layer 13_k @ epoch 2 new loss 1.6139287254191004e-05 old loss 1.6274312656605616e-05 BETTER
I0312 05:38:28.517457 1839672 finetune.py:68] layer 15_q @ epoch 3 new loss 1.8439186533214524e-05 old loss 1.872479151643347e-05 BETTER
I0312 05:38:30.966407 1837271 finetune.py:68] layer 12_k @ epoch 4 new loss 1.521193280495936e-05 old loss 1.5288611393771134e-05 BETTER
I0312 05:38:33.699395 1838903 finetune.py:68] layer 14_k @ epoch 0 new loss 2.085127925965935e-05 old loss 2.1489835489774123e-05 BETTER
I0312 05:38:40.543454 1837271 finetune.py:45] layer 12_o initial loss 4.503748641582206e-05
I0312 05:38:52.641499 1838113 finetune.py:68] layer 13_k @ epoch 3 new loss 1.6019665054045618e-05 old loss 1.6139287254191004e-05 BETTER
I0312 05:39:00.606496 1839672 finetune.py:68] layer 15_q @ epoch 4 new loss 1.817942575144116e-05 old loss 1.8439186533214524e-05 BETTER
I0312 05:39:05.825492 1838903 finetune.py:68] layer 14_k @ epoch 1 new loss 2.0605728423106484e-05 old loss 2.085127925965935e-05 BETTER
I0312 05:39:09.692142 1839672 finetune.py:45] layer 15_k initial loss 2.161973316106014e-05
I0312 05:39:13.292577 1837271 finetune.py:68] layer 12_o @ epoch 0 new loss 4.167473889538087e-05 old loss 4.503748641582206e-05 BETTER
I0312 05:39:24.690150 1838113 finetune.py:68] layer 13_k @ epoch 4 new loss 1.591917680343613e-05 old loss 1.6019665054045618e-05 BETTER
I0312 05:39:33.784785 1838113 finetune.py:45] layer 13_o initial loss 4.66440906166099e-05
I0312 05:39:37.934743 1838903 finetune.py:68] layer 14_k @ epoch 2 new loss 2.041699190158397e-05 old loss 2.0605728423106484e-05 BETTER
I0312 05:39:40.534307 1839672 finetune.py:68] layer 15_k @ epoch 0 new loss 2.0966190277249552e-05 old loss 2.161973316106014e-05 BETTER
I0312 05:39:46.860232 1837271 finetune.py:68] layer 12_o @ epoch 1 new loss 4.005672599305399e-05 old loss 4.167473889538087e-05 BETTER
I0312 05:40:04.396206 1838113 finetune.py:68] layer 13_o @ epoch 0 new loss 4.273399099474773e-05 old loss 4.66440906166099e-05 BETTER
I0312 05:40:10.105046 1838903 finetune.py:68] layer 14_k @ epoch 3 new loss 2.0300818505347706e-05 old loss 2.041699190158397e-05 BETTER
I0312 05:40:12.062070 1839672 finetune.py:68] layer 15_k @ epoch 1 new loss 2.0717381630674936e-05 old loss 2.0966190277249552e-05 BETTER
I0312 05:40:20.398199 1837271 finetune.py:68] layer 12_o @ epoch 2 new loss 3.897752321790904e-05 old loss 4.005672599305399e-05 BETTER
I0312 05:40:35.927082 1838113 finetune.py:68] layer 13_o @ epoch 1 new loss 4.096398697583936e-05 old loss 4.273399099474773e-05 BETTER
I0312 05:40:42.194711 1838903 finetune.py:68] layer 14_k @ epoch 4 new loss 2.0139954358455725e-05 old loss 2.0300818505347706e-05 BETTER
I0312 05:40:43.686182 1839672 finetune.py:68] layer 15_k @ epoch 2 new loss 2.05006763280835e-05 old loss 2.0717381630674936e-05 BETTER
I0312 05:40:51.471040 1838903 finetune.py:45] layer 14_o initial loss 5.9793132095364854e-05
I0312 05:40:53.948557 1837271 finetune.py:68] layer 12_o @ epoch 3 new loss 3.8162164855748415e-05 old loss 3.897752321790904e-05 BETTER
I0312 05:41:07.338819 1838113 finetune.py:68] layer 13_o @ epoch 2 new loss 3.9800455851946026e-05 old loss 4.096398697583936e-05 BETTER
I0312 05:41:15.367752 1839672 finetune.py:68] layer 15_k @ epoch 3 new loss 2.0344883523648605e-05 old loss 2.05006763280835e-05 BETTER
I0312 05:41:22.342911 1838903 finetune.py:68] layer 14_o @ epoch 0 new loss 5.516759119927883e-05 old loss 5.9793132095364854e-05 BETTER
I0312 05:41:27.493658 1837271 finetune.py:68] layer 12_o @ epoch 4 new loss 3.752221891772933e-05 old loss 3.8162164855748415e-05 BETTER
I0312 05:41:38.870084 1838113 finetune.py:68] layer 13_o @ epoch 3 new loss 3.8953097828198224e-05 old loss 3.9800455851946026e-05 BETTER
I0312 05:41:42.523799 1837271 finetune.py:45] layer 12_up initial loss 5.711458652513102e-05
I0312 05:41:47.216066 1839672 finetune.py:68] layer 15_k @ epoch 4 new loss 2.023219713009894e-05 old loss 2.0344883523648605e-05 BETTER
I0312 05:41:54.051431 1838903 finetune.py:68] layer 14_o @ epoch 1 new loss 5.298633186612278e-05 old loss 5.516759119927883e-05 BETTER
I0312 05:41:56.437140 1839672 finetune.py:45] layer 15_o initial loss 5.934673026786186e-05
I0312 05:42:10.516837 1838113 finetune.py:68] layer 13_o @ epoch 4 new loss 3.828402986982837e-05 old loss 3.8953097828198224e-05 BETTER
I0312 05:42:13.338929 1837271 finetune.py:68] layer 12_up @ epoch 0 new loss 5.5660602811258286e-05 old loss 5.711458652513102e-05 BETTER
I0312 05:42:25.547880 1838113 finetune.py:45] layer 13_up initial loss 6.156098243081942e-05
I0312 05:42:25.943933 1838903 finetune.py:68] layer 14_o @ epoch 2 new loss 5.1542399887694046e-05 old loss 5.298633186612278e-05 BETTER
I0312 05:42:26.723616 1839672 finetune.py:68] layer 15_o @ epoch 0 new loss 5.394817344495095e-05 old loss 5.934673026786186e-05 BETTER
I0312 05:42:45.055077 1837271 finetune.py:68] layer 12_up @ epoch 1 new loss 5.477645026985556e-05 old loss 5.5660602811258286e-05 BETTER
I0312 05:42:54.453915 1838113 finetune.py:68] layer 13_up @ epoch 0 new loss 5.9696936659747735e-05 old loss 6.156098243081942e-05 BETTER
I0312 05:42:57.813054 1839672 finetune.py:68] layer 15_o @ epoch 1 new loss 5.170092481421307e-05 old loss 5.394817344495095e-05 BETTER
I0312 05:42:57.833856 1838903 finetune.py:68] layer 14_o @ epoch 3 new loss 5.04666350025218e-05 old loss 5.1542399887694046e-05 BETTER
I0312 05:43:16.827026 1837271 finetune.py:68] layer 12_up @ epoch 2 new loss 5.408198921941221e-05 old loss 5.477645026985556e-05 BETTER
I0312 05:43:24.216521 1838113 finetune.py:68] layer 13_up @ epoch 1 new loss 5.861499084858224e-05 old loss 5.9696936659747735e-05 BETTER
I0312 05:43:28.804864 1839672 finetune.py:68] layer 15_o @ epoch 2 new loss 5.0246719183633104e-05 old loss 5.170092481421307e-05 BETTER
I0312 05:43:29.631107 1838903 finetune.py:68] layer 14_o @ epoch 4 new loss 4.962406092090532e-05 old loss 5.04666350025218e-05 BETTER
I0312 05:43:44.597307 1838903 finetune.py:45] layer 14_up initial loss 7.560168887721375e-05
I0312 05:43:48.535179 1837271 finetune.py:68] layer 12_up @ epoch 3 new loss 5.350757055566646e-05 old loss 5.408198921941221e-05 BETTER
I0312 05:43:54.027337 1838113 finetune.py:68] layer 13_up @ epoch 2 new loss 5.778862032457255e-05 old loss 5.861499084858224e-05 BETTER
I0312 05:43:59.971641 1839672 finetune.py:68] layer 15_o @ epoch 3 new loss 4.920028368360363e-05 old loss 5.0246719183633104e-05 BETTER
I0312 05:44:13.659470 1838903 finetune.py:68] layer 14_up @ epoch 0 new loss 7.357374852290377e-05 old loss 7.560168887721375e-05 BETTER
I0312 05:44:20.204484 1837271 finetune.py:68] layer 12_up @ epoch 4 new loss 5.30213255842682e-05 old loss 5.350757055566646e-05 BETTER
I0312 05:44:23.777141 1838113 finetune.py:68] layer 13_up @ epoch 3 new loss 5.712333586416207e-05 old loss 5.778862032457255e-05 BETTER
I0312 05:44:30.946325 1839672 finetune.py:68] layer 15_o @ epoch 4 new loss 4.8394886107416824e-05 old loss 4.920028368360363e-05 BETTER
I0312 05:44:35.649501 1837271 finetune.py:45] layer 12_gate initial loss 6.682208913844079e-05
I0312 05:44:43.500007 1838903 finetune.py:68] layer 14_up @ epoch 1 new loss 7.238599937409163e-05 old loss 7.357374852290377e-05 BETTER
I0312 05:44:45.838199 1839672 finetune.py:45] layer 15_up initial loss 8.000680827535689e-05
I0312 05:44:53.448487 1838113 finetune.py:68] layer 13_up @ epoch 4 new loss 5.657022848026827e-05 old loss 5.712333586416207e-05 BETTER
I0312 05:45:04.676430 1837271 finetune.py:68] layer 12_gate @ epoch 0 new loss 6.608897092519328e-05 old loss 6.682208913844079e-05 BETTER
I0312 05:45:08.501564 1838113 finetune.py:45] layer 13_gate initial loss 7.324133184738457e-05
I0312 05:45:13.270838 1838903 finetune.py:68] layer 14_up @ epoch 2 new loss 7.147470023483038e-05 old loss 7.238599937409163e-05 BETTER
I0312 05:45:14.479800 1839672 finetune.py:68] layer 15_up @ epoch 0 new loss 7.733791426289827e-05 old loss 8.000680827535689e-05 BETTER
I0312 05:45:34.458681 1837271 finetune.py:68] layer 12_gate @ epoch 1 new loss 6.560876499861479e-05 old loss 6.608897092519328e-05 BETTER
I0312 05:45:36.052763 1838113 finetune.py:68] layer 13_gate @ epoch 0 new loss 7.233158248709515e-05 old loss 7.324133184738457e-05 BETTER
I0312 05:45:43.161278 1838903 finetune.py:68] layer 14_up @ epoch 3 new loss 7.07304643583484e-05 old loss 7.147470023483038e-05 BETTER
I0312 05:45:43.850136 1839672 finetune.py:68] layer 15_up @ epoch 1 new loss 7.591905887238681e-05 old loss 7.733791426289827e-05 BETTER
I0312 05:46:04.245985 1838113 finetune.py:68] layer 13_gate @ epoch 1 new loss 7.174970232881606e-05 old loss 7.233158248709515e-05 BETTER
I0312 05:46:04.304855 1837271 finetune.py:68] layer 12_gate @ epoch 2 new loss 6.521490286104381e-05 old loss 6.560876499861479e-05 BETTER
I0312 05:46:13.232228 1838903 finetune.py:68] layer 14_up @ epoch 4 new loss 7.010632543824613e-05 old loss 7.07304643583484e-05 BETTER
I0312 05:46:13.355336 1839672 finetune.py:68] layer 15_up @ epoch 2 new loss 7.486831600544974e-05 old loss 7.591905887238681e-05 BETTER
I0312 05:46:28.400163 1838903 finetune.py:45] layer 14_gate initial loss 8.962499850895256e-05
I0312 05:46:32.707393 1838113 finetune.py:68] layer 13_gate @ epoch 2 new loss 7.128706783987582e-05 old loss 7.174970232881606e-05 BETTER
I0312 05:46:34.510025 1837271 finetune.py:68] layer 12_gate @ epoch 3 new loss 6.487933569587767e-05 old loss 6.521490286104381e-05 BETTER
I0312 05:46:42.925404 1839672 finetune.py:68] layer 15_up @ epoch 3 new loss 7.401886250590906e-05 old loss 7.486831600544974e-05 BETTER
I0312 05:46:55.996591 1838903 finetune.py:68] layer 14_gate @ epoch 0 new loss 8.860469097271562e-05 old loss 8.962499850895256e-05 BETTER
I0312 05:47:01.045099 1838113 finetune.py:68] layer 13_gate @ epoch 3 new loss 7.089030259521678e-05 old loss 7.128706783987582e-05 BETTER
I0312 05:47:04.650999 1837271 finetune.py:68] layer 12_gate @ epoch 4 new loss 6.458521966123953e-05 old loss 6.487933569587767e-05 BETTER
I0312 05:47:12.615367 1839672 finetune.py:68] layer 15_up @ epoch 4 new loss 7.333826943067834e-05 old loss 7.401886250590906e-05 BETTER
I0312 05:47:20.657938 1837271 finetune.py:45] layer 12_down initial loss 9.578309254720807e-05
I0312 05:47:24.116375 1838903 finetune.py:68] layer 14_gate @ epoch 1 new loss 8.795443864073604e-05 old loss 8.860469097271562e-05 BETTER
I0312 05:47:27.933225 1839672 finetune.py:45] layer 15_gate initial loss 9.740026871440932e-05
I0312 05:47:29.399325 1838113 finetune.py:68] layer 13_gate @ epoch 4 new loss 7.055119203869253e-05 old loss 7.089030259521678e-05 BETTER
I0312 05:47:44.950819 1838113 finetune.py:45] layer 13_down initial loss 0.00010853187995962799
I0312 05:47:48.165558 1837271 finetune.py:68] layer 12_down @ epoch 0 new loss 9.577806486049667e-05 old loss 9.578309254720807e-05 BETTER
I0312 05:47:52.460451 1838903 finetune.py:68] layer 14_gate @ epoch 2 new loss 8.743951912038028e-05 old loss 8.795443864073604e-05 BETTER
I0312 05:47:55.085408 1839672 finetune.py:68] layer 15_gate @ epoch 0 new loss 9.60667894105427e-05 old loss 9.740026871440932e-05 BETTER
I0312 05:48:11.060253 1838113 finetune.py:68] layer 13_down @ epoch 0 new loss 0.00010852218110812828 old loss 0.00010853187995962799 BETTER
I0312 05:48:16.662954 1837271 finetune.py:68] layer 12_down @ epoch 1 new loss 9.577537275617942e-05 old loss 9.577806486049667e-05 BETTER
I0312 05:48:20.713443 1838903 finetune.py:68] layer 14_gate @ epoch 3 new loss 8.700275793671608e-05 old loss 8.743951912038028e-05 BETTER
I0312 05:48:23.192364 1839672 finetune.py:68] layer 15_gate @ epoch 1 new loss 9.529042290523648e-05 old loss 9.60667894105427e-05 BETTER
I0312 05:48:37.637654 1838113 finetune.py:68] layer 13_down @ epoch 1 new loss 0.00010851816477952525 old loss 0.00010852218110812828 BETTER
I0312 05:48:45.118996 1837271 finetune.py:68] layer 12_down @ epoch 2 new loss 9.577394666848704e-05 old loss 9.577537275617942e-05 BETTER
I0312 05:48:49.097102 1838903 finetune.py:68] layer 14_gate @ epoch 4 new loss 8.662007167004049e-05 old loss 8.700275793671608e-05 BETTER
I0312 05:48:51.284301 1839672 finetune.py:68] layer 15_gate @ epoch 2 new loss 9.468475764151663e-05 old loss 9.529042290523648e-05 BETTER
I0312 05:49:04.468492 1838113 finetune.py:68] layer 13_down @ epoch 2 new loss 0.00010851589468074962 old loss 0.00010851816477952525 BETTER
I0312 05:49:05.260787 1838903 finetune.py:45] layer 14_down initial loss 0.000130619402625598
I0312 05:49:13.725858 1837271 finetune.py:68] layer 12_down @ epoch 3 new loss 9.577350283507258e-05 old loss 9.577394666848704e-05 BETTER
I0312 05:49:19.474414 1839672 finetune.py:68] layer 15_gate @ epoch 3 new loss 9.417812543688342e-05 old loss 9.468475764151663e-05 BETTER
I0312 05:49:31.148533 1838903 finetune.py:68] layer 14_down @ epoch 0 new loss 0.00013061206846032292 old loss 0.000130619402625598 BETTER
I0312 05:49:31.161278 1838113 finetune.py:68] layer 13_down @ epoch 3 new loss 0.00010851491242647171 old loss 0.00010851589468074962 BETTER
I0312 05:49:42.195501 1837271 finetune.py:68] layer 12_down @ epoch 4 new loss 9.577179298503324e-05 old loss 9.577350283507258e-05 BETTER
12_v proxy err 0.002349474001675844 tr(WHW.T) 703.318603515625
12_q proxy err 0.00027801431133411825 tr(WHW.T) 7046.3544921875
12_k proxy err 0.00019889124087058008 tr(WHW.T) 10896.1455078125
12_o proxy err 0.0036646677181124687 tr(WHW.T) 39.33597183227539
12_up proxy err 0.0014661034801974893 tr(WHW.T) 1228.2908935546875
12_gate proxy err 0.0007753390818834305 tr(WHW.T) 2382.342529296875
12_down proxy err 0.0020183378364890814 tr(WHW.T) 64.21321868896484
I0312 05:49:48.039157 1839672 finetune.py:68] layer 15_gate @ epoch 4 new loss 9.375110676046461e-05 old loss 9.417812543688342e-05 BETTER
I0312 05:49:58.258857 1838903 finetune.py:68] layer 14_down @ epoch 1 new loss 0.00013060829951427877 old loss 0.00013061206846032292 BETTER
I0312 05:49:58.261282 1838113 finetune.py:68] layer 13_down @ epoch 4 new loss 0.00010851329716388136 old loss 0.00010851491242647171 BETTER
13_v proxy err 0.002386189764365554 tr(WHW.T) 714.5677490234375
13_q proxy err 0.0002836257917806506 tr(WHW.T) 6956.947265625
13_k proxy err 0.00020308964303694665 tr(WHW.T) 10431.0888671875
13_o proxy err 0.0032214175444096327 tr(WHW.T) 45.936798095703125
13_up proxy err 0.0014073321362957358 tr(WHW.T) 1367.423828125
13_gate proxy err 0.0007587363361380994 tr(WHW.T) 2601.502197265625
13_down proxy err 0.0019945905078202486 tr(WHW.T) 79.42132568359375
I0312 05:50:03.738193 1839672 finetune.py:45] layer 15_down initial loss 0.00014870462473481894
I0312 05:50:24.934349 1838903 finetune.py:68] layer 14_down @ epoch 2 new loss 0.00013060591300018132 old loss 0.00013060829951427877 BETTER
I0312 05:50:28.752183 1839672 finetune.py:68] layer 15_down @ epoch 0 new loss 0.00014869756705593318 old loss 0.00014870462473481894 BETTER
I0312 05:50:51.766752 1838903 finetune.py:68] layer 14_down @ epoch 3 new loss 0.00013060423952993006 old loss 0.00013060591300018132 BETTER
I0312 05:50:54.904720 1839672 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00014869231381453574 old loss 0.00014869756705593318 BETTER
I0312 05:51:15.476418 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 16 in 72.82369923591614s
I0312 05:51:18.757898 1838903 finetune.py:68] layer 14_down @ epoch 4 new loss 0.00013060274068266153 old loss 0.00013060423952993006 BETTER
I0312 05:51:18.798799 1849409 config.py:54] PyTorch version 2.1.1 available.
I0312 05:51:19.800981 1799993 quantize_finetune_llama.py:183] layer 17 gpu 1
I0312 05:51:19.872841 1849409 data_utils.py:336] using 256 training seqs, 128 validation seqs
14_v proxy err 0.0025306574534624815 tr(WHW.T) 706.1612548828125
14_q proxy err 0.00029311145772226155 tr(WHW.T) 7077.7412109375
14_k proxy err 0.00020126199524383992 tr(WHW.T) 11299.2587890625
14_o proxy err 0.0036646791268140078 tr(WHW.T) 51.02627182006836
14_up proxy err 0.0014367870753630996 tr(WHW.T) 1464.745849609375
14_gate proxy err 0.0008013654733076692 tr(WHW.T) 2683.971435546875
14_down proxy err 0.002023479202762246 tr(WHW.T) 90.34437561035156
I0312 05:51:21.197072 1839672 finetune.py:68] layer 15_down @ epoch 2 new loss 0.00014868841390125453 old loss 0.00014869231381453574 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:51:28.400690 1849409 finetune.py:45] layer 16_v initial loss 3.570569242583588e-05
I0312 05:51:47.426826 1839672 finetune.py:68] layer 15_down @ epoch 3 new loss 0.00014868621656205505 old loss 0.00014868841390125453 BETTER
I0312 05:52:01.447576 1849409 finetune.py:68] layer 16_v @ epoch 0 new loss 2.587202652648557e-05 old loss 3.570569242583588e-05 BETTER
I0312 05:52:13.638275 1839672 finetune.py:76] layer 15_down @ epoch 4 new loss 0.00014868626021780074 old loss 0.00014868621656205505 WORSE
15_v proxy err 0.002271191915497184 tr(WHW.T) 762.7275390625
15_q proxy err 0.00027523728203959763 tr(WHW.T) 7253.1494140625
15_k proxy err 0.00019490922568365932 tr(WHW.T) 11075.49609375
15_o proxy err 0.003021146170794964 tr(WHW.T) 59.75034713745117
15_up proxy err 0.0013951874570921063 tr(WHW.T) 1641.0286865234375
15_gate proxy err 0.0008037519291974604 tr(WHW.T) 2905.201171875
15_down proxy err 0.002000214532017708 tr(WHW.T) 114.18260192871094
I0312 05:52:32.471771 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 17 in 69.5960648059845s
I0312 05:52:35.682407 1849409 finetune.py:68] layer 16_v @ epoch 1 new loss 2.395430601609405e-05 old loss 2.587202652648557e-05 BETTER
I0312 05:52:35.745721 1850205 config.py:54] PyTorch version 2.1.1 available.
I0312 05:52:36.757726 1799993 quantize_finetune_llama.py:183] layer 18 gpu 2
I0312 05:52:36.821753 1850205 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:52:45.067990 1850205 finetune.py:45] layer 17_v initial loss 3.0283681553555652e-05
I0312 05:53:10.339690 1849409 finetune.py:68] layer 16_v @ epoch 2 new loss 2.2888827515998855e-05 old loss 2.395430601609405e-05 BETTER
I0312 05:53:16.144771 1850205 finetune.py:68] layer 17_v @ epoch 0 new loss 2.2041454940335825e-05 old loss 3.0283681553555652e-05 BETTER
I0312 05:53:45.070084 1849409 finetune.py:68] layer 16_v @ epoch 3 new loss 2.2201189494808204e-05 old loss 2.2888827515998855e-05 BETTER
I0312 05:53:46.653788 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 18 in 69.48862171173096s
I0312 05:53:48.226898 1850205 finetune.py:68] layer 17_v @ epoch 1 new loss 2.051151568593923e-05 old loss 2.2041454940335825e-05 BETTER
I0312 05:53:49.870371 1850983 config.py:54] PyTorch version 2.1.1 available.
I0312 05:53:50.875362 1799993 quantize_finetune_llama.py:183] layer 19 gpu 3
I0312 05:53:50.948621 1850983 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:53:59.153853 1850983 finetune.py:45] layer 18_v initial loss 2.9456836273311637e-05
I0312 05:54:19.981243 1849409 finetune.py:68] layer 16_v @ epoch 4 new loss 2.183651304221712e-05 old loss 2.2201189494808204e-05 BETTER
I0312 05:54:20.609478 1850205 finetune.py:68] layer 17_v @ epoch 2 new loss 1.9716399037861265e-05 old loss 2.051151568593923e-05 BETTER
I0312 05:54:29.205658 1849409 finetune.py:45] layer 16_q initial loss 2.7013027647626586e-05
I0312 05:54:30.545789 1850983 finetune.py:68] layer 18_v @ epoch 0 new loss 2.1870935597689822e-05 old loss 2.9456836273311637e-05 BETTER
I0312 05:54:53.080638 1850205 finetune.py:68] layer 17_v @ epoch 3 new loss 1.925155083881691e-05 old loss 1.9716399037861265e-05 BETTER
I0312 05:54:59.971045 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 19 in 68.67707705497742s
I0312 05:55:02.686365 1850983 finetune.py:68] layer 18_v @ epoch 1 new loss 2.0620282157324255e-05 old loss 2.1870935597689822e-05 BETTER
I0312 05:55:02.697230 1849409 finetune.py:68] layer 16_q @ epoch 0 new loss 2.5010385797941126e-05 old loss 2.7013027647626586e-05 BETTER
I0312 05:55:03.264277 1851750 config.py:54] PyTorch version 2.1.1 available.
I0312 05:55:04.300274 1799993 quantize_finetune_llama.py:183] layer 20 gpu 0
I0312 05:55:04.380555 1851750 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 05:55:12.214000 1851750 finetune.py:45] layer 19_v initial loss 2.8841825042036362e-05
I0312 05:55:25.570813 1850205 finetune.py:68] layer 17_v @ epoch 4 new loss 1.8919241483672522e-05 old loss 1.925155083881691e-05 BETTER
I0312 05:55:34.505486 1850205 finetune.py:45] layer 17_q initial loss 2.391295311099384e-05
I0312 05:55:35.056132 1850983 finetune.py:68] layer 18_v @ epoch 2 new loss 2.0072460756637156e-05 old loss 2.0620282157324255e-05 BETTER
I0312 05:55:36.873519 1849409 finetune.py:68] layer 16_q @ epoch 1 new loss 2.4334891350008547e-05 old loss 2.5010385797941126e-05 BETTER
I0312 05:55:42.978572 1851750 finetune.py:68] layer 19_v @ epoch 0 new loss 2.1568144802586176e-05 old loss 2.8841825042036362e-05 BETTER
I0312 05:56:06.239398 1850205 finetune.py:68] layer 17_q @ epoch 0 new loss 2.1944637410342693e-05 old loss 2.391295311099384e-05 BETTER
I0312 05:56:07.785194 1850983 finetune.py:68] layer 18_v @ epoch 3 new loss 1.9464474462438375e-05 old loss 2.0072460756637156e-05 BETTER
I0312 05:56:11.290798 1849409 finetune.py:68] layer 16_q @ epoch 2 new loss 2.389570727245882e-05 old loss 2.4334891350008547e-05 BETTER
I0312 05:56:14.926480 1851750 finetune.py:68] layer 19_v @ epoch 1 new loss 2.029075585596729e-05 old loss 2.1568144802586176e-05 BETTER
I0312 05:56:38.630919 1850205 finetune.py:68] layer 17_q @ epoch 1 new loss 2.130997381755151e-05 old loss 2.1944637410342693e-05 BETTER
I0312 05:56:41.081676 1850983 finetune.py:68] layer 18_v @ epoch 4 new loss 1.90564041986363e-05 old loss 1.9464474462438375e-05 BETTER
I0312 05:56:45.853927 1849409 finetune.py:68] layer 16_q @ epoch 3 new loss 2.354764728806913e-05 old loss 2.389570727245882e-05 BETTER
I0312 05:56:47.373195 1851750 finetune.py:68] layer 19_v @ epoch 2 new loss 1.9584389519877732e-05 old loss 2.029075585596729e-05 BETTER
I0312 05:56:50.748746 1850983 finetune.py:45] layer 18_q initial loss 2.5459225071244873e-05
I0312 05:57:10.938832 1850205 finetune.py:68] layer 17_q @ epoch 2 new loss 2.0868132196483202e-05 old loss 2.130997381755151e-05 BETTER
I0312 05:57:20.335920 1851750 finetune.py:68] layer 19_v @ epoch 3 new loss 1.9149190848111175e-05 old loss 1.9584389519877732e-05 BETTER
I0312 05:57:21.000479 1849409 finetune.py:68] layer 16_q @ epoch 4 new loss 2.3241842427523807e-05 old loss 2.354764728806913e-05 BETTER
I0312 05:57:22.905118 1850983 finetune.py:68] layer 18_q @ epoch 0 new loss 2.3285123461391777e-05 old loss 2.5459225071244873e-05 BETTER
I0312 05:57:30.851923 1849409 finetune.py:45] layer 16_k initial loss 2.728900835791137e-05
I0312 05:57:43.530807 1850205 finetune.py:68] layer 17_q @ epoch 3 new loss 2.0554634829750285e-05 old loss 2.0868132196483202e-05 BETTER
I0312 05:57:53.097141 1851750 finetune.py:68] layer 19_v @ epoch 4 new loss 1.885333222162444e-05 old loss 1.9149190848111175e-05 BETTER
I0312 05:57:55.614421 1850983 finetune.py:68] layer 18_q @ epoch 1 new loss 2.2632249965681694e-05 old loss 2.3285123461391777e-05 BETTER
I0312 05:58:02.492797 1851750 finetune.py:45] layer 19_q initial loss 2.484716424078215e-05
I0312 05:58:04.202826 1849409 finetune.py:68] layer 16_k @ epoch 0 new loss 2.6609754058881663e-05 old loss 2.728900835791137e-05 BETTER
I0312 05:58:16.179148 1850205 finetune.py:68] layer 17_q @ epoch 4 new loss 2.0309737010393292e-05 old loss 2.0554634829750285e-05 BETTER
I0312 05:58:25.295847 1850205 finetune.py:45] layer 17_k initial loss 2.441595097479876e-05
I0312 05:58:28.285062 1850983 finetune.py:68] layer 18_q @ epoch 2 new loss 2.2182968677952886e-05 old loss 2.2632249965681694e-05 BETTER
I0312 05:58:33.886035 1851750 finetune.py:68] layer 19_q @ epoch 0 new loss 2.270796539960429e-05 old loss 2.484716424078215e-05 BETTER
I0312 05:58:38.062315 1849409 finetune.py:68] layer 16_k @ epoch 1 new loss 2.6345358492108062e-05 old loss 2.6609754058881663e-05 BETTER
I0312 05:58:56.746811 1850205 finetune.py:68] layer 17_k @ epoch 0 new loss 2.3617054466740228e-05 old loss 2.441595097479876e-05 BETTER
I0312 05:59:00.914754 1850983 finetune.py:68] layer 18_q @ epoch 3 new loss 2.1910422219661996e-05 old loss 2.2182968677952886e-05 BETTER
I0312 05:59:05.937448 1851750 finetune.py:68] layer 19_q @ epoch 1 new loss 2.2106440155766904e-05 old loss 2.270796539960429e-05 BETTER
I0312 05:59:12.170896 1849409 finetune.py:68] layer 16_k @ epoch 2 new loss 2.6147477910853922e-05 old loss 2.6345358492108062e-05 BETTER
I0312 05:59:28.876460 1850205 finetune.py:68] layer 17_k @ epoch 1 new loss 2.3349131879513152e-05 old loss 2.3617054466740228e-05 BETTER
I0312 05:59:33.225901 1850983 finetune.py:68] layer 18_q @ epoch 4 new loss 2.168555511161685e-05 old loss 2.1910422219661996e-05 BETTER
I0312 05:59:38.008316 1851750 finetune.py:68] layer 19_q @ epoch 2 new loss 2.1724999896832742e-05 old loss 2.2106440155766904e-05 BETTER
I0312 05:59:42.680925 1850983 finetune.py:45] layer 18_k initial loss 2.6453832106199116e-05
I0312 05:59:46.215622 1849409 finetune.py:68] layer 16_k @ epoch 3 new loss 2.6012403395725414e-05 old loss 2.6147477910853922e-05 BETTER
I0312 06:00:00.926663 1850205 finetune.py:68] layer 17_k @ epoch 2 new loss 2.315817619091831e-05 old loss 2.3349131879513152e-05 BETTER
I0312 06:00:10.057667 1851750 finetune.py:68] layer 19_q @ epoch 3 new loss 2.142217635991983e-05 old loss 2.1724999896832742e-05 BETTER
I0312 06:00:14.092578 1850983 finetune.py:68] layer 18_k @ epoch 0 new loss 2.575663529569283e-05 old loss 2.6453832106199116e-05 BETTER
I0312 06:00:20.095889 1849409 finetune.py:68] layer 16_k @ epoch 4 new loss 2.5888455638778396e-05 old loss 2.6012403395725414e-05 BETTER
I0312 06:00:29.446827 1849409 finetune.py:45] layer 16_o initial loss 7.466928218491375e-05
I0312 06:00:32.917883 1850205 finetune.py:68] layer 17_k @ epoch 3 new loss 2.3026001144899055e-05 old loss 2.315817619091831e-05 BETTER
I0312 06:00:42.107643 1851750 finetune.py:68] layer 19_q @ epoch 4 new loss 2.1181400370551273e-05 old loss 2.142217635991983e-05 BETTER
I0312 06:00:46.144117 1850983 finetune.py:68] layer 18_k @ epoch 1 new loss 2.5520563212921843e-05 old loss 2.575663529569283e-05 BETTER
I0312 06:00:51.169739 1851750 finetune.py:45] layer 19_k initial loss 2.5746670871740207e-05
I0312 06:01:02.011320 1849409 finetune.py:68] layer 16_o @ epoch 0 new loss 6.859856512164697e-05 old loss 7.466928218491375e-05 BETTER
I0312 06:01:05.023597 1850205 finetune.py:68] layer 17_k @ epoch 4 new loss 2.2916525267646648e-05 old loss 2.3026001144899055e-05 BETTER
I0312 06:01:14.123148 1850205 finetune.py:45] layer 17_o initial loss 5.865874118171632e-05
I0312 06:01:18.132358 1850983 finetune.py:68] layer 18_k @ epoch 2 new loss 2.534088162065018e-05 old loss 2.5520563212921843e-05 BETTER
I0312 06:01:22.093211 1851750 finetune.py:68] layer 19_k @ epoch 0 new loss 2.5041781555046327e-05 old loss 2.5746670871740207e-05 BETTER
I0312 06:01:35.755480 1849409 finetune.py:68] layer 16_o @ epoch 1 new loss 6.583559297723696e-05 old loss 6.859856512164697e-05 BETTER
I0312 06:01:44.973348 1850205 finetune.py:68] layer 17_o @ epoch 0 new loss 5.460140164359473e-05 old loss 5.865874118171632e-05 BETTER
I0312 06:01:50.261789 1850983 finetune.py:68] layer 18_k @ epoch 3 new loss 2.5212309992639348e-05 old loss 2.534088162065018e-05 BETTER
I0312 06:01:53.704502 1851750 finetune.py:68] layer 19_k @ epoch 1 new loss 2.4822287741699256e-05 old loss 2.5041781555046327e-05 BETTER
I0312 06:02:08.950048 1849409 finetune.py:68] layer 16_o @ epoch 2 new loss 6.39982390566729e-05 old loss 6.583559297723696e-05 BETTER
I0312 06:02:16.265149 1850205 finetune.py:68] layer 17_o @ epoch 1 new loss 5.280791810946539e-05 old loss 5.460140164359473e-05 BETTER
I0312 06:02:22.373777 1850983 finetune.py:68] layer 18_k @ epoch 4 new loss 2.5116572942351922e-05 old loss 2.5212309992639348e-05 BETTER
I0312 06:02:25.294265 1851750 finetune.py:68] layer 19_k @ epoch 2 new loss 2.4679060516064055e-05 old loss 2.4822287741699256e-05 BETTER
I0312 06:02:31.749011 1850983 finetune.py:45] layer 18_o initial loss 6.296933861449361e-05
I0312 06:02:42.198554 1849409 finetune.py:68] layer 16_o @ epoch 3 new loss 6.265381671255454e-05 old loss 6.39982390566729e-05 BETTER
I0312 06:02:47.533797 1850205 finetune.py:68] layer 17_o @ epoch 2 new loss 5.1633116527227685e-05 old loss 5.280791810946539e-05 BETTER
I0312 06:02:57.004906 1851750 finetune.py:68] layer 19_k @ epoch 3 new loss 2.4568136723246425e-05 old loss 2.4679060516064055e-05 BETTER
I0312 06:03:02.639540 1850983 finetune.py:68] layer 18_o @ epoch 0 new loss 5.8809961046790704e-05 old loss 6.296933861449361e-05 BETTER
I0312 06:03:15.543355 1849409 finetune.py:68] layer 16_o @ epoch 4 new loss 6.160219345474616e-05 old loss 6.265381671255454e-05 BETTER
I0312 06:03:18.925736 1850205 finetune.py:68] layer 17_o @ epoch 3 new loss 5.0783091865014285e-05 old loss 5.1633116527227685e-05 BETTER
I0312 06:03:28.616634 1851750 finetune.py:68] layer 19_k @ epoch 4 new loss 2.4450691853417084e-05 old loss 2.4568136723246425e-05 BETTER
I0312 06:03:30.778134 1849409 finetune.py:45] layer 16_up initial loss 0.00010253074287902564
I0312 06:03:34.150773 1850983 finetune.py:68] layer 18_o @ epoch 1 new loss 5.704879367840476e-05 old loss 5.8809961046790704e-05 BETTER
I0312 06:03:37.886991 1851750 finetune.py:45] layer 19_o initial loss 5.97487305640243e-05
I0312 06:03:50.376020 1850205 finetune.py:68] layer 17_o @ epoch 4 new loss 5.0130634917877614e-05 old loss 5.0783091865014285e-05 BETTER
I0312 06:04:01.629867 1849409 finetune.py:68] layer 16_up @ epoch 0 new loss 9.928556391969323e-05 old loss 0.00010253074287902564 BETTER
I0312 06:04:06.281630 1850205 finetune.py:45] layer 17_up initial loss 9.622996003599837e-05
I0312 06:04:06.628603 1850983 finetune.py:68] layer 18_o @ epoch 2 new loss 5.591589069808833e-05 old loss 5.704879367840476e-05 BETTER
I0312 06:04:08.729397 1851750 finetune.py:68] layer 19_o @ epoch 0 new loss 5.6132743338821456e-05 old loss 5.97487305640243e-05 BETTER
I0312 06:04:33.570533 1849409 finetune.py:68] layer 16_up @ epoch 1 new loss 9.749170567374676e-05 old loss 9.928556391969323e-05 BETTER
I0312 06:04:35.199248 1850205 finetune.py:68] layer 17_up @ epoch 0 new loss 9.323834819952026e-05 old loss 9.622996003599837e-05 BETTER
I0312 06:04:38.603321 1850983 finetune.py:68] layer 18_o @ epoch 3 new loss 5.5108070228016004e-05 old loss 5.591589069808833e-05 BETTER
I0312 06:04:39.991468 1851750 finetune.py:68] layer 19_o @ epoch 1 new loss 5.470254473038949e-05 old loss 5.6132743338821456e-05 BETTER
I0312 06:05:05.208397 1850205 finetune.py:68] layer 17_up @ epoch 1 new loss 9.159096953226253e-05 old loss 9.323834819952026e-05 BETTER
I0312 06:05:05.417170 1849409 finetune.py:68] layer 16_up @ epoch 2 new loss 9.616993338568136e-05 old loss 9.749170567374676e-05 BETTER
I0312 06:05:10.418528 1850983 finetune.py:68] layer 18_o @ epoch 4 new loss 5.449783566291444e-05 old loss 5.5108070228016004e-05 BETTER
I0312 06:05:11.004401 1851750 finetune.py:68] layer 19_o @ epoch 2 new loss 5.379577123676427e-05 old loss 5.470254473038949e-05 BETTER
I0312 06:05:25.393541 1850983 finetune.py:45] layer 18_up initial loss 0.00011009228182956576
I0312 06:05:34.947567 1850205 finetune.py:68] layer 17_up @ epoch 2 new loss 9.042521560331807e-05 old loss 9.159096953226253e-05 BETTER
I0312 06:05:37.245633 1849409 finetune.py:68] layer 16_up @ epoch 3 new loss 9.512773249298334e-05 old loss 9.616993338568136e-05 BETTER
I0312 06:05:42.134425 1851750 finetune.py:68] layer 19_o @ epoch 3 new loss 5.319029150996357e-05 old loss 5.379577123676427e-05 BETTER
I0312 06:05:54.346849 1850983 finetune.py:68] layer 18_up @ epoch 0 new loss 0.00010660248517524451 old loss 0.00011009228182956576 BETTER
I0312 06:06:05.471377 1850205 finetune.py:68] layer 17_up @ epoch 3 new loss 8.950813935371116e-05 old loss 9.042521560331807e-05 BETTER
I0312 06:06:09.138479 1849409 finetune.py:68] layer 16_up @ epoch 4 new loss 9.428809426026419e-05 old loss 9.512773249298334e-05 BETTER
I0312 06:06:13.346198 1851750 finetune.py:68] layer 19_o @ epoch 4 new loss 5.271903137327172e-05 old loss 5.319029150996357e-05 BETTER
I0312 06:06:24.296575 1850983 finetune.py:68] layer 18_up @ epoch 1 new loss 0.00010475675662746653 old loss 0.00010660248517524451 BETTER
I0312 06:06:24.304000 1849409 finetune.py:45] layer 16_gate initial loss 0.00012565818906296045
I0312 06:06:28.652657 1851750 finetune.py:45] layer 19_up initial loss 0.00011544203152880073
I0312 06:06:35.265452 1850205 finetune.py:68] layer 17_up @ epoch 4 new loss 8.877283107722178e-05 old loss 8.950813935371116e-05 BETTER
I0312 06:06:50.271997 1850205 finetune.py:45] layer 17_gate initial loss 0.00012401584535837173
I0312 06:06:53.371077 1849409 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.00012396236706990749 old loss 0.00012565818906296045 BETTER
I0312 06:06:54.355277 1850983 finetune.py:68] layer 18_up @ epoch 2 new loss 0.00010345983173465356 old loss 0.00010475675662746653 BETTER
I0312 06:06:57.229940 1851750 finetune.py:68] layer 19_up @ epoch 0 new loss 0.00011179638386238366 old loss 0.00011544203152880073 BETTER
I0312 06:07:17.830198 1850205 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0001225213782163337 old loss 0.00012401584535837173 BETTER
I0312 06:07:23.180583 1849409 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.00012295997294131666 old loss 0.00012396236706990749 BETTER
I0312 06:07:24.457241 1850983 finetune.py:68] layer 18_up @ epoch 3 new loss 0.00010245816520182416 old loss 0.00010345983173465356 BETTER
I0312 06:07:26.641887 1851750 finetune.py:68] layer 19_up @ epoch 1 new loss 0.00010995232150889933 old loss 0.00011179638386238366 BETTER
I0312 06:07:45.921822 1850205 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0001216014934470877 old loss 0.0001225213782163337 BETTER
I0312 06:07:53.117592 1849409 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.00012218038318678737 old loss 0.00012295997294131666 BETTER
I0312 06:07:54.428660 1850983 finetune.py:68] layer 18_up @ epoch 4 new loss 0.00010167243453906849 old loss 0.00010245816520182416 BETTER
I0312 06:07:56.134160 1851750 finetune.py:68] layer 19_up @ epoch 2 new loss 0.0001086554693756625 old loss 0.00010995232150889933 BETTER
I0312 06:08:09.523088 1850983 finetune.py:45] layer 18_gate initial loss 0.00014315654698293656
I0312 06:08:13.894759 1850205 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.00012091966345906258 old loss 0.0001216014934470877 BETTER
I0312 06:08:23.168854 1849409 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.00012155994772911072 old loss 0.00012218038318678737 BETTER
I0312 06:08:25.606864 1851750 finetune.py:68] layer 19_up @ epoch 3 new loss 0.00010770194057840854 old loss 0.0001086554693756625 BETTER
I0312 06:08:36.857027 1850983 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.00014160689897835255 old loss 0.00014315654698293656 BETTER
I0312 06:08:41.985802 1850205 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.00012036676343996078 old loss 0.00012091966345906258 BETTER
I0312 06:08:53.159177 1849409 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.00012103747576475143 old loss 0.00012155994772911072 BETTER
I0312 06:08:55.161563 1851750 finetune.py:68] layer 19_up @ epoch 4 new loss 0.00010696976096369326 old loss 0.00010770194057840854 BETTER
I0312 06:09:05.067072 1850983 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.00014066397852730006 old loss 0.00014160689897835255 BETTER
I0312 06:09:09.367141 1849409 finetune.py:45] layer 16_down initial loss 0.00019578089995775372
I0312 06:09:10.478103 1851750 finetune.py:45] layer 19_gate initial loss 0.000155863948748447
I0312 06:09:10.504819 1850205 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.00011990914208581671 old loss 0.00012036676343996078 BETTER
I0312 06:09:26.007200 1850205 finetune.py:45] layer 17_down initial loss 0.00020212447270751
I0312 06:09:33.180878 1850983 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.00013995385961607099 old loss 0.00014066397852730006 BETTER
I0312 06:09:36.806148 1849409 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0001957672502612695 old loss 0.00019578089995775372 BETTER
I0312 06:09:37.371521 1851750 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.00015415053348988295 old loss 0.000155863948748447 BETTER
I0312 06:09:52.022224 1850205 finetune.py:68] layer 17_down @ epoch 0 new loss 0.00020211207447573543 old loss 0.00020212447270751 BETTER
I0312 06:10:01.469143 1850983 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.00013939714699517936 old loss 0.00013995385961607099 BETTER
I0312 06:10:05.256393 1849409 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0001957600616151467 old loss 0.0001957672502612695 BETTER
I0312 06:10:05.503035 1851750 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.00015314585471060127 old loss 0.00015415053348988295 BETTER
I0312 06:10:18.954651 1850205 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0002021060645347461 old loss 0.00020211207447573543 BETTER
I0312 06:10:29.762406 1850983 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.00013894721632823348 old loss 0.00013939714699517936 BETTER
I0312 06:10:33.626590 1851750 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.00015241513028740883 old loss 0.00015314585471060127 BETTER
I0312 06:10:33.697851 1849409 finetune.py:68] layer 16_down @ epoch 2 new loss 0.00019575485202949494 old loss 0.0001957600616151467 BETTER
I0312 06:10:45.650162 1850983 finetune.py:45] layer 18_down initial loss 0.00023627071641385555
I0312 06:10:45.887212 1850205 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0002021029795287177 old loss 0.0002021060645347461 BETTER
I0312 06:11:01.710484 1851750 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0001518393837613985 old loss 0.00015241513028740883 BETTER
I0312 06:11:02.174795 1849409 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0001957518106792122 old loss 0.00019575485202949494 BETTER
I0312 06:11:11.608316 1850983 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0002362596133025363 old loss 0.00023627071641385555 BETTER
I0312 06:11:12.758431 1850205 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0002020988758886233 old loss 0.0002021029795287177 BETTER
I0312 06:11:29.829505 1851750 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.0001513906172476709 old loss 0.0001518393837613985 BETTER
I0312 06:11:30.601241 1849409 finetune.py:68] layer 16_down @ epoch 4 new loss 0.00019574935140553862 old loss 0.0001957518106792122 BETTER
16_v proxy err 0.0023586826864629984 tr(WHW.T) 780.7407836914062
16_q proxy err 0.00029176275711506605 tr(WHW.T) 7193.29736328125
16_k proxy err 0.0001978688669623807 tr(WHW.T) 11634.4716796875
16_o proxy err 0.002475983928889036 tr(WHW.T) 88.28684997558594
16_up proxy err 0.0013899627374485135 tr(WHW.T) 1891.0789794921875
16_gate proxy err 0.0007960639777593315 tr(WHW.T) 3371.15234375
16_down proxy err 0.0020319276954978704 tr(WHW.T) 152.17108154296875
I0312 06:11:38.859861 1850983 finetune.py:68] layer 18_down @ epoch 1 new loss 0.00023625313770025969 old loss 0.0002362596133025363 BETTER
I0312 06:11:40.109101 1850205 finetune.py:68] layer 17_down @ epoch 4 new loss 0.00020209689682815224 old loss 0.0002020988758886233 BETTER
17_v proxy err 0.002324154367670417 tr(WHW.T) 845.7654418945312
17_q proxy err 0.0003091422258876264 tr(WHW.T) 7164.34423828125
17_k proxy err 0.00022155509213916957 tr(WHW.T) 10700.84765625
17_o proxy err 0.00267940666526556 tr(WHW.T) 58.25092315673828
17_up proxy err 0.0015401222044602036 tr(WHW.T) 1921.1834716796875
17_gate proxy err 0.0008452584152109921 tr(WHW.T) 3572.462890625
17_down proxy err 0.0020591607317328453 tr(WHW.T) 165.5717010498047
I0312 06:11:45.720435 1851750 finetune.py:45] layer 19_down initial loss 0.00025767943589016795
I0312 06:12:05.632258 1850983 finetune.py:68] layer 18_down @ epoch 2 new loss 0.00023624836467206478 old loss 0.00023625313770025969 BETTER
I0312 06:12:11.112770 1851750 finetune.py:68] layer 19_down @ epoch 0 new loss 0.00025767122860997915 old loss 0.00025767943589016795 BETTER
I0312 06:12:32.485043 1850983 finetune.py:68] layer 18_down @ epoch 3 new loss 0.00023624500317964703 old loss 0.00023624836467206478 BETTER
I0312 06:12:37.447543 1851750 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0002576656697783619 old loss 0.00025767122860997915 BETTER
I0312 06:12:57.085014 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 20 in 72.65070939064026s
I0312 06:12:59.238191 1850983 finetune.py:68] layer 18_down @ epoch 4 new loss 0.00023624197638127953 old loss 0.00023624500317964703 BETTER
I0312 06:13:00.349382 1861485 config.py:54] PyTorch version 2.1.1 available.
18_v proxy err 0.002188424114137888 tr(WHW.T) 1003.7705078125
18_q proxy err 0.000325184955727309 tr(WHW.T) 7510.8232421875
18_k proxy err 0.00024393877538386732 tr(WHW.T) 10466.7919921875
18_o proxy err 0.0023035844787955284 tr(WHW.T) 70.06689453125
18_up proxy err 0.0016454819124192 tr(WHW.T) 2023.327392578125
18_gate proxy err 0.0008986812317743897 tr(WHW.T) 3782.968505859375
18_down proxy err 0.002023127395659685 tr(WHW.T) 198.73362731933594
I0312 06:13:01.396071 1799993 quantize_finetune_llama.py:183] layer 21 gpu 1
I0312 06:13:01.471810 1861485 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 06:13:03.741312 1851750 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0002576627302914858 old loss 0.0002576656697783619 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:13:09.861871 1861485 finetune.py:45] layer 20_v initial loss 3.33166572090704e-05
I0312 06:13:30.075436 1851750 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0002576592960394919 old loss 0.0002576627302914858 BETTER
I0312 06:13:42.925843 1861485 finetune.py:68] layer 20_v @ epoch 0 new loss 2.4662895157234743e-05 old loss 3.33166572090704e-05 BETTER
I0312 06:13:56.363653 1851750 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0002576569386292249 old loss 0.0002576592960394919 BETTER
19_v proxy err 0.0021498436108231544 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.0003477904247120023 tr(WHW.T) 6945.27197265625
19_k proxy err 0.00024405316798947752 tr(WHW.T) 10552.841796875
19_o proxy err 0.002309671835973859 tr(WHW.T) 62.34001541137695
19_up proxy err 0.0016589710721746087 tr(WHW.T) 2149.60546875
19_gate proxy err 0.0009842251893132925 tr(WHW.T) 3688.746337890625
19_down proxy err 0.001972334459424019 tr(WHW.T) 223.1276092529297
I0312 06:14:12.681412 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 21 in 69.31433033943176s
I0312 06:14:15.918275 1862284 config.py:54] PyTorch version 2.1.1 available.
I0312 06:14:16.978337 1799993 quantize_finetune_llama.py:183] layer 22 gpu 2
I0312 06:14:17.053371 1862284 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 06:14:17.267379 1861485 finetune.py:68] layer 20_v @ epoch 1 new loss 2.342373045394197e-05 old loss 2.4662895157234743e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:14:25.128365 1862284 finetune.py:45] layer 21_v initial loss 2.9211298169684596e-05
I0312 06:14:51.877501 1861485 finetune.py:68] layer 20_v @ epoch 2 new loss 2.292793033120688e-05 old loss 2.342373045394197e-05 BETTER
I0312 06:14:56.402988 1862284 finetune.py:68] layer 21_v @ epoch 0 new loss 2.2483227439806797e-05 old loss 2.9211298169684596e-05 BETTER
I0312 06:15:26.886697 1861485 finetune.py:68] layer 20_v @ epoch 3 new loss 2.243706512672361e-05 old loss 2.292793033120688e-05 BETTER
I0312 06:15:28.119460 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 22 in 70.77224397659302s
I0312 06:15:28.611208 1862284 finetune.py:68] layer 21_v @ epoch 1 new loss 2.1520227164728567e-05 old loss 2.2483227439806797e-05 BETTER
I0312 06:15:31.350775 1863081 config.py:54] PyTorch version 2.1.1 available.
I0312 06:15:32.382652 1799993 quantize_finetune_llama.py:183] layer 23 gpu 3
I0312 06:15:32.460227 1863081 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:15:40.848263 1863081 finetune.py:45] layer 22_v initial loss 3.607251710491255e-05
I0312 06:16:01.241677 1862284 finetune.py:68] layer 21_v @ epoch 2 new loss 2.105184830725193e-05 old loss 2.1520227164728567e-05 BETTER
I0312 06:16:02.208220 1861485 finetune.py:68] layer 20_v @ epoch 4 new loss 2.2047266611480154e-05 old loss 2.243706512672361e-05 BETTER
I0312 06:16:11.652863 1861485 finetune.py:45] layer 20_q initial loss 2.897692502301652e-05
I0312 06:16:12.526133 1863081 finetune.py:68] layer 22_v @ epoch 0 new loss 2.7856012820848264e-05 old loss 3.607251710491255e-05 BETTER
I0312 06:16:33.698198 1862284 finetune.py:68] layer 21_v @ epoch 3 new loss 2.0868312276434153e-05 old loss 2.105184830725193e-05 BETTER
I0312 06:16:44.218257 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 23 in 71.43650197982788s
I0312 06:16:45.021763 1863081 finetune.py:68] layer 22_v @ epoch 1 new loss 2.676751319086179e-05 old loss 2.7856012820848264e-05 BETTER
I0312 06:16:45.268402 1861485 finetune.py:68] layer 20_q @ epoch 0 new loss 2.670091635081917e-05 old loss 2.897692502301652e-05 BETTER
I0312 06:16:47.524967 1863873 config.py:54] PyTorch version 2.1.1 available.
I0312 06:16:48.531754 1799993 quantize_finetune_llama.py:183] layer 24 gpu 0
I0312 06:16:48.599304 1863873 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:16:56.874254 1863873 finetune.py:45] layer 23_v initial loss 3.6132314562564716e-05
I0312 06:17:06.328709 1862284 finetune.py:68] layer 21_v @ epoch 4 new loss 2.0565501472447067e-05 old loss 2.0868312276434153e-05 BETTER
I0312 06:17:15.936178 1862284 finetune.py:45] layer 21_q initial loss 2.6119922040379606e-05
I0312 06:17:18.171053 1863081 finetune.py:68] layer 22_v @ epoch 2 new loss 2.6384459488326684e-05 old loss 2.676751319086179e-05 BETTER
I0312 06:17:19.680539 1861485 finetune.py:68] layer 20_q @ epoch 1 new loss 2.602647327876184e-05 old loss 2.670091635081917e-05 BETTER
I0312 06:17:27.710932 1863873 finetune.py:68] layer 23_v @ epoch 0 new loss 2.823780414473731e-05 old loss 3.6132314562564716e-05 BETTER
I0312 06:17:47.573152 1862284 finetune.py:68] layer 21_q @ epoch 0 new loss 2.457176015013829e-05 old loss 2.6119922040379606e-05 BETTER
I0312 06:17:51.105503 1863081 finetune.py:68] layer 22_v @ epoch 3 new loss 2.5715484298416413e-05 old loss 2.6384459488326684e-05 BETTER
I0312 06:17:54.060114 1861485 finetune.py:68] layer 20_q @ epoch 2 new loss 2.5622521206969395e-05 old loss 2.602647327876184e-05 BETTER
I0312 06:17:59.639672 1863873 finetune.py:68] layer 23_v @ epoch 1 new loss 2.7066342227044515e-05 old loss 2.823780414473731e-05 BETTER
I0312 06:18:20.045638 1862284 finetune.py:68] layer 21_q @ epoch 1 new loss 2.4051068976405077e-05 old loss 2.457176015013829e-05 BETTER
I0312 06:18:24.234333 1863081 finetune.py:68] layer 22_v @ epoch 4 new loss 2.5324587113573216e-05 old loss 2.5715484298416413e-05 BETTER
I0312 06:18:28.542198 1861485 finetune.py:68] layer 20_q @ epoch 3 new loss 2.5298813852714375e-05 old loss 2.5622521206969395e-05 BETTER
I0312 06:18:31.793466 1863873 finetune.py:68] layer 23_v @ epoch 2 new loss 2.656197284522932e-05 old loss 2.7066342227044515e-05 BETTER
I0312 06:18:33.951151 1863081 finetune.py:45] layer 22_q initial loss 3.626436955528334e-05
I0312 06:18:52.436063 1862284 finetune.py:68] layer 21_q @ epoch 2 new loss 2.3734603018965572e-05 old loss 2.4051068976405077e-05 BETTER
I0312 06:19:03.104587 1861485 finetune.py:68] layer 20_q @ epoch 4 new loss 2.505713746359106e-05 old loss 2.5298813852714375e-05 BETTER
I0312 06:19:04.006022 1863873 finetune.py:68] layer 23_v @ epoch 3 new loss 2.6225427063764073e-05 old loss 2.656197284522932e-05 BETTER
I0312 06:19:05.563235 1863081 finetune.py:68] layer 22_q @ epoch 0 new loss 3.215470496797934e-05 old loss 3.626436955528334e-05 BETTER
I0312 06:19:12.746105 1861485 finetune.py:45] layer 20_k initial loss 3.0124312615953386e-05
I0312 06:19:24.825153 1862284 finetune.py:68] layer 21_q @ epoch 3 new loss 2.3495140339946374e-05 old loss 2.3734603018965572e-05 BETTER
I0312 06:19:36.295390 1863873 finetune.py:68] layer 23_v @ epoch 4 new loss 2.579459032858722e-05 old loss 2.6225427063764073e-05 BETTER
I0312 06:19:38.012593 1863081 finetune.py:68] layer 22_q @ epoch 1 new loss 3.122368798358366e-05 old loss 3.215470496797934e-05 BETTER
I0312 06:19:45.404292 1863873 finetune.py:45] layer 23_q initial loss 3.345546065247618e-05
I0312 06:19:45.517594 1861485 finetune.py:68] layer 20_k @ epoch 0 new loss 2.950328416773118e-05 old loss 3.0124312615953386e-05 BETTER
I0312 06:19:57.316065 1862284 finetune.py:68] layer 21_q @ epoch 4 new loss 2.330894858459942e-05 old loss 2.3495140339946374e-05 BETTER
I0312 06:20:06.431560 1862284 finetune.py:45] layer 21_k initial loss 2.831203346431721e-05
I0312 06:20:10.458514 1863081 finetune.py:68] layer 22_q @ epoch 2 new loss 3.067367651965469e-05 old loss 3.122368798358366e-05 BETTER
I0312 06:20:16.779248 1863873 finetune.py:68] layer 23_q @ epoch 0 new loss 3.137082967441529e-05 old loss 3.345546065247618e-05 BETTER
I0312 06:20:19.374443 1861485 finetune.py:68] layer 20_k @ epoch 1 new loss 2.9238010029075667e-05 old loss 2.950328416773118e-05 BETTER
I0312 06:20:37.863842 1862284 finetune.py:68] layer 21_k @ epoch 0 new loss 2.773526284727268e-05 old loss 2.831203346431721e-05 BETTER
I0312 06:20:42.973572 1863081 finetune.py:68] layer 22_q @ epoch 3 new loss 3.026993181265425e-05 old loss 3.067367651965469e-05 BETTER
I0312 06:20:48.956998 1863873 finetune.py:68] layer 23_q @ epoch 1 new loss 3.067095167352818e-05 old loss 3.137082967441529e-05 BETTER
I0312 06:20:53.124130 1861485 finetune.py:68] layer 20_k @ epoch 2 new loss 2.9082910259603523e-05 old loss 2.9238010029075667e-05 BETTER
I0312 06:21:09.951327 1862284 finetune.py:68] layer 21_k @ epoch 1 new loss 2.7542058887775056e-05 old loss 2.773526284727268e-05 BETTER
I0312 06:21:15.485671 1863081 finetune.py:68] layer 22_q @ epoch 4 new loss 2.9957862352603115e-05 old loss 3.026993181265425e-05 BETTER
I0312 06:21:21.068234 1863873 finetune.py:68] layer 23_q @ epoch 2 new loss 3.0263327062129974e-05 old loss 3.067095167352818e-05 BETTER
I0312 06:21:24.988008 1863081 finetune.py:45] layer 22_k initial loss 3.721768007380888e-05
I0312 06:21:27.204341 1861485 finetune.py:68] layer 20_k @ epoch 3 new loss 2.8993521482334472e-05 old loss 2.9082910259603523e-05 BETTER
I0312 06:21:41.964013 1862284 finetune.py:68] layer 21_k @ epoch 2 new loss 2.7495520043885335e-05 old loss 2.7542058887775056e-05 BETTER
I0312 06:21:53.198116 1863873 finetune.py:68] layer 23_q @ epoch 3 new loss 2.995935210492462e-05 old loss 3.0263327062129974e-05 BETTER
I0312 06:21:56.388991 1863081 finetune.py:68] layer 22_k @ epoch 0 new loss 3.646513869171031e-05 old loss 3.721768007380888e-05 BETTER
I0312 06:22:00.838376 1861485 finetune.py:68] layer 20_k @ epoch 4 new loss 2.8922442652401514e-05 old loss 2.8993521482334472e-05 BETTER
I0312 06:22:10.161317 1861485 finetune.py:45] layer 20_o initial loss 7.097685011103749e-05
I0312 06:22:14.013969 1862284 finetune.py:68] layer 21_k @ epoch 3 new loss 2.7406686058384366e-05 old loss 2.7495520043885335e-05 BETTER
I0312 06:22:25.298710 1863873 finetune.py:68] layer 23_q @ epoch 4 new loss 2.9727638320764527e-05 old loss 2.995935210492462e-05 BETTER
I0312 06:22:28.521062 1863081 finetune.py:68] layer 22_k @ epoch 1 new loss 3.616350659285672e-05 old loss 3.646513869171031e-05 BETTER
I0312 06:22:34.636825 1863873 finetune.py:45] layer 23_k initial loss 3.656548142316751e-05
I0312 06:22:42.687391 1861485 finetune.py:68] layer 20_o @ epoch 0 new loss 6.646336987614632e-05 old loss 7.097685011103749e-05 BETTER
I0312 06:22:46.043220 1862284 finetune.py:68] layer 21_k @ epoch 4 new loss 2.7357375074643642e-05 old loss 2.7406686058384366e-05 BETTER
I0312 06:22:55.299413 1862284 finetune.py:45] layer 21_o initial loss 6.287897849688306e-05
I0312 06:23:00.571468 1863081 finetune.py:68] layer 22_k @ epoch 2 new loss 3.5958677472081035e-05 old loss 3.616350659285672e-05 BETTER
I0312 06:23:05.392156 1863873 finetune.py:68] layer 23_k @ epoch 0 new loss 3.5981771361548454e-05 old loss 3.656548142316751e-05 BETTER
I0312 06:23:15.922835 1861485 finetune.py:68] layer 20_o @ epoch 1 new loss 6.463212048402056e-05 old loss 6.646336987614632e-05 BETTER
I0312 06:23:25.852052 1862284 finetune.py:68] layer 21_o @ epoch 0 new loss 6.009430217090994e-05 old loss 6.287897849688306e-05 BETTER
I0312 06:23:32.622057 1863081 finetune.py:68] layer 22_k @ epoch 3 new loss 3.579012263799086e-05 old loss 3.5958677472081035e-05 BETTER
I0312 06:23:36.945634 1863873 finetune.py:68] layer 23_k @ epoch 1 new loss 3.573250796762295e-05 old loss 3.5981771361548454e-05 BETTER
I0312 06:23:49.371852 1861485 finetune.py:68] layer 20_o @ epoch 2 new loss 6.349592877086252e-05 old loss 6.463212048402056e-05 BETTER
I0312 06:23:57.202783 1862284 finetune.py:68] layer 21_o @ epoch 1 new loss 5.914407302043401e-05 old loss 6.009430217090994e-05 BETTER
I0312 06:24:04.810795 1863081 finetune.py:68] layer 22_k @ epoch 4 new loss 3.570050103007816e-05 old loss 3.579012263799086e-05 BETTER
I0312 06:24:08.916011 1863873 finetune.py:68] layer 23_k @ epoch 2 new loss 3.559022297849879e-05 old loss 3.573250796762295e-05 BETTER
I0312 06:24:14.886549 1863081 finetune.py:45] layer 22_o initial loss 7.971655577421188e-05
I0312 06:24:22.749564 1861485 finetune.py:68] layer 20_o @ epoch 3 new loss 6.271578604355454e-05 old loss 6.349592877086252e-05 BETTER
I0312 06:24:28.692331 1862284 finetune.py:68] layer 21_o @ epoch 2 new loss 5.8564597566146404e-05 old loss 5.914407302043401e-05 BETTER
I0312 06:24:40.605255 1863873 finetune.py:68] layer 23_k @ epoch 3 new loss 3.552439375198446e-05 old loss 3.559022297849879e-05 BETTER
I0312 06:24:45.739127 1863081 finetune.py:68] layer 22_o @ epoch 0 new loss 7.571367314085364e-05 old loss 7.971655577421188e-05 BETTER
I0312 06:24:56.044656 1861485 finetune.py:68] layer 20_o @ epoch 4 new loss 6.212681910255924e-05 old loss 6.271578604355454e-05 BETTER
I0312 06:25:00.202221 1862284 finetune.py:68] layer 21_o @ epoch 3 new loss 5.8179793995805085e-05 old loss 5.8564597566146404e-05 BETTER
I0312 06:25:11.554615 1861485 finetune.py:45] layer 20_up initial loss 0.00013629597378894687
I0312 06:25:12.376475 1863873 finetune.py:68] layer 23_k @ epoch 4 new loss 3.53901632479392e-05 old loss 3.552439375198446e-05 BETTER
I0312 06:25:17.389285 1863081 finetune.py:68] layer 22_o @ epoch 1 new loss 7.424676732625812e-05 old loss 7.571367314085364e-05 BETTER
I0312 06:25:21.728430 1863873 finetune.py:45] layer 23_o initial loss 7.472658762708306e-05
I0312 06:25:31.734138 1862284 finetune.py:68] layer 21_o @ epoch 4 new loss 5.7923571148421615e-05 old loss 5.8179793995805085e-05 BETTER
I0312 06:25:42.175507 1861485 finetune.py:68] layer 20_up @ epoch 0 new loss 0.0001322004827670753 old loss 0.00013629597378894687 BETTER
I0312 06:25:46.683717 1862284 finetune.py:45] layer 21_up initial loss 0.0001391374389640987
I0312 06:25:49.190555 1863081 finetune.py:68] layer 22_o @ epoch 2 new loss 7.331751839956269e-05 old loss 7.424676732625812e-05 BETTER
I0312 06:25:51.823468 1863873 finetune.py:68] layer 23_o @ epoch 0 new loss 7.190617907326669e-05 old loss 7.472658762708306e-05 BETTER
I0312 06:26:13.867528 1861485 finetune.py:68] layer 20_up @ epoch 1 new loss 0.00013008611858822405 old loss 0.0001322004827670753 BETTER
I0312 06:26:15.478436 1862284 finetune.py:68] layer 21_up @ epoch 0 new loss 0.00013548499555326998 old loss 0.0001391374389640987 BETTER
I0312 06:26:20.967311 1863081 finetune.py:68] layer 22_o @ epoch 3 new loss 7.271796494023874e-05 old loss 7.331751839956269e-05 BETTER
I0312 06:26:22.693935 1863873 finetune.py:68] layer 23_o @ epoch 1 new loss 7.083697710186243e-05 old loss 7.190617907326669e-05 BETTER
I0312 06:26:45.067105 1862284 finetune.py:68] layer 21_up @ epoch 1 new loss 0.00013362195750232786 old loss 0.00013548499555326998 BETTER
I0312 06:26:45.570185 1861485 finetune.py:68] layer 20_up @ epoch 2 new loss 0.00012863404117524624 old loss 0.00013008611858822405 BETTER
I0312 06:26:52.683175 1863081 finetune.py:68] layer 22_o @ epoch 4 new loss 7.232343341456726e-05 old loss 7.271796494023874e-05 BETTER
I0312 06:26:53.643151 1863873 finetune.py:68] layer 23_o @ epoch 2 new loss 7.024862861726433e-05 old loss 7.083697710186243e-05 BETTER
I0312 06:27:07.651150 1863081 finetune.py:45] layer 22_up initial loss 0.0001651051570661366
I0312 06:27:14.642238 1862284 finetune.py:68] layer 21_up @ epoch 2 new loss 0.00013234352809377015 old loss 0.00013362195750232786 BETTER
I0312 06:27:17.256781 1861485 finetune.py:68] layer 20_up @ epoch 3 new loss 0.00012752966722473502 old loss 0.00012863404117524624 BETTER
I0312 06:27:24.491674 1863873 finetune.py:68] layer 23_o @ epoch 3 new loss 6.980906618991867e-05 old loss 7.024862861726433e-05 BETTER
I0312 06:27:36.774909 1863081 finetune.py:68] layer 22_up @ epoch 0 new loss 0.00016116529877763242 old loss 0.0001651051570661366 BETTER
I0312 06:27:44.310183 1862284 finetune.py:68] layer 21_up @ epoch 3 new loss 0.00013141070667188615 old loss 0.00013234352809377015 BETTER
I0312 06:27:48.970488 1861485 finetune.py:68] layer 20_up @ epoch 4 new loss 0.00012669080751948059 old loss 0.00012752966722473502 BETTER
I0312 06:27:55.264607 1863873 finetune.py:68] layer 23_o @ epoch 4 new loss 6.957646110095084e-05 old loss 6.980906618991867e-05 BETTER
I0312 06:28:03.982617 1861485 finetune.py:45] layer 20_gate initial loss 0.0001839830365497619
I0312 06:28:06.756025 1863081 finetune.py:68] layer 22_up @ epoch 1 new loss 0.00015913587412796915 old loss 0.00016116529877763242 BETTER
I0312 06:28:10.353914 1863873 finetune.py:45] layer 23_up initial loss 0.0001719749125186354
I0312 06:28:14.162287 1862284 finetune.py:68] layer 21_up @ epoch 4 new loss 0.00013067819236312062 old loss 0.00013141070667188615 BETTER
I0312 06:28:28.913436 1862284 finetune.py:45] layer 21_gate initial loss 0.00019324598542880267
I0312 06:28:33.024873 1861485 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.00018206093227490783 old loss 0.0001839830365497619 BETTER
I0312 06:28:36.668377 1863081 finetune.py:68] layer 22_up @ epoch 2 new loss 0.00015775694919284433 old loss 0.00015913587412796915 BETTER
I0312 06:28:38.786586 1863873 finetune.py:68] layer 23_up @ epoch 0 new loss 0.00016805779887363315 old loss 0.0001719749125186354 BETTER
I0312 06:28:56.468206 1862284 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.00019159349903929979 old loss 0.00019324598542880267 BETTER
I0312 06:29:02.794397 1861485 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0001809192035580054 old loss 0.00018206093227490783 BETTER
I0312 06:29:06.757516 1863081 finetune.py:68] layer 22_up @ epoch 3 new loss 0.00015673723828513175 old loss 0.00015775694919284433 BETTER
I0312 06:29:08.044724 1863873 finetune.py:68] layer 23_up @ epoch 1 new loss 0.00016596686327829957 old loss 0.00016805779887363315 BETTER
I0312 06:29:24.680390 1862284 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.00019063823856413364 old loss 0.00019159349903929979 BETTER
I0312 06:29:32.665680 1861485 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.00018007417384069413 old loss 0.0001809192035580054 BETTER
I0312 06:29:36.786373 1863081 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0001559880911372602 old loss 0.00015673723828513175 BETTER
I0312 06:29:37.594560 1863873 finetune.py:68] layer 23_up @ epoch 2 new loss 0.0001646194141358137 old loss 0.00016596686327829957 BETTER
I0312 06:29:51.942082 1863081 finetune.py:45] layer 22_gate initial loss 0.0002274810685776174
I0312 06:29:52.770441 1862284 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.00018992692639585584 old loss 0.00019063823856413364 BETTER
I0312 06:30:02.709496 1861485 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.00017944956198334694 old loss 0.00018007417384069413 BETTER
I0312 06:30:07.201714 1863873 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0001636034285183996 old loss 0.0001646194141358137 BETTER
I0312 06:30:19.304472 1863081 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0002256854495499283 old loss 0.0002274810685776174 BETTER
I0312 06:30:20.863260 1862284 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.00018941563030239195 old loss 0.00018992692639585584 BETTER
I0312 06:30:32.915688 1861485 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.00017894581833388656 old loss 0.00017944956198334694 BETTER
I0312 06:30:36.738016 1863873 finetune.py:68] layer 23_up @ epoch 4 new loss 0.00016286519530694932 old loss 0.0001636034285183996 BETTER
I0312 06:30:47.794157 1863081 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.00022464050562120974 old loss 0.0002256854495499283 BETTER
I0312 06:30:49.194255 1862284 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.00018901078146882355 old loss 0.00018941563030239195 BETTER
I0312 06:30:49.262589 1861485 finetune.py:45] layer 20_down initial loss 0.00030925829196348786
I0312 06:30:52.280222 1863873 finetune.py:45] layer 23_gate initial loss 0.00024366470461245626
I0312 06:31:04.983829 1862284 finetune.py:45] layer 21_down initial loss 0.0003235211770515889
I0312 06:31:16.196547 1863081 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.00022391021775547415 old loss 0.00022464050562120974 BETTER
I0312 06:31:16.713818 1861485 finetune.py:68] layer 20_down @ epoch 0 new loss 0.00030924196471460164 old loss 0.00030925829196348786 BETTER
I0312 06:31:19.296805 1863873 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0002419987431494519 old loss 0.00024366470461245626 BETTER
I0312 06:31:30.890838 1862284 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0003235124167986214 old loss 0.0003235211770515889 BETTER
I0312 06:31:44.681160 1863081 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.00022334120876621455 old loss 0.00022391021775547415 BETTER
I0312 06:31:45.278542 1861485 finetune.py:68] layer 20_down @ epoch 1 new loss 0.0003092321567237377 old loss 0.00030924196471460164 BETTER
I0312 06:31:47.575369 1863873 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.00024095007393043488 old loss 0.0002419987431494519 BETTER
I0312 06:31:57.954726 1862284 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0003235065669286996 old loss 0.0003235124167986214 BETTER
I0312 06:32:13.270886 1863081 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.00022297119721770287 old loss 0.00022334120876621455 BETTER
I0312 06:32:14.237439 1861485 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0003092247061431408 old loss 0.0003092321567237377 BETTER
I0312 06:32:15.730802 1863873 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.00024023449805099517 old loss 0.00024095007393043488 BETTER
I0312 06:32:24.624608 1862284 finetune.py:68] layer 21_down @ epoch 2 new loss 0.00032350237597711384 old loss 0.0003235065669286996 BETTER
I0312 06:32:29.445345 1863081 finetune.py:45] layer 22_down initial loss 0.00037550838897004724
I0312 06:32:43.296950 1861485 finetune.py:68] layer 20_down @ epoch 3 new loss 0.00030921894358471036 old loss 0.0003092247061431408 BETTER
I0312 06:32:44.289283 1863873 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.00023968399909790605 old loss 0.00024023449805099517 BETTER
I0312 06:32:51.619836 1862284 finetune.py:68] layer 21_down @ epoch 3 new loss 0.0003234984469600022 old loss 0.00032350237597711384 BETTER
I0312 06:32:55.723455 1863081 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0003755002107936889 old loss 0.00037550838897004724 BETTER
I0312 06:33:12.232198 1861485 finetune.py:68] layer 20_down @ epoch 4 new loss 0.00030921364668756723 old loss 0.00030921894358471036 BETTER
I0312 06:33:12.929107 1863873 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.00023931212490424514 old loss 0.00023968399909790605 BETTER
20_v proxy err 0.0022481349296867847 tr(WHW.T) 990.5983276367188
20_q proxy err 0.00034440282615832984 tr(WHW.T) 7151.2041015625
20_k proxy err 0.0002484189753886312 tr(WHW.T) 10389.80078125
20_o proxy err 0.0016616020584478974 tr(WHW.T) 100.43950653076172
20_up proxy err 0.001634205225855112 tr(WHW.T) 2341.1572265625
20_gate proxy err 0.0009681096998974681 tr(WHW.T) 4026.447998046875
20_down proxy err 0.0019468567334115505 tr(WHW.T) 275.11260986328125
I0312 06:33:19.126710 1862284 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0003234947507735342 old loss 0.0003234984469600022 BETTER
21_v proxy err 0.0021866443566977978 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.00038682870217598975 tr(WHW.T) 7065.49462890625
21_k proxy err 0.0002877137449104339 tr(WHW.T) 9980.759765625
21_o proxy err 0.0018815068760886788 tr(WHW.T) 75.58531188964844
21_up proxy err 0.001724909758195281 tr(WHW.T) 2361.548095703125
21_gate proxy err 0.0010361814638599753 tr(WHW.T) 4004.003173828125
21_down proxy err 0.001987938303500414 tr(WHW.T) 276.825927734375
I0312 06:33:22.958648 1863081 finetune.py:68] layer 22_down @ epoch 1 new loss 0.00037549377884715796 old loss 0.0003755002107936889 BETTER
I0312 06:33:29.511732 1863873 finetune.py:45] layer 23_down initial loss 0.00039629096863791347
I0312 06:33:49.975041 1863081 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0003754897625185549 old loss 0.00037549377884715796 BETTER
I0312 06:33:54.714109 1863873 finetune.py:68] layer 23_down @ epoch 0 new loss 0.00039628223748877645 old loss 0.00039629096863791347 BETTER
I0312 06:34:16.981727 1863081 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0003754834469873458 old loss 0.0003754897625185549 BETTER
I0312 06:34:20.845508 1863873 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0003962751943618059 old loss 0.00039628223748877645 BETTER
I0312 06:34:37.461402 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 24 in 73.54903507232666s
I0312 06:34:40.817666 1873571 config.py:54] PyTorch version 2.1.1 available.
I0312 06:34:41.912786 1799993 quantize_finetune_llama.py:183] layer 25 gpu 1
I0312 06:34:41.998015 1873571 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 06:34:43.992910 1863081 finetune.py:68] layer 22_down @ epoch 4 new loss 0.00037548269028775394 old loss 0.0003754834469873458 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
22_v proxy err 0.0020726516377180815 tr(WHW.T) 1243.2529296875
22_q proxy err 0.0003655694890767336 tr(WHW.T) 7747.82861328125
22_k proxy err 0.0002789534046314657 tr(WHW.T) 10606.734375
22_o proxy err 0.001510132453404367 tr(WHW.T) 114.41606903076172
22_up proxy err 0.0017400163924321532 tr(WHW.T) 2474.6328125
22_gate proxy err 0.0010565078118816018 tr(WHW.T) 4156.837890625
22_down proxy err 0.001986483344808221 tr(WHW.T) 312.1453857421875
I0312 06:34:47.191654 1863873 finetune.py:68] layer 23_down @ epoch 2 new loss 0.000396271439967677 old loss 0.0003962751943618059 BETTER
I0312 06:34:50.682227 1873571 finetune.py:45] layer 24_v initial loss 4.097633791388944e-05
I0312 06:35:13.631606 1863873 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0003962694900110364 old loss 0.000396271439967677 BETTER
I0312 06:35:23.704263 1873571 finetune.py:68] layer 24_v @ epoch 0 new loss 3.193085649400018e-05 old loss 4.097633791388944e-05 BETTER
I0312 06:35:39.945186 1863873 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0003962659975513816 old loss 0.0003962694900110364 BETTER
23_v proxy err 0.0019405949860811234 tr(WHW.T) 1486.037353515625
23_q proxy err 0.0004237212997395545 tr(WHW.T) 7347.2255859375
23_k proxy err 0.0003210051800124347 tr(WHW.T) 9986.3837890625
23_o proxy err 0.0018059619469568133 tr(WHW.T) 85.2193374633789
23_up proxy err 0.0018006993923336267 tr(WHW.T) 2533.68115234375
23_gate proxy err 0.0011324103688821197 tr(WHW.T) 4097.7177734375
23_down proxy err 0.00198515341617167 tr(WHW.T) 321.6361999511719
I0312 06:35:58.019277 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 25 in 69.79241299629211s
I0312 06:35:58.150435 1873571 finetune.py:68] layer 24_v @ epoch 1 new loss 3.05730827676598e-05 old loss 3.193085649400018e-05 BETTER
I0312 06:36:01.007969 1874403 config.py:54] PyTorch version 2.1.1 available.
I0312 06:36:01.972591 1799993 quantize_finetune_llama.py:183] layer 26 gpu 2
I0312 06:36:02.039923 1874403 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:36:10.208616 1874403 finetune.py:45] layer 25_v initial loss 3.77944452338852e-05
I0312 06:36:32.910494 1873571 finetune.py:68] layer 24_v @ epoch 2 new loss 3.0168610464897938e-05 old loss 3.05730827676598e-05 BETTER
I0312 06:36:41.369103 1874403 finetune.py:68] layer 25_v @ epoch 0 new loss 2.816127016558312e-05 old loss 3.77944452338852e-05 BETTER
I0312 06:37:07.677983 1873571 finetune.py:68] layer 24_v @ epoch 3 new loss 2.954002957267221e-05 old loss 3.0168610464897938e-05 BETTER
I0312 06:37:12.457274 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 26 in 70.04896759986877s
I0312 06:37:13.525179 1874403 finetune.py:68] layer 25_v @ epoch 1 new loss 2.71620065177558e-05 old loss 2.816127016558312e-05 BETTER
I0312 06:37:15.656409 1875192 config.py:54] PyTorch version 2.1.1 available.
I0312 06:37:16.642260 1799993 quantize_finetune_llama.py:183] layer 27 gpu 3
I0312 06:37:16.716468 1875192 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:37:26.469202 1875192 finetune.py:45] layer 26_v initial loss 5.7926736189983785e-05
I0312 06:37:42.454638 1873571 finetune.py:68] layer 24_v @ epoch 4 new loss 2.915661025326699e-05 old loss 2.954002957267221e-05 BETTER
I0312 06:37:46.084472 1874403 finetune.py:68] layer 25_v @ epoch 2 new loss 2.687838968995493e-05 old loss 2.71620065177558e-05 BETTER
I0312 06:37:51.923708 1873571 finetune.py:45] layer 24_q initial loss 3.868472049362026e-05
I0312 06:37:57.929483 1875192 finetune.py:68] layer 26_v @ epoch 0 new loss 4.419099059305154e-05 old loss 5.7926736189983785e-05 BETTER
I0312 06:38:18.805941 1874403 finetune.py:68] layer 25_v @ epoch 3 new loss 2.6443027309142053e-05 old loss 2.687838968995493e-05 BETTER
I0312 06:38:25.531958 1873571 finetune.py:68] layer 24_q @ epoch 0 new loss 3.610673229559325e-05 old loss 3.868472049362026e-05 BETTER
I0312 06:38:28.846452 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 27 in 71.7819254398346s
I0312 06:38:30.200576 1875192 finetune.py:68] layer 26_v @ epoch 1 new loss 4.242042632540688e-05 old loss 4.419099059305154e-05 BETTER
I0312 06:38:32.140379 1875993 config.py:54] PyTorch version 2.1.1 available.
I0312 06:38:33.174759 1799993 quantize_finetune_llama.py:183] layer 28 gpu 0
I0312 06:38:33.240650 1875993 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:38:41.393801 1875993 finetune.py:45] layer 27_v initial loss 4.857002568314783e-05
I0312 06:38:51.286943 1874403 finetune.py:68] layer 25_v @ epoch 4 new loss 2.6200403226539493e-05 old loss 2.6443027309142053e-05 BETTER
I0312 06:38:59.937328 1873571 finetune.py:68] layer 24_q @ epoch 1 new loss 3.544809442246333e-05 old loss 3.610673229559325e-05 BETTER
I0312 06:39:00.437841 1874403 finetune.py:45] layer 25_q initial loss 3.496430508675985e-05
I0312 06:39:02.832974 1875192 finetune.py:68] layer 26_v @ epoch 2 new loss 4.1480805521132424e-05 old loss 4.242042632540688e-05 BETTER
I0312 06:39:12.315630 1875993 finetune.py:68] layer 27_v @ epoch 0 new loss 3.880279473378323e-05 old loss 4.857002568314783e-05 BETTER
I0312 06:39:32.006438 1874403 finetune.py:68] layer 25_q @ epoch 0 new loss 3.2761399779701605e-05 old loss 3.496430508675985e-05 BETTER
I0312 06:39:34.557827 1873571 finetune.py:68] layer 24_q @ epoch 2 new loss 3.499908416415565e-05 old loss 3.544809442246333e-05 BETTER
I0312 06:39:35.586553 1875192 finetune.py:68] layer 26_v @ epoch 3 new loss 4.089477806701325e-05 old loss 4.1480805521132424e-05 BETTER
I0312 06:39:44.239810 1875993 finetune.py:68] layer 27_v @ epoch 1 new loss 3.7627491110470146e-05 old loss 3.880279473378323e-05 BETTER
I0312 06:40:04.302326 1874403 finetune.py:68] layer 25_q @ epoch 1 new loss 3.2096671930048615e-05 old loss 3.2761399779701605e-05 BETTER
I0312 06:40:08.673192 1875192 finetune.py:68] layer 26_v @ epoch 4 new loss 4.0597442421130836e-05 old loss 4.089477806701325e-05 BETTER
I0312 06:40:09.241247 1873571 finetune.py:68] layer 24_q @ epoch 3 new loss 3.466688940534368e-05 old loss 3.499908416415565e-05 BETTER
I0312 06:40:16.493997 1875993 finetune.py:68] layer 27_v @ epoch 2 new loss 3.721453686011955e-05 old loss 3.7627491110470146e-05 BETTER
I0312 06:40:17.889097 1875192 finetune.py:45] layer 26_q initial loss 5.337504262570292e-05
I0312 06:40:36.672605 1874403 finetune.py:68] layer 25_q @ epoch 2 new loss 3.1661322282161564e-05 old loss 3.2096671930048615e-05 BETTER
I0312 06:40:43.755445 1873571 finetune.py:68] layer 24_q @ epoch 4 new loss 3.447054768912494e-05 old loss 3.466688940534368e-05 BETTER
I0312 06:40:49.043131 1875993 finetune.py:68] layer 27_v @ epoch 3 new loss 3.675972402561456e-05 old loss 3.721453686011955e-05 BETTER
I0312 06:40:49.614717 1875192 finetune.py:68] layer 26_q @ epoch 0 new loss 5.021521428716369e-05 old loss 5.337504262570292e-05 BETTER
I0312 06:40:54.766353 1873571 finetune.py:45] layer 24_k initial loss 4.200169860268943e-05
I0312 06:41:08.843284 1874403 finetune.py:68] layer 25_q @ epoch 3 new loss 3.14324970531743e-05 old loss 3.1661322282161564e-05 BETTER
I0312 06:41:21.790621 1875993 finetune.py:68] layer 27_v @ epoch 4 new loss 3.620695133577101e-05 old loss 3.675972402561456e-05 BETTER
I0312 06:41:22.310747 1875192 finetune.py:68] layer 26_q @ epoch 1 new loss 4.917249680147506e-05 old loss 5.021521428716369e-05 BETTER
I0312 06:41:27.928614 1873571 finetune.py:68] layer 24_k @ epoch 0 new loss 4.1265313484473154e-05 old loss 4.200169860268943e-05 BETTER
I0312 06:41:31.773507 1875993 finetune.py:45] layer 27_q initial loss 4.872351564699784e-05
I0312 06:41:41.372495 1874403 finetune.py:68] layer 25_q @ epoch 4 new loss 3.130095137748867e-05 old loss 3.14324970531743e-05 BETTER
I0312 06:41:50.557724 1874403 finetune.py:45] layer 25_k initial loss 3.907300560967997e-05
I0312 06:41:54.640769 1875192 finetune.py:68] layer 26_q @ epoch 2 new loss 4.8476216761628166e-05 old loss 4.917249680147506e-05 BETTER
I0312 06:42:01.578980 1873571 finetune.py:68] layer 24_k @ epoch 1 new loss 4.1018221963895485e-05 old loss 4.1265313484473154e-05 BETTER
I0312 06:42:03.217784 1875993 finetune.py:68] layer 27_q @ epoch 0 new loss 4.535935295280069e-05 old loss 4.872351564699784e-05 BETTER
I0312 06:42:22.002385 1874403 finetune.py:68] layer 25_k @ epoch 0 new loss 3.828561239060946e-05 old loss 3.907300560967997e-05 BETTER
I0312 06:42:27.190803 1875192 finetune.py:68] layer 26_q @ epoch 3 new loss 4.7916528274072334e-05 old loss 4.8476216761628166e-05 BETTER
I0312 06:42:35.411202 1875993 finetune.py:68] layer 27_q @ epoch 1 new loss 4.442971112439409e-05 old loss 4.535935295280069e-05 BETTER
I0312 06:42:35.412415 1873571 finetune.py:68] layer 24_k @ epoch 2 new loss 4.086200351594016e-05 old loss 4.1018221963895485e-05 BETTER
I0312 06:42:54.125947 1874403 finetune.py:68] layer 25_k @ epoch 1 new loss 3.8083395338617265e-05 old loss 3.828561239060946e-05 BETTER
I0312 06:42:59.780866 1875192 finetune.py:68] layer 26_q @ epoch 4 new loss 4.756714406539686e-05 old loss 4.7916528274072334e-05 BETTER
I0312 06:43:08.252127 1875993 finetune.py:68] layer 27_q @ epoch 2 new loss 4.3836600525537506e-05 old loss 4.442971112439409e-05 BETTER
I0312 06:43:09.608207 1873571 finetune.py:68] layer 24_k @ epoch 3 new loss 4.0775583329377696e-05 old loss 4.086200351594016e-05 BETTER
I0312 06:43:09.857916 1875192 finetune.py:45] layer 26_k initial loss 5.851360037922859e-05
I0312 06:43:26.192402 1874403 finetune.py:68] layer 25_k @ epoch 2 new loss 3.803094296017662e-05 old loss 3.8083395338617265e-05 BETTER
I0312 06:43:40.735994 1875993 finetune.py:68] layer 27_q @ epoch 3 new loss 4.318108767620288e-05 old loss 4.3836600525537506e-05 BETTER
I0312 06:43:41.483789 1875192 finetune.py:68] layer 26_k @ epoch 0 new loss 5.7546611060388386e-05 old loss 5.851360037922859e-05 BETTER
I0312 06:43:43.743004 1873571 finetune.py:68] layer 24_k @ epoch 4 new loss 4.072173032909632e-05 old loss 4.0775583329377696e-05 BETTER
I0312 06:43:53.251159 1873571 finetune.py:45] layer 24_o initial loss 8.898362284526229e-05
I0312 06:43:58.289935 1874403 finetune.py:68] layer 25_k @ epoch 3 new loss 3.7898000300629064e-05 old loss 3.803094296017662e-05 BETTER
I0312 06:44:12.942526 1875993 finetune.py:68] layer 27_q @ epoch 4 new loss 4.284167880541645e-05 old loss 4.318108767620288e-05 BETTER
I0312 06:44:13.758120 1875192 finetune.py:68] layer 26_k @ epoch 1 new loss 5.720136687159538e-05 old loss 5.7546611060388386e-05 BETTER
I0312 06:44:22.946333 1875993 finetune.py:45] layer 27_k initial loss 5.3171839681454e-05
I0312 06:44:25.937612 1873571 finetune.py:68] layer 24_o @ epoch 0 new loss 8.55396210681647e-05 old loss 8.898362284526229e-05 BETTER
I0312 06:44:30.398789 1874403 finetune.py:68] layer 25_k @ epoch 4 new loss 3.7889793020440266e-05 old loss 3.7898000300629064e-05 BETTER
I0312 06:44:39.734054 1874403 finetune.py:45] layer 25_o initial loss 7.60034381528385e-05
I0312 06:44:45.868618 1875192 finetune.py:68] layer 26_k @ epoch 2 new loss 5.701756526832469e-05 old loss 5.720136687159538e-05 BETTER
I0312 06:44:53.813468 1875993 finetune.py:68] layer 27_k @ epoch 0 new loss 5.2319872338557616e-05 old loss 5.3171839681454e-05 BETTER
I0312 06:44:59.461333 1873571 finetune.py:68] layer 24_o @ epoch 1 new loss 8.444445847999305e-05 old loss 8.55396210681647e-05 BETTER
I0312 06:45:10.247724 1874403 finetune.py:68] layer 25_o @ epoch 0 new loss 7.387813820969313e-05 old loss 7.60034381528385e-05 BETTER
I0312 06:45:17.935557 1875192 finetune.py:68] layer 26_k @ epoch 3 new loss 5.68708564969711e-05 old loss 5.701756526832469e-05 BETTER
I0312 06:45:25.379976 1875993 finetune.py:68] layer 27_k @ epoch 1 new loss 5.1983144658152014e-05 old loss 5.2319872338557616e-05 BETTER
I0312 06:45:32.981719 1873571 finetune.py:68] layer 24_o @ epoch 2 new loss 8.383725071325898e-05 old loss 8.444445847999305e-05 BETTER
I0312 06:45:41.539029 1874403 finetune.py:68] layer 25_o @ epoch 1 new loss 7.306993938982487e-05 old loss 7.387813820969313e-05 BETTER
I0312 06:45:50.426125 1875192 finetune.py:68] layer 26_k @ epoch 4 new loss 5.680473623215221e-05 old loss 5.68708564969711e-05 BETTER
I0312 06:45:57.314241 1875993 finetune.py:68] layer 27_k @ epoch 2 new loss 5.180212974664755e-05 old loss 5.1983144658152014e-05 BETTER
I0312 06:46:00.104189 1875192 finetune.py:45] layer 26_o initial loss 0.00011555589298950508
I0312 06:46:06.355214 1873571 finetune.py:68] layer 24_o @ epoch 3 new loss 8.34955571917817e-05 old loss 8.383725071325898e-05 BETTER
I0312 06:46:13.087952 1874403 finetune.py:68] layer 25_o @ epoch 2 new loss 7.26706421119161e-05 old loss 7.306993938982487e-05 BETTER
I0312 06:46:29.105268 1875993 finetune.py:68] layer 27_k @ epoch 3 new loss 5.1719973271247e-05 old loss 5.180212974664755e-05 BETTER
I0312 06:46:30.962237 1875192 finetune.py:68] layer 26_o @ epoch 0 new loss 0.00011001419625245035 old loss 0.00011555589298950508 BETTER
I0312 06:46:39.675058 1873571 finetune.py:68] layer 24_o @ epoch 4 new loss 8.326739771291614e-05 old loss 8.34955571917817e-05 BETTER
I0312 06:46:44.597966 1874403 finetune.py:68] layer 25_o @ epoch 3 new loss 7.24419514881447e-05 old loss 7.26706421119161e-05 BETTER
I0312 06:46:55.286136 1873571 finetune.py:45] layer 24_up initial loss 0.000194006206584163
I0312 06:47:01.091308 1875993 finetune.py:68] layer 27_k @ epoch 4 new loss 5.1540988351916894e-05 old loss 5.1719973271247e-05 BETTER
I0312 06:47:02.645200 1875192 finetune.py:68] layer 26_o @ epoch 1 new loss 0.00010846880468307063 old loss 0.00011001419625245035 BETTER
I0312 06:47:10.490554 1875993 finetune.py:45] layer 27_o initial loss 0.0001017863251036033
I0312 06:47:15.969594 1874403 finetune.py:68] layer 25_o @ epoch 4 new loss 7.228786125779152e-05 old loss 7.24419514881447e-05 BETTER
I0312 06:47:25.927051 1873571 finetune.py:68] layer 24_up @ epoch 0 new loss 0.00019018900638911873 old loss 0.000194006206584163 BETTER
I0312 06:47:31.021102 1874403 finetune.py:45] layer 25_up initial loss 0.00019624951528385282
I0312 06:47:34.257319 1875192 finetune.py:68] layer 26_o @ epoch 2 new loss 0.0001076149710570462 old loss 0.00010846880468307063 BETTER
I0312 06:47:40.659924 1875993 finetune.py:68] layer 27_o @ epoch 0 new loss 9.728284931043163e-05 old loss 0.0001017863251036033 BETTER
I0312 06:47:57.580886 1873571 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0001882683573057875 old loss 0.00019018900638911873 BETTER
I0312 06:47:59.895272 1874403 finetune.py:68] layer 25_up @ epoch 0 new loss 0.00019187614088878036 old loss 0.00019624951528385282 BETTER
I0312 06:48:06.024179 1875192 finetune.py:68] layer 26_o @ epoch 3 new loss 0.00010710072820074856 old loss 0.0001076149710570462 BETTER
I0312 06:48:11.637167 1875993 finetune.py:68] layer 27_o @ epoch 1 new loss 9.570557449478656e-05 old loss 9.728284931043163e-05 BETTER
I0312 06:48:29.525734 1873571 finetune.py:68] layer 24_up @ epoch 2 new loss 0.00018698109488468617 old loss 0.0001882683573057875 BETTER
I0312 06:48:29.846818 1874403 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0001898279006127268 old loss 0.00019187614088878036 BETTER
I0312 06:48:38.614406 1875192 finetune.py:68] layer 26_o @ epoch 4 new loss 0.00010669166658772156 old loss 0.00010710072820074856 BETTER
I0312 06:48:42.706522 1875993 finetune.py:68] layer 27_o @ epoch 2 new loss 9.488077921560034e-05 old loss 9.570557449478656e-05 BETTER
I0312 06:48:54.129358 1875192 finetune.py:45] layer 26_up initial loss 0.00024221856438089162
I0312 06:48:59.861171 1874403 finetune.py:68] layer 25_up @ epoch 2 new loss 0.00018840798293240368 old loss 0.0001898279006127268 BETTER
I0312 06:49:01.408732 1873571 finetune.py:68] layer 24_up @ epoch 3 new loss 0.00018604793876875192 old loss 0.00018698109488468617 BETTER
I0312 06:49:13.982169 1875993 finetune.py:68] layer 27_o @ epoch 3 new loss 9.436537220608443e-05 old loss 9.488077921560034e-05 BETTER
I0312 06:49:23.760466 1875192 finetune.py:68] layer 26_up @ epoch 0 new loss 0.00023732749104965478 old loss 0.00024221856438089162 BETTER
I0312 06:49:29.723953 1874403 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0001874139124993235 old loss 0.00018840798293240368 BETTER
I0312 06:49:33.228863 1873571 finetune.py:68] layer 24_up @ epoch 4 new loss 0.00018536722927819937 old loss 0.00018604793876875192 BETTER
I0312 06:49:45.222973 1875993 finetune.py:68] layer 27_o @ epoch 4 new loss 9.402977593708783e-05 old loss 9.436537220608443e-05 BETTER
I0312 06:49:48.638356 1873571 finetune.py:45] layer 24_gate initial loss 0.0002752648142632097
I0312 06:49:53.503406 1875192 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0002350062713958323 old loss 0.00023732749104965478 BETTER
I0312 06:49:59.478764 1874403 finetune.py:68] layer 25_up @ epoch 4 new loss 0.00018665328389033675 old loss 0.0001874139124993235 BETTER
I0312 06:50:00.608065 1875993 finetune.py:45] layer 27_up initial loss 0.00024819246027618647
I0312 06:50:14.511045 1874403 finetune.py:45] layer 25_gate initial loss 0.00028704520082101226
I0312 06:50:17.695436 1873571 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.00027368724113330245 old loss 0.0002752648142632097 BETTER
I0312 06:50:23.583695 1875192 finetune.py:68] layer 26_up @ epoch 2 new loss 0.00023344824148807675 old loss 0.0002350062713958323 BETTER
I0312 06:50:29.265142 1875993 finetune.py:68] layer 27_up @ epoch 0 new loss 0.00024158411542885005 old loss 0.00024819246027618647 BETTER
I0312 06:50:42.104522 1874403 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.00028535284218378365 old loss 0.00028704520082101226 BETTER
I0312 06:50:47.522248 1873571 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0002726658130995929 old loss 0.00027368724113330245 BETTER
I0312 06:50:53.773896 1875192 finetune.py:68] layer 26_up @ epoch 3 new loss 0.00023234798572957516 old loss 0.00023344824148807675 BETTER
I0312 06:50:58.678106 1875993 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0002385392872383818 old loss 0.00024158411542885005 BETTER
I0312 06:51:10.315835 1874403 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.0002843259717337787 old loss 0.00028535284218378365 BETTER
I0312 06:51:17.440255 1873571 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.000271971890470013 old loss 0.0002726658130995929 BETTER
I0312 06:51:23.745418 1875192 finetune.py:68] layer 26_up @ epoch 4 new loss 0.00023150345077738166 old loss 0.00023234798572957516 BETTER
I0312 06:51:28.133367 1875993 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0002365835098316893 old loss 0.0002385392872383818 BETTER
I0312 06:51:38.347005 1874403 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.00028361144359223545 old loss 0.0002843259717337787 BETTER
I0312 06:51:39.157812 1875192 finetune.py:45] layer 26_gate initial loss 0.0003448711067903787
I0312 06:51:47.346825 1873571 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0002715019800234586 old loss 0.000271971890470013 BETTER
I0312 06:51:57.621729 1875993 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0002352180308662355 old loss 0.0002365835098316893 BETTER
I0312 06:52:06.370847 1875192 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.00034287996822968125 old loss 0.0003448711067903787 BETTER
I0312 06:52:06.500039 1874403 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.0002831123711075634 old loss 0.00028361144359223545 BETTER
I0312 06:52:17.380971 1873571 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.00027111367671750486 old loss 0.0002715019800234586 BETTER
I0312 06:52:27.197925 1875993 finetune.py:68] layer 27_up @ epoch 4 new loss 0.00023423728998750448 old loss 0.0002352180308662355 BETTER
I0312 06:52:33.473524 1873571 finetune.py:45] layer 24_down initial loss 0.00043662870302796364
I0312 06:52:34.602108 1875192 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.0003416823747102171 old loss 0.00034287996822968125 BETTER
I0312 06:52:34.750079 1874403 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0002827417047228664 old loss 0.0002831123711075634 BETTER
I0312 06:52:42.492026 1875993 finetune.py:45] layer 27_gate initial loss 0.0003648309502750635
I0312 06:52:50.420903 1874403 finetune.py:45] layer 25_down initial loss 0.00045691640116274357
I0312 06:53:00.985063 1873571 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0004366203211247921 old loss 0.00043662870302796364 BETTER
I0312 06:53:02.946656 1875192 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.00034088006941601634 old loss 0.0003416823747102171 BETTER
I0312 06:53:09.714033 1875993 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.00036215296131558716 old loss 0.0003648309502750635 BETTER
I0312 06:53:16.336420 1874403 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0004568842123262584 old loss 0.00045691640116274357 BETTER
I0312 06:53:29.420752 1873571 finetune.py:68] layer 24_down @ epoch 1 new loss 0.00043661409290507436 old loss 0.0004366203211247921 BETTER
I0312 06:53:31.186649 1875192 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.00034028812660835683 old loss 0.00034088006941601634 BETTER
I0312 06:53:37.869670 1875993 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.00036068030749447644 old loss 0.00036215296131558716 BETTER
I0312 06:53:43.268362 1874403 finetune.py:68] layer 25_down @ epoch 1 new loss 0.00045687350211665034 old loss 0.0004568842123262584 BETTER
I0312 06:53:57.952603 1873571 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0004366085340734571 old loss 0.00043661409290507436 BETTER
I0312 06:53:59.598139 1875192 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.00033986775088123977 old loss 0.00034028812660835683 BETTER
I0312 06:54:06.039848 1875993 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.00035968568408861756 old loss 0.00036068030749447644 BETTER
I0312 06:54:10.200975 1874403 finetune.py:68] layer 25_down @ epoch 2 new loss 0.00045686642988584936 old loss 0.00045687350211665034 BETTER
I0312 06:54:15.266313 1875192 finetune.py:45] layer 26_down initial loss 0.0005315496819093823
I0312 06:54:26.464554 1873571 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0004366053326521069 old loss 0.0004366085340734571 BETTER
I0312 06:54:34.243832 1875993 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.00035893701715394855 old loss 0.00035968568408861756 BETTER
I0312 06:54:36.933848 1874403 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0004568621807266027 old loss 0.00045686642988584936 BETTER
I0312 06:54:41.584196 1875192 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0005315322196111083 old loss 0.0005315496819093823 BETTER
I0312 06:54:54.975900 1873571 finetune.py:68] layer 24_down @ epoch 4 new loss 0.00043660090886987746 old loss 0.0004366053326521069 BETTER
24_v proxy err 0.0020083084236830473 tr(WHW.T) 1394.900634765625
24_q proxy err 0.00042966814362443984 tr(WHW.T) 7020.93505859375
24_k proxy err 0.0003066755598410964 tr(WHW.T) 10329.9931640625
24_o proxy err 0.0014140928396955132 tr(WHW.T) 134.0361785888672
24_up proxy err 0.0018294118344783783 tr(WHW.T) 2621.93212890625
24_gate proxy err 0.0011437018401920795 tr(WHW.T) 4263.62841796875
24_down proxy err 0.001969006145372987 tr(WHW.T) 340.5069274902344
I0312 06:55:02.890629 1875993 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.00035842772922478616 old loss 0.00035893701715394855 BETTER
I0312 06:55:04.204238 1874403 finetune.py:68] layer 25_down @ epoch 4 new loss 0.00045685702934861183 old loss 0.0004568621807266027 BETTER
25_v proxy err 0.0018962903413921595 tr(WHW.T) 1707.664794921875
25_q proxy err 0.0004799586022272706 tr(WHW.T) 7162.4150390625
25_k proxy err 0.00036836156505160034 tr(WHW.T) 9615.9677734375
25_o proxy err 0.0017699699383229017 tr(WHW.T) 83.61992645263672
25_up proxy err 0.001811482128687203 tr(WHW.T) 2805.7041015625
25_gate proxy err 0.0011081446427851915 tr(WHW.T) 4666.3173828125
25_down proxy err 0.0018918350106105208 tr(WHW.T) 373.82904052734375
I0312 06:55:08.835623 1875192 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0005315193557180464 old loss 0.0005315322196111083 BETTER
I0312 06:55:18.192363 1875993 finetune.py:45] layer 27_down initial loss 0.000577204511500895
I0312 06:55:35.527937 1875192 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0005315104499459267 old loss 0.0005315193557180464 BETTER
I0312 06:55:43.625528 1875993 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0005771808791905642 old loss 0.000577204511500895 BETTER
I0312 06:56:02.479709 1875192 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0005315015441738069 old loss 0.0005315104499459267 BETTER
I0312 06:56:09.886576 1875993 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0005771613796241581 old loss 0.0005771808791905642 BETTER
I0312 06:56:21.330066 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 28 in 72.55656743049622s
I0312 06:56:24.556808 1884582 config.py:54] PyTorch version 2.1.1 available.
I0312 06:56:25.596769 1799993 quantize_finetune_llama.py:183] layer 29 gpu 1
I0312 06:56:25.666937 1884582 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:56:29.490317 1875192 finetune.py:68] layer 26_down @ epoch 4 new loss 0.0005314943846315145 old loss 0.0005315015441738069 BETTER
26_v proxy err 0.0018524146871641278 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.00044496223563328385 tr(WHW.T) 7470.3583984375
26_k proxy err 0.0003281514218542725 tr(WHW.T) 10491.5283203125
26_o proxy err 0.001127755269408226 tr(WHW.T) 203.00711059570312
26_up proxy err 0.0017009566072374582 tr(WHW.T) 3154.961181640625
26_gate proxy err 0.0010308601194992661 tr(WHW.T) 5304.400390625
26_down proxy err 0.0019389023073017597 tr(WHW.T) 401.6397705078125
I0312 06:56:34.576826 1884582 finetune.py:45] layer 28_v initial loss 7.848340464988723e-05
I0312 06:56:36.114374 1875993 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0005771490396000445 old loss 0.0005771613796241581 BETTER
I0312 06:57:02.348420 1875993 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0005771393189206719 old loss 0.0005771490396000445 BETTER
I0312 06:57:07.762365 1884582 finetune.py:68] layer 28_v @ epoch 0 new loss 5.0509308493928984e-05 old loss 7.848340464988723e-05 BETTER
I0312 06:57:28.982678 1875993 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0005771304131485522 old loss 0.0005771393189206719 BETTER
27_v proxy err 0.001778135891072452 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.0004397634183987975 tr(WHW.T) 7691.76123046875
27_k proxy err 0.000327624729834497 tr(WHW.T) 10623.0625
27_o proxy err 0.001496119424700737 tr(WHW.T) 126.31448364257812
27_up proxy err 0.0015458763809874654 tr(WHW.T) 3691.54736328125
27_gate proxy err 0.0009699000511318445 tr(WHW.T) 5990.6962890625
27_down proxy err 0.0019101935904473066 tr(WHW.T) 467.4091491699219
I0312 06:57:42.356116 1884582 finetune.py:68] layer 28_v @ epoch 1 new loss 4.836877997149713e-05 old loss 5.0509308493928984e-05 BETTER
I0312 06:57:44.454460 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 29 in 70.4683485031128s
I0312 06:57:47.516784 1884833 config.py:54] PyTorch version 2.1.1 available.
I0312 06:57:48.513835 1799993 quantize_finetune_llama.py:183] layer 30 gpu 2
I0312 06:57:48.588614 1884833 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:57:56.657305 1884833 finetune.py:45] layer 29_v initial loss 7.888986874604598e-05
I0312 06:58:17.107996 1884582 finetune.py:68] layer 28_v @ epoch 2 new loss 4.7627843741793185e-05 old loss 4.836877997149713e-05 BETTER
I0312 06:58:27.953609 1884833 finetune.py:68] layer 29_v @ epoch 0 new loss 5.6525303079979494e-05 old loss 7.888986874604598e-05 BETTER
I0312 06:58:52.014585 1884582 finetune.py:68] layer 28_v @ epoch 3 new loss 4.689198613050394e-05 old loss 4.7627843741793185e-05 BETTER
I0312 06:58:59.368445 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 30 in 70.4617760181427s
I0312 06:59:00.346076 1884833 finetune.py:68] layer 29_v @ epoch 1 new loss 5.423553011496551e-05 old loss 5.6525303079979494e-05 BETTER
I0312 06:59:02.432485 1885019 config.py:54] PyTorch version 2.1.1 available.
I0312 06:59:03.443114 1799993 quantize_finetune_llama.py:183] layer 31 gpu 3
I0312 06:59:03.507671 1885019 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 06:59:11.869917 1885019 finetune.py:45] layer 30_v initial loss 8.092686766758561e-05
I0312 06:59:27.051683 1884582 finetune.py:68] layer 28_v @ epoch 4 new loss 4.5926157326903194e-05 old loss 4.689198613050394e-05 BETTER
I0312 06:59:32.819548 1884833 finetune.py:68] layer 29_v @ epoch 2 new loss 5.3602510888595134e-05 old loss 5.423553011496551e-05 BETTER
I0312 06:59:36.690752 1884582 finetune.py:45] layer 28_q initial loss 6.143568316474557e-05
I0312 06:59:43.384581 1885019 finetune.py:68] layer 30_v @ epoch 0 new loss 5.4334173910319805e-05 old loss 8.092686766758561e-05 BETTER
I0312 07:00:05.710638 1884833 finetune.py:68] layer 29_v @ epoch 3 new loss 5.3428095270646736e-05 old loss 5.3602510888595134e-05 BETTER
I0312 07:00:10.432533 1884582 finetune.py:68] layer 28_q @ epoch 0 new loss 5.77164682908915e-05 old loss 6.143568316474557e-05 BETTER
I0312 07:00:15.086106 1799993 quantize_finetune_llama.py:210] computed original embedding for layer 31 in 71.24141097068787s
I0312 07:00:16.099457 1885019 finetune.py:68] layer 30_v @ epoch 1 new loss 5.18337546964176e-05 old loss 5.4334173910319805e-05 BETTER
I0312 07:00:18.383486 1885165 config.py:54] PyTorch version 2.1.1 available.
I0312 07:00:19.473012 1885165 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 07:00:27.334884 1885165 finetune.py:45] layer 31_v initial loss 0.00012895521649625152
I0312 07:00:38.620614 1884833 finetune.py:68] layer 29_v @ epoch 4 new loss 5.2496961870929226e-05 old loss 5.3428095270646736e-05 BETTER
I0312 07:00:44.907066 1884582 finetune.py:68] layer 28_q @ epoch 1 new loss 5.644297561957501e-05 old loss 5.77164682908915e-05 BETTER
I0312 07:00:47.893290 1884833 finetune.py:45] layer 29_q initial loss 6.640003994107246e-05
I0312 07:00:49.008774 1885019 finetune.py:76] layer 30_v @ epoch 2 new loss 5.3217958338791505e-05 old loss 5.18337546964176e-05 WORSE
I0312 07:00:58.408947 1885165 finetune.py:68] layer 31_v @ epoch 0 new loss 9.210290590999648e-05 old loss 0.00012895521649625152 BETTER
I0312 07:01:19.763657 1884833 finetune.py:68] layer 29_q @ epoch 0 new loss 6.215750181581825e-05 old loss 6.640003994107246e-05 BETTER
I0312 07:01:19.792016 1884582 finetune.py:68] layer 28_q @ epoch 2 new loss 5.575535033131018e-05 old loss 5.644297561957501e-05 BETTER
I0312 07:01:21.597352 1885019 finetune.py:68] layer 30_v @ epoch 3 new loss 5.0918049964820966e-05 old loss 5.18337546964176e-05 BETTER
I0312 07:01:30.465406 1885165 finetune.py:76] layer 31_v @ epoch 1 new loss 0.00010237483365926892 old loss 9.210290590999648e-05 WORSE
I0312 07:01:52.355248 1884833 finetune.py:68] layer 29_q @ epoch 1 new loss 6.113126437412575e-05 old loss 6.215750181581825e-05 BETTER
I0312 07:01:54.398388 1884582 finetune.py:68] layer 28_q @ epoch 3 new loss 5.529106783797033e-05 old loss 5.575535033131018e-05 BETTER
I0312 07:01:54.909676 1885019 finetune.py:76] layer 30_v @ epoch 4 new loss 5.180881998967379e-05 old loss 5.0918049964820966e-05 WORSE
I0312 07:02:02.445837 1885165 finetune.py:76] layer 31_v @ epoch 2 new loss 0.0001011109707178548 old loss 9.210290590999648e-05 WORSE
I0312 07:02:03.846708 1885019 finetune.py:45] layer 30_q initial loss 6.942102481843904e-05
I0312 07:02:25.045338 1884833 finetune.py:68] layer 29_q @ epoch 2 new loss 6.04267515882384e-05 old loss 6.113126437412575e-05 BETTER
I0312 07:02:29.249313 1884582 finetune.py:68] layer 28_q @ epoch 4 new loss 5.4713222198188305e-05 old loss 5.529106783797033e-05 BETTER
I0312 07:02:34.488284 1885165 finetune.py:76] layer 31_v @ epoch 3 new loss 0.00011277474550297484 old loss 9.210290590999648e-05 WORSE
I0312 07:02:35.516788 1885019 finetune.py:68] layer 30_q @ epoch 0 new loss 6.45252366666682e-05 old loss 6.942102481843904e-05 BETTER
I0312 07:02:39.199886 1884582 finetune.py:45] layer 28_k initial loss 6.809490878367797e-05
I0312 07:02:57.598178 1884833 finetune.py:68] layer 29_q @ epoch 3 new loss 5.9904559748247266e-05 old loss 6.04267515882384e-05 BETTER
I0312 07:03:06.892379 1885165 finetune.py:76] layer 31_v @ epoch 4 new loss 9.738199878484011e-05 old loss 9.210290590999648e-05 WORSE
I0312 07:03:08.708305 1885019 finetune.py:68] layer 30_q @ epoch 1 new loss 6.226697587408125e-05 old loss 6.45252366666682e-05 BETTER
I0312 07:03:12.273325 1884582 finetune.py:68] layer 28_k @ epoch 0 new loss 6.703607505187392e-05 old loss 6.809490878367797e-05 BETTER
I0312 07:03:15.765532 1885165 finetune.py:45] layer 31_q initial loss 0.0001638340763747692
I0312 07:03:30.369448 1884833 finetune.py:68] layer 29_q @ epoch 4 new loss 5.938662070548162e-05 old loss 5.9904559748247266e-05 BETTER
I0312 07:03:39.564932 1884833 finetune.py:45] layer 29_k initial loss 7.217242091428488e-05
I0312 07:03:41.578645 1885019 finetune.py:68] layer 30_q @ epoch 2 new loss 6.133745773695409e-05 old loss 6.226697587408125e-05 BETTER
I0312 07:03:46.529925 1884582 finetune.py:68] layer 28_k @ epoch 1 new loss 6.673233292531222e-05 old loss 6.703607505187392e-05 BETTER
I0312 07:03:47.207268 1885165 finetune.py:68] layer 31_q @ epoch 0 new loss 0.00013919519551564008 old loss 0.0001638340763747692 BETTER
I0312 07:04:11.074671 1884833 finetune.py:68] layer 29_k @ epoch 0 new loss 7.129879668354988e-05 old loss 7.217242091428488e-05 BETTER
I0312 07:04:14.448076 1885019 finetune.py:68] layer 30_q @ epoch 3 new loss 6.074977500247769e-05 old loss 6.133745773695409e-05 BETTER
I0312 07:04:19.285556 1885165 finetune.py:68] layer 31_q @ epoch 1 new loss 0.00013630531611852348 old loss 0.00013919519551564008 BETTER
I0312 07:04:20.835087 1884582 finetune.py:68] layer 28_k @ epoch 2 new loss 6.645136454608291e-05 old loss 6.673233292531222e-05 BETTER
I0312 07:04:45.413898 1884833 finetune.py:68] layer 29_k @ epoch 1 new loss 7.089840073604137e-05 old loss 7.129879668354988e-05 BETTER
I0312 07:04:48.490543 1885019 finetune.py:68] layer 30_q @ epoch 4 new loss 6.024821777828038e-05 old loss 6.074977500247769e-05 BETTER
I0312 07:04:51.970984 1885165 finetune.py:68] layer 31_q @ epoch 2 new loss 0.00013095120084472 old loss 0.00013630531611852348 BETTER
I0312 07:04:55.321480 1884582 finetune.py:68] layer 28_k @ epoch 3 new loss 6.6371176217217e-05 old loss 6.645136454608291e-05 BETTER
I0312 07:04:59.113765 1885019 finetune.py:45] layer 30_k initial loss 7.573180482722819e-05
I0312 07:05:18.831197 1884833 finetune.py:68] layer 29_k @ epoch 2 new loss 7.071596337482333e-05 old loss 7.089840073604137e-05 BETTER
I0312 07:05:25.449523 1885165 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0001254428643733263 old loss 0.00013095120084472 BETTER
I0312 07:05:30.418901 1884582 finetune.py:68] layer 28_k @ epoch 4 new loss 6.62977690808475e-05 old loss 6.6371176217217e-05 BETTER
I0312 07:05:31.362436 1885019 finetune.py:68] layer 30_k @ epoch 0 new loss 7.424550130963326e-05 old loss 7.573180482722819e-05 BETTER
I0312 07:05:40.777976 1884582 finetune.py:45] layer 28_o initial loss 0.0001278753043152392
I0312 07:05:51.272552 1884833 finetune.py:68] layer 29_k @ epoch 3 new loss 7.064341480145231e-05 old loss 7.071596337482333e-05 BETTER
I0312 07:05:58.833190 1885165 finetune.py:76] layer 31_q @ epoch 4 new loss 0.00013060796482022852 old loss 0.0001254428643733263 WORSE
I0312 07:06:04.363839 1885019 finetune.py:68] layer 30_k @ epoch 1 new loss 7.373569678748026e-05 old loss 7.424550130963326e-05 BETTER
I0312 07:06:10.079552 1885165 finetune.py:45] layer 31_k initial loss 0.000161551361088641
I0312 07:06:13.896612 1884582 finetune.py:68] layer 28_o @ epoch 0 new loss 0.00012218930351082236 old loss 0.0001278753043152392 BETTER
I0312 07:06:23.453055 1884833 finetune.py:68] layer 29_k @ epoch 4 new loss 7.063447992550209e-05 old loss 7.064341480145231e-05 BETTER
I0312 07:06:32.557463 1884833 finetune.py:45] layer 29_o initial loss 0.00013236980885267258
I0312 07:06:36.783635 1885019 finetune.py:76] layer 30_k @ epoch 2 new loss 7.395013381028548e-05 old loss 7.373569678748026e-05 WORSE
I0312 07:06:40.826114 1885165 finetune.py:68] layer 31_k @ epoch 0 new loss 0.00014404692046809942 old loss 0.000161551361088641 BETTER
I0312 07:06:47.128101 1884582 finetune.py:68] layer 28_o @ epoch 1 new loss 0.0001201988197863102 old loss 0.00012218930351082236 BETTER
I0312 07:07:03.322261 1884833 finetune.py:68] layer 29_o @ epoch 0 new loss 0.00012653089652303606 old loss 0.00013236980885267258 BETTER
I0312 07:07:08.192602 1885019 finetune.py:68] layer 30_k @ epoch 3 new loss 7.3484297899995e-05 old loss 7.373569678748026e-05 BETTER
I0312 07:07:12.344580 1885165 finetune.py:68] layer 31_k @ epoch 1 new loss 0.00013831528485752642 old loss 0.00014404692046809942 BETTER
I0312 07:07:20.621069 1884582 finetune.py:68] layer 28_o @ epoch 2 new loss 0.00011906240979442373 old loss 0.0001201988197863102 BETTER
I0312 07:07:35.071148 1884833 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0001248809858225286 old loss 0.00012653089652303606 BETTER
I0312 07:07:40.383403 1885019 finetune.py:68] layer 30_k @ epoch 4 new loss 7.347577775362879e-05 old loss 7.3484297899995e-05 BETTER
I0312 07:07:43.993653 1885165 finetune.py:68] layer 31_k @ epoch 2 new loss 0.00013742623559664935 old loss 0.00013831528485752642 BETTER
I0312 07:07:49.665749 1885019 finetune.py:45] layer 30_o initial loss 0.00014742620987817645
I0312 07:07:53.885071 1884582 finetune.py:68] layer 28_o @ epoch 3 new loss 0.00011834780889330432 old loss 0.00011906240979442373 BETTER
I0312 07:08:06.381212 1884833 finetune.py:68] layer 29_o @ epoch 2 new loss 0.00012402827269397676 old loss 0.0001248809858225286 BETTER
I0312 07:08:15.584448 1885165 finetune.py:68] layer 31_k @ epoch 3 new loss 0.00013739718997385353 old loss 0.00013742623559664935 BETTER
I0312 07:08:20.511466 1885019 finetune.py:68] layer 30_o @ epoch 0 new loss 0.00013612397015094757 old loss 0.00014742620987817645 BETTER
I0312 07:08:27.236669 1884582 finetune.py:68] layer 28_o @ epoch 4 new loss 0.00011789387644967064 old loss 0.00011834780889330432 BETTER
I0312 07:08:37.832316 1884833 finetune.py:68] layer 29_o @ epoch 3 new loss 0.00012353794591035694 old loss 0.00012402827269397676 BETTER
I0312 07:08:42.253581 1884582 finetune.py:45] layer 28_up initial loss 0.00029798646573908627
I0312 07:08:47.174695 1885165 finetune.py:76] layer 31_k @ epoch 4 new loss 0.00013981567462906241 old loss 0.00013739718997385353 WORSE
I0312 07:08:51.963021 1885019 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0001328970247413963 old loss 0.00013612397015094757 BETTER
I0312 07:08:55.887795 1885165 finetune.py:45] layer 31_o initial loss 0.00024669565027579665
I0312 07:09:09.363013 1884833 finetune.py:68] layer 29_o @ epoch 4 new loss 0.00012339258682914078 old loss 0.00012353794591035694 BETTER
I0312 07:09:12.862717 1884582 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0002887573791667819 old loss 0.00029798646573908627 BETTER
I0312 07:09:23.613558 1885019 finetune.py:68] layer 30_o @ epoch 2 new loss 0.00013146462151780725 old loss 0.0001328970247413963 BETTER
I0312 07:09:24.373883 1884833 finetune.py:45] layer 29_up initial loss 0.000336806639097631
I0312 07:09:26.146947 1885165 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0002146964834537357 old loss 0.00024669565027579665 BETTER
I0312 07:09:44.414822 1884582 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0002849376469384879 old loss 0.0002887573791667819 BETTER
I0312 07:09:53.188231 1884833 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0003223848179914057 old loss 0.000336806639097631 BETTER
I0312 07:09:55.302478 1885019 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0001302986202063039 old loss 0.00013146462151780725 BETTER
I0312 07:09:57.074975 1885165 finetune.py:68] layer 31_o @ epoch 1 new loss 0.00020621431758627295 old loss 0.0002146964834537357 BETTER
I0312 07:10:16.073872 1884582 finetune.py:68] layer 28_up @ epoch 2 new loss 0.00028244679560884833 old loss 0.0002849376469384879 BETTER
I0312 07:10:23.140043 1884833 finetune.py:68] layer 29_up @ epoch 1 new loss 0.00031727133318781853 old loss 0.0003223848179914057 BETTER
I0312 07:10:27.012258 1885019 finetune.py:76] layer 30_o @ epoch 4 new loss 0.00013043977378401905 old loss 0.0001302986202063039 WORSE
I0312 07:10:28.004496 1885165 finetune.py:68] layer 31_o @ epoch 2 new loss 0.000202718932996504 old loss 0.00020621431758627295 BETTER
I0312 07:10:41.404627 1885019 finetune.py:45] layer 30_up initial loss 0.00047661460121162236
I0312 07:10:47.746000 1884582 finetune.py:68] layer 28_up @ epoch 3 new loss 0.00028068304527550936 old loss 0.00028244679560884833 BETTER
I0312 07:10:52.999151 1884833 finetune.py:68] layer 29_up @ epoch 2 new loss 0.000314071134198457 old loss 0.00031727133318781853 BETTER
I0312 07:10:59.096970 1885165 finetune.py:68] layer 31_o @ epoch 3 new loss 0.00020103745919186622 old loss 0.000202718932996504 BETTER
I0312 07:11:10.378124 1885019 finetune.py:68] layer 30_up @ epoch 0 new loss 0.00043273012852296233 old loss 0.00047661460121162236 BETTER
I0312 07:11:19.535233 1884582 finetune.py:68] layer 28_up @ epoch 4 new loss 0.0002793656603898853 old loss 0.00028068304527550936 BETTER
I0312 07:11:22.962873 1884833 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0003118609020020813 old loss 0.000314071134198457 BETTER
I0312 07:11:30.162442 1885165 finetune.py:68] layer 31_o @ epoch 4 new loss 0.00020012754248455167 old loss 0.00020103745919186622 BETTER
I0312 07:11:34.675535 1884582 finetune.py:45] layer 28_gate initial loss 0.00043519685277715325
I0312 07:11:40.349599 1885019 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0004188338352832943 old loss 0.00043273012852296233 BETTER
I0312 07:11:44.993841 1885165 finetune.py:45] layer 31_up initial loss 0.0011117091635242105
I0312 07:11:53.053003 1884833 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0003102209884673357 old loss 0.0003118609020020813 BETTER
I0312 07:12:03.712195 1884582 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.00043146597454324365 old loss 0.00043519685277715325 BETTER
I0312 07:12:08.083109 1884833 finetune.py:45] layer 29_gate initial loss 0.000495938234962523
I0312 07:12:10.440375 1885019 finetune.py:68] layer 30_up @ epoch 2 new loss 0.00041012917063198984 old loss 0.0004188338352832943 BETTER
I0312 07:12:13.463506 1885165 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0008575409883633256 old loss 0.0011117091635242105 BETTER
I0312 07:12:33.579408 1884582 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0004294706741347909 old loss 0.00043146597454324365 BETTER
I0312 07:12:35.678137 1884833 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0004900592612102628 old loss 0.000495938234962523 BETTER
I0312 07:12:40.471735 1885019 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0004038272600155324 old loss 0.00041012917063198984 BETTER
I0312 07:12:42.931856 1885165 finetune.py:68] layer 31_up @ epoch 1 new loss 0.0008018948137760162 old loss 0.0008575409883633256 BETTER
I0312 07:13:03.692831 1884582 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.00042807654244825244 old loss 0.0004294706741347909 BETTER
I0312 07:13:03.869619 1884833 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.00048736087046563625 old loss 0.0004900592612102628 BETTER
I0312 07:13:10.422553 1885019 finetune.py:68] layer 30_up @ epoch 4 new loss 0.00039894177461974323 old loss 0.0004038272600155324 BETTER
I0312 07:13:12.372959 1885165 finetune.py:68] layer 31_up @ epoch 2 new loss 0.0007637684466317296 old loss 0.0008018948137760162 BETTER
I0312 07:13:25.514187 1885019 finetune.py:45] layer 30_gate initial loss 0.0006329211755655706
I0312 07:13:31.741352 1884833 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0004855516890529543 old loss 0.00048736087046563625 BETTER
I0312 07:13:33.725395 1884582 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0004271392244845629 old loss 0.00042807654244825244 BETTER
I0312 07:13:41.817838 1885165 finetune.py:68] layer 31_up @ epoch 3 new loss 0.000734737201128155 old loss 0.0007637684466317296 BETTER
I0312 07:13:53.036543 1885019 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0006156422314234078 old loss 0.0006329211755655706 BETTER
I0312 07:13:59.794456 1884833 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.00048423881526105106 old loss 0.0004855516890529543 BETTER
I0312 07:14:03.657204 1884582 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.00042638485319912434 old loss 0.0004271392244845629 BETTER
I0312 07:14:11.273985 1885165 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0007116753840819001 old loss 0.000734737201128155 BETTER
I0312 07:14:19.346965 1884582 finetune.py:45] layer 28_down initial loss 0.0006947885849513113
I0312 07:14:21.080755 1885019 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.0006094821146689355 old loss 0.0006156422314234078 BETTER
I0312 07:14:26.264493 1885165 finetune.py:45] layer 31_gate initial loss 0.0011021585669368505
I0312 07:14:27.805133 1884833 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.000483308540424332 old loss 0.00048423881526105106 BETTER
I0312 07:14:43.016263 1884833 finetune.py:45] layer 29_down initial loss 0.0008223106851801276
I0312 07:14:47.019414 1884582 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0006947441725060344 old loss 0.0006947885849513113 BETTER
I0312 07:14:49.072307 1885019 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0006054181139916182 old loss 0.0006094821146689355 BETTER
I0312 07:14:53.378306 1885165 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.001031145453453064 old loss 0.0011021585669368505 BETTER
I0312 07:15:08.931162 1884833 finetune.py:68] layer 29_down @ epoch 0 new loss 0.0008222114411182702 old loss 0.0008223106851801276 BETTER
I0312 07:15:15.529220 1884582 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0006947120418772101 old loss 0.0006947441725060344 BETTER
I0312 07:15:17.483302 1885019 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.0006024535396136343 old loss 0.0006054181139916182 BETTER
I0312 07:15:21.474787 1885165 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.001011595712043345 old loss 0.001031145453453064 BETTER
I0312 07:15:35.643141 1884833 finetune.py:68] layer 29_down @ epoch 1 new loss 0.000822142930701375 old loss 0.0008222114411182702 BETTER
I0312 07:15:44.027317 1884582 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0006946822977624834 old loss 0.0006947120418772101 BETTER
I0312 07:15:45.995438 1885019 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0006000041030347347 old loss 0.0006024535396136343 BETTER
I0312 07:15:49.606204 1885165 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.0009978041052818298 old loss 0.001011595712043345 BETTER
I0312 07:16:01.787909 1885019 finetune.py:45] layer 30_down initial loss 0.0013571527088060975
I0312 07:16:02.316124 1884833 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0008220848976634443 old loss 0.000822142930701375 BETTER
I0312 07:16:12.646390 1884582 finetune.py:68] layer 28_down @ epoch 3 new loss 0.0006946579087525606 old loss 0.0006946822977624834 BETTER
I0312 07:16:17.698024 1885165 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.0009866963373497128 old loss 0.0009978041052818298 BETTER
I0312 07:16:27.866383 1885019 finetune.py:68] layer 30_down @ epoch 0 new loss 0.0013523971429094672 old loss 0.0013571527088060975 BETTER
I0312 07:16:29.266415 1884833 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0008220326853916049 old loss 0.0008220848976634443 BETTER
I0312 07:16:41.134069 1884582 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0006946375360712409 old loss 0.0006946579087525606 BETTER
28_v proxy err 0.0016445319633930922 tr(WHW.T) 2018.944091796875
28_q proxy err 0.00045542707084678113 tr(WHW.T) 7651.22119140625
28_k proxy err 0.0003384031297173351 tr(WHW.T) 10548.4267578125
28_o proxy err 0.0012246883707121015 tr(WHW.T) 194.99520874023438
28_up proxy err 0.0012878449633717537 tr(WHW.T) 4660.904296875
28_gate proxy err 0.0009301294339820743 tr(WHW.T) 6546.318359375
28_down proxy err 0.0018324515549466014 tr(WHW.T) 604.4044799804688
I0312 07:16:45.819427 1885165 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.0009769948665052652 old loss 0.0009866963373497128 BETTER
I0312 07:16:54.612868 1885019 finetune.py:68] layer 30_down @ epoch 1 new loss 0.0013495699968189 old loss 0.0013523971429094672 BETTER
I0312 07:16:56.218929 1884833 finetune.py:68] layer 29_down @ epoch 4 new loss 0.000821990252006799 old loss 0.0008220326853916049 BETTER
29_v proxy err 0.001726276008412242 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.00045397147187031806 tr(WHW.T) 7227.0556640625
29_k proxy err 0.0003208750276826322 tr(WHW.T) 10563.7333984375
29_o proxy err 0.0011051447363570333 tr(WHW.T) 207.9915008544922
29_up proxy err 0.0010258465772494674 tr(WHW.T) 6071.140625
29_gate proxy err 0.0008541429997421801 tr(WHW.T) 7368.46826171875
29_down proxy err 0.0017824032111093402 tr(WHW.T) 783.3584594726562
I0312 07:17:01.298435 1885165 finetune.py:45] layer 31_down initial loss 0.0027798323426395655
I0312 07:17:21.365531 1885019 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0013474588049575686 old loss 0.0013495699968189 BETTER
I0312 07:17:26.660239 1885165 finetune.py:68] layer 31_down @ epoch 0 new loss 0.0027678385376930237 old loss 0.0027798323426395655 BETTER
I0312 07:17:48.214804 1885019 finetune.py:68] layer 30_down @ epoch 3 new loss 0.0013461221242323518 old loss 0.0013474588049575686 BETTER
I0312 07:17:52.806291 1885165 finetune.py:68] layer 31_down @ epoch 1 new loss 0.0027640315238386393 old loss 0.0027678385376930237 BETTER
I0312 07:18:15.028083 1885019 finetune.py:68] layer 30_down @ epoch 4 new loss 0.001345174154266715 old loss 0.0013461221242323518 BETTER
30_v proxy err 0.001473506330512464 tr(WHW.T) 2261.489501953125
30_q proxy err 0.00044670948409475386 tr(WHW.T) 7816.0439453125
30_k proxy err 0.0003397319815121591 tr(WHW.T) 10527.703125
30_o proxy err 0.001071929931640625 tr(WHW.T) 252.13819885253906
30_up proxy err 0.0006347824819386005 tr(WHW.T) 10016.1875
30_gate proxy err 0.0005849117296747863 tr(WHW.T) 10999.1435546875
30_down proxy err 0.0008713577990420163 tr(WHW.T) 3587.7841796875
I0312 07:18:19.050853 1885165 finetune.py:68] layer 31_down @ epoch 2 new loss 0.0027620471082627773 old loss 0.0027640315238386393 BETTER
I0312 07:18:45.370696 1885165 finetune.py:68] layer 31_down @ epoch 3 new loss 0.002760668285191059 old loss 0.0027620471082627773 BETTER
I0312 07:19:11.862310 1885165 finetune.py:68] layer 31_down @ epoch 4 new loss 0.002759542316198349 old loss 0.002760668285191059 BETTER
31_v proxy err 0.0017824657261371613 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.00035778756137005985 tr(WHW.T) 6858.77587890625
31_k proxy err 0.0002482726704329252 tr(WHW.T) 10241.9130859375
31_o proxy err 0.0007675384404137731 tr(WHW.T) 458.1565856933594
31_up proxy err 0.0003685872070491314 tr(WHW.T) 14560.3486328125
31_gate proxy err 0.0003664381511043757 tr(WHW.T) 14828.8984375
31_down proxy err 0.00042641928303055465 tr(WHW.T) 17896.748046875
