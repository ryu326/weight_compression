I0311 21:00:24.041340 1752459 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.34it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 10.30it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 10.09it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.28it/s]
I0311 21:00:25.827191 1752459 quantize_finetune_llama.py:134] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:23,  1.30it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:21,  1.40it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:20,  1.43it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:19,  1.42it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:18,  1.43it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:18,  1.43it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:17,  1.42it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:16,  1.42it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:15,  1.45it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:06<00:14,  1.47it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:07<00:14,  1.49it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:13,  1.49it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:08<00:12,  1.48it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:09<00:12,  1.47it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:10<00:11,  1.46it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:10,  1.46it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:11<00:10,  1.46it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:12<00:09,  1.46it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:09,  1.42it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:13<00:08,  1.42it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:14<00:07,  1.42it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:07,  1.42it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:06,  1.39it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:16<00:05,  1.39it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:05,  1.38it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.37it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:18<00:03,  1.37it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:19<00:02,  1.37it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:02,  1.36it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.37it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:21<00:00,  1.36it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.37it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.42it/s]
I0311 21:00:58.949746 1752459 quantize_finetune_llama.py:159] loaded compression model
I0311 21:01:13.824121 1752459 quantize_finetune_llama.py:163] loaded dataset and devset
I0311 21:01:19.236541 1752459 quantize_finetune_llama.py:183] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:02:38.382921 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 0 in 79.00473999977112s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0311 21:03:03.988791 1752573 config.py:54] PyTorch version 2.1.1 available.
I0311 21:03:05.086607 1752459 quantize_finetune_llama.py:183] layer 1 gpu 1
I0311 21:03:05.149053 1752573 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:03:13.456361 1752573 finetune.py:45] layer 0_v initial loss 7.280202680703951e-07
I0311 21:03:44.930351 1752573 finetune.py:68] layer 0_v @ epoch 0 new loss 2.862888095478411e-07 old loss 7.280202680703951e-07 BETTER
I0311 21:04:14.106843 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 1 in 68.83641409873962s
I0311 21:04:21.540970 1752573 finetune.py:68] layer 0_v @ epoch 1 new loss 1.398241380456966e-07 old loss 2.862888095478411e-07 BETTER
I0311 21:04:25.486774 1752691 config.py:54] PyTorch version 2.1.1 available.
I0311 21:04:26.469175 1752459 quantize_finetune_llama.py:183] layer 2 gpu 2
I0311 21:04:26.533527 1752691 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:04:34.504501 1752691 finetune.py:45] layer 1_v initial loss 1.6506433894392103e-05
I0311 21:04:54.874006 1752573 finetune.py:68] layer 0_v @ epoch 2 new loss 8.655376859678654e-08 old loss 1.398241380456966e-07 BETTER
I0311 21:05:05.436205 1752691 finetune.py:68] layer 1_v @ epoch 0 new loss 6.69264363750699e-06 old loss 1.6506433894392103e-05 BETTER
I0311 21:05:28.703823 1752573 finetune.py:68] layer 0_v @ epoch 3 new loss 6.510823169492141e-08 old loss 8.655376859678654e-08 BETTER
I0311 21:05:36.625819 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 2 in 69.99340200424194s
I0311 21:05:37.827960 1752691 finetune.py:68] layer 1_v @ epoch 1 new loss 5.7451816246612e-06 old loss 6.69264363750699e-06 BETTER
I0311 21:05:44.553954 1752809 config.py:54] PyTorch version 2.1.1 available.
I0311 21:05:45.547329 1752459 quantize_finetune_llama.py:183] layer 3 gpu 3
I0311 21:05:45.611926 1752809 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:05:53.577150 1752809 finetune.py:45] layer 2_v initial loss 7.434084636770422e-06
I0311 21:06:02.896131 1752573 finetune.py:68] layer 0_v @ epoch 4 new loss 5.526187862869847e-08 old loss 6.510823169492141e-08 BETTER
I0311 21:06:10.644177 1752691 finetune.py:68] layer 1_v @ epoch 2 new loss 4.928138423565542e-06 old loss 5.7451816246612e-06 BETTER
I0311 21:06:12.051952 1752573 finetune.py:45] layer 0_q initial loss 7.545401814468278e-08
I0311 21:06:24.943520 1752809 finetune.py:68] layer 2_v @ epoch 0 new loss 4.9112554734165315e-06 old loss 7.434084636770422e-06 BETTER
I0311 21:06:42.941869 1752691 finetune.py:68] layer 1_v @ epoch 3 new loss 3.538436658345745e-06 old loss 4.928138423565542e-06 BETTER
I0311 21:06:44.977239 1752573 finetune.py:68] layer 0_q @ epoch 0 new loss 5.0454840305746984e-08 old loss 7.545401814468278e-08 BETTER
I0311 21:06:57.426769 1752809 finetune.py:68] layer 2_v @ epoch 1 new loss 3.732554205271299e-06 old loss 4.9112554734165315e-06 BETTER
I0311 21:07:01.336645 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 3 in 75.6677713394165s
I0311 21:07:12.207751 1752927 config.py:54] PyTorch version 2.1.1 available.
I0311 21:07:13.368566 1752459 quantize_finetune_llama.py:183] layer 4 gpu 0
I0311 21:07:13.437851 1752927 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 21:07:15.743121 1752691 finetune.py:68] layer 1_v @ epoch 4 new loss 3.2565390029049013e-06 old loss 3.538436658345745e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:07:19.206890 1752573 finetune.py:68] layer 0_q @ epoch 1 new loss 4.627630190157106e-08 old loss 5.0454840305746984e-08 BETTER
I0311 21:07:22.211289 1752927 finetune.py:45] layer 3_v initial loss 1.3095553185848985e-05
I0311 21:07:25.323127 1752691 finetune.py:45] layer 1_q initial loss 4.416194769873982e-06
I0311 21:07:30.259430 1752809 finetune.py:68] layer 2_v @ epoch 2 new loss 3.1441882129001897e-06 old loss 3.732554205271299e-06 BETTER
I0311 21:07:53.493736 1752573 finetune.py:68] layer 0_q @ epoch 2 new loss 4.337746872806747e-08 old loss 4.627630190157106e-08 BETTER
I0311 21:07:53.628162 1752927 finetune.py:68] layer 3_v @ epoch 0 new loss 7.189170446508797e-06 old loss 1.3095553185848985e-05 BETTER
I0311 21:07:56.975916 1752691 finetune.py:68] layer 1_q @ epoch 0 new loss 3.116036396022537e-06 old loss 4.416194769873982e-06 BETTER
I0311 21:08:03.129972 1752809 finetune.py:68] layer 2_v @ epoch 3 new loss 2.82884820990148e-06 old loss 3.1441882129001897e-06 BETTER
I0311 21:08:25.732774 1752927 finetune.py:68] layer 3_v @ epoch 1 new loss 5.613477242150111e-06 old loss 7.189170446508797e-06 BETTER
I0311 21:08:27.652618 1752573 finetune.py:68] layer 0_q @ epoch 3 new loss 4.112473206419054e-08 old loss 4.337746872806747e-08 BETTER
I0311 21:08:29.248158 1752691 finetune.py:68] layer 1_q @ epoch 1 new loss 2.964802888527629e-06 old loss 3.116036396022537e-06 BETTER
I0311 21:08:36.161760 1752809 finetune.py:68] layer 2_v @ epoch 4 new loss 2.6466225335752824e-06 old loss 2.82884820990148e-06 BETTER
I0311 21:08:45.559638 1752809 finetune.py:45] layer 2_q initial loss 2.959944822578109e-06
I0311 21:08:58.110857 1752927 finetune.py:68] layer 3_v @ epoch 2 new loss 5.0667858886299655e-06 old loss 5.613477242150111e-06 BETTER
I0311 21:09:01.560317 1752691 finetune.py:76] layer 1_q @ epoch 2 new loss 7.647914571862202e-06 old loss 2.964802888527629e-06 WORSE
I0311 21:09:01.957938 1752573 finetune.py:68] layer 0_q @ epoch 4 new loss 3.923225833091237e-08 old loss 4.112473206419054e-08 BETTER
I0311 21:09:11.318671 1752573 finetune.py:45] layer 0_k initial loss 7.109819222250735e-08
I0311 21:09:17.145876 1752809 finetune.py:68] layer 2_q @ epoch 0 new loss 2.694112481549382e-06 old loss 2.959944822578109e-06 BETTER
I0311 21:09:30.441702 1752927 finetune.py:68] layer 3_v @ epoch 3 new loss 4.797763267561095e-06 old loss 5.0667858886299655e-06 BETTER
I0311 21:09:33.409112 1752691 finetune.py:76] layer 1_q @ epoch 3 new loss 4.3468139665492345e-06 old loss 2.964802888527629e-06 WORSE
I0311 21:09:44.098278 1752573 finetune.py:68] layer 0_k @ epoch 0 new loss 3.99735320399941e-08 old loss 7.109819222250735e-08 BETTER
I0311 21:09:49.780517 1752809 finetune.py:68] layer 2_q @ epoch 1 new loss 2.597635329948389e-06 old loss 2.694112481549382e-06 BETTER
I0311 21:10:02.678686 1752927 finetune.py:68] layer 3_v @ epoch 4 new loss 4.626388090400724e-06 old loss 4.797763267561095e-06 BETTER
I0311 21:10:05.000014 1752691 finetune.py:76] layer 1_q @ epoch 4 new loss 6.925965408299817e-06 old loss 2.964802888527629e-06 WORSE
I0311 21:10:11.851400 1752927 finetune.py:45] layer 3_q initial loss 5.547569799091434e-06
I0311 21:10:13.776476 1752691 finetune.py:45] layer 1_k initial loss 3.0885348678566515e-06
I0311 21:10:17.527635 1752573 finetune.py:68] layer 0_k @ epoch 1 new loss 3.8223749498911275e-08 old loss 3.99735320399941e-08 BETTER
I0311 21:10:22.429786 1752809 finetune.py:68] layer 2_q @ epoch 2 new loss 2.528219056330272e-06 old loss 2.597635329948389e-06 BETTER
I0311 21:10:43.267123 1752927 finetune.py:68] layer 3_q @ epoch 0 new loss 5.161730314284796e-06 old loss 5.547569799091434e-06 BETTER
I0311 21:10:45.267058 1752691 finetune.py:76] layer 1_k @ epoch 0 new loss 3.163500423397636e-06 old loss 3.0885348678566515e-06 WORSE
I0311 21:10:51.419646 1752573 finetune.py:68] layer 0_k @ epoch 2 new loss 3.689386929295324e-08 old loss 3.8223749498911275e-08 BETTER
I0311 21:10:55.282609 1752809 finetune.py:68] layer 2_q @ epoch 3 new loss 2.47332036451553e-06 old loss 2.528219056330272e-06 BETTER
I0311 21:11:15.376793 1752927 finetune.py:68] layer 3_q @ epoch 1 new loss 5.017060630052583e-06 old loss 5.161730314284796e-06 BETTER
I0311 21:11:16.737734 1752691 finetune.py:68] layer 1_k @ epoch 1 new loss 2.897556669267942e-06 old loss 3.0885348678566515e-06 BETTER
I0311 21:11:25.106188 1752573 finetune.py:68] layer 0_k @ epoch 3 new loss 3.576202800559258e-08 old loss 3.689386929295324e-08 BETTER
I0311 21:11:27.991529 1752809 finetune.py:68] layer 2_q @ epoch 4 new loss 2.4271803340525366e-06 old loss 2.47332036451553e-06 BETTER
I0311 21:11:37.188293 1752809 finetune.py:45] layer 2_k initial loss 2.6931759293802315e-06
I0311 21:11:47.568142 1752927 finetune.py:68] layer 3_q @ epoch 2 new loss 4.906267804472009e-06 old loss 5.017060630052583e-06 BETTER
I0311 21:11:48.809286 1752691 finetune.py:68] layer 1_k @ epoch 2 new loss 2.866522663680371e-06 old loss 2.897556669267942e-06 BETTER
I0311 21:11:58.802376 1752573 finetune.py:68] layer 0_k @ epoch 4 new loss 3.481629562429589e-08 old loss 3.576202800559258e-08 BETTER
I0311 21:12:08.301458 1752573 finetune.py:45] layer 0_o initial loss 3.0581480814362294e-07
I0311 21:12:08.894317 1752809 finetune.py:68] layer 2_k @ epoch 0 new loss 2.6269894988217857e-06 old loss 2.6931759293802315e-06 BETTER
I0311 21:12:19.695073 1752927 finetune.py:68] layer 3_q @ epoch 3 new loss 4.817458375327988e-06 old loss 4.906267804472009e-06 BETTER
I0311 21:12:20.959696 1752691 finetune.py:68] layer 1_k @ epoch 3 new loss 2.6529944534559036e-06 old loss 2.866522663680371e-06 BETTER
I0311 21:12:40.300345 1752573 finetune.py:68] layer 0_o @ epoch 0 new loss 2.811883632602985e-07 old loss 3.0581480814362294e-07 BETTER
I0311 21:12:41.010236 1752809 finetune.py:68] layer 2_k @ epoch 1 new loss 2.5833437575784046e-06 old loss 2.6269894988217857e-06 BETTER
I0311 21:12:51.841800 1752927 finetune.py:68] layer 3_q @ epoch 4 new loss 4.742149485537084e-06 old loss 4.817458375327988e-06 BETTER
I0311 21:12:53.110844 1752691 finetune.py:76] layer 1_k @ epoch 4 new loss 4.268959401088068e-06 old loss 2.6529944534559036e-06 WORSE
I0311 21:13:01.266893 1752927 finetune.py:45] layer 3_k initial loss 5.4305205594573636e-06
I0311 21:13:01.916054 1752691 finetune.py:45] layer 1_o initial loss 9.963343472918496e-06
I0311 21:13:13.141458 1752809 finetune.py:68] layer 2_k @ epoch 2 new loss 2.5466326860623667e-06 old loss 2.5833437575784046e-06 BETTER
I0311 21:13:13.536502 1752573 finetune.py:68] layer 0_o @ epoch 1 new loss 2.6226345539726026e-07 old loss 2.811883632602985e-07 BETTER
I0311 21:13:32.277039 1752927 finetune.py:68] layer 3_k @ epoch 0 new loss 5.306401817506412e-06 old loss 5.4305205594573636e-06 BETTER
I0311 21:13:32.718573 1752691 finetune.py:68] layer 1_o @ epoch 0 new loss 6.047601345926523e-06 old loss 9.963343472918496e-06 BETTER
I0311 21:13:45.560124 1752809 finetune.py:68] layer 2_k @ epoch 3 new loss 2.514612560844398e-06 old loss 2.5466326860623667e-06 BETTER
I0311 21:13:46.948758 1752573 finetune.py:68] layer 0_o @ epoch 2 new loss 2.4700020162526926e-07 old loss 2.6226345539726026e-07 BETTER
I0311 21:14:04.005210 1752927 finetune.py:68] layer 3_k @ epoch 1 new loss 5.2397472245502286e-06 old loss 5.306401817506412e-06 BETTER
I0311 21:14:04.289465 1752691 finetune.py:68] layer 1_o @ epoch 1 new loss 5.7101278798654675e-06 old loss 6.047601345926523e-06 BETTER
I0311 21:14:17.720695 1752809 finetune.py:68] layer 2_k @ epoch 4 new loss 2.4857170046743704e-06 old loss 2.514612560844398e-06 BETTER
I0311 21:14:20.170760 1752573 finetune.py:68] layer 0_o @ epoch 3 new loss 2.3453956998764625e-07 old loss 2.4700020162526926e-07 BETTER
I0311 21:14:27.157189 1752809 finetune.py:45] layer 2_o initial loss 6.950504484848352e-06
I0311 21:14:35.658675 1752927 finetune.py:68] layer 3_k @ epoch 2 new loss 5.183760549698491e-06 old loss 5.2397472245502286e-06 BETTER
I0311 21:14:35.778872 1752691 finetune.py:68] layer 1_o @ epoch 2 new loss 5.521983894141158e-06 old loss 5.7101278798654675e-06 BETTER
I0311 21:14:53.460714 1752573 finetune.py:68] layer 0_o @ epoch 4 new loss 2.2433181356973364e-07 old loss 2.3453956998764625e-07 BETTER
I0311 21:14:57.948444 1752809 finetune.py:68] layer 2_o @ epoch 0 new loss 6.864183887955733e-06 old loss 6.950504484848352e-06 BETTER
I0311 21:15:07.448434 1752691 finetune.py:68] layer 1_o @ epoch 3 new loss 5.443776444735704e-06 old loss 5.521983894141158e-06 BETTER
I0311 21:15:07.601402 1752927 finetune.py:68] layer 3_k @ epoch 3 new loss 5.134456387168029e-06 old loss 5.183760549698491e-06 BETTER
I0311 21:15:09.430069 1752573 finetune.py:45] layer 0_up initial loss 3.124935403775453e-07
I0311 21:15:29.478983 1752809 finetune.py:68] layer 2_o @ epoch 1 new loss 6.803653377573937e-06 old loss 6.864183887955733e-06 BETTER
I0311 21:15:38.940166 1752691 finetune.py:68] layer 1_o @ epoch 4 new loss 5.343599696061574e-06 old loss 5.443776444735704e-06 BETTER
I0311 21:15:39.337296 1752927 finetune.py:68] layer 3_k @ epoch 4 new loss 5.09052415509359e-06 old loss 5.134456387168029e-06 BETTER
I0311 21:15:39.863497 1752573 finetune.py:68] layer 0_up @ epoch 0 new loss 2.9302958637345e-07 old loss 3.124935403775453e-07 BETTER
I0311 21:15:48.429323 1752927 finetune.py:45] layer 3_o initial loss 1.3145050616003573e-05
I0311 21:15:53.971918 1752691 finetune.py:45] layer 1_up initial loss 8.638096005597617e-06
I0311 21:16:01.143782 1752809 finetune.py:68] layer 2_o @ epoch 2 new loss 6.75306364428252e-06 old loss 6.803653377573937e-06 BETTER
I0311 21:16:11.117239 1752573 finetune.py:68] layer 0_up @ epoch 1 new loss 2.8358729764477175e-07 old loss 2.9302958637345e-07 BETTER
I0311 21:16:18.546084 1752927 finetune.py:68] layer 3_o @ epoch 0 new loss 1.2881273505627178e-05 old loss 1.3145050616003573e-05 BETTER
I0311 21:16:22.866076 1752691 finetune.py:68] layer 1_up @ epoch 0 new loss 6.310057869995944e-06 old loss 8.638096005597617e-06 BETTER
I0311 21:16:32.919757 1752809 finetune.py:68] layer 2_o @ epoch 3 new loss 6.709255103487521e-06 old loss 6.75306364428252e-06 BETTER
I0311 21:16:42.540714 1752573 finetune.py:68] layer 0_up @ epoch 2 new loss 2.7700463078872417e-07 old loss 2.8358729764477175e-07 BETTER
I0311 21:16:49.572023 1752927 finetune.py:68] layer 3_o @ epoch 1 new loss 1.2706605957646389e-05 old loss 1.2881273505627178e-05 BETTER
I0311 21:16:52.395388 1752691 finetune.py:76] layer 1_up @ epoch 1 new loss 6.32942601441755e-06 old loss 6.310057869995944e-06 WORSE
I0311 21:17:04.530744 1752809 finetune.py:68] layer 2_o @ epoch 4 new loss 6.6702868934953585e-06 old loss 6.709255103487521e-06 BETTER
I0311 21:17:14.018331 1752573 finetune.py:68] layer 0_up @ epoch 3 new loss 2.7174971251042734e-07 old loss 2.7700463078872417e-07 BETTER
I0311 21:17:19.979087 1752809 finetune.py:45] layer 2_up initial loss 9.940400559571572e-06
I0311 21:17:20.848813 1752927 finetune.py:68] layer 3_o @ epoch 2 new loss 1.2577814231917728e-05 old loss 1.2706605957646389e-05 BETTER
I0311 21:17:21.716258 1752691 finetune.py:68] layer 1_up @ epoch 2 new loss 6.195134574227268e-06 old loss 6.310057869995944e-06 BETTER
I0311 21:17:45.675029 1752573 finetune.py:68] layer 0_up @ epoch 4 new loss 2.673845358458493e-07 old loss 2.7174971251042734e-07 BETTER
I0311 21:17:49.111878 1752809 finetune.py:68] layer 2_up @ epoch 0 new loss 9.877739103103522e-06 old loss 9.940400559571572e-06 BETTER
I0311 21:17:51.732391 1752691 finetune.py:76] layer 1_up @ epoch 3 new loss 6.261922408157261e-06 old loss 6.195134574227268e-06 WORSE
I0311 21:17:51.991171 1752927 finetune.py:68] layer 3_o @ epoch 3 new loss 1.2471588888729457e-05 old loss 1.2577814231917728e-05 BETTER
I0311 21:18:01.481627 1752573 finetune.py:45] layer 0_gate initial loss 3.474031586847559e-07
I0311 21:18:19.020408 1752809 finetune.py:68] layer 2_up @ epoch 1 new loss 9.835287528403569e-06 old loss 9.877739103103522e-06 BETTER
I0311 21:18:21.027373 1752691 finetune.py:76] layer 1_up @ epoch 4 new loss 6.2262051869765855e-06 old loss 6.195134574227268e-06 WORSE
I0311 21:18:23.110587 1752927 finetune.py:68] layer 3_o @ epoch 4 new loss 1.2381065062072594e-05 old loss 1.2471588888729457e-05 BETTER
I0311 21:18:30.538653 1752573 finetune.py:68] layer 0_gate @ epoch 0 new loss 3.311285468043934e-07 old loss 3.474031586847559e-07 BETTER
I0311 21:18:36.056226 1752691 finetune.py:45] layer 1_gate initial loss 7.320483291550772e-06
I0311 21:18:38.606218 1752927 finetune.py:45] layer 3_up initial loss 1.9925482774851844e-05
I0311 21:18:49.174233 1752809 finetune.py:68] layer 2_up @ epoch 2 new loss 9.799719009606633e-06 old loss 9.835287528403569e-06 BETTER
I0311 21:19:00.412333 1752573 finetune.py:68] layer 0_gate @ epoch 1 new loss 3.250211193517316e-07 old loss 3.311285468043934e-07 BETTER
I0311 21:19:03.734304 1752691 finetune.py:68] layer 1_gate @ epoch 0 new loss 6.913372089911718e-06 old loss 7.320483291550772e-06 BETTER
I0311 21:19:07.335734 1752927 finetune.py:68] layer 3_up @ epoch 0 new loss 1.9774248357862234e-05 old loss 1.9925482774851844e-05 BETTER
I0311 21:19:19.130349 1752809 finetune.py:68] layer 2_up @ epoch 3 new loss 9.767259143700358e-06 old loss 9.799719009606633e-06 BETTER
I0311 21:19:30.433861 1752573 finetune.py:68] layer 0_gate @ epoch 2 new loss 3.2134244065673556e-07 old loss 3.250211193517316e-07 BETTER
I0311 21:19:31.969274 1752691 finetune.py:68] layer 1_gate @ epoch 1 new loss 6.821622719144216e-06 old loss 6.913372089911718e-06 BETTER
I0311 21:19:36.828088 1752927 finetune.py:68] layer 3_up @ epoch 1 new loss 1.9669269022415392e-05 old loss 1.9774248357862234e-05 BETTER
I0311 21:19:49.238791 1752809 finetune.py:68] layer 2_up @ epoch 4 new loss 9.737114851304796e-06 old loss 9.767259143700358e-06 BETTER
I0311 21:19:59.999665 1752691 finetune.py:76] layer 1_gate @ epoch 2 new loss 6.883873538754415e-06 old loss 6.821622719144216e-06 WORSE
I0311 21:20:00.455888 1752573 finetune.py:68] layer 0_gate @ epoch 3 new loss 3.1851865855969663e-07 old loss 3.2134244065673556e-07 BETTER
I0311 21:20:04.673103 1752809 finetune.py:45] layer 2_gate initial loss 1.2065359442203771e-05
I0311 21:20:06.417747 1752927 finetune.py:68] layer 3_up @ epoch 2 new loss 1.957943823072128e-05 old loss 1.9669269022415392e-05 BETTER
I0311 21:20:27.512936 1752691 finetune.py:76] layer 1_gate @ epoch 3 new loss 6.879219654365443e-06 old loss 6.821622719144216e-06 WORSE
I0311 21:20:30.592402 1752573 finetune.py:68] layer 0_gate @ epoch 4 new loss 3.161777044624614e-07 old loss 3.1851865855969663e-07 BETTER
I0311 21:20:32.247426 1752809 finetune.py:68] layer 2_gate @ epoch 0 new loss 1.2005992175545543e-05 old loss 1.2065359442203771e-05 BETTER
I0311 21:20:36.011137 1752927 finetune.py:68] layer 3_up @ epoch 3 new loss 1.9497245375532657e-05 old loss 1.957943823072128e-05 BETTER
I0311 21:20:46.830611 1752573 finetune.py:45] layer 0_down initial loss 7.416620633193816e-07
I0311 21:20:55.092598 1752691 finetune.py:68] layer 1_gate @ epoch 4 new loss 6.788416612835135e-06 old loss 6.821622719144216e-06 BETTER
I0311 21:21:00.517538 1752809 finetune.py:68] layer 2_gate @ epoch 1 new loss 1.1969917977694422e-05 old loss 1.2005992175545543e-05 BETTER
I0311 21:21:05.498420 1752927 finetune.py:68] layer 3_up @ epoch 4 new loss 1.9421864635660313e-05 old loss 1.9497245375532657e-05 BETTER
I0311 21:21:10.936300 1752691 finetune.py:45] layer 1_down initial loss 0.0005115935346111655
I0311 21:21:14.131538 1752573 finetune.py:68] layer 0_down @ epoch 0 new loss 7.390822815978026e-07 old loss 7.416620633193816e-07 BETTER
I0311 21:21:20.592648 1752927 finetune.py:45] layer 3_gate initial loss 2.480598777765408e-05
I0311 21:21:28.831162 1752809 finetune.py:68] layer 2_gate @ epoch 2 new loss 1.1940888725803234e-05 old loss 1.1969917977694422e-05 BETTER
I0311 21:21:36.811296 1752691 finetune.py:68] layer 1_down @ epoch 0 new loss 0.0004183772252872586 old loss 0.0005115935346111655 BETTER
I0311 21:21:42.515450 1752573 finetune.py:68] layer 0_down @ epoch 1 new loss 7.385504545709409e-07 old loss 7.390822815978026e-07 BETTER
I0311 21:21:48.199715 1752927 finetune.py:68] layer 3_gate @ epoch 0 new loss 2.468117600074038e-05 old loss 2.480598777765408e-05 BETTER
I0311 21:21:57.022731 1752809 finetune.py:68] layer 2_gate @ epoch 3 new loss 1.1915379218407907e-05 old loss 1.1940888725803234e-05 BETTER
I0311 21:22:03.543879 1752691 finetune.py:68] layer 1_down @ epoch 1 new loss 0.0003757862141355872 old loss 0.0004183772252872586 BETTER
I0311 21:22:11.004485 1752573 finetune.py:68] layer 0_down @ epoch 2 new loss 7.38269477551512e-07 old loss 7.385504545709409e-07 BETTER
I0311 21:22:16.220510 1752927 finetune.py:68] layer 3_gate @ epoch 1 new loss 2.4598082745797e-05 old loss 2.468117600074038e-05 BETTER
I0311 21:22:25.273985 1752809 finetune.py:68] layer 2_gate @ epoch 4 new loss 1.1892010661540553e-05 old loss 1.1915379218407907e-05 BETTER
I0311 21:22:30.432727 1752691 finetune.py:68] layer 1_down @ epoch 2 new loss 0.00036086575710214674 old loss 0.0003757862141355872 BETTER
I0311 21:22:39.544533 1752573 finetune.py:68] layer 0_down @ epoch 3 new loss 7.380883744190214e-07 old loss 7.38269477551512e-07 BETTER
I0311 21:22:41.523596 1752809 finetune.py:45] layer 2_down initial loss 1.8055008695228025e-05
I0311 21:22:44.365440 1752927 finetune.py:68] layer 3_gate @ epoch 2 new loss 2.4529217625968158e-05 old loss 2.4598082745797e-05 BETTER
I0311 21:22:57.406333 1752691 finetune.py:68] layer 1_down @ epoch 3 new loss 0.0003564740181900561 old loss 0.00036086575710214674 BETTER
I0311 21:23:07.739947 1752809 finetune.py:68] layer 2_down @ epoch 0 new loss 1.8049782738671638e-05 old loss 1.8055008695228025e-05 BETTER
I0311 21:23:08.189570 1752573 finetune.py:68] layer 0_down @ epoch 4 new loss 7.379300086540752e-07 old loss 7.380883744190214e-07 BETTER
0_v proxy err 0.008880347944796085 tr(WHW.T) 4.225186347961426
0_q proxy err 0.00016480298654641956 tr(WHW.T) 2710.489501953125
0_k proxy err 0.0001740863808663562 tr(WHW.T) 1699.01123046875
0_o proxy err 0.0010234080255031586 tr(WHW.T) 0.9708196520805359
0_up proxy err 0.004714897833764553 tr(WHW.T) 43.27153396606445
0_gate proxy err 0.0032610497437417507 tr(WHW.T) 63.482688903808594
0_down proxy err 0.002813325496390462 tr(WHW.T) 0.6576234102249146
I0311 21:23:12.524449 1752927 finetune.py:68] layer 3_gate @ epoch 3 new loss 2.4468221454299055e-05 old loss 2.4529217625968158e-05 BETTER
I0311 21:23:24.451557 1752691 finetune.py:68] layer 1_down @ epoch 4 new loss 0.0003555755829438567 old loss 0.0003564740181900561 BETTER
1_v proxy err 0.019095685333013535 tr(WHW.T) 16.465883255004883
1_q proxy err 0.0002593540702946484 tr(WHW.T) 4779.20751953125
1_k proxy err 0.00027208717074245214 tr(WHW.T) 4996.9873046875
1_o proxy err 0.008072616532444954 tr(WHW.T) 1.1127345561981201
1_up proxy err 0.006508023012429476 tr(WHW.T) 109.77571105957031
1_gate proxy err 0.0032847558613866568 tr(WHW.T) 221.34732055664062
1_down proxy err 0.001042534364387393 tr(WHW.T) 2041.9547119140625
I0311 21:23:35.052306 1752809 finetune.py:68] layer 2_down @ epoch 1 new loss 1.804770181479398e-05 old loss 1.8049782738671638e-05 BETTER
I0311 21:23:40.931615 1752927 finetune.py:68] layer 3_gate @ epoch 4 new loss 2.4411734557361342e-05 old loss 2.4468221454299055e-05 BETTER
I0311 21:23:56.607119 1752927 finetune.py:45] layer 3_down initial loss 3.709324300871231e-05
I0311 21:24:02.183934 1752809 finetune.py:68] layer 2_down @ epoch 2 new loss 1.8046404875349253e-05 old loss 1.804770181479398e-05 BETTER
I0311 21:24:22.218607 1752927 finetune.py:68] layer 3_down @ epoch 0 new loss 3.7084890209371224e-05 old loss 3.709324300871231e-05 BETTER
I0311 21:24:28.976644 1752809 finetune.py:68] layer 2_down @ epoch 3 new loss 1.80455572262872e-05 old loss 1.8046404875349253e-05 BETTER
I0311 21:24:43.424614 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 4 in 73.45996713638306s
I0311 21:24:46.842426 1753043 config.py:54] PyTorch version 2.1.1 available.
I0311 21:24:47.894737 1752459 quantize_finetune_llama.py:183] layer 5 gpu 1
I0311 21:24:47.987981 1753043 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 21:24:48.714015 1752927 finetune.py:68] layer 3_down @ epoch 1 new loss 3.708152871695347e-05 old loss 3.7084890209371224e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:24:55.768771 1752809 finetune.py:68] layer 2_down @ epoch 4 new loss 1.8044929674942978e-05 old loss 1.80455572262872e-05 BETTER
I0311 21:24:56.295149 1753043 finetune.py:45] layer 4_v initial loss 1.738009632390458e-05
2_v proxy err 0.008609087206423283 tr(WHW.T) 136.67332458496094
2_q proxy err 0.00023011017765384167 tr(WHW.T) 7755.2255859375
2_k proxy err 0.00019297549442853779 tr(WHW.T) 10207.3173828125
2_o proxy err 0.009486321359872818 tr(WHW.T) 1.4634281396865845
2_up proxy err 0.007998397573828697 tr(WHW.T) 193.43556213378906
2_gate proxy err 0.005099073983728886 tr(WHW.T) 306.629150390625
2_down proxy err 0.008447492495179176 tr(WHW.T) 3.014430522918701
I0311 21:25:15.144865 1752927 finetune.py:68] layer 3_down @ epoch 2 new loss 3.707929135998711e-05 old loss 3.708152871695347e-05 BETTER
I0311 21:25:29.204058 1753043 finetune.py:68] layer 4_v @ epoch 0 new loss 9.332584340882022e-06 old loss 1.738009632390458e-05 BETTER
I0311 21:25:41.676820 1752927 finetune.py:68] layer 3_down @ epoch 3 new loss 3.70779525837861e-05 old loss 3.707929135998711e-05 BETTER
I0311 21:26:03.683985 1753043 finetune.py:68] layer 4_v @ epoch 1 new loss 7.726995136181358e-06 old loss 9.332584340882022e-06 BETTER
I0311 21:26:08.249492 1752927 finetune.py:68] layer 3_down @ epoch 4 new loss 3.707685391418636e-05 old loss 3.70779525837861e-05 BETTER
3_v proxy err 0.011497619561851025 tr(WHW.T) 284.77557373046875
3_q proxy err 0.0005375324981287122 tr(WHW.T) 7219.02880859375
3_k proxy err 0.0004060740757267922 tr(WHW.T) 10077.3740234375
3_o proxy err 0.008146965876221657 tr(WHW.T) 3.361039876937866
3_up proxy err 0.008979937061667442 tr(WHW.T) 284.7776184082031
3_gate proxy err 0.005414262879639864 tr(WHW.T) 478.1085205078125
3_down proxy err 0.008621339686214924 tr(WHW.T) 6.143070697784424
I0311 21:26:11.672445 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 5 in 70.50965547561646s
I0311 21:26:14.829848 1753159 config.py:54] PyTorch version 2.1.1 available.
I0311 21:26:15.808189 1752459 quantize_finetune_llama.py:183] layer 6 gpu 2
I0311 21:26:15.873299 1753159 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:26:24.068628 1753159 finetune.py:45] layer 5_v initial loss 2.6048293875646777e-05
I0311 21:26:38.570331 1753043 finetune.py:68] layer 4_v @ epoch 2 new loss 7.14215138941654e-06 old loss 7.726995136181358e-06 BETTER
I0311 21:26:55.325314 1753159 finetune.py:68] layer 5_v @ epoch 0 new loss 1.5826133676455356e-05 old loss 2.6048293875646777e-05 BETTER
I0311 21:27:13.335392 1753043 finetune.py:68] layer 4_v @ epoch 3 new loss 6.802491043345071e-06 old loss 7.14215138941654e-06 BETTER
I0311 21:27:25.933243 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 6 in 69.72477698326111s
I0311 21:27:27.480108 1753159 finetune.py:68] layer 5_v @ epoch 1 new loss 1.4034470041224267e-05 old loss 1.5826133676455356e-05 BETTER
I0311 21:27:29.188225 1753275 config.py:54] PyTorch version 2.1.1 available.
I0311 21:27:30.174985 1752459 quantize_finetune_llama.py:183] layer 7 gpu 3
I0311 21:27:30.243806 1753275 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:27:38.464238 1753275 finetune.py:45] layer 6_v initial loss 3.4544460504548624e-05
I0311 21:27:48.135251 1753043 finetune.py:68] layer 4_v @ epoch 4 new loss 6.575197403435595e-06 old loss 6.802491043345071e-06 BETTER
I0311 21:27:57.604468 1753043 finetune.py:45] layer 4_q initial loss 7.973037099873181e-06
I0311 21:28:00.392887 1753159 finetune.py:68] layer 5_v @ epoch 2 new loss 1.3239271538623143e-05 old loss 1.4034470041224267e-05 BETTER
I0311 21:28:10.018843 1753275 finetune.py:68] layer 6_v @ epoch 0 new loss 1.9704999431269243e-05 old loss 3.4544460504548624e-05 BETTER
I0311 21:28:31.232891 1753043 finetune.py:68] layer 4_q @ epoch 0 new loss 7.392008683382301e-06 old loss 7.973037099873181e-06 BETTER
I0311 21:28:32.904572 1753159 finetune.py:68] layer 5_v @ epoch 3 new loss 1.2726933164231014e-05 old loss 1.3239271538623143e-05 BETTER
I0311 21:28:41.534556 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 7 in 70.91623044013977s
I0311 21:28:42.563059 1753275 finetune.py:68] layer 6_v @ epoch 1 new loss 1.7638018107390963e-05 old loss 1.9704999431269243e-05 BETTER
I0311 21:28:44.818156 1753391 config.py:54] PyTorch version 2.1.1 available.
I0311 21:28:45.850515 1752459 quantize_finetune_llama.py:183] layer 8 gpu 0
I0311 21:28:45.918911 1753391 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:28:53.944189 1753391 finetune.py:45] layer 7_v initial loss 4.5593504182761535e-05
I0311 21:29:05.696224 1753043 finetune.py:68] layer 4_q @ epoch 1 new loss 7.181674845924135e-06 old loss 7.392008683382301e-06 BETTER
I0311 21:29:05.793693 1753159 finetune.py:68] layer 5_v @ epoch 4 new loss 1.2354656973911915e-05 old loss 1.2726933164231014e-05 BETTER
I0311 21:29:14.863242 1753159 finetune.py:45] layer 5_q initial loss 1.4536487469740678e-05
I0311 21:29:15.327106 1753275 finetune.py:68] layer 6_v @ epoch 2 new loss 1.6649686585878953e-05 old loss 1.7638018107390963e-05 BETTER
I0311 21:29:25.041140 1753391 finetune.py:68] layer 7_v @ epoch 0 new loss 2.731001040956471e-05 old loss 4.5593504182761535e-05 BETTER
I0311 21:29:40.230411 1753043 finetune.py:68] layer 4_q @ epoch 2 new loss 7.0214855441008694e-06 old loss 7.181674845924135e-06 BETTER
I0311 21:29:46.499186 1753159 finetune.py:68] layer 5_q @ epoch 0 new loss 1.3503051377483644e-05 old loss 1.4536487469740678e-05 BETTER
I0311 21:29:48.320867 1753275 finetune.py:68] layer 6_v @ epoch 3 new loss 1.6024823707994074e-05 old loss 1.6649686585878953e-05 BETTER
I0311 21:29:57.047597 1753391 finetune.py:68] layer 7_v @ epoch 1 new loss 2.475376459187828e-05 old loss 2.731001040956471e-05 BETTER
I0311 21:30:14.769367 1753043 finetune.py:68] layer 4_q @ epoch 3 new loss 6.890373697387986e-06 old loss 7.0214855441008694e-06 BETTER
I0311 21:30:18.941659 1753159 finetune.py:68] layer 5_q @ epoch 1 new loss 1.3156995919416659e-05 old loss 1.3503051377483644e-05 BETTER
I0311 21:30:21.465452 1753275 finetune.py:68] layer 6_v @ epoch 4 new loss 1.55725847434951e-05 old loss 1.6024823707994074e-05 BETTER
I0311 21:30:29.356001 1753391 finetune.py:68] layer 7_v @ epoch 2 new loss 2.349359965592157e-05 old loss 2.475376459187828e-05 BETTER
I0311 21:30:31.064453 1753275 finetune.py:45] layer 6_q initial loss 1.9466058802208863e-05
I0311 21:30:49.646756 1753043 finetune.py:68] layer 4_q @ epoch 4 new loss 6.779104296583682e-06 old loss 6.890373697387986e-06 BETTER
I0311 21:30:51.622580 1753159 finetune.py:68] layer 5_q @ epoch 2 new loss 1.2896246516902465e-05 old loss 1.3156995919416659e-05 BETTER
I0311 21:30:59.149260 1753043 finetune.py:45] layer 4_k initial loss 7.755206752335653e-06
I0311 21:31:01.638906 1753391 finetune.py:68] layer 7_v @ epoch 3 new loss 2.2656879082205705e-05 old loss 2.349359965592157e-05 BETTER
I0311 21:31:02.953064 1753275 finetune.py:68] layer 6_q @ epoch 0 new loss 1.8250546418130398e-05 old loss 1.9466058802208863e-05 BETTER
I0311 21:31:23.852732 1753159 finetune.py:68] layer 5_q @ epoch 3 new loss 1.2679191968345549e-05 old loss 1.2896246516902465e-05 BETTER
I0311 21:31:32.319486 1753043 finetune.py:68] layer 4_k @ epoch 0 new loss 7.5605494203045964e-06 old loss 7.755206752335653e-06 BETTER
I0311 21:31:34.379454 1753391 finetune.py:68] layer 7_v @ epoch 4 new loss 2.2052858184906654e-05 old loss 2.2656879082205705e-05 BETTER
I0311 21:31:35.705751 1753275 finetune.py:68] layer 6_q @ epoch 1 new loss 1.777953002601862e-05 old loss 1.8250546418130398e-05 BETTER
I0311 21:31:43.571192 1753391 finetune.py:45] layer 7_q initial loss 2.7732812668546103e-05
I0311 21:31:56.650851 1753159 finetune.py:68] layer 5_q @ epoch 4 new loss 1.2491954294091556e-05 old loss 1.2679191968345549e-05 BETTER
I0311 21:32:05.992496 1753159 finetune.py:45] layer 5_k initial loss 1.367121785733616e-05
I0311 21:32:06.310406 1753043 finetune.py:68] layer 4_k @ epoch 1 new loss 7.461171207978623e-06 old loss 7.5605494203045964e-06 BETTER
I0311 21:32:08.672596 1753275 finetune.py:68] layer 6_q @ epoch 2 new loss 1.742134554660879e-05 old loss 1.777953002601862e-05 BETTER
I0311 21:32:15.050675 1753391 finetune.py:68] layer 7_q @ epoch 0 new loss 2.5947783797164448e-05 old loss 2.7732812668546103e-05 BETTER
I0311 21:32:37.458158 1753159 finetune.py:68] layer 5_k @ epoch 0 new loss 1.338790389127098e-05 old loss 1.367121785733616e-05 BETTER
I0311 21:32:40.204359 1753043 finetune.py:68] layer 4_k @ epoch 2 new loss 7.37728032618179e-06 old loss 7.461171207978623e-06 BETTER
I0311 21:32:41.425404 1753275 finetune.py:68] layer 6_q @ epoch 3 new loss 1.7129215848399326e-05 old loss 1.742134554660879e-05 BETTER
I0311 21:32:47.107010 1753391 finetune.py:68] layer 7_q @ epoch 1 new loss 2.5261078917537816e-05 old loss 2.5947783797164448e-05 BETTER
I0311 21:33:09.718565 1753159 finetune.py:68] layer 5_k @ epoch 1 new loss 1.322044317930704e-05 old loss 1.338790389127098e-05 BETTER
I0311 21:33:14.074433 1753043 finetune.py:68] layer 4_k @ epoch 3 new loss 7.303805432457011e-06 old loss 7.37728032618179e-06 BETTER
I0311 21:33:14.156198 1753275 finetune.py:68] layer 6_q @ epoch 4 new loss 1.688600423221942e-05 old loss 1.7129215848399326e-05 BETTER
I0311 21:33:19.238657 1753391 finetune.py:68] layer 7_q @ epoch 2 new loss 2.4744947950239293e-05 old loss 2.5261078917537816e-05 BETTER
I0311 21:33:23.471792 1753275 finetune.py:45] layer 6_k initial loss 1.958592292794492e-05
I0311 21:33:41.828677 1753159 finetune.py:68] layer 5_k @ epoch 2 new loss 1.3081225915811956e-05 old loss 1.322044317930704e-05 BETTER
I0311 21:33:47.892090 1753043 finetune.py:68] layer 4_k @ epoch 4 new loss 7.2384091254207306e-06 old loss 7.303805432457011e-06 BETTER
I0311 21:33:51.283836 1753391 finetune.py:68] layer 7_q @ epoch 3 new loss 2.432842848065775e-05 old loss 2.4744947950239293e-05 BETTER
I0311 21:33:54.909500 1753275 finetune.py:68] layer 6_k @ epoch 0 new loss 1.896178764582146e-05 old loss 1.958592292794492e-05 BETTER
I0311 21:33:58.000615 1753043 finetune.py:45] layer 4_o initial loss 2.054724063782487e-05
I0311 21:34:13.919245 1753159 finetune.py:68] layer 5_k @ epoch 3 new loss 1.2959084415342659e-05 old loss 1.3081225915811956e-05 BETTER
I0311 21:34:23.617819 1753391 finetune.py:68] layer 7_q @ epoch 4 new loss 2.3979746401892044e-05 old loss 2.432842848065775e-05 BETTER
I0311 21:34:27.371009 1753275 finetune.py:68] layer 6_k @ epoch 1 new loss 1.8735341654974036e-05 old loss 1.896178764582146e-05 BETTER
I0311 21:34:30.415853 1753043 finetune.py:68] layer 4_o @ epoch 0 new loss 1.9934697775170207e-05 old loss 2.054724063782487e-05 BETTER
I0311 21:34:32.993690 1753391 finetune.py:45] layer 7_k initial loss 2.798238529067021e-05
I0311 21:34:45.929530 1753159 finetune.py:68] layer 5_k @ epoch 4 new loss 1.28526089611114e-05 old loss 1.2959084415342659e-05 BETTER
I0311 21:34:55.085056 1753159 finetune.py:45] layer 5_o initial loss 3.4116375900339335e-05
I0311 21:34:59.561974 1753275 finetune.py:68] layer 6_k @ epoch 2 new loss 1.8547812942415476e-05 old loss 1.8735341654974036e-05 BETTER
I0311 21:35:03.874443 1753043 finetune.py:68] layer 4_o @ epoch 1 new loss 1.963089198397938e-05 old loss 1.9934697775170207e-05 BETTER
I0311 21:35:03.925274 1753391 finetune.py:68] layer 7_k @ epoch 0 new loss 2.7144282285007648e-05 old loss 2.798238529067021e-05 BETTER
I0311 21:35:25.796886 1753159 finetune.py:68] layer 5_o @ epoch 0 new loss 3.2827221730258316e-05 old loss 3.4116375900339335e-05 BETTER
I0311 21:35:31.827970 1753275 finetune.py:68] layer 6_k @ epoch 3 new loss 1.8389322576695122e-05 old loss 1.8547812942415476e-05 BETTER
I0311 21:35:35.564652 1753391 finetune.py:68] layer 7_k @ epoch 1 new loss 2.681468140508514e-05 old loss 2.7144282285007648e-05 BETTER
I0311 21:35:37.199393 1753043 finetune.py:68] layer 4_o @ epoch 2 new loss 1.9400988094275817e-05 old loss 1.963089198397938e-05 BETTER
I0311 21:35:57.266833 1753159 finetune.py:68] layer 5_o @ epoch 1 new loss 3.213435411453247e-05 old loss 3.2827221730258316e-05 BETTER
I0311 21:36:03.950961 1753275 finetune.py:68] layer 6_k @ epoch 4 new loss 1.8251141227665357e-05 old loss 1.8389322576695122e-05 BETTER
I0311 21:36:07.379065 1753391 finetune.py:68] layer 7_k @ epoch 2 new loss 2.6544286811258644e-05 old loss 2.681468140508514e-05 BETTER
I0311 21:36:10.610440 1753043 finetune.py:68] layer 4_o @ epoch 3 new loss 1.9209974198020063e-05 old loss 1.9400988094275817e-05 BETTER
I0311 21:36:13.459136 1753275 finetune.py:45] layer 6_o initial loss 5.04918752994854e-05
I0311 21:36:28.797440 1753159 finetune.py:68] layer 5_o @ epoch 2 new loss 3.160105916322209e-05 old loss 3.213435411453247e-05 BETTER
I0311 21:36:39.374449 1753391 finetune.py:68] layer 7_k @ epoch 3 new loss 2.63194215222029e-05 old loss 2.6544286811258644e-05 BETTER
I0311 21:36:44.176267 1753043 finetune.py:68] layer 4_o @ epoch 4 new loss 1.9040853658225387e-05 old loss 1.9209974198020063e-05 BETTER
I0311 21:36:44.282630 1753275 finetune.py:68] layer 6_o @ epoch 0 new loss 4.8348640120821074e-05 old loss 5.04918752994854e-05 BETTER
I0311 21:36:59.439793 1753043 finetune.py:45] layer 4_up initial loss 3.3063817681977525e-05
I0311 21:37:00.388983 1753159 finetune.py:68] layer 5_o @ epoch 3 new loss 3.115610888926312e-05 old loss 3.160105916322209e-05 BETTER
I0311 21:37:11.227368 1753391 finetune.py:68] layer 7_k @ epoch 4 new loss 2.6117093511857092e-05 old loss 2.63194215222029e-05 BETTER
I0311 21:37:15.762475 1753275 finetune.py:68] layer 6_o @ epoch 1 new loss 4.7269335482269526e-05 old loss 4.8348640120821074e-05 BETTER
I0311 21:37:20.833907 1753391 finetune.py:45] layer 7_o initial loss 7.034422014839947e-05
I0311 21:37:29.940784 1753043 finetune.py:68] layer 4_up @ epoch 0 new loss 3.270431261626072e-05 old loss 3.3063817681977525e-05 BETTER
I0311 21:37:31.853848 1753159 finetune.py:68] layer 5_o @ epoch 4 new loss 3.0779698136029765e-05 old loss 3.115610888926312e-05 BETTER
I0311 21:37:46.968996 1753159 finetune.py:45] layer 5_up initial loss 5.338976552593522e-05
I0311 21:37:47.468715 1753275 finetune.py:68] layer 6_o @ epoch 2 new loss 4.645995795726776e-05 old loss 4.7269335482269526e-05 BETTER
I0311 21:37:50.952130 1753391 finetune.py:68] layer 7_o @ epoch 0 new loss 6.670333095826209e-05 old loss 7.034422014839947e-05 BETTER
I0311 21:38:01.480370 1753043 finetune.py:68] layer 4_up @ epoch 1 new loss 3.2462670787936077e-05 old loss 3.270431261626072e-05 BETTER
I0311 21:38:15.844635 1753159 finetune.py:68] layer 5_up @ epoch 0 new loss 5.26936273672618e-05 old loss 5.338976552593522e-05 BETTER
I0311 21:38:19.259624 1753275 finetune.py:68] layer 6_o @ epoch 3 new loss 4.580712993629277e-05 old loss 4.645995795726776e-05 BETTER
I0311 21:38:21.905997 1753391 finetune.py:68] layer 7_o @ epoch 1 new loss 6.490881060017273e-05 old loss 6.670333095826209e-05 BETTER
I0311 21:38:33.047611 1753043 finetune.py:68] layer 4_up @ epoch 2 new loss 3.226104672648944e-05 old loss 3.2462670787936077e-05 BETTER
I0311 21:38:45.795514 1753159 finetune.py:68] layer 5_up @ epoch 1 new loss 5.220301682129502e-05 old loss 5.26936273672618e-05 BETTER
I0311 21:38:51.050474 1753275 finetune.py:68] layer 6_o @ epoch 4 new loss 4.525725671555847e-05 old loss 4.580712993629277e-05 BETTER
I0311 21:38:52.855983 1753391 finetune.py:68] layer 7_o @ epoch 2 new loss 6.360551196848974e-05 old loss 6.490881060017273e-05 BETTER
I0311 21:39:04.711289 1753043 finetune.py:68] layer 4_up @ epoch 3 new loss 3.207955160178244e-05 old loss 3.226104672648944e-05 BETTER
I0311 21:39:06.338037 1753275 finetune.py:45] layer 6_up initial loss 7.98809778643772e-05
I0311 21:39:15.767215 1753159 finetune.py:68] layer 5_up @ epoch 2 new loss 5.178303035791032e-05 old loss 5.220301682129502e-05 BETTER
I0311 21:39:24.008358 1753391 finetune.py:68] layer 7_o @ epoch 3 new loss 6.257669883780181e-05 old loss 6.360551196848974e-05 BETTER
I0311 21:39:35.477123 1753275 finetune.py:68] layer 6_up @ epoch 0 new loss 7.862423080950975e-05 old loss 7.98809778643772e-05 BETTER
I0311 21:39:36.516517 1753043 finetune.py:68] layer 4_up @ epoch 4 new loss 3.191067298757844e-05 old loss 3.207955160178244e-05 BETTER
I0311 21:39:45.557044 1753159 finetune.py:68] layer 5_up @ epoch 3 new loss 5.1404564146650955e-05 old loss 5.178303035791032e-05 BETTER
I0311 21:39:51.639255 1753043 finetune.py:45] layer 4_gate initial loss 4.091495429747738e-05
I0311 21:39:54.978910 1753391 finetune.py:68] layer 7_o @ epoch 4 new loss 6.17147670709528e-05 old loss 6.257669883780181e-05 BETTER
I0311 21:40:05.283308 1753275 finetune.py:68] layer 6_up @ epoch 1 new loss 7.778775034239516e-05 old loss 7.862423080950975e-05 BETTER
I0311 21:40:10.143951 1753391 finetune.py:45] layer 7_up initial loss 0.00010853420826606452
I0311 21:40:15.348133 1753159 finetune.py:68] layer 5_up @ epoch 4 new loss 5.106395110487938e-05 old loss 5.1404564146650955e-05 BETTER
I0311 21:40:20.718666 1753043 finetune.py:68] layer 4_gate @ epoch 0 new loss 4.065197572344914e-05 old loss 4.091495429747738e-05 BETTER
I0311 21:40:30.745183 1753159 finetune.py:45] layer 5_gate initial loss 6.4770647441037e-05
I0311 21:40:35.273131 1753275 finetune.py:68] layer 6_up @ epoch 2 new loss 7.708204793743789e-05 old loss 7.778775034239516e-05 BETTER
I0311 21:40:38.816480 1753391 finetune.py:68] layer 7_up @ epoch 0 new loss 0.00010647164162946865 old loss 0.00010853420826606452 BETTER
I0311 21:40:50.610424 1753043 finetune.py:68] layer 4_gate @ epoch 1 new loss 4.0466260543325916e-05 old loss 4.065197572344914e-05 BETTER
I0311 21:40:58.286058 1753159 finetune.py:68] layer 5_gate @ epoch 0 new loss 6.43223465885967e-05 old loss 6.4770647441037e-05 BETTER
I0311 21:41:05.325052 1753275 finetune.py:68] layer 6_up @ epoch 3 new loss 7.645625009899959e-05 old loss 7.708204793743789e-05 BETTER
I0311 21:41:08.408944 1753391 finetune.py:68] layer 7_up @ epoch 1 new loss 0.00010515488247619942 old loss 0.00010647164162946865 BETTER
I0311 21:41:20.451351 1753043 finetune.py:68] layer 4_gate @ epoch 2 new loss 4.031270873383619e-05 old loss 4.0466260543325916e-05 BETTER
I0311 21:41:26.585330 1753159 finetune.py:68] layer 5_gate @ epoch 1 new loss 6.398464029189199e-05 old loss 6.43223465885967e-05 BETTER
I0311 21:41:35.458575 1753275 finetune.py:68] layer 6_up @ epoch 4 new loss 7.589723827550188e-05 old loss 7.645625009899959e-05 BETTER
I0311 21:41:38.026843 1753391 finetune.py:68] layer 7_up @ epoch 2 new loss 0.00010405053762951866 old loss 0.00010515488247619942 BETTER
I0311 21:41:50.608218 1753043 finetune.py:68] layer 4_gate @ epoch 3 new loss 4.017624451080337e-05 old loss 4.031270873383619e-05 BETTER
I0311 21:41:51.091210 1753275 finetune.py:45] layer 6_gate initial loss 9.507758659310639e-05
I0311 21:41:54.735533 1753159 finetune.py:68] layer 5_gate @ epoch 2 new loss 6.369442417053506e-05 old loss 6.398464029189199e-05 BETTER
I0311 21:42:07.656991 1753391 finetune.py:68] layer 7_up @ epoch 3 new loss 0.00010309307253919542 old loss 0.00010405053762951866 BETTER
I0311 21:42:18.798050 1753275 finetune.py:68] layer 6_gate @ epoch 0 new loss 9.433746163267642e-05 old loss 9.507758659310639e-05 BETTER
I0311 21:42:20.780285 1753043 finetune.py:68] layer 4_gate @ epoch 4 new loss 4.00502176489681e-05 old loss 4.017624451080337e-05 BETTER
I0311 21:42:22.887383 1753159 finetune.py:68] layer 5_gate @ epoch 3 new loss 6.342935375869274e-05 old loss 6.369442417053506e-05 BETTER
I0311 21:42:36.920012 1753043 finetune.py:45] layer 4_down initial loss 6.391706847352907e-05
I0311 21:42:37.463609 1753391 finetune.py:68] layer 7_up @ epoch 4 new loss 0.00010224260040558875 old loss 0.00010309307253919542 BETTER
I0311 21:42:46.960050 1753275 finetune.py:68] layer 6_gate @ epoch 1 new loss 9.379663970321417e-05 old loss 9.433746163267642e-05 BETTER
I0311 21:42:51.101804 1753159 finetune.py:68] layer 5_gate @ epoch 4 new loss 6.318847590591758e-05 old loss 6.342935375869274e-05 BETTER
I0311 21:42:52.653562 1753391 finetune.py:45] layer 7_gate initial loss 0.00012798797979485244
I0311 21:43:04.333712 1753043 finetune.py:68] layer 4_down @ epoch 0 new loss 6.39011777820997e-05 old loss 6.391706847352907e-05 BETTER
I0311 21:43:07.022377 1753159 finetune.py:45] layer 5_down initial loss 9.874784882413223e-05
I0311 21:43:14.952042 1753275 finetune.py:68] layer 6_gate @ epoch 2 new loss 9.332445915788412e-05 old loss 9.379663970321417e-05 BETTER
I0311 21:43:19.703744 1753391 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.00012688229617197067 old loss 0.00012798797979485244 BETTER
I0311 21:43:32.887940 1753043 finetune.py:68] layer 4_down @ epoch 1 new loss 6.389457848854363e-05 old loss 6.39011777820997e-05 BETTER
I0311 21:43:32.967455 1753159 finetune.py:68] layer 5_down @ epoch 0 new loss 9.873205999610946e-05 old loss 9.874784882413223e-05 BETTER
I0311 21:43:43.197214 1753275 finetune.py:68] layer 6_gate @ epoch 3 new loss 9.290576417697594e-05 old loss 9.332445915788412e-05 BETTER
I0311 21:43:47.886326 1753391 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.00012607894313987345 old loss 0.00012688229617197067 BETTER
I0311 21:43:59.917298 1753159 finetune.py:68] layer 5_down @ epoch 1 new loss 9.872576629277319e-05 old loss 9.873205999610946e-05 BETTER
I0311 21:44:01.432071 1753043 finetune.py:68] layer 4_down @ epoch 2 new loss 6.38912315480411e-05 old loss 6.389457848854363e-05 BETTER
I0311 21:44:11.474670 1753275 finetune.py:68] layer 6_gate @ epoch 4 new loss 9.252601012121886e-05 old loss 9.290576417697594e-05 BETTER
I0311 21:44:15.936622 1753391 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.000125378486700356 old loss 0.00012607894313987345 BETTER
I0311 21:44:26.905569 1753159 finetune.py:68] layer 5_down @ epoch 2 new loss 9.872124064713717e-05 old loss 9.872576629277319e-05 BETTER
I0311 21:44:27.610818 1753275 finetune.py:45] layer 6_down initial loss 0.00014572031795978546
I0311 21:44:29.982059 1753043 finetune.py:68] layer 4_down @ epoch 3 new loss 6.388911424437538e-05 old loss 6.38912315480411e-05 BETTER
I0311 21:44:44.189934 1753391 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.00012476179108489305 old loss 0.000125378486700356 BETTER
I0311 21:44:53.984815 1753159 finetune.py:68] layer 5_down @ epoch 3 new loss 9.871891234070063e-05 old loss 9.872124064713717e-05 BETTER
I0311 21:44:54.059408 1753275 finetune.py:68] layer 6_down @ epoch 0 new loss 0.00014569450286217034 old loss 0.00014572031795978546 BETTER
I0311 21:44:58.870098 1753043 finetune.py:68] layer 4_down @ epoch 4 new loss 6.388747715391219e-05 old loss 6.388911424437538e-05 BETTER
4_v proxy err 0.010586339049041271 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0005057582748122513 tr(WHW.T) 6919.6259765625
4_k proxy err 0.0003561505291145295 tr(WHW.T) 10421.0029296875
4_o proxy err 0.009493492543697357 tr(WHW.T) 5.145541667938232
4_up proxy err 0.008580832742154598 tr(WHW.T) 397.7040710449219
4_gate proxy err 0.004243048839271069 tr(WHW.T) 820.3348999023438
4_down proxy err 0.008664543740451336 tr(WHW.T) 11.608366966247559
I0311 21:45:12.498493 1753391 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.00012420675193425268 old loss 0.00012476179108489305 BETTER
I0311 21:45:21.171451 1753275 finetune.py:68] layer 6_down @ epoch 1 new loss 0.00014568190090358257 old loss 0.00014569450286217034 BETTER
I0311 21:45:21.196223 1753159 finetune.py:68] layer 5_down @ epoch 4 new loss 9.87172606983222e-05 old loss 9.871891234070063e-05 BETTER
5_v proxy err 0.010893704369664192 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0005647347425110638 tr(WHW.T) 6772.8271484375
5_k proxy err 0.00037632600287906826 tr(WHW.T) 10847.498046875
5_o proxy err 0.010713429190218449 tr(WHW.T) 7.9584808349609375
5_up proxy err 0.008477702736854553 tr(WHW.T) 506.762451171875
5_gate proxy err 0.003964826464653015 tr(WHW.T) 1103.9052734375
5_down proxy err 0.009434085339307785 tr(WHW.T) 15.709622383117676
I0311 21:45:28.048176 1753391 finetune.py:45] layer 7_down initial loss 0.00019565616094041616
I0311 21:45:47.942019 1753275 finetune.py:68] layer 6_down @ epoch 2 new loss 0.00014567362086381763 old loss 0.00014568190090358257 BETTER
I0311 21:45:53.462985 1753391 finetune.py:68] layer 7_down @ epoch 0 new loss 0.00019562782836146653 old loss 0.00019565616094041616 BETTER
I0311 21:46:14.753908 1753275 finetune.py:68] layer 6_down @ epoch 3 new loss 0.0001456684258300811 old loss 0.00014567362086381763 BETTER
I0311 21:46:19.661102 1753391 finetune.py:68] layer 7_down @ epoch 1 new loss 0.00019561438239179552 old loss 0.00019562782836146653 BETTER
I0311 21:46:37.219045 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 8 in 71.49637055397034s
I0311 21:46:40.396249 1753507 config.py:54] PyTorch version 2.1.1 available.
I0311 21:46:41.435454 1752459 quantize_finetune_llama.py:183] layer 9 gpu 1
I0311 21:46:41.501807 1753507 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 21:46:41.597671 1753275 finetune.py:68] layer 6_down @ epoch 4 new loss 0.0001456646277802065 old loss 0.0001456684258300811 BETTER
6_v proxy err 0.011725764721632004 tr(WHW.T) 443.5464782714844
6_q proxy err 0.0007716304389759898 tr(WHW.T) 7578.8330078125
6_k proxy err 0.000578342704102397 tr(WHW.T) 10415.751953125
6_o proxy err 0.012189759872853756 tr(WHW.T) 11.615220069885254
6_up proxy err 0.008460108190774918 tr(WHW.T) 617.170654296875
6_gate proxy err 0.0034356550313532352 tr(WHW.T) 1554.2646484375
6_down proxy err 0.00964814517647028 tr(WHW.T) 23.073150634765625
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:46:46.113391 1753391 finetune.py:68] layer 7_down @ epoch 2 new loss 0.00019560675718821585 old loss 0.00019561438239179552 BETTER
I0311 21:46:49.868412 1753507 finetune.py:45] layer 8_v initial loss 6.33405361440964e-05
I0311 21:47:12.425391 1753391 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00019560137297958136 old loss 0.00019560675718821585 BETTER
I0311 21:47:22.837798 1753507 finetune.py:68] layer 8_v @ epoch 0 new loss 3.877648850902915e-05 old loss 6.33405361440964e-05 BETTER
I0311 21:47:38.997649 1753391 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0001955974439624697 old loss 0.00019560137297958136 BETTER
7_v proxy err 0.011643420904874802 tr(WHW.T) 489.9357604980469
7_q proxy err 0.0008283291826955974 tr(WHW.T) 7675.326171875
7_k proxy err 0.0006381827406585217 tr(WHW.T) 10207.603515625
7_o proxy err 0.01351644191890955 tr(WHW.T) 15.210870742797852
7_up proxy err 0.00817751046270132 tr(WHW.T) 735.811767578125
7_gate proxy err 0.003284832928329706 tr(WHW.T) 1875.6004638671875
7_down proxy err 0.009767338633537292 tr(WHW.T) 30.698720932006836
I0311 21:47:56.134738 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 9 in 69.93448638916016s
I0311 21:47:57.170933 1753507 finetune.py:68] layer 8_v @ epoch 1 new loss 3.519424717524089e-05 old loss 3.877648850902915e-05 BETTER
I0311 21:47:59.229281 1753623 config.py:54] PyTorch version 2.1.1 available.
I0311 21:48:00.212099 1752459 quantize_finetune_llama.py:183] layer 10 gpu 2
I0311 21:48:00.283838 1753623 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:48:08.309839 1753623 finetune.py:45] layer 9_v initial loss 5.8102476032217965e-05
I0311 21:48:31.994663 1753507 finetune.py:68] layer 8_v @ epoch 2 new loss 3.338843089295551e-05 old loss 3.519424717524089e-05 BETTER
I0311 21:48:39.705417 1753623 finetune.py:68] layer 9_v @ epoch 0 new loss 4.521113442024216e-05 old loss 5.8102476032217965e-05 BETTER
I0311 21:49:06.971701 1753507 finetune.py:68] layer 8_v @ epoch 3 new loss 3.221693623345345e-05 old loss 3.338843089295551e-05 BETTER
I0311 21:49:10.472395 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 10 in 69.8602967262268s
I0311 21:49:11.862063 1753623 finetune.py:68] layer 9_v @ epoch 1 new loss 4.235394226270728e-05 old loss 4.521113442024216e-05 BETTER
I0311 21:49:13.702196 1753739 config.py:54] PyTorch version 2.1.1 available.
I0311 21:49:14.691346 1752459 quantize_finetune_llama.py:183] layer 11 gpu 3
I0311 21:49:14.753139 1753739 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:49:23.138614 1753739 finetune.py:45] layer 10_v initial loss 8.498364331899211e-05
I0311 21:49:41.995737 1753507 finetune.py:68] layer 8_v @ epoch 4 new loss 3.135211591143161e-05 old loss 3.221693623345345e-05 BETTER
I0311 21:49:44.311049 1753623 finetune.py:68] layer 9_v @ epoch 2 new loss 4.066249312018044e-05 old loss 4.235394226270728e-05 BETTER
I0311 21:49:51.522690 1753507 finetune.py:45] layer 8_q initial loss 3.86627325497102e-05
I0311 21:49:54.669586 1753739 finetune.py:68] layer 10_v @ epoch 0 new loss 6.555479194503278e-05 old loss 8.498364331899211e-05 BETTER
I0311 21:50:17.262709 1753623 finetune.py:68] layer 9_v @ epoch 3 new loss 3.945909338654019e-05 old loss 4.066249312018044e-05 BETTER
I0311 21:50:25.053073 1753507 finetune.py:68] layer 8_q @ epoch 0 new loss 3.6274304875405505e-05 old loss 3.86627325497102e-05 BETTER
I0311 21:50:26.255272 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 11 in 71.19137573242188s
I0311 21:50:27.186836 1753739 finetune.py:68] layer 10_v @ epoch 1 new loss 6.105609645601362e-05 old loss 6.555479194503278e-05 BETTER
I0311 21:50:29.544242 1753855 config.py:54] PyTorch version 2.1.1 available.
I0311 21:50:30.582950 1752459 quantize_finetune_llama.py:183] layer 12 gpu 0
I0311 21:50:30.648579 1753855 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 21:50:39.047961 1753855 finetune.py:45] layer 11_v initial loss 8.670808165334165e-05
I0311 21:50:50.454053 1753623 finetune.py:68] layer 9_v @ epoch 4 new loss 3.853313683066517e-05 old loss 3.945909338654019e-05 BETTER
I0311 21:50:59.560093 1753507 finetune.py:68] layer 8_q @ epoch 1 new loss 3.5314002161612734e-05 old loss 3.6274304875405505e-05 BETTER
I0311 21:50:59.728407 1753623 finetune.py:45] layer 9_q initial loss 4.7296482080128044e-05
I0311 21:50:59.988020 1753739 finetune.py:68] layer 10_v @ epoch 2 new loss 5.832779061165638e-05 old loss 6.105609645601362e-05 BETTER
I0311 21:51:10.108248 1753855 finetune.py:68] layer 11_v @ epoch 0 new loss 6.663314707111567e-05 old loss 8.670808165334165e-05 BETTER
I0311 21:51:31.479759 1753623 finetune.py:68] layer 9_q @ epoch 0 new loss 4.477542461245321e-05 old loss 4.7296482080128044e-05 BETTER
I0311 21:51:33.138445 1753739 finetune.py:68] layer 10_v @ epoch 3 new loss 5.636736750602722e-05 old loss 5.832779061165638e-05 BETTER
I0311 21:51:33.907981 1753507 finetune.py:68] layer 8_q @ epoch 2 new loss 3.460114385234192e-05 old loss 3.5314002161612734e-05 BETTER
I0311 21:51:42.053240 1753855 finetune.py:68] layer 11_v @ epoch 1 new loss 6.217959889909253e-05 old loss 6.663314707111567e-05 BETTER
I0311 21:52:04.465179 1753623 finetune.py:68] layer 9_q @ epoch 1 new loss 4.3715939682442695e-05 old loss 4.477542461245321e-05 BETTER
I0311 21:52:06.438892 1753739 finetune.py:68] layer 10_v @ epoch 4 new loss 5.484990106197074e-05 old loss 5.636736750602722e-05 BETTER
I0311 21:52:08.508290 1753507 finetune.py:68] layer 8_q @ epoch 3 new loss 3.400679997866973e-05 old loss 3.460114385234192e-05 BETTER
I0311 21:52:14.586366 1753855 finetune.py:68] layer 11_v @ epoch 2 new loss 5.954698281129822e-05 old loss 6.217959889909253e-05 BETTER
I0311 21:52:15.983266 1753739 finetune.py:45] layer 10_q initial loss 6.576120358658955e-05
I0311 21:52:36.941886 1753623 finetune.py:68] layer 9_q @ epoch 2 new loss 4.289814023650251e-05 old loss 4.3715939682442695e-05 BETTER
I0311 21:52:43.245343 1753507 finetune.py:68] layer 8_q @ epoch 4 new loss 3.352018757141195e-05 old loss 3.400679997866973e-05 BETTER
I0311 21:52:47.403185 1753855 finetune.py:68] layer 11_v @ epoch 3 new loss 5.769814379164018e-05 old loss 5.954698281129822e-05 BETTER
I0311 21:52:47.774860 1753739 finetune.py:68] layer 10_q @ epoch 0 new loss 6.185085658216849e-05 old loss 6.576120358658955e-05 BETTER
I0311 21:52:53.217683 1753507 finetune.py:45] layer 8_k initial loss 3.852903319057077e-05
I0311 21:53:09.449430 1753623 finetune.py:68] layer 9_q @ epoch 3 new loss 4.2227678932249546e-05 old loss 4.289814023650251e-05 BETTER
I0311 21:53:20.325055 1753855 finetune.py:68] layer 11_v @ epoch 4 new loss 5.631744716083631e-05 old loss 5.769814379164018e-05 BETTER
I0311 21:53:20.385072 1753739 finetune.py:68] layer 10_q @ epoch 1 new loss 6.0297668824205175e-05 old loss 6.185085658216849e-05 BETTER
I0311 21:53:26.286587 1753507 finetune.py:68] layer 8_k @ epoch 0 new loss 3.754159843083471e-05 old loss 3.852903319057077e-05 BETTER
I0311 21:53:29.679785 1753855 finetune.py:45] layer 11_q initial loss 6.812872743466869e-05
I0311 21:53:42.081265 1753623 finetune.py:68] layer 9_q @ epoch 4 new loss 4.1660536226117983e-05 old loss 4.2227678932249546e-05 BETTER
I0311 21:53:51.235921 1753623 finetune.py:45] layer 9_k initial loss 4.854520739172585e-05
I0311 21:53:53.155961 1753739 finetune.py:68] layer 10_q @ epoch 2 new loss 5.9045218222308904e-05 old loss 6.0297668824205175e-05 BETTER
I0311 21:54:00.190899 1753507 finetune.py:68] layer 8_k @ epoch 1 new loss 3.709900920512155e-05 old loss 3.754159843083471e-05 BETTER
I0311 21:54:01.240299 1753855 finetune.py:68] layer 11_q @ epoch 0 new loss 6.45981781417504e-05 old loss 6.812872743466869e-05 BETTER
I0311 21:54:22.733352 1753623 finetune.py:68] layer 9_k @ epoch 0 new loss 4.703944796347059e-05 old loss 4.854520739172585e-05 BETTER
I0311 21:54:25.786160 1753739 finetune.py:68] layer 10_q @ epoch 3 new loss 5.802191299153492e-05 old loss 5.9045218222308904e-05 BETTER
I0311 21:54:33.330087 1753855 finetune.py:68] layer 11_q @ epoch 1 new loss 6.311659672064707e-05 old loss 6.45981781417504e-05 BETTER
I0311 21:54:34.285415 1753507 finetune.py:68] layer 8_k @ epoch 2 new loss 3.6728783015860245e-05 old loss 3.709900920512155e-05 BETTER
I0311 21:54:54.956099 1753623 finetune.py:68] layer 9_k @ epoch 1 new loss 4.647324021789245e-05 old loss 4.703944796347059e-05 BETTER
I0311 21:54:58.489271 1753739 finetune.py:68] layer 10_q @ epoch 4 new loss 5.715661609428935e-05 old loss 5.802191299153492e-05 BETTER
I0311 21:55:05.562887 1753855 finetune.py:68] layer 11_q @ epoch 2 new loss 6.19496131548658e-05 old loss 6.311659672064707e-05 BETTER
I0311 21:55:07.963834 1753739 finetune.py:45] layer 10_k initial loss 6.499658047687262e-05
I0311 21:55:08.123420 1753507 finetune.py:68] layer 8_k @ epoch 3 new loss 3.641435978352092e-05 old loss 3.6728783015860245e-05 BETTER
I0311 21:55:27.127942 1753623 finetune.py:68] layer 9_k @ epoch 2 new loss 4.6021625166758895e-05 old loss 4.647324021789245e-05 BETTER
I0311 21:55:37.804091 1753855 finetune.py:68] layer 11_q @ epoch 3 new loss 6.1003214796073735e-05 old loss 6.19496131548658e-05 BETTER
I0311 21:55:39.477661 1753739 finetune.py:68] layer 10_k @ epoch 0 new loss 6.310595927061513e-05 old loss 6.499658047687262e-05 BETTER
I0311 21:55:42.146361 1753507 finetune.py:68] layer 8_k @ epoch 4 new loss 3.614116576500237e-05 old loss 3.641435978352092e-05 BETTER
I0311 21:55:51.567593 1753507 finetune.py:45] layer 8_o initial loss 0.00010198066593147814
I0311 21:55:59.297891 1753623 finetune.py:68] layer 9_k @ epoch 3 new loss 4.5631768443854526e-05 old loss 4.6021625166758895e-05 BETTER
I0311 21:56:09.998562 1753855 finetune.py:68] layer 11_q @ epoch 4 new loss 6.02185282332357e-05 old loss 6.1003214796073735e-05 BETTER
I0311 21:56:11.693004 1753739 finetune.py:68] layer 10_k @ epoch 1 new loss 6.229933933354914e-05 old loss 6.310595927061513e-05 BETTER
I0311 21:56:19.173111 1753855 finetune.py:45] layer 11_k initial loss 6.882024899823591e-05
I0311 21:56:24.083175 1753507 finetune.py:68] layer 8_o @ epoch 0 new loss 9.62487974902615e-05 old loss 0.00010198066593147814 BETTER
I0311 21:56:31.407618 1753623 finetune.py:68] layer 9_k @ epoch 4 new loss 4.5300745114218444e-05 old loss 4.5631768443854526e-05 BETTER
I0311 21:56:40.787519 1753623 finetune.py:45] layer 9_o initial loss 0.00013090204447507858
I0311 21:56:43.743055 1753739 finetune.py:68] layer 10_k @ epoch 2 new loss 6.16416145930998e-05 old loss 6.229933933354914e-05 BETTER
I0311 21:56:50.089133 1753855 finetune.py:68] layer 11_k @ epoch 0 new loss 6.732593465130776e-05 old loss 6.882024899823591e-05 BETTER
I0311 21:56:57.285861 1753507 finetune.py:68] layer 8_o @ epoch 1 new loss 9.330399916507304e-05 old loss 9.62487974902615e-05 BETTER
I0311 21:57:11.654926 1753623 finetune.py:68] layer 9_o @ epoch 0 new loss 0.00012346227595116943 old loss 0.00013090204447507858 BETTER
I0311 21:57:15.793432 1753739 finetune.py:68] layer 10_k @ epoch 3 new loss 6.108473462518305e-05 old loss 6.16416145930998e-05 BETTER
I0311 21:57:21.778545 1753855 finetune.py:68] layer 11_k @ epoch 1 new loss 6.65688858134672e-05 old loss 6.732593465130776e-05 BETTER
I0311 21:57:30.676238 1753507 finetune.py:68] layer 8_o @ epoch 2 new loss 9.117807348957285e-05 old loss 9.330399916507304e-05 BETTER
I0311 21:57:43.165970 1753623 finetune.py:68] layer 9_o @ epoch 1 new loss 0.00011952163185924292 old loss 0.00012346227595116943 BETTER
I0311 21:57:47.782932 1753739 finetune.py:68] layer 10_k @ epoch 4 new loss 6.056648271623999e-05 old loss 6.108473462518305e-05 BETTER
I0311 21:57:53.425965 1753855 finetune.py:68] layer 11_k @ epoch 2 new loss 6.594355363631621e-05 old loss 6.65688858134672e-05 BETTER
I0311 21:57:57.298510 1753739 finetune.py:45] layer 10_o initial loss 0.00018124442431144416
I0311 21:58:04.118714 1753507 finetune.py:68] layer 8_o @ epoch 3 new loss 8.951478230301291e-05 old loss 9.117807348957285e-05 BETTER
I0311 21:58:14.736011 1753623 finetune.py:68] layer 9_o @ epoch 2 new loss 0.00011669907689793035 old loss 0.00011952163185924292 BETTER
I0311 21:58:25.626876 1753855 finetune.py:68] layer 11_k @ epoch 3 new loss 6.543118797708303e-05 old loss 6.594355363631621e-05 BETTER
I0311 21:58:28.299247 1753739 finetune.py:68] layer 10_o @ epoch 0 new loss 0.00017045812273863703 old loss 0.00018124442431144416 BETTER
I0311 21:58:37.617516 1753507 finetune.py:68] layer 8_o @ epoch 4 new loss 8.815123146632686e-05 old loss 8.951478230301291e-05 BETTER
I0311 21:58:46.365602 1753623 finetune.py:68] layer 9_o @ epoch 3 new loss 0.00011448279110481963 old loss 0.00011669907689793035 BETTER
I0311 21:58:53.222150 1753507 finetune.py:45] layer 8_up initial loss 0.00014358587213791907
I0311 21:58:57.343156 1753855 finetune.py:68] layer 11_k @ epoch 4 new loss 6.49845678708516e-05 old loss 6.543118797708303e-05 BETTER
I0311 21:58:59.981446 1753739 finetune.py:68] layer 10_o @ epoch 1 new loss 0.0001645442534936592 old loss 0.00017045812273863703 BETTER
I0311 21:59:06.837641 1753855 finetune.py:45] layer 11_o initial loss 0.00018741875828709453
I0311 21:59:18.068016 1753623 finetune.py:68] layer 9_o @ epoch 4 new loss 0.0001126629940699786 old loss 0.00011448279110481963 BETTER
I0311 21:59:24.127049 1753507 finetune.py:68] layer 8_up @ epoch 0 new loss 0.00014070338511373848 old loss 0.00014358587213791907 BETTER
I0311 21:59:31.659948 1753739 finetune.py:68] layer 10_o @ epoch 2 new loss 0.00016026926459744573 old loss 0.0001645442534936592 BETTER
I0311 21:59:34.480514 1753623 finetune.py:45] layer 9_up initial loss 0.00017667868814896792
I0311 21:59:37.148957 1753855 finetune.py:68] layer 11_o @ epoch 0 new loss 0.00017661269521340728 old loss 0.00018741875828709453 BETTER
I0311 21:59:55.826810 1753507 finetune.py:68] layer 8_up @ epoch 1 new loss 0.00013882596977055073 old loss 0.00014070338511373848 BETTER
I0311 22:00:03.325931 1753739 finetune.py:68] layer 10_o @ epoch 3 new loss 0.00015687975974287838 old loss 0.00016026926459744573 BETTER
I0311 22:00:03.501631 1753623 finetune.py:68] layer 9_up @ epoch 0 new loss 0.00017307247617281973 old loss 0.00017667868814896792 BETTER
I0311 22:00:08.143036 1753855 finetune.py:68] layer 11_o @ epoch 1 new loss 0.00017092768393922597 old loss 0.00017661269521340728 BETTER
I0311 22:00:27.528760 1753507 finetune.py:68] layer 8_up @ epoch 2 new loss 0.00013727181067224592 old loss 0.00013882596977055073 BETTER
I0311 22:00:33.393855 1753623 finetune.py:68] layer 9_up @ epoch 1 new loss 0.0001707402989268303 old loss 0.00017307247617281973 BETTER
I0311 22:00:35.092007 1753739 finetune.py:68] layer 10_o @ epoch 4 new loss 0.00015412623179145157 old loss 0.00015687975974287838 BETTER
I0311 22:00:39.118318 1753855 finetune.py:68] layer 11_o @ epoch 2 new loss 0.00016688504547346383 old loss 0.00017092768393922597 BETTER
I0311 22:00:50.393251 1753739 finetune.py:45] layer 10_up initial loss 0.0002267247618874535
I0311 22:00:59.357511 1753507 finetune.py:68] layer 8_up @ epoch 3 new loss 0.00013593529001809657 old loss 0.00013727181067224592 BETTER
I0311 22:01:03.377565 1753623 finetune.py:68] layer 9_up @ epoch 2 new loss 0.00016879035683814436 old loss 0.0001707402989268303 BETTER
I0311 22:01:10.250629 1753855 finetune.py:68] layer 11_o @ epoch 3 new loss 0.00016370283265132457 old loss 0.00016688504547346383 BETTER
I0311 22:01:19.498466 1753739 finetune.py:68] layer 10_up @ epoch 0 new loss 0.00022196541249286383 old loss 0.0002267247618874535 BETTER
I0311 22:01:31.202980 1753507 finetune.py:68] layer 8_up @ epoch 4 new loss 0.0001347635843558237 old loss 0.00013593529001809657 BETTER
I0311 22:01:33.254722 1753623 finetune.py:68] layer 9_up @ epoch 3 new loss 0.0001671150530455634 old loss 0.00016879035683814436 BETTER
I0311 22:01:41.257809 1753855 finetune.py:68] layer 11_o @ epoch 4 new loss 0.00016114373283926398 old loss 0.00016370283265132457 BETTER
I0311 22:01:46.754651 1753507 finetune.py:45] layer 8_gate initial loss 0.00016822791076265275
I0311 22:01:49.328636 1753739 finetune.py:68] layer 10_up @ epoch 1 new loss 0.0002188141515944153 old loss 0.00022196541249286383 BETTER
I0311 22:01:56.883559 1753855 finetune.py:45] layer 11_up initial loss 0.0002433538465993479
I0311 22:02:03.102093 1753623 finetune.py:68] layer 9_up @ epoch 4 new loss 0.00016564731777179986 old loss 0.0001671150530455634 BETTER
I0311 22:02:16.039857 1753507 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.00016664968279656023 old loss 0.00016822791076265275 BETTER
I0311 22:02:18.969160 1753623 finetune.py:45] layer 9_gate initial loss 0.0002060789556708187
I0311 22:02:19.593046 1753739 finetune.py:68] layer 10_up @ epoch 2 new loss 0.00021619339531753212 old loss 0.0002188141515944153 BETTER
I0311 22:02:25.723812 1753855 finetune.py:68] layer 11_up @ epoch 0 new loss 0.00023857496853452176 old loss 0.0002433538465993479 BETTER
I0311 22:02:46.330643 1753507 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.00016552166198380291 old loss 0.00016664968279656023 BETTER
I0311 22:02:46.887935 1753623 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0002041274419752881 old loss 0.0002060789556708187 BETTER
I0311 22:02:49.890306 1753739 finetune.py:68] layer 10_up @ epoch 3 new loss 0.00021394660871010274 old loss 0.00021619339531753212 BETTER
I0311 22:02:55.334424 1753855 finetune.py:68] layer 11_up @ epoch 1 new loss 0.00023538453388027847 old loss 0.00023857496853452176 BETTER
I0311 22:03:15.440262 1753623 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.00020273223344702274 old loss 0.0002041274419752881 BETTER
I0311 22:03:16.379325 1753507 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.00016457142191939056 old loss 0.00016552166198380291 BETTER
I0311 22:03:20.022392 1753739 finetune.py:68] layer 10_up @ epoch 4 new loss 0.0002119564451277256 old loss 0.00021394660871010274 BETTER
I0311 22:03:24.967331 1753855 finetune.py:68] layer 11_up @ epoch 2 new loss 0.00023277140280697495 old loss 0.00023538453388027847 BETTER
I0311 22:03:35.644558 1753739 finetune.py:45] layer 10_gate initial loss 0.0002602481981739402
I0311 22:03:43.873804 1753623 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.000201535047381185 old loss 0.00020273223344702274 BETTER
I0311 22:03:46.430588 1753507 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.00016372415120713413 old loss 0.00016457142191939056 BETTER
I0311 22:03:54.578022 1753855 finetune.py:68] layer 11_up @ epoch 3 new loss 0.00023052476171869785 old loss 0.00023277140280697495 BETTER
I0311 22:04:03.005424 1753739 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.00025772571098059416 old loss 0.0002602481981739402 BETTER
I0311 22:04:12.120394 1753623 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.00020047910220455378 old loss 0.000201535047381185 BETTER
I0311 22:04:16.473346 1753507 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.00016296765534207225 old loss 0.00016372415120713413 BETTER
I0311 22:04:24.090433 1753855 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00022854683629702777 old loss 0.00023052476171869785 BETTER
I0311 22:04:31.138435 1753739 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.00025589720462448895 old loss 0.00025772571098059416 BETTER
I0311 22:04:33.165191 1753507 finetune.py:45] layer 8_down initial loss 0.0002490784099791199
I0311 22:04:39.940442 1753855 finetune.py:45] layer 11_gate initial loss 0.0002835450286511332
I0311 22:04:40.655145 1753623 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0001995399798033759 old loss 0.00020047910220455378 BETTER
I0311 22:04:56.722361 1753623 finetune.py:45] layer 9_down initial loss 0.000300370913464576
I0311 22:04:59.229183 1753739 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.00025433494010940194 old loss 0.00025589720462448895 BETTER
I0311 22:05:00.611931 1753507 finetune.py:68] layer 8_down @ epoch 0 new loss 0.0002490458427928388 old loss 0.0002490784099791199 BETTER
I0311 22:05:07.237211 1753855 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.000280969514278695 old loss 0.0002835450286511332 BETTER
I0311 22:05:22.981045 1753623 finetune.py:68] layer 9_down @ epoch 0 new loss 0.0003003347374033183 old loss 0.000300370913464576 BETTER
I0311 22:05:27.829133 1753739 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.0002529453777242452 old loss 0.00025433494010940194 BETTER
I0311 22:05:29.192601 1753507 finetune.py:68] layer 8_down @ epoch 1 new loss 0.00024902872974053025 old loss 0.0002490458427928388 BETTER
I0311 22:05:35.307632 1753855 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.00027912092627957463 old loss 0.000280969514278695 BETTER
I0311 22:05:49.942156 1753623 finetune.py:68] layer 9_down @ epoch 1 new loss 0.0003003148885909468 old loss 0.0003003347374033183 BETTER
I0311 22:05:56.295322 1753739 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.0002517049724701792 old loss 0.0002529453777242452 BETTER
I0311 22:05:57.679647 1753507 finetune.py:68] layer 8_down @ epoch 2 new loss 0.00024901836877688766 old loss 0.00024902872974053025 BETTER
I0311 22:06:03.480051 1753855 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.00027752743335440755 old loss 0.00027912092627957463 BETTER
I0311 22:06:12.676739 1753739 finetune.py:45] layer 10_down initial loss 0.0003692166937980801
I0311 22:06:16.808834 1753623 finetune.py:68] layer 9_down @ epoch 2 new loss 0.0003003019082825631 old loss 0.0003003148885909468 BETTER
I0311 22:06:26.214068 1753507 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0002490110055077821 old loss 0.00024901836877688766 BETTER
I0311 22:06:31.690106 1753855 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.0002761223295237869 old loss 0.00027752743335440755 BETTER
I0311 22:06:38.792564 1753739 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0003691650344990194 old loss 0.0003692166937980801 BETTER
I0311 22:06:43.720740 1753623 finetune.py:68] layer 9_down @ epoch 3 new loss 0.0003002938465215266 old loss 0.0003003019082825631 BETTER
I0311 22:06:54.773720 1753507 finetune.py:68] layer 8_down @ epoch 4 new loss 0.00024900579592213035 old loss 0.0002490110055077821 BETTER
8_v proxy err 0.010533856227993965 tr(WHW.T) 530.9967041015625
8_q proxy err 0.0008653152617625892 tr(WHW.T) 7231.1416015625
8_k proxy err 0.0006085396162234247 tr(WHW.T) 10647.7646484375
8_o proxy err 0.015303034335374832 tr(WHW.T) 20.21820068359375
8_up proxy err 0.007443272043019533 tr(WHW.T) 866.1869506835938
8_gate proxy err 0.003337270114570856 tr(WHW.T) 1971.3673095703125
8_down proxy err 0.009709796868264675 tr(WHW.T) 37.3098030090332
I0311 22:06:59.961721 1753855 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.00027487476472742856 old loss 0.0002761223295237869 BETTER
I0311 22:07:05.991808 1753739 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0003691346209961921 old loss 0.0003691650344990194 BETTER
I0311 22:07:11.011754 1753623 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00030028936453163624 old loss 0.0003002938465215266 BETTER
9_v proxy err 0.010408103466033936 tr(WHW.T) 565.0663452148438
9_q proxy err 0.0009382663411088288 tr(WHW.T) 6973.630859375
9_k proxy err 0.0006214471650309861 tr(WHW.T) 10997.6728515625
9_o proxy err 0.015674836933612823 tr(WHW.T) 25.741069793701172
9_up proxy err 0.007153448183089495 tr(WHW.T) 970.6409912109375
9_gate proxy err 0.00331865344196558 tr(WHW.T) 2132.7890625
9_down proxy err 0.009845948778092861 tr(WHW.T) 43.127628326416016
I0311 22:07:16.546055 1753855 finetune.py:45] layer 11_down initial loss 0.00040341648855246603
I0311 22:07:33.104068 1753739 finetune.py:68] layer 10_down @ epoch 2 new loss 0.0003691171878017485 old loss 0.0003691346209961921 BETTER
I0311 22:07:41.929381 1753855 finetune.py:68] layer 11_down @ epoch 0 new loss 0.0004033613367937505 old loss 0.00040341648855246603 BETTER
I0311 22:08:00.075710 1753739 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0003691056917887181 old loss 0.0003691171878017485 BETTER
I0311 22:08:08.181043 1753855 finetune.py:68] layer 11_down @ epoch 1 new loss 0.00040333098149858415 old loss 0.0004033613367937505 BETTER
I0311 22:08:27.020486 1753739 finetune.py:68] layer 10_down @ epoch 4 new loss 0.00036909914342686534 old loss 0.0003691056917887181 BETTER
10_v proxy err 0.010452589951455593 tr(WHW.T) 578.807373046875
10_q proxy err 0.0009737907676026225 tr(WHW.T) 6919.24462890625
10_k proxy err 0.000636542565189302 tr(WHW.T) 11007.6201171875
10_o proxy err 0.016298893839120865 tr(WHW.T) 35.34529495239258
10_up proxy err 0.006735046859830618 tr(WHW.T) 1080.0594482421875
10_gate proxy err 0.0032692851964384317 tr(WHW.T) 2263.22021484375
10_down proxy err 0.009403209201991558 tr(WHW.T) 52.52419662475586
I0311 22:08:29.086556 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 12 in 73.21267890930176s
I0311 22:08:32.352650 1753971 config.py:54] PyTorch version 2.1.1 available.
I0311 22:08:33.482806 1752459 quantize_finetune_llama.py:183] layer 13 gpu 1
I0311 22:08:33.552488 1753971 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 22:08:34.527939 1753855 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0004033131990581751 old loss 0.00040333098149858415 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:08:42.074218 1753971 finetune.py:45] layer 12_v initial loss 9.042702004080638e-05
I0311 22:09:01.001278 1753855 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0004033008881378919 old loss 0.0004033131990581751 BETTER
I0311 22:09:15.136054 1753971 finetune.py:68] layer 12_v @ epoch 0 new loss 7.12601249688305e-05 old loss 9.042702004080638e-05 BETTER
I0311 22:09:27.317955 1753855 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0004032916040159762 old loss 0.0004033008881378919 BETTER
11_v proxy err 0.010340049862861633 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0011547523317858577 tr(WHW.T) 7030.65234375
11_k proxy err 0.0007949988939799368 tr(WHW.T) 10524.9541015625
11_o proxy err 0.01625419408082962 tr(WHW.T) 36.886043548583984
11_up proxy err 0.006911017466336489 tr(WHW.T) 1139.6953125
11_gate proxy err 0.003344671567901969 tr(WHW.T) 2394.01953125
11_down proxy err 0.009585099294781685 tr(WHW.T) 56.31260299682617
I0311 22:09:44.774416 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 13 in 70.81946277618408s
I0311 22:09:48.044543 1754087 config.py:54] PyTorch version 2.1.1 available.
I0311 22:09:49.261850 1752459 quantize_finetune_llama.py:183] layer 14 gpu 2
I0311 22:09:49.333181 1754087 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 22:09:49.570282 1753971 finetune.py:68] layer 12_v @ epoch 1 new loss 6.686997221549973e-05 old loss 7.12601249688305e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:09:57.585578 1754087 finetune.py:45] layer 13_v initial loss 9.712225437397137e-05
I0311 22:10:24.293816 1753971 finetune.py:68] layer 12_v @ epoch 2 new loss 6.422362639568746e-05 old loss 6.686997221549973e-05 BETTER
I0311 22:10:28.884788 1754087 finetune.py:68] layer 13_v @ epoch 0 new loss 7.646386802662164e-05 old loss 9.712225437397137e-05 BETTER
I0311 22:10:57.770503 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 14 in 68.06195759773254s
I0311 22:10:59.076564 1753971 finetune.py:68] layer 12_v @ epoch 3 new loss 6.232360465219244e-05 old loss 6.422362639568746e-05 BETTER
I0311 22:11:01.000898 1754203 config.py:54] PyTorch version 2.1.1 available.
I0311 22:11:01.197494 1754087 finetune.py:68] layer 13_v @ epoch 1 new loss 7.181998080341145e-05 old loss 7.646386802662164e-05 BETTER
I0311 22:11:01.965655 1752459 quantize_finetune_llama.py:183] layer 15 gpu 3
I0311 22:11:02.040673 1754203 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:11:10.125385 1754203 finetune.py:45] layer 14_v initial loss 0.0001268282503588125
I0311 22:11:33.787721 1754087 finetune.py:68] layer 13_v @ epoch 2 new loss 6.90624801791273e-05 old loss 7.181998080341145e-05 BETTER
I0311 22:11:34.051757 1753971 finetune.py:68] layer 12_v @ epoch 4 new loss 6.0883303376613185e-05 old loss 6.232360465219244e-05 BETTER
I0311 22:11:41.560821 1754203 finetune.py:68] layer 14_v @ epoch 0 new loss 0.0001007931205094792 old loss 0.0001268282503588125 BETTER
I0311 22:11:43.460936 1753971 finetune.py:45] layer 12_q initial loss 7.452786667272449e-05
I0311 22:12:06.718592 1754087 finetune.py:68] layer 13_v @ epoch 3 new loss 6.702675455017015e-05 old loss 6.90624801791273e-05 BETTER
I0311 22:12:12.369704 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 15 in 69.90533590316772s
I0311 22:12:14.174033 1754203 finetune.py:68] layer 14_v @ epoch 1 new loss 9.404690354131162e-05 old loss 0.0001007931205094792 BETTER
I0311 22:12:15.733427 1754319 config.py:54] PyTorch version 2.1.1 available.
I0311 22:12:16.746845 1752459 quantize_finetune_llama.py:183] layer 16 gpu 0
I0311 22:12:16.815352 1754319 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 22:12:17.125622 1753971 finetune.py:68] layer 12_q @ epoch 0 new loss 7.084695243975148e-05 old loss 7.452786667272449e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:12:24.769736 1754319 finetune.py:45] layer 15_v initial loss 0.00012639274063985795
I0311 22:12:39.488692 1754087 finetune.py:68] layer 13_v @ epoch 4 new loss 6.55573676340282e-05 old loss 6.702675455017015e-05 BETTER
I0311 22:12:46.927682 1754203 finetune.py:68] layer 14_v @ epoch 2 new loss 8.993915980681777e-05 old loss 9.404690354131162e-05 BETTER
I0311 22:12:48.665884 1754087 finetune.py:45] layer 13_q initial loss 7.924520468804985e-05
I0311 22:12:51.555085 1753971 finetune.py:68] layer 12_q @ epoch 1 new loss 6.917830614838749e-05 old loss 7.084695243975148e-05 BETTER
I0311 22:12:55.760727 1754319 finetune.py:68] layer 15_v @ epoch 0 new loss 9.869001223705709e-05 old loss 0.00012639274063985795 BETTER
I0311 22:13:19.868822 1754203 finetune.py:68] layer 14_v @ epoch 3 new loss 8.695018186699599e-05 old loss 8.993915980681777e-05 BETTER
I0311 22:13:20.572348 1754087 finetune.py:68] layer 13_q @ epoch 0 new loss 7.533732423326e-05 old loss 7.924520468804985e-05 BETTER
I0311 22:13:26.222979 1753971 finetune.py:68] layer 12_q @ epoch 2 new loss 6.790774205001071e-05 old loss 6.917830614838749e-05 BETTER
I0311 22:13:27.976150 1754319 finetune.py:68] layer 15_v @ epoch 1 new loss 9.237446647603065e-05 old loss 9.869001223705709e-05 BETTER
I0311 22:13:52.965084 1754203 finetune.py:68] layer 14_v @ epoch 4 new loss 8.463836275041103e-05 old loss 8.695018186699599e-05 BETTER
I0311 22:13:52.966964 1754087 finetune.py:68] layer 13_q @ epoch 1 new loss 7.350507803494111e-05 old loss 7.533732423326e-05 BETTER
I0311 22:14:00.329916 1754319 finetune.py:68] layer 15_v @ epoch 2 new loss 8.845208503771573e-05 old loss 9.237446647603065e-05 BETTER
I0311 22:14:01.071139 1753971 finetune.py:68] layer 12_q @ epoch 3 new loss 6.687277345918119e-05 old loss 6.790774205001071e-05 BETTER
I0311 22:14:02.562750 1754203 finetune.py:45] layer 14_q initial loss 0.00010109796858159825
I0311 22:14:25.734560 1754087 finetune.py:68] layer 13_q @ epoch 2 new loss 7.21453907317482e-05 old loss 7.350507803494111e-05 BETTER
I0311 22:14:33.011847 1754319 finetune.py:68] layer 15_v @ epoch 3 new loss 8.554539817851037e-05 old loss 8.845208503771573e-05 BETTER
I0311 22:14:34.431775 1754203 finetune.py:68] layer 14_q @ epoch 0 new loss 9.604826482245699e-05 old loss 0.00010109796858159825 BETTER
I0311 22:14:35.824095 1753971 finetune.py:68] layer 12_q @ epoch 4 new loss 6.597964238608256e-05 old loss 6.687277345918119e-05 BETTER
I0311 22:14:45.158117 1753971 finetune.py:45] layer 12_k initial loss 7.594619819428772e-05
I0311 22:14:58.027519 1754087 finetune.py:68] layer 13_q @ epoch 3 new loss 7.098348578438163e-05 old loss 7.21453907317482e-05 BETTER
I0311 22:15:05.781244 1754319 finetune.py:68] layer 15_v @ epoch 4 new loss 8.33314552437514e-05 old loss 8.554539817851037e-05 BETTER
I0311 22:15:07.102339 1754203 finetune.py:68] layer 14_q @ epoch 1 new loss 9.358713577967137e-05 old loss 9.604826482245699e-05 BETTER
I0311 22:15:14.972618 1754319 finetune.py:45] layer 15_q initial loss 0.00010125605331268162
I0311 22:15:18.269997 1753971 finetune.py:68] layer 12_k @ epoch 0 new loss 7.393940904876217e-05 old loss 7.594619819428772e-05 BETTER
I0311 22:15:30.618379 1754087 finetune.py:68] layer 13_q @ epoch 4 new loss 7.005981024121866e-05 old loss 7.098348578438163e-05 BETTER
I0311 22:15:39.666877 1754203 finetune.py:68] layer 14_q @ epoch 2 new loss 9.165332448901609e-05 old loss 9.358713577967137e-05 BETTER
I0311 22:15:39.940391 1754087 finetune.py:45] layer 13_k initial loss 7.99488989287056e-05
I0311 22:15:46.441517 1754319 finetune.py:68] layer 15_q @ epoch 0 new loss 9.608419350115582e-05 old loss 0.00010125605331268162 BETTER
I0311 22:15:52.185875 1753971 finetune.py:68] layer 12_k @ epoch 1 new loss 7.309302600333467e-05 old loss 7.393940904876217e-05 BETTER
I0311 22:16:11.342666 1754087 finetune.py:68] layer 13_k @ epoch 0 new loss 7.825164357200265e-05 old loss 7.99488989287056e-05 BETTER
I0311 22:16:12.403691 1754203 finetune.py:68] layer 14_q @ epoch 3 new loss 9.0065885160584e-05 old loss 9.165332448901609e-05 BETTER
I0311 22:16:18.432059 1754319 finetune.py:68] layer 15_q @ epoch 1 new loss 9.349895844934508e-05 old loss 9.608419350115582e-05 BETTER
I0311 22:16:26.330529 1753971 finetune.py:68] layer 12_k @ epoch 2 new loss 7.242269202833995e-05 old loss 7.309302600333467e-05 BETTER
I0311 22:16:43.534950 1754087 finetune.py:68] layer 13_k @ epoch 1 new loss 7.737903069937602e-05 old loss 7.825164357200265e-05 BETTER
I0311 22:16:45.074348 1754203 finetune.py:68] layer 14_q @ epoch 4 new loss 8.874583727447316e-05 old loss 9.0065885160584e-05 BETTER
I0311 22:16:50.441893 1754319 finetune.py:68] layer 15_q @ epoch 2 new loss 9.152397979050875e-05 old loss 9.349895844934508e-05 BETTER
I0311 22:16:54.587055 1754203 finetune.py:45] layer 14_k initial loss 0.00010081089567393064
I0311 22:17:00.477865 1753971 finetune.py:68] layer 12_k @ epoch 3 new loss 7.183102570706978e-05 old loss 7.242269202833995e-05 BETTER
I0311 22:17:15.664095 1754087 finetune.py:68] layer 13_k @ epoch 2 new loss 7.668415491934866e-05 old loss 7.737903069937602e-05 BETTER
I0311 22:17:22.569931 1754319 finetune.py:68] layer 15_q @ epoch 3 new loss 8.991331560537219e-05 old loss 9.152397979050875e-05 BETTER
I0311 22:17:26.020687 1754203 finetune.py:68] layer 14_k @ epoch 0 new loss 9.865290485322475e-05 old loss 0.00010081089567393064 BETTER
I0311 22:17:34.675889 1753971 finetune.py:68] layer 12_k @ epoch 4 new loss 7.133172766771168e-05 old loss 7.183102570706978e-05 BETTER
I0311 22:17:44.237598 1753971 finetune.py:45] layer 12_o initial loss 0.00020915118511766195
I0311 22:17:47.783916 1754087 finetune.py:68] layer 13_k @ epoch 3 new loss 7.60707407607697e-05 old loss 7.668415491934866e-05 BETTER
I0311 22:17:54.763778 1754319 finetune.py:68] layer 15_q @ epoch 4 new loss 8.856016211211681e-05 old loss 8.991331560537219e-05 BETTER
I0311 22:17:58.112082 1754203 finetune.py:68] layer 14_k @ epoch 1 new loss 9.745380521053448e-05 old loss 9.865290485322475e-05 BETTER
I0311 22:18:04.135480 1754319 finetune.py:45] layer 15_k initial loss 0.00010206435399595648
I0311 22:18:16.957385 1753971 finetune.py:68] layer 12_o @ epoch 0 new loss 0.00019752717344090343 old loss 0.00020915118511766195 BETTER
I0311 22:18:19.943016 1754087 finetune.py:68] layer 13_k @ epoch 4 new loss 7.554014155175537e-05 old loss 7.60707407607697e-05 BETTER
I0311 22:18:29.200400 1754087 finetune.py:45] layer 13_o initial loss 0.00021736939379479736
I0311 22:18:30.219929 1754203 finetune.py:68] layer 14_k @ epoch 2 new loss 9.650695574237034e-05 old loss 9.745380521053448e-05 BETTER
I0311 22:18:35.144087 1754319 finetune.py:68] layer 15_k @ epoch 0 new loss 9.95176742435433e-05 old loss 0.00010206435399595648 BETTER
I0311 22:18:50.586771 1753971 finetune.py:68] layer 12_o @ epoch 1 new loss 0.0001910695427795872 old loss 0.00019752717344090343 BETTER
I0311 22:18:59.970469 1754087 finetune.py:68] layer 13_o @ epoch 0 new loss 0.00020398623018991202 old loss 0.00021736939379479736 BETTER
I0311 22:19:02.231499 1754203 finetune.py:68] layer 14_k @ epoch 3 new loss 9.567059896653518e-05 old loss 9.650695574237034e-05 BETTER
I0311 22:19:06.820101 1754319 finetune.py:68] layer 15_k @ epoch 1 new loss 9.833298827288672e-05 old loss 9.95176742435433e-05 BETTER
I0311 22:19:24.140745 1753971 finetune.py:68] layer 12_o @ epoch 2 new loss 0.00018646598618943244 old loss 0.0001910695427795872 BETTER
I0311 22:19:31.378145 1754087 finetune.py:68] layer 13_o @ epoch 1 new loss 0.00019689681357704103 old loss 0.00020398623018991202 BETTER
I0311 22:19:34.345704 1754203 finetune.py:68] layer 14_k @ epoch 4 new loss 9.494403639109805e-05 old loss 9.567059896653518e-05 BETTER
I0311 22:19:38.528705 1754319 finetune.py:68] layer 15_k @ epoch 2 new loss 9.735852654557675e-05 old loss 9.833298827288672e-05 BETTER
I0311 22:19:43.855254 1754203 finetune.py:45] layer 14_o initial loss 0.00027912540826946497
I0311 22:19:57.740991 1753971 finetune.py:68] layer 12_o @ epoch 3 new loss 0.00018284448015037924 old loss 0.00018646598618943244 BETTER
I0311 22:20:02.949866 1754087 finetune.py:68] layer 13_o @ epoch 2 new loss 0.00019187381258234382 old loss 0.00019689681357704103 BETTER
I0311 22:20:10.297912 1754319 finetune.py:68] layer 15_k @ epoch 3 new loss 9.652947483118623e-05 old loss 9.735852654557675e-05 BETTER
I0311 22:20:14.873682 1754203 finetune.py:68] layer 14_o @ epoch 0 new loss 0.00026321690529584885 old loss 0.00027912540826946497 BETTER
I0311 22:20:31.323226 1753971 finetune.py:68] layer 12_o @ epoch 4 new loss 0.00017991403001360595 old loss 0.00018284448015037924 BETTER
I0311 22:20:34.480914 1754087 finetune.py:68] layer 13_o @ epoch 3 new loss 0.00018801946134772152 old loss 0.00019187381258234382 BETTER
I0311 22:20:42.106017 1754319 finetune.py:68] layer 15_k @ epoch 4 new loss 9.581941412761807e-05 old loss 9.652947483118623e-05 BETTER
I0311 22:20:46.673957 1754203 finetune.py:68] layer 14_o @ epoch 1 new loss 0.00025455461582168937 old loss 0.00026321690529584885 BETTER
I0311 22:20:46.960730 1753971 finetune.py:45] layer 12_up initial loss 0.00027213897556066513
I0311 22:20:51.718519 1754319 finetune.py:45] layer 15_o initial loss 0.0002773031883407384
I0311 22:21:06.295537 1754087 finetune.py:68] layer 13_o @ epoch 4 new loss 0.00018489691137801856 old loss 0.00018801946134772152 BETTER
I0311 22:21:17.642867 1753971 finetune.py:68] layer 12_up @ epoch 0 new loss 0.0002666957152541727 old loss 0.00027213897556066513 BETTER
I0311 22:21:18.605115 1754203 finetune.py:68] layer 14_o @ epoch 2 new loss 0.00024839432444423437 old loss 0.00025455461582168937 BETTER
I0311 22:21:21.701324 1754087 finetune.py:45] layer 13_up initial loss 0.00029436597833409905
I0311 22:21:22.271143 1754319 finetune.py:68] layer 15_o @ epoch 0 new loss 0.00025889155222103 old loss 0.0002773031883407384 BETTER
I0311 22:21:49.511141 1753971 finetune.py:68] layer 12_up @ epoch 1 new loss 0.00026305249775759876 old loss 0.0002666957152541727 BETTER
I0311 22:21:50.699305 1754203 finetune.py:68] layer 14_o @ epoch 3 new loss 0.00024359425879083574 old loss 0.00024839432444423437 BETTER
I0311 22:21:50.977433 1754087 finetune.py:68] layer 13_up @ epoch 0 new loss 0.0002876025973819196 old loss 0.00029436597833409905 BETTER
I0311 22:21:53.342502 1754319 finetune.py:68] layer 15_o @ epoch 1 new loss 0.00024972320534288883 old loss 0.00025889155222103 BETTER
I0311 22:22:20.920647 1754087 finetune.py:68] layer 13_up @ epoch 1 new loss 0.00028317642863839865 old loss 0.0002876025973819196 BETTER
I0311 22:22:21.496746 1753971 finetune.py:68] layer 12_up @ epoch 2 new loss 0.00026005852851085365 old loss 0.00026305249775759876 BETTER
I0311 22:22:22.796118 1754203 finetune.py:68] layer 14_o @ epoch 4 new loss 0.0002396775089437142 old loss 0.00024359425879083574 BETTER
I0311 22:22:25.074863 1754319 finetune.py:68] layer 15_o @ epoch 2 new loss 0.00024332063912879676 old loss 0.00024972320534288883 BETTER
I0311 22:22:38.533125 1754203 finetune.py:45] layer 14_up initial loss 0.00036235523293726146
I0311 22:22:51.249740 1754087 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0002796101616695523 old loss 0.00028317642863839865 BETTER
I0311 22:22:53.548226 1753971 finetune.py:68] layer 12_up @ epoch 3 new loss 0.00025749538326635957 old loss 0.00026005852851085365 BETTER
I0311 22:22:55.984688 1754319 finetune.py:68] layer 15_o @ epoch 3 new loss 0.00023844733368605375 old loss 0.00024332063912879676 BETTER
I0311 22:23:07.569139 1754203 finetune.py:68] layer 14_up @ epoch 0 new loss 0.00035482991370372474 old loss 0.00036235523293726146 BETTER
I0311 22:23:21.226534 1754087 finetune.py:68] layer 13_up @ epoch 3 new loss 0.0002766056277323514 old loss 0.0002796101616695523 BETTER
I0311 22:23:25.246779 1753971 finetune.py:68] layer 12_up @ epoch 4 new loss 0.0002552578807808459 old loss 0.00025749538326635957 BETTER
I0311 22:23:27.055437 1754319 finetune.py:68] layer 15_o @ epoch 4 new loss 0.00023455116024706513 old loss 0.00023844733368605375 BETTER
I0311 22:23:37.670606 1754203 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0003498969308566302 old loss 0.00035482991370372474 BETTER
I0311 22:23:41.292854 1753971 finetune.py:45] layer 12_gate initial loss 0.00031959410989657044
I0311 22:23:42.726024 1754319 finetune.py:45] layer 15_up initial loss 0.0003837805998045951
I0311 22:23:51.176342 1754087 finetune.py:68] layer 13_up @ epoch 4 new loss 0.00027402082923799753 old loss 0.0002766056277323514 BETTER
I0311 22:24:07.045761 1754087 finetune.py:45] layer 13_gate initial loss 0.00035167156602256
I0311 22:24:08.099912 1754203 finetune.py:68] layer 14_up @ epoch 2 new loss 0.00034591398434713483 old loss 0.0003498969308566302 BETTER
I0311 22:24:10.869149 1753971 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.00031665817368775606 old loss 0.00031959410989657044 BETTER
I0311 22:24:11.739689 1754319 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0003739538951776922 old loss 0.0003837805998045951 BETTER
I0311 22:24:34.733899 1754087 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.00034810451325029135 old loss 0.00035167156602256 BETTER
I0311 22:24:38.535670 1754203 finetune.py:68] layer 14_up @ epoch 3 new loss 0.00034252728801220655 old loss 0.00034591398434713483 BETTER
I0311 22:24:40.735582 1753971 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0003145131631754339 old loss 0.00031665817368775606 BETTER
I0311 22:24:41.449683 1754319 finetune.py:68] layer 15_up @ epoch 1 new loss 0.0003680628433357924 old loss 0.0003739538951776922 BETTER
I0311 22:25:03.191688 1754087 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.00034551514545455575 old loss 0.00034810451325029135 BETTER
I0311 22:25:08.885979 1754203 finetune.py:68] layer 14_up @ epoch 4 new loss 0.00033957831328734756 old loss 0.00034252728801220655 BETTER
I0311 22:25:10.840351 1753971 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.00031269347527995706 old loss 0.0003145131631754339 BETTER
I0311 22:25:11.279026 1754319 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0003634266904555261 old loss 0.0003680628433357924 BETTER
I0311 22:25:24.569741 1754203 finetune.py:45] layer 14_gate initial loss 0.0004310038057155907
I0311 22:25:31.304960 1754087 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0003433500824030489 old loss 0.00034551514545455575 BETTER
I0311 22:25:40.729464 1753971 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.0003110976831521839 old loss 0.00031269347527995706 BETTER
I0311 22:25:40.812626 1754319 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0003595108282752335 old loss 0.0003634266904555261 BETTER
I0311 22:25:52.155977 1754203 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0004269234195817262 old loss 0.0004310038057155907 BETTER
I0311 22:25:59.310318 1754087 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.00034145600511692464 old loss 0.0003433500824030489 BETTER
I0311 22:26:10.685843 1754319 finetune.py:68] layer 15_up @ epoch 4 new loss 0.00035621202550828457 old loss 0.0003595108282752335 BETTER
I0311 22:26:10.959600 1753971 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.0003096800355706364 old loss 0.0003110976831521839 BETTER
I0311 22:26:20.645005 1754203 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0004239631234668195 old loss 0.0004269234195817262 BETTER
I0311 22:26:26.241072 1754319 finetune.py:45] layer 15_gate initial loss 0.00046877775457687676
I0311 22:26:27.436571 1753971 finetune.py:45] layer 12_down initial loss 0.0004554718907456845
I0311 22:26:27.858091 1754087 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0003397903928998858 old loss 0.00034145600511692464 BETTER
I0311 22:26:43.539097 1754087 finetune.py:45] layer 13_down initial loss 0.0005170844378881156
I0311 22:26:48.881197 1754203 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.00042150169610977173 old loss 0.0004239631234668195 BETTER
I0311 22:26:53.696944 1754319 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.00046365780872292817 old loss 0.00046877775457687676 BETTER
I0311 22:26:55.032535 1753971 finetune.py:68] layer 12_down @ epoch 0 new loss 0.00045540588325820863 old loss 0.0004554718907456845 BETTER
I0311 22:27:09.731520 1754087 finetune.py:68] layer 13_down @ epoch 0 new loss 0.0005170130170881748 old loss 0.0005170844378881156 BETTER
I0311 22:27:17.112770 1754203 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.00041936495108529925 old loss 0.00042150169610977173 BETTER
I0311 22:27:21.901758 1754319 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.00046011884114705026 old loss 0.00046365780872292817 BETTER
I0311 22:27:23.613346 1753971 finetune.py:68] layer 12_down @ epoch 1 new loss 0.00045536638936027884 old loss 0.00045540588325820863 BETTER
I0311 22:27:36.784386 1754087 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0005169719806872308 old loss 0.0005170130170881748 BETTER
I0311 22:27:45.523502 1754203 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0004174496862106025 old loss 0.00041936495108529925 BETTER
I0311 22:27:50.134112 1754319 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.00045720738125965 old loss 0.00046011884114705026 BETTER
I0311 22:27:52.049869 1753971 finetune.py:68] layer 12_down @ epoch 2 new loss 0.0004553449689410627 old loss 0.00045536638936027884 BETTER
I0311 22:28:01.313202 1754203 finetune.py:45] layer 14_down initial loss 0.0006227703997865319
I0311 22:28:03.737915 1754087 finetune.py:68] layer 13_down @ epoch 2 new loss 0.000516947649884969 old loss 0.0005169719806872308 BETTER
I0311 22:28:19.018834 1754319 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0004546934796962887 old loss 0.00045720738125965 BETTER
I0311 22:28:21.051028 1753971 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0004553291364572942 old loss 0.0004553449689410627 BETTER
I0311 22:28:27.438823 1754203 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0006226769182831049 old loss 0.0006227703997865319 BETTER
I0311 22:28:30.827418 1754087 finetune.py:68] layer 13_down @ epoch 3 new loss 0.0005169312353245914 old loss 0.000516947649884969 BETTER
I0311 22:28:48.386569 1754319 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.00045249960385262966 old loss 0.0004546934796962887 BETTER
I0311 22:28:50.464948 1753971 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0004553155740723014 old loss 0.0004553291364572942 BETTER
12_v proxy err 0.010678811930119991 tr(WHW.T) 703.318603515625
12_q proxy err 0.0011639377335086465 tr(WHW.T) 7049.34375
12_k proxy err 0.0007812379626557231 tr(WHW.T) 10904.31640625
12_o proxy err 0.01702539436519146 tr(WHW.T) 39.50798034667969
12_up proxy err 0.006871879566460848 tr(WHW.T) 1228.5921630859375
12_gate proxy err 0.0035890177823603153 tr(WHW.T) 2385.12939453125
12_down proxy err 0.009498050436377525 tr(WHW.T) 64.39032745361328
I0311 22:28:54.721460 1754203 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0006226240657269955 old loss 0.0006226769182831049 BETTER
I0311 22:28:58.570739 1754087 finetune.py:68] layer 13_down @ epoch 4 new loss 0.0005169195937924087 old loss 0.0005169312353245914 BETTER
13_v proxy err 0.01093481108546257 tr(WHW.T) 714.5677490234375
13_q proxy err 0.0012123933993279934 tr(WHW.T) 6959.57177734375
13_k proxy err 0.0008293876308016479 tr(WHW.T) 10438.5107421875
13_o proxy err 0.015000908635556698 tr(WHW.T) 46.094573974609375
13_up proxy err 0.006597660947591066 tr(WHW.T) 1367.3070068359375
13_gate proxy err 0.00350775383412838 tr(WHW.T) 2603.2529296875
13_down proxy err 0.009362902492284775 tr(WHW.T) 79.6724853515625
I0311 22:29:05.892158 1754319 finetune.py:45] layer 15_down initial loss 0.0007088204729370773
I0311 22:29:22.133280 1754203 finetune.py:68] layer 14_down @ epoch 2 new loss 0.0006225926335901022 old loss 0.0006226240657269955 BETTER
I0311 22:29:31.280820 1754319 finetune.py:68] layer 15_down @ epoch 0 new loss 0.00070871104253456 old loss 0.0007088204729370773 BETTER
I0311 22:29:49.034900 1754203 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0006225707475095987 old loss 0.0006225926335901022 BETTER
I0311 22:29:57.751831 1754319 finetune.py:68] layer 15_down @ epoch 1 new loss 0.0007086521945893764 old loss 0.00070871104253456 BETTER
I0311 22:30:15.756636 1754203 finetune.py:68] layer 14_down @ epoch 4 new loss 0.0006225536926649511 old loss 0.0006225707475095987 BETTER
I0311 22:30:16.780337 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 16 in 73.48165440559387s
14_v proxy err 0.011586501263082027 tr(WHW.T) 706.1612548828125
14_q proxy err 0.0012486057821661234 tr(WHW.T) 7080.77880859375
14_k proxy err 0.0008104227599687874 tr(WHW.T) 11309.4130859375
14_o proxy err 0.0171138234436512 tr(WHW.T) 51.253021240234375
14_up proxy err 0.006746855564415455 tr(WHW.T) 1464.5159912109375
14_gate proxy err 0.0037164848763495684 tr(WHW.T) 2685.44873046875
14_down proxy err 0.009496085345745087 tr(WHW.T) 90.63443756103516
I0311 22:30:19.879139 1754435 config.py:54] PyTorch version 2.1.1 available.
I0311 22:30:20.936035 1752459 quantize_finetune_llama.py:183] layer 17 gpu 1
I0311 22:30:21.016735 1754435 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:30:24.459447 1754319 finetune.py:68] layer 15_down @ epoch 2 new loss 0.0007086140685714781 old loss 0.0007086521945893764 BETTER
I0311 22:30:29.542366 1754435 finetune.py:45] layer 16_v initial loss 0.00015668966807425022
I0311 22:30:51.156470 1754319 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0007085917168296874 old loss 0.0007086140685714781 BETTER
I0311 22:31:02.679688 1754435 finetune.py:68] layer 16_v @ epoch 0 new loss 0.00012521172175183892 old loss 0.00015668966807425022 BETTER
I0311 22:31:17.759534 1754319 finetune.py:68] layer 15_down @ epoch 4 new loss 0.0007085740799084306 old loss 0.0007085917168296874 BETTER
15_v proxy err 0.010372248478233814 tr(WHW.T) 762.7275390625
15_q proxy err 0.001179301063530147 tr(WHW.T) 7255.552734375
15_k proxy err 0.0007943396340124309 tr(WHW.T) 11084.541015625
15_o proxy err 0.014091207645833492 tr(WHW.T) 59.92552947998047
15_up proxy err 0.006548380944877863 tr(WHW.T) 1641.0767822265625
15_gate proxy err 0.0037304258439689875 tr(WHW.T) 2905.93408203125
15_down proxy err 0.009376593865454197 tr(WHW.T) 114.56544494628906
I0311 22:31:32.589600 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 17 in 71.20746755599976s
I0311 22:31:35.855197 1754551 config.py:54] PyTorch version 2.1.1 available.
I0311 22:31:37.037332 1752459 quantize_finetune_llama.py:183] layer 18 gpu 2
I0311 22:31:37.103479 1754551 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 22:31:37.130583 1754435 finetune.py:68] layer 16_v @ epoch 1 new loss 0.00011739946785382926 old loss 0.00012521172175183892 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:31:45.374695 1754551 finetune.py:45] layer 17_v initial loss 0.00013194914208725095
I0311 22:32:11.852993 1754435 finetune.py:68] layer 16_v @ epoch 2 new loss 0.00011246353096794337 old loss 0.00011739946785382926 BETTER
I0311 22:32:16.714687 1754551 finetune.py:68] layer 17_v @ epoch 0 new loss 0.00010638215462677181 old loss 0.00013194914208725095 BETTER
I0311 22:32:47.043850 1754435 finetune.py:68] layer 16_v @ epoch 3 new loss 0.00010887422831729054 old loss 0.00011246353096794337 BETTER
I0311 22:32:48.113423 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 18 in 70.73480367660522s
I0311 22:32:49.643744 1754551 finetune.py:68] layer 17_v @ epoch 1 new loss 0.00010004120849771425 old loss 0.00010638215462677181 BETTER
I0311 22:32:51.372471 1754667 config.py:54] PyTorch version 2.1.1 available.
I0311 22:32:52.379216 1752459 quantize_finetune_llama.py:183] layer 19 gpu 3
I0311 22:32:52.451506 1754667 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:33:00.473406 1754667 finetune.py:45] layer 18_v initial loss 0.0001296365080634132
I0311 22:33:22.175683 1754551 finetune.py:68] layer 17_v @ epoch 2 new loss 9.601407509762794e-05 old loss 0.00010004120849771425 BETTER
I0311 22:33:22.268635 1754435 finetune.py:68] layer 16_v @ epoch 4 new loss 0.00010610561002977192 old loss 0.00010887422831729054 BETTER
I0311 22:33:31.481902 1754435 finetune.py:45] layer 16_q initial loss 0.0001279237912967801
I0311 22:33:31.945113 1754667 finetune.py:68] layer 18_v @ epoch 0 new loss 0.00010539137292653322 old loss 0.0001296365080634132 BETTER
I0311 22:33:54.726922 1754551 finetune.py:68] layer 17_v @ epoch 3 new loss 9.307063010055572e-05 old loss 9.601407509762794e-05 BETTER
I0311 22:34:02.047312 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 19 in 69.23602676391602s
I0311 22:34:04.462179 1754667 finetune.py:68] layer 18_v @ epoch 1 new loss 9.988275996875018e-05 old loss 0.00010539137292653322 BETTER
I0311 22:34:05.091593 1754435 finetune.py:68] layer 16_q @ epoch 0 new loss 0.00012124549539294094 old loss 0.0001279237912967801 BETTER
I0311 22:34:05.399525 1754783 config.py:54] PyTorch version 2.1.1 available.
I0311 22:34:06.456206 1752459 quantize_finetune_llama.py:183] layer 20 gpu 0
I0311 22:34:06.533298 1754783 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:34:14.626036 1754783 finetune.py:45] layer 19_v initial loss 0.0001268625055672601
I0311 22:34:27.736390 1754551 finetune.py:68] layer 17_v @ epoch 4 new loss 9.087418584385887e-05 old loss 9.307063010055572e-05 BETTER
I0311 22:34:37.212861 1754551 finetune.py:45] layer 17_q initial loss 0.00011141892173327506
I0311 22:34:37.666131 1754667 finetune.py:68] layer 18_v @ epoch 2 new loss 9.645206591812894e-05 old loss 9.988275996875018e-05 BETTER
I0311 22:34:39.640660 1754435 finetune.py:68] layer 16_q @ epoch 1 new loss 0.0001181234692921862 old loss 0.00012124549539294094 BETTER
I0311 22:34:45.798814 1754783 finetune.py:68] layer 19_v @ epoch 0 new loss 0.00010375847341492772 old loss 0.0001268625055672601 BETTER
I0311 22:35:09.057179 1754551 finetune.py:68] layer 17_q @ epoch 0 new loss 0.00010563013347564265 old loss 0.00011141892173327506 BETTER
I0311 22:35:11.042253 1754667 finetune.py:68] layer 18_v @ epoch 3 new loss 9.389859769726172e-05 old loss 9.645206591812894e-05 BETTER
I0311 22:35:14.289055 1754435 finetune.py:68] layer 16_q @ epoch 2 new loss 0.00011575310054467991 old loss 0.0001181234692921862 BETTER
I0311 22:35:17.811682 1754783 finetune.py:68] layer 19_v @ epoch 1 new loss 9.859770216280594e-05 old loss 0.00010375847341492772 BETTER
I0311 22:35:41.803668 1754551 finetune.py:68] layer 17_q @ epoch 1 new loss 0.00010283901065122336 old loss 0.00010563013347564265 BETTER
I0311 22:35:44.091152 1754667 finetune.py:68] layer 18_v @ epoch 4 new loss 9.193269943352789e-05 old loss 9.389859769726172e-05 BETTER
I0311 22:35:49.060249 1754435 finetune.py:68] layer 16_q @ epoch 3 new loss 0.00011384685058146715 old loss 0.00011575310054467991 BETTER
I0311 22:35:50.160841 1754783 finetune.py:68] layer 19_v @ epoch 2 new loss 9.52665795921348e-05 old loss 9.859770216280594e-05 BETTER
I0311 22:35:53.888870 1754667 finetune.py:45] layer 18_q initial loss 0.00011949660256505013
I0311 22:36:14.163468 1754551 finetune.py:68] layer 17_q @ epoch 2 new loss 0.00010064923844765872 old loss 0.00010283901065122336 BETTER
I0311 22:36:22.731476 1754783 finetune.py:68] layer 19_v @ epoch 3 new loss 9.286406566388905e-05 old loss 9.52665795921348e-05 BETTER
I0311 22:36:23.927790 1754435 finetune.py:68] layer 16_q @ epoch 4 new loss 0.00011229347728658468 old loss 0.00011384685058146715 BETTER
I0311 22:36:25.661340 1754667 finetune.py:68] layer 18_q @ epoch 0 new loss 0.0001127006980823353 old loss 0.00011949660256505013 BETTER
I0311 22:36:33.353078 1754435 finetune.py:45] layer 16_k initial loss 0.00012780303950421512
I0311 22:36:46.516955 1754551 finetune.py:68] layer 17_q @ epoch 3 new loss 9.895215043798089e-05 old loss 0.00010064923844765872 BETTER
I0311 22:36:55.440093 1754783 finetune.py:68] layer 19_v @ epoch 4 new loss 9.106348443310708e-05 old loss 9.286406566388905e-05 BETTER
I0311 22:36:58.061121 1754667 finetune.py:68] layer 18_q @ epoch 1 new loss 0.00010972554446198046 old loss 0.0001127006980823353 BETTER
I0311 22:37:04.673548 1754783 finetune.py:45] layer 19_q initial loss 0.00011561997962417081
I0311 22:37:06.463324 1754435 finetune.py:68] layer 16_k @ epoch 0 new loss 0.000125404229038395 old loss 0.00012780303950421512 BETTER
I0311 22:37:18.944691 1754551 finetune.py:68] layer 17_q @ epoch 4 new loss 9.754829807206988e-05 old loss 9.895215043798089e-05 BETTER
I0311 22:37:28.259084 1754551 finetune.py:45] layer 17_k initial loss 0.00011302492930553854
I0311 22:37:30.876489 1754667 finetune.py:68] layer 18_q @ epoch 2 new loss 0.00010753223614301533 old loss 0.00010972554446198046 BETTER
I0311 22:37:36.089424 1754783 finetune.py:68] layer 19_q @ epoch 0 new loss 0.00010943910456262529 old loss 0.00011561997962417081 BETTER
I0311 22:37:40.607968 1754435 finetune.py:68] layer 16_k @ epoch 1 new loss 0.0001240749261341989 old loss 0.000125404229038395 BETTER
I0311 22:37:59.781559 1754551 finetune.py:68] layer 17_k @ epoch 0 new loss 0.000110464527097065 old loss 0.00011302492930553854 BETTER
I0311 22:38:03.608844 1754667 finetune.py:68] layer 18_q @ epoch 3 new loss 0.00010581071546766907 old loss 0.00010753223614301533 BETTER
I0311 22:38:08.321738 1754783 finetune.py:68] layer 19_q @ epoch 1 new loss 0.00010683782602427527 old loss 0.00010943910456262529 BETTER
I0311 22:38:14.526530 1754435 finetune.py:68] layer 16_k @ epoch 2 new loss 0.0001229906629305333 old loss 0.0001240749261341989 BETTER
I0311 22:38:31.857489 1754551 finetune.py:68] layer 17_k @ epoch 1 new loss 0.00010920674685621634 old loss 0.000110464527097065 BETTER
I0311 22:38:36.299940 1754667 finetune.py:68] layer 18_q @ epoch 4 new loss 0.00010439490870339796 old loss 0.00010581071546766907 BETTER
I0311 22:38:40.430071 1754783 finetune.py:68] layer 19_q @ epoch 2 new loss 0.00010490250133443624 old loss 0.00010683782602427527 BETTER
I0311 22:38:45.755714 1754667 finetune.py:45] layer 18_k initial loss 0.0001246667670784518
I0311 22:38:48.373263 1754435 finetune.py:68] layer 16_k @ epoch 3 new loss 0.00012208464613649994 old loss 0.0001229906629305333 BETTER
I0311 22:39:04.036726 1754551 finetune.py:68] layer 17_k @ epoch 2 new loss 0.00010820991883520037 old loss 0.00010920674685621634 BETTER
I0311 22:39:12.642345 1754783 finetune.py:68] layer 19_q @ epoch 3 new loss 0.00010338117863284424 old loss 0.00010490250133443624 BETTER
I0311 22:39:17.227141 1754667 finetune.py:68] layer 18_k @ epoch 0 new loss 0.00012200630590086803 old loss 0.0001246667670784518 BETTER
I0311 22:39:22.255566 1754435 finetune.py:68] layer 16_k @ epoch 4 new loss 0.00012130277173127979 old loss 0.00012208464613649994 BETTER
I0311 22:39:32.009961 1754435 finetune.py:45] layer 16_o initial loss 0.0003486883360892534
I0311 22:39:36.318325 1754551 finetune.py:68] layer 17_k @ epoch 3 new loss 0.00010738459968706593 old loss 0.00010820991883520037 BETTER
I0311 22:39:45.071755 1754783 finetune.py:68] layer 19_q @ epoch 4 new loss 0.00010213244968326762 old loss 0.00010338117863284424 BETTER
I0311 22:39:49.297923 1754667 finetune.py:68] layer 18_k @ epoch 1 new loss 0.00012075226550223306 old loss 0.00012200630590086803 BETTER
I0311 22:39:54.105888 1754783 finetune.py:45] layer 19_k initial loss 0.0001206090091727674
I0311 22:40:04.442676 1754435 finetune.py:68] layer 16_o @ epoch 0 new loss 0.0003284765116404742 old loss 0.0003486883360892534 BETTER
I0311 22:40:08.410328 1754551 finetune.py:68] layer 17_k @ epoch 4 new loss 0.00010668832692317665 old loss 0.00010738459968706593 BETTER
I0311 22:40:17.366257 1754551 finetune.py:45] layer 17_o initial loss 0.0002730016130954027
I0311 22:40:21.367666 1754667 finetune.py:68] layer 18_k @ epoch 2 new loss 0.00011977830581599846 old loss 0.00012075226550223306 BETTER
I0311 22:40:25.097243 1754783 finetune.py:68] layer 19_k @ epoch 0 new loss 0.00011832696327473968 old loss 0.0001206090091727674 BETTER
I0311 22:40:37.623282 1754435 finetune.py:68] layer 16_o @ epoch 1 new loss 0.0003177199687343091 old loss 0.0003284765116404742 BETTER
I0311 22:40:48.077246 1754551 finetune.py:68] layer 17_o @ epoch 0 new loss 0.000259416236076504 old loss 0.0002730016130954027 BETTER
I0311 22:40:53.452697 1754667 finetune.py:68] layer 18_k @ epoch 3 new loss 0.00011897971853613853 old loss 0.00011977830581599846 BETTER
I0311 22:40:56.769734 1754783 finetune.py:68] layer 19_k @ epoch 1 new loss 0.0001171693584183231 old loss 0.00011832696327473968 BETTER
I0311 22:41:10.915314 1754435 finetune.py:68] layer 16_o @ epoch 2 new loss 0.0003099492460023612 old loss 0.0003177199687343091 BETTER
I0311 22:41:19.487577 1754551 finetune.py:68] layer 17_o @ epoch 1 new loss 0.0002523203438613564 old loss 0.000259416236076504 BETTER
I0311 22:41:25.602777 1754667 finetune.py:68] layer 18_k @ epoch 4 new loss 0.00011832504969788715 old loss 0.00011897971853613853 BETTER
I0311 22:41:28.497964 1754783 finetune.py:68] layer 19_k @ epoch 2 new loss 0.00011629481741692871 old loss 0.0001171693584183231 BETTER
I0311 22:41:35.082944 1754667 finetune.py:45] layer 18_o initial loss 0.00029436356271617115
I0311 22:41:44.177090 1754435 finetune.py:68] layer 16_o @ epoch 3 new loss 0.0003039271687157452 old loss 0.0003099492460023612 BETTER
I0311 22:41:51.067421 1754551 finetune.py:68] layer 17_o @ epoch 2 new loss 0.0002472291816957295 old loss 0.0002523203438613564 BETTER
I0311 22:42:00.248111 1754783 finetune.py:68] layer 19_k @ epoch 3 new loss 0.00011555573291843757 old loss 0.00011629481741692871 BETTER
I0311 22:42:06.034622 1754667 finetune.py:68] layer 18_o @ epoch 0 new loss 0.0002806696284096688 old loss 0.00029436356271617115 BETTER
I0311 22:42:17.543315 1754435 finetune.py:68] layer 16_o @ epoch 4 new loss 0.00029897908098064363 old loss 0.0003039271687157452 BETTER
I0311 22:42:22.508936 1754551 finetune.py:68] layer 17_o @ epoch 3 new loss 0.00024330586893483996 old loss 0.0002472291816957295 BETTER
I0311 22:42:32.177220 1754783 finetune.py:68] layer 19_k @ epoch 4 new loss 0.00011496977094793692 old loss 0.00011555573291843757 BETTER
I0311 22:42:32.891695 1754435 finetune.py:45] layer 16_up initial loss 0.0004926411784254014
I0311 22:42:37.584668 1754667 finetune.py:68] layer 18_o @ epoch 1 new loss 0.00027370016323402524 old loss 0.0002806696284096688 BETTER
I0311 22:42:42.392094 1754783 finetune.py:45] layer 19_o initial loss 0.0002791773295029998
I0311 22:42:53.958753 1754551 finetune.py:68] layer 17_o @ epoch 4 new loss 0.00024014912196435034 old loss 0.00024330586893483996 BETTER
I0311 22:43:03.364151 1754435 finetune.py:68] layer 16_up @ epoch 0 new loss 0.00048096763202920556 old loss 0.0004926411784254014 BETTER
I0311 22:43:09.201772 1754667 finetune.py:68] layer 18_o @ epoch 2 new loss 0.00026873021852225065 old loss 0.00027370016323402524 BETTER
I0311 22:43:09.509555 1754551 finetune.py:45] layer 17_up initial loss 0.0004591241595335305
I0311 22:43:12.508194 1754783 finetune.py:68] layer 19_o @ epoch 0 new loss 0.00026689207879826427 old loss 0.0002791773295029998 BETTER
I0311 22:43:34.941703 1754435 finetune.py:68] layer 16_up @ epoch 1 new loss 0.00047369723324663937 old loss 0.00048096763202920556 BETTER
I0311 22:43:38.483037 1754551 finetune.py:68] layer 17_up @ epoch 0 new loss 0.0004483892989810556 old loss 0.0004591241595335305 BETTER
I0311 22:43:40.938858 1754667 finetune.py:68] layer 18_o @ epoch 3 new loss 0.0002649390953592956 old loss 0.00026873021852225065 BETTER
I0311 22:43:43.418250 1754783 finetune.py:68] layer 19_o @ epoch 1 new loss 0.0002610680239740759 old loss 0.00026689207879826427 BETTER
I0311 22:44:06.654046 1754435 finetune.py:68] layer 16_up @ epoch 2 new loss 0.0004679756239056587 old loss 0.00047369723324663937 BETTER
I0311 22:44:08.375253 1754551 finetune.py:68] layer 17_up @ epoch 1 new loss 0.00044187987805344164 old loss 0.0004483892989810556 BETTER
I0311 22:44:12.695187 1754667 finetune.py:68] layer 18_o @ epoch 4 new loss 0.0002618769067339599 old loss 0.0002649390953592956 BETTER
I0311 22:44:14.508585 1754783 finetune.py:68] layer 19_o @ epoch 2 new loss 0.0002570156066212803 old loss 0.0002610680239740759 BETTER
I0311 22:44:27.947509 1754667 finetune.py:45] layer 18_up initial loss 0.000526221061591059
I0311 22:44:38.285723 1754551 finetune.py:68] layer 17_up @ epoch 2 new loss 0.00043687730794772506 old loss 0.00044187987805344164 BETTER
I0311 22:44:38.516562 1754435 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0004631805932149291 old loss 0.0004679756239056587 BETTER
I0311 22:44:45.478409 1754783 finetune.py:68] layer 19_o @ epoch 3 new loss 0.00025405507767573 old loss 0.0002570156066212803 BETTER
I0311 22:44:57.060324 1754667 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0005138127598911524 old loss 0.000526221061591059 BETTER
I0311 22:45:08.117303 1754551 finetune.py:68] layer 17_up @ epoch 3 new loss 0.00043267966248095036 old loss 0.00043687730794772506 BETTER
I0311 22:45:10.347011 1754435 finetune.py:68] layer 16_up @ epoch 4 new loss 0.0004590970929712057 old loss 0.0004631805932149291 BETTER
I0311 22:45:16.569341 1754783 finetune.py:68] layer 19_o @ epoch 4 new loss 0.00025163768441416323 old loss 0.00025405507767573 BETTER
I0311 22:45:25.566716 1754435 finetune.py:45] layer 16_gate initial loss 0.0006066997302696109
I0311 22:45:26.931813 1754667 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0005065673030912876 old loss 0.0005138127598911524 BETTER
I0311 22:45:31.662338 1754783 finetune.py:45] layer 19_up initial loss 0.0005499368417076766
I0311 22:45:38.021327 1754551 finetune.py:68] layer 17_up @ epoch 4 new loss 0.000429143343353644 old loss 0.00043267966248095036 BETTER
I0311 22:45:53.157303 1754551 finetune.py:45] layer 17_gate initial loss 0.0005950187332928181
I0311 22:45:54.610778 1754435 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.0006003160960972309 old loss 0.0006066997302696109 BETTER
I0311 22:45:56.963050 1754667 finetune.py:68] layer 18_up @ epoch 2 new loss 0.0005009818123653531 old loss 0.0005065673030912876 BETTER
I0311 22:46:00.329351 1754783 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0005370694561861455 old loss 0.0005499368417076766 BETTER
I0311 22:46:20.801223 1754551 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0005892987246625125 old loss 0.0005950187332928181 BETTER
I0311 22:46:24.368997 1754435 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0005957204848527908 old loss 0.0006003160960972309 BETTER
I0311 22:46:27.090054 1754667 finetune.py:68] layer 18_up @ epoch 3 new loss 0.0004963588435202837 old loss 0.0005009818123653531 BETTER
I0311 22:46:29.745004 1754783 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0005297843599691987 old loss 0.0005370694561861455 BETTER
I0311 22:46:49.364875 1754551 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0005851486348547041 old loss 0.0005892987246625125 BETTER
I0311 22:46:54.467111 1754435 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.0005919261020608246 old loss 0.0005957204848527908 BETTER
I0311 22:46:57.399765 1754667 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0004925149260088801 old loss 0.0004963588435202837 BETTER
I0311 22:46:59.353036 1754783 finetune.py:68] layer 19_up @ epoch 2 new loss 0.000524187576957047 old loss 0.0005297843599691987 BETTER
I0311 22:47:12.794594 1754667 finetune.py:45] layer 18_gate initial loss 0.0006881681620143354
I0311 22:47:17.815072 1754551 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.0005818232311867177 old loss 0.0005851486348547041 BETTER
I0311 22:47:24.720455 1754435 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0005887271836400032 old loss 0.0005919261020608246 BETTER
I0311 22:47:29.073149 1754783 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0005197321879677474 old loss 0.000524187576957047 BETTER
I0311 22:47:40.247529 1754667 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0006821582210250199 old loss 0.0006881681620143354 BETTER
I0311 22:47:45.944576 1754551 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.000578972278162837 old loss 0.0005818232311867177 BETTER
I0311 22:47:55.476788 1754435 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.0005859155789949 old loss 0.0005887271836400032 BETTER
I0311 22:47:59.248142 1754783 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0005160603323020041 old loss 0.0005197321879677474 BETTER
I0311 22:48:09.521175 1754667 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.0006777695380151272 old loss 0.0006821582210250199 BETTER
I0311 22:48:15.092956 1754435 finetune.py:45] layer 16_down initial loss 0.0009345394792035222
I0311 22:48:15.215904 1754551 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.0005765276146121323 old loss 0.000578972278162837 BETTER
I0311 22:48:17.486825 1754783 finetune.py:45] layer 19_gate initial loss 0.0007468860712833703
I0311 22:48:33.366431 1754551 finetune.py:45] layer 17_down initial loss 0.0009610042325221002
I0311 22:48:37.988188 1754667 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0006741952383890748 old loss 0.0006777695380151272 BETTER
I0311 22:48:42.307987 1754435 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0009343611891381443 old loss 0.0009345394792035222 BETTER
I0311 22:48:44.580456 1754783 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.0007403762429021299 old loss 0.0007468860712833703 BETTER
I0311 22:48:59.103359 1754551 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0009608367690816522 old loss 0.0009610042325221002 BETTER
I0311 22:49:06.244004 1754667 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0006712001049891114 old loss 0.0006741952383890748 BETTER
I0311 22:49:10.817225 1754435 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0009342600242234766 old loss 0.0009343611891381443 BETTER
I0311 22:49:12.942129 1754783 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0007357982685789466 old loss 0.0007403762429021299 BETTER
I0311 22:49:25.825894 1754551 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0009607464307919145 old loss 0.0009608367690816522 BETTER
I0311 22:49:34.696587 1754667 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.0006686371052637696 old loss 0.0006712001049891114 BETTER
I0311 22:49:39.294159 1754435 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0009341974509879947 old loss 0.0009342600242234766 BETTER
I0311 22:49:41.203224 1754783 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0007321472512558103 old loss 0.0007357982685789466 BETTER
I0311 22:49:50.886542 1754667 finetune.py:45] layer 18_down initial loss 0.0011220340384170413
I0311 22:49:52.736708 1754551 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0009606885141693056 old loss 0.0009607464307919145 BETTER
I0311 22:50:07.892565 1754435 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0009341565892100334 old loss 0.0009341974509879947 BETTER
I0311 22:50:09.488379 1754783 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0007290759240277112 old loss 0.0007321472512558103 BETTER
I0311 22:50:16.941020 1754667 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0011218235595151782 old loss 0.0011220340384170413 BETTER
I0311 22:50:19.611623 1754551 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0009606490493752062 old loss 0.0009606885141693056 BETTER
I0311 22:50:36.412150 1754435 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0009341270779259503 old loss 0.0009341565892100334 BETTER
I0311 22:50:37.630030 1754783 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.0007265098392963409 old loss 0.0007290759240277112 BETTER
16_v proxy err 0.01082827802747488 tr(WHW.T) 780.7407836914062
16_q proxy err 0.001263184822164476 tr(WHW.T) 7195.3525390625
16_k proxy err 0.0008065938018262386 tr(WHW.T) 11644.1865234375
16_o proxy err 0.011583331972360611 tr(WHW.T) 88.55142974853516
16_up proxy err 0.00654543237760663 tr(WHW.T) 1890.2236328125
16_gate proxy err 0.0037038119044154882 tr(WHW.T) 3371.08203125
16_down proxy err 0.009522375650703907 tr(WHW.T) 152.72695922851562
I0311 22:50:44.292240 1754667 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0011216941056773067 old loss 0.0011218235595151782 BETTER
I0311 22:50:46.959175 1754551 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0009606204112060368 old loss 0.0009606490493752062 BETTER
17_v proxy err 0.010719168931245804 tr(WHW.T) 845.7654418945312
17_q proxy err 0.0013490555575117469 tr(WHW.T) 7167.50390625
17_k proxy err 0.0009238883503712714 tr(WHW.T) 10710.8017578125
17_o proxy err 0.012534340843558311 tr(WHW.T) 58.41773986816406
17_up proxy err 0.007276264484971762 tr(WHW.T) 1921.0902099609375
17_gate proxy err 0.003949549049139023 tr(WHW.T) 3571.812744140625
17_down proxy err 0.009677659720182419 tr(WHW.T) 166.16590881347656
I0311 22:50:54.109045 1754783 finetune.py:45] layer 19_down initial loss 0.001222047139890492
I0311 22:51:11.305230 1754667 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0011216116836294532 old loss 0.0011216941056773067 BETTER
I0311 22:51:19.553899 1754783 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0012218368938192725 old loss 0.001222047139890492 BETTER
I0311 22:51:38.177508 1754667 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0011215562699362636 old loss 0.0011216116836294532 BETTER
I0311 22:51:46.048764 1754783 finetune.py:68] layer 19_down @ epoch 1 new loss 0.001221711398102343 old loss 0.0012218368938192725 BETTER
I0311 22:52:04.948419 1754667 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0011215159902349114 old loss 0.0011215562699362636 BETTER
I0311 22:52:05.490617 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 20 in 73.80714106559753s
18_v proxy err 0.010135048069059849 tr(WHW.T) 1003.7705078125
18_q proxy err 0.0014332564314827323 tr(WHW.T) 7513.13916015625
18_k proxy err 0.0010461320634931326 tr(WHW.T) 10474.455078125
18_o proxy err 0.010766936466097832 tr(WHW.T) 70.28611755371094
18_up proxy err 0.007782404776662588 tr(WHW.T) 2023.0010986328125
18_gate proxy err 0.004210598301142454 tr(WHW.T) 3780.32568359375
18_down proxy err 0.009474883787333965 tr(WHW.T) 199.45925903320312
I0311 22:52:08.707584 1754899 config.py:54] PyTorch version 2.1.1 available.
I0311 22:52:09.801768 1752459 quantize_finetune_llama.py:183] layer 21 gpu 1
I0311 22:52:09.868547 1754899 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 22:52:12.219794 1754783 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0012216363102197647 old loss 0.001221711398102343 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:52:18.254513 1754899 finetune.py:45] layer 20_v initial loss 0.00014722747437190264
I0311 22:52:38.552229 1754783 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0012215867172926664 old loss 0.0012216363102197647 BETTER
I0311 22:52:51.217837 1754899 finetune.py:68] layer 20_v @ epoch 0 new loss 0.00011869411537190899 old loss 0.00014722747437190264 BETTER
I0311 22:53:04.863813 1754783 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0012215519091114402 old loss 0.0012215867172926664 BETTER
19_v proxy err 0.009977730922400951 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.0015413830988109112 tr(WHW.T) 6948.39501953125
19_k proxy err 0.0010336313862353563 tr(WHW.T) 10563.6171875
19_o proxy err 0.010809710249304771 tr(WHW.T) 62.56621170043945
19_up proxy err 0.007845216430723667 tr(WHW.T) 2149.614990234375
19_gate proxy err 0.004620224703103304 tr(WHW.T) 3688.54638671875
19_down proxy err 0.009246167726814747 tr(WHW.T) 223.92083740234375
I0311 22:53:19.854389 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 21 in 69.60830044746399s
I0311 22:53:22.997099 1755015 config.py:54] PyTorch version 2.1.1 available.
I0311 22:53:24.022314 1752459 quantize_finetune_llama.py:183] layer 22 gpu 2
I0311 22:53:24.094952 1755015 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 22:53:25.549028 1754899 finetune.py:68] layer 20_v @ epoch 1 new loss 0.00011275563883827999 old loss 0.00011869411537190899 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:53:32.130711 1755015 finetune.py:45] layer 21_v initial loss 0.00012933643301948905
I0311 22:54:00.263307 1754899 finetune.py:68] layer 20_v @ epoch 2 new loss 0.00010921304055955261 old loss 0.00011275563883827999 BETTER
I0311 22:54:03.499905 1755015 finetune.py:68] layer 21_v @ epoch 0 new loss 0.00010741530422819778 old loss 0.00012933643301948905 BETTER
I0311 22:54:32.417938 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 22 in 67.9379632472992s
I0311 22:54:35.085652 1754899 finetune.py:68] layer 20_v @ epoch 3 new loss 0.00010677707905415446 old loss 0.00010921304055955261 BETTER
I0311 22:54:35.657527 1755131 config.py:54] PyTorch version 2.1.1 available.
I0311 22:54:35.874660 1755015 finetune.py:68] layer 21_v @ epoch 1 new loss 0.00010314339306205511 old loss 0.00010741530422819778 BETTER
I0311 22:54:36.671920 1752459 quantize_finetune_llama.py:183] layer 23 gpu 3
I0311 22:54:36.750402 1755131 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:54:45.004849 1755131 finetune.py:45] layer 22_v initial loss 0.00015965323837008327
I0311 22:55:08.345995 1755015 finetune.py:68] layer 21_v @ epoch 2 new loss 0.00010048123658634722 old loss 0.00010314339306205511 BETTER
I0311 22:55:09.890440 1754899 finetune.py:68] layer 20_v @ epoch 4 new loss 0.00010479616321390495 old loss 0.00010677707905415446 BETTER
I0311 22:55:16.561997 1755131 finetune.py:68] layer 22_v @ epoch 0 new loss 0.00013352077803574502 old loss 0.00015965323837008327 BETTER
I0311 22:55:19.625257 1754899 finetune.py:45] layer 20_q initial loss 0.000133192355860956
I0311 22:55:41.095092 1755015 finetune.py:68] layer 21_v @ epoch 3 new loss 9.871260408544913e-05 old loss 0.00010048123658634722 BETTER
I0311 22:55:47.783586 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 23 in 70.69132447242737s
I0311 22:55:49.054362 1755131 finetune.py:68] layer 22_v @ epoch 1 new loss 0.00012795614020433277 old loss 0.00013352077803574502 BETTER
I0311 22:55:51.225032 1755247 config.py:54] PyTorch version 2.1.1 available.
I0311 22:55:52.410088 1752459 quantize_finetune_llama.py:183] layer 24 gpu 0
I0311 22:55:52.501200 1755247 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 22:55:53.226728 1754899 finetune.py:68] layer 20_q @ epoch 0 new loss 0.00012732038157992065 old loss 0.000133192355860956 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 22:56:00.869791 1755247 finetune.py:45] layer 23_v initial loss 0.00016056399908848107
I0311 22:56:14.286510 1755015 finetune.py:68] layer 21_v @ epoch 4 new loss 9.727010183269158e-05 old loss 9.871260408544913e-05 BETTER
I0311 22:56:21.925285 1755131 finetune.py:68] layer 22_v @ epoch 2 new loss 0.00012464004976209253 old loss 0.00012795614020433277 BETTER
I0311 22:56:23.582914 1755015 finetune.py:45] layer 21_q initial loss 0.00012244383106008172
I0311 22:56:27.708105 1754899 finetune.py:68] layer 20_q @ epoch 1 new loss 0.00012443060404621065 old loss 0.00012732038157992065 BETTER
I0311 22:56:31.878386 1755247 finetune.py:68] layer 23_v @ epoch 0 new loss 0.0001356362336082384 old loss 0.00016056399908848107 BETTER
I0311 22:56:54.775811 1755131 finetune.py:68] layer 22_v @ epoch 3 new loss 0.00012228575360495597 old loss 0.00012464004976209253 BETTER
I0311 22:56:55.353517 1755015 finetune.py:68] layer 21_q @ epoch 0 new loss 0.00011709709360729903 old loss 0.00012244383106008172 BETTER
I0311 22:57:02.395962 1754899 finetune.py:68] layer 20_q @ epoch 2 new loss 0.00012234225869178772 old loss 0.00012443060404621065 BETTER
I0311 22:57:04.023214 1755247 finetune.py:68] layer 23_v @ epoch 1 new loss 0.00013042247155681252 old loss 0.0001356362336082384 BETTER
I0311 22:57:27.924569 1755015 finetune.py:68] layer 21_q @ epoch 1 new loss 0.00011493085912661627 old loss 0.00011709709360729903 BETTER
I0311 22:57:27.926607 1755131 finetune.py:68] layer 22_v @ epoch 4 new loss 0.0001203395368065685 old loss 0.00012228575360495597 BETTER
I0311 22:57:36.987337 1755247 finetune.py:68] layer 23_v @ epoch 2 new loss 0.0001272113004233688 old loss 0.00013042247155681252 BETTER
I0311 22:57:37.306838 1754899 finetune.py:68] layer 20_q @ epoch 3 new loss 0.00012065222836099565 old loss 0.00012234225869178772 BETTER
I0311 22:57:37.700642 1755131 finetune.py:45] layer 22_q initial loss 0.00016200647223740816
I0311 22:58:00.312747 1755015 finetune.py:68] layer 21_q @ epoch 2 new loss 0.00011335535964462906 old loss 0.00011493085912661627 BETTER
I0311 22:58:09.272149 1755131 finetune.py:68] layer 22_q @ epoch 0 new loss 0.00015360389079432935 old loss 0.00016200647223740816 BETTER
I0311 22:58:09.647231 1755247 finetune.py:68] layer 23_v @ epoch 3 new loss 0.00012491142842918634 old loss 0.0001272113004233688 BETTER
I0311 22:58:12.042578 1754899 finetune.py:68] layer 20_q @ epoch 4 new loss 0.00011931060726055875 old loss 0.00012065222836099565 BETTER
I0311 22:58:21.275701 1754899 finetune.py:45] layer 20_k initial loss 0.00013922253856435418
I0311 22:58:32.663982 1755015 finetune.py:68] layer 21_q @ epoch 3 new loss 0.0001120763408835046 old loss 0.00011335535964462906 BETTER
I0311 22:58:41.683175 1755131 finetune.py:68] layer 22_q @ epoch 1 new loss 0.0001499087957199663 old loss 0.00015360389079432935 BETTER
I0311 22:58:42.193966 1755247 finetune.py:68] layer 23_v @ epoch 4 new loss 0.00012303204857744277 old loss 0.00012491142842918634 BETTER
I0311 22:58:51.304367 1755247 finetune.py:45] layer 23_q initial loss 0.00015784427523612976
I0311 22:58:54.687665 1754899 finetune.py:68] layer 20_k @ epoch 0 new loss 0.00013712530198972672 old loss 0.00013922253856435418 BETTER
I0311 22:59:04.991521 1755015 finetune.py:68] layer 21_q @ epoch 4 new loss 0.00011105703742941841 old loss 0.0001120763408835046 BETTER
I0311 22:59:14.285002 1755015 finetune.py:45] layer 21_k initial loss 0.00013174487685319036
I0311 22:59:14.530635 1755131 finetune.py:68] layer 22_q @ epoch 2 new loss 0.00014710859977640212 old loss 0.0001499087957199663 BETTER
I0311 22:59:23.058122 1755247 finetune.py:68] layer 23_q @ epoch 0 new loss 0.00015061756130307913 old loss 0.00015784427523612976 BETTER
I0311 22:59:28.768905 1754899 finetune.py:68] layer 20_k @ epoch 1 new loss 0.0001359108864562586 old loss 0.00013712530198972672 BETTER
I0311 22:59:46.795337 1755015 finetune.py:68] layer 21_k @ epoch 0 new loss 0.00012955204874742776 old loss 0.00013174487685319036 BETTER
I0311 22:59:47.768749 1755131 finetune.py:68] layer 22_q @ epoch 3 new loss 0.00014493580965790898 old loss 0.00014710859977640212 BETTER
I0311 22:59:55.331318 1755247 finetune.py:68] layer 23_q @ epoch 1 new loss 0.0001477323821745813 old loss 0.00015061756130307913 BETTER
I0311 23:00:03.166714 1754899 finetune.py:68] layer 20_k @ epoch 2 new loss 0.00013502317597158253 old loss 0.0001359108864562586 BETTER
I0311 23:00:19.527754 1755015 finetune.py:68] layer 21_k @ epoch 1 new loss 0.0001285663020098582 old loss 0.00012955204874742776 BETTER
I0311 23:00:20.694590 1755131 finetune.py:68] layer 22_q @ epoch 4 new loss 0.00014311632548924536 old loss 0.00014493580965790898 BETTER
I0311 23:00:27.927181 1755247 finetune.py:68] layer 23_q @ epoch 2 new loss 0.0001456821773899719 old loss 0.0001477323821745813 BETTER
I0311 23:00:31.006955 1755131 finetune.py:45] layer 22_k initial loss 0.00017377856420353055
I0311 23:00:37.382122 1754899 finetune.py:68] layer 20_k @ epoch 3 new loss 0.0001342974283033982 old loss 0.00013502317597158253 BETTER
I0311 23:00:51.780043 1755015 finetune.py:68] layer 21_k @ epoch 2 new loss 0.00012790609616786242 old loss 0.0001285663020098582 BETTER
I0311 23:01:00.174595 1755247 finetune.py:68] layer 23_q @ epoch 3 new loss 0.0001440366468159482 old loss 0.0001456821773899719 BETTER
I0311 23:01:02.538833 1755131 finetune.py:68] layer 22_k @ epoch 0 new loss 0.00017098071111831814 old loss 0.00017377856420353055 BETTER
I0311 23:01:11.369846 1754899 finetune.py:68] layer 20_k @ epoch 4 new loss 0.0001337248395429924 old loss 0.0001342974283033982 BETTER
I0311 23:01:20.777545 1754899 finetune.py:45] layer 20_o initial loss 0.0003292650217190385
I0311 23:01:23.987650 1755015 finetune.py:68] layer 21_k @ epoch 3 new loss 0.00012732572213280946 old loss 0.00012790609616786242 BETTER
I0311 23:01:32.489394 1755247 finetune.py:68] layer 23_q @ epoch 4 new loss 0.0001426991366315633 old loss 0.0001440366468159482 BETTER
I0311 23:01:34.728753 1755131 finetune.py:68] layer 22_k @ epoch 1 new loss 0.0001695085084065795 old loss 0.00017098071111831814 BETTER
I0311 23:01:42.132166 1755247 finetune.py:45] layer 23_k initial loss 0.00017258930893149227
I0311 23:01:53.532990 1754899 finetune.py:68] layer 20_o @ epoch 0 new loss 0.0003145652008242905 old loss 0.0003292650217190385 BETTER
I0311 23:01:56.317874 1755015 finetune.py:68] layer 21_k @ epoch 4 new loss 0.00012689855066128075 old loss 0.00012732572213280946 BETTER
I0311 23:02:05.614429 1755015 finetune.py:45] layer 21_o initial loss 0.0002928411995526403
I0311 23:02:06.856430 1755131 finetune.py:68] layer 22_k @ epoch 2 new loss 0.00016841531032696366 old loss 0.0001695085084065795 BETTER
I0311 23:02:13.151446 1755247 finetune.py:68] layer 23_k @ epoch 0 new loss 0.0001703524321783334 old loss 0.00017258930893149227 BETTER
I0311 23:02:27.280315 1754899 finetune.py:68] layer 20_o @ epoch 1 new loss 0.00030751098529435694 old loss 0.0003145652008242905 BETTER
I0311 23:02:36.506769 1755015 finetune.py:68] layer 21_o @ epoch 0 new loss 0.00028350637876428664 old loss 0.0002928411995526403 BETTER
I0311 23:02:39.350641 1755131 finetune.py:68] layer 22_k @ epoch 3 new loss 0.00016752271039877087 old loss 0.00016841531032696366 BETTER
I0311 23:02:44.921810 1755247 finetune.py:68] layer 23_k @ epoch 1 new loss 0.00016917379980441183 old loss 0.0001703524321783334 BETTER
I0311 23:03:01.057690 1754899 finetune.py:68] layer 20_o @ epoch 2 new loss 0.00030250343843363225 old loss 0.00030751098529435694 BETTER
I0311 23:03:08.253299 1755015 finetune.py:68] layer 21_o @ epoch 1 new loss 0.00027959237922914326 old loss 0.00028350637876428664 BETTER
I0311 23:03:11.537851 1755131 finetune.py:68] layer 22_k @ epoch 4 new loss 0.00016681566194165498 old loss 0.00016752271039877087 BETTER
I0311 23:03:16.646926 1755247 finetune.py:68] layer 23_k @ epoch 2 new loss 0.00016830977983772755 old loss 0.00016917379980441183 BETTER
I0311 23:03:21.115515 1755131 finetune.py:45] layer 22_o initial loss 0.0003710681921802461
I0311 23:03:34.482120 1754899 finetune.py:68] layer 20_o @ epoch 3 new loss 0.0002988108608406037 old loss 0.00030250343843363225 BETTER
I0311 23:03:39.806132 1755015 finetune.py:68] layer 21_o @ epoch 2 new loss 0.0002769076090771705 old loss 0.00027959237922914326 BETTER
I0311 23:03:48.421768 1755247 finetune.py:68] layer 23_k @ epoch 3 new loss 0.00016765858163125813 old loss 0.00016830977983772755 BETTER
I0311 23:03:52.105701 1755131 finetune.py:68] layer 22_o @ epoch 0 new loss 0.0003579147451091558 old loss 0.0003710681921802461 BETTER
I0311 23:04:08.175199 1754899 finetune.py:68] layer 20_o @ epoch 4 new loss 0.00029575361986644566 old loss 0.0002988108608406037 BETTER
I0311 23:04:11.525172 1755015 finetune.py:68] layer 21_o @ epoch 3 new loss 0.0002749359409790486 old loss 0.0002769076090771705 BETTER
I0311 23:04:20.310310 1755247 finetune.py:68] layer 23_k @ epoch 4 new loss 0.00016711953503545374 old loss 0.00016765858163125813 BETTER
I0311 23:04:23.759713 1754899 finetune.py:45] layer 20_up initial loss 0.0006489980733022094
I0311 23:04:23.994998 1755131 finetune.py:68] layer 22_o @ epoch 1 new loss 0.00035203873994760215 old loss 0.0003579147451091558 BETTER
I0311 23:04:29.767560 1755247 finetune.py:45] layer 23_o initial loss 0.00034989716368727386
I0311 23:04:43.166837 1755015 finetune.py:68] layer 21_o @ epoch 4 new loss 0.0002734229783527553 old loss 0.0002749359409790486 BETTER
I0311 23:04:54.351912 1754899 finetune.py:68] layer 20_up @ epoch 0 new loss 0.00063469132874161 old loss 0.0006489980733022094 BETTER
I0311 23:04:55.654933 1755131 finetune.py:68] layer 22_o @ epoch 2 new loss 0.00034787936601787806 old loss 0.00035203873994760215 BETTER
I0311 23:04:58.218881 1755015 finetune.py:45] layer 21_up initial loss 0.0006600402994081378
I0311 23:04:59.853546 1755247 finetune.py:68] layer 23_o @ epoch 0 new loss 0.00034037523437291384 old loss 0.00034989716368727386 BETTER
I0311 23:05:26.894241 1754899 finetune.py:68] layer 20_up @ epoch 1 new loss 0.0006264501716941595 old loss 0.00063469132874161 BETTER
I0311 23:05:28.332601 1755015 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0006473161047324538 old loss 0.0006600402994081378 BETTER
I0311 23:05:28.611720 1755131 finetune.py:68] layer 22_o @ epoch 3 new loss 0.00034475093707442284 old loss 0.00034787936601787806 BETTER
I0311 23:05:31.005394 1755247 finetune.py:68] layer 23_o @ epoch 1 new loss 0.00033603468909859657 old loss 0.00034037523437291384 BETTER
I0311 23:06:00.260760 1755015 finetune.py:68] layer 21_up @ epoch 1 new loss 0.0006401733844541013 old loss 0.0006473161047324538 BETTER
I0311 23:06:01.393860 1754899 finetune.py:68] layer 20_up @ epoch 2 new loss 0.0006201777141541243 old loss 0.0006264501716941595 BETTER
I0311 23:06:02.697747 1755131 finetune.py:68] layer 22_o @ epoch 4 new loss 0.0003423890157137066 old loss 0.00034475093707442284 BETTER
I0311 23:06:03.599643 1755247 finetune.py:68] layer 23_o @ epoch 2 new loss 0.000333145959302783 old loss 0.00033603468909859657 BETTER
I0311 23:06:18.933966 1755131 finetune.py:45] layer 22_up initial loss 0.0007833417039364576
I0311 23:06:31.336456 1755015 finetune.py:68] layer 21_up @ epoch 2 new loss 0.0006347134476527572 old loss 0.0006401733844541013 BETTER
I0311 23:06:35.408374 1754899 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0006149972323328257 old loss 0.0006201777141541243 BETTER
I0311 23:06:36.465198 1755247 finetune.py:68] layer 23_o @ epoch 3 new loss 0.00033091107616201043 old loss 0.000333145959302783 BETTER
I0311 23:06:49.167040 1755131 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0007700182613916695 old loss 0.0007833417039364576 BETTER
I0311 23:07:01.559752 1755015 finetune.py:68] layer 21_up @ epoch 3 new loss 0.00063036271603778 old loss 0.0006347134476527572 BETTER
I0311 23:07:08.325678 1754899 finetune.py:68] layer 20_up @ epoch 4 new loss 0.0006107549415901303 old loss 0.0006149972323328257 BETTER
I0311 23:07:08.879204 1755247 finetune.py:68] layer 23_o @ epoch 4 new loss 0.00032930789166130126 old loss 0.00033091107616201043 BETTER
I0311 23:07:19.239123 1755131 finetune.py:68] layer 22_up @ epoch 1 new loss 0.000762204232160002 old loss 0.0007700182613916695 BETTER
I0311 23:07:24.602132 1754899 finetune.py:45] layer 20_gate initial loss 0.000882371561601758
I0311 23:07:24.680700 1755247 finetune.py:45] layer 23_up initial loss 0.0008161265286616981
I0311 23:07:31.472993 1755015 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0006266883574426174 old loss 0.00063036271603778 BETTER
I0311 23:07:48.026262 1755015 finetune.py:45] layer 21_gate initial loss 0.0009225985850207508
I0311 23:07:49.305436 1755131 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0007562359678559005 old loss 0.000762204232160002 BETTER
I0311 23:07:53.513140 1754899 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.000875029421877116 old loss 0.000882371561601758 BETTER
I0311 23:07:53.659140 1755247 finetune.py:68] layer 23_up @ epoch 0 new loss 0.0008029324235394597 old loss 0.0008161265286616981 BETTER
I0311 23:08:15.605896 1755015 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0009163547656498849 old loss 0.0009225985850207508 BETTER
I0311 23:08:19.638899 1755131 finetune.py:68] layer 22_up @ epoch 3 new loss 0.0007514097960665822 old loss 0.0007562359678559005 BETTER
I0311 23:08:23.275068 1755247 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0007949491264298558 old loss 0.0008029324235394597 BETTER
I0311 23:08:23.324116 1754899 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.000869777228217572 old loss 0.000875029421877116 BETTER
I0311 23:08:43.945108 1755015 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0009119069436565042 old loss 0.0009163547656498849 BETTER
I0311 23:08:49.798185 1755131 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0007475002203136683 old loss 0.0007514097960665822 BETTER
I0311 23:08:52.914097 1755247 finetune.py:68] layer 23_up @ epoch 2 new loss 0.0007891673594713211 old loss 0.0007949491264298558 BETTER
I0311 23:08:53.249197 1754899 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.0008654139237478375 old loss 0.000869777228217572 BETTER
I0311 23:09:05.324073 1755131 finetune.py:45] layer 22_gate initial loss 0.0010852233972400427
I0311 23:09:12.260834 1755015 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0009082227479666471 old loss 0.0009119069436565042 BETTER
I0311 23:09:22.581401 1755247 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0007843559142202139 old loss 0.0007891673594713211 BETTER
I0311 23:09:23.223767 1754899 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0008619136060588062 old loss 0.0008654139237478375 BETTER
I0311 23:09:32.897482 1755131 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.001078491797670722 old loss 0.0010852233972400427 BETTER
I0311 23:09:40.418369 1755015 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.0009052688255906105 old loss 0.0009082227479666471 BETTER
I0311 23:09:52.214661 1755247 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0007805170607753098 old loss 0.0007843559142202139 BETTER
I0311 23:09:53.211089 1754899 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.0008588627679273486 old loss 0.0008619136060588062 BETTER
I0311 23:10:00.969723 1755131 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.0010736781405285 old loss 0.001078491797670722 BETTER
I0311 23:10:07.896460 1755247 finetune.py:45] layer 23_gate initial loss 0.0011626183986663818
I0311 23:10:08.742780 1755015 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.0009027561172842979 old loss 0.0009052688255906105 BETTER
I0311 23:10:09.631394 1754899 finetune.py:45] layer 20_down initial loss 0.0014661370078101754
I0311 23:10:24.357475 1755015 finetune.py:45] layer 21_down initial loss 0.0015302899992093444
I0311 23:10:29.141322 1755131 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0010698112891986966 old loss 0.0010736781405285 BETTER
I0311 23:10:35.092838 1755247 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0011563595617190003 old loss 0.0011626183986663818 BETTER
I0311 23:10:36.887183 1754899 finetune.py:68] layer 20_down @ epoch 0 new loss 0.001465791487134993 old loss 0.0014661370078101754 BETTER
I0311 23:10:50.229461 1755015 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0015300167724490166 old loss 0.0015302899992093444 BETTER
I0311 23:10:57.290336 1755131 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.00106654257979244 old loss 0.0010698112891986966 BETTER
I0311 23:11:03.145647 1755247 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.0011516264639794827 old loss 0.0011563595617190003 BETTER
I0311 23:11:05.379589 1754899 finetune.py:68] layer 20_down @ epoch 1 new loss 0.0014655604027211666 old loss 0.001465791487134993 BETTER
I0311 23:11:17.142538 1755015 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0015298384241759777 old loss 0.0015300167724490166 BETTER
I0311 23:11:25.623453 1755131 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.001063986332155764 old loss 0.00106654257979244 BETTER
I0311 23:11:31.276813 1755247 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.001147841103374958 old loss 0.0011516264639794827 BETTER
I0311 23:11:33.720257 1754899 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0014654105762019753 old loss 0.0014655604027211666 BETTER
I0311 23:11:43.907802 1755131 finetune.py:45] layer 22_down initial loss 0.0017767288954928517
I0311 23:11:46.982134 1755015 finetune.py:68] layer 21_down @ epoch 2 new loss 0.0015297150239348412 old loss 0.0015298384241759777 BETTER
I0311 23:12:02.582611 1755247 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.0011446492280811071 old loss 0.001147841103374958 BETTER
I0311 23:12:04.688990 1754899 finetune.py:68] layer 20_down @ epoch 3 new loss 0.0014653075486421585 old loss 0.0014654105762019753 BETTER
I0311 23:12:10.385713 1755131 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0017764321528375149 old loss 0.0017767288954928517 BETTER
I0311 23:12:14.630435 1755015 finetune.py:68] layer 21_down @ epoch 3 new loss 0.001529629691503942 old loss 0.0015297150239348412 BETTER
I0311 23:12:33.805267 1755247 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0011420976370573044 old loss 0.0011446492280811071 BETTER
I0311 23:12:35.681997 1754899 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0014652367681264877 old loss 0.0014653075486421585 BETTER
20_v proxy err 0.010450759902596474 tr(WHW.T) 990.5983276367188
20_q proxy err 0.001525028725154698 tr(WHW.T) 7153.05859375
20_k proxy err 0.0010682956781238317 tr(WHW.T) 10400.3583984375
20_o proxy err 0.007752991281449795 tr(WHW.T) 100.5889663696289
20_up proxy err 0.0077338749542832375 tr(WHW.T) 2341.051025390625
20_gate proxy err 0.004546052776277065 tr(WHW.T) 4025.39697265625
20_down proxy err 0.009114131331443787 tr(WHW.T) 276.1707458496094
I0311 23:12:38.902022 1755131 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0017762311035767198 old loss 0.0017764321528375149 BETTER
I0311 23:12:43.033528 1755015 finetune.py:68] layer 21_down @ epoch 4 new loss 0.001529563800431788 old loss 0.001529629691503942 BETTER
21_v proxy err 0.010197720490396023 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.0017239907756447792 tr(WHW.T) 7069.046875
21_k proxy err 0.0012397965183481574 tr(WHW.T) 9991.162109375
21_o proxy err 0.008831319399178028 tr(WHW.T) 75.75656127929688
21_up proxy err 0.008169734850525856 tr(WHW.T) 2361.409423828125
21_gate proxy err 0.004870256874710321 tr(WHW.T) 4002.827392578125
21_down proxy err 0.009337428957223892 tr(WHW.T) 277.7419738769531
I0311 23:12:52.649105 1755247 finetune.py:45] layer 23_down initial loss 0.0018741348758339882
I0311 23:13:06.111854 1755131 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0017760952468961477 old loss 0.0017762311035767198 BETTER
I0311 23:13:17.882460 1755247 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0018738886574283242 old loss 0.0018741348758339882 BETTER
I0311 23:13:33.063634 1755131 finetune.py:68] layer 22_down @ epoch 3 new loss 0.001775996177457273 old loss 0.0017760952468961477 BETTER
I0311 23:13:44.253540 1755247 finetune.py:68] layer 23_down @ epoch 1 new loss 0.001873721368610859 old loss 0.0018738886574283242 BETTER
I0311 23:14:00.012422 1755131 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0017759287729859352 old loss 0.001775996177457273 BETTER
I0311 23:14:01.078742 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 24 in 73.30741620063782s
22_v proxy err 0.009652446955442429 tr(WHW.T) 1243.2529296875
22_q proxy err 0.0016261818818747997 tr(WHW.T) 7751.04931640625
22_k proxy err 0.0012064621550962329 tr(WHW.T) 10616.96875
22_o proxy err 0.0070486352778971195 tr(WHW.T) 114.67951202392578
22_up proxy err 0.0082401717081666 tr(WHW.T) 2474.977783203125
22_gate proxy err 0.004969029221683741 tr(WHW.T) 4155.390625
22_down proxy err 0.009337866678833961 tr(WHW.T) 313.1144104003906
I0311 23:14:04.247089 1755363 config.py:54] PyTorch version 2.1.1 available.
I0311 23:14:05.255434 1752459 quantize_finetune_llama.py:183] layer 25 gpu 1
I0311 23:14:05.321055 1755363 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 23:14:10.547708 1755247 finetune.py:68] layer 23_down @ epoch 2 new loss 0.001873604254797101 old loss 0.001873721368610859 BETTER
I0311 23:14:13.799741 1755363 finetune.py:45] layer 24_v initial loss 0.00018036834080703557
I0311 23:14:37.133096 1755247 finetune.py:68] layer 23_down @ epoch 3 new loss 0.001873526256531477 old loss 0.001873604254797101 BETTER
I0311 23:14:46.902636 1755363 finetune.py:68] layer 24_v @ epoch 0 new loss 0.00015203870134428144 old loss 0.00018036834080703557 BETTER
I0311 23:15:03.511549 1755247 finetune.py:68] layer 23_down @ epoch 4 new loss 0.001873467001132667 old loss 0.001873526256531477 BETTER
23_v proxy err 0.009087868966162205 tr(WHW.T) 1486.037353515625
23_q proxy err 0.0019075192976742983 tr(WHW.T) 7349.9521484375
23_k proxy err 0.00141935795545578 tr(WHW.T) 9994.0029296875
23_o proxy err 0.008451633155345917 tr(WHW.T) 85.46665954589844
23_up proxy err 0.00853125099092722 tr(WHW.T) 2533.895263671875
23_gate proxy err 0.005336564965546131 tr(WHW.T) 4098.03271484375
23_down proxy err 0.009325360879302025 tr(WHW.T) 322.56817626953125
I0311 23:15:16.320852 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 25 in 70.66156530380249s
I0311 23:15:19.626783 1755479 config.py:54] PyTorch version 2.1.1 available.
I0311 23:15:20.634717 1752459 quantize_finetune_llama.py:183] layer 26 gpu 2
I0311 23:15:20.707957 1755479 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 23:15:21.335628 1755363 finetune.py:68] layer 24_v @ epoch 1 new loss 0.00014636148989666253 old loss 0.00015203870134428144 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 23:15:29.026085 1755479 finetune.py:45] layer 25_v initial loss 0.00016575766494497657
I0311 23:15:56.067910 1755363 finetune.py:68] layer 24_v @ epoch 2 new loss 0.00014295431901700795 old loss 0.00014636148989666253 BETTER
I0311 23:16:00.376521 1755479 finetune.py:68] layer 25_v @ epoch 0 new loss 0.00013454788131639361 old loss 0.00016575766494497657 BETTER
I0311 23:16:30.989199 1755363 finetune.py:68] layer 24_v @ epoch 3 new loss 0.00014039292000234127 old loss 0.00014295431901700795 BETTER
I0311 23:16:31.328495 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 26 in 70.25397109985352s
I0311 23:16:32.776067 1755479 finetune.py:68] layer 25_v @ epoch 1 new loss 0.000129984415252693 old loss 0.00013454788131639361 BETTER
I0311 23:16:34.662374 1755595 config.py:54] PyTorch version 2.1.1 available.
I0311 23:16:35.660636 1752459 quantize_finetune_llama.py:183] layer 27 gpu 3
I0311 23:16:35.734182 1755595 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 23:16:44.030210 1755595 finetune.py:45] layer 26_v initial loss 0.00025625506532378495
I0311 23:17:05.311427 1755479 finetune.py:68] layer 25_v @ epoch 2 new loss 0.00012746795255225152 old loss 0.000129984415252693 BETTER
I0311 23:17:05.897588 1755363 finetune.py:68] layer 24_v @ epoch 4 new loss 0.00013855163706466556 old loss 0.00014039292000234127 BETTER
I0311 23:17:15.186279 1755363 finetune.py:45] layer 24_q initial loss 0.00018047277990262955
I0311 23:17:15.503194 1755595 finetune.py:68] layer 26_v @ epoch 0 new loss 0.00021294060570653528 old loss 0.00025625506532378495 BETTER
I0311 23:17:38.143215 1755479 finetune.py:68] layer 25_v @ epoch 3 new loss 0.00012547706137411296 old loss 0.00012746795255225152 BETTER
I0311 23:17:46.033119 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 27 in 69.92649745941162s
I0311 23:17:47.961712 1755595 finetune.py:68] layer 26_v @ epoch 1 new loss 0.00020521803526207805 old loss 0.00021294060570653528 BETTER
I0311 23:17:48.613981 1755363 finetune.py:68] layer 24_q @ epoch 0 new loss 0.00017253952682949603 old loss 0.00018047277990262955 BETTER
I0311 23:17:49.271703 1755711 config.py:54] PyTorch version 2.1.1 available.
I0311 23:17:50.278113 1752459 quantize_finetune_llama.py:183] layer 28 gpu 0
I0311 23:17:50.344929 1755711 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 23:17:58.244248 1755711 finetune.py:45] layer 27_v initial loss 0.0002188205544371158
I0311 23:18:11.091573 1755479 finetune.py:68] layer 25_v @ epoch 4 new loss 0.0001241170393768698 old loss 0.00012547706137411296 BETTER
I0311 23:18:21.872592 1755595 finetune.py:68] layer 26_v @ epoch 2 new loss 0.0002001799875870347 old loss 0.00020521803526207805 BETTER
I0311 23:18:21.954610 1755479 finetune.py:45] layer 25_q initial loss 0.00016340057482011616
I0311 23:18:24.068558 1755363 finetune.py:68] layer 24_q @ epoch 1 new loss 0.0001694721431704238 old loss 0.00017253952682949603 BETTER
I0311 23:18:29.752090 1755711 finetune.py:68] layer 27_v @ epoch 0 new loss 0.00018691591685637832 old loss 0.0002188205544371158 BETTER
I0311 23:18:55.108704 1755479 finetune.py:68] layer 25_q @ epoch 0 new loss 0.0001559373049531132 old loss 0.00016340057482011616 BETTER
I0311 23:18:56.578163 1755595 finetune.py:68] layer 26_v @ epoch 3 new loss 0.0001966753916349262 old loss 0.0002001799875870347 BETTER
I0311 23:18:59.414193 1755363 finetune.py:68] layer 24_q @ epoch 2 new loss 0.00016728111950214952 old loss 0.0001694721431704238 BETTER
I0311 23:19:02.678066 1755711 finetune.py:68] layer 27_v @ epoch 1 new loss 0.00018077790446113795 old loss 0.00018691591685637832 BETTER
I0311 23:19:29.003994 1755479 finetune.py:68] layer 25_q @ epoch 1 new loss 0.00015345634892582893 old loss 0.0001559373049531132 BETTER
I0311 23:19:30.936736 1755595 finetune.py:68] layer 26_v @ epoch 4 new loss 0.00019409383821766824 old loss 0.0001966753916349262 BETTER
I0311 23:19:35.176755 1755363 finetune.py:68] layer 24_q @ epoch 3 new loss 0.00016553029126953334 old loss 0.00016728111950214952 BETTER
I0311 23:19:36.341679 1755711 finetune.py:68] layer 27_v @ epoch 2 new loss 0.00017712591215968132 old loss 0.00018077790446113795 BETTER
I0311 23:19:42.910224 1755595 finetune.py:45] layer 26_q initial loss 0.000250987708568573
I0311 23:20:01.539220 1755479 finetune.py:68] layer 25_q @ epoch 2 new loss 0.0001516882621217519 old loss 0.00015345634892582893 BETTER
I0311 23:20:08.449330 1755711 finetune.py:68] layer 27_v @ epoch 3 new loss 0.0001743858738336712 old loss 0.00017712591215968132 BETTER
I0311 23:20:09.793000 1755363 finetune.py:68] layer 24_q @ epoch 4 new loss 0.0001641663839109242 old loss 0.00016553029126953334 BETTER
I0311 23:20:14.457571 1755595 finetune.py:68] layer 26_q @ epoch 0 new loss 0.00024184783978853375 old loss 0.000250987708568573 BETTER
I0311 23:20:19.333173 1755363 finetune.py:45] layer 24_k initial loss 0.00019731959037017077
I0311 23:20:33.649281 1755479 finetune.py:68] layer 25_q @ epoch 3 new loss 0.00015034840907901525 old loss 0.0001516882621217519 BETTER
I0311 23:20:41.302072 1755711 finetune.py:68] layer 27_v @ epoch 4 new loss 0.0001722280285321176 old loss 0.0001743858738336712 BETTER
I0311 23:20:46.987020 1755595 finetune.py:68] layer 26_q @ epoch 1 new loss 0.00023723009508103132 old loss 0.00024184783978853375 BETTER
I0311 23:20:51.280517 1755711 finetune.py:45] layer 27_q initial loss 0.00023074338969308883
I0311 23:20:52.506971 1755363 finetune.py:68] layer 24_k @ epoch 0 new loss 0.0001948831632034853 old loss 0.00019731959037017077 BETTER
I0311 23:21:06.576264 1755479 finetune.py:68] layer 25_q @ epoch 4 new loss 0.00014917868247721344 old loss 0.00015034840907901525 BETTER
I0311 23:21:16.057982 1755479 finetune.py:45] layer 25_k initial loss 0.00018202795763500035
I0311 23:21:19.693678 1755595 finetune.py:68] layer 26_q @ epoch 2 new loss 0.00023397042241413146 old loss 0.00023723009508103132 BETTER
I0311 23:21:22.844028 1755711 finetune.py:68] layer 27_q @ epoch 0 new loss 0.00021929360809735954 old loss 0.00023074338969308883 BETTER
I0311 23:21:26.610124 1755363 finetune.py:68] layer 24_k @ epoch 1 new loss 0.00019367061031516641 old loss 0.0001948831632034853 BETTER
I0311 23:21:47.584452 1755479 finetune.py:68] layer 25_k @ epoch 0 new loss 0.00017989137268159539 old loss 0.00018202795763500035 BETTER
I0311 23:21:52.565677 1755595 finetune.py:68] layer 26_q @ epoch 3 new loss 0.00023127843451220542 old loss 0.00023397042241413146 BETTER
I0311 23:21:55.123786 1755711 finetune.py:68] layer 27_q @ epoch 1 new loss 0.00021496789122465998 old loss 0.00021929360809735954 BETTER
I0311 23:22:00.656440 1755363 finetune.py:68] layer 24_k @ epoch 2 new loss 0.00019277089450042695 old loss 0.00019367061031516641 BETTER
I0311 23:22:19.775120 1755479 finetune.py:68] layer 25_k @ epoch 1 new loss 0.00017880032828543335 old loss 0.00017989137268159539 BETTER
I0311 23:22:25.181732 1755595 finetune.py:68] layer 26_q @ epoch 4 new loss 0.00022916874149814248 old loss 0.00023127843451220542 BETTER
I0311 23:22:27.413428 1755711 finetune.py:68] layer 27_q @ epoch 2 new loss 0.0002118807315127924 old loss 0.00021496789122465998 BETTER
I0311 23:22:34.964016 1755363 finetune.py:68] layer 24_k @ epoch 3 new loss 0.00019210591563023627 old loss 0.00019277089450042695 BETTER
I0311 23:22:35.373944 1755595 finetune.py:45] layer 26_k initial loss 0.00027525884797796607
I0311 23:22:52.231035 1755479 finetune.py:68] layer 25_k @ epoch 2 new loss 0.00017811097495723516 old loss 0.00017880032828543335 BETTER
I0311 23:22:59.804528 1755711 finetune.py:68] layer 27_q @ epoch 3 new loss 0.00020938408852089196 old loss 0.0002118807315127924 BETTER
I0311 23:23:07.002683 1755595 finetune.py:68] layer 26_k @ epoch 0 new loss 0.0002723133366089314 old loss 0.00027525884797796607 BETTER
I0311 23:23:09.192477 1755363 finetune.py:68] layer 24_k @ epoch 4 new loss 0.00019152533786837012 old loss 0.00019210591563023627 BETTER
I0311 23:23:18.816798 1755363 finetune.py:45] layer 24_o initial loss 0.0004149993765167892
I0311 23:23:24.435797 1755479 finetune.py:68] layer 25_k @ epoch 3 new loss 0.00017758505418896675 old loss 0.00017811097495723516 BETTER
I0311 23:23:31.945240 1755711 finetune.py:68] layer 27_q @ epoch 4 new loss 0.00020744805806316435 old loss 0.00020938408852089196 BETTER
I0311 23:23:39.214230 1755595 finetune.py:68] layer 26_k @ epoch 1 new loss 0.00027073457022197545 old loss 0.0002723133366089314 BETTER
I0311 23:23:41.218598 1755711 finetune.py:45] layer 27_k initial loss 0.00025473374989815056
I0311 23:23:51.239753 1755363 finetune.py:68] layer 24_o @ epoch 0 new loss 0.00040430331137031317 old loss 0.0004149993765167892 BETTER
I0311 23:23:56.525900 1755479 finetune.py:68] layer 25_k @ epoch 4 new loss 0.0001772455289028585 old loss 0.00017758505418896675 BETTER
I0311 23:24:05.741090 1755479 finetune.py:45] layer 25_o initial loss 0.0003557722084224224
I0311 23:24:11.817748 1755595 finetune.py:68] layer 26_k @ epoch 2 new loss 0.000269555690465495 old loss 0.00027073457022197545 BETTER
I0311 23:24:12.721002 1755711 finetune.py:68] layer 27_k @ epoch 0 new loss 0.0002505606389604509 old loss 0.00025473374989815056 BETTER
I0311 23:24:24.503612 1755363 finetune.py:68] layer 24_o @ epoch 1 new loss 0.00039993648533709347 old loss 0.00040430331137031317 BETTER
I0311 23:24:36.552313 1755479 finetune.py:68] layer 25_o @ epoch 0 new loss 0.00034843661705963314 old loss 0.0003557722084224224 BETTER
I0311 23:24:44.135265 1755595 finetune.py:68] layer 26_k @ epoch 3 new loss 0.0002686276566237211 old loss 0.000269555690465495 BETTER
I0311 23:24:44.691544 1755711 finetune.py:68] layer 27_k @ epoch 1 new loss 0.0002490240731276572 old loss 0.0002505606389604509 BETTER
I0311 23:24:57.966488 1755363 finetune.py:68] layer 24_o @ epoch 2 new loss 0.00039690834819339216 old loss 0.00039993648533709347 BETTER
I0311 23:25:08.214771 1755479 finetune.py:68] layer 25_o @ epoch 1 new loss 0.00034511592821218073 old loss 0.00034843661705963314 BETTER
I0311 23:25:16.362229 1755595 finetune.py:68] layer 26_k @ epoch 4 new loss 0.00026776769664138556 old loss 0.0002686276566237211 BETTER
I0311 23:25:16.482228 1755711 finetune.py:68] layer 27_k @ epoch 2 new loss 0.00024784044944681227 old loss 0.0002490240731276572 BETTER
I0311 23:25:25.711951 1755595 finetune.py:45] layer 26_o initial loss 0.000538383552338928
I0311 23:25:31.234485 1755363 finetune.py:68] layer 24_o @ epoch 3 new loss 0.00039483216824010015 old loss 0.00039690834819339216 BETTER
I0311 23:25:39.873399 1755479 finetune.py:68] layer 25_o @ epoch 2 new loss 0.00034308782778680325 old loss 0.00034511592821218073 BETTER
I0311 23:25:48.568246 1755711 finetune.py:68] layer 27_k @ epoch 3 new loss 0.0002470854378771037 old loss 0.00024784044944681227 BETTER
I0311 23:25:56.591994 1755595 finetune.py:68] layer 26_o @ epoch 0 new loss 0.0005210644449107349 old loss 0.000538383552338928 BETTER
I0311 23:26:04.864278 1755363 finetune.py:68] layer 24_o @ epoch 4 new loss 0.0003932434192392975 old loss 0.00039483216824010015 BETTER
I0311 23:26:11.403844 1755479 finetune.py:68] layer 25_o @ epoch 3 new loss 0.00034159180358983576 old loss 0.00034308782778680325 BETTER
I0311 23:26:20.217892 1755363 finetune.py:45] layer 24_up initial loss 0.000920349673833698
I0311 23:26:20.894744 1755711 finetune.py:68] layer 27_k @ epoch 4 new loss 0.000246303592575714 old loss 0.0002470854378771037 BETTER
I0311 23:26:28.345945 1755595 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0005149280186742544 old loss 0.0005210644449107349 BETTER
I0311 23:26:31.436908 1755711 finetune.py:45] layer 27_o initial loss 0.0004781954048667103
I0311 23:26:43.004714 1755479 finetune.py:68] layer 25_o @ epoch 4 new loss 0.00034048737143166363 old loss 0.00034159180358983576 BETTER
I0311 23:26:50.785375 1755363 finetune.py:68] layer 24_up @ epoch 0 new loss 0.000907491019461304 old loss 0.000920349673833698 BETTER
I0311 23:26:58.598610 1755479 finetune.py:45] layer 25_up initial loss 0.0009294059709645808
I0311 23:27:00.350121 1755595 finetune.py:68] layer 26_o @ epoch 2 new loss 0.0005106413154862821 old loss 0.0005149280186742544 BETTER
I0311 23:27:01.871245 1755711 finetune.py:68] layer 27_o @ epoch 0 new loss 0.00046335134538821876 old loss 0.0004781954048667103 BETTER
I0311 23:27:22.323173 1755363 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0009000622085295618 old loss 0.000907491019461304 BETTER
I0311 23:27:27.756265 1755479 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0009150771657004952 old loss 0.0009294059709645808 BETTER
I0311 23:27:32.184238 1755595 finetune.py:68] layer 26_o @ epoch 3 new loss 0.0005075664375908673 old loss 0.0005106413154862821 BETTER
I0311 23:27:33.015714 1755711 finetune.py:68] layer 27_o @ epoch 1 new loss 0.00045680656330659986 old loss 0.00046335134538821876 BETTER
I0311 23:27:53.964833 1755363 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0008944492437876761 old loss 0.0009000622085295618 BETTER
I0311 23:27:57.652226 1755479 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0009072577813640237 old loss 0.0009150771657004952 BETTER
I0311 23:28:04.086205 1755711 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0004524503892753273 old loss 0.00045680656330659986 BETTER
I0311 23:28:04.093458 1755595 finetune.py:68] layer 26_o @ epoch 4 new loss 0.000505317933857441 old loss 0.0005075664375908673 BETTER
I0311 23:28:19.246498 1755595 finetune.py:45] layer 26_up initial loss 0.0011499542742967606
I0311 23:28:25.738587 1755363 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0008899561362341046 old loss 0.0008944492437876761 BETTER
I0311 23:28:27.407775 1755479 finetune.py:68] layer 25_up @ epoch 2 new loss 0.0009012259542942047 old loss 0.0009072577813640237 BETTER
I0311 23:28:35.022934 1755711 finetune.py:68] layer 27_o @ epoch 3 new loss 0.00044934326433576643 old loss 0.0004524503892753273 BETTER
I0311 23:28:48.460416 1755595 finetune.py:68] layer 26_up @ epoch 0 new loss 0.0011331425048410892 old loss 0.0011499542742967606 BETTER
I0311 23:28:57.394052 1755479 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0008964523440226912 old loss 0.0009012259542942047 BETTER
I0311 23:28:57.476671 1755363 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0008862864342518151 old loss 0.0008899561362341046 BETTER
I0311 23:29:06.073725 1755711 finetune.py:68] layer 27_o @ epoch 4 new loss 0.00044703204184770584 old loss 0.00044934326433576643 BETTER
I0311 23:29:12.836978 1755363 finetune.py:45] layer 24_gate initial loss 0.0013109096325933933
I0311 23:29:18.632176 1755595 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0011240914463996887 old loss 0.0011331425048410892 BETTER
I0311 23:29:21.407150 1755711 finetune.py:45] layer 27_up initial loss 0.0011790577555075288
I0311 23:29:27.200944 1755479 finetune.py:68] layer 25_up @ epoch 4 new loss 0.0008924689027480781 old loss 0.0008964523440226912 BETTER
I0311 23:29:41.850725 1755363 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0013049970148131251 old loss 0.0013109096325933933 BETTER
I0311 23:29:42.198600 1755479 finetune.py:45] layer 25_gate initial loss 0.0013659807154908776
I0311 23:29:48.622694 1755595 finetune.py:68] layer 26_up @ epoch 2 new loss 0.001117152627557516 old loss 0.0011240914463996887 BETTER
I0311 23:29:50.098268 1755711 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0011561845894902945 old loss 0.0011790577555075288 BETTER
I0311 23:30:09.829638 1755479 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.0013597301440313458 old loss 0.0013659807154908776 BETTER
I0311 23:30:11.662506 1755363 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0013003888307139277 old loss 0.0013049970148131251 BETTER
I0311 23:30:18.512336 1755595 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0011116776149719954 old loss 0.001117152627557516 BETTER
I0311 23:30:19.535156 1755711 finetune.py:68] layer 27_up @ epoch 1 new loss 0.001144424662925303 old loss 0.0011561845894902945 BETTER
I0311 23:30:38.062742 1755479 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.0013551423326134682 old loss 0.0013597301440313458 BETTER
I0311 23:30:41.448162 1755363 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.0012967264046892524 old loss 0.0013003888307139277 BETTER
I0311 23:30:48.999901 1755595 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0011070712935179472 old loss 0.0011116776149719954 BETTER
I0311 23:30:49.344444 1755711 finetune.py:68] layer 27_up @ epoch 2 new loss 0.001135844737291336 old loss 0.001144424662925303 BETTER
I0311 23:31:04.815990 1755595 finetune.py:45] layer 26_gate initial loss 0.001640551956370473
I0311 23:31:06.387729 1755479 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.0013514437014237046 old loss 0.0013551423326134682 BETTER
I0311 23:31:11.391731 1755363 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0012938121799379587 old loss 0.0012967264046892524 BETTER
I0311 23:31:18.912699 1755711 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0011291764676570892 old loss 0.001135844737291336 BETTER
I0311 23:31:32.262612 1755595 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0016333040548488498 old loss 0.001640551956370473 BETTER
I0311 23:31:34.461756 1755479 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.0013483933871611953 old loss 0.0013514437014237046 BETTER
I0311 23:31:41.481427 1755363 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.0012912206584587693 old loss 0.0012938121799379587 BETTER
I0311 23:31:48.519251 1755711 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0011238001752644777 old loss 0.0011291764676570892 BETTER
I0311 23:31:57.841807 1755363 finetune.py:45] layer 24_down initial loss 0.0020618147682398558
I0311 23:32:00.678474 1755595 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.001627925201319158 old loss 0.0016333040548488498 BETTER
I0311 23:32:02.795133 1755479 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0013458200264722109 old loss 0.0013483933871611953 BETTER
I0311 23:32:04.481466 1755711 finetune.py:45] layer 27_gate initial loss 0.00173731311224401
I0311 23:32:18.849224 1755479 finetune.py:45] layer 25_down initial loss 0.0021525437477976084
I0311 23:32:25.382721 1755363 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0020615661051124334 old loss 0.0020618147682398558 BETTER
I0311 23:32:29.108891 1755595 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0016236831434071064 old loss 0.001627925201319158 BETTER
I0311 23:32:31.699258 1755711 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0017277533188462257 old loss 0.00173731311224401 BETTER
I0311 23:32:44.682266 1755479 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0021522773895412683 old loss 0.0021525437477976084 BETTER
I0311 23:32:53.970324 1755363 finetune.py:68] layer 24_down @ epoch 1 new loss 0.002061391482129693 old loss 0.0020615661051124334 BETTER
I0311 23:32:57.595292 1755595 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0016200871905311942 old loss 0.0016236831434071064 BETTER
I0311 23:33:00.408999 1755711 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.0017213416285812855 old loss 0.0017277533188462257 BETTER
I0311 23:33:11.696756 1755479 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0021521062590181828 old loss 0.0021522773895412683 BETTER
I0311 23:33:22.525519 1755363 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0020612687803804874 old loss 0.002061391482129693 BETTER
I0311 23:33:26.140251 1755595 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0016171607421711087 old loss 0.0016200871905311942 BETTER
I0311 23:33:29.075196 1755711 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.001716305618174374 old loss 0.0017213416285812855 BETTER
I0311 23:33:38.672118 1755479 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0021519900765269995 old loss 0.0021521062590181828 BETTER
I0311 23:33:43.007079 1755595 finetune.py:45] layer 26_down initial loss 0.0025068982504308224
I0311 23:33:51.257100 1755363 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0020611807703971863 old loss 0.0020612687803804874 BETTER
I0311 23:33:57.587523 1755711 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.001712004654109478 old loss 0.001716305618174374 BETTER
I0311 23:34:05.681576 1755479 finetune.py:68] layer 25_down @ epoch 3 new loss 0.002151902997866273 old loss 0.0021519900765269995 BETTER
I0311 23:34:09.323132 1755595 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0025066444650292397 old loss 0.0025068982504308224 BETTER
I0311 23:34:19.730863 1755363 finetune.py:68] layer 24_down @ epoch 4 new loss 0.002061112318187952 old loss 0.0020611807703971863 BETTER
24_v proxy err 0.009358677081763744 tr(WHW.T) 1394.900634765625
24_q proxy err 0.0019283825531601906 tr(WHW.T) 7023.93505859375
24_k proxy err 0.0013267210451886058 tr(WHW.T) 10339.3984375
24_o proxy err 0.0065783909521996975 tr(WHW.T) 134.3353729248047
24_up proxy err 0.008671645075082779 tr(WHW.T) 2622.170166015625
24_gate proxy err 0.0053900619968771935 tr(WHW.T) 4264.11962890625
24_down proxy err 0.009234687313437462 tr(WHW.T) 341.5169372558594
I0311 23:34:26.319392 1755711 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.001708481926470995 old loss 0.001712004654109478 BETTER
I0311 23:34:33.033100 1755479 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0021518359426409006 old loss 0.002151902997866273 BETTER
25_v proxy err 0.008900674991309643 tr(WHW.T) 1707.664794921875
25_q proxy err 0.002184399403631687 tr(WHW.T) 7164.98486328125
25_k proxy err 0.0016403814079239964 tr(WHW.T) 9623.015625
25_o proxy err 0.008319566957652569 tr(WHW.T) 83.88919830322266
25_up proxy err 0.008584701456129551 tr(WHW.T) 2805.920166015625
25_gate proxy err 0.005220869090408087 tr(WHW.T) 4666.35009765625
25_down proxy err 0.00883168913424015 tr(WHW.T) 374.89129638671875
I0311 23:34:36.356022 1755595 finetune.py:68] layer 26_down @ epoch 1 new loss 0.002506460063159466 old loss 0.0025066444650292397 BETTER
I0311 23:34:42.232203 1755711 finetune.py:45] layer 27_down initial loss 0.002719666576012969
I0311 23:35:03.108558 1755595 finetune.py:68] layer 26_down @ epoch 2 new loss 0.002506315242499113 old loss 0.002506460063159466 BETTER
I0311 23:35:07.659506 1755711 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0027193964924663305 old loss 0.002719666576012969 BETTER
I0311 23:35:30.081117 1755595 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0025062020868062973 old loss 0.002506315242499113 BETTER
I0311 23:35:34.117869 1755711 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0027191985864192247 old loss 0.0027193964924663305 BETTER
I0311 23:35:51.313185 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 28 in 73.4405779838562s
I0311 23:35:54.607153 1755827 config.py:54] PyTorch version 2.1.1 available.
I0311 23:35:55.655388 1752459 quantize_finetune_llama.py:183] layer 29 gpu 1
I0311 23:35:55.725808 1755827 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 23:35:57.008526 1755595 finetune.py:68] layer 26_down @ epoch 4 new loss 0.00250611687079072 old loss 0.0025062020868062973 BETTER
26_v proxy err 0.008677116595208645 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.0020035055931657553 tr(WHW.T) 7473.2783203125
26_k proxy err 0.0014430928276851773 tr(WHW.T) 10503.3310546875
26_o proxy err 0.005216929130256176 tr(WHW.T) 203.29246520996094
26_up proxy err 0.008056561462581158 tr(WHW.T) 3155.558837890625
26_gate proxy err 0.004848484881222248 tr(WHW.T) 5301.736328125
26_down proxy err 0.009069269523024559 tr(WHW.T) 402.8534240722656
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 23:36:00.549825 1755711 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0027190453838557005 old loss 0.0027191985864192247 BETTER
I0311 23:36:04.141021 1755827 finetune.py:45] layer 28_v initial loss 0.0003725426213350147
I0311 23:36:26.854134 1755711 finetune.py:68] layer 27_down @ epoch 3 new loss 0.00271893129684031 old loss 0.0027190453838557005 BETTER
I0311 23:36:37.088834 1755827 finetune.py:68] layer 28_v @ epoch 0 new loss 0.0002462341508362442 old loss 0.0003725426213350147 BETTER
I0311 23:36:53.094330 1755711 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0027188363019376993 old loss 0.00271893129684031 BETTER
27_v proxy err 0.008374286815524101 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.0020178237464278936 tr(WHW.T) 7693.03369140625
27_k proxy err 0.001473920769058168 tr(WHW.T) 10635.3212890625
27_o proxy err 0.0069436924532055855 tr(WHW.T) 126.79846954345703
27_up proxy err 0.0073171197436749935 tr(WHW.T) 3691.554931640625
27_gate proxy err 0.004554685205221176 tr(WHW.T) 5989.0068359375
27_down proxy err 0.00890322681516409 tr(WHW.T) 468.771240234375
I0311 23:37:11.372554 1755827 finetune.py:68] layer 28_v @ epoch 1 new loss 0.00023585425515193492 old loss 0.0002462341508362442 BETTER
I0311 23:37:11.581728 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 29 in 70.0633156299591s
I0311 23:37:14.776868 1755943 config.py:54] PyTorch version 2.1.1 available.
I0311 23:37:15.765681 1752459 quantize_finetune_llama.py:183] layer 30 gpu 2
I0311 23:37:15.832616 1755943 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 23:37:23.869927 1755943 finetune.py:45] layer 29_v initial loss 0.00036908427136950195
I0311 23:37:45.961437 1755827 finetune.py:68] layer 28_v @ epoch 2 new loss 0.00023001094814389944 old loss 0.00023585425515193492 BETTER
I0311 23:37:55.105996 1755943 finetune.py:68] layer 29_v @ epoch 0 new loss 0.0002737857575993985 old loss 0.00036908427136950195 BETTER
I0311 23:38:20.667007 1755827 finetune.py:68] layer 28_v @ epoch 3 new loss 0.0002257583400933072 old loss 0.00023001094814389944 BETTER
I0311 23:38:25.010956 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 30 in 68.81491136550903s
I0311 23:38:27.197843 1755943 finetune.py:68] layer 29_v @ epoch 1 new loss 0.0002624392800498754 old loss 0.0002737857575993985 BETTER
I0311 23:38:28.197432 1756059 config.py:54] PyTorch version 2.1.1 available.
I0311 23:38:29.178828 1752459 quantize_finetune_llama.py:183] layer 31 gpu 3
I0311 23:38:29.252613 1756059 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 23:38:37.470219 1756059 finetune.py:45] layer 30_v initial loss 0.0003849656495731324
I0311 23:38:55.496683 1755827 finetune.py:68] layer 28_v @ epoch 4 new loss 0.00022238279052544385 old loss 0.0002257583400933072 BETTER
I0311 23:38:59.427671 1755943 finetune.py:68] layer 29_v @ epoch 2 new loss 0.00025596353225409985 old loss 0.0002624392800498754 BETTER
I0311 23:39:04.984669 1755827 finetune.py:45] layer 28_q initial loss 0.0002951810893137008
I0311 23:39:09.168089 1756059 finetune.py:68] layer 30_v @ epoch 0 new loss 0.0002593868412077427 old loss 0.0003849656495731324 BETTER
I0311 23:39:32.121323 1755943 finetune.py:68] layer 29_v @ epoch 3 new loss 0.0002517661196179688 old loss 0.00025596353225409985 BETTER
I0311 23:39:38.469162 1755827 finetune.py:68] layer 28_q @ epoch 0 new loss 0.00028009197558276355 old loss 0.0002951810893137008 BETTER
I0311 23:39:40.856409 1752459 quantize_finetune_llama.py:210] computed original embedding for layer 31 in 71.26973962783813s
I0311 23:39:41.521531 1756059 finetune.py:68] layer 30_v @ epoch 1 new loss 0.0002485748555045575 old loss 0.0002593868412077427 BETTER
I0311 23:39:44.157373 1756175 config.py:54] PyTorch version 2.1.1 available.
I0311 23:39:45.313506 1756175 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 23:39:53.714448 1756175 finetune.py:45] layer 31_v initial loss 0.0005922308773733675
I0311 23:40:04.878322 1755943 finetune.py:68] layer 29_v @ epoch 4 new loss 0.0002481984265614301 old loss 0.0002517661196179688 BETTER
I0311 23:40:13.520917 1755827 finetune.py:68] layer 28_q @ epoch 1 new loss 0.00027479775599204004 old loss 0.00028009197558276355 BETTER
I0311 23:40:14.753281 1756059 finetune.py:68] layer 30_v @ epoch 2 new loss 0.00024345741258002818 old loss 0.0002485748555045575 BETTER
I0311 23:40:14.898014 1755943 finetune.py:45] layer 29_q initial loss 0.0003117698070127517
I0311 23:40:24.931520 1756175 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0004056781472172588 old loss 0.0005922308773733675 BETTER
I0311 23:40:46.777752 1755943 finetune.py:68] layer 29_q @ epoch 0 new loss 0.00030004724976606667 old loss 0.0003117698070127517 BETTER
I0311 23:40:47.936178 1756059 finetune.py:68] layer 30_v @ epoch 3 new loss 0.00023839208006393164 old loss 0.00024345741258002818 BETTER
I0311 23:40:48.154906 1755827 finetune.py:68] layer 28_q @ epoch 2 new loss 0.0002709002001211047 old loss 0.00027479775599204004 BETTER
I0311 23:40:57.093441 1756175 finetune.py:68] layer 31_v @ epoch 1 new loss 0.0003883632889483124 old loss 0.0004056781472172588 BETTER
I0311 23:41:19.408201 1755943 finetune.py:68] layer 29_q @ epoch 1 new loss 0.00029524738783948123 old loss 0.00030004724976606667 BETTER
I0311 23:41:21.250254 1756059 finetune.py:68] layer 30_v @ epoch 4 new loss 0.00023580578272230923 old loss 0.00023839208006393164 BETTER
I0311 23:41:22.805477 1755827 finetune.py:68] layer 28_q @ epoch 3 new loss 0.0002679678436834365 old loss 0.0002709002001211047 BETTER
I0311 23:41:29.391847 1756175 finetune.py:68] layer 31_v @ epoch 2 new loss 0.0003709065495058894 old loss 0.0003883632889483124 BETTER
I0311 23:41:30.906907 1756059 finetune.py:45] layer 30_q initial loss 0.00032832156284712255
I0311 23:41:51.797778 1755943 finetune.py:68] layer 29_q @ epoch 2 new loss 0.00029151421040296555 old loss 0.00029524738783948123 BETTER
I0311 23:41:57.405712 1755827 finetune.py:68] layer 28_q @ epoch 4 new loss 0.0002653498959261924 old loss 0.0002679678436834365 BETTER
I0311 23:42:01.777820 1756175 finetune.py:76] layer 31_v @ epoch 3 new loss 0.00037435011472553015 old loss 0.0003709065495058894 WORSE
I0311 23:42:02.358877 1756059 finetune.py:68] layer 30_q @ epoch 0 new loss 0.00030499754939228296 old loss 0.00032832156284712255 BETTER
I0311 23:42:07.247738 1755827 finetune.py:45] layer 28_k initial loss 0.0003264199767727405
I0311 23:42:24.268070 1755943 finetune.py:68] layer 29_q @ epoch 3 new loss 0.00028880962054245174 old loss 0.00029151421040296555 BETTER
I0311 23:42:33.827563 1756175 finetune.py:68] layer 31_v @ epoch 4 new loss 0.00036647083470597863 old loss 0.0003709065495058894 BETTER
I0311 23:42:34.830553 1756059 finetune.py:68] layer 30_q @ epoch 1 new loss 0.000299068313324824 old loss 0.00030499754939228296 BETTER
I0311 23:42:40.512371 1755827 finetune.py:68] layer 28_k @ epoch 0 new loss 0.00032118154922500253 old loss 0.0003264199767727405 BETTER
I0311 23:42:43.903033 1756175 finetune.py:45] layer 31_q initial loss 0.0007601369870826602
I0311 23:42:56.863381 1755943 finetune.py:68] layer 29_q @ epoch 4 new loss 0.0002863313420675695 old loss 0.00028880962054245174 BETTER
I0311 23:43:06.506671 1755943 finetune.py:45] layer 29_k initial loss 0.0003446976188570261
I0311 23:43:07.359467 1756059 finetune.py:68] layer 30_q @ epoch 2 new loss 0.00029431379516609013 old loss 0.000299068313324824 BETTER
I0311 23:43:14.394451 1755827 finetune.py:68] layer 28_k @ epoch 1 new loss 0.0003192722506355494 old loss 0.00032118154922500253 BETTER
I0311 23:43:15.483264 1756175 finetune.py:68] layer 31_q @ epoch 0 new loss 0.000611461466178298 old loss 0.0007601369870826602 BETTER
I0311 23:43:38.105639 1755943 finetune.py:68] layer 29_k @ epoch 0 new loss 0.0003406110918149352 old loss 0.0003446976188570261 BETTER
I0311 23:43:40.012075 1756059 finetune.py:68] layer 30_q @ epoch 3 new loss 0.00029107453883625567 old loss 0.00029431379516609013 BETTER
I0311 23:43:48.145437 1756175 finetune.py:68] layer 31_q @ epoch 1 new loss 0.000584418885409832 old loss 0.000611461466178298 BETTER
I0311 23:43:48.678641 1755827 finetune.py:68] layer 28_k @ epoch 2 new loss 0.000317752972478047 old loss 0.0003192722506355494 BETTER
I0311 23:44:10.457719 1755943 finetune.py:68] layer 29_k @ epoch 1 new loss 0.00033850877662189305 old loss 0.0003406110918149352 BETTER
I0311 23:44:12.939943 1756059 finetune.py:68] layer 30_q @ epoch 4 new loss 0.0002873809717129916 old loss 0.00029107453883625567 BETTER
I0311 23:44:20.569337 1756175 finetune.py:68] layer 31_q @ epoch 2 new loss 0.0005793392192572355 old loss 0.000584418885409832 BETTER
I0311 23:44:22.731242 1756059 finetune.py:45] layer 30_k initial loss 0.0003632035222835839
I0311 23:44:22.891312 1755827 finetune.py:68] layer 28_k @ epoch 3 new loss 0.0003167151880916208 old loss 0.000317752972478047 BETTER
I0311 23:44:42.743021 1755943 finetune.py:68] layer 29_k @ epoch 2 new loss 0.0003369814658071846 old loss 0.00033850877662189305 BETTER
I0311 23:44:52.984495 1756175 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0005656321300193667 old loss 0.0005793392192572355 BETTER
I0311 23:44:54.513424 1756059 finetune.py:68] layer 30_k @ epoch 0 new loss 0.00035508815199136734 old loss 0.0003632035222835839 BETTER
I0311 23:44:56.759163 1755827 finetune.py:68] layer 28_k @ epoch 4 new loss 0.00031580004724673927 old loss 0.0003167151880916208 BETTER
I0311 23:45:06.139955 1755827 finetune.py:45] layer 28_o initial loss 0.0005998868145979941
I0311 23:45:14.875675 1755943 finetune.py:68] layer 29_k @ epoch 3 new loss 0.0003357782552484423 old loss 0.0003369814658071846 BETTER
I0311 23:45:25.278167 1756175 finetune.py:68] layer 31_q @ epoch 4 new loss 0.0005598388379439712 old loss 0.0005656321300193667 BETTER
I0311 23:45:26.997381 1756059 finetune.py:68] layer 30_k @ epoch 1 new loss 0.00035244220634922385 old loss 0.00035508815199136734 BETTER
I0311 23:45:34.626214 1756175 finetune.py:45] layer 31_k initial loss 0.0007449524709954858
I0311 23:45:38.779208 1755827 finetune.py:68] layer 28_o @ epoch 0 new loss 0.0005801765946671367 old loss 0.0005998868145979941 BETTER
I0311 23:45:47.067014 1755943 finetune.py:68] layer 29_k @ epoch 4 new loss 0.0003348102909512818 old loss 0.0003357782552484423 BETTER
I0311 23:45:56.498140 1755943 finetune.py:45] layer 29_o initial loss 0.0006210723659023643
I0311 23:45:59.187075 1756059 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0003508299414534122 old loss 0.00035244220634922385 BETTER
I0311 23:46:05.825283 1756175 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0006392713985405862 old loss 0.0007449524709954858 BETTER
I0311 23:46:12.343577 1755827 finetune.py:68] layer 28_o @ epoch 1 new loss 0.0005725995870307088 old loss 0.0005801765946671367 BETTER
I0311 23:46:27.471504 1755943 finetune.py:68] layer 29_o @ epoch 0 new loss 0.0006008567288517952 old loss 0.0006210723659023643 BETTER
I0311 23:46:31.278924 1756059 finetune.py:68] layer 30_k @ epoch 3 new loss 0.0003494764678180218 old loss 0.0003508299414534122 BETTER
I0311 23:46:37.912327 1756175 finetune.py:68] layer 31_k @ epoch 1 new loss 0.000630194554105401 old loss 0.0006392713985405862 BETTER
I0311 23:46:45.759883 1755827 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0005672664265148342 old loss 0.0005725995870307088 BETTER
I0311 23:46:59.163498 1755943 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0005941058625467122 old loss 0.0006008567288517952 BETTER
I0311 23:47:03.425292 1756059 finetune.py:68] layer 30_k @ epoch 4 new loss 0.00034824994509108365 old loss 0.0003494764678180218 BETTER
I0311 23:47:09.866572 1756175 finetune.py:68] layer 31_k @ epoch 2 new loss 0.0006238574278540909 old loss 0.000630194554105401 BETTER
I0311 23:47:13.097281 1756059 finetune.py:45] layer 30_o initial loss 0.0006883685127831995
I0311 23:47:19.332451 1755827 finetune.py:68] layer 28_o @ epoch 3 new loss 0.0005636439309455454 old loss 0.0005672664265148342 BETTER
I0311 23:47:30.834645 1755943 finetune.py:68] layer 29_o @ epoch 2 new loss 0.0005897563532926142 old loss 0.0005941058625467122 BETTER
I0311 23:47:42.130250 1756175 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0006184455705806613 old loss 0.0006238574278540909 BETTER
I0311 23:47:44.328216 1756059 finetune.py:68] layer 30_o @ epoch 0 new loss 0.0006499685696326196 old loss 0.0006883685127831995 BETTER
I0311 23:47:52.812749 1755827 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0005607156781479716 old loss 0.0005636439309455454 BETTER
I0311 23:48:02.545642 1755943 finetune.py:68] layer 29_o @ epoch 3 new loss 0.0005868214648216963 old loss 0.0005897563532926142 BETTER
I0311 23:48:08.250514 1755827 finetune.py:45] layer 28_up initial loss 0.0014134958619251847
I0311 23:48:14.365343 1756175 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0006147853564471006 old loss 0.0006184455705806613 BETTER
I0311 23:48:16.185396 1756059 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0006366979796439409 old loss 0.0006499685696326196 BETTER
I0311 23:48:23.694108 1756175 finetune.py:45] layer 31_o initial loss 0.0011788905831053853
I0311 23:48:34.257900 1755943 finetune.py:68] layer 29_o @ epoch 4 new loss 0.0005846424610354006 old loss 0.0005868214648216963 BETTER
I0311 23:48:38.994401 1755827 finetune.py:68] layer 28_up @ epoch 0 new loss 0.001381727633997798 old loss 0.0014134958619251847 BETTER
I0311 23:48:47.955207 1756059 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0006286433199420571 old loss 0.0006366979796439409 BETTER
I0311 23:48:49.146155 1755943 finetune.py:45] layer 29_up initial loss 0.0015918943099677563
I0311 23:48:54.091095 1756175 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0010172462789341807 old loss 0.0011788905831053853 BETTER
I0311 23:49:10.656060 1755827 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0013665537117049098 old loss 0.001381727633997798 BETTER
I0311 23:49:19.271500 1755943 finetune.py:68] layer 29_up @ epoch 0 new loss 0.001542958547361195 old loss 0.0015918943099677563 BETTER
I0311 23:49:20.849266 1756059 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0006227727863006294 old loss 0.0006286433199420571 BETTER
I0311 23:49:25.323787 1756175 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0009739002562128007 old loss 0.0010172462789341807 BETTER
I0311 23:49:42.447032 1755827 finetune.py:68] layer 28_up @ epoch 2 new loss 0.001355599146336317 old loss 0.0013665537117049098 BETTER
I0311 23:49:49.304847 1755943 finetune.py:68] layer 29_up @ epoch 1 new loss 0.0015231063589453697 old loss 0.001542958547361195 BETTER
I0311 23:49:52.889722 1756059 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0006188515690155327 old loss 0.0006227727863006294 BETTER
I0311 23:49:56.683059 1756175 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0009508830262348056 old loss 0.0009739002562128007 BETTER
I0311 23:50:09.046049 1756059 finetune.py:45] layer 30_up initial loss 0.0022358973510563374
I0311 23:50:14.423342 1755827 finetune.py:68] layer 28_up @ epoch 3 new loss 0.0013471036218106747 old loss 0.001355599146336317 BETTER
I0311 23:50:19.496151 1755943 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0015089777298271656 old loss 0.0015231063589453697 BETTER
I0311 23:50:28.012001 1756175 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0009363682474941015 old loss 0.0009508830262348056 BETTER
I0311 23:50:38.518393 1756059 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0020875646732747555 old loss 0.0022358973510563374 BETTER
I0311 23:50:46.441344 1755827 finetune.py:68] layer 28_up @ epoch 4 new loss 0.001340011483989656 old loss 0.0013471036218106747 BETTER
I0311 23:50:49.792831 1755943 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0014981110580265522 old loss 0.0015089777298271656 BETTER
I0311 23:50:59.807256 1756175 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0009266413399018347 old loss 0.0009363682474941015 BETTER
I0311 23:51:03.217272 1755827 finetune.py:45] layer 28_gate initial loss 0.002072844421491027
I0311 23:51:08.701790 1756059 finetune.py:68] layer 30_up @ epoch 1 new loss 0.002035499317571521 old loss 0.0020875646732747555 BETTER
I0311 23:51:15.837161 1756175 finetune.py:45] layer 31_up initial loss 0.005170593969523907
I0311 23:51:19.612418 1755943 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0014893418410792947 old loss 0.0014981110580265522 BETTER
I0311 23:51:32.150354 1755827 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0020593316294252872 old loss 0.002072844421491027 BETTER
I0311 23:51:35.827467 1755943 finetune.py:45] layer 29_gate initial loss 0.0023600023705512285
I0311 23:51:38.716548 1756059 finetune.py:68] layer 30_up @ epoch 2 new loss 0.001999832224100828 old loss 0.002035499317571521 BETTER
I0311 23:51:44.421981 1756175 finetune.py:68] layer 31_up @ epoch 0 new loss 0.004277851898223162 old loss 0.005170593969523907 BETTER
I0311 23:52:02.111688 1755827 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.002050576265901327 old loss 0.0020593316294252872 BETTER
I0311 23:52:03.627024 1755943 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0023398173507303 old loss 0.0023600023705512285 BETTER
I0311 23:52:08.793247 1756059 finetune.py:68] layer 30_up @ epoch 3 new loss 0.00197167182341218 old loss 0.001999832224100828 BETTER
I0311 23:52:13.912186 1756175 finetune.py:68] layer 31_up @ epoch 1 new loss 0.004045760724693537 old loss 0.004277851898223162 BETTER
I0311 23:52:32.286019 1755827 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.002043704967945814 old loss 0.002050576265901327 BETTER
I0311 23:52:32.540375 1755943 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.002328065689653158 old loss 0.0023398173507303 BETTER
I0311 23:52:38.650553 1756059 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0019494476728141308 old loss 0.00197167182341218 BETTER
I0311 23:52:43.462434 1756175 finetune.py:68] layer 31_up @ epoch 2 new loss 0.0038854237645864487 old loss 0.004045760724693537 BETTER
I0311 23:52:54.177002 1756059 finetune.py:45] layer 30_gate initial loss 0.0030291671864688396
I0311 23:53:02.668609 1755943 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.002318819286301732 old loss 0.002328065689653158 BETTER
I0311 23:53:02.771460 1755827 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0020381598733365536 old loss 0.002043704967945814 BETTER
I0311 23:53:13.053634 1756175 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0037592884618788958 old loss 0.0038854237645864487 BETTER
I0311 23:53:21.840921 1756059 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0029753202106803656 old loss 0.0030291671864688396 BETTER
I0311 23:53:31.102418 1755943 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0023114820942282677 old loss 0.002318819286301732 BETTER
I0311 23:53:32.682187 1755827 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.002033462282270193 old loss 0.0020381598733365536 BETTER
I0311 23:53:42.538934 1756175 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0036581195890903473 old loss 0.0037592884618788958 BETTER
I0311 23:53:48.692234 1755827 finetune.py:45] layer 28_down initial loss 0.0032663976307958364
I0311 23:53:49.962883 1756059 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.0029495791532099247 old loss 0.0029753202106803656 BETTER
I0311 23:53:58.031803 1756175 finetune.py:45] layer 31_gate initial loss 0.0055894628167152405
I0311 23:53:59.604302 1755943 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0023055647034198046 old loss 0.0023114820942282677 BETTER
I0311 23:54:15.541080 1755943 finetune.py:45] layer 29_down initial loss 0.0038591958582401276
I0311 23:54:16.018873 1755827 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0032660397700965405 old loss 0.0032663976307958364 BETTER
I0311 23:54:18.407457 1756059 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.002930518239736557 old loss 0.0029495791532099247 BETTER
I0311 23:54:25.631664 1756175 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.0052565340884029865 old loss 0.0055894628167152405 BETTER
I0311 23:54:41.784693 1755943 finetune.py:68] layer 29_down @ epoch 0 new loss 0.003858641255646944 old loss 0.0038591958582401276 BETTER
I0311 23:54:44.751305 1755827 finetune.py:68] layer 28_down @ epoch 1 new loss 0.00326575618237257 old loss 0.0032660397700965405 BETTER
I0311 23:54:46.812920 1756059 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.002915268298238516 old loss 0.002930518239736557 BETTER
I0311 23:54:53.757225 1756175 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.005149717442691326 old loss 0.0052565340884029865 BETTER
I0311 23:55:08.892462 1755943 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0038581998087465763 old loss 0.003858641255646944 BETTER
I0311 23:55:13.399511 1755827 finetune.py:68] layer 28_down @ epoch 2 new loss 0.003265534993261099 old loss 0.00326575618237257 BETTER
I0311 23:55:15.435678 1756059 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.00290202465839684 old loss 0.002915268298238516 BETTER
I0311 23:55:22.017884 1756175 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.005078914109617472 old loss 0.005149717442691326 BETTER
I0311 23:55:32.724176 1756059 finetune.py:45] layer 30_down initial loss 0.0056893047876656055
I0311 23:55:36.012532 1755943 finetune.py:68] layer 29_down @ epoch 2 new loss 0.003857843577861786 old loss 0.0038581998087465763 BETTER
I0311 23:55:41.917556 1755827 finetune.py:68] layer 28_down @ epoch 3 new loss 0.003265354549512267 old loss 0.003265534993261099 BETTER
I0311 23:55:50.284184 1756175 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.005021984688937664 old loss 0.005078914109617472 BETTER
I0311 23:56:00.377030 1756059 finetune.py:68] layer 30_down @ epoch 0 new loss 0.005676411557942629 old loss 0.0056893047876656055 BETTER
I0311 23:56:03.725648 1755943 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0038575457874685526 old loss 0.003857843577861786 BETTER
I0311 23:56:10.446784 1755827 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0032652083318680525 old loss 0.003265354549512267 BETTER
28_v proxy err 0.007751107681542635 tr(WHW.T) 2018.944091796875
28_q proxy err 0.002096484648063779 tr(WHW.T) 7652.29541015625
28_k proxy err 0.0015319096855819225 tr(WHW.T) 10564.892578125
28_o proxy err 0.005668953061103821 tr(WHW.T) 195.61520385742188
28_up proxy err 0.006090150680392981 tr(WHW.T) 4661.22119140625
28_gate proxy err 0.004367569927126169 tr(WHW.T) 6543.60107421875
28_down proxy err 0.008475709706544876 tr(WHW.T) 606.867431640625
I0311 23:56:18.497799 1756175 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.004974036477506161 old loss 0.005021984688937664 BETTER
I0311 23:56:27.528450 1756059 finetune.py:68] layer 30_down @ epoch 1 new loss 0.0056657008826732635 old loss 0.005676411557942629 BETTER
I0311 23:56:30.923104 1755943 finetune.py:68] layer 29_down @ epoch 4 new loss 0.0038572975900024176 old loss 0.0038575457874685526 BETTER
29_v proxy err 0.00813238974660635 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.0020720059983432293 tr(WHW.T) 7229.6640625
29_k proxy err 0.0014338268665596843 tr(WHW.T) 10583.974609375
29_o proxy err 0.00514671066775918 tr(WHW.T) 208.42776489257812
29_up proxy err 0.004840357694774866 tr(WHW.T) 6073.7314453125
29_gate proxy err 0.004005467984825373 tr(WHW.T) 7366.2978515625
29_down proxy err 0.008221824653446674 tr(WHW.T) 787.1956176757812
I0311 23:56:34.587099 1756175 finetune.py:45] layer 31_down initial loss 0.011998403817415237
I0311 23:56:54.984364 1756059 finetune.py:68] layer 30_down @ epoch 2 new loss 0.005656557623296976 old loss 0.0056657008826732635 BETTER
I0311 23:56:59.836810 1756175 finetune.py:68] layer 31_down @ epoch 0 new loss 0.011965563520789146 old loss 0.011998403817415237 BETTER
I0311 23:57:22.015809 1756059 finetune.py:68] layer 30_down @ epoch 3 new loss 0.005648610182106495 old loss 0.005656557623296976 BETTER
I0311 23:57:26.056509 1756175 finetune.py:68] layer 31_down @ epoch 1 new loss 0.011946329846978188 old loss 0.011965563520789146 BETTER
I0311 23:57:48.947965 1756059 finetune.py:68] layer 30_down @ epoch 4 new loss 0.005641688592731953 old loss 0.005648610182106495 BETTER
30_v proxy err 0.006966869346797466 tr(WHW.T) 2261.489501953125
30_q proxy err 0.0020614026580005884 tr(WHW.T) 7817.3603515625
30_k proxy err 0.0015427485341206193 tr(WHW.T) 10546.9873046875
30_o proxy err 0.004940324928611517 tr(WHW.T) 252.81735229492188
30_up proxy err 0.002973336260765791 tr(WHW.T) 10018.6796875
30_gate proxy err 0.002719003241509199 tr(WHW.T) 10988.9296875
30_down proxy err 0.0032699820585548878 tr(WHW.T) 3599.29833984375
I0311 23:57:52.367594 1756175 finetune.py:68] layer 31_down @ epoch 2 new loss 0.011934931389987469 old loss 0.011946329846978188 BETTER
I0311 23:58:18.881556 1756175 finetune.py:68] layer 31_down @ epoch 3 new loss 0.011928037740290165 old loss 0.011934931389987469 BETTER
I0311 23:58:45.452534 1756175 finetune.py:68] layer 31_down @ epoch 4 new loss 0.011923406273126602 old loss 0.011928037740290165 BETTER
31_v proxy err 0.008417473174631596 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.0016342955641448498 tr(WHW.T) 6860.408203125
31_k proxy err 0.0011078179813921452 tr(WHW.T) 10272.69140625
31_o proxy err 0.003520973725244403 tr(WHW.T) 459.1706848144531
31_up proxy err 0.0017102661076933146 tr(WHW.T) 14556.58984375
31_gate proxy err 0.0016843872144818306 tr(WHW.T) 14832.765625
31_down proxy err 0.0016687947791069746 tr(WHW.T) 18036.322265625
