I0318 12:38:57.689087 626522 config.py:54] PyTorch version 2.6.0 available.
W0318 12:38:57.997662 626522 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0318 12:38:58.234887 626522 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.10it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.69it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.52it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.48it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.72it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.08it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.60it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.00it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.09it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.04it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.96it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.09it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.03it/s]
I0318 12:39:00.997879 626522 hfize_llama.py:153] loaded layer 0
I0318 12:39:02.355389 626522 hfize_llama.py:153] loaded layer 1
I0318 12:39:03.744237 626522 hfize_llama.py:153] loaded layer 2
I0318 12:39:05.031673 626522 hfize_llama.py:153] loaded layer 3
I0318 12:39:06.211715 626522 hfize_llama.py:153] loaded layer 4
I0318 12:39:07.300925 626522 hfize_llama.py:153] loaded layer 5
I0318 12:39:08.444426 626522 hfize_llama.py:153] loaded layer 6
I0318 12:39:09.482151 626522 hfize_llama.py:153] loaded layer 7
I0318 12:39:10.579308 626522 hfize_llama.py:153] loaded layer 8
I0318 12:39:11.519640 626522 hfize_llama.py:153] loaded layer 9
I0318 12:39:12.549310 626522 hfize_llama.py:153] loaded layer 10
I0318 12:39:13.466352 626522 hfize_llama.py:153] loaded layer 11
I0318 12:39:14.592056 626522 hfize_llama.py:153] loaded layer 12
I0318 12:39:15.551851 626522 hfize_llama.py:153] loaded layer 13
I0318 12:39:16.542609 626522 hfize_llama.py:153] loaded layer 14
I0318 12:39:17.682340 626522 hfize_llama.py:153] loaded layer 15
I0318 12:39:18.781252 626522 hfize_llama.py:153] loaded layer 16
I0318 12:39:20.463542 626522 hfize_llama.py:153] loaded layer 17
I0318 12:39:22.176990 626522 hfize_llama.py:153] loaded layer 18
I0318 12:39:23.813700 626522 hfize_llama.py:153] loaded layer 19
I0318 12:39:25.582796 626522 hfize_llama.py:153] loaded layer 20
I0318 12:39:27.368099 626522 hfize_llama.py:153] loaded layer 21
I0318 12:39:29.158769 626522 hfize_llama.py:153] loaded layer 22
I0318 12:39:30.654244 626522 hfize_llama.py:153] loaded layer 23
I0318 12:39:32.231239 626522 hfize_llama.py:153] loaded layer 24
I0318 12:39:33.578689 626522 hfize_llama.py:153] loaded layer 25
I0318 12:39:34.662400 626522 hfize_llama.py:153] loaded layer 26
I0318 12:39:36.231639 626522 hfize_llama.py:153] loaded layer 27
I0318 12:39:37.794247 626522 hfize_llama.py:153] loaded layer 28
I0318 12:39:39.443567 626522 hfize_llama.py:153] loaded layer 29
I0318 12:39:41.083251 626522 hfize_llama.py:153] loaded layer 30
I0318 12:39:42.663792 626522 hfize_llama.py:153] loaded layer 31
I0318 12:39:42.664009 626522 hfize_llama.py:157] saving model...
### skip ####
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 186, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 165, in main
    model, _ = model_from_hf_path(args.hf_output_path, device_map='cuda')
  File "/workspace/Weight_compression/comp_lm_qtip/lib/utils/unsafe_import.py", line 44, in model_from_hf_path
    model = model_cls.from_pretrained(path,
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4264, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4777, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 942, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 339, in set_module_tensor_to_device
    new_value = value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 27.00 MiB is free. Process 3347854 has 26.65 GiB memory in use. Process 3350306 has 18.37 GiB memory in use. Process 3350308 has 2.35 GiB memory in use. Of the allocated memory 1.93 GiB is allocated by PyTorch, and 1.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
