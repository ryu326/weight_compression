I0318 12:44:34.828473 632532 config.py:54] PyTorch version 2.6.0 available.
W0318 12:44:35.133521 632532 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0318 12:44:35.367479 632532 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.45it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.29it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.37it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.58it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.71it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.65it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.18it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.41it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.66it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.78it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.94it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.80it/s]
I0318 12:44:37.953278 632532 hfize_llama.py:153] loaded layer 0
I0318 12:44:38.867926 632532 hfize_llama.py:153] loaded layer 1
I0318 12:44:39.992846 632532 hfize_llama.py:153] loaded layer 2
I0318 12:44:41.012681 632532 hfize_llama.py:153] loaded layer 3
I0318 12:44:42.101889 632532 hfize_llama.py:153] loaded layer 4
I0318 12:44:43.164807 632532 hfize_llama.py:153] loaded layer 5
I0318 12:44:44.263694 632532 hfize_llama.py:153] loaded layer 6
I0318 12:44:45.307147 632532 hfize_llama.py:153] loaded layer 7
I0318 12:44:46.655838 632532 hfize_llama.py:153] loaded layer 8
I0318 12:44:47.852297 632532 hfize_llama.py:153] loaded layer 9
I0318 12:44:49.123566 632532 hfize_llama.py:153] loaded layer 10
I0318 12:44:50.279915 632532 hfize_llama.py:153] loaded layer 11
I0318 12:44:51.342924 632532 hfize_llama.py:153] loaded layer 12
I0318 12:44:52.544663 632532 hfize_llama.py:153] loaded layer 13
I0318 12:44:53.767645 632532 hfize_llama.py:153] loaded layer 14
I0318 12:44:54.925354 632532 hfize_llama.py:153] loaded layer 15
I0318 12:44:56.248368 632532 hfize_llama.py:153] loaded layer 16
I0318 12:44:57.263043 632532 hfize_llama.py:153] loaded layer 17
I0318 12:44:58.483083 632532 hfize_llama.py:153] loaded layer 18
I0318 12:44:59.731054 632532 hfize_llama.py:153] loaded layer 19
I0318 12:45:00.732921 632532 hfize_llama.py:153] loaded layer 20
I0318 12:45:01.912797 632532 hfize_llama.py:153] loaded layer 21
I0318 12:45:02.964077 632532 hfize_llama.py:153] loaded layer 22
I0318 12:45:04.027684 632532 hfize_llama.py:153] loaded layer 23
I0318 12:45:05.047241 632532 hfize_llama.py:153] loaded layer 24
I0318 12:45:06.154831 632532 hfize_llama.py:153] loaded layer 25
I0318 12:45:07.163777 632532 hfize_llama.py:153] loaded layer 26
I0318 12:45:08.311222 632532 hfize_llama.py:153] loaded layer 27
I0318 12:45:09.382330 632532 hfize_llama.py:153] loaded layer 28
I0318 12:45:10.506566 632532 hfize_llama.py:153] loaded layer 29
I0318 12:45:11.708528 632532 hfize_llama.py:153] loaded layer 30
I0318 12:45:12.718973 632532 hfize_llama.py:153] loaded layer 31
I0318 12:45:12.719122 632532 hfize_llama.py:157] saving model...
### skip ####
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 186, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 165, in main
    model, _ = model_from_hf_path(args.hf_output_path, device_map='cuda')
  File "/workspace/Weight_compression/comp_lm_qtip/lib/utils/unsafe_import.py", line 44, in model_from_hf_path
    model = model_cls.from_pretrained(path,
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4264, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4777, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 942, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 339, in set_module_tensor_to_device
    new_value = value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 49.00 MiB is free. Process 3356675 has 18.37 GiB memory in use. Process 3358628 has 26.65 GiB memory in use. Process 3358630 has 2.41 GiB memory in use. Of the allocated memory 2.00 GiB is allocated by PyTorch, and 1.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
