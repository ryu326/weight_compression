I0410 02:01:15.224400 1808348 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 02:01:15.224505 1808348 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 02:01:15.224545 1808348 utils.py:162] NumExpr defaulting to 16 threads.
I0410 02:01:15.565450 1808348 config.py:54] PyTorch version 2.6.0 available.
W0410 02:01:15.759552 1808348 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 02:01:16.362049 1808348 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.41it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.89it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.99it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.40it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.51it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.72it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.67it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.66it/s]
I0410 02:01:17.876387 1808348 quantize_finetune_llama.py:163] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:22,  1.39it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:21,  1.40it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:20,  1.43it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:17,  1.58it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:16,  1.65it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:15,  1.70it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:14,  1.72it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:13,  1.75it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:05<00:12,  1.79it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:12,  1.80it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:11,  1.79it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:07<00:10,  1.83it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:10,  1.87it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:08<00:09,  1.91it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  1.95it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:09<00:08,  1.97it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  2.00it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:10<00:06,  2.01it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  2.01it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:11<00:06,  2.00it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.99it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:12<00:05,  1.97it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.97it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:13<00:04,  1.96it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.99it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:14<00:02,  2.02it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  2.05it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:01,  2.03it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  2.03it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:00,  2.05it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  2.05it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  2.09it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.89it/s]
I0410 02:01:44.955922 1808348 quantize_finetune_llama.py:201] loaded compression model
I0410 02:02:04.318335 1808348 quantize_finetune_llama.py:205] loaded dataset and devset
I0410 02:02:06.698730 1808348 quantize_finetune_llama.py:225] layer 0 gpu 0
I0410 02:02:09.602027 1808348 quantize_finetune_llama.py:256] computed original embedding for layer 0 in 2.719954013824463s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0410 02:02:20.815751 1809877 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 02:02:20.815877 1809877 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 02:02:20.815918 1809877 utils.py:162] NumExpr defaulting to 16 threads.
I0410 02:02:21.200239 1809877 config.py:54] PyTorch version 2.6.0 available.
W0410 02:02:21.416885 1809877 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 02:02:22.038969 1809877 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 02:02:22.043349 1808348 quantize_finetune_llama.py:225] layer 1 gpu 0
I0410 02:02:22.058508 1809877 data_utils.py:336] using 256 training seqs, 128 validation seqs
0_v proxy err 0.07492443174123764 err 4.561912536621094 tr(WHW.T) 60.88684844970703
bpp_loss 2.5686113834381104
0_q proxy err 0.00015234753664117306 err 43.89051055908203 tr(WHW.T) 288094.65625
bpp_loss 3.383499503135681
0_k proxy err 0.00014897323853801936 err 14.924247741699219 tr(WHW.T) 100180.734375
bpp_loss 3.9246712923049927
0_o proxy err 0.00931085366755724 err 29.079822540283203 tr(WHW.T) 3123.217529296875
bpp_loss 2.6689544916152954
0_up proxy err 0.018107548356056213 err 161.5999298095703 tr(WHW.T) 8924.451171875
bpp_loss 2.9570471899850026
0_gate proxy err 0.01046028546988964 err 165.04934692382812 tr(WHW.T) 15778.666015625
bpp_loss 3.066056524004255
0_down proxy err 0.012809163890779018 err 138.57215881347656 tr(WHW.T) 10818.205078125
bpp_loss 2.9505288941519603
I0410 02:02:59.037013 1808348 quantize_finetune_llama.py:256] computed original embedding for layer 1 in 0.7266061305999756s
I0410 02:03:03.059314 1810694 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 02:03:03.059422 1810694 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 02:03:03.059464 1810694 utils.py:162] NumExpr defaulting to 16 threads.
I0410 02:03:03.435286 1810694 config.py:54] PyTorch version 2.6.0 available.
W0410 02:03:03.646962 1810694 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 02:03:04.249474 1810694 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 02:03:04.253666 1808348 quantize_finetune_llama.py:225] layer 2 gpu 0
I0410 02:03:04.268126 1810694 data_utils.py:336] using 256 training seqs, 128 validation seqs
1_v proxy err 0.048202548176050186 err 5.257498264312744 tr(WHW.T) 109.07096099853516
bpp_loss 2.678348422050476
1_q proxy err 0.00020187514019198716 err 29.236865997314453 tr(WHW.T) 144826.484375
bpp_loss 3.6344006061553955
1_k proxy err 0.00012200995115563273 err 9.21651554107666 tr(WHW.T) 75539.046875
bpp_loss 4.297932863235474
1_o proxy err 0.018566051498055458 err 36.83192825317383 tr(WHW.T) 1983.8321533203125
bpp_loss 2.7470991611480713
1_up proxy err 0.019899675622582436 err 163.7906494140625 tr(WHW.T) 8230.8203125
bpp_loss 2.9713310514177596
1_gate proxy err 0.011976547539234161 err 167.10116577148438 tr(WHW.T) 13952.365234375
bpp_loss 3.077267646789551
1_down proxy err 0.0011802114313468337 err 16.497032165527344 tr(WHW.T) 13978.03125
bpp_loss 2.966089589255197
I0410 02:03:40.886463 1808348 quantize_finetune_llama.py:256] computed original embedding for layer 2 in 0.6562914848327637s
I0410 02:03:44.612396 1811626 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 02:03:44.612509 1811626 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 02:03:44.612550 1811626 utils.py:162] NumExpr defaulting to 16 threads.
I0410 02:03:44.978722 1811626 config.py:54] PyTorch version 2.6.0 available.
W0410 02:03:45.187126 1811626 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 02:03:45.787672 1811626 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 02:03:45.792088 1808348 quantize_finetune_llama.py:225] layer 3 gpu 0
I0410 02:03:45.806347 1811626 data_utils.py:336] using 256 training seqs, 128 validation seqs
Process Process-3:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 131, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 151, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ql_search/idx1_val1/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
2_v proxy err 0.04798388481140137 err 7.4835429191589355 tr(WHW.T) 155.95950317382812
bpp_loss 2.5885509252548218
2_q proxy err 0.0008536555105820298 err 35.405494689941406 tr(WHW.T) 41475.15625
bpp_loss 3.5871422290802
2_k proxy err 0.00044646082096733153 err 10.096503257751465 tr(WHW.T) 22614.533203125
bpp_loss 4.409902572631836
2_o proxy err 0.016787732020020485 err 33.026885986328125 tr(WHW.T) 1967.3226318359375
bpp_loss 2.7035478353500366
2_up proxy err 0.022001607343554497 err 167.22265625 tr(WHW.T) 7600.474609375
bpp_loss 2.9635911669049944
2_gate proxy err 0.011364876292645931 err 171.76097106933594 tr(WHW.T) 15113.31640625
bpp_loss 3.1101083755493164
I0410 02:04:11.214530 1808348 quantize_finetune_llama.py:256] computed original embedding for layer 3 in 0.7609906196594238s
I0410 02:04:14.781964 1812347 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 02:04:14.782063 1812347 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 02:04:14.782101 1812347 utils.py:162] NumExpr defaulting to 16 threads.
I0410 02:04:15.108055 1812347 config.py:54] PyTorch version 2.6.0 available.
W0410 02:04:15.299331 1812347 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 02:04:15.878908 1812347 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 02:04:15.882685 1808348 quantize_finetune_llama.py:225] layer 4 gpu 0
I0410 02:04:15.896026 1812347 data_utils.py:336] using 256 training seqs, 128 validation seqs
Process Process-4:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 131, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 151, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ql_search/idx1_val1/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
3_v proxy err 0.031131276860833168 err 9.00731086730957 tr(WHW.T) 289.3331604003906
bpp_loss 2.687488079071045
I0410 02:04:19.207707 1808348 quantize_finetune_llama.py:256] computed original embedding for layer 4 in 0.716672420501709s
I0410 02:04:22.694230 1812650 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 02:04:22.694327 1812650 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 02:04:22.694365 1812650 utils.py:162] NumExpr defaulting to 16 threads.
I0410 02:04:23.013605 1812650 config.py:54] PyTorch version 2.6.0 available.
W0410 02:04:23.200145 1812650 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 02:04:23.739393 1812650 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 02:04:23.743173 1808348 quantize_finetune_llama.py:225] layer 5 gpu 0
I0410 02:04:23.756258 1812650 data_utils.py:336] using 256 training seqs, 128 validation seqs
Process Process-5:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 131, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 151, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ql_search/idx1_val1/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
4_v proxy err 0.028883185237646103 err 8.240578651428223 tr(WHW.T) 285.30712890625
bpp_loss 2.7326830625534058
I0410 02:04:26.923520 1808348 quantize_finetune_llama.py:256] computed original embedding for layer 5 in 0.6551475524902344s
