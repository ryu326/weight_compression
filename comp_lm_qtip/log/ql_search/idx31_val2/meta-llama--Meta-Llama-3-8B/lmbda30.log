I0410 04:23:56.676306 1926517 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:23:56.676522 1926517 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:23:56.676566 1926517 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:23:57.002200 1926517 config.py:54] PyTorch version 2.6.0 available.
W0410 04:23:57.189134 1926517 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:23:57.936990 1926517 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.98it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  5.29it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  6.13it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  6.78it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.25it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.56it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  7.85it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.91it/s]
I0410 04:23:59.678816 1926517 quantize_finetune_llama.py:163] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.45it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:20,  1.45it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:19,  1.48it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:17,  1.62it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:15,  1.71it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:14,  1.76it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:13,  1.81it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:13,  1.84it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:05<00:12,  1.86it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.88it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:10,  1.92it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.97it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:09,  2.03it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:08,  2.07it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  2.10it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:07,  2.12it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  2.08it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:06,  2.09it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  2.06it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:05,  2.05it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:10<00:05,  2.04it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:04,  2.06it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:11<00:04,  2.07it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:03,  2.03it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:12<00:03,  2.01it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.98it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.96it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.96it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.60it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.69it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.74it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.78it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.88it/s]
I0410 04:24:27.203576 1926517 quantize_finetune_llama.py:201] loaded compression model
I0410 04:24:52.361418 1926517 quantize_finetune_llama.py:205] loaded dataset and devset
I0410 04:24:55.123069 1926517 quantize_finetune_llama.py:225] layer 0 gpu 0
I0410 04:24:58.927160 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 0 in 3.6501212120056152s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0410 04:25:12.592415 1927882 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:25:12.592542 1927882 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:25:12.592593 1927882 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:25:12.992232 1927882 config.py:54] PyTorch version 2.6.0 available.
W0410 04:25:13.214906 1927882 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:25:13.858757 1927882 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:25:13.863336 1926517 quantize_finetune_llama.py:225] layer 1 gpu 0
I0410 04:25:13.879043 1927882 data_utils.py:336] using 256 training seqs, 128 validation seqs
0_v proxy err 0.20893007516860962 err 12.721094131469727 tr(WHW.T) 60.88684844970703
bpp_loss 1.6846774220466614
0_q proxy err 0.0004110773734282702 err 118.42919158935547 tr(WHW.T) 288094.65625
bpp_loss 2.4519084692001343
0_k proxy err 0.00036382683902047575 err 36.44844055175781 tr(WHW.T) 100180.734375
bpp_loss 2.9445894956588745
0_o proxy err 0.029903067275881767 err 93.39378356933594 tr(WHW.T) 3123.217529296875
bpp_loss 1.8001657128334045
0_up proxy err 0.060492753982543945 err 539.8646240234375 tr(WHW.T) 8924.451171875
bpp_loss 2.0922208513532365
0_gate proxy err 0.03555373474955559 err 560.990478515625 tr(WHW.T) 15778.666015625
bpp_loss 2.1976406233651296
0_down proxy err 0.04266523942351341 err 461.5613098144531 tr(WHW.T) 10818.205078125
bpp_loss 2.086120912006923
I0410 04:25:53.905956 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 1 in 0.9321587085723877s
I0410 04:25:58.123250 1928570 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:25:58.123441 1928570 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:25:58.123525 1928570 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:25:58.548428 1928570 config.py:54] PyTorch version 2.6.0 available.
W0410 04:25:58.770132 1928570 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:25:59.471724 1928570 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:25:59.476035 1926517 quantize_finetune_llama.py:225] layer 2 gpu 0
I0410 04:25:59.492291 1928570 data_utils.py:336] using 256 training seqs, 128 validation seqs
1_v proxy err 0.14767533540725708 err 16.10708999633789 tr(WHW.T) 109.07096099853516
bpp_loss 1.8151487112045288
1_q proxy err 0.0007187006995081902 err 104.08689880371094 tr(WHW.T) 144826.484375
bpp_loss 2.7218306064605713
1_k proxy err 0.00039947102777659893 err 30.175661087036133 tr(WHW.T) 75539.046875
bpp_loss 3.3235710859298706
1_o proxy err 0.060156140476465225 err 119.33968353271484 tr(WHW.T) 1983.8321533203125
bpp_loss 1.8821290731430054
1_up proxy err 0.06633812934160233 err 546.0172119140625 tr(WHW.T) 8230.8203125
bpp_loss 2.1065331867762973
1_gate proxy err 0.040670640766620636 err 567.45166015625 tr(WHW.T) 13952.365234375
bpp_loss 2.208733218056815
1_down proxy err 0.0020061370451003313 err 28.041845321655273 tr(WHW.T) 13978.03125
bpp_loss 2.10089533669608
I0410 04:26:40.330005 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 2 in 0.9603641033172607s
I0410 04:26:44.524371 1929283 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:26:44.524481 1929283 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:26:44.524522 1929283 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:26:44.954858 1929283 config.py:54] PyTorch version 2.6.0 available.
W0410 04:26:45.176588 1929283 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:26:45.862580 1929283 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:26:45.867320 1926517 quantize_finetune_llama.py:225] layer 3 gpu 0
I0410 04:26:45.887994 1929283 data_utils.py:336] using 256 training seqs, 128 validation seqs
2_v proxy err 0.14844438433647156 err 23.15131187438965 tr(WHW.T) 155.95950317382812
bpp_loss 1.71983003616333
2_q proxy err 0.003331081010401249 err 138.1571044921875 tr(WHW.T) 41475.15625
bpp_loss 2.679665446281433
2_k proxy err 0.0017732592532411218 err 40.1014289855957 tr(WHW.T) 22614.533203125
bpp_loss 3.4244394302368164
2_o proxy err 0.05404764413833618 err 106.32915496826172 tr(WHW.T) 1967.3226318359375
bpp_loss 1.8369559049606323
2_up proxy err 0.07307139039039612 err 555.3772583007812 tr(WHW.T) 7600.474609375
bpp_loss 2.0987893513270786
2_gate proxy err 0.03851804882287979 err 582.1354370117188 tr(WHW.T) 15113.31640625
bpp_loss 2.2398565156119212
2_down proxy err 0.06431978195905685 err 496.9812927246094 tr(WHW.T) 7726.72509765625
bpp_loss 2.1033995832715715
I0410 04:27:27.120242 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 3 in 0.9935252666473389s
I0410 04:27:31.370590 1929974 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:27:31.370699 1929974 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:27:31.370740 1929974 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:27:31.790272 1929974 config.py:54] PyTorch version 2.6.0 available.
W0410 04:27:32.010495 1929974 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:27:32.724254 1929974 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:27:32.728863 1926517 quantize_finetune_llama.py:225] layer 4 gpu 0
I0410 04:27:32.768433 1929974 data_utils.py:336] using 256 training seqs, 128 validation seqs
3_v proxy err 0.10121957212686539 err 29.286178588867188 tr(WHW.T) 289.3331604003906
bpp_loss 1.8263331055641174
3_q proxy err 0.0033984885085374117 err 161.6896514892578 tr(WHW.T) 47576.9296875
bpp_loss 2.720979690551758
3_k proxy err 0.0017719612224027514 err 46.380226135253906 tr(WHW.T) 26174.515625
bpp_loss 3.497908353805542
3_o proxy err 0.07010843604803085 err 130.20242309570312 tr(WHW.T) 1857.15771484375
bpp_loss 1.94123375415802
3_up proxy err 0.07272423803806305 err 548.0836791992188 tr(WHW.T) 7536.46484375
bpp_loss 2.0815252576555525
3_gate proxy err 0.028355702757835388 err 592.1903686523438 tr(WHW.T) 20884.34765625
bpp_loss 2.309978485107422
3_down proxy err 0.0737491324543953 err 516.1414184570312 tr(WHW.T) 6998.6103515625
bpp_loss 2.081653050013951
I0410 04:28:14.948698 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 4 in 1.0015838146209717s
I0410 04:28:19.167937 1930709 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:28:19.168044 1930709 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:28:19.168085 1930709 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:28:19.618604 1930709 config.py:54] PyTorch version 2.6.0 available.
W0410 04:28:19.838473 1930709 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:28:20.559725 1930709 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:28:20.564140 1926517 quantize_finetune_llama.py:225] layer 5 gpu 0
I0410 04:28:20.580490 1930709 data_utils.py:336] using 256 training seqs, 128 validation seqs
4_v proxy err 0.09478600323200226 err 27.043123245239258 tr(WHW.T) 285.30712890625
bpp_loss 1.8702280521392822
4_q proxy err 0.0028963619843125343 err 145.15077209472656 tr(WHW.T) 50114.859375
bpp_loss 2.6876137256622314
4_k proxy err 0.0014453609474003315 err 42.3519287109375 tr(WHW.T) 29301.974609375
bpp_loss 3.4828044176101685
4_o proxy err 0.09127691388130188 err 118.4088134765625 tr(WHW.T) 1297.2481689453125
bpp_loss 1.9493895173072815
4_up proxy err 0.07220988720655441 err 533.1608276367188 tr(WHW.T) 7383.48779296875
bpp_loss 2.054471901484898
4_gate proxy err 0.02028506062924862 err 590.923583984375 tr(WHW.T) 29130.974609375
bpp_loss 2.3770216533115933
4_down proxy err 0.0827142596244812 err 529.0174560546875 tr(WHW.T) 6395.72265625
bpp_loss 2.0577762808118547
I0410 04:29:03.153673 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 5 in 1.106464147567749s
I0410 04:29:07.617697 1931412 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:29:07.617821 1931412 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:29:07.617877 1931412 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:29:08.099235 1931412 config.py:54] PyTorch version 2.6.0 available.
W0410 04:29:08.328169 1931412 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:29:09.094590 1931412 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:29:09.098985 1926517 quantize_finetune_llama.py:225] layer 6 gpu 0
I0410 04:29:09.117581 1931412 data_utils.py:336] using 256 training seqs, 128 validation seqs
5_v proxy err 0.12709644436836243 err 26.540266036987305 tr(WHW.T) 208.81988525390625
bpp_loss 1.7567432522773743
5_q proxy err 0.004023056477308273 err 144.7963409423828 tr(WHW.T) 35991.625
bpp_loss 2.6631250381469727
5_k proxy err 0.001828203909099102 err 42.04458236694336 tr(WHW.T) 22997.75390625
bpp_loss 3.4552531242370605
5_o proxy err 0.10626985132694244 err 112.1770248413086 tr(WHW.T) 1055.5865478515625
bpp_loss 1.896180808544159
5_up proxy err 0.0692036971449852 err 529.8534545898438 tr(WHW.T) 7656.43310546875
bpp_loss 2.0598603657313754
5_gate proxy err 0.019317856058478355 err 587.0106811523438 tr(WHW.T) 30386.947265625
bpp_loss 2.3799495697021484
5_down proxy err 0.0811571255326271 err 520.308837890625 tr(WHW.T) 6411.12939453125
bpp_loss 2.0622627053942
I0410 04:29:52.362108 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 6 in 1.0711464881896973s
I0410 04:29:56.564162 1932137 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:29:56.564290 1932137 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:29:56.564332 1932137 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:29:56.994019 1932137 config.py:54] PyTorch version 2.6.0 available.
W0410 04:29:57.213234 1932137 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:29:57.896548 1932137 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:29:57.900856 1926517 quantize_finetune_llama.py:225] layer 7 gpu 0
I0410 04:29:57.919437 1932137 data_utils.py:336] using 256 training seqs, 128 validation seqs
6_v proxy err 0.10847195237874985 err 27.512149810791016 tr(WHW.T) 253.63377380371094
bpp_loss 1.8044180870056152
6_q proxy err 0.004205327946692705 err 149.8761444091797 tr(WHW.T) 35639.5859375
bpp_loss 2.707595109939575
6_k proxy err 0.0016935544554144144 err 44.309574127197266 tr(WHW.T) 26163.654296875
bpp_loss 3.5229077339172363
6_o proxy err 0.12149108201265335 err 122.60253143310547 tr(WHW.T) 1009.1483764648438
bpp_loss 1.9305211901664734
6_up proxy err 0.06576671451330185 err 521.1049194335938 tr(WHW.T) 7923.53564453125
bpp_loss 2.0591941561017717
6_gate proxy err 0.016202248632907867 err 579.204345703125 tr(WHW.T) 35748.39453125
bpp_loss 2.3841202599661693
6_down proxy err 0.08064228296279907 err 522.8490600585938 tr(WHW.T) 6483.5595703125
bpp_loss 2.063042095729283
I0410 04:30:40.470271 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 7 in 1.0589110851287842s
I0410 04:30:44.876685 1932871 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:30:44.876859 1932871 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:30:44.876902 1932871 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:30:45.290747 1932871 config.py:54] PyTorch version 2.6.0 available.
W0410 04:30:45.517175 1932871 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:30:46.246943 1932871 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:30:46.251359 1926517 quantize_finetune_llama.py:225] layer 8 gpu 0
I0410 04:30:46.267424 1932871 data_utils.py:336] using 256 training seqs, 128 validation seqs
7_v proxy err 0.09107766300439835 err 28.181896209716797 tr(WHW.T) 309.4270935058594
bpp_loss 1.8025281429290771
7_q proxy err 0.0044327848590910435 err 155.79000854492188 tr(WHW.T) 35144.953125
bpp_loss 2.637367844581604
7_k proxy err 0.0017545598093420267 err 47.11079406738281 tr(WHW.T) 26850.4921875
bpp_loss 3.5417085886001587
7_o proxy err 0.1235940232872963 err 118.55330657958984 tr(WHW.T) 959.2155151367188
bpp_loss 1.9409180879592896
7_up proxy err 0.05993662029504776 err 517.1002197265625 tr(WHW.T) 8627.4501953125
bpp_loss 2.0713579995291576
7_gate proxy err 0.01628594659268856 err 568.4371337890625 tr(WHW.T) 34903.53515625
bpp_loss 2.356522423880441
7_down proxy err 0.08096864819526672 err 529.0999755859375 tr(WHW.T) 6534.6279296875
bpp_loss 2.0766972473689487
I0410 04:31:30.099460 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 8 in 1.273101806640625s
I0410 04:31:34.567404 1933565 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:31:34.567557 1933565 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:31:34.567600 1933565 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:31:35.015697 1933565 config.py:54] PyTorch version 2.6.0 available.
W0410 04:31:35.243772 1933565 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:31:35.979534 1933565 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:31:35.983982 1926517 quantize_finetune_llama.py:225] layer 9 gpu 0
I0410 04:31:35.999933 1933565 data_utils.py:336] using 256 training seqs, 128 validation seqs
8_v proxy err 0.10259221494197845 err 26.43855094909668 tr(WHW.T) 257.7052307128906
bpp_loss 1.821737289428711
8_q proxy err 0.005092139821499586 err 135.4558868408203 tr(WHW.T) 26600.9765625
bpp_loss 2.6248672008514404
8_k proxy err 0.0017942605772987008 err 40.42244338989258 tr(WHW.T) 22528.748046875
bpp_loss 3.461038112640381
8_o proxy err 0.16101223230361938 err 120.05170440673828 tr(WHW.T) 745.6061401367188
bpp_loss 1.9507015347480774
8_up proxy err 0.06059904769062996 err 514.792236328125 tr(WHW.T) 8495.0546875
bpp_loss 2.06728458404541
8_gate proxy err 0.015206586569547653 err 566.26513671875 tr(WHW.T) 37238.1484375
bpp_loss 2.360641751970564
8_down proxy err 0.0825096145272255 err 535.3898315429688 tr(WHW.T) 6488.8173828125
bpp_loss 2.074810368674142
I0410 04:32:18.883679 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 9 in 0.9106664657592773s
I0410 04:32:23.166397 1934310 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:32:23.166547 1934310 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:32:23.166590 1934310 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:32:23.603524 1934310 config.py:54] PyTorch version 2.6.0 available.
W0410 04:32:23.852023 1934310 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:32:24.577825 1934310 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:32:24.582133 1926517 quantize_finetune_llama.py:225] layer 10 gpu 0
I0410 04:32:24.597569 1934310 data_utils.py:336] using 256 training seqs, 128 validation seqs
9_v proxy err 0.07706370204687119 err 27.08240509033203 tr(WHW.T) 351.4288024902344
bpp_loss 1.9205366373062134
9_q proxy err 0.005267934408038855 err 135.13954162597656 tr(WHW.T) 25653.232421875
bpp_loss 2.6363896131515503
9_k proxy err 0.0019091354915872216 err 39.98233413696289 tr(WHW.T) 20942.638671875
bpp_loss 3.4791146516799927
9_o proxy err 0.15230780839920044 err 118.28250885009766 tr(WHW.T) 776.6017456054688
bpp_loss 2.0042662024497986
9_up proxy err 0.05780329927802086 err 518.5328979492188 tr(WHW.T) 8970.6455078125
bpp_loss 2.0763224193028043
9_gate proxy err 0.014508519321680069 err 571.8637084960938 tr(WHW.T) 39415.71875
bpp_loss 2.373028482709612
9_down proxy err 0.08360347896814346 err 524.4365844726562 tr(WHW.T) 6272.90380859375
bpp_loss 2.075998749051775
I0410 04:33:08.214261 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 10 in 0.9146273136138916s
I0410 04:33:12.556302 1935015 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:33:12.556462 1935015 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:33:12.556507 1935015 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:33:12.997684 1935015 config.py:54] PyTorch version 2.6.0 available.
W0410 04:33:13.219655 1935015 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:33:13.999444 1935015 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:33:14.003875 1926517 quantize_finetune_llama.py:225] layer 11 gpu 0
I0410 04:33:14.019682 1935015 data_utils.py:336] using 256 training seqs, 128 validation seqs
10_v proxy err 0.10172607749700546 err 25.61858367919922 tr(WHW.T) 251.83889770507812
bpp_loss 1.810444176197052
10_q proxy err 0.0056303637102246284 err 131.40231323242188 tr(WHW.T) 23338.15625
bpp_loss 2.632741332054138
10_k proxy err 0.0019838062580674887 err 39.16818618774414 tr(WHW.T) 19743.95703125
bpp_loss 3.477612257003784
10_o proxy err 0.16973312199115753 err 115.53123474121094 tr(WHW.T) 680.6640625
bpp_loss 1.9433330297470093
10_up proxy err 0.057028885930776596 err 524.61767578125 tr(WHW.T) 9199.1572265625
bpp_loss 2.093191010611398
10_gate proxy err 0.015318753197789192 err 572.9117431640625 tr(WHW.T) 37399.37109375
bpp_loss 2.345354897635324
10_down proxy err 0.08101324737071991 err 528.63037109375 tr(WHW.T) 6525.2333984375
bpp_loss 2.0923798084259033
I0410 04:33:58.375850 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 11 in 1.4030261039733887s
I0410 04:34:02.921908 1935740 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:34:02.922046 1935740 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:34:02.922087 1935740 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:34:03.391346 1935740 config.py:54] PyTorch version 2.6.0 available.
W0410 04:34:03.612785 1935740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:34:04.353031 1935740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:34:04.357460 1926517 quantize_finetune_llama.py:225] layer 12 gpu 0
I0410 04:34:04.372245 1935740 data_utils.py:336] using 256 training seqs, 128 validation seqs
11_v proxy err 0.08286979794502258 err 26.482627868652344 tr(WHW.T) 319.5691223144531
bpp_loss 1.820645034313202
11_q proxy err 0.006038947496563196 err 134.00967407226562 tr(WHW.T) 22190.8984375
bpp_loss 2.5812803506851196
11_k proxy err 0.0022110752761363983 err 39.85999298095703 tr(WHW.T) 18027.423828125
bpp_loss 3.478442430496216
11_o proxy err 0.19481977820396423 err 110.25780487060547 tr(WHW.T) 565.9476928710938
bpp_loss 1.9625091552734375
11_up proxy err 0.05616606026887894 err 516.3118896484375 tr(WHW.T) 9192.595703125
bpp_loss 2.0973662648882185
11_gate proxy err 0.015218903310596943 err 561.33544921875 tr(WHW.T) 36884.09375
bpp_loss 2.3247263772147044
11_down proxy err 0.07806464284658432 err 519.5897216796875 tr(WHW.T) 6655.890625
bpp_loss 2.0993992260524204
I0410 04:34:48.433193 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 12 in 1.185988426208496s
I0410 04:34:52.998721 1936499 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:34:52.998881 1936499 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:34:52.998923 1936499 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:34:53.455192 1936499 config.py:54] PyTorch version 2.6.0 available.
W0410 04:34:53.686052 1936499 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:34:54.449027 1936499 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:34:54.453611 1926517 quantize_finetune_llama.py:225] layer 13 gpu 0
I0410 04:34:54.469422 1936499 data_utils.py:336] using 256 training seqs, 128 validation seqs
12_v proxy err 0.07779178023338318 err 28.286911010742188 tr(WHW.T) 363.6233825683594
bpp_loss 1.9373193383216858
12_q proxy err 0.004358019679784775 err 148.61990356445312 tr(WHW.T) 34102.625
bpp_loss 2.6389036178588867
12_k proxy err 0.0018920714501291513 err 43.62556838989258 tr(WHW.T) 23057.041015625
bpp_loss 3.481645107269287
12_o proxy err 0.15197059512138367 err 118.50231170654297 tr(WHW.T) 779.7713012695312
bpp_loss 2.0080376863479614
12_up proxy err 0.05058971419930458 err 507.27093505859375 tr(WHW.T) 10027.1552734375
bpp_loss 2.1150503158569336
12_gate proxy err 0.01468851137906313 err 548.3823852539062 tr(WHW.T) 37334.1015625
bpp_loss 2.3052428109305247
12_down proxy err 0.07515048235654831 err 512.7301025390625 tr(WHW.T) 6822.7119140625
bpp_loss 2.1109847341265
I0410 04:35:38.435983 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 13 in 1.0498278141021729s
I0410 04:35:42.975845 1937196 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:35:42.976004 1937196 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:35:42.976047 1937196 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:35:43.424360 1937196 config.py:54] PyTorch version 2.6.0 available.
W0410 04:35:43.647658 1937196 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:35:44.364755 1937196 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:35:44.369129 1926517 quantize_finetune_llama.py:225] layer 14 gpu 0
I0410 04:35:44.385302 1937196 data_utils.py:336] using 256 training seqs, 128 validation seqs
13_v proxy err 0.09372009336948395 err 26.256017684936523 tr(WHW.T) 280.153564453125
bpp_loss 1.8788566589355469
13_q proxy err 0.006302019581198692 err 131.69566345214844 tr(WHW.T) 20897.375
bpp_loss 2.610450267791748
13_k proxy err 0.0022301364224404097 err 39.69401168823242 tr(WHW.T) 17798.916015625
bpp_loss 3.490813732147217
13_o proxy err 0.1724107414484024 err 116.41291809082031 tr(WHW.T) 675.2068481445312
bpp_loss 1.9906858205795288
13_up proxy err 0.05077844858169556 err 508.306884765625 tr(WHW.T) 10010.2880859375
bpp_loss 2.118748732975551
13_gate proxy err 0.014263472519814968 err 555.15234375 tr(WHW.T) 38921.26171875
bpp_loss 2.3101159504481723
13_down proxy err 0.07777149975299835 err 510.22039794921875 tr(WHW.T) 6560.505859375
bpp_loss 2.110994509288243
I0410 04:36:28.442442 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 14 in 1.2672617435455322s
I0410 04:36:32.856412 1937951 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:36:32.856572 1937951 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:36:32.856614 1937951 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:36:33.284384 1937951 config.py:54] PyTorch version 2.6.0 available.
W0410 04:36:33.519302 1937951 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:36:34.250695 1937951 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:36:34.255079 1926517 quantize_finetune_llama.py:225] layer 15 gpu 0
I0410 04:36:34.272205 1937951 data_utils.py:336] using 256 training seqs, 128 validation seqs
14_v proxy err 0.09041432291269302 err 25.437009811401367 tr(WHW.T) 281.3382873535156
bpp_loss 1.8695977330207825
14_q proxy err 0.006314652040600777 err 131.8615264892578 tr(WHW.T) 20881.8359375
bpp_loss 2.5829306840896606
14_k proxy err 0.002122612902894616 err 39.52156448364258 tr(WHW.T) 18619.298828125
bpp_loss 3.4429142475128174
14_o proxy err 0.17652997374534607 err 121.15210723876953 tr(WHW.T) 686.2976684570312
bpp_loss 1.9824351072311401
14_up proxy err 0.05488113686442375 err 502.9465026855469 tr(WHW.T) 9164.287109375
bpp_loss 2.113440445491246
14_gate proxy err 0.01333985012024641 err 557.4165649414062 tr(WHW.T) 41785.8203125
bpp_loss 2.3380843571254184
14_down proxy err 0.07950685918331146 err 508.12164306640625 tr(WHW.T) 6390.916015625
bpp_loss 2.1071365560804094
I0410 04:37:17.624632 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 15 in 1.1303231716156006s
I0410 04:37:22.230846 1938671 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:37:22.230994 1938671 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:37:22.231036 1938671 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:37:22.645969 1938671 config.py:54] PyTorch version 2.6.0 available.
W0410 04:37:22.862021 1938671 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:37:23.608906 1938671 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:37:23.613456 1926517 quantize_finetune_llama.py:225] layer 16 gpu 0
I0410 04:37:23.630087 1938671 data_utils.py:336] using 256 training seqs, 128 validation seqs
15_v proxy err 0.09534316509962082 err 27.080045700073242 tr(WHW.T) 284.0271301269531
bpp_loss 1.9314613342285156
15_q proxy err 0.004887678660452366 err 137.2426300048828 tr(WHW.T) 28079.306640625
bpp_loss 2.701164126396179
15_k proxy err 0.0021621123887598515 err 40.80266571044922 tr(WHW.T) 18871.66796875
bpp_loss 3.4900583028793335
15_o proxy err 0.1544114500284195 err 127.841552734375 tr(WHW.T) 827.9279174804688
bpp_loss 2.0120944380760193
15_up proxy err 0.056283194571733475 err 506.24310302734375 tr(WHW.T) 8994.5693359375
bpp_loss 2.106809275490897
15_gate proxy err 0.012307856231927872 err 568.8441162109375 tr(WHW.T) 46217.96875
bpp_loss 2.374043737139021
15_down proxy err 0.08053611218929291 err 515.3817138671875 tr(WHW.T) 6399.38671875
bpp_loss 2.1014466285705566
I0410 04:38:08.219210 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 16 in 1.5781850814819336s
I0410 04:38:12.745423 1939418 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:38:12.745588 1939418 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:38:12.745630 1939418 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:38:13.189013 1939418 config.py:54] PyTorch version 2.6.0 available.
W0410 04:38:13.417768 1939418 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:38:14.154944 1939418 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:38:14.159306 1926517 quantize_finetune_llama.py:225] layer 17 gpu 0
I0410 04:38:14.175589 1939418 data_utils.py:336] using 256 training seqs, 128 validation seqs
16_v proxy err 0.09080977737903595 err 24.90745735168457 tr(WHW.T) 274.28167724609375
bpp_loss 1.8905983567237854
16_q proxy err 0.00526789017021656 err 128.99295043945312 tr(WHW.T) 24486.64453125
bpp_loss 2.682610511779785
16_k proxy err 0.0019433351699262857 err 37.90739059448242 tr(WHW.T) 19506.357421875
bpp_loss 3.4825265407562256
16_o proxy err 0.1294948011636734 err 125.49192810058594 tr(WHW.T) 969.0885620117188
bpp_loss 1.9945262670516968
16_up proxy err 0.06208747252821922 err 517.2620849609375 tr(WHW.T) 8331.1826171875
bpp_loss 2.094976084572928
16_gate proxy err 0.014172546565532684 err 583.5706176757812 tr(WHW.T) 41176.12890625
bpp_loss 2.4059881482805525
16_down proxy err 0.08059677481651306 err 506.81512451171875 tr(WHW.T) 6288.2802734375
bpp_loss 2.0880933148520335
I0410 04:38:56.913343 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 17 in 0.6875290870666504s
I0410 04:39:01.249608 1940129 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:39:01.249772 1940129 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:39:01.249816 1940129 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:39:01.696797 1940129 config.py:54] PyTorch version 2.6.0 available.
W0410 04:39:01.933542 1940129 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:39:02.710705 1940129 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:39:02.715397 1926517 quantize_finetune_llama.py:225] layer 18 gpu 0
I0410 04:39:02.732751 1940129 data_utils.py:336] using 256 training seqs, 128 validation seqs
17_v proxy err 0.09557513147592545 err 27.140758514404297 tr(WHW.T) 283.9730224609375
bpp_loss 1.9595608115196228
17_q proxy err 0.005018328782171011 err 138.36256408691406 tr(WHW.T) 27571.44140625
bpp_loss 2.696149706840515
17_k proxy err 0.0023155277594923973 err 40.35921859741211 tr(WHW.T) 17429.814453125
bpp_loss 3.5088021755218506
17_o proxy err 0.1184573695063591 err 131.09669494628906 tr(WHW.T) 1106.6993408203125
bpp_loss 2.021821141242981
17_up proxy err 0.061998043209314346 err 524.0410766601562 tr(WHW.T) 8452.5419921875
bpp_loss 2.093078817640032
17_gate proxy err 0.014259923249483109 err 594.8892211914062 tr(WHW.T) 41717.5625
bpp_loss 2.4192021233694896
17_down proxy err 0.08198830485343933 err 509.337158203125 tr(WHW.T) 6212.31494140625
bpp_loss 2.0840890066964284
I0410 04:39:46.765278 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 18 in 0.8281147480010986s
I0410 04:39:51.410313 1940856 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:39:51.410470 1940856 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:39:51.410516 1940856 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:39:51.903336 1940856 config.py:54] PyTorch version 2.6.0 available.
W0410 04:39:52.120725 1940856 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:39:52.884376 1940856 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:39:52.888800 1926517 quantize_finetune_llama.py:225] layer 19 gpu 0
I0410 04:39:52.903858 1940856 data_utils.py:336] using 256 training seqs, 128 validation seqs
18_v proxy err 0.0958719328045845 err 27.57408905029297 tr(WHW.T) 287.61376953125
bpp_loss 1.8853811025619507
18_q proxy err 0.006300461012870073 err 141.12716674804688 tr(WHW.T) 22399.498046875
bpp_loss 2.6986016035079956
18_k proxy err 0.00238988408818841 err 41.57157897949219 tr(WHW.T) 17394.810546875
bpp_loss 3.5772221088409424
18_o proxy err 0.10538879781961441 err 126.8031234741211 tr(WHW.T) 1203.193603515625
bpp_loss 2.003133773803711
18_up proxy err 0.06688806414604187 err 533.9596557617188 tr(WHW.T) 7982.8837890625
bpp_loss 2.090266704559326
18_gate proxy err 0.017067285254597664 err 601.5117797851562 tr(WHW.T) 35243.5546875
bpp_loss 2.42780944279262
18_down proxy err 0.0811489149928093 err 505.28009033203125 tr(WHW.T) 6226.57861328125
bpp_loss 2.084256274359567
I0410 04:40:37.703869 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 19 in 0.9161365032196045s
I0410 04:40:42.264967 1941592 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:40:42.265155 1941592 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:40:42.265199 1941592 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:40:42.781923 1941592 config.py:54] PyTorch version 2.6.0 available.
W0410 04:40:43.025946 1941592 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:40:43.845453 1941592 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:40:43.850031 1926517 quantize_finetune_llama.py:225] layer 20 gpu 0
I0410 04:40:43.867000 1941592 data_utils.py:336] using 256 training seqs, 128 validation seqs
19_v proxy err 0.08361168950796127 err 28.51657485961914 tr(WHW.T) 341.0596618652344
bpp_loss 1.9303929805755615
19_q proxy err 0.005954152438789606 err 143.12612915039062 tr(WHW.T) 24038.03515625
bpp_loss 2.7012274265289307
19_k proxy err 0.002685388782992959 err 41.766624450683594 tr(WHW.T) 15553.287109375
bpp_loss 3.4956952333450317
19_o proxy err 0.10711506754159927 err 125.2059326171875 tr(WHW.T) 1168.891845703125
bpp_loss 2.0165030360221863
19_up proxy err 0.07013257592916489 err 536.4645385742188 tr(WHW.T) 7649.2919921875
bpp_loss 2.087205273764474
19_gate proxy err 0.018345942720770836 err 601.9669799804688 tr(WHW.T) 32811.99609375
bpp_loss 2.4384994506835938
19_down proxy err 0.08072417974472046 err 499.0962829589844 tr(WHW.T) 6182.73583984375
bpp_loss 2.082441943032401
I0410 04:41:28.554948 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 20 in 1.2075541019439697s
I0410 04:41:33.097384 1942326 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:41:33.097535 1942326 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:41:33.097590 1942326 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:41:33.558274 1942326 config.py:54] PyTorch version 2.6.0 available.
W0410 04:41:33.788510 1942326 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:41:34.537109 1942326 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:41:34.541897 1926517 quantize_finetune_llama.py:225] layer 21 gpu 0
I0410 04:41:34.558382 1942326 data_utils.py:336] using 256 training seqs, 128 validation seqs
20_v proxy err 0.08401723206043243 err 27.74182891845703 tr(WHW.T) 330.192138671875
bpp_loss 1.9643619060516357
20_q proxy err 0.006541346199810505 err 135.6527557373047 tr(WHW.T) 20737.7421875
bpp_loss 2.6747549772262573
20_k proxy err 0.0025848664809018373 err 39.79611587524414 tr(WHW.T) 15395.810546875
bpp_loss 3.456614851951599
20_o proxy err 0.10756637901067734 err 129.63070678710938 tr(WHW.T) 1205.1229248046875
bpp_loss 2.0033291578292847
20_up proxy err 0.07083013653755188 err 538.2848510742188 tr(WHW.T) 7599.658203125
bpp_loss 2.0910539627075195
20_gate proxy err 0.01965203694999218 err 602.6300048828125 tr(WHW.T) 30665.015625
bpp_loss 2.44095584324428
20_down proxy err 0.07892753183841705 err 497.3114929199219 tr(WHW.T) 6300.8623046875
bpp_loss 2.0867505414145335
I0410 04:42:19.686495 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 21 in 1.4085206985473633s
I0410 04:42:24.260388 1943089 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:42:24.260569 1943089 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:42:24.260612 1943089 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:42:24.761974 1943089 config.py:54] PyTorch version 2.6.0 available.
W0410 04:42:24.990760 1943089 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:42:25.705948 1943089 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:42:25.710306 1926517 quantize_finetune_llama.py:225] layer 22 gpu 0
I0410 04:42:25.725661 1943089 data_utils.py:336] using 256 training seqs, 128 validation seqs
21_v proxy err 0.07793718576431274 err 28.281309127807617 tr(WHW.T) 362.87310791015625
bpp_loss 1.9896823167800903
21_q proxy err 0.005425650626420975 err 140.18856811523438 tr(WHW.T) 25838.111328125
bpp_loss 2.6761451959609985
21_k proxy err 0.0024462142027914524 err 41.06570816040039 tr(WHW.T) 16787.453125
bpp_loss 3.4935141801834106
21_o proxy err 0.09094240516424179 err 115.19392395019531 tr(WHW.T) 1266.6689453125
bpp_loss 2.017763912677765
21_up proxy err 0.069117471575737 err 537.220458984375 tr(WHW.T) 7772.57080078125
bpp_loss 2.0938346726553783
21_gate proxy err 0.019107632339000702 err 603.1588134765625 tr(WHW.T) 31566.3828125
bpp_loss 2.451663153512137
21_down proxy err 0.0762886330485344 err 484.5445556640625 tr(WHW.T) 6351.46484375
bpp_loss 2.086880479540144
I0410 04:43:09.819457 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 22 in 1.2334764003753662s
I0410 04:43:14.755436 1943804 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:43:14.755600 1943804 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:43:14.755642 1943804 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:43:15.276664 1943804 config.py:54] PyTorch version 2.6.0 available.
W0410 04:43:15.498974 1943804 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:43:16.269735 1943804 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:43:16.274103 1926517 quantize_finetune_llama.py:225] layer 23 gpu 0
I0410 04:43:16.289990 1943804 data_utils.py:336] using 256 training seqs, 128 validation seqs
22_v proxy err 0.07929825782775879 err 27.443458557128906 tr(WHW.T) 346.0789489746094
bpp_loss 2.037992000579834
22_q proxy err 0.006385594606399536 err 129.7527618408203 tr(WHW.T) 20319.60546875
bpp_loss 2.640724539756775
22_k proxy err 0.0025742463767528534 err 37.9034538269043 tr(WHW.T) 14724.09765625
bpp_loss 3.4383957386016846
22_o proxy err 0.1085638627409935 err 132.6466827392578 tr(WHW.T) 1221.8309326171875
bpp_loss 2.0500916242599487
22_up proxy err 0.07153882086277008 err 539.9217529296875 tr(WHW.T) 7547.25537109375
bpp_loss 2.0981700079781667
22_gate proxy err 0.020435497164726257 err 603.5440063476562 tr(WHW.T) 29534.099609375
bpp_loss 2.4566078186035156
22_down proxy err 0.07555433362722397 err 493.33135986328125 tr(WHW.T) 6529.4912109375
bpp_loss 2.0930543627057756
I0410 04:44:01.747861 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 23 in 1.2733972072601318s
I0410 04:44:06.526900 1944593 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:44:06.527055 1944593 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:44:06.527098 1944593 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:44:07.023379 1944593 config.py:54] PyTorch version 2.6.0 available.
W0410 04:44:07.254751 1944593 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:44:08.052570 1944593 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:44:08.057040 1926517 quantize_finetune_llama.py:225] layer 24 gpu 0
I0410 04:44:08.075890 1944593 data_utils.py:336] using 256 training seqs, 128 validation seqs
23_v proxy err 0.07396911084651947 err 29.432830810546875 tr(WHW.T) 397.90704345703125
bpp_loss 2.0884429216384888
23_q proxy err 0.006110143382102251 err 138.14381408691406 tr(WHW.T) 22608.931640625
bpp_loss 2.6515811681747437
23_k proxy err 0.0026996906381100416 err 40.1105842590332 tr(WHW.T) 14857.474609375
bpp_loss 3.442221522331238
23_o proxy err 0.07778418064117432 err 135.69224548339844 tr(WHW.T) 1744.470947265625
bpp_loss 2.0730303525924683
23_up proxy err 0.07279420644044876 err 540.0753784179688 tr(WHW.T) 7419.2080078125
bpp_loss 2.102006503513881
23_gate proxy err 0.021959181874990463 err 599.3023681640625 tr(WHW.T) 27291.65234375
bpp_loss 2.4587597165788924
23_down proxy err 0.07353852689266205 err 490.2803039550781 tr(WHW.T) 6666.9853515625
bpp_loss 2.09845655305045
I0410 04:44:51.217953 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 24 in 1.104184865951538s
I0410 04:44:55.668354 1945345 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:44:55.668566 1945345 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:44:55.668646 1945345 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:44:56.178627 1945345 config.py:54] PyTorch version 2.6.0 available.
W0410 04:44:56.412615 1945345 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:44:57.218453 1945345 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:44:57.223090 1926517 quantize_finetune_llama.py:225] layer 25 gpu 0
I0410 04:44:57.239176 1945345 data_utils.py:336] using 256 training seqs, 128 validation seqs
24_v proxy err 0.06275264918804169 err 29.322954177856445 tr(WHW.T) 467.2783508300781
bpp_loss 2.179842233657837
24_q proxy err 0.006007705349475145 err 134.7894744873047 tr(WHW.T) 22436.099609375
bpp_loss 2.621151328086853
24_k proxy err 0.002708723535761237 err 38.419002532958984 tr(WHW.T) 14183.4345703125
bpp_loss 3.2944973707199097
24_o proxy err 0.08100932091474533 err 128.763671875 tr(WHW.T) 1589.4920654296875
bpp_loss 2.111969828605652
24_up proxy err 0.07411013543605804 err 541.829833984375 tr(WHW.T) 7311.1435546875
bpp_loss 2.106416770390102
24_gate proxy err 0.02316562458872795 err 599.4629516601562 tr(WHW.T) 25877.26171875
bpp_loss 2.464186532156808
24_down proxy err 0.07167378067970276 err 483.2201232910156 tr(WHW.T) 6741.93701171875
bpp_loss 2.103865078517369
I0410 04:45:41.553032 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 25 in 0.8803970813751221s
I0410 04:45:45.817818 1946057 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:45:45.817969 1946057 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:45:45.818019 1946057 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:45:46.246130 1946057 config.py:54] PyTorch version 2.6.0 available.
W0410 04:45:46.465829 1946057 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:45:47.223069 1946057 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:45:47.227539 1926517 quantize_finetune_llama.py:225] layer 26 gpu 0
I0410 04:45:47.244365 1946057 data_utils.py:336] using 256 training seqs, 128 validation seqs
25_v proxy err 0.05426700413227081 err 30.270605087280273 tr(WHW.T) 557.8086547851562
bpp_loss 2.1910715103149414
25_q proxy err 0.005315449554473162 err 138.79718017578125 tr(WHW.T) 26112.029296875
bpp_loss 2.6048635244369507
25_k proxy err 0.0027178791351616383 err 39.18263244628906 tr(WHW.T) 14416.6201171875
bpp_loss 3.274505138397217
25_o proxy err 0.0647057443857193 err 128.79556274414062 tr(WHW.T) 1990.481201171875
bpp_loss 2.109564185142517
25_up proxy err 0.07320376485586166 err 540.4041748046875 tr(WHW.T) 7382.19091796875
bpp_loss 2.1151533126831055
25_gate proxy err 0.022673042491078377 err 596.4696044921875 tr(WHW.T) 26307.435546875
bpp_loss 2.4720521654401506
25_down proxy err 0.07069605588912964 err 467.7216491699219 tr(WHW.T) 6615.951171875
bpp_loss 2.112436192376273
I0410 04:46:30.389108 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 26 in 0.41998863220214844s
I0410 04:46:34.678265 1946794 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:46:34.678437 1946794 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:46:34.678479 1946794 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:46:35.270493 1946794 config.py:54] PyTorch version 2.6.0 available.
W0410 04:46:35.501387 1946794 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:46:36.311089 1946794 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:46:36.315721 1926517 quantize_finetune_llama.py:225] layer 27 gpu 0
I0410 04:46:36.331480 1946794 data_utils.py:336] using 256 training seqs, 128 validation seqs
26_v proxy err 0.06601326167583466 err 28.685789108276367 tr(WHW.T) 434.54583740234375
bpp_loss 2.236975312232971
26_q proxy err 0.006077110767364502 err 130.0701141357422 tr(WHW.T) 21403.28125
bpp_loss 2.6137542724609375
26_k proxy err 0.0024465022142976522 err 37.738731384277344 tr(WHW.T) 15425.5859375
bpp_loss 3.3474745750427246
26_o proxy err 0.05373120680451393 err 128.34902954101562 tr(WHW.T) 2388.72412109375
bpp_loss 2.1264625787734985
26_up proxy err 0.0708000436425209 err 541.3128051757812 tr(WHW.T) 7645.65625
bpp_loss 2.124152728489467
26_gate proxy err 0.020672934129834175 err 597.4776611328125 tr(WHW.T) 28901.4453125
bpp_loss 2.4764946528843472
26_down proxy err 0.07134596258401871 err 472.2264404296875 tr(WHW.T) 6618.82470703125
bpp_loss 2.1198302677699496
I0410 04:47:21.504003 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 27 in 0.7554240226745605s
I0410 04:47:25.749330 1947527 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:47:25.749493 1947527 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:47:25.749536 1947527 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:47:26.361186 1947527 config.py:54] PyTorch version 2.6.0 available.
W0410 04:47:26.612462 1947527 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:47:27.437707 1947527 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:47:27.442032 1926517 quantize_finetune_llama.py:225] layer 28 gpu 0
I0410 04:47:27.458130 1947527 data_utils.py:336] using 256 training seqs, 128 validation seqs
27_v proxy err 0.046066123992204666 err 31.21872901916504 tr(WHW.T) 677.69384765625
bpp_loss 2.321457266807556
27_q proxy err 0.006498685106635094 err 138.46078491210938 tr(WHW.T) 21305.96875
bpp_loss 2.5854655504226685
27_k proxy err 0.0028836613055318594 err 40.40476608276367 tr(WHW.T) 14011.6201171875
bpp_loss 3.306148886680603
27_o proxy err 0.06137092411518097 err 132.55850219726562 tr(WHW.T) 2159.9560546875
bpp_loss 2.1663979291915894
27_up proxy err 0.06456661969423294 err 547.240478515625 tr(WHW.T) 8475.5947265625
bpp_loss 2.138979434967041
27_gate proxy err 0.018434472382068634 err 604.8173217773438 tr(WHW.T) 32809.0390625
bpp_loss 2.4856737681797574
27_down proxy err 0.062073320150375366 err 406.1679382324219 tr(WHW.T) 6543.35791015625
bpp_loss 2.1302955831800188
I0410 04:48:12.329403 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 28 in 1.150902509689331s
I0410 04:48:16.690780 1948285 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:48:16.690937 1948285 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:48:16.690978 1948285 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:48:17.252100 1948285 config.py:54] PyTorch version 2.6.0 available.
W0410 04:48:17.506540 1948285 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:48:18.450554 1948285 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:48:18.455237 1926517 quantize_finetune_llama.py:225] layer 29 gpu 0
I0410 04:48:18.473529 1948285 data_utils.py:336] using 256 training seqs, 128 validation seqs
28_v proxy err 0.05178561434149742 err 31.14824104309082 tr(WHW.T) 601.4844360351562
bpp_loss 2.368144154548645
28_q proxy err 0.006210686173290014 err 143.8636474609375 tr(WHW.T) 23163.888671875
bpp_loss 2.5960053205490112
28_k proxy err 0.0027487988118082285 err 41.21607208251953 tr(WHW.T) 14994.2119140625
bpp_loss 3.277058720588684
28_o proxy err 0.05352609604597092 err 134.42787170410156 tr(WHW.T) 2511.445556640625
bpp_loss 2.1933481693267822
28_up proxy err 0.05345432460308075 err 547.69677734375 tr(WHW.T) 10246.0703125
bpp_loss 2.1598824773515974
28_gate proxy err 0.016711866483092308 err 600.3062744140625 tr(WHW.T) 35920.9609375
bpp_loss 2.471912111554827
28_down proxy err 0.06170886382460594 err 444.9263000488281 tr(WHW.T) 7210.08740234375
bpp_loss 2.1442619391850064
I0410 04:49:03.919544 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 29 in 1.1763207912445068s
I0410 04:49:08.325739 1949064 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:49:08.325896 1949064 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:49:08.325938 1949064 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:49:08.799793 1949064 config.py:54] PyTorch version 2.6.0 available.
W0410 04:49:09.020858 1949064 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:49:09.775620 1949064 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:49:09.780115 1926517 quantize_finetune_llama.py:225] layer 30 gpu 0
I0410 04:49:09.795999 1949064 data_utils.py:336] using 256 training seqs, 128 validation seqs
29_v proxy err 0.04038940370082855 err 34.34832000732422 tr(WHW.T) 850.4290161132812
bpp_loss 2.432438611984253
29_q proxy err 0.00819135271012783 err 169.1778564453125 tr(WHW.T) 20653.2265625
bpp_loss 2.5883288383483887
29_k proxy err 0.0028894436545670033 err 47.288047790527344 tr(WHW.T) 16365.796875
bpp_loss 3.347841262817383
29_o proxy err 0.03689985349774361 err 114.4945297241211 tr(WHW.T) 3102.844970703125
bpp_loss 2.2262706756591797
29_up proxy err 0.04356471449136734 err 560.61279296875 tr(WHW.T) 12868.505859375
bpp_loss 2.1915831565856934
29_gate proxy err 0.015813833102583885 err 606.6893920898438 tr(WHW.T) 38364.47265625
bpp_loss 2.4631212779453824
29_down proxy err 0.05378328636288643 err 401.78936767578125 tr(WHW.T) 7470.5244140625
bpp_loss 2.15710381099156
I0410 04:49:55.025032 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 30 in 0.8942210674285889s
I0410 04:49:59.252957 1949801 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:49:59.253104 1949801 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:49:59.253156 1949801 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:49:59.699628 1949801 config.py:54] PyTorch version 2.6.0 available.
W0410 04:49:59.923057 1949801 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:50:00.672169 1949801 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:50:00.676567 1926517 quantize_finetune_llama.py:225] layer 31 gpu 0
I0410 04:50:00.692714 1949801 data_utils.py:336] using 256 training seqs, 128 validation seqs
30_v proxy err 0.03803471848368645 err 32.82627487182617 tr(WHW.T) 863.060791015625
bpp_loss 2.6812050342559814
30_q proxy err 0.006086442619562149 err 146.38279724121094 tr(WHW.T) 24050.6328125
bpp_loss 2.4956194162368774
30_k proxy err 0.0030166655778884888 err 42.32559585571289 tr(WHW.T) 14030.5888671875
bpp_loss 3.0635828971862793
30_o proxy err 0.02919192984700203 err 142.14060974121094 tr(WHW.T) 4869.1748046875
bpp_loss 2.306312322616577
30_up proxy err 0.0277758426964283 err 603.0467529296875 tr(WHW.T) 21711.1953125
bpp_loss 2.221611704145159
30_gate proxy err 0.012374626472592354 err 642.9169311523438 tr(WHW.T) 51954.453125
bpp_loss 2.5071450642177036
30_down proxy err 0.03827453404664993 err 337.2054748535156 tr(WHW.T) 8810.1787109375
bpp_loss 2.1559010573795865
I0410 04:50:45.106696 1926517 quantize_finetune_llama.py:256] computed original embedding for layer 31 in 0.9922454357147217s
I0410 04:50:49.531157 1950560 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:50:49.531300 1950560 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:50:49.531342 1950560 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:50:49.984680 1950560 config.py:54] PyTorch version 2.6.0 available.
W0410 04:50:50.199071 1950560 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:50:50.938536 1950560 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:50:50.972537 1950560 data_utils.py:336] using 256 training seqs, 128 validation seqs
31_v proxy err 0.0023068957962095737 err 4.172537326812744 tr(WHW.T) 1808.723876953125
bpp_loss 5.95755672454834
31_q proxy err 0.0003995926817879081 err 18.45145606994629 tr(WHW.T) 46175.66015625
bpp_loss 6.161471128463745
31_k proxy err 0.00024890972417779267 err 5.094595909118652 tr(WHW.T) 20467.64453125
bpp_loss 7.771960496902466
31_o proxy err 0.007542097475379705 err 16.69504737854004 tr(WHW.T) 2213.58154296875
bpp_loss 5.2181525230407715
31_up proxy err 0.0008836613851599395 err 60.98981857299805 tr(WHW.T) 69019.4453125
bpp_loss 5.575569425310407
31_gate proxy err 0.0004345252236817032 err 62.768280029296875 tr(WHW.T) 144452.5625
bpp_loss 6.418456486293247
31_down proxy err 0.005894441623240709 err 58.70975112915039 tr(WHW.T) 9960.1884765625
bpp_loss 4.949719224657331
I0410 04:51:43.951136 1951385 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:51:43.951284 1951385 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:51:43.951324 1951385 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:51:44.276124 1951385 config.py:54] PyTorch version 2.6.0 available.
W0410 04:51:44.484734 1951385 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0410 04:51:44.591640 1951385 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.31it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.83it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  9.03it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  9.20it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  9.30it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.27it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.40it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.22it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  9.21it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  9.29it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  9.24it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  9.31it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  9.26it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.28it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.44it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.34it/s]
I0410 04:51:47.853937 1951385 hfize_llama.py:161] loaded layer 0
I0410 04:51:48.757431 1951385 hfize_llama.py:161] loaded layer 1
I0410 04:51:49.626123 1951385 hfize_llama.py:161] loaded layer 2
I0410 04:51:50.559075 1951385 hfize_llama.py:161] loaded layer 3
I0410 04:51:51.490033 1951385 hfize_llama.py:161] loaded layer 4
I0410 04:51:52.434413 1951385 hfize_llama.py:161] loaded layer 5
I0410 04:51:53.319758 1951385 hfize_llama.py:161] loaded layer 6
I0410 04:51:54.254522 1951385 hfize_llama.py:161] loaded layer 7
I0410 04:51:55.121617 1951385 hfize_llama.py:161] loaded layer 8
I0410 04:51:55.991569 1951385 hfize_llama.py:161] loaded layer 9
I0410 04:51:56.860395 1951385 hfize_llama.py:161] loaded layer 10
I0410 04:51:57.691630 1951385 hfize_llama.py:161] loaded layer 11
I0410 04:51:58.540563 1951385 hfize_llama.py:161] loaded layer 12
I0410 04:51:59.366261 1951385 hfize_llama.py:161] loaded layer 13
I0410 04:52:00.153265 1951385 hfize_llama.py:161] loaded layer 14
I0410 04:52:00.944818 1951385 hfize_llama.py:161] loaded layer 15
I0410 04:52:01.679621 1951385 hfize_llama.py:161] loaded layer 16
I0410 04:52:02.419590 1951385 hfize_llama.py:161] loaded layer 17
I0410 04:52:03.209190 1951385 hfize_llama.py:161] loaded layer 18
I0410 04:52:03.992347 1951385 hfize_llama.py:161] loaded layer 19
I0410 04:52:04.919656 1951385 hfize_llama.py:161] loaded layer 20
I0410 04:52:05.985981 1951385 hfize_llama.py:161] loaded layer 21
I0410 04:52:07.078836 1951385 hfize_llama.py:161] loaded layer 22
I0410 04:52:08.090760 1951385 hfize_llama.py:161] loaded layer 23
I0410 04:52:09.126306 1951385 hfize_llama.py:161] loaded layer 24
I0410 04:52:09.960505 1951385 hfize_llama.py:161] loaded layer 25
I0410 04:52:10.822227 1951385 hfize_llama.py:161] loaded layer 26
I0410 04:52:11.677772 1951385 hfize_llama.py:161] loaded layer 27
I0410 04:52:12.667997 1951385 hfize_llama.py:161] loaded layer 28
I0410 04:52:13.612467 1951385 hfize_llama.py:161] loaded layer 29
I0410 04:52:14.626981 1951385 hfize_llama.py:161] loaded layer 30
I0410 04:52:15.680970 1951385 hfize_llama.py:161] loaded layer 31
I0410 04:52:15.681126 1951385 hfize_llama.py:165] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:09,  1.51s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.21s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.16s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:05<00:03,  1.27s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.20s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.23s/it]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 194, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 173, in main
    model, _ = model_from_hf_path(args.hf_output_path, device_map='cuda')
  File "/workspace/Weight_compression/comp_lm_qtip/lib/utils/unsafe_import.py", line 44, in model_from_hf_path
    model = model_cls.from_pretrained(path,
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4264, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4777, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 942, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 339, in set_module_tensor_to_device
    new_value = value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 187.00 MiB is free. Process 2515695 has 23.50 GiB memory in use. Process 2517456 has 14.34 GiB memory in use. Process 2515962 has 9.47 GiB memory in use. Of the allocated memory 23.08 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
I0410 04:53:12.796715 1952671 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:53:12.796884 1952671 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:53:12.796923 1952671 utils.py:162] NumExpr defaulting to 16 threads.
W0410 04:53:13.167495 1952671 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0410 04:53:13.553000 1952671 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.20s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.18s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.21s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.23s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.22s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.10s/it]
I0410 04:53:21.383715 1952671 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 2.266569137573242:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 2.266569137573242:   1%|          | 1/141 [00:01<04:19,  1.85s/it]avg_loss = 2.556567668914795:   1%|          | 1/141 [00:03<04:19,  1.85s/it]avg_loss = 2.556567668914795:   1%|▏         | 2/141 [00:03<03:45,  1.62s/it]avg_loss = 2.703066349029541:   1%|▏         | 2/141 [00:04<03:45,  1.62s/it]avg_loss = 2.703066349029541:   2%|▏         | 3/141 [00:04<03:34,  1.55s/it]avg_loss = 2.6678346395492554:   2%|▏         | 3/141 [00:06<03:34,  1.55s/it]avg_loss = 2.6678346395492554:   3%|▎         | 4/141 [00:06<03:28,  1.52s/it]avg_loss = 2.6390830516815185:   3%|▎         | 4/141 [00:07<03:28,  1.52s/it]avg_loss = 2.6390830516815185:   4%|▎         | 5/141 [00:07<03:24,  1.50s/it]avg_loss = 2.5544177691141763:   4%|▎         | 5/141 [00:09<03:24,  1.50s/it]avg_loss = 2.5544177691141763:   4%|▍         | 6/141 [00:09<03:22,  1.50s/it]avg_loss = 2.5065195901053294:   4%|▍         | 6/141 [00:10<03:22,  1.50s/it]avg_loss = 2.5065195901053294:   5%|▍         | 7/141 [00:10<03:20,  1.49s/it]avg_loss = 2.5033918619155884:   5%|▍         | 7/141 [00:12<03:20,  1.49s/it]avg_loss = 2.5033918619155884:   6%|▌         | 8/141 [00:12<03:18,  1.49s/it]avg_loss = 2.5521628061930337:   6%|▌         | 8/141 [00:13<03:18,  1.49s/it]avg_loss = 2.5521628061930337:   6%|▋         | 9/141 [00:13<03:16,  1.49s/it]avg_loss = 2.5435133457183836:   6%|▋         | 9/141 [00:15<03:16,  1.49s/it]avg_loss = 2.5435133457183836:   7%|▋         | 10/141 [00:15<03:15,  1.49s/it]avg_loss = 2.5341224020177666:   7%|▋         | 10/141 [00:16<03:15,  1.49s/it]avg_loss = 2.5341224020177666:   8%|▊         | 11/141 [00:16<03:13,  1.49s/it]avg_loss = 2.5576140880584717:   8%|▊         | 11/141 [00:18<03:13,  1.49s/it]avg_loss = 2.5576140880584717:   9%|▊         | 12/141 [00:18<03:12,  1.49s/it]avg_loss = 2.5716001437260556:   9%|▊         | 12/141 [00:19<03:12,  1.49s/it]avg_loss = 2.5716001437260556:   9%|▉         | 13/141 [00:19<03:11,  1.50s/it]avg_loss = 2.592119778905596:   9%|▉         | 13/141 [00:21<03:11,  1.50s/it] avg_loss = 2.592119778905596:  10%|▉         | 14/141 [00:21<03:10,  1.50s/it]avg_loss = 2.6015787919362388:  10%|▉         | 14/141 [00:22<03:10,  1.50s/it]avg_loss = 2.6015787919362388:  11%|█         | 15/141 [00:22<03:09,  1.50s/it]avg_loss = 2.6224371939897537:  11%|█         | 15/141 [00:24<03:09,  1.50s/it]avg_loss = 2.6224371939897537:  11%|█▏        | 16/141 [00:24<03:08,  1.50s/it]avg_loss = 2.6251979996176327:  11%|█▏        | 16/141 [00:25<03:08,  1.50s/it]avg_loss = 2.6251979996176327:  12%|█▏        | 17/141 [00:25<03:07,  1.51s/it]avg_loss = 2.6276059680514865:  12%|█▏        | 17/141 [00:27<03:07,  1.51s/it]avg_loss = 2.6276059680514865:  13%|█▎        | 18/141 [00:27<03:06,  1.51s/it]avg_loss = 2.613146242342497:  13%|█▎        | 18/141 [00:28<03:06,  1.51s/it] avg_loss = 2.613146242342497:  13%|█▎        | 19/141 [00:28<03:04,  1.52s/it]avg_loss = 2.6130332350730896:  13%|█▎        | 19/141 [00:30<03:04,  1.52s/it]avg_loss = 2.6130332350730896:  14%|█▍        | 20/141 [00:30<03:03,  1.52s/it]avg_loss = 2.618393307640439:  14%|█▍        | 20/141 [00:31<03:03,  1.52s/it] avg_loss = 2.618393307640439:  15%|█▍        | 21/141 [00:31<03:02,  1.52s/it]avg_loss = 2.6195833791385996:  15%|█▍        | 21/141 [00:33<03:02,  1.52s/it]avg_loss = 2.6195833791385996:  16%|█▌        | 22/141 [00:33<03:01,  1.52s/it]avg_loss = 2.62228945027227:  16%|█▌        | 22/141 [00:34<03:01,  1.52s/it]  avg_loss = 2.62228945027227:  16%|█▋        | 23/141 [00:34<02:59,  1.52s/it]avg_loss = 2.6260142624378204:  16%|█▋        | 23/141 [00:36<02:59,  1.52s/it]avg_loss = 2.6260142624378204:  17%|█▋        | 24/141 [00:36<02:58,  1.53s/it]avg_loss = 2.6327415466308595:  17%|█▋        | 24/141 [00:37<02:58,  1.53s/it]avg_loss = 2.6327415466308595:  18%|█▊        | 25/141 [00:37<02:57,  1.53s/it]avg_loss = 2.641389067356403:  18%|█▊        | 25/141 [00:39<02:57,  1.53s/it] avg_loss = 2.641389067356403:  18%|█▊        | 26/141 [00:39<02:55,  1.53s/it]avg_loss = 2.6524806993978993:  18%|█▊        | 26/141 [00:40<02:55,  1.53s/it]avg_loss = 2.6524806993978993:  19%|█▉        | 27/141 [00:40<02:54,  1.53s/it]avg_loss = 2.655244563307081:  19%|█▉        | 27/141 [00:42<02:54,  1.53s/it] avg_loss = 2.655244563307081:  20%|█▉        | 28/141 [00:42<02:53,  1.53s/it]avg_loss = 2.6517317377287766:  20%|█▉        | 28/141 [00:44<02:53,  1.53s/it]avg_loss = 2.6517317377287766:  21%|██        | 29/141 [00:44<02:52,  1.54s/it]avg_loss = 2.641995596885681:  21%|██        | 29/141 [00:45<02:52,  1.54s/it] avg_loss = 2.641995596885681:  21%|██▏       | 30/141 [00:45<02:50,  1.54s/it]avg_loss = 2.633380397673576:  21%|██▏       | 30/141 [00:47<02:50,  1.54s/it]avg_loss = 2.633380397673576:  22%|██▏       | 31/141 [00:47<02:49,  1.54s/it]avg_loss = 2.6228254958987236:  22%|██▏       | 31/141 [00:48<02:49,  1.54s/it]avg_loss = 2.6228254958987236:  23%|██▎       | 32/141 [00:48<02:47,  1.54s/it]avg_loss = 2.620969613393148:  23%|██▎       | 32/141 [00:50<02:47,  1.54s/it] avg_loss = 2.620969613393148:  23%|██▎       | 33/141 [00:50<02:46,  1.54s/it]avg_loss = 2.6180685337852028:  23%|██▎       | 33/141 [00:51<02:46,  1.54s/it]avg_loss = 2.6180685337852028:  24%|██▍       | 34/141 [00:51<02:45,  1.55s/it]avg_loss = 2.6223610537392754:  24%|██▍       | 34/141 [00:53<02:45,  1.55s/it]avg_loss = 2.6223610537392754:  25%|██▍       | 35/141 [00:53<02:44,  1.55s/it]avg_loss = 2.604556295606825:  25%|██▍       | 35/141 [00:54<02:44,  1.55s/it] avg_loss = 2.604556295606825:  26%|██▌       | 36/141 [00:54<02:42,  1.55s/it]avg_loss = 2.5920871077357113:  26%|██▌       | 36/141 [00:56<02:42,  1.55s/it]avg_loss = 2.5920871077357113:  26%|██▌       | 37/141 [00:56<02:41,  1.55s/it]avg_loss = 2.578046497545744:  26%|██▌       | 37/141 [00:58<02:41,  1.55s/it] avg_loss = 2.578046497545744:  27%|██▋       | 38/141 [00:58<02:40,  1.55s/it]avg_loss = 2.5637388351636057:  27%|██▋       | 38/141 [00:59<02:40,  1.55s/it]avg_loss = 2.5637388351636057:  28%|██▊       | 39/141 [00:59<02:38,  1.56s/it]avg_loss = 2.555300033092499:  28%|██▊       | 39/141 [01:01<02:38,  1.56s/it] avg_loss = 2.555300033092499:  28%|██▊       | 40/141 [01:01<02:37,  1.56s/it]avg_loss = 2.5609473193564067:  28%|██▊       | 40/141 [01:02<02:37,  1.56s/it]avg_loss = 2.5609473193564067:  29%|██▉       | 41/141 [01:02<02:35,  1.56s/it]avg_loss = 2.5751734234037853:  29%|██▉       | 41/141 [01:04<02:35,  1.56s/it]avg_loss = 2.5751734234037853:  30%|██▉       | 42/141 [01:04<02:34,  1.56s/it]avg_loss = 2.5901458706966665:  30%|██▉       | 42/141 [01:05<02:34,  1.56s/it]avg_loss = 2.5901458706966665:  30%|███       | 43/141 [01:05<02:32,  1.56s/it]avg_loss = 2.599064577709545:  30%|███       | 43/141 [01:07<02:32,  1.56s/it] avg_loss = 2.599064577709545:  31%|███       | 44/141 [01:07<02:31,  1.56s/it]avg_loss = 2.6057001802656385:  31%|███       | 44/141 [01:08<02:31,  1.56s/it]avg_loss = 2.6057001802656385:  32%|███▏      | 45/141 [01:08<02:30,  1.56s/it]avg_loss = 2.6094418556793877:  32%|███▏      | 45/141 [01:10<02:30,  1.56s/it]avg_loss = 2.6094418556793877:  33%|███▎      | 46/141 [01:10<02:28,  1.56s/it]avg_loss = 2.615160120294449:  33%|███▎      | 46/141 [01:12<02:28,  1.56s/it] avg_loss = 2.615160120294449:  33%|███▎      | 47/141 [01:12<02:26,  1.56s/it]avg_loss = 2.6166168550650277:  33%|███▎      | 47/141 [01:13<02:26,  1.56s/it]avg_loss = 2.6166168550650277:  34%|███▍      | 48/141 [01:13<02:25,  1.56s/it]avg_loss = 2.6141970498221263:  34%|███▍      | 48/141 [01:15<02:25,  1.56s/it]avg_loss = 2.6141970498221263:  35%|███▍      | 49/141 [01:15<02:23,  1.56s/it]avg_loss = 2.6115641593933105:  35%|███▍      | 49/141 [01:16<02:23,  1.56s/it]avg_loss = 2.6115641593933105:  35%|███▌      | 50/141 [01:16<02:22,  1.56s/it]avg_loss = 2.604716843249751:  35%|███▌      | 50/141 [01:18<02:22,  1.56s/it] avg_loss = 2.604716843249751:  36%|███▌      | 51/141 [01:18<02:20,  1.57s/it]avg_loss = 2.5990067628713756:  36%|███▌      | 51/141 [01:19<02:20,  1.57s/it]avg_loss = 2.5990067628713756:  37%|███▋      | 52/141 [01:19<02:19,  1.57s/it]avg_loss = 2.592719685356572:  37%|███▋      | 52/141 [01:21<02:19,  1.57s/it] avg_loss = 2.592719685356572:  38%|███▊      | 53/141 [01:21<02:18,  1.57s/it]avg_loss = 2.588897011898182:  38%|███▊      | 53/141 [01:23<02:18,  1.57s/it]avg_loss = 2.588897011898182:  38%|███▊      | 54/141 [01:23<02:16,  1.57s/it]avg_loss = 2.57967191175981:  38%|███▊      | 54/141 [01:24<02:16,  1.57s/it] avg_loss = 2.57967191175981:  39%|███▉      | 55/141 [01:24<02:15,  1.57s/it]avg_loss = 2.5713763875620708:  39%|███▉      | 55/141 [01:26<02:15,  1.57s/it]avg_loss = 2.5713763875620708:  40%|███▉      | 56/141 [01:26<02:13,  1.57s/it]avg_loss = 2.570117335570486:  40%|███▉      | 56/141 [01:27<02:13,  1.57s/it] avg_loss = 2.570117335570486:  40%|████      | 57/141 [01:27<02:12,  1.57s/it]avg_loss = 2.5667104186682868:  40%|████      | 57/141 [01:29<02:12,  1.57s/it]avg_loss = 2.5667104186682868:  41%|████      | 58/141 [01:29<02:10,  1.57s/it]avg_loss = 2.568879523519742:  41%|████      | 58/141 [01:30<02:10,  1.57s/it] avg_loss = 2.568879523519742:  42%|████▏     | 59/141 [01:30<02:09,  1.57s/it]avg_loss = 2.573709321022034:  42%|████▏     | 59/141 [01:32<02:09,  1.57s/it]avg_loss = 2.573709321022034:  43%|████▎     | 60/141 [01:32<02:07,  1.57s/it]avg_loss = 2.5773864847714782:  43%|████▎     | 60/141 [01:34<02:07,  1.57s/it]avg_loss = 2.5773864847714782:  43%|████▎     | 61/141 [01:34<02:06,  1.58s/it]avg_loss = 2.584855491115201:  43%|████▎     | 61/141 [01:35<02:06,  1.58s/it] avg_loss = 2.584855491115201:  44%|████▍     | 62/141 [01:35<02:04,  1.58s/it]avg_loss = 2.5776312313382586:  44%|████▍     | 62/141 [01:37<02:04,  1.58s/it]avg_loss = 2.5776312313382586:  45%|████▍     | 63/141 [01:37<02:03,  1.58s/it]avg_loss = 2.5752404890954494:  45%|████▍     | 63/141 [01:38<02:03,  1.58s/it]avg_loss = 2.5752404890954494:  45%|████▌     | 64/141 [01:38<02:01,  1.58s/it]avg_loss = 2.574124585665189:  45%|████▌     | 64/141 [01:40<02:01,  1.58s/it] avg_loss = 2.574124585665189:  46%|████▌     | 65/141 [01:40<02:00,  1.58s/it]avg_loss = 2.568797642534429:  46%|████▌     | 65/141 [01:42<02:00,  1.58s/it]avg_loss = 2.568797642534429:  47%|████▋     | 66/141 [01:42<01:58,  1.58s/it]avg_loss = 2.5650430366174497:  47%|████▋     | 66/141 [01:43<01:58,  1.58s/it]avg_loss = 2.5650430366174497:  48%|████▊     | 67/141 [01:43<01:57,  1.58s/it]avg_loss = 2.5640328771927776:  48%|████▊     | 67/141 [01:45<01:57,  1.58s/it]avg_loss = 2.5640328771927776:  48%|████▊     | 68/141 [01:45<01:55,  1.58s/it]avg_loss = 2.5624437435813574:  48%|████▊     | 68/141 [01:46<01:55,  1.58s/it]avg_loss = 2.5624437435813574:  49%|████▉     | 69/141 [01:46<01:54,  1.58s/it]avg_loss = 2.563782593182155:  49%|████▉     | 69/141 [01:48<01:54,  1.58s/it] avg_loss = 2.563782593182155:  50%|████▉     | 70/141 [01:48<01:52,  1.58s/it]avg_loss = 2.5683327325632876:  50%|████▉     | 70/141 [01:49<01:52,  1.58s/it]avg_loss = 2.5683327325632876:  50%|█████     | 71/141 [01:49<01:50,  1.58s/it]avg_loss = 2.5708545015917883:  50%|█████     | 71/141 [01:51<01:50,  1.58s/it]avg_loss = 2.5708545015917883:  51%|█████     | 72/141 [01:51<01:49,  1.58s/it]avg_loss = 2.5685504331980664:  51%|█████     | 72/141 [01:53<01:49,  1.58s/it]avg_loss = 2.5685504331980664:  52%|█████▏    | 73/141 [01:53<01:47,  1.58s/it]avg_loss = 2.5688508555695817:  52%|█████▏    | 73/141 [01:54<01:47,  1.58s/it]avg_loss = 2.5688508555695817:  52%|█████▏    | 74/141 [01:54<01:46,  1.59s/it]avg_loss = 2.568929812113444:  52%|█████▏    | 74/141 [01:56<01:46,  1.59s/it] avg_loss = 2.568929812113444:  53%|█████▎    | 75/141 [01:56<01:44,  1.59s/it]avg_loss = 2.5672615295962284:  53%|█████▎    | 75/141 [01:57<01:44,  1.59s/it]avg_loss = 2.5672615295962284:  54%|█████▍    | 76/141 [01:57<01:43,  1.59s/it]avg_loss = 2.5675026472512776:  54%|█████▍    | 76/141 [01:59<01:43,  1.59s/it]avg_loss = 2.5675026472512776:  55%|█████▍    | 77/141 [01:59<01:41,  1.59s/it]avg_loss = 2.5693190464606652:  55%|█████▍    | 77/141 [02:01<01:41,  1.59s/it]avg_loss = 2.5693190464606652:  55%|█████▌    | 78/141 [02:01<01:40,  1.59s/it]avg_loss = 2.571939999544168:  55%|█████▌    | 78/141 [02:02<01:40,  1.59s/it] avg_loss = 2.571939999544168:  56%|█████▌    | 79/141 [02:02<01:38,  1.59s/it]avg_loss = 2.565994477272034:  56%|█████▌    | 79/141 [02:04<01:38,  1.59s/it]avg_loss = 2.565994477272034:  57%|█████▋    | 80/141 [02:04<01:36,  1.59s/it]avg_loss = 2.56331299263754:  57%|█████▋    | 80/141 [02:05<01:36,  1.59s/it] avg_loss = 2.56331299263754:  57%|█████▋    | 81/141 [02:05<01:35,  1.59s/it]avg_loss = 2.5616210873534038:  57%|█████▋    | 81/141 [02:07<01:35,  1.59s/it]avg_loss = 2.5616210873534038:  58%|█████▊    | 82/141 [02:07<01:33,  1.59s/it]avg_loss = 2.5590219239154495:  58%|█████▊    | 82/141 [02:08<01:33,  1.59s/it]avg_loss = 2.5590219239154495:  59%|█████▉    | 83/141 [02:08<01:32,  1.59s/it]avg_loss = 2.556865073385693:  59%|█████▉    | 83/141 [02:10<01:32,  1.59s/it] avg_loss = 2.556865073385693:  60%|█████▉    | 84/141 [02:10<01:30,  1.59s/it]avg_loss = 2.5549365997314455:  60%|█████▉    | 84/141 [02:12<01:30,  1.59s/it]avg_loss = 2.5549365997314455:  60%|██████    | 85/141 [02:12<01:29,  1.59s/it]avg_loss = 2.556090848390446:  60%|██████    | 85/141 [02:13<01:29,  1.59s/it] avg_loss = 2.556090848390446:  61%|██████    | 86/141 [02:13<01:27,  1.59s/it]avg_loss = 2.557505980305288:  61%|██████    | 86/141 [02:15<01:27,  1.59s/it]avg_loss = 2.557505980305288:  62%|██████▏   | 87/141 [02:15<01:26,  1.59s/it]avg_loss = 2.5594082068313253:  62%|██████▏   | 87/141 [02:16<01:26,  1.59s/it]avg_loss = 2.5594082068313253:  62%|██████▏   | 88/141 [02:16<01:24,  1.59s/it]avg_loss = 2.5679759282744334:  62%|██████▏   | 88/141 [02:18<01:24,  1.59s/it]avg_loss = 2.5679759282744334:  63%|██████▎   | 89/141 [02:18<01:22,  1.59s/it]avg_loss = 2.575405152638753:  63%|██████▎   | 89/141 [02:20<01:22,  1.59s/it] avg_loss = 2.575405152638753:  64%|██████▍   | 90/141 [02:20<01:21,  1.60s/it]avg_loss = 2.5779943728184964:  64%|██████▍   | 90/141 [02:21<01:21,  1.60s/it]avg_loss = 2.5779943728184964:  65%|██████▍   | 91/141 [02:21<01:19,  1.60s/it]avg_loss = 2.583245578019515:  65%|██████▍   | 91/141 [02:23<01:19,  1.60s/it] avg_loss = 2.583245578019515:  65%|██████▌   | 92/141 [02:23<01:18,  1.60s/it]avg_loss = 2.5879415235211773:  65%|██████▌   | 92/141 [02:24<01:18,  1.60s/it]avg_loss = 2.5879415235211773:  66%|██████▌   | 93/141 [02:24<01:16,  1.60s/it]avg_loss = 2.587810901885337:  66%|██████▌   | 93/141 [02:26<01:16,  1.60s/it] avg_loss = 2.587810901885337:  67%|██████▋   | 94/141 [02:26<01:15,  1.60s/it]avg_loss = 2.5916203323163485:  67%|██████▋   | 94/141 [02:28<01:15,  1.60s/it]avg_loss = 2.5916203323163485:  67%|██████▋   | 95/141 [02:28<01:13,  1.60s/it]avg_loss = 2.591853121916453:  67%|██████▋   | 95/141 [02:29<01:13,  1.60s/it] avg_loss = 2.591853121916453:  68%|██████▊   | 96/141 [02:29<01:11,  1.60s/it]avg_loss = 2.593373827098571:  68%|██████▊   | 96/141 [02:31<01:11,  1.60s/it]avg_loss = 2.593373827098571:  69%|██████▉   | 97/141 [02:31<01:10,  1.60s/it]avg_loss = 2.592083986924619:  69%|██████▉   | 97/141 [02:32<01:10,  1.60s/it]avg_loss = 2.592083986924619:  70%|██████▉   | 98/141 [02:32<01:08,  1.60s/it]avg_loss = 2.5937122383503:  70%|██████▉   | 98/141 [02:34<01:08,  1.60s/it]  avg_loss = 2.5937122383503:  70%|███████   | 99/141 [02:34<01:07,  1.60s/it]avg_loss = 2.5965784192085266:  70%|███████   | 99/141 [02:36<01:07,  1.60s/it]avg_loss = 2.5965784192085266:  71%|███████   | 100/141 [02:36<01:05,  1.60s/it]avg_loss = 2.5970808373819483:  71%|███████   | 100/141 [02:37<01:05,  1.60s/it]avg_loss = 2.5970808373819483:  72%|███████▏  | 101/141 [02:37<01:03,  1.60s/it]avg_loss = 2.5984638461879657:  72%|███████▏  | 101/141 [02:39<01:03,  1.60s/it]avg_loss = 2.5984638461879657:  72%|███████▏  | 102/141 [02:39<01:02,  1.60s/it]avg_loss = 2.599416767509238:  72%|███████▏  | 102/141 [02:40<01:02,  1.60s/it] avg_loss = 2.599416767509238:  73%|███████▎  | 103/141 [02:40<01:00,  1.60s/it]avg_loss = 2.603805069740002:  73%|███████▎  | 103/141 [02:42<01:00,  1.60s/it]avg_loss = 2.603805069740002:  74%|███████▍  | 104/141 [02:42<00:59,  1.60s/it]avg_loss = 2.6035167671385264:  74%|███████▍  | 104/141 [02:44<00:59,  1.60s/it]avg_loss = 2.6035167671385264:  74%|███████▍  | 105/141 [02:44<00:57,  1.60s/it]avg_loss = 2.6033615121301614:  74%|███████▍  | 105/141 [02:45<00:57,  1.60s/it]avg_loss = 2.6033615121301614:  75%|███████▌  | 106/141 [02:45<00:55,  1.60s/it]avg_loss = 2.6021046326539228:  75%|███████▌  | 106/141 [02:47<00:55,  1.60s/it]avg_loss = 2.6021046326539228:  76%|███████▌  | 107/141 [02:47<00:54,  1.60s/it]avg_loss = 2.600109431478712:  76%|███████▌  | 107/141 [02:48<00:54,  1.60s/it] avg_loss = 2.600109431478712:  77%|███████▋  | 108/141 [02:48<00:52,  1.60s/it]avg_loss = 2.599375536682409:  77%|███████▋  | 108/141 [02:50<00:52,  1.60s/it]avg_loss = 2.599375536682409:  77%|███████▋  | 109/141 [02:50<00:51,  1.60s/it]avg_loss = 2.59629176529971:  77%|███████▋  | 109/141 [02:52<00:51,  1.60s/it] avg_loss = 2.59629176529971:  78%|███████▊  | 110/141 [02:52<00:49,  1.60s/it]avg_loss = 2.599256423142579:  78%|███████▊  | 110/141 [02:53<00:49,  1.60s/it]avg_loss = 2.599256423142579:  79%|███████▊  | 111/141 [02:53<00:48,  1.60s/it]avg_loss = 2.5993747242859433:  79%|███████▊  | 111/141 [02:55<00:48,  1.60s/it]avg_loss = 2.5993747242859433:  79%|███████▉  | 112/141 [02:55<00:46,  1.60s/it]avg_loss = 2.600774199561735:  79%|███████▉  | 112/141 [02:56<00:46,  1.60s/it] avg_loss = 2.600774199561735:  80%|████████  | 113/141 [02:56<00:44,  1.60s/it]avg_loss = 2.6029713467547766:  80%|████████  | 113/141 [02:58<00:44,  1.60s/it]avg_loss = 2.6029713467547766:  81%|████████  | 114/141 [02:58<00:43,  1.60s/it]avg_loss = 2.6013690222864567:  81%|████████  | 114/141 [03:00<00:43,  1.60s/it]avg_loss = 2.6013690222864567:  82%|████████▏ | 115/141 [03:00<00:41,  1.60s/it]avg_loss = 2.599422236968731:  82%|████████▏ | 115/141 [03:01<00:41,  1.60s/it] avg_loss = 2.599422236968731:  82%|████████▏ | 116/141 [03:01<00:40,  1.60s/it]avg_loss = 2.6015020109649396:  82%|████████▏ | 116/141 [03:03<00:40,  1.60s/it]avg_loss = 2.6015020109649396:  83%|████████▎ | 117/141 [03:03<00:38,  1.60s/it]avg_loss = 2.5999155933574096:  83%|████████▎ | 117/141 [03:04<00:38,  1.60s/it]avg_loss = 2.5999155933574096:  84%|████████▎ | 118/141 [03:04<00:36,  1.60s/it]avg_loss = 2.597719086318457:  84%|████████▎ | 118/141 [03:06<00:36,  1.60s/it] avg_loss = 2.597719086318457:  84%|████████▍ | 119/141 [03:06<00:35,  1.60s/it]avg_loss = 2.595150190591812:  84%|████████▍ | 119/141 [03:08<00:35,  1.60s/it]avg_loss = 2.595150190591812:  85%|████████▌ | 120/141 [03:08<00:33,  1.60s/it]avg_loss = 2.595394158166302:  85%|████████▌ | 120/141 [03:09<00:33,  1.60s/it]avg_loss = 2.595394158166302:  86%|████████▌ | 121/141 [03:09<00:32,  1.60s/it]avg_loss = 2.5957015733249853:  86%|████████▌ | 121/141 [03:11<00:32,  1.60s/it]avg_loss = 2.5957015733249853:  87%|████████▋ | 122/141 [03:11<00:30,  1.60s/it]avg_loss = 2.594549873010899:  87%|████████▋ | 122/141 [03:12<00:30,  1.60s/it] avg_loss = 2.594549873010899:  87%|████████▋ | 123/141 [03:12<00:28,  1.60s/it]avg_loss = 2.5945108225268703:  87%|████████▋ | 123/141 [03:14<00:28,  1.60s/it]avg_loss = 2.5945108225268703:  88%|████████▊ | 124/141 [03:14<00:27,  1.60s/it]avg_loss = 2.592869411468506:  88%|████████▊ | 124/141 [03:16<00:27,  1.60s/it] avg_loss = 2.592869411468506:  89%|████████▊ | 125/141 [03:16<00:25,  1.60s/it]avg_loss = 2.5922239716090854:  89%|████████▊ | 125/141 [03:17<00:25,  1.60s/it]avg_loss = 2.5922239716090854:  89%|████████▉ | 126/141 [03:17<00:24,  1.60s/it]avg_loss = 2.5918890397379717:  89%|████████▉ | 126/141 [03:19<00:24,  1.60s/it]avg_loss = 2.5918890397379717:  90%|█████████ | 127/141 [03:19<00:22,  1.60s/it]avg_loss = 2.5902831498533487:  90%|█████████ | 127/141 [03:20<00:22,  1.60s/it]avg_loss = 2.5902831498533487:  91%|█████████ | 128/141 [03:20<00:20,  1.60s/it]avg_loss = 2.5901862318201583:  91%|█████████ | 128/141 [03:22<00:20,  1.60s/it]avg_loss = 2.5901862318201583:  91%|█████████▏| 129/141 [03:22<00:19,  1.60s/it]avg_loss = 2.59125657081604:  91%|█████████▏| 129/141 [03:24<00:19,  1.60s/it]  avg_loss = 2.59125657081604:  92%|█████████▏| 130/141 [03:24<00:17,  1.60s/it]avg_loss = 2.5917839057572927:  92%|█████████▏| 130/141 [03:25<00:17,  1.60s/it]avg_loss = 2.5917839057572927:  93%|█████████▎| 131/141 [03:25<00:16,  1.60s/it]avg_loss = 2.592085417472955:  93%|█████████▎| 131/141 [03:27<00:16,  1.60s/it] avg_loss = 2.592085417472955:  94%|█████████▎| 132/141 [03:27<00:14,  1.60s/it]avg_loss = 2.5883063875642933:  94%|█████████▎| 132/141 [03:29<00:14,  1.60s/it]avg_loss = 2.5883063875642933:  94%|█████████▍| 133/141 [03:29<00:12,  1.60s/it]avg_loss = 2.5822965404880582:  94%|█████████▍| 133/141 [03:30<00:12,  1.60s/it]avg_loss = 2.5822965404880582:  95%|█████████▌| 134/141 [03:30<00:11,  1.60s/it]avg_loss = 2.584657236381813:  95%|█████████▌| 134/141 [03:32<00:11,  1.60s/it] avg_loss = 2.584657236381813:  96%|█████████▌| 135/141 [03:32<00:09,  1.60s/it]avg_loss = 2.588541113278445:  96%|█████████▌| 135/141 [03:33<00:09,  1.60s/it]avg_loss = 2.588541113278445:  96%|█████████▋| 136/141 [03:33<00:08,  1.60s/it]avg_loss = 2.590866936384326:  96%|█████████▋| 136/141 [03:35<00:08,  1.60s/it]avg_loss = 2.590866936384326:  97%|█████████▋| 137/141 [03:35<00:06,  1.60s/it]avg_loss = 2.590651969978775:  97%|█████████▋| 137/141 [03:37<00:06,  1.60s/it]avg_loss = 2.590651969978775:  98%|█████████▊| 138/141 [03:37<00:04,  1.60s/it]avg_loss = 2.5919368678717305:  98%|█████████▊| 138/141 [03:38<00:04,  1.60s/it]avg_loss = 2.5919368678717305:  99%|█████████▊| 139/141 [03:38<00:03,  1.60s/it]avg_loss = 2.5935151611055645:  99%|█████████▊| 139/141 [03:40<00:03,  1.60s/it]avg_loss = 2.5935151611055645:  99%|█████████▉| 140/141 [03:40<00:01,  1.60s/it]avg_loss = 2.5953634816704065:  99%|█████████▉| 140/141 [03:41<00:01,  1.60s/it]avg_loss = 2.5953634816704065: 100%|██████████| 141/141 [03:41<00:00,  1.61s/it]avg_loss = 2.5953634816704065: 100%|██████████| 141/141 [03:41<00:00,  1.57s/it]
I0410 04:57:27.030852 1952671 eval_ppl.py:107] wikitext2 perplexity: 13.401455879211426
wikitext2 perplexity: 13.401
