I0410 04:01:39.577057 1904084 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:01:39.577182 1904084 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:01:39.577229 1904084 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:01:39.906079 1904084 config.py:54] PyTorch version 2.6.0 available.
W0410 04:01:40.095240 1904084 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:01:40.767066 1904084 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.24it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  5.95it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  6.19it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  5.82it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  5.62it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  5.53it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.51it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.67it/s]
I0410 04:01:42.664639 1904084 quantize_finetune_llama.py:163] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:23,  1.34it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:21,  1.43it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:19,  1.46it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:18,  1.55it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:16,  1.61it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:15,  1.66it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:14,  1.70it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:13,  1.74it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:05<00:13,  1.76it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.85it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:11,  1.90it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.89it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:09,  1.96it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.98it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  2.02it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:07,  2.01it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.88it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:10<00:07,  1.80it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:07,  1.76it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:11<00:06,  1.76it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:06,  1.78it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:12<00:05,  1.78it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.81it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:13<00:04,  1.83it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:14<00:03,  1.83it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:14<00:03,  1.77it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:15<00:02,  1.78it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:15<00:02,  1.83it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:16<00:01,  1.88it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:16<00:01,  1.89it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:17<00:00,  1.90it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:17<00:00,  1.86it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:17<00:00,  1.80it/s]
I0410 04:02:11.801908 1904084 quantize_finetune_llama.py:201] loaded compression model
I0410 04:02:33.809140 1904084 quantize_finetune_llama.py:205] loaded dataset and devset
I0410 04:02:36.291400 1904084 quantize_finetune_llama.py:225] layer 0 gpu 0
I0410 04:02:38.800283 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 0 in 2.3237459659576416s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0410 04:02:52.482371 1905321 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:02:52.482531 1905321 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:02:52.482624 1905321 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:02:52.830655 1905321 config.py:54] PyTorch version 2.6.0 available.
W0410 04:02:53.039641 1905321 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:02:53.637988 1905321 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:02:53.642019 1904084 quantize_finetune_llama.py:225] layer 1 gpu 0
I0410 04:02:55.729225 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 1 in 0.8533725738525391s
I0410 04:02:59.764683 1905474 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:02:59.764805 1905474 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:02:59.764850 1905474 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:03:00.165101 1905474 config.py:54] PyTorch version 2.6.0 available.
W0410 04:03:00.387533 1905474 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:03:01.049889 1905474 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:03:01.053958 1904084 quantize_finetune_llama.py:225] layer 2 gpu 0
I0410 04:03:03.223043 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 2 in 0.8622658252716064s
I0410 04:03:07.326828 1905633 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:03:07.326947 1905633 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:03:07.326992 1905633 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:03:07.740494 1905633 config.py:54] PyTorch version 2.6.0 available.
W0410 04:03:07.969729 1905633 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:03:08.664646 1905633 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:03:08.669260 1904084 quantize_finetune_llama.py:225] layer 3 gpu 0
I0410 04:03:11.280428 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 3 in 0.948143720626831s
I0410 04:03:15.167162 1905771 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:03:15.167281 1905771 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:03:15.167322 1905771 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:03:15.563915 1905771 config.py:54] PyTorch version 2.6.0 available.
W0410 04:03:15.785690 1905771 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:03:16.413675 1905771 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:03:16.418034 1904084 quantize_finetune_llama.py:225] layer 4 gpu 0
I0410 04:03:19.024309 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 4 in 1.043372392654419s
I0410 04:03:22.893067 1905958 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:03:22.893192 1905958 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:03:22.893233 1905958 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:03:23.286080 1905958 config.py:54] PyTorch version 2.6.0 available.
W0410 04:03:23.503906 1905958 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:03:24.086919 1905958 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:03:24.091153 1904084 quantize_finetune_llama.py:225] layer 5 gpu 0
I0410 04:03:26.241170 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 5 in 0.9266219139099121s
I0410 04:03:30.300696 1906108 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:03:30.300805 1906108 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:03:30.300847 1906108 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:03:30.695390 1906108 config.py:54] PyTorch version 2.6.0 available.
W0410 04:03:30.923439 1906108 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:03:31.572934 1906108 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:03:31.577215 1904084 quantize_finetune_llama.py:225] layer 6 gpu 0
I0410 04:03:34.175049 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 6 in 1.113295316696167s
I0410 04:03:37.893221 1906317 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:03:37.893328 1906317 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:03:37.893369 1906317 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:03:38.296901 1906317 config.py:54] PyTorch version 2.6.0 available.
W0410 04:03:38.515994 1906317 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:03:39.174142 1906317 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:03:39.178478 1904084 quantize_finetune_llama.py:225] layer 7 gpu 0
I0410 04:03:41.815564 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 7 in 1.0759296417236328s
I0410 04:03:45.808757 1906512 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:03:45.808877 1906512 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:03:45.808917 1906512 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:03:46.204174 1906512 config.py:54] PyTorch version 2.6.0 available.
W0410 04:03:46.420497 1906512 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:03:47.082457 1906512 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:03:47.086554 1904084 quantize_finetune_llama.py:225] layer 8 gpu 0
I0410 04:03:49.891520 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 8 in 1.2613856792449951s
I0410 04:03:54.181883 1906654 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:03:54.181988 1906654 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:03:54.182029 1906654 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:03:54.593243 1906654 config.py:54] PyTorch version 2.6.0 available.
W0410 04:03:54.810876 1906654 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:03:55.430547 1906654 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:03:55.434326 1904084 quantize_finetune_llama.py:225] layer 9 gpu 0
I0410 04:03:57.790178 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 9 in 1.1115269660949707s
I0410 04:04:01.913449 1906778 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:04:01.913599 1906778 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:04:01.913644 1906778 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:04:02.303763 1906778 config.py:54] PyTorch version 2.6.0 available.
W0410 04:04:02.530500 1906778 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:04:03.184903 1906778 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:04:03.189233 1904084 quantize_finetune_llama.py:225] layer 10 gpu 0
I0410 04:04:05.344907 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 10 in 0.8709428310394287s
I0410 04:04:09.290213 1907019 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:04:09.290323 1907019 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:04:09.290366 1907019 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:04:09.666139 1907019 config.py:54] PyTorch version 2.6.0 available.
W0410 04:04:09.883587 1907019 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:04:10.517503 1907019 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:04:10.521808 1904084 quantize_finetune_llama.py:225] layer 11 gpu 0
I0410 04:04:13.268739 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 11 in 1.223188877105713s
I0410 04:04:17.329077 1907219 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:04:17.329190 1907219 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:04:17.329237 1907219 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:04:17.714653 1907219 config.py:54] PyTorch version 2.6.0 available.
W0410 04:04:17.932221 1907219 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:04:18.518617 1907219 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:04:18.522363 1904084 quantize_finetune_llama.py:225] layer 12 gpu 0
I0410 04:04:20.887978 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 12 in 1.225705623626709s
I0410 04:04:25.031635 1907393 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:04:25.031748 1907393 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:04:25.031789 1907393 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:04:25.441863 1907393 config.py:54] PyTorch version 2.6.0 available.
W0410 04:04:25.660443 1907393 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:04:26.355991 1907393 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:04:26.360292 1904084 quantize_finetune_llama.py:225] layer 13 gpu 0
I0410 04:04:28.877478 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 13 in 0.987738847732544s
I0410 04:04:33.264533 1907562 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:04:33.264654 1907562 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:04:33.264696 1907562 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:04:33.723260 1907562 config.py:54] PyTorch version 2.6.0 available.
W0410 04:04:33.946244 1907562 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:04:34.662058 1907562 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:04:34.666290 1904084 quantize_finetune_llama.py:225] layer 14 gpu 0
I0410 04:04:38.022148 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 14 in 1.4327781200408936s
I0410 04:04:41.967232 1907704 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:04:41.967357 1907704 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:04:41.967398 1907704 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:04:42.400473 1907704 config.py:54] PyTorch version 2.6.0 available.
W0410 04:04:42.626802 1907704 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:04:43.350332 1907704 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:04:43.354660 1904084 quantize_finetune_llama.py:225] layer 15 gpu 0
I0410 04:04:46.363639 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 15 in 1.1886672973632812s
I0410 04:04:50.413993 1907916 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:04:50.414111 1907916 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:04:50.414154 1907916 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:04:50.746574 1907916 config.py:54] PyTorch version 2.6.0 available.
W0410 04:04:50.962885 1907916 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:04:51.549715 1907916 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:04:51.553929 1904084 quantize_finetune_llama.py:225] layer 16 gpu 0
I0410 04:04:54.266153 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 16 in 1.209259271621704s
I0410 04:04:58.399040 1908098 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:04:58.399227 1908098 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:04:58.399270 1908098 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:04:58.839670 1908098 config.py:54] PyTorch version 2.6.0 available.
W0410 04:04:59.065557 1908098 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:04:59.701200 1908098 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:04:59.705488 1904084 quantize_finetune_llama.py:225] layer 17 gpu 0
I0410 04:05:01.982915 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 17 in 0.6681938171386719s
I0410 04:05:05.957518 1908245 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:05:05.957703 1908245 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:05:05.957747 1908245 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:05:06.413465 1908245 config.py:54] PyTorch version 2.6.0 available.
W0410 04:05:06.641779 1908245 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:05:07.377659 1908245 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:05:07.382445 1904084 quantize_finetune_llama.py:225] layer 18 gpu 0
I0410 04:05:10.168609 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 18 in 0.8668074607849121s
I0410 04:05:14.381541 1908454 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:05:14.381675 1908454 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:05:14.381715 1908454 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:05:14.745651 1908454 config.py:54] PyTorch version 2.6.0 available.
W0410 04:05:14.963302 1908454 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:05:15.638843 1908454 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:05:15.643453 1904084 quantize_finetune_llama.py:225] layer 19 gpu 0
I0410 04:05:19.154914 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 19 in 1.2610766887664795s
I0410 04:05:23.462470 1908601 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:05:23.462579 1908601 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:05:23.462620 1908601 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:05:23.880317 1908601 config.py:54] PyTorch version 2.6.0 available.
W0410 04:05:24.090603 1908601 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:05:25.020596 1908601 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:05:25.024291 1904084 quantize_finetune_llama.py:225] layer 20 gpu 0
I0410 04:05:27.942773 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 20 in 1.3753838539123535s
I0410 04:05:32.184892 1908756 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:05:32.185050 1908756 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:05:32.185092 1908756 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:05:32.618506 1908756 config.py:54] PyTorch version 2.6.0 available.
W0410 04:05:32.845299 1908756 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:05:33.528331 1908756 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:05:33.532557 1904084 quantize_finetune_llama.py:225] layer 21 gpu 0
I0410 04:05:36.353757 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 21 in 1.1105523109436035s
I0410 04:05:40.390110 1908964 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:05:40.390219 1908964 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:05:40.390260 1908964 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:05:40.799617 1908964 config.py:54] PyTorch version 2.6.0 available.
W0410 04:05:41.024317 1908964 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:05:41.693129 1908964 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:05:41.697551 1904084 quantize_finetune_llama.py:225] layer 22 gpu 0
I0410 04:05:44.671112 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 22 in 1.2651925086975098s
I0410 04:05:48.603381 1909171 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:05:48.603479 1909171 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:05:48.603518 1909171 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:05:48.981209 1909171 config.py:54] PyTorch version 2.6.0 available.
W0410 04:05:49.175600 1909171 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:05:49.800153 1909171 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:05:49.804148 1904084 quantize_finetune_llama.py:225] layer 23 gpu 0
I0410 04:05:52.738479 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 23 in 1.2537777423858643s
I0410 04:05:57.257625 1909364 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:05:57.257770 1909364 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:05:57.257815 1909364 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:05:57.640950 1909364 config.py:54] PyTorch version 2.6.0 available.
W0410 04:05:57.829620 1909364 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:05:58.466146 1909364 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:05:58.470365 1904084 quantize_finetune_llama.py:225] layer 24 gpu 0
I0410 04:06:01.828644 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 24 in 1.6599054336547852s
I0410 04:06:06.729497 1909517 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:06:06.729606 1909517 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:06:06.729648 1909517 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:06:07.169599 1909517 config.py:54] PyTorch version 2.6.0 available.
W0410 04:06:07.392258 1909517 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:06:08.111737 1909517 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:06:08.116008 1904084 quantize_finetune_llama.py:225] layer 25 gpu 0
I0410 04:06:10.736274 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 25 in 0.793402910232544s
I0410 04:06:14.990542 1909696 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:06:14.990712 1909696 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:06:14.990771 1909696 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:06:15.442122 1909696 config.py:54] PyTorch version 2.6.0 available.
W0410 04:06:15.677455 1909696 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:06:16.389904 1909696 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:06:16.394368 1904084 quantize_finetune_llama.py:225] layer 26 gpu 0
I0410 04:06:18.802888 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 26 in 0.4215965270996094s
I0410 04:06:22.606750 1909899 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:06:22.606896 1909899 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:06:22.606938 1909899 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:06:22.987380 1909899 config.py:54] PyTorch version 2.6.0 available.
W0410 04:06:23.194041 1909899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:06:23.882344 1909899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:06:23.886719 1904084 quantize_finetune_llama.py:225] layer 27 gpu 0
I0410 04:06:26.402639 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 27 in 0.8766543865203857s
I0410 04:06:30.626482 1910103 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:06:30.626639 1910103 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:06:30.626691 1910103 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:06:31.025764 1910103 config.py:54] PyTorch version 2.6.0 available.
W0410 04:06:31.237318 1910103 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:06:31.881189 1910103 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:06:31.885534 1904084 quantize_finetune_llama.py:225] layer 28 gpu 0
I0410 04:06:34.391346 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 28 in 0.8720712661743164s
I0410 04:06:38.458855 1910252 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:06:38.458965 1910252 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:06:38.459006 1910252 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:06:38.879270 1910252 config.py:54] PyTorch version 2.6.0 available.
W0410 04:06:39.105052 1910252 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:06:39.785843 1910252 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:06:39.790113 1904084 quantize_finetune_llama.py:225] layer 29 gpu 0
I0410 04:06:42.764894 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 29 in 1.043748140335083s
I0410 04:06:46.840204 1910447 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:06:46.840353 1910447 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:06:46.840398 1910447 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:06:47.244371 1910447 config.py:54] PyTorch version 2.6.0 available.
W0410 04:06:47.461346 1910447 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:06:48.249847 1910447 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:06:48.254585 1904084 quantize_finetune_llama.py:225] layer 30 gpu 0
I0410 04:06:48.288563 1910447 data_utils.py:336] using 256 training seqs, 128 validation seqs
29_v proxy err 0.04038940370082855 err 34.34832000732422 tr(WHW.T) 850.4290161132812
bpp_loss 2.432438611984253
29_q proxy err 0.00819135271012783 err 169.1778564453125 tr(WHW.T) 20653.2265625
bpp_loss 2.5883288383483887
29_k proxy err 0.0028894436545670033 err 47.288047790527344 tr(WHW.T) 16365.796875
bpp_loss 3.347841262817383
29_o proxy err 0.03689985349774361 err 114.4945297241211 tr(WHW.T) 3102.844970703125
bpp_loss 2.2262706756591797
29_up proxy err 0.04356471449136734 err 560.61279296875 tr(WHW.T) 12868.505859375
bpp_loss 2.1915831565856934
29_gate proxy err 0.015813833102583885 err 606.6893920898438 tr(WHW.T) 38364.47265625
bpp_loss 2.4631212779453824
29_down proxy err 0.05378328636288643 err 401.78936767578125 tr(WHW.T) 7470.5244140625
bpp_loss 2.15710381099156
I0410 04:07:32.244996 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 30 in 1.2297770977020264s
I0410 04:07:36.356736 1911182 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:07:36.356898 1911182 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:07:36.356940 1911182 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:07:36.839231 1911182 config.py:54] PyTorch version 2.6.0 available.
W0410 04:07:37.076586 1911182 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:07:37.866668 1911182 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:07:37.871279 1904084 quantize_finetune_llama.py:225] layer 31 gpu 0
I0410 04:07:37.910666 1911182 data_utils.py:336] using 256 training seqs, 128 validation seqs
30_v proxy err 0.03803471848368645 err 32.82627487182617 tr(WHW.T) 863.060791015625
bpp_loss 2.6812050342559814
30_q proxy err 0.006086442619562149 err 146.38279724121094 tr(WHW.T) 24050.6328125
bpp_loss 2.4956194162368774
30_k proxy err 0.0030166655778884888 err 42.32559585571289 tr(WHW.T) 14030.5888671875
bpp_loss 3.0635828971862793
30_o proxy err 0.02919192984700203 err 142.14060974121094 tr(WHW.T) 4869.1748046875
bpp_loss 2.306312322616577
30_up proxy err 0.0277758426964283 err 603.0467529296875 tr(WHW.T) 21711.1953125
bpp_loss 2.221611704145159
30_gate proxy err 0.012374626472592354 err 642.9169311523438 tr(WHW.T) 51954.453125
bpp_loss 2.5071450642177036
30_down proxy err 0.03827453404664993 err 337.2054748535156 tr(WHW.T) 8810.1787109375
bpp_loss 2.1559010573795865
I0410 04:08:21.218048 1904084 quantize_finetune_llama.py:256] computed original embedding for layer 31 in 1.2801268100738525s
I0410 04:08:25.520163 1911919 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:08:25.520276 1911919 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:08:25.520317 1911919 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:08:25.961806 1911919 config.py:54] PyTorch version 2.6.0 available.
W0410 04:08:26.189642 1911919 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0410 04:08:26.910468 1911919 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0410 04:08:26.936477 1911919 data_utils.py:336] using 256 training seqs, 128 validation seqs
31_v proxy err 0.021987291052937508 err 39.76893997192383 tr(WHW.T) 1808.723876953125
bpp_loss 2.52354097366333
31_q proxy err 0.004923222120851278 err 227.33302307128906 tr(WHW.T) 46175.66015625
bpp_loss 2.6325968503952026
31_k proxy err 0.0031459301244467497 err 64.38977813720703 tr(WHW.T) 20467.64453125
bpp_loss 3.2608284950256348
31_o proxy err 0.03415093198418617 err 75.59587097167969 tr(WHW.T) 2213.58154296875
bpp_loss 2.26132333278656
31_up proxy err 0.01108135748654604 err 764.8291625976562 tr(WHW.T) 69019.4453125
bpp_loss 2.39838136945452
31_gate proxy err 0.0058469404466450214 err 844.6055297851562 tr(WHW.T) 144452.5625
bpp_loss 2.6971520015171597
31_down proxy err 0.024376971647143364 err 242.7992401123047 tr(WHW.T) 9960.1884765625
bpp_loss 2.175667013440813
I0410 04:09:13.714713 1912668 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:09:13.714866 1912668 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:09:13.714908 1912668 utils.py:162] NumExpr defaulting to 16 threads.
I0410 04:09:14.028489 1912668 config.py:54] PyTorch version 2.6.0 available.
W0410 04:09:14.237521 1912668 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0410 04:09:14.344522 1912668 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.99it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.44it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.54it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.57it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.54it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.57it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.72it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.59it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.59it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.77it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.79it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.93it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.84it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.97it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.89it/s]
I0410 04:09:17.547184 1912668 hfize_llama.py:161] loaded layer 0
I0410 04:09:18.517856 1912668 hfize_llama.py:161] loaded layer 1
I0410 04:09:19.431679 1912668 hfize_llama.py:161] loaded layer 2
I0410 04:09:20.390854 1912668 hfize_llama.py:161] loaded layer 3
I0410 04:09:21.389569 1912668 hfize_llama.py:161] loaded layer 4
I0410 04:09:22.358338 1912668 hfize_llama.py:161] loaded layer 5
I0410 04:09:23.317681 1912668 hfize_llama.py:161] loaded layer 6
I0410 04:09:24.205179 1912668 hfize_llama.py:161] loaded layer 7
I0410 04:09:25.107723 1912668 hfize_llama.py:161] loaded layer 8
I0410 04:09:26.081506 1912668 hfize_llama.py:161] loaded layer 9
I0410 04:09:27.004586 1912668 hfize_llama.py:161] loaded layer 10
I0410 04:09:27.946511 1912668 hfize_llama.py:161] loaded layer 11
I0410 04:09:28.894554 1912668 hfize_llama.py:161] loaded layer 12
I0410 04:09:29.801979 1912668 hfize_llama.py:161] loaded layer 13
I0410 04:09:30.614207 1912668 hfize_llama.py:161] loaded layer 14
I0410 04:09:31.424010 1912668 hfize_llama.py:161] loaded layer 15
I0410 04:09:32.248586 1912668 hfize_llama.py:161] loaded layer 16
I0410 04:09:33.229242 1912668 hfize_llama.py:161] loaded layer 17
I0410 04:09:34.151404 1912668 hfize_llama.py:161] loaded layer 18
I0410 04:09:35.023264 1912668 hfize_llama.py:161] loaded layer 19
I0410 04:09:35.910437 1912668 hfize_llama.py:161] loaded layer 20
I0410 04:09:36.776505 1912668 hfize_llama.py:161] loaded layer 21
I0410 04:09:37.681867 1912668 hfize_llama.py:161] loaded layer 22
I0410 04:09:38.484036 1912668 hfize_llama.py:161] loaded layer 23
I0410 04:09:39.384553 1912668 hfize_llama.py:161] loaded layer 24
I0410 04:09:40.329972 1912668 hfize_llama.py:161] loaded layer 25
I0410 04:09:41.132156 1912668 hfize_llama.py:161] loaded layer 26
I0410 04:09:41.977762 1912668 hfize_llama.py:161] loaded layer 27
I0410 04:09:42.805243 1912668 hfize_llama.py:161] loaded layer 28
I0410 04:09:43.695661 1912668 hfize_llama.py:161] loaded layer 29
I0410 04:09:44.639152 1912668 hfize_llama.py:161] loaded layer 30
I0410 04:09:45.616650 1912668 hfize_llama.py:161] loaded layer 31
I0410 04:09:45.616794 1912668 hfize_llama.py:165] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.25s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.11s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.07s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.06s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.02s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.04it/s]
I0410 04:10:32.725134 1912668 hfize_llama.py:175] successfully loaded hfized model
I0410 04:10:37.432558 1913940 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0410 04:10:37.432707 1913940 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0410 04:10:37.432749 1913940 utils.py:162] NumExpr defaulting to 16 threads.
W0410 04:10:37.787256 1913940 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0410 04:10:38.183566 1913940 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.23s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.18s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.24s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:05<00:03,  1.28s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.24s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.14s/it]
I0410 04:10:46.259448 1913940 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 2.3211774826049805:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 2.3211774826049805:   1%|          | 1/141 [00:01<04:26,  1.90s/it]avg_loss = 2.601560115814209:   1%|          | 1/141 [00:03<04:26,  1.90s/it] avg_loss = 2.601560115814209:   1%|▏         | 2/141 [00:03<03:51,  1.66s/it]avg_loss = 2.7487712701161704:   1%|▏         | 2/141 [00:04<03:51,  1.66s/it]avg_loss = 2.7487712701161704:   2%|▏         | 3/141 [00:04<03:39,  1.59s/it]avg_loss = 2.7118149399757385:   2%|▏         | 3/141 [00:06<03:39,  1.59s/it]avg_loss = 2.7118149399757385:   3%|▎         | 4/141 [00:06<03:33,  1.55s/it]avg_loss = 2.683794641494751:   3%|▎         | 4/141 [00:07<03:33,  1.55s/it] avg_loss = 2.683794641494751:   4%|▎         | 5/141 [00:07<03:28,  1.54s/it]avg_loss = 2.598796844482422:   4%|▎         | 5/141 [00:09<03:28,  1.54s/it]avg_loss = 2.598796844482422:   4%|▍         | 6/141 [00:09<03:26,  1.53s/it]avg_loss = 2.5510322707039967:   4%|▍         | 6/141 [00:10<03:26,  1.53s/it]avg_loss = 2.5510322707039967:   5%|▍         | 7/141 [00:10<03:24,  1.52s/it]avg_loss = 2.547765791416168:   5%|▍         | 7/141 [00:12<03:24,  1.52s/it] avg_loss = 2.547765791416168:   6%|▌         | 8/141 [00:12<03:22,  1.52s/it]avg_loss = 2.5963948832617865:   6%|▌         | 8/141 [00:13<03:22,  1.52s/it]avg_loss = 2.5963948832617865:   6%|▋         | 9/141 [00:13<03:21,  1.52s/it]avg_loss = 2.586861824989319:   6%|▋         | 9/141 [00:15<03:21,  1.52s/it] avg_loss = 2.586861824989319:   7%|▋         | 10/141 [00:15<03:19,  1.53s/it]avg_loss = 2.5774509473280474:   7%|▋         | 10/141 [00:17<03:19,  1.53s/it]avg_loss = 2.5774509473280474:   8%|▊         | 11/141 [00:17<03:18,  1.53s/it]avg_loss = 2.5981288154919944:   8%|▊         | 11/141 [00:18<03:18,  1.53s/it]avg_loss = 2.5981288154919944:   9%|▊         | 12/141 [00:18<03:17,  1.53s/it]avg_loss = 2.610184596135066:   9%|▊         | 12/141 [00:20<03:17,  1.53s/it] avg_loss = 2.610184596135066:   9%|▉         | 13/141 [00:20<03:16,  1.53s/it]avg_loss = 2.6281621796744212:   9%|▉         | 13/141 [00:21<03:16,  1.53s/it]avg_loss = 2.6281621796744212:  10%|▉         | 14/141 [00:21<03:15,  1.54s/it]avg_loss = 2.635528262456258:  10%|▉         | 14/141 [00:23<03:15,  1.54s/it] avg_loss = 2.635528262456258:  11%|█         | 15/141 [00:23<03:14,  1.54s/it]avg_loss = 2.6554005295038223:  11%|█         | 15/141 [00:24<03:14,  1.54s/it]avg_loss = 2.6554005295038223:  11%|█▏        | 16/141 [00:24<03:13,  1.55s/it]avg_loss = 2.6568126958959244:  11%|█▏        | 16/141 [00:26<03:13,  1.55s/it]avg_loss = 2.6568126958959244:  12%|█▏        | 17/141 [00:26<03:12,  1.55s/it]avg_loss = 2.658415847354465:  12%|█▏        | 17/141 [00:27<03:12,  1.55s/it] avg_loss = 2.658415847354465:  13%|█▎        | 18/141 [00:27<03:10,  1.55s/it]avg_loss = 2.6455387692702446:  13%|█▎        | 18/141 [00:29<03:10,  1.55s/it]avg_loss = 2.6455387692702446:  13%|█▎        | 19/141 [00:29<03:09,  1.56s/it]avg_loss = 2.6461766600608825:  13%|█▎        | 19/141 [00:31<03:09,  1.56s/it]avg_loss = 2.6461766600608825:  14%|█▍        | 20/141 [00:31<03:08,  1.56s/it]avg_loss = 2.6515723183041526:  14%|█▍        | 20/141 [00:32<03:08,  1.56s/it]avg_loss = 2.6515723183041526:  15%|█▍        | 21/141 [00:32<03:07,  1.57s/it]avg_loss = 2.652173649181019:  15%|█▍        | 21/141 [00:34<03:07,  1.57s/it] avg_loss = 2.652173649181019:  16%|█▌        | 22/141 [00:34<03:06,  1.57s/it]avg_loss = 2.655421319215194:  16%|█▌        | 22/141 [00:35<03:06,  1.57s/it]avg_loss = 2.655421319215194:  16%|█▋        | 23/141 [00:35<03:05,  1.57s/it]avg_loss = 2.6587165196736655:  16%|█▋        | 23/141 [00:37<03:05,  1.57s/it]avg_loss = 2.6587165196736655:  17%|█▋        | 24/141 [00:37<03:04,  1.58s/it]avg_loss = 2.6653348636627197:  17%|█▋        | 24/141 [00:38<03:04,  1.58s/it]avg_loss = 2.6653348636627197:  18%|█▊        | 25/141 [00:38<03:03,  1.58s/it]avg_loss = 2.673753784253047:  18%|█▊        | 25/141 [00:40<03:03,  1.58s/it] avg_loss = 2.673753784253047:  18%|█▊        | 26/141 [00:40<03:01,  1.58s/it]avg_loss = 2.685496197806464:  18%|█▊        | 26/141 [00:42<03:01,  1.58s/it]avg_loss = 2.685496197806464:  19%|█▉        | 27/141 [00:42<03:00,  1.59s/it]avg_loss = 2.6894865461758206:  19%|█▉        | 27/141 [00:43<03:00,  1.59s/it]avg_loss = 2.6894865461758206:  20%|█▉        | 28/141 [00:43<02:59,  1.59s/it]avg_loss = 2.685888306847934:  20%|█▉        | 28/141 [00:45<02:59,  1.59s/it] avg_loss = 2.685888306847934:  21%|██        | 29/141 [00:45<02:58,  1.59s/it]avg_loss = 2.675490721066793:  21%|██        | 29/141 [00:46<02:58,  1.59s/it]avg_loss = 2.675490721066793:  21%|██▏       | 30/141 [00:46<02:57,  1.59s/it]avg_loss = 2.6668908134583504:  21%|██▏       | 30/141 [00:48<02:57,  1.59s/it]avg_loss = 2.6668908134583504:  22%|██▏       | 31/141 [00:48<02:55,  1.60s/it]avg_loss = 2.6561426371335983:  22%|██▏       | 31/141 [00:50<02:55,  1.60s/it]avg_loss = 2.6561426371335983:  23%|██▎       | 32/141 [00:50<02:54,  1.60s/it]avg_loss = 2.654658996697628:  23%|██▎       | 32/141 [00:51<02:54,  1.60s/it] avg_loss = 2.654658996697628:  23%|██▎       | 33/141 [00:51<02:53,  1.60s/it]avg_loss = 2.652130330310148:  23%|██▎       | 33/141 [00:53<02:53,  1.60s/it]avg_loss = 2.652130330310148:  24%|██▍       | 34/141 [00:53<02:51,  1.61s/it]avg_loss = 2.657561295373099:  24%|██▍       | 34/141 [00:54<02:51,  1.61s/it]avg_loss = 2.657561295373099:  25%|██▍       | 35/141 [00:54<02:50,  1.61s/it]avg_loss = 2.640282008383009:  25%|██▍       | 35/141 [00:56<02:50,  1.61s/it]avg_loss = 2.640282008383009:  26%|██▌       | 36/141 [00:56<02:49,  1.61s/it]avg_loss = 2.627742702896531:  26%|██▌       | 36/141 [00:58<02:49,  1.61s/it]avg_loss = 2.627742702896531:  26%|██▌       | 37/141 [00:58<02:47,  1.61s/it]avg_loss = 2.613312294608668:  26%|██▌       | 37/141 [00:59<02:47,  1.61s/it]avg_loss = 2.613312294608668:  27%|██▋       | 38/141 [00:59<02:46,  1.62s/it]avg_loss = 2.599391374832545:  27%|██▋       | 38/141 [01:01<02:46,  1.62s/it]avg_loss = 2.599391374832545:  28%|██▊       | 39/141 [01:01<02:45,  1.62s/it]avg_loss = 2.5912981390953065:  28%|██▊       | 39/141 [01:03<02:45,  1.62s/it]avg_loss = 2.5912981390953065:  28%|██▊       | 40/141 [01:03<02:43,  1.62s/it]avg_loss = 2.597216024631407:  28%|██▊       | 40/141 [01:04<02:43,  1.62s/it] avg_loss = 2.597216024631407:  29%|██▉       | 41/141 [01:04<02:42,  1.63s/it]avg_loss = 2.612505521093096:  29%|██▉       | 41/141 [01:06<02:42,  1.63s/it]avg_loss = 2.612505521093096:  30%|██▉       | 42/141 [01:06<02:41,  1.63s/it]avg_loss = 2.6275941992914955:  30%|██▉       | 42/141 [01:07<02:41,  1.63s/it]avg_loss = 2.6275941992914955:  30%|███       | 43/141 [01:07<02:39,  1.63s/it]avg_loss = 2.6360506036064844:  30%|███       | 43/141 [01:09<02:39,  1.63s/it]avg_loss = 2.6360506036064844:  31%|███       | 44/141 [01:09<02:38,  1.63s/it]avg_loss = 2.642512231402927:  31%|███       | 44/141 [01:11<02:38,  1.63s/it] avg_loss = 2.642512231402927:  32%|███▏      | 45/141 [01:11<02:36,  1.63s/it]avg_loss = 2.6459645447523696:  32%|███▏      | 45/141 [01:12<02:36,  1.63s/it]avg_loss = 2.6459645447523696:  33%|███▎      | 46/141 [01:12<02:35,  1.64s/it]avg_loss = 2.6515291650244532:  33%|███▎      | 46/141 [01:14<02:35,  1.64s/it]avg_loss = 2.6515291650244532:  33%|███▎      | 47/141 [01:14<02:33,  1.64s/it]avg_loss = 2.6527037620544434:  33%|███▎      | 47/141 [01:16<02:33,  1.64s/it]avg_loss = 2.6527037620544434:  34%|███▍      | 48/141 [01:16<02:32,  1.64s/it]avg_loss = 2.6505551386852653:  34%|███▍      | 48/141 [01:17<02:32,  1.64s/it]avg_loss = 2.6505551386852653:  35%|███▍      | 49/141 [01:17<02:31,  1.64s/it]avg_loss = 2.648185930252075:  35%|███▍      | 49/141 [01:19<02:31,  1.64s/it] avg_loss = 2.648185930252075:  35%|███▌      | 50/141 [01:19<02:29,  1.64s/it]avg_loss = 2.6418295420852362:  35%|███▌      | 50/141 [01:21<02:29,  1.64s/it]avg_loss = 2.6418295420852362:  36%|███▌      | 51/141 [01:21<02:28,  1.65s/it]avg_loss = 2.6363866604291477:  36%|███▌      | 51/141 [01:22<02:28,  1.65s/it]avg_loss = 2.6363866604291477:  37%|███▋      | 52/141 [01:22<02:26,  1.65s/it]avg_loss = 2.630510109775471:  37%|███▋      | 52/141 [01:24<02:26,  1.65s/it] avg_loss = 2.630510109775471:  38%|███▊      | 53/141 [01:24<02:25,  1.65s/it]avg_loss = 2.62647897667355:  38%|███▊      | 53/141 [01:26<02:25,  1.65s/it] avg_loss = 2.62647897667355:  38%|███▊      | 54/141 [01:26<02:23,  1.65s/it]avg_loss = 2.6171174482865767:  38%|███▊      | 54/141 [01:27<02:23,  1.65s/it]avg_loss = 2.6171174482865767:  39%|███▉      | 55/141 [01:27<02:22,  1.65s/it]avg_loss = 2.6087635031768253:  39%|███▉      | 55/141 [01:29<02:22,  1.65s/it]avg_loss = 2.6087635031768253:  40%|███▉      | 56/141 [01:29<02:20,  1.66s/it]avg_loss = 2.607560007195724:  40%|███▉      | 56/141 [01:31<02:20,  1.66s/it] avg_loss = 2.607560007195724:  40%|████      | 57/141 [01:31<02:19,  1.66s/it]avg_loss = 2.6043677782190255:  40%|████      | 57/141 [01:32<02:19,  1.66s/it]avg_loss = 2.6043677782190255:  41%|████      | 58/141 [01:32<02:17,  1.66s/it]avg_loss = 2.6062945147692145:  41%|████      | 58/141 [01:34<02:17,  1.66s/it]avg_loss = 2.6062945147692145:  42%|████▏     | 59/141 [01:34<02:16,  1.66s/it]avg_loss = 2.6109333395957948:  42%|████▏     | 59/141 [01:36<02:16,  1.66s/it]avg_loss = 2.6109333395957948:  43%|████▎     | 60/141 [01:36<02:14,  1.66s/it]avg_loss = 2.6147054062515007:  43%|████▎     | 60/141 [01:37<02:14,  1.66s/it]avg_loss = 2.6147054062515007:  43%|████▎     | 61/141 [01:37<02:13,  1.66s/it]avg_loss = 2.6220479819082443:  43%|████▎     | 61/141 [01:39<02:13,  1.66s/it]avg_loss = 2.6220479819082443:  44%|████▍     | 62/141 [01:39<02:11,  1.67s/it]avg_loss = 2.615076148320758:  44%|████▍     | 62/141 [01:41<02:11,  1.67s/it] avg_loss = 2.615076148320758:  45%|████▍     | 63/141 [01:41<02:09,  1.67s/it]avg_loss = 2.612928219139576:  45%|████▍     | 63/141 [01:42<02:09,  1.67s/it]avg_loss = 2.612928219139576:  45%|████▌     | 64/141 [01:42<02:08,  1.67s/it]avg_loss = 2.6114648048694318:  45%|████▌     | 64/141 [01:44<02:08,  1.67s/it]avg_loss = 2.6114648048694318:  46%|████▌     | 65/141 [01:44<02:06,  1.67s/it]avg_loss = 2.605988115975351:  46%|████▌     | 65/141 [01:46<02:06,  1.67s/it] avg_loss = 2.605988115975351:  47%|████▋     | 66/141 [01:46<02:05,  1.67s/it]avg_loss = 2.602399466642693:  47%|████▋     | 66/141 [01:47<02:05,  1.67s/it]avg_loss = 2.602399466642693:  48%|████▊     | 67/141 [01:47<02:03,  1.67s/it]avg_loss = 2.6019273049691143:  48%|████▊     | 67/141 [01:49<02:03,  1.67s/it]avg_loss = 2.6019273049691143:  48%|████▊     | 68/141 [01:49<02:02,  1.67s/it]avg_loss = 2.600440211918043:  48%|████▊     | 68/141 [01:51<02:02,  1.67s/it] avg_loss = 2.600440211918043:  49%|████▉     | 69/141 [01:51<02:00,  1.67s/it]avg_loss = 2.6023193155016218:  49%|████▉     | 69/141 [01:52<02:00,  1.67s/it]avg_loss = 2.6023193155016218:  50%|████▉     | 70/141 [01:52<01:58,  1.68s/it]avg_loss = 2.6069663108234673:  50%|████▉     | 70/141 [01:54<01:58,  1.68s/it]avg_loss = 2.6069663108234673:  50%|█████     | 71/141 [01:54<01:57,  1.68s/it]avg_loss = 2.6097461614343853:  50%|█████     | 71/141 [01:56<01:57,  1.68s/it]avg_loss = 2.6097461614343853:  51%|█████     | 72/141 [01:56<01:55,  1.68s/it]avg_loss = 2.607088996939463:  51%|█████     | 72/141 [01:57<01:55,  1.68s/it] avg_loss = 2.607088996939463:  52%|█████▏    | 73/141 [01:57<01:54,  1.68s/it]avg_loss = 2.607376559360607:  52%|█████▏    | 73/141 [01:59<01:54,  1.68s/it]avg_loss = 2.607376559360607:  52%|█████▏    | 74/141 [01:59<01:52,  1.68s/it]avg_loss = 2.6072284921010334:  52%|█████▏    | 74/141 [02:01<01:52,  1.68s/it]avg_loss = 2.6072284921010334:  53%|█████▎    | 75/141 [02:01<01:50,  1.68s/it]avg_loss = 2.6054499400289437:  53%|█████▎    | 75/141 [02:02<01:50,  1.68s/it]avg_loss = 2.6054499400289437:  54%|█████▍    | 76/141 [02:02<01:49,  1.68s/it]avg_loss = 2.6059217019514604:  54%|█████▍    | 76/141 [02:04<01:49,  1.68s/it]avg_loss = 2.6059217019514604:  55%|█████▍    | 77/141 [02:04<01:47,  1.68s/it]avg_loss = 2.607672263414432:  55%|█████▍    | 77/141 [02:06<01:47,  1.68s/it] avg_loss = 2.607672263414432:  55%|█████▌    | 78/141 [02:06<01:46,  1.68s/it]avg_loss = 2.610335021079341:  55%|█████▌    | 78/141 [02:07<01:46,  1.68s/it]avg_loss = 2.610335021079341:  56%|█████▌    | 79/141 [02:07<01:44,  1.69s/it]avg_loss = 2.604480516910553:  56%|█████▌    | 79/141 [02:09<01:44,  1.69s/it]avg_loss = 2.604480516910553:  57%|█████▋    | 80/141 [02:09<01:42,  1.69s/it]avg_loss = 2.6019052899914024:  57%|█████▋    | 80/141 [02:11<01:42,  1.69s/it]avg_loss = 2.6019052899914024:  57%|█████▋    | 81/141 [02:11<01:41,  1.69s/it]avg_loss = 2.600187243484869:  57%|█████▋    | 81/141 [02:12<01:41,  1.69s/it] avg_loss = 2.600187243484869:  58%|█████▊    | 82/141 [02:12<01:39,  1.69s/it]avg_loss = 2.5976616244718254:  58%|█████▊    | 82/141 [02:14<01:39,  1.69s/it]avg_loss = 2.5976616244718254:  59%|█████▉    | 83/141 [02:14<01:37,  1.69s/it]avg_loss = 2.5956348975499473:  59%|█████▉    | 83/141 [02:16<01:37,  1.69s/it]avg_loss = 2.5956348975499473:  60%|█████▉    | 84/141 [02:16<01:36,  1.69s/it]avg_loss = 2.5940686562482047:  60%|█████▉    | 84/141 [02:18<01:36,  1.69s/it]avg_loss = 2.5940686562482047:  60%|██████    | 85/141 [02:18<01:34,  1.69s/it]avg_loss = 2.595545383386834:  60%|██████    | 85/141 [02:19<01:34,  1.69s/it] avg_loss = 2.595545383386834:  61%|██████    | 86/141 [02:19<01:32,  1.69s/it]avg_loss = 2.596606419004243:  61%|██████    | 86/141 [02:21<01:32,  1.69s/it]avg_loss = 2.596606419004243:  62%|██████▏   | 87/141 [02:21<01:31,  1.69s/it]avg_loss = 2.5986380875110626:  62%|██████▏   | 87/141 [02:23<01:31,  1.69s/it]avg_loss = 2.5986380875110626:  62%|██████▏   | 88/141 [02:23<01:29,  1.69s/it]avg_loss = 2.607496116938216:  62%|██████▏   | 88/141 [02:24<01:29,  1.69s/it] avg_loss = 2.607496116938216:  63%|██████▎   | 89/141 [02:24<01:27,  1.69s/it]avg_loss = 2.615140928162469:  63%|██████▎   | 89/141 [02:26<01:27,  1.69s/it]avg_loss = 2.615140928162469:  64%|██████▍   | 90/141 [02:26<01:26,  1.69s/it]avg_loss = 2.618174529337621:  64%|██████▍   | 90/141 [02:28<01:26,  1.69s/it]avg_loss = 2.618174529337621:  65%|██████▍   | 91/141 [02:28<01:24,  1.69s/it]avg_loss = 2.6238694553789883:  65%|██████▍   | 91/141 [02:29<01:24,  1.69s/it]avg_loss = 2.6238694553789883:  65%|██████▌   | 92/141 [02:29<01:23,  1.70s/it]avg_loss = 2.6288856767839:  65%|██████▌   | 92/141 [02:31<01:23,  1.70s/it]   avg_loss = 2.6288856767839:  66%|██████▌   | 93/141 [02:31<01:21,  1.70s/it]avg_loss = 2.6289691620684685:  66%|██████▌   | 93/141 [02:33<01:21,  1.70s/it]avg_loss = 2.6289691620684685:  67%|██████▋   | 94/141 [02:33<01:19,  1.70s/it]avg_loss = 2.6330662200325414:  67%|██████▋   | 94/141 [02:35<01:19,  1.70s/it]avg_loss = 2.6330662200325414:  67%|██████▋   | 95/141 [02:35<01:18,  1.70s/it]avg_loss = 2.633247584104538:  67%|██████▋   | 95/141 [02:36<01:18,  1.70s/it] avg_loss = 2.633247584104538:  68%|██████▊   | 96/141 [02:36<01:16,  1.70s/it]avg_loss = 2.6346203518896987:  68%|██████▊   | 96/141 [02:38<01:16,  1.70s/it]avg_loss = 2.6346203518896987:  69%|██████▉   | 97/141 [02:38<01:14,  1.70s/it]avg_loss = 2.633321180635569:  69%|██████▉   | 97/141 [02:40<01:14,  1.70s/it] avg_loss = 2.633321180635569:  70%|██████▉   | 98/141 [02:40<01:12,  1.70s/it]avg_loss = 2.6348272044249255:  70%|██████▉   | 98/141 [02:41<01:12,  1.70s/it]avg_loss = 2.6348272044249255:  70%|███████   | 99/141 [02:41<01:11,  1.70s/it]avg_loss = 2.6375044775009155:  70%|███████   | 99/141 [02:43<01:11,  1.70s/it]avg_loss = 2.6375044775009155:  71%|███████   | 100/141 [02:43<01:09,  1.70s/it]avg_loss = 2.6378007902957425:  71%|███████   | 100/141 [02:45<01:09,  1.70s/it]avg_loss = 2.6378007902957425:  72%|███████▏  | 101/141 [02:45<01:07,  1.70s/it]avg_loss = 2.639115756633235:  72%|███████▏  | 101/141 [02:46<01:07,  1.70s/it] avg_loss = 2.639115756633235:  72%|███████▏  | 102/141 [02:46<01:06,  1.70s/it]avg_loss = 2.639988061293815:  72%|███████▏  | 102/141 [02:48<01:06,  1.70s/it]avg_loss = 2.639988061293815:  73%|███████▎  | 103/141 [02:48<01:04,  1.70s/it]avg_loss = 2.6442736226778765:  73%|███████▎  | 103/141 [02:50<01:04,  1.70s/it]avg_loss = 2.6442736226778765:  74%|███████▍  | 104/141 [02:50<01:02,  1.70s/it]avg_loss = 2.6438762551262265:  74%|███████▍  | 104/141 [02:52<01:02,  1.70s/it]avg_loss = 2.6438762551262265:  74%|███████▍  | 105/141 [02:52<01:01,  1.70s/it]avg_loss = 2.643788711080011:  74%|███████▍  | 105/141 [02:53<01:01,  1.70s/it] avg_loss = 2.643788711080011:  75%|███████▌  | 106/141 [02:53<00:59,  1.70s/it]avg_loss = 2.6425150577153:  75%|███████▌  | 106/141 [02:55<00:59,  1.70s/it]  avg_loss = 2.6425150577153:  76%|███████▌  | 107/141 [02:55<00:57,  1.70s/it]avg_loss = 2.640410811812789:  76%|███████▌  | 107/141 [02:57<00:57,  1.70s/it]avg_loss = 2.640410811812789:  77%|███████▋  | 108/141 [02:57<00:56,  1.70s/it]avg_loss = 2.639642829195075:  77%|███████▋  | 108/141 [02:58<00:56,  1.70s/it]avg_loss = 2.639642829195075:  77%|███████▋  | 109/141 [02:58<00:54,  1.70s/it]avg_loss = 2.6365146095102485:  77%|███████▋  | 109/141 [03:00<00:54,  1.70s/it]avg_loss = 2.6365146095102485:  78%|███████▊  | 110/141 [03:00<00:52,  1.70s/it]avg_loss = 2.639375821964161:  78%|███████▊  | 110/141 [03:02<00:52,  1.70s/it] avg_loss = 2.639375821964161:  79%|███████▊  | 111/141 [03:02<00:51,  1.70s/it]avg_loss = 2.639481355037008:  79%|███████▊  | 111/141 [03:03<00:51,  1.70s/it]avg_loss = 2.639481355037008:  79%|███████▉  | 112/141 [03:03<00:49,  1.70s/it]avg_loss = 2.6406659784570206:  79%|███████▉  | 112/141 [03:05<00:49,  1.70s/it]avg_loss = 2.6406659784570206:  80%|████████  | 113/141 [03:05<00:47,  1.70s/it]avg_loss = 2.6428131785309104:  80%|████████  | 113/141 [03:07<00:47,  1.70s/it]avg_loss = 2.6428131785309104:  81%|████████  | 114/141 [03:07<00:45,  1.70s/it]avg_loss = 2.6411353588104247:  81%|████████  | 114/141 [03:09<00:45,  1.70s/it]avg_loss = 2.6411353588104247:  82%|████████▏ | 115/141 [03:09<00:44,  1.70s/it]avg_loss = 2.639126348084417:  82%|████████▏ | 115/141 [03:10<00:44,  1.70s/it] avg_loss = 2.639126348084417:  82%|████████▏ | 116/141 [03:10<00:42,  1.70s/it]avg_loss = 2.641121299857767:  82%|████████▏ | 116/141 [03:12<00:42,  1.70s/it]avg_loss = 2.641121299857767:  83%|████████▎ | 117/141 [03:12<00:40,  1.70s/it]avg_loss = 2.639503757832414:  83%|████████▎ | 117/141 [03:14<00:40,  1.70s/it]avg_loss = 2.639503757832414:  84%|████████▎ | 118/141 [03:14<00:39,  1.70s/it]avg_loss = 2.637411908943112:  84%|████████▎ | 118/141 [03:15<00:39,  1.70s/it]avg_loss = 2.637411908943112:  84%|████████▍ | 119/141 [03:15<00:37,  1.70s/it]avg_loss = 2.634664696455002:  84%|████████▍ | 119/141 [03:17<00:37,  1.70s/it]avg_loss = 2.634664696455002:  85%|████████▌ | 120/141 [03:17<00:35,  1.70s/it]avg_loss = 2.634959445511999:  85%|████████▌ | 120/141 [03:19<00:35,  1.70s/it]avg_loss = 2.634959445511999:  86%|████████▌ | 121/141 [03:19<00:34,  1.70s/it]avg_loss = 2.6351401610452623:  86%|████████▌ | 121/141 [03:20<00:34,  1.70s/it]avg_loss = 2.6351401610452623:  87%|████████▋ | 122/141 [03:20<00:32,  1.71s/it]avg_loss = 2.6337359998284318:  87%|████████▋ | 122/141 [03:22<00:32,  1.71s/it]avg_loss = 2.6337359998284318:  87%|████████▋ | 123/141 [03:22<00:30,  1.71s/it]avg_loss = 2.6337220668792725:  87%|████████▋ | 123/141 [03:24<00:30,  1.71s/it]avg_loss = 2.6337220668792725:  88%|████████▊ | 124/141 [03:24<00:28,  1.71s/it]avg_loss = 2.632136173248291:  88%|████████▊ | 124/141 [03:26<00:28,  1.71s/it] avg_loss = 2.632136173248291:  89%|████████▊ | 125/141 [03:26<00:27,  1.70s/it]avg_loss = 2.631711757372296:  89%|████████▊ | 125/141 [03:27<00:27,  1.70s/it]avg_loss = 2.631711757372296:  89%|████████▉ | 126/141 [03:27<00:25,  1.71s/it]avg_loss = 2.6314007635191667:  89%|████████▉ | 126/141 [03:29<00:25,  1.71s/it]avg_loss = 2.6314007635191667:  90%|█████████ | 127/141 [03:29<00:23,  1.71s/it]avg_loss = 2.6297719310969114:  90%|█████████ | 127/141 [03:31<00:23,  1.71s/it]avg_loss = 2.6297719310969114:  91%|█████████ | 128/141 [03:31<00:22,  1.71s/it]avg_loss = 2.629747976628385:  91%|█████████ | 128/141 [03:32<00:22,  1.71s/it] avg_loss = 2.629747976628385:  91%|█████████▏| 129/141 [03:32<00:20,  1.71s/it]avg_loss = 2.630961302610544:  91%|█████████▏| 129/141 [03:34<00:20,  1.71s/it]avg_loss = 2.630961302610544:  92%|█████████▏| 130/141 [03:34<00:18,  1.71s/it]avg_loss = 2.631698901416691:  92%|█████████▏| 130/141 [03:36<00:18,  1.71s/it]avg_loss = 2.631698901416691:  93%|█████████▎| 131/141 [03:36<00:17,  1.71s/it]avg_loss = 2.6321170167489485:  93%|█████████▎| 131/141 [03:38<00:17,  1.71s/it]avg_loss = 2.6321170167489485:  94%|█████████▎| 132/141 [03:38<00:15,  1.71s/it]avg_loss = 2.6282226895927487:  94%|█████████▎| 132/141 [03:39<00:15,  1.71s/it]avg_loss = 2.6282226895927487:  94%|█████████▍| 133/141 [03:39<00:13,  1.71s/it]avg_loss = 2.6222656733954133:  94%|█████████▍| 133/141 [03:41<00:13,  1.71s/it]avg_loss = 2.6222656733954133:  95%|█████████▌| 134/141 [03:41<00:11,  1.71s/it]avg_loss = 2.6246212005615233:  95%|█████████▌| 134/141 [03:43<00:11,  1.71s/it]avg_loss = 2.6246212005615233:  96%|█████████▌| 135/141 [03:43<00:10,  1.71s/it]avg_loss = 2.628505247480729:  96%|█████████▌| 135/141 [03:44<00:10,  1.71s/it] avg_loss = 2.628505247480729:  96%|█████████▋| 136/141 [03:44<00:08,  1.71s/it]avg_loss = 2.6307911942475033:  96%|█████████▋| 136/141 [03:46<00:08,  1.71s/it]avg_loss = 2.6307911942475033:  97%|█████████▋| 137/141 [03:46<00:06,  1.71s/it]avg_loss = 2.6305172046025596:  97%|█████████▋| 137/141 [03:48<00:06,  1.71s/it]avg_loss = 2.6305172046025596:  98%|█████████▊| 138/141 [03:48<00:05,  1.71s/it]avg_loss = 2.631613336878715:  98%|█████████▊| 138/141 [03:50<00:05,  1.71s/it] avg_loss = 2.631613336878715:  99%|█████████▊| 139/141 [03:50<00:03,  1.71s/it]avg_loss = 2.633126667567662:  99%|█████████▊| 139/141 [03:51<00:03,  1.71s/it]avg_loss = 2.633126667567662:  99%|█████████▉| 140/141 [03:51<00:01,  1.71s/it]avg_loss = 2.635133533613056:  99%|█████████▉| 140/141 [03:53<00:01,  1.71s/it]avg_loss = 2.635133533613056: 100%|██████████| 141/141 [03:53<00:00,  1.71s/it]avg_loss = 2.635133533613056: 100%|██████████| 141/141 [03:53<00:00,  1.66s/it]
I0410 04:15:04.778753 1913940 eval_ppl.py:107] wikitext2 perplexity: 13.945174217224121
wikitext2 perplexity: 13.945
