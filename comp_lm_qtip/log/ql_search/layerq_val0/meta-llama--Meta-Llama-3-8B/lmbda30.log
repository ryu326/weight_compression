I0409 07:24:35.250693 1063414 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:24:35.250792 1063414 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:24:35.250833 1063414 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:24:35.576148 1063414 config.py:54] PyTorch version 2.6.0 available.
W0409 07:24:35.765099 1063414 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:24:36.417739 1063414 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.29it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.67it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.85it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.95it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.01it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.99it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.16it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.99it/s]
I0409 07:24:37.865090 1063414 quantize_finetune_llama.py:160] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:20,  1.49it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:20,  1.47it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:19,  1.50it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:16,  1.65it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:15,  1.75it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:14,  1.80it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:13,  1.86it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.88it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:05<00:12,  1.91it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.92it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:10,  1.93it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.95it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:09,  1.94it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.95it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  1.95it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.96it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.98it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.99it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.98it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.97it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  2.00it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:04,  2.07it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  2.10it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:03,  2.15it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:12<00:03,  2.18it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:02,  2.20it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:13<00:02,  2.24it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:01,  2.25it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:14<00:01,  2.26it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:00,  2.27it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:15<00:00,  2.25it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  2.25it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  2.00it/s]
I0409 07:25:01.298736 1063414 quantize_finetune_llama.py:198] loaded compression model
I0409 07:25:19.392581 1063414 quantize_finetune_llama.py:202] loaded dataset and devset
I0409 07:25:21.692990 1063414 quantize_finetune_llama.py:222] layer 0 gpu 0
I0409 07:25:24.587424 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 0 in 2.7370946407318115s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0409 07:25:34.706208 1064739 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:25:34.706305 1064739 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:25:34.706345 1064739 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:25:35.028794 1064739 config.py:54] PyTorch version 2.6.0 available.
W0409 07:25:35.217269 1064739 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:25:35.804445 1064739 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:25:35.808476 1063414 quantize_finetune_llama.py:222] layer 1 gpu 0
I0409 07:25:35.821596 1064739 data_utils.py:336] using 256 training seqs, 128 validation seqs
0_v proxy err 0.20893007516860962 err 12.721094131469727 tr(WHW.T) 60.88684844970703
bpp_loss 1.6846774220466614
0_q proxy err 0.0014231099048629403 err 409.9903564453125 tr(WHW.T) 288094.65625
bpp_loss 2.2643431425094604
0_k proxy err 0.00036382683902047575 err 36.44844055175781 tr(WHW.T) 100180.734375
bpp_loss 2.9445894956588745
0_o proxy err 0.029903067275881767 err 93.39378356933594 tr(WHW.T) 3123.217529296875
bpp_loss 1.8001657128334045
0_up proxy err 0.060492753982543945 err 539.8646240234375 tr(WHW.T) 8924.451171875
bpp_loss 2.0922208513532365
0_gate proxy err 0.03555373474955559 err 560.990478515625 tr(WHW.T) 15778.666015625
bpp_loss 2.1976406233651296
0_down proxy err 0.04266523942351341 err 461.5613098144531 tr(WHW.T) 10818.205078125
bpp_loss 2.086120912006923
I0409 07:26:12.648278 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 1 in 0.6961987018585205s
I0409 07:26:16.366662 1065708 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:26:16.366763 1065708 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:26:16.366801 1065708 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:26:16.686876 1065708 config.py:54] PyTorch version 2.6.0 available.
W0409 07:26:16.892562 1065708 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:26:17.455702 1065708 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:26:17.459369 1063414 quantize_finetune_llama.py:222] layer 2 gpu 0
I0409 07:26:17.473741 1065708 data_utils.py:336] using 256 training seqs, 128 validation seqs
1_v proxy err 0.14767533540725708 err 16.10708999633789 tr(WHW.T) 109.07096099853516
bpp_loss 1.8151487112045288
1_q proxy err 0.003083807649090886 err 446.6170349121094 tr(WHW.T) 144826.484375
bpp_loss 2.5806236267089844
1_k proxy err 0.00039947102777659893 err 30.175661087036133 tr(WHW.T) 75539.046875
bpp_loss 3.3235710859298706
1_o proxy err 0.060156140476465225 err 119.33968353271484 tr(WHW.T) 1983.8321533203125
bpp_loss 1.8821290731430054
1_up proxy err 0.06633812934160233 err 546.0172119140625 tr(WHW.T) 8230.8203125
bpp_loss 2.1065331867762973
1_gate proxy err 0.040670640766620636 err 567.45166015625 tr(WHW.T) 13952.365234375
bpp_loss 2.208733218056815
1_down proxy err 0.0020061370451003313 err 28.041845321655273 tr(WHW.T) 13978.03125
bpp_loss 2.10089533669608
I0409 07:26:56.369923 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 2 in 0.8909146785736084s
I0409 07:27:00.304353 1066585 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:27:00.304456 1066585 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:27:00.304497 1066585 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:27:00.634233 1066585 config.py:54] PyTorch version 2.6.0 available.
W0409 07:27:00.844620 1066585 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:27:01.436340 1066585 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:27:01.440158 1063414 quantize_finetune_llama.py:222] layer 3 gpu 0
I0409 07:27:01.453274 1066585 data_utils.py:336] using 256 training seqs, 128 validation seqs
2_v proxy err 0.14844438433647156 err 23.15131187438965 tr(WHW.T) 155.95950317382812
bpp_loss 1.71983003616333
2_q proxy err 0.007023538928478956 err 291.3023681640625 tr(WHW.T) 41475.15625
bpp_loss 2.566656708717346
2_k proxy err 0.0017732592532411218 err 40.1014289855957 tr(WHW.T) 22614.533203125
bpp_loss 3.4244394302368164
2_o proxy err 0.05404764413833618 err 106.32915496826172 tr(WHW.T) 1967.3226318359375
bpp_loss 1.8369559049606323
2_up proxy err 0.07307139039039612 err 555.3772583007812 tr(WHW.T) 7600.474609375
bpp_loss 2.0987893513270786
2_gate proxy err 0.03851804882287979 err 582.1354370117188 tr(WHW.T) 15113.31640625
bpp_loss 2.2398565156119212
2_down proxy err 0.06431978195905685 err 496.9812927246094 tr(WHW.T) 7726.72509765625
bpp_loss 2.1033995832715715
I0409 07:27:38.463666 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 3 in 0.9651217460632324s
I0409 07:27:42.433768 1067523 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:27:42.433867 1067523 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:27:42.433907 1067523 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:27:42.765955 1067523 config.py:54] PyTorch version 2.6.0 available.
W0409 07:27:42.952284 1067523 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:27:43.487246 1067523 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:27:43.490914 1063414 quantize_finetune_llama.py:222] layer 4 gpu 0
I0409 07:27:43.503778 1067523 data_utils.py:336] using 256 training seqs, 128 validation seqs
3_v proxy err 0.10121957212686539 err 29.286178588867188 tr(WHW.T) 289.3331604003906
bpp_loss 1.8263331055641174
3_q proxy err 0.0060132574290037155 err 286.0923156738281 tr(WHW.T) 47576.9296875
bpp_loss 2.600569009780884
3_k proxy err 0.0017719612224027514 err 46.380226135253906 tr(WHW.T) 26174.515625
bpp_loss 3.497908353805542
3_o proxy err 0.07010843604803085 err 130.20242309570312 tr(WHW.T) 1857.15771484375
bpp_loss 1.94123375415802
3_up proxy err 0.07272423803806305 err 548.0836791992188 tr(WHW.T) 7536.46484375
bpp_loss 2.0815252576555525
3_gate proxy err 0.028355702757835388 err 592.1903686523438 tr(WHW.T) 20884.34765625
bpp_loss 2.309978485107422
3_down proxy err 0.0737491324543953 err 516.1414184570312 tr(WHW.T) 6998.6103515625
bpp_loss 2.081653050013951
I0409 07:28:23.164776 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 4 in 0.949016809463501s
I0409 07:28:27.631290 1068473 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:28:27.631392 1068473 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:28:27.631434 1068473 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:28:27.982167 1068473 config.py:54] PyTorch version 2.6.0 available.
W0409 07:28:28.187162 1068473 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:28:28.735939 1068473 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:28:28.740615 1063414 quantize_finetune_llama.py:222] layer 5 gpu 0
I0409 07:28:28.808400 1068473 data_utils.py:336] using 256 training seqs, 128 validation seqs
4_v proxy err 0.09478600323200226 err 27.043123245239258 tr(WHW.T) 285.30712890625
bpp_loss 1.8702280521392822
4_q proxy err 0.0057528032921254635 err 288.3009338378906 tr(WHW.T) 50114.859375
bpp_loss 2.5705560445785522
4_k proxy err 0.0014453609474003315 err 42.3519287109375 tr(WHW.T) 29301.974609375
bpp_loss 3.4828044176101685
4_o proxy err 0.09127691388130188 err 118.4088134765625 tr(WHW.T) 1297.2481689453125
bpp_loss 1.9493895173072815
4_up proxy err 0.07220988720655441 err 533.1608276367188 tr(WHW.T) 7383.48779296875
bpp_loss 2.054471901484898
4_gate proxy err 0.02028506062924862 err 590.923583984375 tr(WHW.T) 29130.974609375
bpp_loss 2.3770216533115933
4_down proxy err 0.0827142596244812 err 529.0174560546875 tr(WHW.T) 6395.72265625
bpp_loss 2.0577762808118547
I0409 07:29:11.978713 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 5 in 0.9956231117248535s
I0409 07:29:16.016626 1069434 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:29:16.016744 1069434 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:29:16.016785 1069434 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:29:16.401841 1069434 config.py:54] PyTorch version 2.6.0 available.
W0409 07:29:16.618552 1069434 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:29:17.281000 1069434 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:29:17.285341 1063414 quantize_finetune_llama.py:222] layer 6 gpu 0
I0409 07:29:17.304613 1069434 data_utils.py:336] using 256 training seqs, 128 validation seqs
5_v proxy err 0.12709644436836243 err 26.540266036987305 tr(WHW.T) 208.81988525390625
bpp_loss 1.7567432522773743
5_q proxy err 0.007401908282190561 err 266.4067077636719 tr(WHW.T) 35991.625
bpp_loss 2.553543448448181
5_k proxy err 0.001828203909099102 err 42.04458236694336 tr(WHW.T) 22997.75390625
bpp_loss 3.4552531242370605
5_o proxy err 0.10626985132694244 err 112.1770248413086 tr(WHW.T) 1055.5865478515625
bpp_loss 1.896180808544159
5_up proxy err 0.0692036971449852 err 529.8534545898438 tr(WHW.T) 7656.43310546875
bpp_loss 2.0598603657313754
5_gate proxy err 0.019317856058478355 err 587.0106811523438 tr(WHW.T) 30386.947265625
bpp_loss 2.3799495697021484
5_down proxy err 0.0811571255326271 err 520.308837890625 tr(WHW.T) 6411.12939453125
bpp_loss 2.0622627053942
I0409 07:29:56.128557 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 6 in 1.0002686977386475s
I0409 07:30:00.106068 1070225 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:30:00.106182 1070225 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:30:00.106225 1070225 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:30:00.513010 1070225 config.py:54] PyTorch version 2.6.0 available.
W0409 07:30:00.751084 1070225 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:30:01.434786 1070225 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:30:01.439177 1063414 quantize_finetune_llama.py:222] layer 7 gpu 0
I0409 07:30:01.472431 1070225 data_utils.py:336] using 256 training seqs, 128 validation seqs
6_v proxy err 0.10847195237874985 err 27.512149810791016 tr(WHW.T) 253.63377380371094
bpp_loss 1.8044180870056152
6_q proxy err 0.007539276499301195 err 268.6966857910156 tr(WHW.T) 35639.5859375
bpp_loss 2.5968090295791626
6_k proxy err 0.0016935544554144144 err 44.309574127197266 tr(WHW.T) 26163.654296875
bpp_loss 3.5229077339172363
6_o proxy err 0.12149108201265335 err 122.60253143310547 tr(WHW.T) 1009.1483764648438
bpp_loss 1.9305211901664734
6_up proxy err 0.06576671451330185 err 521.1049194335938 tr(WHW.T) 7923.53564453125
bpp_loss 2.0591941561017717
6_gate proxy err 0.016202248632907867 err 579.204345703125 tr(WHW.T) 35748.39453125
bpp_loss 2.3841202599661693
6_down proxy err 0.08064228296279907 err 522.8490600585938 tr(WHW.T) 6483.5595703125
bpp_loss 2.063042095729283
I0409 07:30:40.089529 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 7 in 0.939507246017456s
I0409 07:30:44.147042 1071056 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:30:44.147171 1071056 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:30:44.147212 1071056 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:30:44.560593 1071056 config.py:54] PyTorch version 2.6.0 available.
W0409 07:30:44.788027 1071056 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:30:45.457696 1071056 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:30:45.461971 1063414 quantize_finetune_llama.py:222] layer 8 gpu 0
I0409 07:30:45.477189 1071056 data_utils.py:336] using 256 training seqs, 128 validation seqs
7_v proxy err 0.09107766300439835 err 28.181896209716797 tr(WHW.T) 309.4270935058594
bpp_loss 1.8025281429290771
7_q proxy err 0.007765089627355337 err 272.9037170410156 tr(WHW.T) 35144.953125
bpp_loss 2.534558415412903
7_k proxy err 0.0017545598093420267 err 47.11079406738281 tr(WHW.T) 26850.4921875
bpp_loss 3.5417085886001587
7_o proxy err 0.1235940232872963 err 118.55330657958984 tr(WHW.T) 959.2155151367188
bpp_loss 1.9409180879592896
7_up proxy err 0.05993662029504776 err 517.1002197265625 tr(WHW.T) 8627.4501953125
bpp_loss 2.0713579995291576
7_gate proxy err 0.01628594659268856 err 568.4371337890625 tr(WHW.T) 34903.53515625
bpp_loss 2.356522423880441
7_down proxy err 0.08096864819526672 err 529.0999755859375 tr(WHW.T) 6534.6279296875
bpp_loss 2.0766972473689487
I0409 07:31:23.727185 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 8 in 1.0967319011688232s
I0409 07:31:27.843173 1071906 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:31:27.843302 1071906 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:31:27.843351 1071906 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:31:28.277749 1071906 config.py:54] PyTorch version 2.6.0 available.
W0409 07:31:28.514006 1071906 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:31:29.190515 1071906 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:31:29.195008 1063414 quantize_finetune_llama.py:222] layer 9 gpu 0
I0409 07:31:29.211409 1071906 data_utils.py:336] using 256 training seqs, 128 validation seqs
8_v proxy err 0.10259221494197845 err 26.43855094909668 tr(WHW.T) 257.7052307128906
bpp_loss 1.821737289428711
8_q proxy err 0.009206604212522507 err 244.9046630859375 tr(WHW.T) 26600.9765625
bpp_loss 2.522487163543701
8_k proxy err 0.0017942605772987008 err 40.42244338989258 tr(WHW.T) 22528.748046875
bpp_loss 3.461038112640381
8_o proxy err 0.16101223230361938 err 120.05170440673828 tr(WHW.T) 745.6061401367188
bpp_loss 1.9507015347480774
8_up proxy err 0.06059904769062996 err 514.792236328125 tr(WHW.T) 8495.0546875
bpp_loss 2.06728458404541
8_gate proxy err 0.015206586569547653 err 566.26513671875 tr(WHW.T) 37238.1484375
bpp_loss 2.360641751970564
8_down proxy err 0.0825096145272255 err 535.3898315429688 tr(WHW.T) 6488.8173828125
bpp_loss 2.074810368674142
I0409 07:32:07.358412 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 9 in 0.8405213356018066s
I0409 07:32:11.467970 1072715 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:32:11.468095 1072715 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:32:11.468137 1072715 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:32:11.881351 1072715 config.py:54] PyTorch version 2.6.0 available.
W0409 07:32:12.107504 1072715 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:32:12.776058 1072715 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:32:12.780620 1063414 quantize_finetune_llama.py:222] layer 10 gpu 0
I0409 07:32:12.808567 1072715 data_utils.py:336] using 256 training seqs, 128 validation seqs
9_v proxy err 0.07706370204687119 err 27.08240509033203 tr(WHW.T) 351.4288024902344
bpp_loss 1.9205366373062134
9_q proxy err 0.00944523699581623 err 242.3008575439453 tr(WHW.T) 25653.232421875
bpp_loss 2.5293338298797607
9_k proxy err 0.0019091354915872216 err 39.98233413696289 tr(WHW.T) 20942.638671875
bpp_loss 3.4791146516799927
9_o proxy err 0.15230780839920044 err 118.28250885009766 tr(WHW.T) 776.6017456054688
bpp_loss 2.0042662024497986
9_up proxy err 0.05780329927802086 err 518.5328979492188 tr(WHW.T) 8970.6455078125
bpp_loss 2.0763224193028043
9_gate proxy err 0.014508519321680069 err 571.8637084960938 tr(WHW.T) 39415.71875
bpp_loss 2.373028482709612
9_down proxy err 0.08360347896814346 err 524.4365844726562 tr(WHW.T) 6272.90380859375
bpp_loss 2.075998749051775
I0409 07:32:50.518056 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 10 in 0.8081622123718262s
I0409 07:32:54.590090 1073532 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:32:54.590213 1073532 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:32:54.590256 1073532 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:32:55.001875 1073532 config.py:54] PyTorch version 2.6.0 available.
W0409 07:32:55.229580 1073532 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:32:55.895208 1073532 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:32:55.899672 1063414 quantize_finetune_llama.py:222] layer 11 gpu 0
I0409 07:32:55.928144 1073532 data_utils.py:336] using 256 training seqs, 128 validation seqs
10_v proxy err 0.10172607749700546 err 25.61858367919922 tr(WHW.T) 251.83889770507812
bpp_loss 1.810444176197052
10_q proxy err 0.01026415079832077 err 239.54635620117188 tr(WHW.T) 23338.15625
bpp_loss 2.5343544483184814
10_k proxy err 0.0019838062580674887 err 39.16818618774414 tr(WHW.T) 19743.95703125
bpp_loss 3.477612257003784
10_o proxy err 0.16973312199115753 err 115.53123474121094 tr(WHW.T) 680.6640625
bpp_loss 1.9433330297470093
10_up proxy err 0.057028885930776596 err 524.61767578125 tr(WHW.T) 9199.1572265625
bpp_loss 2.093191010611398
10_gate proxy err 0.015318753197789192 err 572.9117431640625 tr(WHW.T) 37399.37109375
bpp_loss 2.345354897635324
10_down proxy err 0.08101324737071991 err 528.63037109375 tr(WHW.T) 6525.2333984375
bpp_loss 2.0923798084259033
I0409 07:33:33.947959 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 11 in 0.9072849750518799s
I0409 07:33:38.083384 1074394 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:33:38.083507 1074394 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:33:38.083548 1074394 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:33:38.483394 1074394 config.py:54] PyTorch version 2.6.0 available.
W0409 07:33:38.703365 1074394 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:33:39.347487 1074394 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:33:39.351794 1063414 quantize_finetune_llama.py:222] layer 12 gpu 0
I0409 07:33:39.394626 1074394 data_utils.py:336] using 256 training seqs, 128 validation seqs
11_v proxy err 0.08286979794502258 err 26.482627868652344 tr(WHW.T) 319.5691223144531
bpp_loss 1.820645034313202
11_q proxy err 0.010719923302531242 err 237.8847198486328 tr(WHW.T) 22190.8984375
bpp_loss 2.483526349067688
11_k proxy err 0.0022110752761363983 err 39.85999298095703 tr(WHW.T) 18027.423828125
bpp_loss 3.478442430496216
11_o proxy err 0.19481977820396423 err 110.25780487060547 tr(WHW.T) 565.9476928710938
bpp_loss 1.9625091552734375
11_up proxy err 0.05616606026887894 err 516.3118896484375 tr(WHW.T) 9192.595703125
bpp_loss 2.0973662648882185
11_gate proxy err 0.015218903310596943 err 561.33544921875 tr(WHW.T) 36884.09375
bpp_loss 2.3247263772147044
11_down proxy err 0.07806464284658432 err 519.5897216796875 tr(WHW.T) 6655.890625
bpp_loss 2.0993992260524204
I0409 07:34:17.098567 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 12 in 0.9269237518310547s
I0409 07:34:21.225835 1075255 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:34:21.225958 1075255 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:34:21.226006 1075255 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:34:21.627813 1075255 config.py:54] PyTorch version 2.6.0 available.
W0409 07:34:21.853463 1075255 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:34:22.562260 1075255 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:34:22.566641 1063414 quantize_finetune_llama.py:222] layer 13 gpu 0
I0409 07:34:22.584374 1075255 data_utils.py:336] using 256 training seqs, 128 validation seqs
12_v proxy err 0.07779178023338318 err 28.286911010742188 tr(WHW.T) 363.6233825683594
bpp_loss 1.9373193383216858
12_q proxy err 0.007533751893788576 err 256.92071533203125 tr(WHW.T) 34102.625
bpp_loss 2.532454490661621
12_k proxy err 0.0018920714501291513 err 43.62556838989258 tr(WHW.T) 23057.041015625
bpp_loss 3.481645107269287
12_o proxy err 0.15197059512138367 err 118.50231170654297 tr(WHW.T) 779.7713012695312
bpp_loss 2.0080376863479614
12_up proxy err 0.05058971419930458 err 507.27093505859375 tr(WHW.T) 10027.1552734375
bpp_loss 2.1150503158569336
12_gate proxy err 0.01468851137906313 err 548.3823852539062 tr(WHW.T) 37334.1015625
bpp_loss 2.3052428109305247
12_down proxy err 0.07515048235654831 err 512.7301025390625 tr(WHW.T) 6822.7119140625
bpp_loss 2.1109847341265
I0409 07:35:00.451896 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 13 in 0.8701024055480957s
I0409 07:35:04.570442 1076042 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:35:04.570564 1076042 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:35:04.570605 1076042 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:35:04.971362 1076042 config.py:54] PyTorch version 2.6.0 available.
W0409 07:35:05.188248 1076042 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:35:05.853315 1076042 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:35:05.857671 1063414 quantize_finetune_llama.py:222] layer 14 gpu 0
I0409 07:35:05.880491 1076042 data_utils.py:336] using 256 training seqs, 128 validation seqs
13_v proxy err 0.09372009336948395 err 26.256017684936523 tr(WHW.T) 280.153564453125
bpp_loss 1.8788566589355469
13_q proxy err 0.011087342165410519 err 231.69635009765625 tr(WHW.T) 20897.375
bpp_loss 2.511052370071411
13_k proxy err 0.0022301364224404097 err 39.69401168823242 tr(WHW.T) 17798.916015625
bpp_loss 3.490813732147217
13_o proxy err 0.1724107414484024 err 116.41291809082031 tr(WHW.T) 675.2068481445312
bpp_loss 1.9906858205795288
13_up proxy err 0.05077844858169556 err 508.306884765625 tr(WHW.T) 10010.2880859375
bpp_loss 2.118748732975551
13_gate proxy err 0.014263472519814968 err 555.15234375 tr(WHW.T) 38921.26171875
bpp_loss 2.3101159504481723
13_down proxy err 0.07777149975299835 err 510.22039794921875 tr(WHW.T) 6560.505859375
bpp_loss 2.110994509288243
I0409 07:35:43.893277 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 14 in 1.1450765132904053s
I0409 07:35:48.170602 1076851 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:35:48.170722 1076851 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:35:48.170764 1076851 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:35:48.582588 1076851 config.py:54] PyTorch version 2.6.0 available.
W0409 07:35:48.819484 1076851 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:35:49.500996 1076851 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:35:49.505365 1063414 quantize_finetune_llama.py:222] layer 15 gpu 0
I0409 07:35:49.520825 1076851 data_utils.py:336] using 256 training seqs, 128 validation seqs
14_v proxy err 0.09041432291269302 err 25.437009811401367 tr(WHW.T) 281.3382873535156
bpp_loss 1.8695977330207825
14_q proxy err 0.011507200077176094 err 240.2914581298828 tr(WHW.T) 20881.8359375
bpp_loss 2.4893449544906616
14_k proxy err 0.002122612902894616 err 39.52156448364258 tr(WHW.T) 18619.298828125
bpp_loss 3.4429142475128174
14_o proxy err 0.17652997374534607 err 121.15210723876953 tr(WHW.T) 686.2976684570312
bpp_loss 1.9824351072311401
14_up proxy err 0.05488113686442375 err 502.9465026855469 tr(WHW.T) 9164.287109375
bpp_loss 2.113440445491246
14_gate proxy err 0.01333985012024641 err 557.4165649414062 tr(WHW.T) 41785.8203125
bpp_loss 2.3380843571254184
14_down proxy err 0.07950685918331146 err 508.12164306640625 tr(WHW.T) 6390.916015625
bpp_loss 2.1071365560804094
I0409 07:36:27.183962 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 15 in 1.0999224185943604s
I0409 07:36:31.418817 1077694 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:36:31.418944 1077694 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:36:31.418985 1077694 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:36:31.837760 1077694 config.py:54] PyTorch version 2.6.0 available.
W0409 07:36:32.061941 1077694 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:36:32.685498 1077694 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:36:32.689803 1063414 quantize_finetune_llama.py:222] layer 16 gpu 0
I0409 07:36:32.705827 1077694 data_utils.py:336] using 256 training seqs, 128 validation seqs
15_v proxy err 0.09534316509962082 err 27.080045700073242 tr(WHW.T) 284.0271301269531
bpp_loss 1.9314613342285156
15_q proxy err 0.008488960564136505 err 238.3641357421875 tr(WHW.T) 28079.306640625
bpp_loss 2.5930100679397583
15_k proxy err 0.0021621123887598515 err 40.80266571044922 tr(WHW.T) 18871.66796875
bpp_loss 3.4900583028793335
15_o proxy err 0.1544114500284195 err 127.841552734375 tr(WHW.T) 827.9279174804688
bpp_loss 2.0120944380760193
15_up proxy err 0.056283194571733475 err 506.24310302734375 tr(WHW.T) 8994.5693359375
bpp_loss 2.106809275490897
15_gate proxy err 0.012307856231927872 err 568.8441162109375 tr(WHW.T) 46217.96875
bpp_loss 2.374043737139021
15_down proxy err 0.08053611218929291 err 515.3817138671875 tr(WHW.T) 6399.38671875
bpp_loss 2.1014466285705566
I0409 07:37:10.022111 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 16 in 1.0321452617645264s
I0409 07:37:14.320307 1078539 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:37:14.320416 1078539 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:37:14.320460 1078539 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:37:14.738446 1078539 config.py:54] PyTorch version 2.6.0 available.
W0409 07:37:14.967382 1078539 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:37:15.610260 1078539 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:37:15.614631 1063414 quantize_finetune_llama.py:222] layer 17 gpu 0
I0409 07:37:15.632291 1078539 data_utils.py:336] using 256 training seqs, 128 validation seqs
16_v proxy err 0.09080977737903595 err 24.90745735168457 tr(WHW.T) 274.28167724609375
bpp_loss 1.8905983567237854
16_q proxy err 0.009748678654432297 err 238.71243286132812 tr(WHW.T) 24486.64453125
bpp_loss 2.5790700912475586
16_k proxy err 0.0019433351699262857 err 37.90739059448242 tr(WHW.T) 19506.357421875
bpp_loss 3.4825265407562256
16_o proxy err 0.1294948011636734 err 125.49192810058594 tr(WHW.T) 969.0885620117188
bpp_loss 1.9945262670516968
16_up proxy err 0.06208747252821922 err 517.2620849609375 tr(WHW.T) 8331.1826171875
bpp_loss 2.094976084572928
16_gate proxy err 0.014172546565532684 err 583.5706176757812 tr(WHW.T) 41176.12890625
bpp_loss 2.4059881482805525
16_down proxy err 0.08059677481651306 err 506.81512451171875 tr(WHW.T) 6288.2802734375
bpp_loss 2.0880933148520335
I0409 07:37:52.925326 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 17 in 0.9370474815368652s
I0409 07:37:57.181988 1079328 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:37:57.182108 1079328 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:37:57.182152 1079328 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:37:57.586680 1079328 config.py:54] PyTorch version 2.6.0 available.
W0409 07:37:57.810238 1079328 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:37:58.467257 1079328 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:37:58.471792 1063414 quantize_finetune_llama.py:222] layer 18 gpu 0
I0409 07:37:58.488807 1079328 data_utils.py:336] using 256 training seqs, 128 validation seqs
17_v proxy err 0.09557513147592545 err 27.140758514404297 tr(WHW.T) 283.9730224609375
bpp_loss 1.9595608115196228
17_q proxy err 0.0085886986926198 err 236.80279541015625 tr(WHW.T) 27571.44140625
bpp_loss 2.586308002471924
17_k proxy err 0.0023155277594923973 err 40.35921859741211 tr(WHW.T) 17429.814453125
bpp_loss 3.5088021755218506
17_o proxy err 0.1184573695063591 err 131.09669494628906 tr(WHW.T) 1106.6993408203125
bpp_loss 2.021821141242981
17_up proxy err 0.061998043209314346 err 524.0410766601562 tr(WHW.T) 8452.5419921875
bpp_loss 2.093078817640032
17_gate proxy err 0.014259923249483109 err 594.8892211914062 tr(WHW.T) 41717.5625
bpp_loss 2.4192021233694896
17_down proxy err 0.08198830485343933 err 509.337158203125 tr(WHW.T) 6212.31494140625
bpp_loss 2.0840890066964284
I0409 07:38:36.860818 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 18 in 0.9461274147033691s
I0409 07:38:41.074987 1080181 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:38:41.075103 1080181 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:38:41.075146 1080181 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:38:41.478970 1080181 config.py:54] PyTorch version 2.6.0 available.
W0409 07:38:41.697950 1080181 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:38:42.356978 1080181 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:38:42.361500 1063414 quantize_finetune_llama.py:222] layer 19 gpu 0
I0409 07:38:42.377576 1080181 data_utils.py:336] using 256 training seqs, 128 validation seqs
18_v proxy err 0.0958719328045845 err 27.57408905029297 tr(WHW.T) 287.61376953125
bpp_loss 1.8853811025619507
18_q proxy err 0.010442161001265049 err 233.899169921875 tr(WHW.T) 22399.498046875
bpp_loss 2.5896358489990234
18_k proxy err 0.00238988408818841 err 41.57157897949219 tr(WHW.T) 17394.810546875
bpp_loss 3.5772221088409424
18_o proxy err 0.10538879781961441 err 126.8031234741211 tr(WHW.T) 1203.193603515625
bpp_loss 2.003133773803711
18_up proxy err 0.06688806414604187 err 533.9596557617188 tr(WHW.T) 7982.8837890625
bpp_loss 2.090266704559326
18_gate proxy err 0.017067285254597664 err 601.5117797851562 tr(WHW.T) 35243.5546875
bpp_loss 2.42780944279262
18_down proxy err 0.0811489149928093 err 505.28009033203125 tr(WHW.T) 6226.57861328125
bpp_loss 2.084256274359567
I0409 07:39:23.372982 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 19 in 0.7755687236785889s
I0409 07:39:27.603048 1081067 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:39:27.603171 1081067 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:39:27.603219 1081067 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:39:28.019294 1081067 config.py:54] PyTorch version 2.6.0 available.
W0409 07:39:28.238932 1081067 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:39:28.870805 1081067 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:39:28.874868 1063414 quantize_finetune_llama.py:222] layer 20 gpu 0
I0409 07:39:28.891849 1081067 data_utils.py:336] using 256 training seqs, 128 validation seqs
19_v proxy err 0.08361168950796127 err 28.51657485961914 tr(WHW.T) 341.0596618652344
bpp_loss 1.9303929805755615
19_q proxy err 0.009763840585947037 err 234.70355224609375 tr(WHW.T) 24038.03515625
bpp_loss 2.5884331464767456
19_k proxy err 0.002685388782992959 err 41.766624450683594 tr(WHW.T) 15553.287109375
bpp_loss 3.4956952333450317
19_o proxy err 0.10711506754159927 err 125.2059326171875 tr(WHW.T) 1168.891845703125
bpp_loss 2.0165030360221863
19_up proxy err 0.07013257592916489 err 536.4645385742188 tr(WHW.T) 7649.2919921875
bpp_loss 2.087205273764474
19_gate proxy err 0.018345942720770836 err 601.9669799804688 tr(WHW.T) 32811.99609375
bpp_loss 2.4384994506835938
19_down proxy err 0.08072417974472046 err 499.0962829589844 tr(WHW.T) 6182.73583984375
bpp_loss 2.082441943032401
I0409 07:40:10.289395 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 20 in 0.9894039630889893s
I0409 07:40:14.563735 1081940 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:40:14.563885 1081940 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:40:14.563928 1081940 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:40:14.985659 1081940 config.py:54] PyTorch version 2.6.0 available.
W0409 07:40:15.198194 1081940 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:40:15.817257 1081940 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:40:15.821432 1063414 quantize_finetune_llama.py:222] layer 21 gpu 0
I0409 07:40:15.836737 1081940 data_utils.py:336] using 256 training seqs, 128 validation seqs
20_v proxy err 0.08401723206043243 err 27.74182891845703 tr(WHW.T) 330.192138671875
bpp_loss 1.9643619060516357
20_q proxy err 0.011066731996834278 err 229.49903869628906 tr(WHW.T) 20737.7421875
bpp_loss 2.5648456811904907
20_k proxy err 0.0025848664809018373 err 39.79611587524414 tr(WHW.T) 15395.810546875
bpp_loss 3.456614851951599
20_o proxy err 0.10756637901067734 err 129.63070678710938 tr(WHW.T) 1205.1229248046875
bpp_loss 2.0033291578292847
20_up proxy err 0.07083013653755188 err 538.2848510742188 tr(WHW.T) 7599.658203125
bpp_loss 2.0910539627075195
20_gate proxy err 0.01965203694999218 err 602.6300048828125 tr(WHW.T) 30665.015625
bpp_loss 2.44095584324428
20_down proxy err 0.07892753183841705 err 497.3114929199219 tr(WHW.T) 6300.8623046875
bpp_loss 2.0867505414145335
I0409 07:40:57.551963 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 21 in 1.1573262214660645s
I0409 07:41:01.898687 1082760 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:41:01.898797 1082760 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:41:01.898838 1082760 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:41:02.332289 1082760 config.py:54] PyTorch version 2.6.0 available.
W0409 07:41:02.562854 1082760 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:41:03.266995 1082760 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:41:03.271196 1063414 quantize_finetune_llama.py:222] layer 22 gpu 0
I0409 07:41:03.286949 1082760 data_utils.py:336] using 256 training seqs, 128 validation seqs
21_v proxy err 0.07793718576431274 err 28.281309127807617 tr(WHW.T) 362.87310791015625
bpp_loss 1.9896823167800903
21_q proxy err 0.00911123026162386 err 235.41697692871094 tr(WHW.T) 25838.111328125
bpp_loss 2.5639781951904297
21_k proxy err 0.0024462142027914524 err 41.06570816040039 tr(WHW.T) 16787.453125
bpp_loss 3.4935141801834106
21_o proxy err 0.09094240516424179 err 115.19392395019531 tr(WHW.T) 1266.6689453125
bpp_loss 2.017763912677765
21_up proxy err 0.069117471575737 err 537.220458984375 tr(WHW.T) 7772.57080078125
bpp_loss 2.0938346726553783
21_gate proxy err 0.019107632339000702 err 603.1588134765625 tr(WHW.T) 31566.3828125
bpp_loss 2.451663153512137
21_down proxy err 0.0762886330485344 err 484.5445556640625 tr(WHW.T) 6351.46484375
bpp_loss 2.086880479540144
I0409 07:41:45.366936 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 22 in 1.2050533294677734s
I0409 07:41:49.682238 1083614 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:41:49.682409 1083614 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:41:49.682453 1083614 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:41:50.110628 1083614 config.py:54] PyTorch version 2.6.0 available.
W0409 07:41:50.326152 1083614 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:41:50.964287 1083614 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:41:50.968540 1063414 quantize_finetune_llama.py:222] layer 23 gpu 0
I0409 07:41:50.983184 1083614 data_utils.py:336] using 256 training seqs, 128 validation seqs
22_v proxy err 0.07929825782775879 err 27.443458557128906 tr(WHW.T) 346.0789489746094
bpp_loss 2.037992000579834
22_q proxy err 0.01103242114186287 err 224.17445373535156 tr(WHW.T) 20319.60546875
bpp_loss 2.532661199569702
22_k proxy err 0.0025742463767528534 err 37.9034538269043 tr(WHW.T) 14724.09765625
bpp_loss 3.4383957386016846
22_o proxy err 0.1085638627409935 err 132.6466827392578 tr(WHW.T) 1221.8309326171875
bpp_loss 2.0500916242599487
22_up proxy err 0.07153882086277008 err 539.9217529296875 tr(WHW.T) 7547.25537109375
bpp_loss 2.0981700079781667
22_gate proxy err 0.020435497164726257 err 603.5440063476562 tr(WHW.T) 29534.099609375
bpp_loss 2.4566078186035156
22_down proxy err 0.07555433362722397 err 493.33135986328125 tr(WHW.T) 6529.4912109375
bpp_loss 2.0930543627057756
I0409 07:42:33.309368 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 23 in 1.1294617652893066s
I0409 07:42:37.733688 1084503 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:42:37.733903 1084503 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:42:37.733985 1084503 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:42:38.143713 1084503 config.py:54] PyTorch version 2.6.0 available.
W0409 07:42:38.366814 1084503 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:42:39.039005 1084503 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:42:39.043252 1063414 quantize_finetune_llama.py:222] layer 24 gpu 0
I0409 07:42:39.058369 1084503 data_utils.py:336] using 256 training seqs, 128 validation seqs
23_v proxy err 0.07396911084651947 err 29.432830810546875 tr(WHW.T) 397.90704345703125
bpp_loss 2.0884429216384888
23_q proxy err 0.010151318274438381 err 229.51046752929688 tr(WHW.T) 22608.931640625
bpp_loss 2.5383851528167725
23_k proxy err 0.0026996906381100416 err 40.1105842590332 tr(WHW.T) 14857.474609375
bpp_loss 3.442221522331238
23_o proxy err 0.07778418064117432 err 135.69224548339844 tr(WHW.T) 1744.470947265625
bpp_loss 2.0730303525924683
23_up proxy err 0.07279420644044876 err 540.0753784179688 tr(WHW.T) 7419.2080078125
bpp_loss 2.102006503513881
23_gate proxy err 0.021959181874990463 err 599.3023681640625 tr(WHW.T) 27291.65234375
bpp_loss 2.4587597165788924
23_down proxy err 0.07353852689266205 err 490.2803039550781 tr(WHW.T) 6666.9853515625
bpp_loss 2.09845655305045
I0409 07:43:23.095323 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 24 in 1.2116165161132812s
I0409 07:43:27.234744 1085421 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:43:27.234856 1085421 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:43:27.234897 1085421 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:43:27.630841 1085421 config.py:54] PyTorch version 2.6.0 available.
W0409 07:43:27.846426 1085421 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:43:28.495430 1085421 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:43:28.499570 1063414 quantize_finetune_llama.py:222] layer 25 gpu 0
I0409 07:43:28.517239 1085421 data_utils.py:336] using 256 training seqs, 128 validation seqs
24_v proxy err 0.06275264918804169 err 29.322954177856445 tr(WHW.T) 467.2783508300781
bpp_loss 2.179842233657837
24_q proxy err 0.010092251002788544 err 226.4307403564453 tr(WHW.T) 22436.099609375
bpp_loss 2.508088231086731
24_k proxy err 0.002708723535761237 err 38.419002532958984 tr(WHW.T) 14183.4345703125
bpp_loss 3.2944973707199097
24_o proxy err 0.08100932091474533 err 128.763671875 tr(WHW.T) 1589.4920654296875
bpp_loss 2.111969828605652
24_up proxy err 0.07411013543605804 err 541.829833984375 tr(WHW.T) 7311.1435546875
bpp_loss 2.106416770390102
24_gate proxy err 0.02316562458872795 err 599.4629516601562 tr(WHW.T) 25877.26171875
bpp_loss 2.464186532156808
24_down proxy err 0.07167378067970276 err 483.2201232910156 tr(WHW.T) 6741.93701171875
bpp_loss 2.103865078517369
I0409 07:44:11.863675 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 25 in 0.9256296157836914s
I0409 07:44:16.063903 1086323 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:44:16.064059 1086323 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:44:16.064112 1086323 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:44:16.500429 1086323 config.py:54] PyTorch version 2.6.0 available.
W0409 07:44:16.732622 1086323 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:44:17.454493 1086323 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:44:17.459059 1063414 quantize_finetune_llama.py:222] layer 26 gpu 0
I0409 07:44:17.476105 1086323 data_utils.py:336] using 256 training seqs, 128 validation seqs
25_v proxy err 0.05426700413227081 err 30.270605087280273 tr(WHW.T) 557.8086547851562
bpp_loss 2.1910715103149414
25_q proxy err 0.008923789486289024 err 233.01824951171875 tr(WHW.T) 26112.029296875
bpp_loss 2.49079430103302
25_k proxy err 0.0027178791351616383 err 39.18263244628906 tr(WHW.T) 14416.6201171875
bpp_loss 3.274505138397217
25_o proxy err 0.0647057443857193 err 128.79556274414062 tr(WHW.T) 1990.481201171875
bpp_loss 2.109564185142517
25_up proxy err 0.07320376485586166 err 540.4041748046875 tr(WHW.T) 7382.19091796875
bpp_loss 2.1151533126831055
25_gate proxy err 0.022673042491078377 err 596.4696044921875 tr(WHW.T) 26307.435546875
bpp_loss 2.4720521654401506
25_down proxy err 0.07069605588912964 err 467.7216491699219 tr(WHW.T) 6615.951171875
bpp_loss 2.112436192376273
I0409 07:45:00.969702 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 26 in 0.8632242679595947s
I0409 07:45:05.178268 1087147 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:45:05.178425 1087147 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:45:05.178468 1087147 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:45:05.629093 1087147 config.py:54] PyTorch version 2.6.0 available.
W0409 07:45:05.848650 1087147 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:45:06.549549 1087147 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:45:06.553851 1063414 quantize_finetune_llama.py:222] layer 27 gpu 0
I0409 07:45:06.568606 1087147 data_utils.py:336] using 256 training seqs, 128 validation seqs
26_v proxy err 0.06601326167583466 err 28.685789108276367 tr(WHW.T) 434.54583740234375
bpp_loss 2.236975312232971
26_q proxy err 0.010593913495540619 err 226.7445068359375 tr(WHW.T) 21403.28125
bpp_loss 2.497033953666687
26_k proxy err 0.0024465022142976522 err 37.738731384277344 tr(WHW.T) 15425.5859375
bpp_loss 3.3474745750427246
26_o proxy err 0.05373120680451393 err 128.34902954101562 tr(WHW.T) 2388.72412109375
bpp_loss 2.1264625787734985
26_up proxy err 0.0708000436425209 err 541.3128051757812 tr(WHW.T) 7645.65625
bpp_loss 2.124152728489467
26_gate proxy err 0.020672934129834175 err 597.4776611328125 tr(WHW.T) 28901.4453125
bpp_loss 2.4764946528843472
26_down proxy err 0.07134596258401871 err 472.2264404296875 tr(WHW.T) 6618.82470703125
bpp_loss 2.1198302677699496
I0409 07:45:50.846817 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 27 in 1.123056173324585s
I0409 07:45:55.040325 1088016 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:45:55.040471 1088016 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:45:55.040514 1088016 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:45:55.472619 1088016 config.py:54] PyTorch version 2.6.0 available.
W0409 07:45:55.695906 1088016 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:45:56.386991 1088016 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:45:56.391351 1063414 quantize_finetune_llama.py:222] layer 28 gpu 0
I0409 07:45:56.407059 1088016 data_utils.py:336] using 256 training seqs, 128 validation seqs
27_v proxy err 0.046066123992204666 err 31.21872901916504 tr(WHW.T) 677.69384765625
bpp_loss 2.321457266807556
27_q proxy err 0.01065985206514597 err 227.1184844970703 tr(WHW.T) 21305.96875
bpp_loss 2.472119927406311
27_k proxy err 0.0028836613055318594 err 40.40476608276367 tr(WHW.T) 14011.6201171875
bpp_loss 3.306148886680603
27_o proxy err 0.06137092411518097 err 132.55850219726562 tr(WHW.T) 2159.9560546875
bpp_loss 2.1663979291915894
27_up proxy err 0.06456661969423294 err 547.240478515625 tr(WHW.T) 8475.5947265625
bpp_loss 2.138979434967041
27_gate proxy err 0.018434472382068634 err 604.8173217773438 tr(WHW.T) 32809.0390625
bpp_loss 2.4856737681797574
27_down proxy err 0.062073320150375366 err 406.1679382324219 tr(WHW.T) 6543.35791015625
bpp_loss 2.1302955831800188
I0409 07:46:40.850197 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 28 in 1.2960541248321533s
I0409 07:46:44.892964 1088915 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:46:44.893068 1088915 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:46:44.893109 1088915 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:46:45.316635 1088915 config.py:54] PyTorch version 2.6.0 available.
W0409 07:46:45.533346 1088915 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:46:46.222920 1088915 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:46:46.227152 1063414 quantize_finetune_llama.py:222] layer 29 gpu 0
I0409 07:46:46.242460 1088915 data_utils.py:336] using 256 training seqs, 128 validation seqs
28_v proxy err 0.05178561434149742 err 31.14824104309082 tr(WHW.T) 601.4844360351562
bpp_loss 2.368144154548645
28_q proxy err 0.010385623201727867 err 240.57142639160156 tr(WHW.T) 23163.888671875
bpp_loss 2.4799081087112427
28_k proxy err 0.0027487988118082285 err 41.21607208251953 tr(WHW.T) 14994.2119140625
bpp_loss 3.277058720588684
28_o proxy err 0.05352609604597092 err 134.42787170410156 tr(WHW.T) 2511.445556640625
bpp_loss 2.1933481693267822
28_up proxy err 0.05345432460308075 err 547.69677734375 tr(WHW.T) 10246.0703125
bpp_loss 2.1598824773515974
28_gate proxy err 0.016711866483092308 err 600.3062744140625 tr(WHW.T) 35920.9609375
bpp_loss 2.471912111554827
28_down proxy err 0.06170886382460594 err 444.9263000488281 tr(WHW.T) 7210.08740234375
bpp_loss 2.1442619391850064
I0409 07:47:30.629354 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 29 in 1.2999770641326904s
I0409 07:47:34.788000 1089813 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:47:34.788154 1089813 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:47:34.788197 1089813 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:47:35.220968 1089813 config.py:54] PyTorch version 2.6.0 available.
W0409 07:47:35.440545 1089813 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:47:36.138921 1089813 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:47:36.143355 1063414 quantize_finetune_llama.py:222] layer 30 gpu 0
I0409 07:47:36.162131 1089813 data_utils.py:336] using 256 training seqs, 128 validation seqs
29_v proxy err 0.04038940370082855 err 34.34832000732422 tr(WHW.T) 850.4290161132812
bpp_loss 2.432438611984253
29_q proxy err 0.012316429987549782 err 254.3740234375 tr(WHW.T) 20653.2265625
bpp_loss 2.4711971282958984
29_k proxy err 0.0028894436545670033 err 47.288047790527344 tr(WHW.T) 16365.796875
bpp_loss 3.347841262817383
29_o proxy err 0.03689985349774361 err 114.4945297241211 tr(WHW.T) 3102.844970703125
bpp_loss 2.2262706756591797
29_up proxy err 0.04356471449136734 err 560.61279296875 tr(WHW.T) 12868.505859375
bpp_loss 2.1915831565856934
29_gate proxy err 0.015813833102583885 err 606.6893920898438 tr(WHW.T) 38364.47265625
bpp_loss 2.4631212779453824
29_down proxy err 0.05378328636288643 err 401.78936767578125 tr(WHW.T) 7470.5244140625
bpp_loss 2.15710381099156
I0409 07:48:20.980289 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 30 in 1.397021770477295s
I0409 07:48:25.299107 1090730 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:48:25.299269 1090730 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:48:25.299311 1090730 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:48:25.747372 1090730 config.py:54] PyTorch version 2.6.0 available.
W0409 07:48:25.976191 1090730 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:48:26.726974 1090730 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:48:26.731426 1063414 quantize_finetune_llama.py:222] layer 31 gpu 0
I0409 07:48:26.768443 1090730 data_utils.py:336] using 256 training seqs, 128 validation seqs
30_v proxy err 0.03803471848368645 err 32.82627487182617 tr(WHW.T) 863.060791015625
bpp_loss 2.6812050342559814
30_q proxy err 0.010110237635672092 err 243.15762329101562 tr(WHW.T) 24050.6328125
bpp_loss 2.3796744346618652
30_k proxy err 0.0030166655778884888 err 42.32559585571289 tr(WHW.T) 14030.5888671875
bpp_loss 3.0635828971862793
30_o proxy err 0.02919192984700203 err 142.14060974121094 tr(WHW.T) 4869.1748046875
bpp_loss 2.306312322616577
30_up proxy err 0.0277758426964283 err 603.0467529296875 tr(WHW.T) 21711.1953125
bpp_loss 2.221611704145159
30_gate proxy err 0.012374626472592354 err 642.9169311523438 tr(WHW.T) 51954.453125
bpp_loss 2.5071450642177036
30_down proxy err 0.03827453404664993 err 337.2054748535156 tr(WHW.T) 8810.1787109375
bpp_loss 2.1559010573795865
I0409 07:49:12.847479 1063414 quantize_finetune_llama.py:253] computed original embedding for layer 31 in 1.8006203174591064s
I0409 07:49:17.438638 1091654 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:49:17.438803 1091654 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:49:17.438849 1091654 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:49:17.890801 1091654 config.py:54] PyTorch version 2.6.0 available.
W0409 07:49:18.112754 1091654 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 07:49:18.821662 1091654 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 07:49:18.842638 1091654 data_utils.py:336] using 256 training seqs, 128 validation seqs
31_v proxy err 0.021987291052937508 err 39.76893997192383 tr(WHW.T) 1808.723876953125
bpp_loss 2.52354097366333
31_q proxy err 0.006757877767086029 err 312.0494689941406 tr(WHW.T) 46175.66015625
bpp_loss 2.513319492340088
31_k proxy err 0.0031459301244467497 err 64.38977813720703 tr(WHW.T) 20467.64453125
bpp_loss 3.2608284950256348
31_o proxy err 0.03415093198418617 err 75.59587097167969 tr(WHW.T) 2213.58154296875
bpp_loss 2.26132333278656
31_up proxy err 0.01108135748654604 err 764.8291625976562 tr(WHW.T) 69019.4453125
bpp_loss 2.39838136945452
31_gate proxy err 0.0058469404466450214 err 844.6055297851562 tr(WHW.T) 144452.5625
bpp_loss 2.6971520015171597
31_down proxy err 0.024376971647143364 err 242.7992401123047 tr(WHW.T) 9960.1884765625
bpp_loss 2.175667013440813
I0409 07:50:12.768187 1092675 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:50:12.768350 1092675 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:50:12.768389 1092675 utils.py:162] NumExpr defaulting to 16 threads.
I0409 07:50:13.080381 1092675 config.py:54] PyTorch version 2.6.0 available.
W0409 07:50:13.287314 1092675 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0409 07:50:13.399161 1092675 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.87it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.11it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.50it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.73it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.85it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.90it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.07it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.79it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.54it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.85it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.71it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.84it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.79it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.79it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.03it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.88it/s]
I0409 07:50:16.807106 1092675 hfize_llama.py:161] loaded layer 0
I0409 07:50:17.775257 1092675 hfize_llama.py:161] loaded layer 1
I0409 07:50:18.745940 1092675 hfize_llama.py:161] loaded layer 2
I0409 07:50:19.848192 1092675 hfize_llama.py:161] loaded layer 3
I0409 07:50:20.883150 1092675 hfize_llama.py:161] loaded layer 4
I0409 07:50:21.899071 1092675 hfize_llama.py:161] loaded layer 5
I0409 07:50:22.895951 1092675 hfize_llama.py:161] loaded layer 6
I0409 07:50:23.883261 1092675 hfize_llama.py:161] loaded layer 7
I0409 07:50:24.853792 1092675 hfize_llama.py:161] loaded layer 8
I0409 07:50:25.818542 1092675 hfize_llama.py:161] loaded layer 9
I0409 07:50:26.805725 1092675 hfize_llama.py:161] loaded layer 10
I0409 07:50:27.769173 1092675 hfize_llama.py:161] loaded layer 11
I0409 07:50:28.801354 1092675 hfize_llama.py:161] loaded layer 12
I0409 07:50:29.773370 1092675 hfize_llama.py:161] loaded layer 13
I0409 07:50:30.668642 1092675 hfize_llama.py:161] loaded layer 14
I0409 07:50:31.640552 1092675 hfize_llama.py:161] loaded layer 15
I0409 07:50:32.493352 1092675 hfize_llama.py:161] loaded layer 16
I0409 07:50:33.346060 1092675 hfize_llama.py:161] loaded layer 17
I0409 07:50:34.217979 1092675 hfize_llama.py:161] loaded layer 18
I0409 07:50:35.132328 1092675 hfize_llama.py:161] loaded layer 19
I0409 07:50:36.202816 1092675 hfize_llama.py:161] loaded layer 20
I0409 07:50:37.241337 1092675 hfize_llama.py:161] loaded layer 21
I0409 07:50:38.211985 1092675 hfize_llama.py:161] loaded layer 22
I0409 07:50:39.239372 1092675 hfize_llama.py:161] loaded layer 23
I0409 07:50:40.184370 1092675 hfize_llama.py:161] loaded layer 24
I0409 07:50:41.162044 1092675 hfize_llama.py:161] loaded layer 25
I0409 07:50:41.886601 1092675 hfize_llama.py:161] loaded layer 26
I0409 07:50:42.756129 1092675 hfize_llama.py:161] loaded layer 27
I0409 07:50:43.692155 1092675 hfize_llama.py:161] loaded layer 28
I0409 07:50:44.660868 1092675 hfize_llama.py:161] loaded layer 29
I0409 07:50:45.585489 1092675 hfize_llama.py:161] loaded layer 30
I0409 07:50:46.650621 1092675 hfize_llama.py:161] loaded layer 31
I0409 07:50:46.650795 1092675 hfize_llama.py:165] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.07s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:01<00:04,  1.08it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:02<00:03,  1.13it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:03<00:02,  1.15it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:04<00:01,  1.13it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:05<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.16it/s]
I0409 07:51:22.770007 1092675 hfize_llama.py:175] successfully loaded hfized model
I0409 07:51:27.377334 1094184 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 07:51:27.377477 1094184 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 07:51:27.377521 1094184 utils.py:162] NumExpr defaulting to 16 threads.
W0409 07:51:27.718403 1094184 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0409 07:51:28.056454 1094184 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:05,  1.01it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.05s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.05s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.15s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.12s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.12s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.04s/it]
I0409 07:51:35.466978 1094184 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 2.31679368019104:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 2.31679368019104:   1%|          | 1/141 [00:01<04:24,  1.89s/it]avg_loss = 2.633646845817566:   1%|          | 1/141 [00:03<04:24,  1.89s/it]avg_loss = 2.633646845817566:   1%|▏         | 2/141 [00:03<03:51,  1.66s/it]avg_loss = 2.774564027786255:   1%|▏         | 2/141 [00:04<03:51,  1.66s/it]avg_loss = 2.774564027786255:   2%|▏         | 3/141 [00:04<03:40,  1.60s/it]avg_loss = 2.7370212078094482:   2%|▏         | 3/141 [00:06<03:40,  1.60s/it]avg_loss = 2.7370212078094482:   3%|▎         | 4/141 [00:06<03:34,  1.57s/it]avg_loss = 2.7081761360168457:   3%|▎         | 4/141 [00:07<03:34,  1.57s/it]avg_loss = 2.7081761360168457:   4%|▎         | 5/141 [00:07<03:31,  1.55s/it]avg_loss = 2.6229652961095176:   4%|▎         | 5/141 [00:09<03:31,  1.55s/it]avg_loss = 2.6229652961095176:   4%|▍         | 6/141 [00:09<03:28,  1.54s/it]avg_loss = 2.574585165296282:   4%|▍         | 6/141 [00:11<03:28,  1.54s/it] avg_loss = 2.574585165296282:   5%|▍         | 7/141 [00:11<03:26,  1.54s/it]avg_loss = 2.5702567100524902:   5%|▍         | 7/141 [00:12<03:26,  1.54s/it]avg_loss = 2.5702567100524902:   6%|▌         | 8/141 [00:12<03:24,  1.54s/it]avg_loss = 2.620493253072103:   6%|▌         | 8/141 [00:14<03:24,  1.54s/it] avg_loss = 2.620493253072103:   6%|▋         | 9/141 [00:14<03:23,  1.54s/it]avg_loss = 2.609139609336853:   6%|▋         | 9/141 [00:15<03:23,  1.54s/it]avg_loss = 2.609139609336853:   7%|▋         | 10/141 [00:15<03:22,  1.54s/it]avg_loss = 2.5985346273942427:   7%|▋         | 10/141 [00:17<03:22,  1.54s/it]avg_loss = 2.5985346273942427:   8%|▊         | 11/141 [00:17<03:20,  1.54s/it]avg_loss = 2.621161103248596:   8%|▊         | 11/141 [00:18<03:20,  1.54s/it] avg_loss = 2.621161103248596:   9%|▊         | 12/141 [00:18<03:19,  1.55s/it]avg_loss = 2.6340151750124416:   9%|▊         | 12/141 [00:20<03:19,  1.55s/it]avg_loss = 2.6340151750124416:   9%|▉         | 13/141 [00:20<03:18,  1.55s/it]avg_loss = 2.654132434300014:   9%|▉         | 13/141 [00:21<03:18,  1.55s/it] avg_loss = 2.654132434300014:  10%|▉         | 14/141 [00:21<03:17,  1.56s/it]avg_loss = 2.662242301305135:  10%|▉         | 14/141 [00:23<03:17,  1.56s/it]avg_loss = 2.662242301305135:  11%|█         | 15/141 [00:23<03:16,  1.56s/it]avg_loss = 2.6814817786216736:  11%|█         | 15/141 [00:25<03:16,  1.56s/it]avg_loss = 2.6814817786216736:  11%|█▏        | 16/141 [00:25<03:15,  1.56s/it]avg_loss = 2.683662218206069:  11%|█▏        | 16/141 [00:26<03:15,  1.56s/it] avg_loss = 2.683662218206069:  12%|█▏        | 17/141 [00:26<03:14,  1.57s/it]avg_loss = 2.6842931509017944:  12%|█▏        | 17/141 [00:28<03:14,  1.57s/it]avg_loss = 2.6842931509017944:  13%|█▎        | 18/141 [00:28<03:13,  1.57s/it]avg_loss = 2.669894105509708:  13%|█▎        | 18/141 [00:29<03:13,  1.57s/it] avg_loss = 2.669894105509708:  13%|█▎        | 19/141 [00:29<03:12,  1.58s/it]avg_loss = 2.6698200702667236:  13%|█▎        | 19/141 [00:31<03:12,  1.58s/it]avg_loss = 2.6698200702667236:  14%|█▍        | 20/141 [00:31<03:11,  1.58s/it]avg_loss = 2.6753876549857005:  14%|█▍        | 20/141 [00:32<03:11,  1.58s/it]avg_loss = 2.6753876549857005:  15%|█▍        | 21/141 [00:32<03:10,  1.58s/it]avg_loss = 2.675134788859974:  15%|█▍        | 21/141 [00:34<03:10,  1.58s/it] avg_loss = 2.675134788859974:  16%|█▌        | 22/141 [00:34<03:08,  1.59s/it]avg_loss = 2.678047864333443:  16%|█▌        | 22/141 [00:36<03:08,  1.59s/it]avg_loss = 2.678047864333443:  16%|█▋        | 23/141 [00:36<03:07,  1.59s/it]avg_loss = 2.6817386945088706:  16%|█▋        | 23/141 [00:37<03:07,  1.59s/it]avg_loss = 2.6817386945088706:  17%|█▋        | 24/141 [00:37<03:06,  1.59s/it]avg_loss = 2.6884951210021972:  17%|█▋        | 24/141 [00:39<03:06,  1.59s/it]avg_loss = 2.6884951210021972:  18%|█▊        | 25/141 [00:39<03:05,  1.60s/it]avg_loss = 2.696863504556509:  18%|█▊        | 25/141 [00:40<03:05,  1.60s/it] avg_loss = 2.696863504556509:  18%|█▊        | 26/141 [00:40<03:03,  1.60s/it]avg_loss = 2.7080335793671786:  18%|█▊        | 26/141 [00:42<03:03,  1.60s/it]avg_loss = 2.7080335793671786:  19%|█▉        | 27/141 [00:42<03:02,  1.60s/it]avg_loss = 2.711358462061201:  19%|█▉        | 27/141 [00:44<03:02,  1.60s/it] avg_loss = 2.711358462061201:  20%|█▉        | 28/141 [00:44<03:01,  1.61s/it]avg_loss = 2.7075079967235696:  20%|█▉        | 28/141 [00:45<03:01,  1.61s/it]avg_loss = 2.7075079967235696:  21%|██        | 29/141 [00:45<03:00,  1.61s/it]avg_loss = 2.697353490193685:  21%|██        | 29/141 [00:47<03:00,  1.61s/it] avg_loss = 2.697353490193685:  21%|██▏       | 30/141 [00:47<02:59,  1.61s/it]avg_loss = 2.6884707481630388:  21%|██▏       | 30/141 [00:49<02:59,  1.61s/it]avg_loss = 2.6884707481630388:  22%|██▏       | 31/141 [00:49<02:57,  1.62s/it]avg_loss = 2.678080141544342:  22%|██▏       | 31/141 [00:50<02:57,  1.62s/it] avg_loss = 2.678080141544342:  23%|██▎       | 32/141 [00:50<02:56,  1.62s/it]avg_loss = 2.6759183406829834:  23%|██▎       | 32/141 [00:52<02:56,  1.62s/it]avg_loss = 2.6759183406829834:  23%|██▎       | 33/141 [00:52<02:54,  1.62s/it]avg_loss = 2.6735004747615143:  23%|██▎       | 33/141 [00:53<02:54,  1.62s/it]avg_loss = 2.6735004747615143:  24%|██▍       | 34/141 [00:53<02:53,  1.62s/it]avg_loss = 2.6781899452209474:  24%|██▍       | 34/141 [00:55<02:53,  1.62s/it]avg_loss = 2.6781899452209474:  25%|██▍       | 35/141 [00:55<02:52,  1.63s/it]avg_loss = 2.660325871573554:  25%|██▍       | 35/141 [00:57<02:52,  1.63s/it] avg_loss = 2.660325871573554:  26%|██▌       | 36/141 [00:57<02:50,  1.63s/it]avg_loss = 2.646643032898774:  26%|██▌       | 36/141 [00:58<02:50,  1.63s/it]avg_loss = 2.646643032898774:  26%|██▌       | 37/141 [00:58<02:49,  1.63s/it]avg_loss = 2.63236623688748:  26%|██▌       | 37/141 [01:00<02:49,  1.63s/it] avg_loss = 2.63236623688748:  27%|██▋       | 38/141 [01:00<02:48,  1.63s/it]avg_loss = 2.618504707629864:  27%|██▋       | 38/141 [01:02<02:48,  1.63s/it]avg_loss = 2.618504707629864:  28%|██▊       | 39/141 [01:02<02:46,  1.63s/it]avg_loss = 2.6102718651294707:  28%|██▊       | 39/141 [01:03<02:46,  1.63s/it]avg_loss = 2.6102718651294707:  28%|██▊       | 40/141 [01:03<02:45,  1.64s/it]avg_loss = 2.6161396677901103:  28%|██▊       | 40/141 [01:05<02:45,  1.64s/it]avg_loss = 2.6161396677901103:  29%|██▉       | 41/141 [01:05<02:43,  1.64s/it]avg_loss = 2.630650060517447:  29%|██▉       | 41/141 [01:07<02:43,  1.64s/it] avg_loss = 2.630650060517447:  30%|██▉       | 42/141 [01:07<02:42,  1.64s/it]avg_loss = 2.645728266516397:  30%|██▉       | 42/141 [01:08<02:42,  1.64s/it]avg_loss = 2.645728266516397:  30%|███       | 43/141 [01:08<02:40,  1.64s/it]avg_loss = 2.6545342911373484:  30%|███       | 43/141 [01:10<02:40,  1.64s/it]avg_loss = 2.6545342911373484:  31%|███       | 44/141 [01:10<02:39,  1.64s/it]avg_loss = 2.660999478234185:  31%|███       | 44/141 [01:11<02:39,  1.64s/it] avg_loss = 2.660999478234185:  32%|███▏      | 45/141 [01:11<02:37,  1.64s/it]avg_loss = 2.6650293184363325:  32%|███▏      | 45/141 [01:13<02:37,  1.64s/it]avg_loss = 2.6650293184363325:  33%|███▎      | 46/141 [01:13<02:36,  1.65s/it]avg_loss = 2.670942265936669:  33%|███▎      | 46/141 [01:15<02:36,  1.65s/it] avg_loss = 2.670942265936669:  33%|███▎      | 47/141 [01:15<02:34,  1.65s/it]avg_loss = 2.6720941166083017:  33%|███▎      | 47/141 [01:16<02:34,  1.65s/it]avg_loss = 2.6720941166083017:  34%|███▍      | 48/141 [01:16<02:33,  1.65s/it]avg_loss = 2.6697612344002235:  34%|███▍      | 48/141 [01:18<02:33,  1.65s/it]avg_loss = 2.6697612344002235:  35%|███▍      | 49/141 [01:18<02:31,  1.65s/it]avg_loss = 2.6671573448181154:  35%|███▍      | 49/141 [01:20<02:31,  1.65s/it]avg_loss = 2.6671573448181154:  35%|███▌      | 50/141 [01:20<02:30,  1.65s/it]avg_loss = 2.660419936273612:  35%|███▌      | 50/141 [01:21<02:30,  1.65s/it] avg_loss = 2.660419936273612:  36%|███▌      | 51/141 [01:21<02:28,  1.65s/it]avg_loss = 2.654717913040748:  36%|███▌      | 51/141 [01:23<02:28,  1.65s/it]avg_loss = 2.654717913040748:  37%|███▋      | 52/141 [01:23<02:27,  1.66s/it]avg_loss = 2.648481877344959:  37%|███▋      | 52/141 [01:25<02:27,  1.66s/it]avg_loss = 2.648481877344959:  38%|███▊      | 53/141 [01:25<02:25,  1.66s/it]avg_loss = 2.6448848689043962:  38%|███▊      | 53/141 [01:26<02:25,  1.66s/it]avg_loss = 2.6448848689043962:  38%|███▊      | 54/141 [01:26<02:24,  1.66s/it]avg_loss = 2.6357297290455213:  38%|███▊      | 54/141 [01:28<02:24,  1.66s/it]avg_loss = 2.6357297290455213:  39%|███▉      | 55/141 [01:28<02:22,  1.66s/it]avg_loss = 2.627564323799951:  39%|███▉      | 55/141 [01:30<02:22,  1.66s/it] avg_loss = 2.627564323799951:  40%|███▉      | 56/141 [01:30<02:21,  1.66s/it]avg_loss = 2.626390344218204:  40%|███▉      | 56/141 [01:31<02:21,  1.66s/it]avg_loss = 2.626390344218204:  40%|████      | 57/141 [01:31<02:19,  1.66s/it]avg_loss = 2.623218828234179:  40%|████      | 57/141 [01:33<02:19,  1.66s/it]avg_loss = 2.623218828234179:  41%|████      | 58/141 [01:33<02:18,  1.67s/it]avg_loss = 2.62552288023092:  41%|████      | 58/141 [01:35<02:18,  1.67s/it] avg_loss = 2.62552288023092:  42%|████▏     | 59/141 [01:35<02:16,  1.67s/it]avg_loss = 2.630297036965688:  42%|████▏     | 59/141 [01:36<02:16,  1.67s/it]avg_loss = 2.630297036965688:  43%|████▎     | 60/141 [01:36<02:15,  1.67s/it]avg_loss = 2.634036744227175:  43%|████▎     | 60/141 [01:38<02:15,  1.67s/it]avg_loss = 2.634036744227175:  43%|████▎     | 61/141 [01:38<02:13,  1.67s/it]avg_loss = 2.6414916976805656:  43%|████▎     | 61/141 [01:40<02:13,  1.67s/it]avg_loss = 2.6414916976805656:  44%|████▍     | 62/141 [01:40<02:11,  1.67s/it]avg_loss = 2.6343112597389826:  44%|████▍     | 62/141 [01:41<02:11,  1.67s/it]avg_loss = 2.6343112597389826:  45%|████▍     | 63/141 [01:41<02:10,  1.67s/it]avg_loss = 2.6319925412535667:  45%|████▍     | 63/141 [01:43<02:10,  1.67s/it]avg_loss = 2.6319925412535667:  45%|████▌     | 64/141 [01:43<02:08,  1.67s/it]avg_loss = 2.631464719772339:  45%|████▌     | 64/141 [01:45<02:08,  1.67s/it] avg_loss = 2.631464719772339:  46%|████▌     | 65/141 [01:45<02:07,  1.67s/it]avg_loss = 2.6264312809163872:  46%|████▌     | 65/141 [01:46<02:07,  1.67s/it]avg_loss = 2.6264312809163872:  47%|████▋     | 66/141 [01:46<02:05,  1.68s/it]avg_loss = 2.622762277944764:  47%|████▋     | 66/141 [01:48<02:05,  1.68s/it] avg_loss = 2.622762277944764:  48%|████▊     | 67/141 [01:48<02:04,  1.68s/it]avg_loss = 2.6220338660127975:  48%|████▊     | 67/141 [01:50<02:04,  1.68s/it]avg_loss = 2.6220338660127975:  48%|████▊     | 68/141 [01:50<02:02,  1.68s/it]avg_loss = 2.6205716409545015:  48%|████▊     | 68/141 [01:51<02:02,  1.68s/it]avg_loss = 2.6205716409545015:  49%|████▉     | 69/141 [01:51<02:00,  1.68s/it]avg_loss = 2.622483604294913:  49%|████▉     | 69/141 [01:53<02:00,  1.68s/it] avg_loss = 2.622483604294913:  50%|████▉     | 70/141 [01:53<01:59,  1.68s/it]avg_loss = 2.627012078191193:  50%|████▉     | 70/141 [01:55<01:59,  1.68s/it]avg_loss = 2.627012078191193:  50%|█████     | 71/141 [01:55<01:57,  1.68s/it]avg_loss = 2.6297882066832647:  50%|█████     | 71/141 [01:57<01:57,  1.68s/it]avg_loss = 2.6297882066832647:  51%|█████     | 72/141 [01:57<01:56,  1.68s/it]avg_loss = 2.627494883863893:  51%|█████     | 72/141 [01:58<01:56,  1.68s/it] avg_loss = 2.627494883863893:  52%|█████▏    | 73/141 [01:58<01:54,  1.68s/it]avg_loss = 2.6279445854393213:  52%|█████▏    | 73/141 [02:00<01:54,  1.68s/it]avg_loss = 2.6279445854393213:  52%|█████▏    | 74/141 [02:00<01:52,  1.68s/it]avg_loss = 2.6282333946228027:  52%|█████▏    | 74/141 [02:02<01:52,  1.68s/it]avg_loss = 2.6282333946228027:  53%|█████▎    | 75/141 [02:02<01:51,  1.68s/it]avg_loss = 2.6264385831983468:  53%|█████▎    | 75/141 [02:03<01:51,  1.68s/it]avg_loss = 2.6264385831983468:  54%|█████▍    | 76/141 [02:03<01:49,  1.68s/it]avg_loss = 2.62683131168415:  54%|█████▍    | 76/141 [02:05<01:49,  1.68s/it]  avg_loss = 2.62683131168415:  55%|█████▍    | 77/141 [02:05<01:47,  1.69s/it]avg_loss = 2.6284911938202686:  55%|█████▍    | 77/141 [02:07<01:47,  1.69s/it]avg_loss = 2.6284911938202686:  55%|█████▌    | 78/141 [02:07<01:46,  1.69s/it]avg_loss = 2.6311057821104797:  55%|█████▌    | 78/141 [02:08<01:46,  1.69s/it]avg_loss = 2.6311057821104797:  56%|█████▌    | 79/141 [02:08<01:44,  1.69s/it]avg_loss = 2.62510561645031:  56%|█████▌    | 79/141 [02:10<01:44,  1.69s/it]  avg_loss = 2.62510561645031:  57%|█████▋    | 80/141 [02:10<01:42,  1.69s/it]avg_loss = 2.622351661140536:  57%|█████▋    | 80/141 [02:12<01:42,  1.69s/it]avg_loss = 2.622351661140536:  57%|█████▋    | 81/141 [02:12<01:41,  1.69s/it]avg_loss = 2.62095419081246:  57%|█████▋    | 81/141 [02:13<01:41,  1.69s/it] avg_loss = 2.62095419081246:  58%|█████▊    | 82/141 [02:13<01:39,  1.69s/it]avg_loss = 2.61855676662491:  58%|█████▊    | 82/141 [02:15<01:39,  1.69s/it]avg_loss = 2.61855676662491:  59%|█████▉    | 83/141 [02:15<01:38,  1.69s/it]avg_loss = 2.6167136884870983:  59%|█████▉    | 83/141 [02:17<01:38,  1.69s/it]avg_loss = 2.6167136884870983:  60%|█████▉    | 84/141 [02:17<01:36,  1.69s/it]avg_loss = 2.6151113510131836:  60%|█████▉    | 84/141 [02:18<01:36,  1.69s/it]avg_loss = 2.6151113510131836:  60%|██████    | 85/141 [02:18<01:34,  1.69s/it]avg_loss = 2.616634496422701:  60%|██████    | 85/141 [02:20<01:34,  1.69s/it] avg_loss = 2.616634496422701:  61%|██████    | 86/141 [02:20<01:33,  1.69s/it]avg_loss = 2.617893413565625:  61%|██████    | 86/141 [02:22<01:33,  1.69s/it]avg_loss = 2.617893413565625:  62%|██████▏   | 87/141 [02:22<01:31,  1.69s/it]avg_loss = 2.619657031514428:  62%|██████▏   | 87/141 [02:24<01:31,  1.69s/it]avg_loss = 2.619657031514428:  62%|██████▏   | 88/141 [02:24<01:29,  1.69s/it]avg_loss = 2.628613201419959:  62%|██████▏   | 88/141 [02:25<01:29,  1.69s/it]avg_loss = 2.628613201419959:  63%|██████▎   | 89/141 [02:25<01:28,  1.69s/it]avg_loss = 2.636275233162774:  63%|██████▎   | 89/141 [02:27<01:28,  1.69s/it]avg_loss = 2.636275233162774:  64%|██████▍   | 90/141 [02:27<01:26,  1.70s/it]avg_loss = 2.6387423421000387:  64%|██████▍   | 90/141 [02:29<01:26,  1.70s/it]avg_loss = 2.6387423421000387:  65%|██████▍   | 91/141 [02:29<01:24,  1.70s/it]avg_loss = 2.643935794415681:  65%|██████▍   | 91/141 [02:30<01:24,  1.70s/it] avg_loss = 2.643935794415681:  65%|██████▌   | 92/141 [02:30<01:23,  1.70s/it]avg_loss = 2.648793917830272:  65%|██████▌   | 92/141 [02:32<01:23,  1.70s/it]avg_loss = 2.648793917830272:  66%|██████▌   | 93/141 [02:32<01:21,  1.70s/it]avg_loss = 2.648813356744482:  66%|██████▌   | 93/141 [02:34<01:21,  1.70s/it]avg_loss = 2.648813356744482:  67%|██████▋   | 94/141 [02:34<01:19,  1.70s/it]avg_loss = 2.6527556093115554:  67%|██████▋   | 94/141 [02:35<01:19,  1.70s/it]avg_loss = 2.6527556093115554:  67%|██████▋   | 95/141 [02:35<01:17,  1.69s/it]avg_loss = 2.652842715382576:  67%|██████▋   | 95/141 [02:37<01:17,  1.69s/it] avg_loss = 2.652842715382576:  68%|██████▊   | 96/141 [02:37<01:16,  1.70s/it]avg_loss = 2.654210719865622:  68%|██████▊   | 96/141 [02:39<01:16,  1.70s/it]avg_loss = 2.654210719865622:  69%|██████▉   | 97/141 [02:39<01:14,  1.70s/it]avg_loss = 2.6528884513037547:  69%|██████▉   | 97/141 [02:41<01:14,  1.70s/it]avg_loss = 2.6528884513037547:  70%|██████▉   | 98/141 [02:41<01:12,  1.70s/it]avg_loss = 2.65441271993849:  70%|██████▉   | 98/141 [02:42<01:12,  1.70s/it]  avg_loss = 2.65441271993849:  70%|███████   | 99/141 [02:42<01:11,  1.70s/it]avg_loss = 2.657316620349884:  70%|███████   | 99/141 [02:44<01:11,  1.70s/it]avg_loss = 2.657316620349884:  71%|███████   | 100/141 [02:44<01:09,  1.70s/it]avg_loss = 2.6579078612941327:  71%|███████   | 100/141 [02:46<01:09,  1.70s/it]avg_loss = 2.6579078612941327:  72%|███████▏  | 101/141 [02:46<01:07,  1.70s/it]avg_loss = 2.659187258458605:  72%|███████▏  | 101/141 [02:47<01:07,  1.70s/it] avg_loss = 2.659187258458605:  72%|███████▏  | 102/141 [02:47<01:06,  1.70s/it]avg_loss = 2.660094772727744:  72%|███████▏  | 102/141 [02:49<01:06,  1.70s/it]avg_loss = 2.660094772727744:  73%|███████▎  | 103/141 [02:49<01:04,  1.70s/it]avg_loss = 2.6645070085158715:  73%|███████▎  | 103/141 [02:51<01:04,  1.70s/it]avg_loss = 2.6645070085158715:  74%|███████▍  | 104/141 [02:51<01:02,  1.70s/it]avg_loss = 2.6643614337557837:  74%|███████▍  | 104/141 [02:52<01:02,  1.70s/it]avg_loss = 2.6643614337557837:  74%|███████▍  | 105/141 [02:52<01:01,  1.70s/it]avg_loss = 2.6642441727080435:  74%|███████▍  | 105/141 [02:54<01:01,  1.70s/it]avg_loss = 2.6642441727080435:  75%|███████▌  | 106/141 [02:54<00:59,  1.70s/it]avg_loss = 2.662941625185102:  75%|███████▌  | 106/141 [02:56<00:59,  1.70s/it] avg_loss = 2.662941625185102:  76%|███████▌  | 107/141 [02:56<00:57,  1.70s/it]avg_loss = 2.660915352680065:  76%|███████▌  | 107/141 [02:57<00:57,  1.70s/it]avg_loss = 2.660915352680065:  77%|███████▋  | 108/141 [02:57<00:56,  1.70s/it]avg_loss = 2.66016417905825:  77%|███████▋  | 108/141 [02:59<00:56,  1.70s/it] avg_loss = 2.66016417905825:  77%|███████▋  | 109/141 [02:59<00:54,  1.70s/it]avg_loss = 2.6568728750402277:  77%|███████▋  | 109/141 [03:01<00:54,  1.70s/it]avg_loss = 2.6568728750402277:  78%|███████▊  | 110/141 [03:01<00:52,  1.70s/it]avg_loss = 2.6600233993014775:  78%|███████▊  | 110/141 [03:03<00:52,  1.70s/it]avg_loss = 2.6600233993014775:  79%|███████▊  | 111/141 [03:03<00:50,  1.70s/it]avg_loss = 2.6601127279656276:  79%|███████▊  | 111/141 [03:04<00:50,  1.70s/it]avg_loss = 2.6601127279656276:  79%|███████▉  | 112/141 [03:04<00:49,  1.70s/it]avg_loss = 2.6612130857146945:  79%|███████▉  | 112/141 [03:06<00:49,  1.70s/it]avg_loss = 2.6612130857146945:  80%|████████  | 113/141 [03:06<00:47,  1.70s/it]avg_loss = 2.663481597314801:  80%|████████  | 113/141 [03:08<00:47,  1.70s/it] avg_loss = 2.663481597314801:  81%|████████  | 114/141 [03:08<00:45,  1.70s/it]avg_loss = 2.661657080443009:  81%|████████  | 114/141 [03:09<00:45,  1.70s/it]avg_loss = 2.661657080443009:  82%|████████▏ | 115/141 [03:09<00:44,  1.70s/it]avg_loss = 2.6597072071042556:  82%|████████▏ | 115/141 [03:11<00:44,  1.70s/it]avg_loss = 2.6597072071042556:  82%|████████▏ | 116/141 [03:11<00:42,  1.70s/it]avg_loss = 2.661853123933841:  82%|████████▏ | 116/141 [03:13<00:42,  1.70s/it] avg_loss = 2.661853123933841:  83%|████████▎ | 117/141 [03:13<00:40,  1.70s/it]avg_loss = 2.660239613662332:  83%|████████▎ | 117/141 [03:14<00:40,  1.70s/it]avg_loss = 2.660239613662332:  84%|████████▎ | 118/141 [03:14<00:39,  1.70s/it]avg_loss = 2.6579359739768407:  84%|████████▎ | 118/141 [03:16<00:39,  1.70s/it]avg_loss = 2.6579359739768407:  84%|████████▍ | 119/141 [03:16<00:37,  1.70s/it]avg_loss = 2.655247179667155:  84%|████████▍ | 119/141 [03:18<00:37,  1.70s/it] avg_loss = 2.655247179667155:  85%|████████▌ | 120/141 [03:18<00:35,  1.70s/it]avg_loss = 2.65551752492416:  85%|████████▌ | 120/141 [03:20<00:35,  1.70s/it] avg_loss = 2.65551752492416:  86%|████████▌ | 121/141 [03:20<00:34,  1.70s/it]avg_loss = 2.6556445496981262:  86%|████████▌ | 121/141 [03:21<00:34,  1.70s/it]avg_loss = 2.6556445496981262:  87%|████████▋ | 122/141 [03:21<00:32,  1.70s/it]avg_loss = 2.654473657530498:  87%|████████▋ | 122/141 [03:23<00:32,  1.70s/it] avg_loss = 2.654473657530498:  87%|████████▋ | 123/141 [03:23<00:30,  1.70s/it]avg_loss = 2.654265184556284:  87%|████████▋ | 123/141 [03:25<00:30,  1.70s/it]avg_loss = 2.654265184556284:  88%|████████▊ | 124/141 [03:25<00:28,  1.70s/it]avg_loss = 2.6524973106384278:  88%|████████▊ | 124/141 [03:26<00:28,  1.70s/it]avg_loss = 2.6524973106384278:  89%|████████▊ | 125/141 [03:26<00:27,  1.70s/it]avg_loss = 2.6520343413428655:  89%|████████▊ | 125/141 [03:28<00:27,  1.70s/it]avg_loss = 2.6520343413428655:  89%|████████▉ | 126/141 [03:28<00:25,  1.70s/it]avg_loss = 2.651787234103586:  89%|████████▉ | 126/141 [03:30<00:25,  1.70s/it] avg_loss = 2.651787234103586:  90%|█████████ | 127/141 [03:30<00:23,  1.70s/it]avg_loss = 2.6500620767474174:  90%|█████████ | 127/141 [03:31<00:23,  1.70s/it]avg_loss = 2.6500620767474174:  91%|█████████ | 128/141 [03:31<00:22,  1.70s/it]avg_loss = 2.649975523468136:  91%|█████████ | 128/141 [03:33<00:22,  1.70s/it] avg_loss = 2.649975523468136:  91%|█████████▏| 129/141 [03:33<00:20,  1.70s/it]avg_loss = 2.650968498450059:  91%|█████████▏| 129/141 [03:35<00:20,  1.70s/it]avg_loss = 2.650968498450059:  92%|█████████▏| 130/141 [03:35<00:18,  1.70s/it]avg_loss = 2.651510892023567:  92%|█████████▏| 130/141 [03:37<00:18,  1.70s/it]avg_loss = 2.651510892023567:  93%|█████████▎| 131/141 [03:37<00:16,  1.70s/it]avg_loss = 2.651774101185076:  93%|█████████▎| 131/141 [03:38<00:16,  1.70s/it]avg_loss = 2.651774101185076:  94%|█████████▎| 132/141 [03:38<00:15,  1.70s/it]avg_loss = 2.6479288599544897:  94%|█████████▎| 132/141 [03:40<00:15,  1.70s/it]avg_loss = 2.6479288599544897:  94%|█████████▍| 133/141 [03:40<00:13,  1.70s/it]avg_loss = 2.6418987975191714:  94%|█████████▍| 133/141 [03:42<00:13,  1.70s/it]avg_loss = 2.6418987975191714:  95%|█████████▌| 134/141 [03:42<00:11,  1.70s/it]avg_loss = 2.644364943327727:  95%|█████████▌| 134/141 [03:43<00:11,  1.70s/it] avg_loss = 2.644364943327727:  96%|█████████▌| 135/141 [03:43<00:10,  1.70s/it]avg_loss = 2.6482837533249572:  96%|█████████▌| 135/141 [03:45<00:10,  1.70s/it]avg_loss = 2.6482837533249572:  96%|█████████▋| 136/141 [03:45<00:08,  1.70s/it]avg_loss = 2.6505836552947106:  96%|█████████▋| 136/141 [03:47<00:08,  1.70s/it]avg_loss = 2.6505836552947106:  97%|█████████▋| 137/141 [03:47<00:06,  1.70s/it]avg_loss = 2.650200114733931:  97%|█████████▋| 137/141 [03:48<00:06,  1.70s/it] avg_loss = 2.650200114733931:  98%|█████████▊| 138/141 [03:48<00:05,  1.70s/it]avg_loss = 2.6514270151261803:  98%|█████████▊| 138/141 [03:50<00:05,  1.70s/it]avg_loss = 2.6514270151261803:  99%|█████████▊| 139/141 [03:50<00:03,  1.70s/it]avg_loss = 2.653114243916103:  99%|█████████▊| 139/141 [03:52<00:03,  1.70s/it] avg_loss = 2.653114243916103:  99%|█████████▉| 140/141 [03:52<00:01,  1.70s/it]avg_loss = 2.655096152150039:  99%|█████████▉| 140/141 [03:54<00:01,  1.70s/it]avg_loss = 2.655096152150039: 100%|██████████| 141/141 [03:54<00:00,  1.70s/it]avg_loss = 2.655096152150039: 100%|██████████| 141/141 [03:54<00:00,  1.66s/it]
I0409 07:55:57.996049 1094184 eval_ppl.py:107] wikitext2 perplexity: 14.22635269165039
wikitext2 perplexity: 14.226
