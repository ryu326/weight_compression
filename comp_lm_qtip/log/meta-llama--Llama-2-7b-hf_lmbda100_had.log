I0314 08:06:59.078058 2528979 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.69it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.83it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.65it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.12it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.52it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.13it/s]
I0314 08:07:01.139267 2528979 quantize_finetune_llama.py:142] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:23,  1.33it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:22,  1.36it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:21,  1.37it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:20,  1.38it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:19,  1.36it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:19,  1.36it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:05<00:18,  1.35it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:17,  1.34it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:16,  1.35it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:07<00:16,  1.34it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:08<00:15,  1.34it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:14,  1.35it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:14,  1.36it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:10<00:13,  1.36it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:11<00:12,  1.38it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:11,  1.38it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:12<00:10,  1.40it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:13<00:09,  1.41it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:09,  1.41it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:14<00:08,  1.41it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:15<00:07,  1.41it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:06,  1.43it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:06,  1.44it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:05,  1.45it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:18<00:04,  1.43it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.42it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:19<00:03,  1.41it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:20<00:02,  1.41it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:02,  1.43it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.42it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:22<00:00,  1.42it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.40it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.39it/s]
I0314 08:07:34.047925 2528979 quantize_finetune_llama.py:167] loaded compression model
I0314 08:07:55.839967 2528979 quantize_finetune_llama.py:171] loaded dataset and devset
I0314 08:08:01.111137 2528979 quantize_finetune_llama.py:191] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:09:20.203597 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 0 in 78.97864103317261s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 08:09:42.883178 2530748 config.py:54] PyTorch version 2.1.1 available.
I0314 08:09:43.822269 2528979 quantize_finetune_llama.py:191] layer 1 gpu 1
I0314 08:10:51.787734 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 1 in 67.813725233078s
I0314 08:11:02.244306 2531519 config.py:54] PyTorch version 2.1.1 available.
I0314 08:11:03.210562 2528979 quantize_finetune_llama.py:191] layer 2 gpu 2
I0314 08:12:11.264094 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 2 in 67.92664074897766s
I0314 08:12:19.036309 2532285 config.py:54] PyTorch version 2.1.1 available.
I0314 08:12:19.970716 2528979 quantize_finetune_llama.py:191] layer 3 gpu 3
I0314 08:12:20.025143 2532285 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:12:28.581059 2532285 finetune.py:45] layer 2_v initial loss 9.464671165915206e-06
I0314 08:12:37.195432 2532285 finetune.py:45] layer 2_q initial loss 9.99448911898071e-06
I0314 08:12:45.858143 2532285 finetune.py:45] layer 2_k initial loss 1.0578367437119596e-05
I0314 08:12:54.699047 2532285 finetune.py:45] layer 2_o initial loss 1.4224928236217238e-05
I0314 08:13:09.612591 2532285 finetune.py:45] layer 2_up initial loss 1.660810266912449e-05
I0314 08:13:24.568911 2532285 finetune.py:45] layer 2_gate initial loss 1.8404374714009464e-05
I0314 08:13:32.738748 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 3 in 72.59770369529724s
I0314 08:13:42.968240 2532285 finetune.py:45] layer 2_down initial loss 2.33085283980472e-05
I0314 08:13:43.451087 2533300 config.py:54] PyTorch version 2.1.1 available.
2_v proxy err 0.01495397835969925 tr(WHW.T) 136.67332458496094
2_q proxy err 0.0003689636941999197 tr(WHW.T) 7752.85205078125
2_k proxy err 0.0002892393968068063 tr(WHW.T) 10205.837890625
2_o proxy err 0.007906598970293999 tr(WHW.T) 1.4603197574615479
2_up proxy err 0.005999243818223476 tr(WHW.T) 193.43603515625
2_gate proxy err 0.0038906331174075603 tr(WHW.T) 306.6622619628906
2_down proxy err 0.006909828633069992 tr(WHW.T) 3.010739803314209
I0314 08:13:44.395508 2528979 quantize_finetune_llama.py:191] layer 4 gpu 0
I0314 08:13:44.452387 2533300 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:13:53.609581 2533300 finetune.py:45] layer 3_v initial loss 1.4534792171616573e-05
I0314 08:14:02.050238 2533300 finetune.py:45] layer 3_q initial loss 1.5771085600135848e-05
I0314 08:14:10.387331 2533300 finetune.py:45] layer 3_k initial loss 1.7125537851825356e-05
I0314 08:14:18.930615 2533300 finetune.py:45] layer 3_o initial loss 2.5037383238668554e-05
I0314 08:14:33.721914 2533300 finetune.py:45] layer 3_up initial loss 3.074206688324921e-05
I0314 08:14:48.599371 2533300 finetune.py:45] layer 3_gate initial loss 3.4878987207775936e-05
I0314 08:14:54.099488 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 4 in 68.40991592407227s
I0314 08:14:57.251459 2534204 config.py:54] PyTorch version 2.1.1 available.
I0314 08:14:58.359579 2528979 quantize_finetune_llama.py:191] layer 5 gpu 1
I0314 08:14:58.430042 2534204 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:15:04.604139 2533300 finetune.py:45] layer 3_down initial loss 4.524040923570283e-05
3_v proxy err 0.013855881057679653 tr(WHW.T) 284.77557373046875
3_q proxy err 0.0007092630257830024 tr(WHW.T) 7217.63720703125
3_k proxy err 0.0005223305197432637 tr(WHW.T) 10074.73828125
3_o proxy err 0.007895415648818016 tr(WHW.T) 3.3527450561523438
3_up proxy err 0.006845729425549507 tr(WHW.T) 284.7950744628906
3_gate proxy err 0.004196794703602791 tr(WHW.T) 478.13714599609375
3_down proxy err 0.007066783495247364 tr(WHW.T) 6.133229732513428
I0314 08:15:07.303617 2534204 finetune.py:45] layer 4_v initial loss 2.5683382773422636e-05
I0314 08:15:15.941382 2534204 finetune.py:45] layer 4_q initial loss 2.7607142328633927e-05
I0314 08:15:24.661015 2534204 finetune.py:45] layer 4_k initial loss 2.9626844479935244e-05
I0314 08:15:33.530010 2534204 finetune.py:45] layer 4_o initial loss 4.067090048920363e-05
I0314 08:15:48.749084 2534204 finetune.py:45] layer 4_up initial loss 5.142293230164796e-05
I0314 08:16:03.937169 2534204 finetune.py:45] layer 4_gate initial loss 5.841251913807355e-05
I0314 08:16:05.901904 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 5 in 67.17812085151672s
I0314 08:16:08.961642 2535102 config.py:54] PyTorch version 2.1.1 available.
I0314 08:16:09.977865 2528979 quantize_finetune_llama.py:191] layer 6 gpu 2
I0314 08:16:10.036756 2535102 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:16:18.656969 2535102 finetune.py:45] layer 5_v initial loss 3.879802898154594e-05
I0314 08:16:20.464651 2534204 finetune.py:45] layer 4_down initial loss 7.774877303745598e-05
4_v proxy err 0.01343601755797863 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0006854275707155466 tr(WHW.T) 6914.9892578125
4_k proxy err 0.0004732768575195223 tr(WHW.T) 10415.33203125
4_o proxy err 0.00786665640771389 tr(WHW.T) 5.139806270599365
4_up proxy err 0.006744324695318937 tr(WHW.T) 397.6960144042969
4_gate proxy err 0.003410682315006852 tr(WHW.T) 821.1856689453125
4_down proxy err 0.007081431336700916 tr(WHW.T) 11.562739372253418
I0314 08:16:27.177413 2535102 finetune.py:45] layer 5_q initial loss 4.1590621549403295e-05
I0314 08:16:35.834067 2535102 finetune.py:45] layer 5_k initial loss 4.446811362868175e-05
I0314 08:16:44.416081 2535102 finetune.py:45] layer 5_o initial loss 6.657058838754892e-05
I0314 08:16:59.308376 2535102 finetune.py:45] layer 5_up initial loss 8.356809848919511e-05
I0314 08:17:14.284051 2535102 finetune.py:45] layer 5_gate initial loss 9.412459621671587e-05
I0314 08:17:17.534048 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 6 in 67.13737654685974s
I0314 08:17:20.649416 2536000 config.py:54] PyTorch version 2.1.1 available.
I0314 08:17:21.689877 2528979 quantize_finetune_llama.py:191] layer 7 gpu 3
I0314 08:17:21.756718 2536000 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:17:30.262829 2535102 finetune.py:45] layer 5_down initial loss 0.00012145018990850076
I0314 08:17:30.448771 2536000 finetune.py:45] layer 6_v initial loss 4.9252139433519915e-05
5_v proxy err 0.013320306316018105 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0007558940560556948 tr(WHW.T) 6770.97509765625
5_k proxy err 0.00048388971481472254 tr(WHW.T) 10841.955078125
5_o proxy err 0.011164824478328228 tr(WHW.T) 7.947142601013184
5_up proxy err 0.0066215405240654945 tr(WHW.T) 506.6408386230469
5_gate proxy err 0.003177930600941181 tr(WHW.T) 1104.867919921875
5_down proxy err 0.007457731757313013 tr(WHW.T) 15.6494779586792
I0314 08:17:39.163458 2536000 finetune.py:45] layer 6_q initial loss 5.3948897402733564e-05
I0314 08:17:47.929476 2536000 finetune.py:45] layer 6_k initial loss 5.871877510799095e-05
I0314 08:17:56.662069 2536000 finetune.py:45] layer 6_o initial loss 8.697543671587482e-05
I0314 08:18:11.959741 2536000 finetune.py:45] layer 6_up initial loss 0.00011363164958311245
I0314 08:18:27.190337 2536000 finetune.py:45] layer 6_gate initial loss 0.00012886585318483412
I0314 08:18:30.164924 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 7 in 68.05345821380615s
I0314 08:18:33.290087 2536883 config.py:54] PyTorch version 2.1.1 available.
I0314 08:18:34.342951 2528979 quantize_finetune_llama.py:191] layer 8 gpu 0
I0314 08:18:34.421398 2536883 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:18:44.160521 2536883 finetune.py:45] layer 7_v initial loss 6.202182703418657e-05
I0314 08:18:44.667805 2536000 finetune.py:45] layer 6_down initial loss 0.00017078356177080423
6_v proxy err 0.012994624674320221 tr(WHW.T) 443.5464782714844
6_q proxy err 0.0009624926024116576 tr(WHW.T) 7576.53857421875
6_k proxy err 0.0007100931252352893 tr(WHW.T) 10409.4033203125
6_o proxy err 0.010266484692692757 tr(WHW.T) 11.564380645751953
6_up proxy err 0.006633632816374302 tr(WHW.T) 617.2608642578125
6_gate proxy err 0.00278117205016315 tr(WHW.T) 1554.7271728515625
6_down proxy err 0.007697157096117735 tr(WHW.T) 22.988168716430664
I0314 08:18:52.878101 2536883 finetune.py:45] layer 7_q initial loss 6.940764433238655e-05
I0314 08:19:01.558486 2536883 finetune.py:45] layer 7_k initial loss 7.594276394229382e-05
I0314 08:19:10.310468 2536883 finetune.py:45] layer 7_o initial loss 0.00011520290718181059
I0314 08:19:25.114580 2536883 finetune.py:45] layer 7_up initial loss 0.00015213388542179018
I0314 08:19:40.099437 2536883 finetune.py:45] layer 7_gate initial loss 0.00017309663235209882
I0314 08:19:44.962235 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 8 in 69.31114530563354s
I0314 08:19:48.137025 2537799 config.py:54] PyTorch version 2.1.1 available.
I0314 08:19:49.142312 2528979 quantize_finetune_llama.py:191] layer 9 gpu 1
I0314 08:19:49.201514 2537799 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:19:56.685271 2536883 finetune.py:45] layer 7_down initial loss 0.00022931868443265557
7_v proxy err 0.012773698195815086 tr(WHW.T) 489.9357604980469
7_q proxy err 0.0010185425635427237 tr(WHW.T) 7672.17919921875
7_k proxy err 0.0007739675347693264 tr(WHW.T) 10198.3701171875
7_o proxy err 0.011728908866643906 tr(WHW.T) 15.11335563659668
7_up proxy err 0.006542703602463007 tr(WHW.T) 735.8538818359375
7_gate proxy err 0.002710497472435236 tr(WHW.T) 1876.0390625
7_down proxy err 0.0077965655364096165 tr(WHW.T) 30.58672523498535
I0314 08:19:58.159271 2537799 finetune.py:45] layer 8_v initial loss 9.624395170249045e-05
I0314 08:20:06.931419 2537799 finetune.py:45] layer 8_q initial loss 0.00010693948570406064
I0314 08:20:15.943343 2537799 finetune.py:45] layer 8_k initial loss 0.00011571982759051025
I0314 08:20:24.953084 2537799 finetune.py:45] layer 8_o initial loss 0.00017584754095878452
I0314 08:20:40.264280 2537799 finetune.py:45] layer 8_up initial loss 0.00022038636961951852
I0314 08:20:55.317229 2537799 finetune.py:45] layer 8_gate initial loss 0.0002474926586728543
I0314 08:20:57.646632 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 9 in 68.08379697799683s
I0314 08:21:00.772468 2538739 config.py:54] PyTorch version 2.1.1 available.
I0314 08:21:01.808765 2528979 quantize_finetune_llama.py:191] layer 10 gpu 2
I0314 08:21:01.877237 2538739 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:21:10.509757 2538739 finetune.py:45] layer 9_v initial loss 0.0001068325072992593
I0314 08:21:11.517778 2537799 finetune.py:45] layer 8_down initial loss 0.0003158292092848569
8_v proxy err 0.012163190171122551 tr(WHW.T) 530.9967041015625
8_q proxy err 0.0010931880678981543 tr(WHW.T) 7228.1201171875
8_k proxy err 0.0007577102514915168 tr(WHW.T) 10639.1015625
8_o proxy err 0.013333423063158989 tr(WHW.T) 20.092191696166992
8_up proxy err 0.006041212007403374 tr(WHW.T) 866.312744140625
8_gate proxy err 0.0027768590953201056 tr(WHW.T) 1970.857177734375
8_down proxy err 0.007766516413539648 tr(WHW.T) 37.177734375
I0314 08:21:18.994263 2538739 finetune.py:45] layer 9_q initial loss 0.00011805020039901137
I0314 08:21:27.473200 2538739 finetune.py:45] layer 9_k initial loss 0.00012892279482912272
I0314 08:21:36.257433 2538739 finetune.py:45] layer 9_o initial loss 0.00020271324319764972
I0314 08:21:51.058522 2538739 finetune.py:45] layer 9_up initial loss 0.0002536402898840606
I0314 08:22:05.913182 2538739 finetune.py:45] layer 9_gate initial loss 0.00028651091270148754
I0314 08:22:09.120711 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 10 in 66.87930226325989s
I0314 08:22:12.176250 2539759 config.py:54] PyTorch version 2.1.1 available.
I0314 08:22:13.188865 2528979 quantize_finetune_llama.py:191] layer 11 gpu 3
I0314 08:22:13.261626 2539759 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:22:21.975193 2539759 finetune.py:45] layer 10_v initial loss 0.0001444895751774311
I0314 08:22:22.191175 2538739 finetune.py:45] layer 9_down initial loss 0.0003651923325378448
9_v proxy err 0.011334186419844627 tr(WHW.T) 565.0663452148438
9_q proxy err 0.0011426141718402505 tr(WHW.T) 6970.3359375
9_k proxy err 0.0007438203319907188 tr(WHW.T) 10987.3515625
9_o proxy err 0.01329864002764225 tr(WHW.T) 25.610172271728516
9_up proxy err 0.005832672119140625 tr(WHW.T) 970.8984375
9_gate proxy err 0.0027602892369031906 tr(WHW.T) 2132.69384765625
9_down proxy err 0.007788482587784529 tr(WHW.T) 42.99482727050781
I0314 08:22:30.549575 2539759 finetune.py:45] layer 10_q initial loss 0.0001577369257574901
I0314 08:22:39.152194 2539759 finetune.py:45] layer 10_k initial loss 0.00017246004426851869
I0314 08:22:47.873135 2539759 finetune.py:45] layer 10_o initial loss 0.00027424967265687883
I0314 08:23:02.802603 2539759 finetune.py:45] layer 10_up initial loss 0.00033210020046681166
I0314 08:23:17.904634 2539759 finetune.py:45] layer 10_gate initial loss 0.0003711247118189931
I0314 08:23:20.999625 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 11 in 67.37239575386047s
I0314 08:23:24.210092 2540956 config.py:54] PyTorch version 2.1.1 available.
I0314 08:23:25.291878 2528979 quantize_finetune_llama.py:191] layer 12 gpu 0
I0314 08:23:25.361055 2540956 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:23:34.891331 2540956 finetune.py:45] layer 11_v initial loss 0.00016044019139371812
I0314 08:23:34.984912 2539759 finetune.py:45] layer 10_down initial loss 0.00046231623855419457
10_v proxy err 0.011367769911885262 tr(WHW.T) 578.807373046875
10_q proxy err 0.0011795292375609279 tr(WHW.T) 6915.87109375
10_k proxy err 0.0007664550794288516 tr(WHW.T) 10996.2431640625
10_o proxy err 0.013606153428554535 tr(WHW.T) 35.184165954589844
10_up proxy err 0.005512564908713102 tr(WHW.T) 1080.198486328125
10_gate proxy err 0.0027272948063910007 tr(WHW.T) 2260.88330078125
10_down proxy err 0.007433515507727861 tr(WHW.T) 52.33584976196289
I0314 08:23:43.230635 2540956 finetune.py:45] layer 11_q initial loss 0.00017249377560801804
I0314 08:23:51.649849 2540956 finetune.py:45] layer 11_k initial loss 0.0001848760002758354
I0314 08:24:00.027513 2540956 finetune.py:45] layer 11_o initial loss 0.0002884229179471731
I0314 08:24:14.768105 2540956 finetune.py:45] layer 11_up initial loss 0.0003545271174516529
I0314 08:24:29.543804 2540956 finetune.py:45] layer 11_gate initial loss 0.00039877460221759975
I0314 08:24:35.013563 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 12 in 68.41327166557312s
I0314 08:24:38.231019 2542137 config.py:54] PyTorch version 2.1.1 available.
I0314 08:24:39.343851 2528979 quantize_finetune_llama.py:191] layer 13 gpu 1
I0314 08:24:39.413850 2542137 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:24:45.557294 2540956 finetune.py:45] layer 11_down initial loss 0.0004997985670343041
11_v proxy err 0.01084186788648367 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0013368603540584445 tr(WHW.T) 7027.10986328125
11_k proxy err 0.0009098804439418018 tr(WHW.T) 10511.23046875
11_o proxy err 0.013768082484602928 tr(WHW.T) 36.654052734375
11_up proxy err 0.005616991780698299 tr(WHW.T) 1139.6044921875
11_gate proxy err 0.002754107816144824 tr(WHW.T) 2392.716552734375
11_down proxy err 0.007626493461430073 tr(WHW.T) 56.13530731201172
I0314 08:24:48.450269 2542137 finetune.py:45] layer 12_v initial loss 0.00016447367670480162
I0314 08:24:57.022141 2542137 finetune.py:45] layer 12_q initial loss 0.00018091949459630996
I0314 08:25:05.735200 2542137 finetune.py:45] layer 12_k initial loss 0.00020008735009469092
I0314 08:25:14.561031 2542137 finetune.py:45] layer 12_o initial loss 0.0003165849484503269
I0314 08:25:29.593244 2542137 finetune.py:45] layer 12_up initial loss 0.0003890030784532428
I0314 08:25:44.645548 2542137 finetune.py:45] layer 12_gate initial loss 0.00044071191223338246
I0314 08:25:46.233816 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 13 in 66.49039363861084s
I0314 08:25:49.142709 2543130 config.py:54] PyTorch version 2.1.1 available.
I0314 08:25:50.155428 2528979 quantize_finetune_llama.py:191] layer 14 gpu 2
I0314 08:25:50.226370 2543130 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:25:58.977942 2543130 finetune.py:45] layer 13_v initial loss 0.00016060991038102657
I0314 08:26:00.906209 2542137 finetune.py:45] layer 12_down initial loss 0.000556230777874589
12_v proxy err 0.011078215204179287 tr(WHW.T) 703.318603515625
12_q proxy err 0.0013393890112638474 tr(WHW.T) 7045.6435546875
12_k proxy err 0.0008821230148896575 tr(WHW.T) 10893.65625
12_o proxy err 0.013993863016366959 tr(WHW.T) 39.29071044921875
12_up proxy err 0.00556483119726181 tr(WHW.T) 1228.298583984375
12_gate proxy err 0.0029398654587566853 tr(WHW.T) 2381.994873046875
12_down proxy err 0.007617837283760309 tr(WHW.T) 64.17745208740234
I0314 08:26:07.696567 2543130 finetune.py:45] layer 13_q initial loss 0.00017624163592699915
I0314 08:26:16.000062 2543130 finetune.py:45] layer 13_k initial loss 0.00019230176985729486
I0314 08:26:24.553701 2543130 finetune.py:45] layer 13_o initial loss 0.0003112518461421132
I0314 08:26:39.209131 2543130 finetune.py:45] layer 13_up initial loss 0.00039888022001832724
I0314 08:26:53.907090 2543130 finetune.py:45] layer 13_gate initial loss 0.00046174402814358473
I0314 08:26:57.289923 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 14 in 66.7363932132721s
I0314 08:27:00.319027 2544037 config.py:54] PyTorch version 2.1.1 available.
I0314 08:27:01.418321 2528979 quantize_finetune_llama.py:191] layer 15 gpu 3
I0314 08:27:01.488496 2544037 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:27:09.776420 2543130 finetune.py:45] layer 13_down initial loss 0.000605703447945416
I0314 08:27:10.070020 2544037 finetune.py:45] layer 14_v initial loss 0.00020221328304614872
13_v proxy err 0.010946877300739288 tr(WHW.T) 714.5677490234375
13_q proxy err 0.00134132185485214 tr(WHW.T) 6956.03564453125
13_k proxy err 0.0009217669139616191 tr(WHW.T) 10426.6318359375
13_o proxy err 0.012496464885771275 tr(WHW.T) 45.8377571105957
13_up proxy err 0.0053681801073253155 tr(WHW.T) 1367.6221923828125
13_gate proxy err 0.0028757185209542513 tr(WHW.T) 2601.504638671875
13_down proxy err 0.007649940438568592 tr(WHW.T) 79.3589096069336
I0314 08:27:18.410924 2544037 finetune.py:45] layer 14_q initial loss 0.00022078138135839254
I0314 08:27:26.787886 2544037 finetune.py:45] layer 14_k initial loss 0.00023871494340710342
I0314 08:27:35.390397 2544037 finetune.py:45] layer 14_o initial loss 0.00038829317782074213
I0314 08:27:50.101434 2544037 finetune.py:45] layer 14_up initial loss 0.0004871138371527195
I0314 08:28:05.206407 2544037 finetune.py:45] layer 14_gate initial loss 0.0005608579376712441
I0314 08:28:07.822161 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 15 in 65.98937582969666s
I0314 08:28:10.903434 2544958 config.py:54] PyTorch version 2.1.1 available.
I0314 08:28:11.941818 2528979 quantize_finetune_llama.py:191] layer 16 gpu 0
I0314 08:28:12.009717 2544958 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:28:21.224911 2544958 finetune.py:45] layer 15_v initial loss 0.00020102613780181855
I0314 08:28:21.914729 2544037 finetune.py:45] layer 14_down initial loss 0.0007290366920642555
14_v proxy err 0.011596912518143654 tr(WHW.T) 706.1612548828125
14_q proxy err 0.0013803212204948068 tr(WHW.T) 7077.06103515625
14_k proxy err 0.0008862394606694579 tr(WHW.T) 11295.16796875
14_o proxy err 0.013765125535428524 tr(WHW.T) 50.921180725097656
14_up proxy err 0.00542820431292057 tr(WHW.T) 1464.7159423828125
14_gate proxy err 0.0030197736341506243 tr(WHW.T) 2682.584716796875
14_down proxy err 0.00781492330133915 tr(WHW.T) 90.28684997558594
I0314 08:28:29.723683 2544958 finetune.py:45] layer 15_q initial loss 0.00022048718528822064
I0314 08:28:37.962077 2544958 finetune.py:45] layer 15_k initial loss 0.00023676114506088197
I0314 08:28:46.472346 2544958 finetune.py:45] layer 15_o initial loss 0.0003877712006215006
I0314 08:29:01.330619 2544958 finetune.py:45] layer 15_up initial loss 0.0005078533431515098
I0314 08:29:16.633655 2544958 finetune.py:45] layer 15_gate initial loss 0.0005999862332828343
I0314 08:29:21.538524 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 16 in 68.29611349105835s
I0314 08:29:24.974621 2545972 config.py:54] PyTorch version 2.1.1 available.
I0314 08:29:25.999552 2528979 quantize_finetune_llama.py:191] layer 17 gpu 1
I0314 08:29:26.065493 2545972 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:29:33.149635 2544958 finetune.py:45] layer 15_down initial loss 0.0008117870893329382
15_v proxy err 0.010664517059922218 tr(WHW.T) 762.7275390625
15_q proxy err 0.0013271596981212497 tr(WHW.T) 7252.0009765625
15_k proxy err 0.0008919494575820863 tr(WHW.T) 11072.3974609375
15_o proxy err 0.011726339347660542 tr(WHW.T) 59.61664962768555
15_up proxy err 0.0052902791649103165 tr(WHW.T) 1641.0228271484375
15_gate proxy err 0.0030452420469373465 tr(WHW.T) 2905.140380859375
15_down proxy err 0.0078103188425302505 tr(WHW.T) 114.09001922607422
I0314 08:29:35.219729 2545972 finetune.py:45] layer 16_v initial loss 0.00023965525906533003
I0314 08:29:44.014421 2545972 finetune.py:45] layer 16_q initial loss 0.0002648557419888675
I0314 08:29:53.083497 2545972 finetune.py:45] layer 16_k initial loss 0.0002864128618966788
I0314 08:30:02.205206 2545972 finetune.py:45] layer 16_o initial loss 0.000469105871161446
I0314 08:30:17.360495 2545972 finetune.py:45] layer 16_up initial loss 0.0006214909953996539
I0314 08:30:32.681193 2545972 finetune.py:45] layer 16_gate initial loss 0.0007401803159154952
I0314 08:30:34.090314 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 17 in 67.67265558242798s
I0314 08:30:37.212244 2546926 config.py:54] PyTorch version 2.1.1 available.
I0314 08:30:38.637054 2528979 quantize_finetune_llama.py:191] layer 18 gpu 2
I0314 08:30:38.704658 2546926 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:30:47.509166 2546926 finetune.py:45] layer 17_v initial loss 0.00019913735741283745
I0314 08:30:49.335523 2545972 finetune.py:45] layer 16_down initial loss 0.001024890225380659
16_v proxy err 0.010882602073252201 tr(WHW.T) 780.7407836914062
16_q proxy err 0.001368974568322301 tr(WHW.T) 7193.3974609375
16_k proxy err 0.0008636934217065573 tr(WHW.T) 11630.361328125
16_o proxy err 0.009226889349520206 tr(WHW.T) 88.22785186767578
16_up proxy err 0.005173148587346077 tr(WHW.T) 1890.9385986328125
16_gate proxy err 0.0029592299833893776 tr(WHW.T) 3369.859130859375
16_down proxy err 0.007909773848950863 tr(WHW.T) 152.0294952392578
I0314 08:30:56.229286 2546926 finetune.py:45] layer 17_q initial loss 0.00022134650498628616
I0314 08:31:05.187019 2546926 finetune.py:45] layer 17_k initial loss 0.00024487587506882846
I0314 08:31:13.904122 2546926 finetune.py:45] layer 17_o initial loss 0.00038084786501713097
I0314 08:31:28.959987 2546926 finetune.py:45] layer 17_up initial loss 0.0005489821778610349
I0314 08:31:44.000945 2546926 finetune.py:45] layer 17_gate initial loss 0.0006785505102016032
I0314 08:31:47.337116 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 18 in 68.2565541267395s
I0314 08:31:50.356696 2547934 config.py:54] PyTorch version 2.1.1 available.
I0314 08:31:51.375696 2528979 quantize_finetune_llama.py:191] layer 19 gpu 3
I0314 08:31:51.445174 2547934 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:32:00.072802 2546926 finetune.py:45] layer 17_down initial loss 0.000985518330708146
I0314 08:32:00.141971 2547934 finetune.py:45] layer 18_v initial loss 0.0002037433732766658
17_v proxy err 0.010175209492444992 tr(WHW.T) 845.7654418945312
17_q proxy err 0.0013873324496671557 tr(WHW.T) 7163.2734375
17_k proxy err 0.0009509673691354692 tr(WHW.T) 10697.431640625
17_o proxy err 0.01007506251335144 tr(WHW.T) 58.14826965332031
17_up proxy err 0.005633306689560413 tr(WHW.T) 1921.07861328125
17_gate proxy err 0.003103044116869569 tr(WHW.T) 3571.31640625
17_down proxy err 0.00783925037831068 tr(WHW.T) 165.43495178222656
I0314 08:32:08.474674 2547934 finetune.py:45] layer 18_q initial loss 0.00023398826306220144
I0314 08:32:16.975882 2547934 finetune.py:45] layer 18_k initial loss 0.0002767925907392055
I0314 08:32:25.462815 2547934 finetune.py:45] layer 18_o initial loss 0.0004276045074220747
I0314 08:32:40.325904 2547934 finetune.py:45] layer 18_up initial loss 0.000631419534329325
I0314 08:32:56.987016 2547934 finetune.py:45] layer 18_gate initial loss 0.0007849046960473061
I0314 08:33:00.842732 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 19 in 69.05404210090637s
I0314 08:33:04.233355 2549148 config.py:54] PyTorch version 2.1.1 available.
I0314 08:33:05.256400 2528979 quantize_finetune_llama.py:191] layer 20 gpu 0
I0314 08:33:05.325104 2549148 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:33:18.039566 2549148 finetune.py:45] layer 19_v initial loss 0.00019248497847002
I0314 08:33:18.327484 2547934 finetune.py:45] layer 18_down initial loss 0.0011576212709769607
18_v proxy err 0.009383522905409336 tr(WHW.T) 1003.7705078125
18_q proxy err 0.0014289438258856535 tr(WHW.T) 7510.48046875
18_k proxy err 0.0010459802579134703 tr(WHW.T) 10462.6650390625
18_o proxy err 0.008875546045601368 tr(WHW.T) 69.96558380126953
18_up proxy err 0.005988034885376692 tr(WHW.T) 2023.183837890625
18_gate proxy err 0.003286142600700259 tr(WHW.T) 3783.076416015625
18_down proxy err 0.007898966781795025 tr(WHW.T) 198.52699279785156
I0314 08:33:27.777957 2549148 finetune.py:45] layer 19_q initial loss 0.0002226017531938851
I0314 08:33:36.980177 2549148 finetune.py:45] layer 19_k initial loss 0.0002488127793185413
I0314 08:33:45.819074 2549148 finetune.py:45] layer 19_o initial loss 0.0003822777362074703
I0314 08:34:02.250518 2549148 finetune.py:45] layer 19_up initial loss 0.0006090695387683809
I0314 08:34:18.831707 2549148 finetune.py:45] layer 19_gate initial loss 0.0007880802149884403
I0314 08:34:19.546421 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 20 in 72.94617629051208s
I0314 08:34:22.671404 2550272 config.py:54] PyTorch version 2.1.1 available.
I0314 08:34:23.726908 2528979 quantize_finetune_llama.py:191] layer 21 gpu 1
I0314 08:34:23.792983 2550272 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:34:33.036292 2550272 finetune.py:45] layer 20_v initial loss 0.00021787417063023895
I0314 08:34:37.295752 2549148 finetune.py:45] layer 19_down initial loss 0.0011956386733800173
19_v proxy err 0.009197059087455273 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.0015187053941190243 tr(WHW.T) 6944.4130859375
19_k proxy err 0.0010138159850612283 tr(WHW.T) 10548.4892578125
19_o proxy err 0.008766187354922295 tr(WHW.T) 62.291683197021484
19_up proxy err 0.006020944565534592 tr(WHW.T) 2149.330322265625
19_gate proxy err 0.003602025331929326 tr(WHW.T) 3687.5126953125
19_down proxy err 0.007679217029362917 tr(WHW.T) 222.93177795410156
I0314 08:34:42.585767 2550272 finetune.py:45] layer 20_q initial loss 0.00024542826577089727
I0314 08:34:51.810750 2550272 finetune.py:45] layer 20_k initial loss 0.0002697682357393205
I0314 08:35:00.797866 2550272 finetune.py:45] layer 20_o initial loss 0.00042812703759409487
