I0403 06:23:53.982975 3473157 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:23:53.983081 3473157 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:23:53.983128 3473157 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:23:54.342605 3473157 config.py:54] PyTorch version 2.6.0 available.
W0403 06:23:54.553832 3473157 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:23:55.180470 3473157 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  5.17it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  5.31it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  5.34it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  5.37it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  5.39it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.43it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.38it/s]
I0403 06:23:56.527714 3473157 quantize_finetune_llama.py:152] loaded model
I0403 06:23:56.896504 3473157 quantize_finetune_llama.py:190] loaded compression model
I0403 06:24:12.054409 3473157 quantize_finetune_llama.py:194] loaded dataset and devset
I0403 06:24:15.543737 3473157 quantize_finetune_llama.py:214] layer 0 gpu 0
I0403 06:24:18.969362 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 0 in 3.2588765621185303s
Use train scale and shift
tensor(2.2655e-07, device='cuda:0') tensor(0.0204, device='cuda:0')
tensor(0.0204, device='cuda:0') tensor(2.2655e-07, device='cuda:0')
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0403 06:24:32.119115 3473890 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:24:32.119207 3473890 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:24:32.119247 3473890 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:24:32.446761 3473890 config.py:54] PyTorch version 2.6.0 available.
W0403 06:24:32.638720 3473890 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:24:33.264133 3473890 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:24:33.268253 3473157 quantize_finetune_llama.py:214] layer 1 gpu 1
I0403 06:24:33.544708 3473890 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:24:36.450128 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 1 in 2.9889259338378906s
I0403 06:24:40.237554 3474038 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:24:40.237656 3474038 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:24:40.237701 3474038 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:24:40.603399 3474038 config.py:54] PyTorch version 2.6.0 available.
W0403 06:24:40.816296 3474038 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:24:41.493178 3474038 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:24:41.497297 3473157 quantize_finetune_llama.py:214] layer 2 gpu 0
I0403 06:24:41.713626 3474038 data_utils.py:336] using 256 training seqs, 128 validation seqs
0_v proxy err 0.0005071480991318822 err 0.49288561940193176 tr(WHW.T) 971.8771362304688
bpp_loss 3.7105582745862193
0_q proxy err 1.6056615095294546e-06 err 1.0218837261199951 tr(WHW.T) 636425.375
bpp_loss 3.714486271957867
0_k proxy err 2.3992135993466945e-06 err 0.9569603204727173 tr(WHW.T) 398864.15625
bpp_loss 3.812672947649844
0_o proxy err 3.554056820576079e-05 err 0.5660170912742615 tr(WHW.T) 15925.943359375
bpp_loss 3.554694456222933
0_up proxy err 0.0020842503290623426 err 50.26625061035156 tr(WHW.T) 24117.18359375
bpp_loss 3.5907997856306477
0_gate proxy err 0.0014265754725784063 err 50.53986358642578 tr(WHW.T) 35427.40234375
bpp_loss 3.6056829790563083
0_down proxy err 0.0009526885696686804 err 34.10276412963867 tr(WHW.T) 35796.33984375
bpp_loss 3.795893954156443
1_v proxy err 0.0022475856821984053 err 1.4767179489135742 tr(WHW.T) 657.0241088867188
bpp_loss 3.6470985650084913
1_q proxy err 1.2414297088980675e-05 err 2.4259703159332275 tr(WHW.T) 195417.453125
bpp_loss 4.56219525821507
1_k proxy err 1.1963101314904634e-05 err 2.4440085887908936 tr(WHW.T) 204295.5625
bpp_loss 4.556654030224308
1_o proxy err 0.0007053679437376559 err 2.849426031112671 tr(WHW.T) 4039.630859375
bpp_loss 3.5354675304261036
1_up proxy err 0.0029238758143037558 err 67.8615493774414 tr(WHW.T) 23209.44921875
bpp_loss 3.6156760427321113
1_gate proxy err 0.0014533473877236247 err 68.24055480957031 tr(WHW.T) 46954.0546875
bpp_loss 3.6808243423353795
1_down proxy err 3.960260983149055e-06 err 0.16149714589118958 tr(WHW.T) 40779.421875
bpp_loss 3.970152006029736
I0403 06:26:38.464634 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 2 in 0.9775950908660889s
I0403 06:26:42.229362 3475836 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:26:42.229474 3475836 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:26:42.229518 3475836 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:26:42.631408 3475836 config.py:54] PyTorch version 2.6.0 available.
W0403 06:26:42.869055 3475836 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:26:43.556432 3475836 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:26:43.560563 3473157 quantize_finetune_llama.py:214] layer 3 gpu 1
I0403 06:26:43.899380 3475836 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:26:45.054102 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 3 in 0.9773592948913574s
I0403 06:26:48.789590 3475974 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:26:48.789700 3475974 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:26:48.789744 3475974 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:26:49.165346 3475974 config.py:54] PyTorch version 2.6.0 available.
W0403 06:26:49.376757 3475974 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:26:50.044421 3475974 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:26:50.048471 3473157 quantize_finetune_llama.py:214] layer 4 gpu 0
I0403 06:26:50.302196 3475974 data_utils.py:336] using 256 training seqs, 128 validation seqs
2_v proxy err 0.0033847216982394457 err 9.409065246582031 tr(WHW.T) 2779.86376953125
bpp_loss 3.722228854196146
2_q proxy err 6.24091990175657e-05 err 9.954778671264648 tr(WHW.T) 159508.203125
bpp_loss 4.5342517596436664
2_k proxy err 4.779157825396396e-05 err 10.03627872467041 tr(WHW.T) 210000.984375
bpp_loss 4.643460011458956
2_o proxy err 0.0030006496235728264 err 15.90521240234375 tr(WHW.T) 5300.58984375
bpp_loss 3.6033895418513566
2_up proxy err 0.0042375680059194565 err 84.46985626220703 tr(WHW.T) 19933.5703125
bpp_loss 3.623690806849058
2_gate proxy err 0.002681223675608635 err 84.88946533203125 tr(WHW.T) 31660.716796875
bpp_loss 3.7035434285915176
2_down proxy err 0.005342478863894939 err 92.33731079101562 tr(WHW.T) 17283.607421875
bpp_loss 3.6936908961208754
3_v proxy err 0.005864117760211229 err 17.46868133544922 tr(WHW.T) 2978.910400390625
bpp_loss 3.610948656045366
3_q proxy err 0.0002437173534417525 err 18.564939498901367 tr(WHW.T) 76174.0546875
bpp_loss 4.34907433448825
3_k proxy err 0.00017543356807436794 err 18.659217834472656 tr(WHW.T) 106360.5859375
bpp_loss 4.4282198653090745
3_o proxy err 0.003286548890173435 err 17.27272605895996 tr(WHW.T) 5255.58154296875
bpp_loss 3.5982385630486533
3_up proxy err 0.005325764883309603 err 93.02947998046875 tr(WHW.T) 17467.81640625
bpp_loss 3.63247595398232
3_gate proxy err 0.0031847155187278986 err 93.62104797363281 tr(WHW.T) 29396.990234375
bpp_loss 3.7217223232741965
3_down proxy err 0.005418091081082821 err 91.60688018798828 tr(WHW.T) 16907.59375
bpp_loss 3.703980966644405
I0403 06:28:45.849930 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 4 in 0.9312241077423096s
I0403 06:28:49.613858 3477728 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:28:49.613958 3477728 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:28:49.614003 3477728 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:28:49.965654 3477728 config.py:54] PyTorch version 2.6.0 available.
W0403 06:28:50.171470 3477728 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:28:50.795420 3477728 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:28:50.799291 3473157 quantize_finetune_llama.py:214] layer 5 gpu 1
I0403 06:28:51.019504 3477728 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:28:52.156514 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 5 in 0.902015209197998s
I0403 06:28:55.765246 3477875 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:28:55.765345 3477875 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:28:55.765387 3477875 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:28:56.100835 3477875 config.py:54] PyTorch version 2.6.0 available.
W0403 06:28:56.291772 3477875 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:28:56.839556 3477875 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:28:56.843047 3473157 quantize_finetune_llama.py:214] layer 6 gpu 0
I0403 06:28:56.998496 3477875 data_utils.py:336] using 256 training seqs, 128 validation seqs
4_v proxy err 0.005652549210935831 err 17.509084701538086 tr(WHW.T) 3097.555419921875
bpp_loss 3.6570024099200964
4_q proxy err 0.00023354124277830124 err 18.390548706054688 tr(WHW.T) 78746.46875
bpp_loss 4.448369863093831
4_k proxy err 0.00015593957505188882 err 18.50390625 tr(WHW.T) 118660.75
bpp_loss 4.4876207957277074
4_o proxy err 0.004117622971534729 err 21.975658416748047 tr(WHW.T) 5336.9765625
bpp_loss 3.57628798665246
4_up proxy err 0.005241123493760824 err 92.58280181884766 tr(WHW.T) 17664.685546875
bpp_loss 3.62991990686156
4_gate proxy err 0.0025530557613819838 err 93.41156005859375 tr(WHW.T) 36588.140625
bpp_loss 3.7526135811972066
4_down proxy err 0.005325777921825647 err 89.6377944946289 tr(WHW.T) 16830.9296875
bpp_loss 3.6934388006149335
5_v proxy err 0.0060855248011648655 err 19.270252227783203 tr(WHW.T) 3166.572021484375
bpp_loss 3.6626245509833097
5_q proxy err 0.00027918556588701904 err 20.250978469848633 tr(WHW.T) 72535.90625
bpp_loss 4.447594163240865
5_k proxy err 0.00017552863573655486 err 20.397708892822266 tr(WHW.T) 116207.296875
bpp_loss 4.531291990191676
5_o proxy err 0.0042434572242200375 err 15.995915412902832 tr(WHW.T) 3769.5478515625
bpp_loss 3.6495972959091887
5_up proxy err 0.005169591400772333 err 93.13868713378906 tr(WHW.T) 18016.64453125
bpp_loss 3.6283644112043603
5_gate proxy err 0.002383560175076127 err 93.9529800415039 tr(WHW.T) 39417.078125
bpp_loss 3.758994036115879
5_down proxy err 0.005900864489376545 err 94.13838195800781 tr(WHW.T) 15953.3203125
bpp_loss 3.680406982100807
I0403 06:31:09.470100 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 6 in 0.9978067874908447s
I0403 06:31:13.610991 3479876 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:31:13.611088 3479876 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:31:13.611131 3479876 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:31:13.957426 3479876 config.py:54] PyTorch version 2.6.0 available.
W0403 06:31:14.164738 3479876 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:31:14.867001 3479876 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:31:14.870987 3473157 quantize_finetune_llama.py:214] layer 7 gpu 1
I0403 06:31:15.127655 3479876 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:31:16.489603 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 7 in 1.1417782306671143s
I0403 06:31:20.561476 3480027 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:31:20.561583 3480027 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:31:20.561628 3480027 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:31:20.914283 3480027 config.py:54] PyTorch version 2.6.0 available.
W0403 06:31:21.121010 3480027 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:31:21.702071 3480027 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:31:21.705679 3473157 quantize_finetune_llama.py:214] layer 8 gpu 0
I0403 06:31:21.944466 3480027 data_utils.py:336] using 256 training seqs, 128 validation seqs
6_v proxy err 0.006638781167566776 err 21.116859436035156 tr(WHW.T) 3180.833740234375
bpp_loss 3.5905327723012306
6_q proxy err 0.00041212092037312686 err 22.56802749633789 tr(WHW.T) 54760.6953125
bpp_loss 4.306084943236783
6_k proxy err 0.00030034201336093247 err 22.606779098510742 tr(WHW.T) 75270.1171875
bpp_loss 4.344688956975006
6_o proxy err 0.004758561495691538 err 19.23118019104004 tr(WHW.T) 4041.38525390625
bpp_loss 3.5779229915933684
6_up proxy err 0.005265823099762201 err 94.6357650756836 tr(WHW.T) 17971.6953125
bpp_loss 3.625292515425488
6_gate proxy err 0.002103680744767189 err 95.57212829589844 tr(WHW.T) 45430.91015625
bpp_loss 3.7818669331108414
6_down proxy err 0.005950870458036661 err 91.8502197265625 tr(WHW.T) 15434.75390625
bpp_loss 3.6782846639236046
7_v proxy err 0.006619752384722233 err 21.51913833618164 tr(WHW.T) 3250.746826171875
bpp_loss 3.5950966163072735
7_q proxy err 0.0004486491670832038 err 23.015972137451172 tr(WHW.T) 51300.6015625
bpp_loss 4.299308213288896
7_k proxy err 0.0003379441623110324 err 23.056245803833008 tr(WHW.T) 68225.015625
bpp_loss 4.307038872037083
7_o proxy err 0.005211353302001953 err 18.387697219848633 tr(WHW.T) 3528.39208984375
bpp_loss 3.589391314657405
7_up proxy err 0.00511215953156352 err 93.19503784179688 tr(WHW.T) 18230.072265625
bpp_loss 3.6321762705784897
7_gate proxy err 0.0020181862637400627 err 94.14537811279297 tr(WHW.T) 46648.5078125
bpp_loss 3.783962057253649
7_down proxy err 0.005979348439723253 err 91.12751770019531 tr(WHW.T) 15240.3759765625
bpp_loss 3.678019920775537
I0403 06:33:19.493169 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 8 in 0.9579486846923828s
I0403 06:33:23.341420 3481899 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:33:23.341517 3481899 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:33:23.341559 3481899 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:33:23.680833 3481899 config.py:54] PyTorch version 2.6.0 available.
W0403 06:33:23.874491 3481899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:33:24.451359 3481899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:33:24.454866 3473157 quantize_finetune_llama.py:214] layer 9 gpu 1
I0403 06:33:24.714355 3481899 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:33:25.791768 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 9 in 0.8805825710296631s
I0403 06:33:29.471307 3482054 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:33:29.471393 3482054 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:33:29.471431 3482054 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:33:29.795143 3482054 config.py:54] PyTorch version 2.6.0 available.
W0403 06:33:29.988361 3482054 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:33:30.571810 3482054 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:33:30.575359 3473157 quantize_finetune_llama.py:214] layer 10 gpu 0
I0403 06:33:30.833312 3482054 data_utils.py:336] using 256 training seqs, 128 validation seqs
8_v proxy err 0.006035943981260061 err 20.9471492767334 tr(WHW.T) 3470.401611328125
bpp_loss 3.6213718043873087
8_q proxy err 0.00046318559907376766 err 22.044187545776367 tr(WHW.T) 47592.55859375
bpp_loss 4.330782153410837
8_k proxy err 0.00031570010469295084 err 22.130556106567383 tr(WHW.T) 70099.9296875
bpp_loss 4.339584401110187
8_o proxy err 0.00545909721404314 err 17.0252628326416 tr(WHW.T) 3118.695556640625
bpp_loss 3.6172626629122533
8_up proxy err 0.004660898819565773 err 92.53076171875 tr(WHW.T) 19852.556640625
bpp_loss 3.647478371771962
8_gate proxy err 0.002060710685327649 err 93.37458038330078 tr(WHW.T) 45311.83203125
bpp_loss 3.764303456472103
8_down proxy err 0.005939798429608345 err 91.01183319091797 tr(WHW.T) 15322.3779296875
bpp_loss 3.686791442009772
9_v proxy err 0.006086715497076511 err 22.37759780883789 tr(WHW.T) 3676.465087890625
bpp_loss 3.6221235007978976
9_q proxy err 0.000513295759446919 err 23.445512771606445 tr(WHW.T) 45676.421875
bpp_loss 4.322708342107944
9_k proxy err 0.00032728712540119886 err 23.582683563232422 tr(WHW.T) 72055.03125
bpp_loss 4.368705756496638
9_o proxy err 0.005704085808247328 err 17.95991325378418 tr(WHW.T) 3148.60498046875
bpp_loss 3.615692840947304
9_up proxy err 0.004501952324062586 err 92.765625 tr(WHW.T) 20605.642578125
bpp_loss 3.6556055246917314
9_gate proxy err 0.002061084844172001 err 93.58688354492188 tr(WHW.T) 45406.61328125
bpp_loss 3.752290158337632
9_down proxy err 0.006024913862347603 err 92.326171875 tr(WHW.T) 15324.0654296875
bpp_loss 3.6884599866692063
I0403 06:35:23.772445 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 10 in 0.882910966873169s
I0403 06:35:27.346751 3483638 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:35:27.346859 3483638 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:35:27.346902 3483638 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:35:27.681049 3483638 config.py:54] PyTorch version 2.6.0 available.
W0403 06:35:27.868348 3483638 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:35:28.421990 3483638 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:35:28.425477 3473157 quantize_finetune_llama.py:214] layer 11 gpu 1
I0403 06:35:28.620898 3483638 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:35:29.903329 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 11 in 1.0632591247558594s
I0403 06:35:33.469990 3483752 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:35:33.470082 3483752 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:35:33.470123 3483752 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:35:33.799350 3483752 config.py:54] PyTorch version 2.6.0 available.
W0403 06:35:34.028113 3483752 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:35:34.841000 3483752 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:35:34.845228 3473157 quantize_finetune_llama.py:214] layer 12 gpu 0
I0403 06:35:35.676500 3483752 data_utils.py:336] using 256 training seqs, 128 validation seqs
10_v proxy err 0.00618102727457881 err 22.570772171020508 tr(WHW.T) 3651.62158203125
bpp_loss 3.6153741244925186
10_q proxy err 0.0005380077054724097 err 23.63765525817871 tr(WHW.T) 43935.53515625
bpp_loss 4.321647903649136
10_k proxy err 0.00034048574161715806 err 23.804332733154297 tr(WHW.T) 69912.859375
bpp_loss 4.378392411977984
10_o proxy err 0.006151053123176098 err 18.81166648864746 tr(WHW.T) 3058.283935546875
bpp_loss 3.598894655762706
10_up proxy err 0.004232069477438927 err 92.70108795166016 tr(WHW.T) 21904.43359375
bpp_loss 3.6674309983676254
10_gate proxy err 0.002031666459515691 err 93.42224884033203 tr(WHW.T) 45983.0625
bpp_loss 3.7484255832468354
10_down proxy err 0.0056419032625854015 err 90.89746856689453 tr(WHW.T) 16111.1357421875
bpp_loss 3.7012987232138945
11_v proxy err 0.006153583526611328 err 23.94248390197754 tr(WHW.T) 3890.81982421875
bpp_loss 3.6322145767044276
11_q proxy err 0.0006583970971405506 err 25.06723403930664 tr(WHW.T) 38073.125
bpp_loss 4.208197815110907
11_k proxy err 0.0004414949507918209 err 25.1641845703125 tr(WHW.T) 56997.671875
bpp_loss 4.199251789366826
11_o proxy err 0.006298525258898735 err 19.23706817626953 tr(WHW.T) 3054.2177734375
bpp_loss 3.6236930392915383
11_up proxy err 0.004420653451234102 err 95.25758361816406 tr(WHW.T) 21548.3046875
bpp_loss 3.6754160574529062
11_gate proxy err 0.0021158887539058924 err 96.03326416015625 tr(WHW.T) 45386.7265625
bpp_loss 3.7418366442759368
11_down proxy err 0.005955974571406841 err 93.69214630126953 tr(WHW.T) 15730.783203125
bpp_loss 3.7007858560442233
I0403 06:37:36.480111 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 12 in 1.07771897315979s
I0403 06:37:40.497495 3485542 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:37:40.497587 3485542 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:37:40.497629 3485542 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:37:40.832767 3485542 config.py:54] PyTorch version 2.6.0 available.
W0403 06:37:41.020401 3485542 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:37:41.706108 3485542 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:37:41.710023 3473157 quantize_finetune_llama.py:214] layer 13 gpu 1
I0403 06:37:42.114559 3485542 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:37:43.214188 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 13 in 0.9956488609313965s
I0403 06:37:47.138921 3485678 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:37:47.139013 3485678 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:37:47.139053 3485678 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:37:47.477685 3485678 config.py:54] PyTorch version 2.6.0 available.
W0403 06:37:47.666974 3485678 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:37:48.263978 3485678 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:37:48.268003 3473157 quantize_finetune_llama.py:214] layer 14 gpu 0
I0403 06:37:48.482348 3485678 data_utils.py:336] using 256 training seqs, 128 validation seqs
12_v proxy err 0.006485123187303543 err 24.6911563873291 tr(WHW.T) 3807.353515625
bpp_loss 3.6189551893621683
12_q proxy err 0.0006720732781104743 err 25.8116455078125 tr(WHW.T) 38406.0
bpp_loss 4.244094633846544
12_k proxy err 0.00043716432992368937 err 25.980348587036133 tr(WHW.T) 59429.25
bpp_loss 4.29940679250285
12_o proxy err 0.006645782385021448 err 19.91819953918457 tr(WHW.T) 2997.118896484375
bpp_loss 3.6082040507462807
12_up proxy err 0.0044149127788841724 err 96.27269744873047 tr(WHW.T) 21806.251953125
bpp_loss 3.6849558443524115
12_gate proxy err 0.0022861934266984463 err 96.97040557861328 tr(WHW.T) 42415.66015625
bpp_loss 3.7328866393586924
12_down proxy err 0.005921362899243832 err 93.46129608154297 tr(WHW.T) 15783.7470703125
bpp_loss 3.713320988122114
13_v proxy err 0.006753294728696346 err 26.229581832885742 tr(WHW.T) 3883.96826171875
bpp_loss 3.633467825187836
13_q proxy err 0.0007176226936280727 err 27.33554458618164 tr(WHW.T) 38091.8046875
bpp_loss 4.212489381548949
13_k proxy err 0.0004809910024050623 err 27.48467445373535 tr(WHW.T) 57141.765625
bpp_loss 4.242422227049246
13_o proxy err 0.006051134783774614 err 20.527063369750977 tr(WHW.T) 3392.266845703125
bpp_loss 3.6293082279735245
13_up proxy err 0.004190113395452499 err 95.06561279296875 tr(WHW.T) 22688.076171875
bpp_loss 3.6971293194002883
13_gate proxy err 0.002210550010204315 err 95.6870346069336 tr(WHW.T) 43286.52734375
bpp_loss 3.7286660763933215
13_down proxy err 0.005735129117965698 err 90.33153533935547 tr(WHW.T) 15750.5673828125
bpp_loss 3.7321899699610332
I0403 06:39:45.999155 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 14 in 1.0210623741149902s
I0403 06:39:49.908586 3487416 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:39:49.908675 3487416 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:39:49.908713 3487416 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:39:50.231492 3487416 config.py:54] PyTorch version 2.6.0 available.
W0403 06:39:50.424097 3487416 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:39:51.070586 3487416 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:39:51.074815 3473157 quantize_finetune_llama.py:214] layer 15 gpu 1
I0403 06:39:51.355537 3487416 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:39:52.466946 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 15 in 0.9391739368438721s
I0403 06:39:56.144799 3487560 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:39:56.144884 3487560 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:39:56.144920 3487560 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:39:56.466505 3487560 config.py:54] PyTorch version 2.6.0 available.
W0403 06:39:56.651909 3487560 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:39:57.410219 3487560 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:39:57.413895 3473157 quantize_finetune_llama.py:214] layer 16 gpu 0
I0403 06:39:57.647920 3487560 data_utils.py:336] using 256 training seqs, 128 validation seqs
14_v proxy err 0.007027259562164545 err 25.632417678833008 tr(WHW.T) 3647.569580078125
bpp_loss 3.619388063554652
14_q proxy err 0.0007255348027683794 err 26.725706100463867 tr(WHW.T) 36835.87109375
bpp_loss 4.20975117944181
14_k proxy err 0.00045760590001009405 err 26.927616119384766 tr(WHW.T) 58844.5546875
bpp_loss 4.239497164031491
14_o proxy err 0.006827938836067915 err 20.92002296447754 tr(WHW.T) 3063.885498046875
bpp_loss 3.603976917162072
14_up proxy err 0.00433965353295207 err 97.43682098388672 tr(WHW.T) 22452.671875
bpp_loss 3.696331066274366
14_gate proxy err 0.0023766045924276114 err 98.01338958740234 tr(WHW.T) 41240.93359375
bpp_loss 3.724123509297537
14_down proxy err 0.005888551473617554 err 90.73956298828125 tr(WHW.T) 15409.48828125
bpp_loss 3.7348930590547793
15_v proxy err 0.006349216215312481 err 25.41623878479004 tr(WHW.T) 4003.05126953125
bpp_loss 3.6579703696188517
15_q proxy err 0.0006886706687510014 err 26.412824630737305 tr(WHW.T) 38353.34765625
bpp_loss 4.1922825636575
15_k proxy err 0.00045390884042717516 err 26.599489212036133 tr(WHW.T) 58600.94921875
bpp_loss 4.2514927259180695
15_o proxy err 0.005998877342790365 err 21.76618194580078 tr(WHW.T) 3628.3759765625
bpp_loss 3.629195307032205
15_up proxy err 0.004155724309384823 err 95.85741424560547 tr(WHW.T) 23066.35546875
bpp_loss 3.7033206183723237
15_gate proxy err 0.00235305936075747 err 96.34904479980469 tr(WHW.T) 40946.2890625
bpp_loss 3.731524098404618
15_down proxy err 0.005668101366609335 err 87.45336151123047 tr(WHW.T) 15429.0400390625
bpp_loss 3.7484284967703876
I0403 06:41:53.015907 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 16 in 0.8778531551361084s
I0403 06:41:56.710993 3489282 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:41:56.711081 3489282 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:41:56.711120 3489282 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:41:57.032409 3489282 config.py:54] PyTorch version 2.6.0 available.
W0403 06:41:57.228405 3489282 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:41:57.865944 3489282 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:41:57.869505 3473157 quantize_finetune_llama.py:214] layer 17 gpu 1
I0403 06:41:58.120497 3489282 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:41:59.355418 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 17 in 0.9854874610900879s
I0403 06:42:02.822788 3489432 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:42:02.822889 3489432 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:42:02.822931 3489432 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:42:03.149711 3489432 config.py:54] PyTorch version 2.6.0 available.
W0403 06:42:03.336764 3489432 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:42:04.102991 3489432 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:42:04.106860 3473157 quantize_finetune_llama.py:214] layer 18 gpu 0
I0403 06:42:04.304044 3489432 data_utils.py:336] using 256 training seqs, 128 validation seqs
16_v proxy err 0.006582519505172968 err 26.25603675842285 tr(WHW.T) 3988.751708984375
bpp_loss 3.682110835565254
16_q proxy err 0.0007344853365793824 err 27.22342872619629 tr(WHW.T) 37064.6328125
bpp_loss 4.1669901384739205
16_k proxy err 0.0004570604651235044 err 27.413801193237305 tr(WHW.T) 59978.5
bpp_loss 4.210019223741256
16_o proxy err 0.005317760165780783 err 24.968080520629883 tr(WHW.T) 4695.22509765625
bpp_loss 3.6375493866507895
16_up proxy err 0.0040741488337516785 err 96.16459655761719 tr(WHW.T) 23603.60546875
bpp_loss 3.6987544600526956
16_gate proxy err 0.0022921927738934755 err 96.6726303100586 tr(WHW.T) 42174.73828125
bpp_loss 3.7368189228135487
16_down proxy err 0.005797722842544317 err 88.29060363769531 tr(WHW.T) 15228.4970703125
bpp_loss 3.7464922284924014
17_v proxy err 0.006529784295707941 err 27.84174156188965 tr(WHW.T) 4263.80712890625
bpp_loss 3.6603853084961884
17_q proxy err 0.0007928202976472676 err 28.852907180786133 tr(WHW.T) 36392.74609375
bpp_loss 4.134224871871993
17_k proxy err 0.0005337853799574077 err 29.031890869140625 tr(WHW.T) 54388.6953125
bpp_loss 4.169259671994951
17_o proxy err 0.0060829902067780495 err 26.144227981567383 tr(WHW.T) 4297.923828125
bpp_loss 3.6429962735273875
17_up proxy err 0.004645714536309242 err 100.41860961914062 tr(WHW.T) 21615.3203125
bpp_loss 3.6909450746552888
17_gate proxy err 0.0025050591211766005 err 100.95488739013672 tr(WHW.T) 40300.40234375
bpp_loss 3.7458531880794568
17_down proxy err 0.006106007378548384 err 93.7481460571289 tr(WHW.T) 15353.4287109375
bpp_loss 3.7334454090356135
I0403 06:44:00.345862 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 18 in 0.9223034381866455s
I0403 06:44:03.891560 3491132 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:44:03.891646 3491132 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:44:03.891685 3491132 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:44:04.215481 3491132 config.py:54] PyTorch version 2.6.0 available.
W0403 06:44:04.404519 3491132 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:44:05.041692 3491132 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:44:05.045512 3473157 quantize_finetune_llama.py:214] layer 19 gpu 1
I0403 06:44:05.230895 3491132 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:44:06.238901 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 19 in 0.8109822273254395s
I0403 06:44:09.854768 3491328 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:44:09.854853 3491328 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:44:09.854894 3491328 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:44:10.200320 3491328 config.py:54] PyTorch version 2.6.0 available.
W0403 06:44:10.417896 3491328 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:44:11.135358 3491328 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:44:11.139157 3473157 quantize_finetune_llama.py:214] layer 20 gpu 0
I0403 06:44:11.407840 3491328 data_utils.py:336] using 256 training seqs, 128 validation seqs
18_v proxy err 0.006349893286824226 err 29.673612594604492 tr(WHW.T) 4673.08837890625
bpp_loss 3.691901594749652
18_q proxy err 0.0008682750631123781 err 30.596099853515625 tr(WHW.T) 35237.796875
bpp_loss 4.082390502328053
18_k proxy err 0.0006256704800762236 err 30.732437133789062 tr(WHW.T) 49119.20703125
bpp_loss 4.117228781164158
18_o proxy err 0.005334292072802782 err 26.262073516845703 tr(WHW.T) 4923.25390625
bpp_loss 3.686223505414091
18_up proxy err 0.004980505909770727 err 101.10401153564453 tr(WHW.T) 20299.947265625
bpp_loss 3.6865136887618277
18_gate proxy err 0.0026700133457779884 err 101.65345001220703 tr(WHW.T) 38072.26171875
bpp_loss 3.756843287411124
18_down proxy err 0.006023218855261803 err 91.95089721679688 tr(WHW.T) 15266.072265625
bpp_loss 3.742359634228917
19_v proxy err 0.006228031124919653 err 29.825454711914062 tr(WHW.T) 4788.90576171875
bpp_loss 3.6982521982863545
19_q proxy err 0.0009340598480775952 err 30.711671829223633 tr(WHW.T) 32879.76953125
bpp_loss 4.05885166284861
19_k proxy err 0.0006177059840410948 err 30.875886917114258 tr(WHW.T) 49984.76171875
bpp_loss 4.086433360178489
19_o proxy err 0.005720886867493391 err 28.634122848510742 tr(WHW.T) 5005.189453125
bpp_loss 3.6805001331376843
19_up proxy err 0.0050603412091732025 err 102.0803451538086 tr(WHW.T) 20172.62109375
bpp_loss 3.6859015089481377
19_gate proxy err 0.002958830678835511 err 102.66756439208984 tr(WHW.T) 34698.6953125
bpp_loss 3.7611369581589864
19_down proxy err 0.005932693835347891 err 93.12728881835938 tr(WHW.T) 15697.3017578125
bpp_loss 3.7437791976601233
I0403 06:46:11.342954 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 20 in 0.7869973182678223s
I0403 06:46:14.800653 3493025 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:46:14.800742 3493025 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:46:14.800780 3493025 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:46:15.121869 3493025 config.py:54] PyTorch version 2.6.0 available.
W0403 06:46:15.308170 3493025 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:46:15.860808 3493025 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:46:15.864574 3473157 quantize_finetune_llama.py:214] layer 21 gpu 1
I0403 06:46:16.145301 3493025 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:46:17.090764 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 21 in 0.7929794788360596s
I0403 06:46:20.729189 3493216 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:46:20.729290 3493216 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:46:20.729336 3493216 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:46:21.111026 3493216 config.py:54] PyTorch version 2.6.0 available.
W0403 06:46:21.305291 3493216 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:46:22.009037 3493216 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:46:22.013013 3473157 quantize_finetune_llama.py:214] layer 22 gpu 0
I0403 06:46:22.235918 3493216 data_utils.py:336] using 256 training seqs, 128 validation seqs
20_v proxy err 0.006555372383445501 err 30.45513916015625 tr(WHW.T) 4645.8291015625
bpp_loss 3.708146772463806
20_q proxy err 0.0009276045602746308 err 31.3671875 tr(WHW.T) 33815.2578125
bpp_loss 4.063420228892937
20_k proxy err 0.0006415989482775331 err 31.534584045410156 tr(WHW.T) 49149.9921875
bpp_loss 4.089880050159991
20_o proxy err 0.004069791175425053 err 27.7576847076416 tr(WHW.T) 6820.419921875
bpp_loss 3.7059851762023754
20_up proxy err 0.004938384983688593 err 101.74596405029297 tr(WHW.T) 20603.083984375
bpp_loss 3.6843018972769728
20_gate proxy err 0.0028780444990843534 err 102.19930267333984 tr(WHW.T) 35509.98046875
bpp_loss 3.767989103357459
20_down proxy err 0.005787703674286604 err 91.46363830566406 tr(WHW.T) 15803.095703125
bpp_loss 3.7492630295455456
21_v proxy err 0.006414582021534443 err 31.192548751831055 tr(WHW.T) 4862.75634765625
bpp_loss 3.7330629170755856
21_q proxy err 0.001057177665643394 err 31.9918270111084 tr(WHW.T) 30261.54296875
bpp_loss 4.018446052388754
21_k proxy err 0.0007511824951507151 err 32.12689971923828 tr(WHW.T) 42768.43359375
bpp_loss 4.032226391136646
21_o proxy err 0.0051973662339150906 err 33.25926971435547 tr(WHW.T) 6399.25439453125
bpp_loss 3.6918715839856304
21_up proxy err 0.005297946743667126 err 103.64311981201172 tr(WHW.T) 19562.884765625
bpp_loss 3.681938461006381
21_gate proxy err 0.0031316340900957584 err 104.1313705444336 tr(WHW.T) 33251.44921875
bpp_loss 3.776875517309405
21_down proxy err 0.006064333952963352 err 95.76936340332031 tr(WHW.T) 15792.2314453125
bpp_loss 3.7416325239248054
I0403 06:48:17.925689 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 22 in 0.9199578762054443s
I0403 06:48:21.294191 3494730 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:48:21.294278 3494730 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:48:21.294317 3494730 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:48:21.626255 3494730 config.py:54] PyTorch version 2.6.0 available.
W0403 06:48:21.813218 3494730 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:48:22.359689 3494730 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:48:22.363262 3473157 quantize_finetune_llama.py:214] layer 23 gpu 1
I0403 06:48:22.606672 3494730 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:48:23.773591 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 23 in 1.0060486793518066s
I0403 06:48:27.236562 3494863 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:48:27.236644 3494863 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:48:27.236681 3494863 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:48:27.560730 3494863 config.py:54] PyTorch version 2.6.0 available.
W0403 06:48:27.749190 3494863 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:48:28.310723 3494863 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:48:28.314517 3473157 quantize_finetune_llama.py:214] layer 24 gpu 0
I0403 06:48:28.711886 3494863 data_utils.py:336] using 256 training seqs, 128 validation seqs
22_v proxy err 0.006140707526355982 err 31.390880584716797 tr(WHW.T) 5111.93212890625
bpp_loss 3.7396032117540017
22_q proxy err 0.0010031339479610324 err 32.20402145385742 tr(WHW.T) 32103.412109375
bpp_loss 4.055924556741957
22_k proxy err 0.0007354066474363208 err 32.335636138916016 tr(WHW.T) 43969.734375
bpp_loss 4.073747069109231
22_o proxy err 0.00398415420204401 err 30.389089584350586 tr(WHW.T) 7627.48828125
bpp_loss 3.722191760316491
22_up proxy err 0.005379361566156149 err 104.57475280761719 tr(WHW.T) 19439.994140625
bpp_loss 3.6808287225836933
22_gate proxy err 0.0032090479508042336 err 105.0359878540039 tr(WHW.T) 32731.19921875
bpp_loss 3.7844647960087587
22_down proxy err 0.006129714660346508 err 97.30679321289062 tr(WHW.T) 15874.603515625
bpp_loss 3.7381412690115527
23_v proxy err 0.005851406138390303 err 33.15716552734375 tr(WHW.T) 5666.529296875
bpp_loss 3.7786463229567744
23_q proxy err 0.0012009842321276665 err 33.89384078979492 tr(WHW.T) 28221.720703125
bpp_loss 4.02985789522063
23_k proxy err 0.0008858597138896585 err 33.99520492553711 tr(WHW.T) 38375.3828125
bpp_loss 4.040268436772749
23_o proxy err 0.005172531586140394 err 32.82172393798828 tr(WHW.T) 6345.388671875
bpp_loss 3.766481444821693
23_up proxy err 0.005658573471009731 err 106.12049865722656 tr(WHW.T) 18753.931640625
bpp_loss 3.6867490683703923
23_gate proxy err 0.0035042730160057545 err 106.53187561035156 tr(WHW.T) 30400.564453125
bpp_loss 3.784227876815685
23_down proxy err 0.006296962033957243 err 99.59761810302734 tr(WHW.T) 15816.7724609375
bpp_loss 3.7446529990451976
I0403 06:50:17.493935 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 24 in 0.806157112121582s
I0403 06:50:21.032738 3496225 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:50:21.032826 3496225 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:50:21.032866 3496225 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:50:21.361296 3496225 config.py:54] PyTorch version 2.6.0 available.
W0403 06:50:21.550618 3496225 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:50:22.106058 3496225 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:50:22.109566 3473157 quantize_finetune_llama.py:214] layer 25 gpu 1
I0403 06:50:22.333967 3496225 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:50:23.350414 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 25 in 0.8082573413848877s
I0403 06:50:26.809161 3496374 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:50:26.809253 3496374 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:50:26.809292 3496374 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:50:27.135718 3496374 config.py:54] PyTorch version 2.6.0 available.
W0403 06:50:27.322514 3496374 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:50:27.879923 3496374 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:50:27.883504 3473157 quantize_finetune_llama.py:214] layer 26 gpu 0
I0403 06:50:28.071196 3496374 data_utils.py:336] using 256 training seqs, 128 validation seqs
24_v proxy err 0.006081868428736925 err 32.384307861328125 tr(WHW.T) 5324.72998046875
bpp_loss 3.779354603670072
24_q proxy err 0.0012250652071088552 err 33.090694427490234 tr(WHW.T) 27011.373046875
bpp_loss 3.9950919662369415
24_k proxy err 0.0008357003098353744 err 33.22446060180664 tr(WHW.T) 39756.4296875
bpp_loss 3.998965857492294
24_o proxy err 0.003834669478237629 err 30.82543182373047 tr(WHW.T) 8038.615234375
bpp_loss 3.75153416802641
24_up proxy err 0.005796349607408047 err 107.30780029296875 tr(WHW.T) 18512.99609375
bpp_loss 3.6899614532499814
24_gate proxy err 0.003571160137653351 err 107.74784851074219 tr(WHW.T) 30171.666015625
bpp_loss 3.7871866214067436
24_down proxy err 0.006353895645588636 err 100.1135482788086 tr(WHW.T) 15756.2470703125
bpp_loss 3.750944021591094
25_v proxy err 0.005855954717844725 err 34.71121597290039 tr(WHW.T) 5927.5078125
bpp_loss 3.8051207956159487
25_q proxy err 0.0014115660451352596 err 35.347900390625 tr(WHW.T) 25041.619140625
bpp_loss 3.9889386983122677
25_k proxy err 0.0010536814806982875 err 35.43843460083008 tr(WHW.T) 33632.96875
bpp_loss 3.991865966178011
25_o proxy err 0.004968073219060898 err 33.67890167236328 tr(WHW.T) 6779.06689453125
bpp_loss 3.7690825617755763
25_up proxy err 0.005790012888610363 err 107.83336639404297 tr(WHW.T) 18624.029296875
bpp_loss 3.695810892034409
25_gate proxy err 0.0034865722991526127 err 108.27092742919922 tr(WHW.T) 31053.6875
bpp_loss 3.790412575006485
25_down proxy err 0.006048364099115133 err 96.03237915039062 tr(WHW.T) 15877.4140625
bpp_loss 3.773328658128374
I0403 06:52:17.771982 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 26 in 0.7898273468017578s
I0403 06:52:21.137372 3497723 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:52:21.137459 3497723 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:52:21.137501 3497723 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:52:21.463739 3497723 config.py:54] PyTorch version 2.6.0 available.
W0403 06:52:21.650382 3497723 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:52:22.193575 3497723 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:52:22.197105 3473157 quantize_finetune_llama.py:214] layer 27 gpu 1
I0403 06:52:22.424667 3497723 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:52:23.571576 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 27 in 0.9503023624420166s
I0403 06:52:26.975783 3497873 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:52:26.975894 3497873 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:52:26.975937 3497873 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:52:27.297571 3497873 config.py:54] PyTorch version 2.6.0 available.
W0403 06:52:27.482074 3497873 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:52:28.114262 3497873 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:52:28.117830 3473157 quantize_finetune_llama.py:214] layer 28 gpu 0
I0403 06:52:28.383798 3497873 data_utils.py:336] using 256 training seqs, 128 validation seqs
26_v proxy err 0.00572625920176506 err 33.903682708740234 tr(WHW.T) 5920.73828125
bpp_loss 3.8353895366890356
26_q proxy err 0.0012932579265907407 err 34.5449104309082 tr(WHW.T) 26711.5390625
bpp_loss 3.971982288348954
26_k proxy err 0.0009230193682014942 err 34.64424133300781 tr(WHW.T) 37533.6015625
bpp_loss 3.9808269948116504
26_o proxy err 0.0026583296712487936 err 26.227235794067383 tr(WHW.T) 9866.05859375
bpp_loss 3.8588019839371555
26_up proxy err 0.005426645278930664 err 107.75316619873047 tr(WHW.T) 19856.3125
bpp_loss 3.699975551560868
26_gate proxy err 0.0032332539558410645 err 108.16635131835938 tr(WHW.T) 33454.33203125
bpp_loss 3.7946039154952347
26_down proxy err 0.006008877418935299 err 92.6901626586914 tr(WHW.T) 15425.537109375
bpp_loss 3.785957936154202
27_v proxy err 0.005531373433768749 err 36.162986755371094 tr(WHW.T) 6537.79541015625
bpp_loss 3.814122114854399
27_q proxy err 0.0013091772561892867 err 36.83967971801758 tr(WHW.T) 28139.56640625
bpp_loss 4.00855532969581
27_k proxy err 0.0009491074597463012 err 36.89971923828125 tr(WHW.T) 38878.3359375
bpp_loss 4.023520900285803
27_o proxy err 0.0038089598529040813 err 27.67703628540039 tr(WHW.T) 7266.2978515625
bpp_loss 3.8754253632505424
27_up proxy err 0.004896839614957571 err 106.86532592773438 tr(WHW.T) 21823.326171875
bpp_loss 3.706621371729429
27_gate proxy err 0.0030213920399546623 err 107.23222351074219 tr(WHW.T) 35491.0
bpp_loss 3.7965245556172937
27_down proxy err 0.005599598865956068 err 84.660400390625 tr(WHW.T) 15119.01171875
bpp_loss 3.8134031553297887
I0403 06:54:23.768965 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 28 in 0.7802236080169678s
I0403 06:54:27.487190 3499492 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:54:27.487290 3499492 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:54:27.487333 3499492 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:54:27.855653 3499492 config.py:54] PyTorch version 2.6.0 available.
W0403 06:54:28.066364 3499492 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:54:28.683284 3499492 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:54:28.687094 3473157 quantize_finetune_llama.py:214] layer 29 gpu 1
I0403 06:54:28.851770 3499492 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:54:30.256994 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 29 in 1.1202633380889893s
I0403 06:54:33.921541 3499675 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:54:33.921653 3499675 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:54:33.921696 3499675 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:54:34.311174 3499675 config.py:54] PyTorch version 2.6.0 available.
W0403 06:54:34.526279 3499675 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:54:35.169907 3499675 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:54:35.173838 3473157 quantize_finetune_llama.py:214] layer 30 gpu 0
I0403 06:54:35.335306 3499675 data_utils.py:336] using 256 training seqs, 128 validation seqs
28_v proxy err 0.005070813931524754 err 35.88872528076172 tr(WHW.T) 7077.5078125
bpp_loss 3.8567791075329296
28_q proxy err 0.0013511250726878643 err 36.48396682739258 tr(WHW.T) 27002.65625
bpp_loss 3.9604307634290308
28_k proxy err 0.0009810251649469137 err 36.536842346191406 tr(WHW.T) 37243.53125
bpp_loss 3.9780940762721
28_o proxy err 0.0028956448659300804 err 25.724943161010742 tr(WHW.T) 8884.01171875
bpp_loss 3.9039305458427407
28_up proxy err 0.004035699646919966 err 105.57862854003906 tr(WHW.T) 26161.169921875
bpp_loss 3.7194226837262163
28_gate proxy err 0.002879493637010455 err 105.9482650756836 tr(WHW.T) 36794.0625
bpp_loss 3.7911111372847888
28_down proxy err 0.004923217464238405 err 73.77178955078125 tr(WHW.T) 14984.466796875
bpp_loss 3.8496724418689343
29_v proxy err 0.005241531878709793 err 35.025821685791016 tr(WHW.T) 6682.36328125
bpp_loss 3.8670206252718344
29_q proxy err 0.001319729839451611 err 35.64069747924805 tr(WHW.T) 27006.056640625
bpp_loss 3.92291680368362
29_k proxy err 0.0009049386717379093 err 35.73543167114258 tr(WHW.T) 39489.33984375
bpp_loss 3.9329977310844697
29_o proxy err 0.0029507458675652742 err 31.275100708007812 tr(WHW.T) 10599.048828125
bpp_loss 3.877806570439134
29_up proxy err 0.003168398980051279 err 104.18998718261719 tr(WHW.T) 32884.11328125
bpp_loss 3.731286317976408
29_gate proxy err 0.002615870675072074 err 104.48567962646484 tr(WHW.T) 39942.984375
bpp_loss 3.795111767947674
29_down proxy err 0.004233982414007187 err 62.37733840942383 tr(WHW.T) 14732.544921875
bpp_loss 3.8833130754876968
I0403 06:56:29.049011 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 30 in 0.8566854000091553s
I0403 06:56:32.829737 3501365 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:56:32.829836 3501365 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:56:32.829876 3501365 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:56:33.188277 3501365 config.py:54] PyTorch version 2.6.0 available.
W0403 06:56:33.389124 3501365 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:56:33.985800 3501365 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:56:33.989274 3473157 quantize_finetune_llama.py:214] layer 31 gpu 1
I0403 06:56:34.168190 3501365 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 06:56:35.311498 3473157 quantize_finetune_llama.py:245] computed original embedding for layer 31 in 0.8740749359130859s
I0403 06:56:39.045795 3501572 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:56:39.045903 3501572 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:56:39.045951 3501572 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:56:39.438747 3501572 config.py:54] PyTorch version 2.6.0 available.
W0403 06:56:39.661545 3501572 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 06:56:40.318578 3501572 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 06:56:40.635769 3501572 data_utils.py:336] using 256 training seqs, 128 validation seqs
30_v proxy err 0.00441804388538003 err 36.261207580566406 tr(WHW.T) 8207.525390625
bpp_loss 3.881613624165766
30_q proxy err 0.0012874819803982973 err 36.7456169128418 tr(WHW.T) 28540.68359375
bpp_loss 3.913459357514512
30_k proxy err 0.0009576698066666722 err 36.817848205566406 tr(WHW.T) 38445.2421875
bpp_loss 3.9331791829899885
30_o proxy err 0.002440528478473425 err 24.80336570739746 tr(WHW.T) 10163.1123046875
bpp_loss 3.9615711004589684
30_up proxy err 0.001870410400442779 err 100.76641082763672 tr(WHW.T) 53873.95703125
bpp_loss 3.7527168432120668
30_gate proxy err 0.001708118594251573 err 101.06560516357422 tr(WHW.T) 59167.79296875
bpp_loss 3.827105678704589
30_down proxy err 0.0011453187325969338 err 29.597896575927734 tr(WHW.T) 25842.498046875
bpp_loss 3.964065497481199
31_v proxy err 0.005058980546891689 err 34.099239349365234 tr(WHW.T) 6740.33837890625
bpp_loss 3.755952459468972
31_q proxy err 0.0009519506711512804 err 34.93468475341797 tr(WHW.T) 36698.0
bpp_loss 3.9297774535953067
31_k proxy err 0.0006392202922143042 err 35.02531433105469 tr(WHW.T) 54793.80859375
bpp_loss 3.985869661788456
31_o proxy err 0.0014111206401139498 err 18.51615333557129 tr(WHW.T) 13121.5947265625
bpp_loss 3.9092201283201575
31_up proxy err 0.0009774715872481465 err 93.62056732177734 tr(WHW.T) 95778.3046875
bpp_loss 3.805416975083739
31_gate proxy err 0.0009620466153137386 err 93.84807586669922 tr(WHW.T) 97550.4453125
bpp_loss 3.8926461931577947
31_down proxy err 0.0003157783066853881 err 11.692224502563477 tr(WHW.T) 37026.6875
bpp_loss 4.023648490262932
I0403 06:58:48.881965 3503365 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 06:58:48.882114 3503365 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 06:58:48.882156 3503365 utils.py:162] NumExpr defaulting to 16 threads.
I0403 06:58:49.255989 3503365 config.py:54] PyTorch version 2.6.0 available.
W0403 06:58:49.475246 3503365 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0403 06:58:49.591339 3503365 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  5.68it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  6.46it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  6.78it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.06it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  7.02it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.09it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.90it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  6.55it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  6.53it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  6.96it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  6.88it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  7.08it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.00it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.92it/s]
I0403 06:58:52.659827 3503365 hfize_llama.py:161] loaded layer 0
I0403 06:58:53.680505 3503365 hfize_llama.py:161] loaded layer 1
I0403 06:58:54.659548 3503365 hfize_llama.py:161] loaded layer 2
I0403 06:58:55.757896 3503365 hfize_llama.py:161] loaded layer 3
I0403 06:58:56.820124 3503365 hfize_llama.py:161] loaded layer 4
I0403 06:58:58.070526 3503365 hfize_llama.py:161] loaded layer 5
I0403 06:58:59.099765 3503365 hfize_llama.py:161] loaded layer 6
I0403 06:59:00.177271 3503365 hfize_llama.py:161] loaded layer 7
I0403 06:59:01.176974 3503365 hfize_llama.py:161] loaded layer 8
I0403 06:59:02.349087 3503365 hfize_llama.py:161] loaded layer 9
I0403 06:59:03.469323 3503365 hfize_llama.py:161] loaded layer 10
I0403 06:59:04.570040 3503365 hfize_llama.py:161] loaded layer 11
I0403 06:59:05.603862 3503365 hfize_llama.py:161] loaded layer 12
I0403 06:59:06.709157 3503365 hfize_llama.py:161] loaded layer 13
I0403 06:59:07.649746 3503365 hfize_llama.py:161] loaded layer 14
I0403 06:59:08.760520 3503365 hfize_llama.py:161] loaded layer 15
I0403 06:59:09.744449 3503365 hfize_llama.py:161] loaded layer 16
I0403 06:59:10.741350 3503365 hfize_llama.py:161] loaded layer 17
I0403 06:59:11.744243 3503365 hfize_llama.py:161] loaded layer 18
I0403 06:59:12.681824 3503365 hfize_llama.py:161] loaded layer 19
I0403 06:59:13.593877 3503365 hfize_llama.py:161] loaded layer 20
I0403 06:59:14.355678 3503365 hfize_llama.py:161] loaded layer 21
I0403 06:59:15.081622 3503365 hfize_llama.py:161] loaded layer 22
I0403 06:59:15.793145 3503365 hfize_llama.py:161] loaded layer 23
I0403 06:59:16.493252 3503365 hfize_llama.py:161] loaded layer 24
I0403 06:59:17.267750 3503365 hfize_llama.py:161] loaded layer 25
I0403 06:59:18.136535 3503365 hfize_llama.py:161] loaded layer 26
I0403 06:59:19.060909 3503365 hfize_llama.py:161] loaded layer 27
I0403 06:59:19.965185 3503365 hfize_llama.py:161] loaded layer 28
I0403 06:59:20.881904 3503365 hfize_llama.py:161] loaded layer 29
I0403 06:59:21.804463 3503365 hfize_llama.py:161] loaded layer 30
I0403 06:59:22.721616 3503365 hfize_llama.py:161] loaded layer 31
I0403 06:59:22.721755 3503365 hfize_llama.py:165] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:06,  1.21s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.01s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.06it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:01,  1.01it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.13it/s]
I0403 07:00:09.217198 3503365 hfize_llama.py:175] successfully loaded hfized model
I0403 07:00:13.868767 3504833 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:00:13.868858 3504833 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:00:13.868898 3504833 utils.py:162] NumExpr defaulting to 16 threads.
W0403 07:00:14.224990 3504833 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0403 07:00:14.812220 3504833 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.18s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.07s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.02s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.06s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.03it/s]
I0403 07:00:20.764432 3504833 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/166 [00:00<?, ?it/s]avg_loss = 1.3833701610565186:   0%|          | 0/166 [00:01<?, ?it/s]avg_loss = 1.3833701610565186:   1%|          | 1/166 [00:01<04:32,  1.65s/it]avg_loss = 1.6485084295272827:   1%|          | 1/166 [00:02<04:32,  1.65s/it]avg_loss = 1.6485084295272827:   1%|          | 2/166 [00:02<03:46,  1.38s/it]avg_loss = 1.8147560755411785:   1%|          | 2/166 [00:04<03:46,  1.38s/it]avg_loss = 1.8147560755411785:   2%|▏         | 3/166 [00:04<03:30,  1.29s/it]avg_loss = 1.8477373719215393:   2%|▏         | 3/166 [00:05<03:30,  1.29s/it]avg_loss = 1.8477373719215393:   2%|▏         | 4/166 [00:05<03:23,  1.25s/it]avg_loss = 1.7796557426452637:   2%|▏         | 4/166 [00:06<03:23,  1.25s/it]avg_loss = 1.7796557426452637:   3%|▎         | 5/166 [00:06<03:18,  1.23s/it]avg_loss = 1.7558928926785786:   3%|▎         | 5/166 [00:07<03:18,  1.23s/it]avg_loss = 1.7558928926785786:   4%|▎         | 6/166 [00:07<03:15,  1.22s/it]avg_loss = 1.6946745940617152:   4%|▎         | 6/166 [00:08<03:15,  1.22s/it]avg_loss = 1.6946745940617152:   4%|▍         | 7/166 [00:08<03:13,  1.22s/it]avg_loss = 1.637118712067604:   4%|▍         | 7/166 [00:10<03:13,  1.22s/it] avg_loss = 1.637118712067604:   5%|▍         | 8/166 [00:10<03:11,  1.21s/it]avg_loss = 1.6317463053597345:   5%|▍         | 8/166 [00:11<03:11,  1.21s/it]avg_loss = 1.6317463053597345:   5%|▌         | 9/166 [00:11<03:10,  1.21s/it]avg_loss = 1.6366621494293212:   5%|▌         | 9/166 [00:12<03:10,  1.21s/it]avg_loss = 1.6366621494293212:   6%|▌         | 10/166 [00:12<03:08,  1.21s/it]avg_loss = 1.6524782505902378:   6%|▌         | 10/166 [00:13<03:08,  1.21s/it]avg_loss = 1.6524782505902378:   7%|▋         | 11/166 [00:13<03:07,  1.21s/it]avg_loss = 1.6621005137761433:   7%|▋         | 11/166 [00:14<03:07,  1.21s/it]avg_loss = 1.6621005137761433:   7%|▋         | 12/166 [00:14<03:06,  1.21s/it]avg_loss = 1.657439846258897:   7%|▋         | 12/166 [00:16<03:06,  1.21s/it] avg_loss = 1.657439846258897:   8%|▊         | 13/166 [00:16<03:05,  1.21s/it]avg_loss = 1.6695542080061776:   8%|▊         | 13/166 [00:17<03:05,  1.21s/it]avg_loss = 1.6695542080061776:   8%|▊         | 14/166 [00:17<03:04,  1.21s/it]avg_loss = 1.687657634417216:   8%|▊         | 14/166 [00:18<03:04,  1.21s/it] avg_loss = 1.687657634417216:   9%|▉         | 15/166 [00:18<03:03,  1.22s/it]avg_loss = 1.707331769168377:   9%|▉         | 15/166 [00:19<03:03,  1.22s/it]avg_loss = 1.707331769168377:  10%|▉         | 16/166 [00:19<03:02,  1.22s/it]avg_loss = 1.7211428670322193:  10%|▉         | 16/166 [00:20<03:02,  1.22s/it]avg_loss = 1.7211428670322193:  10%|█         | 17/166 [00:20<03:01,  1.22s/it]avg_loss = 1.7359838684399922:  10%|█         | 17/166 [00:22<03:01,  1.22s/it]avg_loss = 1.7359838684399922:  11%|█         | 18/166 [00:22<03:01,  1.22s/it]avg_loss = 1.756004578188846:  11%|█         | 18/166 [00:23<03:01,  1.22s/it] avg_loss = 1.756004578188846:  11%|█▏        | 19/166 [00:23<03:00,  1.23s/it]avg_loss = 1.762763613462448:  11%|█▏        | 19/166 [00:24<03:00,  1.23s/it]avg_loss = 1.762763613462448:  12%|█▏        | 20/166 [00:24<02:59,  1.23s/it]avg_loss = 1.7635711147671653:  12%|█▏        | 20/166 [00:25<02:59,  1.23s/it]avg_loss = 1.7635711147671653:  13%|█▎        | 21/166 [00:25<02:58,  1.23s/it]avg_loss = 1.7540499784729697:  13%|█▎        | 21/166 [00:27<02:58,  1.23s/it]avg_loss = 1.7540499784729697:  13%|█▎        | 22/166 [00:27<02:57,  1.23s/it]avg_loss = 1.7437268858370574:  13%|█▎        | 22/166 [00:28<02:57,  1.23s/it]avg_loss = 1.7437268858370574:  14%|█▍        | 23/166 [00:28<02:56,  1.23s/it]avg_loss = 1.7511131713787715:  14%|█▍        | 23/166 [00:29<02:56,  1.23s/it]avg_loss = 1.7511131713787715:  14%|█▍        | 24/166 [00:29<02:55,  1.24s/it]avg_loss = 1.7586623811721802:  14%|█▍        | 24/166 [00:30<02:55,  1.24s/it]avg_loss = 1.7586623811721802:  15%|█▌        | 25/166 [00:30<02:54,  1.24s/it]avg_loss = 1.7629154370381281:  15%|█▌        | 25/166 [00:32<02:54,  1.24s/it]avg_loss = 1.7629154370381281:  16%|█▌        | 26/166 [00:32<02:53,  1.24s/it]avg_loss = 1.769403170656275:  16%|█▌        | 26/166 [00:33<02:53,  1.24s/it] avg_loss = 1.769403170656275:  16%|█▋        | 27/166 [00:33<02:52,  1.24s/it]avg_loss = 1.7725305642400468:  16%|█▋        | 27/166 [00:34<02:52,  1.24s/it]avg_loss = 1.7725305642400468:  17%|█▋        | 28/166 [00:34<02:51,  1.24s/it]avg_loss = 1.7824307392383445:  17%|█▋        | 28/166 [00:35<02:51,  1.24s/it]avg_loss = 1.7824307392383445:  17%|█▋        | 29/166 [00:35<02:50,  1.24s/it]avg_loss = 1.7825742046038309:  17%|█▋        | 29/166 [00:37<02:50,  1.24s/it]avg_loss = 1.7825742046038309:  18%|█▊        | 30/166 [00:37<02:49,  1.25s/it]avg_loss = 1.7965393566316175:  18%|█▊        | 30/166 [00:38<02:49,  1.25s/it]avg_loss = 1.7965393566316175:  19%|█▊        | 31/166 [00:38<02:48,  1.25s/it]avg_loss = 1.8030497916042805:  19%|█▊        | 31/166 [00:39<02:48,  1.25s/it]avg_loss = 1.8030497916042805:  19%|█▉        | 32/166 [00:39<02:47,  1.25s/it]avg_loss = 1.8080540678717874:  19%|█▉        | 32/166 [00:40<02:47,  1.25s/it]avg_loss = 1.8080540678717874:  20%|█▉        | 33/166 [00:40<02:46,  1.25s/it]avg_loss = 1.806915472535526:  20%|█▉        | 33/166 [00:42<02:46,  1.25s/it] avg_loss = 1.806915472535526:  20%|██        | 34/166 [00:42<02:45,  1.25s/it]avg_loss = 1.8006948028291976:  20%|██        | 34/166 [00:43<02:45,  1.25s/it]avg_loss = 1.8006948028291976:  21%|██        | 35/166 [00:43<02:44,  1.25s/it]avg_loss = 1.7926029397381678:  21%|██        | 35/166 [00:44<02:44,  1.25s/it]avg_loss = 1.7926029397381678:  22%|██▏       | 36/166 [00:44<02:43,  1.26s/it]avg_loss = 1.7829696745485872:  22%|██▏       | 36/166 [00:45<02:43,  1.26s/it]avg_loss = 1.7829696745485872:  22%|██▏       | 37/166 [00:45<02:42,  1.26s/it]avg_loss = 1.7801230487070585:  22%|██▏       | 37/166 [00:47<02:42,  1.26s/it]avg_loss = 1.7801230487070585:  23%|██▎       | 38/166 [00:47<02:41,  1.26s/it]avg_loss = 1.778054918998327:  23%|██▎       | 38/166 [00:48<02:41,  1.26s/it] avg_loss = 1.778054918998327:  23%|██▎       | 39/166 [00:48<02:39,  1.26s/it]avg_loss = 1.7812351495027543:  23%|██▎       | 39/166 [00:49<02:39,  1.26s/it]avg_loss = 1.7812351495027543:  24%|██▍       | 40/166 [00:49<02:38,  1.26s/it]avg_loss = 1.7811984492511284:  24%|██▍       | 40/166 [00:50<02:38,  1.26s/it]avg_loss = 1.7811984492511284:  25%|██▍       | 41/166 [00:50<02:37,  1.26s/it]avg_loss = 1.7687710864203317:  25%|██▍       | 41/166 [00:52<02:37,  1.26s/it]avg_loss = 1.7687710864203317:  25%|██▌       | 42/166 [00:52<02:36,  1.26s/it]avg_loss = 1.7531523538190266:  25%|██▌       | 42/166 [00:53<02:36,  1.26s/it]avg_loss = 1.7531523538190266:  26%|██▌       | 43/166 [00:53<02:35,  1.26s/it]avg_loss = 1.7426828064701774:  26%|██▌       | 43/166 [00:54<02:35,  1.26s/it]avg_loss = 1.7426828064701774:  27%|██▋       | 44/166 [00:54<02:34,  1.27s/it]avg_loss = 1.7288872427410549:  27%|██▋       | 44/166 [00:56<02:34,  1.27s/it]avg_loss = 1.7288872427410549:  27%|██▋       | 45/166 [00:56<02:33,  1.27s/it]avg_loss = 1.7184039898540662:  27%|██▋       | 45/166 [00:57<02:33,  1.27s/it]avg_loss = 1.7184039898540662:  28%|██▊       | 46/166 [00:57<02:32,  1.27s/it]avg_loss = 1.7113384185953344:  28%|██▊       | 46/166 [00:58<02:32,  1.27s/it]avg_loss = 1.7113384185953344:  28%|██▊       | 47/166 [00:58<02:30,  1.27s/it]avg_loss = 1.7123238096634548:  28%|██▊       | 47/166 [00:59<02:30,  1.27s/it]avg_loss = 1.7123238096634548:  29%|██▉       | 48/166 [00:59<02:29,  1.27s/it]avg_loss = 1.7229295166171328:  29%|██▉       | 48/166 [01:01<02:29,  1.27s/it]avg_loss = 1.7229295166171328:  30%|██▉       | 49/166 [01:01<02:28,  1.27s/it]avg_loss = 1.7334281969070435:  30%|██▉       | 49/166 [01:02<02:28,  1.27s/it]avg_loss = 1.7334281969070435:  30%|███       | 50/166 [01:02<02:27,  1.28s/it]avg_loss = 1.7402923948624556:  30%|███       | 50/166 [01:03<02:27,  1.28s/it]avg_loss = 1.7402923948624556:  31%|███       | 51/166 [01:03<02:26,  1.28s/it]avg_loss = 1.7451915374168983:  31%|███       | 51/166 [01:04<02:26,  1.28s/it]avg_loss = 1.7451915374168983:  31%|███▏      | 52/166 [01:04<02:25,  1.28s/it]avg_loss = 1.7486148960185501:  31%|███▏      | 52/166 [01:06<02:25,  1.28s/it]avg_loss = 1.7486148960185501:  32%|███▏      | 53/166 [01:06<02:24,  1.28s/it]avg_loss = 1.7495850633691858:  32%|███▏      | 53/166 [01:07<02:24,  1.28s/it]avg_loss = 1.7495850633691858:  33%|███▎      | 54/166 [01:07<02:23,  1.28s/it]avg_loss = 1.7520445281809027:  33%|███▎      | 54/166 [01:08<02:23,  1.28s/it]avg_loss = 1.7520445281809027:  33%|███▎      | 55/166 [01:08<02:21,  1.28s/it]avg_loss = 1.755484082869121:  33%|███▎      | 55/166 [01:10<02:21,  1.28s/it] avg_loss = 1.755484082869121:  34%|███▎      | 56/166 [01:10<02:20,  1.28s/it]avg_loss = 1.750504550180937:  34%|███▎      | 56/166 [01:11<02:20,  1.28s/it]avg_loss = 1.750504550180937:  34%|███▍      | 57/166 [01:11<02:19,  1.28s/it]avg_loss = 1.7542379909548267:  34%|███▍      | 57/166 [01:12<02:19,  1.28s/it]avg_loss = 1.7542379909548267:  35%|███▍      | 58/166 [01:12<02:18,  1.28s/it]avg_loss = 1.7524633650052346:  35%|███▍      | 58/166 [01:13<02:18,  1.28s/it]avg_loss = 1.7524633650052346:  36%|███▌      | 59/166 [01:13<02:17,  1.28s/it]avg_loss = 1.7476402997970581:  36%|███▌      | 59/166 [01:15<02:17,  1.28s/it]avg_loss = 1.7476402997970581:  36%|███▌      | 60/166 [01:15<02:16,  1.28s/it]avg_loss = 1.743195592379961:  36%|███▌      | 60/166 [01:16<02:16,  1.28s/it] avg_loss = 1.743195592379961:  37%|███▋      | 61/166 [01:16<02:15,  1.29s/it]avg_loss = 1.7392206576562697:  37%|███▋      | 61/166 [01:17<02:15,  1.29s/it]avg_loss = 1.7392206576562697:  37%|███▋      | 62/166 [01:17<02:13,  1.29s/it]avg_loss = 1.733416720042153:  37%|███▋      | 62/166 [01:19<02:13,  1.29s/it] avg_loss = 1.733416720042153:  38%|███▊      | 63/166 [01:19<02:12,  1.29s/it]avg_loss = 1.7291434928774834:  38%|███▊      | 63/166 [01:20<02:12,  1.29s/it]avg_loss = 1.7291434928774834:  39%|███▊      | 64/166 [01:20<02:11,  1.29s/it]avg_loss = 1.7224540673769437:  39%|███▊      | 64/166 [01:21<02:11,  1.29s/it]avg_loss = 1.7224540673769437:  39%|███▉      | 65/166 [01:21<02:10,  1.29s/it]avg_loss = 1.7152280753309077:  39%|███▉      | 65/166 [01:22<02:10,  1.29s/it]avg_loss = 1.7152280753309077:  40%|███▉      | 66/166 [01:22<02:08,  1.29s/it]avg_loss = 1.709648555784083:  40%|███▉      | 66/166 [01:24<02:08,  1.29s/it] avg_loss = 1.709648555784083:  40%|████      | 67/166 [01:24<02:07,  1.29s/it]avg_loss = 1.7085997620049644:  40%|████      | 67/166 [01:25<02:07,  1.29s/it]avg_loss = 1.7085997620049644:  41%|████      | 68/166 [01:25<02:06,  1.29s/it]avg_loss = 1.710676518039427:  41%|████      | 68/166 [01:26<02:06,  1.29s/it] avg_loss = 1.710676518039427:  42%|████▏     | 69/166 [01:26<02:05,  1.29s/it]avg_loss = 1.7137883033071246:  42%|████▏     | 69/166 [01:28<02:05,  1.29s/it]avg_loss = 1.7137883033071246:  42%|████▏     | 70/166 [01:28<02:04,  1.29s/it]avg_loss = 1.7176239490509033:  42%|████▏     | 70/166 [01:29<02:04,  1.29s/it]avg_loss = 1.7176239490509033:  43%|████▎     | 71/166 [01:29<02:03,  1.30s/it]avg_loss = 1.7226081126266055:  43%|████▎     | 71/166 [01:30<02:03,  1.30s/it]avg_loss = 1.7226081126266055:  43%|████▎     | 72/166 [01:30<02:01,  1.30s/it]avg_loss = 1.7287879186133817:  43%|████▎     | 72/166 [01:32<02:01,  1.30s/it]avg_loss = 1.7287879186133817:  44%|████▍     | 73/166 [01:32<02:00,  1.30s/it]avg_loss = 1.7231874594817291:  44%|████▍     | 73/166 [01:33<02:00,  1.30s/it]avg_loss = 1.7231874594817291:  45%|████▍     | 74/166 [01:33<01:59,  1.30s/it]avg_loss = 1.7187148078282675:  45%|████▍     | 74/166 [01:34<01:59,  1.30s/it]avg_loss = 1.7187148078282675:  45%|████▌     | 75/166 [01:34<01:58,  1.30s/it]avg_loss = 1.7179356844801652:  45%|████▌     | 75/166 [01:35<01:58,  1.30s/it]avg_loss = 1.7179356844801652:  46%|████▌     | 76/166 [01:35<01:56,  1.30s/it]avg_loss = 1.7144668659606537:  46%|████▌     | 76/166 [01:37<01:56,  1.30s/it]avg_loss = 1.7144668659606537:  46%|████▋     | 77/166 [01:37<01:55,  1.30s/it]avg_loss = 1.7108759834216192:  46%|████▋     | 77/166 [01:38<01:55,  1.30s/it]avg_loss = 1.7108759834216192:  47%|████▋     | 78/166 [01:38<01:54,  1.30s/it]avg_loss = 1.708239860172513:  47%|████▋     | 78/166 [01:39<01:54,  1.30s/it] avg_loss = 1.708239860172513:  48%|████▊     | 79/166 [01:39<01:53,  1.30s/it]avg_loss = 1.704783946275711:  48%|████▊     | 79/166 [01:41<01:53,  1.30s/it]avg_loss = 1.704783946275711:  48%|████▊     | 80/166 [01:41<01:51,  1.30s/it]avg_loss = 1.6956055944348558:  48%|████▊     | 80/166 [01:42<01:51,  1.30s/it]avg_loss = 1.6956055944348558:  49%|████▉     | 81/166 [01:42<01:50,  1.30s/it]avg_loss = 1.6971885867235137:  49%|████▉     | 81/166 [01:43<01:50,  1.30s/it]avg_loss = 1.6971885867235137:  49%|████▉     | 82/166 [01:43<01:49,  1.30s/it]avg_loss = 1.6991672242980405:  49%|████▉     | 82/166 [01:45<01:49,  1.30s/it]avg_loss = 1.6991672242980405:  50%|█████     | 83/166 [01:45<01:47,  1.30s/it]avg_loss = 1.7022817730903625:  50%|█████     | 83/166 [01:46<01:47,  1.30s/it]avg_loss = 1.7022817730903625:  51%|█████     | 84/166 [01:46<01:46,  1.30s/it]avg_loss = 1.704213310690487:  51%|█████     | 84/166 [01:47<01:46,  1.30s/it] avg_loss = 1.704213310690487:  51%|█████     | 85/166 [01:47<01:45,  1.30s/it]avg_loss = 1.7031482416529988:  51%|█████     | 85/166 [01:48<01:45,  1.30s/it]avg_loss = 1.7031482416529988:  52%|█████▏    | 86/166 [01:48<01:44,  1.30s/it]avg_loss = 1.7034263747862015:  52%|█████▏    | 86/166 [01:50<01:44,  1.30s/it]avg_loss = 1.7034263747862015:  52%|█████▏    | 87/166 [01:50<01:42,  1.30s/it]avg_loss = 1.7035861191424457:  52%|█████▏    | 87/166 [01:51<01:42,  1.30s/it]avg_loss = 1.7035861191424457:  53%|█████▎    | 88/166 [01:51<01:41,  1.30s/it]avg_loss = 1.7047257329640764:  53%|█████▎    | 88/166 [01:52<01:41,  1.30s/it]avg_loss = 1.7047257329640764:  54%|█████▎    | 89/166 [01:52<01:40,  1.30s/it]avg_loss = 1.7046419872177971:  54%|█████▎    | 89/166 [01:54<01:40,  1.30s/it]avg_loss = 1.7046419872177971:  54%|█████▍    | 90/166 [01:54<01:39,  1.30s/it]avg_loss = 1.705098195390387:  54%|█████▍    | 90/166 [01:55<01:39,  1.30s/it] avg_loss = 1.705098195390387:  55%|█████▍    | 91/166 [01:55<01:37,  1.30s/it]avg_loss = 1.7062267414901569:  55%|█████▍    | 91/166 [01:56<01:37,  1.30s/it]avg_loss = 1.7062267414901569:  55%|█████▌    | 92/166 [01:56<01:36,  1.30s/it]avg_loss = 1.7101854316649898:  55%|█████▌    | 92/166 [01:58<01:36,  1.30s/it]avg_loss = 1.7101854316649898:  56%|█████▌    | 93/166 [01:58<01:35,  1.31s/it]avg_loss = 1.7092034461650443:  56%|█████▌    | 93/166 [01:59<01:35,  1.31s/it]avg_loss = 1.7092034461650443:  57%|█████▋    | 94/166 [01:59<01:33,  1.30s/it]avg_loss = 1.7084157504533466:  57%|█████▋    | 94/166 [02:00<01:33,  1.30s/it]avg_loss = 1.7084157504533466:  57%|█████▋    | 95/166 [02:00<01:32,  1.31s/it]avg_loss = 1.7080173542102177:  57%|█████▋    | 95/166 [02:01<01:32,  1.31s/it]avg_loss = 1.7080173542102177:  58%|█████▊    | 96/166 [02:01<01:31,  1.31s/it]avg_loss = 1.7079071052295645:  58%|█████▊    | 96/166 [02:03<01:31,  1.31s/it]avg_loss = 1.7079071052295645:  58%|█████▊    | 97/166 [02:03<01:30,  1.31s/it]avg_loss = 1.7062925720701412:  58%|█████▊    | 97/166 [02:04<01:30,  1.31s/it]avg_loss = 1.7062925720701412:  59%|█████▉    | 98/166 [02:04<01:28,  1.31s/it]avg_loss = 1.7039206100232673:  59%|█████▉    | 98/166 [02:05<01:28,  1.31s/it]avg_loss = 1.7039206100232673:  60%|█████▉    | 99/166 [02:05<01:27,  1.31s/it]avg_loss = 1.701268082857132:  60%|█████▉    | 99/166 [02:07<01:27,  1.31s/it] avg_loss = 1.701268082857132:  60%|██████    | 100/166 [02:07<01:26,  1.31s/it]avg_loss = 1.7016623563105517:  60%|██████    | 100/166 [02:08<01:26,  1.31s/it]avg_loss = 1.7016623563105517:  61%|██████    | 101/166 [02:08<01:25,  1.31s/it]avg_loss = 1.702630594664929:  61%|██████    | 101/166 [02:09<01:25,  1.31s/it] avg_loss = 1.702630594664929:  61%|██████▏   | 102/166 [02:09<01:23,  1.31s/it]avg_loss = 1.7037696757362883:  61%|██████▏   | 102/166 [02:11<01:23,  1.31s/it]avg_loss = 1.7037696757362883:  62%|██████▏   | 103/166 [02:11<01:22,  1.31s/it]avg_loss = 1.7059720353438304:  62%|██████▏   | 103/166 [02:12<01:22,  1.31s/it]avg_loss = 1.7059720353438304:  63%|██████▎   | 104/166 [02:12<01:21,  1.31s/it]avg_loss = 1.71252296879178:  63%|██████▎   | 104/166 [02:13<01:21,  1.31s/it]  avg_loss = 1.71252296879178:  63%|██████▎   | 105/166 [02:13<01:19,  1.31s/it]avg_loss = 1.7177693787610755:  63%|██████▎   | 105/166 [02:15<01:19,  1.31s/it]avg_loss = 1.7177693787610755:  64%|██████▍   | 106/166 [02:15<01:18,  1.31s/it]avg_loss = 1.7214016346173866:  64%|██████▍   | 106/166 [02:16<01:18,  1.31s/it]avg_loss = 1.7214016346173866:  64%|██████▍   | 107/166 [02:16<01:17,  1.31s/it]avg_loss = 1.7245430493796314:  64%|██████▍   | 107/166 [02:17<01:17,  1.31s/it]avg_loss = 1.7245430493796314:  65%|██████▌   | 108/166 [02:17<01:16,  1.31s/it]avg_loss = 1.729274551802819:  65%|██████▌   | 108/166 [02:18<01:16,  1.31s/it] avg_loss = 1.729274551802819:  66%|██████▌   | 109/166 [02:18<01:14,  1.31s/it]avg_loss = 1.732775780287656:  66%|██████▌   | 109/166 [02:20<01:14,  1.31s/it]avg_loss = 1.732775780287656:  66%|██████▋   | 110/166 [02:20<01:13,  1.31s/it]avg_loss = 1.7342490752538045:  66%|██████▋   | 110/166 [02:21<01:13,  1.31s/it]avg_loss = 1.7342490752538045:  67%|██████▋   | 111/166 [02:21<01:12,  1.31s/it]avg_loss = 1.7354616480214256:  67%|██████▋   | 111/166 [02:22<01:12,  1.31s/it]avg_loss = 1.7354616480214256:  67%|██████▋   | 112/166 [02:22<01:10,  1.31s/it]avg_loss = 1.7358437259640314:  67%|██████▋   | 112/166 [02:24<01:10,  1.31s/it]avg_loss = 1.7358437259640314:  68%|██████▊   | 113/166 [02:24<01:09,  1.31s/it]avg_loss = 1.7372354990557621:  68%|██████▊   | 113/166 [02:25<01:09,  1.31s/it]avg_loss = 1.7372354990557621:  69%|██████▊   | 114/166 [02:25<01:08,  1.31s/it]avg_loss = 1.7341762770777163:  69%|██████▊   | 114/166 [02:26<01:08,  1.31s/it]avg_loss = 1.7341762770777163:  69%|██████▉   | 115/166 [02:26<01:06,  1.31s/it]avg_loss = 1.7335779636070645:  69%|██████▉   | 115/166 [02:28<01:06,  1.31s/it]avg_loss = 1.7335779636070645:  70%|██████▉   | 116/166 [02:28<01:05,  1.31s/it]avg_loss = 1.7346018369381244:  70%|██████▉   | 116/166 [02:29<01:05,  1.31s/it]avg_loss = 1.7346018369381244:  70%|███████   | 117/166 [02:29<01:04,  1.31s/it]avg_loss = 1.7347544401378956:  70%|███████   | 117/166 [02:30<01:04,  1.31s/it]avg_loss = 1.7347544401378956:  71%|███████   | 118/166 [02:30<01:02,  1.31s/it]avg_loss = 1.734192918328678:  71%|███████   | 118/166 [02:32<01:02,  1.31s/it] avg_loss = 1.734192918328678:  72%|███████▏  | 119/166 [02:32<01:01,  1.31s/it]avg_loss = 1.7348220964272818:  72%|███████▏  | 119/166 [02:33<01:01,  1.31s/it]avg_loss = 1.7348220964272818:  72%|███████▏  | 120/166 [02:33<01:00,  1.31s/it]avg_loss = 1.734180761762887:  72%|███████▏  | 120/166 [02:34<01:00,  1.31s/it] avg_loss = 1.734180761762887:  73%|███████▎  | 121/166 [02:34<00:59,  1.31s/it]avg_loss = 1.7345035418135222:  73%|███████▎  | 121/166 [02:36<00:59,  1.31s/it]avg_loss = 1.7345035418135222:  73%|███████▎  | 122/166 [02:36<00:57,  1.31s/it]avg_loss = 1.7347771443002593:  73%|███████▎  | 122/166 [02:37<00:57,  1.31s/it]avg_loss = 1.7347771443002593:  74%|███████▍  | 123/166 [02:37<00:56,  1.31s/it]avg_loss = 1.7332964731800942:  74%|███████▍  | 123/166 [02:38<00:56,  1.31s/it]avg_loss = 1.7332964731800942:  75%|███████▍  | 124/166 [02:38<00:55,  1.31s/it]avg_loss = 1.7315907850265504:  75%|███████▍  | 124/166 [02:39<00:55,  1.31s/it]avg_loss = 1.7315907850265504:  75%|███████▌  | 125/166 [02:39<00:53,  1.31s/it]avg_loss = 1.7293799907442122:  75%|███████▌  | 125/166 [02:41<00:53,  1.31s/it]avg_loss = 1.7293799907442122:  76%|███████▌  | 126/166 [02:41<00:52,  1.32s/it]avg_loss = 1.7272374723839947:  76%|███████▌  | 126/166 [02:42<00:52,  1.32s/it]avg_loss = 1.7272374723839947:  77%|███████▋  | 127/166 [02:42<00:51,  1.31s/it]avg_loss = 1.7258118353784084:  77%|███████▋  | 127/166 [02:43<00:51,  1.31s/it]avg_loss = 1.7258118353784084:  77%|███████▋  | 128/166 [02:43<00:49,  1.31s/it]avg_loss = 1.7245423183884732:  77%|███████▋  | 128/166 [02:45<00:49,  1.31s/it]avg_loss = 1.7245423183884732:  78%|███████▊  | 129/166 [02:45<00:48,  1.31s/it]avg_loss = 1.7243972338162936:  78%|███████▊  | 129/166 [02:46<00:48,  1.31s/it]avg_loss = 1.7243972338162936:  78%|███████▊  | 130/166 [02:46<00:47,  1.32s/it]avg_loss = 1.725435590016023:  78%|███████▊  | 130/166 [02:47<00:47,  1.32s/it] avg_loss = 1.725435590016023:  79%|███████▉  | 131/166 [02:47<00:46,  1.32s/it]avg_loss = 1.7260456292918234:  79%|███████▉  | 131/166 [02:49<00:46,  1.32s/it]avg_loss = 1.7260456292918234:  80%|███████▉  | 132/166 [02:49<00:44,  1.32s/it]avg_loss = 1.7270076077683527:  80%|███████▉  | 132/166 [02:50<00:44,  1.32s/it]avg_loss = 1.7270076077683527:  80%|████████  | 133/166 [02:50<00:43,  1.32s/it]avg_loss = 1.7283420491574415:  80%|████████  | 133/166 [02:51<00:43,  1.32s/it]avg_loss = 1.7283420491574415:  81%|████████  | 134/166 [02:51<00:42,  1.32s/it]avg_loss = 1.7262660715315077:  81%|████████  | 134/166 [02:53<00:42,  1.32s/it]avg_loss = 1.7262660715315077:  81%|████████▏ | 135/166 [02:53<00:40,  1.32s/it]avg_loss = 1.726581812781446:  81%|████████▏ | 135/166 [02:54<00:40,  1.32s/it] avg_loss = 1.726581812781446:  82%|████████▏ | 136/166 [02:54<00:39,  1.32s/it]avg_loss = 1.7268926061853005:  82%|████████▏ | 136/166 [02:55<00:39,  1.32s/it]avg_loss = 1.7268926061853005:  83%|████████▎ | 137/166 [02:55<00:38,  1.32s/it]avg_loss = 1.727736166421918:  83%|████████▎ | 137/166 [02:57<00:38,  1.32s/it] avg_loss = 1.727736166421918:  83%|████████▎ | 138/166 [02:57<00:36,  1.32s/it]avg_loss = 1.7268093978758339:  83%|████████▎ | 138/166 [02:58<00:36,  1.32s/it]avg_loss = 1.7268093978758339:  84%|████████▎ | 139/166 [02:58<00:35,  1.32s/it]avg_loss = 1.7254875191620418:  84%|████████▎ | 139/166 [02:59<00:35,  1.32s/it]avg_loss = 1.7254875191620418:  84%|████████▍ | 140/166 [02:59<00:34,  1.32s/it]avg_loss = 1.7241019669999467:  84%|████████▍ | 140/166 [03:01<00:34,  1.32s/it]avg_loss = 1.7241019669999467:  85%|████████▍ | 141/166 [03:01<00:32,  1.32s/it]avg_loss = 1.7237345047400032:  85%|████████▍ | 141/166 [03:02<00:32,  1.32s/it]avg_loss = 1.7237345047400032:  86%|████████▌ | 142/166 [03:02<00:31,  1.32s/it]avg_loss = 1.7220861078142287:  86%|████████▌ | 142/166 [03:03<00:31,  1.32s/it]avg_loss = 1.7220861078142287:  86%|████████▌ | 143/166 [03:03<00:30,  1.32s/it]avg_loss = 1.7232353513439496:  86%|████████▌ | 143/166 [03:05<00:30,  1.32s/it]avg_loss = 1.7232353513439496:  87%|████████▋ | 144/166 [03:05<00:28,  1.32s/it]avg_loss = 1.7224875211715698:  87%|████████▋ | 144/166 [03:06<00:28,  1.32s/it]avg_loss = 1.7224875211715698:  87%|████████▋ | 145/166 [03:06<00:27,  1.32s/it]avg_loss = 1.7224076088160685:  87%|████████▋ | 145/166 [03:07<00:27,  1.32s/it]avg_loss = 1.7224076088160685:  88%|████████▊ | 146/166 [03:07<00:26,  1.32s/it]avg_loss = 1.7212677456083751:  88%|████████▊ | 146/166 [03:08<00:26,  1.32s/it]avg_loss = 1.7212677456083751:  89%|████████▊ | 147/166 [03:08<00:25,  1.32s/it]avg_loss = 1.7203472530519641:  89%|████████▊ | 147/166 [03:10<00:25,  1.32s/it]avg_loss = 1.7203472530519641:  89%|████████▉ | 148/166 [03:10<00:23,  1.32s/it]avg_loss = 1.7186249030516452:  89%|████████▉ | 148/166 [03:11<00:23,  1.32s/it]avg_loss = 1.7186249030516452:  90%|████████▉ | 149/166 [03:11<00:22,  1.32s/it]avg_loss = 1.719575769106547:  90%|████████▉ | 149/166 [03:12<00:22,  1.32s/it] avg_loss = 1.719575769106547:  90%|█████████ | 150/166 [03:12<00:21,  1.32s/it]avg_loss = 1.7186599227766328:  90%|█████████ | 150/166 [03:14<00:21,  1.32s/it]avg_loss = 1.7186599227766328:  91%|█████████ | 151/166 [03:14<00:19,  1.32s/it]avg_loss = 1.7184857177106958:  91%|█████████ | 151/166 [03:15<00:19,  1.32s/it]avg_loss = 1.7184857177106958:  92%|█████████▏| 152/166 [03:15<00:18,  1.32s/it]avg_loss = 1.7182842158024607:  92%|█████████▏| 152/166 [03:16<00:18,  1.32s/it]avg_loss = 1.7182842158024607:  92%|█████████▏| 153/166 [03:16<00:17,  1.32s/it]avg_loss = 1.7198880833464782:  92%|█████████▏| 153/166 [03:18<00:17,  1.32s/it]avg_loss = 1.7198880833464782:  93%|█████████▎| 154/166 [03:18<00:15,  1.32s/it]avg_loss = 1.7194037522039105:  93%|█████████▎| 154/166 [03:19<00:15,  1.32s/it]avg_loss = 1.7194037522039105:  93%|█████████▎| 155/166 [03:19<00:14,  1.32s/it]avg_loss = 1.7192909427178211:  93%|█████████▎| 155/166 [03:20<00:14,  1.32s/it]avg_loss = 1.7192909427178211:  94%|█████████▍| 156/166 [03:20<00:13,  1.32s/it]avg_loss = 1.7175240995018346:  94%|█████████▍| 156/166 [03:22<00:13,  1.32s/it]avg_loss = 1.7175240995018346:  95%|█████████▍| 157/166 [03:22<00:11,  1.32s/it]avg_loss = 1.7132539002201226:  95%|█████████▍| 157/166 [03:23<00:11,  1.32s/it]avg_loss = 1.7132539002201226:  95%|█████████▌| 158/166 [03:23<00:10,  1.32s/it]avg_loss = 1.714047794821877:  95%|█████████▌| 158/166 [03:24<00:10,  1.32s/it] avg_loss = 1.714047794821877:  96%|█████████▌| 159/166 [03:24<00:09,  1.32s/it]avg_loss = 1.7154324434697628:  96%|█████████▌| 159/166 [03:26<00:09,  1.32s/it]avg_loss = 1.7154324434697628:  96%|█████████▋| 160/166 [03:26<00:07,  1.32s/it]avg_loss = 1.7178091092139298:  96%|█████████▋| 160/166 [03:27<00:07,  1.32s/it]avg_loss = 1.7178091092139298:  97%|█████████▋| 161/166 [03:27<00:06,  1.32s/it]avg_loss = 1.7180409880332004:  97%|█████████▋| 161/166 [03:28<00:06,  1.32s/it]avg_loss = 1.7180409880332004:  98%|█████████▊| 162/166 [03:28<00:05,  1.32s/it]avg_loss = 1.7176676469346497:  98%|█████████▊| 162/166 [03:30<00:05,  1.32s/it]avg_loss = 1.7176676469346497:  98%|█████████▊| 163/166 [03:30<00:03,  1.32s/it]avg_loss = 1.7183104709881107:  98%|█████████▊| 163/166 [03:31<00:03,  1.32s/it]avg_loss = 1.7183104709881107:  99%|█████████▉| 164/166 [03:31<00:02,  1.32s/it]avg_loss = 1.7183998938762781:  99%|█████████▉| 164/166 [03:32<00:02,  1.32s/it]avg_loss = 1.7183998938762781:  99%|█████████▉| 165/166 [03:32<00:01,  1.32s/it]avg_loss = 1.7203367266310268:  99%|█████████▉| 165/166 [03:34<00:01,  1.32s/it]avg_loss = 1.7203367266310268: 100%|██████████| 166/166 [03:34<00:00,  1.32s/it]avg_loss = 1.7203367266310268: 100%|██████████| 166/166 [03:34<00:00,  1.29s/it]
I0403 07:04:41.933459 3504833 eval_ppl.py:107] wikitext2 perplexity: 5.586409091949463
wikitext2 perplexity: 5.586
