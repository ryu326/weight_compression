I0403 07:04:49.023257 3509055 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:04:49.023370 3509055 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:04:49.023413 3509055 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:04:49.401447 3509055 config.py:54] PyTorch version 2.6.0 available.
W0403 07:04:49.621903 3509055 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:04:50.314388 3509055 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  5.85it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  5.99it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  6.03it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  6.11it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  6.02it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.31it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.16it/s]
I0403 07:04:51.505620 3509055 quantize_finetune_llama.py:152] loaded model
I0403 07:04:51.954392 3509055 quantize_finetune_llama.py:190] loaded compression model
I0403 07:05:07.652281 3509055 quantize_finetune_llama.py:194] loaded dataset and devset
I0403 07:05:12.634769 3509055 quantize_finetune_llama.py:214] layer 0 gpu 0
I0403 07:05:15.319385 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 0 in 2.500258207321167s
Use train scale and shift
tensor(2.2655e-07, device='cuda:0') tensor(0.0204, device='cuda:0')
tensor(0.0204, device='cuda:0') tensor(2.2655e-07, device='cuda:0')
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0403 07:05:28.609343 3509784 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:05:28.609477 3509784 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:05:28.609518 3509784 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:05:29.030804 3509784 config.py:54] PyTorch version 2.6.0 available.
W0403 07:05:29.257052 3509784 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:05:29.962507 3509784 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:05:29.966933 3509055 quantize_finetune_llama.py:214] layer 1 gpu 1
I0403 07:05:30.255281 3509784 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:05:33.525566 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 1 in 3.3553755283355713s
I0403 07:05:37.379090 3509955 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:05:37.379208 3509955 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:05:37.379257 3509955 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:05:37.758681 3509955 config.py:54] PyTorch version 2.6.0 available.
W0403 07:05:37.977160 3509955 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:05:38.619716 3509955 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:05:38.623440 3509055 quantize_finetune_llama.py:214] layer 2 gpu 0
I0403 07:05:38.793905 3509955 data_utils.py:336] using 256 training seqs, 128 validation seqs
0_v proxy err 0.00016997936472762376 err 0.16519905626773834 tr(WHW.T) 971.8771362304688
bpp_loss 4.499349916470237
0_q proxy err 5.268528298074671e-07 err 0.3353025019168854 tr(WHW.T) 636425.375
bpp_loss 4.495525011327118
0_k proxy err 7.935859684948809e-07 err 0.31653299927711487 tr(WHW.T) 398864.15625
bpp_loss 4.593830641941167
0_o proxy err 1.193808111565886e-05 err 0.19012519717216492 tr(WHW.T) 15925.943359375
bpp_loss 4.337885473505594
0_up proxy err 0.0006999315810389817 err 16.88037872314453 tr(WHW.T) 24117.18359375
bpp_loss 4.376827973352615
0_gate proxy err 0.0004789301019627601 err 16.967248916625977 tr(WHW.T) 35427.40234375
bpp_loss 4.3914428108999894
0_down proxy err 0.00031895108986645937 err 11.417281150817871 tr(WHW.T) 35796.33984375
bpp_loss 4.583593205483847
1_v proxy err 0.0007549654692411423 err 0.4960305094718933 tr(WHW.T) 657.0241088867188
bpp_loss 4.4415508734527975
1_q proxy err 4.109201199753443e-06 err 0.8030096292495728 tr(WHW.T) 195417.453125
bpp_loss 5.355166470864788
1_k proxy err 3.959299647249281e-06 err 0.8088673949241638 tr(WHW.T) 204295.5625
bpp_loss 5.349383629974909
1_o proxy err 0.0002369608700973913 err 0.9572344422340393 tr(WHW.T) 4039.630859375
bpp_loss 4.31682862166781
1_up proxy err 0.00098143529612571 err 22.778573989868164 tr(WHW.T) 23209.44921875
bpp_loss 4.400686678758194
1_gate proxy err 0.0004873237921856344 err 22.88182830810547 tr(WHW.T) 46954.0546875
bpp_loss 4.465885411428157
1_down proxy err 1.3192129699746147e-06 err 0.05379674211144447 tr(WHW.T) 40779.421875
bpp_loss 4.799801093461209
I0403 07:07:28.544786 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 2 in 0.885263204574585s
I0403 07:07:31.952949 3511444 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:07:31.953045 3511444 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:07:31.953083 3511444 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:07:32.280807 3511444 config.py:54] PyTorch version 2.6.0 available.
W0403 07:07:32.470830 3511444 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:07:33.048176 3511444 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:07:33.051756 3509055 quantize_finetune_llama.py:214] layer 3 gpu 1
I0403 07:07:33.238978 3511444 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:07:34.411415 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 3 in 0.936011791229248s
I0403 07:07:37.923108 3511596 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:07:37.923208 3511596 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:07:37.923246 3511596 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:07:38.247606 3511596 config.py:54] PyTorch version 2.6.0 available.
W0403 07:07:38.436981 3511596 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:07:39.021512 3511596 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:07:39.025032 3509055 quantize_finetune_llama.py:214] layer 4 gpu 0
I0403 07:07:39.187597 3511596 data_utils.py:336] using 256 training seqs, 128 validation seqs
2_v proxy err 0.0011346503160893917 err 3.1541733741760254 tr(WHW.T) 2779.86376953125
bpp_loss 4.515171290957369
2_q proxy err 2.0692245016107336e-05 err 3.3005828857421875 tr(WHW.T) 159508.203125
bpp_loss 5.329793100710958
2_k proxy err 1.582280128786806e-05 err 3.3228037357330322 tr(WHW.T) 210000.984375
bpp_loss 5.440439981757663
2_o proxy err 0.0010074094170704484 err 5.339864253997803 tr(WHW.T) 5300.58984375
bpp_loss 4.386345792096108
2_up proxy err 0.0014223167672753334 err 28.351850509643555 tr(WHW.T) 19933.5703125
bpp_loss 4.409784321694873
2_gate proxy err 0.0008988308254629374 err 28.45762825012207 tr(WHW.T) 31660.716796875
bpp_loss 4.48996989706228
2_down proxy err 0.0017913992051035166 err 30.96183967590332 tr(WHW.T) 17283.607421875
bpp_loss 4.481738871309993
3_v proxy err 0.0019688305910676718 err 5.8649702072143555 tr(WHW.T) 2978.910400390625
bpp_loss 4.401669916464016
3_q proxy err 8.100508421193808e-05 err 6.170485496520996 tr(WHW.T) 76174.0546875
bpp_loss 5.143743866006844
3_k proxy err 5.825317566632293e-05 err 6.1958417892456055 tr(WHW.T) 106360.5859375
bpp_loss 5.223619647091255
3_o proxy err 0.001103592454455793 err 5.800020217895508 tr(WHW.T) 5255.58154296875
bpp_loss 4.384826161898673
3_up proxy err 0.0017873289762064815 err 31.220733642578125 tr(WHW.T) 17467.81640625
bpp_loss 4.419518253657707
3_gate proxy err 0.0010673237266018987 err 31.3761043548584 tr(WHW.T) 29396.990234375
bpp_loss 4.509349173757919
3_down proxy err 0.0018165776273235679 err 30.713956832885742 tr(WHW.T) 16907.59375
bpp_loss 4.492293629578726
I0403 07:09:29.803304 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 4 in 0.8493444919586182s
I0403 07:09:33.220109 3512899 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:09:33.220214 3512899 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:09:33.220256 3512899 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:09:33.544413 3512899 config.py:54] PyTorch version 2.6.0 available.
W0403 07:09:33.734591 3512899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:09:34.328086 3512899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:09:34.331630 3509055 quantize_finetune_llama.py:214] layer 5 gpu 1
I0403 07:09:34.518449 3512899 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:09:35.760947 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 5 in 0.9768514633178711s
I0403 07:09:39.235903 3513044 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:09:39.235995 3513044 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:09:39.236033 3513044 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:09:39.554573 3513044 config.py:54] PyTorch version 2.6.0 available.
W0403 07:09:39.741670 3513044 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:09:40.280681 3513044 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:09:40.284178 3509055 quantize_finetune_llama.py:214] layer 6 gpu 0
I0403 07:09:40.552374 3513044 data_utils.py:336] using 256 training seqs, 128 validation seqs
4_v proxy err 0.001896488363854587 err 5.874477863311768 tr(WHW.T) 3097.555419921875
bpp_loss 4.4486371914390475
4_q proxy err 7.756445847917348e-05 err 6.107927322387695 tr(WHW.T) 78746.46875
bpp_loss 5.244525280082598
4_k proxy err 5.175275873625651e-05 err 6.141021251678467 tr(WHW.T) 118660.75
bpp_loss 5.283861660747789
4_o proxy err 0.0013832713011652231 err 7.382486820220947 tr(WHW.T) 5336.9765625
bpp_loss 4.362727271509357
4_up proxy err 0.0017591067589819431 err 31.074068069458008 tr(WHW.T) 17664.685546875
bpp_loss 4.417170973018158
4_gate proxy err 0.0008552663493901491 err 31.292604446411133 tr(WHW.T) 36588.140625
bpp_loss 4.540609626402689
4_down proxy err 0.0017857446800917387 err 30.055742263793945 tr(WHW.T) 16830.9296875
bpp_loss 4.481803582244834
5_v proxy err 0.002041851170361042 err 6.465668678283691 tr(WHW.T) 3166.572021484375
bpp_loss 4.45419732760638
5_q proxy err 9.271074668504298e-05 err 6.724858283996582 tr(WHW.T) 72535.90625
bpp_loss 5.244169239187613
5_k proxy err 5.823277388117276e-05 err 6.767073154449463 tr(WHW.T) 116207.296875
bpp_loss 5.32848858111538
5_o proxy err 0.0014237918658182025 err 5.367051601409912 tr(WHW.T) 3769.5478515625
bpp_loss 4.436742647783831
5_up proxy err 0.0017350954003632069 err 31.260597229003906 tr(WHW.T) 18016.64453125
bpp_loss 4.415657478276381
5_gate proxy err 0.0007984235999174416 err 31.471525192260742 tr(WHW.T) 39417.078125
bpp_loss 4.547056998537723
5_down proxy err 0.001979093300178647 err 31.573110580444336 tr(WHW.T) 15953.3203125
bpp_loss 4.468672440019112
I0403 07:11:30.173763 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 6 in 0.8542134761810303s
I0403 07:11:33.956623 3514397 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:11:33.956727 3514397 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:11:33.956769 3514397 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:11:34.297699 3514397 config.py:54] PyTorch version 2.6.0 available.
W0403 07:11:34.492580 3514397 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:11:35.097901 3514397 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:11:35.101413 3509055 quantize_finetune_llama.py:214] layer 7 gpu 1
I0403 07:11:35.316980 3514397 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:11:36.484534 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 7 in 0.9547905921936035s
I0403 07:11:40.053136 3514545 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:11:40.053232 3514545 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:11:40.053272 3514545 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:11:40.376778 3514545 config.py:54] PyTorch version 2.6.0 available.
W0403 07:11:40.561958 3514545 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:11:41.109537 3514545 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:11:41.113328 3509055 quantize_finetune_llama.py:214] layer 8 gpu 0
I0403 07:11:41.355841 3514545 data_utils.py:336] using 256 training seqs, 128 validation seqs
6_v proxy err 0.002229976700618863 err 7.093184947967529 tr(WHW.T) 3180.833740234375
bpp_loss 4.3807452413020656
6_q proxy err 0.0001370561367366463 err 7.505289554595947 tr(WHW.T) 54760.6953125
bpp_loss 5.100257750367746
6_k proxy err 9.984571806853637e-05 err 7.515398979187012 tr(WHW.T) 75270.1171875
bpp_loss 5.13905122934375
6_o proxy err 0.001598459086380899 err 6.459989070892334 tr(WHW.T) 4041.38525390625
bpp_loss 4.3648524400778115
6_up proxy err 0.0017675356939435005 err 31.765613555908203 tr(WHW.T) 17971.6953125
bpp_loss 4.412694649034461
6_gate proxy err 0.0007044661906547844 err 32.004539489746094 tr(WHW.T) 45430.91015625
bpp_loss 4.570250796197459
6_down proxy err 0.0019958913326263428 err 30.80609130859375 tr(WHW.T) 15434.75390625
bpp_loss 4.466557401427349
7_v proxy err 0.0022231757175177336 err 7.2269816398620605 tr(WHW.T) 3250.746826171875
bpp_loss 4.3853287026286125
7_q proxy err 0.00014922552509233356 err 7.655359268188477 tr(WHW.T) 51300.6015625
bpp_loss 5.093415383016691
7_k proxy err 0.00011238847946515307 err 7.667705535888672 tr(WHW.T) 68225.015625
bpp_loss 5.101023612776771
7_o proxy err 0.001750417286530137 err 6.176158428192139 tr(WHW.T) 3528.39208984375
bpp_loss 4.376740778563544
7_up proxy err 0.0017157266847789288 err 31.277822494506836 tr(WHW.T) 18230.072265625
bpp_loss 4.419727551209372
7_gate proxy err 0.0006758123636245728 err 31.525638580322266 tr(WHW.T) 46648.5078125
bpp_loss 4.572434233284967
7_down proxy err 0.002005476737394929 err 30.564220428466797 tr(WHW.T) 15240.3759765625
bpp_loss 4.4662563916035865
I0403 07:13:39.118722 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 8 in 0.9146945476531982s
I0403 07:13:42.693745 3516210 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:13:42.693846 3516210 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:13:42.693886 3516210 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:13:43.023040 3516210 config.py:54] PyTorch version 2.6.0 available.
W0403 07:13:43.214354 3516210 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:13:43.773784 3516210 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:13:43.777680 3509055 quantize_finetune_llama.py:214] layer 9 gpu 1
I0403 07:13:44.519237 3516210 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:13:45.336870 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 9 in 1.14646577835083s
I0403 07:13:49.103751 3516414 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:13:49.103872 3516414 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:13:49.103912 3516414 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:13:49.444112 3516414 config.py:54] PyTorch version 2.6.0 available.
W0403 07:13:49.660813 3516414 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:13:50.329477 3516414 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:13:50.333441 3509055 quantize_finetune_llama.py:214] layer 10 gpu 0
I0403 07:13:50.748452 3516414 data_utils.py:336] using 256 training seqs, 128 validation seqs
8_v proxy err 0.002026064321398735 err 7.031257152557373 tr(WHW.T) 3470.401611328125
bpp_loss 4.411977847456001
8_q proxy err 0.00015400207485072315 err 7.329352855682373 tr(WHW.T) 47592.55859375
bpp_loss 5.125598842976615
8_k proxy err 0.00010494859452592209 err 7.356889247894287 tr(WHW.T) 70099.9296875
bpp_loss 5.133961056475528
8_o proxy err 0.0018328462028875947 err 5.716089248657227 tr(WHW.T) 3118.695556640625
bpp_loss 4.404814210371114
8_up proxy err 0.001563838217407465 err 31.046186447143555 tr(WHW.T) 19852.556640625
bpp_loss 4.435136509236208
8_gate proxy err 0.0006902439054101706 err 31.276214599609375 tr(WHW.T) 45311.83203125
bpp_loss 4.552613382682551
8_down proxy err 0.001991929952055216 err 30.521102905273438 tr(WHW.T) 15322.3779296875
bpp_loss 4.4751007705926895
9_v proxy err 0.0020432579331099987 err 7.511966228485107 tr(WHW.T) 3676.465087890625
bpp_loss 4.412559553398751
9_q proxy err 0.0001707049523247406 err 7.797191619873047 tr(WHW.T) 45676.421875
bpp_loss 5.11759290099144
9_k proxy err 0.0001087729906430468 err 7.83764123916626 tr(WHW.T) 72055.03125
bpp_loss 5.163790642400272
9_o proxy err 0.0019150139996781945 err 6.029622554779053 tr(WHW.T) 3148.60498046875
bpp_loss 4.4027191932545975
9_up proxy err 0.0015103318728506565 err 31.12135887145996 tr(WHW.T) 20605.642578125
bpp_loss 4.443306884966617
9_gate proxy err 0.0006904784822836518 err 31.3522891998291 tr(WHW.T) 45406.61328125
bpp_loss 4.540504131812689
9_down proxy err 0.002020404674112797 err 30.960813522338867 tr(WHW.T) 15324.0654296875
bpp_loss 4.47679206473363
I0403 07:15:46.353560 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 10 in 0.8404603004455566s
I0403 07:15:49.985368 3518118 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:15:49.985462 3518118 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:15:49.985501 3518118 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:15:50.309346 3518118 config.py:54] PyTorch version 2.6.0 available.
W0403 07:15:50.518954 3518118 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:15:51.181213 3518118 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:15:51.185066 3509055 quantize_finetune_llama.py:214] layer 11 gpu 1
I0403 07:15:51.487650 3518118 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:15:52.761852 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 11 in 1.156083345413208s
I0403 07:15:56.414483 3518338 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:15:56.414576 3518338 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:15:56.414613 3518338 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:15:56.780894 3518338 config.py:54] PyTorch version 2.6.0 available.
W0403 07:15:57.001293 3518338 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:15:57.646225 3518338 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:15:57.650168 3509055 quantize_finetune_llama.py:214] layer 12 gpu 0
I0403 07:15:57.849194 3518338 data_utils.py:336] using 256 training seqs, 128 validation seqs
10_v proxy err 0.002075312426313758 err 7.578255653381348 tr(WHW.T) 3651.62158203125
bpp_loss 4.405872158240527
10_q proxy err 0.00017892422329168767 err 7.861131191253662 tr(WHW.T) 43935.53515625
bpp_loss 5.116243708180264
10_k proxy err 0.00011314520088490099 err 7.910304546356201 tr(WHW.T) 69912.859375
bpp_loss 5.173316073836759
10_o proxy err 0.00206565298140049 err 6.317353248596191 tr(WHW.T) 3058.283935546875
bpp_loss 4.386354239890352
10_up proxy err 0.001419535488821566 err 31.094120025634766 tr(WHW.T) 21904.43359375
bpp_loss 4.455173326959444
10_gate proxy err 0.0006806581513956189 err 31.29874610900879 tr(WHW.T) 45983.0625
bpp_loss 4.536572249216396
10_down proxy err 0.0018916208064183593 err 30.476160049438477 tr(WHW.T) 16111.1357421875
bpp_loss 4.489736758822272
11_v proxy err 0.0020654629915952682 err 8.036344528198242 tr(WHW.T) 3890.81982421875
bpp_loss 4.422371163382195
11_q proxy err 0.000219207358895801 err 8.345909118652344 tr(WHW.T) 38073.125
bpp_loss 5.001308767357841
11_k proxy err 0.00014700561587233096 err 8.37897777557373 tr(WHW.T) 56997.671875
bpp_loss 4.991690630209632
11_o proxy err 0.002114353235810995 err 6.457695484161377 tr(WHW.T) 3054.2177734375
bpp_loss 4.411494627944194
11_up proxy err 0.0014826476108282804 err 31.948543548583984 tr(WHW.T) 21548.3046875
bpp_loss 4.46331619999783
11_gate proxy err 0.0007089463178999722 err 32.176753997802734 tr(WHW.T) 45386.7265625
bpp_loss 4.530027470616407
11_down proxy err 0.00199698144569993 err 31.41408348083496 tr(WHW.T) 15730.783203125
bpp_loss 4.489231145269303
I0403 07:17:55.662472 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 12 in 0.842066764831543s
I0403 07:17:59.299511 3520176 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:17:59.299623 3520176 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:17:59.299663 3520176 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:17:59.673261 3520176 config.py:54] PyTorch version 2.6.0 available.
W0403 07:17:59.882619 3520176 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:18:00.703213 3520176 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:18:00.706677 3509055 quantize_finetune_llama.py:214] layer 13 gpu 1
I0403 07:18:00.942071 3520176 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:18:02.162797 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 13 in 0.990248441696167s
I0403 07:18:06.081746 3520369 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:18:06.081855 3520369 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:18:06.081896 3520369 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:18:06.505104 3520369 config.py:54] PyTorch version 2.6.0 available.
W0403 07:18:06.729784 3520369 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:18:07.411126 3520369 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:18:07.415299 3509055 quantize_finetune_llama.py:214] layer 14 gpu 0
I0403 07:18:07.705996 3520369 data_utils.py:336] using 256 training seqs, 128 validation seqs
12_v proxy err 0.0021773369517177343 err 8.289891242980957 tr(WHW.T) 3807.353515625
bpp_loss 4.409040551283397
12_q proxy err 0.00022370830993168056 err 8.591741561889648 tr(WHW.T) 38406.0
bpp_loss 5.037915646564215
12_k proxy err 0.0001454038283554837 err 8.641240119934082 tr(WHW.T) 59429.25
bpp_loss 5.093333022436127
12_o proxy err 0.002231418387964368 err 6.687826156616211 tr(WHW.T) 2997.118896484375
bpp_loss 4.395897539914586
12_up proxy err 0.001480479259043932 err 32.2837028503418 tr(WHW.T) 21806.251953125
bpp_loss 4.472975385050441
12_gate proxy err 0.0007661314448341727 err 32.4959716796875 tr(WHW.T) 42415.66015625
bpp_loss 4.521078203739815
12_down proxy err 0.0019849552772939205 err 31.330032348632812 tr(WHW.T) 15783.7470703125
bpp_loss 4.501903980060718
13_v proxy err 0.0022668405435979366 err 8.804336547851562 tr(WHW.T) 3883.96826171875
bpp_loss 4.423354475409724
13_q proxy err 0.00023894160403870046 err 9.101716995239258 tr(WHW.T) 38091.8046875
bpp_loss 5.005913621978834
13_k proxy err 0.00016009922546800226 err 9.14835262298584 tr(WHW.T) 57141.765625
bpp_loss 5.035884304903448
13_o proxy err 0.0020310869440436363 err 6.889988899230957 tr(WHW.T) 3392.266845703125
bpp_loss 4.41702339949552
13_up proxy err 0.001404880080372095 err 31.874025344848633 tr(WHW.T) 22688.076171875
bpp_loss 4.485218459350425
13_gate proxy err 0.0007408242672681808 err 32.067710876464844 tr(WHW.T) 43286.52734375
bpp_loss 4.516793284305306
13_down proxy err 0.0019221034599468112 err 30.274219512939453 tr(WHW.T) 15750.5673828125
bpp_loss 4.52089748644205
I0403 07:20:11.399016 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 14 in 0.822490930557251s
I0403 07:20:15.252929 3522020 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:20:15.253024 3522020 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:20:15.253064 3522020 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:20:15.654872 3522020 config.py:54] PyTorch version 2.6.0 available.
W0403 07:20:15.848552 3522020 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:20:16.516508 3522020 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:20:16.520503 3509055 quantize_finetune_llama.py:214] layer 15 gpu 1
I0403 07:20:16.705657 3522020 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:20:18.061104 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 15 in 1.0978741645812988s
I0403 07:20:21.939176 3522220 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:20:21.939326 3522220 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:20:21.939369 3522220 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:20:22.358513 3522220 config.py:54] PyTorch version 2.6.0 available.
W0403 07:20:22.574021 3522220 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:20:23.251040 3522220 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:20:23.255159 3509055 quantize_finetune_llama.py:214] layer 16 gpu 0
I0403 07:20:23.421001 3522220 data_utils.py:336] using 256 training seqs, 128 validation seqs
14_v proxy err 0.002359117614105344 err 8.605045318603516 tr(WHW.T) 3647.569580078125
bpp_loss 4.409206076641567
14_q proxy err 0.0002415795752312988 err 8.898794174194336 tr(WHW.T) 36835.87109375
bpp_loss 5.003088028053753
14_k proxy err 0.0001523111277492717 err 8.96268081665039 tr(WHW.T) 58844.5546875
bpp_loss 5.03255011420697
14_o proxy err 0.0022927115205675364 err 7.0246052742004395 tr(WHW.T) 3063.885498046875
bpp_loss 4.39161614398472
14_up proxy err 0.0014550838386639953 err 32.6705207824707 tr(WHW.T) 22452.671875
bpp_loss 4.4844473649769325
14_gate proxy err 0.0007964990800246596 err 32.848365783691406 tr(WHW.T) 41240.93359375
bpp_loss 4.512252983745447
14_down proxy err 0.0019733186345547438 err 30.4078311920166 tr(WHW.T) 15409.48828125
bpp_loss 4.523661882811507
15_v proxy err 0.002130357548594475 err 8.52793025970459 tr(WHW.T) 4003.05126953125
bpp_loss 4.448238987242803
15_q proxy err 0.00022934730804990977 err 8.796236991882324 tr(WHW.T) 38353.34765625
bpp_loss 4.98577110178303
15_k proxy err 0.00015105123748071492 err 8.85174560546875 tr(WHW.T) 58600.94921875
bpp_loss 5.044951223535463
15_o proxy err 0.0020133955404162407 err 7.305356025695801 tr(WHW.T) 3628.3759765625
bpp_loss 4.416893454617821
15_up proxy err 0.0013932415749877691 err 32.13700485229492 tr(WHW.T) 23066.35546875
bpp_loss 4.491483445344276
15_gate proxy err 0.0007885383674874902 err 32.2877197265625 tr(WHW.T) 40946.2890625
bpp_loss 4.519712397698746
15_down proxy err 0.0018992217956110835 err 29.30316925048828 tr(WHW.T) 15429.0400390625
bpp_loss 4.537247463107803
I0403 07:22:27.137069 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 16 in 1.0641708374023438s
I0403 07:22:31.101601 3523862 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:22:31.101699 3523862 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:22:31.101737 3523862 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:22:31.512241 3523862 config.py:54] PyTorch version 2.6.0 available.
W0403 07:22:31.707737 3523862 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:22:32.380501 3523862 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:22:32.384361 3509055 quantize_finetune_llama.py:214] layer 17 gpu 1
I0403 07:22:32.532592 3523862 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:22:34.609759 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 17 in 1.7605876922607422s
I0403 07:22:38.661492 3524080 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:22:38.661624 3524080 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:22:38.661668 3524080 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:22:39.062945 3524080 config.py:54] PyTorch version 2.6.0 available.
W0403 07:22:39.293324 3524080 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:22:40.015502 3524080 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:22:40.019696 3509055 quantize_finetune_llama.py:214] layer 18 gpu 0
I0403 07:22:40.264575 3524080 data_utils.py:336] using 256 training seqs, 128 validation seqs
16_v proxy err 0.002207718789577484 err 8.806041717529297 tr(WHW.T) 3988.751708984375
bpp_loss 4.472471564891748
16_q proxy err 0.00024469150230288506 err 9.069400787353516 tr(WHW.T) 37064.6328125
bpp_loss 4.9600898082135245
16_k proxy err 0.00015217707550618798 err 9.127352714538574 tr(WHW.T) 59978.5
bpp_loss 5.003027610247955
16_o proxy err 0.0017847209237515926 err 8.379666328430176 tr(WHW.T) 4695.22509765625
bpp_loss 4.424828858813271
16_up proxy err 0.0013659711694344878 err 32.241844177246094 tr(WHW.T) 23603.60546875
bpp_loss 4.4867924943912865
16_gate proxy err 0.0007680643466301262 err 32.392913818359375 tr(WHW.T) 42174.73828125
bpp_loss 4.524974457313155
16_down proxy err 0.001942626666277647 err 29.583284378051758 tr(WHW.T) 15228.4970703125
bpp_loss 4.535357135586267
17_v proxy err 0.0021907149348407984 err 9.34078598022461 tr(WHW.T) 4263.80712890625
bpp_loss 4.449989918619394
17_q proxy err 0.00026420652284286916 err 9.615200996398926 tr(WHW.T) 36392.74609375
bpp_loss 4.926837734179571
17_k proxy err 0.00017780759662855417 err 9.670722961425781 tr(WHW.T) 54388.6953125
bpp_loss 4.9618267639307305
17_o proxy err 0.002041330561041832 err 8.773483276367188 tr(WHW.T) 4297.923828125
bpp_loss 4.430692603928037
17_up proxy err 0.001557769370265305 err 33.67168426513672 tr(WHW.T) 21615.3203125
bpp_loss 4.479020569646774
17_gate proxy err 0.0008393039461225271 err 33.82428741455078 tr(WHW.T) 40300.40234375
bpp_loss 4.534177889830844
17_down proxy err 0.002046240959316492 err 31.41681480407715 tr(WHW.T) 15353.4287109375
bpp_loss 4.522219580446565
I0403 07:24:44.574944 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 18 in 0.8404474258422852s
I0403 07:24:47.987378 3525648 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:24:47.987480 3525648 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:24:47.987520 3525648 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:24:48.309260 3525648 config.py:54] PyTorch version 2.6.0 available.
W0403 07:24:48.496587 3525648 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:24:49.043390 3525648 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:24:49.046894 3509055 quantize_finetune_llama.py:214] layer 19 gpu 1
I0403 07:24:49.480727 3525648 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:24:50.383599 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 19 in 0.9062745571136475s
I0403 07:24:53.804856 3525855 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:24:53.804949 3525855 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:24:53.804987 3525855 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:24:54.126534 3525855 config.py:54] PyTorch version 2.6.0 available.
W0403 07:24:54.313652 3525855 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:24:54.845487 3525855 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:24:54.849094 3509055 quantize_finetune_llama.py:214] layer 20 gpu 0
I0403 07:24:55.030672 3525855 data_utils.py:336] using 256 training seqs, 128 validation seqs
18_v proxy err 0.0021293608006089926 err 9.950691223144531 tr(WHW.T) 4673.08837890625
bpp_loss 4.4816529750823975
18_q proxy err 0.000289552757749334 err 10.203201293945312 tr(WHW.T) 35237.796875
bpp_loss 4.874500449281186
18_k proxy err 0.00020854841568507254 err 10.243732452392578 tr(WHW.T) 49119.20703125
bpp_loss 4.909450285485946
18_o proxy err 0.001788854831829667 err 8.806986808776855 tr(WHW.T) 4923.25390625
bpp_loss 4.474233215209097
18_up proxy err 0.0016701037529855967 err 33.903018951416016 tr(WHW.T) 20299.947265625
bpp_loss 4.474601650047441
18_gate proxy err 0.000894426426384598 err 34.05283737182617 tr(WHW.T) 38072.26171875
bpp_loss 4.545329507786867
18_down proxy err 0.002018323866650462 err 30.81187629699707 tr(WHW.T) 15266.072265625
bpp_loss 4.531245673941665
19_v proxy err 0.0020882163662463427 err 10.00027084350586 tr(WHW.T) 4788.90576171875
bpp_loss 4.487826707074419
19_q proxy err 0.00031160053913481534 err 10.245353698730469 tr(WHW.T) 32879.76953125
bpp_loss 4.850600300123915
19_k proxy err 0.00020598423725459725 err 10.296072959899902 tr(WHW.T) 49984.76171875
bpp_loss 4.878046574187465
19_o proxy err 0.001918709371238947 err 9.603504180908203 tr(WHW.T) 5005.189453125
bpp_loss 4.468594046775252
19_up proxy err 0.0016969722928479314 err 34.23237991333008 tr(WHW.T) 20172.62109375
bpp_loss 4.474026160617901
19_gate proxy err 0.0009911134839057922 err 34.39034652709961 tr(WHW.T) 34698.6953125
bpp_loss 4.549747477610444
19_down proxy err 0.001987892435863614 err 31.204545974731445 tr(WHW.T) 15697.3017578125
bpp_loss 4.532658530355886
I0403 07:26:47.076059 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 20 in 0.8246169090270996s
I0403 07:26:50.475850 3527138 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:26:50.475952 3527138 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:26:50.475991 3527138 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:26:50.802913 3527138 config.py:54] PyTorch version 2.6.0 available.
W0403 07:26:50.990508 3527138 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:26:51.536811 3527138 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:26:51.540561 3509055 quantize_finetune_llama.py:214] layer 21 gpu 1
I0403 07:26:51.786805 3527138 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:26:52.867577 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 21 in 0.9069259166717529s
I0403 07:26:56.309927 3527264 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:26:56.310025 3527264 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:26:56.310064 3527264 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:26:56.635863 3527264 config.py:54] PyTorch version 2.6.0 available.
W0403 07:26:56.823325 3527264 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:26:57.390271 3527264 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:26:57.393846 3509055 quantize_finetune_llama.py:214] layer 22 gpu 0
I0403 07:26:57.552810 3527264 data_utils.py:336] using 256 training seqs, 128 validation seqs
20_v proxy err 0.0021977864671498537 err 10.210539817810059 tr(WHW.T) 4645.8291015625
bpp_loss 4.497998762060888
20_q proxy err 0.0003094262210652232 err 10.463327407836914 tr(WHW.T) 33815.2578125
bpp_loss 4.855237585958093
20_k proxy err 0.00021392606140580028 err 10.514464378356934 tr(WHW.T) 49149.9921875
bpp_loss 4.881610885960981
20_o proxy err 0.0013644018908962607 err 9.305793762207031 tr(WHW.T) 6820.419921875
bpp_loss 4.493967791670002
20_up proxy err 0.0016561580123379827 err 34.12196350097656 tr(WHW.T) 20603.083984375
bpp_loss 4.472346980384616
20_gate proxy err 0.000963943253736943 err 34.22960662841797 tr(WHW.T) 35509.98046875
bpp_loss 4.556582737020975
20_down proxy err 0.0019391585374251008 err 30.64470863342285 tr(WHW.T) 15803.095703125
bpp_loss 4.538226281314395
21_v proxy err 0.002149682492017746 err 10.45338249206543 tr(WHW.T) 4862.75634765625
bpp_loss 4.522907666745596
21_q proxy err 0.00035282588214613497 err 10.677055358886719 tr(WHW.T) 30261.54296875
bpp_loss 4.809842374408618
21_k proxy err 0.0002506615128368139 err 10.720399856567383 tr(WHW.T) 42768.43359375
bpp_loss 4.823374776053242
21_o proxy err 0.0017427169950678945 err 11.15208911895752 tr(WHW.T) 6399.25439453125
bpp_loss 4.479918028577231
21_up proxy err 0.0017767196986824274 err 34.75776290893555 tr(WHW.T) 19562.884765625
bpp_loss 4.470021402766538
21_gate proxy err 0.00104878272395581 err 34.8735466003418 tr(WHW.T) 33251.44921875
bpp_loss 4.565609169647444
21_down proxy err 0.002031937474384904 err 32.08882522583008 tr(WHW.T) 15792.2314453125
bpp_loss 4.530550950185158
I0403 07:28:46.314312 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 22 in 0.8139760494232178s
I0403 07:28:49.701046 3528450 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:28:49.701138 3528450 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:28:49.701176 3528450 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:28:50.021339 3528450 config.py:54] PyTorch version 2.6.0 available.
W0403 07:28:50.209310 3528450 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:28:50.740661 3528450 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:28:50.744308 3509055 quantize_finetune_llama.py:214] layer 23 gpu 1
I0403 07:28:51.088704 3528450 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:28:52.248447 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 23 in 1.0862503051757812s
I0403 07:28:55.717108 3528570 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:28:55.717197 3528570 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:28:55.717235 3528570 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:28:56.042093 3528570 config.py:54] PyTorch version 2.6.0 available.
W0403 07:28:56.229162 3528570 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:28:56.792681 3528570 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:28:56.796237 3509055 quantize_finetune_llama.py:214] layer 24 gpu 0
I0403 07:28:57.086248 3528570 data_utils.py:336] using 256 training seqs, 128 validation seqs
22_v proxy err 0.0020576633978635073 err 10.518635749816895 tr(WHW.T) 5111.93212890625
bpp_loss 4.529687842354178
22_q proxy err 0.0003346459416206926 err 10.743276596069336 tr(WHW.T) 32103.412109375
bpp_loss 4.847811779240146
22_k proxy err 0.0002452668850310147 err 10.784319877624512 tr(WHW.T) 43969.734375
bpp_loss 4.865630122367293
22_o proxy err 0.0013354468392208219 err 10.186104774475098 tr(WHW.T) 7627.48828125
bpp_loss 4.510453518596478
22_up proxy err 0.0018041115254163742 err 35.07191848754883 tr(WHW.T) 19439.994140625
bpp_loss 4.4689113842540005
22_gate proxy err 0.0010745810577645898 err 35.172325134277344 tr(WHW.T) 32731.19921875
bpp_loss 4.573293465341246
22_down proxy err 0.0020541108679026365 err 32.60819625854492 tr(WHW.T) 15874.603515625
bpp_loss 4.527015471224521
23_v proxy err 0.0019597357604652643 err 11.104900360107422 tr(WHW.T) 5666.529296875
bpp_loss 4.568703046417795
23_q proxy err 0.00040081102633848786 err 11.311576843261719 tr(WHW.T) 28221.720703125
bpp_loss 4.821585770347156
23_k proxy err 0.000295567064313218 err 11.342498779296875 tr(WHW.T) 38375.3828125
bpp_loss 4.831899228389375
23_o proxy err 0.0017325924709439278 err 10.993972778320312 tr(WHW.T) 6345.388671875
bpp_loss 4.555507256533019
23_up proxy err 0.0018976441351696849 err 35.588287353515625 tr(WHW.T) 18753.931640625
bpp_loss 4.47494250146109
23_gate proxy err 0.0011734111467376351 err 35.672359466552734 tr(WHW.T) 30400.564453125
bpp_loss 4.573154897710611
23_down proxy err 0.002109829569235444 err 33.37069320678711 tr(WHW.T) 15816.7724609375
bpp_loss 4.533639070642895
I0403 07:30:48.174663 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 24 in 0.7794275283813477s
I0403 07:30:51.620545 3529866 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:30:51.620646 3529866 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:30:51.620685 3529866 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:30:51.942919 3529866 config.py:54] PyTorch version 2.6.0 available.
W0403 07:30:52.129983 3529866 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:30:52.690161 3529866 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:30:52.693740 3509055 quantize_finetune_llama.py:214] layer 25 gpu 1
I0403 07:30:52.926938 3529866 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:30:53.999425 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 25 in 0.913177490234375s
I0403 07:30:57.552133 3529989 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:30:57.552231 3529989 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:30:57.552269 3529989 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:30:57.874382 3529989 config.py:54] PyTorch version 2.6.0 available.
W0403 07:30:58.059909 3529989 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:30:58.643142 3529989 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:30:58.646729 3509055 quantize_finetune_llama.py:214] layer 26 gpu 0
I0403 07:30:58.845055 3529989 data_utils.py:336] using 256 training seqs, 128 validation seqs
24_v proxy err 0.0020367742981761694 err 10.8452730178833 tr(WHW.T) 5324.72998046875
bpp_loss 4.56974495400209
24_q proxy err 0.00040895448182709515 err 11.046422004699707 tr(WHW.T) 27011.373046875
bpp_loss 4.786467341240495
24_k proxy err 0.00027897238032892346 err 11.090946197509766 tr(WHW.T) 39756.4296875
bpp_loss 4.789937500259839
24_o proxy err 0.001284765312448144 err 10.327733993530273 tr(WHW.T) 8038.615234375
bpp_loss 4.540224014897831
24_up proxy err 0.0019436512375250459 err 35.98280715942383 tr(WHW.T) 18512.99609375
bpp_loss 4.478234598109888
24_gate proxy err 0.0011957407696172595 err 36.077491760253906 tr(WHW.T) 30171.666015625
bpp_loss 4.576178500558748
24_down proxy err 0.0021288376301527023 err 33.5424919128418 tr(WHW.T) 15756.2470703125
bpp_loss 4.540122446842318
25_v proxy err 0.0019603187683969736 err 11.619804382324219 tr(WHW.T) 5927.5078125
bpp_loss 4.595423870836385
25_q proxy err 0.000471297389594838 err 11.80204963684082 tr(WHW.T) 25041.619140625
bpp_loss 4.780333790928125
25_k proxy err 0.00035178748657926917 err 11.831657409667969 tr(WHW.T) 33632.96875
bpp_loss 4.7831051968969405
25_o proxy err 0.0016640094108879566 err 11.280430793762207 tr(WHW.T) 6779.06689453125
bpp_loss 4.557802937575616
25_up proxy err 0.0019412620458751917 err 36.15412139892578 tr(WHW.T) 18624.029296875
bpp_loss 4.484187896081875
25_gate proxy err 0.0011674464913085103 err 36.253517150878906 tr(WHW.T) 31053.6875
bpp_loss 4.5795105790156265
25_down proxy err 0.002025916473940015 err 32.16631317138672 tr(WHW.T) 15877.4140625
bpp_loss 4.562693285517568
I0403 07:32:47.509404 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 26 in 0.8530986309051514s
I0403 07:32:51.016003 3531074 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:32:51.016103 3531074 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:32:51.016143 3531074 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:32:51.340138 3531074 config.py:54] PyTorch version 2.6.0 available.
W0403 07:32:51.527565 3531074 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:32:52.146159 3531074 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:32:52.149957 3509055 quantize_finetune_llama.py:214] layer 27 gpu 1
I0403 07:32:52.334937 3531074 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:32:53.493014 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 27 in 0.8785598278045654s
I0403 07:32:56.910502 3531188 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:32:56.910594 3531188 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:32:56.910631 3531188 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:32:57.233623 3531188 config.py:54] PyTorch version 2.6.0 available.
W0403 07:32:57.421524 3531188 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:32:58.073743 3531188 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:32:58.077298 3509055 quantize_finetune_llama.py:214] layer 28 gpu 0
I0403 07:32:58.308338 3531188 data_utils.py:336] using 256 training seqs, 128 validation seqs
26_v proxy err 0.0019163351971656084 err 11.346118927001953 tr(WHW.T) 5920.73828125
bpp_loss 4.626245597144589
26_q proxy err 0.0004319398431107402 err 11.5377779006958 tr(WHW.T) 26711.5390625
bpp_loss 4.763134331442416
26_k proxy err 0.0003082082839682698 err 11.568166732788086 tr(WHW.T) 37533.6015625
bpp_loss 4.771795570850372
26_o proxy err 0.0008894004859030247 err 8.774877548217773 tr(WHW.T) 9866.05859375
bpp_loss 4.6482052826322615
26_up proxy err 0.00181943632196635 err 36.127296447753906 tr(WHW.T) 19856.3125
bpp_loss 4.488410982195028
26_gate proxy err 0.0010825450299307704 err 36.2158203125 tr(WHW.T) 33454.33203125
bpp_loss 4.583755348484184
26_down proxy err 0.0020121242851018906 err 31.03809928894043 tr(WHW.T) 15425.537109375
bpp_loss 4.575469417280929
27_v proxy err 0.001851536799222231 err 12.104969024658203 tr(WHW.T) 6537.79541015625
bpp_loss 4.604320988175459
27_q proxy err 0.0004370271635707468 err 12.297755241394043 tr(WHW.T) 28139.56640625
bpp_loss 4.800253557274118
27_k proxy err 0.0003167677787132561 err 12.315403938293457 tr(WHW.T) 38878.3359375
bpp_loss 4.815191792906262
27_o proxy err 0.0012738743098452687 err 9.25635051727295 tr(WHW.T) 7266.2978515625
bpp_loss 4.665349851711653
27_up proxy err 0.0016416340367868543 err 35.8259162902832 tr(WHW.T) 21823.326171875
bpp_loss 4.495125614799733
27_gate proxy err 0.0010115699842572212 err 35.90163040161133 tr(WHW.T) 35491.0
bpp_loss 4.585700569457786
27_down proxy err 0.0018744205590337515 err 28.339385986328125 tr(WHW.T) 15119.01171875
bpp_loss 4.6031932346089635
I0403 07:34:49.036784 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 28 in 0.8283147811889648s
I0403 07:34:52.425826 3532217 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:34:52.425924 3532217 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:34:52.425961 3532217 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:34:52.744776 3532217 config.py:54] PyTorch version 2.6.0 available.
W0403 07:34:52.930372 3532217 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:34:53.466162 3532217 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:34:53.469765 3509055 quantize_finetune_llama.py:214] layer 29 gpu 1
I0403 07:34:54.030229 3532217 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:34:54.907881 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 29 in 1.0554838180541992s
I0403 07:34:58.358586 3532328 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:34:58.358683 3532328 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:34:58.358721 3532328 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:34:58.678315 3532328 config.py:54] PyTorch version 2.6.0 available.
W0403 07:34:58.866266 3532328 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:34:59.406652 3532328 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:34:59.410163 3509055 quantize_finetune_llama.py:214] layer 30 gpu 0
I0403 07:34:59.575551 3532328 data_utils.py:336] using 256 training seqs, 128 validation seqs
28_v proxy err 0.001696443185210228 err 12.006589889526367 tr(WHW.T) 7077.5078125
bpp_loss 4.647367434808984
28_q proxy err 0.00045127945486456156 err 12.185744285583496 tr(WHW.T) 27002.65625
bpp_loss 4.751513330847956
28_k proxy err 0.0003275801718700677 err 12.200242042541504 tr(WHW.T) 37243.53125
bpp_loss 4.7691790636163205
28_o proxy err 0.0009680568473413587 err 8.600228309631348 tr(WHW.T) 8884.01171875
bpp_loss 4.693620874546468
28_up proxy err 0.0013527234550565481 err 35.38882827758789 tr(WHW.T) 26161.169921875
bpp_loss 4.508018965897865
28_gate proxy err 0.0009641128708608449 err 35.473628997802734 tr(WHW.T) 36794.0625
bpp_loss 4.580264845299859
28_down proxy err 0.001647215336561203 err 24.68264389038086 tr(WHW.T) 14984.466796875
bpp_loss 4.63981027667259
29_v proxy err 0.0017532507190480828 err 11.715858459472656 tr(WHW.T) 6682.36328125
bpp_loss 4.657776843872853
29_q proxy err 0.00044095722842030227 err 11.908515930175781 tr(WHW.T) 27006.056640625
bpp_loss 4.713559817406349
29_k proxy err 0.00030233804136514664 err 11.939129829406738 tr(WHW.T) 39489.33984375
bpp_loss 4.723422407754697
29_o proxy err 0.0009868881898000836 err 10.460076332092285 tr(WHW.T) 10599.048828125
bpp_loss 4.667545050382614
29_up proxy err 0.0010617529042065144 err 34.91480255126953 tr(WHW.T) 32884.11328125
bpp_loss 4.519982144409834
29_gate proxy err 0.0008757890318520367 err 34.98162841796875 tr(WHW.T) 39942.984375
bpp_loss 4.584287216410387
29_down proxy err 0.001415969803929329 err 20.860837936401367 tr(WHW.T) 14732.544921875
bpp_loss 4.67371580606803
I0403 07:36:47.949987 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 30 in 0.8266723155975342s
I0403 07:36:51.450317 3533336 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:36:51.450421 3533336 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:36:51.450459 3533336 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:36:51.777132 3533336 config.py:54] PyTorch version 2.6.0 available.
W0403 07:36:51.965493 3533336 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:36:52.512779 3533336 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:36:52.516399 3509055 quantize_finetune_llama.py:214] layer 31 gpu 1
I0403 07:36:52.774778 3533336 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 07:36:53.866715 3509055 quantize_finetune_llama.py:245] computed original embedding for layer 31 in 0.9358649253845215s
I0403 07:36:57.285688 3533447 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:36:57.285786 3533447 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:36:57.285824 3533447 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:36:57.606912 3533447 config.py:54] PyTorch version 2.6.0 available.
W0403 07:36:57.794097 3533447 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 07:36:58.339585 3533447 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 07:36:58.647031 3533447 data_utils.py:336] using 256 training seqs, 128 validation seqs
30_v proxy err 0.001477520912885666 err 12.126790046691895 tr(WHW.T) 8207.525390625
bpp_loss 4.672142921714112
30_q proxy err 0.0004303008900023997 err 12.281081199645996 tr(WHW.T) 28540.68359375
bpp_loss 4.703954688622616
30_k proxy err 0.0003199552884325385 err 12.300758361816406 tr(WHW.T) 38445.2421875
bpp_loss 4.723737466847524
30_o proxy err 0.0008153629023581743 err 8.286624908447266 tr(WHW.T) 10163.1123046875
bpp_loss 4.75217201653868
30_up proxy err 0.0006265552365221083 err 33.755008697509766 tr(WHW.T) 53873.95703125
bpp_loss 4.541592647862989
30_gate proxy err 0.0005716060986742377 err 33.82067108154297 tr(WHW.T) 59167.79296875
bpp_loss 4.616625637769006
30_down proxy err 0.0003823318984359503 err 9.880411148071289 tr(WHW.T) 25842.498046875
bpp_loss 4.760266438992911
31_v proxy err 0.001694809994660318 err 11.423592567443848 tr(WHW.T) 6740.33837890625
bpp_loss 4.545011500362307
31_q proxy err 0.00031809069332666695 err 11.67329216003418 tr(WHW.T) 36698.0
bpp_loss 4.719989145640284
31_k proxy err 0.00021342758554965258 err 11.694510459899902 tr(WHW.T) 54793.80859375
bpp_loss 4.776535171782598
31_o proxy err 0.00047168537275865674 err 6.189264297485352 tr(WHW.T) 13121.5947265625
bpp_loss 4.698661889880896
31_up proxy err 0.00032721360912546515 err 31.3399658203125 tr(WHW.T) 95778.3046875
bpp_loss 4.594437815595505
31_gate proxy err 0.0003216617042198777 err 31.37824249267578 tr(WHW.T) 97550.4453125
bpp_loss 4.682496878123561
31_down proxy err 0.00010543612734181806 err 3.9039504528045654 tr(WHW.T) 37026.6875
bpp_loss 4.813567408208931
I0403 07:38:57.421800 3534489 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:38:57.421943 3534489 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:38:57.421989 3534489 utils.py:162] NumExpr defaulting to 16 threads.
I0403 07:38:57.855398 3534489 config.py:54] PyTorch version 2.6.0 available.
W0403 07:38:58.057245 3534489 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0403 07:38:58.164668 3534489 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  4.26it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  4.90it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  5.41it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  4.77it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  5.21it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.07it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.02it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  5.30it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  5.76it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  5.39it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  4.96it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  4.76it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.67it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.90it/s]
I0403 07:39:01.941815 3534489 hfize_llama.py:161] loaded layer 0
I0403 07:39:02.934933 3534489 hfize_llama.py:161] loaded layer 1
I0403 07:39:03.847779 3534489 hfize_llama.py:161] loaded layer 2
I0403 07:39:04.784836 3534489 hfize_llama.py:161] loaded layer 3
I0403 07:39:05.707579 3534489 hfize_llama.py:161] loaded layer 4
I0403 07:39:06.606309 3534489 hfize_llama.py:161] loaded layer 5
I0403 07:39:07.527092 3534489 hfize_llama.py:161] loaded layer 6
I0403 07:39:08.487190 3534489 hfize_llama.py:161] loaded layer 7
I0403 07:39:09.415832 3534489 hfize_llama.py:161] loaded layer 8
I0403 07:39:10.447674 3534489 hfize_llama.py:161] loaded layer 9
I0403 07:39:11.239535 3534489 hfize_llama.py:161] loaded layer 10
I0403 07:39:12.032898 3534489 hfize_llama.py:161] loaded layer 11
I0403 07:39:12.837316 3534489 hfize_llama.py:161] loaded layer 12
I0403 07:39:13.678750 3534489 hfize_llama.py:161] loaded layer 13
I0403 07:39:14.732682 3534489 hfize_llama.py:161] loaded layer 14
I0403 07:39:15.632300 3534489 hfize_llama.py:161] loaded layer 15
I0403 07:39:16.498388 3534489 hfize_llama.py:161] loaded layer 16
I0403 07:39:17.331756 3534489 hfize_llama.py:161] loaded layer 17
I0403 07:39:18.256547 3534489 hfize_llama.py:161] loaded layer 18
I0403 07:39:19.112764 3534489 hfize_llama.py:161] loaded layer 19
I0403 07:39:19.813160 3534489 hfize_llama.py:161] loaded layer 20
I0403 07:39:20.537369 3534489 hfize_llama.py:161] loaded layer 21
I0403 07:39:21.235165 3534489 hfize_llama.py:161] loaded layer 22
I0403 07:39:21.981626 3534489 hfize_llama.py:161] loaded layer 23
I0403 07:39:23.119006 3534489 hfize_llama.py:161] loaded layer 24
I0403 07:39:24.043215 3534489 hfize_llama.py:161] loaded layer 25
I0403 07:39:24.948702 3534489 hfize_llama.py:161] loaded layer 26
I0403 07:39:25.847421 3534489 hfize_llama.py:161] loaded layer 27
I0403 07:39:26.730737 3534489 hfize_llama.py:161] loaded layer 28
I0403 07:39:27.639055 3534489 hfize_llama.py:161] loaded layer 29
I0403 07:39:28.555899 3534489 hfize_llama.py:161] loaded layer 30
I0403 07:39:29.466832 3534489 hfize_llama.py:161] loaded layer 31
I0403 07:39:29.467016 3534489 hfize_llama.py:165] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:07,  1.51s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.13s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.09s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.05s/it]
I0403 07:40:22.734676 3534489 hfize_llama.py:175] successfully loaded hfized model
I0403 07:40:27.349589 3535447 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 07:40:27.349708 3535447 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 07:40:27.349750 3535447 utils.py:162] NumExpr defaulting to 16 threads.
W0403 07:40:27.717224 3535447 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0403 07:40:28.224772 3535447 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.08it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.04it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.02it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.02s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.02s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.07it/s]
I0403 07:40:33.942309 3535447 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/166 [00:00<?, ?it/s]avg_loss = 1.3677341938018799:   0%|          | 0/166 [00:01<?, ?it/s]avg_loss = 1.3677341938018799:   1%|          | 1/166 [00:01<04:22,  1.59s/it]avg_loss = 1.6333139538764954:   1%|          | 1/166 [00:02<04:22,  1.59s/it]avg_loss = 1.6333139538764954:   1%|          | 2/166 [00:02<03:41,  1.35s/it]avg_loss = 1.7985849777857463:   1%|          | 2/166 [00:03<03:41,  1.35s/it]avg_loss = 1.7985849777857463:   2%|▏         | 3/166 [00:03<03:28,  1.28s/it]avg_loss = 1.830586701631546:   2%|▏         | 3/166 [00:05<03:28,  1.28s/it] avg_loss = 1.830586701631546:   2%|▏         | 4/166 [00:05<03:21,  1.25s/it]avg_loss = 1.763895082473755:   2%|▏         | 4/166 [00:06<03:21,  1.25s/it]avg_loss = 1.763895082473755:   3%|▎         | 5/166 [00:06<03:17,  1.23s/it]avg_loss = 1.7399940093358357:   3%|▎         | 5/166 [00:07<03:17,  1.23s/it]avg_loss = 1.7399940093358357:   4%|▎         | 6/166 [00:07<03:14,  1.22s/it]avg_loss = 1.6784929037094116:   4%|▎         | 6/166 [00:08<03:14,  1.22s/it]avg_loss = 1.6784929037094116:   4%|▍         | 7/166 [00:08<03:12,  1.21s/it]avg_loss = 1.6209593713283539:   4%|▍         | 7/166 [00:09<03:12,  1.21s/it]avg_loss = 1.6209593713283539:   5%|▍         | 8/166 [00:09<03:11,  1.21s/it]avg_loss = 1.6157879167132907:   5%|▍         | 8/166 [00:11<03:11,  1.21s/it]avg_loss = 1.6157879167132907:   5%|▌         | 9/166 [00:11<03:09,  1.21s/it]avg_loss = 1.6214389324188232:   5%|▌         | 9/166 [00:12<03:09,  1.21s/it]avg_loss = 1.6214389324188232:   6%|▌         | 10/166 [00:12<03:08,  1.21s/it]avg_loss = 1.6376710046421399:   6%|▌         | 10/166 [00:13<03:08,  1.21s/it]avg_loss = 1.6376710046421399:   7%|▋         | 11/166 [00:13<03:07,  1.21s/it]avg_loss = 1.6476833025614421:   7%|▋         | 11/166 [00:14<03:07,  1.21s/it]avg_loss = 1.6476833025614421:   7%|▋         | 12/166 [00:14<03:06,  1.21s/it]avg_loss = 1.6432044872870812:   7%|▋         | 12/166 [00:16<03:06,  1.21s/it]avg_loss = 1.6432044872870812:   8%|▊         | 13/166 [00:16<03:05,  1.21s/it]avg_loss = 1.6558208465576172:   8%|▊         | 13/166 [00:17<03:05,  1.21s/it]avg_loss = 1.6558208465576172:   8%|▊         | 14/166 [00:17<03:04,  1.21s/it]avg_loss = 1.6739885330200195:   8%|▊         | 14/166 [00:18<03:04,  1.21s/it]avg_loss = 1.6739885330200195:   9%|▉         | 15/166 [00:18<03:03,  1.22s/it]avg_loss = 1.6938519552350044:   9%|▉         | 15/166 [00:19<03:03,  1.22s/it]avg_loss = 1.6938519552350044:  10%|▉         | 16/166 [00:19<03:02,  1.22s/it]avg_loss = 1.7076009021085852:  10%|▉         | 16/166 [00:20<03:02,  1.22s/it]avg_loss = 1.7076009021085852:  10%|█         | 17/166 [00:20<03:01,  1.22s/it]avg_loss = 1.7223738365703158:  10%|█         | 17/166 [00:22<03:01,  1.22s/it]avg_loss = 1.7223738365703158:  11%|█         | 18/166 [00:22<03:00,  1.22s/it]avg_loss = 1.742498517036438:  11%|█         | 18/166 [00:23<03:00,  1.22s/it] avg_loss = 1.742498517036438:  11%|█▏        | 19/166 [00:23<03:00,  1.22s/it]avg_loss = 1.749161946773529:  11%|█▏        | 19/166 [00:24<03:00,  1.22s/it]avg_loss = 1.749161946773529:  12%|█▏        | 20/166 [00:24<02:59,  1.23s/it]avg_loss = 1.7497616892769223:  12%|█▏        | 20/166 [00:25<02:59,  1.23s/it]avg_loss = 1.7497616892769223:  13%|█▎        | 21/166 [00:25<02:58,  1.23s/it]avg_loss = 1.7400134205818176:  13%|█▎        | 21/166 [00:27<02:58,  1.23s/it]avg_loss = 1.7400134205818176:  13%|█▎        | 22/166 [00:27<02:57,  1.23s/it]avg_loss = 1.7292379503664763:  13%|█▎        | 22/166 [00:28<02:57,  1.23s/it]avg_loss = 1.7292379503664763:  14%|█▍        | 23/166 [00:28<02:56,  1.23s/it]avg_loss = 1.7366536657015483:  14%|█▍        | 23/166 [00:29<02:56,  1.23s/it]avg_loss = 1.7366536657015483:  14%|█▍        | 24/166 [00:29<02:55,  1.23s/it]avg_loss = 1.7441581344604493:  14%|█▍        | 24/166 [00:30<02:55,  1.23s/it]avg_loss = 1.7441581344604493:  15%|█▌        | 25/166 [00:30<02:54,  1.24s/it]avg_loss = 1.74869704246521:  15%|█▌        | 25/166 [00:32<02:54,  1.24s/it]  avg_loss = 1.74869704246521:  16%|█▌        | 26/166 [00:32<02:53,  1.24s/it]avg_loss = 1.7553354192663122:  16%|█▌        | 26/166 [00:33<02:53,  1.24s/it]avg_loss = 1.7553354192663122:  16%|█▋        | 27/166 [00:33<02:52,  1.24s/it]avg_loss = 1.7584523132869176:  16%|█▋        | 27/166 [00:34<02:52,  1.24s/it]avg_loss = 1.7584523132869176:  17%|█▋        | 28/166 [00:34<02:51,  1.24s/it]avg_loss = 1.7682669080536941:  17%|█▋        | 28/166 [00:35<02:51,  1.24s/it]avg_loss = 1.7682669080536941:  17%|█▋        | 29/166 [00:35<02:50,  1.24s/it]avg_loss = 1.7683273116747538:  17%|█▋        | 29/166 [00:37<02:50,  1.24s/it]avg_loss = 1.7683273116747538:  18%|█▊        | 30/166 [00:37<02:49,  1.24s/it]avg_loss = 1.782444542454135:  18%|█▊        | 30/166 [00:38<02:49,  1.24s/it] avg_loss = 1.782444542454135:  19%|█▊        | 31/166 [00:38<02:48,  1.25s/it]avg_loss = 1.7889933921396732:  19%|█▊        | 31/166 [00:39<02:48,  1.25s/it]avg_loss = 1.7889933921396732:  19%|█▉        | 32/166 [00:39<02:47,  1.25s/it]avg_loss = 1.7940821394775852:  19%|█▉        | 32/166 [00:40<02:47,  1.25s/it]avg_loss = 1.7940821394775852:  20%|█▉        | 33/166 [00:40<02:46,  1.25s/it]avg_loss = 1.7930471546509688:  20%|█▉        | 33/166 [00:42<02:46,  1.25s/it]avg_loss = 1.7930471546509688:  20%|██        | 34/166 [00:42<02:45,  1.25s/it]avg_loss = 1.7867771455219814:  20%|██        | 34/166 [00:43<02:45,  1.25s/it]avg_loss = 1.7867771455219814:  21%|██        | 35/166 [00:43<02:44,  1.25s/it]avg_loss = 1.7784007390340169:  21%|██        | 35/166 [00:44<02:44,  1.25s/it]avg_loss = 1.7784007390340169:  22%|██▏       | 36/166 [00:44<02:43,  1.25s/it]avg_loss = 1.7685036659240723:  22%|██▏       | 36/166 [00:45<02:43,  1.25s/it]avg_loss = 1.7685036659240723:  22%|██▏       | 37/166 [00:45<02:42,  1.26s/it]avg_loss = 1.765635675505588:  22%|██▏       | 37/166 [00:47<02:42,  1.26s/it] avg_loss = 1.765635675505588:  23%|██▎       | 38/166 [00:47<02:40,  1.26s/it]avg_loss = 1.763592310440846:  23%|██▎       | 38/166 [00:48<02:40,  1.26s/it]avg_loss = 1.763592310440846:  23%|██▎       | 39/166 [00:48<02:39,  1.26s/it]avg_loss = 1.766943246126175:  23%|██▎       | 39/166 [00:49<02:39,  1.26s/it]avg_loss = 1.766943246126175:  24%|██▍       | 40/166 [00:49<02:38,  1.26s/it]avg_loss = 1.766829871549839:  24%|██▍       | 40/166 [00:50<02:38,  1.26s/it]avg_loss = 1.766829871549839:  25%|██▍       | 41/166 [00:50<02:37,  1.26s/it]avg_loss = 1.754472914196196:  25%|██▍       | 41/166 [00:52<02:37,  1.26s/it]avg_loss = 1.754472914196196:  25%|██▌       | 42/166 [00:52<02:36,  1.26s/it]avg_loss = 1.7389782750329306:  25%|██▌       | 42/166 [00:53<02:36,  1.26s/it]avg_loss = 1.7389782750329306:  26%|██▌       | 43/166 [00:53<02:35,  1.26s/it]avg_loss = 1.7286456579511815:  26%|██▌       | 43/166 [00:54<02:35,  1.26s/it]avg_loss = 1.7286456579511815:  27%|██▋       | 44/166 [00:54<02:34,  1.26s/it]avg_loss = 1.7149199538760715:  27%|██▋       | 44/166 [00:55<02:34,  1.26s/it]avg_loss = 1.7149199538760715:  27%|██▋       | 45/166 [00:55<02:33,  1.27s/it]avg_loss = 1.7044494514879973:  27%|██▋       | 45/166 [00:57<02:33,  1.27s/it]avg_loss = 1.7044494514879973:  28%|██▊       | 46/166 [00:57<02:32,  1.27s/it]avg_loss = 1.6973691214906408:  28%|██▊       | 46/166 [00:58<02:32,  1.27s/it]avg_loss = 1.6973691214906408:  28%|██▊       | 47/166 [00:58<02:30,  1.27s/it]avg_loss = 1.6983405848344166:  28%|██▊       | 47/166 [00:59<02:30,  1.27s/it]avg_loss = 1.6983405848344166:  29%|██▉       | 48/166 [00:59<02:29,  1.27s/it]avg_loss = 1.7090718746185303:  29%|██▉       | 48/166 [01:00<02:29,  1.27s/it]avg_loss = 1.7090718746185303:  30%|██▉       | 49/166 [01:00<02:28,  1.27s/it]avg_loss = 1.7196783638000488:  30%|██▉       | 49/166 [01:02<02:28,  1.27s/it]avg_loss = 1.7196783638000488:  30%|███       | 50/166 [01:02<02:27,  1.27s/it]avg_loss = 1.7265846402037377:  30%|███       | 50/166 [01:03<02:27,  1.27s/it]avg_loss = 1.7265846402037377:  31%|███       | 51/166 [01:03<02:26,  1.27s/it]avg_loss = 1.7315654640014355:  31%|███       | 51/166 [01:04<02:26,  1.27s/it]avg_loss = 1.7315654640014355:  31%|███▏      | 52/166 [01:04<02:25,  1.28s/it]avg_loss = 1.734938990395024:  31%|███▏      | 52/166 [01:06<02:25,  1.28s/it] avg_loss = 1.734938990395024:  32%|███▏      | 53/166 [01:06<02:24,  1.28s/it]avg_loss = 1.7360111430839256:  32%|███▏      | 53/166 [01:07<02:24,  1.28s/it]avg_loss = 1.7360111430839256:  33%|███▎      | 54/166 [01:07<02:23,  1.28s/it]avg_loss = 1.7385849129069935:  33%|███▎      | 54/166 [01:08<02:23,  1.28s/it]avg_loss = 1.7385849129069935:  33%|███▎      | 55/166 [01:08<02:21,  1.28s/it]avg_loss = 1.7421120469059264:  33%|███▎      | 55/166 [01:09<02:21,  1.28s/it]avg_loss = 1.7421120469059264:  34%|███▎      | 56/166 [01:09<02:20,  1.28s/it]avg_loss = 1.7371963931803118:  34%|███▎      | 56/166 [01:11<02:20,  1.28s/it]avg_loss = 1.7371963931803118:  34%|███▍      | 57/166 [01:11<02:19,  1.28s/it]avg_loss = 1.740885736613438:  34%|███▍      | 57/166 [01:12<02:19,  1.28s/it] avg_loss = 1.740885736613438:  35%|███▍      | 58/166 [01:12<02:18,  1.28s/it]avg_loss = 1.73915563599538:  35%|███▍      | 58/166 [01:13<02:18,  1.28s/it] avg_loss = 1.73915563599538:  36%|███▌      | 59/166 [01:13<02:17,  1.28s/it]avg_loss = 1.7342543025811514:  36%|███▌      | 59/166 [01:15<02:17,  1.28s/it]avg_loss = 1.7342543025811514:  36%|███▌      | 60/166 [01:15<02:16,  1.28s/it]avg_loss = 1.7298452052913729:  36%|███▌      | 60/166 [01:16<02:16,  1.28s/it]avg_loss = 1.7298452052913729:  37%|███▋      | 61/166 [01:16<02:14,  1.28s/it]avg_loss = 1.7259097579986817:  37%|███▋      | 61/166 [01:17<02:14,  1.28s/it]avg_loss = 1.7259097579986817:  37%|███▋      | 62/166 [01:17<02:13,  1.28s/it]avg_loss = 1.7200352842845614:  37%|███▋      | 62/166 [01:18<02:13,  1.28s/it]avg_loss = 1.7200352842845614:  38%|███▊      | 63/166 [01:18<02:12,  1.29s/it]avg_loss = 1.7157558761537075:  38%|███▊      | 63/166 [01:20<02:12,  1.29s/it]avg_loss = 1.7157558761537075:  39%|███▊      | 64/166 [01:20<02:11,  1.29s/it]avg_loss = 1.7090869903564454:  39%|███▊      | 64/166 [01:21<02:11,  1.29s/it]avg_loss = 1.7090869903564454:  39%|███▉      | 65/166 [01:21<02:09,  1.29s/it]avg_loss = 1.7018762483741299:  39%|███▉      | 65/166 [01:22<02:09,  1.29s/it]avg_loss = 1.7018762483741299:  40%|███▉      | 66/166 [01:22<02:08,  1.29s/it]avg_loss = 1.695936386264972:  40%|███▉      | 66/166 [01:24<02:08,  1.29s/it] avg_loss = 1.695936386264972:  40%|████      | 67/166 [01:24<02:07,  1.29s/it]avg_loss = 1.6948286470244913:  40%|████      | 67/166 [01:25<02:07,  1.29s/it]avg_loss = 1.6948286470244913:  41%|████      | 68/166 [01:25<02:06,  1.29s/it]avg_loss = 1.6968132412951926:  41%|████      | 68/166 [01:26<02:06,  1.29s/it]avg_loss = 1.6968132412951926:  42%|████▏     | 69/166 [01:26<02:05,  1.29s/it]avg_loss = 1.6998564992632186:  42%|████▏     | 69/166 [01:27<02:05,  1.29s/it]avg_loss = 1.6998564992632186:  42%|████▏     | 70/166 [01:27<02:03,  1.29s/it]avg_loss = 1.70379127750934:  42%|████▏     | 70/166 [01:29<02:03,  1.29s/it]  avg_loss = 1.70379127750934:  43%|████▎     | 71/166 [01:29<02:02,  1.29s/it]avg_loss = 1.708755577603976:  43%|████▎     | 71/166 [01:30<02:02,  1.29s/it]avg_loss = 1.708755577603976:  43%|████▎     | 72/166 [01:30<02:01,  1.29s/it]avg_loss = 1.7149158095660275:  43%|████▎     | 72/166 [01:31<02:01,  1.29s/it]avg_loss = 1.7149158095660275:  44%|████▍     | 73/166 [01:31<02:00,  1.29s/it]avg_loss = 1.7092612965686902:  44%|████▍     | 73/166 [01:33<02:00,  1.29s/it]avg_loss = 1.7092612965686902:  45%|████▍     | 74/166 [01:33<01:58,  1.29s/it]avg_loss = 1.7047349262237548:  45%|████▍     | 74/166 [01:34<01:58,  1.29s/it]avg_loss = 1.7047349262237548:  45%|████▌     | 75/166 [01:34<01:57,  1.30s/it]avg_loss = 1.7038932144641876:  45%|████▌     | 75/166 [01:35<01:57,  1.30s/it]avg_loss = 1.7038932144641876:  46%|████▌     | 76/166 [01:35<01:56,  1.30s/it]avg_loss = 1.700433035949608:  46%|████▌     | 76/166 [01:37<01:56,  1.30s/it] avg_loss = 1.700433035949608:  46%|████▋     | 77/166 [01:37<01:55,  1.30s/it]avg_loss = 1.696806861804082:  46%|████▋     | 77/166 [01:38<01:55,  1.30s/it]avg_loss = 1.696806861804082:  47%|████▋     | 78/166 [01:38<01:54,  1.30s/it]avg_loss = 1.694168585765211:  47%|████▋     | 78/166 [01:39<01:54,  1.30s/it]avg_loss = 1.694168585765211:  48%|████▊     | 79/166 [01:39<01:52,  1.30s/it]avg_loss = 1.6907170340418816:  48%|████▊     | 79/166 [01:40<01:52,  1.30s/it]avg_loss = 1.6907170340418816:  48%|████▊     | 80/166 [01:40<01:51,  1.30s/it]avg_loss = 1.6814000429930511:  48%|████▊     | 80/166 [01:42<01:51,  1.30s/it]avg_loss = 1.6814000429930511:  49%|████▉     | 81/166 [01:42<01:50,  1.30s/it]avg_loss = 1.6830537333721067:  49%|████▉     | 81/166 [01:43<01:50,  1.30s/it]avg_loss = 1.6830537333721067:  49%|████▉     | 82/166 [01:43<01:49,  1.30s/it]avg_loss = 1.68502174946199:  49%|████▉     | 82/166 [01:44<01:49,  1.30s/it]  avg_loss = 1.68502174946199:  50%|█████     | 83/166 [01:44<01:47,  1.30s/it]avg_loss = 1.68814541328521:  50%|█████     | 83/166 [01:46<01:47,  1.30s/it]avg_loss = 1.68814541328521:  51%|█████     | 84/166 [01:46<01:46,  1.30s/it]avg_loss = 1.690055415209602:  51%|█████     | 84/166 [01:47<01:46,  1.30s/it]avg_loss = 1.690055415209602:  51%|█████     | 85/166 [01:47<01:45,  1.30s/it]avg_loss = 1.6890241171038427:  51%|█████     | 85/166 [01:48<01:45,  1.30s/it]avg_loss = 1.6890241171038427:  52%|█████▏    | 86/166 [01:48<01:43,  1.30s/it]avg_loss = 1.6893321864906399:  52%|█████▏    | 86/166 [01:50<01:43,  1.30s/it]avg_loss = 1.6893321864906399:  52%|█████▏    | 87/166 [01:50<01:42,  1.30s/it]avg_loss = 1.6895550408146598:  52%|█████▏    | 87/166 [01:51<01:42,  1.30s/it]avg_loss = 1.6895550408146598:  53%|█████▎    | 88/166 [01:51<01:41,  1.30s/it]avg_loss = 1.6907497189018164:  53%|█████▎    | 88/166 [01:52<01:41,  1.30s/it]avg_loss = 1.6907497189018164:  54%|█████▎    | 89/166 [01:52<01:40,  1.30s/it]avg_loss = 1.6906512988938225:  54%|█████▎    | 89/166 [01:53<01:40,  1.30s/it]avg_loss = 1.6906512988938225:  54%|█████▍    | 90/166 [01:53<01:38,  1.30s/it]avg_loss = 1.6911280652978917:  54%|█████▍    | 90/166 [01:55<01:38,  1.30s/it]avg_loss = 1.6911280652978917:  55%|█████▍    | 91/166 [01:55<01:37,  1.30s/it]avg_loss = 1.692291232554809:  55%|█████▍    | 91/166 [01:56<01:37,  1.30s/it] avg_loss = 1.692291232554809:  55%|█████▌    | 92/166 [01:56<01:36,  1.30s/it]avg_loss = 1.6962358067112584:  55%|█████▌    | 92/166 [01:57<01:36,  1.30s/it]avg_loss = 1.6962358067112584:  56%|█████▌    | 93/166 [01:57<01:34,  1.30s/it]avg_loss = 1.6953372765094676:  56%|█████▌    | 93/166 [01:59<01:34,  1.30s/it]avg_loss = 1.6953372765094676:  57%|█████▋    | 94/166 [01:59<01:33,  1.30s/it]avg_loss = 1.694590203385604:  57%|█████▋    | 94/166 [02:00<01:33,  1.30s/it] avg_loss = 1.694590203385604:  57%|█████▋    | 95/166 [02:00<01:32,  1.30s/it]avg_loss = 1.6942255112032096:  57%|█████▋    | 95/166 [02:01<01:32,  1.30s/it]avg_loss = 1.6942255112032096:  58%|█████▊    | 96/166 [02:01<01:31,  1.30s/it]avg_loss = 1.6941198221187002:  58%|█████▊    | 96/166 [02:03<01:31,  1.30s/it]avg_loss = 1.6941198221187002:  58%|█████▊    | 97/166 [02:03<01:29,  1.30s/it]avg_loss = 1.6924985038990876:  58%|█████▊    | 97/166 [02:04<01:29,  1.30s/it]avg_loss = 1.6924985038990876:  59%|█████▉    | 98/166 [02:04<01:28,  1.30s/it]avg_loss = 1.6901196530371:  59%|█████▉    | 98/166 [02:05<01:28,  1.30s/it]   avg_loss = 1.6901196530371:  60%|█████▉    | 99/166 [02:05<01:27,  1.30s/it]avg_loss = 1.68745232462883:  60%|█████▉    | 99/166 [02:06<01:27,  1.30s/it]avg_loss = 1.68745232462883:  60%|██████    | 100/166 [02:06<01:25,  1.30s/it]avg_loss = 1.6878676874802845:  60%|██████    | 100/166 [02:08<01:25,  1.30s/it]avg_loss = 1.6878676874802845:  61%|██████    | 101/166 [02:08<01:24,  1.30s/it]avg_loss = 1.6888586259355731:  61%|██████    | 101/166 [02:09<01:24,  1.30s/it]avg_loss = 1.6888586259355731:  61%|██████▏   | 102/166 [02:09<01:23,  1.30s/it]avg_loss = 1.6899594867113725:  61%|██████▏   | 102/166 [02:10<01:23,  1.30s/it]avg_loss = 1.6899594867113725:  62%|██████▏   | 103/166 [02:10<01:22,  1.30s/it]avg_loss = 1.692164537998346:  62%|██████▏   | 103/166 [02:12<01:22,  1.30s/it] avg_loss = 1.692164537998346:  63%|██████▎   | 104/166 [02:12<01:20,  1.30s/it]avg_loss = 1.6987568809872582:  63%|██████▎   | 104/166 [02:13<01:20,  1.30s/it]avg_loss = 1.6987568809872582:  63%|██████▎   | 105/166 [02:13<01:19,  1.31s/it]avg_loss = 1.7039822969796523:  63%|██████▎   | 105/166 [02:14<01:19,  1.31s/it]avg_loss = 1.7039822969796523:  64%|██████▍   | 106/166 [02:14<01:18,  1.31s/it]avg_loss = 1.7076090251173928:  64%|██████▍   | 106/166 [02:16<01:18,  1.31s/it]avg_loss = 1.7076090251173928:  64%|██████▍   | 107/166 [02:16<01:17,  1.31s/it]avg_loss = 1.7107865413029988:  64%|██████▍   | 107/166 [02:17<01:17,  1.31s/it]avg_loss = 1.7107865413029988:  65%|██████▌   | 108/166 [02:17<01:15,  1.31s/it]avg_loss = 1.715494451172855:  65%|██████▌   | 108/166 [02:18<01:15,  1.31s/it] avg_loss = 1.715494451172855:  66%|██████▌   | 109/166 [02:18<01:14,  1.31s/it]avg_loss = 1.7189713044600052:  66%|██████▌   | 109/166 [02:20<01:14,  1.31s/it]avg_loss = 1.7189713044600052:  66%|██████▋   | 110/166 [02:20<01:13,  1.31s/it]avg_loss = 1.720485208270786:  66%|██████▋   | 110/166 [02:21<01:13,  1.31s/it] avg_loss = 1.720485208270786:  67%|██████▋   | 111/166 [02:21<01:11,  1.31s/it]avg_loss = 1.7217423064368111:  67%|██████▋   | 111/166 [02:22<01:11,  1.31s/it]avg_loss = 1.7217423064368111:  67%|██████▋   | 112/166 [02:22<01:10,  1.31s/it]avg_loss = 1.7221308144847904:  67%|██████▋   | 112/166 [02:23<01:10,  1.31s/it]avg_loss = 1.7221308144847904:  68%|██████▊   | 113/166 [02:23<01:09,  1.31s/it]avg_loss = 1.7235219604090641:  68%|██████▊   | 113/166 [02:25<01:09,  1.31s/it]avg_loss = 1.7235219604090641:  69%|██████▊   | 114/166 [02:25<01:08,  1.31s/it]avg_loss = 1.720351180822953:  69%|██████▊   | 114/166 [02:26<01:08,  1.31s/it] avg_loss = 1.720351180822953:  69%|██████▉   | 115/166 [02:26<01:06,  1.31s/it]avg_loss = 1.7196756591056954:  69%|██████▉   | 115/166 [02:27<01:06,  1.31s/it]avg_loss = 1.7196756591056954:  70%|██████▉   | 116/166 [02:27<01:05,  1.31s/it]avg_loss = 1.7206650167448907:  70%|██████▉   | 116/166 [02:29<01:05,  1.31s/it]avg_loss = 1.7206650167448907:  70%|███████   | 117/166 [02:29<01:04,  1.31s/it]avg_loss = 1.7207784925476979:  70%|███████   | 117/166 [02:30<01:04,  1.31s/it]avg_loss = 1.7207784925476979:  71%|███████   | 118/166 [02:30<01:02,  1.31s/it]avg_loss = 1.7201458315889375:  71%|███████   | 118/166 [02:31<01:02,  1.31s/it]avg_loss = 1.7201458315889375:  72%|███████▏  | 119/166 [02:31<01:01,  1.31s/it]avg_loss = 1.7207469711701076:  72%|███████▏  | 119/166 [02:33<01:01,  1.31s/it]avg_loss = 1.7207469711701076:  72%|███████▏  | 120/166 [02:33<01:00,  1.31s/it]avg_loss = 1.7200243561721045:  72%|███████▏  | 120/166 [02:34<01:00,  1.31s/it]avg_loss = 1.7200243561721045:  73%|███████▎  | 121/166 [02:34<00:58,  1.31s/it]avg_loss = 1.7202671365659745:  73%|███████▎  | 121/166 [02:35<00:58,  1.31s/it]avg_loss = 1.7202671365659745:  73%|███████▎  | 122/166 [02:35<00:57,  1.31s/it]avg_loss = 1.720494522311823:  73%|███████▎  | 122/166 [02:37<00:57,  1.31s/it] avg_loss = 1.720494522311823:  74%|███████▍  | 123/166 [02:37<00:56,  1.31s/it]avg_loss = 1.7189874649047852:  74%|███████▍  | 123/166 [02:38<00:56,  1.31s/it]avg_loss = 1.7189874649047852:  75%|███████▍  | 124/166 [02:38<00:55,  1.31s/it]avg_loss = 1.717254123687744:  75%|███████▍  | 124/166 [02:39<00:55,  1.31s/it] avg_loss = 1.717254123687744:  75%|███████▌  | 125/166 [02:39<00:53,  1.31s/it]avg_loss = 1.7150049323127383:  75%|███████▌  | 125/166 [02:40<00:53,  1.31s/it]avg_loss = 1.7150049323127383:  76%|███████▌  | 126/166 [02:40<00:52,  1.31s/it]avg_loss = 1.7128243906291452:  76%|███████▌  | 126/166 [02:42<00:52,  1.31s/it]avg_loss = 1.7128243906291452:  77%|███████▋  | 127/166 [02:42<00:51,  1.31s/it]avg_loss = 1.7113332357257605:  77%|███████▋  | 127/166 [02:43<00:51,  1.31s/it]avg_loss = 1.7113332357257605:  77%|███████▋  | 128/166 [02:43<00:49,  1.31s/it]avg_loss = 1.710039267244265:  77%|███████▋  | 128/166 [02:44<00:49,  1.31s/it] avg_loss = 1.710039267244265:  78%|███████▊  | 129/166 [02:44<00:48,  1.31s/it]avg_loss = 1.7099039242817806:  78%|███████▊  | 129/166 [02:46<00:48,  1.31s/it]avg_loss = 1.7099039242817806:  78%|███████▊  | 130/166 [02:46<00:47,  1.31s/it]avg_loss = 1.7109407668805305:  78%|███████▊  | 130/166 [02:47<00:47,  1.31s/it]avg_loss = 1.7109407668805305:  79%|███████▉  | 131/166 [02:47<00:45,  1.31s/it]avg_loss = 1.7115296522776287:  79%|███████▉  | 131/166 [02:48<00:45,  1.31s/it]avg_loss = 1.7115296522776287:  80%|███████▉  | 132/166 [02:48<00:44,  1.31s/it]avg_loss = 1.7124641156734381:  80%|███████▉  | 132/166 [02:50<00:44,  1.31s/it]avg_loss = 1.7124641156734381:  80%|████████  | 133/166 [02:50<00:43,  1.31s/it]avg_loss = 1.7137937430125565:  80%|████████  | 133/166 [02:51<00:43,  1.31s/it]avg_loss = 1.7137937430125565:  81%|████████  | 134/166 [02:51<00:41,  1.31s/it]avg_loss = 1.711733720920704:  81%|████████  | 134/166 [02:52<00:41,  1.31s/it] avg_loss = 1.711733720920704:  81%|████████▏ | 135/166 [02:52<00:40,  1.31s/it]avg_loss = 1.7120478381128872:  81%|████████▏ | 135/166 [02:54<00:40,  1.31s/it]avg_loss = 1.7120478381128872:  82%|████████▏ | 136/166 [02:54<00:39,  1.31s/it]avg_loss = 1.7123306572002217:  82%|████████▏ | 136/166 [02:55<00:39,  1.31s/it]avg_loss = 1.7123306572002217:  83%|████████▎ | 137/166 [02:55<00:38,  1.31s/it]avg_loss = 1.7131687912388125:  83%|████████▎ | 137/166 [02:56<00:38,  1.31s/it]avg_loss = 1.7131687912388125:  83%|████████▎ | 138/166 [02:56<00:36,  1.31s/it]avg_loss = 1.7122796868248809:  83%|████████▎ | 138/166 [02:58<00:36,  1.31s/it]avg_loss = 1.7122796868248809:  84%|████████▎ | 139/166 [02:58<00:35,  1.31s/it]avg_loss = 1.7109781886850084:  84%|████████▎ | 139/166 [02:59<00:35,  1.31s/it]avg_loss = 1.7109781886850084:  84%|████████▍ | 140/166 [02:59<00:34,  1.31s/it]avg_loss = 1.7096238719656112:  84%|████████▍ | 140/166 [03:00<00:34,  1.31s/it]avg_loss = 1.7096238719656112:  85%|████████▍ | 141/166 [03:00<00:32,  1.31s/it]avg_loss = 1.7092414296848673:  85%|████████▍ | 141/166 [03:01<00:32,  1.31s/it]avg_loss = 1.7092414296848673:  86%|████████▌ | 142/166 [03:01<00:31,  1.31s/it]avg_loss = 1.7075944655425066:  86%|████████▌ | 142/166 [03:03<00:31,  1.31s/it]avg_loss = 1.7075944655425066:  86%|████████▌ | 143/166 [03:03<00:30,  1.31s/it]avg_loss = 1.7087804302573204:  86%|████████▌ | 143/166 [03:04<00:30,  1.31s/it]avg_loss = 1.7087804302573204:  87%|████████▋ | 144/166 [03:04<00:28,  1.31s/it]avg_loss = 1.7080279605142001:  87%|████████▋ | 144/166 [03:05<00:28,  1.31s/it]avg_loss = 1.7080279605142001:  87%|████████▋ | 145/166 [03:05<00:27,  1.31s/it]avg_loss = 1.7079677434816753:  87%|████████▋ | 145/166 [03:07<00:27,  1.31s/it]avg_loss = 1.7079677434816753:  88%|████████▊ | 146/166 [03:07<00:26,  1.31s/it]avg_loss = 1.706836657459233:  88%|████████▊ | 146/166 [03:08<00:26,  1.31s/it] avg_loss = 1.706836657459233:  89%|████████▊ | 147/166 [03:08<00:24,  1.31s/it]avg_loss = 1.70591323601233:  89%|████████▊ | 147/166 [03:09<00:24,  1.31s/it] avg_loss = 1.70591323601233:  89%|████████▉ | 148/166 [03:09<00:23,  1.31s/it]avg_loss = 1.7042021767405056:  89%|████████▉ | 148/166 [03:11<00:23,  1.31s/it]avg_loss = 1.7042021767405056:  90%|████████▉ | 149/166 [03:11<00:22,  1.31s/it]avg_loss = 1.705165445804596:  90%|████████▉ | 149/166 [03:12<00:22,  1.31s/it] avg_loss = 1.705165445804596:  90%|█████████ | 150/166 [03:12<00:21,  1.31s/it]avg_loss = 1.7042575316713346:  90%|█████████ | 150/166 [03:13<00:21,  1.31s/it]avg_loss = 1.7042575316713346:  91%|█████████ | 151/166 [03:13<00:19,  1.32s/it]avg_loss = 1.7040713672575198:  91%|█████████ | 151/166 [03:15<00:19,  1.32s/it]avg_loss = 1.7040713672575198:  92%|█████████▏| 152/166 [03:15<00:18,  1.32s/it]avg_loss = 1.7038587869382371:  92%|█████████▏| 152/166 [03:16<00:18,  1.32s/it]avg_loss = 1.7038587869382371:  92%|█████████▏| 153/166 [03:16<00:17,  1.32s/it]avg_loss = 1.7054008988590983:  92%|█████████▏| 153/166 [03:17<00:17,  1.32s/it]avg_loss = 1.7054008988590983:  93%|█████████▎| 154/166 [03:17<00:15,  1.32s/it]avg_loss = 1.704903834096847:  93%|█████████▎| 154/166 [03:19<00:15,  1.32s/it] avg_loss = 1.704903834096847:  93%|█████████▎| 155/166 [03:19<00:14,  1.31s/it]avg_loss = 1.7047939201196034:  93%|█████████▎| 155/166 [03:20<00:14,  1.31s/it]avg_loss = 1.7047939201196034:  94%|█████████▍| 156/166 [03:20<00:13,  1.32s/it]avg_loss = 1.7030347547713358:  94%|█████████▍| 156/166 [03:21<00:13,  1.32s/it]avg_loss = 1.7030347547713358:  95%|█████████▍| 157/166 [03:21<00:11,  1.32s/it]avg_loss = 1.6988012254992617:  95%|█████████▍| 157/166 [03:23<00:11,  1.32s/it]avg_loss = 1.6988012254992617:  95%|█████████▌| 158/166 [03:23<00:10,  1.32s/it]avg_loss = 1.6996087278210141:  95%|█████████▌| 158/166 [03:24<00:10,  1.32s/it]avg_loss = 1.6996087278210141:  96%|█████████▌| 159/166 [03:24<00:09,  1.32s/it]avg_loss = 1.7010099582374096:  96%|█████████▌| 159/166 [03:25<00:09,  1.32s/it]avg_loss = 1.7010099582374096:  96%|█████████▋| 160/166 [03:25<00:07,  1.32s/it]avg_loss = 1.703379033515172:  96%|█████████▋| 160/166 [03:26<00:07,  1.32s/it] avg_loss = 1.703379033515172:  97%|█████████▋| 161/166 [03:26<00:06,  1.32s/it]avg_loss = 1.7035411221009713:  97%|█████████▋| 161/166 [03:28<00:06,  1.32s/it]avg_loss = 1.7035411221009713:  98%|█████████▊| 162/166 [03:28<00:05,  1.32s/it]avg_loss = 1.7031286106519172:  98%|█████████▊| 162/166 [03:29<00:05,  1.32s/it]avg_loss = 1.7031286106519172:  98%|█████████▊| 163/166 [03:29<00:03,  1.32s/it]avg_loss = 1.7037412967623733:  98%|█████████▊| 163/166 [03:30<00:03,  1.32s/it]avg_loss = 1.7037412967623733:  99%|█████████▉| 164/166 [03:30<00:02,  1.32s/it]avg_loss = 1.7038235151406491:  99%|█████████▉| 164/166 [03:32<00:02,  1.32s/it]avg_loss = 1.7038235151406491:  99%|█████████▉| 165/166 [03:32<00:01,  1.32s/it]avg_loss = 1.7057593507939075:  99%|█████████▉| 165/166 [03:33<00:01,  1.32s/it]avg_loss = 1.7057593507939075: 100%|██████████| 166/166 [03:33<00:00,  1.32s/it]avg_loss = 1.7057593507939075: 100%|██████████| 166/166 [03:33<00:00,  1.29s/it]
I0403 07:44:57.817452 3535447 eval_ppl.py:107] wikitext2 perplexity: 5.505565166473389
wikitext2 perplexity: 5.506
