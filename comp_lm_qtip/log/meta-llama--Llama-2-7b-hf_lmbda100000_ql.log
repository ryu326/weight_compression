I0312 10:30:48.222284 1891709 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.73it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.28it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.59it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.49it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.04it/s]
I0312 10:30:50.286258 1891709 quantize_finetune_llama.py:134] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:23,  1.33it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:21,  1.36it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:20,  1.38it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:20,  1.38it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:19,  1.39it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:19,  1.32it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:05<00:19,  1.25it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:06<00:19,  1.21it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:07<00:19,  1.19it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:07<00:18,  1.17it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:08<00:17,  1.18it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:09<00:16,  1.24it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:10<00:14,  1.28it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:10<00:13,  1.31it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:11<00:12,  1.33it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:12<00:11,  1.35it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:13<00:10,  1.37it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:13<00:10,  1.37it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:14<00:09,  1.37it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:15<00:08,  1.38it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:15<00:07,  1.41it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:16<00:07,  1.41it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:17<00:06,  1.41it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:18<00:05,  1.43it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:18<00:04,  1.46it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:19<00:04,  1.47it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:20<00:03,  1.49it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:20<00:02,  1.50it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:21<00:01,  1.51it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.52it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:22<00:00,  1.53it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.53it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.38it/s]
I0312 10:31:22.764284 1891709 quantize_finetune_llama.py:159] loaded compression model
I0312 10:31:37.019943 1891709 quantize_finetune_llama.py:163] loaded dataset and devset
I0312 10:31:42.709254 1891709 quantize_finetune_llama.py:183] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:32:51.487260 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 0 in 68.66370725631714s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0312 10:33:09.233169 1891823 config.py:54] PyTorch version 2.1.1 available.
I0312 10:33:10.163856 1891709 quantize_finetune_llama.py:183] layer 1 gpu 1
I0312 10:33:10.228324 1891823 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:33:18.004215 1891823 finetune.py:45] layer 0_v initial loss 2.8238139293534914e-07
I0312 10:33:49.727653 1891823 finetune.py:68] layer 0_v @ epoch 0 new loss 7.796153767003489e-08 old loss 2.8238139293534914e-07 BETTER
I0312 10:34:16.542608 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 1 in 66.23954510688782s
I0312 10:34:23.819752 1891823 finetune.py:68] layer 0_v @ epoch 1 new loss 3.275666671243016e-08 old loss 7.796153767003489e-08 BETTER
I0312 10:34:24.502143 1891941 config.py:54] PyTorch version 2.1.1 available.
I0312 10:34:25.492403 1891709 quantize_finetune_llama.py:183] layer 2 gpu 2
I0312 10:34:25.557596 1891941 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:34:33.263185 1891941 finetune.py:45] layer 1_v initial loss 3.638467205746565e-06
I0312 10:34:57.272572 1891823 finetune.py:68] layer 0_v @ epoch 2 new loss 1.8824255931804146e-08 old loss 3.275666671243016e-08 BETTER
I0312 10:35:04.161616 1891941 finetune.py:68] layer 1_v @ epoch 0 new loss 2.29220154324139e-06 old loss 3.638467205746565e-06 BETTER
I0312 10:35:31.241369 1891823 finetune.py:68] layer 0_v @ epoch 3 new loss 1.38243461123011e-08 old loss 1.8824255931804146e-08 BETTER
I0312 10:35:34.372527 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 2 in 68.7452073097229s
I0312 10:35:37.249035 1891941 finetune.py:76] layer 1_v @ epoch 1 new loss 2.438558567519067e-06 old loss 2.29220154324139e-06 WORSE
I0312 10:35:42.250352 1892059 config.py:54] PyTorch version 2.1.1 available.
I0312 10:35:43.285344 1891709 quantize_finetune_llama.py:183] layer 3 gpu 3
I0312 10:35:43.356571 1892059 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:35:51.307296 1892059 finetune.py:45] layer 2_v initial loss 4.3588551079665194e-07
I0312 10:36:05.554837 1891823 finetune.py:68] layer 0_v @ epoch 4 new loss 1.163048946040135e-08 old loss 1.38243461123011e-08 BETTER
I0312 10:36:09.185029 1891941 finetune.py:76] layer 1_v @ epoch 2 new loss 3.98067004425684e-06 old loss 2.29220154324139e-06 WORSE
I0312 10:36:14.669606 1891823 finetune.py:45] layer 0_q initial loss 2.8979185273669827e-08
I0312 10:36:22.472867 1892059 finetune.py:68] layer 2_v @ epoch 0 new loss 3.1720901461085305e-07 old loss 4.3588551079665194e-07 BETTER
I0312 10:36:41.029620 1891941 finetune.py:68] layer 1_v @ epoch 3 new loss 7.663263659196673e-07 old loss 2.29220154324139e-06 BETTER
I0312 10:36:47.540395 1891823 finetune.py:68] layer 0_q @ epoch 0 new loss 1.1723806814245563e-08 old loss 2.8979185273669827e-08 BETTER
I0312 10:36:54.576406 1892059 finetune.py:68] layer 2_v @ epoch 1 new loss 2.5727658226060157e-07 old loss 3.1720901461085305e-07 BETTER
I0312 10:36:56.529449 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 3 in 73.10264492034912s
I0312 10:37:07.662469 1892177 config.py:54] PyTorch version 2.1.1 available.
I0312 10:37:08.808616 1891709 quantize_finetune_llama.py:183] layer 4 gpu 0
I0312 10:37:08.900858 1892177 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:37:14.072017 1891941 finetune.py:76] layer 1_v @ epoch 4 new loss 1.2623487464225036e-06 old loss 7.663263659196673e-07 WORSE
I0312 10:37:17.839718 1892177 finetune.py:45] layer 3_v initial loss 6.431422434616252e-07
I0312 10:37:21.709835 1891823 finetune.py:68] layer 0_q @ epoch 1 new loss 1.0420291296497908e-08 old loss 1.1723806814245563e-08 BETTER
I0312 10:37:23.083942 1891941 finetune.py:45] layer 1_q initial loss 9.150766686616407e-07
I0312 10:37:27.508473 1892059 finetune.py:68] layer 2_v @ epoch 2 new loss 2.2475735761418036e-07 old loss 2.5727658226060157e-07 BETTER
I0312 10:37:48.436067 1892177 finetune.py:68] layer 3_v @ epoch 0 new loss 3.7594014656860963e-07 old loss 6.431422434616252e-07 BETTER
I0312 10:37:54.583455 1891941 finetune.py:68] layer 1_q @ epoch 0 new loss 8.909182724892162e-07 old loss 9.150766686616407e-07 BETTER
I0312 10:37:55.865134 1891823 finetune.py:68] layer 0_q @ epoch 2 new loss 9.551161639365091e-09 old loss 1.0420291296497908e-08 BETTER
I0312 10:38:00.395047 1892059 finetune.py:68] layer 2_v @ epoch 3 new loss 2.0484245055740757e-07 old loss 2.2475735761418036e-07 BETTER
I0312 10:38:20.308666 1892177 finetune.py:68] layer 3_v @ epoch 1 new loss 3.167332636166975e-07 old loss 3.7594014656860963e-07 BETTER
I0312 10:38:26.908349 1891941 finetune.py:68] layer 1_q @ epoch 1 new loss 6.572304869223444e-07 old loss 8.909182724892162e-07 BETTER
I0312 10:38:30.289386 1891823 finetune.py:68] layer 0_q @ epoch 3 new loss 8.872289569694658e-09 old loss 9.551161639365091e-09 BETTER
I0312 10:38:33.368632 1892059 finetune.py:68] layer 2_v @ epoch 4 new loss 1.9133862849685102e-07 old loss 2.0484245055740757e-07 BETTER
I0312 10:38:42.565936 1892059 finetune.py:45] layer 2_q initial loss 2.835784584931389e-07
I0312 10:38:52.190227 1892177 finetune.py:68] layer 3_v @ epoch 2 new loss 2.8917040140186145e-07 old loss 3.167332636166975e-07 BETTER
I0312 10:38:59.174681 1891941 finetune.py:76] layer 1_q @ epoch 2 new loss 1.3439567965178867e-06 old loss 6.572304869223444e-07 WORSE
I0312 10:39:04.667249 1891823 finetune.py:68] layer 0_q @ epoch 4 new loss 8.31785662569473e-09 old loss 8.872289569694658e-09 BETTER
I0312 10:39:13.955362 1891823 finetune.py:45] layer 0_k initial loss 2.7557982562598227e-08
I0312 10:39:14.156991 1892059 finetune.py:68] layer 2_q @ epoch 0 new loss 2.115069435149053e-07 old loss 2.835784584931389e-07 BETTER
I0312 10:39:24.147060 1892177 finetune.py:68] layer 3_v @ epoch 3 new loss 2.708187025746156e-07 old loss 2.8917040140186145e-07 BETTER
I0312 10:39:30.866396 1891941 finetune.py:76] layer 1_q @ epoch 3 new loss 2.1372773062466877e-06 old loss 6.572304869223444e-07 WORSE
I0312 10:39:46.638395 1892059 finetune.py:68] layer 2_q @ epoch 1 new loss 1.9772852510868688e-07 old loss 2.115069435149053e-07 BETTER
I0312 10:39:46.669620 1891823 finetune.py:68] layer 0_k @ epoch 0 new loss 1.1450237202836888e-08 old loss 2.7557982562598227e-08 BETTER
I0312 10:39:56.360807 1892177 finetune.py:68] layer 3_v @ epoch 4 new loss 2.5724665420057136e-07 old loss 2.708187025746156e-07 BETTER
I0312 10:40:02.577570 1891941 finetune.py:76] layer 1_q @ epoch 4 new loss 4.8775668801681604e-06 old loss 6.572304869223444e-07 WORSE
I0312 10:40:05.940113 1892177 finetune.py:45] layer 3_q initial loss 4.876554271504574e-07
I0312 10:40:11.425226 1891941 finetune.py:45] layer 1_k initial loss 1.4565708852387615e-06
I0312 10:40:19.337947 1892059 finetune.py:68] layer 2_q @ epoch 2 new loss 1.8801152634750906e-07 old loss 1.9772852510868688e-07 BETTER
I0312 10:40:20.186337 1891823 finetune.py:68] layer 0_k @ epoch 1 new loss 1.0611604928101315e-08 old loss 1.1450237202836888e-08 BETTER
I0312 10:40:37.315348 1892177 finetune.py:68] layer 3_q @ epoch 0 new loss 3.249047040299047e-07 old loss 4.876554271504574e-07 BETTER
I0312 10:40:42.895998 1891941 finetune.py:68] layer 1_k @ epoch 0 new loss 6.173846145429707e-07 old loss 1.4565708852387615e-06 BETTER
I0312 10:40:51.810265 1892059 finetune.py:68] layer 2_q @ epoch 3 new loss 1.8039193605545734e-07 old loss 1.8801152634750906e-07 BETTER
I0312 10:40:53.922188 1891823 finetune.py:68] layer 0_k @ epoch 2 new loss 1.0013764928373803e-08 old loss 1.0611604928101315e-08 BETTER
I0312 10:41:09.337741 1892177 finetune.py:68] layer 3_q @ epoch 1 new loss 3.0245374205151165e-07 old loss 3.249047040299047e-07 BETTER
I0312 10:41:15.031242 1891941 finetune.py:76] layer 1_k @ epoch 1 new loss 9.515105716673133e-07 old loss 6.173846145429707e-07 WORSE
I0312 10:41:24.397873 1892059 finetune.py:68] layer 2_q @ epoch 4 new loss 1.7403760921297362e-07 old loss 1.8039193605545734e-07 BETTER
I0312 10:41:27.515302 1891823 finetune.py:68] layer 0_k @ epoch 3 new loss 9.527540534293166e-09 old loss 1.0013764928373803e-08 BETTER
I0312 10:41:33.713950 1892059 finetune.py:45] layer 2_k initial loss 2.6508504902267305e-07
I0312 10:41:41.369412 1892177 finetune.py:68] layer 3_q @ epoch 2 new loss 2.879304190628318e-07 old loss 3.0245374205151165e-07 BETTER
I0312 10:41:46.484138 1891941 finetune.py:68] layer 1_k @ epoch 2 new loss 4.842686394113116e-07 old loss 6.173846145429707e-07 BETTER
I0312 10:42:01.219657 1891823 finetune.py:68] layer 0_k @ epoch 4 new loss 9.122684829776517e-09 old loss 9.527540534293166e-09 BETTER
I0312 10:42:05.144858 1892059 finetune.py:68] layer 2_k @ epoch 0 new loss 2.3791290004737675e-07 old loss 2.6508504902267305e-07 BETTER
I0312 10:42:10.586998 1891823 finetune.py:45] layer 0_o initial loss 4.643941053927847e-08
I0312 10:42:13.375040 1892177 finetune.py:68] layer 3_q @ epoch 3 new loss 2.7718388651010173e-07 old loss 2.879304190628318e-07 BETTER
I0312 10:42:18.714142 1891941 finetune.py:76] layer 1_k @ epoch 3 new loss 1.3824657116856542e-06 old loss 4.842686394113116e-07 WORSE
I0312 10:42:37.193962 1892059 finetune.py:68] layer 2_k @ epoch 1 new loss 2.278945743228178e-07 old loss 2.3791290004737675e-07 BETTER
I0312 10:42:42.872753 1891823 finetune.py:68] layer 0_o @ epoch 0 new loss 4.066917114187163e-08 old loss 4.643941053927847e-08 BETTER
I0312 10:42:45.730206 1892177 finetune.py:68] layer 3_q @ epoch 4 new loss 2.685768833998736e-07 old loss 2.7718388651010173e-07 BETTER
I0312 10:42:50.104147 1891941 finetune.py:76] layer 1_k @ epoch 4 new loss 1.8488460682419827e-06 old loss 4.842686394113116e-07 WORSE
I0312 10:42:54.842485 1892177 finetune.py:45] layer 3_k initial loss 4.73272393719526e-07
I0312 10:42:58.858546 1891941 finetune.py:45] layer 1_o initial loss 7.521990710301907e-07
I0312 10:43:09.164162 1892059 finetune.py:68] layer 2_k @ epoch 2 new loss 2.2075035133184429e-07 old loss 2.278945743228178e-07 BETTER
I0312 10:43:15.985749 1891823 finetune.py:68] layer 0_o @ epoch 1 new loss 3.6891396604232796e-08 old loss 4.066917114187163e-08 BETTER
I0312 10:43:25.720392 1892177 finetune.py:68] layer 3_k @ epoch 0 new loss 4.2752651552291354e-07 old loss 4.73272393719526e-07 BETTER
I0312 10:43:29.491087 1891941 finetune.py:68] layer 1_o @ epoch 0 new loss 6.468694664363284e-07 old loss 7.521990710301907e-07 BETTER
I0312 10:43:41.235792 1892059 finetune.py:68] layer 2_k @ epoch 3 new loss 2.1502580693777418e-07 old loss 2.2075035133184429e-07 BETTER
I0312 10:43:49.113505 1891823 finetune.py:68] layer 0_o @ epoch 2 new loss 3.39977255237045e-08 old loss 3.6891396604232796e-08 BETTER
I0312 10:43:57.306949 1892177 finetune.py:68] layer 3_k @ epoch 1 new loss 4.1199416500603547e-07 old loss 4.2752651552291354e-07 BETTER
I0312 10:44:00.900409 1891941 finetune.py:68] layer 1_o @ epoch 1 new loss 6.241701839826419e-07 old loss 6.468694664363284e-07 BETTER
I0312 10:44:13.265268 1892059 finetune.py:68] layer 2_k @ epoch 4 new loss 2.1014949425079976e-07 old loss 2.1502580693777418e-07 BETTER
I0312 10:44:22.181382 1892059 finetune.py:45] layer 2_o initial loss 4.231262948906078e-07
I0312 10:44:22.263589 1891823 finetune.py:68] layer 0_o @ epoch 3 new loss 3.172408469254151e-08 old loss 3.39977255237045e-08 BETTER
I0312 10:44:29.012669 1892177 finetune.py:68] layer 3_k @ epoch 2 new loss 4.0104791310113796e-07 old loss 4.1199416500603547e-07 BETTER
I0312 10:44:32.171158 1891941 finetune.py:76] layer 1_o @ epoch 2 new loss 7.031406425994646e-07 old loss 6.241701839826419e-07 WORSE
I0312 10:44:53.013087 1892059 finetune.py:68] layer 2_o @ epoch 0 new loss 4.1694710262163426e-07 old loss 4.231262948906078e-07 BETTER
I0312 10:44:55.473371 1891823 finetune.py:68] layer 0_o @ epoch 4 new loss 2.995250270032557e-08 old loss 3.172408469254151e-08 BETTER
I0312 10:45:00.567842 1892177 finetune.py:68] layer 3_k @ epoch 3 new loss 3.9225236037054856e-07 old loss 4.0104791310113796e-07 BETTER
I0312 10:45:02.882275 1891941 finetune.py:76] layer 1_o @ epoch 3 new loss 6.455279617512133e-07 old loss 6.241701839826419e-07 WORSE
I0312 10:45:10.542988 1891823 finetune.py:45] layer 0_up initial loss 3.486353250536922e-08
I0312 10:45:24.543109 1892059 finetune.py:68] layer 2_o @ epoch 1 new loss 4.12055612741824e-07 old loss 4.1694710262163426e-07 BETTER
I0312 10:45:32.199369 1892177 finetune.py:68] layer 3_k @ epoch 4 new loss 3.8485171671709395e-07 old loss 3.9225236037054856e-07 BETTER
I0312 10:45:33.612751 1891941 finetune.py:76] layer 1_o @ epoch 4 new loss 6.783863568671222e-07 old loss 6.241701839826419e-07 WORSE
I0312 10:45:40.837362 1891823 finetune.py:68] layer 0_up @ epoch 0 new loss 3.331675912932042e-08 old loss 3.486353250536922e-08 BETTER
I0312 10:45:41.292608 1892177 finetune.py:45] layer 3_o initial loss 8.215886850848619e-07
I0312 10:45:48.242211 1891941 finetune.py:45] layer 1_up initial loss 8.292632855955162e-07
I0312 10:45:56.213968 1892059 finetune.py:68] layer 2_o @ epoch 2 new loss 4.079928430655855e-07 old loss 4.12055612741824e-07 BETTER
I0312 10:46:11.361545 1892177 finetune.py:68] layer 3_o @ epoch 0 new loss 7.874173206801061e-07 old loss 8.215886850848619e-07 BETTER
I0312 10:46:12.230327 1891823 finetune.py:68] layer 0_up @ epoch 1 new loss 3.2113465664451724e-08 old loss 3.331675912932042e-08 BETTER
I0312 10:46:17.097495 1891941 finetune.py:68] layer 1_up @ epoch 0 new loss 6.72446560656681e-07 old loss 8.292632855955162e-07 BETTER
I0312 10:46:27.624042 1892059 finetune.py:68] layer 2_o @ epoch 3 new loss 4.0449171478940116e-07 old loss 4.079928430655855e-07 BETTER
I0312 10:46:42.098357 1892177 finetune.py:68] layer 3_o @ epoch 1 new loss 7.677118674109806e-07 old loss 7.874173206801061e-07 BETTER
I0312 10:46:43.664308 1891823 finetune.py:68] layer 0_up @ epoch 2 new loss 3.1131499156344944e-08 old loss 3.2113465664451724e-08 BETTER
I0312 10:46:46.630094 1891941 finetune.py:76] layer 1_up @ epoch 1 new loss 7.088374331942759e-07 old loss 6.72446560656681e-07 WORSE
I0312 10:46:59.255361 1892059 finetune.py:68] layer 2_o @ epoch 4 new loss 4.0137629753189685e-07 old loss 4.0449171478940116e-07 BETTER
I0312 10:47:13.612051 1892177 finetune.py:68] layer 3_o @ epoch 2 new loss 7.541343052253069e-07 old loss 7.677118674109806e-07 BETTER
I0312 10:47:14.867174 1892059 finetune.py:45] layer 2_up initial loss 5.596837127086474e-07
I0312 10:47:15.714185 1891823 finetune.py:68] layer 0_up @ epoch 3 new loss 3.033494877513476e-08 old loss 3.1131499156344944e-08 BETTER
I0312 10:47:16.101211 1891941 finetune.py:76] layer 1_up @ epoch 2 new loss 7.264425221364945e-07 old loss 6.72446560656681e-07 WORSE
I0312 10:47:44.082377 1892059 finetune.py:68] layer 2_up @ epoch 0 new loss 5.565469223256514e-07 old loss 5.596837127086474e-07 BETTER
I0312 10:47:44.910165 1892177 finetune.py:68] layer 3_o @ epoch 3 new loss 7.435519933096657e-07 old loss 7.541343052253069e-07 BETTER
I0312 10:47:45.332652 1891941 finetune.py:76] layer 1_up @ epoch 3 new loss 7.715171932431986e-07 old loss 6.72446560656681e-07 WORSE
I0312 10:47:47.495910 1891823 finetune.py:68] layer 0_up @ epoch 4 new loss 2.967555978727887e-08 old loss 3.033494877513476e-08 BETTER
I0312 10:48:02.570101 1891823 finetune.py:45] layer 0_gate initial loss 3.4779084501224133e-08
I0312 10:48:14.103463 1892059 finetune.py:68] layer 2_up @ epoch 1 new loss 5.53760287402838e-07 old loss 5.565469223256514e-07 BETTER
I0312 10:48:14.524323 1891941 finetune.py:76] layer 1_up @ epoch 4 new loss 8.415919410253991e-07 old loss 6.72446560656681e-07 WORSE
I0312 10:48:16.050652 1892177 finetune.py:68] layer 3_o @ epoch 4 new loss 7.348056101363909e-07 old loss 7.435519933096657e-07 BETTER
I0312 10:48:28.794560 1891941 finetune.py:45] layer 1_gate initial loss 1.2045167068208684e-06
I0312 10:48:30.799552 1892177 finetune.py:45] layer 3_up initial loss 1.0889335726460558e-06
I0312 10:48:31.684494 1891823 finetune.py:68] layer 0_gate @ epoch 0 new loss 3.4053400099764985e-08 old loss 3.4779084501224133e-08 BETTER
I0312 10:48:43.884365 1892059 finetune.py:68] layer 2_up @ epoch 2 new loss 5.512314373845584e-07 old loss 5.53760287402838e-07 BETTER
I0312 10:48:56.390584 1891941 finetune.py:68] layer 1_gate @ epoch 0 new loss 7.374783876912261e-07 old loss 1.2045167068208684e-06 BETTER
I0312 10:48:59.494801 1892177 finetune.py:68] layer 3_up @ epoch 0 new loss 1.0790040505526122e-06 old loss 1.0889335726460558e-06 BETTER
I0312 10:49:01.540663 1891823 finetune.py:68] layer 0_gate @ epoch 1 new loss 3.354261224330912e-08 old loss 3.4053400099764985e-08 BETTER
I0312 10:49:13.874276 1892059 finetune.py:68] layer 2_up @ epoch 3 new loss 5.488623173732776e-07 old loss 5.512314373845584e-07 BETTER
I0312 10:49:24.702838 1891941 finetune.py:76] layer 1_gate @ epoch 1 new loss 7.726529815954564e-07 old loss 7.374783876912261e-07 WORSE
I0312 10:49:28.980685 1892177 finetune.py:68] layer 3_up @ epoch 1 new loss 1.0708952231652802e-06 old loss 1.0790040505526122e-06 BETTER
I0312 10:49:31.718945 1891823 finetune.py:68] layer 0_gate @ epoch 2 new loss 3.3104850416521003e-08 old loss 3.354261224330912e-08 BETTER
I0312 10:49:44.004547 1892059 finetune.py:68] layer 2_up @ epoch 4 new loss 5.466331458592322e-07 old loss 5.488623173732776e-07 BETTER
I0312 10:49:52.414314 1891941 finetune.py:76] layer 1_gate @ epoch 2 new loss 7.96300867023092e-07 old loss 7.374783876912261e-07 WORSE
I0312 10:49:58.529792 1892177 finetune.py:68] layer 3_up @ epoch 2 new loss 1.0638138974172762e-06 old loss 1.0708952231652802e-06 BETTER
I0312 10:49:59.218618 1892059 finetune.py:45] layer 2_gate initial loss 6.79634467815049e-07
I0312 10:50:01.629836 1891823 finetune.py:68] layer 0_gate @ epoch 3 new loss 3.272539927934304e-08 old loss 3.3104850416521003e-08 BETTER
I0312 10:50:20.006035 1891941 finetune.py:76] layer 1_gate @ epoch 3 new loss 8.56384815506317e-07 old loss 7.374783876912261e-07 WORSE
I0312 10:50:26.830936 1892059 finetune.py:68] layer 2_gate @ epoch 0 new loss 6.773427685402567e-07 old loss 6.79634467815049e-07 BETTER
I0312 10:50:28.050590 1892177 finetune.py:68] layer 3_up @ epoch 3 new loss 1.057375811797101e-06 old loss 1.0638138974172762e-06 BETTER
I0312 10:50:31.509582 1891823 finetune.py:68] layer 0_gate @ epoch 4 new loss 3.2405733207951926e-08 old loss 3.272539927934304e-08 BETTER
I0312 10:50:47.115131 1891823 finetune.py:45] layer 0_down initial loss 6.708788902187734e-08
I0312 10:50:47.539371 1891941 finetune.py:76] layer 1_gate @ epoch 4 new loss 8.450224413536489e-07 old loss 7.374783876912261e-07 WORSE
I0312 10:50:54.944772 1892059 finetune.py:68] layer 2_gate @ epoch 1 new loss 6.754169135092525e-07 old loss 6.773427685402567e-07 BETTER
I0312 10:50:57.522150 1892177 finetune.py:68] layer 3_up @ epoch 4 new loss 1.0515119583942578e-06 old loss 1.057375811797101e-06 BETTER
I0312 10:51:02.689610 1891941 finetune.py:45] layer 1_down initial loss 0.00020615087123587728
I0312 10:51:12.878421 1892177 finetune.py:45] layer 3_gate initial loss 1.3476795857059187e-06
I0312 10:51:14.401966 1891823 finetune.py:68] layer 0_down @ epoch 0 new loss 6.700108912127689e-08 old loss 6.708788902187734e-08 BETTER
I0312 10:51:23.115880 1892059 finetune.py:68] layer 2_gate @ epoch 2 new loss 6.735895112797152e-07 old loss 6.754169135092525e-07 BETTER
I0312 10:51:28.563985 1891941 finetune.py:68] layer 1_down @ epoch 0 new loss 0.0001593168854014948 old loss 0.00020615087123587728 BETTER
I0312 10:51:39.785174 1892177 finetune.py:68] layer 3_gate @ epoch 0 new loss 1.340468315902399e-06 old loss 1.3476795857059187e-06 BETTER
I0312 10:51:42.791199 1891823 finetune.py:68] layer 0_down @ epoch 1 new loss 6.697681698142333e-08 old loss 6.700108912127689e-08 BETTER
I0312 10:51:51.348744 1892059 finetune.py:68] layer 2_gate @ epoch 3 new loss 6.718652230119915e-07 old loss 6.735895112797152e-07 BETTER
I0312 10:51:55.274756 1891941 finetune.py:68] layer 1_down @ epoch 1 new loss 0.00015035686374176294 old loss 0.0001593168854014948 BETTER
I0312 10:52:07.681542 1892177 finetune.py:68] layer 3_gate @ epoch 1 new loss 1.3352125733945286e-06 old loss 1.340468315902399e-06 BETTER
I0312 10:52:11.283651 1891823 finetune.py:68] layer 0_down @ epoch 2 new loss 6.695054111105492e-08 old loss 6.697681698142333e-08 BETTER
I0312 10:52:19.501172 1892059 finetune.py:68] layer 2_gate @ epoch 4 new loss 6.702209702780237e-07 old loss 6.718652230119915e-07 BETTER
I0312 10:52:22.140233 1891941 finetune.py:68] layer 1_down @ epoch 2 new loss 0.00014955336519051343 old loss 0.00015035686374176294 BETTER
I0312 10:52:35.132023 1892059 finetune.py:45] layer 2_down initial loss 9.798606015465339e-07
I0312 10:52:35.651527 1892177 finetune.py:68] layer 3_gate @ epoch 2 new loss 1.330387817688461e-06 old loss 1.3352125733945286e-06 BETTER
I0312 10:52:39.740069 1891823 finetune.py:68] layer 0_down @ epoch 3 new loss 6.692516052453357e-08 old loss 6.695054111105492e-08 BETTER
I0312 10:52:49.301242 1891941 finetune.py:68] layer 1_down @ epoch 3 new loss 0.0001495465257903561 old loss 0.00014955336519051343 BETTER
I0312 10:53:01.604196 1892059 finetune.py:68] layer 2_down @ epoch 0 new loss 9.79475885287684e-07 old loss 9.798606015465339e-07 BETTER
I0312 10:53:03.975950 1892177 finetune.py:68] layer 3_gate @ epoch 3 new loss 1.3259905244922265e-06 old loss 1.330387817688461e-06 BETTER
I0312 10:53:08.351047 1891823 finetune.py:68] layer 0_down @ epoch 4 new loss 6.69054145419068e-08 old loss 6.692516052453357e-08 BETTER
0_v proxy err 0.0017843166133388877 tr(WHW.T) 4.225186347961426
0_q proxy err 0.00012495601549744606 tr(WHW.T) 2710.30224609375
0_k proxy err 0.00011637061106739566 tr(WHW.T) 1698.9615478515625
0_o proxy err 0.00016165905981324613 tr(WHW.T) 0.9703740477561951
0_up proxy err 0.0003080961178056896 tr(WHW.T) 43.27195358276367
0_gate proxy err 0.00024392838531639427 tr(WHW.T) 63.47618865966797
0_down proxy err 0.00022149499272927642 tr(WHW.T) 0.6570097804069519
I0312 10:53:16.473128 1891941 finetune.py:76] layer 1_down @ epoch 4 new loss 0.000149594183312729 old loss 0.0001495465257903561 WORSE
1_v proxy err 0.002689515706151724 tr(WHW.T) 16.465883255004883
1_q proxy err 0.00011042554979212582 tr(WHW.T) 4778.5009765625
1_k proxy err 0.0001236438110936433 tr(WHW.T) 4995.67431640625
1_o proxy err 0.0005391804734244943 tr(WHW.T) 1.113130807876587
1_up proxy err 0.00034213741309940815 tr(WHW.T) 109.71495056152344
1_gate proxy err 0.0002162949531339109 tr(WHW.T) 221.3954620361328
1_down proxy err 0.00041654263623058796 tr(WHW.T) 2041.5435791015625
I0312 10:53:28.481285 1892059 finetune.py:68] layer 2_down @ epoch 1 new loss 9.793487834031112e-07 old loss 9.79475885287684e-07 BETTER
I0312 10:53:32.314473 1892177 finetune.py:68] layer 3_gate @ epoch 4 new loss 1.3218063941167202e-06 old loss 1.3259905244922265e-06 BETTER
I0312 10:53:47.248826 1892177 finetune.py:45] layer 3_down initial loss 1.9503947896737373e-06
I0312 10:53:55.250100 1892059 finetune.py:68] layer 2_down @ epoch 2 new loss 9.79259425548662e-07 old loss 9.793487834031112e-07 BETTER
I0312 10:54:12.622245 1892177 finetune.py:68] layer 3_down @ epoch 0 new loss 1.9497701941872947e-06 old loss 1.9503947896737373e-06 BETTER
I0312 10:54:21.921922 1892059 finetune.py:68] layer 2_down @ epoch 3 new loss 9.791820048121735e-07 old loss 9.79259425548662e-07 BETTER
I0312 10:54:33.366421 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 4 in 72.3085069656372s
I0312 10:54:36.744057 1892293 config.py:54] PyTorch version 2.1.1 available.
I0312 10:54:37.812341 1891709 quantize_finetune_llama.py:183] layer 5 gpu 1
I0312 10:54:37.888610 1892293 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 10:54:39.091203 1892177 finetune.py:68] layer 3_down @ epoch 1 new loss 1.9495387277856935e-06 old loss 1.9497701941872947e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:54:46.868136 1892293 finetune.py:45] layer 4_v initial loss 1.2263338931006729e-06
I0312 10:54:48.842466 1892059 finetune.py:68] layer 2_down @ epoch 4 new loss 9.791151569515932e-07 old loss 9.791820048121735e-07 BETTER
2_v proxy err 0.0007175011560320854 tr(WHW.T) 136.67332458496094
2_q proxy err 7.263638690346852e-05 tr(WHW.T) 7753.72265625
2_k proxy err 6.904366455273703e-05 tr(WHW.T) 10207.14453125
2_o proxy err 0.000462439376860857 tr(WHW.T) 1.4601540565490723
2_up proxy err 0.0003854656533803791 tr(WHW.T) 193.43569946289062
2_gate proxy err 0.00027896385290659964 tr(WHW.T) 306.6646423339844
2_down proxy err 0.0004214584769215435 tr(WHW.T) 3.0111000537872314
I0312 10:55:05.565620 1892177 finetune.py:68] layer 3_down @ epoch 2 new loss 1.9492929368425393e-06 old loss 1.9495387277856935e-06 BETTER
I0312 10:55:19.979081 1892293 finetune.py:68] layer 4_v @ epoch 0 new loss 5.987890290271025e-07 old loss 1.2263338931006729e-06 BETTER
I0312 10:55:31.839685 1892177 finetune.py:68] layer 3_down @ epoch 3 new loss 1.949081479324377e-06 old loss 1.9492929368425393e-06 BETTER
I0312 10:55:54.311367 1892293 finetune.py:68] layer 4_v @ epoch 1 new loss 4.970112286173389e-07 old loss 5.987890290271025e-07 BETTER
I0312 10:55:58.431734 1892177 finetune.py:68] layer 3_down @ epoch 4 new loss 1.948955286934506e-06 old loss 1.949081479324377e-06 BETTER
3_v proxy err 0.0007298805867321789 tr(WHW.T) 284.77557373046875
3_q proxy err 9.693089668871835e-05 tr(WHW.T) 7218.03759765625
3_k proxy err 8.61361259012483e-05 tr(WHW.T) 10077.4697265625
3_o proxy err 0.00045105390017852187 tr(WHW.T) 3.353517770767212
3_up proxy err 0.00042337802005931735 tr(WHW.T) 284.7901611328125
3_gate proxy err 0.00029303328483365476 tr(WHW.T) 478.1421813964844
3_down proxy err 0.00042648566886782646 tr(WHW.T) 6.133144855499268
I0312 10:56:03.949984 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 5 in 70.33832836151123s
I0312 10:56:06.936807 1892409 config.py:54] PyTorch version 2.1.1 available.
I0312 10:56:07.894058 1891709 quantize_finetune_llama.py:183] layer 6 gpu 2
I0312 10:56:07.966449 1892409 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:56:15.931003 1892409 finetune.py:45] layer 5_v initial loss 1.795253638192662e-06
I0312 10:56:29.092422 1892293 finetune.py:68] layer 4_v @ epoch 2 new loss 4.4947893229618785e-07 old loss 4.970112286173389e-07 BETTER
I0312 10:56:47.063907 1892409 finetune.py:68] layer 5_v @ epoch 0 new loss 8.790970582595037e-07 old loss 1.795253638192662e-06 BETTER
I0312 10:57:03.931372 1892293 finetune.py:68] layer 4_v @ epoch 3 new loss 4.1865598632284673e-07 old loss 4.4947893229618785e-07 BETTER
I0312 10:57:18.096475 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 6 in 69.86159300804138s
I0312 10:57:19.211999 1892409 finetune.py:68] layer 5_v @ epoch 1 new loss 7.434562121488852e-07 old loss 8.790970582595037e-07 BETTER
I0312 10:57:21.326598 1892525 config.py:54] PyTorch version 2.1.1 available.
I0312 10:57:22.447632 1891709 quantize_finetune_llama.py:183] layer 7 gpu 3
I0312 10:57:22.521544 1892525 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:57:30.311079 1892525 finetune.py:45] layer 6_v initial loss 2.1663843199348776e-06
I0312 10:57:38.823493 1892293 finetune.py:68] layer 4_v @ epoch 4 new loss 4.0041990700956376e-07 old loss 4.1865598632284673e-07 BETTER
I0312 10:57:48.054907 1892293 finetune.py:45] layer 4_q initial loss 7.104032420102158e-07
I0312 10:57:51.347743 1892409 finetune.py:68] layer 5_v @ epoch 2 new loss 6.786552262383339e-07 old loss 7.434562121488852e-07 BETTER
I0312 10:58:01.690843 1892525 finetune.py:68] layer 6_v @ epoch 0 new loss 1.0437573791932664e-06 old loss 2.1663843199348776e-06 BETTER
I0312 10:58:21.988255 1892293 finetune.py:68] layer 4_q @ epoch 0 new loss 4.886097144662926e-07 old loss 7.104032420102158e-07 BETTER
I0312 10:58:23.958525 1892409 finetune.py:68] layer 5_v @ epoch 3 new loss 6.383562549672206e-07 old loss 6.786552262383339e-07 BETTER
I0312 10:58:34.290560 1892525 finetune.py:68] layer 6_v @ epoch 1 new loss 8.977168590718065e-07 old loss 1.0437573791932664e-06 BETTER
I0312 10:58:34.819391 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 7 in 71.8993558883667s
I0312 10:58:38.097999 1892641 config.py:54] PyTorch version 2.1.1 available.
I0312 10:58:39.124875 1891709 quantize_finetune_llama.py:183] layer 8 gpu 0
I0312 10:58:39.204478 1892641 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:58:47.288384 1892641 finetune.py:45] layer 7_v initial loss 2.9074385565763805e-06
I0312 10:58:56.675945 1892293 finetune.py:68] layer 4_q @ epoch 1 new loss 4.5486046928999713e-07 old loss 4.886097144662926e-07 BETTER
I0312 10:58:56.789539 1892409 finetune.py:68] layer 5_v @ epoch 4 new loss 6.119644808677549e-07 old loss 6.383562549672206e-07 BETTER
I0312 10:59:06.235376 1892409 finetune.py:45] layer 5_q initial loss 1.0926153208856704e-06
I0312 10:59:07.051151 1892525 finetune.py:68] layer 6_v @ epoch 2 new loss 8.274764695670456e-07 old loss 8.977168590718065e-07 BETTER
I0312 10:59:18.458147 1892641 finetune.py:68] layer 7_v @ epoch 0 new loss 1.3722014955419581e-06 old loss 2.9074385565763805e-06 BETTER
I0312 10:59:31.385966 1892293 finetune.py:68] layer 4_q @ epoch 2 new loss 4.324986946357967e-07 old loss 4.5486046928999713e-07 BETTER
I0312 10:59:37.776757 1892409 finetune.py:68] layer 5_q @ epoch 0 new loss 7.388257472484838e-07 old loss 1.0926153208856704e-06 BETTER
I0312 10:59:39.942663 1892525 finetune.py:68] layer 6_v @ epoch 3 new loss 8.015986736609193e-07 old loss 8.274764695670456e-07 BETTER
I0312 10:59:50.550032 1892641 finetune.py:68] layer 7_v @ epoch 1 new loss 1.1924656746487017e-06 old loss 1.3722014955419581e-06 BETTER
I0312 11:00:06.175241 1892293 finetune.py:68] layer 4_q @ epoch 3 new loss 4.1566073605281417e-07 old loss 4.324986946357967e-07 BETTER
I0312 11:00:10.187978 1892409 finetune.py:68] layer 5_q @ epoch 1 new loss 6.91417483267287e-07 old loss 7.388257472484838e-07 BETTER
I0312 11:00:12.814322 1892525 finetune.py:68] layer 6_v @ epoch 4 new loss 7.724535748820927e-07 old loss 8.015986736609193e-07 BETTER
I0312 11:00:21.991369 1892525 finetune.py:45] layer 6_q initial loss 1.3712062809645431e-06
I0312 11:00:22.922396 1892641 finetune.py:68] layer 7_v @ epoch 2 new loss 1.105722844840784e-06 old loss 1.1924656746487017e-06 BETTER
I0312 11:00:40.929443 1892293 finetune.py:68] layer 4_q @ epoch 4 new loss 4.0208954033005284e-07 old loss 4.1566073605281417e-07 BETTER
I0312 11:00:42.737149 1892409 finetune.py:68] layer 5_q @ epoch 2 new loss 6.618197403440718e-07 old loss 6.91417483267287e-07 BETTER
I0312 11:00:50.548069 1892293 finetune.py:45] layer 4_k initial loss 6.734137514285976e-07
I0312 11:00:53.738222 1892525 finetune.py:68] layer 6_q @ epoch 0 new loss 1.0389813951405813e-06 old loss 1.3712062809645431e-06 BETTER
I0312 11:00:55.521107 1892641 finetune.py:68] layer 7_v @ epoch 3 new loss 1.047284285959904e-06 old loss 1.105722844840784e-06 BETTER
I0312 11:01:15.028498 1892409 finetune.py:68] layer 5_q @ epoch 3 new loss 6.393861440301407e-07 old loss 6.618197403440718e-07 BETTER
I0312 11:01:24.064132 1892293 finetune.py:68] layer 4_k @ epoch 0 new loss 5.764044885836483e-07 old loss 6.734137514285976e-07 BETTER
I0312 11:01:26.411665 1892525 finetune.py:68] layer 6_q @ epoch 1 new loss 9.605108743926394e-07 old loss 1.0389813951405813e-06 BETTER
I0312 11:01:28.229620 1892641 finetune.py:68] layer 7_v @ epoch 4 new loss 1.0346071803724044e-06 old loss 1.047284285959904e-06 BETTER
I0312 11:01:37.365213 1892641 finetune.py:45] layer 7_q initial loss 1.9317676560604014e-06
I0312 11:01:47.499411 1892409 finetune.py:68] layer 5_q @ epoch 4 new loss 6.2149973700798e-07 old loss 6.393861440301407e-07 BETTER
I0312 11:01:56.750139 1892409 finetune.py:45] layer 5_k initial loss 9.218014156431309e-07
I0312 11:01:58.287905 1892293 finetune.py:68] layer 4_k @ epoch 1 new loss 5.529997793018993e-07 old loss 5.764044885836483e-07 BETTER
I0312 11:01:59.280603 1892525 finetune.py:68] layer 6_q @ epoch 2 new loss 9.181574114336399e-07 old loss 9.605108743926394e-07 BETTER
I0312 11:02:08.739929 1892641 finetune.py:68] layer 7_q @ epoch 0 new loss 1.3680199799637194e-06 old loss 1.9317676560604014e-06 BETTER
I0312 11:02:28.255390 1892409 finetune.py:68] layer 5_k @ epoch 0 new loss 8.325617386617523e-07 old loss 9.218014156431309e-07 BETTER
I0312 11:02:32.052785 1892525 finetune.py:68] layer 6_q @ epoch 3 new loss 8.86291786628135e-07 old loss 9.181574114336399e-07 BETTER
I0312 11:02:32.504010 1892293 finetune.py:68] layer 4_k @ epoch 2 new loss 5.385468853091879e-07 old loss 5.529997793018993e-07 BETTER
I0312 11:02:40.830012 1892641 finetune.py:68] layer 7_q @ epoch 1 new loss 1.2754668432535254e-06 old loss 1.3680199799637194e-06 BETTER
I0312 11:03:00.453694 1892409 finetune.py:68] layer 5_k @ epoch 1 new loss 8.072439072748239e-07 old loss 8.325617386617523e-07 BETTER
I0312 11:03:04.524040 1892525 finetune.py:68] layer 6_q @ epoch 4 new loss 8.616950708528748e-07 old loss 8.86291786628135e-07 BETTER
I0312 11:03:06.726007 1892293 finetune.py:68] layer 4_k @ epoch 3 new loss 5.279905508359661e-07 old loss 5.385468853091879e-07 BETTER
I0312 11:03:13.301920 1892641 finetune.py:68] layer 7_q @ epoch 2 new loss 1.221896354763885e-06 old loss 1.2754668432535254e-06 BETTER
I0312 11:03:14.168452 1892525 finetune.py:45] layer 6_k initial loss 1.4655444147138041e-06
I0312 11:03:32.505464 1892409 finetune.py:68] layer 5_k @ epoch 2 new loss 7.890293431955797e-07 old loss 8.072439072748239e-07 BETTER
I0312 11:03:40.869395 1892293 finetune.py:68] layer 4_k @ epoch 4 new loss 5.187805527384626e-07 old loss 5.279905508359661e-07 BETTER
I0312 11:03:45.411853 1892641 finetune.py:68] layer 7_q @ epoch 3 new loss 1.183233280244167e-06 old loss 1.221896354763885e-06 BETTER
I0312 11:03:45.691702 1892525 finetune.py:68] layer 6_k @ epoch 0 new loss 1.3060181345281308e-06 old loss 1.4655444147138041e-06 BETTER
I0312 11:03:50.694442 1892293 finetune.py:45] layer 4_o initial loss 1.1580513046283158e-06
I0312 11:04:04.556428 1892409 finetune.py:68] layer 5_k @ epoch 3 new loss 7.745869652353576e-07 old loss 7.890293431955797e-07 BETTER
I0312 11:04:18.285420 1892641 finetune.py:68] layer 7_q @ epoch 4 new loss 1.1525422678460018e-06 old loss 1.183233280244167e-06 BETTER
I0312 11:04:18.588074 1892525 finetune.py:68] layer 6_k @ epoch 1 new loss 1.2645951983358827e-06 old loss 1.3060181345281308e-06 BETTER
I0312 11:04:23.608119 1892293 finetune.py:68] layer 4_o @ epoch 0 new loss 1.103328145291016e-06 old loss 1.1580513046283158e-06 BETTER
I0312 11:04:27.636617 1892641 finetune.py:45] layer 7_k initial loss 1.9738909031730145e-06
I0312 11:04:36.606765 1892409 finetune.py:68] layer 5_k @ epoch 4 new loss 7.631967378074478e-07 old loss 7.745869652353576e-07 BETTER
I0312 11:04:45.840291 1892409 finetune.py:45] layer 5_o initial loss 1.955299921974074e-06
I0312 11:04:50.630862 1892525 finetune.py:68] layer 6_k @ epoch 2 new loss 1.237481001226115e-06 old loss 1.2645951983358827e-06 BETTER
I0312 11:04:57.207200 1892293 finetune.py:68] layer 4_o @ epoch 1 new loss 1.0809236528075417e-06 old loss 1.103328145291016e-06 BETTER
I0312 11:04:58.487963 1892641 finetune.py:68] layer 7_k @ epoch 0 new loss 1.6839641148180817e-06 old loss 1.9738909031730145e-06 BETTER
I0312 11:05:16.553443 1892409 finetune.py:68] layer 5_o @ epoch 0 new loss 1.8316222849534824e-06 old loss 1.955299921974074e-06 BETTER
I0312 11:05:22.702406 1892525 finetune.py:68] layer 6_k @ epoch 3 new loss 1.2194213923066854e-06 old loss 1.237481001226115e-06 BETTER
I0312 11:05:29.975873 1892641 finetune.py:68] layer 7_k @ epoch 1 new loss 1.6359817891498096e-06 old loss 1.6839641148180817e-06 BETTER
I0312 11:05:30.788764 1892293 finetune.py:68] layer 4_o @ epoch 2 new loss 1.0645690053934231e-06 old loss 1.0809236528075417e-06 BETTER
I0312 11:05:47.883234 1892409 finetune.py:68] layer 5_o @ epoch 1 new loss 1.7697553857942694e-06 old loss 1.8316222849534824e-06 BETTER
I0312 11:05:54.850515 1892525 finetune.py:68] layer 6_k @ epoch 4 new loss 1.2011323633487336e-06 old loss 1.2194213923066854e-06 BETTER
I0312 11:06:01.638118 1892641 finetune.py:68] layer 7_k @ epoch 2 new loss 1.605413899596897e-06 old loss 1.6359817891498096e-06 BETTER
I0312 11:06:04.101192 1892293 finetune.py:68] layer 4_o @ epoch 3 new loss 1.051391791406786e-06 old loss 1.0645690053934231e-06 BETTER
I0312 11:06:04.199131 1892525 finetune.py:45] layer 6_o initial loss 2.7255116492597153e-06
I0312 11:06:19.194390 1892409 finetune.py:68] layer 5_o @ epoch 2 new loss 1.724623302834516e-06 old loss 1.7697553857942694e-06 BETTER
I0312 11:06:33.299005 1892641 finetune.py:68] layer 7_k @ epoch 3 new loss 1.5819434793229448e-06 old loss 1.605413899596897e-06 BETTER
I0312 11:06:34.977277 1892525 finetune.py:68] layer 6_o @ epoch 0 new loss 2.522567456253455e-06 old loss 2.7255116492597153e-06 BETTER
I0312 11:06:37.630201 1892293 finetune.py:68] layer 4_o @ epoch 4 new loss 1.0399630809843075e-06 old loss 1.051391791406786e-06 BETTER
I0312 11:06:50.546019 1892409 finetune.py:68] layer 5_o @ epoch 3 new loss 1.6887966012291145e-06 old loss 1.724623302834516e-06 BETTER
I0312 11:06:53.169665 1892293 finetune.py:45] layer 4_up initial loss 1.7192737686855253e-06
I0312 11:07:05.050308 1892641 finetune.py:68] layer 7_k @ epoch 4 new loss 1.5626602589691174e-06 old loss 1.5819434793229448e-06 BETTER
I0312 11:07:06.589059 1892525 finetune.py:68] layer 6_o @ epoch 1 new loss 2.444517576805083e-06 old loss 2.522567456253455e-06 BETTER
I0312 11:07:14.222065 1892641 finetune.py:45] layer 7_o initial loss 3.697601187013788e-06
I0312 11:07:21.854439 1892409 finetune.py:68] layer 5_o @ epoch 4 new loss 1.6594340195297264e-06 old loss 1.6887966012291145e-06 BETTER
I0312 11:07:23.895698 1892293 finetune.py:68] layer 4_up @ epoch 0 new loss 1.6923008843150456e-06 old loss 1.7192737686855253e-06 BETTER
I0312 11:07:36.781409 1892409 finetune.py:45] layer 5_up initial loss 2.754359002210549e-06
I0312 11:07:38.218456 1892525 finetune.py:68] layer 6_o @ epoch 2 new loss 2.391129100942635e-06 old loss 2.444517576805083e-06 BETTER
I0312 11:07:44.637026 1892641 finetune.py:68] layer 7_o @ epoch 0 new loss 3.35328581968497e-06 old loss 3.697601187013788e-06 BETTER
I0312 11:07:55.633667 1892293 finetune.py:68] layer 4_up @ epoch 1 new loss 1.6760059224907309e-06 old loss 1.6923008843150456e-06 BETTER
I0312 11:08:05.880339 1892409 finetune.py:68] layer 5_up @ epoch 0 new loss 2.6963975869875867e-06 old loss 2.754359002210549e-06 BETTER
I0312 11:08:09.867604 1892525 finetune.py:68] layer 6_o @ epoch 3 new loss 2.3502270778408274e-06 old loss 2.391129100942635e-06 BETTER
I0312 11:08:15.466081 1892641 finetune.py:68] layer 7_o @ epoch 1 new loss 3.2280834147968562e-06 old loss 3.35328581968497e-06 BETTER
I0312 11:08:27.391299 1892293 finetune.py:68] layer 4_up @ epoch 2 new loss 1.6623662304482423e-06 old loss 1.6760059224907309e-06 BETTER
I0312 11:08:35.313349 1892409 finetune.py:68] layer 5_up @ epoch 1 new loss 2.6599111606628867e-06 old loss 2.6963975869875867e-06 BETTER
I0312 11:08:41.459304 1892525 finetune.py:68] layer 6_o @ epoch 4 new loss 2.317202188351075e-06 old loss 2.3502270778408274e-06 BETTER
I0312 11:08:46.199603 1892641 finetune.py:68] layer 7_o @ epoch 2 new loss 3.1466177006223006e-06 old loss 3.2280834147968562e-06 BETTER
I0312 11:08:56.572701 1892525 finetune.py:45] layer 6_up initial loss 3.9970213947526645e-06
I0312 11:08:59.416009 1892293 finetune.py:68] layer 4_up @ epoch 3 new loss 1.6501762729603797e-06 old loss 1.6623662304482423e-06 BETTER
I0312 11:09:04.843990 1892409 finetune.py:68] layer 5_up @ epoch 2 new loss 2.629458776937099e-06 old loss 2.6599111606628867e-06 BETTER
I0312 11:09:17.096067 1892641 finetune.py:68] layer 7_o @ epoch 3 new loss 3.0861331197229447e-06 old loss 3.1466177006223006e-06 BETTER
I0312 11:09:25.565061 1892525 finetune.py:68] layer 6_up @ epoch 0 new loss 3.886396825691918e-06 old loss 3.9970213947526645e-06 BETTER
I0312 11:09:31.176135 1892293 finetune.py:68] layer 4_up @ epoch 4 new loss 1.6390533801313723e-06 old loss 1.6501762729603797e-06 BETTER
I0312 11:09:34.654496 1892409 finetune.py:68] layer 5_up @ epoch 3 new loss 2.602815584396012e-06 old loss 2.629458776937099e-06 BETTER
I0312 11:09:46.405764 1892293 finetune.py:45] layer 4_gate initial loss 2.17742262975662e-06
I0312 11:09:48.478951 1892641 finetune.py:68] layer 7_o @ epoch 4 new loss 3.0378530482266797e-06 old loss 3.0861331197229447e-06 BETTER
I0312 11:09:56.057161 1892525 finetune.py:68] layer 6_up @ epoch 1 new loss 3.826879947155248e-06 old loss 3.886396825691918e-06 BETTER
I0312 11:10:05.434121 1892409 finetune.py:68] layer 5_up @ epoch 4 new loss 2.5793942768359557e-06 old loss 2.602815584396012e-06 BETTER
I0312 11:10:06.571877 1892641 finetune.py:45] layer 7_up initial loss 5.353500000637723e-06
I0312 11:10:15.766078 1892293 finetune.py:68] layer 4_gate @ epoch 0 new loss 2.1571634079009527e-06 old loss 2.17742262975662e-06 BETTER
I0312 11:10:21.193608 1892409 finetune.py:45] layer 5_gate initial loss 3.3957860523514682e-06
I0312 11:10:26.048727 1892525 finetune.py:68] layer 6_up @ epoch 2 new loss 3.780104862016742e-06 old loss 3.826879947155248e-06 BETTER
I0312 11:10:34.880996 1892641 finetune.py:68] layer 7_up @ epoch 0 new loss 5.153063284524251e-06 old loss 5.353500000637723e-06 BETTER
I0312 11:10:45.725455 1892293 finetune.py:68] layer 4_gate @ epoch 1 new loss 2.1451412521855673e-06 old loss 2.1571634079009527e-06 BETTER
I0312 11:10:48.669724 1892409 finetune.py:68] layer 5_gate @ epoch 0 new loss 3.356019988132175e-06 old loss 3.3957860523514682e-06 BETTER
I0312 11:10:55.839744 1892525 finetune.py:68] layer 6_up @ epoch 3 new loss 3.7403904116217745e-06 old loss 3.780104862016742e-06 BETTER
I0312 11:11:03.888771 1892641 finetune.py:68] layer 7_up @ epoch 1 new loss 5.0589032980496995e-06 old loss 5.153063284524251e-06 BETTER
I0312 11:11:15.736403 1892293 finetune.py:68] layer 4_gate @ epoch 2 new loss 2.134497663064394e-06 old loss 2.1451412521855673e-06 BETTER
I0312 11:11:16.922883 1892409 finetune.py:68] layer 5_gate @ epoch 1 new loss 3.3316264307359233e-06 old loss 3.356019988132175e-06 BETTER
I0312 11:11:25.730523 1892525 finetune.py:68] layer 6_up @ epoch 4 new loss 3.7062766296003247e-06 old loss 3.7403904116217745e-06 BETTER
I0312 11:11:33.287905 1892641 finetune.py:68] layer 7_up @ epoch 2 new loss 4.986385192751186e-06 old loss 5.0589032980496995e-06 BETTER
I0312 11:11:40.682458 1892525 finetune.py:45] layer 6_gate initial loss 4.880113010585774e-06
I0312 11:11:45.083547 1892409 finetune.py:68] layer 5_gate @ epoch 2 new loss 3.310421334390412e-06 old loss 3.3316264307359233e-06 BETTER
I0312 11:11:45.828423 1892293 finetune.py:68] layer 4_gate @ epoch 3 new loss 2.12482814276882e-06 old loss 2.134497663064394e-06 BETTER
I0312 11:12:02.740534 1892641 finetune.py:68] layer 7_up @ epoch 3 new loss 4.927263489662437e-06 old loss 4.986385192751186e-06 BETTER
I0312 11:12:08.276517 1892525 finetune.py:68] layer 6_gate @ epoch 0 new loss 4.810687642020639e-06 old loss 4.880113010585774e-06 BETTER
I0312 11:12:13.322515 1892409 finetune.py:68] layer 5_gate @ epoch 3 new loss 3.2910625122895e-06 old loss 3.310421334390412e-06 BETTER
I0312 11:12:15.938275 1892293 finetune.py:68] layer 4_gate @ epoch 4 new loss 2.115699999194476e-06 old loss 2.12482814276882e-06 BETTER
I0312 11:12:31.863979 1892293 finetune.py:45] layer 4_down initial loss 3.2942234611255117e-06
I0312 11:12:32.272521 1892641 finetune.py:68] layer 7_up @ epoch 4 new loss 4.876821094512707e-06 old loss 4.927263489662437e-06 BETTER
I0312 11:12:36.448679 1892525 finetune.py:68] layer 6_gate @ epoch 1 new loss 4.7712615014461335e-06 old loss 4.810687642020639e-06 BETTER
I0312 11:12:41.638046 1892409 finetune.py:68] layer 5_gate @ epoch 4 new loss 3.2735458717070287e-06 old loss 3.2910625122895e-06 BETTER
I0312 11:12:47.278805 1892641 finetune.py:45] layer 7_gate initial loss 6.493410637631314e-06
I0312 11:12:57.426694 1892409 finetune.py:45] layer 5_down initial loss 4.953729785484029e-06
I0312 11:12:59.580451 1892293 finetune.py:68] layer 4_down @ epoch 0 new loss 3.293010877314373e-06 old loss 3.2942234611255117e-06 BETTER
I0312 11:13:04.539489 1892525 finetune.py:68] layer 6_gate @ epoch 2 new loss 4.738204097520793e-06 old loss 4.7712615014461335e-06 BETTER
I0312 11:13:14.531956 1892641 finetune.py:68] layer 7_gate @ epoch 0 new loss 6.371140898409067e-06 old loss 6.493410637631314e-06 BETTER
I0312 11:13:23.383228 1892409 finetune.py:68] layer 5_down @ epoch 0 new loss 4.952254585077753e-06 old loss 4.953729785484029e-06 BETTER
I0312 11:13:28.411776 1892293 finetune.py:68] layer 4_down @ epoch 1 new loss 3.2927443953667535e-06 old loss 3.293010877314373e-06 BETTER
I0312 11:13:32.993932 1892525 finetune.py:68] layer 6_gate @ epoch 3 new loss 4.709015229309443e-06 old loss 4.738204097520793e-06 BETTER
I0312 11:13:42.610709 1892641 finetune.py:68] layer 7_gate @ epoch 1 new loss 6.308384854492033e-06 old loss 6.371140898409067e-06 BETTER
I0312 11:13:50.456219 1892409 finetune.py:68] layer 5_down @ epoch 1 new loss 4.9519371714268345e-06 old loss 4.952254585077753e-06 BETTER
I0312 11:13:57.083157 1892293 finetune.py:68] layer 4_down @ epoch 2 new loss 3.292559540568618e-06 old loss 3.2927443953667535e-06 BETTER
I0312 11:14:01.243118 1892525 finetune.py:68] layer 6_gate @ epoch 4 new loss 4.683121915149968e-06 old loss 4.709015229309443e-06 BETTER
I0312 11:14:10.661522 1892641 finetune.py:68] layer 7_gate @ epoch 2 new loss 6.257061158976285e-06 old loss 6.308384854492033e-06 BETTER
I0312 11:14:16.894628 1892525 finetune.py:45] layer 6_down initial loss 7.206738700915594e-06
I0312 11:14:17.368555 1892409 finetune.py:68] layer 5_down @ epoch 2 new loss 4.9517329898662865e-06 old loss 4.9519371714268345e-06 BETTER
I0312 11:14:25.548709 1892293 finetune.py:68] layer 4_down @ epoch 3 new loss 3.2924376682785805e-06 old loss 3.292559540568618e-06 BETTER
I0312 11:14:38.713052 1892641 finetune.py:68] layer 7_gate @ epoch 3 new loss 6.213582310010679e-06 old loss 6.257061158976285e-06 BETTER
I0312 11:14:43.047969 1892525 finetune.py:68] layer 6_down @ epoch 0 new loss 7.20434127288172e-06 old loss 7.206738700915594e-06 BETTER
I0312 11:14:44.346393 1892409 finetune.py:68] layer 5_down @ epoch 3 new loss 4.951655682816636e-06 old loss 4.9517329898662865e-06 BETTER
I0312 11:14:54.054505 1892293 finetune.py:68] layer 4_down @ epoch 4 new loss 3.292361952844658e-06 old loss 3.2924376682785805e-06 BETTER
4_v proxy err 0.0007163977716118097 tr(WHW.T) 274.6131286621094
4_q proxy err 9.564732317812741e-05 tr(WHW.T) 6916.564453125
4_k proxy err 8.587151387473568e-05 tr(WHW.T) 10418.279296875
4_o proxy err 0.000461730407550931 tr(WHW.T) 5.13647985458374
4_up proxy err 0.0004176894435659051 tr(WHW.T) 397.6750183105469
4_gate proxy err 0.0002530637721065432 tr(WHW.T) 820.7872314453125
4_down proxy err 0.00042776099871844053 tr(WHW.T) 11.56481647491455
I0312 11:15:07.125187 1892641 finetune.py:68] layer 7_gate @ epoch 4 new loss 6.175761427584803e-06 old loss 6.213582310010679e-06 BETTER
I0312 11:15:10.087940 1892525 finetune.py:68] layer 6_down @ epoch 1 new loss 7.203789664345095e-06 old loss 7.20434127288172e-06 BETTER
I0312 11:15:11.432092 1892409 finetune.py:68] layer 5_down @ epoch 4 new loss 4.951525170326931e-06 old loss 4.951655682816636e-06 BETTER
5_v proxy err 0.0007138362852856517 tr(WHW.T) 298.47540283203125
5_q proxy err 9.452533413423225e-05 tr(WHW.T) 6771.7626953125
5_k proxy err 8.541268471162766e-05 tr(WHW.T) 10844.62890625
5_o proxy err 0.0006020502769388258 tr(WHW.T) 7.947836875915527
5_up proxy err 0.00041065108962357044 tr(WHW.T) 506.6611633300781
5_gate proxy err 0.0002330490096937865 tr(WHW.T) 1104.4483642578125
5_down proxy err 0.0004483770753722638 tr(WHW.T) 15.649876594543457
I0312 11:15:22.700507 1892641 finetune.py:45] layer 7_down initial loss 9.56156691245269e-06
I0312 11:15:36.933083 1892525 finetune.py:68] layer 6_down @ epoch 2 new loss 7.203501354524633e-06 old loss 7.203789664345095e-06 BETTER
I0312 11:15:48.069920 1892641 finetune.py:68] layer 7_down @ epoch 0 new loss 9.559325917507522e-06 old loss 9.56156691245269e-06 BETTER
I0312 11:16:03.606000 1892525 finetune.py:68] layer 6_down @ epoch 3 new loss 7.203266250144225e-06 old loss 7.203501354524633e-06 BETTER
I0312 11:16:14.344237 1892641 finetune.py:68] layer 7_down @ epoch 1 new loss 9.558811143506318e-06 old loss 9.559325917507522e-06 BETTER
I0312 11:16:28.446617 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 8 in 72.91035461425781s
I0312 11:16:30.398208 1892525 finetune.py:68] layer 6_down @ epoch 4 new loss 7.203100267361151e-06 old loss 7.203266250144225e-06 BETTER
I0312 11:16:31.825538 1892757 config.py:54] PyTorch version 2.1.1 available.
6_v proxy err 0.0006923641194589436 tr(WHW.T) 443.5464782714844
6_q proxy err 0.00010384531924501061 tr(WHW.T) 7576.73828125
6_k proxy err 8.912938210414723e-05 tr(WHW.T) 10412.2724609375
6_o proxy err 0.0005792076699435711 tr(WHW.T) 11.569969177246094
6_up proxy err 0.00040991551941260695 tr(WHW.T) 617.2415161132812
6_gate proxy err 0.00020927435252815485 tr(WHW.T) 1554.4053955078125
6_down proxy err 0.00046050146920606494 tr(WHW.T) 22.984146118164062
I0312 11:16:32.811917 1891709 quantize_finetune_llama.py:183] layer 9 gpu 1
I0312 11:16:32.877873 1892757 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 11:16:40.774602 1892641 finetune.py:68] layer 7_down @ epoch 2 new loss 9.55848918238189e-06 old loss 9.558811143506318e-06 BETTER
I0312 11:16:41.147283 1892757 finetune.py:45] layer 8_v initial loss 4.533336323220283e-06
I0312 11:17:07.193590 1892641 finetune.py:68] layer 7_down @ epoch 3 new loss 9.558245437801816e-06 old loss 9.55848918238189e-06 BETTER
I0312 11:17:14.097766 1892757 finetune.py:68] layer 8_v @ epoch 0 new loss 1.9522944967320655e-06 old loss 4.533336323220283e-06 BETTER
I0312 11:17:33.520210 1892641 finetune.py:68] layer 7_down @ epoch 4 new loss 9.558090823702514e-06 old loss 9.558245437801816e-06 BETTER
7_v proxy err 0.0006903729517944157 tr(WHW.T) 489.9357604980469
7_q proxy err 0.00010810825187945738 tr(WHW.T) 7672.70068359375
7_k proxy err 9.120286267716438e-05 tr(WHW.T) 10201.697265625
7_o proxy err 0.0006507112411782146 tr(WHW.T) 15.130585670471191
7_up proxy err 0.0004046250833198428 tr(WHW.T) 735.7979736328125
7_gate proxy err 0.00020644295727834105 tr(WHW.T) 1875.026123046875
7_down proxy err 0.0004655396332964301 tr(WHW.T) 30.588972091674805
I0312 11:17:44.109777 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 9 in 70.11563658714294s
I0312 11:17:47.389369 1892873 config.py:54] PyTorch version 2.1.1 available.
I0312 11:17:48.489650 1891709 quantize_finetune_llama.py:183] layer 10 gpu 2
I0312 11:17:48.556515 1892873 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 11:17:48.616384 1892757 finetune.py:68] layer 8_v @ epoch 1 new loss 1.685477855062345e-06 old loss 1.9522944967320655e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 11:17:56.661181 1892873 finetune.py:45] layer 9_v initial loss 5.395251719164662e-06
I0312 11:18:23.381741 1892757 finetune.py:68] layer 8_v @ epoch 2 new loss 1.556464098939614e-06 old loss 1.685477855062345e-06 BETTER
I0312 11:18:27.891291 1892873 finetune.py:68] layer 9_v @ epoch 0 new loss 2.3187317310657818e-06 old loss 5.395251719164662e-06 BETTER
I0312 11:18:58.219500 1892757 finetune.py:68] layer 8_v @ epoch 3 new loss 1.4984892686698004e-06 old loss 1.556464098939614e-06 BETTER
I0312 11:18:58.670168 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 10 in 69.79263520240784s
I0312 11:19:00.112148 1892873 finetune.py:68] layer 9_v @ epoch 1 new loss 2.0222873899911065e-06 old loss 2.3187317310657818e-06 BETTER
I0312 11:19:01.894482 1892989 config.py:54] PyTorch version 2.1.1 available.
I0312 11:19:02.973258 1891709 quantize_finetune_llama.py:183] layer 11 gpu 3
I0312 11:19:03.040212 1892989 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 11:19:11.414338 1892989 finetune.py:45] layer 10_v initial loss 7.621467375429347e-06
I0312 11:19:32.450146 1892873 finetune.py:68] layer 9_v @ epoch 2 new loss 1.8645989712240407e-06 old loss 2.0222873899911065e-06 BETTER
I0312 11:19:33.125218 1892757 finetune.py:68] layer 8_v @ epoch 4 new loss 1.4881362631058437e-06 old loss 1.4984892686698004e-06 BETTER
I0312 11:19:42.523734 1892757 finetune.py:45] layer 8_q initial loss 2.6207642349618254e-06
I0312 11:19:42.922480 1892989 finetune.py:68] layer 10_v @ epoch 0 new loss 3.2090490549308015e-06 old loss 7.621467375429347e-06 BETTER
I0312 11:20:05.279777 1892873 finetune.py:68] layer 9_v @ epoch 3 new loss 1.794655759113084e-06 old loss 1.8645989712240407e-06 BETTER
I0312 11:20:14.185587 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 11 in 70.77434659004211s
I0312 11:20:15.125006 1892989 finetune.py:68] layer 10_v @ epoch 1 new loss 2.7813769065687666e-06 old loss 3.2090490549308015e-06 BETTER
I0312 11:20:16.105065 1892757 finetune.py:68] layer 8_q @ epoch 0 new loss 1.8956139911097125e-06 old loss 2.6207642349618254e-06 BETTER
I0312 11:20:17.403160 1893105 config.py:54] PyTorch version 2.1.1 available.
I0312 11:20:18.588621 1891709 quantize_finetune_llama.py:183] layer 12 gpu 0
I0312 11:20:18.668943 1893105 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 11:20:26.470067 1893105 finetune.py:45] layer 11_v initial loss 7.997961802175269e-06
I0312 11:20:37.929858 1892873 finetune.py:68] layer 9_v @ epoch 4 new loss 1.7284725117860944e-06 old loss 1.794655759113084e-06 BETTER
I0312 11:20:46.846060 1892873 finetune.py:45] layer 9_q initial loss 2.8853348794655176e-06
I0312 11:20:47.496381 1892989 finetune.py:68] layer 10_v @ epoch 2 new loss 2.5832523533608764e-06 old loss 2.7813769065687666e-06 BETTER
I0312 11:20:50.465875 1892757 finetune.py:68] layer 8_q @ epoch 1 new loss 1.762529677762359e-06 old loss 1.8956139911097125e-06 BETTER
I0312 11:20:58.261976 1893105 finetune.py:68] layer 11_v @ epoch 0 new loss 3.366912778801634e-06 old loss 7.997961802175269e-06 BETTER
I0312 11:21:20.127950 1892873 finetune.py:68] layer 9_q @ epoch 0 new loss 2.226707010777318e-06 old loss 2.8853348794655176e-06 BETTER
I0312 11:21:21.741980 1892989 finetune.py:68] layer 10_v @ epoch 3 new loss 2.517105031074607e-06 old loss 2.5832523533608764e-06 BETTER
I0312 11:21:25.287943 1892757 finetune.py:68] layer 8_q @ epoch 2 new loss 1.6890896858967608e-06 old loss 1.762529677762359e-06 BETTER
I0312 11:21:30.358940 1893105 finetune.py:68] layer 11_v @ epoch 1 new loss 2.9358852771110833e-06 old loss 3.366912778801634e-06 BETTER
I0312 11:21:52.806654 1892873 finetune.py:68] layer 9_q @ epoch 1 new loss 2.0955576474079862e-06 old loss 2.226707010777318e-06 BETTER
I0312 11:21:54.674676 1892989 finetune.py:68] layer 10_v @ epoch 4 new loss 2.4869098069757456e-06 old loss 2.517105031074607e-06 BETTER
I0312 11:21:59.713314 1892757 finetune.py:68] layer 8_q @ epoch 3 new loss 1.6271076219709357e-06 old loss 1.6890896858967608e-06 BETTER
I0312 11:22:02.776241 1893105 finetune.py:68] layer 11_v @ epoch 2 new loss 2.736890110099921e-06 old loss 2.9358852771110833e-06 BETTER
I0312 11:22:04.330833 1892989 finetune.py:45] layer 10_q initial loss 4.003260073659476e-06
I0312 11:22:25.117486 1892873 finetune.py:68] layer 9_q @ epoch 2 new loss 2.0520587895589415e-06 old loss 2.0955576474079862e-06 BETTER
I0312 11:22:34.714078 1892757 finetune.py:68] layer 8_q @ epoch 4 new loss 1.5938267097226344e-06 old loss 1.6271076219709357e-06 BETTER
I0312 11:22:35.825297 1893105 finetune.py:68] layer 11_v @ epoch 3 new loss 2.650744136190042e-06 old loss 2.736890110099921e-06 BETTER
I0312 11:22:36.383279 1892989 finetune.py:68] layer 10_q @ epoch 0 new loss 3.0369192245416343e-06 old loss 4.003260073659476e-06 BETTER
I0312 11:22:44.438963 1892757 finetune.py:45] layer 8_k initial loss 2.621183057271992e-06
I0312 11:22:57.167812 1892873 finetune.py:68] layer 9_q @ epoch 3 new loss 2.0173970369796734e-06 old loss 2.0520587895589415e-06 BETTER
I0312 11:23:08.656295 1893105 finetune.py:76] layer 11_v @ epoch 4 new loss 2.6596926545607857e-06 old loss 2.650744136190042e-06 WORSE
I0312 11:23:09.073624 1892989 finetune.py:68] layer 10_q @ epoch 1 new loss 2.854184685929795e-06 old loss 3.0369192245416343e-06 BETTER
I0312 11:23:17.515147 1892757 finetune.py:68] layer 8_k @ epoch 0 new loss 2.254463424833375e-06 old loss 2.621183057271992e-06 BETTER
I0312 11:23:17.600294 1893105 finetune.py:45] layer 11_q initial loss 4.244022420607507e-06
I0312 11:23:29.396802 1892873 finetune.py:68] layer 9_q @ epoch 4 new loss 1.991172666748753e-06 old loss 2.0173970369796734e-06 BETTER
I0312 11:23:38.588558 1892873 finetune.py:45] layer 9_k initial loss 3.188129539921647e-06
I0312 11:23:41.682972 1892989 finetune.py:68] layer 10_q @ epoch 2 new loss 2.7360320018487982e-06 old loss 2.854184685929795e-06 BETTER
I0312 11:23:49.101466 1893105 finetune.py:68] layer 11_q @ epoch 0 new loss 3.397475211386336e-06 old loss 4.244022420607507e-06 BETTER
I0312 11:23:51.609350 1892757 finetune.py:68] layer 8_k @ epoch 1 new loss 2.1922999167145463e-06 old loss 2.254463424833375e-06 BETTER
I0312 11:24:10.353384 1892873 finetune.py:68] layer 9_k @ epoch 0 new loss 2.6934483230434125e-06 old loss 3.188129539921647e-06 BETTER
I0312 11:24:14.513573 1892989 finetune.py:68] layer 10_q @ epoch 3 new loss 2.6474801870790543e-06 old loss 2.7360320018487982e-06 BETTER
I0312 11:24:21.316617 1893105 finetune.py:68] layer 11_q @ epoch 1 new loss 3.1749957543070195e-06 old loss 3.397475211386336e-06 BETTER
I0312 11:24:25.576450 1892757 finetune.py:68] layer 8_k @ epoch 2 new loss 2.153409013772034e-06 old loss 2.1922999167145463e-06 BETTER
I0312 11:24:42.473631 1892873 finetune.py:68] layer 9_k @ epoch 1 new loss 2.6305363007850247e-06 old loss 2.6934483230434125e-06 BETTER
I0312 11:24:47.161034 1892989 finetune.py:68] layer 10_q @ epoch 4 new loss 2.5767983515834203e-06 old loss 2.6474801870790543e-06 BETTER
I0312 11:24:53.481570 1893105 finetune.py:68] layer 11_q @ epoch 2 new loss 3.0546405014320044e-06 old loss 3.1749957543070195e-06 BETTER
I0312 11:24:56.418302 1892989 finetune.py:45] layer 10_k initial loss 3.968460077885538e-06
I0312 11:24:59.576483 1892757 finetune.py:68] layer 8_k @ epoch 3 new loss 2.1174255380174145e-06 old loss 2.153409013772034e-06 BETTER
I0312 11:25:14.521725 1892873 finetune.py:68] layer 9_k @ epoch 2 new loss 2.5750225631782087e-06 old loss 2.6305363007850247e-06 BETTER
I0312 11:25:25.569899 1893105 finetune.py:68] layer 11_q @ epoch 3 new loss 2.950810994661879e-06 old loss 3.0546405014320044e-06 BETTER
I0312 11:25:27.849658 1892989 finetune.py:68] layer 10_k @ epoch 0 new loss 3.518385938150459e-06 old loss 3.968460077885538e-06 BETTER
I0312 11:25:33.465151 1892757 finetune.py:68] layer 8_k @ epoch 4 new loss 2.0997381398046855e-06 old loss 2.1174255380174145e-06 BETTER
I0312 11:25:42.903716 1892757 finetune.py:45] layer 8_o initial loss 5.2891077757522e-06
I0312 11:25:46.493088 1892873 finetune.py:68] layer 9_k @ epoch 3 new loss 2.533590077291592e-06 old loss 2.5750225631782087e-06 BETTER
I0312 11:25:57.683624 1893105 finetune.py:68] layer 11_q @ epoch 4 new loss 2.918079871960799e-06 old loss 2.950810994661879e-06 BETTER
I0312 11:25:59.959762 1892989 finetune.py:68] layer 10_k @ epoch 1 new loss 3.422768259042641e-06 old loss 3.518385938150459e-06 BETTER
I0312 11:26:06.600911 1893105 finetune.py:45] layer 11_k initial loss 4.46784133600886e-06
I0312 11:26:15.530990 1892757 finetune.py:68] layer 8_o @ epoch 0 new loss 4.7315902520495e-06 old loss 5.2891077757522e-06 BETTER
I0312 11:26:18.482980 1892873 finetune.py:68] layer 9_k @ epoch 4 new loss 2.5118631583609385e-06 old loss 2.533590077291592e-06 BETTER
I0312 11:26:27.577948 1892873 finetune.py:45] layer 9_o initial loss 6.516490429930855e-06
I0312 11:26:32.785670 1892989 finetune.py:68] layer 10_k @ epoch 2 new loss 3.364834583408083e-06 old loss 3.422768259042641e-06 BETTER
I0312 11:26:38.683082 1893105 finetune.py:68] layer 11_k @ epoch 0 new loss 4.009542863059323e-06 old loss 4.46784133600886e-06 BETTER
I0312 11:26:49.511069 1892757 finetune.py:68] layer 8_o @ epoch 1 new loss 4.532278580882121e-06 old loss 4.7315902520495e-06 BETTER
I0312 11:26:58.208880 1892873 finetune.py:68] layer 9_o @ epoch 0 new loss 5.841729489475256e-06 old loss 6.516490429930855e-06 BETTER
I0312 11:27:04.978103 1892989 finetune.py:68] layer 10_k @ epoch 3 new loss 3.3601068025745917e-06 old loss 3.364834583408083e-06 BETTER
I0312 11:27:10.673088 1893105 finetune.py:68] layer 11_k @ epoch 1 new loss 3.9501833271060605e-06 old loss 4.009542863059323e-06 BETTER
I0312 11:27:22.929389 1892757 finetune.py:68] layer 8_o @ epoch 2 new loss 4.403704224387184e-06 old loss 4.532278580882121e-06 BETTER
I0312 11:27:29.499705 1892873 finetune.py:68] layer 9_o @ epoch 1 new loss 5.579115622822428e-06 old loss 5.841729489475256e-06 BETTER
I0312 11:27:37.108435 1892989 finetune.py:76] layer 10_k @ epoch 4 new loss 3.360917389727547e-06 old loss 3.3601068025745917e-06 WORSE
I0312 11:27:42.413777 1893105 finetune.py:68] layer 11_k @ epoch 2 new loss 3.86268766305875e-06 old loss 3.9501833271060605e-06 BETTER
I0312 11:27:46.081163 1892989 finetune.py:45] layer 10_o initial loss 8.965434062702116e-06
I0312 11:27:56.559980 1892757 finetune.py:68] layer 8_o @ epoch 3 new loss 4.3099407776026055e-06 old loss 4.403704224387184e-06 BETTER
I0312 11:28:00.782364 1892873 finetune.py:68] layer 9_o @ epoch 2 new loss 5.418086402642075e-06 old loss 5.579115622822428e-06 BETTER
I0312 11:28:14.070358 1893105 finetune.py:68] layer 11_k @ epoch 3 new loss 3.82889174943557e-06 old loss 3.86268766305875e-06 BETTER
I0312 11:28:16.916129 1892989 finetune.py:68] layer 10_o @ epoch 0 new loss 8.005347808648366e-06 old loss 8.965434062702116e-06 BETTER
I0312 11:28:29.803934 1892757 finetune.py:68] layer 8_o @ epoch 4 new loss 4.236965651216451e-06 old loss 4.3099407776026055e-06 BETTER
I0312 11:28:32.160458 1892873 finetune.py:68] layer 9_o @ epoch 3 new loss 5.299057193042245e-06 old loss 5.418086402642075e-06 BETTER
I0312 11:28:45.404027 1892757 finetune.py:45] layer 8_up initial loss 7.062687018333236e-06
I0312 11:28:45.833624 1893105 finetune.py:68] layer 11_k @ epoch 4 new loss 3.826400643447414e-06 old loss 3.82889174943557e-06 BETTER
I0312 11:28:48.625907 1892989 finetune.py:68] layer 10_o @ epoch 1 new loss 7.615277354489081e-06 old loss 8.005347808648366e-06 BETTER
I0312 11:28:55.142692 1893105 finetune.py:45] layer 11_o initial loss 9.626061000744812e-06
I0312 11:29:03.590183 1892873 finetune.py:68] layer 9_o @ epoch 4 new loss 5.207303274801234e-06 old loss 5.299057193042245e-06 BETTER
I0312 11:29:15.927621 1892757 finetune.py:68] layer 8_up @ epoch 0 new loss 6.767933882656507e-06 old loss 7.062687018333236e-06 BETTER
I0312 11:29:18.758339 1892873 finetune.py:45] layer 9_up initial loss 8.518301910953596e-06
I0312 11:29:20.196581 1892989 finetune.py:68] layer 10_o @ epoch 2 new loss 7.369813374680234e-06 old loss 7.615277354489081e-06 BETTER
I0312 11:29:25.212575 1893105 finetune.py:68] layer 11_o @ epoch 0 new loss 8.574478670198005e-06 old loss 9.626061000744812e-06 BETTER
I0312 11:29:47.696434 1892757 finetune.py:68] layer 8_up @ epoch 1 new loss 6.633296379732201e-06 old loss 6.767933882656507e-06 BETTER
I0312 11:29:47.922014 1892873 finetune.py:68] layer 9_up @ epoch 0 new loss 8.148565029841848e-06 old loss 8.518301910953596e-06 BETTER
I0312 11:29:51.973637 1892989 finetune.py:68] layer 10_o @ epoch 3 new loss 7.190058113337727e-06 old loss 7.369813374680234e-06 BETTER
I0312 11:29:56.252691 1893105 finetune.py:68] layer 11_o @ epoch 1 new loss 8.183046702470165e-06 old loss 8.574478670198005e-06 BETTER
I0312 11:30:17.552694 1892873 finetune.py:68] layer 9_up @ epoch 1 new loss 7.981647286214866e-06 old loss 8.148565029841848e-06 BETTER
I0312 11:30:19.517935 1892757 finetune.py:68] layer 8_up @ epoch 2 new loss 6.531943654408678e-06 old loss 6.633296379732201e-06 BETTER
I0312 11:30:23.804320 1892989 finetune.py:68] layer 10_o @ epoch 4 new loss 7.052018190734088e-06 old loss 7.190058113337727e-06 BETTER
I0312 11:30:27.438905 1893105 finetune.py:68] layer 11_o @ epoch 2 new loss 7.947721314849332e-06 old loss 8.183046702470165e-06 BETTER
I0312 11:30:39.051501 1892989 finetune.py:45] layer 10_up initial loss 1.0848466445168015e-05
I0312 11:30:47.478137 1892873 finetune.py:68] layer 9_up @ epoch 2 new loss 7.856790034566075e-06 old loss 7.981647286214866e-06 BETTER
I0312 11:30:51.675225 1892757 finetune.py:68] layer 8_up @ epoch 3 new loss 6.450698492699303e-06 old loss 6.531943654408678e-06 BETTER
I0312 11:30:58.730061 1893105 finetune.py:68] layer 11_o @ epoch 3 new loss 7.77741297497414e-06 old loss 7.947721314849332e-06 BETTER
I0312 11:31:08.174808 1892989 finetune.py:68] layer 10_up @ epoch 0 new loss 1.0367868526373059e-05 old loss 1.0848466445168015e-05 BETTER
I0312 11:31:17.292217 1892873 finetune.py:68] layer 9_up @ epoch 3 new loss 7.756860213703476e-06 old loss 7.856790034566075e-06 BETTER
I0312 11:31:23.707048 1892757 finetune.py:68] layer 8_up @ epoch 4 new loss 6.382735591614619e-06 old loss 6.450698492699303e-06 BETTER
I0312 11:31:30.233407 1893105 finetune.py:68] layer 11_o @ epoch 4 new loss 7.648254722880665e-06 old loss 7.77741297497414e-06 BETTER
I0312 11:31:39.165037 1892989 finetune.py:68] layer 10_up @ epoch 1 new loss 1.014934605336748e-05 old loss 1.0367868526373059e-05 BETTER
I0312 11:31:40.251086 1892757 finetune.py:45] layer 8_gate initial loss 8.487358172715176e-06
I0312 11:31:46.511070 1893105 finetune.py:45] layer 11_up initial loss 1.1909688510058913e-05
I0312 11:31:47.316149 1892873 finetune.py:68] layer 9_up @ epoch 4 new loss 7.674403605051339e-06 old loss 7.756860213703476e-06 BETTER
I0312 11:32:04.309790 1892873 finetune.py:45] layer 9_gate initial loss 1.0206607839791104e-05
I0312 11:32:11.944654 1892757 finetune.py:68] layer 8_gate @ epoch 0 new loss 8.302788046421483e-06 old loss 8.487358172715176e-06 BETTER
I0312 11:32:12.299134 1892989 finetune.py:68] layer 10_up @ epoch 2 new loss 9.988007150241174e-06 old loss 1.014934605336748e-05 BETTER
I0312 11:32:16.720464 1893105 finetune.py:68] layer 11_up @ epoch 0 new loss 1.139355754276039e-05 old loss 1.1909688510058913e-05 BETTER
I0312 11:32:32.620589 1892873 finetune.py:68] layer 9_gate @ epoch 0 new loss 9.976654837373644e-06 old loss 1.0206607839791104e-05 BETTER
I0312 11:32:42.212788 1892757 finetune.py:68] layer 8_gate @ epoch 1 new loss 8.212516149797011e-06 old loss 8.302788046421483e-06 BETTER
I0312 11:32:42.769077 1892989 finetune.py:68] layer 10_up @ epoch 3 new loss 9.860808859230019e-06 old loss 9.988007150241174e-06 BETTER
I0312 11:32:46.413638 1893105 finetune.py:68] layer 11_up @ epoch 1 new loss 1.1162370356032625e-05 old loss 1.139355754276039e-05 BETTER
I0312 11:33:00.857077 1892873 finetune.py:68] layer 9_gate @ epoch 1 new loss 9.864319508778863e-06 old loss 9.976654837373644e-06 BETTER
I0312 11:33:12.415601 1892757 finetune.py:68] layer 8_gate @ epoch 2 new loss 8.143019840645138e-06 old loss 8.212516149797011e-06 BETTER
I0312 11:33:13.244302 1892989 finetune.py:68] layer 10_up @ epoch 4 new loss 9.753921403898858e-06 old loss 9.860808859230019e-06 BETTER
I0312 11:33:16.005916 1893105 finetune.py:68] layer 11_up @ epoch 2 new loss 1.099493329093093e-05 old loss 1.1162370356032625e-05 BETTER
I0312 11:33:28.809748 1892989 finetune.py:45] layer 10_gate initial loss 1.2804302969016135e-05
I0312 11:33:29.246918 1892873 finetune.py:68] layer 9_gate @ epoch 2 new loss 9.777775630936958e-06 old loss 9.864319508778863e-06 BETTER
I0312 11:33:42.912320 1892757 finetune.py:68] layer 8_gate @ epoch 3 new loss 8.08388813311467e-06 old loss 8.143019840645138e-06 BETTER
I0312 11:33:46.059409 1893105 finetune.py:68] layer 11_up @ epoch 3 new loss 1.0864009709621314e-05 old loss 1.099493329093093e-05 BETTER
I0312 11:33:57.085465 1892989 finetune.py:68] layer 10_gate @ epoch 0 new loss 1.248517946805805e-05 old loss 1.2804302969016135e-05 BETTER
I0312 11:33:58.067608 1892873 finetune.py:68] layer 9_gate @ epoch 3 new loss 9.705851880426053e-06 old loss 9.777775630936958e-06 BETTER
I0312 11:34:13.046610 1892757 finetune.py:68] layer 8_gate @ epoch 4 new loss 8.03387683845358e-06 old loss 8.08388813311467e-06 BETTER
I0312 11:34:15.667993 1893105 finetune.py:68] layer 11_up @ epoch 4 new loss 1.0755044058896601e-05 old loss 1.0864009709621314e-05 BETTER
I0312 11:34:25.691180 1892989 finetune.py:68] layer 10_gate @ epoch 1 new loss 1.2341031833784655e-05 old loss 1.248517946805805e-05 BETTER
I0312 11:34:26.550373 1892873 finetune.py:68] layer 9_gate @ epoch 4 new loss 9.645125828683376e-06 old loss 9.705851880426053e-06 BETTER
I0312 11:34:29.707249 1892757 finetune.py:45] layer 8_down initial loss 1.2123180567868985e-05
I0312 11:34:31.300223 1893105 finetune.py:45] layer 11_gate initial loss 1.4135947822069284e-05
I0312 11:34:42.800800 1892873 finetune.py:45] layer 9_down initial loss 1.4382215340447146e-05
I0312 11:34:54.145833 1892989 finetune.py:68] layer 10_gate @ epoch 2 new loss 1.2232088920427486e-05 old loss 1.2341031833784655e-05 BETTER
I0312 11:34:57.623593 1892757 finetune.py:68] layer 8_down @ epoch 0 new loss 1.2120666724513285e-05 old loss 1.2123180567868985e-05 BETTER
I0312 11:34:58.819519 1893105 finetune.py:68] layer 11_gate @ epoch 0 new loss 1.3808858966513071e-05 old loss 1.4135947822069284e-05 BETTER
I0312 11:35:08.815811 1892873 finetune.py:68] layer 9_down @ epoch 0 new loss 1.4379327694769017e-05 old loss 1.4382215340447146e-05 BETTER
I0312 11:35:22.518871 1892989 finetune.py:68] layer 10_gate @ epoch 3 new loss 1.2142305422457866e-05 old loss 1.2232088920427486e-05 BETTER
I0312 11:35:26.311382 1892757 finetune.py:68] layer 8_down @ epoch 1 new loss 1.2120116480218712e-05 old loss 1.2120666724513285e-05 BETTER
I0312 11:35:27.844435 1893105 finetune.py:68] layer 11_gate @ epoch 1 new loss 1.3660117474501021e-05 old loss 1.3808858966513071e-05 BETTER
I0312 11:35:36.073143 1892873 finetune.py:68] layer 9_down @ epoch 1 new loss 1.4378591913555283e-05 old loss 1.4379327694769017e-05 BETTER
I0312 11:35:50.739022 1892989 finetune.py:68] layer 10_gate @ epoch 4 new loss 1.2066064300597645e-05 old loss 1.2142305422457866e-05 BETTER
I0312 11:35:54.913301 1892757 finetune.py:68] layer 8_down @ epoch 2 new loss 1.2119693565182388e-05 old loss 1.2120116480218712e-05 BETTER
I0312 11:35:56.149868 1893105 finetune.py:68] layer 11_gate @ epoch 2 new loss 1.3547246453526895e-05 old loss 1.3660117474501021e-05 BETTER
I0312 11:36:03.400068 1892873 finetune.py:68] layer 9_down @ epoch 2 new loss 1.4378300875250716e-05 old loss 1.4378591913555283e-05 BETTER
I0312 11:36:07.760651 1892989 finetune.py:45] layer 10_down initial loss 1.760485793056432e-05
I0312 11:36:23.553796 1892757 finetune.py:68] layer 8_down @ epoch 3 new loss 1.2119460734538734e-05 old loss 1.2119693565182388e-05 BETTER
I0312 11:36:24.686546 1893105 finetune.py:68] layer 11_gate @ epoch 3 new loss 1.3455104635795578e-05 old loss 1.3547246453526895e-05 BETTER
I0312 11:36:30.286847 1892873 finetune.py:68] layer 9_down @ epoch 3 new loss 1.4378037121787202e-05 old loss 1.4378300875250716e-05 BETTER
I0312 11:36:33.779530 1892989 finetune.py:68] layer 10_down @ epoch 0 new loss 1.7596456018509343e-05 old loss 1.760485793056432e-05 BETTER
I0312 11:36:52.184478 1892757 finetune.py:68] layer 8_down @ epoch 4 new loss 1.2119316124881152e-05 old loss 1.2119460734538734e-05 BETTER
I0312 11:36:53.137900 1893105 finetune.py:68] layer 11_gate @ epoch 4 new loss 1.337770572718e-05 old loss 1.3455104635795578e-05 BETTER
8_v proxy err 0.000653158756904304 tr(WHW.T) 530.9967041015625
8_q proxy err 0.00011701548646669835 tr(WHW.T) 7228.50830078125
8_k proxy err 9.815674275159836e-05 tr(WHW.T) 10640.03125
8_o proxy err 0.0007360855815932155 tr(WHW.T) 20.109663009643555
8_up proxy err 0.00037991104181855917 tr(WHW.T) 866.2200927734375
8_gate proxy err 0.00021010453929193318 tr(WHW.T) 1968.8665771484375
8_down proxy err 0.0004649557522498071 tr(WHW.T) 37.17536926269531
I0312 11:36:57.140724 1892873 finetune.py:68] layer 9_down @ epoch 4 new loss 1.437792343494948e-05 old loss 1.4378037121787202e-05 BETTER
9_v proxy err 0.0006226433324627578 tr(WHW.T) 565.0663452148438
9_q proxy err 0.0001114932238124311 tr(WHW.T) 6970.42822265625
9_k proxy err 9.030716319102794e-05 tr(WHW.T) 10989.62890625
9_o proxy err 0.0007276407559402287 tr(WHW.T) 25.607845306396484
9_up proxy err 0.0003695768828038126 tr(WHW.T) 970.8685302734375
9_gate proxy err 0.00020812808361370116 tr(WHW.T) 2132.209228515625
9_down proxy err 0.00046703135012649 tr(WHW.T) 42.99708938598633
I0312 11:37:00.991650 1892989 finetune.py:68] layer 10_down @ epoch 1 new loss 1.7594866221770644e-05 old loss 1.7596456018509343e-05 BETTER
I0312 11:37:09.425002 1893105 finetune.py:45] layer 11_down initial loss 1.946886368386913e-05
I0312 11:37:28.561570 1892989 finetune.py:68] layer 10_down @ epoch 2 new loss 1.7594167729839683e-05 old loss 1.7594866221770644e-05 BETTER
I0312 11:37:35.038940 1893105 finetune.py:68] layer 11_down @ epoch 0 new loss 1.946440534084104e-05 old loss 1.946886368386913e-05 BETTER
I0312 11:37:55.412970 1892989 finetune.py:68] layer 10_down @ epoch 3 new loss 1.759370934450999e-05 old loss 1.7594167729839683e-05 BETTER
I0312 11:38:01.258099 1893105 finetune.py:68] layer 11_down @ epoch 1 new loss 1.9463144781184383e-05 old loss 1.946440534084104e-05 BETTER
I0312 11:38:17.035397 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 12 in 74.92688703536987s
I0312 11:38:20.381510 1893221 config.py:54] PyTorch version 2.1.1 available.
I0312 11:38:21.448915 1891709 quantize_finetune_llama.py:183] layer 13 gpu 1
I0312 11:38:21.545163 1893221 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 11:38:22.304244 1892989 finetune.py:68] layer 10_down @ epoch 4 new loss 1.7593645679880865e-05 old loss 1.759370934450999e-05 BETTER
10_v proxy err 0.0006300024106167257 tr(WHW.T) 578.807373046875
10_q proxy err 0.00011457197979325429 tr(WHW.T) 6916.16162109375
10_k proxy err 9.51379188336432e-05 tr(WHW.T) 10999.369140625
10_o proxy err 0.0007518591592088342 tr(WHW.T) 35.178680419921875
10_up proxy err 0.00035147121525369585 tr(WHW.T) 1080.051513671875
10_gate proxy err 0.00020561103883665055 tr(WHW.T) 2259.551513671875
10_down proxy err 0.00044901331420987844 tr(WHW.T) 52.35393142700195
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 11:38:27.509317 1893105 finetune.py:68] layer 11_down @ epoch 2 new loss 1.9462509953882545e-05 old loss 1.9463144781184383e-05 BETTER
I0312 11:38:30.000673 1893221 finetune.py:45] layer 12_v initial loss 8.141158104990609e-06
I0312 11:38:54.058373 1893105 finetune.py:68] layer 11_down @ epoch 3 new loss 1.9462187992758118e-05 old loss 1.9462509953882545e-05 BETTER
I0312 11:39:03.122154 1893221 finetune.py:68] layer 12_v @ epoch 0 new loss 3.549849225237267e-06 old loss 8.141158104990609e-06 BETTER
I0312 11:39:20.314785 1893105 finetune.py:68] layer 11_down @ epoch 4 new loss 1.946193333424162e-05 old loss 1.9462187992758118e-05 BETTER
11_v proxy err 0.0006050899974070489 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0001280217693420127 tr(WHW.T) 7027.32958984375
11_k proxy err 0.00010290792852174491 tr(WHW.T) 10511.68359375
11_o proxy err 0.0007638586685061455 tr(WHW.T) 36.66096496582031
11_up proxy err 0.0003572236164472997 tr(WHW.T) 1139.7913818359375
11_gate proxy err 0.0002051679912256077 tr(WHW.T) 2392.490966796875
11_down proxy err 0.00045871257316321135 tr(WHW.T) 56.13525390625
I0312 11:39:36.581499 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 13 in 70.15722966194153s
I0312 11:39:37.458866 1893221 finetune.py:68] layer 12_v @ epoch 1 new loss 3.1093277357285842e-06 old loss 3.549849225237267e-06 BETTER
I0312 11:39:39.610447 1893337 config.py:54] PyTorch version 2.1.1 available.
I0312 11:39:40.573437 1891709 quantize_finetune_llama.py:183] layer 14 gpu 2
I0312 11:39:40.641188 1893337 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 11:39:48.616794 1893337 finetune.py:45] layer 13_v initial loss 8.335865459230263e-06
I0312 11:40:11.925799 1893221 finetune.py:68] layer 12_v @ epoch 2 new loss 2.919767439379939e-06 old loss 3.1093277357285842e-06 BETTER
I0312 11:40:19.767907 1893337 finetune.py:68] layer 13_v @ epoch 0 new loss 3.719720325534581e-06 old loss 8.335865459230263e-06 BETTER
I0312 11:40:46.666908 1893221 finetune.py:68] layer 12_v @ epoch 3 new loss 2.8346348699415103e-06 old loss 2.919767439379939e-06 BETTER
I0312 11:40:50.193403 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 14 in 69.22397565841675s
I0312 11:40:51.986072 1893337 finetune.py:68] layer 13_v @ epoch 1 new loss 3.349101234562113e-06 old loss 3.719720325534581e-06 BETTER
I0312 11:40:53.391568 1893453 config.py:54] PyTorch version 2.1.1 available.
I0312 11:40:54.403880 1891709 quantize_finetune_llama.py:183] layer 15 gpu 3
I0312 11:40:54.472330 1893453 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 11:41:02.525096 1893453 finetune.py:45] layer 14_v initial loss 1.0384117558714934e-05
I0312 11:41:21.508284 1893221 finetune.py:76] layer 12_v @ epoch 4 new loss 2.836697376551456e-06 old loss 2.8346348699415103e-06 WORSE
I0312 11:41:24.416531 1893337 finetune.py:68] layer 13_v @ epoch 2 new loss 3.2737918900238583e-06 old loss 3.349101234562113e-06 BETTER
I0312 11:41:30.485765 1893221 finetune.py:45] layer 12_q initial loss 4.557623014989076e-06
I0312 11:41:33.958530 1893453 finetune.py:68] layer 14_v @ epoch 0 new loss 4.754716428578831e-06 old loss 1.0384117558714934e-05 BETTER
I0312 11:41:57.027235 1893337 finetune.py:68] layer 13_v @ epoch 3 new loss 3.1139252314460464e-06 old loss 3.2737918900238583e-06 BETTER
I0312 11:42:04.171165 1893221 finetune.py:68] layer 12_q @ epoch 0 new loss 3.618532446125755e-06 old loss 4.557623014989076e-06 BETTER
I0312 11:42:05.013777 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 15 in 70.28517246246338s
I0312 11:42:06.291688 1893453 finetune.py:68] layer 14_v @ epoch 1 new loss 4.1706121010065544e-06 old loss 4.754716428578831e-06 BETTER
I0312 11:42:08.397235 1893569 config.py:54] PyTorch version 2.1.1 available.
I0312 11:42:09.444882 1891709 quantize_finetune_llama.py:183] layer 16 gpu 0
I0312 11:42:09.521236 1893569 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 11:42:17.684426 1893569 finetune.py:45] layer 15_v initial loss 1.0539924915065058e-05
I0312 11:42:29.980364 1893337 finetune.py:68] layer 13_v @ epoch 4 new loss 3.1058011700224597e-06 old loss 3.1139252314460464e-06 BETTER
I0312 11:42:38.784989 1893221 finetune.py:68] layer 12_q @ epoch 1 new loss 3.3701248867146205e-06 old loss 3.618532446125755e-06 BETTER
I0312 11:42:39.042734 1893453 finetune.py:68] layer 14_v @ epoch 2 new loss 3.896179805451538e-06 old loss 4.1706121010065544e-06 BETTER
I0312 11:42:39.433008 1893337 finetune.py:45] layer 13_q initial loss 4.644723958335817e-06
I0312 11:42:48.632714 1893569 finetune.py:68] layer 15_v @ epoch 0 new loss 4.735764377983287e-06 old loss 1.0539924915065058e-05 BETTER
I0312 11:43:11.262831 1893337 finetune.py:68] layer 13_q @ epoch 0 new loss 3.6180633742333157e-06 old loss 4.644723958335817e-06 BETTER
I0312 11:43:12.556545 1893453 finetune.py:68] layer 14_v @ epoch 3 new loss 3.773106072912924e-06 old loss 3.896179805451538e-06 BETTER
I0312 11:43:13.576971 1893221 finetune.py:68] layer 12_q @ epoch 2 new loss 3.2373000067309476e-06 old loss 3.3701248867146205e-06 BETTER
I0312 11:43:20.592093 1893569 finetune.py:68] layer 15_v @ epoch 1 new loss 4.158637693763012e-06 old loss 4.735764377983287e-06 BETTER
I0312 11:43:43.595371 1893337 finetune.py:68] layer 13_q @ epoch 1 new loss 3.4129666346416343e-06 old loss 3.6180633742333157e-06 BETTER
I0312 11:43:45.725715 1893453 finetune.py:68] layer 14_v @ epoch 4 new loss 3.699402668644325e-06 old loss 3.773106072912924e-06 BETTER
I0312 11:43:48.275911 1893221 finetune.py:68] layer 12_q @ epoch 3 new loss 3.1256902275345055e-06 old loss 3.2373000067309476e-06 BETTER
I0312 11:43:52.707670 1893569 finetune.py:68] layer 15_v @ epoch 2 new loss 3.86931014872971e-06 old loss 4.158637693763012e-06 BETTER
I0312 11:43:55.308947 1893453 finetune.py:45] layer 14_q initial loss 5.7147781262756325e-06
I0312 11:44:16.391185 1893337 finetune.py:68] layer 13_q @ epoch 2 new loss 3.346958465044736e-06 old loss 3.4129666346416343e-06 BETTER
I0312 11:44:23.325533 1893221 finetune.py:68] layer 12_q @ epoch 4 new loss 3.048866119570448e-06 old loss 3.1256902275345055e-06 BETTER
I0312 11:44:25.432887 1893569 finetune.py:68] layer 15_v @ epoch 3 new loss 3.7324712138797622e-06 old loss 3.86931014872971e-06 BETTER
I0312 11:44:27.172594 1893453 finetune.py:68] layer 14_q @ epoch 0 new loss 4.593149242282379e-06 old loss 5.7147781262756325e-06 BETTER
I0312 11:44:33.629789 1893221 finetune.py:45] layer 12_k initial loss 4.799588168680202e-06
I0312 11:44:48.786673 1893337 finetune.py:76] layer 13_q @ epoch 3 new loss 3.3477786018920597e-06 old loss 3.346958465044736e-06 WORSE
I0312 11:44:58.349998 1893569 finetune.py:68] layer 15_v @ epoch 4 new loss 3.7099532619322417e-06 old loss 3.7324712138797622e-06 BETTER
I0312 11:45:00.208981 1893453 finetune.py:68] layer 14_q @ epoch 1 new loss 4.32061324318056e-06 old loss 4.593149242282379e-06 BETTER
I0312 11:45:06.830414 1893221 finetune.py:68] layer 12_k @ epoch 0 new loss 4.222974439471727e-06 old loss 4.799588168680202e-06 BETTER
I0312 11:45:08.658407 1893569 finetune.py:45] layer 15_q initial loss 5.904410954826744e-06
I0312 11:45:20.786376 1893337 finetune.py:68] layer 13_q @ epoch 4 new loss 3.2862678835954284e-06 old loss 3.346958465044736e-06 BETTER
I0312 11:45:30.108217 1893337 finetune.py:45] layer 13_k initial loss 4.9515688260726165e-06
I0312 11:45:32.739226 1893453 finetune.py:68] layer 14_q @ epoch 2 new loss 4.154143425694201e-06 old loss 4.32061324318056e-06 BETTER
I0312 11:45:40.734763 1893569 finetune.py:68] layer 15_q @ epoch 0 new loss 4.479728886508383e-06 old loss 5.904410954826744e-06 BETTER
I0312 11:45:40.919000 1893221 finetune.py:68] layer 12_k @ epoch 1 new loss 4.145219463680405e-06 old loss 4.222974439471727e-06 BETTER
I0312 11:46:02.017576 1893337 finetune.py:68] layer 13_k @ epoch 0 new loss 4.356784756964771e-06 old loss 4.9515688260726165e-06 BETTER
I0312 11:46:05.636391 1893453 finetune.py:68] layer 14_q @ epoch 3 new loss 4.035302481497638e-06 old loss 4.154143425694201e-06 BETTER
I0312 11:46:13.129045 1893569 finetune.py:68] layer 15_q @ epoch 1 new loss 4.24215159000596e-06 old loss 4.479728886508383e-06 BETTER
I0312 11:46:14.932042 1893221 finetune.py:68] layer 12_k @ epoch 2 new loss 4.095286840311019e-06 old loss 4.145219463680405e-06 BETTER
I0312 11:46:34.282914 1893337 finetune.py:68] layer 13_k @ epoch 1 new loss 4.219290985929547e-06 old loss 4.356784756964771e-06 BETTER
I0312 11:46:38.940869 1893453 finetune.py:68] layer 14_q @ epoch 4 new loss 3.938865120289847e-06 old loss 4.035302481497638e-06 BETTER
I0312 11:46:46.046251 1893569 finetune.py:68] layer 15_q @ epoch 2 new loss 4.0730506043473724e-06 old loss 4.24215159000596e-06 BETTER
I0312 11:46:49.256979 1893221 finetune.py:68] layer 12_k @ epoch 3 new loss 4.021641416329658e-06 old loss 4.095286840311019e-06 BETTER
I0312 11:46:49.928835 1893453 finetune.py:45] layer 14_k initial loss 6.110785307100741e-06
I0312 11:47:06.396217 1893337 finetune.py:68] layer 13_k @ epoch 2 new loss 4.1551475078449585e-06 old loss 4.219290985929547e-06 BETTER
I0312 11:47:18.457355 1893569 finetune.py:68] layer 15_q @ epoch 3 new loss 3.98631300413399e-06 old loss 4.0730506043473724e-06 BETTER
I0312 11:47:21.730846 1893453 finetune.py:68] layer 14_k @ epoch 0 new loss 5.487118869496044e-06 old loss 6.110785307100741e-06 BETTER
I0312 11:47:23.502519 1893221 finetune.py:68] layer 12_k @ epoch 4 new loss 4.0005179471336305e-06 old loss 4.021641416329658e-06 BETTER
I0312 11:47:33.832226 1893221 finetune.py:45] layer 12_o initial loss 1.0285621101502329e-05
I0312 11:47:38.939130 1893337 finetune.py:68] layer 13_k @ epoch 3 new loss 4.105840162083041e-06 old loss 4.1551475078449585e-06 BETTER
I0312 11:47:50.798186 1893569 finetune.py:68] layer 15_q @ epoch 4 new loss 3.884835678036325e-06 old loss 3.98631300413399e-06 BETTER
I0312 11:47:54.171755 1893453 finetune.py:68] layer 14_k @ epoch 1 new loss 5.367278390622232e-06 old loss 5.487118869496044e-06 BETTER
I0312 11:48:01.322175 1893569 finetune.py:45] layer 15_k initial loss 5.949257229076466e-06
I0312 11:48:07.173215 1893221 finetune.py:68] layer 12_o @ epoch 0 new loss 9.175689228868578e-06 old loss 1.0285621101502329e-05 BETTER
I0312 11:48:11.371945 1893337 finetune.py:68] layer 13_k @ epoch 4 new loss 4.055723820783896e-06 old loss 4.105840162083041e-06 BETTER
I0312 11:48:21.296236 1893337 finetune.py:45] layer 13_o initial loss 1.0650415788404644e-05
I0312 11:48:26.357777 1893453 finetune.py:68] layer 14_k @ epoch 2 new loss 5.286820396577241e-06 old loss 5.367278390622232e-06 BETTER
I0312 11:48:32.396446 1893569 finetune.py:68] layer 15_k @ epoch 0 new loss 5.3615372053172905e-06 old loss 5.949257229076466e-06 BETTER
I0312 11:48:40.957059 1893221 finetune.py:68] layer 12_o @ epoch 1 new loss 8.764593985688407e-06 old loss 9.175689228868578e-06 BETTER
I0312 11:48:52.203618 1893337 finetune.py:68] layer 13_o @ epoch 0 new loss 9.404542652191594e-06 old loss 1.0650415788404644e-05 BETTER
I0312 11:48:59.623089 1893453 finetune.py:76] layer 14_k @ epoch 3 new loss 5.288849479256896e-06 old loss 5.286820396577241e-06 WORSE
I0312 11:49:04.194376 1893569 finetune.py:68] layer 15_k @ epoch 1 new loss 5.256674739939626e-06 old loss 5.3615372053172905e-06 BETTER
I0312 11:49:14.650817 1893221 finetune.py:68] layer 12_o @ epoch 2 new loss 8.513034117640927e-06 old loss 8.764593985688407e-06 BETTER
I0312 11:49:23.909933 1893337 finetune.py:68] layer 13_o @ epoch 1 new loss 8.961155799624976e-06 old loss 9.404542652191594e-06 BETTER
I0312 11:49:31.202468 1893453 finetune.py:68] layer 14_k @ epoch 4 new loss 5.199777206144063e-06 old loss 5.286820396577241e-06 BETTER
I0312 11:49:36.288597 1893569 finetune.py:68] layer 15_k @ epoch 2 new loss 5.153587153472472e-06 old loss 5.256674739939626e-06 BETTER
I0312 11:49:41.155703 1893453 finetune.py:45] layer 14_o initial loss 1.337363210041076e-05
I0312 11:49:48.220536 1893221 finetune.py:68] layer 12_o @ epoch 3 new loss 8.33028752822429e-06 old loss 8.513034117640927e-06 BETTER
I0312 11:49:55.319439 1893337 finetune.py:68] layer 13_o @ epoch 2 new loss 8.692723895364907e-06 old loss 8.961155799624976e-06 BETTER
I0312 11:50:08.403419 1893569 finetune.py:68] layer 15_k @ epoch 3 new loss 5.099909230921185e-06 old loss 5.153587153472472e-06 BETTER
I0312 11:50:12.085358 1893453 finetune.py:68] layer 14_o @ epoch 0 new loss 1.1999456546618603e-05 old loss 1.337363210041076e-05 BETTER
I0312 11:50:21.946769 1893221 finetune.py:68] layer 12_o @ epoch 4 new loss 8.192705536202993e-06 old loss 8.33028752822429e-06 BETTER
I0312 11:50:26.818136 1893337 finetune.py:68] layer 13_o @ epoch 3 new loss 8.507136953994632e-06 old loss 8.692723895364907e-06 BETTER
I0312 11:50:37.914075 1893221 finetune.py:45] layer 12_up initial loss 1.2963609151483979e-05
I0312 11:50:41.000977 1893569 finetune.py:76] layer 15_k @ epoch 4 new loss 5.102952854940668e-06 old loss 5.099909230921185e-06 WORSE
I0312 11:50:44.157406 1893453 finetune.py:68] layer 14_o @ epoch 1 new loss 1.147508191934321e-05 old loss 1.1999456546618603e-05 BETTER
I0312 11:50:51.213155 1893569 finetune.py:45] layer 15_o initial loss 1.3499980923370458e-05
I0312 11:50:58.379035 1893337 finetune.py:68] layer 13_o @ epoch 4 new loss 8.366178008145653e-06 old loss 8.507136953994632e-06 BETTER
I0312 11:51:08.534679 1893221 finetune.py:68] layer 12_up @ epoch 0 new loss 1.2395406884024851e-05 old loss 1.2963609151483979e-05 BETTER
I0312 11:51:14.108896 1893337 finetune.py:45] layer 13_up initial loss 1.4114679288468324e-05
I0312 11:51:15.926162 1893453 finetune.py:68] layer 14_o @ epoch 2 new loss 1.1157184417243116e-05 old loss 1.147508191934321e-05 BETTER
I0312 11:51:21.586520 1893569 finetune.py:68] layer 15_o @ epoch 0 new loss 1.1823412933154032e-05 old loss 1.3499980923370458e-05 BETTER
I0312 11:51:40.170731 1893221 finetune.py:68] layer 12_up @ epoch 1 new loss 1.2139395039412193e-05 old loss 1.2395406884024851e-05 BETTER
I0312 11:51:43.027123 1893337 finetune.py:68] layer 13_up @ epoch 0 new loss 1.3343773389351554e-05 old loss 1.4114679288468324e-05 BETTER
I0312 11:51:47.797612 1893453 finetune.py:68] layer 14_o @ epoch 3 new loss 1.092931142920861e-05 old loss 1.1157184417243116e-05 BETTER
I0312 11:51:52.658952 1893569 finetune.py:68] layer 15_o @ epoch 1 new loss 1.1286398148513399e-05 old loss 1.1823412933154032e-05 BETTER
I0312 11:52:11.959491 1893221 finetune.py:68] layer 12_up @ epoch 2 new loss 1.1954334695474245e-05 old loss 1.2139395039412193e-05 BETTER
I0312 11:52:12.896618 1893337 finetune.py:68] layer 13_up @ epoch 1 new loss 1.3027079148741905e-05 old loss 1.3343773389351554e-05 BETTER
I0312 11:52:19.679383 1893453 finetune.py:68] layer 14_o @ epoch 4 new loss 1.0756723895610776e-05 old loss 1.092931142920861e-05 BETTER
I0312 11:52:23.912007 1893569 finetune.py:68] layer 15_o @ epoch 2 new loss 1.096093728847336e-05 old loss 1.1286398148513399e-05 BETTER
I0312 11:52:35.100720 1893453 finetune.py:45] layer 14_up initial loss 1.708403397060465e-05
I0312 11:52:42.848599 1893337 finetune.py:68] layer 13_up @ epoch 2 new loss 1.2806372069462668e-05 old loss 1.3027079148741905e-05 BETTER
I0312 11:52:43.907094 1893221 finetune.py:68] layer 12_up @ epoch 3 new loss 1.1809836905740667e-05 old loss 1.1954334695474245e-05 BETTER
I0312 11:52:55.103878 1893569 finetune.py:68] layer 15_o @ epoch 3 new loss 1.0738160199252889e-05 old loss 1.096093728847336e-05 BETTER
I0312 11:53:04.153695 1893453 finetune.py:68] layer 14_up @ epoch 0 new loss 1.6295785826514475e-05 old loss 1.708403397060465e-05 BETTER
I0312 11:53:12.799782 1893337 finetune.py:68] layer 13_up @ epoch 3 new loss 1.2639219676202629e-05 old loss 1.2806372069462668e-05 BETTER
I0312 11:53:15.896096 1893221 finetune.py:68] layer 12_up @ epoch 4 new loss 1.1692580301314592e-05 old loss 1.1809836905740667e-05 BETTER
I0312 11:53:27.132005 1893569 finetune.py:68] layer 15_o @ epoch 4 new loss 1.057070585375186e-05 old loss 1.0738160199252889e-05 BETTER
I0312 11:53:33.628927 1893221 finetune.py:45] layer 12_gate initial loss 1.5563473425572738e-05
I0312 11:53:34.930761 1893453 finetune.py:68] layer 14_up @ epoch 1 new loss 1.5956955394358374e-05 old loss 1.6295785826514475e-05 BETTER
I0312 11:53:43.304420 1893337 finetune.py:68] layer 13_up @ epoch 4 new loss 1.2505686754593626e-05 old loss 1.2639219676202629e-05 BETTER
I0312 11:53:45.927639 1893569 finetune.py:45] layer 15_up initial loss 1.8321497918805107e-05
I0312 11:53:59.108422 1893337 finetune.py:45] layer 13_gate initial loss 1.7205768017447554e-05
I0312 11:54:02.520522 1893221 finetune.py:68] layer 12_gate @ epoch 0 new loss 1.5201528185571078e-05 old loss 1.5563473425572738e-05 BETTER
I0312 11:54:05.055962 1893453 finetune.py:68] layer 14_up @ epoch 2 new loss 1.5720655937911943e-05 old loss 1.5956955394358374e-05 BETTER
I0312 11:54:14.376982 1893569 finetune.py:68] layer 15_up @ epoch 0 new loss 1.726162008708343e-05 old loss 1.8321497918805107e-05 BETTER
I0312 11:54:26.594756 1893337 finetune.py:68] layer 13_gate @ epoch 0 new loss 1.672618236625567e-05 old loss 1.7205768017447554e-05 BETTER
I0312 11:54:32.405872 1893221 finetune.py:68] layer 12_gate @ epoch 1 new loss 1.5033643649076112e-05 old loss 1.5201528185571078e-05 BETTER
I0312 11:54:34.943789 1893453 finetune.py:68] layer 14_up @ epoch 3 new loss 1.553976835566573e-05 old loss 1.5720655937911943e-05 BETTER
I0312 11:54:43.649550 1893569 finetune.py:68] layer 15_up @ epoch 1 new loss 1.6853606211952865e-05 old loss 1.726162008708343e-05 BETTER
I0312 11:54:54.807311 1893337 finetune.py:68] layer 13_gate @ epoch 1 new loss 1.651875390962232e-05 old loss 1.672618236625567e-05 BETTER
I0312 11:55:02.305567 1893221 finetune.py:68] layer 12_gate @ epoch 2 new loss 1.4908882803865708e-05 old loss 1.5033643649076112e-05 BETTER
I0312 11:55:04.894394 1893453 finetune.py:68] layer 14_up @ epoch 4 new loss 1.5394554793601856e-05 old loss 1.553976835566573e-05 BETTER
I0312 11:55:13.271745 1893569 finetune.py:68] layer 15_up @ epoch 2 new loss 1.6580101146246307e-05 old loss 1.6853606211952865e-05 BETTER
I0312 11:55:20.209363 1893453 finetune.py:45] layer 14_gate initial loss 2.0764766304637305e-05
I0312 11:55:22.939470 1893337 finetune.py:68] layer 13_gate @ epoch 2 new loss 1.636868910281919e-05 old loss 1.651875390962232e-05 BETTER
I0312 11:55:32.140219 1893221 finetune.py:68] layer 12_gate @ epoch 3 new loss 1.4807697880314663e-05 old loss 1.4908882803865708e-05 BETTER
I0312 11:55:42.719412 1893569 finetune.py:68] layer 15_up @ epoch 3 new loss 1.6372252503060736e-05 old loss 1.6580101146246307e-05 BETTER
I0312 11:55:47.840339 1893453 finetune.py:68] layer 14_gate @ epoch 0 new loss 2.02602896024473e-05 old loss 2.0764766304637305e-05 BETTER
I0312 11:55:51.285801 1893337 finetune.py:68] layer 13_gate @ epoch 3 new loss 1.6247626263066195e-05 old loss 1.636868910281919e-05 BETTER
I0312 11:56:02.001220 1893221 finetune.py:68] layer 12_gate @ epoch 4 new loss 1.4722574633196928e-05 old loss 1.4807697880314663e-05 BETTER
I0312 11:56:12.336506 1893569 finetune.py:68] layer 15_up @ epoch 4 new loss 1.6213205526582897e-05 old loss 1.6372252503060736e-05 BETTER
I0312 11:56:16.206526 1893453 finetune.py:68] layer 14_gate @ epoch 1 new loss 2.0036031855852343e-05 old loss 2.02602896024473e-05 BETTER
I0312 11:56:18.226396 1893221 finetune.py:45] layer 12_down initial loss 2.1704399841837585e-05
I0312 11:56:19.760973 1893337 finetune.py:68] layer 13_gate @ epoch 4 new loss 1.6149075236171484e-05 old loss 1.6247626263066195e-05 BETTER
I0312 11:56:28.343155 1893569 finetune.py:45] layer 15_gate initial loss 2.277581370435655e-05
I0312 11:56:35.730103 1893337 finetune.py:45] layer 13_down initial loss 2.4809929527691565e-05
I0312 11:56:44.686486 1893453 finetune.py:68] layer 14_gate @ epoch 2 new loss 1.9873559722327627e-05 old loss 2.0036031855852343e-05 BETTER
I0312 11:56:45.562044 1893221 finetune.py:68] layer 12_down @ epoch 0 new loss 2.1699899662053213e-05 old loss 2.1704399841837585e-05 BETTER
I0312 11:56:55.278791 1893569 finetune.py:68] layer 15_gate @ epoch 0 new loss 2.2132639060146175e-05 old loss 2.277581370435655e-05 BETTER
I0312 11:57:01.544984 1893337 finetune.py:68] layer 13_down @ epoch 0 new loss 2.4801996914902702e-05 old loss 2.4809929527691565e-05 BETTER
I0312 11:57:12.950879 1893453 finetune.py:68] layer 14_gate @ epoch 3 new loss 1.9744786186493002e-05 old loss 1.9873559722327627e-05 BETTER
I0312 11:57:14.025658 1893221 finetune.py:68] layer 12_down @ epoch 1 new loss 2.1698811906389892e-05 old loss 2.1699899662053213e-05 BETTER
I0312 11:57:23.200597 1893569 finetune.py:68] layer 15_gate @ epoch 1 new loss 2.186212623200845e-05 old loss 2.2132639060146175e-05 BETTER
I0312 11:57:28.201707 1893337 finetune.py:68] layer 13_down @ epoch 1 new loss 2.4799479433568195e-05 old loss 2.4801996914902702e-05 BETTER
I0312 11:57:41.194478 1893453 finetune.py:68] layer 14_gate @ epoch 4 new loss 1.963712929864414e-05 old loss 1.9744786186493002e-05 BETTER
I0312 11:57:42.487947 1893221 finetune.py:68] layer 12_down @ epoch 2 new loss 2.1698224372812547e-05 old loss 2.1698811906389892e-05 BETTER
I0312 11:57:51.231785 1893569 finetune.py:68] layer 15_gate @ epoch 2 new loss 2.1671214199159294e-05 old loss 2.186212623200845e-05 BETTER
I0312 11:57:55.010647 1893337 finetune.py:68] layer 13_down @ epoch 2 new loss 2.4798340746201575e-05 old loss 2.4799479433568195e-05 BETTER
I0312 11:57:56.920224 1893453 finetune.py:45] layer 14_down initial loss 2.9687609639950097e-05
I0312 11:58:10.890143 1893221 finetune.py:68] layer 12_down @ epoch 3 new loss 2.1697982447221875e-05 old loss 2.1698224372812547e-05 BETTER
I0312 11:58:19.625841 1893569 finetune.py:68] layer 15_gate @ epoch 3 new loss 2.1521676899283193e-05 old loss 2.1671214199159294e-05 BETTER
I0312 11:58:21.900669 1893337 finetune.py:68] layer 13_down @ epoch 3 new loss 2.479762588336598e-05 old loss 2.4798340746201575e-05 BETTER
I0312 11:58:23.193891 1893453 finetune.py:68] layer 14_down @ epoch 0 new loss 2.968153239635285e-05 old loss 2.9687609639950097e-05 BETTER
I0312 11:58:39.386496 1893221 finetune.py:68] layer 12_down @ epoch 4 new loss 2.169765684811864e-05 old loss 2.1697982447221875e-05 BETTER
12_v proxy err 0.000623742351308465 tr(WHW.T) 703.318603515625
12_q proxy err 0.0001265277824131772 tr(WHW.T) 7045.70654296875
12_k proxy err 0.0001080814836313948 tr(WHW.T) 10895.6435546875
12_o proxy err 0.0007684699958190322 tr(WHW.T) 39.291534423828125
12_up proxy err 0.00035416308674030006 tr(WHW.T) 1228.3287353515625
12_gate proxy err 0.0002151171356672421 tr(WHW.T) 2380.788330078125
12_down proxy err 0.00045829860027879477 tr(WHW.T) 64.1785659790039
I0312 11:58:48.793283 1893569 finetune.py:68] layer 15_gate @ epoch 4 new loss 2.1400084733613767e-05 old loss 2.1521676899283193e-05 BETTER
I0312 11:58:49.474268 1893337 finetune.py:68] layer 13_down @ epoch 4 new loss 2.4797085643513128e-05 old loss 2.479762588336598e-05 BETTER
I0312 11:58:50.326443 1893453 finetune.py:68] layer 14_down @ epoch 1 new loss 2.967986074509099e-05 old loss 2.968153239635285e-05 BETTER
13_v proxy err 0.0006079758168198168 tr(WHW.T) 714.5677490234375
13_q proxy err 0.00012329051969572902 tr(WHW.T) 6956.2490234375
13_k proxy err 9.869789209915325e-05 tr(WHW.T) 10428.6669921875
13_o proxy err 0.0006892718956805766 tr(WHW.T) 45.88602828979492
13_up proxy err 0.00034345328458584845 tr(WHW.T) 1367.4957275390625
13_gate proxy err 0.0002117582189384848 tr(WHW.T) 2600.32763671875
13_down proxy err 0.00045974753447808325 tr(WHW.T) 79.37787628173828
I0312 11:59:04.187783 1893569 finetune.py:45] layer 15_down initial loss 3.409372948226519e-05
I0312 11:59:17.178912 1893453 finetune.py:68] layer 14_down @ epoch 2 new loss 2.9679225917789154e-05 old loss 2.967986074509099e-05 BETTER
I0312 11:59:29.378578 1893569 finetune.py:68] layer 15_down @ epoch 0 new loss 3.408602060517296e-05 old loss 3.409372948226519e-05 BETTER
I0312 11:59:44.070667 1893453 finetune.py:68] layer 14_down @ epoch 3 new loss 2.9678529244847596e-05 old loss 2.9679225917789154e-05 BETTER
I0312 11:59:55.769719 1893569 finetune.py:68] layer 15_down @ epoch 1 new loss 3.408376869629137e-05 old loss 3.408602060517296e-05 BETTER
I0312 12:00:07.384333 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 16 in 73.7306797504425s
I0312 12:00:10.894591 1893685 config.py:54] PyTorch version 2.1.1 available.
I0312 12:00:11.421312 1893453 finetune.py:68] layer 14_down @ epoch 4 new loss 2.9678298233193345e-05 old loss 2.9678529244847596e-05 BETTER
I0312 12:00:11.956963 1891709 quantize_finetune_llama.py:183] layer 17 gpu 1
I0312 12:00:12.046005 1893685 data_utils.py:336] using 256 training seqs, 128 validation seqs
14_v proxy err 0.000644054205622524 tr(WHW.T) 706.1612548828125
14_q proxy err 0.00012678696657530963 tr(WHW.T) 7077.24609375
14_k proxy err 0.00010050620039692149 tr(WHW.T) 11298.0634765625
14_o proxy err 0.0007535493350587785 tr(WHW.T) 50.93901443481445
14_up proxy err 0.00034662510734051466 tr(WHW.T) 1464.853515625
14_gate proxy err 0.00021717202616855502 tr(WHW.T) 2682.69970703125
14_down proxy err 0.00046639543143101037 tr(WHW.T) 90.31037139892578
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:00:20.710985 1893685 finetune.py:45] layer 16_v initial loss 1.23866602734779e-05
I0312 12:00:21.908371 1893569 finetune.py:68] layer 15_down @ epoch 2 new loss 3.408272095839493e-05 old loss 3.408376869629137e-05 BETTER
I0312 12:00:48.157383 1893569 finetune.py:68] layer 15_down @ epoch 3 new loss 3.408164775464684e-05 old loss 3.408272095839493e-05 BETTER
I0312 12:00:53.621171 1893685 finetune.py:68] layer 16_v @ epoch 0 new loss 6.159198619570816e-06 old loss 1.23866602734779e-05 BETTER
I0312 12:01:14.608525 1893569 finetune.py:68] layer 15_down @ epoch 4 new loss 3.408163320273161e-05 old loss 3.408164775464684e-05 BETTER
15_v proxy err 0.0005939583643339574 tr(WHW.T) 762.7275390625
15_q proxy err 0.00012078493455192074 tr(WHW.T) 7252.42626953125
15_k proxy err 9.742430120240897e-05 tr(WHW.T) 11073.009765625
15_o proxy err 0.0006474732654169202 tr(WHW.T) 59.62822723388672
15_up proxy err 0.0003388790355529636 tr(WHW.T) 1640.5987548828125
15_gate proxy err 0.0002167251950595528 tr(WHW.T) 2902.7236328125
15_down proxy err 0.0004649764741770923 tr(WHW.T) 114.10382843017578
I0312 12:01:26.310629 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 17 in 70.76362919807434s
I0312 12:01:27.947665 1893685 finetune.py:68] layer 16_v @ epoch 1 new loss 5.438349035102874e-06 old loss 6.159198619570816e-06 BETTER
I0312 12:01:29.431914 1893801 config.py:54] PyTorch version 2.1.1 available.
I0312 12:01:32.010298 1891709 quantize_finetune_llama.py:183] layer 18 gpu 2
I0312 12:01:32.085035 1893801 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:01:40.439003 1893801 finetune.py:45] layer 17_v initial loss 1.023804179567378e-05
I0312 12:02:02.470448 1893685 finetune.py:68] layer 16_v @ epoch 2 new loss 5.131691068527289e-06 old loss 5.438349035102874e-06 BETTER
I0312 12:02:11.581742 1893801 finetune.py:68] layer 17_v @ epoch 0 new loss 5.140645498613594e-06 old loss 1.023804179567378e-05 BETTER
I0312 12:02:36.954240 1893685 finetune.py:68] layer 16_v @ epoch 3 new loss 5.079673883301439e-06 old loss 5.131691068527289e-06 BETTER
I0312 12:02:42.258386 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 18 in 69.84864926338196s
I0312 12:02:43.597818 1893801 finetune.py:68] layer 17_v @ epoch 1 new loss 4.623406766768312e-06 old loss 5.140645498613594e-06 BETTER
I0312 12:02:45.402508 1893917 config.py:54] PyTorch version 2.1.1 available.
I0312 12:02:46.412211 1891709 quantize_finetune_llama.py:183] layer 19 gpu 3
I0312 12:02:46.488620 1893917 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:02:54.563108 1893917 finetune.py:45] layer 18_v initial loss 9.942219548975118e-06
I0312 12:03:12.023982 1893685 finetune.py:76] layer 16_v @ epoch 4 new loss 5.087936642667046e-06 old loss 5.079673883301439e-06 WORSE
I0312 12:03:16.010554 1893801 finetune.py:68] layer 17_v @ epoch 2 new loss 4.482496478885878e-06 old loss 4.623406766768312e-06 BETTER
I0312 12:03:21.177604 1893685 finetune.py:45] layer 16_q initial loss 7.539822490798542e-06
I0312 12:03:26.598827 1893917 finetune.py:68] layer 18_v @ epoch 0 new loss 5.17231546837138e-06 old loss 9.942219548975118e-06 BETTER
I0312 12:03:48.721702 1893801 finetune.py:76] layer 17_v @ epoch 3 new loss 4.4965445340494625e-06 old loss 4.482496478885878e-06 WORSE
I0312 12:03:54.750684 1893685 finetune.py:68] layer 16_q @ epoch 0 new loss 6.13095562584931e-06 old loss 7.539822490798542e-06 BETTER
I0312 12:03:58.949037 1893917 finetune.py:68] layer 18_v @ epoch 1 new loss 4.8430606511828955e-06 old loss 5.17231546837138e-06 BETTER
I0312 12:03:59.941568 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 19 in 73.14919447898865s
I0312 12:04:03.176807 1894033 config.py:54] PyTorch version 2.1.1 available.
I0312 12:04:04.247146 1891709 quantize_finetune_llama.py:183] layer 20 gpu 0
I0312 12:04:04.319134 1894033 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:04:12.181834 1894033 finetune.py:45] layer 19_v initial loss 9.745811439643148e-06
I0312 12:04:20.672516 1893801 finetune.py:68] layer 17_v @ epoch 4 new loss 4.4292833081271965e-06 old loss 4.482496478885878e-06 BETTER
I0312 12:04:29.099760 1893685 finetune.py:68] layer 16_q @ epoch 1 new loss 5.729175882152049e-06 old loss 6.13095562584931e-06 BETTER
I0312 12:04:29.708065 1893801 finetune.py:45] layer 17_q initial loss 6.8448430283751804e-06
I0312 12:04:31.680896 1893917 finetune.py:76] layer 18_v @ epoch 2 new loss 4.846171123062959e-06 old loss 4.8430606511828955e-06 WORSE
I0312 12:04:43.160191 1894033 finetune.py:68] layer 19_v @ epoch 0 new loss 5.086498276796192e-06 old loss 9.745811439643148e-06 BETTER
I0312 12:05:01.117434 1893801 finetune.py:68] layer 17_q @ epoch 0 new loss 5.261996648187051e-06 old loss 6.8448430283751804e-06 BETTER
I0312 12:05:03.828209 1893685 finetune.py:68] layer 16_q @ epoch 2 new loss 5.525970209419029e-06 old loss 5.729175882152049e-06 BETTER
I0312 12:05:03.909880 1893917 finetune.py:68] layer 18_v @ epoch 3 new loss 4.509243353822967e-06 old loss 4.8430606511828955e-06 BETTER
I0312 12:05:15.228242 1894033 finetune.py:68] layer 19_v @ epoch 1 new loss 4.664390871766955e-06 old loss 5.086498276796192e-06 BETTER
I0312 12:05:33.349504 1893801 finetune.py:68] layer 17_q @ epoch 1 new loss 4.951670689479215e-06 old loss 5.261996648187051e-06 BETTER
I0312 12:05:36.781129 1893917 finetune.py:68] layer 18_v @ epoch 4 new loss 4.359036665846361e-06 old loss 4.509243353822967e-06 BETTER
I0312 12:05:38.311053 1893685 finetune.py:68] layer 16_q @ epoch 3 new loss 5.373111434892053e-06 old loss 5.525970209419029e-06 BETTER
I0312 12:05:45.918147 1893917 finetune.py:45] layer 18_q initial loss 7.195606031018542e-06
I0312 12:05:47.305300 1894033 finetune.py:68] layer 19_v @ epoch 2 new loss 4.458583134692162e-06 old loss 4.664390871766955e-06 BETTER
I0312 12:06:06.169064 1893801 finetune.py:68] layer 17_q @ epoch 2 new loss 4.776358309754869e-06 old loss 4.951670689479215e-06 BETTER
I0312 12:06:12.907547 1893685 finetune.py:68] layer 16_q @ epoch 4 new loss 5.256134500086773e-06 old loss 5.373111434892053e-06 BETTER
I0312 12:06:17.720989 1893917 finetune.py:68] layer 18_q @ epoch 0 new loss 5.538054210774135e-06 old loss 7.195606031018542e-06 BETTER
I0312 12:06:19.974107 1894033 finetune.py:68] layer 19_v @ epoch 3 new loss 4.41760448666173e-06 old loss 4.458583134692162e-06 BETTER
I0312 12:06:22.915056 1893685 finetune.py:45] layer 16_k initial loss 7.604346592415823e-06
I0312 12:06:38.549188 1893801 finetune.py:68] layer 17_q @ epoch 3 new loss 4.6745808504056185e-06 old loss 4.776358309754869e-06 BETTER
I0312 12:06:50.488373 1893917 finetune.py:68] layer 18_q @ epoch 1 new loss 5.268668246571906e-06 old loss 5.538054210774135e-06 BETTER
I0312 12:06:52.994736 1894033 finetune.py:68] layer 19_v @ epoch 4 new loss 4.343376531323884e-06 old loss 4.41760448666173e-06 BETTER
I0312 12:06:56.490000 1893685 finetune.py:68] layer 16_k @ epoch 0 new loss 7.056482900225092e-06 old loss 7.604346592415823e-06 BETTER
I0312 12:07:02.803908 1894033 finetune.py:45] layer 19_q initial loss 7.172486220952123e-06
I0312 12:07:10.960303 1893801 finetune.py:68] layer 17_q @ epoch 4 new loss 4.620849267666927e-06 old loss 4.6745808504056185e-06 BETTER
I0312 12:07:20.155868 1893801 finetune.py:45] layer 17_k initial loss 7.062449640216073e-06
I0312 12:07:23.087194 1893917 finetune.py:68] layer 18_q @ epoch 2 new loss 5.065790901426226e-06 old loss 5.268668246571906e-06 BETTER
I0312 12:07:30.196356 1893685 finetune.py:68] layer 16_k @ epoch 1 new loss 6.926014975761063e-06 old loss 7.056482900225092e-06 BETTER
I0312 12:07:34.231767 1894033 finetune.py:68] layer 19_q @ epoch 0 new loss 5.484374469233444e-06 old loss 7.172486220952123e-06 BETTER
I0312 12:07:51.598574 1893801 finetune.py:68] layer 17_k @ epoch 0 new loss 6.2649692154082e-06 old loss 7.062449640216073e-06 BETTER
I0312 12:07:55.713855 1893917 finetune.py:68] layer 18_q @ epoch 3 new loss 5.000875717087183e-06 old loss 5.065790901426226e-06 BETTER
I0312 12:08:04.175893 1893685 finetune.py:68] layer 16_k @ epoch 2 new loss 6.851995294709923e-06 old loss 6.926014975761063e-06 BETTER
I0312 12:08:06.757524 1894033 finetune.py:68] layer 19_q @ epoch 1 new loss 5.144823262526188e-06 old loss 5.484374469233444e-06 BETTER
I0312 12:08:23.716831 1893801 finetune.py:68] layer 17_k @ epoch 1 new loss 6.12863141213893e-06 old loss 6.2649692154082e-06 BETTER
I0312 12:08:28.249099 1893917 finetune.py:68] layer 18_q @ epoch 4 new loss 4.914500550512457e-06 old loss 5.000875717087183e-06 BETTER
I0312 12:08:38.019895 1893917 finetune.py:45] layer 18_k initial loss 7.793511940690223e-06
I0312 12:08:38.366391 1893685 finetune.py:68] layer 16_k @ epoch 3 new loss 6.8028630266780965e-06 old loss 6.851995294709923e-06 BETTER
I0312 12:08:39.313242 1894033 finetune.py:68] layer 19_q @ epoch 2 new loss 4.9633604248811025e-06 old loss 5.144823262526188e-06 BETTER
I0312 12:08:55.825596 1893801 finetune.py:68] layer 17_k @ epoch 2 new loss 6.0523907450260594e-06 old loss 6.12863141213893e-06 BETTER
I0312 12:09:09.668294 1893917 finetune.py:68] layer 18_k @ epoch 0 new loss 6.857681000838056e-06 old loss 7.793511940690223e-06 BETTER
I0312 12:09:11.639168 1894033 finetune.py:68] layer 19_q @ epoch 3 new loss 4.835133495362243e-06 old loss 4.9633604248811025e-06 BETTER
I0312 12:09:12.501717 1893685 finetune.py:68] layer 16_k @ epoch 4 new loss 6.748443411197513e-06 old loss 6.8028630266780965e-06 BETTER
I0312 12:09:22.187042 1893685 finetune.py:45] layer 16_o initial loss 1.7297836166108027e-05
I0312 12:09:27.877541 1893801 finetune.py:68] layer 17_k @ epoch 3 new loss 6.018492058501579e-06 old loss 6.0523907450260594e-06 BETTER
I0312 12:09:41.907346 1893917 finetune.py:68] layer 18_k @ epoch 1 new loss 6.672189101664117e-06 old loss 6.857681000838056e-06 BETTER
I0312 12:09:43.953056 1894033 finetune.py:68] layer 19_q @ epoch 4 new loss 4.745217211166164e-06 old loss 4.835133495362243e-06 BETTER
I0312 12:09:53.049530 1894033 finetune.py:45] layer 19_k initial loss 7.587639174744254e-06
I0312 12:09:54.639990 1893685 finetune.py:68] layer 16_o @ epoch 0 new loss 1.531712041469291e-05 old loss 1.7297836166108027e-05 BETTER
I0312 12:10:00.211033 1893801 finetune.py:68] layer 17_k @ epoch 4 new loss 5.975575277261669e-06 old loss 6.018492058501579e-06 BETTER
I0312 12:10:11.866726 1893801 finetune.py:45] layer 17_o initial loss 1.368322227790486e-05
I0312 12:10:15.058155 1893917 finetune.py:68] layer 18_k @ epoch 2 new loss 6.554114861501148e-06 old loss 6.672189101664117e-06 BETTER
I0312 12:10:24.568111 1894033 finetune.py:68] layer 19_k @ epoch 0 new loss 6.749254680471495e-06 old loss 7.587639174744254e-06 BETTER
I0312 12:10:28.476610 1893685 finetune.py:68] layer 16_o @ epoch 1 new loss 1.462854470446473e-05 old loss 1.531712041469291e-05 BETTER
I0312 12:10:42.704638 1893801 finetune.py:68] layer 17_o @ epoch 0 new loss 1.2358633284748066e-05 old loss 1.368322227790486e-05 BETTER
I0312 12:10:48.280036 1893917 finetune.py:68] layer 18_k @ epoch 3 new loss 6.48649847789784e-06 old loss 6.554114861501148e-06 BETTER
I0312 12:10:57.223564 1894033 finetune.py:68] layer 19_k @ epoch 1 new loss 6.6133475229435135e-06 old loss 6.749254680471495e-06 BETTER
I0312 12:11:02.223827 1893685 finetune.py:68] layer 16_o @ epoch 2 new loss 1.4214999282557983e-05 old loss 1.462854470446473e-05 BETTER
I0312 12:11:14.743033 1893801 finetune.py:68] layer 17_o @ epoch 1 new loss 1.1916116818611044e-05 old loss 1.2358633284748066e-05 BETTER
I0312 12:11:21.212218 1893917 finetune.py:68] layer 18_k @ epoch 4 new loss 6.446535280701937e-06 old loss 6.48649847789784e-06 BETTER
I0312 12:11:29.958065 1894033 finetune.py:68] layer 19_k @ epoch 2 new loss 6.5480248849780764e-06 old loss 6.6133475229435135e-06 BETTER
I0312 12:11:33.851991 1893917 finetune.py:45] layer 18_o initial loss 1.4887746146996506e-05
I0312 12:11:36.537802 1893685 finetune.py:68] layer 16_o @ epoch 3 new loss 1.3918527656642254e-05 old loss 1.4214999282557983e-05 BETTER
I0312 12:11:46.554835 1893801 finetune.py:68] layer 17_o @ epoch 2 new loss 1.165716184914345e-05 old loss 1.1916116818611044e-05 BETTER
I0312 12:12:01.978658 1894033 finetune.py:68] layer 19_k @ epoch 3 new loss 6.499869414255954e-06 old loss 6.5480248849780764e-06 BETTER
I0312 12:12:04.606848 1893917 finetune.py:68] layer 18_o @ epoch 0 new loss 1.343687381449854e-05 old loss 1.4887746146996506e-05 BETTER
I0312 12:12:10.217767 1893685 finetune.py:68] layer 16_o @ epoch 4 new loss 1.3705057426705025e-05 old loss 1.3918527656642254e-05 BETTER
I0312 12:12:18.064985 1893801 finetune.py:68] layer 17_o @ epoch 3 new loss 1.14769745778176e-05 old loss 1.165716184914345e-05 BETTER
I0312 12:12:25.581926 1893685 finetune.py:45] layer 16_up initial loss 2.366678927501198e-05
I0312 12:12:33.601222 1894033 finetune.py:68] layer 19_k @ epoch 4 new loss 6.41964561509667e-06 old loss 6.499869414255954e-06 BETTER
I0312 12:12:35.931722 1893917 finetune.py:68] layer 18_o @ epoch 1 new loss 1.2985915418539662e-05 old loss 1.343687381449854e-05 BETTER
I0312 12:12:42.759204 1894033 finetune.py:45] layer 19_o initial loss 1.423775938746985e-05
I0312 12:12:49.394715 1893801 finetune.py:68] layer 17_o @ epoch 4 new loss 1.1350125532771926e-05 old loss 1.14769745778176e-05 BETTER
I0312 12:12:56.064062 1893685 finetune.py:68] layer 16_up @ epoch 0 new loss 2.233725354017224e-05 old loss 2.366678927501198e-05 BETTER
I0312 12:13:04.254812 1893801 finetune.py:45] layer 17_up initial loss 2.2239935788093135e-05
I0312 12:13:07.383102 1893917 finetune.py:68] layer 18_o @ epoch 2 new loss 1.2723909094347619e-05 old loss 1.2985915418539662e-05 BETTER
I0312 12:13:12.867758 1894033 finetune.py:68] layer 19_o @ epoch 0 new loss 1.3034702533332165e-05 old loss 1.423775938746985e-05 BETTER
I0312 12:13:27.603215 1893685 finetune.py:68] layer 16_up @ epoch 1 new loss 2.1819036192027852e-05 old loss 2.233725354017224e-05 BETTER
I0312 12:13:33.198262 1893801 finetune.py:68] layer 17_up @ epoch 0 new loss 2.1066269255243242e-05 old loss 2.2239935788093135e-05 BETTER
I0312 12:13:38.930846 1893917 finetune.py:68] layer 18_o @ epoch 3 new loss 1.255600272997981e-05 old loss 1.2723909094347619e-05 BETTER
I0312 12:13:43.697718 1894033 finetune.py:68] layer 19_o @ epoch 1 new loss 1.2694819815806113e-05 old loss 1.3034702533332165e-05 BETTER
I0312 12:13:59.122511 1893685 finetune.py:68] layer 16_up @ epoch 2 new loss 2.1479165297932923e-05 old loss 2.1819036192027852e-05 BETTER
I0312 12:14:02.831786 1893801 finetune.py:68] layer 17_up @ epoch 1 new loss 2.058121026493609e-05 old loss 2.1066269255243242e-05 BETTER
I0312 12:14:10.502480 1893917 finetune.py:68] layer 18_o @ epoch 4 new loss 1.243758470081957e-05 old loss 1.255600272997981e-05 BETTER
I0312 12:14:14.464408 1894033 finetune.py:68] layer 19_o @ epoch 2 new loss 1.2465119652915746e-05 old loss 1.2694819815806113e-05 BETTER
I0312 12:14:25.441471 1893917 finetune.py:45] layer 18_up initial loss 2.5312976504210383e-05
I0312 12:14:30.756986 1893685 finetune.py:68] layer 16_up @ epoch 3 new loss 2.1227459001238458e-05 old loss 2.1479165297932923e-05 BETTER
I0312 12:14:32.331948 1893801 finetune.py:68] layer 17_up @ epoch 2 new loss 2.0275630959076807e-05 old loss 2.058121026493609e-05 BETTER
I0312 12:14:45.280167 1894033 finetune.py:68] layer 19_o @ epoch 3 new loss 1.2341160982032306e-05 old loss 1.2465119652915746e-05 BETTER
I0312 12:14:54.209093 1893917 finetune.py:68] layer 18_up @ epoch 0 new loss 2.3997132302611135e-05 old loss 2.5312976504210383e-05 BETTER
I0312 12:15:02.034721 1893801 finetune.py:68] layer 17_up @ epoch 3 new loss 2.0051416868227534e-05 old loss 2.0275630959076807e-05 BETTER
I0312 12:15:02.429663 1893685 finetune.py:68] layer 16_up @ epoch 4 new loss 2.103717633872293e-05 old loss 2.1227459001238458e-05 BETTER
I0312 12:15:16.159713 1894033 finetune.py:68] layer 19_o @ epoch 4 new loss 1.2248483471921645e-05 old loss 1.2341160982032306e-05 BETTER
I0312 12:15:17.656409 1893685 finetune.py:45] layer 16_gate initial loss 2.9524231649702415e-05
I0312 12:15:23.901026 1893917 finetune.py:68] layer 18_up @ epoch 1 new loss 2.3470116502721794e-05 old loss 2.3997132302611135e-05 BETTER
I0312 12:15:31.265249 1894033 finetune.py:45] layer 19_up initial loss 2.675242103578057e-05
I0312 12:15:31.865613 1893801 finetune.py:68] layer 17_up @ epoch 4 new loss 1.9883254935848527e-05 old loss 2.0051416868227534e-05 BETTER
I0312 12:15:46.672707 1893801 finetune.py:45] layer 17_gate initial loss 2.9164681109250523e-05
I0312 12:15:46.720149 1893685 finetune.py:68] layer 16_gate @ epoch 0 new loss 2.8730466510751285e-05 old loss 2.9524231649702415e-05 BETTER
I0312 12:15:53.730415 1893917 finetune.py:68] layer 18_up @ epoch 2 new loss 2.313703043910209e-05 old loss 2.3470116502721794e-05 BETTER
I0312 12:15:59.856569 1894033 finetune.py:68] layer 19_up @ epoch 0 new loss 2.5343890229123645e-05 old loss 2.675242103578057e-05 BETTER
I0312 12:16:14.169990 1893801 finetune.py:68] layer 17_gate @ epoch 0 new loss 2.8476213628891855e-05 old loss 2.9164681109250523e-05 BETTER
I0312 12:16:16.487963 1893685 finetune.py:68] layer 16_gate @ epoch 1 new loss 2.8382110031088814e-05 old loss 2.8730466510751285e-05 BETTER
I0312 12:16:23.543217 1893917 finetune.py:68] layer 18_up @ epoch 3 new loss 2.2896954760653898e-05 old loss 2.313703043910209e-05 BETTER
I0312 12:16:29.155799 1894033 finetune.py:68] layer 19_up @ epoch 1 new loss 2.48087162617594e-05 old loss 2.5343890229123645e-05 BETTER
I0312 12:16:42.560913 1893801 finetune.py:68] layer 17_gate @ epoch 1 new loss 2.8149695936008357e-05 old loss 2.8476213628891855e-05 BETTER
I0312 12:16:46.615104 1893685 finetune.py:68] layer 16_gate @ epoch 2 new loss 2.8138067136751488e-05 old loss 2.8382110031088814e-05 BETTER
I0312 12:16:53.733833 1893917 finetune.py:68] layer 18_up @ epoch 4 new loss 2.2720305423717946e-05 old loss 2.2896954760653898e-05 BETTER
I0312 12:16:58.739162 1894033 finetune.py:68] layer 19_up @ epoch 2 new loss 2.4478958948748186e-05 old loss 2.48087162617594e-05 BETTER
I0312 12:17:08.865856 1893917 finetune.py:45] layer 18_gate initial loss 3.345794902998023e-05
I0312 12:17:10.815083 1893801 finetune.py:68] layer 17_gate @ epoch 2 new loss 2.7927755581913516e-05 old loss 2.8149695936008357e-05 BETTER
I0312 12:17:16.541425 1893685 finetune.py:68] layer 16_gate @ epoch 3 new loss 2.7956968551734462e-05 old loss 2.8138067136751488e-05 BETTER
I0312 12:17:28.205688 1894033 finetune.py:68] layer 19_up @ epoch 3 new loss 2.425060301902704e-05 old loss 2.4478958948748186e-05 BETTER
I0312 12:17:36.499574 1893917 finetune.py:68] layer 18_gate @ epoch 0 new loss 3.278098665759899e-05 old loss 3.345794902998023e-05 BETTER
I0312 12:17:39.061474 1893801 finetune.py:68] layer 17_gate @ epoch 3 new loss 2.7759997465182096e-05 old loss 2.7927755581913516e-05 BETTER
I0312 12:17:46.464739 1893685 finetune.py:68] layer 16_gate @ epoch 4 new loss 2.7812246116809547e-05 old loss 2.7956968551734462e-05 BETTER
I0312 12:17:57.543699 1894033 finetune.py:68] layer 19_up @ epoch 4 new loss 2.408833461231552e-05 old loss 2.425060301902704e-05 BETTER
I0312 12:18:02.253663 1893685 finetune.py:45] layer 16_down initial loss 4.503883246798068e-05
I0312 12:18:04.593600 1893917 finetune.py:68] layer 18_gate @ epoch 1 new loss 3.24472930515185e-05 old loss 3.278098665759899e-05 BETTER
I0312 12:18:07.297231 1893801 finetune.py:68] layer 17_gate @ epoch 4 new loss 2.7627082090475596e-05 old loss 2.7759997465182096e-05 BETTER
I0312 12:18:12.751134 1894033 finetune.py:45] layer 19_gate initial loss 3.655621185316704e-05
I0312 12:18:23.047645 1893801 finetune.py:45] layer 17_down initial loss 4.635662480723113e-05
I0312 12:18:29.617873 1893685 finetune.py:68] layer 16_down @ epoch 0 new loss 4.502858791965991e-05 old loss 4.503883246798068e-05 BETTER
I0312 12:18:32.642630 1893917 finetune.py:68] layer 18_gate @ epoch 2 new loss 3.221374572603963e-05 old loss 3.24472930515185e-05 BETTER
I0312 12:18:39.617743 1894033 finetune.py:68] layer 19_gate @ epoch 0 new loss 3.5811597626889125e-05 old loss 3.655621185316704e-05 BETTER
I0312 12:18:48.582644 1893801 finetune.py:68] layer 17_down @ epoch 0 new loss 4.634499055100605e-05 old loss 4.635662480723113e-05 BETTER
I0312 12:18:58.113666 1893685 finetune.py:68] layer 16_down @ epoch 1 new loss 4.502563751884736e-05 old loss 4.502858791965991e-05 BETTER
I0312 12:19:00.594145 1893917 finetune.py:68] layer 18_gate @ epoch 3 new loss 3.204448148608208e-05 old loss 3.221374572603963e-05 BETTER
I0312 12:19:07.408645 1894033 finetune.py:68] layer 19_gate @ epoch 1 new loss 3.5447985283099115e-05 old loss 3.5811597626889125e-05 BETTER
I0312 12:19:14.974572 1893801 finetune.py:68] layer 17_down @ epoch 1 new loss 4.634228389477357e-05 old loss 4.634499055100605e-05 BETTER
I0312 12:19:26.603450 1893685 finetune.py:68] layer 16_down @ epoch 2 new loss 4.50238112534862e-05 old loss 4.502563751884736e-05 BETTER
I0312 12:19:28.466077 1893917 finetune.py:68] layer 18_gate @ epoch 4 new loss 3.191125506418757e-05 old loss 3.204448148608208e-05 BETTER
I0312 12:19:35.119151 1894033 finetune.py:68] layer 19_gate @ epoch 2 new loss 3.5207860491937026e-05 old loss 3.5447985283099115e-05 BETTER
I0312 12:19:41.604040 1893801 finetune.py:68] layer 17_down @ epoch 2 new loss 4.634063952835277e-05 old loss 4.634228389477357e-05 BETTER
I0312 12:19:44.357567 1893917 finetune.py:45] layer 18_down initial loss 5.45872280781623e-05
I0312 12:19:55.078968 1893685 finetune.py:68] layer 16_down @ epoch 3 new loss 4.502267620409839e-05 old loss 4.50238112534862e-05 BETTER
I0312 12:20:02.983309 1894033 finetune.py:68] layer 19_gate @ epoch 3 new loss 3.5030250728596e-05 old loss 3.5207860491937026e-05 BETTER
I0312 12:20:08.461319 1893801 finetune.py:68] layer 17_down @ epoch 3 new loss 4.633951175492257e-05 old loss 4.634063952835277e-05 BETTER
I0312 12:20:10.249713 1893917 finetune.py:68] layer 18_down @ epoch 0 new loss 5.457287988974713e-05 old loss 5.45872280781623e-05 BETTER
I0312 12:20:23.540531 1893685 finetune.py:68] layer 16_down @ epoch 4 new loss 4.502192314248532e-05 old loss 4.502267620409839e-05 BETTER
16_v proxy err 0.0006016971892677248 tr(WHW.T) 780.7407836914062
16_q proxy err 0.00012306449934840202 tr(WHW.T) 7193.45458984375
16_k proxy err 9.891386434901506e-05 tr(WHW.T) 11630.173828125
16_o proxy err 0.0005355249741114676 tr(WHW.T) 88.24213409423828
16_up proxy err 0.0003334184002596885 tr(WHW.T) 1891.0250244140625
16_gate proxy err 0.00021214046864770353 tr(WHW.T) 3369.748046875
16_down proxy err 0.0004711761139333248 tr(WHW.T) 152.03768920898438
I0312 12:20:31.372700 1894033 finetune.py:68] layer 19_gate @ epoch 4 new loss 3.4899752790806815e-05 old loss 3.5030250728596e-05 BETTER
I0312 12:20:35.410854 1893801 finetune.py:68] layer 17_down @ epoch 4 new loss 4.633862408809364e-05 old loss 4.633951175492257e-05 BETTER
17_v proxy err 0.000573158438783139 tr(WHW.T) 845.7654418945312
17_q proxy err 0.00011974496737821028 tr(WHW.T) 7163.3642578125
17_k proxy err 0.00010147136345040053 tr(WHW.T) 10698.8037109375
17_o proxy err 0.000576336809899658 tr(WHW.T) 58.147552490234375
17_up proxy err 0.00035949895391240716 tr(WHW.T) 1921.1673583984375
17_gate proxy err 0.00021881463180761784 tr(WHW.T) 3570.857177734375
17_down proxy err 0.0004694749368354678 tr(WHW.T) 165.4668731689453
I0312 12:20:37.463662 1893917 finetune.py:68] layer 18_down @ epoch 1 new loss 5.4568907216889784e-05 old loss 5.457287988974713e-05 BETTER
I0312 12:20:46.815282 1894033 finetune.py:45] layer 19_down initial loss 5.965825403109193e-05
I0312 12:21:04.353769 1893917 finetune.py:68] layer 18_down @ epoch 2 new loss 5.4566367907682434e-05 old loss 5.4568907216889784e-05 BETTER
I0312 12:21:12.031222 1894033 finetune.py:68] layer 19_down @ epoch 0 new loss 5.964544470771216e-05 old loss 5.965825403109193e-05 BETTER
I0312 12:21:31.007537 1893917 finetune.py:68] layer 18_down @ epoch 3 new loss 5.4564516176469624e-05 old loss 5.4566367907682434e-05 BETTER
I0312 12:21:38.129096 1894033 finetune.py:68] layer 19_down @ epoch 1 new loss 5.964240335742943e-05 old loss 5.964544470771216e-05 BETTER
I0312 12:21:53.553252 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 20 in 73.8345263004303s
I0312 12:21:56.845500 1894149 config.py:54] PyTorch version 2.1.1 available.
I0312 12:21:57.879036 1893917 finetune.py:68] layer 18_down @ epoch 4 new loss 5.4562926379730925e-05 old loss 5.4564516176469624e-05 BETTER
I0312 12:21:57.883917 1891709 quantize_finetune_llama.py:183] layer 21 gpu 1
I0312 12:21:57.960217 1894149 data_utils.py:336] using 256 training seqs, 128 validation seqs
18_v proxy err 0.0005334526067599654 tr(WHW.T) 1003.7705078125
18_q proxy err 0.0001224039151566103 tr(WHW.T) 7510.58203125
18_k proxy err 0.0001020807831082493 tr(WHW.T) 10465.732421875
18_o proxy err 0.0005134709062986076 tr(WHW.T) 69.90751647949219
18_up proxy err 0.00037741867708973587 tr(WHW.T) 2023.2733154296875
18_gate proxy err 0.00022843437909614295 tr(WHW.T) 3782.4423828125
18_down proxy err 0.00047211901983246207 tr(WHW.T) 198.58004760742188
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:22:04.260146 1894033 finetune.py:68] layer 19_down @ epoch 2 new loss 5.964066804153845e-05 old loss 5.964240335742943e-05 BETTER
I0312 12:22:07.254651 1894149 finetune.py:45] layer 20_v initial loss 1.1701932635332923e-05
I0312 12:22:30.593901 1894033 finetune.py:68] layer 19_down @ epoch 3 new loss 5.963926741969772e-05 old loss 5.964066804153845e-05 BETTER
I0312 12:22:40.278509 1894149 finetune.py:68] layer 20_v @ epoch 0 new loss 6.083071639295667e-06 old loss 1.1701932635332923e-05 BETTER
I0312 12:22:56.624802 1894033 finetune.py:68] layer 19_down @ epoch 4 new loss 5.963833973510191e-05 old loss 5.963926741969772e-05 BETTER
19_v proxy err 0.0005239188903942704 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.00012779967801179737 tr(WHW.T) 6944.42529296875
19_k proxy err 0.0001050946448231116 tr(WHW.T) 10550.1611328125
19_o proxy err 0.000509853707626462 tr(WHW.T) 62.29766845703125
19_up proxy err 0.0003794497752096504 tr(WHW.T) 2149.341064453125
19_gate proxy err 0.0002472813939675689 tr(WHW.T) 3686.89892578125
19_down proxy err 0.0004594773054122925 tr(WHW.T) 222.9711151123047
I0312 12:23:14.179139 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 21 in 72.40027713775635s
I0312 12:23:14.565923 1894149 finetune.py:68] layer 20_v @ epoch 1 new loss 5.806418812426273e-06 old loss 6.083071639295667e-06 BETTER
I0312 12:23:17.178722 1894265 config.py:54] PyTorch version 2.1.1 available.
I0312 12:23:18.119843 1891709 quantize_finetune_llama.py:183] layer 22 gpu 2
I0312 12:23:18.185482 1894265 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:23:26.399029 1894265 finetune.py:45] layer 21_v initial loss 1.0007243872678373e-05
I0312 12:23:49.141412 1894149 finetune.py:76] layer 20_v @ epoch 2 new loss 6.067460162739735e-06 old loss 5.806418812426273e-06 WORSE
I0312 12:23:57.563747 1894265 finetune.py:68] layer 21_v @ epoch 0 new loss 5.515451448445674e-06 old loss 1.0007243872678373e-05 BETTER
I0312 12:24:23.359640 1894149 finetune.py:68] layer 20_v @ epoch 3 new loss 5.738746949646156e-06 old loss 5.806418812426273e-06 BETTER
I0312 12:24:28.514427 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 22 in 70.02584481239319s
I0312 12:24:29.713154 1894265 finetune.py:68] layer 21_v @ epoch 1 new loss 5.1963261284981854e-06 old loss 5.515451448445674e-06 BETTER
I0312 12:24:31.826332 1894381 config.py:54] PyTorch version 2.1.1 available.
I0312 12:24:32.874732 1891709 quantize_finetune_llama.py:183] layer 23 gpu 3
I0312 12:24:32.959537 1894381 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:24:41.315500 1894381 finetune.py:45] layer 22_v initial loss 1.2090347809134983e-05
I0312 12:24:58.256423 1894149 finetune.py:68] layer 20_v @ epoch 4 new loss 5.456144208437763e-06 old loss 5.738746949646156e-06 BETTER
I0312 12:25:02.105470 1894265 finetune.py:68] layer 21_v @ epoch 2 new loss 5.189080638956511e-06 old loss 5.1963261284981854e-06 BETTER
I0312 12:25:07.764390 1894149 finetune.py:45] layer 20_q initial loss 8.498313945892733e-06
I0312 12:25:13.059854 1894381 finetune.py:68] layer 22_v @ epoch 0 new loss 6.79783761370345e-06 old loss 1.2090347809134983e-05 BETTER
I0312 12:25:34.576698 1894265 finetune.py:76] layer 21_v @ epoch 3 new loss 5.280424829834374e-06 old loss 5.189080638956511e-06 WORSE
I0312 12:25:41.107145 1894149 finetune.py:68] layer 20_q @ epoch 0 new loss 6.7737405515799765e-06 old loss 8.498313945892733e-06 BETTER
I0312 12:25:43.409122 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 23 in 70.14635920524597s
I0312 12:25:45.865209 1894381 finetune.py:68] layer 22_v @ epoch 1 new loss 6.710687557642814e-06 old loss 6.79783761370345e-06 BETTER
I0312 12:25:46.690502 1894497 config.py:54] PyTorch version 2.1.1 available.
I0312 12:25:47.706714 1891709 quantize_finetune_llama.py:183] layer 24 gpu 0
I0312 12:25:47.776711 1894497 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:25:55.621743 1894497 finetune.py:45] layer 23_v initial loss 1.1948062820010819e-05
I0312 12:26:06.387998 1894265 finetune.py:68] layer 21_v @ epoch 4 new loss 5.0555586312839296e-06 old loss 5.189080638956511e-06 BETTER
I0312 12:26:15.350022 1894149 finetune.py:68] layer 20_q @ epoch 1 new loss 6.402704912034096e-06 old loss 6.7737405515799765e-06 BETTER
I0312 12:26:15.544306 1894265 finetune.py:45] layer 21_q initial loss 7.688535333727486e-06
I0312 12:26:18.417136 1894381 finetune.py:76] layer 22_v @ epoch 2 new loss 6.881910849187989e-06 old loss 6.710687557642814e-06 WORSE
I0312 12:26:26.849293 1894497 finetune.py:68] layer 23_v @ epoch 0 new loss 6.738845058862353e-06 old loss 1.1948062820010819e-05 BETTER
I0312 12:26:47.893536 1894265 finetune.py:68] layer 21_q @ epoch 0 new loss 6.156026756798383e-06 old loss 7.688535333727486e-06 BETTER
I0312 12:26:50.472419 1894149 finetune.py:68] layer 20_q @ epoch 2 new loss 6.232697614905192e-06 old loss 6.402704912034096e-06 BETTER
I0312 12:26:51.727766 1894381 finetune.py:68] layer 22_v @ epoch 3 new loss 6.388739620888373e-06 old loss 6.710687557642814e-06 BETTER
I0312 12:26:58.994482 1894497 finetune.py:68] layer 23_v @ epoch 1 new loss 6.371154540829593e-06 old loss 6.738845058862353e-06 BETTER
I0312 12:27:20.730097 1894265 finetune.py:68] layer 21_q @ epoch 1 new loss 5.839787718286971e-06 old loss 6.156026756798383e-06 BETTER
I0312 12:27:25.694346 1894381 finetune.py:68] layer 22_v @ epoch 4 new loss 6.151234629214741e-06 old loss 6.388739620888373e-06 BETTER
I0312 12:27:26.064216 1894149 finetune.py:68] layer 20_q @ epoch 3 new loss 6.130189376563067e-06 old loss 6.232697614905192e-06 BETTER
I0312 12:27:31.184214 1894497 finetune.py:76] layer 23_v @ epoch 2 new loss 6.39658674117527e-06 old loss 6.371154540829593e-06 WORSE
I0312 12:27:36.012580 1894381 finetune.py:45] layer 22_q initial loss 1.0020289664680604e-05
I0312 12:27:53.574079 1894265 finetune.py:68] layer 21_q @ epoch 2 new loss 5.680349659087369e-06 old loss 5.839787718286971e-06 BETTER
I0312 12:28:01.772210 1894149 finetune.py:68] layer 20_q @ epoch 4 new loss 6.01229066887754e-06 old loss 6.130189376563067e-06 BETTER
I0312 12:28:03.685853 1894497 finetune.py:76] layer 23_v @ epoch 3 new loss 6.393750936695142e-06 old loss 6.371154540829593e-06 WORSE
I0312 12:28:08.084060 1894381 finetune.py:68] layer 22_q @ epoch 0 new loss 7.919888957985677e-06 old loss 1.0020289664680604e-05 BETTER
I0312 12:28:12.748519 1894149 finetune.py:45] layer 20_k initial loss 9.32567309064325e-06
I0312 12:28:26.144481 1894265 finetune.py:68] layer 21_q @ epoch 3 new loss 5.568785581999691e-06 old loss 5.680349659087369e-06 BETTER
I0312 12:28:35.439236 1894497 finetune.py:68] layer 23_v @ epoch 4 new loss 6.1546584220195655e-06 old loss 6.371154540829593e-06 BETTER
I0312 12:28:40.673719 1894381 finetune.py:68] layer 22_q @ epoch 1 new loss 7.470091986760963e-06 old loss 7.919888957985677e-06 BETTER
I0312 12:28:44.946760 1894497 finetune.py:45] layer 23_q initial loss 9.266605047741905e-06
I0312 12:28:45.737458 1894149 finetune.py:68] layer 20_k @ epoch 0 new loss 8.48045510792872e-06 old loss 9.32567309064325e-06 BETTER
I0312 12:28:58.411880 1894265 finetune.py:68] layer 21_q @ epoch 4 new loss 5.487980160978623e-06 old loss 5.568785581999691e-06 BETTER
I0312 12:29:07.432148 1894265 finetune.py:45] layer 21_k initial loss 8.005088602658361e-06
I0312 12:29:13.344746 1894381 finetune.py:68] layer 22_q @ epoch 2 new loss 7.243922482302878e-06 old loss 7.470091986760963e-06 BETTER
I0312 12:29:16.570143 1894497 finetune.py:68] layer 23_q @ epoch 0 new loss 7.512908268836327e-06 old loss 9.266605047741905e-06 BETTER
I0312 12:29:19.523022 1894149 finetune.py:68] layer 20_k @ epoch 1 new loss 8.23606205813121e-06 old loss 8.48045510792872e-06 BETTER
I0312 12:29:39.082147 1894265 finetune.py:68] layer 21_k @ epoch 0 new loss 7.422791895805858e-06 old loss 8.005088602658361e-06 BETTER
I0312 12:29:45.536333 1894381 finetune.py:68] layer 22_q @ epoch 3 new loss 7.095217370078899e-06 old loss 7.243922482302878e-06 BETTER
I0312 12:29:48.703994 1894497 finetune.py:68] layer 23_q @ epoch 1 new loss 7.112427283573197e-06 old loss 7.512908268836327e-06 BETTER
I0312 12:29:53.252202 1894149 finetune.py:68] layer 20_k @ epoch 2 new loss 8.129075467877556e-06 old loss 8.23606205813121e-06 BETTER
I0312 12:30:11.112977 1894265 finetune.py:68] layer 21_k @ epoch 1 new loss 7.316242772503756e-06 old loss 7.422791895805858e-06 BETTER
I0312 12:30:18.066983 1894381 finetune.py:68] layer 22_q @ epoch 4 new loss 6.944918368390063e-06 old loss 7.095217370078899e-06 BETTER
I0312 12:30:20.807861 1894497 finetune.py:68] layer 23_q @ epoch 2 new loss 6.920038231328363e-06 old loss 7.112427283573197e-06 BETTER
I0312 12:30:26.965393 1894149 finetune.py:68] layer 20_k @ epoch 3 new loss 8.097773388726637e-06 old loss 8.129075467877556e-06 BETTER
I0312 12:30:27.380218 1894381 finetune.py:45] layer 22_k initial loss 1.0574175576039124e-05
I0312 12:30:43.183110 1894265 finetune.py:68] layer 21_k @ epoch 2 new loss 7.31233603801229e-06 old loss 7.316242772503756e-06 BETTER
I0312 12:30:52.840713 1894497 finetune.py:68] layer 23_q @ epoch 3 new loss 6.8166214077791665e-06 old loss 6.920038231328363e-06 BETTER
I0312 12:30:58.885227 1894381 finetune.py:68] layer 22_k @ epoch 0 new loss 9.721177775645629e-06 old loss 1.0574175576039124e-05 BETTER
I0312 12:31:00.850765 1894149 finetune.py:68] layer 20_k @ epoch 4 new loss 8.067301678238437e-06 old loss 8.097773388726637e-06 BETTER
I0312 12:31:10.150563 1894149 finetune.py:45] layer 20_o initial loss 1.8207834727945738e-05
I0312 12:31:15.212742 1894265 finetune.py:68] layer 21_k @ epoch 3 new loss 7.285642823262606e-06 old loss 7.31233603801229e-06 BETTER
I0312 12:31:24.843614 1894497 finetune.py:68] layer 23_q @ epoch 4 new loss 6.724159902660176e-06 old loss 6.8166214077791665e-06 BETTER
I0312 12:31:31.081342 1894381 finetune.py:68] layer 22_k @ epoch 1 new loss 9.541288818581961e-06 old loss 9.721177775645629e-06 BETTER
I0312 12:31:34.299628 1894497 finetune.py:45] layer 23_k initial loss 9.396747373102698e-06
I0312 12:31:42.703627 1894149 finetune.py:68] layer 20_o @ epoch 0 new loss 1.6328052879543975e-05 old loss 1.8207834727945738e-05 BETTER
I0312 12:31:47.201942 1894265 finetune.py:68] layer 21_k @ epoch 4 new loss 7.245365395647241e-06 old loss 7.285642823262606e-06 BETTER
I0312 12:31:56.297980 1894265 finetune.py:45] layer 21_o initial loss 1.527629683550913e-05
I0312 12:32:03.191829 1894381 finetune.py:68] layer 22_k @ epoch 2 new loss 9.411576684215106e-06 old loss 9.541288818581961e-06 BETTER
I0312 12:32:05.187170 1894497 finetune.py:68] layer 23_k @ epoch 0 new loss 9.026513907883782e-06 old loss 9.396747373102698e-06 BETTER
I0312 12:32:15.912503 1894149 finetune.py:68] layer 20_o @ epoch 1 new loss 1.576680915604811e-05 old loss 1.6328052879543975e-05 BETTER
I0312 12:32:26.809057 1894265 finetune.py:68] layer 21_o @ epoch 0 new loss 1.4203255886968691e-05 old loss 1.527629683550913e-05 BETTER
I0312 12:32:35.259724 1894381 finetune.py:68] layer 22_k @ epoch 3 new loss 9.320669960288797e-06 old loss 9.411576684215106e-06 BETTER
I0312 12:32:36.781810 1894497 finetune.py:68] layer 23_k @ epoch 1 new loss 8.890056051313877e-06 old loss 9.026513907883782e-06 BETTER
I0312 12:32:49.087185 1894149 finetune.py:68] layer 20_o @ epoch 2 new loss 1.5482029994018376e-05 old loss 1.576680915604811e-05 BETTER
I0312 12:32:58.018770 1894265 finetune.py:68] layer 21_o @ epoch 1 new loss 1.394003538734978e-05 old loss 1.4203255886968691e-05 BETTER
I0312 12:33:07.505393 1894381 finetune.py:68] layer 22_k @ epoch 4 new loss 9.28554254642222e-06 old loss 9.320669960288797e-06 BETTER
I0312 12:33:08.458862 1894497 finetune.py:68] layer 23_k @ epoch 2 new loss 8.792306289251428e-06 old loss 8.890056051313877e-06 BETTER
I0312 12:33:16.929597 1894381 finetune.py:45] layer 22_o initial loss 2.100947494909633e-05
I0312 12:33:22.390058 1894149 finetune.py:68] layer 20_o @ epoch 3 new loss 1.5291489035007544e-05 old loss 1.5482029994018376e-05 BETTER
I0312 12:33:29.203510 1894265 finetune.py:68] layer 21_o @ epoch 2 new loss 1.3802401554130483e-05 old loss 1.394003538734978e-05 BETTER
I0312 12:33:39.967515 1894497 finetune.py:68] layer 23_k @ epoch 3 new loss 8.763319783611223e-06 old loss 8.792306289251428e-06 BETTER
I0312 12:33:47.638939 1894381 finetune.py:68] layer 22_o @ epoch 0 new loss 1.8574604837340303e-05 old loss 2.100947494909633e-05 BETTER
I0312 12:33:55.814920 1894149 finetune.py:68] layer 20_o @ epoch 4 new loss 1.5182528841251042e-05 old loss 1.5291489035007544e-05 BETTER
I0312 12:34:00.381247 1894265 finetune.py:68] layer 21_o @ epoch 3 new loss 1.3723283700528555e-05 old loss 1.3802401554130483e-05 BETTER
I0312 12:34:11.183951 1894149 finetune.py:45] layer 20_up initial loss 3.246225242037326e-05
I0312 12:34:11.560634 1894497 finetune.py:68] layer 23_k @ epoch 4 new loss 8.741832971281838e-06 old loss 8.763319783611223e-06 BETTER
I0312 12:34:19.057899 1894381 finetune.py:68] layer 22_o @ epoch 1 new loss 1.808359957067296e-05 old loss 1.8574604837340303e-05 BETTER
I0312 12:34:21.072161 1894497 finetune.py:45] layer 23_o initial loss 1.8111944882548414e-05
I0312 12:34:31.551071 1894265 finetune.py:68] layer 21_o @ epoch 4 new loss 1.369622623315081e-05 old loss 1.3723283700528555e-05 BETTER
I0312 12:34:41.558513 1894149 finetune.py:68] layer 20_up @ epoch 0 new loss 3.079106318182312e-05 old loss 3.246225242037326e-05 BETTER
I0312 12:34:46.382100 1894265 finetune.py:45] layer 21_up initial loss 3.225382897653617e-05
I0312 12:34:50.737808 1894381 finetune.py:68] layer 22_o @ epoch 2 new loss 1.779907506715972e-05 old loss 1.808359957067296e-05 BETTER
I0312 12:34:51.418525 1894497 finetune.py:68] layer 23_o @ epoch 0 new loss 1.7005662812152877e-05 old loss 1.8111944882548414e-05 BETTER
I0312 12:35:12.932482 1894149 finetune.py:68] layer 20_up @ epoch 1 new loss 3.0147421057336032e-05 old loss 3.079106318182312e-05 BETTER
I0312 12:35:15.274602 1894265 finetune.py:68] layer 21_up @ epoch 0 new loss 3.0791645258432254e-05 old loss 3.225382897653617e-05 BETTER
I0312 12:35:22.209634 1894381 finetune.py:68] layer 22_o @ epoch 3 new loss 1.7668127838987857e-05 old loss 1.779907506715972e-05 BETTER
I0312 12:35:22.218091 1894497 finetune.py:68] layer 23_o @ epoch 1 new loss 1.67116795637412e-05 old loss 1.7005662812152877e-05 BETTER
I0312 12:35:44.518900 1894149 finetune.py:68] layer 20_up @ epoch 2 new loss 2.9763206839561462e-05 old loss 3.0147421057336032e-05 BETTER
I0312 12:35:44.848630 1894265 finetune.py:68] layer 21_up @ epoch 1 new loss 3.023763383680489e-05 old loss 3.0791645258432254e-05 BETTER
I0312 12:35:53.135866 1894497 finetune.py:68] layer 23_o @ epoch 2 new loss 1.6562293239985593e-05 old loss 1.67116795637412e-05 BETTER
I0312 12:35:53.840169 1894381 finetune.py:68] layer 22_o @ epoch 4 new loss 1.7599975763005204e-05 old loss 1.7668127838987857e-05 BETTER
I0312 12:36:08.853033 1894381 finetune.py:45] layer 22_up initial loss 3.8857404433656484e-05
I0312 12:36:14.428496 1894265 finetune.py:68] layer 21_up @ epoch 2 new loss 2.9898925276938826e-05 old loss 3.023763383680489e-05 BETTER
I0312 12:36:15.950687 1894149 finetune.py:68] layer 20_up @ epoch 3 new loss 2.9490409360732883e-05 old loss 2.9763206839561462e-05 BETTER
I0312 12:36:23.878458 1894497 finetune.py:68] layer 23_o @ epoch 3 new loss 1.6455680452054366e-05 old loss 1.6562293239985593e-05 BETTER
I0312 12:36:37.707742 1894381 finetune.py:68] layer 22_up @ epoch 0 new loss 3.723419285961427e-05 old loss 3.8857404433656484e-05 BETTER
I0312 12:36:44.029648 1894265 finetune.py:68] layer 21_up @ epoch 3 new loss 2.9676430131075904e-05 old loss 2.9898925276938826e-05 BETTER
I0312 12:36:47.331701 1894149 finetune.py:68] layer 20_up @ epoch 4 new loss 2.929780566773843e-05 old loss 2.9490409360732883e-05 BETTER
I0312 12:36:54.648154 1894497 finetune.py:68] layer 23_o @ epoch 4 new loss 1.6437779777334072e-05 old loss 1.6455680452054366e-05 BETTER
I0312 12:37:02.530903 1894149 finetune.py:45] layer 20_gate initial loss 4.414526847540401e-05
I0312 12:37:07.637152 1894381 finetune.py:68] layer 22_up @ epoch 1 new loss 3.6592708056559786e-05 old loss 3.723419285961427e-05 BETTER
I0312 12:37:09.609894 1894497 finetune.py:45] layer 23_up initial loss 3.9709066186333075e-05
I0312 12:37:13.721443 1894265 finetune.py:68] layer 21_up @ epoch 4 new loss 2.9514039852074347e-05 old loss 2.9676430131075904e-05 BETTER
I0312 12:37:28.843213 1894265 finetune.py:45] layer 21_gate initial loss 4.543923932942562e-05
I0312 12:37:31.479285 1894149 finetune.py:68] layer 20_gate @ epoch 0 new loss 4.324524343246594e-05 old loss 4.414526847540401e-05 BETTER
I0312 12:37:37.518823 1894381 finetune.py:68] layer 22_up @ epoch 2 new loss 3.6217628803569824e-05 old loss 3.6592708056559786e-05 BETTER
I0312 12:37:38.191558 1894497 finetune.py:68] layer 23_up @ epoch 0 new loss 3.815752643276937e-05 old loss 3.9709066186333075e-05 BETTER
I0312 12:37:56.324592 1894265 finetune.py:68] layer 21_gate @ epoch 0 new loss 4.470582280191593e-05 old loss 4.543923932942562e-05 BETTER
I0312 12:38:01.211048 1894149 finetune.py:68] layer 20_gate @ epoch 1 new loss 4.2813939217012376e-05 old loss 4.324524343246594e-05 BETTER
I0312 12:38:07.480036 1894381 finetune.py:68] layer 22_up @ epoch 3 new loss 3.5960416425950825e-05 old loss 3.6217628803569824e-05 BETTER
I0312 12:38:07.583409 1894497 finetune.py:68] layer 23_up @ epoch 1 new loss 3.752303746296093e-05 old loss 3.815752643276937e-05 BETTER
I0312 12:38:24.518455 1894265 finetune.py:68] layer 21_gate @ epoch 1 new loss 4.434094444150105e-05 old loss 4.470582280191593e-05 BETTER
I0312 12:38:30.927330 1894149 finetune.py:68] layer 20_gate @ epoch 2 new loss 4.2526193283265457e-05 old loss 4.2813939217012376e-05 BETTER
I0312 12:38:36.977448 1894497 finetune.py:68] layer 23_up @ epoch 2 new loss 3.7167523259995505e-05 old loss 3.752303746296093e-05 BETTER
I0312 12:38:37.265545 1894381 finetune.py:68] layer 22_up @ epoch 4 new loss 3.578801988624036e-05 old loss 3.5960416425950825e-05 BETTER
I0312 12:38:52.163070 1894381 finetune.py:45] layer 22_gate initial loss 5.407588469097391e-05
I0312 12:38:52.670420 1894265 finetune.py:68] layer 21_gate @ epoch 2 new loss 4.409602843225002e-05 old loss 4.434094444150105e-05 BETTER
I0312 12:39:00.671720 1894149 finetune.py:68] layer 20_gate @ epoch 3 new loss 4.232465653331019e-05 old loss 4.2526193283265457e-05 BETTER
I0312 12:39:06.272440 1894497 finetune.py:68] layer 23_up @ epoch 3 new loss 3.692247628350742e-05 old loss 3.7167523259995505e-05 BETTER
I0312 12:39:19.796401 1894381 finetune.py:68] layer 22_gate @ epoch 0 new loss 5.32490957994014e-05 old loss 5.407588469097391e-05 BETTER
I0312 12:39:20.880167 1894265 finetune.py:68] layer 21_gate @ epoch 3 new loss 4.392542177811265e-05 old loss 4.409602843225002e-05 BETTER
I0312 12:39:30.491063 1894149 finetune.py:68] layer 20_gate @ epoch 4 new loss 4.216766683384776e-05 old loss 4.232465653331019e-05 BETTER
I0312 12:39:35.744140 1894497 finetune.py:68] layer 23_up @ epoch 4 new loss 3.6759753129445016e-05 old loss 3.692247628350742e-05 BETTER
I0312 12:39:46.611804 1894149 finetune.py:45] layer 20_down initial loss 7.28124359739013e-05
I0312 12:39:48.135521 1894381 finetune.py:68] layer 22_gate @ epoch 1 new loss 5.2833256631856784e-05 old loss 5.32490957994014e-05 BETTER
I0312 12:39:49.272858 1894265 finetune.py:68] layer 21_gate @ epoch 4 new loss 4.380153040983714e-05 old loss 4.392542177811265e-05 BETTER
I0312 12:39:51.258853 1894497 finetune.py:45] layer 23_gate initial loss 5.708711250917986e-05
I0312 12:40:04.945463 1894265 finetune.py:45] layer 21_down initial loss 7.476715109078214e-05
I0312 12:40:13.850086 1894149 finetune.py:68] layer 20_down @ epoch 0 new loss 7.278683187905699e-05 old loss 7.28124359739013e-05 BETTER
I0312 12:40:16.479238 1894381 finetune.py:68] layer 22_gate @ epoch 2 new loss 5.2562572818715125e-05 old loss 5.2833256631856784e-05 BETTER
I0312 12:40:18.347659 1894497 finetune.py:68] layer 23_gate @ epoch 0 new loss 5.631489693769254e-05 old loss 5.708711250917986e-05 BETTER
I0312 12:40:30.658691 1894265 finetune.py:68] layer 21_down @ epoch 0 new loss 7.475136226275936e-05 old loss 7.476715109078214e-05 BETTER
I0312 12:40:42.271788 1894149 finetune.py:68] layer 20_down @ epoch 1 new loss 7.277957047335804e-05 old loss 7.278683187905699e-05 BETTER
I0312 12:40:44.871830 1894381 finetune.py:68] layer 22_gate @ epoch 3 new loss 5.2367446187417954e-05 old loss 5.2562572818715125e-05 BETTER
I0312 12:40:46.460941 1894497 finetune.py:68] layer 23_gate @ epoch 1 new loss 5.5895987316034734e-05 old loss 5.631489693769254e-05 BETTER
I0312 12:40:57.331213 1894265 finetune.py:68] layer 21_down @ epoch 1 new loss 7.47480517020449e-05 old loss 7.475136226275936e-05 BETTER
I0312 12:41:10.839930 1894149 finetune.py:68] layer 20_down @ epoch 2 new loss 7.277603435795754e-05 old loss 7.277957047335804e-05 BETTER
I0312 12:41:13.358730 1894381 finetune.py:68] layer 22_gate @ epoch 4 new loss 5.2249411965021864e-05 old loss 5.2367446187417954e-05 BETTER
I0312 12:41:14.450735 1894497 finetune.py:68] layer 23_gate @ epoch 2 new loss 5.563184822676703e-05 old loss 5.5895987316034734e-05 BETTER
I0312 12:41:24.023830 1894265 finetune.py:68] layer 21_down @ epoch 2 new loss 7.474607991753146e-05 old loss 7.47480517020449e-05 BETTER
I0312 12:41:29.072987 1894381 finetune.py:45] layer 22_down initial loss 8.726340456632897e-05
I0312 12:41:39.470575 1894149 finetune.py:68] layer 20_down @ epoch 3 new loss 7.277294207597151e-05 old loss 7.277603435795754e-05 BETTER
I0312 12:41:42.483223 1894497 finetune.py:68] layer 23_gate @ epoch 3 new loss 5.5443684686906636e-05 old loss 5.563184822676703e-05 BETTER
I0312 12:41:50.889165 1894265 finetune.py:68] layer 21_down @ epoch 3 new loss 7.474428275600076e-05 old loss 7.474607991753146e-05 BETTER
I0312 12:41:55.299759 1894381 finetune.py:68] layer 22_down @ epoch 0 new loss 8.724704093765467e-05 old loss 8.726340456632897e-05 BETTER
I0312 12:42:07.965363 1894149 finetune.py:68] layer 20_down @ epoch 4 new loss 7.277097756741568e-05 old loss 7.277294207597151e-05 BETTER
20_v proxy err 0.0005401814705692232 tr(WHW.T) 990.5983276367188
20_q proxy err 0.00012852110376115888 tr(WHW.T) 7150.712890625
20_k proxy err 0.00010657675738912076 tr(WHW.T) 10389.515625
20_o proxy err 0.00039872966590337455 tr(WHW.T) 100.24583435058594
20_up proxy err 0.0003749565512407571 tr(WHW.T) 2341.19775390625
20_gate proxy err 0.00024499581195414066 tr(WHW.T) 4024.572265625
20_down proxy err 0.0004565440758597106 tr(WHW.T) 274.96875
I0312 12:42:10.536355 1894497 finetune.py:68] layer 23_gate @ epoch 4 new loss 5.532571958610788e-05 old loss 5.5443684686906636e-05 BETTER
I0312 12:42:18.506402 1894265 finetune.py:68] layer 21_down @ epoch 4 new loss 7.474306767107919e-05 old loss 7.474428275600076e-05 BETTER
21_v proxy err 0.0005218906444497406 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.0001330533850705251 tr(WHW.T) 7064.47998046875
21_k proxy err 0.00010903443035203964 tr(WHW.T) 9978.111328125
21_o proxy err 0.00042506519821472466 tr(WHW.T) 75.49723815917969
21_up proxy err 0.0003903155156876892 tr(WHW.T) 2361.494873046875
21_gate proxy err 0.0002588371280580759 tr(WHW.T) 4002.8427734375
21_down proxy err 0.00045690342085435987 tr(WHW.T) 276.6017761230469
I0312 12:42:22.660996 1894381 finetune.py:68] layer 22_down @ epoch 1 new loss 8.724333747522905e-05 old loss 8.724704093765467e-05 BETTER
I0312 12:42:26.646543 1894497 finetune.py:45] layer 23_down initial loss 9.150306141236797e-05
I0312 12:42:49.533070 1894381 finetune.py:68] layer 22_down @ epoch 2 new loss 8.724084182176739e-05 old loss 8.724333747522905e-05 BETTER
I0312 12:42:51.998128 1894497 finetune.py:68] layer 23_down @ epoch 0 new loss 9.148690878646448e-05 old loss 9.150306141236797e-05 BETTER
I0312 12:43:16.304357 1894381 finetune.py:68] layer 22_down @ epoch 3 new loss 8.723932842258364e-05 old loss 8.724084182176739e-05 BETTER
I0312 12:43:18.273140 1894497 finetune.py:68] layer 23_down @ epoch 1 new loss 9.1483278083615e-05 old loss 9.148690878646448e-05 BETTER
I0312 12:43:35.777214 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 24 in 72.70791125297546s
I0312 12:43:39.064921 1894613 config.py:54] PyTorch version 2.1.1 available.
I0312 12:43:40.129984 1891709 quantize_finetune_llama.py:183] layer 25 gpu 1
I0312 12:43:40.194205 1894613 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 12:43:43.261808 1894381 finetune.py:68] layer 22_down @ epoch 4 new loss 8.723790233489126e-05 old loss 8.723932842258364e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:43:44.642848 1894497 finetune.py:68] layer 23_down @ epoch 2 new loss 9.148156823357567e-05 old loss 9.1483278083615e-05 BETTER
22_v proxy err 0.0005018203519284725 tr(WHW.T) 1243.2529296875
22_q proxy err 0.00013070605928078294 tr(WHW.T) 7746.97900390625
22_k proxy err 0.00011170413199579343 tr(WHW.T) 10606.47265625
22_o proxy err 0.00040263019036501646 tr(WHW.T) 114.30228424072266
22_up proxy err 0.0003934804699383676 tr(WHW.T) 2474.5869140625
22_gate proxy err 0.00026464846450835466 tr(WHW.T) 4155.6494140625
22_down proxy err 0.00045544366003014147 tr(WHW.T) 311.862548828125
I0312 12:43:48.805960 1894613 finetune.py:45] layer 24_v initial loss 1.3936924915469717e-05
I0312 12:44:11.196968 1894497 finetune.py:68] layer 23_down @ epoch 3 new loss 9.148051321972162e-05 old loss 9.148156823357567e-05 BETTER
I0312 12:44:21.838104 1894613 finetune.py:68] layer 24_v @ epoch 0 new loss 8.052618795773014e-06 old loss 1.3936924915469717e-05 BETTER
I0312 12:44:37.711286 1894497 finetune.py:68] layer 23_down @ epoch 4 new loss 9.14796328288503e-05 old loss 9.148051321972162e-05 BETTER
23_v proxy err 0.00046654659672640264 tr(WHW.T) 1486.037353515625
23_q proxy err 0.0001391363621223718 tr(WHW.T) 7346.59033203125
23_k proxy err 0.00011651651584543288 tr(WHW.T) 9983.4248046875
23_o proxy err 0.0004289124917704612 tr(WHW.T) 85.1707534790039
23_up proxy err 0.00040520113543607295 tr(WHW.T) 2533.654052734375
23_gate proxy err 0.00027910462813451886 tr(WHW.T) 4096.6484375
23_down proxy err 0.0004566957941278815 tr(WHW.T) 321.367919921875
I0312 12:44:56.372868 1894613 finetune.py:68] layer 24_v @ epoch 1 new loss 7.539942998846527e-06 old loss 8.052618795773014e-06 BETTER
I0312 12:44:56.566447 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 25 in 69.20119190216064s
I0312 12:44:59.660793 1894729 config.py:54] PyTorch version 2.1.1 available.
I0312 12:45:00.631458 1891709 quantize_finetune_llama.py:183] layer 26 gpu 2
I0312 12:45:00.696569 1894729 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:45:08.917219 1894729 finetune.py:45] layer 25_v initial loss 1.3563794709625654e-05
I0312 12:45:31.120896 1894613 finetune.py:76] layer 24_v @ epoch 2 new loss 7.757983439660165e-06 old loss 7.539942998846527e-06 WORSE
I0312 12:45:40.251804 1894729 finetune.py:68] layer 25_v @ epoch 0 new loss 6.952168860152597e-06 old loss 1.3563794709625654e-05 BETTER
I0312 12:46:05.307320 1894613 finetune.py:68] layer 24_v @ epoch 3 new loss 7.319614724110579e-06 old loss 7.539942998846527e-06 BETTER
I0312 12:46:10.740931 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 26 in 69.70184183120728s
I0312 12:46:12.308701 1894729 finetune.py:68] layer 25_v @ epoch 1 new loss 6.7089822550769895e-06 old loss 6.952168860152597e-06 BETTER
I0312 12:46:14.018617 1894845 config.py:54] PyTorch version 2.1.1 available.
I0312 12:46:15.048932 1891709 quantize_finetune_llama.py:183] layer 27 gpu 3
I0312 12:46:15.113599 1894845 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:46:23.180345 1894845 finetune.py:45] layer 26_v initial loss 1.9132656234432943e-05
I0312 12:46:40.134916 1894613 finetune.py:68] layer 24_v @ epoch 4 new loss 7.117437235137913e-06 old loss 7.319614724110579e-06 BETTER
I0312 12:46:44.682981 1894729 finetune.py:76] layer 25_v @ epoch 2 new loss 6.880354249005904e-06 old loss 6.7089822550769895e-06 WORSE
I0312 12:46:49.638932 1894613 finetune.py:45] layer 24_q initial loss 1.1367633305781055e-05
I0312 12:46:54.643011 1894845 finetune.py:68] layer 26_v @ epoch 0 new loss 1.0560247574176174e-05 old loss 1.9132656234432943e-05 BETTER
I0312 12:47:16.652654 1894729 finetune.py:68] layer 25_v @ epoch 3 new loss 6.591204510186799e-06 old loss 6.7089822550769895e-06 BETTER
I0312 12:47:23.185736 1894613 finetune.py:68] layer 24_q @ epoch 0 new loss 9.129627869697288e-06 old loss 1.1367633305781055e-05 BETTER
I0312 12:47:25.831275 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 27 in 70.3493115901947s
I0312 12:47:27.040688 1894845 finetune.py:68] layer 26_v @ epoch 1 new loss 1.007938953989651e-05 old loss 1.0560247574176174e-05 BETTER
I0312 12:47:29.057186 1894961 config.py:54] PyTorch version 2.1.1 available.
I0312 12:47:30.140465 1891709 quantize_finetune_llama.py:183] layer 28 gpu 0
I0312 12:47:30.216861 1894961 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 12:47:38.420557 1894961 finetune.py:45] layer 27_v initial loss 1.5221974535961635e-05
I0312 12:47:49.203088 1894729 finetune.py:68] layer 25_v @ epoch 4 new loss 6.391131591954036e-06 old loss 6.591204510186799e-06 BETTER
I0312 12:47:57.539999 1894613 finetune.py:68] layer 24_q @ epoch 1 new loss 8.749401786189992e-06 old loss 9.129627869697288e-06 BETTER
I0312 12:47:58.278078 1894729 finetune.py:45] layer 25_q initial loss 9.77336003415985e-06
I0312 12:47:59.334241 1894845 finetune.py:68] layer 26_v @ epoch 2 new loss 1.0000382644648198e-05 old loss 1.007938953989651e-05 BETTER
I0312 12:48:09.273263 1894961 finetune.py:68] layer 27_v @ epoch 0 new loss 9.01336534298025e-06 old loss 1.5221974535961635e-05 BETTER
I0312 12:48:29.803956 1894729 finetune.py:68] layer 25_q @ epoch 0 new loss 8.157359843607992e-06 old loss 9.77336003415985e-06 BETTER
I0312 12:48:32.045063 1894845 finetune.py:68] layer 26_v @ epoch 3 new loss 9.869772839010693e-06 old loss 1.0000382644648198e-05 BETTER
I0312 12:48:32.170381 1894613 finetune.py:68] layer 24_q @ epoch 2 new loss 8.477150913677178e-06 old loss 8.749401786189992e-06 BETTER
I0312 12:48:41.312332 1894961 finetune.py:68] layer 27_v @ epoch 1 new loss 8.87302121554967e-06 old loss 9.01336534298025e-06 BETTER
I0312 12:49:01.961451 1894729 finetune.py:68] layer 25_q @ epoch 1 new loss 7.770641786919441e-06 old loss 8.157359843607992e-06 BETTER
I0312 12:49:04.734411 1894845 finetune.py:68] layer 26_v @ epoch 4 new loss 9.74034355749609e-06 old loss 9.869772839010693e-06 BETTER
I0312 12:49:06.544283 1894613 finetune.py:68] layer 24_q @ epoch 3 new loss 8.332997822435573e-06 old loss 8.477150913677178e-06 BETTER
I0312 12:49:13.403106 1894961 finetune.py:76] layer 27_v @ epoch 2 new loss 9.184635928249918e-06 old loss 8.87302121554967e-06 WORSE
I0312 12:49:13.770622 1894845 finetune.py:45] layer 26_q initial loss 1.5276120393536985e-05
I0312 12:49:34.432265 1894729 finetune.py:68] layer 25_q @ epoch 2 new loss 7.500955234718276e-06 old loss 7.770641786919441e-06 BETTER
I0312 12:49:41.070569 1894613 finetune.py:68] layer 24_q @ epoch 4 new loss 8.229081686295103e-06 old loss 8.332997822435573e-06 BETTER
I0312 12:49:46.186987 1894961 finetune.py:76] layer 27_v @ epoch 3 new loss 8.964937478594948e-06 old loss 8.87302121554967e-06 WORSE
I0312 12:49:46.951653 1894845 finetune.py:68] layer 26_q @ epoch 0 new loss 1.2037099622830283e-05 old loss 1.5276120393536985e-05 BETTER
I0312 12:49:53.018071 1894613 finetune.py:45] layer 24_k initial loss 1.2580071597767528e-05
I0312 12:50:07.298222 1894729 finetune.py:68] layer 25_q @ epoch 3 new loss 7.399952664854936e-06 old loss 7.500955234718276e-06 BETTER
I0312 12:50:18.318984 1894961 finetune.py:68] layer 27_v @ epoch 4 new loss 8.477129995299038e-06 old loss 8.87302121554967e-06 BETTER
I0312 12:50:20.264482 1894845 finetune.py:68] layer 26_q @ epoch 1 new loss 1.1469353012216743e-05 old loss 1.2037099622830283e-05 BETTER
I0312 12:50:27.016162 1894613 finetune.py:68] layer 24_k @ epoch 0 new loss 1.1528420145623386e-05 old loss 1.2580071597767528e-05 BETTER
I0312 12:50:29.914180 1894961 finetune.py:45] layer 27_q initial loss 1.3138340364093892e-05
I0312 12:50:40.296157 1894729 finetune.py:76] layer 25_q @ epoch 4 new loss 7.490477855753852e-06 old loss 7.399952664854936e-06 WORSE
I0312 12:50:51.155089 1894729 finetune.py:45] layer 25_k initial loss 1.0833781743713189e-05
I0312 12:50:53.427882 1894845 finetune.py:68] layer 26_q @ epoch 2 new loss 1.1128446203656495e-05 old loss 1.1469353012216743e-05 BETTER
I0312 12:51:01.350198 1894613 finetune.py:68] layer 24_k @ epoch 1 new loss 1.1242787877563387e-05 old loss 1.1528420145623386e-05 BETTER
I0312 12:51:02.146549 1894961 finetune.py:68] layer 27_q @ epoch 0 new loss 1.053929008776322e-05 old loss 1.3138340364093892e-05 BETTER
I0312 12:51:22.694790 1894729 finetune.py:68] layer 25_k @ epoch 0 new loss 1.0264195225317962e-05 old loss 1.0833781743713189e-05 BETTER
I0312 12:51:26.469261 1894845 finetune.py:68] layer 26_q @ epoch 3 new loss 1.0851877959794365e-05 old loss 1.1128446203656495e-05 BETTER
I0312 12:51:34.979321 1894961 finetune.py:68] layer 27_q @ epoch 1 new loss 1.0089934221468866e-05 old loss 1.053929008776322e-05 BETTER
I0312 12:51:35.568981 1894613 finetune.py:68] layer 24_k @ epoch 2 new loss 1.1081559023295995e-05 old loss 1.1242787877563387e-05 BETTER
I0312 12:51:55.001284 1894729 finetune.py:68] layer 25_k @ epoch 1 new loss 1.0131874660146423e-05 old loss 1.0264195225317962e-05 BETTER
I0312 12:51:59.501466 1894845 finetune.py:68] layer 26_q @ epoch 4 new loss 1.0724717867560685e-05 old loss 1.0851877959794365e-05 BETTER
I0312 12:52:07.280991 1894961 finetune.py:68] layer 27_q @ epoch 2 new loss 9.842461622611154e-06 old loss 1.0089934221468866e-05 BETTER
I0312 12:52:08.847495 1894845 finetune.py:45] layer 26_k initial loss 1.635470471228473e-05
I0312 12:52:09.421776 1894613 finetune.py:68] layer 24_k @ epoch 3 new loss 1.1010795788024552e-05 old loss 1.1081559023295995e-05 BETTER
I0312 12:52:27.316244 1894729 finetune.py:68] layer 25_k @ epoch 2 new loss 1.0059344276669435e-05 old loss 1.0131874660146423e-05 BETTER
I0312 12:52:39.620391 1894961 finetune.py:68] layer 27_q @ epoch 3 new loss 9.41762209549779e-06 old loss 9.842461622611154e-06 BETTER
I0312 12:52:40.487161 1894845 finetune.py:68] layer 26_k @ epoch 0 new loss 1.543003600090742e-05 old loss 1.635470471228473e-05 BETTER
I0312 12:52:43.033042 1894613 finetune.py:76] layer 24_k @ epoch 4 new loss 1.1047004591091536e-05 old loss 1.1010795788024552e-05 WORSE
I0312 12:52:51.816213 1894613 finetune.py:45] layer 24_o initial loss 2.2983824237599038e-05
I0312 12:52:59.408453 1894729 finetune.py:68] layer 25_k @ epoch 3 new loss 9.922077879309654e-06 old loss 1.0059344276669435e-05 BETTER
I0312 12:53:11.790736 1894961 finetune.py:68] layer 27_q @ epoch 4 new loss 9.273422620026395e-06 old loss 9.41762209549779e-06 BETTER
I0312 12:53:12.639556 1894845 finetune.py:68] layer 26_k @ epoch 1 new loss 1.5084650840435643e-05 old loss 1.543003600090742e-05 BETTER
I0312 12:53:20.935602 1894961 finetune.py:45] layer 27_k initial loss 1.4131569514574949e-05
I0312 12:53:24.292667 1894613 finetune.py:68] layer 24_o @ epoch 0 new loss 2.1214687876636162e-05 old loss 2.2983824237599038e-05 BETTER
I0312 12:53:31.518145 1894729 finetune.py:68] layer 25_k @ epoch 4 new loss 9.919455806084443e-06 old loss 9.922077879309654e-06 BETTER
I0312 12:53:40.633697 1894729 finetune.py:45] layer 25_o initial loss 1.8841859855456278e-05
I0312 12:53:44.649345 1894845 finetune.py:68] layer 26_k @ epoch 2 new loss 1.492573574068956e-05 old loss 1.5084650840435643e-05 BETTER
I0312 12:53:51.799776 1894961 finetune.py:68] layer 27_k @ epoch 0 new loss 1.3241788110462949e-05 old loss 1.4131569514574949e-05 BETTER
I0312 12:53:57.442419 1894613 finetune.py:68] layer 24_o @ epoch 1 new loss 2.084262450807728e-05 old loss 2.1214687876636162e-05 BETTER
I0312 12:54:11.206175 1894729 finetune.py:68] layer 25_o @ epoch 0 new loss 1.8027343685389496e-05 old loss 1.8841859855456278e-05 BETTER
I0312 12:54:16.662075 1894845 finetune.py:68] layer 26_k @ epoch 3 new loss 1.4822478078713175e-05 old loss 1.492573574068956e-05 BETTER
I0312 12:54:23.299591 1894961 finetune.py:68] layer 27_k @ epoch 1 new loss 1.2997445992368739e-05 old loss 1.3241788110462949e-05 BETTER
I0312 12:54:30.712755 1894613 finetune.py:68] layer 24_o @ epoch 2 new loss 2.071329072350636e-05 old loss 2.084262450807728e-05 BETTER
I0312 12:54:42.495578 1894729 finetune.py:68] layer 25_o @ epoch 1 new loss 1.7780486814444885e-05 old loss 1.8027343685389496e-05 BETTER
I0312 12:54:48.688739 1894845 finetune.py:68] layer 26_k @ epoch 4 new loss 1.4784599443373736e-05 old loss 1.4822478078713175e-05 BETTER
I0312 12:54:54.941086 1894961 finetune.py:68] layer 27_k @ epoch 2 new loss 1.2869407328253146e-05 old loss 1.2997445992368739e-05 BETTER
I0312 12:54:57.998726 1894845 finetune.py:45] layer 26_o initial loss 3.058970833080821e-05
I0312 12:55:04.205865 1894613 finetune.py:68] layer 24_o @ epoch 3 new loss 2.0622061128960922e-05 old loss 2.071329072350636e-05 BETTER
I0312 12:55:13.886202 1894729 finetune.py:68] layer 25_o @ epoch 2 new loss 1.76709199877223e-05 old loss 1.7780486814444885e-05 BETTER
I0312 12:55:26.697196 1894961 finetune.py:68] layer 27_k @ epoch 3 new loss 1.2822283679270186e-05 old loss 1.2869407328253146e-05 BETTER
I0312 12:55:28.798242 1894845 finetune.py:68] layer 26_o @ epoch 0 new loss 2.726061393332202e-05 old loss 3.058970833080821e-05 BETTER
I0312 12:55:37.317548 1894613 finetune.py:68] layer 24_o @ epoch 4 new loss 2.0572384528350085e-05 old loss 2.0622061128960922e-05 BETTER
I0312 12:55:45.335353 1894729 finetune.py:68] layer 25_o @ epoch 3 new loss 1.7643729734118097e-05 old loss 1.76709199877223e-05 BETTER
I0312 12:55:52.466182 1894613 finetune.py:45] layer 24_up initial loss 4.564108166960068e-05
I0312 12:55:58.341525 1894961 finetune.py:68] layer 27_k @ epoch 4 new loss 1.2644605703826528e-05 old loss 1.2822283679270186e-05 BETTER
I0312 12:56:00.267979 1894845 finetune.py:68] layer 26_o @ epoch 1 new loss 2.679515273484867e-05 old loss 2.726061393332202e-05 BETTER
I0312 12:56:07.556961 1894961 finetune.py:45] layer 27_o initial loss 2.5529587219352834e-05
I0312 12:56:16.567338 1894729 finetune.py:68] layer 25_o @ epoch 4 new loss 1.7619404388824478e-05 old loss 1.7643729734118097e-05 BETTER
I0312 12:56:22.995903 1894613 finetune.py:68] layer 24_up @ epoch 0 new loss 4.410188557812944e-05 old loss 4.564108166960068e-05 BETTER
I0312 12:56:31.451443 1894729 finetune.py:45] layer 25_up initial loss 4.567908763419837e-05
I0312 12:56:31.739045 1894845 finetune.py:68] layer 26_o @ epoch 2 new loss 2.657251025084406e-05 old loss 2.679515273484867e-05 BETTER
I0312 12:56:37.649607 1894961 finetune.py:68] layer 27_o @ epoch 0 new loss 2.356564800720662e-05 old loss 2.5529587219352834e-05 BETTER
I0312 12:56:54.536678 1894613 finetune.py:68] layer 24_up @ epoch 1 new loss 4.351237657829188e-05 old loss 4.410188557812944e-05 BETTER
I0312 12:57:00.312050 1894729 finetune.py:68] layer 25_up @ epoch 0 new loss 4.389027526485734e-05 old loss 4.567908763419837e-05 BETTER
I0312 12:57:03.395676 1894845 finetune.py:68] layer 26_o @ epoch 3 new loss 2.6451441954122856e-05 old loss 2.657251025084406e-05 BETTER
I0312 12:57:08.514591 1894961 finetune.py:68] layer 27_o @ epoch 1 new loss 2.311566458956804e-05 old loss 2.356564800720662e-05 BETTER
I0312 12:57:26.198587 1894613 finetune.py:68] layer 24_up @ epoch 2 new loss 4.316409831517376e-05 old loss 4.351237657829188e-05 BETTER
I0312 12:57:29.953109 1894729 finetune.py:68] layer 25_up @ epoch 1 new loss 4.327365240897052e-05 old loss 4.389027526485734e-05 BETTER
I0312 12:57:35.078794 1894845 finetune.py:68] layer 26_o @ epoch 4 new loss 2.6294927010894753e-05 old loss 2.6451441954122856e-05 BETTER
I0312 12:57:39.470600 1894961 finetune.py:68] layer 27_o @ epoch 2 new loss 2.298446088389028e-05 old loss 2.311566458956804e-05 BETTER
I0312 12:57:50.510431 1894845 finetune.py:45] layer 26_up initial loss 5.727687675971538e-05
I0312 12:57:57.995828 1894613 finetune.py:68] layer 24_up @ epoch 3 new loss 4.2939504055539146e-05 old loss 4.316409831517376e-05 BETTER
I0312 12:57:59.735335 1894729 finetune.py:68] layer 25_up @ epoch 2 new loss 4.289778007660061e-05 old loss 4.327365240897052e-05 BETTER
I0312 12:58:10.581077 1894961 finetune.py:68] layer 27_o @ epoch 3 new loss 2.2875376089359634e-05 old loss 2.298446088389028e-05 BETTER
I0312 12:58:19.542804 1894845 finetune.py:68] layer 26_up @ epoch 0 new loss 5.5291762691922486e-05 old loss 5.727687675971538e-05 BETTER
I0312 12:58:29.721497 1894613 finetune.py:68] layer 24_up @ epoch 4 new loss 4.278302367310971e-05 old loss 4.2939504055539146e-05 BETTER
I0312 12:58:29.792201 1894729 finetune.py:68] layer 25_up @ epoch 3 new loss 4.266399992047809e-05 old loss 4.289778007660061e-05 BETTER
I0312 12:58:41.629753 1894961 finetune.py:68] layer 27_o @ epoch 4 new loss 2.2854688722873107e-05 old loss 2.2875376089359634e-05 BETTER
I0312 12:58:45.042194 1894613 finetune.py:45] layer 24_gate initial loss 6.534143176395446e-05
I0312 12:58:49.276036 1894845 finetune.py:68] layer 26_up @ epoch 1 new loss 5.4578136769123375e-05 old loss 5.5291762691922486e-05 BETTER
I0312 12:58:56.685072 1894961 finetune.py:45] layer 27_up initial loss 5.867694926564582e-05
I0312 12:58:59.557063 1894729 finetune.py:68] layer 25_up @ epoch 4 new loss 4.250075653544627e-05 old loss 4.266399992047809e-05 BETTER
I0312 12:59:14.165544 1894613 finetune.py:68] layer 24_gate @ epoch 0 new loss 6.456823757616803e-05 old loss 6.534143176395446e-05 BETTER
I0312 12:59:14.562928 1894729 finetune.py:45] layer 25_gate initial loss 6.774380744900554e-05
I0312 12:59:19.206503 1894845 finetune.py:68] layer 26_up @ epoch 2 new loss 5.416172280092724e-05 old loss 5.4578136769123375e-05 BETTER
I0312 12:59:25.013691 1894961 finetune.py:68] layer 27_up @ epoch 0 new loss 5.591449735220522e-05 old loss 5.867694926564582e-05 BETTER
I0312 12:59:42.140798 1894729 finetune.py:68] layer 25_gate @ epoch 0 new loss 6.68881184537895e-05 old loss 6.774380744900554e-05 BETTER
I0312 12:59:44.114026 1894613 finetune.py:68] layer 24_gate @ epoch 1 new loss 6.41382357571274e-05 old loss 6.456823757616803e-05 BETTER
I0312 12:59:49.236760 1894845 finetune.py:68] layer 26_up @ epoch 3 new loss 5.389245052356273e-05 old loss 5.416172280092724e-05 BETTER
I0312 12:59:54.424739 1894961 finetune.py:68] layer 27_up @ epoch 1 new loss 5.4931653721723706e-05 old loss 5.591449735220522e-05 BETTER
I0312 13:00:10.434248 1894729 finetune.py:68] layer 25_gate @ epoch 1 new loss 6.644923269050196e-05 old loss 6.68881184537895e-05 BETTER
I0312 13:00:14.096000 1894613 finetune.py:68] layer 24_gate @ epoch 2 new loss 6.386872701114044e-05 old loss 6.41382357571274e-05 BETTER
I0312 13:00:19.047981 1894845 finetune.py:68] layer 26_up @ epoch 4 new loss 5.371122824726626e-05 old loss 5.389245052356273e-05 BETTER
I0312 13:00:23.819840 1894961 finetune.py:68] layer 27_up @ epoch 2 new loss 5.439764208858833e-05 old loss 5.4931653721723706e-05 BETTER
I0312 13:00:34.277524 1894845 finetune.py:45] layer 26_gate initial loss 8.242881449405104e-05
I0312 13:00:38.725649 1894729 finetune.py:68] layer 25_gate @ epoch 2 new loss 6.617182225454599e-05 old loss 6.644923269050196e-05 BETTER
I0312 13:00:45.168723 1894613 finetune.py:68] layer 24_gate @ epoch 3 new loss 6.370150367729366e-05 old loss 6.386872701114044e-05 BETTER
I0312 13:00:54.683129 1894961 finetune.py:68] layer 27_up @ epoch 3 new loss 5.405745832831599e-05 old loss 5.439764208858833e-05 BETTER
I0312 13:01:02.092149 1894845 finetune.py:68] layer 26_gate @ epoch 0 new loss 8.141606667777523e-05 old loss 8.242881449405104e-05 BETTER
I0312 13:01:07.055570 1894729 finetune.py:68] layer 25_gate @ epoch 3 new loss 6.598675099667162e-05 old loss 6.617182225454599e-05 BETTER
I0312 13:01:15.326015 1894613 finetune.py:68] layer 24_gate @ epoch 4 new loss 6.356858648359776e-05 old loss 6.370150367729366e-05 BETTER
I0312 13:01:24.546353 1894961 finetune.py:68] layer 27_up @ epoch 4 new loss 5.384404357755557e-05 old loss 5.405745832831599e-05 BETTER
I0312 13:01:31.137158 1894845 finetune.py:68] layer 26_gate @ epoch 1 new loss 8.089717448456213e-05 old loss 8.141606667777523e-05 BETTER
I0312 13:01:33.341545 1894613 finetune.py:45] layer 24_down initial loss 0.00010212744382442906
I0312 13:01:35.902642 1894729 finetune.py:68] layer 25_gate @ epoch 4 new loss 6.586123345186934e-05 old loss 6.598675099667162e-05 BETTER
I0312 13:01:43.683443 1894961 finetune.py:45] layer 27_gate initial loss 8.746092498768121e-05
I0312 13:01:54.864474 1894729 finetune.py:45] layer 25_down initial loss 0.00010757379641290754
I0312 13:02:01.416904 1894845 finetune.py:68] layer 26_gate @ epoch 2 new loss 8.058049570536241e-05 old loss 8.089717448456213e-05 BETTER
I0312 13:02:02.070013 1894613 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0001021070929709822 old loss 0.00010212744382442906 BETTER
I0312 13:02:11.479745 1894961 finetune.py:68] layer 27_gate @ epoch 0 new loss 8.6018699221313e-05 old loss 8.746092498768121e-05 BETTER
I0312 13:02:20.905112 1894729 finetune.py:68] layer 25_down @ epoch 0 new loss 0.00010749292414402589 old loss 0.00010757379641290754 BETTER
I0312 13:02:31.806542 1894845 finetune.py:68] layer 26_gate @ epoch 3 new loss 8.03568764240481e-05 old loss 8.058049570536241e-05 BETTER
I0312 13:02:32.003363 1894613 finetune.py:68] layer 24_down @ epoch 1 new loss 0.00010210119216935709 old loss 0.0001021070929709822 BETTER
I0312 13:02:39.673525 1894961 finetune.py:68] layer 27_gate @ epoch 1 new loss 8.535356028005481e-05 old loss 8.6018699221313e-05 BETTER
I0312 13:02:47.883921 1894729 finetune.py:68] layer 25_down @ epoch 1 new loss 0.00010747028863988817 old loss 0.00010749292414402589 BETTER
I0312 13:03:00.534837 1894845 finetune.py:68] layer 26_gate @ epoch 4 new loss 8.021944086067379e-05 old loss 8.03568764240481e-05 BETTER
I0312 13:03:00.728101 1894613 finetune.py:68] layer 24_down @ epoch 2 new loss 0.000102098478237167 old loss 0.00010210119216935709 BETTER
I0312 13:03:07.778433 1894961 finetune.py:68] layer 27_gate @ epoch 2 new loss 8.495022484567016e-05 old loss 8.535356028005481e-05 BETTER
I0312 13:03:14.717035 1894729 finetune.py:68] layer 25_down @ epoch 2 new loss 0.00010745944746304303 old loss 0.00010747028863988817 BETTER
I0312 13:03:16.468770 1894845 finetune.py:45] layer 26_down initial loss 0.0001258020638488233
I0312 13:03:29.228590 1894613 finetune.py:68] layer 24_down @ epoch 3 new loss 0.00010209586616838351 old loss 0.000102098478237167 BETTER
I0312 13:03:35.898842 1894961 finetune.py:68] layer 27_gate @ epoch 3 new loss 8.466264262096956e-05 old loss 8.495022484567016e-05 BETTER
I0312 13:03:41.787672 1894729 finetune.py:68] layer 25_down @ epoch 3 new loss 0.00010745403415057808 old loss 0.00010745944746304303 BETTER
I0312 13:03:42.623258 1894845 finetune.py:68] layer 26_down @ epoch 0 new loss 0.00012577621964737773 old loss 0.0001258020638488233 BETTER
I0312 13:03:57.724629 1894613 finetune.py:68] layer 24_down @ epoch 4 new loss 0.00010209270112682134 old loss 0.00010209586616838351 BETTER
24_v proxy err 0.0004918082850053906 tr(WHW.T) 1394.900634765625
24_q proxy err 0.00014702140470035374 tr(WHW.T) 7020.138671875
24_k proxy err 0.0001189950926345773 tr(WHW.T) 10326.294921875
24_o proxy err 0.00035066771670244634 tr(WHW.T) 133.91001892089844
24_up proxy err 0.00041001284262165427 tr(WHW.T) 2621.7412109375
24_gate proxy err 0.0002818434440996498 tr(WHW.T) 4262.0400390625
24_down proxy err 0.0004572851466946304 tr(WHW.T) 340.2356872558594
I0312 13:04:04.610910 1894961 finetune.py:68] layer 27_gate @ epoch 4 new loss 8.448288281215355e-05 old loss 8.466264262096956e-05 BETTER
I0312 13:04:09.144423 1894729 finetune.py:68] layer 25_down @ epoch 4 new loss 0.00010745001054601744 old loss 0.00010745403415057808 BETTER
I0312 13:04:09.815573 1894845 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0001257705589523539 old loss 0.00012577621964737773 BETTER
25_v proxy err 0.00045466350275091827 tr(WHW.T) 1707.664794921875
25_q proxy err 0.00015162533964030445 tr(WHW.T) 7161.671875
25_k proxy err 0.0001245607272721827 tr(WHW.T) 9614.189453125
25_o proxy err 0.000413176923757419 tr(WHW.T) 83.4935531616211
25_up proxy err 0.0004066915425937623 tr(WHW.T) 2805.962158203125
25_gate proxy err 0.0002748735132627189 tr(WHW.T) 4666.0517578125
25_down proxy err 0.0004541135858744383 tr(WHW.T) 373.5145263671875
I0312 13:04:20.270775 1894961 finetune.py:45] layer 27_down initial loss 0.00013775884872302413
I0312 13:04:36.588045 1894845 finetune.py:68] layer 26_down @ epoch 2 new loss 0.00012576670269481838 old loss 0.0001257705589523539 BETTER
I0312 13:04:45.672791 1894961 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0001377356966258958 old loss 0.00013775884872302413 BETTER
I0312 13:05:03.502618 1894845 finetune.py:68] layer 26_down @ epoch 3 new loss 0.00012576337030623108 old loss 0.00012576670269481838 BETTER
I0312 13:05:11.981162 1894961 finetune.py:68] layer 27_down @ epoch 1 new loss 0.00013772638340014964 old loss 0.0001377356966258958 BETTER
I0312 13:05:25.593098 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 28 in 72.2254102230072s
I0312 13:05:28.906584 1895077 config.py:54] PyTorch version 2.1.1 available.
I0312 13:05:29.994154 1891709 quantize_finetune_llama.py:183] layer 29 gpu 1
I0312 13:05:30.077189 1895077 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 13:05:30.389569 1894845 finetune.py:68] layer 26_down @ epoch 4 new loss 0.00012575945584103465 old loss 0.00012576337030623108 BETTER
26_v proxy err 0.0004537679778877646 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.0001468725095037371 tr(WHW.T) 7469.7392578125
26_k proxy err 0.00012324632552918047 tr(WHW.T) 10490.9296875
26_o proxy err 0.00030433348729275167 tr(WHW.T) 202.6919403076172
26_up proxy err 0.000385756982723251 tr(WHW.T) 3154.932861328125
26_gate proxy err 0.00025792495580390096 tr(WHW.T) 5303.18408203125
26_down proxy err 0.00046091320109553635 tr(WHW.T) 401.2810974121094
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 13:05:38.349008 1894961 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0001377203589072451 old loss 0.00013772638340014964 BETTER
I0312 13:05:38.601357 1895077 finetune.py:45] layer 28_v initial loss 1.9751358195208013e-05
I0312 13:06:04.512384 1894961 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0001377162552671507 old loss 0.0001377203589072451 BETTER
I0312 13:06:11.590479 1895077 finetune.py:68] layer 28_v @ epoch 0 new loss 1.1975199413427617e-05 old loss 1.9751358195208013e-05 BETTER
I0312 13:06:30.802692 1894961 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0001377128210151568 old loss 0.0001377162552671507 BETTER
27_v proxy err 0.00042512553045526147 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.00014564517186954618 tr(WHW.T) 7691.3837890625
27_k proxy err 0.00012050193618051708 tr(WHW.T) 10619.62890625
27_o proxy err 0.0003839550481643528 tr(WHW.T) 126.14488983154297
27_up proxy err 0.00035728837247006595 tr(WHW.T) 3691.796630859375
27_gate proxy err 0.0002474446955602616 tr(WHW.T) 5990.2158203125
27_down proxy err 0.0004643282445613295 tr(WHW.T) 467.021484375
I0312 13:06:44.243385 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 29 in 69.9738130569458s
I0312 13:06:45.866507 1895077 finetune.py:68] layer 28_v @ epoch 1 new loss 1.1536410056578461e-05 old loss 1.1975199413427617e-05 BETTER
I0312 13:06:47.342798 1895193 config.py:54] PyTorch version 2.1.1 available.
I0312 13:06:48.333806 1891709 quantize_finetune_llama.py:183] layer 30 gpu 2
I0312 13:06:48.408995 1895193 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 13:06:56.630214 1895193 finetune.py:45] layer 29_v initial loss 1.97135959751904e-05
I0312 13:07:20.375136 1895077 finetune.py:68] layer 28_v @ epoch 2 new loss 1.1518975043145474e-05 old loss 1.1536410056578461e-05 BETTER
I0312 13:07:27.768706 1895193 finetune.py:68] layer 29_v @ epoch 0 new loss 1.323615651926957e-05 old loss 1.97135959751904e-05 BETTER
I0312 13:07:55.104063 1895077 finetune.py:68] layer 28_v @ epoch 3 new loss 1.1259036909905262e-05 old loss 1.1518975043145474e-05 BETTER
I0312 13:07:58.727411 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 30 in 70.00622391700745s
I0312 13:07:59.744439 1895193 finetune.py:68] layer 29_v @ epoch 1 new loss 1.2961672837263905e-05 old loss 1.323615651926957e-05 BETTER
I0312 13:08:01.903675 1895309 config.py:54] PyTorch version 2.1.1 available.
I0312 13:08:02.922805 1891709 quantize_finetune_llama.py:183] layer 31 gpu 3
I0312 13:08:03.001578 1895309 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 13:08:11.206122 1895309 finetune.py:45] layer 30_v initial loss 2.0857005438301712e-05
I0312 13:08:29.952157 1895077 finetune.py:68] layer 28_v @ epoch 4 new loss 1.0565049706201535e-05 old loss 1.1259036909905262e-05 BETTER
I0312 13:08:32.005898 1895193 finetune.py:76] layer 29_v @ epoch 2 new loss 1.3617950571642723e-05 old loss 1.2961672837263905e-05 WORSE
I0312 13:08:39.519322 1895077 finetune.py:45] layer 28_q initial loss 1.72492418641923e-05
I0312 13:08:42.647394 1895309 finetune.py:68] layer 30_v @ epoch 0 new loss 1.4034988453204278e-05 old loss 2.0857005438301712e-05 BETTER
I0312 13:09:03.768910 1895193 finetune.py:76] layer 29_v @ epoch 3 new loss 1.4082875168242026e-05 old loss 1.2961672837263905e-05 WORSE
I0312 13:09:12.990961 1895077 finetune.py:68] layer 28_q @ epoch 0 new loss 1.3782070709567051e-05 old loss 1.72492418641923e-05 BETTER
I0312 13:09:14.999343 1895309 finetune.py:68] layer 30_v @ epoch 1 new loss 1.3909423614677507e-05 old loss 1.4034988453204278e-05 BETTER
I0312 13:09:15.126421 1891709 quantize_finetune_llama.py:210] computed original embedding for layer 31 in 71.73623895645142s
I0312 13:09:18.358235 1895425 config.py:54] PyTorch version 2.1.1 available.
I0312 13:09:19.530068 1895425 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 13:09:27.500350 1895425 finetune.py:45] layer 31_v initial loss 5.885722930543125e-05
I0312 13:09:35.956638 1895193 finetune.py:76] layer 29_v @ epoch 4 new loss 1.3307813787832856e-05 old loss 1.2961672837263905e-05 WORSE
I0312 13:09:44.574164 1895193 finetune.py:45] layer 29_q initial loss 1.8849359548767097e-05
I0312 13:09:47.388609 1895077 finetune.py:68] layer 28_q @ epoch 1 new loss 1.3015178410569206e-05 old loss 1.3782070709567051e-05 BETTER
I0312 13:09:47.810921 1895309 finetune.py:76] layer 30_v @ epoch 2 new loss 1.7194530300912447e-05 old loss 1.3909423614677507e-05 WORSE
I0312 13:09:58.784106 1895425 finetune.py:68] layer 31_v @ epoch 0 new loss 3.098022716585547e-05 old loss 5.885722930543125e-05 BETTER
I0312 13:10:16.166706 1895193 finetune.py:68] layer 29_q @ epoch 0 new loss 1.588015948073007e-05 old loss 1.8849359548767097e-05 BETTER
I0312 13:10:20.162161 1895309 finetune.py:76] layer 30_v @ epoch 3 new loss 1.45732801684062e-05 old loss 1.3909423614677507e-05 WORSE
I0312 13:10:22.076119 1895077 finetune.py:68] layer 28_q @ epoch 2 new loss 1.2745489584631287e-05 old loss 1.3015178410569206e-05 BETTER
I0312 13:10:30.689732 1895425 finetune.py:76] layer 31_v @ epoch 1 new loss 4.305890979594551e-05 old loss 3.098022716585547e-05 WORSE
I0312 13:10:48.477197 1895193 finetune.py:68] layer 29_q @ epoch 1 new loss 1.482448169554118e-05 old loss 1.588015948073007e-05 BETTER
I0312 13:10:52.786476 1895309 finetune.py:76] layer 30_v @ epoch 4 new loss 1.4919075510988478e-05 old loss 1.3909423614677507e-05 WORSE
I0312 13:10:56.614943 1895077 finetune.py:68] layer 28_q @ epoch 3 new loss 1.2528417755675036e-05 old loss 1.2745489584631287e-05 BETTER
I0312 13:11:01.528538 1895309 finetune.py:45] layer 30_q initial loss 2.2404543415177613e-05
I0312 13:11:02.179597 1895425 finetune.py:76] layer 31_v @ epoch 2 new loss 4.255021485732868e-05 old loss 3.098022716585547e-05 WORSE
I0312 13:11:20.786767 1895193 finetune.py:68] layer 29_q @ epoch 2 new loss 1.436486763850553e-05 old loss 1.482448169554118e-05 BETTER
I0312 13:11:31.122447 1895077 finetune.py:68] layer 28_q @ epoch 4 new loss 1.220954982272815e-05 old loss 1.2528417755675036e-05 BETTER
I0312 13:11:33.360343 1895309 finetune.py:68] layer 30_q @ epoch 0 new loss 1.7017040590872057e-05 old loss 2.2404543415177613e-05 BETTER
I0312 13:11:34.114923 1895425 finetune.py:76] layer 31_v @ epoch 3 new loss 5.237432196736336e-05 old loss 3.098022716585547e-05 WORSE
I0312 13:11:40.634844 1895077 finetune.py:45] layer 28_k initial loss 1.7972346540773287e-05
I0312 13:11:54.127315 1895193 finetune.py:68] layer 29_q @ epoch 3 new loss 1.4131839634501375e-05 old loss 1.436486763850553e-05 BETTER
I0312 13:12:06.740712 1895309 finetune.py:68] layer 30_q @ epoch 1 new loss 1.5445119061041623e-05 old loss 1.7017040590872057e-05 BETTER
I0312 13:12:07.613707 1895425 finetune.py:76] layer 31_v @ epoch 4 new loss 4.610220275935717e-05 old loss 3.098022716585547e-05 WORSE
I0312 13:12:13.876558 1895077 finetune.py:68] layer 28_k @ epoch 0 new loss 1.6992242308333516e-05 old loss 1.7972346540773287e-05 BETTER
I0312 13:12:17.231198 1895425 finetune.py:45] layer 31_q initial loss 6.985842628637329e-05
I0312 13:12:26.691129 1895193 finetune.py:68] layer 29_q @ epoch 4 new loss 1.3937335097580217e-05 old loss 1.4131839634501375e-05 BETTER
I0312 13:12:38.150306 1895193 finetune.py:45] layer 29_k initial loss 2.001052962441463e-05
I0312 13:12:40.191753 1895309 finetune.py:68] layer 30_q @ epoch 2 new loss 1.4970189113228116e-05 old loss 1.5445119061041623e-05 BETTER
I0312 13:12:48.664668 1895077 finetune.py:68] layer 28_k @ epoch 1 new loss 1.6777657947386615e-05 old loss 1.6992242308333516e-05 BETTER
I0312 13:12:49.999938 1895425 finetune.py:68] layer 31_q @ epoch 0 new loss 4.921244180877693e-05 old loss 6.985842628637329e-05 BETTER
I0312 13:13:09.671119 1895193 finetune.py:68] layer 29_k @ epoch 0 new loss 1.8966782590723597e-05 old loss 2.001052962441463e-05 BETTER
I0312 13:13:13.221392 1895309 finetune.py:76] layer 30_q @ epoch 3 new loss 1.5071762391016819e-05 old loss 1.4970189113228116e-05 WORSE
I0312 13:13:23.071806 1895077 finetune.py:68] layer 28_k @ epoch 2 new loss 1.6578900613239966e-05 old loss 1.6777657947386615e-05 BETTER
I0312 13:13:23.163291 1895425 finetune.py:68] layer 31_q @ epoch 1 new loss 4.649012771551497e-05 old loss 4.921244180877693e-05 BETTER
I0312 13:13:41.863675 1895193 finetune.py:68] layer 29_k @ epoch 1 new loss 1.855600748967845e-05 old loss 1.8966782590723597e-05 BETTER
I0312 13:13:45.121306 1895309 finetune.py:68] layer 30_q @ epoch 4 new loss 1.466383309889352e-05 old loss 1.4970189113228116e-05 BETTER
I0312 13:13:54.317210 1895309 finetune.py:45] layer 30_k initial loss 2.211774881288875e-05
I0312 13:13:55.569889 1895425 finetune.py:76] layer 31_q @ epoch 2 new loss 5.125652751303278e-05 old loss 4.649012771551497e-05 WORSE
I0312 13:13:56.752872 1895077 finetune.py:76] layer 28_k @ epoch 3 new loss 1.65789697348373e-05 old loss 1.6578900613239966e-05 WORSE
I0312 13:14:13.954156 1895193 finetune.py:68] layer 29_k @ epoch 2 new loss 1.840202276071068e-05 old loss 1.855600748967845e-05 BETTER
I0312 13:14:25.786331 1895309 finetune.py:68] layer 30_k @ epoch 0 new loss 2.0425986804184504e-05 old loss 2.211774881288875e-05 BETTER
I0312 13:14:27.028035 1895425 finetune.py:68] layer 31_q @ epoch 3 new loss 3.9914113585837185e-05 old loss 4.649012771551497e-05 BETTER
I0312 13:14:29.787744 1895077 finetune.py:68] layer 28_k @ epoch 4 new loss 1.6547208360861987e-05 old loss 1.6578900613239966e-05 BETTER
I0312 13:14:38.947466 1895077 finetune.py:45] layer 28_o initial loss 3.3067459298763424e-05
I0312 13:14:45.964545 1895193 finetune.py:68] layer 29_k @ epoch 3 new loss 1.8290556909050792e-05 old loss 1.840202276071068e-05 BETTER
I0312 13:14:57.998172 1895309 finetune.py:68] layer 30_k @ epoch 1 new loss 1.978744512598496e-05 old loss 2.0425986804184504e-05 BETTER
I0312 13:14:59.112843 1895425 finetune.py:76] layer 31_q @ epoch 4 new loss 4.0540071495343e-05 old loss 3.9914113585837185e-05 WORSE
I0312 13:15:07.462928 1895425 finetune.py:45] layer 31_k initial loss 8.67352937348187e-05
I0312 13:15:11.531146 1895077 finetune.py:68] layer 28_o @ epoch 0 new loss 3.066620047320612e-05 old loss 3.3067459298763424e-05 BETTER
I0312 13:15:17.954827 1895193 finetune.py:76] layer 29_k @ epoch 4 new loss 1.842314304667525e-05 old loss 1.8290556909050792e-05 WORSE
I0312 13:15:26.430324 1895193 finetune.py:45] layer 29_o initial loss 3.4541844797786325e-05
I0312 13:15:30.026635 1895309 finetune.py:76] layer 30_k @ epoch 2 new loss 2.02262781385798e-05 old loss 1.978744512598496e-05 WORSE
I0312 13:15:38.569865 1895425 finetune.py:68] layer 31_k @ epoch 0 new loss 5.281188714434393e-05 old loss 8.67352937348187e-05 BETTER
I0312 13:15:44.836824 1895077 finetune.py:68] layer 28_o @ epoch 1 new loss 2.98761351587018e-05 old loss 3.066620047320612e-05 BETTER
I0312 13:15:57.102074 1895193 finetune.py:68] layer 29_o @ epoch 0 new loss 3.191491850884631e-05 old loss 3.4541844797786325e-05 BETTER
I0312 13:16:01.457758 1895309 finetune.py:76] layer 30_k @ epoch 3 new loss 1.983893889700994e-05 old loss 1.978744512598496e-05 WORSE
I0312 13:16:10.331324 1895425 finetune.py:68] layer 31_k @ epoch 1 new loss 4.5593216782435775e-05 old loss 5.281188714434393e-05 BETTER
I0312 13:16:18.450027 1895077 finetune.py:68] layer 28_o @ epoch 2 new loss 2.9562905183411203e-05 old loss 2.98761351587018e-05 BETTER
I0312 13:16:28.552423 1895193 finetune.py:68] layer 29_o @ epoch 1 new loss 3.1304174626711756e-05 old loss 3.191491850884631e-05 BETTER
I0312 13:16:32.872591 1895309 finetune.py:68] layer 30_k @ epoch 4 new loss 1.9714489098987542e-05 old loss 1.978744512598496e-05 BETTER
I0312 13:16:41.904549 1895309 finetune.py:45] layer 30_o initial loss 4.0448248910252005e-05
I0312 13:16:42.141498 1895425 finetune.py:68] layer 31_k @ epoch 2 new loss 4.3470874516060576e-05 old loss 4.5593216782435775e-05 BETTER
I0312 13:16:52.000957 1895077 finetune.py:68] layer 28_o @ epoch 3 new loss 2.9377117243711837e-05 old loss 2.9562905183411203e-05 BETTER
I0312 13:17:00.101775 1895193 finetune.py:68] layer 29_o @ epoch 2 new loss 3.115170693490654e-05 old loss 3.1304174626711756e-05 BETTER
I0312 13:17:12.816514 1895309 finetune.py:68] layer 30_o @ epoch 0 new loss 3.500692400848493e-05 old loss 4.0448248910252005e-05 BETTER
I0312 13:17:13.913072 1895425 finetune.py:68] layer 31_k @ epoch 3 new loss 4.3108862882945687e-05 old loss 4.3470874516060576e-05 BETTER
I0312 13:17:25.590507 1895077 finetune.py:68] layer 28_o @ epoch 4 new loss 2.9274990083649755e-05 old loss 2.9377117243711837e-05 BETTER
I0312 13:17:31.533721 1895193 finetune.py:68] layer 29_o @ epoch 3 new loss 3.094791463809088e-05 old loss 3.115170693490654e-05 BETTER
I0312 13:17:40.584498 1895077 finetune.py:45] layer 28_up initial loss 7.279554120032117e-05
I0312 13:17:44.586616 1895309 finetune.py:68] layer 30_o @ epoch 1 new loss 3.39911384799052e-05 old loss 3.500692400848493e-05 BETTER
I0312 13:17:45.802638 1895425 finetune.py:76] layer 31_k @ epoch 4 new loss 4.4087781134294346e-05 old loss 4.3108862882945687e-05 WORSE
I0312 13:17:54.394693 1895425 finetune.py:45] layer 31_o initial loss 0.00011234224803047255
I0312 13:18:02.964085 1895193 finetune.py:76] layer 29_o @ epoch 4 new loss 3.123629721812904e-05 old loss 3.094791463809088e-05 WORSE
I0312 13:18:11.341429 1895077 finetune.py:68] layer 28_up @ epoch 0 new loss 6.850125646451488e-05 old loss 7.279554120032117e-05 BETTER
I0312 13:18:16.197659 1895309 finetune.py:68] layer 30_o @ epoch 2 new loss 3.370782724232413e-05 old loss 3.39911384799052e-05 BETTER
I0312 13:18:17.396966 1895193 finetune.py:45] layer 29_up initial loss 8.473273919662461e-05
I0312 13:18:24.629798 1895425 finetune.py:68] layer 31_o @ epoch 0 new loss 7.000526238698512e-05 old loss 0.00011234224803047255 BETTER
I0312 13:18:43.011424 1895077 finetune.py:68] layer 28_up @ epoch 1 new loss 6.728894368279725e-05 old loss 6.850125646451488e-05 BETTER
I0312 13:18:46.488996 1895193 finetune.py:68] layer 29_up @ epoch 0 new loss 7.770649972371757e-05 old loss 8.473273919662461e-05 BETTER
I0312 13:18:48.187663 1895309 finetune.py:68] layer 30_o @ epoch 3 new loss 3.349534745211713e-05 old loss 3.370782724232413e-05 BETTER
I0312 13:18:55.700162 1895425 finetune.py:68] layer 31_o @ epoch 1 new loss 6.440905417548493e-05 old loss 7.000526238698512e-05 BETTER
I0312 13:19:14.948612 1895077 finetune.py:68] layer 28_up @ epoch 2 new loss 6.658079655608162e-05 old loss 6.728894368279725e-05 BETTER
I0312 13:19:16.123196 1895193 finetune.py:68] layer 29_up @ epoch 1 new loss 7.604467100463808e-05 old loss 7.770649972371757e-05 BETTER
I0312 13:19:19.756888 1895309 finetune.py:76] layer 30_o @ epoch 4 new loss 3.3901273127412423e-05 old loss 3.349534745211713e-05 WORSE
I0312 13:19:26.732000 1895425 finetune.py:68] layer 31_o @ epoch 2 new loss 6.0979320551268756e-05 old loss 6.440905417548493e-05 BETTER
I0312 13:19:34.120053 1895309 finetune.py:45] layer 30_up initial loss 0.00013947946717962623
I0312 13:19:46.108056 1895193 finetune.py:68] layer 29_up @ epoch 2 new loss 7.511743751820177e-05 old loss 7.604467100463808e-05 BETTER
I0312 13:19:46.885627 1895077 finetune.py:68] layer 28_up @ epoch 3 new loss 6.61373051116243e-05 old loss 6.658079655608162e-05 BETTER
I0312 13:19:57.841130 1895425 finetune.py:68] layer 31_o @ epoch 3 new loss 6.070153176551685e-05 old loss 6.0979320551268756e-05 BETTER
I0312 13:20:03.221129 1895309 finetune.py:68] layer 30_up @ epoch 0 new loss 0.00010997720528393984 old loss 0.00013947946717962623 BETTER
I0312 13:20:15.842927 1895193 finetune.py:68] layer 29_up @ epoch 3 new loss 7.452911086147651e-05 old loss 7.511743751820177e-05 BETTER
I0312 13:20:18.784276 1895077 finetune.py:68] layer 28_up @ epoch 4 new loss 6.585381197510287e-05 old loss 6.61373051116243e-05 BETTER
I0312 13:20:28.991844 1895425 finetune.py:76] layer 31_o @ epoch 4 new loss 6.411795038729906e-05 old loss 6.070153176551685e-05 WORSE
I0312 13:20:33.014238 1895309 finetune.py:68] layer 30_up @ epoch 1 new loss 0.00010350983939133584 old loss 0.00010997720528393984 BETTER
I0312 13:20:34.026893 1895077 finetune.py:45] layer 28_gate initial loss 0.00010659638064680621
I0312 13:20:43.623511 1895425 finetune.py:45] layer 31_up initial loss 0.00039789549191482365
I0312 13:20:45.493424 1895193 finetune.py:68] layer 29_up @ epoch 4 new loss 7.413014100166038e-05 old loss 7.452911086147651e-05 BETTER
I0312 13:21:00.202114 1895193 finetune.py:45] layer 29_gate initial loss 0.00012384206638671458
I0312 13:21:02.904723 1895309 finetune.py:68] layer 30_up @ epoch 2 new loss 9.997783490689471e-05 old loss 0.00010350983939133584 BETTER
I0312 13:21:03.158514 1895077 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.00010447084787301719 old loss 0.00010659638064680621 BETTER
I0312 13:21:12.371980 1895425 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0002140859141945839 old loss 0.00039789549191482365 BETTER
I0312 13:21:27.819720 1895193 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.00012044742470607162 old loss 0.00012384206638671458 BETTER
I0312 13:21:32.675369 1895309 finetune.py:68] layer 30_up @ epoch 3 new loss 9.800989937502891e-05 old loss 9.997783490689471e-05 BETTER
I0312 13:21:33.009231 1895077 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.00010356972779845819 old loss 0.00010447084787301719 BETTER
I0312 13:21:41.869972 1895425 finetune.py:68] layer 31_up @ epoch 1 new loss 0.00019538612104952335 old loss 0.0002140859141945839 BETTER
I0312 13:21:56.125849 1895193 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.00011925124272238463 old loss 0.00012044742470607162 BETTER
I0312 13:22:02.668221 1895309 finetune.py:68] layer 30_up @ epoch 4 new loss 9.673187742009759e-05 old loss 9.800989937502891e-05 BETTER
I0312 13:22:02.923060 1895077 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.00010301222937414423 old loss 0.00010356972779845819 BETTER
I0312 13:22:11.527009 1895425 finetune.py:68] layer 31_up @ epoch 2 new loss 0.0001839789911173284 old loss 0.00019538612104952335 BETTER
I0312 13:22:18.274436 1895309 finetune.py:45] layer 30_gate initial loss 0.00017576172831468284
I0312 13:22:24.360598 1895193 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.00011848737631225958 old loss 0.00011925124272238463 BETTER
I0312 13:22:33.010486 1895077 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.00010264097363688052 old loss 0.00010301222937414423 BETTER
I0312 13:22:41.892679 1895425 finetune.py:68] layer 31_up @ epoch 3 new loss 0.00017576542450115085 old loss 0.0001839789911173284 BETTER
I0312 13:22:45.905631 1895309 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.00016250326007138938 old loss 0.00017576172831468284 BETTER
I0312 13:22:52.707472 1895193 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.00011800423817476258 old loss 0.00011848737631225958 BETTER
I0312 13:23:03.163305 1895077 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.00010235868830932304 old loss 0.00010264097363688052 BETTER
I0312 13:23:11.644775 1895425 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0001695326209301129 old loss 0.00017576542450115085 BETTER
I0312 13:23:14.183603 1895309 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.00015813390200491995 old loss 0.00016250326007138938 BETTER
I0312 13:23:19.753904 1895077 finetune.py:45] layer 28_down initial loss 0.00017043398111127317
I0312 13:23:21.225840 1895193 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.00011766144598368555 old loss 0.00011800423817476258 BETTER
I0312 13:23:27.569317 1895425 finetune.py:45] layer 31_gate initial loss 0.00031770308851264417
I0312 13:23:37.037201 1895193 finetune.py:45] layer 29_down initial loss 0.00020583363948389888
I0312 13:23:42.612534 1895309 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.00015513869584538043 old loss 0.00015813390200491995 BETTER
I0312 13:23:47.050555 1895077 finetune.py:68] layer 28_down @ epoch 0 new loss 0.00017039173690136522 old loss 0.00017043398111127317 BETTER
I0312 13:23:54.623690 1895425 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.00026316195726394653 old loss 0.00031770308851264417 BETTER
I0312 13:24:02.929765 1895193 finetune.py:68] layer 29_down @ epoch 0 new loss 0.00020577103714458644 old loss 0.00020583363948389888 BETTER
I0312 13:24:11.430679 1895309 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.00015329677262343466 old loss 0.00015513869584538043 BETTER
I0312 13:24:16.204083 1895077 finetune.py:68] layer 28_down @ epoch 1 new loss 0.00017037020006682724 old loss 0.00017039173690136522 BETTER
I0312 13:24:23.729481 1895425 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.00025509082479402423 old loss 0.00026316195726394653 BETTER
I0312 13:24:29.857117 1895193 finetune.py:68] layer 29_down @ epoch 1 new loss 0.00020573448273353279 old loss 0.00020577103714458644 BETTER
I0312 13:24:40.275265 1895309 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.00015215434541460127 old loss 0.00015329677262343466 BETTER
I0312 13:24:45.042032 1895077 finetune.py:68] layer 28_down @ epoch 2 new loss 0.00017035652126651257 old loss 0.00017037020006682724 BETTER
I0312 13:24:53.218595 1895425 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.000249966949922964 old loss 0.00025509082479402423 BETTER
I0312 13:24:57.971114 1895193 finetune.py:68] layer 29_down @ epoch 2 new loss 0.00020570684864651412 old loss 0.00020573448273353279 BETTER
I0312 13:25:00.214987 1895309 finetune.py:45] layer 30_down initial loss 0.0004676147655118257
I0312 13:25:14.522223 1895077 finetune.py:68] layer 28_down @ epoch 3 new loss 0.00017034682969097048 old loss 0.00017035652126651257 BETTER
I0312 13:25:22.512669 1895425 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.00024620472686365247 old loss 0.000249966949922964 BETTER
I0312 13:25:26.537377 1895193 finetune.py:68] layer 29_down @ epoch 3 new loss 0.00020568518084473908 old loss 0.00020570684864651412 BETTER
I0312 13:25:27.570131 1895309 finetune.py:68] layer 30_down @ epoch 0 new loss 0.0004601568798534572 old loss 0.0004676147655118257 BETTER
I0312 13:25:43.956458 1895077 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0001703375019133091 old loss 0.00017034682969097048 BETTER
28_v proxy err 0.0004010626580566168 tr(WHW.T) 2018.944091796875
28_q proxy err 0.0001502776431152597 tr(WHW.T) 7650.80078125
28_k proxy err 0.000122272118460387 tr(WHW.T) 10545.498046875
28_o proxy err 0.00032813919824548066 tr(WHW.T) 194.8413543701172
28_up proxy err 0.0003099236055277288 tr(WHW.T) 4660.06884765625
28_gate proxy err 0.00024141647736541927 tr(WHW.T) 6544.75732421875
28_down proxy err 0.00046385053428821266 tr(WHW.T) 603.8861694335938
I0312 13:25:51.141485 1895425 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.00024318936630152166 old loss 0.00024620472686365247 BETTER
I0312 13:25:53.880872 1895193 finetune.py:68] layer 29_down @ epoch 4 new loss 0.00020566761668305844 old loss 0.00020568518084473908 BETTER
I0312 13:25:54.734070 1895309 finetune.py:68] layer 30_down @ epoch 1 new loss 0.00045438407687470317 old loss 0.0004601568798534572 BETTER
29_v proxy err 0.00041980959940701723 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.0001525007392046973 tr(WHW.T) 7226.9599609375
29_k proxy err 0.0001209167021443136 tr(WHW.T) 10559.00390625
29_o proxy err 0.0002932292700279504 tr(WHW.T) 207.89561462402344
29_up proxy err 0.0002589890791568905 tr(WHW.T) 6070.302734375
29_gate proxy err 0.00022763105516787618 tr(WHW.T) 7368.3359375
29_down proxy err 0.0004623367276508361 tr(WHW.T) 782.326171875
I0312 13:26:06.999227 1895425 finetune.py:45] layer 31_down initial loss 0.0011685920180752873
I0312 13:26:21.526746 1895309 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0004499636706896126 old loss 0.00045438407687470317 BETTER
I0312 13:26:32.397389 1895425 finetune.py:68] layer 31_down @ epoch 0 new loss 0.0011544510489329696 old loss 0.0011685920180752873 BETTER
I0312 13:26:48.437897 1895309 finetune.py:68] layer 30_down @ epoch 3 new loss 0.00044628235627897084 old loss 0.0004499636706896126 BETTER
I0312 13:26:58.649879 1895425 finetune.py:68] layer 31_down @ epoch 1 new loss 0.0011493416968733072 old loss 0.0011544510489329696 BETTER
I0312 13:27:15.329758 1895309 finetune.py:68] layer 30_down @ epoch 4 new loss 0.00044338079169392586 old loss 0.00044628235627897084 BETTER
30_v proxy err 0.0003606806567404419 tr(WHW.T) 2261.489501953125
30_q proxy err 0.00015267454728018492 tr(WHW.T) 7816.16015625
30_k proxy err 0.0001281547883991152 tr(WHW.T) 10523.5927734375
30_o proxy err 0.0002999000425916165 tr(WHW.T) 252.0828399658203
30_up proxy err 0.0001880585914477706 tr(WHW.T) 10020.2236328125
30_gate proxy err 0.00018109114898834378 tr(WHW.T) 11002.2353515625
30_down proxy err 0.00034875498386099935 tr(WHW.T) 3585.826416015625
I0312 13:27:24.960524 1895425 finetune.py:68] layer 31_down @ epoch 2 new loss 0.0011463563423603773 old loss 0.0011493416968733072 BETTER
I0312 13:27:51.274678 1895425 finetune.py:68] layer 31_down @ epoch 3 new loss 0.0011443174444139004 old loss 0.0011463563423603773 BETTER
I0312 13:28:17.574801 1895425 finetune.py:68] layer 31_down @ epoch 4 new loss 0.0011427579447627068 old loss 0.0011443174444139004 BETTER
31_v proxy err 0.00041541518294252455 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.00014677261060569435 tr(WHW.T) 6858.96826171875
31_k proxy err 0.0001198938480229117 tr(WHW.T) 10234.2314453125
31_o proxy err 0.0002496028027962893 tr(WHW.T) 457.8674621582031
31_up proxy err 0.0001424982910975814 tr(WHW.T) 14556.8701171875
31_gate proxy err 0.00013956360635347664 tr(WHW.T) 14825.6123046875
31_down proxy err 0.0002145986509276554 tr(WHW.T) 17874.05859375
