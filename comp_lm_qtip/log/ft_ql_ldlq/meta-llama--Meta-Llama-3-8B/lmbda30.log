I0327 20:41:14.353429 2041475 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 20:41:14.353532 2041475 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 20:41:14.353570 2041475 utils.py:162] NumExpr defaulting to 16 threads.
I0327 20:41:14.674706 2041475 config.py:54] PyTorch version 2.6.0 available.
W0327 20:41:14.862457 2041475 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 20:41:15.416847 2041475 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.13it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.64it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.83it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.95it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.64it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.81it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.98it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.83it/s]
I0327 20:41:16.879382 2041475 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.42it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:17,  1.69it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.80it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:14,  1.87it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.89it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.89it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.89it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:12,  1.90it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.91it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:10,  1.92it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.92it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:09,  1.92it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.92it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:08,  1.93it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.92it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.93it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.94it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.94it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.94it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.95it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.95it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.96it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:02,  2.02it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  2.08it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:01,  2.11it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:14<00:01,  2.13it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:00,  2.14it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:15<00:00,  2.15it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  2.18it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.96it/s]
I0327 20:41:38.751873 2041475 quantize_finetune_llama.py:185] loaded compression model
I0327 20:41:57.182034 2041475 quantize_finetune_llama.py:189] loaded dataset and devset
I0327 20:42:02.339121 2041475 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 20:43:01.755626 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 59.26975107192993s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0327 20:43:27.883576 2041604 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 20:43:27.883675 2041604 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 20:43:27.883713 2041604 utils.py:162] NumExpr defaulting to 16 threads.
I0327 20:43:28.204759 2041604 config.py:54] PyTorch version 2.6.0 available.
W0327 20:43:28.390087 2041604 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 20:43:28.928184 2041604 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 20:43:28.943238 2041475 quantize_finetune_llama.py:209] layer 1 gpu 1
I0327 20:43:28.945768 2041604 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 20:43:45.062828 2041604 finetune.py:45] layer 0_v initial loss 3.4173692711192416e-06
W0327 20:43:45.063081 2041604 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 20:44:19.540306 2041604 finetune.py:68] layer 0_v @ epoch 0 new loss 3.03671686197049e-06 old loss 3.4173692711192416e-06 BETTER
I0327 20:44:23.603774 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 54.4962317943573s
I0327 20:44:35.347640 2041676 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 20:44:35.347743 2041676 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 20:44:35.347782 2041676 utils.py:162] NumExpr defaulting to 16 threads.
I0327 20:44:35.699677 2041676 config.py:54] PyTorch version 2.6.0 available.
W0327 20:44:35.890814 2041676 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 20:44:36.468618 2041676 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 20:44:36.472861 2041475 quantize_finetune_llama.py:209] layer 2 gpu 2
I0327 20:44:36.486885 2041676 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 20:44:52.787636 2041676 finetune.py:45] layer 1_v initial loss 2.849926022463478e-05
W0327 20:44:52.787970 2041676 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 20:44:55.397050 2041604 finetune.py:68] layer 0_v @ epoch 1 new loss 2.9001894290558994e-06 old loss 3.03671686197049e-06 BETTER
I0327 20:45:25.856009 2041676 finetune.py:68] layer 1_v @ epoch 0 new loss 6.339568699331721e-06 old loss 2.849926022463478e-05 BETTER
I0327 20:45:31.910947 2041604 finetune.py:68] layer 0_v @ epoch 2 new loss 2.827410753525328e-06 old loss 2.9001894290558994e-06 BETTER
I0327 20:45:40.663617 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 63.99306845664978s
I0327 20:45:49.067508 2041748 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 20:45:49.067604 2041748 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 20:45:49.067642 2041748 utils.py:162] NumExpr defaulting to 16 threads.
I0327 20:45:49.411432 2041748 config.py:54] PyTorch version 2.6.0 available.
W0327 20:45:49.608036 2041748 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 20:45:50.188835 2041748 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 20:45:50.192454 2041475 quantize_finetune_llama.py:209] layer 3 gpu 3
I0327 20:45:50.206073 2041748 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 20:46:00.094003 2041676 finetune.py:68] layer 1_v @ epoch 1 new loss 4.548349807009799e-06 old loss 6.339568699331721e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 20:46:06.820650 2041748 finetune.py:45] layer 2_v initial loss 5.802720625069924e-05
W0327 20:46:06.821098 2041748 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 20:46:08.857642 2041604 finetune.py:68] layer 0_v @ epoch 3 new loss 2.7801975193142425e-06 old loss 2.827410753525328e-06 BETTER
I0327 20:46:34.764633 2041676 finetune.py:68] layer 1_v @ epoch 2 new loss 4.158942829235457e-06 old loss 4.548349807009799e-06 BETTER
I0327 20:46:40.385143 2041748 finetune.py:68] layer 2_v @ epoch 0 new loss 1.0281330105499364e-05 old loss 5.802720625069924e-05 BETTER
I0327 20:46:45.934932 2041604 finetune.py:68] layer 0_v @ epoch 4 new loss 2.744176526903175e-06 old loss 2.7801975193142425e-06 BETTER
I0327 20:46:57.077616 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 66.65794968605042s
I0327 20:47:06.676124 2041820 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 20:47:06.676265 2041820 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 20:47:06.676324 2041820 utils.py:162] NumExpr defaulting to 16 threads.
I0327 20:47:07.060817 2041820 config.py:54] PyTorch version 2.6.0 available.
I0327 20:47:07.086480 2041604 finetune.py:45] layer 0_q initial loss 2.7487076295074075e-06
W0327 20:47:07.280578 2041820 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 20:47:07.924984 2041820 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 20:47:07.928843 2041475 quantize_finetune_llama.py:209] layer 4 gpu 0
I0327 20:47:07.942664 2041820 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 20:47:09.935157 2041676 finetune.py:68] layer 1_v @ epoch 3 new loss 3.9610254134458955e-06 old loss 4.158942829235457e-06 BETTER
I0327 20:47:15.283686 2041748 finetune.py:68] layer 2_v @ epoch 1 new loss 6.5794060901680496e-06 old loss 1.0281330105499364e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 20:47:24.962400 2041820 finetune.py:45] layer 3_v initial loss 6.514760752907023e-05
W0327 20:47:24.962793 2041820 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 20:47:42.281136 2041604 finetune.py:68] layer 0_q @ epoch 0 new loss 2.718443511184887e-06 old loss 2.7487076295074075e-06 BETTER
I0327 20:47:44.997383 2041676 finetune.py:68] layer 1_v @ epoch 4 new loss 3.821038717433112e-06 old loss 3.9610254134458955e-06 BETTER
I0327 20:47:50.473904 2041748 finetune.py:68] layer 2_v @ epoch 2 new loss 5.999985660309903e-06 old loss 6.5794060901680496e-06 BETTER
I0327 20:47:57.990179 2041820 finetune.py:68] layer 3_v @ epoch 0 new loss 1.5133184206206352e-05 old loss 6.514760752907023e-05 BETTER
I0327 20:48:04.176773 2041676 finetune.py:45] layer 1_q initial loss 3.886930699081859e-06
I0327 20:48:18.771032 2041604 finetune.py:68] layer 0_q @ epoch 1 new loss 2.6929815248877276e-06 old loss 2.718443511184887e-06 BETTER
I0327 20:48:25.808602 2041748 finetune.py:68] layer 2_v @ epoch 3 new loss 5.806065018987283e-06 old loss 5.999985660309903e-06 BETTER
I0327 20:48:32.287286 2041820 finetune.py:68] layer 3_v @ epoch 1 new loss 1.074167448678054e-05 old loss 1.5133184206206352e-05 BETTER
I0327 20:48:37.800267 2041676 finetune.py:68] layer 1_q @ epoch 0 new loss 3.7460554267454427e-06 old loss 3.886930699081859e-06 BETTER
I0327 20:48:55.629489 2041604 finetune.py:68] layer 0_q @ epoch 2 new loss 2.6707648430601694e-06 old loss 2.6929815248877276e-06 BETTER
I0327 20:49:01.286256 2041748 finetune.py:68] layer 2_v @ epoch 4 new loss 5.684059942723252e-06 old loss 5.806065018987283e-06 BETTER
I0327 20:49:06.844108 2041820 finetune.py:68] layer 3_v @ epoch 2 new loss 9.808762115426362e-06 old loss 1.074167448678054e-05 BETTER
I0327 20:49:12.251467 2041676 finetune.py:68] layer 1_q @ epoch 1 new loss 3.6403839658305515e-06 old loss 3.7460554267454427e-06 BETTER
I0327 20:49:20.248556 2041748 finetune.py:45] layer 2_q initial loss 6.53141114526079e-06
I0327 20:49:32.584345 2041604 finetune.py:68] layer 0_q @ epoch 3 new loss 2.650879196153255e-06 old loss 2.6707648430601694e-06 BETTER
I0327 20:49:41.604212 2041820 finetune.py:68] layer 3_v @ epoch 3 new loss 9.409893209522124e-06 old loss 9.808762115426362e-06 BETTER
I0327 20:49:46.797978 2041676 finetune.py:68] layer 1_q @ epoch 2 new loss 3.5544817365007475e-06 old loss 3.6403839658305515e-06 BETTER
I0327 20:49:54.066501 2041748 finetune.py:68] layer 2_q @ epoch 0 new loss 6.352757736749481e-06 old loss 6.53141114526079e-06 BETTER
I0327 20:50:09.411006 2041604 finetune.py:68] layer 0_q @ epoch 4 new loss 2.632301175253815e-06 old loss 2.650879196153255e-06 BETTER
I0327 20:50:16.372838 2041820 finetune.py:68] layer 3_v @ epoch 4 new loss 9.163508366327733e-06 old loss 9.409893209522124e-06 BETTER
I0327 20:50:21.467287 2041676 finetune.py:68] layer 1_q @ epoch 3 new loss 3.4827410217985744e-06 old loss 3.5544817365007475e-06 BETTER
I0327 20:50:27.125992 2041604 finetune.py:45] layer 0_k initial loss 2.637234047142556e-06
I0327 20:50:28.733953 2041748 finetune.py:68] layer 2_q @ epoch 1 new loss 6.244643827812979e-06 old loss 6.352757736749481e-06 BETTER
I0327 20:50:35.684333 2041820 finetune.py:45] layer 3_q initial loss 1.082211656466825e-05
I0327 20:50:56.018662 2041676 finetune.py:68] layer 1_q @ epoch 4 new loss 3.4222812246298417e-06 old loss 3.4827410217985744e-06 BETTER
I0327 20:51:02.720281 2041604 finetune.py:68] layer 0_k @ epoch 0 new loss 2.620913392092916e-06 old loss 2.637234047142556e-06 BETTER
I0327 20:51:03.765495 2041748 finetune.py:68] layer 2_q @ epoch 2 new loss 6.159331405797275e-06 old loss 6.244643827812979e-06 BETTER
I0327 20:51:08.761592 2041820 finetune.py:68] layer 3_q @ epoch 0 new loss 1.0526751793804578e-05 old loss 1.082211656466825e-05 BETTER
I0327 20:51:13.660376 2041676 finetune.py:45] layer 1_k initial loss 3.5325324461155105e-06
I0327 20:51:38.578094 2041748 finetune.py:68] layer 2_q @ epoch 3 new loss 6.0879256125190295e-06 old loss 6.159331405797275e-06 BETTER
I0327 20:51:39.120055 2041604 finetune.py:68] layer 0_k @ epoch 1 new loss 2.6062980396091007e-06 old loss 2.620913392092916e-06 BETTER
I0327 20:51:42.813115 2041820 finetune.py:68] layer 3_q @ epoch 1 new loss 1.0336809282307513e-05 old loss 1.0526751793804578e-05 BETTER
I0327 20:51:47.341099 2041676 finetune.py:68] layer 1_k @ epoch 0 new loss 3.46081355928618e-06 old loss 3.5325324461155105e-06 BETTER
I0327 20:52:13.698300 2041748 finetune.py:68] layer 2_q @ epoch 4 new loss 6.027618383086519e-06 old loss 6.0879256125190295e-06 BETTER
I0327 20:52:15.707551 2041604 finetune.py:68] layer 0_k @ epoch 2 new loss 2.5928022751031676e-06 old loss 2.6062980396091007e-06 BETTER
I0327 20:52:17.022881 2041820 finetune.py:68] layer 3_q @ epoch 2 new loss 1.0190761713602114e-05 old loss 1.0336809282307513e-05 BETTER
I0327 20:52:21.818998 2041676 finetune.py:68] layer 1_k @ epoch 1 new loss 3.4088818665622966e-06 old loss 3.46081355928618e-06 BETTER
I0327 20:52:31.857661 2041748 finetune.py:45] layer 2_k initial loss 6.349483101075748e-06
I0327 20:52:51.627852 2041820 finetune.py:68] layer 3_q @ epoch 3 new loss 1.0070993994304445e-05 old loss 1.0190761713602114e-05 BETTER
I0327 20:52:52.397373 2041604 finetune.py:68] layer 0_k @ epoch 3 new loss 2.5798774458962725e-06 old loss 2.5928022751031676e-06 BETTER
I0327 20:52:56.196066 2041676 finetune.py:68] layer 1_k @ epoch 2 new loss 3.3640399124124087e-06 old loss 3.4088818665622966e-06 BETTER
I0327 20:53:05.566441 2041748 finetune.py:68] layer 2_k @ epoch 0 new loss 6.271859092521481e-06 old loss 6.349483101075748e-06 BETTER
I0327 20:53:25.982998 2041820 finetune.py:68] layer 3_q @ epoch 4 new loss 9.96118615148589e-06 old loss 1.0070993994304445e-05 BETTER
I0327 20:53:29.322222 2041604 finetune.py:68] layer 0_k @ epoch 4 new loss 2.5678011752461316e-06 old loss 2.5798774458962725e-06 BETTER
I0327 20:53:30.935698 2041676 finetune.py:68] layer 1_k @ epoch 3 new loss 3.327166268718429e-06 old loss 3.3640399124124087e-06 BETTER
I0327 20:53:40.337467 2041748 finetune.py:68] layer 2_k @ epoch 1 new loss 6.223839136509923e-06 old loss 6.271859092521481e-06 BETTER
I0327 20:53:43.930954 2041820 finetune.py:45] layer 3_k initial loss 1.0693989679566585e-05
I0327 20:53:48.708170 2041604 finetune.py:45] layer 0_o initial loss 4.470127350941766e-06
I0327 20:54:05.477389 2041676 finetune.py:68] layer 1_k @ epoch 4 new loss 3.2901618851610692e-06 old loss 3.327166268718429e-06 BETTER
I0327 20:54:14.945734 2041748 finetune.py:68] layer 2_k @ epoch 2 new loss 6.180250693432754e-06 old loss 6.223839136509923e-06 BETTER
I0327 20:54:17.006453 2041820 finetune.py:68] layer 3_k @ epoch 0 new loss 1.059720671037212e-05 old loss 1.0693989679566585e-05 BETTER
I0327 20:54:23.530751 2041604 finetune.py:68] layer 0_o @ epoch 0 new loss 4.4290750338404905e-06 old loss 4.470127350941766e-06 BETTER
I0327 20:54:24.636396 2041676 finetune.py:45] layer 1_o initial loss 1.3978476090414915e-05
I0327 20:54:50.071421 2041748 finetune.py:68] layer 2_k @ epoch 3 new loss 6.143485279608285e-06 old loss 6.180250693432754e-06 BETTER
I0327 20:54:51.006658 2041820 finetune.py:68] layer 3_k @ epoch 1 new loss 1.0517645023355726e-05 old loss 1.059720671037212e-05 BETTER
I0327 20:54:57.618145 2041676 finetune.py:68] layer 1_o @ epoch 0 new loss 1.2721731764031574e-05 old loss 1.3978476090414915e-05 BETTER
I0327 20:54:59.573964 2041604 finetune.py:68] layer 0_o @ epoch 1 new loss 4.396434633235913e-06 old loss 4.4290750338404905e-06 BETTER
I0327 20:55:24.995541 2041820 finetune.py:68] layer 3_k @ epoch 2 new loss 1.0446831765875686e-05 old loss 1.0517645023355726e-05 BETTER
I0327 20:55:25.093849 2041748 finetune.py:68] layer 2_k @ epoch 4 new loss 6.106631644797744e-06 old loss 6.143485279608285e-06 BETTER
I0327 20:55:31.601279 2041676 finetune.py:68] layer 1_o @ epoch 1 new loss 1.1780426575569436e-05 old loss 1.2721731764031574e-05 BETTER
I0327 20:55:35.681983 2041604 finetune.py:68] layer 0_o @ epoch 2 new loss 4.3691552491509356e-06 old loss 4.396434633235913e-06 BETTER
I0327 20:55:44.475932 2041748 finetune.py:45] layer 2_o initial loss 1.9885079382220283e-05
I0327 20:55:59.037614 2041820 finetune.py:68] layer 3_k @ epoch 3 new loss 1.0384756023995578e-05 old loss 1.0446831765875686e-05 BETTER
I0327 20:56:05.464177 2041676 finetune.py:68] layer 1_o @ epoch 2 new loss 1.1058983545808587e-05 old loss 1.1780426575569436e-05 BETTER
I0327 20:56:11.666523 2041604 finetune.py:68] layer 0_o @ epoch 3 new loss 4.345687102613738e-06 old loss 4.3691552491509356e-06 BETTER
I0327 20:56:17.707771 2041748 finetune.py:68] layer 2_o @ epoch 0 new loss 1.778597288648598e-05 old loss 1.9885079382220283e-05 BETTER
I0327 20:56:33.132353 2041820 finetune.py:68] layer 3_k @ epoch 4 new loss 1.0327801646781154e-05 old loss 1.0384756023995578e-05 BETTER
I0327 20:56:39.558348 2041676 finetune.py:68] layer 1_o @ epoch 3 new loss 1.049631646310445e-05 old loss 1.1058983545808587e-05 BETTER
I0327 20:56:47.776492 2041604 finetune.py:68] layer 0_o @ epoch 4 new loss 4.325262125348672e-06 old loss 4.345687102613738e-06 BETTER
I0327 20:56:51.779064 2041748 finetune.py:68] layer 2_o @ epoch 1 new loss 1.645303564146161e-05 old loss 1.778597288648598e-05 BETTER
I0327 20:56:52.594158 2041820 finetune.py:45] layer 3_o initial loss 2.849791053449735e-05
I0327 20:57:13.518295 2041676 finetune.py:68] layer 1_o @ epoch 4 new loss 1.0060265594802331e-05 old loss 1.049631646310445e-05 BETTER
I0327 20:57:18.890453 2041604 finetune.py:45] layer 0_up initial loss 6.061709882487776e-06
I0327 20:57:24.945413 2041820 finetune.py:68] layer 3_o @ epoch 0 new loss 2.5530676794005558e-05 old loss 2.849791053449735e-05 BETTER
I0327 20:57:25.969648 2041748 finetune.py:68] layer 2_o @ epoch 2 new loss 1.5574198187096044e-05 old loss 1.645303564146161e-05 BETTER
I0327 20:57:43.953459 2041676 finetune.py:45] layer 1_up initial loss 1.3989711078465916e-05
I0327 20:57:51.604255 2041604 finetune.py:68] layer 0_up @ epoch 0 new loss 6.019085958541837e-06 old loss 6.061709882487776e-06 BETTER
I0327 20:57:58.491569 2041820 finetune.py:68] layer 3_o @ epoch 1 new loss 2.430698077660054e-05 old loss 2.5530676794005558e-05 BETTER
I0327 20:58:00.604304 2041748 finetune.py:68] layer 2_o @ epoch 3 new loss 1.497169887443306e-05 old loss 1.5574198187096044e-05 BETTER
I0327 20:58:14.645042 2041676 finetune.py:68] layer 1_up @ epoch 0 new loss 1.3649023458128795e-05 old loss 1.3989711078465916e-05 BETTER
I0327 20:58:25.210204 2041604 finetune.py:68] layer 0_up @ epoch 1 new loss 5.986232281429693e-06 old loss 6.019085958541837e-06 BETTER
I0327 20:58:32.025692 2041820 finetune.py:68] layer 3_o @ epoch 2 new loss 2.3664813852519728e-05 old loss 2.430698077660054e-05 BETTER
I0327 20:58:34.873561 2041748 finetune.py:68] layer 2_o @ epoch 4 new loss 1.453225650038803e-05 old loss 1.497169887443306e-05 BETTER
I0327 20:58:46.357202 2041676 finetune.py:68] layer 1_up @ epoch 1 new loss 1.339495247520972e-05 old loss 1.3649023458128795e-05 BETTER
I0327 20:58:58.666162 2041604 finetune.py:68] layer 0_up @ epoch 2 new loss 5.959630470897537e-06 old loss 5.986232281429693e-06 BETTER
I0327 20:59:05.501521 2041820 finetune.py:68] layer 3_o @ epoch 3 new loss 2.3242148017743602e-05 old loss 2.3664813852519728e-05 BETTER
I0327 20:59:05.514875 2041748 finetune.py:45] layer 2_up initial loss 2.4070217477856204e-05
I0327 20:59:18.353722 2041676 finetune.py:68] layer 1_up @ epoch 2 new loss 1.319035163760418e-05 old loss 1.339495247520972e-05 BETTER
I0327 20:59:32.234042 2041604 finetune.py:68] layer 0_up @ epoch 3 new loss 5.937059995630989e-06 old loss 5.959630470897537e-06 BETTER
I0327 20:59:36.347366 2041748 finetune.py:68] layer 2_up @ epoch 0 new loss 2.3695387426414527e-05 old loss 2.4070217477856204e-05 BETTER
I0327 20:59:39.113245 2041820 finetune.py:68] layer 3_o @ epoch 4 new loss 2.2927702957531437e-05 old loss 2.3242148017743602e-05 BETTER
I0327 20:59:50.504111 2041676 finetune.py:68] layer 1_up @ epoch 3 new loss 1.3024628060520627e-05 old loss 1.319035163760418e-05 BETTER
I0327 21:00:05.877504 2041604 finetune.py:68] layer 0_up @ epoch 4 new loss 5.9180483731324784e-06 old loss 5.937059995630989e-06 BETTER
I0327 21:00:08.537556 2041748 finetune.py:68] layer 2_up @ epoch 1 new loss 2.3423790480592288e-05 old loss 2.3695387426414527e-05 BETTER
I0327 21:00:10.094718 2041820 finetune.py:45] layer 3_up initial loss 4.3873089452972636e-05
I0327 21:00:22.557844 2041676 finetune.py:68] layer 1_up @ epoch 4 new loss 1.2888386663689744e-05 old loss 1.3024628060520627e-05 BETTER
I0327 21:00:37.086008 2041604 finetune.py:45] layer 0_gate initial loss 7.2688058025960345e-06
I0327 21:00:40.426105 2041820 finetune.py:68] layer 3_up @ epoch 0 new loss 4.3493706471053883e-05 old loss 4.3873089452972636e-05 BETTER
I0327 21:00:40.855919 2041748 finetune.py:68] layer 2_up @ epoch 2 new loss 2.3213016902445816e-05 old loss 2.3423790480592288e-05 BETTER
I0327 21:00:53.694728 2041676 finetune.py:45] layer 1_gate initial loss 1.569707819726318e-05
I0327 21:01:07.243332 2041604 finetune.py:68] layer 0_gate @ epoch 0 new loss 7.226049547170987e-06 old loss 7.2688058025960345e-06 BETTER
I0327 21:01:11.794821 2041820 finetune.py:68] layer 3_up @ epoch 1 new loss 4.323717439547181e-05 old loss 4.3493706471053883e-05 BETTER
I0327 21:01:13.110738 2041748 finetune.py:68] layer 2_up @ epoch 3 new loss 2.3040876840241253e-05 old loss 2.3213016902445816e-05 BETTER
I0327 21:01:22.062793 2041676 finetune.py:68] layer 1_gate @ epoch 0 new loss 1.5542911569355056e-05 old loss 1.569707819726318e-05 BETTER
I0327 21:01:38.737400 2041604 finetune.py:68] layer 0_gate @ epoch 1 new loss 7.188805284386035e-06 old loss 7.226049547170987e-06 BETTER
I0327 21:01:43.327080 2041820 finetune.py:68] layer 3_up @ epoch 2 new loss 4.3031748646171764e-05 old loss 4.323717439547181e-05 BETTER
I0327 21:01:45.426375 2041748 finetune.py:68] layer 2_up @ epoch 4 new loss 2.289782241859939e-05 old loss 2.3040876840241253e-05 BETTER
I0327 21:01:51.502936 2041676 finetune.py:68] layer 1_gate @ epoch 1 new loss 1.544375299999956e-05 old loss 1.5542911569355056e-05 BETTER
I0327 21:02:10.251126 2041604 finetune.py:68] layer 0_gate @ epoch 2 new loss 7.156664196372731e-06 old loss 7.188805284386035e-06 BETTER
I0327 21:02:14.764325 2041820 finetune.py:68] layer 3_up @ epoch 3 new loss 4.28561725129839e-05 old loss 4.3031748646171764e-05 BETTER
I0327 21:02:16.292292 2041748 finetune.py:45] layer 2_gate initial loss 2.9266700948937796e-05
I0327 21:02:21.137279 2041676 finetune.py:68] layer 1_gate @ epoch 2 new loss 1.5378516764030792e-05 old loss 1.544375299999956e-05 BETTER
I0327 21:02:41.977437 2041604 finetune.py:68] layer 0_gate @ epoch 3 new loss 7.1291924541583285e-06 old loss 7.156664196372731e-06 BETTER
I0327 21:02:45.237583 2041748 finetune.py:68] layer 2_gate @ epoch 0 new loss 2.9143655410734937e-05 old loss 2.9266700948937796e-05 BETTER
I0327 21:02:46.512023 2041820 finetune.py:68] layer 3_up @ epoch 4 new loss 4.2699582991190255e-05 old loss 4.28561725129839e-05 BETTER
I0327 21:02:51.077484 2041676 finetune.py:68] layer 1_gate @ epoch 3 new loss 1.5325558706535958e-05 old loss 1.5378516764030792e-05 BETTER
I0327 21:03:13.701445 2041604 finetune.py:68] layer 0_gate @ epoch 4 new loss 7.104551968950545e-06 old loss 7.1291924541583285e-06 BETTER
I0327 21:03:15.157494 2041748 finetune.py:68] layer 2_gate @ epoch 1 new loss 2.9036646083113737e-05 old loss 2.9143655410734937e-05 BETTER
I0327 21:03:17.931680 2041820 finetune.py:45] layer 3_gate initial loss 5.306675666361116e-05
I0327 21:03:21.033796 2041676 finetune.py:68] layer 1_gate @ epoch 4 new loss 1.527795393485576e-05 old loss 1.5325558706535958e-05 BETTER
I0327 21:03:45.231844 2041748 finetune.py:68] layer 2_gate @ epoch 2 new loss 2.8941782147740014e-05 old loss 2.9036646083113737e-05 BETTER
I0327 21:03:46.030018 2041820 finetune.py:68] layer 3_gate @ epoch 0 new loss 5.288089960231446e-05 old loss 5.306675666361116e-05 BETTER
I0327 21:04:09.386174 2041604 finetune.py:45] layer 0_down initial loss 1.0612166079226881e-05
I0327 21:04:15.214900 2041820 finetune.py:68] layer 3_gate @ epoch 1 new loss 5.273191709420644e-05 old loss 5.288089960231446e-05 BETTER
I0327 21:04:15.381660 2041748 finetune.py:68] layer 2_gate @ epoch 3 new loss 2.8857339202659205e-05 old loss 2.8941782147740014e-05 BETTER
I0327 21:04:16.149083 2041676 finetune.py:45] layer 1_down initial loss 2.1542515241890214e-05
I0327 21:04:36.831028 2041604 finetune.py:68] layer 0_down @ epoch 0 new loss 1.0594994819257408e-05 old loss 1.0612166079226881e-05 BETTER
I0327 21:04:42.292078 2041676 finetune.py:68] layer 1_down @ epoch 0 new loss 2.1516179913305677e-05 old loss 2.1542515241890214e-05 BETTER
I0327 21:04:44.703250 2041820 finetune.py:68] layer 3_gate @ epoch 2 new loss 5.259592944639735e-05 old loss 5.273191709420644e-05 BETTER
I0327 21:04:45.515062 2041748 finetune.py:68] layer 2_gate @ epoch 4 new loss 2.8777809347957373e-05 old loss 2.8857339202659205e-05 BETTER
I0327 21:05:05.412444 2041604 finetune.py:68] layer 0_down @ epoch 1 new loss 1.0580384696368128e-05 old loss 1.0594994819257408e-05 BETTER
I0327 21:05:09.728730 2041676 finetune.py:68] layer 1_down @ epoch 1 new loss 2.1496027329703793e-05 old loss 2.1516179913305677e-05 BETTER
I0327 21:05:14.026930 2041820 finetune.py:68] layer 3_gate @ epoch 3 new loss 5.2472769311862066e-05 old loss 5.259592944639735e-05 BETTER
I0327 21:05:34.212311 2041604 finetune.py:68] layer 0_down @ epoch 2 new loss 1.0567963727226015e-05 old loss 1.0580384696368128e-05 BETTER
I0327 21:05:37.280085 2041676 finetune.py:68] layer 1_down @ epoch 2 new loss 2.1477688278537244e-05 old loss 2.1496027329703793e-05 BETTER
I0327 21:05:41.464024 2041748 finetune.py:45] layer 2_down initial loss 4.240298585500568e-05
I0327 21:05:43.437520 2041820 finetune.py:68] layer 3_gate @ epoch 4 new loss 5.235630305833183e-05 old loss 5.2472769311862066e-05 BETTER
I0327 21:06:03.079536 2041604 finetune.py:68] layer 0_down @ epoch 3 new loss 1.0557138011790812e-05 old loss 1.0567963727226015e-05 BETTER
I0327 21:06:05.045662 2041676 finetune.py:68] layer 1_down @ epoch 3 new loss 2.146032238670159e-05 old loss 2.1477688278537244e-05 BETTER
I0327 21:06:08.033976 2041748 finetune.py:68] layer 2_down @ epoch 0 new loss 4.236856693751179e-05 old loss 4.240298585500568e-05 BETTER
I0327 21:06:32.270776 2041604 finetune.py:68] layer 0_down @ epoch 4 new loss 1.0547722013143357e-05 old loss 1.0557138011790812e-05 BETTER
I0327 21:06:32.775426 2041676 finetune.py:68] layer 1_down @ epoch 4 new loss 2.1444893718580715e-05 old loss 2.146032238670159e-05 BETTER
0_v proxy err 0.1315954029560089 tr(WHW.T) 60.88684844970703
bpp_loss 1.7512992132396903
0_q proxy err 0.00015053701645229012 tr(WHW.T) 287965.875
bpp_loss 2.5322641954699066
0_k proxy err 0.00013094600581098348 tr(WHW.T) 100044.390625
bpp_loss 3.0267528736731037
0_o proxy err 0.01975253038108349 tr(WHW.T) 3125.009033203125
bpp_loss 1.818401172873564
0_up proxy err 0.040915943682193756 tr(WHW.T) 8924.7158203125
bpp_loss 2.1083252317870835
0_gate proxy err 0.02367311529815197 tr(WHW.T) 15779.130859375
bpp_loss 2.2161518764416024
0_down proxy err 0.031991660594940186 tr(WHW.T) 10893.373046875
bpp_loss 2.0980981913827628
1_v proxy err 0.05806703865528107 tr(WHW.T) 109.07096099853516
bpp_loss 1.8536025191133376
1_q proxy err 0.0002112871443387121 tr(WHW.T) 144740.65625
bpp_loss 2.7557531335332897
1_k proxy err 0.00011606982297962531 tr(WHW.T) 75375.9453125
bpp_loss 3.356591446994571
1_o proxy err 0.03743376582860947 tr(WHW.T) 1994.787841796875
bpp_loss 1.9075884078338277
1_up proxy err 0.04698507860302925 tr(WHW.T) 8231.3876953125
bpp_loss 2.1224551430038576
1_gate proxy err 0.02840415947139263 tr(WHW.T) 13944.8779296875
bpp_loss 2.2267014676971093
1_down proxy err 0.0008578865090385079 tr(WHW.T) 13997.001953125
bpp_loss 2.10400237203742
I0327 21:06:35.714806 2041748 finetune.py:68] layer 2_down @ epoch 1 new loss 4.233576328260824e-05 old loss 4.236856693751179e-05 BETTER
I0327 21:06:40.713575 2041820 finetune.py:45] layer 3_down initial loss 7.975940388860181e-05
I0327 21:07:04.210938 2041748 finetune.py:68] layer 2_down @ epoch 2 new loss 4.230581544106826e-05 old loss 4.233576328260824e-05 BETTER
I0327 21:07:06.687514 2041820 finetune.py:68] layer 3_down @ epoch 0 new loss 7.973295578267425e-05 old loss 7.975940388860181e-05 BETTER
I0327 21:07:31.986759 2041748 finetune.py:68] layer 2_down @ epoch 3 new loss 4.2278152250219136e-05 old loss 4.230581544106826e-05 BETTER
I0327 21:07:33.662551 2041820 finetune.py:68] layer 3_down @ epoch 1 new loss 7.97116372268647e-05 old loss 7.973295578267425e-05 BETTER
I0327 21:07:46.986713 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 66.8602032661438s
I0327 21:07:50.744261 2041890 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:07:50.744358 2041890 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:07:50.744398 2041890 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:07:51.092831 2041890 config.py:54] PyTorch version 2.6.0 available.
W0327 21:07:51.303690 2041890 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:07:51.906973 2041890 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:07:51.910738 2041475 quantize_finetune_llama.py:209] layer 5 gpu 1
I0327 21:07:51.924360 2041890 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 21:07:59.876613 2041748 finetune.py:68] layer 2_down @ epoch 4 new loss 4.225406883051619e-05 old loss 4.2278152250219136e-05 BETTER
I0327 21:08:00.855490 2041820 finetune.py:68] layer 3_down @ epoch 2 new loss 7.969325815793127e-05 old loss 7.97116372268647e-05 BETTER
2_v proxy err 0.0880877673625946 tr(WHW.T) 155.95950317382812
bpp_loss 1.7508019272063393
2_q proxy err 0.0016116767656058073 tr(WHW.T) 41468.59765625
bpp_loss 2.7001990485587157
2_k proxy err 0.0008467278676107526 tr(WHW.T) 22587.74609375
bpp_loss 3.4453403083316516
2_o proxy err 0.03626493737101555 tr(WHW.T) 1978.7926025390625
bpp_loss 1.8553444058052264
2_up proxy err 0.055401235818862915 tr(WHW.T) 7606.27685546875
bpp_loss 2.1095222025510987
2_gate proxy err 0.02882651798427105 tr(WHW.T) 15102.791015625
bpp_loss 2.2517848628506596
2_down proxy err 0.051148783415555954 tr(WHW.T) 7840.00048828125
bpp_loss 2.1107649226240546
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:08:08.558772 2041890 finetune.py:45] layer 4_v initial loss 6.138525350252166e-05
W0327 21:08:08.559064 2041890 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:08:28.367904 2041820 finetune.py:68] layer 3_down @ epoch 3 new loss 7.967679266585037e-05 old loss 7.969325815793127e-05 BETTER
I0327 21:08:43.821842 2041890 finetune.py:68] layer 4_v @ epoch 0 new loss 1.7529382603242993e-05 old loss 6.138525350252166e-05 BETTER
I0327 21:08:55.798807 2041820 finetune.py:68] layer 3_down @ epoch 4 new loss 7.966247358126566e-05 old loss 7.967679266585037e-05 BETTER
3_v proxy err 0.06859932094812393 tr(WHW.T) 289.3331604003906
bpp_loss 1.8448684312752448
3_q proxy err 0.002023979788646102 tr(WHW.T) 47553.453125
bpp_loss 2.7361839236400556
3_k proxy err 0.0010580109665170312 tr(WHW.T) 26134.556640625
bpp_loss 3.5154874532308895
3_o proxy err 0.04384208098053932 tr(WHW.T) 1858.44677734375
bpp_loss 1.9519636187178548
3_up proxy err 0.054908376187086105 tr(WHW.T) 7533.087890625
bpp_loss 2.092083416613085
3_gate proxy err 0.020953916013240814 tr(WHW.T) 20819.431640625
bpp_loss 2.3230751188738004
3_down proxy err 0.058635976165533066 tr(WHW.T) 7114.1484375
bpp_loss 2.088055041410761
I0327 21:09:08.121623 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 62.83165788650513s
I0327 21:09:11.750860 2041960 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:09:11.750960 2041960 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:09:11.750998 2041960 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:09:12.095474 2041960 config.py:54] PyTorch version 2.6.0 available.
W0327 21:09:12.286860 2041960 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:09:12.862193 2041960 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:09:12.865799 2041475 quantize_finetune_llama.py:209] layer 6 gpu 2
I0327 21:09:12.885599 2041960 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 21:09:20.727636 2041890 finetune.py:68] layer 4_v @ epoch 1 new loss 1.4051485777599737e-05 old loss 1.7529382603242993e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:09:29.752120 2041960 finetune.py:45] layer 5_v initial loss 5.925561345065944e-05
W0327 21:09:29.752420 2041960 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:09:57.640450 2041890 finetune.py:68] layer 4_v @ epoch 2 new loss 1.3102818229526747e-05 old loss 1.4051485777599737e-05 BETTER
I0327 21:10:03.106956 2041960 finetune.py:68] layer 5_v @ epoch 0 new loss 2.285516166011803e-05 old loss 5.925561345065944e-05 BETTER
I0327 21:10:15.858272 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 62.527515172958374s
I0327 21:10:19.645080 2042030 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:10:19.645180 2042030 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:10:19.645220 2042030 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:10:20.010452 2042030 config.py:54] PyTorch version 2.6.0 available.
W0327 21:10:20.204498 2042030 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:10:20.815055 2042030 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:10:20.818945 2041475 quantize_finetune_llama.py:209] layer 7 gpu 3
I0327 21:10:20.832969 2042030 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:10:34.844837 2041890 finetune.py:68] layer 4_v @ epoch 3 new loss 1.2635438906727359e-05 old loss 1.3102818229526747e-05 BETTER
I0327 21:10:37.618978 2041960 finetune.py:68] layer 5_v @ epoch 1 new loss 2.023490196734201e-05 old loss 2.285516166011803e-05 BETTER
I0327 21:10:37.702270 2042030 finetune.py:45] layer 6_v initial loss 5.3074105380801484e-05
W0327 21:10:37.702626 2042030 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:11:11.344102 2042030 finetune.py:68] layer 6_v @ epoch 0 new loss 2.7141182727064006e-05 old loss 5.3074105380801484e-05 BETTER
I0327 21:11:12.021729 2041890 finetune.py:68] layer 4_v @ epoch 4 new loss 1.2332224287092686e-05 old loss 1.2635438906727359e-05 BETTER
I0327 21:11:12.476605 2041960 finetune.py:68] layer 5_v @ epoch 2 new loss 1.9313054508529603e-05 old loss 2.023490196734201e-05 BETTER
I0327 21:11:24.259280 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 62.94853615760803s
I0327 21:11:28.231360 2042100 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:11:28.231542 2042100 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:11:28.231623 2042100 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:11:28.604713 2042100 config.py:54] PyTorch version 2.6.0 available.
W0327 21:11:28.836211 2042100 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:11:29.461911 2042100 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:11:29.466032 2041475 quantize_finetune_llama.py:209] layer 8 gpu 0
I0327 21:11:29.481138 2042100 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 21:11:31.108006 2041890 finetune.py:45] layer 4_q initial loss 1.50084497363423e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:11:46.191478 2042030 finetune.py:68] layer 6_v @ epoch 1 new loss 2.5011979232658632e-05 old loss 2.7141182727064006e-05 BETTER
I0327 21:11:46.563084 2042100 finetune.py:45] layer 7_v initial loss 5.238392259343527e-05
W0327 21:11:46.563317 2042100 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:11:47.467384 2041960 finetune.py:68] layer 5_v @ epoch 3 new loss 1.879053161246702e-05 old loss 1.9313054508529603e-05 BETTER
I0327 21:12:06.398595 2041890 finetune.py:68] layer 4_q @ epoch 0 new loss 1.4476147043751553e-05 old loss 1.50084497363423e-05 BETTER
I0327 21:12:19.668939 2042100 finetune.py:68] layer 7_v @ epoch 0 new loss 3.300335811218247e-05 old loss 5.238392259343527e-05 BETTER
I0327 21:12:21.315810 2042030 finetune.py:68] layer 6_v @ epoch 2 new loss 2.41417783399811e-05 old loss 2.5011979232658632e-05 BETTER
I0327 21:12:22.569139 2041960 finetune.py:68] layer 5_v @ epoch 4 new loss 1.842892925196793e-05 old loss 1.879053161246702e-05 BETTER
I0327 21:12:41.559138 2041960 finetune.py:45] layer 5_q initial loss 2.243608105345629e-05
I0327 21:12:42.976998 2041890 finetune.py:68] layer 4_q @ epoch 1 new loss 1.4205657862476073e-05 old loss 1.4476147043751553e-05 BETTER
I0327 21:12:54.062379 2042100 finetune.py:68] layer 7_v @ epoch 1 new loss 3.10659634124022e-05 old loss 3.300335811218247e-05 BETTER
I0327 21:12:56.702827 2042030 finetune.py:68] layer 6_v @ epoch 3 new loss 2.3597247491125017e-05 old loss 2.41417783399811e-05 BETTER
I0327 21:13:15.048033 2041960 finetune.py:68] layer 5_q @ epoch 0 new loss 2.182347816415131e-05 old loss 2.243608105345629e-05 BETTER
I0327 21:13:19.809468 2041890 finetune.py:68] layer 4_q @ epoch 2 new loss 1.3992606909596361e-05 old loss 1.4205657862476073e-05 BETTER
I0327 21:13:28.720693 2042100 finetune.py:68] layer 7_v @ epoch 2 new loss 3.014577305293642e-05 old loss 3.10659634124022e-05 BETTER
I0327 21:13:32.132686 2042030 finetune.py:68] layer 6_v @ epoch 4 new loss 2.3194763343781233e-05 old loss 2.3597247491125017e-05 BETTER
I0327 21:13:49.727148 2041960 finetune.py:68] layer 5_q @ epoch 1 new loss 2.1479147108038887e-05 old loss 2.182347816415131e-05 BETTER
I0327 21:13:51.741158 2042030 finetune.py:45] layer 6_q initial loss 2.7995289201498963e-05
I0327 21:13:56.703070 2041890 finetune.py:68] layer 4_q @ epoch 3 new loss 1.3819000741932541e-05 old loss 1.3992606909596361e-05 BETTER
I0327 21:14:03.363533 2042100 finetune.py:68] layer 7_v @ epoch 3 new loss 2.952874820039142e-05 old loss 3.014577305293642e-05 BETTER
I0327 21:14:24.349841 2041960 finetune.py:68] layer 5_q @ epoch 2 new loss 2.1204008589847945e-05 old loss 2.1479147108038887e-05 BETTER
I0327 21:14:25.548028 2042030 finetune.py:68] layer 6_q @ epoch 0 new loss 2.735539965215139e-05 old loss 2.7995289201498963e-05 BETTER
I0327 21:14:33.638251 2041890 finetune.py:68] layer 4_q @ epoch 4 new loss 1.366736523777945e-05 old loss 1.3819000741932541e-05 BETTER
I0327 21:14:38.055490 2042100 finetune.py:68] layer 7_v @ epoch 4 new loss 2.9064485715935007e-05 old loss 2.952874820039142e-05 BETTER
I0327 21:14:51.238826 2041890 finetune.py:45] layer 4_k initial loss 1.4577341971744318e-05
I0327 21:14:57.262519 2042100 finetune.py:45] layer 7_q initial loss 3.6152854590909556e-05
I0327 21:14:59.002047 2041960 finetune.py:68] layer 5_q @ epoch 3 new loss 2.097584911098238e-05 old loss 2.1204008589847945e-05 BETTER
I0327 21:15:00.253335 2042030 finetune.py:68] layer 6_q @ epoch 1 new loss 2.6958010494126938e-05 old loss 2.735539965215139e-05 BETTER
I0327 21:15:26.930389 2041890 finetune.py:68] layer 4_k @ epoch 0 new loss 1.4418905266211368e-05 old loss 1.4577341971744318e-05 BETTER
I0327 21:15:30.432215 2042100 finetune.py:68] layer 7_q @ epoch 0 new loss 3.5303470212966204e-05 old loss 3.6152854590909556e-05 BETTER
I0327 21:15:33.775368 2041960 finetune.py:68] layer 5_q @ epoch 4 new loss 2.077728640870191e-05 old loss 2.097584911098238e-05 BETTER
I0327 21:15:35.239650 2042030 finetune.py:68] layer 6_q @ epoch 2 new loss 2.6631501896190457e-05 old loss 2.6958010494126938e-05 BETTER
I0327 21:15:51.386337 2041960 finetune.py:45] layer 5_k initial loss 2.1961552192806266e-05
I0327 21:16:03.917714 2041890 finetune.py:68] layer 4_k @ epoch 1 new loss 1.4299186659627594e-05 old loss 1.4418905266211368e-05 BETTER
I0327 21:16:04.690628 2042100 finetune.py:68] layer 7_q @ epoch 1 new loss 3.479853694443591e-05 old loss 3.5303470212966204e-05 BETTER
I0327 21:16:10.310979 2042030 finetune.py:68] layer 6_q @ epoch 3 new loss 2.6352387067163363e-05 old loss 2.6631501896190457e-05 BETTER
I0327 21:16:24.936765 2041960 finetune.py:68] layer 5_k @ epoch 0 new loss 2.1713809474022128e-05 old loss 2.1961552192806266e-05 BETTER
I0327 21:16:38.862625 2042100 finetune.py:68] layer 7_q @ epoch 2 new loss 3.438396015553735e-05 old loss 3.479853694443591e-05 BETTER
I0327 21:16:40.807785 2041890 finetune.py:68] layer 4_k @ epoch 2 new loss 1.4195292351359967e-05 old loss 1.4299186659627594e-05 BETTER
I0327 21:16:45.433628 2042030 finetune.py:68] layer 6_q @ epoch 4 new loss 2.6104460630449466e-05 old loss 2.6352387067163363e-05 BETTER
I0327 21:16:59.265408 2041960 finetune.py:68] layer 5_k @ epoch 1 new loss 2.155280344595667e-05 old loss 2.1713809474022128e-05 BETTER
I0327 21:17:03.063917 2042030 finetune.py:45] layer 6_k initial loss 2.7678976039169356e-05
I0327 21:17:13.168522 2042100 finetune.py:68] layer 7_q @ epoch 3 new loss 3.40434635290876e-05 old loss 3.438396015553735e-05 BETTER
I0327 21:17:17.658921 2041890 finetune.py:68] layer 4_k @ epoch 3 new loss 1.4104331967246253e-05 old loss 1.4195292351359967e-05 BETTER
I0327 21:17:33.885841 2041960 finetune.py:68] layer 5_k @ epoch 2 new loss 2.1409308828879148e-05 old loss 2.155280344595667e-05 BETTER
I0327 21:17:36.673756 2042030 finetune.py:68] layer 6_k @ epoch 0 new loss 2.734528243308887e-05 old loss 2.7678976039169356e-05 BETTER
I0327 21:17:47.473239 2042100 finetune.py:68] layer 7_q @ epoch 4 new loss 3.372895662323572e-05 old loss 3.40434635290876e-05 BETTER
I0327 21:17:54.510956 2041890 finetune.py:68] layer 4_k @ epoch 4 new loss 1.401669669576222e-05 old loss 1.4104331967246253e-05 BETTER
I0327 21:18:05.125694 2042100 finetune.py:45] layer 7_k initial loss 3.571448178263381e-05
I0327 21:18:08.383013 2041960 finetune.py:68] layer 5_k @ epoch 3 new loss 2.1280722648953088e-05 old loss 2.1409308828879148e-05 BETTER
I0327 21:18:11.170067 2042030 finetune.py:68] layer 6_k @ epoch 1 new loss 2.7147354558110237e-05 old loss 2.734528243308887e-05 BETTER
I0327 21:18:13.666139 2041890 finetune.py:45] layer 4_o initial loss 3.540609031915665e-05
I0327 21:18:38.020879 2042100 finetune.py:68] layer 7_k @ epoch 0 new loss 3.528969318722375e-05 old loss 3.571448178263381e-05 BETTER
I0327 21:18:42.936013 2041960 finetune.py:68] layer 5_k @ epoch 4 new loss 2.116377800120972e-05 old loss 2.1280722648953088e-05 BETTER
I0327 21:18:45.855212 2042030 finetune.py:68] layer 6_k @ epoch 2 new loss 2.6971189072355628e-05 old loss 2.7147354558110237e-05 BETTER
I0327 21:18:48.478640 2041890 finetune.py:68] layer 4_o @ epoch 0 new loss 3.2180745620280504e-05 old loss 3.540609031915665e-05 BETTER
I0327 21:19:02.521188 2041960 finetune.py:45] layer 5_o initial loss 4.663718573283404e-05
I0327 21:19:12.024698 2042100 finetune.py:68] layer 7_k @ epoch 1 new loss 3.503409243421629e-05 old loss 3.528969318722375e-05 BETTER
I0327 21:19:20.642600 2042030 finetune.py:68] layer 6_k @ epoch 3 new loss 2.6809060727828182e-05 old loss 2.6971189072355628e-05 BETTER
I0327 21:19:24.208915 2041890 finetune.py:68] layer 4_o @ epoch 1 new loss 3.1273742933990434e-05 old loss 3.2180745620280504e-05 BETTER
I0327 21:19:35.426447 2041960 finetune.py:68] layer 5_o @ epoch 0 new loss 4.3290059693390504e-05 old loss 4.663718573283404e-05 BETTER
I0327 21:19:46.147139 2042100 finetune.py:68] layer 7_k @ epoch 2 new loss 3.48081775882747e-05 old loss 3.503409243421629e-05 BETTER
I0327 21:19:55.550899 2042030 finetune.py:68] layer 6_k @ epoch 4 new loss 2.6667852580430917e-05 old loss 2.6809060727828182e-05 BETTER
I0327 21:20:00.243637 2041890 finetune.py:68] layer 4_o @ epoch 2 new loss 3.073746484005824e-05 old loss 3.1273742933990434e-05 BETTER
I0327 21:20:09.237224 2041960 finetune.py:68] layer 5_o @ epoch 1 new loss 4.2331292206654325e-05 old loss 4.3290059693390504e-05 BETTER
I0327 21:20:14.899723 2042030 finetune.py:45] layer 6_o initial loss 6.224911339813843e-05
I0327 21:20:20.147930 2042100 finetune.py:68] layer 7_k @ epoch 3 new loss 3.4610420698300004e-05 old loss 3.48081775882747e-05 BETTER
I0327 21:20:36.412494 2041890 finetune.py:68] layer 4_o @ epoch 3 new loss 3.0348997825058177e-05 old loss 3.073746484005824e-05 BETTER
I0327 21:20:43.131918 2041960 finetune.py:68] layer 5_o @ epoch 2 new loss 4.174093919573352e-05 old loss 4.2331292206654325e-05 BETTER
I0327 21:20:47.953294 2042030 finetune.py:68] layer 6_o @ epoch 0 new loss 5.894010973861441e-05 old loss 6.224911339813843e-05 BETTER
I0327 21:20:54.180656 2042100 finetune.py:68] layer 7_k @ epoch 4 new loss 3.442459274083376e-05 old loss 3.4610420698300004e-05 BETTER
I0327 21:21:12.511346 2041890 finetune.py:68] layer 4_o @ epoch 4 new loss 3.004058999067638e-05 old loss 3.0348997825058177e-05 BETTER
I0327 21:21:13.462059 2042100 finetune.py:45] layer 7_o initial loss 7.682199066039175e-05
I0327 21:21:17.037834 2041960 finetune.py:68] layer 5_o @ epoch 3 new loss 4.130538218305446e-05 old loss 4.174093919573352e-05 BETTER
I0327 21:21:22.013183 2042030 finetune.py:68] layer 6_o @ epoch 1 new loss 5.792855517938733e-05 old loss 5.894010973861441e-05 BETTER
I0327 21:21:43.197409 2041890 finetune.py:45] layer 4_up initial loss 6.908347859280184e-05
I0327 21:21:46.080780 2042100 finetune.py:68] layer 7_o @ epoch 0 new loss 7.376826397376135e-05 old loss 7.682199066039175e-05 BETTER
I0327 21:21:50.868683 2041960 finetune.py:68] layer 5_o @ epoch 4 new loss 4.095999611308798e-05 old loss 4.130538218305446e-05 BETTER
I0327 21:21:55.986700 2042030 finetune.py:68] layer 6_o @ epoch 2 new loss 5.724314178223722e-05 old loss 5.792855517938733e-05 BETTER
I0327 21:22:15.365394 2041890 finetune.py:68] layer 4_up @ epoch 0 new loss 6.832191866124049e-05 old loss 6.908347859280184e-05 BETTER
I0327 21:22:19.273360 2042100 finetune.py:68] layer 7_o @ epoch 1 new loss 7.26373036741279e-05 old loss 7.376826397376135e-05 BETTER
I0327 21:22:22.057552 2041960 finetune.py:45] layer 5_up initial loss 9.75826260400936e-05
I0327 21:22:30.052462 2042030 finetune.py:68] layer 6_o @ epoch 3 new loss 5.670935206580907e-05 old loss 5.724314178223722e-05 BETTER
I0327 21:22:48.800399 2041890 finetune.py:68] layer 4_up @ epoch 1 new loss 6.784834113204852e-05 old loss 6.832191866124049e-05 BETTER
I0327 21:22:52.647192 2041960 finetune.py:68] layer 5_up @ epoch 0 new loss 9.636179311200976e-05 old loss 9.75826260400936e-05 BETTER
I0327 21:22:52.839107 2042100 finetune.py:68] layer 7_o @ epoch 2 new loss 7.182623085100204e-05 old loss 7.26373036741279e-05 BETTER
I0327 21:23:04.363069 2042030 finetune.py:68] layer 6_o @ epoch 4 new loss 5.6257547839777544e-05 old loss 5.670935206580907e-05 BETTER
I0327 21:23:22.603667 2041890 finetune.py:68] layer 4_up @ epoch 2 new loss 6.746265717083588e-05 old loss 6.784834113204852e-05 BETTER
I0327 21:23:24.650012 2041960 finetune.py:68] layer 5_up @ epoch 1 new loss 9.560986654832959e-05 old loss 9.636179311200976e-05 BETTER
I0327 21:23:26.501311 2042100 finetune.py:68] layer 7_o @ epoch 3 new loss 7.116557389963418e-05 old loss 7.182623085100204e-05 BETTER
I0327 21:23:35.974789 2042030 finetune.py:45] layer 6_up initial loss 0.0001277395786019042
I0327 21:23:56.536674 2041890 finetune.py:68] layer 4_up @ epoch 3 new loss 6.71168600092642e-05 old loss 6.746265717083588e-05 BETTER
I0327 21:23:56.666198 2041960 finetune.py:68] layer 5_up @ epoch 2 new loss 9.498297731624916e-05 old loss 9.560986654832959e-05 BETTER
I0327 21:23:59.980954 2042100 finetune.py:68] layer 7_o @ epoch 4 new loss 7.060918142087758e-05 old loss 7.116557389963418e-05 BETTER
I0327 21:24:06.790330 2042030 finetune.py:68] layer 6_up @ epoch 0 new loss 0.00012607581447809935 old loss 0.0001277395786019042 BETTER
I0327 21:24:28.904968 2041960 finetune.py:68] layer 5_up @ epoch 3 new loss 9.443130693398416e-05 old loss 9.498297731624916e-05 BETTER
I0327 21:24:30.471926 2041890 finetune.py:68] layer 4_up @ epoch 4 new loss 6.680424849037081e-05 old loss 6.71168600092642e-05 BETTER
I0327 21:24:30.796388 2042100 finetune.py:45] layer 7_up initial loss 0.0001476747856941074
I0327 21:24:38.915965 2042030 finetune.py:68] layer 6_up @ epoch 1 new loss 0.00012500958109740168 old loss 0.00012607581447809935 BETTER
I0327 21:25:00.944261 2041890 finetune.py:45] layer 4_gate initial loss 8.174991671694443e-05
I0327 21:25:01.075980 2042100 finetune.py:68] layer 7_up @ epoch 0 new loss 0.00014576787361875176 old loss 0.0001476747856941074 BETTER
I0327 21:25:01.232921 2041960 finetune.py:68] layer 5_up @ epoch 4 new loss 9.392711945110932e-05 old loss 9.443130693398416e-05 BETTER
I0327 21:25:11.109319 2042030 finetune.py:68] layer 6_up @ epoch 2 new loss 0.00012410742056090385 old loss 0.00012500958109740168 BETTER
I0327 21:25:31.270937 2041890 finetune.py:68] layer 4_gate @ epoch 0 new loss 8.132695802487433e-05 old loss 8.174991671694443e-05 BETTER
I0327 21:25:32.113963 2041960 finetune.py:45] layer 5_gate initial loss 0.00011524352157721296
I0327 21:25:32.306700 2042100 finetune.py:68] layer 7_up @ epoch 1 new loss 0.00014455185737460852 old loss 0.00014576787361875176 BETTER
I0327 21:25:43.345744 2042030 finetune.py:68] layer 6_up @ epoch 3 new loss 0.00012329977471381426 old loss 0.00012410742056090385 BETTER
I0327 21:26:00.708140 2041960 finetune.py:68] layer 5_gate @ epoch 0 new loss 0.0001145794740295969 old loss 0.00011524352157721296 BETTER
I0327 21:26:02.605732 2041890 finetune.py:68] layer 4_gate @ epoch 1 new loss 8.099526166915894e-05 old loss 8.132695802487433e-05 BETTER
I0327 21:26:03.775619 2042100 finetune.py:68] layer 7_up @ epoch 2 new loss 0.00014350157289300114 old loss 0.00014455185737460852 BETTER
I0327 21:26:15.646604 2042030 finetune.py:68] layer 6_up @ epoch 4 new loss 0.00012254937610123307 old loss 0.00012329977471381426 BETTER
I0327 21:26:30.362981 2041960 finetune.py:68] layer 5_gate @ epoch 1 new loss 0.00011404795804992318 old loss 0.0001145794740295969 BETTER
I0327 21:26:34.260849 2041890 finetune.py:68] layer 4_gate @ epoch 2 new loss 8.069285104284063e-05 old loss 8.099526166915894e-05 BETTER
I0327 21:26:35.326979 2042100 finetune.py:68] layer 7_up @ epoch 3 new loss 0.00014254912093747407 old loss 0.00014350157289300114 BETTER
I0327 21:26:46.794300 2042030 finetune.py:45] layer 6_gate initial loss 0.00014741862833034247
I0327 21:27:00.112053 2041960 finetune.py:68] layer 5_gate @ epoch 2 new loss 0.00011356847971910611 old loss 0.00011404795804992318 BETTER
I0327 21:27:05.964301 2041890 finetune.py:68] layer 4_gate @ epoch 3 new loss 8.041133696679026e-05 old loss 8.069285104284063e-05 BETTER
I0327 21:27:06.989531 2042100 finetune.py:68] layer 7_up @ epoch 4 new loss 0.00014166414621286094 old loss 0.00014254912093747407 BETTER
I0327 21:27:15.670059 2042030 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.0001464547822251916 old loss 0.00014741862833034247 BETTER
I0327 21:27:30.003294 2041960 finetune.py:68] layer 5_gate @ epoch 3 new loss 0.00011313516006339341 old loss 0.00011356847971910611 BETTER
I0327 21:27:37.666385 2041890 finetune.py:68] layer 4_gate @ epoch 4 new loss 8.014456398086622e-05 old loss 8.041133696679026e-05 BETTER
I0327 21:27:37.895526 2042100 finetune.py:45] layer 7_gate initial loss 0.00017142108117695898
I0327 21:27:45.547930 2042030 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.00014569213089998811 old loss 0.0001464547822251916 BETTER
I0327 21:27:59.731210 2041960 finetune.py:68] layer 5_gate @ epoch 4 new loss 0.00011271734547335654 old loss 0.00011313516006339341 BETTER
I0327 21:28:06.375835 2042100 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.0001702792214928195 old loss 0.00017142108117695898 BETTER
I0327 21:28:15.493782 2042030 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.00014501421537715942 old loss 0.00014569213089998811 BETTER
I0327 21:28:32.998527 2041890 finetune.py:45] layer 4_down initial loss 0.00012794797657988966
I0327 21:28:35.490637 2042100 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.00016940754721872509 old loss 0.0001702792214928195 BETTER
I0327 21:28:45.308858 2042030 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.0001443797373212874 old loss 0.00014501421537715942 BETTER
I0327 21:28:55.804788 2041960 finetune.py:45] layer 5_down initial loss 0.00017840640794020146
I0327 21:29:00.752863 2041890 finetune.py:68] layer 4_down @ epoch 0 new loss 0.00012790760956704617 old loss 0.00012794797657988966 BETTER
I0327 21:29:04.731338 2042100 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.0001686326286289841 old loss 0.00016940754721872509 BETTER
I0327 21:29:15.379459 2042030 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.00014378568448591977 old loss 0.0001443797373212874 BETTER
I0327 21:29:22.081006 2041960 finetune.py:68] layer 5_down @ epoch 0 new loss 0.00017836474580690265 old loss 0.00017840640794020146 BETTER
I0327 21:29:29.205806 2041890 finetune.py:68] layer 4_down @ epoch 1 new loss 0.00012787281593773514 old loss 0.00012790760956704617 BETTER
I0327 21:29:34.075200 2042100 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.00016790626978036016 old loss 0.0001686326286289841 BETTER
I0327 21:29:49.355538 2041960 finetune.py:68] layer 5_down @ epoch 1 new loss 0.00017832944286055863 old loss 0.00017836474580690265 BETTER
I0327 21:29:57.916417 2041890 finetune.py:68] layer 4_down @ epoch 2 new loss 0.00012784329010173678 old loss 0.00012787281593773514 BETTER
I0327 21:30:03.442790 2042100 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.00016723033331800252 old loss 0.00016790626978036016 BETTER
I0327 21:30:12.145308 2042030 finetune.py:45] layer 6_down initial loss 0.0002234544517705217
I0327 21:30:16.967218 2041960 finetune.py:68] layer 5_down @ epoch 2 new loss 0.0001782972103683278 old loss 0.00017832944286055863 BETTER
I0327 21:30:26.872637 2041890 finetune.py:68] layer 4_down @ epoch 3 new loss 0.00012781692203134298 old loss 0.00012784329010173678 BETTER
I0327 21:30:38.592386 2042030 finetune.py:68] layer 6_down @ epoch 0 new loss 0.00022341875592246652 old loss 0.0002234544517705217 BETTER
I0327 21:30:44.517254 2041960 finetune.py:68] layer 5_down @ epoch 3 new loss 0.00017826931434683502 old loss 0.0001782972103683278 BETTER
I0327 21:30:56.290333 2041890 finetune.py:68] layer 4_down @ epoch 4 new loss 0.00012779393000528216 old loss 0.00012781692203134298 BETTER
4_v proxy err 0.06267482042312622 tr(WHW.T) 285.30712890625
bpp_loss 1.8879948928952217
4_q proxy err 0.0016914955340325832 tr(WHW.T) 50134.4921875
bpp_loss 2.7025544260104652
4_k proxy err 0.0008333819569088519 tr(WHW.T) 29267.79296875
bpp_loss 3.500669501721859
4_o proxy err 0.05017551779747009 tr(WHW.T) 1306.5306396484375
bpp_loss 1.9595321930246428
4_up proxy err 0.0555829256772995 tr(WHW.T) 7373.642578125
bpp_loss 2.06408840856914
4_gate proxy err 0.015270276926457882 tr(WHW.T) 28899.58203125
bpp_loss 2.388936373637989
4_down proxy err 0.06302351504564285 tr(WHW.T) 6491.5634765625
bpp_loss 2.06526855750209
I0327 21:31:00.005263 2042100 finetune.py:45] layer 7_down initial loss 0.00025407870998606086
I0327 21:31:07.276528 2042030 finetune.py:68] layer 6_down @ epoch 1 new loss 0.00022338720737025142 old loss 0.00022341875592246652 BETTER
I0327 21:31:12.922999 2041960 finetune.py:68] layer 5_down @ epoch 4 new loss 0.00017824381939135492 old loss 0.00017826931434683502 BETTER
5_v proxy err 0.08659811317920685 tr(WHW.T) 208.81988525390625
bpp_loss 1.7756945817673113
5_q proxy err 0.00243663159199059 tr(WHW.T) 36019.90234375
bpp_loss 2.6788291590637527
5_k proxy err 0.0010983007960021496 tr(WHW.T) 22978.275390625
bpp_loss 3.4726305207586847
5_o proxy err 0.05363239347934723 tr(WHW.T) 1069.2933349609375
bpp_loss 1.9135236449074
5_up proxy err 0.053551990538835526 tr(WHW.T) 7648.6318359375
bpp_loss 2.068491459691099
5_gate proxy err 0.014622959308326244 tr(WHW.T) 30133.8125
bpp_loss 2.3909454345037893
5_down proxy err 0.060823358595371246 tr(WHW.T) 6507.87939453125
bpp_loss 2.0700928194731074
I0327 21:31:26.672284 2042100 finetune.py:68] layer 7_down @ epoch 0 new loss 0.0002540399436838925 old loss 0.00025407870998606086 BETTER
I0327 21:31:35.012830 2042030 finetune.py:68] layer 6_down @ epoch 2 new loss 0.00022335900575853884 old loss 0.00022338720737025142 BETTER
I0327 21:31:53.686758 2042100 finetune.py:68] layer 7_down @ epoch 1 new loss 0.00025400397134944797 old loss 0.0002540399436838925 BETTER
I0327 21:32:02.837218 2042030 finetune.py:68] layer 6_down @ epoch 3 new loss 0.0002233320992672816 old loss 0.00022335900575853884 BETTER
I0327 21:32:20.820516 2042100 finetune.py:68] layer 7_down @ epoch 2 new loss 0.00025397184072062373 old loss 0.00025400397134944797 BETTER
I0327 21:32:23.742704 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 66.14191508293152s
I0327 21:32:27.661710 2042170 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:32:27.661817 2042170 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:32:27.661861 2042170 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:32:28.053394 2042170 config.py:54] PyTorch version 2.6.0 available.
W0327 21:32:28.279494 2042170 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:32:28.905025 2042170 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:32:28.909078 2041475 quantize_finetune_llama.py:209] layer 9 gpu 1
I0327 21:32:28.924804 2042170 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 21:32:30.751276 2042030 finetune.py:68] layer 6_down @ epoch 4 new loss 0.00022330843785312027 old loss 0.0002233320992672816 BETTER
6_v proxy err 0.07379081100225449 tr(WHW.T) 253.63377380371094
bpp_loss 1.821423571149353
6_q proxy err 0.002548698103055358 tr(WHW.T) 35682.15234375
bpp_loss 2.722395781369414
6_k proxy err 0.0009991596452891827 tr(WHW.T) 26130.9296875
bpp_loss 3.5410192033741623
6_o proxy err 0.06239347532391548 tr(WHW.T) 1021.3829345703125
bpp_loss 1.9403716095548589
6_up proxy err 0.05048578232526779 tr(WHW.T) 7912.38720703125
bpp_loss 2.067824569530785
6_gate proxy err 0.012126008048653603 tr(WHW.T) 35465.69921875
bpp_loss 2.3952367108847414
6_down proxy err 0.0584515817463398 tr(WHW.T) 6572.22900390625
bpp_loss 2.071256792771497
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:32:45.634370 2042170 finetune.py:45] layer 8_v initial loss 5.484597204485908e-05
W0327 21:32:45.634563 2042170 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:32:48.051597 2042100 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00025394235854037106 old loss 0.00025397184072062373 BETTER
I0327 21:33:15.507094 2042100 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0002539156994316727 old loss 0.00025394235854037106 BETTER
7_v proxy err 0.061189260333776474 tr(WHW.T) 309.4270935058594
bpp_loss 1.8197095473587979
7_q proxy err 0.002593749901279807 tr(WHW.T) 35170.703125
bpp_loss 2.6539946015691385
7_k proxy err 0.0009939299197867513 tr(WHW.T) 26802.923828125
bpp_loss 3.5614957193029113
7_o proxy err 0.05458175390958786 tr(WHW.T) 975.4956665039062
bpp_loss 1.9515928706969135
7_up proxy err 0.04582182317972183 tr(WHW.T) 8610.955078125
bpp_loss 2.0803479266885136
7_gate proxy err 0.012121735140681267 tr(WHW.T) 34726.30078125
bpp_loss 2.367808608471283
7_down proxy err 0.05864362791180611 tr(WHW.T) 6603.16748046875
bpp_loss 2.0844870521520664
I0327 21:33:20.940784 2042170 finetune.py:68] layer 8_v @ epoch 0 new loss 3.781072882702574e-05 old loss 5.484597204485908e-05 BETTER
I0327 21:33:37.559484 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 62.560909271240234s
I0327 21:33:41.093151 2042240 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:33:41.093248 2042240 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:33:41.093286 2042240 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:33:41.422838 2042240 config.py:54] PyTorch version 2.6.0 available.
W0327 21:33:41.609811 2042240 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:33:42.175331 2042240 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:33:42.179726 2041475 quantize_finetune_llama.py:209] layer 10 gpu 2
I0327 21:33:42.193027 2042240 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:33:57.718160 2042170 finetune.py:68] layer 8_v @ epoch 1 new loss 3.578936230042018e-05 old loss 3.781072882702574e-05 BETTER
I0327 21:33:58.637891 2042240 finetune.py:45] layer 9_v initial loss 5.938355025136843e-05
W0327 21:33:58.638110 2042240 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:34:32.002264 2042240 finetune.py:68] layer 9_v @ epoch 0 new loss 3.873442983604036e-05 old loss 5.938355025136843e-05 BETTER
I0327 21:34:35.009460 2042170 finetune.py:68] layer 8_v @ epoch 2 new loss 3.4758366382448e-05 old loss 3.578936230042018e-05 BETTER
I0327 21:34:45.638309 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 62.986833572387695s
I0327 21:34:49.475691 2042310 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:34:49.475791 2042310 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:34:49.475844 2042310 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:34:49.851375 2042310 config.py:54] PyTorch version 2.6.0 available.
W0327 21:34:50.040294 2042310 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:34:50.640098 2042310 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:34:50.643938 2041475 quantize_finetune_llama.py:209] layer 11 gpu 3
I0327 21:34:50.657617 2042310 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:35:06.917865 2042240 finetune.py:68] layer 9_v @ epoch 1 new loss 3.6425939470063895e-05 old loss 3.873442983604036e-05 BETTER
I0327 21:35:07.406744 2042310 finetune.py:45] layer 10_v initial loss 7.10634994902648e-05
W0327 21:35:07.406973 2042310 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:35:12.106792 2042170 finetune.py:68] layer 8_v @ epoch 3 new loss 3.40448459610343e-05 old loss 3.4758366382448e-05 BETTER
I0327 21:35:40.973047 2042310 finetune.py:68] layer 10_v @ epoch 0 new loss 5.120339847053401e-05 old loss 7.10634994902648e-05 BETTER
I0327 21:35:41.697129 2042240 finetune.py:68] layer 9_v @ epoch 2 new loss 3.527384251356125e-05 old loss 3.6425939470063895e-05 BETTER
I0327 21:35:49.300072 2042170 finetune.py:68] layer 8_v @ epoch 4 new loss 3.3499170967843384e-05 old loss 3.40448459610343e-05 BETTER
I0327 21:35:54.248644 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 63.10915517807007s
I0327 21:35:57.977215 2042380 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:35:57.977323 2042380 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:35:57.977369 2042380 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:35:58.318795 2042380 config.py:54] PyTorch version 2.6.0 available.
W0327 21:35:58.528431 2042380 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:35:59.133900 2042380 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:35:59.138109 2041475 quantize_finetune_llama.py:209] layer 12 gpu 0
I0327 21:35:59.157526 2042380 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 21:36:08.224977 2042170 finetune.py:45] layer 8_q initial loss 4.146681749261916e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:36:15.799772 2042310 finetune.py:68] layer 10_v @ epoch 1 new loss 4.868639007327147e-05 old loss 5.120339847053401e-05 BETTER
I0327 21:36:16.325040 2042380 finetune.py:45] layer 11_v initial loss 6.296160427154973e-05
W0327 21:36:16.325443 2042380 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:36:16.616423 2042240 finetune.py:68] layer 9_v @ epoch 3 new loss 3.4508109820308164e-05 old loss 3.527384251356125e-05 BETTER
I0327 21:36:43.465295 2042170 finetune.py:68] layer 8_q @ epoch 0 new loss 4.034757148474455e-05 old loss 4.146681749261916e-05 BETTER
I0327 21:36:49.469018 2042380 finetune.py:68] layer 11_v @ epoch 0 new loss 4.6160173951648176e-05 old loss 6.296160427154973e-05 BETTER
I0327 21:36:51.068839 2042310 finetune.py:68] layer 10_v @ epoch 2 new loss 4.7328416258096695e-05 old loss 4.868639007327147e-05 BETTER
I0327 21:36:51.780964 2042240 finetune.py:68] layer 9_v @ epoch 4 new loss 3.393537917872891e-05 old loss 3.4508109820308164e-05 BETTER
I0327 21:37:10.812648 2042240 finetune.py:45] layer 9_q initial loss 4.2627194488886744e-05
I0327 21:37:20.122911 2042170 finetune.py:68] layer 8_q @ epoch 1 new loss 3.9772450691089034e-05 old loss 4.034757148474455e-05 BETTER
I0327 21:37:23.821202 2042380 finetune.py:68] layer 11_v @ epoch 1 new loss 4.377801815280691e-05 old loss 4.6160173951648176e-05 BETTER
I0327 21:37:26.299342 2042310 finetune.py:68] layer 10_v @ epoch 3 new loss 4.636708035832271e-05 old loss 4.7328416258096695e-05 BETTER
I0327 21:37:44.370699 2042240 finetune.py:68] layer 9_q @ epoch 0 new loss 4.161234028288163e-05 old loss 4.2627194488886744e-05 BETTER
I0327 21:37:56.896894 2042170 finetune.py:68] layer 8_q @ epoch 2 new loss 3.9309714338742197e-05 old loss 3.9772450691089034e-05 BETTER
I0327 21:37:58.314840 2042380 finetune.py:68] layer 11_v @ epoch 2 new loss 4.249714038451202e-05 old loss 4.377801815280691e-05 BETTER
I0327 21:38:01.501879 2042310 finetune.py:68] layer 10_v @ epoch 4 new loss 4.5612920075654984e-05 old loss 4.636708035832271e-05 BETTER
I0327 21:38:18.738197 2042240 finetune.py:68] layer 9_q @ epoch 1 new loss 4.101721060578711e-05 old loss 4.161234028288163e-05 BETTER
I0327 21:38:21.077260 2042310 finetune.py:45] layer 10_q initial loss 5.567876360146329e-05
I0327 21:38:32.986457 2042380 finetune.py:68] layer 11_v @ epoch 3 new loss 4.1614999645389616e-05 old loss 4.249714038451202e-05 BETTER
I0327 21:38:33.907637 2042170 finetune.py:68] layer 8_q @ epoch 3 new loss 3.8897305785212666e-05 old loss 3.9309714338742197e-05 BETTER
I0327 21:38:53.389624 2042240 finetune.py:68] layer 9_q @ epoch 2 new loss 4.053130396641791e-05 old loss 4.101721060578711e-05 BETTER
I0327 21:38:54.785391 2042310 finetune.py:68] layer 10_q @ epoch 0 new loss 5.4215190175455064e-05 old loss 5.567876360146329e-05 BETTER
I0327 21:39:07.831200 2042380 finetune.py:68] layer 11_v @ epoch 4 new loss 4.091674418305047e-05 old loss 4.1614999645389616e-05 BETTER
I0327 21:39:10.874429 2042170 finetune.py:68] layer 8_q @ epoch 4 new loss 3.85548519261647e-05 old loss 3.8897305785212666e-05 BETTER
I0327 21:39:27.039252 2042380 finetune.py:45] layer 11_q initial loss 5.272596899885684e-05
I0327 21:39:28.230137 2042240 finetune.py:68] layer 9_q @ epoch 3 new loss 4.011965575045906e-05 old loss 4.053130396641791e-05 BETTER
I0327 21:39:28.264771 2042170 finetune.py:45] layer 8_k initial loss 4.08765918109566e-05
I0327 21:39:29.810411 2042310 finetune.py:68] layer 10_q @ epoch 1 new loss 5.3438790928339586e-05 old loss 5.4215190175455064e-05 BETTER
I0327 21:40:00.239984 2042380 finetune.py:68] layer 11_q @ epoch 0 new loss 5.135643368703313e-05 old loss 5.272596899885684e-05 BETTER
I0327 21:40:03.072214 2042240 finetune.py:68] layer 9_q @ epoch 4 new loss 3.976584048359655e-05 old loss 4.011965575045906e-05 BETTER
I0327 21:40:04.027262 2042170 finetune.py:68] layer 8_k @ epoch 0 new loss 4.018254185211845e-05 old loss 4.08765918109566e-05 BETTER
I0327 21:40:04.725276 2042310 finetune.py:68] layer 10_q @ epoch 2 new loss 5.2794894145336e-05 old loss 5.3438790928339586e-05 BETTER
I0327 21:40:20.972819 2042240 finetune.py:45] layer 9_k initial loss 4.2448089516256005e-05
I0327 21:40:34.509522 2042380 finetune.py:68] layer 11_q @ epoch 1 new loss 5.063010394223966e-05 old loss 5.135643368703313e-05 BETTER
I0327 21:40:39.564330 2042310 finetune.py:68] layer 10_q @ epoch 3 new loss 5.224515916779637e-05 old loss 5.2794894145336e-05 BETTER
I0327 21:40:40.866070 2042170 finetune.py:68] layer 8_k @ epoch 1 new loss 3.9880313124740496e-05 old loss 4.018254185211845e-05 BETTER
I0327 21:40:54.415202 2042240 finetune.py:68] layer 9_k @ epoch 0 new loss 4.1804025386227295e-05 old loss 4.2448089516256005e-05 BETTER
I0327 21:41:08.832226 2042380 finetune.py:68] layer 11_q @ epoch 2 new loss 5.002109537599608e-05 old loss 5.063010394223966e-05 BETTER
I0327 21:41:14.572965 2042310 finetune.py:68] layer 10_q @ epoch 4 new loss 5.1761460781563073e-05 old loss 5.224515916779637e-05 BETTER
I0327 21:41:17.834706 2042170 finetune.py:68] layer 8_k @ epoch 2 new loss 3.962375194532797e-05 old loss 3.9880313124740496e-05 BETTER
I0327 21:41:29.035652 2042240 finetune.py:68] layer 9_k @ epoch 1 new loss 4.1482257074676454e-05 old loss 4.1804025386227295e-05 BETTER
I0327 21:41:32.676476 2042310 finetune.py:45] layer 10_k initial loss 5.4252741392701864e-05
I0327 21:41:43.155066 2042380 finetune.py:68] layer 11_q @ epoch 3 new loss 4.9508023948874325e-05 old loss 5.002109537599608e-05 BETTER
I0327 21:41:54.592906 2042170 finetune.py:68] layer 8_k @ epoch 3 new loss 3.938965892302804e-05 old loss 3.962375194532797e-05 BETTER
I0327 21:42:03.627475 2042240 finetune.py:68] layer 9_k @ epoch 2 new loss 4.120578523725271e-05 old loss 4.1482257074676454e-05 BETTER
I0327 21:42:06.328974 2042310 finetune.py:68] layer 10_k @ epoch 0 new loss 5.358416819944978e-05 old loss 5.4252741392701864e-05 BETTER
I0327 21:42:17.397334 2042380 finetune.py:68] layer 11_q @ epoch 4 new loss 4.905820242129266e-05 old loss 4.9508023948874325e-05 BETTER
I0327 21:42:31.581408 2042170 finetune.py:68] layer 8_k @ epoch 4 new loss 3.91793146263808e-05 old loss 3.938965892302804e-05 BETTER
I0327 21:42:35.260027 2042380 finetune.py:45] layer 11_k initial loss 5.164465255802497e-05
I0327 21:42:38.339581 2042240 finetune.py:68] layer 9_k @ epoch 3 new loss 4.096085831406526e-05 old loss 4.120578523725271e-05 BETTER
I0327 21:42:40.815312 2042310 finetune.py:68] layer 10_k @ epoch 1 new loss 5.319601405062713e-05 old loss 5.358416819944978e-05 BETTER
I0327 21:42:51.033416 2042170 finetune.py:45] layer 8_o initial loss 9.064680489245802e-05
I0327 21:43:08.416984 2042380 finetune.py:68] layer 11_k @ epoch 0 new loss 5.101535134599544e-05 old loss 5.164465255802497e-05 BETTER
I0327 21:43:13.046391 2042240 finetune.py:68] layer 9_k @ epoch 4 new loss 4.073034506291151e-05 old loss 4.096085831406526e-05 BETTER
I0327 21:43:15.550959 2042310 finetune.py:68] layer 10_k @ epoch 2 new loss 5.283346035866998e-05 old loss 5.319601405062713e-05 BETTER
I0327 21:43:25.851242 2042170 finetune.py:68] layer 8_o @ epoch 0 new loss 8.719395555090159e-05 old loss 9.064680489245802e-05 BETTER
I0327 21:43:32.418755 2042240 finetune.py:45] layer 9_o initial loss 9.487148781772703e-05
I0327 21:43:42.368942 2042380 finetune.py:68] layer 11_k @ epoch 1 new loss 5.062107084086165e-05 old loss 5.101535134599544e-05 BETTER
I0327 21:43:50.193537 2042310 finetune.py:68] layer 10_k @ epoch 3 new loss 5.251293259789236e-05 old loss 5.283346035866998e-05 BETTER
I0327 21:44:02.091519 2042170 finetune.py:68] layer 8_o @ epoch 1 new loss 8.59224091982469e-05 old loss 8.719395555090159e-05 BETTER
I0327 21:44:05.205448 2042240 finetune.py:68] layer 9_o @ epoch 0 new loss 9.118732123170048e-05 old loss 9.487148781772703e-05 BETTER
I0327 21:44:16.700429 2042380 finetune.py:68] layer 11_k @ epoch 2 new loss 5.0291739171370864e-05 old loss 5.062107084086165e-05 BETTER
I0327 21:44:25.044887 2042310 finetune.py:68] layer 10_k @ epoch 4 new loss 5.2229795983294025e-05 old loss 5.251293259789236e-05 BETTER
I0327 21:44:38.115916 2042170 finetune.py:68] layer 8_o @ epoch 2 new loss 8.500435069436207e-05 old loss 8.59224091982469e-05 BETTER
I0327 21:44:39.109412 2042240 finetune.py:68] layer 9_o @ epoch 1 new loss 8.976934623206034e-05 old loss 9.118732123170048e-05 BETTER
I0327 21:44:44.343286 2042310 finetune.py:45] layer 10_o initial loss 0.0001156323342001997
I0327 21:44:50.671335 2042380 finetune.py:68] layer 11_k @ epoch 3 new loss 4.9973081331700087e-05 old loss 5.0291739171370864e-05 BETTER
I0327 21:45:12.863362 2042240 finetune.py:68] layer 9_o @ epoch 2 new loss 8.871439058566466e-05 old loss 8.976934623206034e-05 BETTER
I0327 21:45:14.195647 2042170 finetune.py:68] layer 8_o @ epoch 3 new loss 8.425270061707124e-05 old loss 8.500435069436207e-05 BETTER
I0327 21:45:17.390778 2042310 finetune.py:68] layer 10_o @ epoch 0 new loss 0.00011096898379037157 old loss 0.0001156323342001997 BETTER
I0327 21:45:24.777818 2042380 finetune.py:68] layer 11_k @ epoch 4 new loss 4.969389192410745e-05 old loss 4.9973081331700087e-05 BETTER
I0327 21:45:43.923546 2042380 finetune.py:45] layer 11_o initial loss 0.00011709290993167087
I0327 21:45:46.934194 2042240 finetune.py:68] layer 9_o @ epoch 3 new loss 8.786458056420088e-05 old loss 8.871439058566466e-05 BETTER
I0327 21:45:50.386030 2042170 finetune.py:68] layer 8_o @ epoch 4 new loss 8.360434003407136e-05 old loss 8.425270061707124e-05 BETTER
I0327 21:45:51.336092 2042310 finetune.py:68] layer 10_o @ epoch 1 new loss 0.00010913334699580446 old loss 0.00011096898379037157 BETTER
I0327 21:46:16.277756 2042380 finetune.py:68] layer 11_o @ epoch 0 new loss 0.00011230249947402626 old loss 0.00011709290993167087 BETTER
I0327 21:46:21.078031 2042240 finetune.py:68] layer 9_o @ epoch 4 new loss 8.712822454981506e-05 old loss 8.786458056420088e-05 BETTER
I0327 21:46:21.330941 2042170 finetune.py:45] layer 8_up initial loss 0.0001688778429524973
I0327 21:46:25.745611 2042310 finetune.py:68] layer 10_o @ epoch 2 new loss 0.0001077962078852579 old loss 0.00010913334699580446 BETTER
I0327 21:46:49.885481 2042380 finetune.py:68] layer 11_o @ epoch 1 new loss 0.00011048695159843192 old loss 0.00011230249947402626 BETTER
I0327 21:46:51.970310 2042240 finetune.py:45] layer 9_up initial loss 0.00018039517453871667
I0327 21:46:53.813690 2042170 finetune.py:68] layer 8_up @ epoch 0 new loss 0.00016680543194524944 old loss 0.0001688778429524973 BETTER
I0327 21:46:59.990044 2042310 finetune.py:68] layer 10_o @ epoch 3 new loss 0.00010669539187802002 old loss 0.0001077962078852579 BETTER
I0327 21:47:22.705894 2042240 finetune.py:68] layer 9_up @ epoch 0 new loss 0.00017800825298763812 old loss 0.00018039517453871667 BETTER
I0327 21:47:23.506253 2042380 finetune.py:68] layer 11_o @ epoch 2 new loss 0.00010913416190305725 old loss 0.00011048695159843192 BETTER
I0327 21:47:27.267758 2042170 finetune.py:68] layer 8_up @ epoch 1 new loss 0.00016542599769309163 old loss 0.00016680543194524944 BETTER
I0327 21:47:34.226688 2042310 finetune.py:68] layer 10_o @ epoch 4 new loss 0.00010574707266641781 old loss 0.00010669539187802002 BETTER
I0327 21:47:54.924320 2042240 finetune.py:68] layer 9_up @ epoch 1 new loss 0.00017644427134655416 old loss 0.00017800825298763812 BETTER
I0327 21:47:57.131519 2042380 finetune.py:68] layer 11_o @ epoch 3 new loss 0.00010802518954733387 old loss 0.00010913416190305725 BETTER
I0327 21:48:01.071686 2042170 finetune.py:68] layer 8_up @ epoch 2 new loss 0.00016422249609604478 old loss 0.00016542599769309163 BETTER
I0327 21:48:05.693145 2042310 finetune.py:45] layer 10_up initial loss 0.00020139844855293632
I0327 21:48:27.092447 2042240 finetune.py:68] layer 9_up @ epoch 2 new loss 0.0001750826631905511 old loss 0.00017644427134655416 BETTER
I0327 21:48:30.677098 2042380 finetune.py:68] layer 11_o @ epoch 4 new loss 0.00010707635374274105 old loss 0.00010802518954733387 BETTER
I0327 21:48:35.046518 2042170 finetune.py:68] layer 8_up @ epoch 3 new loss 0.000163133125170134 old loss 0.00016422249609604478 BETTER
I0327 21:48:36.791977 2042310 finetune.py:68] layer 10_up @ epoch 0 new loss 0.00019886407244484872 old loss 0.00020139844855293632 BETTER
I0327 21:48:59.322529 2042240 finetune.py:68] layer 9_up @ epoch 3 new loss 0.00017384208331350237 old loss 0.0001750826631905511 BETTER
I0327 21:49:01.685676 2042380 finetune.py:45] layer 11_up initial loss 0.000208323443075642
I0327 21:49:08.905004 2042310 finetune.py:68] layer 10_up @ epoch 1 new loss 0.00019716302631422877 old loss 0.00019886407244484872 BETTER
I0327 21:49:09.020055 2042170 finetune.py:68] layer 8_up @ epoch 4 new loss 0.000162110838573426 old loss 0.000163133125170134 BETTER
I0327 21:49:31.607410 2042240 finetune.py:68] layer 9_up @ epoch 4 new loss 0.00017270317766815424 old loss 0.00017384208331350237 BETTER
I0327 21:49:31.976649 2042380 finetune.py:68] layer 11_up @ epoch 0 new loss 0.00020567777391988784 old loss 0.000208323443075642 BETTER
I0327 21:49:39.703745 2042170 finetune.py:45] layer 8_gate initial loss 0.00019402186444494873
I0327 21:49:41.188239 2042310 finetune.py:68] layer 10_up @ epoch 2 new loss 0.0001956888590939343 old loss 0.00019716302631422877 BETTER
I0327 21:50:02.629270 2042240 finetune.py:45] layer 9_gate initial loss 0.00020717635925393552
I0327 21:50:03.232279 2042380 finetune.py:68] layer 11_up @ epoch 1 new loss 0.00020384023082442582 old loss 0.00020567777391988784 BETTER
I0327 21:50:10.063327 2042170 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.00019276952662039548 old loss 0.00019402186444494873 BETTER
I0327 21:50:13.427703 2042310 finetune.py:68] layer 10_up @ epoch 3 new loss 0.00019432742556091398 old loss 0.0001956888590939343 BETTER
I0327 21:50:31.345007 2042240 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0002057226956821978 old loss 0.00020717635925393552 BETTER
I0327 21:50:34.577918 2042380 finetune.py:68] layer 11_up @ epoch 2 new loss 0.00020222776220180094 old loss 0.00020384023082442582 BETTER
I0327 21:50:41.303463 2042170 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.00019177688227500767 old loss 0.00019276952662039548 BETTER
I0327 21:50:45.605542 2042310 finetune.py:68] layer 10_up @ epoch 4 new loss 0.00019307811453472823 old loss 0.00019432742556091398 BETTER
I0327 21:51:00.939897 2042240 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.00020463537657633424 old loss 0.0002057226956821978 BETTER
I0327 21:51:06.016684 2042380 finetune.py:68] layer 11_up @ epoch 3 new loss 0.00020074614440090954 old loss 0.00020222776220180094 BETTER
I0327 21:51:12.737964 2042170 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.00019088343833573163 old loss 0.00019177688227500767 BETTER
I0327 21:51:16.832499 2042310 finetune.py:45] layer 10_gate initial loss 0.0002306632522959262
I0327 21:51:30.564719 2042240 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.00020364546799100935 old loss 0.00020463537657633424 BETTER
I0327 21:51:37.440332 2042380 finetune.py:68] layer 11_up @ epoch 4 new loss 0.0001993667392525822 old loss 0.00020074614440090954 BETTER
I0327 21:51:44.260468 2042170 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.0001900623319670558 old loss 0.00019088343833573163 BETTER
I0327 21:51:45.526353 2042310 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.0002290772827109322 old loss 0.0002306632522959262 BETTER
I0327 21:52:00.309312 2042240 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.00020272214896976948 old loss 0.00020364546799100935 BETTER
I0327 21:52:08.301361 2042380 finetune.py:45] layer 11_gate initial loss 0.00023954969947226346
I0327 21:52:15.191609 2042310 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.00022788178466726094 old loss 0.0002290772827109322 BETTER
I0327 21:52:15.870714 2042170 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.00018927556811831892 old loss 0.0001900623319670558 BETTER
I0327 21:52:30.189265 2042240 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0002018612576648593 old loss 0.00020272214896976948 BETTER
I0327 21:52:36.387261 2042380 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.00023796592722646892 old loss 0.00023954969947226346 BETTER
I0327 21:52:45.025770 2042310 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.0002268067328259349 old loss 0.00022788178466726094 BETTER
I0327 21:53:05.512922 2042380 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.00023670366499572992 old loss 0.00023796592722646892 BETTER
I0327 21:53:10.804049 2042170 finetune.py:45] layer 8_down initial loss 0.0002825958654284477
I0327 21:53:14.901226 2042310 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.0002258158492622897 old loss 0.0002268067328259349 BETTER
I0327 21:53:26.317703 2042240 finetune.py:45] layer 9_down initial loss 0.0003038927970919758
I0327 21:53:34.803282 2042380 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.00023556205269414932 old loss 0.00023670366499572992 BETTER
I0327 21:53:38.253317 2042170 finetune.py:68] layer 8_down @ epoch 0 new loss 0.0002825470583047718 old loss 0.0002825958654284477 BETTER
I0327 21:53:44.874632 2042310 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.0002248670207336545 old loss 0.0002258158492622897 BETTER
I0327 21:53:52.559891 2042240 finetune.py:68] layer 9_down @ epoch 0 new loss 0.0003038497525267303 old loss 0.0003038927970919758 BETTER
I0327 21:54:04.260695 2042380 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.00023447975399903953 old loss 0.00023556205269414932 BETTER
I0327 21:54:07.047883 2042170 finetune.py:68] layer 8_down @ epoch 1 new loss 0.0002825057599693537 old loss 0.0002825470583047718 BETTER
I0327 21:54:20.057991 2042240 finetune.py:68] layer 9_down @ epoch 1 new loss 0.0003038105496671051 old loss 0.0003038497525267303 BETTER
I0327 21:54:33.655849 2042380 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.00023346900707110763 old loss 0.00023447975399903953 BETTER
I0327 21:54:36.156217 2042170 finetune.py:68] layer 8_down @ epoch 2 new loss 0.0002824676630552858 old loss 0.0002825057599693537 BETTER
I0327 21:54:42.007026 2042310 finetune.py:45] layer 10_down initial loss 0.000329932983731851
I0327 21:54:47.867537 2042240 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00030377585790120065 old loss 0.0003038105496671051 BETTER
I0327 21:55:05.626136 2042170 finetune.py:68] layer 8_down @ epoch 3 new loss 0.00028242976986803114 old loss 0.0002824676630552858 BETTER
I0327 21:55:08.424934 2042310 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0003298729716334492 old loss 0.000329932983731851 BETTER
I0327 21:55:15.532880 2042240 finetune.py:68] layer 9_down @ epoch 3 new loss 0.0003037411079276353 old loss 0.00030377585790120065 BETTER
I0327 21:55:30.331974 2042380 finetune.py:45] layer 11_down initial loss 0.00034226433490402997
I0327 21:55:35.052390 2042170 finetune.py:68] layer 8_down @ epoch 4 new loss 0.00028239621315151453 old loss 0.00028242976986803114 BETTER
I0327 21:55:36.117584 2042310 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0003298184019513428 old loss 0.0003298729716334492 BETTER
8_v proxy err 0.07198154926300049 tr(WHW.T) 257.7052307128906
bpp_loss 1.83815965015674
8_q proxy err 0.0033292199950665236 tr(WHW.T) 26614.96484375
bpp_loss 2.6400912986136973
8_k proxy err 0.0011399908689782023 tr(WHW.T) 22464.998046875
bpp_loss 3.479199845169205
8_o proxy err 0.06830868870019913 tr(WHW.T) 758.013916015625
bpp_loss 1.962380472192308
8_up proxy err 0.04695567116141319 tr(WHW.T) 8488.458984375
bpp_loss 2.0765411686484834
8_gate proxy err 0.011477859690785408 tr(WHW.T) 37117.01171875
bpp_loss 2.372256140823343
8_down proxy err 0.05942266061902046 tr(WHW.T) 6556.5263671875
bpp_loss 2.0827478821322853
I0327 21:55:44.271612 2042240 finetune.py:68] layer 9_down @ epoch 4 new loss 0.0003037095011677593 old loss 0.0003037411079276353 BETTER
9_v proxy err 0.05468771234154701 tr(WHW.T) 351.4288024902344
bpp_loss 1.9364420414785855
9_q proxy err 0.0034898039884865284 tr(WHW.T) 25670.4375
bpp_loss 2.6514547816477716
9_k proxy err 0.0012417597463354468 tr(WHW.T) 20907.126953125
bpp_loss 3.49861321906792
9_o proxy err 0.06174987554550171 tr(WHW.T) 788.240966796875
bpp_loss 2.0121800095948856
9_up proxy err 0.04448755085468292 tr(WHW.T) 8960.8525390625
bpp_loss 2.085572632827929
9_gate proxy err 0.010865533724427223 tr(WHW.T) 39260.12890625
bpp_loss 2.3846709901200875
9_down proxy err 0.058849554508924484 tr(WHW.T) 6344.951171875
bpp_loss 2.0838818595678146
I0327 21:55:57.034988 2042380 finetune.py:68] layer 11_down @ epoch 0 new loss 0.0003422002773731947 old loss 0.00034226433490402997 BETTER
I0327 21:56:04.463621 2042310 finetune.py:68] layer 10_down @ epoch 2 new loss 0.0003297649382147938 old loss 0.0003298184019513428 BETTER
I0327 21:56:23.911758 2042380 finetune.py:68] layer 11_down @ epoch 1 new loss 0.00034214535844512284 old loss 0.0003422002773731947 BETTER
I0327 21:56:32.263040 2042310 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0003297148796264082 old loss 0.0003297649382147938 BETTER
I0327 21:56:51.018179 2042380 finetune.py:68] layer 11_down @ epoch 2 new loss 0.00034209329169243574 old loss 0.00034214535844512284 BETTER
I0327 21:56:55.116995 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 66.15626692771912s
I0327 21:56:59.054676 2042450 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:56:59.054777 2042450 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:56:59.054820 2042450 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:56:59.424225 2042450 config.py:54] PyTorch version 2.6.0 available.
W0327 21:56:59.638930 2042450 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0327 21:57:00.109418 2042310 finetune.py:68] layer 10_down @ epoch 4 new loss 0.00032966950675472617 old loss 0.0003297148796264082 BETTER
W0327 21:57:00.214836 2042450 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:57:00.218509 2041475 quantize_finetune_llama.py:209] layer 13 gpu 1
I0327 21:57:00.232296 2042450 data_utils.py:336] using 256 training seqs, 128 validation seqs
10_v proxy err 0.07050593942403793 tr(WHW.T) 251.83889770507812
bpp_loss 1.828864999464713
10_q proxy err 0.0036366144195199013 tr(WHW.T) 23361.681640625
bpp_loss 2.6477383709861897
10_k proxy err 0.001249989029020071 tr(WHW.T) 19707.296875
bpp_loss 3.496045660634991
10_o proxy err 0.07392692565917969 tr(WHW.T) 692.8115234375
bpp_loss 1.9533408947463613
10_up proxy err 0.04395424574613571 tr(WHW.T) 9191.57421875
bpp_loss 2.1024907462831055
10_gate proxy err 0.011458826251327991 tr(WHW.T) 37321.96875
bpp_loss 2.356860831413152
10_down proxy err 0.05727796629071236 tr(WHW.T) 6597.97021484375
bpp_loss 2.100124954711646
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:57:16.859096 2042450 finetune.py:45] layer 12_v initial loss 6.93553447490558e-05
W0327 21:57:16.859284 2042450 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:57:18.295271 2042380 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0003420399152673781 old loss 0.00034209329169243574 BETTER
I0327 21:57:45.587231 2042380 finetune.py:68] layer 11_down @ epoch 4 new loss 0.00034199198125861585 old loss 0.0003420399152673781 BETTER
11_v proxy err 0.058285560458898544 tr(WHW.T) 319.5691223144531
bpp_loss 1.8361409261706285
11_q proxy err 0.003982704132795334 tr(WHW.T) 22213.310546875
bpp_loss 2.5963552290340886
11_k proxy err 0.001442221226170659 tr(WHW.T) 17963.31640625
bpp_loss 3.4971385475364514
11_o proxy err 0.07529550045728683 tr(WHW.T) 577.8248901367188
bpp_loss 1.973350131913321
11_up proxy err 0.043081555515527725 tr(WHW.T) 9184.0625
bpp_loss 2.106353269324505
11_gate proxy err 0.011379382573068142 tr(WHW.T) 36612.99609375
bpp_loss 2.335880500164681
11_down proxy err 0.054838355630636215 tr(WHW.T) 6732.4580078125
bpp_loss 2.107144984707702
I0327 21:57:52.057964 2042450 finetune.py:68] layer 12_v @ epoch 0 new loss 5.257272641756572e-05 old loss 6.93553447490558e-05 BETTER
I0327 21:58:07.272484 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 62.902228116989136s
I0327 21:58:10.818171 2042520 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:58:10.818269 2042520 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:58:10.818312 2042520 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:58:11.140040 2042520 config.py:54] PyTorch version 2.6.0 available.
W0327 21:58:11.334086 2042520 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:58:11.892634 2042520 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:58:11.896532 2041475 quantize_finetune_llama.py:209] layer 14 gpu 2
I0327 21:58:11.911826 2042520 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:58:28.181899 2042520 finetune.py:45] layer 13_v initial loss 7.346586062340066e-05
W0327 21:58:28.182109 2042520 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:58:28.772826 2042450 finetune.py:68] layer 12_v @ epoch 1 new loss 4.994813207304105e-05 old loss 5.257272641756572e-05 BETTER
I0327 21:59:01.424992 2042520 finetune.py:68] layer 13_v @ epoch 0 new loss 5.6135722843464464e-05 old loss 7.346586062340066e-05 BETTER
I0327 21:59:05.899319 2042450 finetune.py:68] layer 12_v @ epoch 2 new loss 4.834949140786193e-05 old loss 4.994813207304105e-05 BETTER
I0327 21:59:15.473486 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 63.12133765220642s
I0327 21:59:19.251806 2042590 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 21:59:19.251914 2042590 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 21:59:19.251952 2042590 utils.py:162] NumExpr defaulting to 16 threads.
I0327 21:59:19.643440 2042590 config.py:54] PyTorch version 2.6.0 available.
W0327 21:59:19.861134 2042590 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 21:59:20.495875 2042590 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 21:59:20.499702 2041475 quantize_finetune_llama.py:209] layer 15 gpu 3
I0327 21:59:20.515025 2042590 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 21:59:36.085579 2042520 finetune.py:68] layer 13_v @ epoch 1 new loss 5.3387193474918604e-05 old loss 5.6135722843464464e-05 BETTER
I0327 21:59:37.220328 2042590 finetune.py:45] layer 14_v initial loss 7.66185112297535e-05
W0327 21:59:37.220529 2042590 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 21:59:42.830997 2042450 finetune.py:68] layer 12_v @ epoch 3 new loss 4.715352042694576e-05 old loss 4.834949140786193e-05 BETTER
I0327 22:00:10.771680 2042590 finetune.py:68] layer 14_v @ epoch 0 new loss 6.032992314430885e-05 old loss 7.66185112297535e-05 BETTER
I0327 22:00:10.955832 2042520 finetune.py:68] layer 13_v @ epoch 2 new loss 5.18359083798714e-05 old loss 5.3387193474918604e-05 BETTER
I0327 22:00:19.866533 2042450 finetune.py:68] layer 12_v @ epoch 4 new loss 4.623959102900699e-05 old loss 4.715352042694576e-05 BETTER
I0327 22:00:23.225033 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 62.22059226036072s
I0327 22:00:26.878507 2042660 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 22:00:26.878607 2042660 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 22:00:26.878646 2042660 utils.py:162] NumExpr defaulting to 16 threads.
I0327 22:00:27.210432 2042660 config.py:54] PyTorch version 2.6.0 available.
W0327 22:00:27.405785 2042660 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 22:00:28.001413 2042660 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 22:00:28.005221 2041475 quantize_finetune_llama.py:209] layer 16 gpu 0
I0327 22:00:28.039536 2042660 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 22:00:38.940280 2042450 finetune.py:45] layer 12_q initial loss 5.551101276068948e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 22:00:44.655991 2042660 finetune.py:45] layer 15_v initial loss 0.00010028314864030108
W0327 22:00:44.656218 2042660 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 22:00:45.552624 2042590 finetune.py:68] layer 14_v @ epoch 1 new loss 5.763368244515732e-05 old loss 6.032992314430885e-05 BETTER
I0327 22:00:46.127755 2042520 finetune.py:68] layer 13_v @ epoch 3 new loss 5.0729751819744706e-05 old loss 5.18359083798714e-05 BETTER
I0327 22:01:14.606505 2042450 finetune.py:68] layer 12_q @ epoch 0 new loss 5.4150630603544414e-05 old loss 5.551101276068948e-05 BETTER
I0327 22:01:17.773062 2042660 finetune.py:68] layer 15_v @ epoch 0 new loss 6.780827243346721e-05 old loss 0.00010028314864030108 BETTER
I0327 22:01:20.677557 2042590 finetune.py:68] layer 14_v @ epoch 2 new loss 5.602679084404372e-05 old loss 5.763368244515732e-05 BETTER
I0327 22:01:21.396970 2042520 finetune.py:68] layer 13_v @ epoch 4 new loss 4.9878803110914305e-05 old loss 5.0729751819744706e-05 BETTER
I0327 22:01:40.257527 2042520 finetune.py:45] layer 13_q initial loss 6.088818918215111e-05
I0327 22:01:51.461222 2042450 finetune.py:68] layer 12_q @ epoch 1 new loss 5.319963747751899e-05 old loss 5.4150630603544414e-05 BETTER
I0327 22:01:52.211647 2042660 finetune.py:68] layer 15_v @ epoch 1 new loss 6.389572081388906e-05 old loss 6.780827243346721e-05 BETTER
I0327 22:01:56.015877 2042590 finetune.py:68] layer 14_v @ epoch 3 new loss 5.4823674872750416e-05 old loss 5.602679084404372e-05 BETTER
I0327 22:02:13.847594 2042520 finetune.py:68] layer 13_q @ epoch 0 new loss 5.9572797908913344e-05 old loss 6.088818918215111e-05 BETTER
I0327 22:02:26.742212 2042660 finetune.py:68] layer 15_v @ epoch 2 new loss 6.174521695356816e-05 old loss 6.389572081388906e-05 BETTER
I0327 22:02:28.583067 2042450 finetune.py:68] layer 12_q @ epoch 2 new loss 5.2410865464480594e-05 old loss 5.319963747751899e-05 BETTER
I0327 22:02:31.354533 2042590 finetune.py:68] layer 14_v @ epoch 4 new loss 5.387490091379732e-05 old loss 5.4823674872750416e-05 BETTER
I0327 22:02:48.426406 2042520 finetune.py:68] layer 13_q @ epoch 1 new loss 5.876217983313836e-05 old loss 5.9572797908913344e-05 BETTER
I0327 22:02:50.584298 2042590 finetune.py:45] layer 14_q initial loss 6.866299372632056e-05
I0327 22:03:01.470728 2042660 finetune.py:68] layer 15_v @ epoch 3 new loss 6.024876711308025e-05 old loss 6.174521695356816e-05 BETTER
I0327 22:03:05.704601 2042450 finetune.py:68] layer 12_q @ epoch 3 new loss 5.1725459343288094e-05 old loss 5.2410865464480594e-05 BETTER
I0327 22:03:23.201443 2042520 finetune.py:68] layer 13_q @ epoch 2 new loss 5.810573202325031e-05 old loss 5.876217983313836e-05 BETTER
I0327 22:03:24.537832 2042590 finetune.py:68] layer 14_q @ epoch 0 new loss 6.681784725515172e-05 old loss 6.866299372632056e-05 BETTER
I0327 22:03:36.064316 2042660 finetune.py:68] layer 15_v @ epoch 4 new loss 5.9091013099532574e-05 old loss 6.024876711308025e-05 BETTER
I0327 22:03:42.757094 2042450 finetune.py:68] layer 12_q @ epoch 4 new loss 5.113081351737492e-05 old loss 5.1725459343288094e-05 BETTER
I0327 22:03:55.028154 2042660 finetune.py:45] layer 15_q initial loss 6.809912883909419e-05
I0327 22:03:58.078665 2042520 finetune.py:68] layer 13_q @ epoch 3 new loss 5.7520981499692425e-05 old loss 5.810573202325031e-05 BETTER
I0327 22:03:59.549949 2042590 finetune.py:68] layer 14_q @ epoch 1 new loss 6.575042061740533e-05 old loss 6.681784725515172e-05 BETTER
I0327 22:04:00.908457 2042450 finetune.py:45] layer 12_k initial loss 5.4724041547160596e-05
I0327 22:04:28.142013 2042660 finetune.py:68] layer 15_q @ epoch 0 new loss 6.660218787146732e-05 old loss 6.809912883909419e-05 BETTER
I0327 22:04:32.843647 2042520 finetune.py:68] layer 13_q @ epoch 4 new loss 5.7025612477445975e-05 old loss 5.7520981499692425e-05 BETTER
I0327 22:04:34.487786 2042590 finetune.py:68] layer 14_q @ epoch 2 new loss 6.487112113973126e-05 old loss 6.575042061740533e-05 BETTER
I0327 22:04:36.239470 2042450 finetune.py:68] layer 12_k @ epoch 0 new loss 5.3986550483386964e-05 old loss 5.4724041547160596e-05 BETTER
I0327 22:04:50.817321 2042520 finetune.py:45] layer 13_k initial loss 5.973422230454162e-05
I0327 22:05:02.113672 2042660 finetune.py:68] layer 15_q @ epoch 1 new loss 6.556518928846344e-05 old loss 6.660218787146732e-05 BETTER
I0327 22:05:09.452816 2042590 finetune.py:68] layer 14_q @ epoch 3 new loss 6.412985385395586e-05 old loss 6.487112113973126e-05 BETTER
I0327 22:05:12.656028 2042450 finetune.py:68] layer 12_k @ epoch 1 new loss 5.346668331185356e-05 old loss 5.3986550483386964e-05 BETTER
I0327 22:05:24.248629 2042520 finetune.py:68] layer 13_k @ epoch 0 new loss 5.9149206208530813e-05 old loss 5.973422230454162e-05 BETTER
I0327 22:05:36.046731 2042660 finetune.py:68] layer 15_q @ epoch 2 new loss 6.473716348409653e-05 old loss 6.556518928846344e-05 BETTER
I0327 22:05:44.417829 2042590 finetune.py:68] layer 14_q @ epoch 4 new loss 6.349055911414325e-05 old loss 6.412985385395586e-05 BETTER
I0327 22:05:49.453068 2042450 finetune.py:68] layer 12_k @ epoch 2 new loss 5.3007104725111276e-05 old loss 5.346668331185356e-05 BETTER
I0327 22:05:58.560426 2042520 finetune.py:68] layer 13_k @ epoch 1 new loss 5.872751717106439e-05 old loss 5.9149206208530813e-05 BETTER
I0327 22:06:02.236854 2042590 finetune.py:45] layer 14_k initial loss 6.833714724052697e-05
I0327 22:06:10.220805 2042660 finetune.py:68] layer 15_q @ epoch 3 new loss 6.404145096894354e-05 old loss 6.473716348409653e-05 BETTER
I0327 22:06:26.328795 2042450 finetune.py:68] layer 12_k @ epoch 3 new loss 5.260170655674301e-05 old loss 5.3007104725111276e-05 BETTER
I0327 22:06:33.071408 2042520 finetune.py:68] layer 13_k @ epoch 2 new loss 5.837034041178413e-05 old loss 5.872751717106439e-05 BETTER
I0327 22:06:35.892655 2042590 finetune.py:68] layer 14_k @ epoch 0 new loss 6.731592293363065e-05 old loss 6.833714724052697e-05 BETTER
I0327 22:06:44.418878 2042660 finetune.py:68] layer 15_q @ epoch 4 new loss 6.342088454402983e-05 old loss 6.404145096894354e-05 BETTER
I0327 22:07:01.651301 2042660 finetune.py:45] layer 15_k initial loss 6.714423216180876e-05
I0327 22:07:03.250841 2042450 finetune.py:68] layer 12_k @ epoch 4 new loss 5.223153857514262e-05 old loss 5.260170655674301e-05 BETTER
I0327 22:07:07.574714 2042520 finetune.py:68] layer 13_k @ epoch 3 new loss 5.8034638641402125e-05 old loss 5.837034041178413e-05 BETTER
I0327 22:07:10.500077 2042590 finetune.py:68] layer 14_k @ epoch 1 new loss 6.674208998447284e-05 old loss 6.731592293363065e-05 BETTER
I0327 22:07:22.801355 2042450 finetune.py:45] layer 12_o initial loss 0.00011926622391911224
I0327 22:07:34.582067 2042660 finetune.py:68] layer 15_k @ epoch 0 new loss 6.644600944127887e-05 old loss 6.714423216180876e-05 BETTER
I0327 22:07:42.138082 2042520 finetune.py:68] layer 13_k @ epoch 4 new loss 5.7721499615581706e-05 old loss 5.8034638641402125e-05 BETTER
I0327 22:07:45.295696 2042590 finetune.py:68] layer 14_k @ epoch 2 new loss 6.624752859352157e-05 old loss 6.674208998447284e-05 BETTER
I0327 22:07:57.581162 2042450 finetune.py:68] layer 12_o @ epoch 0 new loss 0.00011502348206704482 old loss 0.00011926622391911224 BETTER
I0327 22:08:01.596671 2042520 finetune.py:45] layer 13_o initial loss 0.00014395774633157998
I0327 22:08:08.318392 2042660 finetune.py:68] layer 15_k @ epoch 1 new loss 6.59351862850599e-05 old loss 6.644600944127887e-05 BETTER
I0327 22:08:20.061765 2042590 finetune.py:68] layer 14_k @ epoch 3 new loss 6.581253546755761e-05 old loss 6.624752859352157e-05 BETTER
I0327 22:08:33.594693 2042450 finetune.py:68] layer 12_o @ epoch 1 new loss 0.0001131790122599341 old loss 0.00011502348206704482 BETTER
I0327 22:08:34.550144 2042520 finetune.py:68] layer 13_o @ epoch 0 new loss 0.00013874561409465969 old loss 0.00014395774633157998 BETTER
I0327 22:08:42.486771 2042660 finetune.py:68] layer 15_k @ epoch 2 new loss 6.547874363604933e-05 old loss 6.59351862850599e-05 BETTER
I0327 22:08:54.915121 2042590 finetune.py:68] layer 14_k @ epoch 4 new loss 6.541063339682296e-05 old loss 6.581253546755761e-05 BETTER
I0327 22:09:08.222747 2042520 finetune.py:68] layer 13_o @ epoch 1 new loss 0.00013642442354466766 old loss 0.00013874561409465969 BETTER
I0327 22:09:09.766515 2042450 finetune.py:68] layer 12_o @ epoch 2 new loss 0.00011174868996022269 old loss 0.0001131790122599341 BETTER
I0327 22:09:14.116799 2042590 finetune.py:45] layer 14_o initial loss 0.0001568043080624193
I0327 22:09:16.613557 2042660 finetune.py:68] layer 15_k @ epoch 3 new loss 6.507959187729284e-05 old loss 6.547874363604933e-05 BETTER
I0327 22:09:42.147120 2042520 finetune.py:68] layer 13_o @ epoch 2 new loss 0.00013466906966641545 old loss 0.00013642442354466766 BETTER
I0327 22:09:45.959898 2042450 finetune.py:68] layer 12_o @ epoch 3 new loss 0.0001105500923586078 old loss 0.00011174868996022269 BETTER
I0327 22:09:47.265560 2042590 finetune.py:68] layer 14_o @ epoch 0 new loss 0.00015094091941136867 old loss 0.0001568043080624193 BETTER
I0327 22:09:50.583845 2042660 finetune.py:68] layer 15_k @ epoch 4 new loss 6.472456880146638e-05 old loss 6.507959187729284e-05 BETTER
I0327 22:10:09.340862 2042660 finetune.py:45] layer 15_o initial loss 0.0001597421069163829
I0327 22:10:16.366006 2042520 finetune.py:68] layer 13_o @ epoch 3 new loss 0.00013320230937097222 old loss 0.00013466906966641545 BETTER
I0327 22:10:21.624427 2042590 finetune.py:68] layer 14_o @ epoch 1 new loss 0.00014823699893895537 old loss 0.00015094091941136867 BETTER
I0327 22:10:22.229626 2042450 finetune.py:68] layer 12_o @ epoch 4 new loss 0.00010949993884423748 old loss 0.0001105500923586078 BETTER
I0327 22:10:41.487020 2042660 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0001523789542261511 old loss 0.0001597421069163829 BETTER
I0327 22:10:50.372909 2042520 finetune.py:68] layer 13_o @ epoch 4 new loss 0.00013194741040933877 old loss 0.00013320230937097222 BETTER
I0327 22:10:53.051514 2042450 finetune.py:45] layer 12_up initial loss 0.00021543289767578244
I0327 22:10:55.719896 2042590 finetune.py:68] layer 14_o @ epoch 2 new loss 0.00014613702660426497 old loss 0.00014823699893895537 BETTER
I0327 22:11:14.699405 2042660 finetune.py:68] layer 15_o @ epoch 1 new loss 0.00014947779709473252 old loss 0.0001523789542261511 BETTER
I0327 22:11:21.124559 2042520 finetune.py:45] layer 13_up initial loss 0.000253005709964782
I0327 22:11:25.063583 2042450 finetune.py:68] layer 12_up @ epoch 0 new loss 0.0002124121820088476 old loss 0.00021543289767578244 BETTER
I0327 22:11:29.972397 2042590 finetune.py:68] layer 14_o @ epoch 3 new loss 0.0001444041117792949 old loss 0.00014613702660426497 BETTER
I0327 22:11:48.169676 2042660 finetune.py:68] layer 15_o @ epoch 2 new loss 0.00014728933456353843 old loss 0.00014947779709473252 BETTER
I0327 22:11:51.530359 2042520 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00024939532158896327 old loss 0.000253005709964782 BETTER
I0327 22:11:58.532302 2042450 finetune.py:68] layer 12_up @ epoch 1 new loss 0.0002102373109664768 old loss 0.0002124121820088476 BETTER
I0327 22:12:04.463463 2042590 finetune.py:68] layer 14_o @ epoch 4 new loss 0.00014291933621279895 old loss 0.0001444041117792949 BETTER
I0327 22:12:21.665173 2042660 finetune.py:68] layer 15_o @ epoch 3 new loss 0.00014550787454936653 old loss 0.00014728933456353843 BETTER
I0327 22:12:23.477761 2042520 finetune.py:68] layer 13_up @ epoch 1 new loss 0.0002468671300448477 old loss 0.00024939532158896327 BETTER
I0327 22:12:32.265692 2042450 finetune.py:68] layer 12_up @ epoch 2 new loss 0.0002083211875287816 old loss 0.0002102373109664768 BETTER
I0327 22:12:35.439350 2042590 finetune.py:45] layer 14_up initial loss 0.0002896107325796038
I0327 22:12:55.201955 2042660 finetune.py:68] layer 15_o @ epoch 4 new loss 0.0001439805782865733 old loss 0.00014550787454936653 BETTER
I0327 22:12:55.613334 2042520 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0002446542785037309 old loss 0.0002468671300448477 BETTER
I0327 22:13:06.294830 2042450 finetune.py:68] layer 12_up @ epoch 3 new loss 0.00020659533038269728 old loss 0.0002083211875287816 BETTER
I0327 22:13:06.386256 2042590 finetune.py:68] layer 14_up @ epoch 0 new loss 0.0002850086893886328 old loss 0.0002896107325796038 BETTER
I0327 22:13:25.639436 2042660 finetune.py:45] layer 15_up initial loss 0.0003199024358764291
I0327 22:13:27.798530 2042520 finetune.py:68] layer 13_up @ epoch 3 new loss 0.00024265219690278172 old loss 0.0002446542785037309 BETTER
I0327 22:13:38.537986 2042590 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0002816690830513835 old loss 0.0002850086893886328 BETTER
I0327 22:13:40.099156 2042450 finetune.py:68] layer 12_up @ epoch 4 new loss 0.00020498088269960135 old loss 0.00020659533038269728 BETTER
I0327 22:13:55.887413 2042660 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0003140459884889424 old loss 0.0003199024358764291 BETTER
I0327 22:13:59.908347 2042520 finetune.py:68] layer 13_up @ epoch 4 new loss 0.0002408119908068329 old loss 0.00024265219690278172 BETTER
I0327 22:14:10.777474 2042590 finetune.py:68] layer 14_up @ epoch 2 new loss 0.0002787827979773283 old loss 0.0002816690830513835 BETTER
I0327 22:14:10.846704 2042450 finetune.py:45] layer 12_gate initial loss 0.0002505071461200714
I0327 22:14:27.441480 2042660 finetune.py:68] layer 15_up @ epoch 1 new loss 0.00030991568928584456 old loss 0.0003140459884889424 BETTER
I0327 22:14:30.435723 2042520 finetune.py:45] layer 13_gate initial loss 0.00029118306702002883
I0327 22:14:41.165465 2042450 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.00024868277250789106 old loss 0.0002505071461200714 BETTER
I0327 22:14:42.961000 2042590 finetune.py:68] layer 14_up @ epoch 3 new loss 0.00027619380853138864 old loss 0.0002787827979773283 BETTER
I0327 22:14:58.756917 2042660 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0003063804470002651 old loss 0.00030991568928584456 BETTER
I0327 22:14:59.055286 2042520 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.0002890193718485534 old loss 0.00029118306702002883 BETTER
I0327 22:15:12.771680 2042450 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0002471987681929022 old loss 0.00024868277250789106 BETTER
I0327 22:15:15.411708 2042590 finetune.py:68] layer 14_up @ epoch 4 new loss 0.0002738134644459933 old loss 0.00027619380853138864 BETTER
I0327 22:15:28.692114 2042520 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.00028730579651892185 old loss 0.0002890193718485534 BETTER
I0327 22:15:30.505444 2042660 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0003032141539733857 old loss 0.0003063804470002651 BETTER
I0327 22:15:44.319350 2042450 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.00024585030041635036 old loss 0.0002471987681929022 BETTER
I0327 22:15:46.480105 2042590 finetune.py:45] layer 14_gate initial loss 0.0003289659507572651
I0327 22:15:58.688454 2042520 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0002857563958968967 old loss 0.00028730579651892185 BETTER
I0327 22:16:01.989059 2042660 finetune.py:68] layer 15_up @ epoch 4 new loss 0.00030030394555069506 old loss 0.0003032141539733857 BETTER
I0327 22:16:15.179106 2042590 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0003262522513978183 old loss 0.0003289659507572651 BETTER
I0327 22:16:16.189502 2042450 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.00024458643747493625 old loss 0.00024585030041635036 BETTER
I0327 22:16:28.530619 2042520 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.00028429162921383977 old loss 0.0002857563958968967 BETTER
I0327 22:16:32.793593 2042660 finetune.py:45] layer 15_gate initial loss 0.0003626616089604795
I0327 22:16:45.001877 2042590 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0003240799414925277 old loss 0.0003262522513978183 BETTER
I0327 22:16:47.875009 2042450 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.00024339818628504872 old loss 0.00024458643747493625 BETTER
I0327 22:16:58.481393 2042520 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0002829348959494382 old loss 0.00028429162921383977 BETTER
I0327 22:17:00.969661 2042660 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.0003594227891881019 old loss 0.0003626616089604795 BETTER
I0327 22:17:14.978586 2042590 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.00032209939672611654 old loss 0.0003240799414925277 BETTER
I0327 22:17:30.063443 2042660 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.0003566870582289994 old loss 0.0003594227891881019 BETTER
I0327 22:17:43.140379 2042450 finetune.py:45] layer 12_down initial loss 0.0003584664664231241
I0327 22:17:44.785368 2042590 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.00032023966195993125 old loss 0.00032209939672611654 BETTER
I0327 22:17:54.439885 2042520 finetune.py:45] layer 13_down initial loss 0.0004198459500912577
I0327 22:17:59.495331 2042660 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.00035419652704149485 old loss 0.0003566870582289994 BETTER
I0327 22:18:10.599005 2042450 finetune.py:68] layer 12_down @ epoch 0 new loss 0.0003584108781069517 old loss 0.0003584664664231241 BETTER
I0327 22:18:14.700807 2042590 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0003184930537827313 old loss 0.00032023966195993125 BETTER
I0327 22:18:20.586716 2042520 finetune.py:68] layer 13_down @ epoch 0 new loss 0.00041976451757363975 old loss 0.0004198459500912577 BETTER
I0327 22:18:28.946810 2042660 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.00035187657340429723 old loss 0.00035419652704149485 BETTER
I0327 22:18:39.044874 2042450 finetune.py:68] layer 12_down @ epoch 1 new loss 0.00035836180904880166 old loss 0.0003584108781069517 BETTER
I0327 22:18:47.763235 2042520 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0004196894005872309 old loss 0.00041976451757363975 BETTER
I0327 22:18:58.457821 2042660 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0003497096477076411 old loss 0.00035187657340429723 BETTER
I0327 22:19:07.998716 2042450 finetune.py:68] layer 12_down @ epoch 2 new loss 0.00035831768764182925 old loss 0.00035836180904880166 BETTER
I0327 22:19:10.575884 2042590 finetune.py:45] layer 14_down initial loss 0.00048439743113704026
I0327 22:19:15.347528 2042520 finetune.py:68] layer 13_down @ epoch 2 new loss 0.00041962615796364844 old loss 0.0004196894005872309 BETTER
I0327 22:19:36.920647 2042590 finetune.py:68] layer 14_down @ epoch 0 new loss 0.000484293996123597 old loss 0.00048439743113704026 BETTER
I0327 22:19:36.938472 2042450 finetune.py:68] layer 12_down @ epoch 3 new loss 0.00035827280953526497 old loss 0.00035831768764182925 BETTER
I0327 22:19:43.017796 2042520 finetune.py:68] layer 13_down @ epoch 3 new loss 0.00041956582572311163 old loss 0.00041962615796364844 BETTER
I0327 22:19:54.312096 2042660 finetune.py:45] layer 15_down initial loss 0.0005609613726846874
I0327 22:20:04.715678 2042590 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0004841970803681761 old loss 0.000484293996123597 BETTER
I0327 22:20:06.278943 2042450 finetune.py:68] layer 12_down @ epoch 4 new loss 0.00035823354846797884 old loss 0.00035827280953526497 BETTER
12_v proxy err 0.05530766770243645 tr(WHW.T) 363.6233825683594
bpp_loss 1.9500996650313027
12_q proxy err 0.002740431809797883 tr(WHW.T) 34157.07421875
bpp_loss 2.6528874583309516
12_k proxy err 0.001179919927380979 tr(WHW.T) 23012.369140625
bpp_loss 3.499976431892719
12_o proxy err 0.06238480284810066 tr(WHW.T) 790.4432983398438
bpp_loss 2.018245228653541
12_up proxy err 0.03880202770233154 tr(WHW.T) 10006.83203125
bpp_loss 2.1238645118262087
12_gate proxy err 0.010907916352152824 tr(WHW.T) 37117.1875
bpp_loss 2.316455870900037
12_down proxy err 0.05196497589349747 tr(WHW.T) 6882.990234375
bpp_loss 2.1192956806709327
I0327 22:20:10.787956 2042520 finetune.py:68] layer 13_down @ epoch 4 new loss 0.00041950796730816364 old loss 0.00041956582572311163 BETTER
13_v proxy err 0.06690120697021484 tr(WHW.T) 280.153564453125
bpp_loss 1.8913471988635138
13_q proxy err 0.004209162201732397 tr(WHW.T) 20908.96484375
bpp_loss 2.624642336915713
13_k proxy err 0.0014464997220784426 tr(WHW.T) 17761.771484375
bpp_loss 3.509578577708453
13_o proxy err 0.06581668555736542 tr(WHW.T) 682.8536376953125
bpp_loss 1.9994231933669653
13_up proxy err 0.038062769919633865 tr(WHW.T) 9995.2412109375
bpp_loss 2.1282020502923324
13_gate proxy err 0.01023910753428936 tr(WHW.T) 38750.55859375
bpp_loss 2.3219163671601564
13_down proxy err 0.053134944289922714 tr(WHW.T) 6620.3046875
bpp_loss 2.1191661899626655
I0327 22:20:21.320608 2042660 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0005608345381915569 old loss 0.0005609613726846874 BETTER
I0327 22:20:33.379475 2042590 finetune.py:68] layer 14_down @ epoch 2 new loss 0.00048410994349978864 old loss 0.0004841970803681761 BETTER
I0327 22:20:48.246276 2042660 finetune.py:68] layer 15_down @ epoch 1 new loss 0.0005607170751318336 old loss 0.0005608345381915569 BETTER
I0327 22:21:01.206204 2042590 finetune.py:68] layer 14_down @ epoch 3 new loss 0.00048402792890556157 old loss 0.00048410994349978864 BETTER
I0327 22:21:15.294748 2042660 finetune.py:68] layer 15_down @ epoch 2 new loss 0.0005606096819974482 old loss 0.0005607170751318336 BETTER
I0327 22:21:16.997761 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 60.361151695251465s
I0327 22:21:20.706440 2042730 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 22:21:20.706544 2042730 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 22:21:20.706585 2042730 utils.py:162] NumExpr defaulting to 16 threads.
I0327 22:21:21.074556 2042730 config.py:54] PyTorch version 2.6.0 available.
W0327 22:21:21.290199 2042730 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 22:21:22.006382 2042730 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 22:21:22.010220 2041475 quantize_finetune_llama.py:209] layer 17 gpu 1
I0327 22:21:22.024065 2042730 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 22:21:29.000111 2042590 finetune.py:68] layer 14_down @ epoch 4 new loss 0.00048395220073871315 old loss 0.00048402792890556157 BETTER
14_v proxy err 0.0627264752984047 tr(WHW.T) 281.3382873535156
bpp_loss 1.8835729311977047
14_q proxy err 0.003965509589761496 tr(WHW.T) 20898.751953125
bpp_loss 2.5976054125349037
14_k proxy err 0.001302217016927898 tr(WHW.T) 18567.6015625
bpp_loss 3.461788595537655
14_o proxy err 0.06999370455741882 tr(WHW.T) 697.529541015625
bpp_loss 1.990425894036889
14_up proxy err 0.041299305856227875 tr(WHW.T) 9159.642578125
bpp_loss 2.121902850090659
14_gate proxy err 0.009511254727840424 tr(WHW.T) 41773.9453125
bpp_loss 2.348885854938999
14_down proxy err 0.05607540160417557 tr(WHW.T) 6456.17822265625
bpp_loss 2.1154901787272786
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 22:21:38.984046 2042730 finetune.py:45] layer 16_v initial loss 0.00012006445467704907
W0327 22:21:38.984261 2042730 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 22:21:42.539313 2042660 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0005605121841654181 old loss 0.0005606096819974482 BETTER
I0327 22:22:09.840507 2042660 finetune.py:68] layer 15_down @ epoch 4 new loss 0.0005604217876680195 old loss 0.0005605121841654181 BETTER
15_v proxy err 0.06958887726068497 tr(WHW.T) 284.0271301269531
bpp_loss 1.943294731201604
15_q proxy err 0.00332032423466444 tr(WHW.T) 28093.208984375
bpp_loss 2.713904887030367
15_k proxy err 0.001420737011358142 tr(WHW.T) 18819.91796875
bpp_loss 3.5070724736433476
15_o proxy err 0.07217835634946823 tr(WHW.T) 835.0098266601562
bpp_loss 2.0196505619969685
15_up proxy err 0.042064256966114044 tr(WHW.T) 8987.46875
bpp_loss 2.115601446047159
15_gate proxy err 0.008745341561734676 tr(WHW.T) 45828.4296875
bpp_loss 2.385376229549625
15_down proxy err 0.05623984709382057 tr(WHW.T) 6479.43701171875
bpp_loss 2.1109386802584464
I0327 22:22:14.298606 2042730 finetune.py:68] layer 16_v @ epoch 0 new loss 7.595478382427245e-05 old loss 0.00012006445467704907 BETTER
I0327 22:22:35.024631 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 61.78475522994995s
I0327 22:22:38.543693 2042800 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 22:22:38.543796 2042800 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 22:22:38.543846 2042800 utils.py:162] NumExpr defaulting to 16 threads.
I0327 22:22:38.859452 2042800 config.py:54] PyTorch version 2.6.0 available.
W0327 22:22:39.057735 2042800 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 22:22:39.688511 2042800 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 22:22:39.691986 2041475 quantize_finetune_llama.py:209] layer 18 gpu 2
I0327 22:22:39.704759 2042800 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 22:22:51.101994 2042730 finetune.py:68] layer 16_v @ epoch 1 new loss 7.097461639204994e-05 old loss 7.595478382427245e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 22:22:56.415240 2042800 finetune.py:45] layer 17_v initial loss 0.00012806257291231304
W0327 22:22:56.415436 2042800 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 22:23:28.144054 2042730 finetune.py:68] layer 16_v @ epoch 2 new loss 6.824479351053014e-05 old loss 7.097461639204994e-05 BETTER
I0327 22:23:29.809437 2042800 finetune.py:68] layer 17_v @ epoch 0 new loss 7.029273547232151e-05 old loss 0.00012806257291231304 BETTER
I0327 22:23:41.494745 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 61.350541830062866s
I0327 22:23:45.181518 2042870 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 22:23:45.181640 2042870 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 22:23:45.181683 2042870 utils.py:162] NumExpr defaulting to 16 threads.
I0327 22:23:45.540284 2042870 config.py:54] PyTorch version 2.6.0 available.
W0327 22:23:45.741137 2042870 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 22:23:46.359336 2042870 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 22:23:46.363156 2041475 quantize_finetune_llama.py:209] layer 19 gpu 3
I0327 22:23:46.377072 2042870 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 22:24:03.136568 2042870 finetune.py:45] layer 18_v initial loss 0.00015141569019760936
W0327 22:24:03.137096 2042870 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 22:24:04.391727 2042800 finetune.py:68] layer 17_v @ epoch 1 new loss 6.477334682131186e-05 old loss 7.029273547232151e-05 BETTER
I0327 22:24:05.380298 2042730 finetune.py:68] layer 16_v @ epoch 3 new loss 6.631420546909794e-05 old loss 6.824479351053014e-05 BETTER
I0327 22:24:36.839033 2042870 finetune.py:68] layer 18_v @ epoch 0 new loss 5.815244367113337e-05 old loss 0.00015141569019760936 BETTER
I0327 22:24:39.414758 2042800 finetune.py:68] layer 17_v @ epoch 2 new loss 6.213645974639803e-05 old loss 6.477334682131186e-05 BETTER
I0327 22:24:42.497668 2042730 finetune.py:68] layer 16_v @ epoch 4 new loss 6.483294419012964e-05 old loss 6.631420546909794e-05 BETTER
I0327 22:24:48.670089 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 61.84909963607788s
I0327 22:24:52.314667 2042940 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 22:24:52.314760 2042940 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 22:24:52.314799 2042940 utils.py:162] NumExpr defaulting to 16 threads.
I0327 22:24:52.675837 2042940 config.py:54] PyTorch version 2.6.0 available.
W0327 22:24:52.877117 2042940 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 22:24:53.481563 2042940 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 22:24:53.485059 2041475 quantize_finetune_llama.py:209] layer 20 gpu 0
I0327 22:24:53.498119 2042940 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 22:25:01.392554 2042730 finetune.py:45] layer 16_q initial loss 7.568159344373271e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 22:25:10.142215 2042940 finetune.py:45] layer 19_v initial loss 0.00016790513473097235
W0327 22:25:10.142725 2042940 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 22:25:11.533530 2042870 finetune.py:68] layer 18_v @ epoch 1 new loss 5.245346255833283e-05 old loss 5.815244367113337e-05 BETTER
I0327 22:25:14.559260 2042800 finetune.py:68] layer 17_v @ epoch 3 new loss 6.0424397815950215e-05 old loss 6.213645974639803e-05 BETTER
I0327 22:25:36.993468 2042730 finetune.py:68] layer 16_q @ epoch 0 new loss 7.356927380897105e-05 old loss 7.568159344373271e-05 BETTER
I0327 22:25:43.126877 2042940 finetune.py:68] layer 19_v @ epoch 0 new loss 6.007241245242767e-05 old loss 0.00016790513473097235 BETTER
I0327 22:25:46.771689 2042870 finetune.py:68] layer 18_v @ epoch 2 new loss 5.003238402423449e-05 old loss 5.245346255833283e-05 BETTER
I0327 22:25:49.691201 2042800 finetune.py:68] layer 17_v @ epoch 4 new loss 5.911568950978108e-05 old loss 6.0424397815950215e-05 BETTER
I0327 22:26:09.141758 2042800 finetune.py:45] layer 17_q initial loss 6.852218939457089e-05
I0327 22:26:13.578454 2042730 finetune.py:68] layer 16_q @ epoch 1 new loss 7.21491887816228e-05 old loss 7.356927380897105e-05 BETTER
I0327 22:26:17.377835 2042940 finetune.py:68] layer 19_v @ epoch 1 new loss 5.394805339165032e-05 old loss 6.007241245242767e-05 BETTER
I0327 22:26:22.050893 2042870 finetune.py:68] layer 18_v @ epoch 3 new loss 4.8568472266197205e-05 old loss 5.003238402423449e-05 BETTER
I0327 22:26:42.699495 2042800 finetune.py:68] layer 17_q @ epoch 0 new loss 6.68535431032069e-05 old loss 6.852218939457089e-05 BETTER
I0327 22:26:50.455752 2042730 finetune.py:68] layer 16_q @ epoch 2 new loss 7.098977221176028e-05 old loss 7.21491887816228e-05 BETTER
I0327 22:26:51.889805 2042940 finetune.py:68] layer 19_v @ epoch 2 new loss 5.1534145313780755e-05 old loss 5.394805339165032e-05 BETTER
I0327 22:26:57.376836 2042870 finetune.py:68] layer 18_v @ epoch 4 new loss 4.75250999443233e-05 old loss 4.8568472266197205e-05 BETTER
I0327 22:27:16.489244 2042870 finetune.py:45] layer 18_q initial loss 5.720410990761593e-05
I0327 22:27:17.087731 2042800 finetune.py:68] layer 17_q @ epoch 1 new loss 6.568557000719011e-05 old loss 6.68535431032069e-05 BETTER
I0327 22:27:26.320945 2042940 finetune.py:68] layer 19_v @ epoch 3 new loss 5.007715299143456e-05 old loss 5.1534145313780755e-05 BETTER
I0327 22:27:27.321733 2042730 finetune.py:68] layer 16_q @ epoch 3 new loss 6.999008473940194e-05 old loss 7.098977221176028e-05 BETTER
I0327 22:27:50.263215 2042870 finetune.py:68] layer 18_q @ epoch 0 new loss 5.552598668145947e-05 old loss 5.720410990761593e-05 BETTER
I0327 22:27:51.771482 2042800 finetune.py:68] layer 17_q @ epoch 2 new loss 6.474999827332795e-05 old loss 6.568557000719011e-05 BETTER
I0327 22:28:00.885434 2042940 finetune.py:68] layer 19_v @ epoch 4 new loss 4.909943527309224e-05 old loss 5.007715299143456e-05 BETTER
I0327 22:28:04.278666 2042730 finetune.py:68] layer 16_q @ epoch 4 new loss 6.914180994499475e-05 old loss 6.999008473940194e-05 BETTER
I0327 22:28:19.894852 2042940 finetune.py:45] layer 19_q initial loss 5.748812327510677e-05
I0327 22:28:21.804310 2042730 finetune.py:45] layer 16_k initial loss 7.279217970790341e-05
I0327 22:28:24.915242 2042870 finetune.py:68] layer 18_q @ epoch 1 new loss 5.452775440062396e-05 old loss 5.552598668145947e-05 BETTER
I0327 22:28:26.477289 2042800 finetune.py:68] layer 17_q @ epoch 3 new loss 6.396003300324082e-05 old loss 6.474999827332795e-05 BETTER
I0327 22:28:53.014013 2042940 finetune.py:68] layer 19_q @ epoch 0 new loss 5.5821219575591385e-05 old loss 5.748812327510677e-05 BETTER
I0327 22:28:57.296242 2042730 finetune.py:68] layer 16_k @ epoch 0 new loss 7.179941167123616e-05 old loss 7.279217970790341e-05 BETTER
I0327 22:28:59.795328 2042870 finetune.py:68] layer 18_q @ epoch 2 new loss 5.373685416998342e-05 old loss 5.452775440062396e-05 BETTER
I0327 22:29:01.120451 2042800 finetune.py:68] layer 17_q @ epoch 4 new loss 6.324914284050465e-05 old loss 6.396003300324082e-05 BETTER
I0327 22:29:18.665728 2042800 finetune.py:45] layer 17_k initial loss 6.67832136969082e-05
I0327 22:29:27.048235 2042940 finetune.py:68] layer 19_q @ epoch 1 new loss 5.488151509780437e-05 old loss 5.5821219575591385e-05 BETTER
I0327 22:29:33.914568 2042730 finetune.py:68] layer 16_k @ epoch 1 new loss 7.107928831828758e-05 old loss 7.179941167123616e-05 BETTER
I0327 22:29:34.754973 2042870 finetune.py:68] layer 18_q @ epoch 3 new loss 5.311440327204764e-05 old loss 5.373685416998342e-05 BETTER
I0327 22:29:52.341609 2042800 finetune.py:68] layer 17_k @ epoch 0 new loss 6.602088251383975e-05 old loss 6.67832136969082e-05 BETTER
I0327 22:30:01.053307 2042940 finetune.py:68] layer 19_q @ epoch 2 new loss 5.416330532170832e-05 old loss 5.488151509780437e-05 BETTER
I0327 22:30:09.720953 2042870 finetune.py:68] layer 18_q @ epoch 4 new loss 5.256077565718442e-05 old loss 5.311440327204764e-05 BETTER
I0327 22:30:10.701450 2042730 finetune.py:68] layer 16_k @ epoch 2 new loss 7.047804683679715e-05 old loss 7.107928831828758e-05 BETTER
I0327 22:30:26.631742 2042800 finetune.py:68] layer 17_k @ epoch 1 new loss 6.544776260852814e-05 old loss 6.602088251383975e-05 BETTER
I0327 22:30:27.292122 2042870 finetune.py:45] layer 18_k initial loss 5.545524982153438e-05
I0327 22:30:35.265226 2042940 finetune.py:68] layer 19_q @ epoch 3 new loss 5.353155211196281e-05 old loss 5.416330532170832e-05 BETTER
I0327 22:30:47.625418 2042730 finetune.py:68] layer 16_k @ epoch 3 new loss 6.992254930082709e-05 old loss 7.047804683679715e-05 BETTER
I0327 22:31:00.961409 2042870 finetune.py:68] layer 18_k @ epoch 0 new loss 5.489382238010876e-05 old loss 5.545524982153438e-05 BETTER
I0327 22:31:01.226685 2042800 finetune.py:68] layer 17_k @ epoch 2 new loss 6.497788854176179e-05 old loss 6.544776260852814e-05 BETTER
I0327 22:31:09.336794 2042940 finetune.py:68] layer 19_q @ epoch 4 new loss 5.303140642354265e-05 old loss 5.353155211196281e-05 BETTER
I0327 22:31:24.363143 2042730 finetune.py:68] layer 16_k @ epoch 4 new loss 6.941708124941215e-05 old loss 6.992254930082709e-05 BETTER
I0327 22:31:27.079506 2042940 finetune.py:45] layer 19_k initial loss 5.6388715165667236e-05
I0327 22:31:35.690301 2042870 finetune.py:68] layer 18_k @ epoch 1 new loss 5.4416472266893834e-05 old loss 5.489382238010876e-05 BETTER
I0327 22:31:35.758493 2042800 finetune.py:68] layer 17_k @ epoch 3 new loss 6.453901005443186e-05 old loss 6.497788854176179e-05 BETTER
I0327 22:31:43.779366 2042730 finetune.py:45] layer 16_o initial loss 0.0001620195689611137
I0327 22:32:00.036447 2042940 finetune.py:68] layer 19_k @ epoch 0 new loss 5.580146171269007e-05 old loss 5.6388715165667236e-05 BETTER
I0327 22:32:10.218914 2042800 finetune.py:68] layer 17_k @ epoch 4 new loss 6.415194366127253e-05 old loss 6.453901005443186e-05 BETTER
I0327 22:32:10.511990 2042870 finetune.py:68] layer 18_k @ epoch 2 new loss 5.4013478802517056e-05 old loss 5.4416472266893834e-05 BETTER
I0327 22:32:18.910323 2042730 finetune.py:68] layer 16_o @ epoch 0 new loss 0.0001549863809486851 old loss 0.0001620195689611137 BETTER
I0327 22:32:29.428652 2042800 finetune.py:45] layer 17_o initial loss 0.0001551646855659783
I0327 22:32:34.033705 2042940 finetune.py:68] layer 19_k @ epoch 1 new loss 5.5401706049451604e-05 old loss 5.580146171269007e-05 BETTER
I0327 22:32:45.369557 2042870 finetune.py:68] layer 18_k @ epoch 3 new loss 5.367062476580031e-05 old loss 5.4013478802517056e-05 BETTER
I0327 22:32:54.788830 2042730 finetune.py:68] layer 16_o @ epoch 1 new loss 0.0001522057573311031 old loss 0.0001549863809486851 BETTER
I0327 22:33:02.351495 2042800 finetune.py:68] layer 17_o @ epoch 0 new loss 0.00014708461822010577 old loss 0.0001551646855659783 BETTER
I0327 22:33:07.965375 2042940 finetune.py:68] layer 19_k @ epoch 2 new loss 5.504992077476345e-05 old loss 5.5401706049451604e-05 BETTER
I0327 22:33:20.144042 2042870 finetune.py:68] layer 18_k @ epoch 4 new loss 5.3354695410234854e-05 old loss 5.367062476580031e-05 BETTER
I0327 22:33:30.743625 2042730 finetune.py:68] layer 16_o @ epoch 2 new loss 0.00015012935909908265 old loss 0.0001522057573311031 BETTER
I0327 22:33:36.132293 2042800 finetune.py:68] layer 17_o @ epoch 1 new loss 0.00014433961769100279 old loss 0.00014708461822010577 BETTER
I0327 22:33:39.373410 2042870 finetune.py:45] layer 18_o initial loss 0.0001312976673943922
I0327 22:33:41.932399 2042940 finetune.py:68] layer 19_k @ epoch 3 new loss 5.4734362493036315e-05 old loss 5.504992077476345e-05 BETTER
I0327 22:34:06.815034 2042730 finetune.py:68] layer 16_o @ epoch 3 new loss 0.0001483836240367964 old loss 0.00015012935909908265 BETTER
I0327 22:34:09.969232 2042800 finetune.py:68] layer 17_o @ epoch 2 new loss 0.00014234795526135713 old loss 0.00014433961769100279 BETTER
I0327 22:34:12.438532 2042870 finetune.py:68] layer 18_o @ epoch 0 new loss 0.00012025336036458611 old loss 0.0001312976673943922 BETTER
I0327 22:34:15.906231 2042940 finetune.py:68] layer 19_k @ epoch 4 new loss 5.444702037493698e-05 old loss 5.4734362493036315e-05 BETTER
I0327 22:34:34.960302 2042940 finetune.py:45] layer 19_o initial loss 0.00012508434883784503
I0327 22:34:42.936696 2042730 finetune.py:68] layer 16_o @ epoch 4 new loss 0.00014690475654788315 old loss 0.0001483836240367964 BETTER
I0327 22:34:43.945535 2042800 finetune.py:68] layer 17_o @ epoch 3 new loss 0.00014073928468860686 old loss 0.00014234795526135713 BETTER
I0327 22:34:46.536122 2042870 finetune.py:68] layer 18_o @ epoch 1 new loss 0.00011810811702162027 old loss 0.00012025336036458611 BETTER
I0327 22:35:07.201279 2042940 finetune.py:68] layer 19_o @ epoch 0 new loss 0.00011481982801342383 old loss 0.00012508434883784503 BETTER
I0327 22:35:14.122865 2042730 finetune.py:45] layer 16_up initial loss 0.0003375069354660809
I0327 22:35:18.063489 2042800 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0001393474085489288 old loss 0.00014073928468860686 BETTER
I0327 22:35:20.550702 2042870 finetune.py:68] layer 18_o @ epoch 2 new loss 0.00011669020022964105 old loss 0.00011810811702162027 BETTER
I0327 22:35:40.514517 2042940 finetune.py:68] layer 19_o @ epoch 1 new loss 0.00011293841089354828 old loss 0.00011481982801342383 BETTER
I0327 22:35:46.155812 2042730 finetune.py:68] layer 16_up @ epoch 0 new loss 0.0003316343354526907 old loss 0.0003375069354660809 BETTER
I0327 22:35:48.982232 2042800 finetune.py:45] layer 17_up initial loss 0.0003558204625733197
I0327 22:35:54.552623 2042870 finetune.py:68] layer 18_o @ epoch 3 new loss 0.00011557769903447479 old loss 0.00011669020022964105 BETTER
I0327 22:36:14.168533 2042940 finetune.py:68] layer 19_o @ epoch 2 new loss 0.00011171273945365101 old loss 0.00011293841089354828 BETTER
I0327 22:36:19.650054 2042800 finetune.py:68] layer 17_up @ epoch 0 new loss 0.0003493606927804649 old loss 0.0003558204625733197 BETTER
I0327 22:36:19.664481 2042730 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0003275014751125127 old loss 0.0003316343354526907 BETTER
I0327 22:36:29.027702 2042870 finetune.py:68] layer 18_o @ epoch 4 new loss 0.00011468335287645459 old loss 0.00011557769903447479 BETTER
I0327 22:36:47.509002 2042940 finetune.py:68] layer 19_o @ epoch 3 new loss 0.00011073357745772228 old loss 0.00011171273945365101 BETTER
I0327 22:36:51.556630 2042800 finetune.py:68] layer 17_up @ epoch 1 new loss 0.00034483737545087934 old loss 0.0003493606927804649 BETTER
I0327 22:36:53.172235 2042730 finetune.py:68] layer 16_up @ epoch 2 new loss 0.00032394303707405925 old loss 0.0003275014751125127 BETTER
I0327 22:37:00.069673 2042870 finetune.py:45] layer 18_up initial loss 0.0003382430295459926
I0327 22:37:20.882455 2042940 finetune.py:68] layer 19_o @ epoch 4 new loss 0.00010993903561029583 old loss 0.00011073357745772228 BETTER
I0327 22:37:23.780024 2042800 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0003410413337405771 old loss 0.00034483737545087934 BETTER
I0327 22:37:26.813776 2042730 finetune.py:68] layer 16_up @ epoch 3 new loss 0.00032078256481327116 old loss 0.00032394303707405925 BETTER
I0327 22:37:31.079603 2042870 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0003322105621919036 old loss 0.0003382430295459926 BETTER
I0327 22:37:51.846140 2042940 finetune.py:45] layer 19_up initial loss 0.0003516854194458574
I0327 22:37:55.969252 2042800 finetune.py:68] layer 17_up @ epoch 3 new loss 0.000337672681780532 old loss 0.0003410413337405771 BETTER
I0327 22:38:00.609072 2042730 finetune.py:68] layer 16_up @ epoch 4 new loss 0.00031788815977051854 old loss 0.00032078256481327116 BETTER
I0327 22:38:03.196337 2042870 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0003281804674770683 old loss 0.0003322105621919036 BETTER
I0327 22:38:22.021088 2042940 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0003458687860984355 old loss 0.0003516854194458574 BETTER
I0327 22:38:28.215076 2042800 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0003346252487972379 old loss 0.000337672681780532 BETTER
I0327 22:38:31.865560 2042730 finetune.py:45] layer 16_gate initial loss 0.0003861406585201621
I0327 22:38:35.266183 2042870 finetune.py:68] layer 18_up @ epoch 2 new loss 0.00032484400435350835 old loss 0.0003281804674770683 BETTER
I0327 22:38:53.294322 2042940 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0003418995183892548 old loss 0.0003458687860984355 BETTER
I0327 22:38:59.185161 2042800 finetune.py:45] layer 17_gate initial loss 0.0004140217206440866
I0327 22:39:02.140311 2042730 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.000382971455110237 old loss 0.0003861406585201621 BETTER
I0327 22:39:07.393143 2042870 finetune.py:68] layer 18_up @ epoch 3 new loss 0.0003218672354705632 old loss 0.00032484400435350835 BETTER
I0327 22:39:24.693469 2042940 finetune.py:68] layer 19_up @ epoch 2 new loss 0.00033856098889373243 old loss 0.0003418995183892548 BETTER
I0327 22:39:27.781829 2042800 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.00041047148988582194 old loss 0.0004140217206440866 BETTER
I0327 22:39:33.487088 2042730 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0003802980063483119 old loss 0.000382971455110237 BETTER
I0327 22:39:39.678090 2042870 finetune.py:68] layer 18_up @ epoch 4 new loss 0.00031920894980430603 old loss 0.0003218672354705632 BETTER
I0327 22:39:56.219140 2042940 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0003356474335305393 old loss 0.00033856098889373243 BETTER
I0327 22:39:57.405009 2042800 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.000407587387599051 old loss 0.00041047148988582194 BETTER
I0327 22:40:04.987474 2042730 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.00037782490835525095 old loss 0.0003802980063483119 BETTER
I0327 22:40:10.752054 2042870 finetune.py:45] layer 18_gate initial loss 0.0004045150999445468
I0327 22:40:27.129559 2042800 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.0004049389681313187 old loss 0.000407587387599051 BETTER
I0327 22:40:27.810539 2042940 finetune.py:68] layer 19_up @ epoch 4 new loss 0.00033301825169473886 old loss 0.0003356474335305393 BETTER
I0327 22:40:36.667424 2042730 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.000375528383301571 old loss 0.00037782490835525095 BETTER
I0327 22:40:39.484256 2042870 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0004013813741039485 old loss 0.0004045150999445468 BETTER
I0327 22:40:56.975451 2042800 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.00040244541014544666 old loss 0.0004049389681313187 BETTER
I0327 22:40:58.924942 2042940 finetune.py:45] layer 19_gate initial loss 0.00042561767622828484
I0327 22:41:08.411602 2042730 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.00037338421680033207 old loss 0.000375528383301571 BETTER
I0327 22:41:09.448598 2042870 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.0003988516691606492 old loss 0.0004013813741039485 BETTER
I0327 22:41:27.164045 2042800 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.00040014085243456066 old loss 0.00040244541014544666 BETTER
I0327 22:41:27.286256 2042940 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.000422581157181412 old loss 0.00042561767622828484 BETTER
I0327 22:41:39.506288 2042870 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0003965153591707349 old loss 0.0003988516691606492 BETTER
I0327 22:41:56.439048 2042940 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0004200838156975806 old loss 0.000422581157181412 BETTER
I0327 22:42:04.484646 2042730 finetune.py:45] layer 16_down initial loss 0.0006066425121389329
I0327 22:42:09.695119 2042870 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0003943678457289934 old loss 0.0003965153591707349 BETTER
I0327 22:42:23.636096 2042800 finetune.py:45] layer 17_down initial loss 0.0006841363501735032
I0327 22:42:26.207901 2042940 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0004177434602752328 old loss 0.0004200838156975806 BETTER
I0327 22:42:31.943222 2042730 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0006064952467568219 old loss 0.0006066425121389329 BETTER
I0327 22:42:39.822507 2042870 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.00039233919233083725 old loss 0.0003943678457289934 BETTER
I0327 22:42:50.110864 2042800 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0006839589914306998 old loss 0.0006841363501735032 BETTER
I0327 22:42:55.838978 2042940 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0004156093636993319 old loss 0.0004177434602752328 BETTER
I0327 22:43:00.401527 2042730 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0006063604378141463 old loss 0.0006064952467568219 BETTER
I0327 22:43:17.579698 2042800 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0006837976397946477 old loss 0.0006839589914306998 BETTER
I0327 22:43:25.998562 2042940 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.0004136218340136111 old loss 0.0004156093636993319 BETTER
I0327 22:43:29.718308 2042730 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0006062329630367458 old loss 0.0006063604378141463 BETTER
I0327 22:43:38.452797 2042870 finetune.py:45] layer 18_down initial loss 0.0006796480738557875
I0327 22:43:45.916859 2042800 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0006836532265879214 old loss 0.0006837976397946477 BETTER
I0327 22:43:59.288568 2042730 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0006061183521524072 old loss 0.0006062329630367458 BETTER
I0327 22:44:05.181117 2042870 finetune.py:68] layer 18_down @ epoch 0 new loss 0.000679501099511981 old loss 0.0006796480738557875 BETTER
I0327 22:44:14.261672 2042800 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0006835150998085737 old loss 0.0006836532265879214 BETTER
I0327 22:44:26.507216 2042940 finetune.py:45] layer 19_down initial loss 0.0007149089360609651
I0327 22:44:30.094459 2042730 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0006060096784494817 old loss 0.0006061183521524072 BETTER
16_v proxy err 0.06484292447566986 tr(WHW.T) 274.28167724609375
bpp_loss 1.9010555223794654
16_q proxy err 0.0034514092840254307 tr(WHW.T) 24486.2109375
bpp_loss 2.695306359906681
16_k proxy err 0.0012482365127652884 tr(WHW.T) 19440.03515625
bpp_loss 3.496747436700389
16_o proxy err 0.05994585528969765 tr(WHW.T) 978.1992797851562
bpp_loss 2.003665506868856
16_up proxy err 0.04740274325013161 tr(WHW.T) 8331.994140625
bpp_loss 2.1031746352424046
16_gate proxy err 0.01033881027251482 tr(WHW.T) 40938.23046875
bpp_loss 2.416401313384995
16_down proxy err 0.05873393267393112 tr(WHW.T) 6368.43701171875
bpp_loss 2.096685607377107
I0327 22:44:33.692172 2042870 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0006793664069846272 old loss 0.000679501099511981 BETTER
I0327 22:44:42.952493 2042800 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0006833907100372016 old loss 0.0006835150998085737 BETTER
17_v proxy err 0.07144491374492645 tr(WHW.T) 283.9730224609375
bpp_loss 1.9681594518769998
17_q proxy err 0.0034291266929358244 tr(WHW.T) 27579.13671875
bpp_loss 2.7062328679603525
17_k proxy err 0.0015695174224674702 tr(WHW.T) 17373.92578125
bpp_loss 3.5218189845909365
17_o proxy err 0.06540437042713165 tr(WHW.T) 1113.8572998046875
bpp_loss 2.0314106812584214
17_up proxy err 0.046790435910224915 tr(WHW.T) 8451.3134765625
bpp_loss 2.1015033882576972
17_gate proxy err 0.010302585549652576 tr(WHW.T) 41304.046875
bpp_loss 2.429870648841773
17_down proxy err 0.05958620831370354 tr(WHW.T) 6292.90771484375
bpp_loss 2.092968001479416
I0327 22:44:53.099208 2042940 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0007147639407776296 old loss 0.0007149089360609651 BETTER
I0327 22:45:02.087812 2042870 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0006792428903281689 old loss 0.0006793664069846272 BETTER
I0327 22:45:20.032705 2042940 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0007146330899558961 old loss 0.0007147639407776296 BETTER
I0327 22:45:29.874097 2042870 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0006791259511373937 old loss 0.0006792428903281689 BETTER
I0327 22:45:47.269572 2042940 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0007145119016058743 old loss 0.0007146330899558961 BETTER
I0327 22:45:52.511301 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 64.86855149269104s
I0327 22:45:56.530323 2043010 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 22:45:56.530507 2043010 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 22:45:56.530581 2043010 utils.py:162] NumExpr defaulting to 16 threads.
I0327 22:45:56.913839 2043010 config.py:54] PyTorch version 2.6.0 available.
W0327 22:45:57.142895 2043010 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0327 22:45:57.628151 2042870 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0006790159386582673 old loss 0.0006791259511373937 BETTER
W0327 22:45:57.728217 2043010 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 22:45:57.731907 2041475 quantize_finetune_llama.py:209] layer 21 gpu 1
I0327 22:45:57.749959 2043010 data_utils.py:336] using 256 training seqs, 128 validation seqs
18_v proxy err 0.07091806828975677 tr(WHW.T) 287.61376953125
bpp_loss 1.8961419870320242
18_q proxy err 0.00433909660205245 tr(WHW.T) 22422.203125
bpp_loss 2.706429398385808
18_k proxy err 0.001628634869121015 tr(WHW.T) 17365.408203125
bpp_loss 3.5887583883595653
18_o proxy err 0.06441029906272888 tr(WHW.T) 1213.3604736328125
bpp_loss 2.0120909198594745
18_up proxy err 0.05149250105023384 tr(WHW.T) 7988.31396484375
bpp_loss 2.0979469906139587
18_gate proxy err 0.012708901427686214 tr(WHW.T) 34940.45703125
bpp_loss 2.4373370018188973
18_down proxy err 0.06126664578914642 tr(WHW.T) 6314.1357421875
bpp_loss 2.091904887264328
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 22:46:14.419852 2043010 finetune.py:45] layer 20_v initial loss 0.00015759174129925668
W0327 22:46:14.420315 2043010 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 22:46:14.659511 2042940 finetune.py:68] layer 19_down @ epoch 3 new loss 0.000714398454874754 old loss 0.0007145119016058743 BETTER
I0327 22:46:42.052593 2042940 finetune.py:68] layer 19_down @ epoch 4 new loss 0.000714292109478265 old loss 0.000714398454874754 BETTER
19_v proxy err 0.06243611127138138 tr(WHW.T) 341.0596618652344
bpp_loss 1.9408229142136406
19_q proxy err 0.004172584041953087 tr(WHW.T) 24058.46875
bpp_loss 2.709237545612268
19_k proxy err 0.0018527701031416655 tr(WHW.T) 15520.8154296875
bpp_loss 3.505369591177441
19_o proxy err 0.06683523207902908 tr(WHW.T) 1174.47802734375
bpp_loss 2.0236980770423543
19_up proxy err 0.05461844801902771 tr(WHW.T) 7651.05419921875
bpp_loss 2.094483732778047
19_gate proxy err 0.013899585232138634 tr(WHW.T) 32554.74609375
bpp_loss 2.447333845110344
19_down proxy err 0.06204714626073837 tr(WHW.T) 6258.42138671875
bpp_loss 2.089694989138349
I0327 22:46:49.588969 2043010 finetune.py:68] layer 20_v @ epoch 0 new loss 6.288984150160104e-05 old loss 0.00015759174129925668 BETTER
I0327 22:47:03.422687 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 61.572937965393066s
I0327 22:47:06.970021 2043080 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 22:47:06.970118 2043080 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 22:47:06.970159 2043080 utils.py:162] NumExpr defaulting to 16 threads.
I0327 22:47:07.293922 2043080 config.py:54] PyTorch version 2.6.0 available.
W0327 22:47:07.484333 2043080 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 22:47:08.044473 2043080 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 22:47:08.047973 2041475 quantize_finetune_llama.py:209] layer 22 gpu 2
I0327 22:47:08.060994 2043080 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 22:47:24.449131 2043080 finetune.py:45] layer 21_v initial loss 0.00017223981558345258
W0327 22:47:24.449332 2043080 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 22:47:26.235048 2043010 finetune.py:68] layer 20_v @ epoch 1 new loss 5.735651939176023e-05 old loss 6.288984150160104e-05 BETTER
I0327 22:47:57.900380 2043080 finetune.py:68] layer 21_v @ epoch 0 new loss 7.691181235713884e-05 old loss 0.00017223981558345258 BETTER
I0327 22:48:03.131018 2043010 finetune.py:68] layer 20_v @ epoch 2 new loss 5.5099037126637995e-05 old loss 5.735651939176023e-05 BETTER
I0327 22:48:09.883930 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 61.3957462310791s
I0327 22:48:13.616879 2043150 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 22:48:13.616979 2043150 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 22:48:13.617017 2043150 utils.py:162] NumExpr defaulting to 16 threads.
I0327 22:48:13.992877 2043150 config.py:54] PyTorch version 2.6.0 available.
W0327 22:48:14.189765 2043150 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 22:48:14.783900 2043150 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 22:48:14.787711 2041475 quantize_finetune_llama.py:209] layer 23 gpu 3
I0327 22:48:14.802793 2043150 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 22:48:31.507158 2043150 finetune.py:45] layer 22_v initial loss 0.00019629820599220693
W0327 22:48:31.507376 2043150 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 22:48:32.332168 2043080 finetune.py:68] layer 21_v @ epoch 1 new loss 6.99131196597591e-05 old loss 7.691181235713884e-05 BETTER
I0327 22:48:40.118848 2043010 finetune.py:68] layer 20_v @ epoch 3 new loss 5.3635256335837767e-05 old loss 5.5099037126637995e-05 BETTER
I0327 22:49:05.131297 2043150 finetune.py:68] layer 22_v @ epoch 0 new loss 6.42053855699487e-05 old loss 0.00019629820599220693 BETTER
I0327 22:49:07.303503 2043080 finetune.py:68] layer 21_v @ epoch 2 new loss 6.687130371574312e-05 old loss 6.99131196597591e-05 BETTER
I0327 22:49:17.069935 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 61.82212781906128s
I0327 22:49:17.131707 2043010 finetune.py:68] layer 20_v @ epoch 4 new loss 5.258356759441085e-05 old loss 5.3635256335837767e-05 BETTER
I0327 22:49:20.751381 2043220 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 22:49:20.751489 2043220 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 22:49:20.751532 2043220 utils.py:162] NumExpr defaulting to 16 threads.
I0327 22:49:21.085506 2043220 config.py:54] PyTorch version 2.6.0 available.
W0327 22:49:21.287617 2043220 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 22:49:21.861894 2043220 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 22:49:21.865392 2041475 quantize_finetune_llama.py:209] layer 24 gpu 0
I0327 22:49:21.878598 2043220 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 22:49:36.079045 2043010 finetune.py:45] layer 20_q initial loss 6.232957093743607e-05
I0327 22:49:38.567924 2043220 finetune.py:45] layer 23_v initial loss 0.00021546188509091735
W0327 22:49:38.568156 2043220 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 22:49:39.882723 2043150 finetune.py:68] layer 22_v @ epoch 1 new loss 5.669502206728794e-05 old loss 6.42053855699487e-05 BETTER
I0327 22:49:42.339718 2043080 finetune.py:68] layer 21_v @ epoch 3 new loss 6.490635132649913e-05 old loss 6.687130371574312e-05 BETTER
I0327 22:50:11.560798 2043220 finetune.py:68] layer 23_v @ epoch 0 new loss 6.579331966349855e-05 old loss 0.00021546188509091735 BETTER
I0327 22:50:11.771164 2043010 finetune.py:68] layer 20_q @ epoch 0 new loss 6.059374209144153e-05 old loss 6.232957093743607e-05 BETTER
I0327 22:50:15.084351 2043150 finetune.py:68] layer 22_v @ epoch 2 new loss 5.398924258770421e-05 old loss 5.669502206728794e-05 BETTER
I0327 22:50:17.626406 2043080 finetune.py:68] layer 21_v @ epoch 4 new loss 6.343131099129096e-05 old loss 6.490635132649913e-05 BETTER
I0327 22:50:36.694234 2043080 finetune.py:45] layer 21_q initial loss 7.836769509594887e-05
I0327 22:50:45.814559 2043220 finetune.py:68] layer 23_v @ epoch 1 new loss 5.7177134294761345e-05 old loss 6.579331966349855e-05 BETTER
I0327 22:50:48.521453 2043010 finetune.py:68] layer 20_q @ epoch 1 new loss 5.9480593336047605e-05 old loss 6.059374209144153e-05 BETTER
I0327 22:50:50.425014 2043150 finetune.py:68] layer 22_v @ epoch 3 new loss 5.241429971647449e-05 old loss 5.398924258770421e-05 BETTER
I0327 22:51:10.161321 2043080 finetune.py:68] layer 21_q @ epoch 0 new loss 7.59745089453645e-05 old loss 7.836769509594887e-05 BETTER
I0327 22:51:20.325558 2043220 finetune.py:68] layer 23_v @ epoch 2 new loss 5.4300639021676034e-05 old loss 5.7177134294761345e-05 BETTER
I0327 22:51:25.455151 2043010 finetune.py:68] layer 20_q @ epoch 2 new loss 5.86185633437708e-05 old loss 5.9480593336047605e-05 BETTER
I0327 22:51:25.732602 2043150 finetune.py:68] layer 22_v @ epoch 4 new loss 5.133025115355849e-05 old loss 5.241429971647449e-05 BETTER
I0327 22:51:44.599500 2043080 finetune.py:68] layer 21_q @ epoch 1 new loss 7.439688488375396e-05 old loss 7.59745089453645e-05 BETTER
I0327 22:51:45.302198 2043150 finetune.py:45] layer 22_q initial loss 6.416363612515852e-05
I0327 22:51:55.034730 2043220 finetune.py:68] layer 23_v @ epoch 3 new loss 5.275345029076561e-05 old loss 5.4300639021676034e-05 BETTER
I0327 22:52:02.223756 2043010 finetune.py:68] layer 20_q @ epoch 3 new loss 5.788737689726986e-05 old loss 5.86185633437708e-05 BETTER
I0327 22:52:19.099549 2043150 finetune.py:68] layer 22_q @ epoch 0 new loss 6.222874799277633e-05 old loss 6.416363612515852e-05 BETTER
I0327 22:52:19.231454 2043080 finetune.py:68] layer 21_q @ epoch 2 new loss 7.314945833059028e-05 old loss 7.439688488375396e-05 BETTER
I0327 22:52:29.545933 2043220 finetune.py:68] layer 23_v @ epoch 4 new loss 5.166655682842247e-05 old loss 5.275345029076561e-05 BETTER
I0327 22:52:39.051625 2043010 finetune.py:68] layer 20_q @ epoch 4 new loss 5.730283373850398e-05 old loss 5.788737689726986e-05 BETTER
I0327 22:52:48.315951 2043220 finetune.py:45] layer 23_q initial loss 6.339129322441295e-05
I0327 22:52:53.714339 2043150 finetune.py:68] layer 22_q @ epoch 1 new loss 6.108576781116426e-05 old loss 6.222874799277633e-05 BETTER
I0327 22:52:53.818036 2043080 finetune.py:68] layer 21_q @ epoch 3 new loss 7.206383452285081e-05 old loss 7.314945833059028e-05 BETTER
I0327 22:52:56.938907 2043010 finetune.py:45] layer 20_k initial loss 6.071268580853939e-05
I0327 22:53:21.166862 2043220 finetune.py:68] layer 23_q @ epoch 0 new loss 6.169008702272549e-05 old loss 6.339129322441295e-05 BETTER
I0327 22:53:28.487732 2043080 finetune.py:68] layer 21_q @ epoch 4 new loss 7.114291656762362e-05 old loss 7.206383452285081e-05 BETTER
I0327 22:53:28.755411 2043150 finetune.py:68] layer 22_q @ epoch 2 new loss 6.02324289502576e-05 old loss 6.108576781116426e-05 BETTER
I0327 22:53:32.404851 2043010 finetune.py:68] layer 20_k @ epoch 0 new loss 5.9993137256242335e-05 old loss 6.071268580853939e-05 BETTER
I0327 22:53:46.159672 2043080 finetune.py:45] layer 21_k initial loss 7.749527867417783e-05
I0327 22:53:55.121510 2043220 finetune.py:68] layer 23_q @ epoch 1 new loss 6.0657002904918045e-05 old loss 6.169008702272549e-05 BETTER
I0327 22:54:03.753975 2043150 finetune.py:68] layer 22_q @ epoch 3 new loss 5.9527039411477745e-05 old loss 6.02324289502576e-05 BETTER
I0327 22:54:08.802357 2043010 finetune.py:68] layer 20_k @ epoch 1 new loss 5.947067984379828e-05 old loss 5.9993137256242335e-05 BETTER
I0327 22:54:19.530938 2043080 finetune.py:68] layer 21_k @ epoch 0 new loss 7.577308133477345e-05 old loss 7.749527867417783e-05 BETTER
I0327 22:54:29.267972 2043220 finetune.py:68] layer 23_q @ epoch 2 new loss 5.986243922961876e-05 old loss 6.0657002904918045e-05 BETTER
I0327 22:54:38.859499 2043150 finetune.py:68] layer 22_q @ epoch 4 new loss 5.8920300944009796e-05 old loss 5.9527039411477745e-05 BETTER
I0327 22:54:45.478950 2043010 finetune.py:68] layer 20_k @ epoch 2 new loss 5.9020472690463066e-05 old loss 5.947067984379828e-05 BETTER
I0327 22:54:53.887211 2043080 finetune.py:68] layer 21_k @ epoch 1 new loss 7.500581705244258e-05 old loss 7.577308133477345e-05 BETTER
I0327 22:54:56.534408 2043150 finetune.py:45] layer 22_k initial loss 6.35377800790593e-05
I0327 22:55:03.428087 2043220 finetune.py:68] layer 23_q @ epoch 3 new loss 5.920548574067652e-05 old loss 5.986243922961876e-05 BETTER
I0327 22:55:22.284499 2043010 finetune.py:68] layer 20_k @ epoch 3 new loss 5.8643803640734404e-05 old loss 5.9020472690463066e-05 BETTER
I0327 22:55:28.544101 2043080 finetune.py:68] layer 21_k @ epoch 2 new loss 7.433539576595649e-05 old loss 7.500581705244258e-05 BETTER
I0327 22:55:30.133362 2043150 finetune.py:68] layer 22_k @ epoch 0 new loss 6.285163544816896e-05 old loss 6.35377800790593e-05 BETTER
I0327 22:55:37.605499 2043220 finetune.py:68] layer 23_q @ epoch 4 new loss 5.86504174862057e-05 old loss 5.920548574067652e-05 BETTER
I0327 22:55:54.932856 2043220 finetune.py:45] layer 23_k initial loss 6.428857886930928e-05
I0327 22:55:59.171621 2043010 finetune.py:68] layer 20_k @ epoch 4 new loss 5.8303470723330975e-05 old loss 5.8643803640734404e-05 BETTER
I0327 22:56:03.355395 2043080 finetune.py:68] layer 21_k @ epoch 3 new loss 7.375895802397281e-05 old loss 7.433539576595649e-05 BETTER
I0327 22:56:04.765485 2043150 finetune.py:68] layer 22_k @ epoch 1 new loss 6.23811429250054e-05 old loss 6.285163544816896e-05 BETTER
I0327 22:56:18.445367 2043010 finetune.py:45] layer 20_o initial loss 0.00013349104847293347
I0327 22:56:27.972512 2043220 finetune.py:68] layer 23_k @ epoch 0 new loss 6.361379928421229e-05 old loss 6.428857886930928e-05 BETTER
I0327 22:56:38.018009 2043080 finetune.py:68] layer 21_k @ epoch 4 new loss 7.32414991944097e-05 old loss 7.375895802397281e-05 BETTER
I0327 22:56:39.616653 2043150 finetune.py:68] layer 22_k @ epoch 2 new loss 6.197951006470248e-05 old loss 6.23811429250054e-05 BETTER
I0327 22:56:53.395766 2043010 finetune.py:68] layer 20_o @ epoch 0 new loss 0.0001235652161994949 old loss 0.00013349104847293347 BETTER
I0327 22:56:57.495059 2043080 finetune.py:45] layer 21_o initial loss 0.00016702730499673635
I0327 22:57:01.962220 2043220 finetune.py:68] layer 23_k @ epoch 1 new loss 6.316346843959764e-05 old loss 6.361379928421229e-05 BETTER
I0327 22:57:14.424822 2043150 finetune.py:68] layer 22_k @ epoch 3 new loss 6.16186807746999e-05 old loss 6.197951006470248e-05 BETTER
I0327 22:57:29.324892 2043010 finetune.py:68] layer 20_o @ epoch 1 new loss 0.00012147967208875343 old loss 0.0001235652161994949 BETTER
I0327 22:57:30.547256 2043080 finetune.py:68] layer 21_o @ epoch 0 new loss 0.0001558357907924801 old loss 0.00016702730499673635 BETTER
I0327 22:57:35.989667 2043220 finetune.py:68] layer 23_k @ epoch 2 new loss 6.276106432778761e-05 old loss 6.316346843959764e-05 BETTER
I0327 22:57:49.257232 2043150 finetune.py:68] layer 22_k @ epoch 4 new loss 6.131211557658389e-05 old loss 6.16186807746999e-05 BETTER
I0327 22:58:04.358593 2043080 finetune.py:68] layer 21_o @ epoch 1 new loss 0.00015296060882974416 old loss 0.0001558357907924801 BETTER
I0327 22:58:05.439981 2043010 finetune.py:68] layer 20_o @ epoch 2 new loss 0.00012006771430606022 old loss 0.00012147967208875343 BETTER
I0327 22:58:08.927946 2043150 finetune.py:45] layer 22_o initial loss 0.00015373995120171458
I0327 22:58:09.958518 2043220 finetune.py:68] layer 23_k @ epoch 3 new loss 6.242659583222121e-05 old loss 6.276106432778761e-05 BETTER
I0327 22:58:38.496708 2043080 finetune.py:68] layer 21_o @ epoch 2 new loss 0.0001509318535681814 old loss 0.00015296060882974416 BETTER
I0327 22:58:41.688103 2043010 finetune.py:68] layer 20_o @ epoch 3 new loss 0.00011893585178768262 old loss 0.00012006771430606022 BETTER
I0327 22:58:42.079729 2043150 finetune.py:68] layer 22_o @ epoch 0 new loss 0.00014185518375597894 old loss 0.00015373995120171458 BETTER
I0327 22:58:44.085808 2043220 finetune.py:68] layer 23_k @ epoch 4 new loss 6.212458538357168e-05 old loss 6.242659583222121e-05 BETTER
I0327 22:59:03.015465 2043220 finetune.py:45] layer 23_o initial loss 0.0001610456092748791
I0327 22:59:12.395378 2043080 finetune.py:68] layer 21_o @ epoch 3 new loss 0.00014930919860489666 old loss 0.0001509318535681814 BETTER
I0327 22:59:16.166049 2043150 finetune.py:68] layer 22_o @ epoch 1 new loss 0.00013972098531667143 old loss 0.00014185518375597894 BETTER
I0327 22:59:17.687765 2043010 finetune.py:68] layer 20_o @ epoch 4 new loss 0.00011800238280557096 old loss 0.00011893585178768262 BETTER
I0327 22:59:35.116661 2043220 finetune.py:68] layer 23_o @ epoch 0 new loss 0.0001469918352086097 old loss 0.0001610456092748791 BETTER
I0327 22:59:46.576852 2043080 finetune.py:68] layer 21_o @ epoch 4 new loss 0.00014795844617765397 old loss 0.00014930919860489666 BETTER
I0327 22:59:48.793264 2043010 finetune.py:45] layer 20_up initial loss 0.0003774265060201287
I0327 22:59:50.315931 2043150 finetune.py:68] layer 22_o @ epoch 2 new loss 0.0001382860791636631 old loss 0.00013972098531667143 BETTER
I0327 23:00:08.803872 2043220 finetune.py:68] layer 23_o @ epoch 1 new loss 0.00014459030353464186 old loss 0.0001469918352086097 BETTER
I0327 23:00:17.871465 2043080 finetune.py:45] layer 21_up initial loss 0.0004446285020094365
I0327 23:00:20.981636 2043010 finetune.py:68] layer 20_up @ epoch 0 new loss 0.00037139540654607117 old loss 0.0003774265060201287 BETTER
I0327 23:00:24.614291 2043150 finetune.py:68] layer 22_o @ epoch 3 new loss 0.00013721681898459792 old loss 0.0001382860791636631 BETTER
I0327 23:00:42.274178 2043220 finetune.py:68] layer 23_o @ epoch 2 new loss 0.0001430743286618963 old loss 0.00014459030353464186 BETTER
I0327 23:00:48.425466 2043080 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0004377298173494637 old loss 0.0004446285020094365 BETTER
I0327 23:00:54.395464 2043010 finetune.py:68] layer 20_up @ epoch 1 new loss 0.00036725480458699167 old loss 0.00037139540654607117 BETTER
I0327 23:00:58.954885 2043150 finetune.py:68] layer 22_o @ epoch 4 new loss 0.0001363240007776767 old loss 0.00013721681898459792 BETTER
I0327 23:01:16.606609 2043220 finetune.py:68] layer 23_o @ epoch 3 new loss 0.00014192443632055074 old loss 0.0001430743286618963 BETTER
I0327 23:01:20.794737 2043080 finetune.py:68] layer 21_up @ epoch 1 new loss 0.00043284581624902785 old loss 0.0004377298173494637 BETTER
I0327 23:01:28.050325 2043010 finetune.py:68] layer 20_up @ epoch 2 new loss 0.0003638062335085124 old loss 0.00036725480458699167 BETTER
I0327 23:01:30.980437 2043150 finetune.py:45] layer 22_up initial loss 0.00044777573202736676
I0327 23:01:50.287127 2043220 finetune.py:68] layer 23_o @ epoch 4 new loss 0.00014102265413384885 old loss 0.00014192443632055074 BETTER
I0327 23:01:53.032982 2043080 finetune.py:68] layer 21_up @ epoch 2 new loss 0.00042872672202065587 old loss 0.00043284581624902785 BETTER
I0327 23:02:02.117449 2043150 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0004411631671246141 old loss 0.00044777573202736676 BETTER
I0327 23:02:02.170190 2043010 finetune.py:68] layer 20_up @ epoch 3 new loss 0.000360735080903396 old loss 0.0003638062335085124 BETTER
I0327 23:02:22.320042 2043220 finetune.py:45] layer 23_up initial loss 0.00047898662160150707
I0327 23:02:25.287290 2043080 finetune.py:68] layer 21_up @ epoch 3 new loss 0.0004251102509442717 old loss 0.00042872672202065587 BETTER
I0327 23:02:34.584357 2043150 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0004366666835267097 old loss 0.0004411631671246141 BETTER
I0327 23:02:36.146964 2043010 finetune.py:68] layer 20_up @ epoch 4 new loss 0.00035803375067189336 old loss 0.000360735080903396 BETTER
I0327 23:02:53.041816 2043220 finetune.py:68] layer 23_up @ epoch 0 new loss 0.00047275712131522596 old loss 0.00047898662160150707 BETTER
I0327 23:02:57.681282 2043080 finetune.py:68] layer 21_up @ epoch 4 new loss 0.00042186875361949205 old loss 0.0004251102509442717 BETTER
I0327 23:03:07.087121 2043150 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0004328880168031901 old loss 0.0004366666835267097 BETTER
I0327 23:03:08.825022 2043010 finetune.py:45] layer 20_gate initial loss 0.00046219717478379607
I0327 23:03:25.304264 2043220 finetune.py:68] layer 23_up @ epoch 1 new loss 0.00046839896822348237 old loss 0.00047275712131522596 BETTER
I0327 23:03:31.451551 2043080 finetune.py:45] layer 21_gate initial loss 0.0005413624458014965
I0327 23:03:40.491127 2043010 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.0004590272146742791 old loss 0.00046219717478379607 BETTER
I0327 23:03:41.345678 2043150 finetune.py:68] layer 22_up @ epoch 3 new loss 0.0004295907565392554 old loss 0.0004328880168031901 BETTER
I0327 23:03:56.920696 2043220 finetune.py:68] layer 23_up @ epoch 2 new loss 0.00046478683361783624 old loss 0.00046839896822348237 BETTER
I0327 23:04:00.424317 2043080 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0005376876797527075 old loss 0.0005413624458014965 BETTER
I0327 23:04:12.058998 2043010 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0004564154369290918 old loss 0.0004590272146742791 BETTER
I0327 23:04:13.815508 2043150 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0004266128526069224 old loss 0.0004295907565392554 BETTER
I0327 23:04:29.153461 2043220 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0004616480437107384 old loss 0.00046478683361783624 BETTER
I0327 23:04:30.660629 2043080 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0005346451653167605 old loss 0.0005376876797527075 BETTER
I0327 23:04:43.699024 2043010 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.0004540208028629422 old loss 0.0004564154369290918 BETTER
I0327 23:04:46.738109 2043150 finetune.py:45] layer 22_gate initial loss 0.0005556169198825955
I0327 23:05:00.787541 2043080 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0005318181938491762 old loss 0.0005346451653167605 BETTER
I0327 23:05:01.073096 2043220 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0004588477604556829 old loss 0.0004616480437107384 BETTER
I0327 23:05:15.501178 2043010 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0004518119676504284 old loss 0.0004540208028629422 BETTER
I0327 23:05:15.918924 2043150 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0005522079300135374 old loss 0.0005556169198825955 BETTER
I0327 23:05:30.581114 2043080 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.00052922178292647 old loss 0.0005318181938491762 BETTER
I0327 23:05:35.176749 2043220 finetune.py:45] layer 23_gate initial loss 0.0006029142532497644
I0327 23:05:46.130074 2043150 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.0005493495264090598 old loss 0.0005522079300135374 BETTER
I0327 23:05:47.320450 2043010 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.00044974632328376174 old loss 0.0004518119676504284 BETTER
I0327 23:06:01.739271 2043080 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.0005267640808597207 old loss 0.00052922178292647 BETTER
I0327 23:06:04.155758 2043220 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0005997549160383642 old loss 0.0006029142532497644 BETTER
I0327 23:06:16.133684 2043150 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0005467355367727578 old loss 0.0005493495264090598 BETTER
I0327 23:06:33.568642 2043220 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.0005971386563032866 old loss 0.0005997549160383642 BETTER
I0327 23:06:45.751619 2043010 finetune.py:45] layer 20_down initial loss 0.0007715022074989974
I0327 23:06:47.545134 2043150 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.0005443057743832469 old loss 0.0005467355367727578 BETTER
I0327 23:07:03.945332 2043220 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0005947129102423787 old loss 0.0005971386563032866 BETTER
I0327 23:07:04.528820 2043080 finetune.py:45] layer 21_down initial loss 0.0009029803331941366
I0327 23:07:13.757380 2043010 finetune.py:68] layer 20_down @ epoch 0 new loss 0.0007713654777035117 old loss 0.0007715022074989974 BETTER
I0327 23:07:18.327828 2043150 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0005420435918495059 old loss 0.0005443057743832469 BETTER
I0327 23:07:31.094759 2043080 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0009028132189996541 old loss 0.0009029803331941366 BETTER
I0327 23:07:34.288043 2043220 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.0005924644647166133 old loss 0.0005947129102423787 BETTER
I0327 23:07:42.786499 2043010 finetune.py:68] layer 20_down @ epoch 1 new loss 0.0007712418446317315 old loss 0.0007713654777035117 BETTER
I0327 23:07:58.696110 2043080 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0009026615880429745 old loss 0.0009028132189996541 BETTER
I0327 23:08:04.486336 2043220 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0005903978017158806 old loss 0.0005924644647166133 BETTER
I0327 23:08:12.277151 2043010 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0007711260113865137 old loss 0.0007712418446317315 BETTER
I0327 23:08:19.888065 2043150 finetune.py:45] layer 22_down initial loss 0.000939706398639828
I0327 23:08:26.717280 2043080 finetune.py:68] layer 21_down @ epoch 2 new loss 0.0009025202016346157 old loss 0.0009026615880429745 BETTER
I0327 23:08:41.868921 2043010 finetune.py:68] layer 20_down @ epoch 3 new loss 0.0007710189092904329 old loss 0.0007711260113865137 BETTER
I0327 23:08:46.736608 2043150 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0009395210072398186 old loss 0.000939706398639828 BETTER
I0327 23:08:55.684795 2043080 finetune.py:68] layer 21_down @ epoch 3 new loss 0.0009023866732604802 old loss 0.0009025202016346157 BETTER
I0327 23:09:06.118856 2043220 finetune.py:45] layer 23_down initial loss 0.0010123446118086576
I0327 23:09:11.939302 2043010 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0007709200726822019 old loss 0.0007710189092904329 BETTER
20_v proxy err 0.06299115717411041 tr(WHW.T) 330.192138671875
bpp_loss 1.9726499523385428
20_q proxy err 0.004631466697901487 tr(WHW.T) 20779.0
bpp_loss 2.6826722504920326
20_k proxy err 0.0017918241210281849 tr(WHW.T) 15387.1953125
bpp_loss 3.467163199558854
20_o proxy err 0.07010968774557114 tr(WHW.T) 1213.7655029296875
bpp_loss 2.0101858758716844
20_up proxy err 0.055520690977573395 tr(WHW.T) 7607.9853515625
bpp_loss 2.09791649854742
20_gate proxy err 0.015023760497570038 tr(WHW.T) 30416.62890625
bpp_loss 2.449319605582527
20_down proxy err 0.06121985614299774 tr(WHW.T) 6382.32080078125
bpp_loss 2.0939640729588325
I0327 23:09:15.117732 2043150 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0009393501095473766 old loss 0.0009395210072398186 BETTER
I0327 23:09:24.485275 2043080 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0009022603626362979 old loss 0.0009023866732604802 BETTER
21_v proxy err 0.05848994478583336 tr(WHW.T) 362.87310791015625
bpp_loss 1.998263080167817
21_q proxy err 0.003775784047320485 tr(WHW.T) 25880.515625
bpp_loss 2.683651924948208
21_k proxy err 0.0016799279255792499 tr(WHW.T) 16762.234375
bpp_loss 3.5034602199448273
21_o proxy err 0.04727917164564133 tr(WHW.T) 1279.374267578125
bpp_loss 2.0281168698274996
21_up proxy err 0.0533481128513813 tr(WHW.T) 7778.21533203125
bpp_loss 2.100552383444405
21_gate proxy err 0.014382537454366684 tr(WHW.T) 31286.5703125
bpp_loss 2.4597560197580606
21_down proxy err 0.05777770280838013 tr(WHW.T) 6426.2783203125
bpp_loss 2.094450415128709
I0327 23:09:32.686038 2043220 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0010121682425960898 old loss 0.0010123446118086576 BETTER
I0327 23:09:43.665557 2043150 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0009391861385665834 old loss 0.0009393501095473766 BETTER
I0327 23:09:59.839724 2043220 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0010120010701939464 old loss 0.0010121682425960898 BETTER
I0327 23:10:11.471370 2043150 finetune.py:68] layer 22_down @ epoch 3 new loss 0.000939035031478852 old loss 0.0009391861385665834 BETTER
I0327 23:10:26.992951 2043220 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0010118504287675023 old loss 0.0010120010701939464 BETTER
I0327 23:10:28.915866 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 59.57700157165527s
I0327 23:10:32.701857 2043290 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 23:10:32.701967 2043290 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 23:10:32.702009 2043290 utils.py:162] NumExpr defaulting to 16 threads.
I0327 23:10:33.054060 2043290 config.py:54] PyTorch version 2.6.0 available.
W0327 23:10:33.262860 2043290 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 23:10:33.884617 2043290 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 23:10:33.888568 2041475 quantize_finetune_llama.py:209] layer 25 gpu 1
I0327 23:10:33.903652 2043290 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 23:10:39.269961 2043150 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0009388970793224871 old loss 0.000939035031478852 BETTER
22_v proxy err 0.05852571129798889 tr(WHW.T) 346.0789489746094
bpp_loss 2.0469409323704895
22_q proxy err 0.0044892351143062115 tr(WHW.T) 20366.39453125
bpp_loss 2.6474959974875674
22_k proxy err 0.0017852947348728776 tr(WHW.T) 14709.7197265625
bpp_loss 3.447463667835109
22_o proxy err 0.06623739749193192 tr(WHW.T) 1227.4532470703125
bpp_loss 2.0551002989232074
22_up proxy err 0.055616676807403564 tr(WHW.T) 7551.5986328125
bpp_loss 2.104662663070485
22_gate proxy err 0.015526468865573406 tr(WHW.T) 29326.474609375
bpp_loss 2.46429722616449
22_down proxy err 0.05797411501407623 tr(WHW.T) 6608.74560546875
bpp_loss 2.1004367833291844
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 23:10:50.824519 2043290 finetune.py:45] layer 24_v initial loss 0.0002623475738801062
W0327 23:10:50.824790 2043290 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 23:10:54.145107 2043220 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0010117089841514826 old loss 0.0010118504287675023 BETTER
I0327 23:11:21.437031 2043220 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0010115751065313816 old loss 0.0010117089841514826 BETTER
23_v proxy err 0.05547742173075676 tr(WHW.T) 397.90704345703125
bpp_loss 2.0959126909729093
23_q proxy err 0.004352016840130091 tr(WHW.T) 22652.107421875
bpp_loss 2.65842976141721
23_k proxy err 0.0019070629496127367 tr(WHW.T) 14848.857421875
bpp_loss 3.4504245062125847
23_o proxy err 0.056220412254333496 tr(WHW.T) 1746.6510009765625
bpp_loss 2.078015572100412
23_up proxy err 0.05658089369535446 tr(WHW.T) 7426.60205078125
bpp_loss 2.108227317986478
23_gate proxy err 0.016814475879073143 tr(WHW.T) 27079.42578125
bpp_loss 2.465974924753287
23_down proxy err 0.058051999658346176 tr(WHW.T) 6744.2216796875
bpp_loss 2.105048008105119
I0327 23:11:26.094601 2043290 finetune.py:68] layer 24_v @ epoch 0 new loss 7.128229481168091e-05 old loss 0.0002623475738801062 BETTER
I0327 23:11:44.611009 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 61.02744150161743s
I0327 23:11:48.160302 2043360 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 23:11:48.160407 2043360 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 23:11:48.160446 2043360 utils.py:162] NumExpr defaulting to 16 threads.
I0327 23:11:48.481727 2043360 config.py:54] PyTorch version 2.6.0 available.
W0327 23:11:48.689386 2043360 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 23:11:49.282203 2043360 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 23:11:49.285708 2041475 quantize_finetune_llama.py:209] layer 26 gpu 2
I0327 23:11:49.298818 2043360 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 23:12:02.516655 2043290 finetune.py:68] layer 24_v @ epoch 1 new loss 6.108620436862111e-05 old loss 7.128229481168091e-05 BETTER
I0327 23:12:05.981474 2043360 finetune.py:45] layer 25_v initial loss 0.00029674911638721824
W0327 23:12:05.981664 2043360 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 23:12:39.239357 2043360 finetune.py:68] layer 25_v @ epoch 0 new loss 8.414324111072347e-05 old loss 0.00029674911638721824 BETTER
I0327 23:12:39.349246 2043290 finetune.py:68] layer 24_v @ epoch 2 new loss 5.768311893916689e-05 old loss 6.108620436862111e-05 BETTER
I0327 23:12:51.878326 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 62.160587549209595s
I0327 23:12:55.644662 2043430 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 23:12:55.644753 2043430 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 23:12:55.644791 2043430 utils.py:162] NumExpr defaulting to 16 threads.
I0327 23:12:56.021257 2043430 config.py:54] PyTorch version 2.6.0 available.
W0327 23:12:56.223755 2043430 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 23:12:56.826216 2043430 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 23:12:56.830096 2041475 quantize_finetune_llama.py:209] layer 27 gpu 3
I0327 23:12:56.845029 2043430 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 23:13:13.527971 2043360 finetune.py:68] layer 25_v @ epoch 1 new loss 7.250809721881524e-05 old loss 8.414324111072347e-05 BETTER
I0327 23:13:13.669293 2043430 finetune.py:45] layer 26_v initial loss 0.000269824085989967
W0327 23:13:13.669526 2043430 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 23:13:16.211463 2043290 finetune.py:68] layer 24_v @ epoch 3 new loss 5.5968637752812356e-05 old loss 5.768311893916689e-05 BETTER
I0327 23:13:47.382699 2043430 finetune.py:68] layer 26_v @ epoch 0 new loss 0.00011521932174218819 old loss 0.000269824085989967 BETTER
I0327 23:13:48.290428 2043360 finetune.py:68] layer 25_v @ epoch 2 new loss 6.864700117148459e-05 old loss 7.250809721881524e-05 BETTER
I0327 23:13:53.163450 2043290 finetune.py:68] layer 24_v @ epoch 4 new loss 5.476238948176615e-05 old loss 5.5968637752812356e-05 BETTER
I0327 23:13:58.916427 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 61.628103256225586s
I0327 23:14:02.693753 2043500 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 23:14:02.693844 2043500 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 23:14:02.693883 2043500 utils.py:162] NumExpr defaulting to 16 threads.
I0327 23:14:03.048051 2043500 config.py:54] PyTorch version 2.6.0 available.
W0327 23:14:03.236922 2043500 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 23:14:03.829146 2043500 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 23:14:03.833137 2041475 quantize_finetune_llama.py:209] layer 28 gpu 0
I0327 23:14:03.849983 2043500 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 23:14:12.023191 2043290 finetune.py:45] layer 24_q initial loss 6.925493653398007e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 23:14:21.346807 2043500 finetune.py:45] layer 27_v initial loss 0.000275818892987445
W0327 23:14:21.347022 2043500 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 23:14:22.212615 2043430 finetune.py:68] layer 26_v @ epoch 1 new loss 0.00010535169712966308 old loss 0.00011521932174218819 BETTER
I0327 23:14:23.284577 2043360 finetune.py:68] layer 25_v @ epoch 3 new loss 6.671438313787803e-05 old loss 6.864700117148459e-05 BETTER
I0327 23:14:47.318688 2043290 finetune.py:68] layer 24_q @ epoch 0 new loss 6.707308784825727e-05 old loss 6.925493653398007e-05 BETTER
I0327 23:14:54.593073 2043500 finetune.py:68] layer 27_v @ epoch 0 new loss 0.00010573483450571075 old loss 0.000275818892987445 BETTER
I0327 23:14:57.470046 2043430 finetune.py:68] layer 26_v @ epoch 2 new loss 0.00010134223703062162 old loss 0.00010535169712966308 BETTER
I0327 23:14:58.433476 2043360 finetune.py:68] layer 25_v @ epoch 4 new loss 6.528809171868488e-05 old loss 6.671438313787803e-05 BETTER
I0327 23:15:17.850888 2043360 finetune.py:45] layer 25_q initial loss 9.231455624103546e-05
I0327 23:15:23.972693 2043290 finetune.py:68] layer 24_q @ epoch 1 new loss 6.594835576834157e-05 old loss 6.707308784825727e-05 BETTER
I0327 23:15:29.234870 2043500 finetune.py:68] layer 27_v @ epoch 1 new loss 9.785700967768207e-05 old loss 0.00010573483450571075 BETTER
I0327 23:15:32.841467 2043430 finetune.py:68] layer 26_v @ epoch 3 new loss 9.886437328532338e-05 old loss 0.00010134223703062162 BETTER
I0327 23:15:51.408757 2043360 finetune.py:68] layer 25_q @ epoch 0 new loss 8.867462020134553e-05 old loss 9.231455624103546e-05 BETTER
I0327 23:16:00.915451 2043290 finetune.py:68] layer 24_q @ epoch 2 new loss 6.506501813419163e-05 old loss 6.594835576834157e-05 BETTER
I0327 23:16:03.874075 2043500 finetune.py:68] layer 27_v @ epoch 2 new loss 9.461043373448774e-05 old loss 9.785700967768207e-05 BETTER
I0327 23:16:08.188002 2043430 finetune.py:68] layer 26_v @ epoch 4 new loss 9.697506175143644e-05 old loss 9.886437328532338e-05 BETTER
I0327 23:16:26.110527 2043360 finetune.py:68] layer 25_q @ epoch 1 new loss 8.655151032144204e-05 old loss 8.867462020134553e-05 BETTER
I0327 23:16:27.384807 2043430 finetune.py:45] layer 26_q initial loss 0.00011907803127542138
I0327 23:16:38.124055 2043290 finetune.py:68] layer 24_q @ epoch 3 new loss 6.442813173634931e-05 old loss 6.506501813419163e-05 BETTER
I0327 23:16:38.730366 2043500 finetune.py:68] layer 27_v @ epoch 3 new loss 9.266539564123377e-05 old loss 9.461043373448774e-05 BETTER
I0327 23:17:00.880945 2043360 finetune.py:68] layer 25_q @ epoch 2 new loss 8.502398122800514e-05 old loss 8.655151032144204e-05 BETTER
I0327 23:17:01.204943 2043430 finetune.py:68] layer 26_q @ epoch 0 new loss 0.00011585890024434775 old loss 0.00011907803127542138 BETTER
I0327 23:17:13.514181 2043500 finetune.py:68] layer 27_v @ epoch 4 new loss 9.111013059737161e-05 old loss 9.266539564123377e-05 BETTER
I0327 23:17:15.267424 2043290 finetune.py:68] layer 24_q @ epoch 4 new loss 6.381317507475615e-05 old loss 6.442813173634931e-05 BETTER
I0327 23:17:33.581187 2043290 finetune.py:45] layer 24_k initial loss 7.132536848075688e-05
I0327 23:17:33.898706 2043500 finetune.py:45] layer 27_q initial loss 0.00012567905650939792
I0327 23:17:36.158280 2043360 finetune.py:68] layer 25_q @ epoch 3 new loss 8.374901517527178e-05 old loss 8.502398122800514e-05 BETTER
I0327 23:17:36.775083 2043430 finetune.py:68] layer 26_q @ epoch 1 new loss 0.00011388835991965607 old loss 0.00011585890024434775 BETTER
I0327 23:18:07.499320 2043500 finetune.py:68] layer 27_q @ epoch 0 new loss 0.00012204508675495163 old loss 0.00012567905650939792 BETTER
I0327 23:18:08.939897 2043290 finetune.py:68] layer 24_k @ epoch 0 new loss 7.071771688060835e-05 old loss 7.132536848075688e-05 BETTER
I0327 23:18:11.049220 2043360 finetune.py:68] layer 25_q @ epoch 4 new loss 8.27158655738458e-05 old loss 8.374901517527178e-05 BETTER
I0327 23:18:11.726168 2043430 finetune.py:68] layer 26_q @ epoch 2 new loss 0.00011232314864173532 old loss 0.00011388835991965607 BETTER
I0327 23:18:29.309359 2043360 finetune.py:45] layer 25_k initial loss 9.57301672315225e-05
I0327 23:18:42.164969 2043500 finetune.py:68] layer 27_q @ epoch 1 new loss 0.00012015938409604132 old loss 0.00012204508675495163 BETTER
I0327 23:18:45.394022 2043290 finetune.py:68] layer 24_k @ epoch 1 new loss 7.024272053968161e-05 old loss 7.071771688060835e-05 BETTER
I0327 23:18:46.762105 2043430 finetune.py:68] layer 26_q @ epoch 3 new loss 0.0001109838267439045 old loss 0.00011232314864173532 BETTER
I0327 23:19:02.862075 2043360 finetune.py:68] layer 25_k @ epoch 0 new loss 9.443329327041283e-05 old loss 9.57301672315225e-05 BETTER
I0327 23:19:16.736527 2043500 finetune.py:68] layer 27_q @ epoch 2 new loss 0.00011875276686623693 old loss 0.00012015938409604132 BETTER
I0327 23:19:21.705443 2043430 finetune.py:68] layer 26_q @ epoch 4 new loss 0.00010990506416419521 old loss 0.0001109838267439045 BETTER
I0327 23:19:22.152342 2043290 finetune.py:68] layer 24_k @ epoch 2 new loss 6.988101085880771e-05 old loss 7.024272053968161e-05 BETTER
I0327 23:19:37.432347 2043360 finetune.py:68] layer 25_k @ epoch 1 new loss 9.361925185658038e-05 old loss 9.443329327041283e-05 BETTER
I0327 23:19:39.586446 2043430 finetune.py:45] layer 26_k initial loss 0.00011934898793697357
I0327 23:19:51.057922 2043500 finetune.py:68] layer 27_q @ epoch 3 new loss 0.00011757126776501536 old loss 0.00011875276686623693 BETTER
I0327 23:19:58.903559 2043290 finetune.py:68] layer 24_k @ epoch 3 new loss 6.952549301786348e-05 old loss 6.988101085880771e-05 BETTER
I0327 23:20:12.030230 2043360 finetune.py:68] layer 25_k @ epoch 2 new loss 9.291985770687461e-05 old loss 9.361925185658038e-05 BETTER
I0327 23:20:13.393095 2043430 finetune.py:68] layer 26_k @ epoch 0 new loss 0.00011803188681369647 old loss 0.00011934898793697357 BETTER
I0327 23:20:25.469496 2043500 finetune.py:68] layer 27_q @ epoch 4 new loss 0.0001166046058642678 old loss 0.00011757126776501536 BETTER
I0327 23:20:35.770932 2043290 finetune.py:68] layer 24_k @ epoch 4 new loss 6.925357592990622e-05 old loss 6.952549301786348e-05 BETTER
I0327 23:20:43.472553 2043500 finetune.py:45] layer 27_k initial loss 0.00013432599371299148
I0327 23:20:46.779927 2043360 finetune.py:68] layer 25_k @ epoch 3 new loss 9.23186307772994e-05 old loss 9.291985770687461e-05 BETTER
I0327 23:20:48.075618 2043430 finetune.py:68] layer 26_k @ epoch 1 new loss 0.00011713144340319559 old loss 0.00011803188681369647 BETTER
I0327 23:20:55.225367 2043290 finetune.py:45] layer 24_o initial loss 0.0001759795268299058
I0327 23:21:16.703895 2043500 finetune.py:68] layer 27_k @ epoch 0 new loss 0.00013263772416394204 old loss 0.00013432599371299148 BETTER
I0327 23:21:21.566198 2043360 finetune.py:68] layer 25_k @ epoch 4 new loss 9.181843051919714e-05 old loss 9.23186307772994e-05 BETTER
I0327 23:21:23.044448 2043430 finetune.py:68] layer 26_k @ epoch 2 new loss 0.00011628477659542114 old loss 0.00011713144340319559 BETTER
I0327 23:21:29.937377 2043290 finetune.py:68] layer 24_o @ epoch 0 new loss 0.00016238477837760001 old loss 0.0001759795268299058 BETTER
I0327 23:21:41.437285 2043360 finetune.py:45] layer 25_o initial loss 0.0002147894847439602
I0327 23:21:51.392871 2043500 finetune.py:68] layer 27_k @ epoch 1 new loss 0.00013170753663871437 old loss 0.00013263772416394204 BETTER
I0327 23:21:57.913596 2043430 finetune.py:68] layer 26_k @ epoch 3 new loss 0.0001156263897428289 old loss 0.00011628477659542114 BETTER
I0327 23:22:06.072853 2043290 finetune.py:68] layer 24_o @ epoch 1 new loss 0.00016042169590946287 old loss 0.00016238477837760001 BETTER
I0327 23:22:14.505706 2043360 finetune.py:68] layer 25_o @ epoch 0 new loss 0.00019699330732692033 old loss 0.0002147894847439602 BETTER
I0327 23:22:25.532951 2043500 finetune.py:68] layer 27_k @ epoch 2 new loss 0.0001309961371589452 old loss 0.00013170753663871437 BETTER
I0327 23:22:32.990609 2043430 finetune.py:68] layer 26_k @ epoch 4 new loss 0.00011499937681946903 old loss 0.0001156263897428289 BETTER
I0327 23:22:42.464394 2043290 finetune.py:68] layer 24_o @ epoch 2 new loss 0.00015920538862701505 old loss 0.00016042169590946287 BETTER
I0327 23:22:48.477697 2043360 finetune.py:68] layer 25_o @ epoch 1 new loss 0.00019424337369855493 old loss 0.00019699330732692033 BETTER
I0327 23:22:54.359970 2043430 finetune.py:45] layer 26_o initial loss 0.00027747932472266257
I0327 23:23:00.192113 2043500 finetune.py:68] layer 27_k @ epoch 3 new loss 0.00013039055920671672 old loss 0.0001309961371589452 BETTER
I0327 23:23:18.785745 2043290 finetune.py:68] layer 24_o @ epoch 3 new loss 0.00015831427299417555 old loss 0.00015920538862701505 BETTER
I0327 23:23:22.326661 2043360 finetune.py:68] layer 25_o @ epoch 2 new loss 0.0001925136602949351 old loss 0.00019424337369855493 BETTER
I0327 23:23:27.536203 2043430 finetune.py:68] layer 26_o @ epoch 0 new loss 0.00026055198395624757 old loss 0.00027747932472266257 BETTER
I0327 23:23:34.451522 2043500 finetune.py:68] layer 27_k @ epoch 4 new loss 0.00012988837261218578 old loss 0.00013039055920671672 BETTER
I0327 23:23:56.309653 2043500 finetune.py:45] layer 27_o initial loss 0.00032501635723747313
I0327 23:23:57.473873 2043290 finetune.py:68] layer 24_o @ epoch 4 new loss 0.00015759660163894296 old loss 0.00015831427299417555 BETTER
I0327 23:23:58.507706 2043360 finetune.py:68] layer 25_o @ epoch 3 new loss 0.00019121458171866834 old loss 0.0001925136602949351 BETTER
I0327 23:24:02.061436 2043430 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0002573216916061938 old loss 0.00026055198395624757 BETTER
I0327 23:24:30.260041 2043500 finetune.py:68] layer 27_o @ epoch 0 new loss 0.00030636825249530375 old loss 0.00032501635723747313 BETTER
I0327 23:24:33.204467 2043290 finetune.py:45] layer 24_up initial loss 0.0005228290683589876
I0327 23:24:33.679519 2043360 finetune.py:68] layer 25_o @ epoch 4 new loss 0.00019016009173355997 old loss 0.00019121458171866834 BETTER
I0327 23:24:36.775007 2043430 finetune.py:68] layer 26_o @ epoch 2 new loss 0.00025506599922664464 old loss 0.0002573216916061938 BETTER
I0327 23:25:04.657998 2043500 finetune.py:68] layer 27_o @ epoch 1 new loss 0.00030273330048657954 old loss 0.00030636825249530375 BETTER
I0327 23:25:05.867949 2043290 finetune.py:68] layer 24_up @ epoch 0 new loss 0.0005167780327610672 old loss 0.0005228290683589876 BETTER
I0327 23:25:07.970958 2043360 finetune.py:45] layer 25_up initial loss 0.0006003062590025365
I0327 23:25:11.442673 2043430 finetune.py:68] layer 26_o @ epoch 3 new loss 0.00025327023467980325 old loss 0.00025506599922664464 BETTER
I0327 23:25:38.735435 2043500 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0003002148005180061 old loss 0.00030273330048657954 BETTER
I0327 23:25:38.917483 2043360 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0005934376968070865 old loss 0.0006003062590025365 BETTER
I0327 23:25:39.766729 2043290 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0005127046024426818 old loss 0.0005167780327610672 BETTER
I0327 23:25:46.461307 2043430 finetune.py:68] layer 26_o @ epoch 4 new loss 0.0002518010442145169 old loss 0.00025327023467980325 BETTER
I0327 23:26:12.474439 2043360 finetune.py:68] layer 25_up @ epoch 1 new loss 0.000588847731705755 old loss 0.0005934376968070865 BETTER
I0327 23:26:13.393429 2043500 finetune.py:68] layer 27_o @ epoch 3 new loss 0.00029808582621626556 old loss 0.0003002148005180061 BETTER
I0327 23:26:14.797764 2043290 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0005093571380712092 old loss 0.0005127046024426818 BETTER
I0327 23:26:21.995279 2043430 finetune.py:45] layer 26_up initial loss 0.0007319631986320019
I0327 23:26:45.486401 2043360 finetune.py:68] layer 25_up @ epoch 2 new loss 0.0005851149326190352 old loss 0.000588847731705755 BETTER
I0327 23:26:48.280044 2043500 finetune.py:68] layer 27_o @ epoch 4 new loss 0.00029642361914739013 old loss 0.00029808582621626556 BETTER
I0327 23:26:49.510423 2043290 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0005064050783403218 old loss 0.0005093571380712092 BETTER
I0327 23:26:53.329126 2043430 finetune.py:68] layer 26_up @ epoch 0 new loss 0.000723948935046792 old loss 0.0007319631986320019 BETTER
I0327 23:27:18.364468 2043360 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0005818451754748821 old loss 0.0005851149326190352 BETTER
I0327 23:27:23.644808 2043500 finetune.py:45] layer 27_up initial loss 0.0008582228911109269
I0327 23:27:24.501264 2043290 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0005037928931415081 old loss 0.0005064050783403218 BETTER
I0327 23:27:26.242518 2043430 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0007184304995462298 old loss 0.000723948935046792 BETTER
I0327 23:27:51.828914 2043360 finetune.py:68] layer 25_up @ epoch 4 new loss 0.0005789318238385022 old loss 0.0005818451754748821 BETTER
I0327 23:27:54.790938 2043500 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0008482467965222895 old loss 0.0008582228911109269 BETTER
I0327 23:27:59.108974 2043430 finetune.py:68] layer 26_up @ epoch 2 new loss 0.0007138904766179621 old loss 0.0007184304995462298 BETTER
I0327 23:27:59.604454 2043290 finetune.py:45] layer 24_gate initial loss 0.0006665698019787669
I0327 23:28:27.637373 2043500 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0008417125791311264 old loss 0.0008482467965222895 BETTER
I0327 23:28:29.050943 2043360 finetune.py:45] layer 25_gate initial loss 0.0007654217770323157
I0327 23:28:31.203786 2043290 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0006636092439293861 old loss 0.0006665698019787669 BETTER
I0327 23:28:32.768561 2043430 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0007098802016116679 old loss 0.0007138904766179621 BETTER
I0327 23:28:57.935552 2043360 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.0007621892727911472 old loss 0.0007654217770323157 BETTER
I0327 23:29:00.086799 2043500 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0008363763918168843 old loss 0.0008417125791311264 BETTER
I0327 23:29:04.149694 2043290 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0006610752898268402 old loss 0.0006636092439293861 BETTER
I0327 23:29:06.139081 2043430 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0007063364610075951 old loss 0.0007098802016116679 BETTER
I0327 23:29:27.839255 2043360 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.000759453687351197 old loss 0.0007621892727911472 BETTER
I0327 23:29:32.461519 2043500 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0008317329338751733 old loss 0.0008363763918168843 BETTER
I0327 23:29:36.164599 2043290 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.000658840814139694 old loss 0.0006610752898268402 BETTER
I0327 23:29:42.406198 2043430 finetune.py:45] layer 26_gate initial loss 0.0009269929141737521
I0327 23:29:58.578991 2043360 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.0007569954032078385 old loss 0.000759453687351197 BETTER
I0327 23:30:04.964252 2043500 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0008274827850982547 old loss 0.0008317329338751733 BETTER
I0327 23:30:08.315195 2043290 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0006566966767422855 old loss 0.000658840814139694 BETTER
I0327 23:30:11.506328 2043430 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0009231118019670248 old loss 0.0009269929141737521 BETTER
I0327 23:30:29.093228 2043360 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.000754720123950392 old loss 0.0007569954032078385 BETTER
I0327 23:30:41.217528 2043500 finetune.py:45] layer 27_gate initial loss 0.0011011718306690454
I0327 23:30:41.633386 2043290 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.0006547566736117005 old loss 0.0006566966767422855 BETTER
I0327 23:30:42.867329 2043430 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.000919899670407176 old loss 0.0009231118019670248 BETTER
I0327 23:30:59.397884 2043360 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0007526101544499397 old loss 0.000754720123950392 BETTER
I0327 23:31:09.286633 2043500 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0010963703971356153 old loss 0.0011011718306690454 BETTER
I0327 23:31:13.318948 2043430 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0009170100092887878 old loss 0.000919899670407176 BETTER
I0327 23:31:39.023404 2043500 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.0010924518574029207 old loss 0.0010963703971356153 BETTER
I0327 23:31:41.268728 2043290 finetune.py:45] layer 24_down initial loss 0.0010971492156386375
I0327 23:31:44.099509 2043430 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0009142616181634367 old loss 0.0009170100092887878 BETTER
I0327 23:32:00.713708 2043360 finetune.py:45] layer 25_down initial loss 0.0012291291495785117
I0327 23:32:09.740910 2043290 finetune.py:68] layer 24_down @ epoch 0 new loss 0.001096988096833229 old loss 0.0010971492156386375 BETTER
I0327 23:32:09.743439 2043500 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.0010889120167121291 old loss 0.0010924518574029207 BETTER
I0327 23:32:15.114994 2043430 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0009117896552197635 old loss 0.0009142616181634367 BETTER
I0327 23:32:27.267132 2043360 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0012289879377931356 old loss 0.0012291291495785117 BETTER
I0327 23:32:39.997794 2043290 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0010968385031446815 old loss 0.001096988096833229 BETTER
I0327 23:32:40.831338 2043500 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0010856539011001587 old loss 0.0010889120167121291 BETTER
I0327 23:32:54.979005 2043360 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0012288519646972418 old loss 0.0012289879377931356 BETTER
I0327 23:33:10.265487 2043290 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0010967005509883165 old loss 0.0010968385031446815 BETTER
I0327 23:33:11.693986 2043500 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.0010826560901477933 old loss 0.0010856539011001587 BETTER
I0327 23:33:17.625558 2043430 finetune.py:45] layer 26_down initial loss 0.0014598005218431354
I0327 23:33:23.157304 2043360 finetune.py:68] layer 25_down @ epoch 2 new loss 0.001228725421242416 old loss 0.0012288519646972418 BETTER
I0327 23:33:39.923478 2043290 finetune.py:68] layer 24_down @ epoch 3 new loss 0.001096570398658514 old loss 0.0010967005509883165 BETTER
I0327 23:33:44.261204 2043430 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0014596249675378203 old loss 0.0014598005218431354 BETTER
I0327 23:33:51.864050 2043360 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0012286092387512326 old loss 0.001228725421242416 BETTER
I0327 23:34:09.733477 2043290 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0010964535176753998 old loss 0.001096570398658514 BETTER
I0327 23:34:12.669742 2043430 finetune.py:68] layer 26_down @ epoch 1 new loss 0.001459464430809021 old loss 0.0014596249675378203 BETTER
24_v proxy err 0.046822674572467804 tr(WHW.T) 467.2783508300781
bpp_loss 2.18604642196442
24_q proxy err 0.004239802714437246 tr(WHW.T) 22489.564453125
bpp_loss 2.6282740585156716
24_k proxy err 0.0018941815942525864 tr(WHW.T) 14176.9169921875
bpp_loss 3.3020106917247176
24_o proxy err 0.05512185022234917 tr(WHW.T) 1599.4462890625
bpp_loss 2.117421621311223
24_up proxy err 0.05812004953622818 tr(WHW.T) 7311.74658203125
bpp_loss 2.1120959296489934
24_gate proxy err 0.01791580580174923 tr(WHW.T) 25721.349609375
bpp_loss 2.4707132933129157
24_down proxy err 0.05760100856423378 tr(WHW.T) 6819.56884765625
bpp_loss 2.1094061712335264
I0327 23:34:14.645897 2043500 finetune.py:45] layer 27_down initial loss 0.0017235138220712543
I0327 23:34:20.951802 2043360 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0012284971307963133 old loss 0.0012286092387512326 BETTER
25_v proxy err 0.04040604829788208 tr(WHW.T) 557.8086547851562
bpp_loss 2.1988578415766824
25_q proxy err 0.003725675633177161 tr(WHW.T) 26173.96484375
bpp_loss 2.6113457356696017
25_k proxy err 0.0019083095248788595 tr(WHW.T) 14408.0673828125
bpp_loss 3.2823276472627185
25_o proxy err 0.046280637383461 tr(WHW.T) 1994.973388671875
bpp_loss 2.1157995216490235
25_up proxy err 0.05752328410744667 tr(WHW.T) 7392.43017578125
bpp_loss 2.1205513391510715
25_gate proxy err 0.017622938379645348 tr(WHW.T) 26156.763671875
bpp_loss 2.4782067739537785
25_down proxy err 0.057341646403074265 tr(WHW.T) 6695.52197265625
bpp_loss 2.1174167987441512
I0327 23:34:41.279525 2043430 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0014593147207051516 old loss 0.001459464430809021 BETTER
I0327 23:34:41.335791 2043500 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0017233239486813545 old loss 0.0017235138220712543 BETTER
I0327 23:35:08.394388 2043500 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0017231538658961654 old loss 0.0017233239486813545 BETTER
I0327 23:35:09.000679 2043430 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0014591736253350973 old loss 0.0014593147207051516 BETTER
I0327 23:35:31.519207 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 65.61946988105774s
I0327 23:35:35.548990 2043500 finetune.py:68] layer 27_down @ epoch 2 new loss 0.001722995424643159 old loss 0.0017231538658961654 BETTER
I0327 23:35:35.670855 2043570 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 23:35:35.670955 2043570 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 23:35:35.670995 2043570 utils.py:162] NumExpr defaulting to 16 threads.
I0327 23:35:36.061074 2043570 config.py:54] PyTorch version 2.6.0 available.
W0327 23:35:36.272160 2043570 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 23:35:36.920339 2043570 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 23:35:36.924324 2041475 quantize_finetune_llama.py:209] layer 29 gpu 1
I0327 23:35:36.941230 2043570 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 23:35:37.107123 2043430 finetune.py:68] layer 26_down @ epoch 4 new loss 0.0014590484788641334 old loss 0.0014591736253350973 BETTER
26_v proxy err 0.04967596381902695 tr(WHW.T) 434.54583740234375
bpp_loss 2.24074423770071
26_q proxy err 0.004314655438065529 tr(WHW.T) 21428.9609375
bpp_loss 2.6203189757070504
26_k proxy err 0.0017097099917009473 tr(WHW.T) 15408.759765625
bpp_loss 3.3558078939677216
26_o proxy err 0.033510722219944 tr(WHW.T) 2391.775634765625
bpp_loss 2.1339941407204606
26_up proxy err 0.05559805408120155 tr(WHW.T) 7661.7265625
bpp_loss 2.1295814342198094
26_gate proxy err 0.016021428629755974 tr(WHW.T) 28810.353515625
bpp_loss 2.482707888919062
26_down proxy err 0.05745677277445793 tr(WHW.T) 6695.96923828125
bpp_loss 2.124610807122995
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 23:35:54.036586 2043570 finetune.py:45] layer 28_v initial loss 0.00035962951369583607
W0327 23:35:54.036865 2043570 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 23:36:02.982581 2043500 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0017228471115231514 old loss 0.001722995424643159 BETTER
I0327 23:36:29.329648 2043570 finetune.py:68] layer 28_v @ epoch 0 new loss 0.00014867661229800433 old loss 0.00035962951369583607 BETTER
I0327 23:36:30.454684 2043500 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0017227069474756718 old loss 0.0017228471115231514 BETTER
27_v proxy err 0.034560173749923706 tr(WHW.T) 677.69384765625
bpp_loss 2.3256026072776876
27_q proxy err 0.0045987446792423725 tr(WHW.T) 21336.61328125
bpp_loss 2.591570874705212
27_k proxy err 0.0019928652327507734 tr(WHW.T) 14007.4306640625
bpp_loss 3.3138012186973356
27_o proxy err 0.04002055898308754 tr(WHW.T) 2168.962158203125
bpp_loss 2.1707173867034726
27_up proxy err 0.050790537148714066 tr(WHW.T) 8483.0380859375
bpp_loss 2.144100204309715
27_gate proxy err 0.014278020709753036 tr(WHW.T) 32658.076171875
bpp_loss 2.4914747885361845
27_down proxy err 0.04779297485947609 tr(WHW.T) 6620.2216796875
bpp_loss 2.1350023896250474
I0327 23:36:43.005713 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 61.59890127182007s
I0327 23:36:46.535719 2043640 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 23:36:46.535835 2043640 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 23:36:46.535876 2043640 utils.py:162] NumExpr defaulting to 16 threads.
I0327 23:36:46.867282 2043640 config.py:54] PyTorch version 2.6.0 available.
W0327 23:36:47.055989 2043640 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 23:36:47.614416 2043640 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 23:36:47.617905 2041475 quantize_finetune_llama.py:209] layer 30 gpu 2
I0327 23:36:47.630697 2043640 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 23:37:04.174032 2043640 finetune.py:45] layer 29_v initial loss 0.00045633703120984137
W0327 23:37:04.174282 2043640 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 23:37:06.242608 2043570 finetune.py:68] layer 28_v @ epoch 1 new loss 0.00014052317419555038 old loss 0.00014867661229800433 BETTER
I0327 23:37:37.573629 2043640 finetune.py:68] layer 29_v @ epoch 0 new loss 0.00017347399261780083 old loss 0.00045633703120984137 BETTER
I0327 23:37:43.167690 2043570 finetune.py:68] layer 28_v @ epoch 2 new loss 0.00013642052363138646 old loss 0.00014052317419555038 BETTER
I0327 23:37:49.839519 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 61.78098464012146s
I0327 23:37:53.586940 2043710 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 23:37:53.587039 2043710 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 23:37:53.587078 2043710 utils.py:162] NumExpr defaulting to 16 threads.
I0327 23:37:53.940284 2043710 config.py:54] PyTorch version 2.6.0 available.
W0327 23:37:54.129461 2043710 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 23:37:54.707580 2043710 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 23:37:54.711735 2041475 quantize_finetune_llama.py:209] layer 31 gpu 3
I0327 23:37:54.730496 2043710 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 23:38:11.341696 2043710 finetune.py:45] layer 30_v initial loss 0.0005302850040607154
W0327 23:38:11.342040 2043710 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 23:38:12.012588 2043640 finetune.py:68] layer 29_v @ epoch 1 new loss 0.0001644128788029775 old loss 0.00017347399261780083 BETTER
I0327 23:38:20.163071 2043570 finetune.py:68] layer 28_v @ epoch 3 new loss 0.00013357972784433514 old loss 0.00013642052363138646 BETTER
I0327 23:38:45.004102 2043710 finetune.py:68] layer 30_v @ epoch 0 new loss 0.0002952931681647897 old loss 0.0005302850040607154 BETTER
I0327 23:38:46.875237 2043640 finetune.py:68] layer 29_v @ epoch 2 new loss 0.00015971048560459167 old loss 0.0001644128788029775 BETTER
I0327 23:38:51.178896 2041475 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 56.002326250076294s
I0327 23:38:55.093583 2043780 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0327 23:38:55.093698 2043780 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0327 23:38:55.093745 2043780 utils.py:162] NumExpr defaulting to 16 threads.
I0327 23:38:55.448030 2043780 config.py:54] PyTorch version 2.6.0 available.
W0327 23:38:55.670086 2043780 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0327 23:38:56.303482 2043780 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0327 23:38:56.320329 2043780 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0327 23:38:57.225253 2043570 finetune.py:68] layer 28_v @ epoch 4 new loss 0.0001316330599365756 old loss 0.00013357972784433514 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0327 23:39:15.163056 2043780 finetune.py:45] layer 31_v initial loss 0.0008191267843358219
W0327 23:39:15.163310 2043780 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0327 23:39:18.295561 2043570 finetune.py:45] layer 28_q initial loss 0.00017174688400700688
I0327 23:39:20.802591 2043710 finetune.py:68] layer 30_v @ epoch 1 new loss 0.00028217246290296316 old loss 0.0002952931681647897 BETTER
I0327 23:39:22.722906 2043640 finetune.py:68] layer 29_v @ epoch 3 new loss 0.00015712107415311038 old loss 0.00015971048560459167 BETTER
I0327 23:39:48.863387 2043780 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0005064958240836859 old loss 0.0008191267843358219 BETTER
I0327 23:39:53.651970 2043570 finetune.py:68] layer 28_q @ epoch 0 new loss 0.00016710841737221926 old loss 0.00017174688400700688 BETTER
I0327 23:39:56.476192 2043710 finetune.py:68] layer 30_v @ epoch 2 new loss 0.000274774240097031 old loss 0.00028217246290296316 BETTER
I0327 23:39:58.005818 2043640 finetune.py:68] layer 29_v @ epoch 4 new loss 0.0001548073487356305 old loss 0.00015712107415311038 BETTER
I0327 23:40:18.002954 2043640 finetune.py:45] layer 29_q initial loss 0.0002615375560708344
I0327 23:40:24.264690 2043780 finetune.py:76] layer 31_v @ epoch 1 new loss 0.0005069669568911195 old loss 0.0005064958240836859 WORSE
I0327 23:40:30.612453 2043570 finetune.py:68] layer 28_q @ epoch 1 new loss 0.00016433559358119965 old loss 0.00016710841737221926 BETTER
I0327 23:40:32.445058 2043710 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0002697723393794149 old loss 0.000274774240097031 BETTER
I0327 23:40:51.443478 2043640 finetune.py:68] layer 29_q @ epoch 0 new loss 0.00024788029259070754 old loss 0.0002615375560708344 BETTER
I0327 23:40:58.604363 2043780 finetune.py:68] layer 31_v @ epoch 2 new loss 0.00048107109614647925 old loss 0.0005064958240836859 BETTER
I0327 23:41:07.585522 2043570 finetune.py:68] layer 28_q @ epoch 2 new loss 0.00016212969785556197 old loss 0.00016433559358119965 BETTER
I0327 23:41:07.845370 2043710 finetune.py:68] layer 30_v @ epoch 4 new loss 0.0002654748677741736 old loss 0.0002697723393794149 BETTER
I0327 23:41:26.336157 2043640 finetune.py:68] layer 29_q @ epoch 1 new loss 0.00024079892318695784 old loss 0.00024788029259070754 BETTER
I0327 23:41:28.361555 2043710 finetune.py:45] layer 30_q initial loss 0.0003365393786225468
I0327 23:41:33.561954 2043780 finetune.py:68] layer 31_v @ epoch 3 new loss 0.00044847463141195476 old loss 0.00048107109614647925 BETTER
I0327 23:41:44.500636 2043570 finetune.py:68] layer 28_q @ epoch 3 new loss 0.0001602823904249817 old loss 0.00016212969785556197 BETTER
I0327 23:42:01.506067 2043640 finetune.py:68] layer 29_q @ epoch 2 new loss 0.00023589222109876573 old loss 0.00024079892318695784 BETTER
I0327 23:42:02.198921 2043710 finetune.py:68] layer 30_q @ epoch 0 new loss 0.0003293022746220231 old loss 0.0003365393786225468 BETTER
I0327 23:42:08.528684 2043780 finetune.py:68] layer 31_v @ epoch 4 new loss 0.00044562979019246995 old loss 0.00044847463141195476 BETTER
I0327 23:42:21.559731 2043570 finetune.py:68] layer 28_q @ epoch 4 new loss 0.00015873744268901646 old loss 0.0001602823904249817 BETTER
I0327 23:42:28.516749 2043780 finetune.py:45] layer 31_q initial loss 0.0006544838543049991
I0327 23:42:36.811193 2043640 finetune.py:68] layer 29_q @ epoch 3 new loss 0.00023230530496221036 old loss 0.00023589222109876573 BETTER
I0327 23:42:37.255954 2043710 finetune.py:68] layer 30_q @ epoch 1 new loss 0.0003246505802962929 old loss 0.0003293022746220231 BETTER
I0327 23:42:40.339211 2043570 finetune.py:45] layer 28_k initial loss 0.00017742329509928823
I0327 23:43:02.981878 2043780 finetune.py:68] layer 31_q @ epoch 0 new loss 0.0005958115798421204 old loss 0.0006544838543049991 BETTER
I0327 23:43:11.741180 2043640 finetune.py:68] layer 29_q @ epoch 4 new loss 0.0002290242991875857 old loss 0.00023230530496221036 BETTER
I0327 23:43:12.338083 2043710 finetune.py:68] layer 30_q @ epoch 2 new loss 0.0003212761948816478 old loss 0.0003246505802962929 BETTER
I0327 23:43:15.730095 2043570 finetune.py:68] layer 28_k @ epoch 0 new loss 0.00017524750728625804 old loss 0.00017742329509928823 BETTER
I0327 23:43:30.842696 2043640 finetune.py:45] layer 29_k initial loss 0.0002607910428196192
I0327 23:43:38.073124 2043780 finetune.py:68] layer 31_q @ epoch 1 new loss 0.0005592347588390112 old loss 0.0005958115798421204 BETTER
I0327 23:43:47.672513 2043710 finetune.py:68] layer 30_q @ epoch 3 new loss 0.0003181544307153672 old loss 0.0003212761948816478 BETTER
I0327 23:43:52.523608 2043570 finetune.py:68] layer 28_k @ epoch 1 new loss 0.0001739577710395679 old loss 0.00017524750728625804 BETTER
I0327 23:44:04.320600 2043640 finetune.py:68] layer 29_k @ epoch 0 new loss 0.0002566975890658796 old loss 0.0002607910428196192 BETTER
I0327 23:44:12.974618 2043780 finetune.py:68] layer 31_q @ epoch 2 new loss 0.0005388251738622785 old loss 0.0005592347588390112 BETTER
I0327 23:44:22.884056 2043710 finetune.py:68] layer 30_q @ epoch 4 new loss 0.00031578424386680126 old loss 0.0003181544307153672 BETTER
I0327 23:44:29.611473 2043570 finetune.py:68] layer 28_k @ epoch 2 new loss 0.00017286452930420637 old loss 0.0001739577710395679 BETTER
I0327 23:44:38.833099 2043640 finetune.py:68] layer 29_k @ epoch 1 new loss 0.0002542799338698387 old loss 0.0002566975890658796 BETTER
I0327 23:44:42.599458 2043710 finetune.py:45] layer 30_k initial loss 0.00035701468004845083
I0327 23:44:47.566495 2043780 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0005240029422566295 old loss 0.0005388251738622785 BETTER
I0327 23:45:06.692605 2043570 finetune.py:68] layer 28_k @ epoch 3 new loss 0.00017189494974445552 old loss 0.00017286452930420637 BETTER
I0327 23:45:13.833888 2043640 finetune.py:68] layer 29_k @ epoch 2 new loss 0.00025239450042136014 old loss 0.0002542799338698387 BETTER
I0327 23:45:16.530154 2043710 finetune.py:68] layer 30_k @ epoch 0 new loss 0.0003533877315931022 old loss 0.00035701468004845083 BETTER
I0327 23:45:22.539720 2043780 finetune.py:68] layer 31_q @ epoch 4 new loss 0.000511548831127584 old loss 0.0005240029422566295 BETTER
I0327 23:45:40.713578 2043780 finetune.py:45] layer 31_k initial loss 0.0006225400138646364
I0327 23:45:43.828396 2043570 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0001711154473014176 old loss 0.00017189494974445552 BETTER
I0327 23:45:48.663408 2043640 finetune.py:68] layer 29_k @ epoch 3 new loss 0.00025054175057448447 old loss 0.00025239450042136014 BETTER
I0327 23:45:51.270780 2043710 finetune.py:68] layer 30_k @ epoch 1 new loss 0.00035165224107913673 old loss 0.0003533877315931022 BETTER
I0327 23:46:04.569108 2043570 finetune.py:45] layer 28_o initial loss 0.0004239773261360824
I0327 23:46:14.947849 2043780 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0005953998188488185 old loss 0.0006225400138646364 BETTER
I0327 23:46:23.291415 2043640 finetune.py:68] layer 29_k @ epoch 4 new loss 0.0002493660431355238 old loss 0.00025054175057448447 BETTER
I0327 23:46:25.945857 2043710 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0003506361972540617 old loss 0.00035165224107913673 BETTER
I0327 23:46:39.660265 2043570 finetune.py:68] layer 28_o @ epoch 0 new loss 0.0003983393544331193 old loss 0.0004239773261360824 BETTER
I0327 23:46:43.150743 2043640 finetune.py:45] layer 29_o initial loss 0.0005049135070294142
I0327 23:46:49.499417 2043780 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0005815589101985097 old loss 0.0005953998188488185 BETTER
I0327 23:47:01.145193 2043710 finetune.py:68] layer 30_k @ epoch 3 new loss 0.00034757479443214834 old loss 0.0003506361972540617 BETTER
I0327 23:47:15.606160 2043570 finetune.py:68] layer 28_o @ epoch 1 new loss 0.00039294062298722565 old loss 0.0003983393544331193 BETTER
I0327 23:47:16.179840 2043640 finetune.py:68] layer 29_o @ epoch 0 new loss 0.00047719781287014484 old loss 0.0005049135070294142 BETTER
I0327 23:47:23.993800 2043780 finetune.py:68] layer 31_k @ epoch 2 new loss 0.0005727933603338897 old loss 0.0005815589101985097 BETTER
I0327 23:47:36.067975 2043710 finetune.py:68] layer 30_k @ epoch 4 new loss 0.00034681838587857783 old loss 0.00034757479443214834 BETTER
I0327 23:47:50.227541 2043640 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0004713645321317017 old loss 0.00047719781287014484 BETTER
I0327 23:47:52.020731 2043570 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0003887056955136359 old loss 0.00039294062298722565 BETTER
I0327 23:47:57.279676 2043710 finetune.py:45] layer 30_o initial loss 0.0007705896277911961
I0327 23:47:58.519731 2043780 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0005636850837618113 old loss 0.0005727933603338897 BETTER
I0327 23:48:24.318258 2043640 finetune.py:68] layer 29_o @ epoch 2 new loss 0.00046744541032239795 old loss 0.0004713645321317017 BETTER
I0327 23:48:28.382609 2043570 finetune.py:68] layer 28_o @ epoch 3 new loss 0.0003853274101857096 old loss 0.0003887056955136359 BETTER
I0327 23:48:30.886065 2043710 finetune.py:68] layer 30_o @ epoch 0 new loss 0.0007428302196785808 old loss 0.0007705896277911961 BETTER
I0327 23:48:34.116902 2043780 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0005576384137384593 old loss 0.0005636850837618113 BETTER
I0327 23:48:55.362055 2043780 finetune.py:45] layer 31_o initial loss 0.0014171372167766094
I0327 23:48:58.919184 2043640 finetune.py:68] layer 29_o @ epoch 3 new loss 0.00046386494068428874 old loss 0.00046744541032239795 BETTER
I0327 23:49:05.088242 2043570 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0003824409213848412 old loss 0.0003853274101857096 BETTER
I0327 23:49:05.357699 2043710 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0007351311505772173 old loss 0.0007428302196785808 BETTER
I0327 23:49:28.652627 2043780 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0012801878619939089 old loss 0.0014171372167766094 BETTER
I0327 23:49:33.061228 2043640 finetune.py:68] layer 29_o @ epoch 4 new loss 0.0004610222822520882 old loss 0.00046386494068428874 BETTER
I0327 23:49:39.520906 2043710 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0007290929206646979 old loss 0.0007351311505772173 BETTER
I0327 23:49:39.752487 2043570 finetune.py:45] layer 28_up initial loss 0.001085698720999062
I0327 23:50:03.031518 2043780 finetune.py:68] layer 31_o @ epoch 1 new loss 0.001233865856193006 old loss 0.0012801878619939089 BETTER
I0327 23:50:08.199116 2043640 finetune.py:45] layer 29_up initial loss 0.0014259666204452515
I0327 23:50:12.210057 2043570 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0010702242143452168 old loss 0.001085698720999062 BETTER
I0327 23:50:14.407272 2043710 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0007243261788971722 old loss 0.0007290929206646979 BETTER
I0327 23:50:37.415665 2043780 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0012030183570459485 old loss 0.001233865856193006 BETTER
I0327 23:50:39.151951 2043640 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0014026496792212129 old loss 0.0014259666204452515 BETTER
I0327 23:50:46.989430 2043570 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0010607403237372637 old loss 0.0010702242143452168 BETTER
I0327 23:50:49.703702 2043710 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0007202473934739828 old loss 0.0007243261788971722 BETTER
I0327 23:51:11.685072 2043640 finetune.py:68] layer 29_up @ epoch 1 new loss 0.0013886302476748824 old loss 0.0014026496792212129 BETTER
I0327 23:51:12.696785 2043780 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0011777631007134914 old loss 0.0012030183570459485 BETTER
I0327 23:51:21.112746 2043570 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0010531932348385453 old loss 0.0010607403237372637 BETTER
I0327 23:51:25.729682 2043710 finetune.py:45] layer 30_up initial loss 0.0026081930845975876
I0327 23:51:44.443014 2043640 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0013772023376077414 old loss 0.0013886302476748824 BETTER
I0327 23:51:47.945764 2043780 finetune.py:68] layer 31_o @ epoch 4 new loss 0.001158788800239563 old loss 0.0011777631007134914 BETTER
I0327 23:51:55.388942 2043570 finetune.py:68] layer 28_up @ epoch 3 new loss 0.0010466371895745397 old loss 0.0010531932348385453 BETTER
I0327 23:51:57.212809 2043710 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0025504278019070625 old loss 0.0026081930845975876 BETTER
I0327 23:52:17.041255 2043640 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0013673361390829086 old loss 0.0013772023376077414 BETTER
I0327 23:52:23.390255 2043780 finetune.py:45] layer 31_up initial loss 0.007693702355027199
I0327 23:52:29.725762 2043570 finetune.py:68] layer 28_up @ epoch 4 new loss 0.001040797564201057 old loss 0.0010466371895745397 BETTER
I0327 23:52:30.316599 2043710 finetune.py:68] layer 30_up @ epoch 1 new loss 0.00251310458406806 old loss 0.0025504278019070625 BETTER
I0327 23:52:49.720156 2043640 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0013585808919742703 old loss 0.0013673361390829086 BETTER
I0327 23:52:54.366001 2043780 finetune.py:68] layer 31_up @ epoch 0 new loss 0.007341144606471062 old loss 0.007693702355027199 BETTER
I0327 23:53:03.021989 2043710 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0024826093576848507 old loss 0.00251310458406806 BETTER
I0327 23:53:05.214418 2043570 finetune.py:45] layer 28_gate initial loss 0.0013954596361145377
I0327 23:53:26.245673 2043640 finetune.py:45] layer 29_gate initial loss 0.001829999266192317
I0327 23:53:27.634545 2043780 finetune.py:68] layer 31_up @ epoch 1 new loss 0.007128654047846794 old loss 0.007341144606471062 BETTER
I0327 23:53:36.149285 2043570 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.001388609642162919 old loss 0.0013954596361145377 BETTER
I0327 23:53:36.890293 2043710 finetune.py:68] layer 30_up @ epoch 3 new loss 0.002456391928717494 old loss 0.0024826093576848507 BETTER
I0327 23:53:55.422788 2043640 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0018206877866759896 old loss 0.001829999266192317 BETTER
I0327 23:54:00.816906 2043780 finetune.py:68] layer 31_up @ epoch 2 new loss 0.006959598045796156 old loss 0.007128654047846794 BETTER
I0327 23:54:08.040196 2043570 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0013831970281898975 old loss 0.001388609642162919 BETTER
I0327 23:54:09.861533 2043710 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0024336446076631546 old loss 0.002456391928717494 BETTER
I0327 23:54:25.760475 2043640 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.0018134430283680558 old loss 0.0018206877866759896 BETTER
I0327 23:54:33.408085 2043780 finetune.py:68] layer 31_up @ epoch 3 new loss 0.006809953600168228 old loss 0.006959598045796156 BETTER
I0327 23:54:40.155305 2043570 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.0013781271409243345 old loss 0.0013831970281898975 BETTER
I0327 23:54:45.936960 2043710 finetune.py:45] layer 30_gate initial loss 0.0031294473446905613
I0327 23:54:55.877072 2043640 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0018069756915792823 old loss 0.0018134430283680558 BETTER
I0327 23:55:06.368137 2043780 finetune.py:68] layer 31_up @ epoch 4 new loss 0.006676214747130871 old loss 0.006809953600168228 BETTER
I0327 23:55:11.880702 2043570 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0013736753026023507 old loss 0.0013781271409243345 BETTER
I0327 23:55:14.936316 2043710 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.003105740761384368 old loss 0.0031294473446905613 BETTER
I0327 23:55:25.917859 2043640 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.001801066449843347 old loss 0.0018069756915792823 BETTER
I0327 23:55:42.893485 2043780 finetune.py:45] layer 31_gate initial loss 0.008126888424158096
I0327 23:55:44.847162 2043570 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.0013693514047190547 old loss 0.0013736753026023507 BETTER
I0327 23:55:45.884063 2043710 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.0030885576270520687 old loss 0.003105740761384368 BETTER
I0327 23:55:55.929139 2043640 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0017957197269424796 old loss 0.001801066449843347 BETTER
I0327 23:56:11.844123 2043780 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.00798780471086502 old loss 0.008126888424158096 BETTER
I0327 23:56:16.118039 2043710 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.003072560066357255 old loss 0.0030885576270520687 BETTER
I0327 23:56:42.670921 2043780 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.007892191410064697 old loss 0.00798780471086502 BETTER
I0327 23:56:46.144693 2043570 finetune.py:45] layer 28_down initial loss 0.002189313294366002
I0327 23:56:46.934908 2043710 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.0030578402802348137 old loss 0.003072560066357255 BETTER
I0327 23:56:58.618389 2043640 finetune.py:45] layer 29_down initial loss 0.002892209915444255
I0327 23:57:13.621291 2043780 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.007811275776475668 old loss 0.007892191410064697 BETTER
I0327 23:57:14.520181 2043570 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0021890525240451097 old loss 0.002189313294366002 BETTER
I0327 23:57:17.752646 2043710 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0030450099147856236 old loss 0.0030578402802348137 BETTER
I0327 23:57:25.126230 2043640 finetune.py:68] layer 29_down @ epoch 0 new loss 0.002891907701268792 old loss 0.002892209915444255 BETTER
I0327 23:57:43.932432 2043570 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0021888085175305605 old loss 0.0021890525240451097 BETTER
I0327 23:57:44.558196 2043780 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.00773827126249671 old loss 0.007811275776475668 BETTER
I0327 23:57:52.969345 2043640 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0028916262090206146 old loss 0.002891907701268792 BETTER
I0327 23:58:13.461321 2043570 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0021885866299271584 old loss 0.0021888085175305605 BETTER
I0327 23:58:15.179796 2043780 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.007668138947337866 old loss 0.00773827126249671 BETTER
I0327 23:58:20.324856 2043710 finetune.py:45] layer 30_down initial loss 0.004802392330020666
I0327 23:58:21.191876 2043640 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0028913712594658136 old loss 0.0028916262090206146 BETTER
I0327 23:58:42.894050 2043570 finetune.py:68] layer 28_down @ epoch 3 new loss 0.002188362879678607 old loss 0.0021885866299271584 BETTER
I0327 23:58:46.965260 2043710 finetune.py:68] layer 30_down @ epoch 0 new loss 0.004802087787538767 old loss 0.004802392330020666 BETTER
I0327 23:58:49.483758 2043640 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0028911319095641375 old loss 0.0028913712594658136 BETTER
I0327 23:59:12.735694 2043570 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0021881565917283297 old loss 0.002188362879678607 BETTER
I0327 23:59:15.810973 2043710 finetune.py:68] layer 30_down @ epoch 1 new loss 0.004801804665476084 old loss 0.004802087787538767 BETTER
28_v proxy err 0.038670171052217484 tr(WHW.T) 601.4844360351562
bpp_loss 2.373296601290349
28_q proxy err 0.004151506815105677 tr(WHW.T) 23203.841796875
bpp_loss 2.603955143596977
28_k proxy err 0.0018161819316446781 tr(WHW.T) 14979.6455078125
bpp_loss 3.2876204884960316
28_o proxy err 0.0346904993057251 tr(WHW.T) 2515.998779296875
bpp_loss 2.1950114300998393
28_up proxy err 0.04111456125974655 tr(WHW.T) 10247.28515625
bpp_loss 2.165148757930313
28_gate proxy err 0.012638016603887081 tr(WHW.T) 35804.03515625
bpp_loss 2.4780673963749513
28_down proxy err 0.045326028019189835 tr(WHW.T) 7288.27294921875
bpp_loss 2.149205678458592
I0327 23:59:18.110234 2043640 finetune.py:68] layer 29_down @ epoch 4 new loss 0.0028908951207995415 old loss 0.0028911319095641375 BETTER
I0327 23:59:19.129848 2043780 finetune.py:45] layer 31_down initial loss 0.012716211378574371
29_v proxy err 0.029811378568410873 tr(WHW.T) 850.4290161132812
bpp_loss 2.4366016748535912
29_q proxy err 0.005010335706174374 tr(WHW.T) 20675.515625
bpp_loss 2.595540910202544
29_k proxy err 0.0018154900753870606 tr(WHW.T) 16372.30078125
bpp_loss 3.3567332383827306
29_o proxy err 0.022064996883273125 tr(WHW.T) 3104.872314453125
bpp_loss 2.2354688440682366
29_up proxy err 0.03193488344550133 tr(WHW.T) 12920.5068359375
bpp_loss 2.1983112768003985
29_gate proxy err 0.011500954627990723 tr(WHW.T) 38211.23828125
bpp_loss 2.47068891102182
29_down proxy err 0.03710591420531273 tr(WHW.T) 7565.3212890625
bpp_loss 2.163066562506304
I0327 23:59:43.929793 2043710 finetune.py:68] layer 30_down @ epoch 2 new loss 0.004801531322300434 old loss 0.004801804665476084 BETTER
I0327 23:59:45.636686 2043780 finetune.py:68] layer 31_down @ epoch 0 new loss 0.012714630924165249 old loss 0.012716211378574371 BETTER
I0328 00:00:11.750376 2043710 finetune.py:68] layer 30_down @ epoch 3 new loss 0.004801264964044094 old loss 0.004801531322300434 BETTER
I0328 00:00:13.338379 2043780 finetune.py:68] layer 31_down @ epoch 1 new loss 0.01271339412778616 old loss 0.012714630924165249 BETTER
I0328 00:00:39.581850 2043710 finetune.py:68] layer 30_down @ epoch 4 new loss 0.0048010158352553844 old loss 0.004801264964044094 BETTER
I0328 00:00:40.972481 2043780 finetune.py:68] layer 31_down @ epoch 2 new loss 0.01271230261772871 old loss 0.01271339412778616 BETTER
30_v proxy err 0.02741745486855507 tr(WHW.T) 863.060791015625
bpp_loss 2.682257758016931
30_q proxy err 0.003738946281373501 tr(WHW.T) 24099.287109375
bpp_loss 2.506392320850864
30_k proxy err 0.001793081988580525 tr(WHW.T) 14015.6474609375
bpp_loss 3.075442271423526
30_o proxy err 0.016176428645849228 tr(WHW.T) 4885.51220703125
bpp_loss 2.3153531193675008
30_up proxy err 0.018999353051185608 tr(WHW.T) 21668.056640625
bpp_loss 2.2293179235088507
30_gate proxy err 0.008543126285076141 tr(WHW.T) 51523.51171875
bpp_loss 2.515185078639271
30_down proxy err 0.021155325695872307 tr(WHW.T) 8887.1376953125
bpp_loss 2.1628885365457142
I0328 00:01:08.733941 2043780 finetune.py:68] layer 31_down @ epoch 3 new loss 0.01271128561347723 old loss 0.01271230261772871 BETTER
I0328 00:01:36.392479 2043780 finetune.py:68] layer 31_down @ epoch 4 new loss 0.012710331939160824 old loss 0.01271128561347723 BETTER
31_v proxy err 0.013691514730453491 tr(WHW.T) 1808.723876953125
bpp_loss 2.5347584804985672
31_q proxy err 0.0021779388189315796 tr(WHW.T) 46210.80859375
bpp_loss 2.6440993676078506
31_k proxy err 0.0013788503129035234 tr(WHW.T) 20454.25
bpp_loss 3.275053489487618
31_o proxy err 0.013253951445221901 tr(WHW.T) 2215.50634765625
bpp_loss 2.272371717117494
31_up proxy err 0.005911113694310188 tr(WHW.T) 68968.1328125
bpp_loss 2.410793127359024
31_gate proxy err 0.0030555296689271927 tr(WHW.T) 142888.96875
bpp_loss 2.70929294491985
31_down proxy err 0.008035045117139816 tr(WHW.T) 10074.880859375
bpp_loss 2.1945964954377684
I0328 00:02:05.457662 2043850 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 00:02:05.457864 2043850 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 00:02:05.457903 2043850 utils.py:162] NumExpr defaulting to 16 threads.
I0328 00:02:05.773298 2043850 config.py:54] PyTorch version 2.6.0 available.
W0328 00:02:05.978477 2043850 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 186, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 24, in main
    assert os.path.exists(args.quantized_path)
AssertionError
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
  File "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../hf_model_comp/comp_qtip/hf/ft_ql_ldlq/meta-llama--Meta-Llama-3-8B/lmbda30'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 124, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 71, in main
    model, model_str = model_from_hf_path(
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 34, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1021, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py", line 590, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py", line 649, in _get_config_dict
    resolved_config_file = cached_file(
  File "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: '../hf_model_comp/comp_qtip/hf/ft_ql_ldlq/meta-llama--Meta-Llama-3-8B/lmbda30'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
I0328 02:04:55.305367 2077488 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:04:55.305574 2077488 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:04:55.305614 2077488 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:04:55.622497 2077488 config.py:54] PyTorch version 2.6.0 available.
W0328 02:04:55.823618 2077488 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 02:04:55.932582 2077488 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.09it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.64it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.82it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.78it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.91it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.04it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.29it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.00it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.73it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  9.03it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  9.12it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  9.15it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  9.18it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.08it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.15it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.11it/s]
I0328 02:04:58.636560 2077488 hfize_llama.py:153] loaded layer 0
I0328 02:04:58.908997 2077488 hfize_llama.py:153] loaded layer 1
I0328 02:04:59.151060 2077488 hfize_llama.py:153] loaded layer 2
I0328 02:04:59.389042 2077488 hfize_llama.py:153] loaded layer 3
I0328 02:04:59.640714 2077488 hfize_llama.py:153] loaded layer 4
I0328 02:04:59.906054 2077488 hfize_llama.py:153] loaded layer 5
I0328 02:05:00.225157 2077488 hfize_llama.py:153] loaded layer 6
I0328 02:05:00.493953 2077488 hfize_llama.py:153] loaded layer 7
I0328 02:05:00.753679 2077488 hfize_llama.py:153] loaded layer 8
I0328 02:05:01.133158 2077488 hfize_llama.py:153] loaded layer 9
I0328 02:05:01.419982 2077488 hfize_llama.py:153] loaded layer 10
I0328 02:05:01.691443 2077488 hfize_llama.py:153] loaded layer 11
I0328 02:05:01.998806 2077488 hfize_llama.py:153] loaded layer 12
I0328 02:05:02.284380 2077488 hfize_llama.py:153] loaded layer 13
I0328 02:05:02.544253 2077488 hfize_llama.py:153] loaded layer 14
I0328 02:05:02.841233 2077488 hfize_llama.py:153] loaded layer 15
I0328 02:05:03.190626 2077488 hfize_llama.py:153] loaded layer 16
I0328 02:05:03.470649 2077488 hfize_llama.py:153] loaded layer 17
I0328 02:05:03.853836 2077488 hfize_llama.py:153] loaded layer 18
I0328 02:05:04.097527 2077488 hfize_llama.py:153] loaded layer 19
I0328 02:05:04.361163 2077488 hfize_llama.py:153] loaded layer 20
I0328 02:05:04.601988 2077488 hfize_llama.py:153] loaded layer 21
I0328 02:05:04.848226 2077488 hfize_llama.py:153] loaded layer 22
I0328 02:05:05.149774 2077488 hfize_llama.py:153] loaded layer 23
I0328 02:05:05.416946 2077488 hfize_llama.py:153] loaded layer 24
I0328 02:05:05.696668 2077488 hfize_llama.py:153] loaded layer 25
I0328 02:05:05.962061 2077488 hfize_llama.py:153] loaded layer 26
I0328 02:05:06.253800 2077488 hfize_llama.py:153] loaded layer 27
I0328 02:05:06.517431 2077488 hfize_llama.py:153] loaded layer 28
I0328 02:05:06.814683 2077488 hfize_llama.py:153] loaded layer 29
I0328 02:05:07.080883 2077488 hfize_llama.py:153] loaded layer 30
I0328 02:05:07.356781 2077488 hfize_llama.py:153] loaded layer 31
I0328 02:05:07.356892 2077488 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.07s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.01s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:02,  1.00it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:01,  1.02it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:05<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.09it/s]
I0328 02:05:48.383303 2077488 hfize_llama.py:167] successfully loaded hfized model
I0328 02:05:53.475229 2078293 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:05:53.475404 2078293 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:05:53.475444 2078293 utils.py:162] NumExpr defaulting to 16 threads.
W0328 02:05:53.816334 2078293 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 02:05:54.136317 2078293 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.17s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.09s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.13s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.15s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.10s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.02s/it]
I0328 02:06:01.357386 2078293 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 2.001904010772705:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 2.001904010772705:   1%|          | 1/141 [00:01<04:31,  1.94s/it]avg_loss = 2.2738507986068726:   1%|          | 1/141 [00:03<04:31,  1.94s/it]avg_loss = 2.2738507986068726:   1%|▏         | 2/141 [00:03<03:59,  1.72s/it]avg_loss = 2.418825070063273:   1%|▏         | 2/141 [00:05<03:59,  1.72s/it] avg_loss = 2.418825070063273:   2%|▏         | 3/141 [00:05<03:48,  1.65s/it]avg_loss = 2.3733425736427307:   2%|▏         | 3/141 [00:06<03:48,  1.65s/it]avg_loss = 2.3733425736427307:   3%|▎         | 4/141 [00:06<03:43,  1.63s/it]avg_loss = 2.3368227005004885:   3%|▎         | 4/141 [00:08<03:43,  1.63s/it]avg_loss = 2.3368227005004885:   4%|▎         | 5/141 [00:08<03:39,  1.61s/it]avg_loss = 2.242821534474691:   4%|▎         | 5/141 [00:09<03:39,  1.61s/it] avg_loss = 2.242821534474691:   4%|▍         | 6/141 [00:09<03:37,  1.61s/it]avg_loss = 2.1839245557785034:   4%|▍         | 6/141 [00:11<03:37,  1.61s/it]avg_loss = 2.1839245557785034:   5%|▍         | 7/141 [00:11<03:35,  1.61s/it]avg_loss = 2.180814728140831:   5%|▍         | 7/141 [00:13<03:35,  1.61s/it] avg_loss = 2.180814728140831:   6%|▌         | 8/141 [00:13<03:33,  1.61s/it]avg_loss = 2.2166332801183066:   6%|▌         | 8/141 [00:14<03:33,  1.61s/it]avg_loss = 2.2166332801183066:   6%|▋         | 9/141 [00:14<03:32,  1.61s/it]avg_loss = 2.206639564037323:   6%|▋         | 9/141 [00:16<03:32,  1.61s/it] avg_loss = 2.206639564037323:   7%|▋         | 10/141 [00:16<03:31,  1.61s/it]avg_loss = 2.1974688551642676:   7%|▋         | 10/141 [00:17<03:31,  1.61s/it]avg_loss = 2.1974688551642676:   8%|▊         | 11/141 [00:17<03:29,  1.61s/it]avg_loss = 2.2180155217647552:   8%|▊         | 11/141 [00:19<03:29,  1.61s/it]avg_loss = 2.2180155217647552:   9%|▊         | 12/141 [00:19<03:28,  1.62s/it]avg_loss = 2.222837053812467:   9%|▊         | 12/141 [00:21<03:28,  1.62s/it] avg_loss = 2.222837053812467:   9%|▉         | 13/141 [00:21<03:27,  1.62s/it]avg_loss = 2.2358389667102267:   9%|▉         | 13/141 [00:22<03:27,  1.62s/it]avg_loss = 2.2358389667102267:  10%|▉         | 14/141 [00:22<03:25,  1.62s/it]avg_loss = 2.241635282834371:  10%|▉         | 14/141 [00:24<03:25,  1.62s/it] avg_loss = 2.241635282834371:  11%|█         | 15/141 [00:24<03:24,  1.63s/it]avg_loss = 2.25856926292181:  11%|█         | 15/141 [00:26<03:24,  1.63s/it] avg_loss = 2.25856926292181:  11%|█▏        | 16/141 [00:26<03:23,  1.63s/it]avg_loss = 2.25856973844416:  11%|█▏        | 16/141 [00:27<03:23,  1.63s/it]avg_loss = 2.25856973844416:  12%|█▏        | 17/141 [00:27<03:22,  1.63s/it]avg_loss = 2.2596384353107877:  12%|█▏        | 17/141 [00:29<03:22,  1.63s/it]avg_loss = 2.2596384353107877:  13%|█▎        | 18/141 [00:29<03:21,  1.63s/it]avg_loss = 2.2506881826802303:  13%|█▎        | 18/141 [00:30<03:21,  1.63s/it]avg_loss = 2.2506881826802303:  13%|█▎        | 19/141 [00:30<03:19,  1.64s/it]avg_loss = 2.247777742147446:  13%|█▎        | 19/141 [00:32<03:19,  1.64s/it] avg_loss = 2.247777742147446:  14%|█▍        | 20/141 [00:32<03:18,  1.64s/it]avg_loss = 2.2525348379498435:  14%|█▍        | 20/141 [00:34<03:18,  1.64s/it]avg_loss = 2.2525348379498435:  15%|█▍        | 21/141 [00:34<03:16,  1.64s/it]avg_loss = 2.255319579081102:  15%|█▍        | 21/141 [00:35<03:16,  1.64s/it] avg_loss = 2.255319579081102:  16%|█▌        | 22/141 [00:35<03:15,  1.64s/it]avg_loss = 2.257578782413317:  16%|█▌        | 22/141 [00:37<03:15,  1.64s/it]avg_loss = 2.257578782413317:  16%|█▋        | 23/141 [00:37<03:14,  1.65s/it]avg_loss = 2.262168601155281:  16%|█▋        | 23/141 [00:39<03:14,  1.65s/it]avg_loss = 2.262168601155281:  17%|█▋        | 24/141 [00:39<03:12,  1.65s/it]avg_loss = 2.26923912525177:  17%|█▋        | 24/141 [00:40<03:12,  1.65s/it] avg_loss = 2.26923912525177:  18%|█▊        | 25/141 [00:40<03:11,  1.65s/it]avg_loss = 2.279603320818681:  18%|█▊        | 25/141 [00:42<03:11,  1.65s/it]avg_loss = 2.279603320818681:  18%|█▊        | 26/141 [00:42<03:10,  1.65s/it]avg_loss = 2.290485483628732:  18%|█▊        | 26/141 [00:44<03:10,  1.65s/it]avg_loss = 2.290485483628732:  19%|█▉        | 27/141 [00:44<03:08,  1.66s/it]avg_loss = 2.295686436550958:  19%|█▉        | 27/141 [00:45<03:08,  1.66s/it]avg_loss = 2.295686436550958:  20%|█▉        | 28/141 [00:45<03:07,  1.66s/it]avg_loss = 2.2924183771528046:  20%|█▉        | 28/141 [00:47<03:07,  1.66s/it]avg_loss = 2.2924183771528046:  21%|██        | 29/141 [00:47<03:06,  1.66s/it]avg_loss = 2.2853825370470684:  21%|██        | 29/141 [00:49<03:06,  1.66s/it]avg_loss = 2.2853825370470684:  21%|██▏       | 30/141 [00:49<03:04,  1.66s/it]avg_loss = 2.276052524966578:  21%|██▏       | 30/141 [00:50<03:04,  1.66s/it] avg_loss = 2.276052524966578:  22%|██▏       | 31/141 [00:50<03:03,  1.66s/it]avg_loss = 2.2681351490318775:  22%|██▏       | 31/141 [00:52<03:03,  1.66s/it]avg_loss = 2.2681351490318775:  23%|██▎       | 32/141 [00:52<03:01,  1.67s/it]avg_loss = 2.266127835620533:  23%|██▎       | 32/141 [00:54<03:01,  1.67s/it] avg_loss = 2.266127835620533:  23%|██▎       | 33/141 [00:54<02:59,  1.67s/it]avg_loss = 2.2644433659665726:  23%|██▎       | 33/141 [00:55<02:59,  1.67s/it]avg_loss = 2.2644433659665726:  24%|██▍       | 34/141 [00:55<02:58,  1.67s/it]avg_loss = 2.2659018891198293:  24%|██▍       | 34/141 [00:57<02:58,  1.67s/it]avg_loss = 2.2659018891198293:  25%|██▍       | 35/141 [00:57<02:56,  1.67s/it]avg_loss = 2.248372402456072:  25%|██▍       | 35/141 [00:59<02:56,  1.67s/it] avg_loss = 2.248372402456072:  26%|██▌       | 36/141 [00:59<02:55,  1.67s/it]avg_loss = 2.232847455385569:  26%|██▌       | 36/141 [01:00<02:55,  1.67s/it]avg_loss = 2.232847455385569:  26%|██▌       | 37/141 [01:00<02:53,  1.67s/it]avg_loss = 2.217367881222775:  26%|██▌       | 37/141 [01:02<02:53,  1.67s/it]avg_loss = 2.217367881222775:  27%|██▋       | 38/141 [01:02<02:52,  1.67s/it]avg_loss = 2.2036189299363356:  27%|██▋       | 38/141 [01:04<02:52,  1.67s/it]avg_loss = 2.2036189299363356:  28%|██▊       | 39/141 [01:04<02:50,  1.67s/it]avg_loss = 2.194418579339981:  28%|██▊       | 39/141 [01:05<02:50,  1.67s/it] avg_loss = 2.194418579339981:  28%|██▊       | 40/141 [01:05<02:48,  1.67s/it]avg_loss = 2.200742739002879:  28%|██▊       | 40/141 [01:07<02:48,  1.67s/it]avg_loss = 2.200742739002879:  29%|██▉       | 41/141 [01:07<02:47,  1.68s/it]avg_loss = 2.2152161314373924:  29%|██▉       | 41/141 [01:09<02:47,  1.68s/it]avg_loss = 2.2152161314373924:  30%|██▉       | 42/141 [01:09<02:46,  1.68s/it]avg_loss = 2.2303990541502485:  30%|██▉       | 42/141 [01:10<02:46,  1.68s/it]avg_loss = 2.2303990541502485:  30%|███       | 43/141 [01:10<02:44,  1.68s/it]avg_loss = 2.237394777211276:  30%|███       | 43/141 [01:12<02:44,  1.68s/it] avg_loss = 2.237394777211276:  31%|███       | 44/141 [01:12<02:42,  1.68s/it]avg_loss = 2.243014192581177:  31%|███       | 44/141 [01:14<02:42,  1.68s/it]avg_loss = 2.243014192581177:  32%|███▏      | 45/141 [01:14<02:41,  1.68s/it]avg_loss = 2.2459489625433218:  32%|███▏      | 45/141 [01:16<02:41,  1.68s/it]avg_loss = 2.2459489625433218:  33%|███▎      | 46/141 [01:16<02:39,  1.68s/it]avg_loss = 2.250218574036943:  33%|███▎      | 46/141 [01:17<02:39,  1.68s/it] avg_loss = 2.250218574036943:  33%|███▎      | 47/141 [01:17<02:38,  1.68s/it]avg_loss = 2.2512336522340775:  33%|███▎      | 47/141 [01:19<02:38,  1.68s/it]avg_loss = 2.2512336522340775:  34%|███▍      | 48/141 [01:19<02:36,  1.68s/it]avg_loss = 2.250854034813083:  34%|███▍      | 48/141 [01:21<02:36,  1.68s/it] avg_loss = 2.250854034813083:  35%|███▍      | 49/141 [01:21<02:34,  1.68s/it]avg_loss = 2.249668674468994:  35%|███▍      | 49/141 [01:22<02:34,  1.68s/it]avg_loss = 2.249668674468994:  35%|███▌      | 50/141 [01:22<02:33,  1.68s/it]avg_loss = 2.245943139581119:  35%|███▌      | 50/141 [01:24<02:33,  1.68s/it]avg_loss = 2.245943139581119:  36%|███▌      | 51/141 [01:24<02:31,  1.69s/it]avg_loss = 2.2418516415816088:  36%|███▌      | 51/141 [01:26<02:31,  1.69s/it]avg_loss = 2.2418516415816088:  37%|███▋      | 52/141 [01:26<02:30,  1.69s/it]avg_loss = 2.2360407856275453:  37%|███▋      | 52/141 [01:27<02:30,  1.69s/it]avg_loss = 2.2360407856275453:  38%|███▊      | 53/141 [01:27<02:28,  1.69s/it]avg_loss = 2.232992008880333:  38%|███▊      | 53/141 [01:29<02:28,  1.69s/it] avg_loss = 2.232992008880333:  38%|███▊      | 54/141 [01:29<02:26,  1.69s/it]avg_loss = 2.224864526228471:  38%|███▊      | 54/141 [01:31<02:26,  1.69s/it]avg_loss = 2.224864526228471:  39%|███▉      | 55/141 [01:31<02:25,  1.69s/it]avg_loss = 2.217408063156264:  39%|███▉      | 55/141 [01:32<02:25,  1.69s/it]avg_loss = 2.217408063156264:  40%|███▉      | 56/141 [01:32<02:23,  1.69s/it]avg_loss = 2.2162628194742036:  40%|███▉      | 56/141 [01:34<02:23,  1.69s/it]avg_loss = 2.2162628194742036:  40%|████      | 57/141 [01:34<02:21,  1.69s/it]avg_loss = 2.214143594791149:  40%|████      | 57/141 [01:36<02:21,  1.69s/it] avg_loss = 2.214143594791149:  41%|████      | 58/141 [01:36<02:20,  1.69s/it]avg_loss = 2.216541740853908:  41%|████      | 58/141 [01:37<02:20,  1.69s/it]avg_loss = 2.216541740853908:  42%|████▏     | 59/141 [01:37<02:18,  1.69s/it]avg_loss = 2.2210006455580396:  42%|████▏     | 59/141 [01:39<02:18,  1.69s/it]avg_loss = 2.2210006455580396:  43%|████▎     | 60/141 [01:39<02:17,  1.69s/it]avg_loss = 2.225102270235781:  43%|████▎     | 60/141 [01:41<02:17,  1.69s/it] avg_loss = 2.225102270235781:  43%|████▎     | 61/141 [01:41<02:15,  1.69s/it]avg_loss = 2.2319571721938347:  43%|████▎     | 61/141 [01:43<02:15,  1.69s/it]avg_loss = 2.2319571721938347:  44%|████▍     | 62/141 [01:43<02:13,  1.69s/it]avg_loss = 2.2251051494053433:  44%|████▍     | 62/141 [01:44<02:13,  1.69s/it]avg_loss = 2.2251051494053433:  45%|████▍     | 63/141 [01:44<02:12,  1.69s/it]avg_loss = 2.222398843616247:  45%|████▍     | 63/141 [01:46<02:12,  1.69s/it] avg_loss = 2.222398843616247:  45%|████▌     | 64/141 [01:46<02:10,  1.70s/it]avg_loss = 2.2212758504427397:  45%|████▌     | 64/141 [01:48<02:10,  1.70s/it]avg_loss = 2.2212758504427397:  46%|████▌     | 65/141 [01:48<02:08,  1.70s/it]avg_loss = 2.2162925601005554:  46%|████▌     | 65/141 [01:49<02:08,  1.70s/it]avg_loss = 2.2162925601005554:  47%|████▋     | 66/141 [01:49<02:07,  1.70s/it]avg_loss = 2.2129569356121235:  47%|████▋     | 66/141 [01:51<02:07,  1.70s/it]avg_loss = 2.2129569356121235:  48%|████▊     | 67/141 [01:51<02:05,  1.70s/it]avg_loss = 2.2108808002051186:  48%|████▊     | 67/141 [01:53<02:05,  1.70s/it]avg_loss = 2.2108808002051186:  48%|████▊     | 68/141 [01:53<02:03,  1.70s/it]avg_loss = 2.2096871275832686:  48%|████▊     | 68/141 [01:54<02:03,  1.70s/it]avg_loss = 2.2096871275832686:  49%|████▉     | 69/141 [01:54<02:02,  1.70s/it]avg_loss = 2.2106321113450185:  49%|████▉     | 69/141 [01:56<02:02,  1.70s/it]avg_loss = 2.2106321113450185:  50%|████▉     | 70/141 [01:56<02:00,  1.70s/it]avg_loss = 2.2139082609767646:  50%|████▉     | 70/141 [01:58<02:00,  1.70s/it]avg_loss = 2.2139082609767646:  50%|█████     | 71/141 [01:58<01:58,  1.70s/it]avg_loss = 2.2159928431113562:  50%|█████     | 71/141 [02:00<01:58,  1.70s/it]avg_loss = 2.2159928431113562:  51%|█████     | 72/141 [02:00<01:57,  1.70s/it]avg_loss = 2.2135434950867743:  51%|█████     | 72/141 [02:01<01:57,  1.70s/it]avg_loss = 2.2135434950867743:  52%|█████▏    | 73/141 [02:01<01:55,  1.70s/it]avg_loss = 2.2148298621177673:  52%|█████▏    | 73/141 [02:03<01:55,  1.70s/it]avg_loss = 2.2148298621177673:  52%|█████▏    | 74/141 [02:03<01:53,  1.70s/it]avg_loss = 2.2139463726679485:  52%|█████▏    | 74/141 [02:05<01:53,  1.70s/it]avg_loss = 2.2139463726679485:  53%|█████▎    | 75/141 [02:05<01:52,  1.70s/it]avg_loss = 2.211893302829642:  53%|█████▎    | 75/141 [02:06<01:52,  1.70s/it] avg_loss = 2.211893302829642:  54%|█████▍    | 76/141 [02:06<01:50,  1.70s/it]avg_loss = 2.2130624365496945:  54%|█████▍    | 76/141 [02:08<01:50,  1.70s/it]avg_loss = 2.2130624365496945:  55%|█████▍    | 77/141 [02:08<01:48,  1.70s/it]avg_loss = 2.2151659528414407:  55%|█████▍    | 77/141 [02:10<01:48,  1.70s/it]avg_loss = 2.2151659528414407:  55%|█████▌    | 78/141 [02:10<01:47,  1.70s/it]avg_loss = 2.218193128139158:  55%|█████▌    | 78/141 [02:11<01:47,  1.70s/it] avg_loss = 2.218193128139158:  56%|█████▌    | 79/141 [02:11<01:45,  1.70s/it]avg_loss = 2.2134165331721305:  56%|█████▌    | 79/141 [02:13<01:45,  1.70s/it]avg_loss = 2.2134165331721305:  57%|█████▋    | 80/141 [02:13<01:43,  1.70s/it]avg_loss = 2.2119015779024287:  57%|█████▋    | 80/141 [02:15<01:43,  1.70s/it]avg_loss = 2.2119015779024287:  57%|█████▋    | 81/141 [02:15<01:41,  1.70s/it]avg_loss = 2.210569685552178:  57%|█████▋    | 81/141 [02:17<01:41,  1.70s/it] avg_loss = 2.210569685552178:  58%|█████▊    | 82/141 [02:17<01:40,  1.70s/it]avg_loss = 2.2085759309400994:  58%|█████▊    | 82/141 [02:18<01:40,  1.70s/it]avg_loss = 2.2085759309400994:  59%|█████▉    | 83/141 [02:18<01:38,  1.70s/it]avg_loss = 2.2057607840924036:  59%|█████▉    | 83/141 [02:20<01:38,  1.70s/it]avg_loss = 2.2057607840924036:  60%|█████▉    | 84/141 [02:20<01:36,  1.70s/it]avg_loss = 2.203407743397881:  60%|█████▉    | 84/141 [02:22<01:36,  1.70s/it] avg_loss = 2.203407743397881:  60%|██████    | 85/141 [02:22<01:35,  1.70s/it]avg_loss = 2.2042987665464713:  60%|██████    | 85/141 [02:23<01:35,  1.70s/it]avg_loss = 2.2042987665464713:  61%|██████    | 86/141 [02:23<01:33,  1.70s/it]avg_loss = 2.2057237419588813:  61%|██████    | 86/141 [02:25<01:33,  1.70s/it]avg_loss = 2.2057237419588813:  62%|██████▏   | 87/141 [02:25<01:31,  1.70s/it]avg_loss = 2.207550922578031:  62%|██████▏   | 87/141 [02:27<01:31,  1.70s/it] avg_loss = 2.207550922578031:  62%|██████▏   | 88/141 [02:27<01:30,  1.70s/it]avg_loss = 2.2157311774371715:  62%|██████▏   | 88/141 [02:28<01:30,  1.70s/it]avg_loss = 2.2157311774371715:  63%|██████▎   | 89/141 [02:28<01:28,  1.70s/it]avg_loss = 2.2226716134283278:  63%|██████▎   | 89/141 [02:30<01:28,  1.70s/it]avg_loss = 2.2226716134283278:  64%|██████▍   | 90/141 [02:30<01:26,  1.70s/it]avg_loss = 2.2258228115983063:  64%|██████▍   | 90/141 [02:32<01:26,  1.70s/it]avg_loss = 2.2258228115983063:  65%|██████▍   | 91/141 [02:32<01:24,  1.70s/it]avg_loss = 2.2307650822660197:  65%|██████▍   | 91/141 [02:34<01:24,  1.70s/it]avg_loss = 2.2307650822660197:  65%|██████▌   | 92/141 [02:34<01:23,  1.70s/it]avg_loss = 2.2357036029138873:  65%|██████▌   | 92/141 [02:35<01:23,  1.70s/it]avg_loss = 2.2357036029138873:  66%|██████▌   | 93/141 [02:35<01:21,  1.70s/it]avg_loss = 2.2356973011442958:  66%|██████▌   | 93/141 [02:37<01:21,  1.70s/it]avg_loss = 2.2356973011442958:  67%|██████▋   | 94/141 [02:37<01:19,  1.70s/it]avg_loss = 2.2394460013038233:  67%|██████▋   | 94/141 [02:39<01:19,  1.70s/it]avg_loss = 2.2394460013038233:  67%|██████▋   | 95/141 [02:39<01:18,  1.70s/it]avg_loss = 2.239455090214809:  67%|██████▋   | 95/141 [02:40<01:18,  1.70s/it] avg_loss = 2.239455090214809:  68%|██████▊   | 96/141 [02:40<01:16,  1.70s/it]avg_loss = 2.2408325389488457:  68%|██████▊   | 96/141 [02:42<01:16,  1.70s/it]avg_loss = 2.2408325389488457:  69%|██████▉   | 97/141 [02:42<01:14,  1.70s/it]avg_loss = 2.239321174670239:  69%|██████▉   | 97/141 [02:44<01:14,  1.70s/it] avg_loss = 2.239321174670239:  70%|██████▉   | 98/141 [02:44<01:13,  1.70s/it]avg_loss = 2.2404591061852197:  70%|██████▉   | 98/141 [02:45<01:13,  1.70s/it]avg_loss = 2.2404591061852197:  70%|███████   | 99/141 [02:45<01:11,  1.70s/it]avg_loss = 2.242988191843033:  70%|███████   | 99/141 [02:47<01:11,  1.70s/it] avg_loss = 2.242988191843033:  71%|███████   | 100/141 [02:47<01:09,  1.70s/it]avg_loss = 2.2437186819492:  71%|███████   | 100/141 [02:49<01:09,  1.70s/it]  avg_loss = 2.2437186819492:  72%|███████▏  | 101/141 [02:49<01:08,  1.70s/it]avg_loss = 2.245351862673666:  72%|███████▏  | 101/141 [02:51<01:08,  1.70s/it]avg_loss = 2.245351862673666:  72%|███████▏  | 102/141 [02:51<01:06,  1.70s/it]avg_loss = 2.2461563325622707:  72%|███████▏  | 102/141 [02:52<01:06,  1.70s/it]avg_loss = 2.2461563325622707:  73%|███████▎  | 103/141 [02:52<01:04,  1.70s/it]avg_loss = 2.2504450667362947:  73%|███████▎  | 103/141 [02:54<01:04,  1.70s/it]avg_loss = 2.2504450667362947:  74%|███████▍  | 104/141 [02:54<01:02,  1.70s/it]avg_loss = 2.2503039984476:  74%|███████▍  | 104/141 [02:56<01:02,  1.70s/it]   avg_loss = 2.2503039984476:  74%|███████▍  | 105/141 [02:56<01:01,  1.70s/it]avg_loss = 2.2503151679938695:  74%|███████▍  | 105/141 [02:57<01:01,  1.70s/it]avg_loss = 2.2503151679938695:  75%|███████▌  | 106/141 [02:57<00:59,  1.70s/it]avg_loss = 2.2495321481027335:  75%|███████▌  | 106/141 [02:59<00:59,  1.70s/it]avg_loss = 2.2495321481027335:  76%|███████▌  | 107/141 [02:59<00:57,  1.70s/it]avg_loss = 2.2477731031400188:  76%|███████▌  | 107/141 [03:01<00:57,  1.70s/it]avg_loss = 2.2477731031400188:  77%|███████▋  | 108/141 [03:01<00:56,  1.70s/it]avg_loss = 2.2466186744357466:  77%|███████▋  | 108/141 [03:02<00:56,  1.70s/it]avg_loss = 2.2466186744357466:  77%|███████▋  | 109/141 [03:02<00:54,  1.70s/it]avg_loss = 2.2442044019699097:  77%|███████▋  | 109/141 [03:04<00:54,  1.70s/it]avg_loss = 2.2442044019699097:  78%|███████▊  | 110/141 [03:04<00:52,  1.70s/it]avg_loss = 2.246801318349065:  78%|███████▊  | 110/141 [03:06<00:52,  1.70s/it] avg_loss = 2.246801318349065:  79%|███████▊  | 111/141 [03:06<00:51,  1.70s/it]avg_loss = 2.2459481869425093:  79%|███████▊  | 111/141 [03:08<00:51,  1.70s/it]avg_loss = 2.2459481869425093:  79%|███████▉  | 112/141 [03:08<00:49,  1.70s/it]avg_loss = 2.246731479611017:  79%|███████▉  | 112/141 [03:09<00:49,  1.70s/it] avg_loss = 2.246731479611017:  80%|████████  | 113/141 [03:09<00:47,  1.70s/it]avg_loss = 2.2484718540258575:  80%|████████  | 113/141 [03:11<00:47,  1.70s/it]avg_loss = 2.2484718540258575:  81%|████████  | 114/141 [03:11<00:46,  1.70s/it]avg_loss = 2.2473873242087987:  81%|████████  | 114/141 [03:13<00:46,  1.70s/it]avg_loss = 2.2473873242087987:  82%|████████▏ | 115/141 [03:13<00:44,  1.70s/it]avg_loss = 2.246461971052762:  82%|████████▏ | 115/141 [03:14<00:44,  1.70s/it] avg_loss = 2.246461971052762:  82%|████████▏ | 116/141 [03:14<00:42,  1.70s/it]avg_loss = 2.2487913889762683:  82%|████████▏ | 116/141 [03:16<00:42,  1.70s/it]avg_loss = 2.2487913889762683:  83%|████████▎ | 117/141 [03:16<00:40,  1.70s/it]avg_loss = 2.2478782059782643:  83%|████████▎ | 117/141 [03:18<00:40,  1.70s/it]avg_loss = 2.2478782059782643:  84%|████████▎ | 118/141 [03:18<00:39,  1.70s/it]avg_loss = 2.24621036874146:  84%|████████▎ | 118/141 [03:20<00:39,  1.70s/it]  avg_loss = 2.24621036874146:  84%|████████▍ | 119/141 [03:20<00:37,  1.70s/it]avg_loss = 2.24426873922348:  84%|████████▍ | 119/141 [03:21<00:37,  1.70s/it]avg_loss = 2.24426873922348:  85%|████████▌ | 120/141 [03:21<00:35,  1.70s/it]avg_loss = 2.2450366788659215:  85%|████████▌ | 120/141 [03:23<00:35,  1.70s/it]avg_loss = 2.2450366788659215:  86%|████████▌ | 121/141 [03:23<00:34,  1.70s/it]avg_loss = 2.246077643066156:  86%|████████▌ | 121/141 [03:25<00:34,  1.70s/it] avg_loss = 2.246077643066156:  87%|████████▋ | 122/141 [03:25<00:32,  1.70s/it]avg_loss = 2.2449879394314154:  87%|████████▋ | 122/141 [03:26<00:32,  1.70s/it]avg_loss = 2.2449879394314154:  87%|████████▋ | 123/141 [03:26<00:30,  1.70s/it]avg_loss = 2.244936771931187:  87%|████████▋ | 123/141 [03:28<00:30,  1.70s/it] avg_loss = 2.244936771931187:  88%|████████▊ | 124/141 [03:28<00:28,  1.70s/it]avg_loss = 2.243018859863281:  88%|████████▊ | 124/141 [03:30<00:28,  1.70s/it]avg_loss = 2.243018859863281:  89%|████████▊ | 125/141 [03:30<00:27,  1.70s/it]avg_loss = 2.2427730976589144:  89%|████████▊ | 125/141 [03:31<00:27,  1.70s/it]avg_loss = 2.2427730976589144:  89%|████████▉ | 126/141 [03:31<00:25,  1.70s/it]avg_loss = 2.2424326055631862:  89%|████████▉ | 126/141 [03:33<00:25,  1.70s/it]avg_loss = 2.2424326055631862:  90%|█████████ | 127/141 [03:33<00:23,  1.70s/it]avg_loss = 2.2411118261516094:  90%|█████████ | 127/141 [03:35<00:23,  1.70s/it]avg_loss = 2.2411118261516094:  91%|█████████ | 128/141 [03:35<00:22,  1.70s/it]avg_loss = 2.240652221117833:  91%|█████████ | 128/141 [03:37<00:22,  1.70s/it] avg_loss = 2.240652221117833:  91%|█████████▏| 129/141 [03:37<00:20,  1.70s/it]avg_loss = 2.24185777260707:  91%|█████████▏| 129/141 [03:38<00:20,  1.70s/it] avg_loss = 2.24185777260707:  92%|█████████▏| 130/141 [03:38<00:18,  1.70s/it]avg_loss = 2.242611792251354:  92%|█████████▏| 130/141 [03:40<00:18,  1.70s/it]avg_loss = 2.242611792251354:  93%|█████████▎| 131/141 [03:40<00:17,  1.70s/it]avg_loss = 2.2428418506275523:  93%|█████████▎| 131/141 [03:42<00:17,  1.70s/it]avg_loss = 2.2428418506275523:  94%|█████████▎| 132/141 [03:42<00:15,  1.70s/it]avg_loss = 2.2393757561991987:  94%|█████████▎| 132/141 [03:43<00:15,  1.70s/it]avg_loss = 2.2393757561991987:  94%|█████████▍| 133/141 [03:43<00:13,  1.71s/it]avg_loss = 2.234216998762159:  94%|█████████▍| 133/141 [03:45<00:13,  1.71s/it] avg_loss = 2.234216998762159:  95%|█████████▌| 134/141 [03:45<00:11,  1.70s/it]avg_loss = 2.2361658175786334:  95%|█████████▌| 134/141 [03:47<00:11,  1.70s/it]avg_loss = 2.2361658175786334:  96%|█████████▌| 135/141 [03:47<00:10,  1.70s/it]avg_loss = 2.239861985339838:  96%|█████████▌| 135/141 [03:48<00:10,  1.70s/it] avg_loss = 2.239861985339838:  96%|█████████▋| 136/141 [03:48<00:08,  1.70s/it]avg_loss = 2.2413129850025593:  96%|█████████▋| 136/141 [03:50<00:08,  1.70s/it]avg_loss = 2.2413129850025593:  97%|█████████▋| 137/141 [03:50<00:06,  1.70s/it]avg_loss = 2.240992538307024:  97%|█████████▋| 137/141 [03:52<00:06,  1.70s/it] avg_loss = 2.240992538307024:  98%|█████████▊| 138/141 [03:52<00:05,  1.70s/it]avg_loss = 2.2419495128041547:  98%|█████████▊| 138/141 [03:54<00:05,  1.70s/it]avg_loss = 2.2419495128041547:  99%|█████████▊| 139/141 [03:54<00:03,  1.70s/it]avg_loss = 2.2430854022502897:  99%|█████████▊| 139/141 [03:55<00:03,  1.70s/it]avg_loss = 2.2430854022502897:  99%|█████████▉| 140/141 [03:55<00:01,  1.70s/it]avg_loss = 2.244463730365672:  99%|█████████▉| 140/141 [03:57<00:01,  1.70s/it] avg_loss = 2.244463730365672: 100%|██████████| 141/141 [03:57<00:00,  1.70s/it]avg_loss = 2.244463730365672: 100%|██████████| 141/141 [03:57<00:00,  1.68s/it]
I0328 02:10:21.848086 2078293 eval_ppl.py:107] wikitext2 perplexity: 9.435354232788086
wikitext2 perplexity: 9.435
