I0328 19:21:43.124640 2316731 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:21:43.124739 2316731 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:21:43.124780 2316731 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:21:43.469967 2316731 config.py:54] PyTorch version 2.6.0 available.
W0328 19:21:43.676413 2316731 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 19:21:44.263239 2316731 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.38it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.03it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.36it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.46it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.28it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.33it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.47it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.33it/s]
I0328 19:21:45.828253 2316731 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.45it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:17,  1.69it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.78it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.84it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.85it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.86it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.88it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.88it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:12,  1.84it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.85it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:11,  1.86it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.87it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:10,  1.88it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.88it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  1.90it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.90it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.93it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.94it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.93it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.93it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.93it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.92it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.91it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.93it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.93it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.93it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.95it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.97it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.99it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.99it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.99it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
I0328 19:22:08.701471 2316731 quantize_finetune_llama.py:185] loaded compression model
I0328 19:22:28.148709 2316731 quantize_finetune_llama.py:189] loaded dataset and devset
I0328 19:22:33.137516 2316731 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 19:23:25.884750 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 52.60830354690552s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0328 19:23:52.058332 2316861 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:23:52.058423 2316861 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:23:52.058462 2316861 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:23:52.384186 2316861 config.py:54] PyTorch version 2.6.0 available.
W0328 19:23:52.584578 2316861 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 19:23:53.152085 2316861 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 19:23:53.156195 2316731 quantize_finetune_llama.py:209] layer 1 gpu 1
I0328 19:23:53.170799 2316861 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 19:24:09.704288 2316861 finetune.py:45] layer 0_v initial loss 5.470317532285662e-08
W0328 19:24:09.704500 2316861 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 19:24:43.489779 2316861 finetune.py:68] layer 0_v @ epoch 0 new loss 3.8963761994637025e-08 old loss 5.470317532285662e-08 BETTER
I0328 19:24:48.783599 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 55.45944333076477s
I0328 19:25:00.469907 2316933 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:25:00.470006 2316933 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:25:00.470045 2316933 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:25:00.821290 2316933 config.py:54] PyTorch version 2.6.0 available.
W0328 19:25:01.021323 2316933 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 19:25:01.632692 2316933 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 19:25:01.636412 2316731 quantize_finetune_llama.py:209] layer 2 gpu 2
I0328 19:25:01.649384 2316933 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 19:25:18.379563 2316933 finetune.py:45] layer 1_v initial loss 4.0531864442527876e-07
W0328 19:25:18.380050 2316933 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 19:25:18.685218 2316861 finetune.py:68] layer 0_v @ epoch 1 new loss 3.498891132380777e-08 old loss 3.8963761994637025e-08 BETTER
I0328 19:25:51.010516 2316933 finetune.py:68] layer 1_v @ epoch 0 new loss 1.9111115534542478e-07 old loss 4.0531864442527876e-07 BETTER
I0328 19:25:54.509763 2316861 finetune.py:68] layer 0_v @ epoch 2 new loss 3.3275124877718554e-08 old loss 3.498891132380777e-08 BETTER
I0328 19:26:07.001473 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 65.16249227523804s
I0328 19:26:18.596362 2317005 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:26:18.596459 2317005 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:26:18.596496 2317005 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:26:19.235458 2317005 config.py:54] PyTorch version 2.6.0 available.
W0328 19:26:19.438994 2317005 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 19:26:20.118742 2317005 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 19:26:20.122262 2316731 quantize_finetune_llama.py:209] layer 3 gpu 3
I0328 19:26:20.136294 2317005 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 19:26:25.175975 2316933 finetune.py:68] layer 1_v @ epoch 1 new loss 1.1754795536944584e-07 old loss 1.9111115534542478e-07 BETTER
I0328 19:26:30.834785 2316861 finetune.py:68] layer 0_v @ epoch 3 new loss 3.228295142321258e-08 old loss 3.3275124877718554e-08 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 19:26:37.209352 2317005 finetune.py:45] layer 2_v initial loss 5.907091917833895e-07
W0328 19:26:37.209690 2317005 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 19:26:59.532812 2316933 finetune.py:68] layer 1_v @ epoch 2 new loss 8.603119283634442e-08 old loss 1.1754795536944584e-07 BETTER
I0328 19:27:07.328945 2316861 finetune.py:68] layer 0_v @ epoch 4 new loss 3.16112185316797e-08 old loss 3.228295142321258e-08 BETTER
I0328 19:27:10.278451 2317005 finetune.py:68] layer 2_v @ epoch 0 new loss 1.9308330934109108e-07 old loss 5.907091917833895e-07 BETTER
I0328 19:27:25.639903 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 65.33469820022583s
I0328 19:27:27.624181 2316861 finetune.py:45] layer 0_q initial loss 3.181946084396259e-08
I0328 19:27:34.300616 2316933 finetune.py:68] layer 1_v @ epoch 3 new loss 7.068354079819983e-08 old loss 8.603119283634442e-08 BETTER
I0328 19:27:35.118098 2317077 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:27:35.118188 2317077 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:27:35.118228 2317077 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:27:35.507648 2317077 config.py:54] PyTorch version 2.6.0 available.
W0328 19:27:35.722103 2317077 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 19:27:36.351978 2317077 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 19:27:36.356309 2316731 quantize_finetune_llama.py:209] layer 4 gpu 0
I0328 19:27:36.370811 2317077 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 19:27:44.595430 2317005 finetune.py:68] layer 2_v @ epoch 1 new loss 1.179238395820903e-07 old loss 1.9308330934109108e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 19:27:53.247003 2317077 finetune.py:45] layer 3_v initial loss 5.206712785366108e-07
W0328 19:27:53.247229 2317077 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 19:28:02.724436 2316861 finetune.py:68] layer 0_q @ epoch 0 new loss 3.1288443835819635e-08 old loss 3.181946084396259e-08 BETTER
I0328 19:28:09.287336 2316933 finetune.py:68] layer 1_v @ epoch 4 new loss 6.189443979565112e-08 old loss 7.068354079819983e-08 BETTER
I0328 19:28:19.187420 2317005 finetune.py:68] layer 2_v @ epoch 2 new loss 9.394058508860326e-08 old loss 1.179238395820903e-07 BETTER
I0328 19:28:25.964683 2317077 finetune.py:68] layer 3_v @ epoch 0 new loss 1.7529231399748824e-07 old loss 5.206712785366108e-07 BETTER
I0328 19:28:28.778620 2316933 finetune.py:45] layer 1_q initial loss 6.292567178434183e-08
I0328 19:28:38.627111 2316861 finetune.py:68] layer 0_q @ epoch 1 new loss 3.0859155231155455e-08 old loss 3.1288443835819635e-08 BETTER
I0328 19:28:54.216785 2317005 finetune.py:68] layer 2_v @ epoch 3 new loss 8.17842291667148e-08 old loss 9.394058508860326e-08 BETTER
I0328 19:28:59.729365 2317077 finetune.py:68] layer 3_v @ epoch 1 new loss 1.2168275986823573e-07 old loss 1.7529231399748824e-07 BETTER
I0328 19:29:02.192794 2316933 finetune.py:68] layer 1_q @ epoch 0 new loss 5.674782954656621e-08 old loss 6.292567178434183e-08 BETTER
I0328 19:29:14.672989 2316861 finetune.py:68] layer 0_q @ epoch 2 new loss 3.050529073789221e-08 old loss 3.0859155231155455e-08 BETTER
I0328 19:29:29.109326 2317005 finetune.py:68] layer 2_v @ epoch 4 new loss 7.384764444395842e-08 old loss 8.17842291667148e-08 BETTER
I0328 19:29:33.920568 2317077 finetune.py:68] layer 3_v @ epoch 2 new loss 1.007771999184115e-07 old loss 1.2168275986823573e-07 BETTER
I0328 19:29:36.567701 2316933 finetune.py:68] layer 1_q @ epoch 1 new loss 5.251594004107574e-08 old loss 5.674782954656621e-08 BETTER
I0328 19:29:48.734167 2317005 finetune.py:45] layer 2_q initial loss 8.342043145148637e-08
I0328 19:29:50.842377 2316861 finetune.py:68] layer 0_q @ epoch 3 new loss 3.019889405209142e-08 old loss 3.050529073789221e-08 BETTER
I0328 19:30:08.345952 2317077 finetune.py:68] layer 3_v @ epoch 3 new loss 9.019080948746705e-08 old loss 1.007771999184115e-07 BETTER
I0328 19:30:10.802612 2316933 finetune.py:68] layer 1_q @ epoch 2 new loss 4.9332484763908724e-08 old loss 5.251594004107574e-08 BETTER
I0328 19:30:22.592961 2317005 finetune.py:68] layer 2_q @ epoch 0 new loss 7.628034381923499e-08 old loss 8.342043145148637e-08 BETTER
I0328 19:30:26.989545 2316861 finetune.py:68] layer 0_q @ epoch 4 new loss 2.992671710444483e-08 old loss 3.019889405209142e-08 BETTER
I0328 19:30:43.304023 2317077 finetune.py:68] layer 3_v @ epoch 4 new loss 8.313827493111603e-08 old loss 9.019080948746705e-08 BETTER
I0328 19:30:45.165105 2316861 finetune.py:45] layer 0_k initial loss 3.106723056589544e-08
I0328 19:30:45.300759 2316933 finetune.py:68] layer 1_q @ epoch 3 new loss 4.6850821888710925e-08 old loss 4.9332484763908724e-08 BETTER
I0328 19:30:57.089848 2317005 finetune.py:68] layer 2_q @ epoch 1 new loss 7.164695858818959e-08 old loss 7.628034381923499e-08 BETTER
I0328 19:31:02.835537 2317077 finetune.py:45] layer 3_q initial loss 9.934434075375975e-08
I0328 19:31:19.676083 2316933 finetune.py:68] layer 1_q @ epoch 4 new loss 4.480160242792408e-08 old loss 4.6850821888710925e-08 BETTER
I0328 19:31:20.127210 2316861 finetune.py:68] layer 0_k @ epoch 0 new loss 3.057252939697719e-08 old loss 3.106723056589544e-08 BETTER
I0328 19:31:31.669618 2317005 finetune.py:68] layer 2_q @ epoch 2 new loss 6.825332832249842e-08 old loss 7.164695858818959e-08 BETTER
I0328 19:31:35.703650 2317077 finetune.py:68] layer 3_q @ epoch 0 new loss 9.346501173013166e-08 old loss 9.934434075375975e-08 BETTER
I0328 19:31:37.592005 2316933 finetune.py:45] layer 1_k initial loss 4.9461586826282655e-08
I0328 19:31:56.201930 2316861 finetune.py:68] layer 0_k @ epoch 1 new loss 3.027753336937167e-08 old loss 3.057252939697719e-08 BETTER
I0328 19:32:06.278363 2317005 finetune.py:68] layer 2_q @ epoch 3 new loss 6.567670141066628e-08 old loss 6.825332832249842e-08 BETTER
I0328 19:32:09.622689 2317077 finetune.py:68] layer 3_q @ epoch 1 new loss 8.965727005261215e-08 old loss 9.346501173013166e-08 BETTER
I0328 19:32:10.854951 2316933 finetune.py:68] layer 1_k @ epoch 0 new loss 4.6530026054369955e-08 old loss 4.9461586826282655e-08 BETTER
I0328 19:32:32.461548 2316861 finetune.py:68] layer 0_k @ epoch 2 new loss 3.006300630659098e-08 old loss 3.027753336937167e-08 BETTER
I0328 19:32:41.023354 2317005 finetune.py:68] layer 2_q @ epoch 4 new loss 6.364220439536439e-08 old loss 6.567670141066628e-08 BETTER
I0328 19:32:43.819539 2317077 finetune.py:68] layer 3_q @ epoch 2 new loss 8.707175425115565e-08 old loss 8.965727005261215e-08 BETTER
I0328 19:32:45.118375 2316933 finetune.py:68] layer 1_k @ epoch 1 new loss 4.488003924052464e-08 old loss 4.6530026054369955e-08 BETTER
I0328 19:32:59.177612 2317005 finetune.py:45] layer 2_k initial loss 7.124081946585648e-08
I0328 19:33:08.735087 2316861 finetune.py:68] layer 0_k @ epoch 3 new loss 2.985901659258161e-08 old loss 3.006300630659098e-08 BETTER
I0328 19:33:18.015087 2317077 finetune.py:68] layer 3_q @ epoch 3 new loss 8.499670656192393e-08 old loss 8.707175425115565e-08 BETTER
I0328 19:33:19.337820 2316933 finetune.py:68] layer 1_k @ epoch 2 new loss 4.3527411008881245e-08 old loss 4.488003924052464e-08 BETTER
I0328 19:33:32.619299 2317005 finetune.py:68] layer 2_k @ epoch 0 new loss 6.875884395185494e-08 old loss 7.124081946585648e-08 BETTER
I0328 19:33:44.866457 2316861 finetune.py:68] layer 0_k @ epoch 4 new loss 2.967906098660933e-08 old loss 2.985901659258161e-08 BETTER
I0328 19:33:51.924282 2317077 finetune.py:68] layer 3_q @ epoch 4 new loss 8.335661050296039e-08 old loss 8.499670656192393e-08 BETTER
I0328 19:33:53.627215 2316933 finetune.py:68] layer 1_k @ epoch 3 new loss 4.2373308417609223e-08 old loss 4.3527411008881245e-08 BETTER
I0328 19:34:04.182445 2316861 finetune.py:45] layer 0_o initial loss 4.401521636054895e-08
I0328 19:34:06.955133 2317005 finetune.py:68] layer 2_k @ epoch 1 new loss 6.72768152298886e-08 old loss 6.875884395185494e-08 BETTER
I0328 19:34:09.684783 2317077 finetune.py:45] layer 3_k initial loss 9.54606562686422e-08
I0328 19:34:27.778018 2316933 finetune.py:68] layer 1_k @ epoch 4 new loss 4.13767189400005e-08 old loss 4.2373308417609223e-08 BETTER
I0328 19:34:38.521659 2316861 finetune.py:68] layer 0_o @ epoch 0 new loss 4.3890175049909885e-08 old loss 4.401521636054895e-08 BETTER
I0328 19:34:41.442957 2317005 finetune.py:68] layer 2_k @ epoch 2 new loss 6.608762248561106e-08 old loss 6.72768152298886e-08 BETTER
I0328 19:34:42.485371 2317077 finetune.py:68] layer 3_k @ epoch 0 new loss 9.335482076266999e-08 old loss 9.54606562686422e-08 BETTER
I0328 19:34:47.045059 2316933 finetune.py:45] layer 1_o initial loss 7.743457075548577e-08
I0328 19:35:14.238143 2316861 finetune.py:68] layer 0_o @ epoch 1 new loss 4.3792638848572096e-08 old loss 4.3890175049909885e-08 BETTER
I0328 19:35:16.128435 2317005 finetune.py:68] layer 2_k @ epoch 3 new loss 6.506140692863482e-08 old loss 6.608762248561106e-08 BETTER
I0328 19:35:16.296678 2317077 finetune.py:68] layer 3_k @ epoch 1 new loss 9.206795681393487e-08 old loss 9.335482076266999e-08 BETTER
I0328 19:35:19.540617 2316933 finetune.py:68] layer 1_o @ epoch 0 new loss 7.504047516704304e-08 old loss 7.743457075548577e-08 BETTER
I0328 19:35:49.993249 2316861 finetune.py:68] layer 0_o @ epoch 2 new loss 4.3703995089572345e-08 old loss 4.3792638848572096e-08 BETTER
I0328 19:35:50.315840 2317077 finetune.py:68] layer 3_k @ epoch 2 new loss 9.106983611673058e-08 old loss 9.206795681393487e-08 BETTER
I0328 19:35:50.938216 2317005 finetune.py:68] layer 2_k @ epoch 4 new loss 6.419268316903981e-08 old loss 6.506140692863482e-08 BETTER
I0328 19:35:53.199721 2316933 finetune.py:68] layer 1_o @ epoch 1 new loss 7.460955231408661e-08 old loss 7.504047516704304e-08 BETTER
I0328 19:36:10.358493 2317005 finetune.py:45] layer 2_o initial loss 1.1973411062626838e-07
I0328 19:36:24.238200 2317077 finetune.py:68] layer 3_k @ epoch 3 new loss 9.016240909431872e-08 old loss 9.106983611673058e-08 BETTER
I0328 19:36:25.812116 2316861 finetune.py:68] layer 0_o @ epoch 3 new loss 4.361871219771274e-08 old loss 4.3703995089572345e-08 BETTER
I0328 19:36:26.825031 2316933 finetune.py:68] layer 1_o @ epoch 2 new loss 7.42452428426077e-08 old loss 7.460955231408661e-08 BETTER
I0328 19:36:43.187802 2317005 finetune.py:68] layer 2_o @ epoch 0 new loss 1.18868598519839e-07 old loss 1.1973411062626838e-07 BETTER
I0328 19:36:58.170290 2317077 finetune.py:68] layer 3_k @ epoch 4 new loss 8.933638184771553e-08 old loss 9.016240909431872e-08 BETTER
I0328 19:37:00.604583 2316933 finetune.py:68] layer 1_o @ epoch 3 new loss 7.394049106324019e-08 old loss 7.42452428426077e-08 BETTER
I0328 19:37:01.514414 2316861 finetune.py:68] layer 0_o @ epoch 4 new loss 4.353879390350812e-08 old loss 4.361871219771274e-08 BETTER
I0328 19:37:16.903716 2317005 finetune.py:68] layer 2_o @ epoch 1 new loss 1.1827258816765607e-07 old loss 1.18868598519839e-07 BETTER
I0328 19:37:17.660601 2317077 finetune.py:45] layer 3_o initial loss 1.8539488166879892e-07
I0328 19:37:32.936631 2316861 finetune.py:45] layer 0_up initial loss 5.551392590064097e-08
I0328 19:37:34.257282 2316933 finetune.py:76] layer 1_o @ epoch 4 new loss 7.547577496325175e-08 old loss 7.394049106324019e-08 WORSE
I0328 19:37:49.471168 2317077 finetune.py:68] layer 3_o @ epoch 0 new loss 1.8379834898496483e-07 old loss 1.8539488166879892e-07 BETTER
I0328 19:37:50.865721 2317005 finetune.py:68] layer 2_o @ epoch 2 new loss 1.1777129316214996e-07 old loss 1.1827258816765607e-07 BETTER
I0328 19:38:04.701293 2316933 finetune.py:45] layer 1_up initial loss 1.4178752394400362e-07
I0328 19:38:04.725286 2316861 finetune.py:68] layer 0_up @ epoch 0 new loss 5.5446385260893294e-08 old loss 5.551392590064097e-08 BETTER
I0328 19:38:22.677911 2317077 finetune.py:68] layer 3_o @ epoch 1 new loss 1.8276375612913398e-07 old loss 1.8379834898496483e-07 BETTER
I0328 19:38:24.761291 2317005 finetune.py:68] layer 2_o @ epoch 3 new loss 1.1731049909258218e-07 old loss 1.1777129316214996e-07 BETTER
I0328 19:38:35.057660 2316933 finetune.py:68] layer 1_up @ epoch 0 new loss 1.0264733418807737e-07 old loss 1.4178752394400362e-07 BETTER
I0328 19:38:37.864896 2316861 finetune.py:68] layer 0_up @ epoch 1 new loss 5.538331748766723e-08 old loss 5.5446385260893294e-08 BETTER
I0328 19:38:55.929529 2317077 finetune.py:68] layer 3_o @ epoch 2 new loss 1.8188812589414738e-07 old loss 1.8276375612913398e-07 BETTER
I0328 19:38:58.810293 2317005 finetune.py:68] layer 2_o @ epoch 4 new loss 1.1688619139249568e-07 old loss 1.1731049909258218e-07 BETTER
I0328 19:39:06.449672 2316933 finetune.py:68] layer 1_up @ epoch 1 new loss 1.0195221733511062e-07 old loss 1.0264733418807737e-07 BETTER
I0328 19:39:11.568429 2316861 finetune.py:68] layer 0_up @ epoch 2 new loss 5.532263003260596e-08 old loss 5.538331748766723e-08 BETTER
I0328 19:39:29.326346 2317077 finetune.py:68] layer 3_o @ epoch 3 new loss 1.8108100618974277e-07 old loss 1.8188812589414738e-07 BETTER
I0328 19:39:30.434911 2317005 finetune.py:45] layer 2_up initial loss 1.8515608246616466e-07
I0328 19:39:38.350706 2316933 finetune.py:68] layer 1_up @ epoch 2 new loss 1.0145316053922215e-07 old loss 1.0195221733511062e-07 BETTER
I0328 19:39:44.923312 2316861 finetune.py:68] layer 0_up @ epoch 3 new loss 5.526551305479188e-08 old loss 5.532263003260596e-08 BETTER
I0328 19:40:01.034359 2317005 finetune.py:68] layer 2_up @ epoch 0 new loss 1.8471412488452188e-07 old loss 1.8515608246616466e-07 BETTER
I0328 19:40:02.614391 2317077 finetune.py:68] layer 3_o @ epoch 4 new loss 1.8033408366591175e-07 old loss 1.8108100618974277e-07 BETTER
I0328 19:40:10.105903 2316933 finetune.py:68] layer 1_up @ epoch 3 new loss 1.0126537119958812e-07 old loss 1.0145316053922215e-07 BETTER
I0328 19:40:18.221431 2316861 finetune.py:68] layer 0_up @ epoch 4 new loss 5.5208726479349934e-08 old loss 5.526551305479188e-08 BETTER
I0328 19:40:32.881923 2317005 finetune.py:68] layer 2_up @ epoch 1 new loss 1.843162920067698e-07 old loss 1.8471412488452188e-07 BETTER
I0328 19:40:33.415630 2317077 finetune.py:45] layer 3_up initial loss 3.3496874607408245e-07
I0328 19:40:41.872221 2316933 finetune.py:68] layer 1_up @ epoch 4 new loss 1.0107272174764148e-07 old loss 1.0126537119958812e-07 BETTER
I0328 19:40:49.513929 2316861 finetune.py:45] layer 0_gate initial loss 6.314346023827966e-08
I0328 19:41:03.549962 2317077 finetune.py:68] layer 3_up @ epoch 0 new loss 3.3384958442184143e-07 old loss 3.3496874607408245e-07 BETTER
I0328 19:41:04.990660 2317005 finetune.py:68] layer 2_up @ epoch 2 new loss 1.8393777168057568e-07 old loss 1.843162920067698e-07 BETTER
I0328 19:41:13.301955 2316933 finetune.py:45] layer 1_gate initial loss 1.242559619640815e-07
I0328 19:41:19.363994 2316861 finetune.py:68] layer 0_gate @ epoch 0 new loss 6.310168032541696e-08 old loss 6.314346023827966e-08 BETTER
I0328 19:41:34.764700 2317077 finetune.py:68] layer 3_up @ epoch 1 new loss 3.329050457523408e-07 old loss 3.3384958442184143e-07 BETTER
I0328 19:41:37.241077 2317005 finetune.py:68] layer 2_up @ epoch 3 new loss 1.835834098073974e-07 old loss 1.8393777168057568e-07 BETTER
I0328 19:41:41.763172 2316933 finetune.py:68] layer 1_gate @ epoch 0 new loss 1.2101953927867726e-07 old loss 1.242559619640815e-07 BETTER
I0328 19:41:50.362323 2316861 finetune.py:68] layer 0_gate @ epoch 1 new loss 6.305656796712356e-08 old loss 6.310168032541696e-08 BETTER
I0328 19:42:05.992270 2317077 finetune.py:68] layer 3_up @ epoch 2 new loss 3.320286623420543e-07 old loss 3.329050457523408e-07 BETTER
I0328 19:42:09.609869 2317005 finetune.py:68] layer 2_up @ epoch 4 new loss 1.8324074346764974e-07 old loss 1.835834098073974e-07 BETTER
I0328 19:42:10.989412 2316933 finetune.py:68] layer 1_gate @ epoch 1 new loss 1.2085124012628512e-07 old loss 1.2101953927867726e-07 BETTER
I0328 19:42:21.563201 2316861 finetune.py:68] layer 0_gate @ epoch 2 new loss 6.300661681279962e-08 old loss 6.305656796712356e-08 BETTER
I0328 19:42:37.330819 2317077 finetune.py:68] layer 3_up @ epoch 3 new loss 3.3118047326752276e-07 old loss 3.320286623420543e-07 BETTER
I0328 19:42:40.380511 2316933 finetune.py:68] layer 1_gate @ epoch 2 new loss 1.2075832955815713e-07 old loss 1.2085124012628512e-07 BETTER
I0328 19:42:41.197358 2317005 finetune.py:45] layer 2_gate initial loss 2.2871074634167599e-07
I0328 19:42:52.935103 2316861 finetune.py:68] layer 0_gate @ epoch 3 new loss 6.296424714946625e-08 old loss 6.300661681279962e-08 BETTER
I0328 19:43:08.779614 2317077 finetune.py:68] layer 3_up @ epoch 4 new loss 3.303869675619353e-07 old loss 3.3118047326752276e-07 BETTER
I0328 19:43:09.919077 2317005 finetune.py:68] layer 2_gate @ epoch 0 new loss 2.284135973695811e-07 old loss 2.2871074634167599e-07 BETTER
I0328 19:43:10.069070 2316933 finetune.py:76] layer 1_gate @ epoch 3 new loss 1.2374542279758316e-07 old loss 1.2075832955815713e-07 WORSE
I0328 19:43:24.359447 2316861 finetune.py:68] layer 0_gate @ epoch 4 new loss 6.292678023100962e-08 old loss 6.296424714946625e-08 BETTER
I0328 19:43:39.070010 2316933 finetune.py:68] layer 1_gate @ epoch 4 new loss 1.2052454678723734e-07 old loss 1.2075832955815713e-07 BETTER
I0328 19:43:39.515336 2317005 finetune.py:68] layer 2_gate @ epoch 1 new loss 2.2814991496034054e-07 old loss 2.284135973695811e-07 BETTER
I0328 19:43:39.840440 2317077 finetune.py:45] layer 3_gate initial loss 4.09748679430777e-07
I0328 19:44:07.892006 2317077 finetune.py:68] layer 3_gate @ epoch 0 new loss 4.0899979580899526e-07 old loss 4.09748679430777e-07 BETTER
I0328 19:44:09.103774 2317005 finetune.py:68] layer 2_gate @ epoch 2 new loss 2.2789427589486877e-07 old loss 2.2814991496034054e-07 BETTER
I0328 19:44:19.992561 2316861 finetune.py:45] layer 0_down initial loss 9.391762034738349e-08
I0328 19:44:35.146243 2316933 finetune.py:45] layer 1_down initial loss 2.0545434153973474e-07
I0328 19:44:37.247346 2317077 finetune.py:68] layer 3_gate @ epoch 1 new loss 4.083278781763511e-07 old loss 4.0899979580899526e-07 BETTER
I0328 19:44:38.990637 2317005 finetune.py:68] layer 2_gate @ epoch 3 new loss 2.276468649142771e-07 old loss 2.2789427589486877e-07 BETTER
I0328 19:44:47.207971 2316861 finetune.py:68] layer 0_down @ epoch 0 new loss 9.388739385940426e-08 old loss 9.391762034738349e-08 BETTER
I0328 19:45:01.353620 2316933 finetune.py:76] layer 1_down @ epoch 0 new loss 2.078566438967755e-07 old loss 2.0545434153973474e-07 WORSE
I0328 19:45:06.426967 2317077 finetune.py:68] layer 3_gate @ epoch 2 new loss 4.0767397990748577e-07 old loss 4.083278781763511e-07 BETTER
I0328 19:45:08.798312 2317005 finetune.py:68] layer 2_gate @ epoch 4 new loss 2.2740275085197936e-07 old loss 2.276468649142771e-07 BETTER
I0328 19:45:15.878498 2316861 finetune.py:68] layer 0_down @ epoch 1 new loss 9.38802315886278e-08 old loss 9.388739385940426e-08 BETTER
I0328 19:45:27.810511 2316933 finetune.py:68] layer 1_down @ epoch 1 new loss 2.0508873888047674e-07 old loss 2.0545434153973474e-07 BETTER
I0328 19:45:35.717022 2317077 finetune.py:68] layer 3_gate @ epoch 3 new loss 4.070585077897704e-07 old loss 4.0767397990748577e-07 BETTER
I0328 19:45:44.627013 2316861 finetune.py:68] layer 0_down @ epoch 2 new loss 9.387368038460409e-08 old loss 9.38802315886278e-08 BETTER
I0328 19:45:55.235248 2316933 finetune.py:68] layer 1_down @ epoch 2 new loss 2.0500692698988132e-07 old loss 2.0508873888047674e-07 BETTER
I0328 19:46:04.977368 2317077 finetune.py:68] layer 3_gate @ epoch 4 new loss 4.06460799240449e-07 old loss 4.070585077897704e-07 BETTER
I0328 19:46:05.771921 2317005 finetune.py:45] layer 2_down initial loss 3.4609877275215695e-07
I0328 19:46:13.593835 2316861 finetune.py:68] layer 0_down @ epoch 3 new loss 9.386696575575115e-08 old loss 9.387368038460409e-08 BETTER
I0328 19:46:22.845334 2316933 finetune.py:68] layer 1_down @ epoch 3 new loss 2.0492974783792306e-07 old loss 2.0500692698988132e-07 BETTER
I0328 19:46:31.990931 2317005 finetune.py:68] layer 2_down @ epoch 0 new loss 3.460742448169185e-07 old loss 3.4609877275215695e-07 BETTER
I0328 19:46:42.482712 2316861 finetune.py:68] layer 0_down @ epoch 4 new loss 9.385910004766629e-08 old loss 9.386696575575115e-08 BETTER
0_v proxy err 0.0012924019247293472 tr(WHW.T) 60.88684844970703
bpp_loss 5.405414399690926
0_q proxy err 7.107397323125042e-06 tr(WHW.T) 288093.59375
bpp_loss 6.3259209882235155
0_k proxy err 1.1563714906515088e-05 tr(WHW.T) 100183.4140625
bpp_loss 7.116586737683974
0_o proxy err 0.00017685562488622963 tr(WHW.T) 3117.7763671875
bpp_loss 5.48617488250602
0_up proxy err 0.0003085896314587444 tr(WHW.T) 8924.599609375
bpp_loss 5.767622471121805
0_gate proxy err 0.00018212926806882024 tr(WHW.T) 15778.947265625
bpp_loss 5.8798490505931635
0_down proxy err 0.00027009559562429786 tr(WHW.T) 10819.8115234375
bpp_loss 5.763396912088085
I0328 19:46:51.040899 2316933 finetune.py:68] layer 1_down @ epoch 4 new loss 2.0488516838668147e-07 old loss 2.0492974783792306e-07 BETTER
1_v proxy err 0.0005146187031641603 tr(WHW.T) 109.07096099853516
bpp_loss 5.507113839499652
1_q proxy err 3.0682153919769917e-06 tr(WHW.T) 144802.75
bpp_loss 6.62660840107128
1_k proxy err 2.6963948585034814e-06 tr(WHW.T) 75513.734375
bpp_loss 7.646916681085713
1_o proxy err 0.00029904107213951647 tr(WHW.T) 1985.046875
bpp_loss 5.569419225677848
1_up proxy err 0.0003561651101335883 tr(WHW.T) 8230.8994140625
bpp_loss 5.782294036421392
1_gate proxy err 0.00021877980907447636 tr(WHW.T) 13951.322265625
bpp_loss 5.892865772758212
1_down proxy err 1.0381927495473064e-05 tr(WHW.T) 13984.984375
bpp_loss 5.778372237159472
I0328 19:46:59.701315 2317005 finetune.py:68] layer 2_down @ epoch 1 new loss 3.460567370439094e-07 old loss 3.460742448169185e-07 BETTER
I0328 19:47:01.387787 2317077 finetune.py:45] layer 3_down initial loss 6.423624085982738e-07
I0328 19:47:27.210626 2317077 finetune.py:68] layer 3_down @ epoch 0 new loss 6.422719707188662e-07 old loss 6.423624085982738e-07 BETTER
I0328 19:47:27.433658 2317005 finetune.py:68] layer 2_down @ epoch 2 new loss 3.460502853158687e-07 old loss 3.460567370439094e-07 BETTER
I0328 19:47:54.102658 2317077 finetune.py:68] layer 3_down @ epoch 1 new loss 6.422327487598523e-07 old loss 6.422719707188662e-07 BETTER
I0328 19:47:55.091382 2317005 finetune.py:68] layer 2_down @ epoch 3 new loss 3.4603681342559867e-07 old loss 3.460502853158687e-07 BETTER
I0328 19:48:05.727757 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 68.16274356842041s
I0328 19:48:09.566677 2317147 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:48:09.566782 2317147 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:48:09.566826 2317147 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:48:09.937605 2317147 config.py:54] PyTorch version 2.6.0 available.
W0328 19:48:10.160549 2317147 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 19:48:10.755276 2317147 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 19:48:10.759159 2316731 quantize_finetune_llama.py:209] layer 5 gpu 1
I0328 19:48:10.773360 2317147 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 19:48:21.138067 2317077 finetune.py:68] layer 3_down @ epoch 2 new loss 6.422200158340274e-07 old loss 6.422327487598523e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 19:48:22.918893 2317005 finetune.py:68] layer 2_down @ epoch 4 new loss 3.460265531884943e-07 old loss 3.4603681342559867e-07 BETTER
2_v proxy err 0.0007287510670721531 tr(WHW.T) 155.95950317382812
bpp_loss 5.41510351782199
2_q proxy err 1.8047810954158194e-05 tr(WHW.T) 41472.88671875
bpp_loss 6.515789668890648
2_k proxy err 1.485499433329096e-05 tr(WHW.T) 22612.5625
bpp_loss 7.6727712523425
2_o proxy err 0.0002953101065941155 tr(WHW.T) 1967.20849609375
bpp_loss 5.515389684587717
2_up proxy err 0.00041703012539073825 tr(WHW.T) 7600.77490234375
bpp_loss 5.776711465657821
2_gate proxy err 0.00022179381630849093 tr(WHW.T) 15114.3642578125
bpp_loss 5.928242514708212
2_down proxy err 0.0004098907229490578 tr(WHW.T) 7727.49560546875
bpp_loss 5.780187875564609
I0328 19:48:27.963392 2317147 finetune.py:45] layer 4_v initial loss 5.011532948628883e-07
W0328 19:48:27.963586 2317147 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 19:48:48.230729 2317077 finetune.py:68] layer 3_down @ epoch 3 new loss 6.421943226087024e-07 old loss 6.422200158340274e-07 BETTER
I0328 19:49:02.615185 2317147 finetune.py:68] layer 4_v @ epoch 0 new loss 1.5996860724953876e-07 old loss 5.011532948628883e-07 BETTER
I0328 19:49:15.341825 2317077 finetune.py:68] layer 3_down @ epoch 4 new loss 6.421764737751801e-07 old loss 6.421943226087024e-07 BETTER
3_v proxy err 0.0005188498180359602 tr(WHW.T) 289.3331604003906
bpp_loss 5.509653848828748
3_q proxy err 1.873928340501152e-05 tr(WHW.T) 47577.1953125
bpp_loss 6.547337501193397
3_k proxy err 1.2784561477019452e-05 tr(WHW.T) 26170.611328125
bpp_loss 7.745508268359117
3_o proxy err 0.0003420772554818541 tr(WHW.T) 1857.6778564453125
bpp_loss 5.6115336056100205
3_up proxy err 0.0004126488056499511 tr(WHW.T) 7536.35302734375
bpp_loss 5.757475411253316
3_gate proxy err 0.00016317478730343282 tr(WHW.T) 20885.90234375
bpp_loss 6.009652950934002
3_down proxy err 0.0004617756640072912 tr(WHW.T) 6999.7841796875
bpp_loss 5.755245922465941
I0328 19:49:33.433933 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 64.49900388717651s
I0328 19:49:37.340773 2317217 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:49:37.340867 2317217 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:49:37.340909 2317217 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:49:37.721405 2317217 config.py:54] PyTorch version 2.6.0 available.
W0328 19:49:37.942432 2317217 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 19:49:38.571233 2317217 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 19:49:38.575075 2316731 quantize_finetune_llama.py:209] layer 6 gpu 2
I0328 19:49:38.588935 2317217 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 19:49:38.722801 2317147 finetune.py:68] layer 4_v @ epoch 1 new loss 1.266227798168984e-07 old loss 1.5996860724953876e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 19:49:55.363801 2317217 finetune.py:45] layer 5_v initial loss 5.774738838226767e-07
W0328 19:49:55.363997 2317217 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 19:50:15.157964 2317147 finetune.py:68] layer 4_v @ epoch 2 new loss 1.1354874374092105e-07 old loss 1.266227798168984e-07 BETTER
I0328 19:50:28.241176 2317217 finetune.py:68] layer 5_v @ epoch 0 new loss 2.0377899545565015e-07 old loss 5.774738838226767e-07 BETTER
I0328 19:50:41.593526 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 62.58177852630615s
I0328 19:50:45.366491 2317287 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:50:45.366590 2317287 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:50:45.366633 2317287 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:50:45.720383 2317287 config.py:54] PyTorch version 2.6.0 available.
W0328 19:50:45.917994 2317287 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 19:50:46.571851 2317287 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 19:50:46.576081 2316731 quantize_finetune_llama.py:209] layer 7 gpu 3
I0328 19:50:46.605264 2317287 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 19:50:51.856995 2317147 finetune.py:68] layer 4_v @ epoch 3 new loss 1.0684759388368548e-07 old loss 1.1354874374092105e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 19:51:02.349699 2317217 finetune.py:68] layer 5_v @ epoch 1 new loss 1.712026289624191e-07 old loss 2.0377899545565015e-07 BETTER
I0328 19:51:04.053956 2317287 finetune.py:45] layer 6_v initial loss 4.5856631913920864e-07
W0328 19:51:04.054144 2317287 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 19:51:28.756561 2317147 finetune.py:68] layer 4_v @ epoch 4 new loss 1.0324366428449139e-07 old loss 1.0684759388368548e-07 BETTER
I0328 19:51:36.833426 2317217 finetune.py:68] layer 5_v @ epoch 2 new loss 1.5850599766054074e-07 old loss 1.712026289624191e-07 BETTER
I0328 19:51:37.291291 2317287 finetune.py:68] layer 6_v @ epoch 0 new loss 2.1165725172522798e-07 old loss 4.5856631913920864e-07 BETTER
I0328 19:51:48.076233 2317147 finetune.py:45] layer 4_q initial loss 1.3019355549204192e-07
I0328 19:51:50.557841 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 63.45749258995056s
I0328 19:51:54.448989 2317357 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:51:54.449089 2317357 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:51:54.449132 2317357 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:51:54.830901 2317357 config.py:54] PyTorch version 2.6.0 available.
W0328 19:51:55.063611 2317357 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 19:51:55.698644 2317357 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 19:51:55.702368 2316731 quantize_finetune_llama.py:209] layer 8 gpu 0
I0328 19:51:55.717513 2317357 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 19:52:11.449315 2317217 finetune.py:68] layer 5_v @ epoch 3 new loss 1.5241910489294241e-07 old loss 1.5850599766054074e-07 BETTER
I0328 19:52:11.760635 2317287 finetune.py:68] layer 6_v @ epoch 1 new loss 1.9227012160172308e-07 old loss 2.1165725172522798e-07 BETTER
I0328 19:52:13.345772 2317357 finetune.py:45] layer 7_v initial loss 4.4507490315481846e-07
W0328 19:52:13.345998 2317357 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 19:52:23.244109 2317147 finetune.py:68] layer 4_q @ epoch 0 new loss 1.2287378581277153e-07 old loss 1.3019355549204192e-07 BETTER
I0328 19:52:46.148502 2317357 finetune.py:68] layer 7_v @ epoch 0 new loss 2.4906702833504824e-07 old loss 4.4507490315481846e-07 BETTER
I0328 19:52:46.425902 2317217 finetune.py:68] layer 5_v @ epoch 4 new loss 1.4896723143920099e-07 old loss 1.5241910489294241e-07 BETTER
I0328 19:52:46.515430 2317287 finetune.py:68] layer 6_v @ epoch 2 new loss 1.838272112308914e-07 old loss 1.9227012160172308e-07 BETTER
I0328 19:52:59.317129 2317147 finetune.py:68] layer 4_q @ epoch 1 new loss 1.1895372864501041e-07 old loss 1.2287378581277153e-07 BETTER
I0328 19:53:05.642863 2317217 finetune.py:45] layer 5_q initial loss 1.8658424494333303e-07
I0328 19:53:20.142390 2317357 finetune.py:68] layer 7_v @ epoch 1 new loss 2.3371462987142877e-07 old loss 2.4906702833504824e-07 BETTER
I0328 19:53:21.256903 2317287 finetune.py:68] layer 6_v @ epoch 3 new loss 1.7925877671132184e-07 old loss 1.838272112308914e-07 BETTER
I0328 19:53:35.695874 2317147 finetune.py:68] layer 4_q @ epoch 2 new loss 1.1621970941178006e-07 old loss 1.1895372864501041e-07 BETTER
I0328 19:53:38.841570 2317217 finetune.py:68] layer 5_q @ epoch 0 new loss 1.779513212341044e-07 old loss 1.8658424494333303e-07 BETTER
I0328 19:53:54.474024 2317357 finetune.py:68] layer 7_v @ epoch 2 new loss 2.2760383444619947e-07 old loss 2.3371462987142877e-07 BETTER
I0328 19:53:56.312707 2317287 finetune.py:68] layer 6_v @ epoch 4 new loss 1.7514484795810858e-07 old loss 1.7925877671132184e-07 BETTER
I0328 19:54:12.436231 2317147 finetune.py:68] layer 4_q @ epoch 3 new loss 1.1403230359974259e-07 old loss 1.1621970941178006e-07 BETTER
I0328 19:54:13.073637 2317217 finetune.py:68] layer 5_q @ epoch 1 new loss 1.739485071539093e-07 old loss 1.779513212341044e-07 BETTER
I0328 19:54:16.143357 2317287 finetune.py:45] layer 6_q initial loss 2.1983215958698565e-07
I0328 19:54:28.944125 2317357 finetune.py:68] layer 7_v @ epoch 3 new loss 2.2388361742287088e-07 old loss 2.2760383444619947e-07 BETTER
I0328 19:54:47.710642 2317217 finetune.py:68] layer 5_q @ epoch 2 new loss 1.7103731408951717e-07 old loss 1.739485071539093e-07 BETTER
I0328 19:54:49.293504 2317147 finetune.py:68] layer 4_q @ epoch 4 new loss 1.1218549644809173e-07 old loss 1.1403230359974259e-07 BETTER
I0328 19:54:50.019973 2317287 finetune.py:68] layer 6_q @ epoch 0 new loss 2.1373602976382244e-07 old loss 2.1983215958698565e-07 BETTER
I0328 19:55:03.583372 2317357 finetune.py:76] layer 7_v @ epoch 4 new loss 2.356886881216269e-07 old loss 2.2388361742287088e-07 WORSE
I0328 19:55:07.489501 2317147 finetune.py:45] layer 4_k initial loss 1.3266887322060938e-07
I0328 19:55:22.300919 2317217 finetune.py:68] layer 5_q @ epoch 3 new loss 1.6881357112197293e-07 old loss 1.7103731408951717e-07 BETTER
I0328 19:55:22.563555 2317357 finetune.py:45] layer 7_q initial loss 2.899207345308241e-07
I0328 19:55:24.469533 2317287 finetune.py:68] layer 6_q @ epoch 1 new loss 2.094515139106079e-07 old loss 2.1373602976382244e-07 BETTER
I0328 19:55:42.766358 2317147 finetune.py:68] layer 4_k @ epoch 0 new loss 1.2786257741481677e-07 old loss 1.3266887322060938e-07 BETTER
I0328 19:55:55.399222 2317357 finetune.py:68] layer 7_q @ epoch 0 new loss 2.79889093235397e-07 old loss 2.899207345308241e-07 BETTER
I0328 19:55:57.173042 2317217 finetune.py:68] layer 5_q @ epoch 4 new loss 1.6693324766947626e-07 old loss 1.6881357112197293e-07 BETTER
I0328 19:55:59.188842 2317287 finetune.py:68] layer 6_q @ epoch 2 new loss 2.068833282464766e-07 old loss 2.094515139106079e-07 BETTER
I0328 19:56:14.868288 2317217 finetune.py:45] layer 5_k initial loss 1.9482483537558437e-07
I0328 19:56:18.923003 2317147 finetune.py:68] layer 4_k @ epoch 1 new loss 1.261370528027328e-07 old loss 1.2786257741481677e-07 BETTER
I0328 19:56:29.415483 2317357 finetune.py:68] layer 7_q @ epoch 1 new loss 2.7943480063186144e-07 old loss 2.79889093235397e-07 BETTER
I0328 19:56:33.824693 2317287 finetune.py:68] layer 6_q @ epoch 3 new loss 2.0399953370997537e-07 old loss 2.068833282464766e-07 BETTER
I0328 19:56:48.136281 2317217 finetune.py:68] layer 5_k @ epoch 0 new loss 1.8702395720993081e-07 old loss 1.9482483537558437e-07 BETTER
I0328 19:56:55.175196 2317147 finetune.py:68] layer 4_k @ epoch 2 new loss 1.247848757657266e-07 old loss 1.261370528027328e-07 BETTER
I0328 19:57:03.337243 2317357 finetune.py:68] layer 7_q @ epoch 2 new loss 2.721393741467182e-07 old loss 2.7943480063186144e-07 BETTER
I0328 19:57:08.532912 2317287 finetune.py:68] layer 6_q @ epoch 4 new loss 2.020658342871684e-07 old loss 2.0399953370997537e-07 BETTER
I0328 19:57:22.250695 2317217 finetune.py:68] layer 5_k @ epoch 1 new loss 1.8478003482869099e-07 old loss 1.8702395720993081e-07 BETTER
I0328 19:57:26.566350 2317287 finetune.py:45] layer 6_k initial loss 2.254852233818383e-07
I0328 19:57:31.553000 2317147 finetune.py:68] layer 4_k @ epoch 3 new loss 1.2374958657801471e-07 old loss 1.247848757657266e-07 BETTER
I0328 19:57:37.286990 2317357 finetune.py:68] layer 7_q @ epoch 3 new loss 2.710212925194355e-07 old loss 2.721393741467182e-07 BETTER
I0328 19:57:56.564882 2317217 finetune.py:68] layer 5_k @ epoch 2 new loss 1.8327050099742337e-07 old loss 1.8478003482869099e-07 BETTER
I0328 19:57:59.982295 2317287 finetune.py:68] layer 6_k @ epoch 0 new loss 2.199226827315215e-07 old loss 2.254852233818383e-07 BETTER
I0328 19:58:08.115173 2317147 finetune.py:68] layer 4_k @ epoch 4 new loss 1.2276423433377204e-07 old loss 1.2374958657801471e-07 BETTER
I0328 19:58:11.259335 2317357 finetune.py:68] layer 7_q @ epoch 4 new loss 2.673601215974486e-07 old loss 2.710212925194355e-07 BETTER
I0328 19:58:27.451186 2317147 finetune.py:45] layer 4_o initial loss 2.581868443485291e-07
I0328 19:58:28.895330 2317357 finetune.py:45] layer 7_k initial loss 2.9721724104092573e-07
I0328 19:58:30.784561 2317217 finetune.py:68] layer 5_k @ epoch 3 new loss 1.819168460315268e-07 old loss 1.8327050099742337e-07 BETTER
I0328 19:58:34.274891 2317287 finetune.py:68] layer 6_k @ epoch 1 new loss 2.1780103054425126e-07 old loss 2.199226827315215e-07 BETTER
I0328 19:59:01.962997 2317357 finetune.py:68] layer 7_k @ epoch 0 new loss 2.910523733135051e-07 old loss 2.9721724104092573e-07 BETTER
I0328 19:59:02.562809 2317147 finetune.py:68] layer 4_o @ epoch 0 new loss 2.5493460498182685e-07 old loss 2.581868443485291e-07 BETTER
I0328 19:59:05.418600 2317217 finetune.py:68] layer 5_k @ epoch 4 new loss 1.807450757951301e-07 old loss 1.819168460315268e-07 BETTER
I0328 19:59:08.811311 2317287 finetune.py:68] layer 6_k @ epoch 2 new loss 2.163881163141923e-07 old loss 2.1780103054425126e-07 BETTER
I0328 19:59:24.887191 2317217 finetune.py:45] layer 5_o initial loss 3.540312718541827e-07
I0328 19:59:35.690913 2317357 finetune.py:76] layer 7_k @ epoch 1 new loss 2.9299718562469934e-07 old loss 2.910523733135051e-07 WORSE
I0328 19:59:38.231956 2317147 finetune.py:68] layer 4_o @ epoch 1 new loss 2.528306026761129e-07 old loss 2.5493460498182685e-07 BETTER
I0328 19:59:43.275712 2317287 finetune.py:68] layer 6_k @ epoch 3 new loss 2.1621551127282146e-07 old loss 2.163881163141923e-07 BETTER
I0328 19:59:57.511542 2317217 finetune.py:68] layer 5_o @ epoch 0 new loss 3.48962515772655e-07 old loss 3.540312718541827e-07 BETTER
I0328 20:00:08.842344 2317357 finetune.py:68] layer 7_k @ epoch 2 new loss 2.8686847031167417e-07 old loss 2.910523733135051e-07 BETTER
I0328 20:00:14.166811 2317147 finetune.py:68] layer 4_o @ epoch 2 new loss 2.5101462597376667e-07 old loss 2.528306026761129e-07 BETTER
I0328 20:00:17.844263 2317287 finetune.py:68] layer 6_k @ epoch 4 new loss 2.1428252239275025e-07 old loss 2.1621551127282146e-07 BETTER
I0328 20:00:31.096108 2317217 finetune.py:68] layer 5_o @ epoch 1 new loss 3.45807364965367e-07 old loss 3.48962515772655e-07 BETTER
I0328 20:00:37.267988 2317287 finetune.py:45] layer 6_o initial loss 4.693018240686797e-07
I0328 20:00:42.793117 2317357 finetune.py:68] layer 7_k @ epoch 3 new loss 2.853149680959177e-07 old loss 2.8686847031167417e-07 BETTER
I0328 20:00:50.161854 2317147 finetune.py:68] layer 4_o @ epoch 3 new loss 2.494873854175239e-07 old loss 2.5101462597376667e-07 BETTER
I0328 20:01:05.002382 2317217 finetune.py:68] layer 5_o @ epoch 2 new loss 3.4326782838434156e-07 old loss 3.45807364965367e-07 BETTER
I0328 20:01:09.998986 2317287 finetune.py:68] layer 6_o @ epoch 0 new loss 4.616155990788684e-07 old loss 4.693018240686797e-07 BETTER
I0328 20:01:16.638582 2317357 finetune.py:68] layer 7_k @ epoch 4 new loss 2.808614851801394e-07 old loss 2.853149680959177e-07 BETTER
I0328 20:01:26.228800 2317147 finetune.py:68] layer 4_o @ epoch 4 new loss 2.4798902131806244e-07 old loss 2.494873854175239e-07 BETTER
I0328 20:01:35.718897 2317357 finetune.py:45] layer 7_o initial loss 6.114138955126691e-07
I0328 20:01:38.661309 2317217 finetune.py:68] layer 5_o @ epoch 3 new loss 3.4108927593479166e-07 old loss 3.4326782838434156e-07 BETTER
I0328 20:01:43.971446 2317287 finetune.py:68] layer 6_o @ epoch 1 new loss 4.5600523890243494e-07 old loss 4.616155990788684e-07 BETTER
I0328 20:01:57.336786 2317147 finetune.py:45] layer 4_up initial loss 5.356516794563504e-07
I0328 20:02:07.861459 2317357 finetune.py:68] layer 7_o @ epoch 0 new loss 5.949542014604958e-07 old loss 6.114138955126691e-07 BETTER
I0328 20:02:12.278083 2317217 finetune.py:68] layer 5_o @ epoch 4 new loss 3.391882330561202e-07 old loss 3.4108927593479166e-07 BETTER
I0328 20:02:17.689117 2317287 finetune.py:68] layer 6_o @ epoch 2 new loss 4.519858123330778e-07 old loss 4.5600523890243494e-07 BETTER
I0328 20:02:29.231750 2317147 finetune.py:68] layer 4_up @ epoch 0 new loss 5.321001026459271e-07 old loss 5.356516794563504e-07 BETTER
I0328 20:02:41.114293 2317357 finetune.py:68] layer 7_o @ epoch 1 new loss 5.853724474036426e-07 old loss 5.949542014604958e-07 BETTER
I0328 20:02:43.596691 2317217 finetune.py:45] layer 5_up initial loss 7.568946216451877e-07
I0328 20:02:51.469168 2317287 finetune.py:68] layer 6_o @ epoch 3 new loss 4.485380031837849e-07 old loss 4.519858123330778e-07 BETTER
I0328 20:03:02.218795 2317147 finetune.py:68] layer 4_up @ epoch 1 new loss 5.29195347098721e-07 old loss 5.321001026459271e-07 BETTER
I0328 20:03:14.001003 2317217 finetune.py:68] layer 5_up @ epoch 0 new loss 7.498930472138454e-07 old loss 7.568946216451877e-07 BETTER
I0328 20:03:14.342337 2317357 finetune.py:68] layer 7_o @ epoch 2 new loss 5.788112957816338e-07 old loss 5.853724474036426e-07 BETTER
I0328 20:03:25.751997 2317287 finetune.py:68] layer 6_o @ epoch 4 new loss 4.455008308923425e-07 old loss 4.485380031837849e-07 BETTER
I0328 20:03:35.715886 2317147 finetune.py:68] layer 4_up @ epoch 2 new loss 5.265145546218264e-07 old loss 5.29195347098721e-07 BETTER
I0328 20:03:45.485919 2317217 finetune.py:68] layer 5_up @ epoch 1 new loss 7.44405781460955e-07 old loss 7.498930472138454e-07 BETTER
I0328 20:03:47.659773 2317357 finetune.py:68] layer 7_o @ epoch 3 new loss 5.732969725613657e-07 old loss 5.788112957816338e-07 BETTER
I0328 20:03:57.002953 2317287 finetune.py:45] layer 6_up initial loss 9.769333928488777e-07
I0328 20:04:09.329937 2317147 finetune.py:68] layer 4_up @ epoch 3 new loss 5.240215159574291e-07 old loss 5.265145546218264e-07 BETTER
I0328 20:04:17.165202 2317217 finetune.py:68] layer 5_up @ epoch 2 new loss 7.394201020360924e-07 old loss 7.44405781460955e-07 BETTER
I0328 20:04:20.980963 2317357 finetune.py:68] layer 7_o @ epoch 4 new loss 5.687036832568992e-07 old loss 5.732969725613657e-07 BETTER
I0328 20:04:27.596904 2317287 finetune.py:68] layer 6_up @ epoch 0 new loss 9.65217623161152e-07 old loss 9.769333928488777e-07 BETTER
I0328 20:04:43.081812 2317147 finetune.py:68] layer 4_up @ epoch 4 new loss 5.216784870754054e-07 old loss 5.240215159574291e-07 BETTER
I0328 20:04:49.141998 2317217 finetune.py:68] layer 5_up @ epoch 3 new loss 7.349016755142657e-07 old loss 7.394201020360924e-07 BETTER
I0328 20:04:52.363956 2317357 finetune.py:45] layer 7_up initial loss 1.1480877901703934e-06
I0328 20:04:59.429820 2317287 finetune.py:68] layer 6_up @ epoch 1 new loss 9.55910991251585e-07 old loss 9.65217623161152e-07 BETTER
I0328 20:05:14.425304 2317147 finetune.py:45] layer 4_gate initial loss 6.373795145009353e-07
I0328 20:05:21.200732 2317217 finetune.py:68] layer 5_up @ epoch 4 new loss 7.307145324375597e-07 old loss 7.349016755142657e-07 BETTER
I0328 20:05:22.561611 2317357 finetune.py:68] layer 7_up @ epoch 0 new loss 1.132952547777677e-06 old loss 1.1480877901703934e-06 BETTER
I0328 20:05:31.622838 2317287 finetune.py:68] layer 6_up @ epoch 2 new loss 9.477269031776814e-07 old loss 9.55910991251585e-07 BETTER
I0328 20:05:44.418739 2317147 finetune.py:68] layer 4_gate @ epoch 0 new loss 6.351058345899219e-07 old loss 6.373795145009353e-07 BETTER
I0328 20:05:52.384640 2317217 finetune.py:45] layer 5_gate initial loss 8.957583759183763e-07
I0328 20:05:53.598606 2317357 finetune.py:68] layer 7_up @ epoch 1 new loss 1.1212353001610609e-06 old loss 1.132952547777677e-06 BETTER
I0328 20:06:03.794698 2317287 finetune.py:68] layer 6_up @ epoch 3 new loss 9.403634635418712e-07 old loss 9.477269031776814e-07 BETTER
I0328 20:06:15.460352 2317147 finetune.py:68] layer 4_gate @ epoch 1 new loss 6.330337782856077e-07 old loss 6.351058345899219e-07 BETTER
I0328 20:06:20.803558 2317217 finetune.py:68] layer 5_gate @ epoch 0 new loss 8.91561967364396e-07 old loss 8.957583759183763e-07 BETTER
I0328 20:06:24.624472 2317357 finetune.py:68] layer 7_up @ epoch 2 new loss 1.1109581237178645e-06 old loss 1.1212353001610609e-06 BETTER
I0328 20:06:36.016123 2317287 finetune.py:68] layer 6_up @ epoch 4 new loss 9.335832373835729e-07 old loss 9.403634635418712e-07 BETTER
I0328 20:06:46.582936 2317147 finetune.py:68] layer 4_gate @ epoch 2 new loss 6.310556841526704e-07 old loss 6.330337782856077e-07 BETTER
I0328 20:06:50.013692 2317217 finetune.py:68] layer 5_gate @ epoch 1 new loss 8.877541404217482e-07 old loss 8.91561967364396e-07 BETTER
I0328 20:06:55.757576 2317357 finetune.py:68] layer 7_up @ epoch 3 new loss 1.1017814358638134e-06 old loss 1.1109581237178645e-06 BETTER
I0328 20:07:07.564441 2317287 finetune.py:45] layer 6_gate initial loss 1.1314565426800982e-06
I0328 20:07:17.732976 2317147 finetune.py:68] layer 4_gate @ epoch 3 new loss 6.291929821600206e-07 old loss 6.310556841526704e-07 BETTER
I0328 20:07:19.599475 2317217 finetune.py:68] layer 5_gate @ epoch 2 new loss 8.84178405158309e-07 old loss 8.877541404217482e-07 BETTER
I0328 20:07:26.966013 2317357 finetune.py:68] layer 7_up @ epoch 4 new loss 1.0933855492112343e-06 old loss 1.1017814358638134e-06 BETTER
I0328 20:07:35.978297 2317287 finetune.py:68] layer 6_gate @ epoch 0 new loss 1.1245936093473574e-06 old loss 1.1314565426800982e-06 BETTER
I0328 20:07:49.156879 2317147 finetune.py:68] layer 4_gate @ epoch 4 new loss 6.273982080529095e-07 old loss 6.291929821600206e-07 BETTER
I0328 20:07:49.385573 2317217 finetune.py:68] layer 5_gate @ epoch 3 new loss 8.808995630715799e-07 old loss 8.84178405158309e-07 BETTER
I0328 20:07:58.076570 2317357 finetune.py:45] layer 7_gate initial loss 1.3337688642423018e-06
I0328 20:08:05.673036 2317287 finetune.py:68] layer 6_gate @ epoch 1 new loss 1.11851989004208e-06 old loss 1.1245936093473574e-06 BETTER
I0328 20:08:18.859378 2317217 finetune.py:68] layer 5_gate @ epoch 4 new loss 8.776884783401329e-07 old loss 8.808995630715799e-07 BETTER
I0328 20:08:25.823917 2317357 finetune.py:68] layer 7_gate @ epoch 0 new loss 1.3253411452751607e-06 old loss 1.3337688642423018e-06 BETTER
I0328 20:08:35.239305 2317287 finetune.py:68] layer 6_gate @ epoch 2 new loss 1.112872837438772e-06 old loss 1.11851989004208e-06 BETTER
I0328 20:08:44.694368 2317147 finetune.py:45] layer 4_down initial loss 1.0289281817676965e-06
I0328 20:08:54.661000 2317357 finetune.py:68] layer 7_gate @ epoch 1 new loss 1.3178923836676404e-06 old loss 1.3253411452751607e-06 BETTER
I0328 20:09:04.953126 2317287 finetune.py:68] layer 6_gate @ epoch 3 new loss 1.1076498367401655e-06 old loss 1.112872837438772e-06 BETTER
I0328 20:09:11.978025 2317147 finetune.py:68] layer 4_down @ epoch 0 new loss 1.0288267731084488e-06 old loss 1.0289281817676965e-06 BETTER
I0328 20:09:14.197648 2317217 finetune.py:45] layer 5_down initial loss 1.4350393939821515e-06
I0328 20:09:23.650599 2317357 finetune.py:68] layer 7_gate @ epoch 2 new loss 1.3111078942529275e-06 old loss 1.3178923836676404e-06 BETTER
I0328 20:09:34.773613 2317287 finetune.py:68] layer 6_gate @ epoch 4 new loss 1.1026836546079721e-06 old loss 1.1076498367401655e-06 BETTER
I0328 20:09:40.282437 2317147 finetune.py:68] layer 4_down @ epoch 1 new loss 1.028778115141904e-06 old loss 1.0288267731084488e-06 BETTER
I0328 20:09:40.315042 2317217 finetune.py:68] layer 5_down @ epoch 0 new loss 1.4348847798828501e-06 old loss 1.4350393939821515e-06 BETTER
I0328 20:09:52.803250 2317357 finetune.py:68] layer 7_gate @ epoch 3 new loss 1.3047457514403504e-06 old loss 1.3111078942529275e-06 BETTER
I0328 20:10:07.396928 2317217 finetune.py:68] layer 5_down @ epoch 1 new loss 1.4348020158649888e-06 old loss 1.4348847798828501e-06 BETTER
I0328 20:10:09.139661 2317147 finetune.py:68] layer 4_down @ epoch 2 new loss 1.0287506029271754e-06 old loss 1.028778115141904e-06 BETTER
I0328 20:10:21.914493 2317357 finetune.py:68] layer 7_gate @ epoch 4 new loss 1.2988224398213788e-06 old loss 1.3047457514403504e-06 BETTER
I0328 20:10:32.361990 2317287 finetune.py:45] layer 6_down initial loss 1.7705978052617866e-06
I0328 20:10:34.904993 2317217 finetune.py:68] layer 5_down @ epoch 2 new loss 1.4347546084536589e-06 old loss 1.4348020158649888e-06 BETTER
I0328 20:10:38.205856 2317147 finetune.py:68] layer 4_down @ epoch 3 new loss 1.0287366194461356e-06 old loss 1.0287506029271754e-06 BETTER
I0328 20:10:58.599019 2317287 finetune.py:68] layer 6_down @ epoch 0 new loss 1.7704148831398925e-06 old loss 1.7705978052617866e-06 BETTER
I0328 20:11:02.455323 2317217 finetune.py:68] layer 5_down @ epoch 3 new loss 1.434721070836531e-06 old loss 1.4347546084536589e-06 BETTER
I0328 20:11:07.407566 2317147 finetune.py:68] layer 4_down @ epoch 4 new loss 1.0287177474310738e-06 old loss 1.0287366194461356e-06 BETTER
4_v proxy err 0.0004894581506960094 tr(WHW.T) 285.30712890625
bpp_loss 5.557002360466868
4_q proxy err 1.6612630133749917e-05 tr(WHW.T) 50117.8828125
bpp_loss 6.5112698775483295
4_k proxy err 1.1267617082921788e-05 tr(WHW.T) 29299.33984375
bpp_loss 7.695038146222942
4_o proxy err 0.00040501775220036507 tr(WHW.T) 1297.3052978515625
bpp_loss 5.626130153541453
4_up proxy err 0.0004175248323008418 tr(WHW.T) 7383.5341796875
bpp_loss 5.731187749521008
4_gate proxy err 0.00012052731472067535 tr(WHW.T) 29129.541015625
bpp_loss 6.08961382401841
4_down proxy err 0.0004887505201622844 tr(WHW.T) 6396.447265625
bpp_loss 5.729673641467733
I0328 20:11:18.388032 2317357 finetune.py:45] layer 7_down initial loss 2.0269021661079023e-06
I0328 20:11:26.100237 2317287 finetune.py:68] layer 6_down @ epoch 1 new loss 1.7703118828649167e-06 old loss 1.7704148831398925e-06 BETTER
I0328 20:11:30.339967 2317217 finetune.py:68] layer 5_down @ epoch 4 new loss 1.4346791203934117e-06 old loss 1.434721070836531e-06 BETTER
5_v proxy err 0.0006774713983759284 tr(WHW.T) 208.81988525390625
bpp_loss 5.44476489291992
5_q proxy err 2.4076998670352623e-05 tr(WHW.T) 35993.04296875
bpp_loss 6.486536155454814
5_k proxy err 1.6549756765016355e-05 tr(WHW.T) 22997.013671875
bpp_loss 7.6639727904694155
5_o proxy err 0.00043647742131724954 tr(WHW.T) 1056.510498046875
bpp_loss 5.568604689906351
5_up proxy err 0.00040237128268927336 tr(WHW.T) 7656.51318359375
bpp_loss 5.736871000379324
5_gate proxy err 0.00011520762927830219 tr(WHW.T) 30386.82421875
bpp_loss 6.089819403764393
5_down proxy err 0.00047413413994945586 tr(WHW.T) 6411.77783203125
bpp_loss 5.734631652910529
I0328 20:11:44.257132 2317357 finetune.py:68] layer 7_down @ epoch 0 new loss 2.0267000309104333e-06 old loss 2.0269021661079023e-06 BETTER
I0328 20:11:53.819763 2317287 finetune.py:68] layer 6_down @ epoch 2 new loss 1.770251742527762e-06 old loss 1.7703118828649167e-06 BETTER
I0328 20:12:11.078131 2317357 finetune.py:68] layer 7_down @ epoch 1 new loss 2.0266138562874403e-06 old loss 2.0267000309104333e-06 BETTER
I0328 20:12:21.896584 2317287 finetune.py:68] layer 6_down @ epoch 3 new loss 1.7702155901133665e-06 old loss 1.770251742527762e-06 BETTER
I0328 20:12:38.388437 2317357 finetune.py:68] layer 7_down @ epoch 2 new loss 2.026568608926027e-06 old loss 2.0266138562874403e-06 BETTER
I0328 20:12:43.040297 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 67.61477017402649s
I0328 20:12:46.964985 2317427 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 20:12:46.965078 2317427 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 20:12:46.965120 2317427 utils.py:162] NumExpr defaulting to 16 threads.
I0328 20:12:47.360915 2317427 config.py:54] PyTorch version 2.6.0 available.
W0328 20:12:47.585662 2317427 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 20:12:48.234359 2317427 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 20:12:48.238241 2316731 quantize_finetune_llama.py:209] layer 9 gpu 1
I0328 20:12:48.251185 2317427 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 20:12:49.678491 2317287 finetune.py:68] layer 6_down @ epoch 4 new loss 1.7701918295642827e-06 old loss 1.7702155901133665e-06 BETTER
6_v proxy err 0.0005648234509862959 tr(WHW.T) 253.63377380371094
bpp_loss 5.491223455872387
6_q proxy err 2.417410541966092e-05 tr(WHW.T) 35641.0859375
bpp_loss 6.5383336714003235
6_k proxy err 1.2596201486303471e-05 tr(WHW.T) 26163.083984375
bpp_loss 7.722205441095866
6_o proxy err 0.0004879913467448205 tr(WHW.T) 1009.7474975585938
bpp_loss 5.601188047090545
6_up proxy err 0.0003803597646765411 tr(WHW.T) 7923.45068359375
bpp_loss 5.736448930177305
6_gate proxy err 9.620050695957616e-05 tr(WHW.T) 35743.26171875
bpp_loss 6.097680879889855
6_down proxy err 0.000451840169262141 tr(WHW.T) 6484.0341796875
bpp_loss 5.734236461443028
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 20:13:04.986318 2317427 finetune.py:45] layer 8_v initial loss 4.3974634422738745e-07
W0328 20:13:04.986622 2317427 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 20:13:05.442787 2317357 finetune.py:68] layer 7_down @ epoch 3 new loss 2.026534730248386e-06 old loss 2.026568608926027e-06 BETTER
I0328 20:13:32.635358 2317357 finetune.py:68] layer 7_down @ epoch 4 new loss 2.0265108560124645e-06 old loss 2.026534730248386e-06 BETTER
7_v proxy err 0.00046718138037249446 tr(WHW.T) 309.4270935058594
bpp_loss 5.4874655661405995
7_q proxy err 2.3759772375342436e-05 tr(WHW.T) 35145.62109375
bpp_loss 6.446879077353515
7_k proxy err 1.1966475540248211e-05 tr(WHW.T) 26848.7421875
bpp_loss 7.757322531659156
7_o proxy err 0.0004367632791399956 tr(WHW.T) 959.8359375
bpp_loss 5.613860088284127
7_up proxy err 0.0003468439099378884 tr(WHW.T) 8626.6748046875
bpp_loss 5.750446688916002
7_gate proxy err 9.645268437452614e-05 tr(WHW.T) 34897.1640625
bpp_loss 6.068313551667545
7_down proxy err 0.0004519377544056624 tr(WHW.T) 6534.81396484375
bpp_loss 5.7489106873981655
I0328 20:13:39.592217 2317427 finetune.py:68] layer 8_v @ epoch 0 new loss 2.916027881383343e-07 old loss 4.3974634422738745e-07 BETTER
I0328 20:13:58.089485 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 63.90781283378601s
I0328 20:14:01.762998 2317497 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 20:14:01.763095 2317497 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 20:14:01.763133 2317497 utils.py:162] NumExpr defaulting to 16 threads.
I0328 20:14:02.125658 2317497 config.py:54] PyTorch version 2.6.0 available.
W0328 20:14:02.314793 2317497 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 20:14:02.900232 2317497 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 20:14:02.903993 2316731 quantize_finetune_llama.py:209] layer 10 gpu 2
I0328 20:14:02.917625 2317497 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 20:14:15.632439 2317427 finetune.py:68] layer 8_v @ epoch 1 new loss 2.743116738201934e-07 old loss 2.916027881383343e-07 BETTER
I0328 20:14:19.888028 2317497 finetune.py:45] layer 9_v initial loss 5.110453571433027e-07
W0328 20:14:19.888249 2317497 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 20:14:51.956784 2317427 finetune.py:68] layer 8_v @ epoch 2 new loss 2.656699109593319e-07 old loss 2.743116738201934e-07 BETTER
I0328 20:14:52.773346 2317497 finetune.py:68] layer 9_v @ epoch 0 new loss 3.0430322794927633e-07 old loss 5.110453571433027e-07 BETTER
I0328 20:15:07.027342 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 63.653711795806885s
I0328 20:15:10.699369 2317567 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 20:15:10.699456 2317567 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 20:15:10.699495 2317567 utils.py:162] NumExpr defaulting to 16 threads.
I0328 20:15:11.062891 2317567 config.py:54] PyTorch version 2.6.0 available.
W0328 20:15:11.265200 2317567 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 20:15:11.884500 2317567 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 20:15:11.888473 2316731 quantize_finetune_llama.py:209] layer 11 gpu 3
I0328 20:15:11.902364 2317567 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 20:15:27.008368 2317497 finetune.py:68] layer 9_v @ epoch 1 new loss 2.861708310319955e-07 old loss 3.0430322794927633e-07 BETTER
I0328 20:15:28.805598 2317427 finetune.py:68] layer 8_v @ epoch 3 new loss 2.6029559307971795e-07 old loss 2.656699109593319e-07 BETTER
I0328 20:15:29.359445 2317567 finetune.py:45] layer 10_v initial loss 6.029474661772838e-07
W0328 20:15:29.359853 2317567 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 20:16:01.590534 2317497 finetune.py:68] layer 9_v @ epoch 2 new loss 2.763102031622111e-07 old loss 2.861708310319955e-07 BETTER
I0328 20:16:02.600605 2317567 finetune.py:68] layer 10_v @ epoch 0 new loss 4.0489854313818796e-07 old loss 6.029474661772838e-07 BETTER
I0328 20:16:05.726262 2317427 finetune.py:68] layer 8_v @ epoch 4 new loss 2.592349233054847e-07 old loss 2.6029559307971795e-07 BETTER
I0328 20:16:16.161953 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 63.78465437889099s
I0328 20:16:20.091145 2317637 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 20:16:20.091251 2317637 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 20:16:20.091295 2317637 utils.py:162] NumExpr defaulting to 16 threads.
I0328 20:16:20.469264 2317637 config.py:54] PyTorch version 2.6.0 available.
W0328 20:16:20.689101 2317637 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 20:16:21.339220 2317637 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 20:16:21.343947 2316731 quantize_finetune_llama.py:209] layer 12 gpu 0
I0328 20:16:21.363404 2317637 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 20:16:24.645576 2317427 finetune.py:45] layer 8_q initial loss 3.320094208447699e-07
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 20:16:36.437933 2317497 finetune.py:68] layer 9_v @ epoch 3 new loss 2.7328465534992574e-07 old loss 2.763102031622111e-07 BETTER
I0328 20:16:37.291863 2317567 finetune.py:68] layer 10_v @ epoch 1 new loss 3.822346741344518e-07 old loss 4.0489854313818796e-07 BETTER
I0328 20:16:39.255159 2317637 finetune.py:45] layer 11_v initial loss 5.334489401320752e-07
W0328 20:16:39.255432 2317637 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 20:16:59.537056 2317427 finetune.py:68] layer 8_q @ epoch 0 new loss 3.2054606435849564e-07 old loss 3.320094208447699e-07 BETTER
I0328 20:17:11.667731 2317497 finetune.py:76] layer 9_v @ epoch 4 new loss 2.8205028002048493e-07 old loss 2.7328465534992574e-07 WORSE
I0328 20:17:12.223834 2317637 finetune.py:68] layer 11_v @ epoch 0 new loss 3.591954111925588e-07 old loss 5.334489401320752e-07 BETTER
I0328 20:17:12.391742 2317567 finetune.py:68] layer 10_v @ epoch 2 new loss 3.744177092812606e-07 old loss 3.822346741344518e-07 BETTER
I0328 20:17:31.007408 2317497 finetune.py:45] layer 9_q initial loss 3.5382984719944943e-07
I0328 20:17:35.818934 2317427 finetune.py:68] layer 8_q @ epoch 1 new loss 3.126303340650338e-07 old loss 3.2054606435849564e-07 BETTER
I0328 20:17:46.053665 2317637 finetune.py:68] layer 11_v @ epoch 1 new loss 3.391899099369766e-07 old loss 3.591954111925588e-07 BETTER
I0328 20:17:47.413331 2317567 finetune.py:68] layer 10_v @ epoch 3 new loss 3.7115958662070625e-07 old loss 3.744177092812606e-07 BETTER
I0328 20:18:04.185931 2317497 finetune.py:68] layer 9_q @ epoch 0 new loss 3.4336608223384246e-07 old loss 3.5382984719944943e-07 BETTER
I0328 20:18:12.014478 2317427 finetune.py:76] layer 8_q @ epoch 2 new loss 3.1404042033500446e-07 old loss 3.126303340650338e-07 WORSE
I0328 20:18:20.239259 2317637 finetune.py:68] layer 11_v @ epoch 2 new loss 3.3532987231410516e-07 old loss 3.391899099369766e-07 BETTER
I0328 20:18:22.379507 2317567 finetune.py:68] layer 10_v @ epoch 4 new loss 3.6882502740809286e-07 old loss 3.7115958662070625e-07 BETTER
I0328 20:18:38.484272 2317497 finetune.py:68] layer 9_q @ epoch 1 new loss 3.336717782076448e-07 old loss 3.4336608223384246e-07 BETTER
I0328 20:18:42.210247 2317567 finetune.py:45] layer 10_q initial loss 4.6357814653674723e-07
I0328 20:18:47.863295 2317427 finetune.py:68] layer 8_q @ epoch 3 new loss 3.1084402962733293e-07 old loss 3.126303340650338e-07 BETTER
I0328 20:18:54.671371 2317637 finetune.py:76] layer 11_v @ epoch 3 new loss 3.45966441273049e-07 old loss 3.3532987231410516e-07 WORSE
I0328 20:19:12.875280 2317497 finetune.py:68] layer 9_q @ epoch 2 new loss 3.3328902304674557e-07 old loss 3.336717782076448e-07 BETTER
I0328 20:19:15.887035 2317567 finetune.py:68] layer 10_q @ epoch 0 new loss 4.516282956501527e-07 old loss 4.6357814653674723e-07 BETTER
I0328 20:19:24.292096 2317427 finetune.py:76] layer 8_q @ epoch 4 new loss 3.1468991323890805e-07 old loss 3.1084402962733293e-07 WORSE
I0328 20:19:28.363856 2317637 finetune.py:76] layer 11_v @ epoch 4 new loss 3.485617412479769e-07 old loss 3.3532987231410516e-07 WORSE
I0328 20:19:41.293790 2317427 finetune.py:45] layer 8_k initial loss 3.4183483421657e-07
I0328 20:19:46.913459 2317637 finetune.py:45] layer 11_q initial loss 4.406340963214461e-07
I0328 20:19:47.370367 2317497 finetune.py:76] layer 9_q @ epoch 3 new loss 3.3760545647965046e-07 old loss 3.3328902304674557e-07 WORSE
I0328 20:19:50.519637 2317567 finetune.py:68] layer 10_q @ epoch 1 new loss 4.356438410013652e-07 old loss 4.516282956501527e-07 BETTER
I0328 20:20:16.664621 2317427 finetune.py:68] layer 8_k @ epoch 0 new loss 3.346679591231805e-07 old loss 3.4183483421657e-07 BETTER
I0328 20:20:19.811053 2317637 finetune.py:68] layer 11_q @ epoch 0 new loss 4.3080609657408786e-07 old loss 4.406340963214461e-07 BETTER
I0328 20:20:21.524211 2317497 finetune.py:76] layer 9_q @ epoch 4 new loss 3.360773916938342e-07 old loss 3.3328902304674557e-07 WORSE
I0328 20:20:25.073445 2317567 finetune.py:68] layer 10_q @ epoch 2 new loss 4.2667042521316034e-07 old loss 4.356438410013652e-07 BETTER
I0328 20:20:39.131046 2317497 finetune.py:45] layer 9_k initial loss 3.73639181816543e-07
I0328 20:20:52.964889 2317427 finetune.py:68] layer 8_k @ epoch 1 new loss 3.333946665406984e-07 old loss 3.346679591231805e-07 BETTER
I0328 20:20:53.637753 2317637 finetune.py:68] layer 11_q @ epoch 1 new loss 4.19258441297643e-07 old loss 4.3080609657408786e-07 BETTER
I0328 20:20:59.651681 2317567 finetune.py:76] layer 10_q @ epoch 3 new loss 4.3013386630263994e-07 old loss 4.2667042521316034e-07 WORSE
I0328 20:21:12.480867 2317497 finetune.py:76] layer 9_k @ epoch 0 new loss 3.7677327213714307e-07 old loss 3.73639181816543e-07 WORSE
I0328 20:21:27.645013 2317637 finetune.py:68] layer 11_q @ epoch 2 new loss 4.1402492456654727e-07 old loss 4.19258441297643e-07 BETTER
I0328 20:21:29.406551 2317427 finetune.py:68] layer 8_k @ epoch 2 new loss 3.302940569938073e-07 old loss 3.333946665406984e-07 BETTER
I0328 20:21:33.779384 2317567 finetune.py:68] layer 10_q @ epoch 4 new loss 4.196025145120075e-07 old loss 4.2667042521316034e-07 BETTER
I0328 20:21:46.013496 2317497 finetune.py:68] layer 9_k @ epoch 1 new loss 3.677830591186648e-07 old loss 3.73639181816543e-07 BETTER
I0328 20:21:51.572468 2317567 finetune.py:45] layer 10_k initial loss 4.6799726760582416e-07
I0328 20:22:01.759888 2317637 finetune.py:76] layer 11_q @ epoch 3 new loss 4.355618443696585e-07 old loss 4.1402492456654727e-07 WORSE
I0328 20:22:05.958686 2317427 finetune.py:76] layer 8_k @ epoch 3 new loss 3.355799549353833e-07 old loss 3.302940569938073e-07 WORSE
I0328 20:22:20.490952 2317497 finetune.py:68] layer 9_k @ epoch 2 new loss 3.671546551231586e-07 old loss 3.677830591186648e-07 BETTER
I0328 20:22:25.104175 2317567 finetune.py:68] layer 10_k @ epoch 0 new loss 4.629944214684656e-07 old loss 4.6799726760582416e-07 BETTER
I0328 20:22:35.365851 2317637 finetune.py:76] layer 11_q @ epoch 4 new loss 4.7470285835515824e-07 old loss 4.1402492456654727e-07 WORSE
I0328 20:22:41.838153 2317427 finetune.py:76] layer 8_k @ epoch 4 new loss 3.371142440755648e-07 old loss 3.302940569938073e-07 WORSE
I0328 20:22:52.441223 2317637 finetune.py:45] layer 11_k initial loss 4.5460876663128147e-07
I0328 20:22:54.860806 2317497 finetune.py:68] layer 9_k @ epoch 3 new loss 3.618543473749014e-07 old loss 3.671546551231586e-07 BETTER
I0328 20:22:59.547191 2317567 finetune.py:68] layer 10_k @ epoch 1 new loss 4.5558317651739344e-07 old loss 4.629944214684656e-07 BETTER
I0328 20:23:00.519765 2317427 finetune.py:45] layer 8_o initial loss 7.273561095644254e-07
I0328 20:23:25.258488 2317637 finetune.py:76] layer 11_k @ epoch 0 new loss 4.592511118062248e-07 old loss 4.5460876663128147e-07 WORSE
I0328 20:23:29.471838 2317497 finetune.py:68] layer 9_k @ epoch 4 new loss 3.5335480674802966e-07 old loss 3.618543473749014e-07 BETTER
I0328 20:23:34.077596 2317567 finetune.py:68] layer 10_k @ epoch 2 new loss 4.532654998001817e-07 old loss 4.5558317651739344e-07 BETTER
I0328 20:23:35.282363 2317427 finetune.py:68] layer 8_o @ epoch 0 new loss 7.094805596352671e-07 old loss 7.273561095644254e-07 BETTER
I0328 20:23:49.371220 2317497 finetune.py:45] layer 9_o initial loss 7.823890086910978e-07
I0328 20:23:58.282504 2317637 finetune.py:68] layer 11_k @ epoch 1 new loss 4.5170926910031994e-07 old loss 4.5460876663128147e-07 BETTER
I0328 20:24:08.612469 2317567 finetune.py:68] layer 10_k @ epoch 3 new loss 4.5105855406291084e-07 old loss 4.532654998001817e-07 BETTER
I0328 20:24:10.988115 2317427 finetune.py:68] layer 8_o @ epoch 1 new loss 6.981247224757681e-07 old loss 7.094805596352671e-07 BETTER
I0328 20:24:21.938109 2317497 finetune.py:68] layer 9_o @ epoch 0 new loss 7.550167424597021e-07 old loss 7.823890086910978e-07 BETTER
I0328 20:24:32.129091 2317637 finetune.py:76] layer 11_k @ epoch 2 new loss 4.7025861249494483e-07 old loss 4.5170926910031994e-07 WORSE
I0328 20:24:43.164142 2317567 finetune.py:68] layer 10_k @ epoch 4 new loss 4.492515586207446e-07 old loss 4.5105855406291084e-07 BETTER
I0328 20:24:46.849433 2317427 finetune.py:68] layer 8_o @ epoch 2 new loss 6.909035619173665e-07 old loss 6.981247224757681e-07 BETTER
I0328 20:24:55.564076 2317497 finetune.py:68] layer 9_o @ epoch 1 new loss 7.415500817842258e-07 old loss 7.550167424597021e-07 BETTER
I0328 20:25:02.736415 2317567 finetune.py:45] layer 10_o initial loss 9.799100553209428e-07
I0328 20:25:05.344262 2317637 finetune.py:68] layer 11_k @ epoch 3 new loss 4.505431547840999e-07 old loss 4.5170926910031994e-07 BETTER
I0328 20:25:22.708355 2317427 finetune.py:68] layer 8_o @ epoch 3 new loss 6.846507858426776e-07 old loss 6.909035619173665e-07 BETTER
I0328 20:25:29.234859 2317497 finetune.py:68] layer 9_o @ epoch 2 new loss 7.323460522457026e-07 old loss 7.415500817842258e-07 BETTER
I0328 20:25:35.695177 2317567 finetune.py:68] layer 10_o @ epoch 0 new loss 9.355524639431678e-07 old loss 9.799100553209428e-07 BETTER
I0328 20:25:39.253977 2317637 finetune.py:68] layer 11_k @ epoch 4 new loss 4.336450274422532e-07 old loss 4.505431547840999e-07 BETTER
I0328 20:25:58.535789 2317427 finetune.py:68] layer 8_o @ epoch 4 new loss 6.793594025111815e-07 old loss 6.846507858426776e-07 BETTER
I0328 20:25:58.977744 2317637 finetune.py:45] layer 11_o initial loss 9.70092514762655e-07
I0328 20:26:03.028932 2317497 finetune.py:68] layer 9_o @ epoch 3 new loss 7.243038453452755e-07 old loss 7.323460522457026e-07 BETTER
I0328 20:26:09.709317 2317567 finetune.py:68] layer 10_o @ epoch 1 new loss 9.159295473182283e-07 old loss 9.355524639431678e-07 BETTER
I0328 20:26:29.670922 2317427 finetune.py:45] layer 8_up initial loss 1.320442038377223e-06
I0328 20:26:31.275127 2317637 finetune.py:68] layer 11_o @ epoch 0 new loss 9.352135066365008e-07 old loss 9.70092514762655e-07 BETTER
I0328 20:26:36.985673 2317497 finetune.py:68] layer 9_o @ epoch 4 new loss 7.17734394584113e-07 old loss 7.243038453452755e-07 BETTER
I0328 20:26:43.685268 2317567 finetune.py:68] layer 10_o @ epoch 2 new loss 9.019140634336509e-07 old loss 9.159295473182283e-07 BETTER
I0328 20:27:01.495208 2317427 finetune.py:68] layer 8_up @ epoch 0 new loss 1.3023898191022454e-06 old loss 1.320442038377223e-06 BETTER
I0328 20:27:04.767240 2317637 finetune.py:68] layer 11_o @ epoch 1 new loss 9.126272857429285e-07 old loss 9.352135066365008e-07 BETTER
I0328 20:27:08.607329 2317497 finetune.py:45] layer 9_up initial loss 1.420921876160719e-06
I0328 20:27:18.004889 2317567 finetune.py:68] layer 10_o @ epoch 3 new loss 8.916276215131802e-07 old loss 9.019140634336509e-07 BETTER
I0328 20:27:35.014686 2317427 finetune.py:68] layer 8_up @ epoch 1 new loss 1.2884887610198348e-06 old loss 1.3023898191022454e-06 BETTER
I0328 20:27:38.320320 2317637 finetune.py:68] layer 11_o @ epoch 2 new loss 8.997539566735213e-07 old loss 9.126272857429285e-07 BETTER
I0328 20:27:39.070149 2317497 finetune.py:68] layer 9_up @ epoch 0 new loss 1.3990425031806808e-06 old loss 1.420921876160719e-06 BETTER
I0328 20:27:52.082653 2317567 finetune.py:68] layer 10_o @ epoch 4 new loss 8.829808280097495e-07 old loss 8.916276215131802e-07 BETTER
I0328 20:28:08.704129 2317427 finetune.py:68] layer 8_up @ epoch 2 new loss 1.276407488148834e-06 old loss 1.2884887610198348e-06 BETTER
I0328 20:28:10.562152 2317497 finetune.py:68] layer 9_up @ epoch 1 new loss 1.3823658946421347e-06 old loss 1.3990425031806808e-06 BETTER
I0328 20:28:11.721244 2317637 finetune.py:68] layer 11_o @ epoch 3 new loss 8.891373681763071e-07 old loss 8.997539566735213e-07 BETTER
I0328 20:28:23.550362 2317567 finetune.py:45] layer 10_up initial loss 1.6057443872341537e-06
I0328 20:28:42.185648 2317427 finetune.py:68] layer 8_up @ epoch 3 new loss 1.2657031902563176e-06 old loss 1.276407488148834e-06 BETTER
I0328 20:28:42.334418 2317497 finetune.py:68] layer 9_up @ epoch 2 new loss 1.368001790069684e-06 old loss 1.3823658946421347e-06 BETTER
I0328 20:28:45.055582 2317637 finetune.py:68] layer 11_o @ epoch 4 new loss 8.805085371932364e-07 old loss 8.891373681763071e-07 BETTER
I0328 20:28:54.360504 2317567 finetune.py:68] layer 10_up @ epoch 0 new loss 1.581849460308149e-06 old loss 1.6057443872341537e-06 BETTER
I0328 20:29:14.303833 2317497 finetune.py:68] layer 9_up @ epoch 3 new loss 1.3551139090850484e-06 old loss 1.368001790069684e-06 BETTER
I0328 20:29:15.755609 2317427 finetune.py:68] layer 8_up @ epoch 4 new loss 1.2558360822367831e-06 old loss 1.2657031902563176e-06 BETTER
I0328 20:29:16.016722 2317637 finetune.py:45] layer 11_up initial loss 1.650462877478276e-06
I0328 20:29:26.463844 2317567 finetune.py:68] layer 10_up @ epoch 1 new loss 1.5636077250746894e-06 old loss 1.581849460308149e-06 BETTER
I0328 20:29:46.180390 2317637 finetune.py:68] layer 11_up @ epoch 0 new loss 1.6236044757533818e-06 old loss 1.650462877478276e-06 BETTER
I0328 20:29:46.488507 2317497 finetune.py:68] layer 9_up @ epoch 4 new loss 1.3436626886687009e-06 old loss 1.3551139090850484e-06 BETTER
I0328 20:29:46.616835 2317427 finetune.py:45] layer 8_gate initial loss 1.5162527233769652e-06
I0328 20:29:58.425176 2317567 finetune.py:68] layer 10_up @ epoch 2 new loss 1.5482044091186253e-06 old loss 1.5636077250746894e-06 BETTER
I0328 20:30:16.757153 2317427 finetune.py:68] layer 8_gate @ epoch 0 new loss 1.5063310456753243e-06 old loss 1.5162527233769652e-06 BETTER
I0328 20:30:17.418009 2317637 finetune.py:68] layer 11_up @ epoch 1 new loss 1.603359351065592e-06 old loss 1.6236044757533818e-06 BETTER
I0328 20:30:17.949834 2317497 finetune.py:45] layer 9_gate initial loss 1.6250753560598241e-06
I0328 20:30:30.551617 2317567 finetune.py:68] layer 10_up @ epoch 3 new loss 1.5343215409302502e-06 old loss 1.5482044091186253e-06 BETTER
I0328 20:30:46.388806 2317497 finetune.py:68] layer 9_gate @ epoch 0 new loss 1.6134081306518055e-06 old loss 1.6250753560598241e-06 BETTER
I0328 20:30:48.065976 2317427 finetune.py:68] layer 8_gate @ epoch 1 new loss 1.4976590136939194e-06 old loss 1.5063310456753243e-06 BETTER
I0328 20:30:48.795730 2317637 finetune.py:68] layer 11_up @ epoch 2 new loss 1.585987547514378e-06 old loss 1.603359351065592e-06 BETTER
I0328 20:31:02.813773 2317567 finetune.py:68] layer 10_up @ epoch 4 new loss 1.5218989801724092e-06 old loss 1.5343215409302502e-06 BETTER
I0328 20:31:16.179942 2317497 finetune.py:68] layer 9_gate @ epoch 1 new loss 1.603352757229004e-06 old loss 1.6134081306518055e-06 BETTER
I0328 20:31:19.472630 2317427 finetune.py:68] layer 8_gate @ epoch 2 new loss 1.4896722859703004e-06 old loss 1.4976590136939194e-06 BETTER
I0328 20:31:20.287504 2317637 finetune.py:68] layer 11_up @ epoch 3 new loss 1.5706094700362883e-06 old loss 1.585987547514378e-06 BETTER
I0328 20:31:34.311254 2317567 finetune.py:45] layer 10_gate initial loss 1.8270043256052304e-06
I0328 20:31:45.850414 2317497 finetune.py:68] layer 9_gate @ epoch 2 new loss 1.5941537867547595e-06 old loss 1.603352757229004e-06 BETTER
I0328 20:31:50.928021 2317427 finetune.py:68] layer 8_gate @ epoch 3 new loss 1.4823580158918048e-06 old loss 1.4896722859703004e-06 BETTER
I0328 20:31:51.614352 2317637 finetune.py:68] layer 11_up @ epoch 4 new loss 1.5567475202260539e-06 old loss 1.5706094700362883e-06 BETTER
I0328 20:32:02.924574 2317567 finetune.py:68] layer 10_gate @ epoch 0 new loss 1.81455379788531e-06 old loss 1.8270043256052304e-06 BETTER
I0328 20:32:15.426366 2317497 finetune.py:68] layer 9_gate @ epoch 3 new loss 1.5856718391660252e-06 old loss 1.5941537867547595e-06 BETTER
I0328 20:32:22.338022 2317427 finetune.py:68] layer 8_gate @ epoch 4 new loss 1.4753703680980834e-06 old loss 1.4823580158918048e-06 BETTER
I0328 20:32:22.693210 2317637 finetune.py:45] layer 11_gate initial loss 1.8841567452909658e-06
I0328 20:32:32.547974 2317567 finetune.py:68] layer 10_gate @ epoch 1 new loss 1.8037245581581374e-06 old loss 1.81455379788531e-06 BETTER
I0328 20:32:45.088718 2317497 finetune.py:68] layer 9_gate @ epoch 4 new loss 1.5777568478370085e-06 old loss 1.5856718391660252e-06 BETTER
I0328 20:32:50.695197 2317637 finetune.py:68] layer 11_gate @ epoch 0 new loss 1.8702723991737003e-06 old loss 1.8841567452909658e-06 BETTER
I0328 20:33:02.226359 2317567 finetune.py:68] layer 10_gate @ epoch 2 new loss 1.7938131122718914e-06 old loss 1.8037245581581374e-06 BETTER
I0328 20:33:18.601311 2317427 finetune.py:45] layer 8_down initial loss 2.2605179310630774e-06
I0328 20:33:19.566656 2317637 finetune.py:68] layer 11_gate @ epoch 1 new loss 1.8583200471766759e-06 old loss 1.8702723991737003e-06 BETTER
I0328 20:33:32.122094 2317567 finetune.py:68] layer 10_gate @ epoch 3 new loss 1.7848810784926172e-06 old loss 1.7938131122718914e-06 BETTER
I0328 20:33:42.275120 2317497 finetune.py:45] layer 9_down initial loss 2.4357932488783263e-06
I0328 20:33:45.925946 2317427 finetune.py:68] layer 8_down @ epoch 0 new loss 2.2602866920351516e-06 old loss 2.2605179310630774e-06 BETTER
I0328 20:33:49.035580 2317637 finetune.py:68] layer 11_gate @ epoch 2 new loss 1.8475482193025528e-06 old loss 1.8583200471766759e-06 BETTER
I0328 20:34:01.965247 2317567 finetune.py:68] layer 10_gate @ epoch 4 new loss 1.776311250978324e-06 old loss 1.7848810784926172e-06 BETTER
I0328 20:34:08.519949 2317497 finetune.py:68] layer 9_down @ epoch 0 new loss 2.4355326786462683e-06 old loss 2.4357932488783263e-06 BETTER
I0328 20:34:14.321280 2317427 finetune.py:68] layer 8_down @ epoch 1 new loss 2.2601743694394827e-06 old loss 2.2602866920351516e-06 BETTER
I0328 20:34:18.307665 2317637 finetune.py:68] layer 11_gate @ epoch 3 new loss 1.837523541325936e-06 old loss 1.8475482193025528e-06 BETTER
I0328 20:34:35.675884 2317497 finetune.py:68] layer 9_down @ epoch 1 new loss 2.4354280867555644e-06 old loss 2.4355326786462683e-06 BETTER
I0328 20:34:43.151031 2317427 finetune.py:68] layer 8_down @ epoch 2 new loss 2.2601066120842006e-06 old loss 2.2601743694394827e-06 BETTER
I0328 20:34:47.604713 2317637 finetune.py:68] layer 11_gate @ epoch 4 new loss 1.8283371900906786e-06 old loss 1.837523541325936e-06 BETTER
I0328 20:34:59.685679 2317567 finetune.py:45] layer 10_down initial loss 2.663490022314363e-06
I0328 20:35:03.010404 2317497 finetune.py:68] layer 9_down @ epoch 2 new loss 2.4353648768737912e-06 old loss 2.4354280867555644e-06 BETTER
I0328 20:35:12.141437 2317427 finetune.py:68] layer 8_down @ epoch 3 new loss 2.260059545733384e-06 old loss 2.2601066120842006e-06 BETTER
I0328 20:35:26.044148 2317567 finetune.py:68] layer 10_down @ epoch 0 new loss 2.6632649223756744e-06 old loss 2.663490022314363e-06 BETTER
I0328 20:35:30.620609 2317497 finetune.py:68] layer 9_down @ epoch 3 new loss 2.4353225853701588e-06 old loss 2.4353648768737912e-06 BETTER
I0328 20:35:41.190870 2317427 finetune.py:68] layer 8_down @ epoch 4 new loss 2.2600302145292517e-06 old loss 2.260059545733384e-06 BETTER
8_v proxy err 0.0005599232972599566 tr(WHW.T) 257.7052307128906
bpp_loss 5.512529646512121
8_q proxy err 3.180709609296173e-05 tr(WHW.T) 26601.72265625
bpp_loss 6.435963431955315
8_k proxy err 1.511148366262205e-05 tr(WHW.T) 22526.765625
bpp_loss 7.655264269327745
8_o proxy err 0.0005378864007070661 tr(WHW.T) 745.8627319335938
bpp_loss 5.627158902469091
8_up proxy err 0.0003563145291991532 tr(WHW.T) 8494.8837890625
bpp_loss 5.745811629242131
8_gate proxy err 9.180939377984032e-05 tr(WHW.T) 37237.7890625
bpp_loss 6.075423609066222
8_down proxy err 0.00045887025771662593 tr(WHW.T) 6489.296875
bpp_loss 5.746133796604616
I0328 20:35:43.769146 2317637 finetune.py:45] layer 11_down initial loss 2.7500202577357413e-06
I0328 20:35:54.280725 2317567 finetune.py:68] layer 10_down @ epoch 1 new loss 2.6631480523064965e-06 old loss 2.6632649223756744e-06 BETTER
I0328 20:35:58.715291 2317497 finetune.py:76] layer 9_down @ epoch 4 new loss 2.4353496428375365e-06 old loss 2.4353225853701588e-06 WORSE
9_v proxy err 0.00043341729906387627 tr(WHW.T) 351.4288024902344
bpp_loss 5.615990342921577
9_q proxy err 3.3302978408755735e-05 tr(WHW.T) 25655.12890625
bpp_loss 6.446312080486678
9_k proxy err 1.6572037566220388e-05 tr(WHW.T) 20943.224609375
bpp_loss 7.6770888599567115
9_o proxy err 0.0004954975447617471 tr(WHW.T) 776.854736328125
bpp_loss 5.684240661095828
9_up proxy err 0.0003379792906343937 tr(WHW.T) 8970.3798828125
bpp_loss 5.755335580557585
9_gate proxy err 8.71894953888841e-05 tr(WHW.T) 39408.26953125
bpp_loss 6.093513876199722
9_down proxy err 0.00045512174256145954 tr(WHW.T) 6273.79052734375
bpp_loss 5.749243433188115
I0328 20:36:10.064394 2317637 finetune.py:68] layer 11_down @ epoch 0 new loss 2.7497069368109806e-06 old loss 2.7500202577357413e-06 BETTER
I0328 20:36:22.150325 2317567 finetune.py:68] layer 10_down @ epoch 2 new loss 2.6630598313204246e-06 old loss 2.6631480523064965e-06 BETTER
I0328 20:36:36.945290 2317637 finetune.py:68] layer 11_down @ epoch 1 new loss 2.749569148363662e-06 old loss 2.7497069368109806e-06 BETTER
I0328 20:36:49.937925 2317567 finetune.py:68] layer 10_down @ epoch 3 new loss 2.663047325768275e-06 old loss 2.6630598313204246e-06 BETTER
I0328 20:37:03.917408 2317637 finetune.py:68] layer 11_down @ epoch 2 new loss 2.7495048016135115e-06 old loss 2.749569148363662e-06 BETTER
I0328 20:37:10.227749 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 66.90953183174133s
I0328 20:37:14.088360 2317707 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 20:37:14.088457 2317707 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 20:37:14.088500 2317707 utils.py:162] NumExpr defaulting to 16 threads.
I0328 20:37:14.461872 2317707 config.py:54] PyTorch version 2.6.0 available.
W0328 20:37:14.686394 2317707 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 20:37:15.356142 2317707 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 20:37:15.360189 2316731 quantize_finetune_llama.py:209] layer 13 gpu 1
I0328 20:37:15.374041 2317707 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 20:37:17.964150 2317567 finetune.py:68] layer 10_down @ epoch 4 new loss 2.6629884359863354e-06 old loss 2.663047325768275e-06 BETTER
10_v proxy err 0.0005583289894275367 tr(WHW.T) 251.83889770507812
bpp_loss 5.503635169705376
10_q proxy err 3.567282692529261e-05 tr(WHW.T) 23339.68359375
bpp_loss 6.462294208351523
10_k proxy err 1.7945823856280185e-05 tr(WHW.T) 19743.330078125
bpp_loss 7.689353656256571
10_o proxy err 0.0006042347522452474 tr(WHW.T) 680.82421875
bpp_loss 5.618331668782048
10_up proxy err 0.00033391345641575754 tr(WHW.T) 9198.712890625
bpp_loss 5.771856035638068
10_gate proxy err 9.142736234935e-05 tr(WHW.T) 37386.15234375
bpp_loss 6.057246092041688
10_down proxy err 0.0004445631639100611 tr(WHW.T) 6525.89892578125
bpp_loss 5.7646112707443535
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 20:37:30.891670 2317637 finetune.py:68] layer 11_down @ epoch 3 new loss 2.749493660303415e-06 old loss 2.7495048016135115e-06 BETTER
I0328 20:37:32.430963 2317707 finetune.py:45] layer 12_v initial loss 5.734173669225129e-07
W0328 20:37:32.431407 2317707 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 20:37:57.983433 2317637 finetune.py:68] layer 11_down @ epoch 4 new loss 2.7494079404277727e-06 old loss 2.749493660303415e-06 BETTER
11_v proxy err 0.00045883210259489715 tr(WHW.T) 319.5691223144531
bpp_loss 5.503986765397713
11_q proxy err 3.7177767808316275e-05 tr(WHW.T) 22191.025390625
bpp_loss 6.376913943677209
11_k proxy err 1.8826453015208244e-05 tr(WHW.T) 18025.576171875
bpp_loss 7.674286259454675
11_o proxy err 0.0006049146177247167 tr(WHW.T) 566.0090942382812
bpp_loss 5.637137013836764
11_up proxy err 0.000328755471855402 tr(WHW.T) 9193.27734375
bpp_loss 5.778362338963364
11_gate proxy err 9.047246567206457e-05 tr(WHW.T) 36879.78515625
bpp_loss 6.037644825476621
11_down proxy err 0.0004271346260793507 tr(WHW.T) 6656.9931640625
bpp_loss 5.772468485204237
I0328 20:38:07.013114 2317707 finetune.py:68] layer 12_v @ epoch 0 new loss 4.030348748074175e-07 old loss 5.734173669225129e-07 BETTER
I0328 20:38:26.497630 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 63.98732233047485s
I0328 20:38:30.117555 2317777 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 20:38:30.117660 2317777 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 20:38:30.117704 2317777 utils.py:162] NumExpr defaulting to 16 threads.
I0328 20:38:30.460356 2317777 config.py:54] PyTorch version 2.6.0 available.
W0328 20:38:30.656416 2317777 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 20:38:31.231518 2317777 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 20:38:31.235314 2316731 quantize_finetune_llama.py:209] layer 14 gpu 2
I0328 20:38:31.249056 2317777 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 20:38:42.946395 2317707 finetune.py:68] layer 12_v @ epoch 1 new loss 3.814760987097543e-07 old loss 4.030348748074175e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 20:38:48.431002 2317777 finetune.py:45] layer 13_v initial loss 5.982836910334299e-07
W0328 20:38:48.431192 2317777 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 20:39:19.400131 2317707 finetune.py:68] layer 12_v @ epoch 2 new loss 3.661741629912285e-07 old loss 3.814760987097543e-07 BETTER
I0328 20:39:21.199449 2317777 finetune.py:68] layer 13_v @ epoch 0 new loss 4.3994265297442325e-07 old loss 5.982836910334299e-07 BETTER
I0328 20:39:34.794133 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 63.10787534713745s
I0328 20:39:38.507525 2317847 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 20:39:38.507611 2317847 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 20:39:38.507650 2317847 utils.py:162] NumExpr defaulting to 16 threads.
I0328 20:39:38.854092 2317847 config.py:54] PyTorch version 2.6.0 available.
W0328 20:39:39.044633 2317847 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 20:39:39.683451 2317847 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 20:39:39.688029 2316731 quantize_finetune_llama.py:209] layer 15 gpu 3
I0328 20:39:39.702324 2317847 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 20:39:55.487470 2317777 finetune.py:68] layer 13_v @ epoch 1 new loss 4.1660166516521713e-07 old loss 4.3994265297442325e-07 BETTER
I0328 20:39:56.063523 2317707 finetune.py:76] layer 12_v @ epoch 3 new loss 3.686002401082078e-07 old loss 3.661741629912285e-07 WORSE
I0328 20:39:56.971868 2317847 finetune.py:45] layer 14_v initial loss 6.606575766454625e-07
W0328 20:39:56.972251 2317847 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 20:40:30.101106 2317777 finetune.py:68] layer 13_v @ epoch 2 new loss 4.051089206313918e-07 old loss 4.1660166516521713e-07 BETTER
I0328 20:40:30.353885 2317847 finetune.py:68] layer 14_v @ epoch 0 new loss 4.832320428249659e-07 old loss 6.606575766454625e-07 BETTER
I0328 20:40:32.308076 2317707 finetune.py:76] layer 12_v @ epoch 4 new loss 4.0204812989941274e-07 old loss 3.661741629912285e-07 WORSE
I0328 20:40:43.742615 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 63.55758237838745s
I0328 20:40:47.663447 2317917 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 20:40:47.663670 2317917 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 20:40:47.663779 2317917 utils.py:162] NumExpr defaulting to 16 threads.
I0328 20:40:48.029271 2317917 config.py:54] PyTorch version 2.6.0 available.
W0328 20:40:48.249082 2317917 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 20:40:48.902827 2317917 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 20:40:48.906797 2316731 quantize_finetune_llama.py:209] layer 16 gpu 0
I0328 20:40:48.921307 2317917 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 20:40:50.964412 2317707 finetune.py:45] layer 12_q initial loss 4.4973739932174794e-07
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 20:41:04.712130 2317777 finetune.py:76] layer 13_v @ epoch 3 new loss 4.0560513525633723e-07 old loss 4.051089206313918e-07 WORSE
I0328 20:41:04.956550 2317847 finetune.py:68] layer 14_v @ epoch 1 new loss 4.5545792204393365e-07 old loss 4.832320428249659e-07 BETTER
I0328 20:41:06.064730 2317917 finetune.py:45] layer 15_v initial loss 8.732989158488635e-07
W0328 20:41:06.065050 2317917 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 20:41:25.910776 2317707 finetune.py:68] layer 12_q @ epoch 0 new loss 4.381583664780919e-07 old loss 4.4973739932174794e-07 BETTER
I0328 20:41:38.820312 2317777 finetune.py:76] layer 13_v @ epoch 4 new loss 4.2090533725058776e-07 old loss 4.051089206313918e-07 WORSE
I0328 20:41:39.073273 2317917 finetune.py:68] layer 15_v @ epoch 0 new loss 5.411582719716534e-07 old loss 8.732989158488635e-07 BETTER
I0328 20:41:39.817981 2317847 finetune.py:68] layer 14_v @ epoch 2 new loss 4.386982652704319e-07 old loss 4.5545792204393365e-07 BETTER
I0328 20:41:57.452774 2317777 finetune.py:45] layer 13_q initial loss 5.043598321208265e-07
I0328 20:42:01.935872 2317707 finetune.py:68] layer 12_q @ epoch 1 new loss 4.3580016040323244e-07 old loss 4.381583664780919e-07 BETTER
I0328 20:42:13.599104 2317917 finetune.py:68] layer 15_v @ epoch 1 new loss 5.07455979459337e-07 old loss 5.411582719716534e-07 BETTER
I0328 20:42:15.194089 2317847 finetune.py:68] layer 14_v @ epoch 3 new loss 4.3253837134216155e-07 old loss 4.386982652704319e-07 BETTER
I0328 20:42:30.828727 2317777 finetune.py:68] layer 13_q @ epoch 0 new loss 4.929774490847194e-07 old loss 5.043598321208265e-07 BETTER
I0328 20:42:38.285730 2317707 finetune.py:76] layer 12_q @ epoch 2 new loss 4.3768613977590576e-07 old loss 4.3580016040323244e-07 WORSE
I0328 20:42:47.983997 2317917 finetune.py:68] layer 15_v @ epoch 2 new loss 4.949262688569434e-07 old loss 5.07455979459337e-07 BETTER
I0328 20:42:50.487576 2317847 finetune.py:76] layer 14_v @ epoch 4 new loss 4.427188571298757e-07 old loss 4.3253837134216155e-07 WORSE
I0328 20:43:05.083052 2317777 finetune.py:68] layer 13_q @ epoch 1 new loss 4.878985464529251e-07 old loss 4.929774490847194e-07 BETTER
I0328 20:43:09.396684 2317847 finetune.py:45] layer 14_q initial loss 5.702563043996633e-07
I0328 20:43:14.115109 2317707 finetune.py:68] layer 12_q @ epoch 3 new loss 4.1913983750418993e-07 old loss 4.3580016040323244e-07 BETTER
I0328 20:43:22.501301 2317917 finetune.py:76] layer 15_v @ epoch 3 new loss 5.045685043114645e-07 old loss 4.949262688569434e-07 WORSE
I0328 20:43:39.468249 2317777 finetune.py:68] layer 13_q @ epoch 2 new loss 4.81206029689929e-07 old loss 4.878985464529251e-07 BETTER
I0328 20:43:43.037643 2317847 finetune.py:68] layer 14_q @ epoch 0 new loss 5.46979947557702e-07 old loss 5.702563043996633e-07 BETTER
I0328 20:43:50.560726 2317707 finetune.py:76] layer 12_q @ epoch 4 new loss 4.2841915615099424e-07 old loss 4.1913983750418993e-07 WORSE
I0328 20:43:56.334540 2317917 finetune.py:76] layer 15_v @ epoch 4 new loss 5.528619340111618e-07 old loss 4.949262688569434e-07 WORSE
I0328 20:44:07.660603 2317707 finetune.py:45] layer 12_k initial loss 4.6615383553216816e-07
I0328 20:44:13.940613 2317777 finetune.py:68] layer 13_q @ epoch 3 new loss 4.761354261972883e-07 old loss 4.81206029689929e-07 BETTER
I0328 20:44:14.776368 2317917 finetune.py:45] layer 15_q initial loss 5.871855250916269e-07
I0328 20:44:17.558886 2317847 finetune.py:68] layer 14_q @ epoch 1 new loss 5.336054869076179e-07 old loss 5.46979947557702e-07 BETTER
I0328 20:44:42.879794 2317707 finetune.py:68] layer 12_k @ epoch 0 new loss 4.619855076271051e-07 old loss 4.6615383553216816e-07 BETTER
I0328 20:44:47.696580 2317917 finetune.py:68] layer 15_q @ epoch 0 new loss 5.662437843056978e-07 old loss 5.871855250916269e-07 BETTER
I0328 20:44:48.427349 2317777 finetune.py:76] layer 13_q @ epoch 4 new loss 4.780495146405883e-07 old loss 4.761354261972883e-07 WORSE
I0328 20:44:52.151902 2317847 finetune.py:76] layer 14_q @ epoch 2 new loss 5.505004310180084e-07 old loss 5.336054869076179e-07 WORSE
I0328 20:45:05.752006 2317777 finetune.py:45] layer 13_k initial loss 5.264173523755744e-07
I0328 20:45:18.977319 2317707 finetune.py:68] layer 12_k @ epoch 1 new loss 4.5100790657670586e-07 old loss 4.619855076271051e-07 BETTER
I0328 20:45:21.566057 2317917 finetune.py:68] layer 15_q @ epoch 1 new loss 5.587721716437954e-07 old loss 5.662437843056978e-07 BETTER
I0328 20:45:26.173772 2317847 finetune.py:68] layer 14_q @ epoch 3 new loss 5.282255415295367e-07 old loss 5.336054869076179e-07 BETTER
I0328 20:45:38.929034 2317777 finetune.py:76] layer 13_k @ epoch 0 new loss 5.422688786893559e-07 old loss 5.264173523755744e-07 WORSE
I0328 20:45:55.354918 2317707 finetune.py:68] layer 12_k @ epoch 2 new loss 4.4469769022725814e-07 old loss 4.5100790657670586e-07 BETTER
I0328 20:45:55.657297 2317917 finetune.py:68] layer 15_q @ epoch 2 new loss 5.497890924743842e-07 old loss 5.587721716437954e-07 BETTER
I0328 20:46:00.657925 2317847 finetune.py:68] layer 14_q @ epoch 4 new loss 5.200981263442372e-07 old loss 5.282255415295367e-07 BETTER
I0328 20:46:12.840788 2317777 finetune.py:68] layer 13_k @ epoch 1 new loss 5.263476623440511e-07 old loss 5.264173523755744e-07 BETTER
I0328 20:46:18.420230 2317847 finetune.py:45] layer 14_k initial loss 6.016783231643785e-07
I0328 20:46:29.970955 2317917 finetune.py:68] layer 15_q @ epoch 3 new loss 5.491633601195645e-07 old loss 5.497890924743842e-07 BETTER
I0328 20:46:32.045912 2317707 finetune.py:68] layer 12_k @ epoch 3 new loss 4.428113982157811e-07 old loss 4.4469769022725814e-07 BETTER
I0328 20:46:47.409718 2317777 finetune.py:68] layer 13_k @ epoch 2 new loss 5.110865117785579e-07 old loss 5.263476623440511e-07 BETTER
I0328 20:46:52.131350 2317847 finetune.py:68] layer 14_k @ epoch 0 new loss 5.836801051373186e-07 old loss 6.016783231643785e-07 BETTER
I0328 20:47:03.976945 2317917 finetune.py:76] layer 15_q @ epoch 4 new loss 5.56037434762402e-07 old loss 5.491633601195645e-07 WORSE
I0328 20:47:08.479653 2317707 finetune.py:76] layer 12_k @ epoch 4 new loss 4.4329448201096966e-07 old loss 4.428113982157811e-07 WORSE
I0328 20:47:20.863363 2317917 finetune.py:45] layer 15_k initial loss 6.216013161974843e-07
I0328 20:47:21.680777 2317777 finetune.py:68] layer 13_k @ epoch 3 new loss 5.060177272753208e-07 old loss 5.110865117785579e-07 BETTER
I0328 20:47:26.499107 2317847 finetune.py:68] layer 14_k @ epoch 1 new loss 5.754855010309257e-07 old loss 5.836801051373186e-07 BETTER
I0328 20:47:27.695849 2317707 finetune.py:45] layer 12_o initial loss 9.89136538009916e-07
I0328 20:47:53.851534 2317917 finetune.py:68] layer 15_k @ epoch 0 new loss 6.074586167414964e-07 old loss 6.216013161974843e-07 BETTER
I0328 20:47:55.945059 2317777 finetune.py:68] layer 13_k @ epoch 4 new loss 5.041122790316876e-07 old loss 5.060177272753208e-07 BETTER
I0328 20:48:00.994007 2317847 finetune.py:76] layer 14_k @ epoch 2 new loss 5.757002554673818e-07 old loss 5.754855010309257e-07 WORSE
I0328 20:48:02.114431 2317707 finetune.py:68] layer 12_o @ epoch 0 new loss 9.509060987511475e-07 old loss 9.89136538009916e-07 BETTER
I0328 20:48:15.576186 2317777 finetune.py:45] layer 13_o initial loss 1.2043966535202344e-06
I0328 20:48:27.709966 2317917 finetune.py:68] layer 15_k @ epoch 1 new loss 6.040272637619637e-07 old loss 6.074586167414964e-07 BETTER
I0328 20:48:34.805649 2317847 finetune.py:68] layer 14_k @ epoch 3 new loss 5.699407097381481e-07 old loss 5.754855010309257e-07 BETTER
I0328 20:48:37.706574 2317707 finetune.py:68] layer 12_o @ epoch 1 new loss 9.288485216529807e-07 old loss 9.509060987511475e-07 BETTER
I0328 20:48:48.031875 2317777 finetune.py:68] layer 13_o @ epoch 0 new loss 1.1452532362454804e-06 old loss 1.2043966535202344e-06 BETTER
I0328 20:49:01.504437 2317917 finetune.py:68] layer 15_k @ epoch 2 new loss 5.947699150965491e-07 old loss 6.040272637619637e-07 BETTER
I0328 20:49:09.438855 2317847 finetune.py:76] layer 14_k @ epoch 4 new loss 5.893758157071716e-07 old loss 5.699407097381481e-07 WORSE
I0328 20:49:13.498922 2317707 finetune.py:68] layer 12_o @ epoch 2 new loss 9.155818361250567e-07 old loss 9.288485216529807e-07 BETTER
I0328 20:49:21.666384 2317777 finetune.py:68] layer 13_o @ epoch 1 new loss 1.1202336054338957e-06 old loss 1.1452532362454804e-06 BETTER
I0328 20:49:28.380947 2317847 finetune.py:45] layer 14_o initial loss 1.3084548982078559e-06
I0328 20:49:35.290381 2317917 finetune.py:76] layer 15_k @ epoch 3 new loss 6.128825589257758e-07 old loss 5.947699150965491e-07 WORSE
I0328 20:49:49.297288 2317707 finetune.py:68] layer 12_o @ epoch 3 new loss 9.038907364811166e-07 old loss 9.155818361250567e-07 BETTER
I0328 20:49:55.385447 2317777 finetune.py:68] layer 13_o @ epoch 2 new loss 1.1030897439923137e-06 old loss 1.1202336054338957e-06 BETTER
I0328 20:50:01.210936 2317847 finetune.py:68] layer 14_o @ epoch 0 new loss 1.2566403029268258e-06 old loss 1.3084548982078559e-06 BETTER
I0328 20:50:08.615315 2317917 finetune.py:76] layer 15_k @ epoch 4 new loss 5.984102244838141e-07 old loss 5.947699150965491e-07 WORSE
I0328 20:50:25.094138 2317707 finetune.py:68] layer 12_o @ epoch 4 new loss 8.943832199292956e-07 old loss 9.038907364811166e-07 BETTER
I0328 20:50:27.331745 2317917 finetune.py:45] layer 15_o initial loss 1.3266736687000957e-06
I0328 20:50:28.992420 2317777 finetune.py:68] layer 13_o @ epoch 3 new loss 1.0890405519603519e-06 old loss 1.1030897439923137e-06 BETTER
I0328 20:50:35.265874 2317847 finetune.py:68] layer 14_o @ epoch 1 new loss 1.2297305147512816e-06 old loss 1.2566403029268258e-06 BETTER
I0328 20:50:56.149417 2317707 finetune.py:45] layer 12_up initial loss 1.7068015267796e-06
I0328 20:50:59.742474 2317917 finetune.py:68] layer 15_o @ epoch 0 new loss 1.272437202715082e-06 old loss 1.3266736687000957e-06 BETTER
I0328 20:51:02.730956 2317777 finetune.py:68] layer 13_o @ epoch 4 new loss 1.0775925147754606e-06 old loss 1.0890405519603519e-06 BETTER
I0328 20:51:09.170409 2317847 finetune.py:68] layer 14_o @ epoch 2 new loss 1.2095225656594266e-06 old loss 1.2297305147512816e-06 BETTER
I0328 20:51:28.066201 2317707 finetune.py:68] layer 12_up @ epoch 0 new loss 1.6731341929698829e-06 old loss 1.7068015267796e-06 BETTER
I0328 20:51:32.851276 2317917 finetune.py:68] layer 15_o @ epoch 1 new loss 1.2384128922349191e-06 old loss 1.272437202715082e-06 BETTER
I0328 20:51:34.172719 2317777 finetune.py:45] layer 13_up initial loss 2.0044096800120315e-06
I0328 20:51:43.042593 2317847 finetune.py:68] layer 14_o @ epoch 3 new loss 1.1936135706491768e-06 old loss 1.2095225656594266e-06 BETTER
I0328 20:52:01.334430 2317707 finetune.py:68] layer 12_up @ epoch 1 new loss 1.6486836784679326e-06 old loss 1.6731341929698829e-06 BETTER
I0328 20:52:04.697321 2317777 finetune.py:68] layer 13_up @ epoch 0 new loss 1.963739123311825e-06 old loss 2.0044096800120315e-06 BETTER
I0328 20:52:06.300191 2317917 finetune.py:68] layer 15_o @ epoch 2 new loss 1.218448801409977e-06 old loss 1.2384128922349191e-06 BETTER
I0328 20:52:17.091631 2317847 finetune.py:68] layer 14_o @ epoch 4 new loss 1.1805418580479454e-06 old loss 1.1936135706491768e-06 BETTER
I0328 20:52:34.907060 2317707 finetune.py:68] layer 12_up @ epoch 2 new loss 1.628039854040253e-06 old loss 1.6486836784679326e-06 BETTER
I0328 20:52:36.316381 2317777 finetune.py:68] layer 13_up @ epoch 1 new loss 1.9344151951372623e-06 old loss 1.963739123311825e-06 BETTER
I0328 20:52:39.575703 2317917 finetune.py:68] layer 15_o @ epoch 3 new loss 1.201601435241173e-06 old loss 1.218448801409977e-06 BETTER
I0328 20:52:48.657735 2317847 finetune.py:45] layer 14_up initial loss 2.3023108042252716e-06
I0328 20:53:08.072735 2317777 finetune.py:68] layer 13_up @ epoch 2 new loss 1.9100596091448097e-06 old loss 1.9344151951372623e-06 BETTER
I0328 20:53:08.475528 2317707 finetune.py:68] layer 12_up @ epoch 3 new loss 1.610181357136753e-06 old loss 1.628039854040253e-06 BETTER
I0328 20:53:13.009647 2317917 finetune.py:68] layer 15_o @ epoch 4 new loss 1.1880094916705275e-06 old loss 1.201601435241173e-06 BETTER
I0328 20:53:19.517506 2317847 finetune.py:68] layer 14_up @ epoch 0 new loss 2.249080125693581e-06 old loss 2.3023108042252716e-06 BETTER
I0328 20:53:40.008494 2317777 finetune.py:68] layer 13_up @ epoch 3 new loss 1.8889057855631108e-06 old loss 1.9100596091448097e-06 BETTER
I0328 20:53:42.453334 2317707 finetune.py:68] layer 12_up @ epoch 4 new loss 1.594081709299644e-06 old loss 1.610181357136753e-06 BETTER
I0328 20:53:44.089059 2317917 finetune.py:45] layer 15_up initial loss 2.5207712042174535e-06
I0328 20:53:51.354213 2317847 finetune.py:68] layer 14_up @ epoch 1 new loss 2.2103999981482048e-06 old loss 2.249080125693581e-06 BETTER
I0328 20:54:12.111230 2317777 finetune.py:68] layer 13_up @ epoch 4 new loss 1.8701897488426766e-06 old loss 1.8889057855631108e-06 BETTER
I0328 20:54:13.770101 2317707 finetune.py:45] layer 12_gate initial loss 1.9685207917063963e-06
I0328 20:54:14.180377 2317917 finetune.py:68] layer 15_up @ epoch 0 new loss 2.4502751330146566e-06 old loss 2.5207712042174535e-06 BETTER
I0328 20:54:23.653638 2317847 finetune.py:68] layer 14_up @ epoch 2 new loss 2.1786863726447336e-06 old loss 2.2103999981482048e-06 BETTER
I0328 20:54:43.434362 2317777 finetune.py:45] layer 13_gate initial loss 2.2788449314248282e-06
I0328 20:54:43.800357 2317707 finetune.py:68] layer 12_gate @ epoch 0 new loss 1.951373405972845e-06 old loss 1.9685207917063963e-06 BETTER
I0328 20:54:45.073380 2317917 finetune.py:68] layer 15_up @ epoch 1 new loss 2.4001640213100472e-06 old loss 2.4502751330146566e-06 BETTER
I0328 20:54:55.904608 2317847 finetune.py:68] layer 14_up @ epoch 3 new loss 2.1516752894967794e-06 old loss 2.1786863726447336e-06 BETTER
I0328 20:55:11.858186 2317777 finetune.py:68] layer 13_gate @ epoch 0 new loss 2.258556605738704e-06 old loss 2.2788449314248282e-06 BETTER
I0328 20:55:14.899042 2317707 finetune.py:68] layer 12_gate @ epoch 1 new loss 1.9369629171706038e-06 old loss 1.951373405972845e-06 BETTER
I0328 20:55:16.244846 2317917 finetune.py:68] layer 15_up @ epoch 2 new loss 2.3597585823154077e-06 old loss 2.4001640213100472e-06 BETTER
I0328 20:55:28.094343 2317847 finetune.py:68] layer 14_up @ epoch 4 new loss 2.1277062387525802e-06 old loss 2.1516752894967794e-06 BETTER
I0328 20:55:41.310215 2317777 finetune.py:68] layer 13_gate @ epoch 1 new loss 2.24155905925727e-06 old loss 2.258556605738704e-06 BETTER
I0328 20:55:46.137626 2317707 finetune.py:68] layer 12_gate @ epoch 2 new loss 1.9240549136156915e-06 old loss 1.9369629171706038e-06 BETTER
I0328 20:55:47.527418 2317917 finetune.py:68] layer 15_up @ epoch 3 new loss 2.325391051272163e-06 old loss 2.3597585823154077e-06 BETTER
I0328 20:55:59.378011 2317847 finetune.py:45] layer 14_gate initial loss 2.5776114398468053e-06
I0328 20:56:10.917794 2317777 finetune.py:68] layer 13_gate @ epoch 2 new loss 2.226531023552525e-06 old loss 2.24155905925727e-06 BETTER
I0328 20:56:17.574859 2317707 finetune.py:68] layer 12_gate @ epoch 3 new loss 1.912166680995142e-06 old loss 1.9240549136156915e-06 BETTER
I0328 20:56:18.837953 2317917 finetune.py:68] layer 15_up @ epoch 4 new loss 2.295594867973705e-06 old loss 2.325391051272163e-06 BETTER
I0328 20:56:27.852424 2317847 finetune.py:68] layer 14_gate @ epoch 0 new loss 2.552379328335519e-06 old loss 2.5776114398468053e-06 BETTER
I0328 20:56:40.550230 2317777 finetune.py:68] layer 13_gate @ epoch 3 new loss 2.2126000658317935e-06 old loss 2.226531023552525e-06 BETTER
I0328 20:56:49.164679 2317707 finetune.py:68] layer 12_gate @ epoch 4 new loss 1.9013124301636708e-06 old loss 1.912166680995142e-06 BETTER
I0328 20:56:50.027467 2317917 finetune.py:45] layer 15_gate initial loss 2.7883038455911446e-06
I0328 20:56:57.568910 2317847 finetune.py:68] layer 14_gate @ epoch 1 new loss 2.531375912440126e-06 old loss 2.552379328335519e-06 BETTER
I0328 20:57:10.204115 2317777 finetune.py:68] layer 13_gate @ epoch 4 new loss 2.200187509515672e-06 old loss 2.2126000658317935e-06 BETTER
I0328 20:57:18.163514 2317917 finetune.py:68] layer 15_gate @ epoch 0 new loss 2.756784397206502e-06 old loss 2.7883038455911446e-06 BETTER
I0328 20:57:27.198303 2317847 finetune.py:68] layer 14_gate @ epoch 2 new loss 2.513044364604866e-06 old loss 2.531375912440126e-06 BETTER
I0328 20:57:44.355040 2317707 finetune.py:45] layer 12_down initial loss 2.8762174224539194e-06
I0328 20:57:47.157264 2317917 finetune.py:68] layer 15_gate @ epoch 1 new loss 2.7302635317028034e-06 old loss 2.756784397206502e-06 BETTER
I0328 20:57:57.001363 2317847 finetune.py:68] layer 14_gate @ epoch 3 new loss 2.4962662337202346e-06 old loss 2.513044364604866e-06 BETTER
I0328 20:58:06.686409 2317777 finetune.py:45] layer 13_down initial loss 3.358287358423695e-06
I0328 20:58:11.717439 2317707 finetune.py:68] layer 12_down @ epoch 0 new loss 2.8759050110238604e-06 old loss 2.8762174224539194e-06 BETTER
I0328 20:58:16.229684 2317917 finetune.py:68] layer 15_gate @ epoch 2 new loss 2.7072790089732734e-06 old loss 2.7302635317028034e-06 BETTER
I0328 20:58:26.909646 2317847 finetune.py:68] layer 14_gate @ epoch 4 new loss 2.4811035927996272e-06 old loss 2.4962662337202346e-06 BETTER
I0328 20:58:32.825312 2317777 finetune.py:68] layer 13_down @ epoch 0 new loss 3.3578480724827386e-06 old loss 3.358287358423695e-06 BETTER
I0328 20:58:40.304714 2317707 finetune.py:68] layer 12_down @ epoch 1 new loss 2.8757845029758755e-06 old loss 2.8759050110238604e-06 BETTER
I0328 20:58:45.699364 2317917 finetune.py:68] layer 15_gate @ epoch 3 new loss 2.6867332962865476e-06 old loss 2.7072790089732734e-06 BETTER
I0328 20:59:00.222302 2317777 finetune.py:68] layer 13_down @ epoch 1 new loss 3.3577316571609117e-06 old loss 3.3578480724827386e-06 BETTER
I0328 20:59:09.262973 2317707 finetune.py:68] layer 12_down @ epoch 2 new loss 2.8757362997566815e-06 old loss 2.8757845029758755e-06 BETTER
I0328 20:59:15.171796 2317917 finetune.py:68] layer 15_gate @ epoch 4 new loss 2.6682719180826098e-06 old loss 2.6867332962865476e-06 BETTER
I0328 20:59:24.370839 2317847 finetune.py:45] layer 14_down initial loss 3.885227670252789e-06
I0328 20:59:27.757987 2317777 finetune.py:68] layer 13_down @ epoch 2 new loss 3.3576811802049633e-06 old loss 3.3577316571609117e-06 BETTER
I0328 20:59:38.416818 2317707 finetune.py:68] layer 12_down @ epoch 3 new loss 2.875688323911163e-06 old loss 2.8757362997566815e-06 BETTER
I0328 20:59:50.690913 2317847 finetune.py:68] layer 14_down @ epoch 0 new loss 3.8847042560519185e-06 old loss 3.885227670252789e-06 BETTER
I0328 20:59:55.466310 2317777 finetune.py:68] layer 13_down @ epoch 3 new loss 3.357576815687935e-06 old loss 3.3576811802049633e-06 BETTER
I0328 21:00:07.439686 2317707 finetune.py:68] layer 12_down @ epoch 4 new loss 2.875664222301566e-06 old loss 2.875688323911163e-06 BETTER
12_v proxy err 0.00043522598571144044 tr(WHW.T) 363.6233825683594
bpp_loss 5.6217046396341175
12_q proxy err 2.5434192139073275e-05 tr(WHW.T) 34103.4765625
bpp_loss 6.429890209226869
12_k proxy err 1.4119207662588451e-05 tr(WHW.T) 23055.408203125
bpp_loss 7.63954737293534
12_o proxy err 0.0005048824241384864 tr(WHW.T) 780.076416015625
bpp_loss 5.68546528625302
12_up proxy err 0.00029959995299577713 tr(WHW.T) 10027.0361328125
bpp_loss 5.798958608880639
12_gate proxy err 8.772083674557507e-05 tr(WHW.T) 37319.31640625
bpp_loss 6.019799734199686
12_down proxy err 0.0004084911197423935 tr(WHW.T) 6823.43603515625
bpp_loss 5.7875164030785005
I0328 21:00:11.431963 2317917 finetune.py:45] layer 15_down initial loss 4.429242835612968e-06
I0328 21:00:18.464456 2317847 finetune.py:68] layer 14_down @ epoch 1 new loss 3.884516445396002e-06 old loss 3.8847042560519185e-06 BETTER
I0328 21:00:23.393456 2317777 finetune.py:68] layer 13_down @ epoch 4 new loss 3.3575679481145926e-06 old loss 3.357576815687935e-06 BETTER
13_v proxy err 0.0005364608368836343 tr(WHW.T) 280.153564453125
bpp_loss 5.562051376793534
13_q proxy err 4.053345037391409e-05 tr(WHW.T) 20898.392578125
bpp_loss 6.41735741798766
13_k proxy err 2.1707599444198422e-05 tr(WHW.T) 17798.38671875
bpp_loss 7.7026801612228155
13_o proxy err 0.0005281317862682045 tr(WHW.T) 675.384765625
bpp_loss 5.66444399242755
13_up proxy err 0.0002938276156783104 tr(WHW.T) 10010.1728515625
bpp_loss 5.802224562237305
13_gate proxy err 8.258177695097402e-05 tr(WHW.T) 38914.38671875
bpp_loss 6.0231384772009084
13_down proxy err 0.00041716909618116915 tr(WHW.T) 6561.10986328125
bpp_loss 5.787573299537014
I0328 21:00:37.859337 2317917 finetune.py:68] layer 15_down @ epoch 0 new loss 4.42860800831113e-06 old loss 4.429242835612968e-06 BETTER
I0328 21:00:46.255505 2317847 finetune.py:68] layer 14_down @ epoch 2 new loss 3.884408670273842e-06 old loss 3.884516445396002e-06 BETTER
I0328 21:01:04.909598 2317917 finetune.py:68] layer 15_down @ epoch 1 new loss 4.428416104929056e-06 old loss 4.42860800831113e-06 BETTER
I0328 21:01:14.039494 2317847 finetune.py:68] layer 14_down @ epoch 3 new loss 3.88434591513942e-06 old loss 3.884408670273842e-06 BETTER
I0328 21:01:26.439712 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 57.9627959728241s
I0328 21:01:30.578786 2317987 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:01:30.578885 2317987 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:01:30.578927 2317987 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:01:30.959234 2317987 config.py:54] PyTorch version 2.6.0 available.
W0328 21:01:31.184809 2317987 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:01:31.824026 2317987 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:01:31.828260 2316731 quantize_finetune_llama.py:209] layer 17 gpu 1
I0328 21:01:31.845949 2317987 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 21:01:32.088255 2317917 finetune.py:68] layer 15_down @ epoch 2 new loss 4.428330612427089e-06 old loss 4.428416104929056e-06 BETTER
I0328 21:01:41.986608 2317847 finetune.py:68] layer 14_down @ epoch 4 new loss 3.884297257172875e-06 old loss 3.88434591513942e-06 BETTER
14_v proxy err 0.0005104686133563519 tr(WHW.T) 281.3382873535156
bpp_loss 5.553883933695033
14_q proxy err 3.836789255728945e-05 tr(WHW.T) 20882.05859375
bpp_loss 6.3789964012103155
14_k proxy err 2.0819779820158146e-05 tr(WHW.T) 18617.15234375
bpp_loss 7.638439118978567
14_o proxy err 0.0005595952970907092 tr(WHW.T) 686.383544921875
bpp_loss 5.658951704856008
14_up proxy err 0.0003202123334631324 tr(WHW.T) 9163.6083984375
bpp_loss 5.797311940496521
14_gate proxy err 7.800306048011407e-05 tr(WHW.T) 41776.7578125
bpp_loss 6.059509934325304
14_down proxy err 0.0004441927303560078 tr(WHW.T) 6391.3291015625
bpp_loss 5.784499382334096
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:01:49.095288 2317987 finetune.py:45] layer 16_v initial loss 1.0844171356438892e-06
W0328 21:01:49.095676 2317987 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:01:59.221198 2317917 finetune.py:68] layer 15_down @ epoch 3 new loss 4.428258307598298e-06 old loss 4.428330612427089e-06 BETTER
I0328 21:02:24.009832 2317987 finetune.py:68] layer 16_v @ epoch 0 new loss 6.149481919237587e-07 old loss 1.0844171356438892e-06 BETTER
I0328 21:02:26.393098 2317917 finetune.py:68] layer 15_down @ epoch 4 new loss 4.428208285389701e-06 old loss 4.428258307598298e-06 BETTER
15_v proxy err 0.0005674110725522041 tr(WHW.T) 284.0271301269531
bpp_loss 5.617056197603233
15_q proxy err 3.4343716833973303e-05 tr(WHW.T) 28080.529296875
bpp_loss 6.557101015001535
15_k proxy err 2.0656751075875945e-05 tr(WHW.T) 18870.99609375
bpp_loss 7.664186060195789
15_o proxy err 0.0005614893743768334 tr(WHW.T) 828.2736206054688
bpp_loss 5.688852100400254
15_up proxy err 0.0003247627755627036 tr(WHW.T) 8994.3076171875
bpp_loss 5.788647682539055
15_gate proxy err 7.134822226362303e-05 tr(WHW.T) 46192.48828125
bpp_loss 6.0979732220460265
15_down proxy err 0.00044247054029256105 tr(WHW.T) 6400.49755859375
bpp_loss 5.778574005035417
I0328 21:02:51.043161 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 64.33972334861755s
I0328 21:02:54.616932 2318057 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:02:54.617032 2318057 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:02:54.617072 2318057 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:02:54.954414 2318057 config.py:54] PyTorch version 2.6.0 available.
W0328 21:02:55.150530 2318057 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:02:55.759270 2318057 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:02:55.763290 2316731 quantize_finetune_llama.py:209] layer 18 gpu 2
I0328 21:02:55.777610 2318057 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 21:03:00.331712 2317987 finetune.py:68] layer 16_v @ epoch 1 new loss 5.665035018864728e-07 old loss 6.149481919237587e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:03:12.702820 2318057 finetune.py:45] layer 17_v initial loss 1.122904677686165e-06
W0328 21:03:12.703018 2318057 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:03:37.040791 2317987 finetune.py:68] layer 16_v @ epoch 2 new loss 5.484844791681098e-07 old loss 5.665035018864728e-07 BETTER
I0328 21:03:45.744572 2318057 finetune.py:68] layer 17_v @ epoch 0 new loss 5.632352326756518e-07 old loss 1.122904677686165e-06 BETTER
I0328 21:03:58.988803 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 62.723684310913086s
I0328 21:04:02.711219 2318127 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:04:02.711334 2318127 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:04:02.711380 2318127 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:04:03.067625 2318127 config.py:54] PyTorch version 2.6.0 available.
W0328 21:04:03.271387 2318127 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:04:03.880367 2318127 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:04:03.884246 2316731 quantize_finetune_llama.py:209] layer 19 gpu 3
I0328 21:04:03.897510 2318127 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 21:04:13.873353 2317987 finetune.py:76] layer 16_v @ epoch 3 new loss 5.729261829401366e-07 old loss 5.484844791681098e-07 WORSE
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:04:20.173388 2318057 finetune.py:68] layer 17_v @ epoch 1 new loss 5.241744247541646e-07 old loss 5.632352326756518e-07 BETTER
I0328 21:04:21.045620 2318127 finetune.py:45] layer 18_v initial loss 1.6551546195842093e-06
W0328 21:04:21.046079 2318127 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:04:50.178591 2317987 finetune.py:76] layer 16_v @ epoch 4 new loss 5.879132913833018e-07 old loss 5.484844791681098e-07 WORSE
I0328 21:04:54.472085 2318127 finetune.py:68] layer 18_v @ epoch 0 new loss 4.976087666364037e-07 old loss 1.6551546195842093e-06 BETTER
I0328 21:04:54.790673 2318057 finetune.py:68] layer 17_v @ epoch 2 new loss 5.20003879955766e-07 old loss 5.241744247541646e-07 BETTER
I0328 21:05:08.300328 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 63.910311460494995s
I0328 21:05:09.023056 2317987 finetune.py:45] layer 16_q initial loss 6.647773602708185e-07
I0328 21:05:12.204856 2318197 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:05:12.204954 2318197 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:05:12.204995 2318197 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:05:12.557898 2318197 config.py:54] PyTorch version 2.6.0 available.
W0328 21:05:12.773069 2318197 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:05:13.367028 2318197 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:05:13.372055 2316731 quantize_finetune_llama.py:209] layer 20 gpu 0
I0328 21:05:13.387613 2318197 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:05:28.947027 2318127 finetune.py:68] layer 18_v @ epoch 1 new loss 4.5364609491116425e-07 old loss 4.976087666364037e-07 BETTER
I0328 21:05:29.500842 2318057 finetune.py:76] layer 17_v @ epoch 3 new loss 5.925233494963322e-07 old loss 5.20003879955766e-07 WORSE
I0328 21:05:30.342408 2318197 finetune.py:45] layer 19_v initial loss 1.8667068388822372e-06
W0328 21:05:30.342645 2318197 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:05:43.993427 2317987 finetune.py:68] layer 16_q @ epoch 0 new loss 6.554941478498222e-07 old loss 6.647773602708185e-07 BETTER
I0328 21:06:03.371323 2318197 finetune.py:68] layer 19_v @ epoch 0 new loss 5.194186201151751e-07 old loss 1.8667068388822372e-06 BETTER
I0328 21:06:03.960395 2318057 finetune.py:76] layer 17_v @ epoch 4 new loss 6.143590667306853e-07 old loss 5.20003879955766e-07 WORSE
I0328 21:06:04.220623 2318127 finetune.py:68] layer 18_v @ epoch 2 new loss 4.359033027867554e-07 old loss 4.5364609491116425e-07 BETTER
I0328 21:06:20.113797 2317987 finetune.py:68] layer 16_q @ epoch 1 new loss 6.417008080461528e-07 old loss 6.554941478498222e-07 BETTER
I0328 21:06:22.898037 2318057 finetune.py:45] layer 17_q initial loss 6.165772674648906e-07
I0328 21:06:37.415605 2318197 finetune.py:68] layer 19_v @ epoch 1 new loss 4.664989035063627e-07 old loss 5.194186201151751e-07 BETTER
I0328 21:06:39.046841 2318127 finetune.py:68] layer 18_v @ epoch 3 new loss 4.304482104089402e-07 old loss 4.359033027867554e-07 BETTER
I0328 21:06:56.071236 2318057 finetune.py:68] layer 17_q @ epoch 0 new loss 5.896616812606226e-07 old loss 6.165772674648906e-07 BETTER
I0328 21:06:56.432692 2317987 finetune.py:68] layer 16_q @ epoch 2 new loss 6.179777756187832e-07 old loss 6.417008080461528e-07 BETTER
I0328 21:07:11.741786 2318197 finetune.py:76] layer 19_v @ epoch 2 new loss 4.75242217135019e-07 old loss 4.664989035063627e-07 WORSE
I0328 21:07:14.087269 2318127 finetune.py:76] layer 18_v @ epoch 4 new loss 4.520044285527547e-07 old loss 4.304482104089402e-07 WORSE
I0328 21:07:30.341256 2318057 finetune.py:68] layer 17_q @ epoch 1 new loss 5.769642257291707e-07 old loss 5.896616812606226e-07 BETTER
I0328 21:07:32.873187 2317987 finetune.py:68] layer 16_q @ epoch 3 new loss 5.972057124381536e-07 old loss 6.179777756187832e-07 BETTER
I0328 21:07:33.004914 2318127 finetune.py:45] layer 18_q initial loss 5.203032742429059e-07
I0328 21:07:45.913083 2318197 finetune.py:76] layer 19_v @ epoch 3 new loss 4.861224169872003e-07 old loss 4.664989035063627e-07 WORSE
I0328 21:08:04.980379 2318057 finetune.py:68] layer 17_q @ epoch 2 new loss 5.720144713450281e-07 old loss 5.769642257291707e-07 BETTER
I0328 21:08:06.720354 2318127 finetune.py:68] layer 18_q @ epoch 0 new loss 5.063859589427011e-07 old loss 5.203032742429059e-07 BETTER
I0328 21:08:09.414987 2317987 finetune.py:68] layer 16_q @ epoch 4 new loss 5.925429036324203e-07 old loss 5.972057124381536e-07 BETTER
I0328 21:08:19.922767 2318197 finetune.py:68] layer 19_v @ epoch 4 new loss 4.612427915162698e-07 old loss 4.664989035063627e-07 BETTER
I0328 21:08:27.066107 2317987 finetune.py:45] layer 16_k initial loss 6.798793492635014e-07
I0328 21:08:39.073093 2318197 finetune.py:45] layer 19_q initial loss 5.494971446751151e-07
I0328 21:08:39.497420 2318057 finetune.py:68] layer 17_q @ epoch 3 new loss 5.604725856755977e-07 old loss 5.720144713450281e-07 BETTER
I0328 21:08:41.274574 2318127 finetune.py:68] layer 18_q @ epoch 1 new loss 4.809415372619696e-07 old loss 5.063859589427011e-07 BETTER
I0328 21:09:02.395271 2317987 finetune.py:68] layer 16_k @ epoch 0 new loss 6.55664109672216e-07 old loss 6.798793492635014e-07 BETTER
I0328 21:09:11.925314 2318197 finetune.py:68] layer 19_q @ epoch 0 new loss 5.061076535639586e-07 old loss 5.494971446751151e-07 BETTER
I0328 21:09:14.081122 2318057 finetune.py:76] layer 17_q @ epoch 4 new loss 5.60594969556405e-07 old loss 5.604725856755977e-07 WORSE
I0328 21:09:15.813438 2318127 finetune.py:68] layer 18_q @ epoch 2 new loss 4.7182322759908857e-07 old loss 4.809415372619696e-07 BETTER
I0328 21:09:31.540463 2318057 finetune.py:45] layer 17_k initial loss 6.427517860174703e-07
I0328 21:09:38.589837 2317987 finetune.py:68] layer 16_k @ epoch 1 new loss 6.538613774864643e-07 old loss 6.55664109672216e-07 BETTER
I0328 21:09:45.936262 2318197 finetune.py:68] layer 19_q @ epoch 1 new loss 4.911343012281577e-07 old loss 5.061076535639586e-07 BETTER
I0328 21:09:50.428790 2318127 finetune.py:68] layer 18_q @ epoch 3 new loss 4.626805321095162e-07 old loss 4.7182322759908857e-07 BETTER
I0328 21:10:04.780838 2318057 finetune.py:68] layer 17_k @ epoch 0 new loss 6.264357921281771e-07 old loss 6.427517860174703e-07 BETTER
I0328 21:10:14.880759 2317987 finetune.py:76] layer 16_k @ epoch 2 new loss 6.624434831792314e-07 old loss 6.538613774864643e-07 WORSE
I0328 21:10:20.046128 2318197 finetune.py:68] layer 19_q @ epoch 2 new loss 4.707012237759045e-07 old loss 4.911343012281577e-07 BETTER
I0328 21:10:25.046736 2318127 finetune.py:68] layer 18_q @ epoch 4 new loss 4.569349698613223e-07 old loss 4.626805321095162e-07 BETTER
I0328 21:10:38.911833 2318057 finetune.py:68] layer 17_k @ epoch 1 new loss 6.104533554207592e-07 old loss 6.264357921281771e-07 BETTER
I0328 21:10:43.101990 2318127 finetune.py:45] layer 18_k initial loss 5.271072041068692e-07
I0328 21:10:50.662020 2317987 finetune.py:76] layer 16_k @ epoch 3 new loss 6.661221618742275e-07 old loss 6.538613774864643e-07 WORSE
I0328 21:10:53.996801 2318197 finetune.py:76] layer 19_q @ epoch 3 new loss 4.7303657879638195e-07 old loss 4.707012237759045e-07 WORSE
I0328 21:11:13.146280 2318057 finetune.py:76] layer 17_k @ epoch 2 new loss 6.112439336902753e-07 old loss 6.104533554207592e-07 WORSE
I0328 21:11:16.454373 2318127 finetune.py:68] layer 18_k @ epoch 0 new loss 5.155075086804572e-07 old loss 5.271072041068692e-07 BETTER
I0328 21:11:26.479148 2317987 finetune.py:68] layer 16_k @ epoch 4 new loss 6.483641641352733e-07 old loss 6.538613774864643e-07 BETTER
I0328 21:11:27.446514 2318197 finetune.py:68] layer 19_q @ epoch 4 new loss 4.5574620344268624e-07 old loss 4.707012237759045e-07 BETTER
I0328 21:11:44.917844 2318197 finetune.py:45] layer 19_k initial loss 5.349572802515468e-07
I0328 21:11:45.798315 2317987 finetune.py:45] layer 16_o initial loss 1.380919229632127e-06
I0328 21:11:46.821288 2318057 finetune.py:68] layer 17_k @ epoch 3 new loss 6.022409024808439e-07 old loss 6.104533554207592e-07 BETTER
I0328 21:11:50.996552 2318127 finetune.py:68] layer 18_k @ epoch 1 new loss 5.081063818579423e-07 old loss 5.155075086804572e-07 BETTER
I0328 21:12:17.782858 2318197 finetune.py:68] layer 19_k @ epoch 0 new loss 5.218405476625776e-07 old loss 5.349572802515468e-07 BETTER
I0328 21:12:20.254756 2317987 finetune.py:68] layer 16_o @ epoch 0 new loss 1.3191712469051708e-06 old loss 1.380919229632127e-06 BETTER
I0328 21:12:21.242004 2318057 finetune.py:76] layer 17_k @ epoch 4 new loss 6.089159683142498e-07 old loss 6.022409024808439e-07 WORSE
I0328 21:12:25.446365 2318127 finetune.py:76] layer 18_k @ epoch 2 new loss 5.092700234854419e-07 old loss 5.081063818579423e-07 WORSE
I0328 21:12:40.541656 2318057 finetune.py:45] layer 17_o initial loss 1.3094072528474499e-06
I0328 21:12:51.658500 2318197 finetune.py:68] layer 19_k @ epoch 1 new loss 5.197864538786234e-07 old loss 5.218405476625776e-07 BETTER
I0328 21:12:55.968678 2317987 finetune.py:68] layer 16_o @ epoch 1 new loss 1.2845374612879823e-06 old loss 1.3191712469051708e-06 BETTER
I0328 21:12:59.563561 2318127 finetune.py:76] layer 18_k @ epoch 3 new loss 5.139590939506888e-07 old loss 5.081063818579423e-07 WORSE
I0328 21:13:13.092261 2318057 finetune.py:68] layer 17_o @ epoch 0 new loss 1.2580637758219382e-06 old loss 1.3094072528474499e-06 BETTER
I0328 21:13:25.590228 2318197 finetune.py:68] layer 19_k @ epoch 2 new loss 5.174783836991992e-07 old loss 5.197864538786234e-07 BETTER
I0328 21:13:31.914574 2317987 finetune.py:68] layer 16_o @ epoch 2 new loss 1.2646435152419144e-06 old loss 1.2845374612879823e-06 BETTER
I0328 21:13:33.448888 2318127 finetune.py:68] layer 18_k @ epoch 4 new loss 5.048823936704139e-07 old loss 5.081063818579423e-07 BETTER
I0328 21:13:46.628441 2318057 finetune.py:68] layer 17_o @ epoch 1 new loss 1.2254600960659445e-06 old loss 1.2580637758219382e-06 BETTER
I0328 21:13:52.628412 2318127 finetune.py:45] layer 18_o initial loss 1.07821676920139e-06
I0328 21:13:59.658089 2318197 finetune.py:68] layer 19_k @ epoch 3 new loss 5.168537882127566e-07 old loss 5.174783836991992e-07 BETTER
I0328 21:14:07.743401 2317987 finetune.py:68] layer 16_o @ epoch 3 new loss 1.2481337989811436e-06 old loss 1.2646435152419144e-06 BETTER
I0328 21:14:20.320718 2318057 finetune.py:68] layer 17_o @ epoch 2 new loss 1.2019742143820622e-06 old loss 1.2254600960659445e-06 BETTER
I0328 21:14:25.488955 2318127 finetune.py:68] layer 18_o @ epoch 0 new loss 1.0329109727535979e-06 old loss 1.07821676920139e-06 BETTER
I0328 21:14:33.543999 2318197 finetune.py:76] layer 19_k @ epoch 4 new loss 5.197406380830216e-07 old loss 5.168537882127566e-07 WORSE
I0328 21:14:43.587160 2317987 finetune.py:68] layer 16_o @ epoch 4 new loss 1.234823230333859e-06 old loss 1.2481337989811436e-06 BETTER
I0328 21:14:52.340137 2318197 finetune.py:45] layer 19_o initial loss 1.0407188710814808e-06
I0328 21:14:53.962167 2318057 finetune.py:68] layer 17_o @ epoch 3 new loss 1.18542914151476e-06 old loss 1.2019742143820622e-06 BETTER
I0328 21:14:59.208163 2318127 finetune.py:68] layer 18_o @ epoch 1 new loss 1.010490336739167e-06 old loss 1.0329109727535979e-06 BETTER
I0328 21:15:14.698312 2317987 finetune.py:45] layer 16_up initial loss 2.6665882160159526e-06
I0328 21:15:24.251280 2318197 finetune.py:68] layer 19_o @ epoch 0 new loss 1.0045282579085324e-06 old loss 1.0407188710814808e-06 BETTER
I0328 21:15:27.578329 2318057 finetune.py:68] layer 17_o @ epoch 4 new loss 1.172402562588104e-06 old loss 1.18542914151476e-06 BETTER
I0328 21:15:33.182639 2318127 finetune.py:68] layer 18_o @ epoch 2 new loss 9.974012300517643e-07 old loss 1.010490336739167e-06 BETTER
I0328 21:15:46.754416 2317987 finetune.py:68] layer 16_up @ epoch 0 new loss 2.596871581772575e-06 old loss 2.6665882160159526e-06 BETTER
I0328 21:15:57.463193 2318197 finetune.py:68] layer 19_o @ epoch 1 new loss 9.854869631453766e-07 old loss 1.0045282579085324e-06 BETTER
I0328 21:15:58.856860 2318057 finetune.py:45] layer 17_up initial loss 2.7950402454735013e-06
I0328 21:16:07.249023 2318127 finetune.py:68] layer 18_o @ epoch 3 new loss 9.874225952444249e-07 old loss 9.974012300517643e-07 BETTER
I0328 21:16:19.826675 2317987 finetune.py:68] layer 16_up @ epoch 1 new loss 2.547179747125483e-06 old loss 2.596871581772575e-06 BETTER
I0328 21:16:29.260944 2318057 finetune.py:68] layer 17_up @ epoch 0 new loss 2.7137800771015463e-06 old loss 2.7950402454735013e-06 BETTER
I0328 21:16:30.770449 2318197 finetune.py:68] layer 19_o @ epoch 2 new loss 9.766978337211185e-07 old loss 9.854869631453766e-07 BETTER
I0328 21:16:41.357666 2318127 finetune.py:68] layer 18_o @ epoch 4 new loss 9.798958444662276e-07 old loss 9.874225952444249e-07 BETTER
I0328 21:16:52.990738 2317987 finetune.py:68] layer 16_up @ epoch 2 new loss 2.5067583919735625e-06 old loss 2.547179747125483e-06 BETTER
I0328 21:17:00.649061 2318057 finetune.py:68] layer 17_up @ epoch 1 new loss 2.6565844564174768e-06 old loss 2.7137800771015463e-06 BETTER
I0328 21:17:04.045809 2318197 finetune.py:68] layer 19_o @ epoch 3 new loss 9.672512533143163e-07 old loss 9.766978337211185e-07 BETTER
I0328 21:17:12.763340 2318127 finetune.py:45] layer 18_up initial loss 2.6509669623919763e-06
I0328 21:17:26.591590 2317987 finetune.py:68] layer 16_up @ epoch 3 new loss 2.4726034553168574e-06 old loss 2.5067583919735625e-06 BETTER
I0328 21:17:32.317301 2318057 finetune.py:68] layer 17_up @ epoch 2 new loss 2.611783202155493e-06 old loss 2.6565844564174768e-06 BETTER
I0328 21:17:37.715523 2318197 finetune.py:68] layer 19_o @ epoch 4 new loss 9.601906185707776e-07 old loss 9.672512533143163e-07 BETTER
I0328 21:17:43.519589 2318127 finetune.py:68] layer 18_up @ epoch 0 new loss 2.578440899014822e-06 old loss 2.6509669623919763e-06 BETTER
I0328 21:18:00.159477 2317987 finetune.py:68] layer 16_up @ epoch 4 new loss 2.4430162284261314e-06 old loss 2.4726034553168574e-06 BETTER
I0328 21:18:04.285735 2318057 finetune.py:68] layer 17_up @ epoch 3 new loss 2.574411155364942e-06 old loss 2.611783202155493e-06 BETTER
I0328 21:18:08.938516 2318197 finetune.py:45] layer 19_up initial loss 2.7713908821169753e-06
I0328 21:18:15.310859 2318127 finetune.py:68] layer 18_up @ epoch 1 new loss 2.5287081371061504e-06 old loss 2.578440899014822e-06 BETTER
I0328 21:18:31.432863 2317987 finetune.py:45] layer 16_gate initial loss 2.974687731693848e-06
I0328 21:18:36.010179 2318057 finetune.py:68] layer 17_up @ epoch 4 new loss 2.542720267229015e-06 old loss 2.574411155364942e-06 BETTER
I0328 21:18:39.028783 2318197 finetune.py:68] layer 19_up @ epoch 0 new loss 2.697623813219252e-06 old loss 2.7713908821169753e-06 BETTER
I0328 21:18:47.403328 2318127 finetune.py:68] layer 18_up @ epoch 2 new loss 2.4901416963984957e-06 old loss 2.5287081371061504e-06 BETTER
I0328 21:19:01.495329 2317987 finetune.py:68] layer 16_gate @ epoch 0 new loss 2.944147809103015e-06 old loss 2.974687731693848e-06 BETTER
I0328 21:19:07.539639 2318057 finetune.py:45] layer 17_gate initial loss 3.1532895263808314e-06
I0328 21:19:10.086401 2318197 finetune.py:68] layer 19_up @ epoch 1 new loss 2.64643654190877e-06 old loss 2.697623813219252e-06 BETTER
I0328 21:19:19.581045 2318127 finetune.py:68] layer 18_up @ epoch 3 new loss 2.457291429891484e-06 old loss 2.4901416963984957e-06 BETTER
I0328 21:19:32.804460 2317987 finetune.py:68] layer 16_gate @ epoch 1 new loss 2.918854988820385e-06 old loss 2.944147809103015e-06 BETTER
I0328 21:19:35.896294 2318057 finetune.py:68] layer 17_gate @ epoch 0 new loss 3.1185775242192904e-06 old loss 3.1532895263808314e-06 BETTER
I0328 21:19:41.356021 2318197 finetune.py:68] layer 19_up @ epoch 2 new loss 2.6065215479320614e-06 old loss 2.64643654190877e-06 BETTER
I0328 21:19:51.802820 2318127 finetune.py:68] layer 18_up @ epoch 4 new loss 2.4297173695231322e-06 old loss 2.457291429891484e-06 BETTER
I0328 21:20:04.191780 2317987 finetune.py:68] layer 16_gate @ epoch 2 new loss 2.8966114768991247e-06 old loss 2.918854988820385e-06 BETTER
I0328 21:20:05.409987 2318057 finetune.py:68] layer 17_gate @ epoch 1 new loss 3.0912613055988913e-06 old loss 3.1185775242192904e-06 BETTER
I0328 21:20:12.564224 2318197 finetune.py:68] layer 19_up @ epoch 3 new loss 2.5741683202795684e-06 old loss 2.6065215479320614e-06 BETTER
I0328 21:20:23.110539 2318127 finetune.py:45] layer 18_gate initial loss 3.0888445508026052e-06
I0328 21:20:34.865111 2318057 finetune.py:68] layer 17_gate @ epoch 2 new loss 3.0673954825033434e-06 old loss 3.0912613055988913e-06 BETTER
I0328 21:20:35.604483 2317987 finetune.py:68] layer 16_gate @ epoch 3 new loss 2.8767587991751498e-06 old loss 2.8966114768991247e-06 BETTER
I0328 21:20:43.953009 2318197 finetune.py:68] layer 19_up @ epoch 4 new loss 2.5463857582508354e-06 old loss 2.5741683202795684e-06 BETTER
I0328 21:20:51.902132 2318127 finetune.py:68] layer 18_gate @ epoch 0 new loss 3.0590410915465327e-06 old loss 3.0888445508026052e-06 BETTER
I0328 21:21:04.489211 2318057 finetune.py:68] layer 17_gate @ epoch 3 new loss 3.046267238460132e-06 old loss 3.0673954825033434e-06 BETTER
I0328 21:21:07.128009 2317987 finetune.py:68] layer 16_gate @ epoch 4 new loss 2.859089590856456e-06 old loss 2.8767587991751498e-06 BETTER
I0328 21:21:15.121386 2318197 finetune.py:45] layer 19_gate initial loss 3.2755997381173074e-06
I0328 21:21:21.512031 2318127 finetune.py:68] layer 18_gate @ epoch 1 new loss 3.035329882550286e-06 old loss 3.0590410915465327e-06 BETTER
I0328 21:21:34.266378 2318057 finetune.py:68] layer 17_gate @ epoch 4 new loss 3.0275027711468283e-06 old loss 3.046267238460132e-06 BETTER
I0328 21:21:43.094705 2318197 finetune.py:68] layer 19_gate @ epoch 0 new loss 3.2454399843118154e-06 old loss 3.2755997381173074e-06 BETTER
I0328 21:21:51.375324 2318127 finetune.py:68] layer 18_gate @ epoch 2 new loss 3.014380808963324e-06 old loss 3.035329882550286e-06 BETTER
I0328 21:22:02.294497 2317987 finetune.py:45] layer 16_down initial loss 4.821897618967341e-06
I0328 21:22:12.165447 2318197 finetune.py:68] layer 19_gate @ epoch 1 new loss 3.2214766179095022e-06 old loss 3.2454399843118154e-06 BETTER
I0328 21:22:21.267488 2318127 finetune.py:68] layer 18_gate @ epoch 3 new loss 2.9964469376864145e-06 old loss 3.014380808963324e-06 BETTER
I0328 21:22:29.550198 2317987 finetune.py:68] layer 16_down @ epoch 0 new loss 4.82106088384171e-06 old loss 4.821897618967341e-06 BETTER
I0328 21:22:30.888356 2318057 finetune.py:45] layer 17_down initial loss 5.403257091529667e-06
I0328 21:22:41.557850 2318197 finetune.py:68] layer 19_gate @ epoch 2 new loss 3.2004343211156083e-06 old loss 3.2214766179095022e-06 BETTER
I0328 21:22:51.186913 2318127 finetune.py:68] layer 18_gate @ epoch 4 new loss 2.9800410175084835e-06 old loss 2.9964469376864145e-06 BETTER
I0328 21:22:57.029724 2318057 finetune.py:68] layer 17_down @ epoch 0 new loss 5.4023353186494205e-06 old loss 5.403257091529667e-06 BETTER
I0328 21:22:58.121336 2317987 finetune.py:68] layer 16_down @ epoch 1 new loss 4.820837602892425e-06 old loss 4.82106088384171e-06 BETTER
I0328 21:23:10.628708 2318197 finetune.py:68] layer 19_gate @ epoch 3 new loss 3.1824238249100745e-06 old loss 3.2004343211156083e-06 BETTER
I0328 21:23:24.071139 2318057 finetune.py:68] layer 17_down @ epoch 1 new loss 5.402058377512731e-06 old loss 5.4023353186494205e-06 BETTER
I0328 21:23:27.067541 2317987 finetune.py:68] layer 16_down @ epoch 2 new loss 4.820708454644773e-06 old loss 4.820837602892425e-06 BETTER
I0328 21:23:39.960316 2318197 finetune.py:68] layer 19_gate @ epoch 4 new loss 3.1663805657444755e-06 old loss 3.1824238249100745e-06 BETTER
I0328 21:23:48.036033 2318127 finetune.py:45] layer 18_down initial loss 5.40110886504408e-06
I0328 21:23:51.492167 2318057 finetune.py:68] layer 17_down @ epoch 2 new loss 5.401953785622027e-06 old loss 5.402058377512731e-06 BETTER
I0328 21:23:55.881424 2317987 finetune.py:68] layer 16_down @ epoch 3 new loss 4.820600224775262e-06 old loss 4.820708454644773e-06 BETTER
I0328 21:24:14.375748 2318127 finetune.py:68] layer 18_down @ epoch 0 new loss 5.400227564678062e-06 old loss 5.40110886504408e-06 BETTER
I0328 21:24:19.161236 2318057 finetune.py:68] layer 17_down @ epoch 3 new loss 5.4018228183849715e-06 old loss 5.401953785622027e-06 BETTER
I0328 21:24:25.018780 2317987 finetune.py:68] layer 16_down @ epoch 4 new loss 4.820548383577261e-06 old loss 4.820600224775262e-06 BETTER
16_v proxy err 0.0005468816380016506 tr(WHW.T) 274.28167724609375
bpp_loss 5.570835082326084
16_q proxy err 3.59094119630754e-05 tr(WHW.T) 24487.359375
bpp_loss 6.510969249880873
16_k proxy err 2.1959480363875628e-05 tr(WHW.T) 19503.408203125
bpp_loss 7.667052206583321
16_o proxy err 0.0004759287985507399 tr(WHW.T) 969.3191528320312
bpp_loss 5.669264515396208
16_up proxy err 0.0003638674970716238 tr(WHW.T) 8330.6357421875
bpp_loss 5.7742673279717565
16_gate proxy err 8.426720160059631e-05 tr(WHW.T) 41171.0625
bpp_loss 6.134832940463509
16_down proxy err 0.0004644129076041281 tr(WHW.T) 6289.1328125
bpp_loss 5.764244279724413
I0328 21:24:36.662604 2318197 finetune.py:45] layer 19_down initial loss 5.749525826104218e-06
I0328 21:24:42.168157 2318127 finetune.py:68] layer 18_down @ epoch 1 new loss 5.399998372013215e-06 old loss 5.400227564678062e-06 BETTER
I0328 21:24:47.116347 2318057 finetune.py:68] layer 17_down @ epoch 4 new loss 5.401773250923725e-06 old loss 5.4018228183849715e-06 BETTER
17_v proxy err 0.0005894552450627089 tr(WHW.T) 283.9730224609375
bpp_loss 5.6444308393402025
17_q proxy err 3.538911187206395e-05 tr(WHW.T) 27572.580078125
bpp_loss 6.531047501368448
17_k proxy err 2.745488382061012e-05 tr(WHW.T) 17428.951171875
bpp_loss 7.701074230019003
17_o proxy err 0.0005199616425670683 tr(WHW.T) 1107.0526123046875
bpp_loss 5.698590945452452
17_up proxy err 0.0003586263337638229 tr(WHW.T) 8451.9169921875
bpp_loss 5.770370237396231
17_gate proxy err 8.363966480828822e-05 tr(WHW.T) 41706.66015625
bpp_loss 6.150863785429725
17_down proxy err 0.0004696095420513302 tr(WHW.T) 6212.58642578125
bpp_loss 5.76178032502399
I0328 21:25:02.608174 2318197 finetune.py:68] layer 19_down @ epoch 0 new loss 5.748359399149194e-06 old loss 5.749525826104218e-06 BETTER
I0328 21:25:09.848129 2318127 finetune.py:68] layer 18_down @ epoch 2 new loss 5.399838755693054e-06 old loss 5.399998372013215e-06 BETTER
I0328 21:25:29.500746 2318197 finetune.py:68] layer 19_down @ epoch 1 new loss 5.7480342547933105e-06 old loss 5.748359399149194e-06 BETTER
I0328 21:25:37.596204 2318127 finetune.py:68] layer 18_down @ epoch 3 new loss 5.399769634095719e-06 old loss 5.399838755693054e-06 BETTER
I0328 21:25:56.637557 2318197 finetune.py:68] layer 19_down @ epoch 2 new loss 5.747845079895342e-06 old loss 5.7480342547933105e-06 BETTER
I0328 21:25:59.895069 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 67.76443123817444s
I0328 21:26:03.842205 2318267 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:26:03.842304 2318267 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:26:03.842343 2318267 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:26:04.226349 2318267 config.py:54] PyTorch version 2.6.0 available.
W0328 21:26:04.454038 2318267 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:26:05.115072 2318267 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:26:05.119128 2316731 quantize_finetune_llama.py:209] layer 21 gpu 1
I0328 21:26:05.138840 2318267 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 21:26:05.357241 2318127 finetune.py:68] layer 18_down @ epoch 4 new loss 5.399709152698051e-06 old loss 5.399769634095719e-06 BETTER
18_v proxy err 0.000586214242503047 tr(WHW.T) 287.61376953125
bpp_loss 5.5657358868047595
18_q proxy err 4.382816041470505e-05 tr(WHW.T) 22397.802734375
bpp_loss 6.514537594397552
18_k proxy err 2.6535710276220925e-05 tr(WHW.T) 17391.732421875
bpp_loss 7.806282165576704
18_o proxy err 0.0005195985431782901 tr(WHW.T) 1202.7972412109375
bpp_loss 5.677308129728772
18_up proxy err 0.00039252120768651366 tr(WHW.T) 7983.8115234375
bpp_loss 5.765381364817066
18_gate proxy err 0.00010255280358251184 tr(WHW.T) 35254.8515625
bpp_loss 6.155248972720334
18_down proxy err 0.00048424117267131805 tr(WHW.T) 6227.9912109375
bpp_loss 5.7591831538427085
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:26:22.206300 2318267 finetune.py:45] layer 20_v initial loss 2.0884124296571827e-06
W0328 21:26:22.206717 2318267 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:26:23.871306 2318197 finetune.py:68] layer 19_down @ epoch 3 new loss 5.747752766183112e-06 old loss 5.747845079895342e-06 BETTER
I0328 21:26:51.040930 2318197 finetune.py:68] layer 19_down @ epoch 4 new loss 5.747669092670549e-06 old loss 5.747752766183112e-06 BETTER
19_v proxy err 0.0005127132753841579 tr(WHW.T) 341.0596618652344
bpp_loss 5.608814376289956
19_q proxy err 4.206900121062063e-05 tr(WHW.T) 24041.4921875
bpp_loss 6.517311559058726
19_k proxy err 2.8414680855348706e-05 tr(WHW.T) 15552.3505859375
bpp_loss 7.663364105159417
19_o proxy err 0.0005383184761740267 tr(WHW.T) 1168.9835205078125
bpp_loss 5.690171770751476
19_up proxy err 0.0004152256587985903 tr(WHW.T) 7649.1923828125
bpp_loss 5.7613341728491445
19_gate proxy err 0.000112144771264866 tr(WHW.T) 32791.50390625
bpp_loss 6.165881926459925
19_down proxy err 0.0004951086011715233 tr(WHW.T) 6184.671875
bpp_loss 5.756953846896067
I0328 21:26:56.903696 2318267 finetune.py:68] layer 20_v @ epoch 0 new loss 5.763822059634549e-07 old loss 2.0884124296571827e-06 BETTER
I0328 21:27:14.265372 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 64.42577457427979s
I0328 21:27:17.803102 2318337 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:27:17.803199 2318337 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:27:17.803239 2318337 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:27:18.139754 2318337 config.py:54] PyTorch version 2.6.0 available.
W0328 21:27:18.332350 2318337 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:27:18.934580 2318337 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:27:18.938302 2316731 quantize_finetune_llama.py:209] layer 22 gpu 2
I0328 21:27:18.951735 2318337 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:27:33.082074 2318267 finetune.py:68] layer 20_v @ epoch 1 new loss 5.087493377686769e-07 old loss 5.763822059634549e-07 BETTER
I0328 21:27:35.512841 2318337 finetune.py:45] layer 21_v initial loss 1.7617231833355618e-06
W0328 21:27:35.513252 2318337 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:28:08.672981 2318337 finetune.py:68] layer 21_v @ epoch 0 new loss 6.375590828611166e-07 old loss 1.7617231833355618e-06 BETTER
I0328 21:28:09.714599 2318267 finetune.py:68] layer 20_v @ epoch 2 new loss 4.82232280774042e-07 old loss 5.087493377686769e-07 BETTER
I0328 21:28:22.784136 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 63.382503509521484s
I0328 21:28:26.543699 2318407 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:28:26.543807 2318407 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:28:26.543858 2318407 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:28:26.913912 2318407 config.py:54] PyTorch version 2.6.0 available.
W0328 21:28:27.118412 2318407 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:28:27.719432 2318407 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:28:27.723366 2316731 quantize_finetune_llama.py:209] layer 23 gpu 3
I0328 21:28:27.736967 2318407 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:28:43.270514 2318337 finetune.py:68] layer 21_v @ epoch 1 new loss 5.77908281229611e-07 old loss 6.375590828611166e-07 BETTER
I0328 21:28:45.336664 2318407 finetune.py:45] layer 22_v initial loss 2.147571194655029e-06
W0328 21:28:45.336912 2318407 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:28:46.644290 2318267 finetune.py:76] layer 20_v @ epoch 3 new loss 4.927780423713557e-07 old loss 4.82232280774042e-07 WORSE
I0328 21:29:18.180446 2318337 finetune.py:68] layer 21_v @ epoch 2 new loss 5.702026442122587e-07 old loss 5.77908281229611e-07 BETTER
I0328 21:29:18.997357 2318407 finetune.py:68] layer 22_v @ epoch 0 new loss 5.731147894039168e-07 old loss 2.147571194655029e-06 BETTER
I0328 21:29:23.051580 2318267 finetune.py:68] layer 20_v @ epoch 4 new loss 4.79133063890913e-07 old loss 4.82232280774042e-07 BETTER
I0328 21:29:32.366664 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 64.14791202545166s
I0328 21:29:36.282346 2318477 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:29:36.282462 2318477 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:29:36.282502 2318477 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:29:36.667655 2318477 config.py:54] PyTorch version 2.6.0 available.
W0328 21:29:36.897009 2318477 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:29:37.525431 2318477 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:29:37.529111 2316731 quantize_finetune_llama.py:209] layer 24 gpu 0
I0328 21:29:37.542540 2318477 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 21:29:42.341740 2318267 finetune.py:45] layer 20_q initial loss 5.82596101139643e-07
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:29:53.215889 2318337 finetune.py:76] layer 21_v @ epoch 3 new loss 5.777614546786936e-07 old loss 5.702026442122587e-07 WORSE
I0328 21:29:53.674384 2318407 finetune.py:68] layer 22_v @ epoch 1 new loss 5.13092459186737e-07 old loss 5.731147894039168e-07 BETTER
I0328 21:29:55.071223 2318477 finetune.py:45] layer 23_v initial loss 3.13662167172879e-06
W0328 21:29:55.071590 2318477 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:30:17.477253 2318267 finetune.py:68] layer 20_q @ epoch 0 new loss 5.440408585855039e-07 old loss 5.82596101139643e-07 BETTER
I0328 21:30:27.670564 2318337 finetune.py:76] layer 21_v @ epoch 4 new loss 5.737070409850276e-07 old loss 5.702026442122587e-07 WORSE
I0328 21:30:28.209304 2318477 finetune.py:68] layer 23_v @ epoch 0 new loss 6.18856290657277e-07 old loss 3.13662167172879e-06 BETTER
I0328 21:30:28.804946 2318407 finetune.py:68] layer 22_v @ epoch 2 new loss 4.851226549362764e-07 old loss 5.13092459186737e-07 BETTER
I0328 21:30:46.522943 2318337 finetune.py:45] layer 21_q initial loss 7.191351301116811e-07
I0328 21:30:53.531230 2318267 finetune.py:68] layer 20_q @ epoch 1 new loss 5.245696570455038e-07 old loss 5.440408585855039e-07 BETTER
I0328 21:31:02.357812 2318477 finetune.py:68] layer 23_v @ epoch 1 new loss 5.357615577850083e-07 old loss 6.18856290657277e-07 BETTER
I0328 21:31:04.025377 2318407 finetune.py:76] layer 22_v @ epoch 3 new loss 4.927981080982136e-07 old loss 4.851226549362764e-07 WORSE
I0328 21:31:19.915656 2318337 finetune.py:68] layer 21_q @ epoch 0 new loss 6.631222504438483e-07 old loss 7.191351301116811e-07 BETTER
I0328 21:31:29.951228 2318267 finetune.py:68] layer 20_q @ epoch 2 new loss 5.114492296343087e-07 old loss 5.245696570455038e-07 BETTER
I0328 21:31:36.724551 2318477 finetune.py:68] layer 23_v @ epoch 2 new loss 5.126181576997624e-07 old loss 5.357615577850083e-07 BETTER
I0328 21:31:38.510449 2318407 finetune.py:68] layer 22_v @ epoch 4 new loss 4.808580342796631e-07 old loss 4.851226549362764e-07 BETTER
I0328 21:31:54.092942 2318337 finetune.py:68] layer 21_q @ epoch 1 new loss 6.491864041890949e-07 old loss 6.631222504438483e-07 BETTER
I0328 21:31:58.296082 2318407 finetune.py:45] layer 22_q initial loss 6.13918132330582e-07
I0328 21:32:06.336462 2318267 finetune.py:68] layer 20_q @ epoch 3 new loss 5.029255589761306e-07 old loss 5.114492296343087e-07 BETTER
I0328 21:32:11.200366 2318477 finetune.py:76] layer 23_v @ epoch 3 new loss 5.169330847820675e-07 old loss 5.126181576997624e-07 WORSE
I0328 21:32:28.394700 2318337 finetune.py:68] layer 21_q @ epoch 2 new loss 6.361548230415792e-07 old loss 6.491864041890949e-07 BETTER
I0328 21:32:31.900281 2318407 finetune.py:68] layer 22_q @ epoch 0 new loss 5.933375177846756e-07 old loss 6.13918132330582e-07 BETTER
I0328 21:32:42.867800 2318267 finetune.py:68] layer 20_q @ epoch 4 new loss 4.957707915309584e-07 old loss 5.029255589761306e-07 BETTER
I0328 21:32:44.910093 2318477 finetune.py:76] layer 23_v @ epoch 4 new loss 5.486538157128962e-07 old loss 5.126181576997624e-07 WORSE
I0328 21:33:00.884902 2318267 finetune.py:45] layer 20_k initial loss 6.289621978794457e-07
I0328 21:33:02.902869 2318337 finetune.py:68] layer 21_q @ epoch 3 new loss 6.105933039179945e-07 old loss 6.361548230415792e-07 BETTER
I0328 21:33:03.668490 2318477 finetune.py:45] layer 23_q initial loss 6.400747452062205e-07
I0328 21:33:06.682959 2318407 finetune.py:68] layer 22_q @ epoch 1 new loss 5.590476916950138e-07 old loss 5.933375177846756e-07 BETTER
I0328 21:33:35.986412 2318267 finetune.py:68] layer 20_k @ epoch 0 new loss 5.865045409336744e-07 old loss 6.289621978794457e-07 BETTER
I0328 21:33:36.752295 2318477 finetune.py:68] layer 23_q @ epoch 0 new loss 5.997284802106151e-07 old loss 6.400747452062205e-07 BETTER
I0328 21:33:37.377892 2318337 finetune.py:68] layer 21_q @ epoch 4 new loss 6.037025741534308e-07 old loss 6.105933039179945e-07 BETTER
I0328 21:33:41.282070 2318407 finetune.py:68] layer 22_q @ epoch 2 new loss 5.416438284555625e-07 old loss 5.590476916950138e-07 BETTER
I0328 21:33:55.303482 2318337 finetune.py:45] layer 21_k initial loss 7.259400831571838e-07
I0328 21:34:10.762114 2318477 finetune.py:68] layer 23_q @ epoch 1 new loss 5.791107469121926e-07 old loss 5.997284802106151e-07 BETTER
I0328 21:34:12.180946 2318267 finetune.py:68] layer 20_k @ epoch 1 new loss 5.833392719978292e-07 old loss 5.865045409336744e-07 BETTER
I0328 21:34:15.912719 2318407 finetune.py:68] layer 22_q @ epoch 3 new loss 5.330776957634953e-07 old loss 5.416438284555625e-07 BETTER
I0328 21:34:28.810675 2318337 finetune.py:68] layer 21_k @ epoch 0 new loss 6.961557232898485e-07 old loss 7.259400831571838e-07 BETTER
I0328 21:34:44.780964 2318477 finetune.py:68] layer 23_q @ epoch 2 new loss 5.621331524707784e-07 old loss 5.791107469121926e-07 BETTER
I0328 21:34:48.551934 2318267 finetune.py:68] layer 20_k @ epoch 2 new loss 5.756735390605172e-07 old loss 5.833392719978292e-07 BETTER
I0328 21:34:50.560770 2318407 finetune.py:68] layer 22_q @ epoch 4 new loss 5.290138460622984e-07 old loss 5.330776957634953e-07 BETTER
I0328 21:35:03.428215 2318337 finetune.py:68] layer 21_k @ epoch 1 new loss 6.918573944858508e-07 old loss 6.961557232898485e-07 BETTER
I0328 21:35:08.573014 2318407 finetune.py:45] layer 22_k initial loss 6.653965556324692e-07
I0328 21:35:19.203454 2318477 finetune.py:68] layer 23_q @ epoch 3 new loss 5.547416890294699e-07 old loss 5.621331524707784e-07 BETTER
I0328 21:35:24.949926 2318267 finetune.py:68] layer 20_k @ epoch 3 new loss 5.701139684788359e-07 old loss 5.756735390605172e-07 BETTER
I0328 21:35:37.543873 2318337 finetune.py:68] layer 21_k @ epoch 2 new loss 6.915287826814165e-07 old loss 6.918573944858508e-07 BETTER
I0328 21:35:42.010403 2318407 finetune.py:68] layer 22_k @ epoch 0 new loss 6.452681304836005e-07 old loss 6.653965556324692e-07 BETTER
I0328 21:35:53.533293 2318477 finetune.py:68] layer 23_q @ epoch 4 new loss 5.514572762876924e-07 old loss 5.547416890294699e-07 BETTER
I0328 21:36:01.322204 2318267 finetune.py:68] layer 20_k @ epoch 4 new loss 5.691157412002212e-07 old loss 5.701139684788359e-07 BETTER
I0328 21:36:10.977573 2318477 finetune.py:45] layer 23_k initial loss 7.04846286225802e-07
I0328 21:36:12.085756 2318337 finetune.py:68] layer 21_k @ epoch 3 new loss 6.85815336964879e-07 old loss 6.915287826814165e-07 BETTER
I0328 21:36:16.416165 2318407 finetune.py:68] layer 22_k @ epoch 1 new loss 6.402192411769647e-07 old loss 6.452681304836005e-07 BETTER
I0328 21:36:21.237427 2318267 finetune.py:45] layer 20_o initial loss 1.1338875083310995e-06
I0328 21:36:43.916494 2318477 finetune.py:68] layer 23_k @ epoch 0 new loss 6.658780193902203e-07 old loss 7.04846286225802e-07 BETTER
I0328 21:36:46.322807 2318337 finetune.py:76] layer 21_k @ epoch 4 new loss 7.028915547380166e-07 old loss 6.85815336964879e-07 WORSE
I0328 21:36:50.912395 2318407 finetune.py:68] layer 22_k @ epoch 2 new loss 6.361913733599067e-07 old loss 6.402192411769647e-07 BETTER
I0328 21:36:55.992790 2318267 finetune.py:68] layer 20_o @ epoch 0 new loss 1.0944797850243049e-06 old loss 1.1338875083310995e-06 BETTER
I0328 21:37:05.324203 2318337 finetune.py:45] layer 21_o initial loss 1.4270389101511682e-06
I0328 21:37:17.666231 2318477 finetune.py:68] layer 23_k @ epoch 1 new loss 6.534312433359446e-07 old loss 6.658780193902203e-07 BETTER
I0328 21:37:25.357586 2318407 finetune.py:68] layer 22_k @ epoch 3 new loss 6.342368124023778e-07 old loss 6.361913733599067e-07 BETTER
I0328 21:37:31.528187 2318267 finetune.py:68] layer 20_o @ epoch 1 new loss 1.0749124612630112e-06 old loss 1.0944797850243049e-06 BETTER
I0328 21:37:37.865489 2318337 finetune.py:68] layer 21_o @ epoch 0 new loss 1.3576712944995961e-06 old loss 1.4270389101511682e-06 BETTER
I0328 21:37:51.477678 2318477 finetune.py:68] layer 23_k @ epoch 2 new loss 6.425256060538231e-07 old loss 6.534312433359446e-07 BETTER
I0328 21:37:59.877571 2318407 finetune.py:68] layer 22_k @ epoch 4 new loss 6.316385565696692e-07 old loss 6.342368124023778e-07 BETTER
I0328 21:38:07.238705 2318267 finetune.py:68] layer 20_o @ epoch 2 new loss 1.062688852471183e-06 old loss 1.0749124612630112e-06 BETTER
I0328 21:38:11.378253 2318337 finetune.py:68] layer 21_o @ epoch 1 new loss 1.3285473414725857e-06 old loss 1.3576712944995961e-06 BETTER
I0328 21:38:19.812775 2318407 finetune.py:45] layer 22_o initial loss 1.3081498764222488e-06
I0328 21:38:25.686167 2318477 finetune.py:68] layer 23_k @ epoch 3 new loss 6.375423140525527e-07 old loss 6.425256060538231e-07 BETTER
I0328 21:38:43.171796 2318267 finetune.py:68] layer 20_o @ epoch 3 new loss 1.0546439170866506e-06 old loss 1.062688852471183e-06 BETTER
I0328 21:38:45.120893 2318337 finetune.py:68] layer 21_o @ epoch 2 new loss 1.3076436289338744e-06 old loss 1.3285473414725857e-06 BETTER
I0328 21:38:52.633506 2318407 finetune.py:68] layer 22_o @ epoch 0 new loss 1.2690285302596749e-06 old loss 1.3081498764222488e-06 BETTER
I0328 21:39:00.107298 2318477 finetune.py:68] layer 23_k @ epoch 4 new loss 6.321340606518788e-07 old loss 6.375423140525527e-07 BETTER
I0328 21:39:18.825404 2318337 finetune.py:68] layer 21_o @ epoch 3 new loss 1.2924878092235303e-06 old loss 1.3076436289338744e-06 BETTER
I0328 21:39:19.290805 2318267 finetune.py:68] layer 20_o @ epoch 4 new loss 1.0467221045473707e-06 old loss 1.0546439170866506e-06 BETTER
I0328 21:39:19.460321 2318477 finetune.py:45] layer 23_o initial loss 1.3546234640671173e-06
I0328 21:39:26.409191 2318407 finetune.py:68] layer 22_o @ epoch 1 new loss 1.2505307722676662e-06 old loss 1.2690285302596749e-06 BETTER
I0328 21:39:50.896099 2318267 finetune.py:45] layer 20_up initial loss 2.9949301278975327e-06
I0328 21:39:51.457174 2318477 finetune.py:68] layer 23_o @ epoch 0 new loss 1.3111726957504288e-06 old loss 1.3546234640671173e-06 BETTER
I0328 21:39:52.998804 2318337 finetune.py:68] layer 21_o @ epoch 4 new loss 1.280215315091482e-06 old loss 1.2924878092235303e-06 BETTER
I0328 21:40:00.072206 2318407 finetune.py:68] layer 22_o @ epoch 2 new loss 1.2398763828969095e-06 old loss 1.2505307722676662e-06 BETTER
I0328 21:40:22.807581 2318267 finetune.py:68] layer 20_up @ epoch 0 new loss 2.9180907858972205e-06 old loss 2.9949301278975327e-06 BETTER
I0328 21:40:24.081618 2318337 finetune.py:45] layer 21_up initial loss 3.4985844195034588e-06
I0328 21:40:24.686925 2318477 finetune.py:68] layer 23_o @ epoch 1 new loss 1.2895932286483003e-06 old loss 1.3111726957504288e-06 BETTER
I0328 21:40:34.000348 2318407 finetune.py:68] layer 22_o @ epoch 3 new loss 1.2327295735303778e-06 old loss 1.2398763828969095e-06 BETTER
I0328 21:40:54.534463 2318337 finetune.py:68] layer 21_up @ epoch 0 new loss 3.3998255730693927e-06 old loss 3.4985844195034588e-06 BETTER
I0328 21:40:55.939041 2318267 finetune.py:68] layer 20_up @ epoch 1 new loss 2.8660506359301507e-06 old loss 2.9180907858972205e-06 BETTER
I0328 21:40:57.841350 2318477 finetune.py:68] layer 23_o @ epoch 2 new loss 1.2787505738742766e-06 old loss 1.2895932286483003e-06 BETTER
I0328 21:41:08.129527 2318407 finetune.py:68] layer 22_o @ epoch 4 new loss 1.2249565770616755e-06 old loss 1.2327295735303778e-06 BETTER
I0328 21:41:26.167704 2318337 finetune.py:68] layer 21_up @ epoch 1 new loss 3.3350572721246863e-06 old loss 3.3998255730693927e-06 BETTER
I0328 21:41:29.256960 2318267 finetune.py:68] layer 20_up @ epoch 2 new loss 2.8254187327547697e-06 old loss 2.8660506359301507e-06 BETTER
I0328 21:41:31.231706 2318477 finetune.py:68] layer 23_o @ epoch 3 new loss 1.2674380513999495e-06 old loss 1.2787505738742766e-06 BETTER
I0328 21:41:39.687708 2318407 finetune.py:45] layer 22_up initial loss 3.5587468119047116e-06
I0328 21:41:57.805042 2318337 finetune.py:68] layer 21_up @ epoch 2 new loss 3.285489356130711e-06 old loss 3.3350572721246863e-06 BETTER
I0328 21:42:02.710514 2318267 finetune.py:68] layer 20_up @ epoch 3 new loss 2.7917019451706437e-06 old loss 2.8254187327547697e-06 BETTER
I0328 21:42:04.599578 2318477 finetune.py:68] layer 23_o @ epoch 4 new loss 1.264094407815719e-06 old loss 1.2674380513999495e-06 BETTER
I0328 21:42:10.129308 2318407 finetune.py:68] layer 22_up @ epoch 0 new loss 3.4631300422915956e-06 old loss 3.5587468119047116e-06 BETTER
I0328 21:42:29.565028 2318337 finetune.py:68] layer 21_up @ epoch 3 new loss 3.245953848818317e-06 old loss 3.285489356130711e-06 BETTER
I0328 21:42:36.127738 2318477 finetune.py:45] layer 23_up initial loss 3.795169277509558e-06
I0328 21:42:36.305259 2318267 finetune.py:68] layer 20_up @ epoch 4 new loss 2.76363698503701e-06 old loss 2.7917019451706437e-06 BETTER
I0328 21:42:41.926872 2318407 finetune.py:68] layer 22_up @ epoch 1 new loss 3.403082700970117e-06 old loss 3.4631300422915956e-06 BETTER
I0328 21:43:01.457319 2318337 finetune.py:68] layer 21_up @ epoch 4 new loss 3.2131436000781832e-06 old loss 3.245953848818317e-06 BETTER
I0328 21:43:06.215158 2318477 finetune.py:68] layer 23_up @ epoch 0 new loss 3.701563855429413e-06 old loss 3.795169277509558e-06 BETTER
I0328 21:43:07.677252 2318267 finetune.py:45] layer 20_gate initial loss 3.593026576709235e-06
I0328 21:43:14.044288 2318407 finetune.py:68] layer 22_up @ epoch 2 new loss 3.357922196300933e-06 old loss 3.403082700970117e-06 BETTER
I0328 21:43:32.942949 2318337 finetune.py:45] layer 21_gate initial loss 4.16012426285306e-06
I0328 21:43:37.588264 2318477 finetune.py:68] layer 23_up @ epoch 1 new loss 3.643572881628643e-06 old loss 3.701563855429413e-06 BETTER
I0328 21:43:37.793271 2318267 finetune.py:68] layer 20_gate @ epoch 0 new loss 3.5619682421383914e-06 old loss 3.593026576709235e-06 BETTER
I0328 21:43:46.311159 2318407 finetune.py:68] layer 22_up @ epoch 3 new loss 3.3215455914614722e-06 old loss 3.357922196300933e-06 BETTER
I0328 21:44:01.362261 2318337 finetune.py:68] layer 21_gate @ epoch 0 new loss 4.121841357118683e-06 old loss 4.16012426285306e-06 BETTER
I0328 21:44:08.816836 2318477 finetune.py:68] layer 23_up @ epoch 2 new loss 3.5993339224660303e-06 old loss 3.643572881628643e-06 BETTER
I0328 21:44:09.015201 2318267 finetune.py:68] layer 20_gate @ epoch 1 new loss 3.536846406859695e-06 old loss 3.5619682421383914e-06 BETTER
I0328 21:44:18.549294 2318407 finetune.py:68] layer 22_up @ epoch 4 new loss 3.291242592240451e-06 old loss 3.3215455914614722e-06 BETTER
I0328 21:44:30.820635 2318337 finetune.py:68] layer 21_gate @ epoch 1 new loss 4.092205472261412e-06 old loss 4.121841357118683e-06 BETTER
I0328 21:44:40.049287 2318477 finetune.py:68] layer 23_up @ epoch 3 new loss 3.5649145502247848e-06 old loss 3.5993339224660303e-06 BETTER
I0328 21:44:40.202356 2318267 finetune.py:68] layer 20_gate @ epoch 2 new loss 3.5151565498381387e-06 old loss 3.536846406859695e-06 BETTER
I0328 21:44:50.108537 2318407 finetune.py:45] layer 22_gate initial loss 4.326261660025921e-06
I0328 21:45:00.267326 2318337 finetune.py:68] layer 21_gate @ epoch 2 new loss 4.067615464009577e-06 old loss 4.092205472261412e-06 BETTER
I0328 21:45:11.383589 2318477 finetune.py:68] layer 23_up @ epoch 4 new loss 3.5363332244742196e-06 old loss 3.5649145502247848e-06 BETTER
I0328 21:45:11.562401 2318267 finetune.py:68] layer 20_gate @ epoch 3 new loss 3.4965855775226373e-06 old loss 3.5151565498381387e-06 BETTER
I0328 21:45:18.530626 2318407 finetune.py:68] layer 22_gate @ epoch 0 new loss 4.290453489375068e-06 old loss 4.326261660025921e-06 BETTER
I0328 21:45:29.928667 2318337 finetune.py:68] layer 21_gate @ epoch 3 new loss 4.046502454002621e-06 old loss 4.067615464009577e-06 BETTER
I0328 21:45:42.520747 2318477 finetune.py:45] layer 23_gate initial loss 4.703851118392777e-06
I0328 21:45:42.980668 2318267 finetune.py:68] layer 20_gate @ epoch 4 new loss 3.4800273169821594e-06 old loss 3.4965855775226373e-06 BETTER
I0328 21:45:48.077046 2318407 finetune.py:68] layer 22_gate @ epoch 1 new loss 4.262894890416646e-06 old loss 4.290453489375068e-06 BETTER
I0328 21:45:59.623158 2318337 finetune.py:68] layer 21_gate @ epoch 4 new loss 4.027669547213009e-06 old loss 4.046502454002621e-06 BETTER
I0328 21:46:10.518299 2318477 finetune.py:68] layer 23_gate @ epoch 0 new loss 4.6706277316843625e-06 old loss 4.703851118392777e-06 BETTER
I0328 21:46:17.712712 2318407 finetune.py:68] layer 22_gate @ epoch 2 new loss 4.240546786604682e-06 old loss 4.262894890416646e-06 BETTER
I0328 21:46:39.395797 2318267 finetune.py:45] layer 20_down initial loss 6.226071491255425e-06
I0328 21:46:39.811896 2318477 finetune.py:68] layer 23_gate @ epoch 1 new loss 4.645065473596333e-06 old loss 4.6706277316843625e-06 BETTER
I0328 21:46:47.496147 2318407 finetune.py:68] layer 22_gate @ epoch 3 new loss 4.221160452289041e-06 old loss 4.240546786604682e-06 BETTER
I0328 21:46:56.404848 2318337 finetune.py:45] layer 21_down initial loss 7.258591267600423e-06
I0328 21:47:06.970775 2318267 finetune.py:68] layer 20_down @ epoch 0 new loss 6.225072411325527e-06 old loss 6.226071491255425e-06 BETTER
I0328 21:47:08.970435 2318477 finetune.py:68] layer 23_gate @ epoch 2 new loss 4.624075700121466e-06 old loss 4.645065473596333e-06 BETTER
I0328 21:47:17.477077 2318407 finetune.py:68] layer 22_gate @ epoch 4 new loss 4.203967819194077e-06 old loss 4.221160452289041e-06 BETTER
I0328 21:47:22.602712 2318337 finetune.py:68] layer 21_down @ epoch 0 new loss 7.257439847307978e-06 old loss 7.258591267600423e-06 BETTER
I0328 21:47:35.644201 2318267 finetune.py:68] layer 20_down @ epoch 1 new loss 6.2247031564766075e-06 old loss 6.225072411325527e-06 BETTER
I0328 21:47:38.276602 2318477 finetune.py:68] layer 23_gate @ epoch 3 new loss 4.606049060384976e-06 old loss 4.624075700121466e-06 BETTER
I0328 21:47:49.718325 2318337 finetune.py:68] layer 21_down @ epoch 1 new loss 7.257079232658725e-06 old loss 7.257439847307978e-06 BETTER
I0328 21:48:04.665020 2318267 finetune.py:68] layer 20_down @ epoch 2 new loss 6.2245512708614115e-06 old loss 6.2247031564766075e-06 BETTER
I0328 21:48:07.526115 2318477 finetune.py:68] layer 23_gate @ epoch 4 new loss 4.590736807585927e-06 old loss 4.606049060384976e-06 BETTER
I0328 21:48:15.164240 2318407 finetune.py:45] layer 22_down initial loss 7.608560281369137e-06
I0328 21:48:17.180804 2318337 finetune.py:68] layer 21_down @ epoch 2 new loss 7.256902790686581e-06 old loss 7.257079232658725e-06 BETTER
I0328 21:48:33.647581 2318267 finetune.py:68] layer 20_down @ epoch 3 new loss 6.224455773917725e-06 old loss 6.2245512708614115e-06 BETTER
I0328 21:48:41.459283 2318407 finetune.py:68] layer 22_down @ epoch 0 new loss 7.607334282511147e-06 old loss 7.608560281369137e-06 BETTER
I0328 21:48:44.833932 2318337 finetune.py:68] layer 21_down @ epoch 3 new loss 7.25682502888958e-06 old loss 7.256902790686581e-06 BETTER
I0328 21:49:02.850837 2318267 finetune.py:68] layer 20_down @ epoch 4 new loss 6.224329354154179e-06 old loss 6.224455773917725e-06 BETTER
I0328 21:49:03.963430 2318477 finetune.py:45] layer 23_down initial loss 8.24101698526647e-06
20_v proxy err 0.00052523153135553 tr(WHW.T) 330.192138671875
bpp_loss 5.646952123497613
20_q proxy err 4.737457857117988e-05 tr(WHW.T) 20735.876953125
bpp_loss 6.486958400928415
20_k proxy err 2.9568665922852233e-05 tr(WHW.T) 15393.5322265625
bpp_loss 7.597671053255908
20_o proxy err 0.0005559729179367423 tr(WHW.T) 1205.2769775390625
bpp_loss 5.679378809407353
20_up proxy err 0.00042258889880031347 tr(WHW.T) 7601.083984375
bpp_loss 5.764893341942558
20_gate proxy err 0.00012107586371712387 tr(WHW.T) 30666.392578125
bpp_loss 6.166266250424087
20_down proxy err 0.0004871576093137264 tr(WHW.T) 6301.94482421875
bpp_loss 5.76091518472614
I0328 21:49:08.729845 2318407 finetune.py:68] layer 22_down @ epoch 1 new loss 7.606941380799981e-06 old loss 7.607334282511147e-06 BETTER
I0328 21:49:13.082190 2318337 finetune.py:68] layer 21_down @ epoch 4 new loss 7.256782737385947e-06 old loss 7.25682502888958e-06 BETTER
21_v proxy err 0.0004860068438574672 tr(WHW.T) 362.87310791015625
bpp_loss 5.673929242882878
21_q proxy err 3.8024441892048344e-05 tr(WHW.T) 25838.01953125
bpp_loss 6.476383737870492
21_k proxy err 2.6509742383495905e-05 tr(WHW.T) 16784.58203125
bpp_loss 7.648131961119361
21_o proxy err 0.000388293934520334 tr(WHW.T) 1266.3070068359375
bpp_loss 5.700258364900947
21_up proxy err 0.0004059452621731907 tr(WHW.T) 7772.7998046875
bpp_loss 5.767884890948023
21_gate proxy err 0.0001160541723947972 tr(WHW.T) 31560.783203125
bpp_loss 6.179196070214467
21_down proxy err 0.0004635634832084179 tr(WHW.T) 6353.08642578125
bpp_loss 5.7633265700923015
I0328 21:49:30.013846 2318477 finetune.py:68] layer 23_down @ epoch 0 new loss 8.23962182039395e-06 old loss 8.24101698526647e-06 BETTER
I0328 21:49:37.097970 2318407 finetune.py:68] layer 22_down @ epoch 2 new loss 7.606705366924871e-06 old loss 7.606941380799981e-06 BETTER
I0328 21:49:56.948713 2318477 finetune.py:68] layer 23_down @ epoch 1 new loss 8.23902701085899e-06 old loss 8.23962182039395e-06 BETTER
I0328 21:50:04.892837 2318407 finetune.py:68] layer 22_down @ epoch 3 new loss 7.606613053212641e-06 old loss 7.606705366924871e-06 BETTER
I0328 21:50:19.300277 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 60.97344946861267s
I0328 21:50:23.210568 2318547 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:50:23.210681 2318547 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:50:23.210726 2318547 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:50:23.598875 2318547 config.py:54] PyTorch version 2.6.0 available.
W0328 21:50:23.808492 2318547 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 21:50:24.072250 2318477 finetune.py:68] layer 23_down @ epoch 2 new loss 8.238728696596809e-06 old loss 8.23902701085899e-06 BETTER
W0328 21:50:24.390356 2318547 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:50:24.394106 2316731 quantize_finetune_llama.py:209] layer 25 gpu 1
I0328 21:50:24.407402 2318547 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 21:50:32.632006 2318407 finetune.py:68] layer 22_down @ epoch 4 new loss 7.6065102803113405e-06 old loss 7.606613053212641e-06 BETTER
22_v proxy err 0.0004969841102138162 tr(WHW.T) 346.0789489746094
bpp_loss 5.72497646859847
22_q proxy err 4.593904668581672e-05 tr(WHW.T) 20320.533203125
bpp_loss 6.429021558724344
22_k proxy err 3.0335262636072002e-05 tr(WHW.T) 14722.0546875
bpp_loss 7.5641343666939065
22_o proxy err 0.0005209720693528652 tr(WHW.T) 1221.9869384765625
bpp_loss 5.722872045473196
22_up proxy err 0.00042267004027962685 tr(WHW.T) 7547.95263671875
bpp_loss 5.771526995114982
22_gate proxy err 0.00012527024955488741 tr(WHW.T) 29532.3359375
bpp_loss 6.180232958335962
22_down proxy err 0.00046324433060362935 tr(WHW.T) 6531.0927734375
bpp_loss 5.767433467088267
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:50:41.575278 2318547 finetune.py:45] layer 24_v initial loss 2.9376658403634792e-06
W0328 21:50:41.575542 2318547 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:50:51.175728 2318477 finetune.py:68] layer 23_down @ epoch 3 new loss 8.238514965341892e-06 old loss 8.238728696596809e-06 BETTER
I0328 21:51:16.560336 2318547 finetune.py:68] layer 24_v @ epoch 0 new loss 6.210014475982462e-07 old loss 2.9376658403634792e-06 BETTER
I0328 21:51:18.368544 2318477 finetune.py:68] layer 23_down @ epoch 4 new loss 8.23842128738761e-06 old loss 8.238514965341892e-06 BETTER
23_v proxy err 0.00046107955859042704 tr(WHW.T) 397.90704345703125
bpp_loss 5.77848870260641
23_q proxy err 4.3423602619441226e-05 tr(WHW.T) 22613.216796875
bpp_loss 6.43871101224795
23_k proxy err 2.9821021598763764e-05 tr(WHW.T) 14858.58203125
bpp_loss 7.559394005220383
23_o proxy err 0.000449848041171208 tr(WHW.T) 1744.9473876953125
bpp_loss 5.743045195704326
23_up proxy err 0.0004296930565033108 tr(WHW.T) 7418.556640625
bpp_loss 5.7755808016019206
23_gate proxy err 0.00013534212484955788 tr(WHW.T) 27270.8359375
bpp_loss 6.182909611346466
23_down proxy err 0.00046630235738120973 tr(WHW.T) 6669.55029296875
bpp_loss 5.772092378897859
I0328 21:51:41.484921 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 64.19658970832825s
I0328 21:51:45.110572 2318617 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:51:45.110670 2318617 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:51:45.110708 2318617 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:51:45.450717 2318617 config.py:54] PyTorch version 2.6.0 available.
W0328 21:51:45.649780 2318617 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:51:46.258665 2318617 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:51:46.262191 2316731 quantize_finetune_llama.py:209] layer 26 gpu 2
I0328 21:51:46.275090 2318617 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 21:51:53.046480 2318547 finetune.py:68] layer 24_v @ epoch 1 new loss 5.495977006830799e-07 old loss 6.210014475982462e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:52:03.293427 2318617 finetune.py:45] layer 25_v initial loss 3.2594102776783984e-06
W0328 21:52:03.293637 2318617 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:52:29.989866 2318547 finetune.py:68] layer 24_v @ epoch 2 new loss 5.369681730371667e-07 old loss 5.495977006830799e-07 BETTER
I0328 21:52:36.555377 2318617 finetune.py:68] layer 25_v @ epoch 0 new loss 6.944163146727078e-07 old loss 3.2594102776783984e-06 BETTER
I0328 21:52:49.967922 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 63.24264740943909s
I0328 21:52:53.670041 2318687 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:52:53.670131 2318687 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:52:53.670169 2318687 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:52:54.034438 2318687 config.py:54] PyTorch version 2.6.0 available.
W0328 21:52:54.229540 2318687 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:52:54.826600 2318687 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:52:54.830395 2316731 quantize_finetune_llama.py:209] layer 27 gpu 3
I0328 21:52:54.843325 2318687 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 21:53:07.058087 2318547 finetune.py:76] layer 24_v @ epoch 3 new loss 5.598857342192787e-07 old loss 5.369681730371667e-07 WORSE
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:53:10.908408 2318617 finetune.py:68] layer 25_v @ epoch 1 new loss 6.53949314255442e-07 old loss 6.944163146727078e-07 BETTER
I0328 21:53:12.373398 2318687 finetune.py:45] layer 26_v initial loss 3.062025825784076e-06
W0328 21:53:12.373765 2318687 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:53:43.433379 2318547 finetune.py:76] layer 24_v @ epoch 4 new loss 5.957431881142838e-07 old loss 5.369681730371667e-07 WORSE
I0328 21:53:45.694319 2318617 finetune.py:76] layer 25_v @ epoch 2 new loss 6.854328944427834e-07 old loss 6.53949314255442e-07 WORSE
I0328 21:53:45.963576 2318687 finetune.py:68] layer 26_v @ epoch 0 new loss 9.67736923485063e-07 old loss 3.062025825784076e-06 BETTER
I0328 21:53:59.092243 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 63.78254723548889s
I0328 21:54:02.048866 2318547 finetune.py:45] layer 24_q initial loss 6.994164891693799e-07
I0328 21:54:03.077977 2318757 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 21:54:03.078063 2318757 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 21:54:03.078104 2318757 utils.py:162] NumExpr defaulting to 16 threads.
I0328 21:54:03.465621 2318757 config.py:54] PyTorch version 2.6.0 available.
W0328 21:54:03.672693 2318757 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 21:54:04.307914 2318757 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 21:54:04.311635 2316731 quantize_finetune_llama.py:209] layer 28 gpu 0
I0328 21:54:04.325283 2318757 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 21:54:19.991039 2318617 finetune.py:76] layer 25_v @ epoch 3 new loss 9.239204246114241e-07 old loss 6.53949314255442e-07 WORSE
I0328 21:54:20.715041 2318687 finetune.py:68] layer 26_v @ epoch 1 new loss 9.17785371257196e-07 old loss 9.67736923485063e-07 BETTER
I0328 21:54:21.576668 2318757 finetune.py:45] layer 27_v initial loss 3.507271003400092e-06
W0328 21:54:21.577080 2318757 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 21:54:37.471907 2318547 finetune.py:68] layer 24_q @ epoch 0 new loss 6.676534667349188e-07 old loss 6.994164891693799e-07 BETTER
I0328 21:54:54.194258 2318617 finetune.py:76] layer 25_v @ epoch 4 new loss 7.218919222395925e-07 old loss 6.53949314255442e-07 WORSE
I0328 21:54:54.593564 2318757 finetune.py:68] layer 27_v @ epoch 0 new loss 9.404974434801261e-07 old loss 3.507271003400092e-06 BETTER
I0328 21:54:55.593245 2318687 finetune.py:76] layer 26_v @ epoch 2 new loss 1.0626862376739155e-06 old loss 9.17785371257196e-07 WORSE
I0328 21:55:13.164052 2318617 finetune.py:45] layer 25_q initial loss 9.437499670639227e-07
I0328 21:55:13.673020 2318547 finetune.py:68] layer 24_q @ epoch 1 new loss 6.359063604577386e-07 old loss 6.676534667349188e-07 BETTER
I0328 21:55:28.768661 2318757 finetune.py:76] layer 27_v @ epoch 1 new loss 9.643291605243576e-07 old loss 9.404974434801261e-07 WORSE
I0328 21:55:29.980447 2318687 finetune.py:76] layer 26_v @ epoch 3 new loss 9.896427854982903e-07 old loss 9.17785371257196e-07 WORSE
I0328 21:55:46.443578 2318617 finetune.py:68] layer 25_q @ epoch 0 new loss 8.41506221149757e-07 old loss 9.437499670639227e-07 BETTER
I0328 21:55:50.120009 2318547 finetune.py:68] layer 24_q @ epoch 2 new loss 6.260937084334728e-07 old loss 6.359063604577386e-07 BETTER
I0328 21:56:02.518371 2318757 finetune.py:76] layer 27_v @ epoch 2 new loss 1.134523131440801e-06 old loss 9.404974434801261e-07 WORSE
I0328 21:56:04.487703 2318687 finetune.py:76] layer 26_v @ epoch 4 new loss 9.702965826363652e-07 old loss 9.17785371257196e-07 WORSE
I0328 21:56:20.778665 2318617 finetune.py:68] layer 25_q @ epoch 1 new loss 7.942252864268085e-07 old loss 8.41506221149757e-07 BETTER
I0328 21:56:23.357572 2318687 finetune.py:45] layer 26_q initial loss 1.1742937431336031e-06
I0328 21:56:26.852228 2318547 finetune.py:76] layer 24_q @ epoch 3 new loss 6.313560447779309e-07 old loss 6.260937084334728e-07 WORSE
I0328 21:56:36.268158 2318757 finetune.py:76] layer 27_v @ epoch 3 new loss 1.2555548210002598e-06 old loss 9.404974434801261e-07 WORSE
I0328 21:56:55.203623 2318617 finetune.py:68] layer 25_q @ epoch 2 new loss 7.817807841092872e-07 old loss 7.942252864268085e-07 BETTER
I0328 21:56:57.072585 2318687 finetune.py:68] layer 26_q @ epoch 0 new loss 1.108914375436143e-06 old loss 1.1742937431336031e-06 BETTER
I0328 21:57:03.037921 2318547 finetune.py:68] layer 24_q @ epoch 4 new loss 6.137017294349789e-07 old loss 6.260937084334728e-07 BETTER
I0328 21:57:10.040366 2318757 finetune.py:76] layer 27_v @ epoch 4 new loss 1.0409066817373969e-06 old loss 9.404974434801261e-07 WORSE
I0328 21:57:20.864249 2318547 finetune.py:45] layer 24_k initial loss 8.394449082516076e-07
I0328 21:57:28.487696 2318757 finetune.py:45] layer 27_q initial loss 1.300944745707966e-06
I0328 21:57:29.630511 2318617 finetune.py:68] layer 25_q @ epoch 3 new loss 7.754131274850806e-07 old loss 7.817807841092872e-07 BETTER
I0328 21:57:31.593960 2318687 finetune.py:68] layer 26_q @ epoch 1 new loss 1.0575815849733772e-06 old loss 1.108914375436143e-06 BETTER
I0328 21:57:56.380390 2318547 finetune.py:68] layer 24_k @ epoch 0 new loss 8.159033768606605e-07 old loss 8.394449082516076e-07 BETTER
I0328 21:58:01.540161 2318757 finetune.py:68] layer 27_q @ epoch 0 new loss 1.2240761861903593e-06 old loss 1.300944745707966e-06 BETTER
I0328 21:58:04.387634 2318617 finetune.py:68] layer 25_q @ epoch 4 new loss 7.521271072619129e-07 old loss 7.754131274850806e-07 BETTER
I0328 21:58:06.076561 2318687 finetune.py:76] layer 26_q @ epoch 2 new loss 1.0723806553869508e-06 old loss 1.0575815849733772e-06 WORSE
I0328 21:58:22.290909 2318617 finetune.py:45] layer 25_k initial loss 1.129842303271289e-06
I0328 21:58:32.346965 2318547 finetune.py:68] layer 24_k @ epoch 1 new loss 7.966588100316585e-07 old loss 8.159033768606605e-07 BETTER
I0328 21:58:35.850410 2318757 finetune.py:68] layer 27_q @ epoch 1 new loss 1.1483291473268764e-06 old loss 1.2240761861903593e-06 BETTER
I0328 21:58:40.200606 2318687 finetune.py:76] layer 26_q @ epoch 3 new loss 1.0667904462025035e-06 old loss 1.0575815849733772e-06 WORSE
I0328 21:58:55.705067 2318617 finetune.py:68] layer 25_k @ epoch 0 new loss 1.0817203701662947e-06 old loss 1.129842303271289e-06 BETTER
I0328 21:59:08.693827 2318547 finetune.py:76] layer 24_k @ epoch 2 new loss 7.991189363565354e-07 old loss 7.966588100316585e-07 WORSE
I0328 21:59:09.915008 2318757 finetune.py:76] layer 27_q @ epoch 2 new loss 1.1818507346106344e-06 old loss 1.1483291473268764e-06 WORSE
I0328 21:59:14.210094 2318687 finetune.py:76] layer 26_q @ epoch 4 new loss 1.1346875226081465e-06 old loss 1.0575815849733772e-06 WORSE
I0328 21:59:29.851146 2318617 finetune.py:68] layer 25_k @ epoch 1 new loss 1.0577273314993363e-06 old loss 1.0817203701662947e-06 BETTER
I0328 21:59:31.635454 2318687 finetune.py:45] layer 26_k initial loss 1.4869038977849414e-06
I0328 21:59:43.455291 2318757 finetune.py:76] layer 27_q @ epoch 3 new loss 1.2225192449477618e-06 old loss 1.1483291473268764e-06 WORSE
I0328 21:59:44.416581 2318547 finetune.py:76] layer 24_k @ epoch 3 new loss 8.005813469935674e-07 old loss 7.966588100316585e-07 WORSE
I0328 22:00:04.046872 2318617 finetune.py:68] layer 25_k @ epoch 2 new loss 1.0479133152330178e-06 old loss 1.0577273314993363e-06 BETTER
I0328 22:00:05.169490 2318687 finetune.py:68] layer 26_k @ epoch 0 new loss 1.4023961512066307e-06 old loss 1.4869038977849414e-06 BETTER
I0328 22:00:16.902034 2318757 finetune.py:76] layer 27_q @ epoch 4 new loss 1.2170416994194966e-06 old loss 1.1483291473268764e-06 WORSE
I0328 22:00:20.259299 2318547 finetune.py:68] layer 24_k @ epoch 4 new loss 7.86718601375469e-07 old loss 7.966588100316585e-07 BETTER
I0328 22:00:33.929345 2318757 finetune.py:45] layer 27_k initial loss 1.8577693481347524e-06
I0328 22:00:38.427580 2318617 finetune.py:76] layer 25_k @ epoch 3 new loss 1.0524749995965976e-06 old loss 1.0479133152330178e-06 WORSE
I0328 22:00:39.742118 2318687 finetune.py:68] layer 26_k @ epoch 1 new loss 1.398456788592739e-06 old loss 1.4023961512066307e-06 BETTER
I0328 22:00:39.832690 2318547 finetune.py:45] layer 24_o initial loss 1.576425347593613e-06
I0328 22:01:06.705928 2318757 finetune.py:68] layer 27_k @ epoch 0 new loss 1.7944139472092502e-06 old loss 1.8577693481347524e-06 BETTER
I0328 22:01:12.242322 2318617 finetune.py:68] layer 25_k @ epoch 4 new loss 1.0426248309158836e-06 old loss 1.0479133152330178e-06 BETTER
I0328 22:01:14.307910 2318687 finetune.py:76] layer 26_k @ epoch 2 new loss 1.4148959053272847e-06 old loss 1.398456788592739e-06 WORSE
I0328 22:01:14.581424 2318547 finetune.py:68] layer 24_o @ epoch 0 new loss 1.5441753475897713e-06 old loss 1.576425347593613e-06 BETTER
I0328 22:01:31.984040 2318617 finetune.py:45] layer 25_o initial loss 1.9700460143212695e-06
I0328 22:01:40.575813 2318757 finetune.py:68] layer 27_k @ epoch 1 new loss 1.6935865687628393e-06 old loss 1.7944139472092502e-06 BETTER
I0328 22:01:48.362526 2318687 finetune.py:76] layer 26_k @ epoch 3 new loss 1.3993133052281337e-06 old loss 1.398456788592739e-06 WORSE
I0328 22:01:50.207570 2318547 finetune.py:68] layer 24_o @ epoch 1 new loss 1.5233979411277687e-06 old loss 1.5441753475897713e-06 BETTER
I0328 22:02:04.683509 2318617 finetune.py:68] layer 25_o @ epoch 0 new loss 1.9126475763187045e-06 old loss 1.9700460143212695e-06 BETTER
I0328 22:02:14.404923 2318757 finetune.py:68] layer 27_k @ epoch 2 new loss 1.6821375083964085e-06 old loss 1.6935865687628393e-06 BETTER
I0328 22:02:22.354422 2318687 finetune.py:68] layer 26_k @ epoch 4 new loss 1.390039642501506e-06 old loss 1.398456788592739e-06 BETTER
I0328 22:02:26.326452 2318547 finetune.py:68] layer 24_o @ epoch 2 new loss 1.5131419104363886e-06 old loss 1.5233979411277687e-06 BETTER
I0328 22:02:38.334520 2318617 finetune.py:68] layer 25_o @ epoch 1 new loss 1.8918044588644989e-06 old loss 1.9126475763187045e-06 BETTER
I0328 22:02:42.020203 2318687 finetune.py:45] layer 26_o initial loss 2.6864481696975417e-06
I0328 22:02:48.314277 2318757 finetune.py:76] layer 27_k @ epoch 3 new loss 1.6977371615212178e-06 old loss 1.6821375083964085e-06 WORSE
I0328 22:03:02.000162 2318547 finetune.py:68] layer 24_o @ epoch 3 new loss 1.5037998082334525e-06 old loss 1.5131419104363886e-06 BETTER
I0328 22:03:12.079904 2318617 finetune.py:68] layer 25_o @ epoch 2 new loss 1.8621009303387837e-06 old loss 1.8918044588644989e-06 BETTER
I0328 22:03:14.902383 2318687 finetune.py:68] layer 26_o @ epoch 0 new loss 2.5455631202930817e-06 old loss 2.6864481696975417e-06 BETTER
I0328 22:03:21.485295 2318757 finetune.py:76] layer 27_k @ epoch 4 new loss 1.7021744724843302e-06 old loss 1.6821375083964085e-06 WORSE
I0328 22:03:37.726761 2318547 finetune.py:68] layer 24_o @ epoch 4 new loss 1.5016734096207074e-06 old loss 1.5037998082334525e-06 BETTER
I0328 22:03:40.228849 2318757 finetune.py:45] layer 27_o initial loss 3.2409684536105487e-06
I0328 22:03:45.650565 2318617 finetune.py:68] layer 25_o @ epoch 3 new loss 1.8364094103162643e-06 old loss 1.8621009303387837e-06 BETTER
I0328 22:03:48.552311 2318687 finetune.py:68] layer 26_o @ epoch 1 new loss 2.4763319288467756e-06 old loss 2.5455631202930817e-06 BETTER
I0328 22:04:08.786489 2318547 finetune.py:45] layer 24_up initial loss 4.241218903189292e-06
I0328 22:04:12.745541 2318757 finetune.py:68] layer 27_o @ epoch 0 new loss 3.1566123652737588e-06 old loss 3.2409684536105487e-06 BETTER
I0328 22:04:19.183352 2318617 finetune.py:76] layer 25_o @ epoch 4 new loss 1.86549118552648e-06 old loss 1.8364094103162643e-06 WORSE
I0328 22:04:22.358736 2318687 finetune.py:68] layer 26_o @ epoch 2 new loss 2.4560149540775456e-06 old loss 2.4763319288467756e-06 BETTER
I0328 22:04:40.625080 2318547 finetune.py:68] layer 24_up @ epoch 0 new loss 4.149021606281167e-06 old loss 4.241218903189292e-06 BETTER
I0328 22:04:45.875245 2318757 finetune.py:68] layer 27_o @ epoch 1 new loss 3.1075094284460647e-06 old loss 3.1566123652737588e-06 BETTER
I0328 22:04:50.145887 2318617 finetune.py:45] layer 25_up initial loss 4.9163541007146705e-06
I0328 22:04:56.184365 2318687 finetune.py:68] layer 26_o @ epoch 3 new loss 2.4299770302604884e-06 old loss 2.4560149540775456e-06 BETTER
I0328 22:05:13.724133 2318547 finetune.py:68] layer 24_up @ epoch 1 new loss 4.091275513928849e-06 old loss 4.149021606281167e-06 BETTER
I0328 22:05:19.082213 2318757 finetune.py:68] layer 27_o @ epoch 2 new loss 3.070635784752085e-06 old loss 3.1075094284460647e-06 BETTER
I0328 22:05:20.529357 2318617 finetune.py:68] layer 25_up @ epoch 0 new loss 4.813819032278843e-06 old loss 4.9163541007146705e-06 BETTER
I0328 22:05:30.147497 2318687 finetune.py:68] layer 26_o @ epoch 4 new loss 2.4133628357958514e-06 old loss 2.4299770302604884e-06 BETTER
I0328 22:05:47.030105 2318547 finetune.py:68] layer 24_up @ epoch 2 new loss 4.049392373417504e-06 old loss 4.091275513928849e-06 BETTER
I0328 22:05:51.835699 2318617 finetune.py:68] layer 25_up @ epoch 1 new loss 4.74648732051719e-06 old loss 4.813819032278843e-06 BETTER
I0328 22:05:52.178741 2318757 finetune.py:68] layer 27_o @ epoch 3 new loss 3.048636699531926e-06 old loss 3.070635784752085e-06 BETTER
I0328 22:06:01.635173 2318687 finetune.py:45] layer 26_up initial loss 6.031596967659425e-06
I0328 22:06:20.603131 2318547 finetune.py:68] layer 24_up @ epoch 3 new loss 4.015847935079364e-06 old loss 4.049392373417504e-06 BETTER
I0328 22:06:23.326765 2318617 finetune.py:68] layer 25_up @ epoch 2 new loss 4.6993059186206665e-06 old loss 4.74648732051719e-06 BETTER
I0328 22:06:25.431724 2318757 finetune.py:76] layer 27_o @ epoch 4 new loss 3.059077243960928e-06 old loss 3.048636699531926e-06 WORSE
I0328 22:06:32.237768 2318687 finetune.py:68] layer 26_up @ epoch 0 new loss 5.904797944822349e-06 old loss 6.031596967659425e-06 BETTER
I0328 22:06:54.219703 2318547 finetune.py:68] layer 24_up @ epoch 4 new loss 3.989131982962135e-06 old loss 4.015847935079364e-06 BETTER
I0328 22:06:54.980860 2318617 finetune.py:68] layer 25_up @ epoch 3 new loss 4.661668299377197e-06 old loss 4.6993059186206665e-06 BETTER
I0328 22:06:55.672929 2318757 finetune.py:45] layer 27_up initial loss 7.2842663030314725e-06
I0328 22:07:03.987549 2318687 finetune.py:68] layer 26_up @ epoch 1 new loss 5.82242773816688e-06 old loss 5.904797944822349e-06 BETTER
I0328 22:07:25.337105 2318547 finetune.py:45] layer 24_gate initial loss 5.329409304977162e-06
I0328 22:07:25.885194 2318757 finetune.py:68] layer 27_up @ epoch 0 new loss 7.120203918020707e-06 old loss 7.2842663030314725e-06 BETTER
I0328 22:07:26.760736 2318617 finetune.py:68] layer 25_up @ epoch 4 new loss 4.631512638297863e-06 old loss 4.661668299377197e-06 BETTER
I0328 22:07:36.140958 2318687 finetune.py:68] layer 26_up @ epoch 2 new loss 5.764094112237217e-06 old loss 5.82242773816688e-06 BETTER
I0328 22:07:55.061718 2318547 finetune.py:68] layer 24_gate @ epoch 0 new loss 5.297251391311875e-06 old loss 5.329409304977162e-06 BETTER
I0328 22:07:56.975640 2318757 finetune.py:68] layer 27_up @ epoch 1 new loss 7.013088634266751e-06 old loss 7.120203918020707e-06 BETTER
I0328 22:07:57.968838 2318617 finetune.py:45] layer 25_gate initial loss 6.1828354773751926e-06
I0328 22:08:08.287369 2318687 finetune.py:68] layer 26_up @ epoch 3 new loss 5.717674866900779e-06 old loss 5.764094112237217e-06 BETTER
I0328 22:08:26.133650 2318547 finetune.py:68] layer 24_gate @ epoch 1 new loss 5.272213456919417e-06 old loss 5.297251391311875e-06 BETTER
I0328 22:08:26.443966 2318617 finetune.py:68] layer 25_gate @ epoch 0 new loss 6.1465452745324e-06 old loss 6.1828354773751926e-06 BETTER
I0328 22:08:28.230657 2318757 finetune.py:68] layer 27_up @ epoch 2 new loss 6.939916602277663e-06 old loss 7.013088634266751e-06 BETTER
I0328 22:08:40.461085 2318687 finetune.py:68] layer 26_up @ epoch 4 new loss 5.679887181031518e-06 old loss 5.717674866900779e-06 BETTER
I0328 22:08:55.836374 2318617 finetune.py:68] layer 25_gate @ epoch 1 new loss 6.119321369624231e-06 old loss 6.1465452745324e-06 BETTER
I0328 22:08:57.361539 2318547 finetune.py:68] layer 24_gate @ epoch 2 new loss 5.252160462987376e-06 old loss 5.272213456919417e-06 BETTER
I0328 22:08:59.501309 2318757 finetune.py:68] layer 27_up @ epoch 3 new loss 6.883503829158144e-06 old loss 6.939916602277663e-06 BETTER
I0328 22:09:12.349590 2318687 finetune.py:45] layer 26_gate initial loss 7.526937224611174e-06
I0328 22:09:25.646454 2318617 finetune.py:68] layer 25_gate @ epoch 2 new loss 6.096541710576275e-06 old loss 6.119321369624231e-06 BETTER
I0328 22:09:28.792583 2318547 finetune.py:68] layer 24_gate @ epoch 3 new loss 5.235189746599644e-06 old loss 5.252160462987376e-06 BETTER
I0328 22:09:30.970605 2318757 finetune.py:68] layer 27_up @ epoch 4 new loss 6.839844900241587e-06 old loss 6.883503829158144e-06 BETTER
I0328 22:09:40.916610 2318687 finetune.py:68] layer 26_gate @ epoch 0 new loss 7.4822892202064395e-06 old loss 7.526937224611174e-06 BETTER
I0328 22:09:55.237472 2318617 finetune.py:68] layer 25_gate @ epoch 3 new loss 6.077516900404589e-06 old loss 6.096541710576275e-06 BETTER
I0328 22:10:00.002293 2318547 finetune.py:68] layer 24_gate @ epoch 4 new loss 5.220840648689773e-06 old loss 5.235189746599644e-06 BETTER
I0328 22:10:02.708212 2318757 finetune.py:45] layer 27_gate initial loss 9.137102097156458e-06
I0328 22:10:10.515854 2318687 finetune.py:68] layer 26_gate @ epoch 1 new loss 7.448921678587794e-06 old loss 7.4822892202064395e-06 BETTER
I0328 22:10:24.809191 2318617 finetune.py:68] layer 25_gate @ epoch 4 new loss 6.0624797697528265e-06 old loss 6.077516900404589e-06 BETTER
I0328 22:10:30.858014 2318757 finetune.py:68] layer 27_gate @ epoch 0 new loss 9.077156391867902e-06 old loss 9.137102097156458e-06 BETTER
I0328 22:10:40.269099 2318687 finetune.py:68] layer 26_gate @ epoch 2 new loss 7.423103852488566e-06 old loss 7.448921678587794e-06 BETTER
I0328 22:10:55.650600 2318547 finetune.py:45] layer 24_down initial loss 9.075213711184915e-06
I0328 22:10:59.864369 2318757 finetune.py:68] layer 27_gate @ epoch 1 new loss 9.031205081555527e-06 old loss 9.077156391867902e-06 BETTER
I0328 22:11:10.149505 2318687 finetune.py:68] layer 26_gate @ epoch 3 new loss 7.400998129014624e-06 old loss 7.423103852488566e-06 BETTER
I0328 22:11:22.683405 2318617 finetune.py:45] layer 25_down initial loss 1.0316982297808863e-05
I0328 22:11:23.268237 2318547 finetune.py:68] layer 24_down @ epoch 0 new loss 9.073508408619091e-06 old loss 9.075213711184915e-06 BETTER
I0328 22:11:29.106515 2318757 finetune.py:68] layer 27_gate @ epoch 2 new loss 8.997687473311089e-06 old loss 9.031205081555527e-06 BETTER
I0328 22:11:40.181418 2318687 finetune.py:68] layer 26_gate @ epoch 4 new loss 7.3833216447383165e-06 old loss 7.400998129014624e-06 BETTER
I0328 22:11:48.720010 2318617 finetune.py:68] layer 25_down @ epoch 0 new loss 1.03146412584465e-05 old loss 1.0316982297808863e-05 BETTER
I0328 22:11:51.728615 2318547 finetune.py:68] layer 24_down @ epoch 1 new loss 9.072817192645743e-06 old loss 9.073508408619091e-06 BETTER
I0328 22:11:58.385305 2318757 finetune.py:68] layer 27_gate @ epoch 3 new loss 8.970178896561265e-06 old loss 8.997687473311089e-06 BETTER
I0328 22:12:15.898472 2318617 finetune.py:68] layer 25_down @ epoch 1 new loss 1.0313773600501008e-05 old loss 1.03146412584465e-05 BETTER
I0328 22:12:20.576665 2318547 finetune.py:68] layer 24_down @ epoch 2 new loss 9.072434295376297e-06 old loss 9.072817192645743e-06 BETTER
I0328 22:12:27.796129 2318757 finetune.py:68] layer 27_gate @ epoch 4 new loss 8.948733920988161e-06 old loss 8.970178896561265e-06 BETTER
I0328 22:12:38.510387 2318687 finetune.py:45] layer 26_down initial loss 1.2261513802513946e-05
I0328 22:12:43.272138 2318617 finetune.py:68] layer 25_down @ epoch 2 new loss 1.0313054190191906e-05 old loss 1.0313773600501008e-05 BETTER
I0328 22:12:49.617628 2318547 finetune.py:68] layer 24_down @ epoch 3 new loss 9.072238754015416e-06 old loss 9.072434295376297e-06 BETTER
I0328 22:13:04.765834 2318687 finetune.py:68] layer 26_down @ epoch 0 new loss 1.2258848983037751e-05 old loss 1.2261513802513946e-05 BETTER
I0328 22:13:10.838908 2318617 finetune.py:68] layer 25_down @ epoch 3 new loss 1.0312608537788037e-05 old loss 1.0313054190191906e-05 BETTER
I0328 22:13:18.656937 2318547 finetune.py:68] layer 24_down @ epoch 4 new loss 9.072125976672396e-06 old loss 9.072238754015416e-06 BETTER
24_v proxy err 0.0003990161349065602 tr(WHW.T) 467.2783508300781
bpp_loss 5.880180637584999
24_q proxy err 4.2765630496433005e-05 tr(WHW.T) 22437.568359375
bpp_loss 6.40285086655058
24_k proxy err 3.056293644476682e-05 tr(WHW.T) 14182.7568359375
bpp_loss 7.353122845757753
24_o proxy err 0.0004431754641700536 tr(WHW.T) 1589.8907470703125
bpp_loss 5.788227639044635
24_up proxy err 0.0004406596126500517 tr(WHW.T) 7311.4423828125
bpp_loss 5.780356025589364
24_gate proxy err 0.0001440610212739557 tr(WHW.T) 25876.271484375
bpp_loss 6.189986012210803
24_down proxy err 0.00046309514436870813 tr(WHW.T) 6742.84130859375
bpp_loss 5.7774520335452895
I0328 22:13:24.837835 2318757 finetune.py:45] layer 27_down initial loss 1.4706492947880179e-05
I0328 22:13:32.570474 2318687 finetune.py:68] layer 26_down @ epoch 1 new loss 1.2257683010830078e-05 old loss 1.2258848983037751e-05 BETTER
I0328 22:13:38.888156 2318617 finetune.py:68] layer 25_down @ epoch 4 new loss 1.0312247468391433e-05 old loss 1.0312608537788037e-05 BETTER
25_v proxy err 0.0003388788318261504 tr(WHW.T) 557.8086547851562
bpp_loss 5.8901161924004555
25_q proxy err 3.655338514363393e-05 tr(WHW.T) 26113.4609375
bpp_loss 6.374841504264623
25_k proxy err 2.7831483748741448e-05 tr(WHW.T) 14414.6689453125
bpp_loss 7.332852210034616
25_o proxy err 0.0003826768370345235 tr(WHW.T) 1990.3438720703125
bpp_loss 5.7909566551679745
25_up proxy err 0.0004370260867290199 tr(WHW.T) 7382.63916015625
bpp_loss 5.789711782974856
25_gate proxy err 0.00014186327462084591 tr(WHW.T) 26305.53125
bpp_loss 6.2016734302576095
25_down proxy err 0.0004694363451562822 tr(WHW.T) 6617.73291015625
bpp_loss 5.786634876692135
I0328 22:13:51.025439 2318757 finetune.py:68] layer 27_down @ epoch 0 new loss 1.4703351553180255e-05 old loss 1.4706492947880179e-05 BETTER
I0328 22:14:00.370593 2318687 finetune.py:68] layer 26_down @ epoch 2 new loss 1.2257061825948767e-05 old loss 1.2257683010830078e-05 BETTER
I0328 22:14:17.895025 2318757 finetune.py:68] layer 27_down @ epoch 1 new loss 1.4701727195642889e-05 old loss 1.4703351553180255e-05 BETTER
I0328 22:14:28.118134 2318687 finetune.py:68] layer 26_down @ epoch 3 new loss 1.2256589798198547e-05 old loss 1.2257061825948767e-05 BETTER
I0328 22:14:44.833569 2318757 finetune.py:68] layer 27_down @ epoch 2 new loss 1.4700798601552378e-05 old loss 1.4701727195642889e-05 BETTER
I0328 22:14:51.212455 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 67.15232515335083s
I0328 22:14:55.187568 2318827 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 22:14:55.187670 2318827 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 22:14:55.187711 2318827 utils.py:162] NumExpr defaulting to 16 threads.
I0328 22:14:55.580868 2318827 config.py:54] PyTorch version 2.6.0 available.
W0328 22:14:55.804967 2318827 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 22:14:55.957977 2318687 finetune.py:68] layer 26_down @ epoch 4 new loss 1.2256143236299977e-05 old loss 1.2256589798198547e-05 BETTER
W0328 22:14:56.402980 2318827 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 22:14:56.406504 2316731 quantize_finetune_llama.py:209] layer 29 gpu 1
I0328 22:14:56.421398 2318827 data_utils.py:336] using 256 training seqs, 128 validation seqs
26_v proxy err 0.0004208095488138497 tr(WHW.T) 434.54583740234375
bpp_loss 5.950286053819582
26_q proxy err 4.389090827316977e-05 tr(WHW.T) 21403.05859375
bpp_loss 6.38571839465294
26_k proxy err 2.8653214030782692e-05 tr(WHW.T) 15422.7255859375
bpp_loss 7.435379054979421
26_o proxy err 0.00027140710153616965 tr(WHW.T) 2389.033935546875
bpp_loss 5.810663080774248
26_up proxy err 0.00042306474642828107 tr(WHW.T) 7646.8173828125
bpp_loss 5.798817874863744
26_gate proxy err 0.00012937896826770157 tr(WHW.T) 28903.228515625
bpp_loss 6.212675991734224
26_down proxy err 0.00046940878382883966 tr(WHW.T) 6620.4912109375
bpp_loss 5.7951875776052475
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 22:15:11.843644 2318757 finetune.py:68] layer 27_down @ epoch 3 new loss 1.4700011888635345e-05 old loss 1.4700798601552378e-05 BETTER
I0328 22:15:13.521109 2318827 finetune.py:45] layer 28_v initial loss 4.1147854972223286e-06
W0328 22:15:13.521534 2318827 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 22:15:39.017129 2318757 finetune.py:68] layer 27_down @ epoch 4 new loss 1.4699182429467328e-05 old loss 1.4700011888635345e-05 BETTER
27_v proxy err 0.00029800739139318466 tr(WHW.T) 677.69384765625
bpp_loss 6.046397380414419
27_q proxy err 4.4686064939014614e-05 tr(WHW.T) 21305.8046875
bpp_loss 6.352176193962805
27_k proxy err 3.123232090729289e-05 tr(WHW.T) 14010.4755859375
bpp_loss 7.383823374868371
27_o proxy err 0.0003287557337898761 tr(WHW.T) 2159.64208984375
bpp_loss 5.846909365383908
27_up proxy err 0.00038635535747744143 tr(WHW.T) 8476.646484375
bpp_loss 5.813581954421742
27_gate proxy err 0.00011531044583534822 tr(WHW.T) 32814.55078125
bpp_loss 6.226950901294393
27_down proxy err 0.0003931130049750209 tr(WHW.T) 6545.61083984375
bpp_loss 5.806917162977958
I0328 22:15:48.251911 2318827 finetune.py:68] layer 28_v @ epoch 0 new loss 1.2462847962524393e-06 old loss 4.1147854972223286e-06 BETTER
I0328 22:16:04.567216 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 64.06160855293274s
I0328 22:16:08.193118 2318897 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 22:16:08.193211 2318897 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 22:16:08.193250 2318897 utils.py:162] NumExpr defaulting to 16 threads.
I0328 22:16:08.548546 2318897 config.py:54] PyTorch version 2.6.0 available.
W0328 22:16:08.737167 2318897 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 22:16:09.477110 2318897 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 22:16:09.480606 2316731 quantize_finetune_llama.py:209] layer 30 gpu 2
I0328 22:16:09.494038 2318897 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 22:16:24.577169 2318827 finetune.py:68] layer 28_v @ epoch 1 new loss 1.2408422662701923e-06 old loss 1.2462847962524393e-06 BETTER
I0328 22:16:26.479178 2318897 finetune.py:45] layer 29_v initial loss 5.2648765631602146e-06
W0328 22:16:26.479410 2318897 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 22:16:59.641190 2318897 finetune.py:68] layer 29_v @ epoch 0 new loss 1.5312443792936392e-06 old loss 5.2648765631602146e-06 BETTER
I0328 22:17:01.400840 2318827 finetune.py:76] layer 28_v @ epoch 2 new loss 1.5792908243383863e-06 old loss 1.2408422662701923e-06 WORSE
I0328 22:17:14.241747 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 64.31255912780762s
I0328 22:17:17.991098 2318967 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 22:17:17.991185 2318967 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 22:17:17.991222 2318967 utils.py:162] NumExpr defaulting to 16 threads.
I0328 22:17:18.339161 2318967 config.py:54] PyTorch version 2.6.0 available.
W0328 22:17:18.527900 2318967 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 22:17:19.096117 2318967 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 22:17:19.100219 2316731 quantize_finetune_llama.py:209] layer 31 gpu 3
I0328 22:17:19.118246 2318967 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 22:17:34.110189 2318897 finetune.py:76] layer 29_v @ epoch 1 new loss 1.85331407465128e-06 old loss 1.5312443792936392e-06 WORSE
I0328 22:17:36.791406 2318967 finetune.py:45] layer 30_v initial loss 5.429745215224102e-06
W0328 22:17:36.791628 2318967 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 22:17:37.705496 2318827 finetune.py:76] layer 28_v @ epoch 3 new loss 1.6868759757926455e-06 old loss 1.2408422662701923e-06 WORSE
I0328 22:18:08.196041 2318897 finetune.py:76] layer 29_v @ epoch 2 new loss 2.691239160412806e-06 old loss 1.5312443792936392e-06 WORSE
I0328 22:18:10.457949 2318967 finetune.py:68] layer 30_v @ epoch 0 new loss 3.0356302431755466e-06 old loss 5.429745215224102e-06 BETTER
I0328 22:18:14.036118 2318827 finetune.py:76] layer 28_v @ epoch 4 new loss 1.6689211861375952e-06 old loss 1.2408422662701923e-06 WORSE
I0328 22:18:24.496107 2316731 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 64.88334512710571s
I0328 22:18:28.342257 2319037 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 22:18:28.342352 2319037 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 22:18:28.342395 2319037 utils.py:162] NumExpr defaulting to 16 threads.
I0328 22:18:28.700720 2319037 config.py:54] PyTorch version 2.6.0 available.
W0328 22:18:28.908809 2319037 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 22:18:29.512984 2319037 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 22:18:29.533496 2319037 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 22:18:32.880685 2318827 finetune.py:45] layer 28_q initial loss 1.6473569530717214e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 22:18:42.453980 2318897 finetune.py:76] layer 29_v @ epoch 3 new loss 2.8204133286635624e-06 old loss 1.5312443792936392e-06 WORSE
I0328 22:18:45.234664 2318967 finetune.py:76] layer 30_v @ epoch 1 new loss 3.547926553437719e-06 old loss 3.0356302431755466e-06 WORSE
I0328 22:18:47.118962 2319037 finetune.py:45] layer 31_v initial loss 8.016843821678776e-06
W0328 22:18:47.119312 2319037 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 22:19:08.307064 2318827 finetune.py:68] layer 28_q @ epoch 0 new loss 1.6037537307056482e-06 old loss 1.6473569530717214e-06 BETTER
I0328 22:19:17.105890 2318897 finetune.py:76] layer 29_v @ epoch 4 new loss 2.4097109871945577e-06 old loss 1.5312443792936392e-06 WORSE
I0328 22:19:19.735739 2318967 finetune.py:76] layer 30_v @ epoch 2 new loss 5.680118192685768e-06 old loss 3.0356302431755466e-06 WORSE
I0328 22:19:20.344242 2319037 finetune.py:76] layer 31_v @ epoch 0 new loss 1.2780304132320452e-05 old loss 8.016843821678776e-06 WORSE
I0328 22:19:36.532021 2318897 finetune.py:45] layer 29_q initial loss 2.6386605895822868e-06
I0328 22:19:44.645997 2318827 finetune.py:68] layer 28_q @ epoch 1 new loss 1.5709285889897728e-06 old loss 1.6037537307056482e-06 BETTER
I0328 22:19:54.148641 2319037 finetune.py:76] layer 31_v @ epoch 1 new loss 0.00013613179908134043 old loss 8.016843821678776e-06 WORSE
I0328 22:19:54.375511 2318967 finetune.py:76] layer 30_v @ epoch 3 new loss 5.722177775169257e-06 old loss 3.0356302431755466e-06 WORSE
I0328 22:20:10.005614 2318897 finetune.py:68] layer 29_q @ epoch 0 new loss 2.3520119611930568e-06 old loss 2.6386605895822868e-06 BETTER
I0328 22:20:21.003308 2318827 finetune.py:68] layer 28_q @ epoch 2 new loss 1.5429495761054568e-06 old loss 1.5709285889897728e-06 BETTER
I0328 22:20:28.090304 2319037 finetune.py:76] layer 31_v @ epoch 2 new loss 3.5229189961683005e-05 old loss 8.016843821678776e-06 WORSE
I0328 22:20:29.012414 2318967 finetune.py:76] layer 30_v @ epoch 4 new loss 4.003132744401228e-06 old loss 3.0356302431755466e-06 WORSE
I0328 22:20:44.473804 2318897 finetune.py:76] layer 29_q @ epoch 1 new loss 2.469092351020663e-06 old loss 2.3520119611930568e-06 WORSE
I0328 22:20:48.221472 2318967 finetune.py:45] layer 30_q initial loss 3.7782931485708104e-06
I0328 22:20:57.668241 2318827 finetune.py:68] layer 28_q @ epoch 3 new loss 1.5015466487966478e-06 old loss 1.5429495761054568e-06 BETTER
I0328 22:21:01.969944 2319037 finetune.py:76] layer 31_v @ epoch 3 new loss 3.839661439997144e-05 old loss 8.016843821678776e-06 WORSE
I0328 22:21:18.329400 2318897 finetune.py:76] layer 29_q @ epoch 2 new loss 2.487507117621135e-06 old loss 2.3520119611930568e-06 WORSE
I0328 22:21:21.966450 2318967 finetune.py:68] layer 30_q @ epoch 0 new loss 3.654443162304233e-06 old loss 3.7782931485708104e-06 BETTER
I0328 22:21:34.374760 2318827 finetune.py:76] layer 28_q @ epoch 4 new loss 1.677779437159188e-06 old loss 1.5015466487966478e-06 WORSE
I0328 22:21:35.951620 2319037 finetune.py:76] layer 31_v @ epoch 4 new loss 2.8978085538255982e-05 old loss 8.016843821678776e-06 WORSE
I0328 22:21:52.033968 2318827 finetune.py:45] layer 28_k initial loss 1.972862037291634e-06
I0328 22:21:52.706344 2318897 finetune.py:68] layer 29_q @ epoch 3 new loss 2.2700151021126658e-06 old loss 2.3520119611930568e-06 BETTER
I0328 22:21:55.090574 2319037 finetune.py:45] layer 31_q initial loss 9.60501165536698e-06
I0328 22:21:56.830156 2318967 finetune.py:76] layer 30_q @ epoch 1 new loss 3.911913154297508e-06 old loss 3.654443162304233e-06 WORSE
I0328 22:22:27.586399 2318827 finetune.py:68] layer 28_k @ epoch 0 new loss 1.8748437469184864e-06 old loss 1.972862037291634e-06 BETTER
I0328 22:22:27.949756 2318897 finetune.py:76] layer 29_q @ epoch 4 new loss 2.319206259926432e-06 old loss 2.2700151021126658e-06 WORSE
I0328 22:22:28.664805 2319037 finetune.py:76] layer 31_q @ epoch 0 new loss 1.2781536497641355e-05 old loss 9.60501165536698e-06 WORSE
I0328 22:22:31.040365 2318967 finetune.py:76] layer 30_q @ epoch 2 new loss 4.057386377098737e-06 old loss 3.654443162304233e-06 WORSE
I0328 22:22:45.472472 2318897 finetune.py:45] layer 29_k initial loss 2.8667291189776734e-06
I0328 22:23:02.272197 2319037 finetune.py:76] layer 31_q @ epoch 1 new loss 2.595631667645648e-05 old loss 9.60501165536698e-06 WORSE
I0328 22:23:04.169124 2318827 finetune.py:68] layer 28_k @ epoch 1 new loss 1.7685530337985256e-06 old loss 1.8748437469184864e-06 BETTER
I0328 22:23:05.334518 2318967 finetune.py:76] layer 30_q @ epoch 3 new loss 4.036551672470523e-06 old loss 3.654443162304233e-06 WORSE
I0328 22:23:19.091532 2318897 finetune.py:68] layer 29_k @ epoch 0 new loss 2.7072344437328866e-06 old loss 2.8667291189776734e-06 BETTER
I0328 22:23:36.014558 2319037 finetune.py:76] layer 31_q @ epoch 2 new loss 1.9628072550403886e-05 old loss 9.60501165536698e-06 WORSE
I0328 22:23:39.643742 2318967 finetune.py:76] layer 30_q @ epoch 4 new loss 4.540540885500377e-06 old loss 3.654443162304233e-06 WORSE
I0328 22:23:40.972527 2318827 finetune.py:76] layer 28_k @ epoch 2 new loss 1.8030334558716277e-06 old loss 1.7685530337985256e-06 WORSE
I0328 22:23:53.490534 2318897 finetune.py:68] layer 29_k @ epoch 1 new loss 2.471658717695391e-06 old loss 2.7072344437328866e-06 BETTER
I0328 22:23:57.178928 2318967 finetune.py:45] layer 30_k initial loss 4.913910288451007e-06
I0328 22:24:09.752817 2319037 finetune.py:76] layer 31_q @ epoch 3 new loss 1.9294007870485075e-05 old loss 9.60501165536698e-06 WORSE
I0328 22:24:17.319127 2318827 finetune.py:68] layer 28_k @ epoch 3 new loss 1.7314184788119746e-06 old loss 1.7685530337985256e-06 BETTER
I0328 22:24:28.157552 2318897 finetune.py:76] layer 29_k @ epoch 2 new loss 2.538340368118952e-06 old loss 2.471658717695391e-06 WORSE
I0328 22:24:30.998982 2318967 finetune.py:76] layer 30_k @ epoch 0 new loss 5.900779342482565e-06 old loss 4.913910288451007e-06 WORSE
I0328 22:24:43.257243 2319037 finetune.py:76] layer 31_q @ epoch 4 new loss 1.965929550351575e-05 old loss 9.60501165536698e-06 WORSE
I0328 22:24:54.238262 2318827 finetune.py:76] layer 28_k @ epoch 4 new loss 1.823662273636728e-06 old loss 1.7314184788119746e-06 WORSE
I0328 22:25:00.243915 2319037 finetune.py:45] layer 31_k initial loss 1.1362607438059058e-05
I0328 22:25:01.996091 2318897 finetune.py:76] layer 29_k @ epoch 3 new loss 2.491503892088076e-06 old loss 2.471658717695391e-06 WORSE
I0328 22:25:04.876282 2318967 finetune.py:76] layer 30_k @ epoch 1 new loss 7.197535978775704e-06 old loss 4.913910288451007e-06 WORSE
I0328 22:25:13.725071 2318827 finetune.py:45] layer 28_o initial loss 3.7907384466961958e-06
I0328 22:25:33.440716 2319037 finetune.py:76] layer 31_k @ epoch 0 new loss 1.1731028280337341e-05 old loss 1.1362607438059058e-05 WORSE
I0328 22:25:35.943425 2318897 finetune.py:76] layer 29_k @ epoch 4 new loss 2.6295072075299686e-06 old loss 2.471658717695391e-06 WORSE
I0328 22:25:38.861518 2318967 finetune.py:76] layer 30_k @ epoch 2 new loss 5.590844011749141e-06 old loss 4.913910288451007e-06 WORSE
I0328 22:25:48.356368 2318827 finetune.py:68] layer 28_o @ epoch 0 new loss 3.7321240142773604e-06 old loss 3.7907384466961958e-06 BETTER
I0328 22:25:55.100082 2318897 finetune.py:45] layer 29_o initial loss 4.620428626367357e-06
I0328 22:26:06.815170 2319037 finetune.py:76] layer 31_k @ epoch 1 new loss 1.8716113117989153e-05 old loss 1.1362607438059058e-05 WORSE
I0328 22:26:12.945981 2318967 finetune.py:76] layer 30_k @ epoch 3 new loss 5.451192919281311e-06 old loss 4.913910288451007e-06 WORSE
I0328 22:26:23.997831 2318827 finetune.py:68] layer 28_o @ epoch 1 new loss 3.5393300095165614e-06 old loss 3.7321240142773604e-06 BETTER
I0328 22:26:27.753507 2318897 finetune.py:68] layer 29_o @ epoch 0 new loss 4.444786100066267e-06 old loss 4.620428626367357e-06 BETTER
I0328 22:26:39.882478 2319037 finetune.py:76] layer 31_k @ epoch 2 new loss 1.5916502889012918e-05 old loss 1.1362607438059058e-05 WORSE
I0328 22:26:46.829090 2318967 finetune.py:76] layer 30_k @ epoch 4 new loss 4.923018423141912e-06 old loss 4.913910288451007e-06 WORSE
I0328 22:27:00.150686 2318827 finetune.py:68] layer 28_o @ epoch 2 new loss 3.521382950566476e-06 old loss 3.5393300095165614e-06 BETTER
I0328 22:27:01.419309 2318897 finetune.py:68] layer 29_o @ epoch 1 new loss 4.388813067635056e-06 old loss 4.444786100066267e-06 BETTER
I0328 22:27:06.126968 2318967 finetune.py:45] layer 30_o initial loss 8.390184120798949e-06
I0328 22:27:12.969598 2319037 finetune.py:76] layer 31_k @ epoch 3 new loss 1.9180959498044103e-05 old loss 1.1362607438059058e-05 WORSE
I0328 22:27:35.473824 2318897 finetune.py:68] layer 29_o @ epoch 2 new loss 4.305797574488679e-06 old loss 4.388813067635056e-06 BETTER
I0328 22:27:36.460407 2318827 finetune.py:76] layer 28_o @ epoch 3 new loss 3.549705979821738e-06 old loss 3.521382950566476e-06 WORSE
I0328 22:27:39.023157 2318967 finetune.py:68] layer 30_o @ epoch 0 new loss 8.028867341636214e-06 old loss 8.390184120798949e-06 BETTER
I0328 22:27:45.982262 2319037 finetune.py:76] layer 31_k @ epoch 4 new loss 1.8769702364807017e-05 old loss 1.1362607438059058e-05 WORSE
I0328 22:28:04.362634 2319037 finetune.py:45] layer 31_o initial loss 1.8925074982689694e-05
I0328 22:28:09.488963 2318897 finetune.py:76] layer 29_o @ epoch 3 new loss 4.413724582263967e-06 old loss 4.305797574488679e-06 WORSE
I0328 22:28:11.758574 2318827 finetune.py:76] layer 28_o @ epoch 4 new loss 3.5646978631120874e-06 old loss 3.521382950566476e-06 WORSE
I0328 22:28:13.017527 2318967 finetune.py:68] layer 30_o @ epoch 1 new loss 7.83646282798145e-06 old loss 8.028867341636214e-06 BETTER
I0328 22:28:36.342567 2319037 finetune.py:68] layer 31_o @ epoch 0 new loss 1.5658415577490814e-05 old loss 1.8925074982689694e-05 BETTER
I0328 22:28:42.951471 2318897 finetune.py:68] layer 29_o @ epoch 4 new loss 4.278199867258081e-06 old loss 4.305797574488679e-06 BETTER
I0328 22:28:42.953182 2318827 finetune.py:45] layer 28_up initial loss 8.831964805722237e-06
I0328 22:28:47.203966 2318967 finetune.py:68] layer 30_o @ epoch 2 new loss 7.63760726840701e-06 old loss 7.83646282798145e-06 BETTER
I0328 22:29:09.499770 2319037 finetune.py:76] layer 31_o @ epoch 1 new loss 1.61075313371839e-05 old loss 1.5658415577490814e-05 WORSE
I0328 22:29:14.852428 2318827 finetune.py:68] layer 28_up @ epoch 0 new loss 8.649431947560515e-06 old loss 8.831964805722237e-06 BETTER
I0328 22:29:14.916818 2318897 finetune.py:45] layer 29_up initial loss 1.1634901056822855e-05
I0328 22:29:21.485410 2318967 finetune.py:76] layer 30_o @ epoch 3 new loss 7.996463864401449e-06 old loss 7.63760726840701e-06 WORSE
I0328 22:29:42.350546 2319037 finetune.py:76] layer 31_o @ epoch 2 new loss 1.9554692698875442e-05 old loss 1.5658415577490814e-05 WORSE
I0328 22:29:45.504713 2318897 finetune.py:68] layer 29_up @ epoch 0 new loss 1.119835906138178e-05 old loss 1.1634901056822855e-05 BETTER
I0328 22:29:47.984737 2318827 finetune.py:68] layer 28_up @ epoch 1 new loss 8.441536010650452e-06 old loss 8.649431947560515e-06 BETTER
I0328 22:29:55.110712 2318967 finetune.py:68] layer 30_o @ epoch 4 new loss 7.493210432585329e-06 old loss 7.63760726840701e-06 BETTER
I0328 22:30:15.070905 2319037 finetune.py:76] layer 31_o @ epoch 3 new loss 2.0238496290403418e-05 old loss 1.5658415577490814e-05 WORSE
I0328 22:30:17.125557 2318897 finetune.py:68] layer 29_up @ epoch 1 new loss 1.0979419130308088e-05 old loss 1.119835906138178e-05 BETTER
I0328 22:30:21.674264 2318827 finetune.py:68] layer 28_up @ epoch 2 new loss 8.347961738763843e-06 old loss 8.441536010650452e-06 BETTER
I0328 22:30:27.154532 2318967 finetune.py:45] layer 30_up initial loss 2.2200296371011063e-05
I0328 22:30:47.948301 2319037 finetune.py:76] layer 31_o @ epoch 4 new loss 1.714429527055472e-05 old loss 1.5658415577490814e-05 WORSE
I0328 22:30:48.975904 2318897 finetune.py:68] layer 29_up @ epoch 2 new loss 1.0838019079528749e-05 old loss 1.0979419130308088e-05 BETTER
I0328 22:30:55.491282 2318827 finetune.py:68] layer 28_up @ epoch 3 new loss 8.269009413197637e-06 old loss 8.347961738763843e-06 BETTER
I0328 22:30:57.686496 2318967 finetune.py:68] layer 30_up @ epoch 0 new loss 2.1028234186815098e-05 old loss 2.2200296371011063e-05 BETTER
I0328 22:31:19.219698 2319037 finetune.py:45] layer 31_up initial loss 6.724906415911391e-05
I0328 22:31:21.024817 2318897 finetune.py:68] layer 29_up @ epoch 3 new loss 1.0721494618337601e-05 old loss 1.0838019079528749e-05 BETTER
I0328 22:31:29.508397 2318827 finetune.py:68] layer 28_up @ epoch 4 new loss 8.20752393337898e-06 old loss 8.269009413197637e-06 BETTER
I0328 22:31:29.966624 2318967 finetune.py:68] layer 30_up @ epoch 1 new loss 2.0313107597758062e-05 old loss 2.1028234186815098e-05 BETTER
I0328 22:31:49.362866 2319037 finetune.py:68] layer 31_up @ epoch 0 new loss 5.736429011449218e-05 old loss 6.724906415911391e-05 BETTER
I0328 22:31:52.949759 2318897 finetune.py:68] layer 29_up @ epoch 4 new loss 1.0627094525261782e-05 old loss 1.0721494618337601e-05 BETTER
I0328 22:32:01.111744 2318827 finetune.py:45] layer 28_gate initial loss 1.1189646102138795e-05
I0328 22:32:02.285948 2318967 finetune.py:68] layer 30_up @ epoch 2 new loss 1.983662514248863e-05 old loss 2.0313107597758062e-05 BETTER
I0328 22:32:20.621089 2319037 finetune.py:68] layer 31_up @ epoch 1 new loss 5.232749754213728e-05 old loss 5.736429011449218e-05 BETTER
I0328 22:32:24.928871 2318897 finetune.py:45] layer 29_gate initial loss 1.45680987770902e-05
I0328 22:32:31.107686 2318827 finetune.py:68] layer 28_gate @ epoch 0 new loss 1.1105911653430667e-05 old loss 1.1189646102138795e-05 BETTER
I0328 22:32:34.691387 2318967 finetune.py:68] layer 30_up @ epoch 3 new loss 1.9485281882225536e-05 old loss 1.983662514248863e-05 BETTER
I0328 22:32:52.028108 2319037 finetune.py:68] layer 31_up @ epoch 2 new loss 4.902364162262529e-05 old loss 5.232749754213728e-05 BETTER
I0328 22:32:53.570325 2318897 finetune.py:68] layer 29_gate @ epoch 0 new loss 1.4455012205871753e-05 old loss 1.45680987770902e-05 BETTER
I0328 22:33:02.306619 2318827 finetune.py:68] layer 28_gate @ epoch 1 new loss 1.1044085113098845e-05 old loss 1.1105911653430667e-05 BETTER
I0328 22:33:07.110705 2318967 finetune.py:68] layer 30_up @ epoch 4 new loss 1.9215736756450497e-05 old loss 1.9485281882225536e-05 BETTER
I0328 22:33:23.329086 2318897 finetune.py:68] layer 29_gate @ epoch 1 new loss 1.4367076801136136e-05 old loss 1.4455012205871753e-05 BETTER
I0328 22:33:23.617067 2319037 finetune.py:68] layer 31_up @ epoch 3 new loss 4.6413319068960845e-05 old loss 4.902364162262529e-05 BETTER
I0328 22:33:33.906328 2318827 finetune.py:68] layer 28_gate @ epoch 2 new loss 1.099470773624489e-05 old loss 1.1044085113098845e-05 BETTER
I0328 22:33:39.489367 2318967 finetune.py:45] layer 30_gate initial loss 2.4939034119597636e-05
I0328 22:33:52.906322 2318897 finetune.py:68] layer 29_gate @ epoch 2 new loss 1.4308350728242658e-05 old loss 1.4367076801136136e-05 BETTER
I0328 22:33:55.159076 2319037 finetune.py:68] layer 31_up @ epoch 4 new loss 4.451970016816631e-05 old loss 4.6413319068960845e-05 BETTER
I0328 22:34:05.293832 2318827 finetune.py:68] layer 28_gate @ epoch 3 new loss 1.095786410587607e-05 old loss 1.099470773624489e-05 BETTER
I0328 22:34:08.060645 2318967 finetune.py:68] layer 30_gate @ epoch 0 new loss 2.4584956918261014e-05 old loss 2.4939034119597636e-05 BETTER
I0328 22:34:22.670537 2318897 finetune.py:68] layer 29_gate @ epoch 3 new loss 1.4256238500820473e-05 old loss 1.4308350728242658e-05 BETTER
I0328 22:34:26.941743 2319037 finetune.py:45] layer 31_gate initial loss 5.583868551184423e-05
I0328 22:34:36.710734 2318827 finetune.py:68] layer 28_gate @ epoch 4 new loss 1.0923959052888677e-05 old loss 1.095786410587607e-05 BETTER
I0328 22:34:37.918064 2318967 finetune.py:68] layer 30_gate @ epoch 1 new loss 2.4316434064530768e-05 old loss 2.4584956918261014e-05 BETTER
I0328 22:34:52.284263 2318897 finetune.py:68] layer 29_gate @ epoch 4 new loss 1.4222563549992628e-05 old loss 1.4256238500820473e-05 BETTER
I0328 22:34:54.868350 2319037 finetune.py:68] layer 31_gate @ epoch 0 new loss 5.3693438530899584e-05 old loss 5.583868551184423e-05 BETTER
I0328 22:35:07.616377 2318967 finetune.py:68] layer 30_gate @ epoch 2 new loss 2.415647031739354e-05 old loss 2.4316434064530768e-05 BETTER
I0328 22:35:23.840894 2319037 finetune.py:68] layer 31_gate @ epoch 1 new loss 5.219386002863757e-05 old loss 5.3693438530899584e-05 BETTER
I0328 22:35:33.240154 2318827 finetune.py:45] layer 28_down initial loss 1.8277127310284413e-05
I0328 22:35:37.451593 2318967 finetune.py:68] layer 30_gate @ epoch 3 new loss 2.3987147869775072e-05 old loss 2.415647031739354e-05 BETTER
I0328 22:35:49.318311 2318897 finetune.py:45] layer 29_down initial loss 2.4316434064530768e-05
I0328 22:35:53.134191 2319037 finetune.py:68] layer 31_gate @ epoch 2 new loss 5.110720303491689e-05 old loss 5.219386002863757e-05 BETTER
I0328 22:36:00.628021 2318827 finetune.py:68] layer 28_down @ epoch 0 new loss 1.8273778550792485e-05 old loss 1.8277127310284413e-05 BETTER
I0328 22:36:07.373949 2318967 finetune.py:68] layer 30_gate @ epoch 4 new loss 2.3888547730166465e-05 old loss 2.3987147869775072e-05 BETTER
I0328 22:36:15.449248 2318897 finetune.py:68] layer 29_down @ epoch 0 new loss 2.4310313165187836e-05 old loss 2.4316434064530768e-05 BETTER
I0328 22:36:22.327781 2319037 finetune.py:68] layer 31_gate @ epoch 3 new loss 5.0194568757433444e-05 old loss 5.110720303491689e-05 BETTER
I0328 22:36:29.419002 2318827 finetune.py:68] layer 28_down @ epoch 1 new loss 1.8272297893418e-05 old loss 1.8273778550792485e-05 BETTER
I0328 22:36:42.489143 2318897 finetune.py:68] layer 29_down @ epoch 1 new loss 2.4307331841555424e-05 old loss 2.4310313165187836e-05 BETTER
I0328 22:36:51.604841 2319037 finetune.py:68] layer 31_gate @ epoch 4 new loss 4.944422471453436e-05 old loss 5.0194568757433444e-05 BETTER
I0328 22:36:58.196459 2318827 finetune.py:68] layer 28_down @ epoch 2 new loss 1.827136475185398e-05 old loss 1.8272297893418e-05 BETTER
I0328 22:37:04.250998 2318967 finetune.py:45] layer 30_down initial loss 4.121295205550268e-05
I0328 22:37:09.840188 2318897 finetune.py:68] layer 29_down @ epoch 2 new loss 2.4306089471792802e-05 old loss 2.4307331841555424e-05 BETTER
I0328 22:37:27.107441 2318827 finetune.py:68] layer 28_down @ epoch 3 new loss 1.8270884538651444e-05 old loss 1.827136475185398e-05 BETTER
I0328 22:37:30.529830 2318967 finetune.py:68] layer 30_down @ epoch 0 new loss 4.119809091207571e-05 old loss 4.121295205550268e-05 BETTER
I0328 22:37:37.451215 2318897 finetune.py:68] layer 29_down @ epoch 3 new loss 2.4304848921019584e-05 old loss 2.4306089471792802e-05 BETTER
I0328 22:37:48.524737 2319037 finetune.py:45] layer 31_down initial loss 0.00010724826279329136
I0328 22:37:56.082776 2318827 finetune.py:68] layer 28_down @ epoch 4 new loss 1.8270464352099225e-05 old loss 1.8270884538651444e-05 BETTER
I0328 22:37:58.113389 2318967 finetune.py:68] layer 30_down @ epoch 1 new loss 4.1191560740116984e-05 old loss 4.119809091207571e-05 BETTER
28_v proxy err 0.0003275049093645066 tr(WHW.T) 601.4844360351562
bpp_loss 6.105957684805617
28_q proxy err 4.0191313019022346e-05 tr(WHW.T) 23165.142578125
bpp_loss 6.359547790372744
28_k proxy err 2.477394991728943e-05 tr(WHW.T) 14993.1318359375
bpp_loss 7.293489467585459
28_o proxy err 0.00028895711875520647 tr(WHW.T) 2510.75048828125
bpp_loss 5.877345170592889
28_up proxy err 0.0003141407505609095 tr(WHW.T) 10245.6533203125
bpp_loss 5.837033248639533
28_gate proxy err 0.00010237856622552499 tr(WHW.T) 35922.6640625
bpp_loss 6.215506885732923
28_down proxy err 0.00037359812995418906 tr(WHW.T) 7211.8994140625
bpp_loss 5.824381038380254
I0328 22:38:05.023263 2318897 finetune.py:68] layer 29_down @ epoch 4 new loss 2.4303832105943002e-05 old loss 2.4304848921019584e-05 BETTER
29_v proxy err 0.0002510783670004457 tr(WHW.T) 850.4290161132812
bpp_loss 6.175353225320578
29_q proxy err 4.6306431613629684e-05 tr(WHW.T) 20653.26171875
bpp_loss 6.345763331279159
29_k proxy err 2.3508797312388197e-05 tr(WHW.T) 16367.0107421875
bpp_loss 7.420371081214398
29_o proxy err 0.0001893969892989844 tr(WHW.T) 3101.5458984375
bpp_loss 5.917408764013089
29_up proxy err 0.00024589477106928825 tr(WHW.T) 12869.5771484375
bpp_loss 5.870236773842147
29_gate proxy err 9.310706809628755e-05 tr(WHW.T) 38376.83203125
bpp_loss 6.210140821790056
29_down proxy err 0.00031579536153003573 tr(WHW.T) 7472.310546875
bpp_loss 5.842215576275651
I0328 22:38:14.419562 2319037 finetune.py:68] layer 31_down @ epoch 0 new loss 0.0001071957012754865 old loss 0.00010724826279329136 BETTER
I0328 22:38:25.798447 2318967 finetune.py:68] layer 30_down @ epoch 2 new loss 4.1187282477039844e-05 old loss 4.1191560740116984e-05 BETTER
I0328 22:38:41.281227 2319037 finetune.py:68] layer 31_down @ epoch 1 new loss 0.00010717203986132517 old loss 0.0001071957012754865 BETTER
I0328 22:38:53.549650 2318967 finetune.py:68] layer 30_down @ epoch 3 new loss 4.1184968722518533e-05 old loss 4.1187282477039844e-05 BETTER
I0328 22:39:08.236922 2319037 finetune.py:68] layer 31_down @ epoch 2 new loss 0.00010715352982515469 old loss 0.00010717203986132517 BETTER
I0328 22:39:21.266146 2318967 finetune.py:68] layer 30_down @ epoch 4 new loss 4.118266951991245e-05 old loss 4.1184968722518533e-05 BETTER
30_v proxy err 0.00025379739236086607 tr(WHW.T) 863.060791015625
bpp_loss 6.506117347162217
30_q proxy err 3.6894627555739135e-05 tr(WHW.T) 24050.92578125
bpp_loss 6.24584396649152
30_k proxy err 3.0266410249168985e-05 tr(WHW.T) 14030.7314453125
bpp_loss 6.986852645874023
30_o proxy err 0.000131632958073169 tr(WHW.T) 4869.06884765625
bpp_loss 6.007687895442359
30_up proxy err 0.00014579038543161005 tr(WHW.T) 21709.501953125
bpp_loss 5.902064508891532
30_gate proxy err 6.942402251297608e-05 tr(WHW.T) 51948.11328125
bpp_loss 6.270277877870415
30_down proxy err 0.0001948646386153996 tr(WHW.T) 8809.12109375
bpp_loss 5.846070291807076
I0328 22:39:35.268879 2319037 finetune.py:68] layer 31_down @ epoch 3 new loss 0.00010714208474382758 old loss 0.00010715352982515469 BETTER
I0328 22:40:02.380044 2319037 finetune.py:68] layer 31_down @ epoch 4 new loss 0.00010713399387896061 old loss 0.00010714208474382758 BETTER
31_v proxy err 0.00012048245844198391 tr(WHW.T) 1808.723876953125
bpp_loss 6.28263864654582
31_q proxy err 2.0650573787861504e-05 tr(WHW.T) 46175.66015625
bpp_loss 6.420444000978023
31_k proxy err 1.8031470972346142e-05 tr(WHW.T) 20467.64453125
bpp_loss 7.277849041623995
31_o proxy err 0.00011605103645706549 tr(WHW.T) 2213.58154296875
bpp_loss 5.979711925028823
31_up proxy err 4.6699315134901553e-05 tr(WHW.T) 68997.3984375
bpp_loss 6.109050086034196
31_gate proxy err 2.5679722966742702e-05 tr(WHW.T) 144359.640625
bpp_loss 6.52285848664386
31_down proxy err 8.92236566869542e-05 tr(WHW.T) 9962.2353515625
bpp_loss 5.878449603416292
I0328 22:40:29.123716 2319107 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 22:40:29.123866 2319107 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 22:40:29.123910 2319107 utils.py:162] NumExpr defaulting to 16 threads.
I0328 22:40:29.458761 2319107 config.py:54] PyTorch version 2.6.0 available.
W0328 22:40:29.677178 2319107 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 22:40:29.790924 2319107 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.29it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.51it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.87it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.87it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.10it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.36it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.62it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.34it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.38it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.60it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.47it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.65it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.82it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.43it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.64it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.60it/s]
I0328 22:40:32.783808 2319107 hfize_llama.py:153] loaded layer 0
I0328 22:40:33.113008 2319107 hfize_llama.py:153] loaded layer 1
I0328 22:40:33.483720 2319107 hfize_llama.py:153] loaded layer 2
I0328 22:40:33.847941 2319107 hfize_llama.py:153] loaded layer 3
I0328 22:40:34.244464 2319107 hfize_llama.py:153] loaded layer 4
I0328 22:40:34.635920 2319107 hfize_llama.py:153] loaded layer 5
I0328 22:40:34.957056 2319107 hfize_llama.py:153] loaded layer 6
I0328 22:40:35.295251 2319107 hfize_llama.py:153] loaded layer 7
I0328 22:40:35.629011 2319107 hfize_llama.py:153] loaded layer 8
I0328 22:40:35.928264 2319107 hfize_llama.py:153] loaded layer 9
I0328 22:40:36.307286 2319107 hfize_llama.py:153] loaded layer 10
I0328 22:40:36.609063 2319107 hfize_llama.py:153] loaded layer 11
I0328 22:40:36.937881 2319107 hfize_llama.py:153] loaded layer 12
I0328 22:40:37.259205 2319107 hfize_llama.py:153] loaded layer 13
I0328 22:40:37.589770 2319107 hfize_llama.py:153] loaded layer 14
I0328 22:40:37.893404 2319107 hfize_llama.py:153] loaded layer 15
I0328 22:40:38.226572 2319107 hfize_llama.py:153] loaded layer 16
I0328 22:40:38.593861 2319107 hfize_llama.py:153] loaded layer 17
I0328 22:40:38.968173 2319107 hfize_llama.py:153] loaded layer 18
I0328 22:40:39.305017 2319107 hfize_llama.py:153] loaded layer 19
I0328 22:40:39.641523 2319107 hfize_llama.py:153] loaded layer 20
I0328 22:40:39.958900 2319107 hfize_llama.py:153] loaded layer 21
I0328 22:40:40.306841 2319107 hfize_llama.py:153] loaded layer 22
I0328 22:40:40.645645 2319107 hfize_llama.py:153] loaded layer 23
I0328 22:40:40.964384 2319107 hfize_llama.py:153] loaded layer 24
I0328 22:40:41.303677 2319107 hfize_llama.py:153] loaded layer 25
I0328 22:40:41.652698 2319107 hfize_llama.py:153] loaded layer 26
I0328 22:40:41.996066 2319107 hfize_llama.py:153] loaded layer 27
I0328 22:40:42.375263 2319107 hfize_llama.py:153] loaded layer 28
I0328 22:40:42.726912 2319107 hfize_llama.py:153] loaded layer 29
I0328 22:40:43.324941 2319107 hfize_llama.py:153] loaded layer 30
I0328 22:40:43.695050 2319107 hfize_llama.py:153] loaded layer 31
I0328 22:40:43.695173 2319107 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:08,  1.46s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.14s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.03s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.19s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.29s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.20s/it]
I0328 22:41:34.065496 2319107 hfize_llama.py:167] successfully loaded hfized model
I0328 22:41:39.216586 2319325 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 22:41:39.216722 2319325 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 22:41:39.216764 2319325 utils.py:162] NumExpr defaulting to 16 threads.
W0328 22:41:39.572174 2319325 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 22:41:39.912050 2319325 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.17s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.08s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.05s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.21s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.35s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:08<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.27s/it]
I0328 22:41:48.910205 2319325 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.510103702545166:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.510103702545166:   1%|          | 1/141 [00:01<04:22,  1.87s/it]avg_loss = 1.8376002311706543:   1%|          | 1/141 [00:03<04:22,  1.87s/it]avg_loss = 1.8376002311706543:   1%|▏         | 2/141 [00:03<03:47,  1.64s/it]avg_loss = 1.977222204208374:   1%|▏         | 2/141 [00:04<03:47,  1.64s/it] avg_loss = 1.977222204208374:   2%|▏         | 3/141 [00:04<03:35,  1.56s/it]avg_loss = 1.934118628501892:   2%|▏         | 3/141 [00:06<03:35,  1.56s/it]avg_loss = 1.934118628501892:   3%|▎         | 4/141 [00:06<03:29,  1.53s/it]avg_loss = 1.8865232706069945:   3%|▎         | 4/141 [00:07<03:29,  1.53s/it]avg_loss = 1.8865232706069945:   4%|▎         | 5/141 [00:07<03:25,  1.51s/it]avg_loss = 1.7904204726219177:   4%|▎         | 5/141 [00:09<03:25,  1.51s/it]avg_loss = 1.7904204726219177:   4%|▍         | 6/141 [00:09<03:22,  1.50s/it]avg_loss = 1.7254502773284912:   4%|▍         | 6/141 [00:10<03:22,  1.50s/it]avg_loss = 1.7254502773284912:   5%|▍         | 7/141 [00:10<03:20,  1.50s/it]avg_loss = 1.7229164987802505:   5%|▍         | 7/141 [00:12<03:20,  1.50s/it]avg_loss = 1.7229164987802505:   6%|▌         | 8/141 [00:12<03:18,  1.50s/it]avg_loss = 1.7579973141352336:   6%|▌         | 8/141 [00:13<03:18,  1.50s/it]avg_loss = 1.7579973141352336:   6%|▋         | 9/141 [00:13<03:17,  1.49s/it]avg_loss = 1.763942015171051:   6%|▋         | 9/141 [00:15<03:17,  1.49s/it] avg_loss = 1.763942015171051:   7%|▋         | 10/141 [00:15<03:15,  1.50s/it]avg_loss = 1.7614190795204856:   7%|▋         | 10/141 [00:16<03:15,  1.50s/it]avg_loss = 1.7614190795204856:   8%|▊         | 11/141 [00:16<03:14,  1.50s/it]avg_loss = 1.7845558524131775:   8%|▊         | 11/141 [00:18<03:14,  1.50s/it]avg_loss = 1.7845558524131775:   9%|▊         | 12/141 [00:18<03:13,  1.50s/it]avg_loss = 1.7975530349291289:   9%|▊         | 12/141 [00:19<03:13,  1.50s/it]avg_loss = 1.7975530349291289:   9%|▉         | 13/141 [00:19<03:12,  1.50s/it]avg_loss = 1.816441033567701:   9%|▉         | 13/141 [00:21<03:12,  1.50s/it] avg_loss = 1.816441033567701:  10%|▉         | 14/141 [00:21<03:10,  1.50s/it]avg_loss = 1.8267994244893393:  10%|▉         | 14/141 [00:22<03:10,  1.50s/it]avg_loss = 1.8267994244893393:  11%|█         | 15/141 [00:22<03:09,  1.51s/it]avg_loss = 1.8516613692045212:  11%|█         | 15/141 [00:24<03:09,  1.51s/it]avg_loss = 1.8516613692045212:  11%|█▏        | 16/141 [00:24<03:08,  1.51s/it]avg_loss = 1.8553497019936056:  11%|█▏        | 16/141 [00:25<03:08,  1.51s/it]avg_loss = 1.8553497019936056:  12%|█▏        | 17/141 [00:25<03:07,  1.51s/it]avg_loss = 1.8572967516051397:  12%|█▏        | 17/141 [00:27<03:07,  1.51s/it]avg_loss = 1.8572967516051397:  13%|█▎        | 18/141 [00:27<03:06,  1.51s/it]avg_loss = 1.8356973622974597:  13%|█▎        | 18/141 [00:28<03:06,  1.51s/it]avg_loss = 1.8356973622974597:  13%|█▎        | 19/141 [00:28<03:04,  1.52s/it]avg_loss = 1.834401136636734:  13%|█▎        | 19/141 [00:30<03:04,  1.52s/it] avg_loss = 1.834401136636734:  14%|█▍        | 20/141 [00:30<03:03,  1.52s/it]avg_loss = 1.8396122796194894:  14%|█▍        | 20/141 [00:31<03:03,  1.52s/it]avg_loss = 1.8396122796194894:  15%|█▍        | 21/141 [00:31<03:02,  1.52s/it]avg_loss = 1.8421275236389854:  15%|█▍        | 21/141 [00:33<03:02,  1.52s/it]avg_loss = 1.8421275236389854:  16%|█▌        | 22/141 [00:33<03:01,  1.52s/it]avg_loss = 1.843889075776805:  16%|█▌        | 22/141 [00:34<03:01,  1.52s/it] avg_loss = 1.843889075776805:  16%|█▋        | 23/141 [00:34<03:00,  1.53s/it]avg_loss = 1.8490646878878276:  16%|█▋        | 23/141 [00:36<03:00,  1.53s/it]avg_loss = 1.8490646878878276:  17%|█▋        | 24/141 [00:36<02:58,  1.53s/it]avg_loss = 1.855133991241455:  17%|█▋        | 24/141 [00:38<02:58,  1.53s/it] avg_loss = 1.855133991241455:  18%|█▊        | 25/141 [00:38<02:57,  1.53s/it]avg_loss = 1.8670884187404926:  18%|█▊        | 25/141 [00:39<02:57,  1.53s/it]avg_loss = 1.8670884187404926:  18%|█▊        | 26/141 [00:39<02:56,  1.53s/it]avg_loss = 1.8804125874130815:  18%|█▊        | 26/141 [00:41<02:56,  1.53s/it]avg_loss = 1.8804125874130815:  19%|█▉        | 27/141 [00:41<02:55,  1.54s/it]avg_loss = 1.8874695471354894:  19%|█▉        | 27/141 [00:42<02:55,  1.54s/it]avg_loss = 1.8874695471354894:  20%|█▉        | 28/141 [00:42<02:53,  1.54s/it]avg_loss = 1.8841195353146256:  20%|█▉        | 28/141 [00:44<02:53,  1.54s/it]avg_loss = 1.8841195353146256:  21%|██        | 29/141 [00:44<02:52,  1.54s/it]avg_loss = 1.8744885961214701:  21%|██        | 29/141 [00:45<02:52,  1.54s/it]avg_loss = 1.8744885961214701:  21%|██▏       | 30/141 [00:45<02:51,  1.54s/it]avg_loss = 1.8599897623062134:  21%|██▏       | 30/141 [00:47<02:51,  1.54s/it]avg_loss = 1.8599897623062134:  22%|██▏       | 31/141 [00:47<02:49,  1.54s/it]avg_loss = 1.8477244563400745:  22%|██▏       | 31/141 [00:48<02:49,  1.54s/it]avg_loss = 1.8477244563400745:  23%|██▎       | 32/141 [00:48<02:48,  1.55s/it]avg_loss = 1.8468642668290571:  23%|██▎       | 32/141 [00:50<02:48,  1.55s/it]avg_loss = 1.8468642668290571:  23%|██▎       | 33/141 [00:50<02:47,  1.55s/it]avg_loss = 1.8455925092977636:  23%|██▎       | 33/141 [00:51<02:47,  1.55s/it]avg_loss = 1.8455925092977636:  24%|██▍       | 34/141 [00:51<02:45,  1.55s/it]avg_loss = 1.8484990358352662:  24%|██▍       | 34/141 [00:53<02:45,  1.55s/it]avg_loss = 1.8484990358352662:  25%|██▍       | 35/141 [00:53<02:44,  1.55s/it]avg_loss = 1.8321805000305176:  25%|██▍       | 35/141 [00:55<02:44,  1.55s/it]avg_loss = 1.8321805000305176:  26%|██▌       | 36/141 [00:55<02:42,  1.55s/it]avg_loss = 1.816699224549371:  26%|██▌       | 36/141 [00:56<02:42,  1.55s/it] avg_loss = 1.816699224549371:  26%|██▌       | 37/141 [00:56<02:41,  1.55s/it]avg_loss = 1.8019948758577045:  26%|██▌       | 37/141 [00:58<02:41,  1.55s/it]avg_loss = 1.8019948758577045:  27%|██▋       | 38/141 [00:58<02:39,  1.55s/it]avg_loss = 1.7878138285416822:  27%|██▋       | 38/141 [00:59<02:39,  1.55s/it]avg_loss = 1.7878138285416822:  28%|██▊       | 39/141 [00:59<02:38,  1.55s/it]avg_loss = 1.7796039760112763:  28%|██▊       | 39/141 [01:01<02:38,  1.55s/it]avg_loss = 1.7796039760112763:  28%|██▊       | 40/141 [01:01<02:36,  1.55s/it]avg_loss = 1.7841366587615595:  28%|██▊       | 40/141 [01:02<02:36,  1.55s/it]avg_loss = 1.7841366587615595:  29%|██▉       | 41/141 [01:02<02:35,  1.55s/it]avg_loss = 1.8015889468647184:  29%|██▉       | 41/141 [01:04<02:35,  1.55s/it]avg_loss = 1.8015889468647184:  30%|██▉       | 42/141 [01:04<02:34,  1.56s/it]avg_loss = 1.8182436948598817:  30%|██▉       | 42/141 [01:05<02:34,  1.56s/it]avg_loss = 1.8182436948598817:  30%|███       | 43/141 [01:05<02:32,  1.56s/it]avg_loss = 1.8208585760810159:  30%|███       | 43/141 [01:07<02:32,  1.56s/it]avg_loss = 1.8208585760810159:  31%|███       | 44/141 [01:07<02:31,  1.56s/it]avg_loss = 1.825156683391995:  31%|███       | 44/141 [01:09<02:31,  1.56s/it] avg_loss = 1.825156683391995:  32%|███▏      | 45/141 [01:09<02:29,  1.56s/it]avg_loss = 1.8308063434517903:  32%|███▏      | 45/141 [01:10<02:29,  1.56s/it]avg_loss = 1.8308063434517903:  33%|███▎      | 46/141 [01:10<02:28,  1.56s/it]avg_loss = 1.8376399557641212:  33%|███▎      | 46/141 [01:12<02:28,  1.56s/it]avg_loss = 1.8376399557641212:  33%|███▎      | 47/141 [01:12<02:26,  1.56s/it]avg_loss = 1.8410873065392177:  33%|███▎      | 47/141 [01:13<02:26,  1.56s/it]avg_loss = 1.8410873065392177:  34%|███▍      | 48/141 [01:13<02:25,  1.56s/it]avg_loss = 1.839739449170171:  34%|███▍      | 48/141 [01:15<02:25,  1.56s/it] avg_loss = 1.839739449170171:  35%|███▍      | 49/141 [01:15<02:23,  1.57s/it]avg_loss = 1.839522898197174:  35%|███▍      | 49/141 [01:16<02:23,  1.57s/it]avg_loss = 1.839522898197174:  35%|███▌      | 50/141 [01:16<02:22,  1.57s/it]avg_loss = 1.8330328300887464:  35%|███▌      | 50/141 [01:18<02:22,  1.57s/it]avg_loss = 1.8330328300887464:  36%|███▌      | 51/141 [01:18<02:20,  1.57s/it]avg_loss = 1.8294183153372545:  36%|███▌      | 51/141 [01:20<02:20,  1.57s/it]avg_loss = 1.8294183153372545:  37%|███▋      | 52/141 [01:20<02:19,  1.57s/it]avg_loss = 1.8230750650729772:  37%|███▋      | 52/141 [01:21<02:19,  1.57s/it]avg_loss = 1.8230750650729772:  38%|███▊      | 53/141 [01:21<02:18,  1.57s/it]avg_loss = 1.8202096201755382:  38%|███▊      | 53/141 [01:23<02:18,  1.57s/it]avg_loss = 1.8202096201755382:  38%|███▊      | 54/141 [01:23<02:16,  1.57s/it]avg_loss = 1.8126142328435724:  38%|███▊      | 54/141 [01:24<02:16,  1.57s/it]avg_loss = 1.8126142328435724:  39%|███▉      | 55/141 [01:24<02:15,  1.57s/it]avg_loss = 1.804994740656444:  39%|███▉      | 55/141 [01:26<02:15,  1.57s/it] avg_loss = 1.804994740656444:  40%|███▉      | 56/141 [01:26<02:13,  1.57s/it]avg_loss = 1.798216930606909:  40%|███▉      | 56/141 [01:27<02:13,  1.57s/it]avg_loss = 1.798216930606909:  40%|████      | 57/141 [01:27<02:12,  1.57s/it]avg_loss = 1.7955156544159199:  40%|████      | 57/141 [01:29<02:12,  1.57s/it]avg_loss = 1.7955156544159199:  41%|████      | 58/141 [01:29<02:10,  1.57s/it]avg_loss = 1.7977617837614932:  41%|████      | 58/141 [01:31<02:10,  1.57s/it]avg_loss = 1.7977617837614932:  42%|████▏     | 59/141 [01:31<02:09,  1.58s/it]avg_loss = 1.8036012172698974:  42%|████▏     | 59/141 [01:32<02:09,  1.58s/it]avg_loss = 1.8036012172698974:  43%|████▎     | 60/141 [01:32<02:07,  1.58s/it]avg_loss = 1.8096655744021055:  43%|████▎     | 60/141 [01:34<02:07,  1.58s/it]avg_loss = 1.8096655744021055:  43%|████▎     | 61/141 [01:34<02:06,  1.58s/it]avg_loss = 1.8170981560983965:  43%|████▎     | 61/141 [01:35<02:06,  1.58s/it]avg_loss = 1.8170981560983965:  44%|████▍     | 62/141 [01:35<02:04,  1.58s/it]avg_loss = 1.8077149864227053:  44%|████▍     | 62/141 [01:37<02:04,  1.58s/it]avg_loss = 1.8077149864227053:  45%|████▍     | 63/141 [01:37<02:03,  1.58s/it]avg_loss = 1.8057153206318617:  45%|████▍     | 63/141 [01:38<02:03,  1.58s/it]avg_loss = 1.8057153206318617:  45%|████▌     | 64/141 [01:38<02:01,  1.58s/it]avg_loss = 1.8030918194697454:  45%|████▌     | 64/141 [01:40<02:01,  1.58s/it]avg_loss = 1.8030918194697454:  46%|████▌     | 65/141 [01:40<02:00,  1.58s/it]avg_loss = 1.7970656897082473:  46%|████▌     | 65/141 [01:42<02:00,  1.58s/it]avg_loss = 1.7970656897082473:  47%|████▋     | 66/141 [01:42<01:58,  1.58s/it]avg_loss = 1.7945207072727716:  47%|████▋     | 66/141 [01:43<01:58,  1.58s/it]avg_loss = 1.7945207072727716:  48%|████▊     | 67/141 [01:43<01:57,  1.58s/it]avg_loss = 1.7911268227240618:  48%|████▊     | 67/141 [01:45<01:57,  1.58s/it]avg_loss = 1.7911268227240618:  48%|████▊     | 68/141 [01:45<01:55,  1.59s/it]avg_loss = 1.7881558753442073:  48%|████▊     | 68/141 [01:46<01:55,  1.59s/it]avg_loss = 1.7881558753442073:  49%|████▉     | 69/141 [01:46<01:54,  1.59s/it]avg_loss = 1.7890514373779296:  49%|████▉     | 69/141 [01:48<01:54,  1.59s/it]avg_loss = 1.7890514373779296:  50%|████▉     | 70/141 [01:48<01:52,  1.59s/it]avg_loss = 1.792803898663588:  50%|████▉     | 70/141 [01:50<01:52,  1.59s/it] avg_loss = 1.792803898663588:  50%|█████     | 71/141 [01:50<01:51,  1.59s/it]avg_loss = 1.7951903740564983:  50%|█████     | 71/141 [01:51<01:51,  1.59s/it]avg_loss = 1.7951903740564983:  51%|█████     | 72/141 [01:51<01:49,  1.59s/it]avg_loss = 1.7938603064785266:  51%|█████     | 72/141 [01:53<01:49,  1.59s/it]avg_loss = 1.7938603064785266:  52%|█████▏    | 73/141 [01:53<01:47,  1.59s/it]avg_loss = 1.795656942032479:  52%|█████▏    | 73/141 [01:54<01:47,  1.59s/it] avg_loss = 1.795656942032479:  52%|█████▏    | 74/141 [01:54<01:46,  1.59s/it]avg_loss = 1.7960413471857706:  52%|█████▏    | 74/141 [01:56<01:46,  1.59s/it]avg_loss = 1.7960413471857706:  53%|█████▎    | 75/141 [01:56<01:44,  1.59s/it]avg_loss = 1.7950698921554966:  53%|█████▎    | 75/141 [01:57<01:44,  1.59s/it]avg_loss = 1.7950698921554966:  54%|█████▍    | 76/141 [01:57<01:43,  1.59s/it]avg_loss = 1.7963426345354552:  54%|█████▍    | 76/141 [01:59<01:43,  1.59s/it]avg_loss = 1.7963426345354552:  55%|█████▍    | 77/141 [01:59<01:41,  1.59s/it]avg_loss = 1.7988880261396751:  55%|█████▍    | 77/141 [02:01<01:41,  1.59s/it]avg_loss = 1.7988880261396751:  55%|█████▌    | 78/141 [02:01<01:39,  1.59s/it]avg_loss = 1.8032252245311495:  55%|█████▌    | 78/141 [02:02<01:39,  1.59s/it]avg_loss = 1.8032252245311495:  56%|█████▌    | 79/141 [02:02<01:38,  1.59s/it]avg_loss = 1.800629623234272:  56%|█████▌    | 79/141 [02:04<01:38,  1.59s/it] avg_loss = 1.800629623234272:  57%|█████▋    | 80/141 [02:04<01:36,  1.59s/it]avg_loss = 1.7997012432710624:  57%|█████▋    | 80/141 [02:05<01:36,  1.59s/it]avg_loss = 1.7997012432710624:  57%|█████▋    | 81/141 [02:05<01:35,  1.59s/it]avg_loss = 1.7991361981484948:  57%|█████▋    | 81/141 [02:07<01:35,  1.59s/it]avg_loss = 1.7991361981484948:  58%|█████▊    | 82/141 [02:07<01:33,  1.59s/it]avg_loss = 1.7974298732826508:  58%|█████▊    | 82/141 [02:09<01:33,  1.59s/it]avg_loss = 1.7974298732826508:  59%|█████▉    | 83/141 [02:09<01:32,  1.59s/it]avg_loss = 1.795388978152048:  59%|█████▉    | 83/141 [02:10<01:32,  1.59s/it] avg_loss = 1.795388978152048:  60%|█████▉    | 84/141 [02:10<01:30,  1.59s/it]avg_loss = 1.793154356058906:  60%|█████▉    | 84/141 [02:12<01:30,  1.59s/it]avg_loss = 1.793154356058906:  60%|██████    | 85/141 [02:12<01:28,  1.59s/it]avg_loss = 1.794940639373868:  60%|██████    | 85/141 [02:13<01:28,  1.59s/it]avg_loss = 1.794940639373868:  61%|██████    | 86/141 [02:13<01:27,  1.59s/it]avg_loss = 1.7969621817270915:  61%|██████    | 86/141 [02:15<01:27,  1.59s/it]avg_loss = 1.7969621817270915:  62%|██████▏   | 87/141 [02:15<01:25,  1.59s/it]avg_loss = 1.797084304419431:  62%|██████▏   | 87/141 [02:17<01:25,  1.59s/it] avg_loss = 1.797084304419431:  62%|██████▏   | 88/141 [02:17<01:24,  1.59s/it]avg_loss = 1.8059240753731032:  62%|██████▏   | 88/141 [02:18<01:24,  1.59s/it]avg_loss = 1.8059240753731032:  63%|██████▎   | 89/141 [02:18<01:22,  1.59s/it]avg_loss = 1.813516206211514:  63%|██████▎   | 89/141 [02:20<01:22,  1.59s/it] avg_loss = 1.813516206211514:  64%|██████▍   | 90/141 [02:20<01:21,  1.59s/it]avg_loss = 1.8167040321853134:  64%|██████▍   | 90/141 [02:21<01:21,  1.59s/it]avg_loss = 1.8167040321853134:  65%|██████▍   | 91/141 [02:21<01:19,  1.59s/it]avg_loss = 1.821704265863999:  65%|██████▍   | 91/141 [02:23<01:19,  1.59s/it] avg_loss = 1.821704265863999:  65%|██████▌   | 92/141 [02:23<01:17,  1.59s/it]avg_loss = 1.8267474661591232:  65%|██████▌   | 92/141 [02:24<01:17,  1.59s/it]avg_loss = 1.8267474661591232:  66%|██████▌   | 93/141 [02:24<01:16,  1.59s/it]avg_loss = 1.8279034355853467:  66%|██████▌   | 93/141 [02:26<01:16,  1.59s/it]avg_loss = 1.8279034355853467:  67%|██████▋   | 94/141 [02:26<01:14,  1.59s/it]avg_loss = 1.831723087712338:  67%|██████▋   | 94/141 [02:28<01:14,  1.59s/it] avg_loss = 1.831723087712338:  67%|██████▋   | 95/141 [02:28<01:13,  1.59s/it]avg_loss = 1.8326146006584167:  67%|██████▋   | 95/141 [02:29<01:13,  1.59s/it]avg_loss = 1.8326146006584167:  68%|██████▊   | 96/141 [02:29<01:11,  1.59s/it]avg_loss = 1.8346355649613844:  68%|██████▊   | 96/141 [02:31<01:11,  1.59s/it]avg_loss = 1.8346355649613844:  69%|██████▉   | 97/141 [02:31<01:10,  1.59s/it]avg_loss = 1.828960154737745:  69%|██████▉   | 97/141 [02:32<01:10,  1.59s/it] avg_loss = 1.828960154737745:  70%|██████▉   | 98/141 [02:32<01:08,  1.59s/it]avg_loss = 1.8295781491982817:  70%|██████▉   | 98/141 [02:34<01:08,  1.59s/it]avg_loss = 1.8295781491982817:  70%|███████   | 99/141 [02:34<01:06,  1.59s/it]avg_loss = 1.8311285805702209:  70%|███████   | 99/141 [02:36<01:06,  1.59s/it]avg_loss = 1.8311285805702209:  71%|███████   | 100/141 [02:36<01:05,  1.60s/it]avg_loss = 1.8294570493226003:  71%|███████   | 100/141 [02:37<01:05,  1.60s/it]avg_loss = 1.8294570493226003:  72%|███████▏  | 101/141 [02:37<01:03,  1.60s/it]avg_loss = 1.8293967901491652:  72%|███████▏  | 101/141 [02:39<01:03,  1.60s/it]avg_loss = 1.8293967901491652:  72%|███████▏  | 102/141 [02:39<01:02,  1.60s/it]avg_loss = 1.8268836123272054:  72%|███████▏  | 102/141 [02:40<01:02,  1.60s/it]avg_loss = 1.8268836123272054:  73%|███████▎  | 103/141 [02:40<01:00,  1.60s/it]avg_loss = 1.828865913244394:  73%|███████▎  | 103/141 [02:42<01:00,  1.60s/it] avg_loss = 1.828865913244394:  74%|███████▍  | 104/141 [02:42<00:59,  1.60s/it]avg_loss = 1.826358077639625:  74%|███████▍  | 104/141 [02:44<00:59,  1.60s/it]avg_loss = 1.826358077639625:  74%|███████▍  | 105/141 [02:44<00:57,  1.60s/it]avg_loss = 1.8248222515268147:  74%|███████▍  | 105/141 [02:45<00:57,  1.60s/it]avg_loss = 1.8248222515268147:  75%|███████▌  | 106/141 [02:45<00:55,  1.60s/it]avg_loss = 1.8223228354320349:  75%|███████▌  | 106/141 [02:47<00:55,  1.60s/it]avg_loss = 1.8223228354320349:  76%|███████▌  | 107/141 [02:47<00:54,  1.60s/it]avg_loss = 1.8197794104063953:  76%|███████▌  | 107/141 [02:48<00:54,  1.60s/it]avg_loss = 1.8197794104063953:  77%|███████▋  | 108/141 [02:48<00:52,  1.60s/it]avg_loss = 1.8169525774247055:  77%|███████▋  | 108/141 [02:50<00:52,  1.60s/it]avg_loss = 1.8169525774247055:  77%|███████▋  | 109/141 [02:50<00:51,  1.60s/it]avg_loss = 1.8144714203747836:  77%|███████▋  | 109/141 [02:52<00:51,  1.60s/it]avg_loss = 1.8144714203747836:  78%|███████▊  | 110/141 [02:52<00:49,  1.60s/it]avg_loss = 1.8168733184402053:  78%|███████▊  | 110/141 [02:53<00:49,  1.60s/it]avg_loss = 1.8168733184402053:  79%|███████▊  | 111/141 [02:53<00:47,  1.60s/it]avg_loss = 1.8166765017168862:  79%|███████▊  | 111/141 [02:55<00:47,  1.60s/it]avg_loss = 1.8166765017168862:  79%|███████▉  | 112/141 [02:55<00:46,  1.60s/it]avg_loss = 1.8179012927333866:  79%|███████▉  | 112/141 [02:56<00:46,  1.60s/it]avg_loss = 1.8179012927333866:  80%|████████  | 113/141 [02:56<00:44,  1.59s/it]avg_loss = 1.8189078423014857:  80%|████████  | 113/141 [02:58<00:44,  1.59s/it]avg_loss = 1.8189078423014857:  81%|████████  | 114/141 [02:58<00:43,  1.60s/it]avg_loss = 1.818337747325068:  81%|████████  | 114/141 [03:00<00:43,  1.60s/it] avg_loss = 1.818337747325068:  82%|████████▏ | 115/141 [03:00<00:41,  1.60s/it]avg_loss = 1.816828624955539:  82%|████████▏ | 115/141 [03:01<00:41,  1.60s/it]avg_loss = 1.816828624955539:  82%|████████▏ | 116/141 [03:01<00:39,  1.60s/it]avg_loss = 1.8189582152244372:  82%|████████▏ | 116/141 [03:03<00:39,  1.60s/it]avg_loss = 1.8189582152244372:  83%|████████▎ | 117/141 [03:03<00:38,  1.60s/it]avg_loss = 1.8187466061721413:  83%|████████▎ | 117/141 [03:04<00:38,  1.60s/it]avg_loss = 1.8187466061721413:  84%|████████▎ | 118/141 [03:04<00:36,  1.60s/it]avg_loss = 1.817476188435274:  84%|████████▎ | 118/141 [03:06<00:36,  1.60s/it] avg_loss = 1.817476188435274:  84%|████████▍ | 119/141 [03:06<00:35,  1.60s/it]avg_loss = 1.8158273875713349:  84%|████████▍ | 119/141 [03:08<00:35,  1.60s/it]avg_loss = 1.8158273875713349:  85%|████████▌ | 120/141 [03:08<00:33,  1.60s/it]avg_loss = 1.8156734852751424:  85%|████████▌ | 120/141 [03:09<00:33,  1.60s/it]avg_loss = 1.8156734852751424:  86%|████████▌ | 121/141 [03:09<00:31,  1.60s/it]avg_loss = 1.815977364289956:  86%|████████▌ | 121/141 [03:11<00:31,  1.60s/it] avg_loss = 1.815977364289956:  87%|████████▋ | 122/141 [03:11<00:30,  1.60s/it]avg_loss = 1.8159202395415888:  87%|████████▋ | 122/141 [03:12<00:30,  1.60s/it]avg_loss = 1.8159202395415888:  87%|████████▋ | 123/141 [03:12<00:28,  1.60s/it]avg_loss = 1.816213212666973:  87%|████████▋ | 123/141 [03:14<00:28,  1.60s/it] avg_loss = 1.816213212666973:  88%|████████▊ | 124/141 [03:14<00:27,  1.60s/it]avg_loss = 1.8151078929901123:  88%|████████▊ | 124/141 [03:16<00:27,  1.60s/it]avg_loss = 1.8151078929901123:  89%|████████▊ | 125/141 [03:16<00:25,  1.60s/it]avg_loss = 1.8155927270177812:  89%|████████▊ | 125/141 [03:17<00:25,  1.60s/it]avg_loss = 1.8155927270177812:  89%|████████▉ | 126/141 [03:17<00:23,  1.60s/it]avg_loss = 1.8154199245407825:  89%|████████▉ | 126/141 [03:19<00:23,  1.60s/it]avg_loss = 1.8154199245407825:  90%|█████████ | 127/141 [03:19<00:22,  1.60s/it]avg_loss = 1.8140525529161096:  90%|█████████ | 127/141 [03:20<00:22,  1.60s/it]avg_loss = 1.8140525529161096:  91%|█████████ | 128/141 [03:20<00:20,  1.60s/it]avg_loss = 1.814319791719895:  91%|█████████ | 128/141 [03:22<00:20,  1.60s/it] avg_loss = 1.814319791719895:  91%|█████████▏| 129/141 [03:22<00:19,  1.60s/it]avg_loss = 1.8150137635377737:  91%|█████████▏| 129/141 [03:24<00:19,  1.60s/it]avg_loss = 1.8150137635377737:  92%|█████████▏| 130/141 [03:24<00:17,  1.60s/it]avg_loss = 1.8160111549246403:  92%|█████████▏| 130/141 [03:25<00:17,  1.60s/it]avg_loss = 1.8160111549246403:  93%|█████████▎| 131/141 [03:25<00:15,  1.60s/it]avg_loss = 1.8166911773609393:  93%|█████████▎| 131/141 [03:27<00:15,  1.60s/it]avg_loss = 1.8166911773609393:  94%|█████████▎| 132/141 [03:27<00:14,  1.60s/it]avg_loss = 1.8139268267423587:  94%|█████████▎| 132/141 [03:28<00:14,  1.60s/it]avg_loss = 1.8139268267423587:  94%|█████████▍| 133/141 [03:28<00:12,  1.60s/it]avg_loss = 1.8096929708523537:  94%|█████████▍| 133/141 [03:30<00:12,  1.60s/it]avg_loss = 1.8096929708523537:  95%|█████████▌| 134/141 [03:30<00:11,  1.59s/it]avg_loss = 1.8122590921543262:  95%|█████████▌| 134/141 [03:32<00:11,  1.59s/it]avg_loss = 1.8122590921543262:  96%|█████████▌| 135/141 [03:32<00:09,  1.59s/it]avg_loss = 1.8157795501105927:  96%|█████████▌| 135/141 [03:33<00:09,  1.59s/it]avg_loss = 1.8157795501105927:  96%|█████████▋| 136/141 [03:33<00:07,  1.59s/it]avg_loss = 1.816606649517143:  96%|█████████▋| 136/141 [03:35<00:07,  1.59s/it] avg_loss = 1.816606649517143:  97%|█████████▋| 137/141 [03:35<00:06,  1.60s/it]avg_loss = 1.8152228343314019:  97%|█████████▋| 137/141 [03:36<00:06,  1.60s/it]avg_loss = 1.8152228343314019:  98%|█████████▊| 138/141 [03:36<00:04,  1.59s/it]avg_loss = 1.8153826607217034:  98%|█████████▊| 138/141 [03:38<00:04,  1.59s/it]avg_loss = 1.8153826607217034:  99%|█████████▊| 139/141 [03:38<00:03,  1.60s/it]avg_loss = 1.8158344541277205:  99%|█████████▊| 139/141 [03:40<00:03,  1.60s/it]avg_loss = 1.8158344541277205:  99%|█████████▉| 140/141 [03:40<00:01,  1.59s/it]avg_loss = 1.8171461727602263:  99%|█████████▉| 140/141 [03:41<00:01,  1.59s/it]avg_loss = 1.8171461727602263: 100%|██████████| 141/141 [03:41<00:00,  1.59s/it]avg_loss = 1.8171461727602263: 100%|██████████| 141/141 [03:41<00:00,  1.57s/it]
I0328 22:45:54.288761 2319325 eval_ppl.py:107] wikitext2 perplexity: 6.154270172119141
wikitext2 perplexity: 6.154
