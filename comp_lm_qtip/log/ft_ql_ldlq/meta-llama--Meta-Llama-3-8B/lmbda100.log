I0328 05:40:03.052562 2210624 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:40:03.052667 2210624 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:40:03.052708 2210624 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:40:03.373714 2210624 config.py:54] PyTorch version 2.6.0 available.
W0328 05:40:03.562035 2210624 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 05:40:04.125166 2210624 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.27it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.72it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.85it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.93it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.61it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.76it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.93it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.81it/s]
I0328 05:40:05.593857 2210624 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:22,  1.40it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:18,  1.66it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.77it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.83it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.86it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.87it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.89it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:12,  1.92it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.91it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:10,  1.91it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.91it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:09,  1.91it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.91it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  1.92it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.88it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.91it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.93it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.94it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.94it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.94it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.95it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.95it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.95it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.96it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.96it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.96it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.96it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.95it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]
I0328 05:40:28.227450 2210624 quantize_finetune_llama.py:185] loaded compression model
I0328 05:40:46.660983 2210624 quantize_finetune_llama.py:189] loaded dataset and devset
I0328 05:40:51.703468 2210624 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 05:41:50.324656 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 58.47732973098755s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0328 05:42:09.502027 2211980 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:42:09.502129 2211980 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:42:09.502167 2211980 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:42:09.823377 2211980 config.py:54] PyTorch version 2.6.0 available.
W0328 05:42:10.011124 2211980 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 05:42:10.558771 2211980 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 05:42:10.562442 2210624 quantize_finetune_llama.py:209] layer 1 gpu 1
I0328 05:42:10.578030 2211980 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 05:42:26.657649 2211980 finetune.py:45] layer 0_v initial loss 1.0249929118799628e-06
W0328 05:42:26.657887 2211980 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 05:43:01.033779 2211980 finetune.py:68] layer 0_v @ epoch 0 new loss 9.172014188152389e-07 old loss 1.0249929118799628e-06 BETTER
I0328 05:43:05.736821 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 54.992305755615234s
I0328 05:43:14.690838 2212675 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:43:14.690947 2212675 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:43:14.690991 2212675 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:43:15.015522 2212675 config.py:54] PyTorch version 2.6.0 available.
W0328 05:43:15.230340 2212675 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 05:43:15.812141 2212675 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 05:43:15.815684 2210624 quantize_finetune_llama.py:209] layer 2 gpu 2
I0328 05:43:15.829740 2212675 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 05:43:32.279784 2212675 finetune.py:45] layer 1_v initial loss 1.0785691301862244e-05
W0328 05:43:32.279982 2212675 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 05:43:36.915659 2211980 finetune.py:68] layer 0_v @ epoch 1 new loss 8.812709211269976e-07 old loss 9.172014188152389e-07 BETTER
I0328 05:44:05.002210 2212675 finetune.py:68] layer 1_v @ epoch 0 new loss 2.605856934678741e-06 old loss 1.0785691301862244e-05 BETTER
I0328 05:44:12.923606 2211980 finetune.py:68] layer 0_v @ epoch 2 new loss 8.624446081739734e-07 old loss 8.812709211269976e-07 BETTER
I0328 05:44:20.196182 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 64.18492412567139s
I0328 05:44:28.978159 2213487 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:44:28.978269 2213487 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:44:28.978313 2213487 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:44:29.339367 2213487 config.py:54] PyTorch version 2.6.0 available.
W0328 05:44:29.539671 2213487 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 05:44:30.163604 2213487 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 05:44:30.167555 2210624 quantize_finetune_llama.py:209] layer 3 gpu 3
I0328 05:44:30.182608 2213487 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 05:44:39.088134 2212675 finetune.py:68] layer 1_v @ epoch 1 new loss 1.611399966350291e-06 old loss 2.605856934678741e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 05:44:46.950557 2213487 finetune.py:45] layer 2_v initial loss 2.0308179955463856e-05
W0328 05:44:46.950871 2213487 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 05:44:49.126003 2211980 finetune.py:68] layer 0_v @ epoch 3 new loss 8.502979085278639e-07 old loss 8.624446081739734e-07 BETTER
I0328 05:45:13.380771 2212675 finetune.py:68] layer 1_v @ epoch 2 new loss 1.3799218550047954e-06 old loss 1.611399966350291e-06 BETTER
I0328 05:45:20.174736 2213487 finetune.py:68] layer 2_v @ epoch 0 new loss 3.737644647117122e-06 old loss 2.0308179955463856e-05 BETTER
I0328 05:45:25.791830 2211980 finetune.py:68] layer 0_v @ epoch 4 new loss 8.414399417233653e-07 old loss 8.502979085278639e-07 BETTER
I0328 05:45:35.063248 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 64.72167944908142s
I0328 05:45:47.421320 2214371 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:45:47.421447 2214371 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:45:47.421502 2214371 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:45:47.617587 2211980 finetune.py:45] layer 0_q initial loss 8.425952842117113e-07
I0328 05:45:47.794366 2214371 config.py:54] PyTorch version 2.6.0 available.
I0328 05:45:47.909127 2212675 finetune.py:68] layer 1_v @ epoch 3 new loss 1.2824347095374833e-06 old loss 1.3799218550047954e-06 BETTER
W0328 05:45:48.003245 2214371 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 05:45:48.611000 2214371 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 05:45:48.614899 2210624 quantize_finetune_llama.py:209] layer 4 gpu 0
I0328 05:45:48.627737 2214371 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 05:45:54.506693 2213487 finetune.py:68] layer 2_v @ epoch 1 new loss 2.1617315724142827e-06 old loss 3.737644647117122e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 05:46:05.444648 2214371 finetune.py:45] layer 3_v initial loss 2.0120676708756946e-05
W0328 05:46:05.444954 2214371 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 05:46:22.429474 2211980 finetune.py:68] layer 0_q @ epoch 0 new loss 8.352720328730356e-07 old loss 8.425952842117113e-07 BETTER
I0328 05:46:22.540202 2212675 finetune.py:68] layer 1_v @ epoch 4 new loss 1.2225589216541266e-06 old loss 1.2824347095374833e-06 BETTER
I0328 05:46:29.037381 2213487 finetune.py:68] layer 2_v @ epoch 2 new loss 1.9030319435842102e-06 old loss 2.1617315724142827e-06 BETTER
I0328 05:46:38.217757 2214371 finetune.py:68] layer 3_v @ epoch 0 new loss 4.976174295734381e-06 old loss 2.0120676708756946e-05 BETTER
I0328 05:46:42.028896 2212675 finetune.py:45] layer 1_q initial loss 1.2422003692336148e-06
I0328 05:46:58.256251 2211980 finetune.py:68] layer 0_q @ epoch 1 new loss 8.291128210657916e-07 old loss 8.352720328730356e-07 BETTER
I0328 05:47:03.889445 2213487 finetune.py:68] layer 2_v @ epoch 3 new loss 1.8164798802899895e-06 old loss 1.9030319435842102e-06 BETTER
I0328 05:47:12.469316 2214371 finetune.py:68] layer 3_v @ epoch 1 new loss 3.4029023936454905e-06 old loss 4.976174295734381e-06 BETTER
I0328 05:47:15.361869 2212675 finetune.py:68] layer 1_q @ epoch 0 new loss 1.1866204658872448e-06 old loss 1.2422003692336148e-06 BETTER
I0328 05:47:34.390576 2211980 finetune.py:68] layer 0_q @ epoch 2 new loss 8.238320106102037e-07 old loss 8.291128210657916e-07 BETTER
I0328 05:47:38.923232 2213487 finetune.py:68] layer 2_v @ epoch 4 new loss 1.7652415635893703e-06 old loss 1.8164798802899895e-06 BETTER
I0328 05:47:46.860396 2214371 finetune.py:68] layer 3_v @ epoch 2 new loss 3.023606041097082e-06 old loss 3.4029023936454905e-06 BETTER
I0328 05:47:49.455959 2212675 finetune.py:68] layer 1_q @ epoch 1 new loss 1.1462753946034354e-06 old loss 1.1866204658872448e-06 BETTER
I0328 05:47:58.189375 2213487 finetune.py:45] layer 2_q initial loss 2.008117917284835e-06
I0328 05:48:10.860576 2211980 finetune.py:68] layer 0_q @ epoch 3 new loss 8.191127562895417e-07 old loss 8.238320106102037e-07 BETTER
I0328 05:48:21.370990 2214371 finetune.py:68] layer 3_v @ epoch 3 new loss 2.858856078091776e-06 old loss 3.023606041097082e-06 BETTER
I0328 05:48:23.805054 2212675 finetune.py:68] layer 1_q @ epoch 2 new loss 1.114532892643183e-06 old loss 1.1462753946034354e-06 BETTER
I0328 05:48:31.883367 2213487 finetune.py:68] layer 2_q @ epoch 0 new loss 1.95061670638097e-06 old loss 2.008117917284835e-06 BETTER
I0328 05:48:47.159849 2211980 finetune.py:68] layer 0_q @ epoch 4 new loss 8.147233074851101e-07 old loss 8.191127562895417e-07 BETTER
I0328 05:48:56.088574 2214371 finetune.py:68] layer 3_v @ epoch 4 new loss 2.7613918973656837e-06 old loss 2.858856078091776e-06 BETTER
I0328 05:48:58.160394 2212675 finetune.py:68] layer 1_q @ epoch 3 new loss 1.088575345420395e-06 old loss 1.114532892643183e-06 BETTER
I0328 05:49:04.819307 2211980 finetune.py:45] layer 0_k initial loss 8.170465548573702e-07
I0328 05:49:06.394091 2213487 finetune.py:68] layer 2_q @ epoch 1 new loss 1.9140184122079518e-06 old loss 1.95061670638097e-06 BETTER
I0328 05:49:15.449325 2214371 finetune.py:45] layer 3_q initial loss 3.251786438340787e-06
I0328 05:49:32.488886 2212675 finetune.py:68] layer 1_q @ epoch 4 new loss 1.0667973810996045e-06 old loss 1.088575345420395e-06 BETTER
I0328 05:49:39.911622 2211980 finetune.py:68] layer 0_k @ epoch 0 new loss 8.131075333039917e-07 old loss 8.170465548573702e-07 BETTER
I0328 05:49:40.968924 2213487 finetune.py:68] layer 2_q @ epoch 2 new loss 1.8856418364521232e-06 old loss 1.9140184122079518e-06 BETTER
I0328 05:49:48.555958 2214371 finetune.py:68] layer 3_q @ epoch 0 new loss 3.1561669402435655e-06 old loss 3.251786438340787e-06 BETTER
I0328 05:49:50.072824 2212675 finetune.py:45] layer 1_k initial loss 1.0866119737329427e-06
I0328 05:50:15.548954 2213487 finetune.py:68] layer 2_q @ epoch 3 new loss 1.8631722014106344e-06 old loss 1.8856418364521232e-06 BETTER
I0328 05:50:16.018071 2211980 finetune.py:68] layer 0_k @ epoch 1 new loss 8.094692702798056e-07 old loss 8.131075333039917e-07 BETTER
I0328 05:50:22.873363 2214371 finetune.py:68] layer 3_q @ epoch 1 new loss 3.095696911259438e-06 old loss 3.1561669402435655e-06 BETTER
I0328 05:50:23.288551 2212675 finetune.py:68] layer 1_k @ epoch 0 new loss 1.0653758408807334e-06 old loss 1.0866119737329427e-06 BETTER
I0328 05:50:50.359135 2213487 finetune.py:68] layer 2_q @ epoch 4 new loss 1.8439052382746013e-06 old loss 1.8631722014106344e-06 BETTER
I0328 05:50:52.611735 2211980 finetune.py:68] layer 0_k @ epoch 2 new loss 8.061643370638194e-07 old loss 8.094692702798056e-07 BETTER
I0328 05:50:56.967550 2214371 finetune.py:68] layer 3_q @ epoch 2 new loss 3.0518376661348157e-06 old loss 3.095696911259438e-06 BETTER
I0328 05:50:57.308136 2212675 finetune.py:68] layer 1_k @ epoch 1 new loss 1.0487232202649466e-06 old loss 1.0653758408807334e-06 BETTER
I0328 05:51:08.095491 2213487 finetune.py:45] layer 2_k initial loss 1.9344956854183692e-06
I0328 05:51:29.375082 2211980 finetune.py:68] layer 0_k @ epoch 3 new loss 8.0296484838982e-07 old loss 8.061643370638194e-07 BETTER
I0328 05:51:31.231401 2214371 finetune.py:68] layer 3_q @ epoch 3 new loss 3.0148792120598955e-06 old loss 3.0518376661348157e-06 BETTER
I0328 05:51:31.517046 2212675 finetune.py:68] layer 1_k @ epoch 2 new loss 1.034698811963608e-06 old loss 1.0487232202649466e-06 BETTER
I0328 05:51:41.599116 2213487 finetune.py:68] layer 2_k @ epoch 0 new loss 1.913293090183288e-06 old loss 1.9344956854183692e-06 BETTER
I0328 05:52:05.591252 2214371 finetune.py:68] layer 3_q @ epoch 4 new loss 2.984397042382625e-06 old loss 3.0148792120598955e-06 BETTER
I0328 05:52:05.856450 2212675 finetune.py:68] layer 1_k @ epoch 3 new loss 1.0220455806120299e-06 old loss 1.034698811963608e-06 BETTER
I0328 05:52:06.218411 2211980 finetune.py:68] layer 0_k @ epoch 4 new loss 7.999936428859655e-07 old loss 8.0296484838982e-07 BETTER
I0328 05:52:16.129160 2213487 finetune.py:68] layer 2_k @ epoch 1 new loss 1.898785285447957e-06 old loss 1.913293090183288e-06 BETTER
I0328 05:52:22.993123 2214371 finetune.py:45] layer 3_k initial loss 3.1881497761787614e-06
I0328 05:52:25.546552 2211980 finetune.py:45] layer 0_o initial loss 1.3139301699993666e-06
I0328 05:52:40.402635 2212675 finetune.py:68] layer 1_k @ epoch 4 new loss 1.0107518164659268e-06 old loss 1.0220455806120299e-06 BETTER
I0328 05:52:50.652936 2213487 finetune.py:68] layer 2_k @ epoch 2 new loss 1.886117274807475e-06 old loss 1.898785285447957e-06 BETTER
I0328 05:52:55.966798 2214371 finetune.py:68] layer 3_k @ epoch 0 new loss 3.1574807053402765e-06 old loss 3.1881497761787614e-06 BETTER
I0328 05:52:59.789498 2212675 finetune.py:45] layer 1_o initial loss 2.968906301248353e-06
I0328 05:53:00.393065 2211980 finetune.py:68] layer 0_o @ epoch 0 new loss 1.3078823712930898e-06 old loss 1.3139301699993666e-06 BETTER
I0328 05:53:25.274411 2213487 finetune.py:68] layer 2_k @ epoch 3 new loss 1.8749749415292172e-06 old loss 1.886117274807475e-06 BETTER
I0328 05:53:29.929594 2214371 finetune.py:68] layer 3_k @ epoch 1 new loss 3.1346562536782585e-06 old loss 3.1574807053402765e-06 BETTER
I0328 05:53:32.673087 2212675 finetune.py:68] layer 1_o @ epoch 0 new loss 2.8348308660497423e-06 old loss 2.968906301248353e-06 BETTER
I0328 05:53:35.971898 2211980 finetune.py:68] layer 0_o @ epoch 1 new loss 1.3031645949013182e-06 old loss 1.3078823712930898e-06 BETTER
I0328 05:53:59.952892 2213487 finetune.py:68] layer 2_k @ epoch 4 new loss 1.8645022237251396e-06 old loss 1.8749749415292172e-06 BETTER
I0328 05:54:04.187285 2214371 finetune.py:68] layer 3_k @ epoch 2 new loss 3.1145932553044986e-06 old loss 3.1346562536782585e-06 BETTER
I0328 05:54:06.318631 2212675 finetune.py:68] layer 1_o @ epoch 1 new loss 2.736483338594553e-06 old loss 2.8348308660497423e-06 BETTER
I0328 05:54:11.599060 2211980 finetune.py:68] layer 0_o @ epoch 2 new loss 1.299196810577996e-06 old loss 1.3031645949013182e-06 BETTER
I0328 05:54:19.470574 2213487 finetune.py:45] layer 2_o initial loss 4.668306701205438e-06
I0328 05:54:38.342983 2214371 finetune.py:68] layer 3_k @ epoch 3 new loss 3.096766022281372e-06 old loss 3.1145932553044986e-06 BETTER
I0328 05:54:40.063678 2212675 finetune.py:68] layer 1_o @ epoch 2 new loss 2.6609484393702587e-06 old loss 2.736483338594553e-06 BETTER
I0328 05:54:47.217344 2211980 finetune.py:68] layer 0_o @ epoch 3 new loss 1.295769720854878e-06 old loss 1.299196810577996e-06 BETTER
I0328 05:54:52.425190 2213487 finetune.py:68] layer 2_o @ epoch 0 new loss 4.408797394717112e-06 old loss 4.668306701205438e-06 BETTER
I0328 05:55:12.487522 2214371 finetune.py:68] layer 3_k @ epoch 4 new loss 3.0801950288150692e-06 old loss 3.096766022281372e-06 BETTER
I0328 05:55:13.775122 2212675 finetune.py:68] layer 1_o @ epoch 3 new loss 2.6040111151814926e-06 old loss 2.6609484393702587e-06 BETTER
I0328 05:55:22.917022 2211980 finetune.py:68] layer 0_o @ epoch 4 new loss 1.2927874877277645e-06 old loss 1.295769720854878e-06 BETTER
I0328 05:55:26.294886 2213487 finetune.py:68] layer 2_o @ epoch 1 new loss 4.256635293131694e-06 old loss 4.408797394717112e-06 BETTER
I0328 05:55:31.553191 2214371 finetune.py:45] layer 3_o initial loss 7.3109858931275085e-06
I0328 05:55:47.477243 2212675 finetune.py:68] layer 1_o @ epoch 4 new loss 2.556069830461638e-06 old loss 2.6040111151814926e-06 BETTER
I0328 05:55:53.698476 2211980 finetune.py:45] layer 0_up initial loss 1.781067339834408e-06
I0328 05:56:00.240720 2213487 finetune.py:68] layer 2_o @ epoch 2 new loss 4.161138804192888e-06 old loss 4.256635293131694e-06 BETTER
I0328 05:56:04.109103 2214371 finetune.py:68] layer 3_o @ epoch 0 new loss 6.968883099034429e-06 old loss 7.3109858931275085e-06 BETTER
I0328 05:56:18.409993 2212675 finetune.py:45] layer 1_up initial loss 3.683670911414083e-06
I0328 05:56:25.549450 2211980 finetune.py:68] layer 0_up @ epoch 0 new loss 1.7749739527062047e-06 old loss 1.781067339834408e-06 BETTER
I0328 05:56:34.290288 2213487 finetune.py:68] layer 2_o @ epoch 3 new loss 4.097292730875779e-06 old loss 4.161138804192888e-06 BETTER
I0328 05:56:37.408588 2214371 finetune.py:68] layer 3_o @ epoch 1 new loss 6.8490885496430565e-06 old loss 6.968883099034429e-06 BETTER
I0328 05:56:48.854922 2212675 finetune.py:68] layer 1_up @ epoch 0 new loss 3.64845504918776e-06 old loss 3.683670911414083e-06 BETTER
I0328 05:56:58.625988 2211980 finetune.py:68] layer 0_up @ epoch 1 new loss 1.7706238395476248e-06 old loss 1.7749739527062047e-06 BETTER
I0328 05:57:08.313864 2213487 finetune.py:68] layer 2_o @ epoch 4 new loss 4.050581082992721e-06 old loss 4.097292730875779e-06 BETTER
I0328 05:57:10.751973 2214371 finetune.py:68] layer 3_o @ epoch 2 new loss 6.782169748476008e-06 old loss 6.8490885496430565e-06 BETTER
I0328 05:57:20.416113 2212675 finetune.py:68] layer 1_up @ epoch 1 new loss 3.6214003102941206e-06 old loss 3.64845504918776e-06 BETTER
I0328 05:57:31.784911 2211980 finetune.py:68] layer 0_up @ epoch 2 new loss 1.7672261947154766e-06 old loss 1.7706238395476248e-06 BETTER
I0328 05:57:39.265621 2213487 finetune.py:45] layer 2_up initial loss 6.794268756493693e-06
I0328 05:57:43.956606 2214371 finetune.py:68] layer 3_o @ epoch 3 new loss 6.732394012942677e-06 old loss 6.782169748476008e-06 BETTER
I0328 05:57:52.086046 2212675 finetune.py:68] layer 1_up @ epoch 2 new loss 3.5991790809930535e-06 old loss 3.6214003102941206e-06 BETTER
I0328 05:58:05.177267 2211980 finetune.py:68] layer 0_up @ epoch 3 new loss 1.7644240415393142e-06 old loss 1.7672261947154766e-06 BETTER
I0328 05:58:09.943551 2213487 finetune.py:68] layer 2_up @ epoch 0 new loss 6.752983608748764e-06 old loss 6.794268756493693e-06 BETTER
I0328 05:58:17.308407 2214371 finetune.py:68] layer 3_o @ epoch 4 new loss 6.691582711937372e-06 old loss 6.732394012942677e-06 BETTER
I0328 05:58:23.743301 2212675 finetune.py:68] layer 1_up @ epoch 3 new loss 3.5812886380881537e-06 old loss 3.5991790809930535e-06 BETTER
I0328 05:58:38.415971 2211980 finetune.py:68] layer 0_up @ epoch 4 new loss 1.7619909158383962e-06 old loss 1.7644240415393142e-06 BETTER
I0328 05:58:41.556802 2213487 finetune.py:68] layer 2_up @ epoch 1 new loss 6.722466423525475e-06 old loss 6.752983608748764e-06 BETTER
I0328 05:58:48.073216 2214371 finetune.py:45] layer 3_up initial loss 1.2872724255430512e-05
I0328 05:58:55.489522 2212675 finetune.py:68] layer 1_up @ epoch 4 new loss 3.5656992167787394e-06 old loss 3.5812886380881537e-06 BETTER
I0328 05:59:09.060569 2211980 finetune.py:45] layer 0_gate initial loss 2.090557472911314e-06
I0328 05:59:13.666642 2213487 finetune.py:68] layer 2_up @ epoch 2 new loss 6.698046036035521e-06 old loss 6.722466423525475e-06 BETTER
I0328 05:59:18.185834 2214371 finetune.py:68] layer 3_up @ epoch 0 new loss 1.2816158232453745e-05 old loss 1.2872724255430512e-05 BETTER
I0328 05:59:26.471590 2212675 finetune.py:45] layer 1_gate initial loss 4.346593868831405e-06
I0328 05:59:38.914235 2211980 finetune.py:68] layer 0_gate @ epoch 0 new loss 2.0837605916312896e-06 old loss 2.090557472911314e-06 BETTER
I0328 05:59:45.901700 2213487 finetune.py:68] layer 2_up @ epoch 3 new loss 6.677455985482084e-06 old loss 6.698046036035521e-06 BETTER
I0328 05:59:49.219273 2214371 finetune.py:68] layer 3_up @ epoch 1 new loss 1.277355204365449e-05 old loss 1.2816158232453745e-05 BETTER
I0328 05:59:54.926461 2212675 finetune.py:68] layer 1_gate @ epoch 0 new loss 4.328161594457924e-06 old loss 4.346593868831405e-06 BETTER
I0328 06:00:09.848251 2211980 finetune.py:68] layer 0_gate @ epoch 1 new loss 2.078750412692898e-06 old loss 2.0837605916312896e-06 BETTER
I0328 06:00:18.145310 2213487 finetune.py:68] layer 2_up @ epoch 4 new loss 6.659415248577716e-06 old loss 6.677455985482084e-06 BETTER
I0328 06:00:20.389275 2214371 finetune.py:68] layer 3_up @ epoch 2 new loss 1.2736889402731322e-05 old loss 1.277355204365449e-05 BETTER
I0328 06:00:24.188056 2212675 finetune.py:68] layer 1_gate @ epoch 1 new loss 4.318104402045719e-06 old loss 4.328161594457924e-06 BETTER
I0328 06:00:40.949945 2211980 finetune.py:68] layer 0_gate @ epoch 2 new loss 2.074894382531056e-06 old loss 2.078750412692898e-06 BETTER
I0328 06:00:49.129987 2213487 finetune.py:45] layer 2_gate initial loss 8.42307235870976e-06
I0328 06:00:51.664207 2214371 finetune.py:68] layer 3_up @ epoch 3 new loss 1.270349275728222e-05 old loss 1.2736889402731322e-05 BETTER
I0328 06:00:53.563837 2212675 finetune.py:68] layer 1_gate @ epoch 2 new loss 4.30989166488871e-06 old loss 4.318104402045719e-06 BETTER
I0328 06:01:12.222309 2211980 finetune.py:68] layer 0_gate @ epoch 3 new loss 2.0718264295283007e-06 old loss 2.074894382531056e-06 BETTER
I0328 06:01:17.768319 2213487 finetune.py:68] layer 2_gate @ epoch 0 new loss 8.40383563627256e-06 old loss 8.42307235870976e-06 BETTER
I0328 06:01:23.071786 2214371 finetune.py:68] layer 3_up @ epoch 4 new loss 1.2672662705881521e-05 old loss 1.270349275728222e-05 BETTER
I0328 06:01:23.098338 2212675 finetune.py:68] layer 1_gate @ epoch 3 new loss 4.303301921027014e-06 old loss 4.30989166488871e-06 BETTER
I0328 06:01:43.519493 2211980 finetune.py:68] layer 0_gate @ epoch 4 new loss 2.0692623365903273e-06 old loss 2.0718264295283007e-06 BETTER
I0328 06:01:47.355275 2213487 finetune.py:68] layer 2_gate @ epoch 1 new loss 8.387801244680304e-06 old loss 8.40383563627256e-06 BETTER
I0328 06:01:52.536173 2212675 finetune.py:68] layer 1_gate @ epoch 4 new loss 4.296988208807306e-06 old loss 4.303301921027014e-06 BETTER
I0328 06:01:53.867483 2214371 finetune.py:45] layer 3_gate initial loss 1.5673635061830282e-05
I0328 06:02:17.148308 2213487 finetune.py:68] layer 2_gate @ epoch 2 new loss 8.373817763640545e-06 old loss 8.387801244680304e-06 BETTER
I0328 06:02:22.091467 2214371 finetune.py:68] layer 3_gate @ epoch 0 new loss 1.5634976080036722e-05 old loss 1.5673635061830282e-05 BETTER
I0328 06:02:38.870739 2211980 finetune.py:45] layer 0_down initial loss 3.1936008326738374e-06
I0328 06:02:46.848221 2213487 finetune.py:68] layer 2_gate @ epoch 3 new loss 8.361506843357347e-06 old loss 8.373817763640545e-06 BETTER
I0328 06:02:48.649677 2212675 finetune.py:45] layer 1_down initial loss 6.29646137895179e-06
I0328 06:02:51.606221 2214371 finetune.py:68] layer 3_gate @ epoch 1 new loss 1.5604300642735325e-05 old loss 1.5634976080036722e-05 BETTER
I0328 06:03:06.199263 2211980 finetune.py:68] layer 0_down @ epoch 0 new loss 3.1924173526931554e-06 old loss 3.1936008326738374e-06 BETTER
I0328 06:03:14.743538 2212675 finetune.py:68] layer 1_down @ epoch 0 new loss 6.294674676610157e-06 old loss 6.29646137895179e-06 BETTER
I0328 06:03:16.868405 2213487 finetune.py:68] layer 2_gate @ epoch 4 new loss 8.350148164026905e-06 old loss 8.361506843357347e-06 BETTER
I0328 06:03:21.098186 2214371 finetune.py:68] layer 3_gate @ epoch 2 new loss 1.557676660013385e-05 old loss 1.5604300642735325e-05 BETTER
I0328 06:03:34.956763 2211980 finetune.py:68] layer 0_down @ epoch 1 new loss 3.1914335067995125e-06 old loss 3.1924173526931554e-06 BETTER
I0328 06:03:41.908926 2212675 finetune.py:68] layer 1_down @ epoch 1 new loss 6.293118531175423e-06 old loss 6.294674676610157e-06 BETTER
I0328 06:03:50.347262 2214371 finetune.py:68] layer 3_gate @ epoch 3 new loss 1.5551831893390045e-05 old loss 1.557676660013385e-05 BETTER
I0328 06:04:03.598182 2211980 finetune.py:68] layer 0_down @ epoch 2 new loss 3.1906383810564876e-06 old loss 3.1914335067995125e-06 BETTER
I0328 06:04:09.256409 2212675 finetune.py:68] layer 1_down @ epoch 2 new loss 6.2918325056671165e-06 old loss 6.293118531175423e-06 BETTER
I0328 06:04:13.861097 2213487 finetune.py:45] layer 2_down initial loss 1.2754452654917259e-05
I0328 06:04:19.663916 2214371 finetune.py:68] layer 3_gate @ epoch 4 new loss 1.5528428775724024e-05 old loss 1.5551831893390045e-05 BETTER
I0328 06:04:32.557517 2211980 finetune.py:68] layer 0_down @ epoch 3 new loss 3.1899508030619472e-06 old loss 3.1906383810564876e-06 BETTER
I0328 06:04:36.963192 2212675 finetune.py:68] layer 1_down @ epoch 3 new loss 6.290631972660776e-06 old loss 6.2918325056671165e-06 BETTER
I0328 06:04:40.123232 2213487 finetune.py:68] layer 2_down @ epoch 0 new loss 1.2752389011438936e-05 old loss 1.2754452654917259e-05 BETTER
I0328 06:05:01.559463 2211980 finetune.py:68] layer 0_down @ epoch 4 new loss 3.189317112628487e-06 old loss 3.1899508030619472e-06 BETTER
0_v proxy err 0.04042349010705948 tr(WHW.T) 60.88684844970703
bpp_loss 2.591415971692186
0_q proxy err 4.371475370135158e-05 tr(WHW.T) 288047.71875
bpp_loss 3.3827084942604415
0_k proxy err 3.5624561860458925e-05 tr(WHW.T) 100125.921875
bpp_loss 3.897472418320831
0_o proxy err 0.005994276609271765 tr(WHW.T) 3119.123046875
bpp_loss 2.6792344384011813
0_up proxy err 0.012250781059265137 tr(WHW.T) 8924.48046875
bpp_loss 2.9648966256396045
0_gate proxy err 0.007066261954605579 tr(WHW.T) 15779.025390625
bpp_loss 3.071537255109953
0_down proxy err 0.009654042311012745 tr(WHW.T) 10837.9560546875
bpp_loss 2.9572939073799978
I0328 06:05:04.686295 2212675 finetune.py:68] layer 1_down @ epoch 4 new loss 6.289580142038176e-06 old loss 6.290631972660776e-06 BETTER
1_v proxy err 0.017670106142759323 tr(WHW.T) 109.07096099853516
bpp_loss 2.6969408033182845
1_q proxy err 6.196379399625584e-05 tr(WHW.T) 144770.890625
bpp_loss 3.6182657125173137
1_k proxy err 3.233257666579448e-05 tr(WHW.T) 75440.7578125
bpp_loss 4.246224456117488
1_o proxy err 0.011260919272899628 tr(WHW.T) 1997.4031982421875
bpp_loss 2.760110742994584
1_up proxy err 0.014071897603571415 tr(WHW.T) 8227.21875
bpp_loss 2.978919525298157
1_gate proxy err 0.008480646647512913 tr(WHW.T) 13938.7470703125
bpp_loss 3.0821899156352237
1_down proxy err 0.0002580746659077704 tr(WHW.T) 13981.8291015625
bpp_loss 2.969499538635968
I0328 06:05:07.641153 2213487 finetune.py:68] layer 2_down @ epoch 1 new loss 1.2750829228025395e-05 old loss 1.2752389011438936e-05 BETTER
I0328 06:05:16.674066 2214371 finetune.py:45] layer 3_down initial loss 2.4442675567115657e-05
I0328 06:05:35.921509 2213487 finetune.py:68] layer 2_down @ epoch 2 new loss 1.2749644156428985e-05 old loss 1.2750829228025395e-05 BETTER
I0328 06:05:42.627486 2214371 finetune.py:68] layer 3_down @ epoch 0 new loss 2.4441182176815346e-05 old loss 2.4442675567115657e-05 BETTER
I0328 06:06:03.746016 2213487 finetune.py:68] layer 2_down @ epoch 3 new loss 1.2748645531246439e-05 old loss 1.2749644156428985e-05 BETTER
I0328 06:06:09.622963 2214371 finetune.py:68] layer 3_down @ epoch 1 new loss 2.4440458219032735e-05 old loss 2.4441182176815346e-05 BETTER
I0328 06:06:18.907038 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 67.5778546333313s
I0328 06:06:22.748611 2226924 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:06:22.748715 2226924 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:06:22.748757 2226924 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:06:23.108933 2226924 config.py:54] PyTorch version 2.6.0 available.
W0328 06:06:23.318695 2226924 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:06:23.919915 2226924 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:06:23.923640 2210624 quantize_finetune_llama.py:209] layer 5 gpu 1
I0328 06:06:23.937138 2226924 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 06:06:31.552397 2213487 finetune.py:68] layer 2_down @ epoch 4 new loss 1.2747775144816842e-05 old loss 1.2748645531246439e-05 BETTER
2_v proxy err 0.02686077542603016 tr(WHW.T) 155.95950317382812
bpp_loss 2.603851256542839
2_q proxy err 0.0004764671321026981 tr(WHW.T) 41476.58203125
bpp_loss 3.5693469399702735
2_k proxy err 0.00024030219356063753 tr(WHW.T) 22605.78125
bpp_loss 4.352544658991974
2_o proxy err 0.01098149549216032 tr(WHW.T) 1970.2586669921875
bpp_loss 2.7147797359502874
2_up proxy err 0.01661095581948757 tr(WHW.T) 7599.88330078125
bpp_loss 2.969627234752157
2_gate proxy err 0.008601631969213486 tr(WHW.T) 15105.779296875
bpp_loss 3.1118250196533546
2_down proxy err 0.01551565807312727 tr(WHW.T) 7755.87646484375
bpp_loss 2.973240634260167
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:06:36.756440 2214371 finetune.py:68] layer 3_down @ epoch 2 new loss 2.4439514163532294e-05 old loss 2.4440458219032735e-05 BETTER
I0328 06:06:40.972317 2226924 finetune.py:45] layer 4_v initial loss 1.8232707589049824e-05
W0328 06:06:40.972506 2226924 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:07:04.088600 2214371 finetune.py:68] layer 3_down @ epoch 3 new loss 2.443884477543179e-05 old loss 2.4439514163532294e-05 BETTER
I0328 06:07:15.916766 2226924 finetune.py:68] layer 4_v @ epoch 0 new loss 5.469396000989946e-06 old loss 1.8232707589049824e-05 BETTER
I0328 06:07:31.400847 2214371 finetune.py:68] layer 3_down @ epoch 4 new loss 2.4438222681055777e-05 old loss 2.443884477543179e-05 BETTER
3_v proxy err 0.020749827846884727 tr(WHW.T) 289.3331604003906
bpp_loss 2.7006069412454963
3_q proxy err 0.0005967834731563926 tr(WHW.T) 47554.46875
bpp_loss 3.608891278214287
3_k proxy err 0.00030207366216927767 tr(WHW.T) 26158.796875
bpp_loss 4.425875179760624
3_o proxy err 0.013188548386096954 tr(WHW.T) 1854.198974609375
bpp_loss 2.812196791637689
3_up proxy err 0.01644754782319069 tr(WHW.T) 7537.998046875
bpp_loss 2.9523535517177413
3_gate proxy err 0.006223031785339117 tr(WHW.T) 20886.501953125
bpp_loss 3.1828396981582046
3_down proxy err 0.017812374979257584 tr(WHW.T) 7033.900390625
bpp_loss 2.9510494396256814
I0328 06:07:41.264923 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 63.62132024765015s
I0328 06:07:44.901367 2227833 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:07:44.901463 2227833 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:07:44.901502 2227833 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:07:45.252547 2227833 config.py:54] PyTorch version 2.6.0 available.
W0328 06:07:45.456767 2227833 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:07:46.032136 2227833 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:07:46.036136 2210624 quantize_finetune_llama.py:209] layer 6 gpu 2
I0328 06:07:46.050040 2227833 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 06:07:52.066398 2226924 finetune.py:68] layer 4_v @ epoch 1 new loss 4.333897777542006e-06 old loss 5.469396000989946e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:08:02.531775 2227833 finetune.py:45] layer 5_v initial loss 1.899282869999297e-05
W0328 06:08:02.532010 2227833 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:08:28.737747 2226924 finetune.py:68] layer 4_v @ epoch 2 new loss 3.994788585259812e-06 old loss 4.333897777542006e-06 BETTER
I0328 06:08:35.622198 2227833 finetune.py:68] layer 5_v @ epoch 0 new loss 7.152068064897321e-06 old loss 1.899282869999297e-05 BETTER
I0328 06:08:49.896411 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 63.3812575340271s
I0328 06:08:53.605628 2228587 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:08:53.605732 2228587 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:08:53.605772 2228587 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:08:53.970707 2228587 config.py:54] PyTorch version 2.6.0 available.
W0328 06:08:54.175438 2228587 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:08:54.756273 2228587 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:08:54.760086 2210624 quantize_finetune_llama.py:209] layer 7 gpu 3
I0328 06:08:54.773386 2228587 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 06:09:05.576053 2226924 finetune.py:68] layer 4_v @ epoch 3 new loss 3.828551143669756e-06 old loss 3.994788585259812e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:09:09.961584 2227833 finetune.py:68] layer 5_v @ epoch 1 new loss 6.2058211369731e-06 old loss 7.152068064897321e-06 BETTER
I0328 06:09:11.815805 2228587 finetune.py:45] layer 6_v initial loss 1.618619353394024e-05
W0328 06:09:11.816138 2228587 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:09:42.523943 2226924 finetune.py:68] layer 4_v @ epoch 4 new loss 3.725085662154015e-06 old loss 3.828551143669756e-06 BETTER
I0328 06:09:44.557146 2227833 finetune.py:68] layer 5_v @ epoch 2 new loss 5.871400389878545e-06 old loss 6.2058211369731e-06 BETTER
I0328 06:09:45.365359 2228587 finetune.py:68] layer 6_v @ epoch 0 new loss 8.292523489217274e-06 old loss 1.618619353394024e-05 BETTER
I0328 06:09:58.591810 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 63.33994650840759s
I0328 06:10:01.851742 2226924 finetune.py:45] layer 4_q initial loss 4.498794169194298e-06
I0328 06:10:02.554150 2229326 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:10:02.554259 2229326 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:10:02.554299 2229326 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:10:02.905344 2229326 config.py:54] PyTorch version 2.6.0 available.
W0328 06:10:03.119119 2229326 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:10:03.748618 2229326 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:10:03.752447 2210624 quantize_finetune_llama.py:209] layer 8 gpu 0
I0328 06:10:03.765664 2229326 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:10:19.254634 2227833 finetune.py:68] layer 5_v @ epoch 3 new loss 5.692823833669536e-06 old loss 5.871400389878545e-06 BETTER
I0328 06:10:20.058563 2228587 finetune.py:68] layer 6_v @ epoch 1 new loss 7.580292731290683e-06 old loss 8.292523489217274e-06 BETTER
I0328 06:10:20.968185 2229326 finetune.py:45] layer 7_v initial loss 1.623533171368763e-05
W0328 06:10:20.968428 2229326 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:10:36.879684 2226924 finetune.py:68] layer 4_q @ epoch 0 new loss 4.35642505181022e-06 old loss 4.498794169194298e-06 BETTER
I0328 06:10:54.294487 2229326 finetune.py:68] layer 7_v @ epoch 0 new loss 1.0017170097853523e-05 old loss 1.623533171368763e-05 BETTER
I0328 06:10:54.728962 2227833 finetune.py:68] layer 5_v @ epoch 4 new loss 5.5769955906725954e-06 old loss 5.692823833669536e-06 BETTER
I0328 06:10:55.206892 2228587 finetune.py:68] layer 6_v @ epoch 2 new loss 7.288540473382454e-06 old loss 7.580292731290683e-06 BETTER
I0328 06:11:13.158014 2226924 finetune.py:68] layer 4_q @ epoch 1 new loss 4.275399987818673e-06 old loss 4.35642505181022e-06 BETTER
I0328 06:11:13.854950 2227833 finetune.py:45] layer 5_q initial loss 6.764870704500936e-06
I0328 06:11:28.597121 2229326 finetune.py:68] layer 7_v @ epoch 1 new loss 9.40824429562781e-06 old loss 1.0017170097853523e-05 BETTER
I0328 06:11:30.219676 2228587 finetune.py:68] layer 6_v @ epoch 3 new loss 7.1159106482809875e-06 old loss 7.288540473382454e-06 BETTER
I0328 06:11:47.251644 2227833 finetune.py:68] layer 5_q @ epoch 0 new loss 6.601353106816532e-06 old loss 6.764870704500936e-06 BETTER
I0328 06:11:49.610292 2226924 finetune.py:68] layer 4_q @ epoch 2 new loss 4.213155534671387e-06 old loss 4.275399987818673e-06 BETTER
I0328 06:12:03.075849 2229326 finetune.py:68] layer 7_v @ epoch 2 new loss 9.118406524066813e-06 old loss 9.40824429562781e-06 BETTER
I0328 06:12:05.493156 2228587 finetune.py:68] layer 6_v @ epoch 4 new loss 6.9928764787619e-06 old loss 7.1159106482809875e-06 BETTER
I0328 06:12:21.687077 2227833 finetune.py:68] layer 5_q @ epoch 1 new loss 6.5010781327146105e-06 old loss 6.601353106816532e-06 BETTER
I0328 06:12:24.932326 2228587 finetune.py:45] layer 6_q initial loss 8.433391485596076e-06
I0328 06:12:26.076392 2226924 finetune.py:68] layer 4_q @ epoch 3 new loss 4.161787728662603e-06 old loss 4.213155534671387e-06 BETTER
I0328 06:12:37.641303 2229326 finetune.py:68] layer 7_v @ epoch 3 new loss 8.93005562829785e-06 old loss 9.118406524066813e-06 BETTER
I0328 06:12:56.209058 2227833 finetune.py:68] layer 5_q @ epoch 2 new loss 6.422284968721215e-06 old loss 6.5010781327146105e-06 BETTER
I0328 06:12:58.501809 2228587 finetune.py:68] layer 6_q @ epoch 0 new loss 8.262678420578595e-06 old loss 8.433391485596076e-06 BETTER
I0328 06:13:02.765820 2226924 finetune.py:68] layer 4_q @ epoch 4 new loss 4.118273864150979e-06 old loss 4.161787728662603e-06 BETTER
I0328 06:13:12.254945 2229326 finetune.py:68] layer 7_v @ epoch 4 new loss 8.787456863501575e-06 old loss 8.93005562829785e-06 BETTER
I0328 06:13:20.307868 2226924 finetune.py:45] layer 4_k initial loss 4.386675300338538e-06
I0328 06:13:30.796427 2227833 finetune.py:68] layer 5_q @ epoch 3 new loss 6.3571510509063955e-06 old loss 6.422284968721215e-06 BETTER
I0328 06:13:31.641143 2229326 finetune.py:45] layer 7_q initial loss 1.0904695045610424e-05
I0328 06:13:33.275846 2228587 finetune.py:68] layer 6_q @ epoch 1 new loss 8.141839316522237e-06 old loss 8.262678420578595e-06 BETTER
I0328 06:13:55.520989 2226924 finetune.py:68] layer 4_k @ epoch 0 new loss 4.338356575317448e-06 old loss 4.386675300338538e-06 BETTER
I0328 06:14:04.754800 2229326 finetune.py:68] layer 7_q @ epoch 0 new loss 1.069029349309858e-05 old loss 1.0904695045610424e-05 BETTER
I0328 06:14:05.505817 2227833 finetune.py:68] layer 5_q @ epoch 4 new loss 6.301193934632465e-06 old loss 6.3571510509063955e-06 BETTER
I0328 06:14:07.956167 2228587 finetune.py:68] layer 6_q @ epoch 2 new loss 8.047554729273543e-06 old loss 8.141839316522237e-06 BETTER
I0328 06:14:23.218300 2227833 finetune.py:45] layer 5_k initial loss 6.626984031754546e-06
I0328 06:14:31.744202 2226924 finetune.py:68] layer 4_k @ epoch 1 new loss 4.303581135900458e-06 old loss 4.338356575317448e-06 BETTER
I0328 06:14:38.850787 2229326 finetune.py:68] layer 7_q @ epoch 1 new loss 1.0545743862167e-05 old loss 1.069029349309858e-05 BETTER
I0328 06:14:42.586894 2228587 finetune.py:68] layer 6_q @ epoch 3 new loss 7.9677110988996e-06 old loss 8.047554729273543e-06 BETTER
I0328 06:14:56.549432 2227833 finetune.py:68] layer 5_k @ epoch 0 new loss 6.559623670909787e-06 old loss 6.626984031754546e-06 BETTER
I0328 06:15:08.114147 2226924 finetune.py:68] layer 4_k @ epoch 2 new loss 4.274208095012e-06 old loss 4.303581135900458e-06 BETTER
I0328 06:15:13.221513 2229326 finetune.py:68] layer 7_q @ epoch 2 new loss 1.0428231689729728e-05 old loss 1.0545743862167e-05 BETTER
I0328 06:15:17.370098 2228587 finetune.py:68] layer 6_q @ epoch 4 new loss 7.898343937995378e-06 old loss 7.9677110988996e-06 BETTER
I0328 06:15:30.824072 2227833 finetune.py:68] layer 5_k @ epoch 1 new loss 6.513813332276186e-06 old loss 6.559623670909787e-06 BETTER
I0328 06:15:35.121282 2228587 finetune.py:45] layer 6_k initial loss 8.323010661115404e-06
I0328 06:15:44.629954 2226924 finetune.py:68] layer 4_k @ epoch 3 new loss 4.2484070945647545e-06 old loss 4.274208095012e-06 BETTER
I0328 06:15:47.707623 2229326 finetune.py:68] layer 7_q @ epoch 3 new loss 1.0334541002521291e-05 old loss 1.0428231689729728e-05 BETTER
I0328 06:16:05.157824 2227833 finetune.py:68] layer 5_k @ epoch 2 new loss 6.474549536505947e-06 old loss 6.513813332276186e-06 BETTER
I0328 06:16:08.573049 2228587 finetune.py:68] layer 6_k @ epoch 0 new loss 8.245079698099289e-06 old loss 8.323010661115404e-06 BETTER
I0328 06:16:21.100755 2226924 finetune.py:68] layer 4_k @ epoch 4 new loss 4.223385076329578e-06 old loss 4.2484070945647545e-06 BETTER
I0328 06:16:21.916756 2229326 finetune.py:68] layer 7_q @ epoch 4 new loss 1.0247488717141096e-05 old loss 1.0334541002521291e-05 BETTER
I0328 06:16:39.546275 2229326 finetune.py:45] layer 7_k initial loss 1.0822623153217137e-05
I0328 06:16:39.678811 2227833 finetune.py:68] layer 5_k @ epoch 3 new loss 6.440610832214588e-06 old loss 6.474549536505947e-06 BETTER
I0328 06:16:40.277969 2226924 finetune.py:45] layer 4_o initial loss 9.750404387887102e-06
I0328 06:16:42.989089 2228587 finetune.py:68] layer 6_k @ epoch 1 new loss 8.190742846636567e-06 old loss 8.245079698099289e-06 BETTER
I0328 06:17:12.452450 2229326 finetune.py:68] layer 7_k @ epoch 0 new loss 1.0707000910770148e-05 old loss 1.0822623153217137e-05 BETTER
I0328 06:17:14.226212 2227833 finetune.py:68] layer 5_k @ epoch 4 new loss 6.407332875824068e-06 old loss 6.440610832214588e-06 BETTER
I0328 06:17:14.890529 2226924 finetune.py:68] layer 4_o @ epoch 0 new loss 9.334423339169007e-06 old loss 9.750404387887102e-06 BETTER
I0328 06:17:17.373268 2228587 finetune.py:68] layer 6_k @ epoch 2 new loss 8.142185834003612e-06 old loss 8.190742846636567e-06 BETTER
I0328 06:17:33.517674 2227833 finetune.py:45] layer 5_o initial loss 1.3287236470205244e-05
I0328 06:17:46.418695 2229326 finetune.py:68] layer 7_k @ epoch 1 new loss 1.0643035238899756e-05 old loss 1.0707000910770148e-05 BETTER
I0328 06:17:50.341779 2226924 finetune.py:68] layer 4_o @ epoch 1 new loss 9.208247320202645e-06 old loss 9.334423339169007e-06 BETTER
I0328 06:17:51.842320 2228587 finetune.py:68] layer 6_k @ epoch 3 new loss 8.09895482234424e-06 old loss 8.142185834003612e-06 BETTER
I0328 06:18:06.074456 2227833 finetune.py:68] layer 5_o @ epoch 0 new loss 1.2834703738917597e-05 old loss 1.3287236470205244e-05 BETTER
I0328 06:18:20.320515 2229326 finetune.py:68] layer 7_k @ epoch 2 new loss 1.0579655281617306e-05 old loss 1.0643035238899756e-05 BETTER
I0328 06:18:26.063566 2226924 finetune.py:68] layer 4_o @ epoch 2 new loss 9.119532478507608e-06 old loss 9.208247320202645e-06 BETTER
I0328 06:18:26.313016 2228587 finetune.py:68] layer 6_k @ epoch 4 new loss 8.058763341978192e-06 old loss 8.09895482234424e-06 BETTER
I0328 06:18:39.580998 2227833 finetune.py:68] layer 5_o @ epoch 1 new loss 1.268794949282892e-05 old loss 1.2834703738917597e-05 BETTER
I0328 06:18:45.480366 2228587 finetune.py:45] layer 6_o initial loss 1.8263703168486245e-05
I0328 06:18:54.239098 2229326 finetune.py:68] layer 7_k @ epoch 3 new loss 1.0526153346290812e-05 old loss 1.0579655281617306e-05 BETTER
I0328 06:19:01.640047 2226924 finetune.py:68] layer 4_o @ epoch 3 new loss 9.049871550814714e-06 old loss 9.119532478507608e-06 BETTER
I0328 06:19:13.048356 2227833 finetune.py:68] layer 5_o @ epoch 2 new loss 1.2583080206240993e-05 old loss 1.268794949282892e-05 BETTER
I0328 06:19:18.409646 2228587 finetune.py:68] layer 6_o @ epoch 0 new loss 1.773967596818693e-05 old loss 1.8263703168486245e-05 BETTER
I0328 06:19:28.141934 2229326 finetune.py:68] layer 7_k @ epoch 4 new loss 1.0474115697434172e-05 old loss 1.0526153346290812e-05 BETTER
I0328 06:19:37.262016 2226924 finetune.py:68] layer 4_o @ epoch 4 new loss 8.990046808321495e-06 old loss 9.049871550814714e-06 BETTER
I0328 06:19:46.929010 2227833 finetune.py:68] layer 5_o @ epoch 3 new loss 1.2498574506025761e-05 old loss 1.2583080206240993e-05 BETTER
I0328 06:19:47.265026 2229326 finetune.py:45] layer 7_o initial loss 2.3119770048651844e-05
I0328 06:19:52.105130 2228587 finetune.py:68] layer 6_o @ epoch 1 new loss 1.7531174307805486e-05 old loss 1.773967596818693e-05 BETTER
I0328 06:20:08.071720 2226924 finetune.py:45] layer 4_up initial loss 2.050409784715157e-05
I0328 06:20:19.341450 2229326 finetune.py:68] layer 7_o @ epoch 0 new loss 2.251787191198673e-05 old loss 2.3119770048651844e-05 BETTER
I0328 06:20:21.050153 2227833 finetune.py:68] layer 5_o @ epoch 4 new loss 1.242737107531866e-05 old loss 1.2498574506025761e-05 BETTER
I0328 06:20:25.974102 2228587 finetune.py:68] layer 6_o @ epoch 2 new loss 1.7374239178025164e-05 old loss 1.7531174307805486e-05 BETTER
I0328 06:20:40.143533 2226924 finetune.py:68] layer 4_up @ epoch 0 new loss 2.0354524167487398e-05 old loss 2.050409784715157e-05 BETTER
I0328 06:20:51.998806 2227833 finetune.py:45] layer 5_up initial loss 2.9147164241294377e-05
I0328 06:20:52.591009 2229326 finetune.py:68] layer 7_o @ epoch 1 new loss 2.223067895101849e-05 old loss 2.251787191198673e-05 BETTER
I0328 06:20:59.810486 2228587 finetune.py:68] layer 6_o @ epoch 3 new loss 1.7243935872102156e-05 old loss 1.7374239178025164e-05 BETTER
I0328 06:21:13.109684 2226924 finetune.py:68] layer 4_up @ epoch 1 new loss 2.0246128769940697e-05 old loss 2.0354524167487398e-05 BETTER
I0328 06:21:22.488587 2227833 finetune.py:68] layer 5_up @ epoch 0 new loss 2.8880076570203528e-05 old loss 2.9147164241294377e-05 BETTER
I0328 06:21:25.928107 2229326 finetune.py:68] layer 7_o @ epoch 2 new loss 2.201179631811101e-05 old loss 2.223067895101849e-05 BETTER
I0328 06:21:33.667639 2228587 finetune.py:68] layer 6_o @ epoch 4 new loss 1.7129123079939745e-05 old loss 1.7243935872102156e-05 BETTER
I0328 06:21:46.699668 2226924 finetune.py:68] layer 4_up @ epoch 2 new loss 2.0151239368715324e-05 old loss 2.0246128769940697e-05 BETTER
I0328 06:21:53.892656 2227833 finetune.py:68] layer 5_up @ epoch 1 new loss 2.86897429759847e-05 old loss 2.8880076570203528e-05 BETTER
I0328 06:21:59.333887 2229326 finetune.py:68] layer 7_o @ epoch 3 new loss 2.182766911573708e-05 old loss 2.201179631811101e-05 BETTER
I0328 06:22:04.776330 2228587 finetune.py:45] layer 6_up initial loss 3.8342783227562904e-05
I0328 06:22:19.985679 2226924 finetune.py:68] layer 4_up @ epoch 3 new loss 2.0062692783540115e-05 old loss 2.0151239368715324e-05 BETTER
I0328 06:22:25.659924 2227833 finetune.py:68] layer 5_up @ epoch 2 new loss 2.8520193154690787e-05 old loss 2.86897429759847e-05 BETTER
I0328 06:22:32.664946 2229326 finetune.py:68] layer 7_o @ epoch 4 new loss 2.167017555620987e-05 old loss 2.182766911573708e-05 BETTER
I0328 06:22:35.530418 2228587 finetune.py:68] layer 6_up @ epoch 0 new loss 3.793374344240874e-05 old loss 3.8342783227562904e-05 BETTER
I0328 06:22:53.311577 2226924 finetune.py:68] layer 4_up @ epoch 4 new loss 1.998047264351044e-05 old loss 2.0062692783540115e-05 BETTER
I0328 06:22:57.485821 2227833 finetune.py:68] layer 5_up @ epoch 3 new loss 2.8365309844957665e-05 old loss 2.8520193154690787e-05 BETTER
I0328 06:23:03.586742 2229326 finetune.py:45] layer 7_up initial loss 4.461199205252342e-05
I0328 06:23:07.273939 2228587 finetune.py:68] layer 6_up @ epoch 1 new loss 3.762708729482256e-05 old loss 3.793374344240874e-05 BETTER
I0328 06:23:24.208940 2226924 finetune.py:45] layer 4_gate initial loss 2.4258402845589444e-05
I0328 06:23:29.333843 2227833 finetune.py:68] layer 5_up @ epoch 4 new loss 2.8221020329510793e-05 old loss 2.8365309844957665e-05 BETTER
I0328 06:23:33.739124 2229326 finetune.py:68] layer 7_up @ epoch 0 new loss 4.411064219311811e-05 old loss 4.461199205252342e-05 BETTER
I0328 06:23:39.491656 2228587 finetune.py:68] layer 6_up @ epoch 2 new loss 3.735468635568395e-05 old loss 3.762708729482256e-05 BETTER
I0328 06:23:54.122707 2226924 finetune.py:68] layer 4_gate @ epoch 0 new loss 2.4158420274034142e-05 old loss 2.4258402845589444e-05 BETTER
I0328 06:24:00.296234 2227833 finetune.py:45] layer 5_gate initial loss 3.4361040889052674e-05
I0328 06:24:04.792108 2229326 finetune.py:68] layer 7_up @ epoch 1 new loss 4.373970296001062e-05 old loss 4.411064219311811e-05 BETTER
I0328 06:24:11.661733 2228587 finetune.py:68] layer 6_up @ epoch 3 new loss 3.710717282956466e-05 old loss 3.735468635568395e-05 BETTER
I0328 06:24:24.950440 2226924 finetune.py:68] layer 4_gate @ epoch 1 new loss 2.4075989131233655e-05 old loss 2.4158420274034142e-05 BETTER
I0328 06:24:28.536434 2227833 finetune.py:68] layer 5_gate @ epoch 0 new loss 3.4189641155535355e-05 old loss 3.4361040889052674e-05 BETTER
I0328 06:24:35.944607 2229326 finetune.py:68] layer 7_up @ epoch 2 new loss 4.3409931095084175e-05 old loss 4.373970296001062e-05 BETTER
I0328 06:24:43.832016 2228587 finetune.py:68] layer 6_up @ epoch 4 new loss 3.687489152071066e-05 old loss 3.710717282956466e-05 BETTER
I0328 06:24:55.999291 2226924 finetune.py:68] layer 4_gate @ epoch 2 new loss 2.400021367066074e-05 old loss 2.4075989131233655e-05 BETTER
I0328 06:24:57.703422 2227833 finetune.py:68] layer 5_gate @ epoch 1 new loss 3.4048964153043926e-05 old loss 3.4189641155535355e-05 BETTER
I0328 06:25:07.154638 2229326 finetune.py:68] layer 7_up @ epoch 3 new loss 4.31094340456184e-05 old loss 4.3409931095084175e-05 BETTER
I0328 06:25:14.716872 2228587 finetune.py:45] layer 6_gate initial loss 4.415407602209598e-05
I0328 06:25:27.011681 2227833 finetune.py:68] layer 5_gate @ epoch 2 new loss 3.3918258850462735e-05 old loss 3.4048964153043926e-05 BETTER
I0328 06:25:27.173214 2226924 finetune.py:68] layer 4_gate @ epoch 3 new loss 2.393040449533146e-05 old loss 2.400021367066074e-05 BETTER
I0328 06:25:38.303732 2229326 finetune.py:68] layer 7_up @ epoch 4 new loss 4.2830088204937056e-05 old loss 4.31094340456184e-05 BETTER
I0328 06:25:43.112358 2228587 finetune.py:68] layer 6_gate @ epoch 0 new loss 4.389443711261265e-05 old loss 4.415407602209598e-05 BETTER
I0328 06:25:56.615526 2227833 finetune.py:68] layer 5_gate @ epoch 3 new loss 3.3799511584220454e-05 old loss 3.3918258850462735e-05 BETTER
I0328 06:25:58.425609 2226924 finetune.py:68] layer 4_gate @ epoch 4 new loss 2.386458800174296e-05 old loss 2.393040449533146e-05 BETTER
I0328 06:26:09.088294 2229326 finetune.py:45] layer 7_gate initial loss 5.165741822565906e-05
I0328 06:26:12.488351 2228587 finetune.py:68] layer 6_gate @ epoch 1 new loss 4.367938527138904e-05 old loss 4.389443711261265e-05 BETTER
I0328 06:26:25.957637 2227833 finetune.py:68] layer 5_gate @ epoch 4 new loss 3.368424222571775e-05 old loss 3.3799511584220454e-05 BETTER
I0328 06:26:37.002819 2229326 finetune.py:68] layer 7_gate @ epoch 0 new loss 5.1348299166420475e-05 old loss 5.165741822565906e-05 BETTER
I0328 06:26:42.039704 2228587 finetune.py:68] layer 6_gate @ epoch 2 new loss 4.348324364400469e-05 old loss 4.367938527138904e-05 BETTER
I0328 06:26:53.719625 2226924 finetune.py:45] layer 4_down initial loss 3.931221363018267e-05
I0328 06:27:06.215339 2229326 finetune.py:68] layer 7_gate @ epoch 1 new loss 5.109643825562671e-05 old loss 5.1348299166420475e-05 BETTER
I0328 06:27:11.717515 2228587 finetune.py:68] layer 6_gate @ epoch 3 new loss 4.3298499804222956e-05 old loss 4.348324364400469e-05 BETTER
I0328 06:27:20.958242 2226924 finetune.py:68] layer 4_down @ epoch 0 new loss 3.931008177460171e-05 old loss 3.931221363018267e-05 BETTER
I0328 06:27:22.052623 2227833 finetune.py:45] layer 5_down initial loss 5.493449134519324e-05
I0328 06:27:35.407845 2229326 finetune.py:68] layer 7_gate @ epoch 2 new loss 5.086648525320925e-05 old loss 5.109643825562671e-05 BETTER
I0328 06:27:41.776777 2228587 finetune.py:68] layer 6_gate @ epoch 4 new loss 4.3123625800944865e-05 old loss 4.3298499804222956e-05 BETTER
I0328 06:27:48.289327 2227833 finetune.py:68] layer 5_down @ epoch 0 new loss 5.4932042985456064e-05 old loss 5.493449134519324e-05 BETTER
I0328 06:27:49.208863 2226924 finetune.py:68] layer 4_down @ epoch 1 new loss 3.930843740818091e-05 old loss 3.931008177460171e-05 BETTER
I0328 06:28:04.500881 2229326 finetune.py:68] layer 7_gate @ epoch 3 new loss 5.06500291521661e-05 old loss 5.086648525320925e-05 BETTER
I0328 06:28:15.320580 2227833 finetune.py:68] layer 5_down @ epoch 1 new loss 5.4929576435824856e-05 old loss 5.4932042985456064e-05 BETTER
I0328 06:28:17.897648 2226924 finetune.py:68] layer 4_down @ epoch 2 new loss 3.93070477002766e-05 old loss 3.930843740818091e-05 BETTER
I0328 06:28:33.650813 2229326 finetune.py:68] layer 7_gate @ epoch 4 new loss 5.044670979259536e-05 old loss 5.06500291521661e-05 BETTER
I0328 06:28:38.288123 2228587 finetune.py:45] layer 6_down initial loss 6.884854519739747e-05
I0328 06:28:42.593474 2227833 finetune.py:68] layer 5_down @ epoch 2 new loss 5.492753189173527e-05 old loss 5.4929576435824856e-05 BETTER
I0328 06:28:46.853302 2226924 finetune.py:68] layer 4_down @ epoch 3 new loss 3.930583625333384e-05 old loss 3.93070477002766e-05 BETTER
I0328 06:29:04.485230 2228587 finetune.py:68] layer 6_down @ epoch 0 new loss 6.884463800815865e-05 old loss 6.884854519739747e-05 BETTER
I0328 06:29:10.106515 2227833 finetune.py:68] layer 5_down @ epoch 3 new loss 5.492599666467868e-05 old loss 5.492753189173527e-05 BETTER
I0328 06:29:15.751991 2226924 finetune.py:68] layer 4_down @ epoch 4 new loss 3.9304886740865186e-05 old loss 3.930583625333384e-05 BETTER
4_v proxy err 0.018954560160636902 tr(WHW.T) 285.30712890625
bpp_loss 2.744360188022256
4_q proxy err 0.000499830930493772 tr(WHW.T) 50116.28125
bpp_loss 3.5741832962376066
4_k proxy err 0.00023850631259847432 tr(WHW.T) 29282.580078125
bpp_loss 4.408581755997147
4_o proxy err 0.015180564485490322 tr(WHW.T) 1300.0960693359375
bpp_loss 2.8204119361471385
4_up proxy err 0.016661813482642174 tr(WHW.T) 7381.88427734375
bpp_loss 2.9252156815491617
4_gate proxy err 0.004515960346907377 tr(WHW.T) 29089.748046875
bpp_loss 3.250320562094982
4_down proxy err 0.01913401111960411 tr(WHW.T) 6423.802734375
bpp_loss 2.9275354177890613
I0328 06:29:30.251209 2229326 finetune.py:45] layer 7_down initial loss 7.848827226553112e-05
I0328 06:29:31.976539 2228587 finetune.py:68] layer 6_down @ epoch 1 new loss 6.884134927531704e-05 old loss 6.884463800815865e-05 BETTER
I0328 06:29:37.883754 2227833 finetune.py:68] layer 5_down @ epoch 4 new loss 5.492434138432145e-05 old loss 5.492599666467868e-05 BETTER
5_v proxy err 0.02636907994747162 tr(WHW.T) 208.81988525390625
bpp_loss 2.6332043108996004
5_q proxy err 0.0007206365116871893 tr(WHW.T) 35998.015625
bpp_loss 3.5502774768392555
5_k proxy err 0.00031370483338832855 tr(WHW.T) 22985.951171875
bpp_loss 4.38305476232199
5_o proxy err 0.016278332099318504 tr(WHW.T) 1061.344482421875
bpp_loss 2.7708228025003336
5_up proxy err 0.0160558819770813 tr(WHW.T) 7653.94921875
bpp_loss 2.9301564104056785
5_gate proxy err 0.004320396110415459 tr(WHW.T) 30352.513671875
bpp_loss 3.2528148127852807
5_down proxy err 0.018459545448422432 tr(WHW.T) 6439.55712890625
bpp_loss 2.9325025431712026
I0328 06:29:56.239785 2229326 finetune.py:68] layer 7_down @ epoch 0 new loss 7.848264067433774e-05 old loss 7.848827226553112e-05 BETTER
I0328 06:29:59.698135 2228587 finetune.py:68] layer 6_down @ epoch 2 new loss 6.883854075567797e-05 old loss 6.884134927531704e-05 BETTER
I0328 06:30:23.274877 2229326 finetune.py:68] layer 7_down @ epoch 1 new loss 7.847831875551492e-05 old loss 7.848264067433774e-05 BETTER
I0328 06:30:27.430357 2228587 finetune.py:68] layer 6_down @ epoch 3 new loss 6.883621244924143e-05 old loss 6.883854075567797e-05 BETTER
I0328 06:30:50.227363 2229326 finetune.py:68] layer 7_down @ epoch 2 new loss 7.847465167287737e-05 old loss 7.847831875551492e-05 BETTER
I0328 06:30:50.392335 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 67.41235256195068s
I0328 06:30:54.329213 2242034 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:30:54.329322 2242034 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:30:54.329364 2242034 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:30:54.713130 2242034 config.py:54] PyTorch version 2.6.0 available.
W0328 06:30:54.930638 2242034 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 06:30:55.083840 2228587 finetune.py:68] layer 6_down @ epoch 4 new loss 6.88340442138724e-05 old loss 6.883621244924143e-05 BETTER
W0328 06:30:55.527140 2242034 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:30:55.530950 2210624 quantize_finetune_llama.py:209] layer 9 gpu 1
I0328 06:30:55.544215 2242034 data_utils.py:336] using 256 training seqs, 128 validation seqs
6_v proxy err 0.022364897653460503 tr(WHW.T) 253.63377380371094
bpp_loss 2.678304518747609
6_q proxy err 0.0007519430364482105 tr(WHW.T) 35658.2890625
bpp_loss 3.5946873596985824
6_k proxy err 0.0002855357888620347 tr(WHW.T) 26148.205078125
bpp_loss 4.45086426922353
6_o proxy err 0.018900204449892044 tr(WHW.T) 1014.287353515625
bpp_loss 2.8016496304189786
6_up proxy err 0.015133421868085861 tr(WHW.T) 7920.18798828125
bpp_loss 2.9293127856882557
6_gate proxy err 0.003590069944038987 tr(WHW.T) 35655.1796875
bpp_loss 3.2571167897299995
6_down proxy err 0.017711937427520752 tr(WHW.T) 6513.47314453125
bpp_loss 2.9331494729932666
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:31:12.109195 2242034 finetune.py:45] layer 8_v initial loss 1.6320182112394832e-05
W0328 06:31:12.109390 2242034 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:31:17.260769 2229326 finetune.py:68] layer 7_down @ epoch 3 new loss 7.847160304663703e-05 old loss 7.847465167287737e-05 BETTER
I0328 06:31:44.351546 2229326 finetune.py:68] layer 7_down @ epoch 4 new loss 7.846916560083628e-05 old loss 7.847160304663703e-05 BETTER
7_v proxy err 0.018532298505306244 tr(WHW.T) 309.4270935058594
bpp_loss 2.6765297953388654
7_q proxy err 0.0007659630500711501 tr(WHW.T) 35159.38671875
bpp_loss 3.523853662016336
7_k proxy err 0.0002838322543539107 tr(WHW.T) 26830.359375
bpp_loss 4.472742018697318
7_o proxy err 0.016583992168307304 tr(WHW.T) 966.2687377929688
bpp_loss 2.8124980951542966
7_up proxy err 0.013726253993809223 tr(WHW.T) 8620.75390625
bpp_loss 2.9417136405993785
7_gate proxy err 0.0035974804777652025 tr(WHW.T) 34850.0
bpp_loss 3.229533733001777
7_down proxy err 0.017728891223669052 tr(WHW.T) 6558.68603515625
bpp_loss 2.9469127615448087
I0328 06:31:46.677763 2242034 finetune.py:68] layer 8_v @ epoch 0 new loss 1.1365949831088074e-05 old loss 1.6320182112394832e-05 BETTER
I0328 06:32:03.433119 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 63.4594190120697s
I0328 06:32:07.037836 2242841 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:32:07.037943 2242841 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:32:07.037984 2242841 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:32:07.365277 2242841 config.py:54] PyTorch version 2.6.0 available.
W0328 06:32:07.574684 2242841 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:32:08.156041 2242841 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:32:08.159565 2210624 quantize_finetune_llama.py:209] layer 10 gpu 2
I0328 06:32:08.172608 2242841 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:32:22.532608 2242034 finetune.py:68] layer 8_v @ epoch 1 new loss 1.0762469173641875e-05 old loss 1.1365949831088074e-05 BETTER
I0328 06:32:24.862822 2242841 finetune.py:45] layer 9_v initial loss 1.7957429008674808e-05
W0328 06:32:24.863037 2242841 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:32:57.690983 2242841 finetune.py:68] layer 9_v @ epoch 0 new loss 1.167183927464066e-05 old loss 1.7957429008674808e-05 BETTER
I0328 06:32:58.850188 2242034 finetune.py:68] layer 8_v @ epoch 2 new loss 1.0455418305355124e-05 old loss 1.0762469173641875e-05 BETTER
I0328 06:33:11.784018 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 63.174858808517456s
I0328 06:33:15.495178 2243602 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:33:15.495274 2243602 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:33:15.495317 2243602 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:33:15.857522 2243602 config.py:54] PyTorch version 2.6.0 available.
W0328 06:33:16.067872 2243602 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:33:16.662380 2243602 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:33:16.666337 2210624 quantize_finetune_llama.py:209] layer 11 gpu 3
I0328 06:33:16.680000 2243602 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:33:32.223364 2242841 finetune.py:68] layer 9_v @ epoch 1 new loss 1.0974411452480126e-05 old loss 1.167183927464066e-05 BETTER
I0328 06:33:33.852458 2243602 finetune.py:45] layer 10_v initial loss 2.151393891836051e-05
W0328 06:33:33.852833 2243602 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:33:35.311657 2242034 finetune.py:68] layer 8_v @ epoch 3 new loss 1.0239171388093382e-05 old loss 1.0455418305355124e-05 BETTER
I0328 06:34:06.961549 2242841 finetune.py:68] layer 9_v @ epoch 2 new loss 1.0626403309288435e-05 old loss 1.0974411452480126e-05 BETTER
I0328 06:34:07.110264 2243602 finetune.py:68] layer 10_v @ epoch 0 new loss 1.5576159057673067e-05 old loss 2.151393891836051e-05 BETTER
I0328 06:34:11.955518 2242034 finetune.py:68] layer 8_v @ epoch 4 new loss 1.008285744319437e-05 old loss 1.0239171388093382e-05 BETTER
I0328 06:34:20.025500 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 62.840168714523315s
I0328 06:34:23.828456 2244327 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:34:23.828564 2244327 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:34:23.828613 2244327 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:34:24.211714 2244327 config.py:54] PyTorch version 2.6.0 available.
W0328 06:34:24.440197 2244327 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:34:25.131187 2244327 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:34:25.135451 2210624 quantize_finetune_llama.py:209] layer 12 gpu 0
I0328 06:34:25.154534 2244327 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 06:34:31.198010 2242034 finetune.py:45] layer 8_q initial loss 1.2457810043997597e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:34:41.601744 2242841 finetune.py:68] layer 9_v @ epoch 3 new loss 1.0395659955975134e-05 old loss 1.0626403309288435e-05 BETTER
I0328 06:34:41.671777 2243602 finetune.py:68] layer 10_v @ epoch 1 new loss 1.4828116036369465e-05 old loss 1.5576159057673067e-05 BETTER
I0328 06:34:42.331719 2244327 finetune.py:45] layer 11_v initial loss 1.9220511603634804e-05
W0328 06:34:42.332137 2244327 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:35:06.019565 2242034 finetune.py:68] layer 8_q @ epoch 0 new loss 1.2194923328934237e-05 old loss 1.2457810043997597e-05 BETTER
I0328 06:35:15.238268 2244327 finetune.py:68] layer 11_v @ epoch 0 new loss 1.3970397048979066e-05 old loss 1.9220511603634804e-05 BETTER
I0328 06:35:16.546284 2242841 finetune.py:68] layer 9_v @ epoch 4 new loss 1.0231164196738973e-05 old loss 1.0395659955975134e-05 BETTER
I0328 06:35:16.551802 2243602 finetune.py:68] layer 10_v @ epoch 2 new loss 1.4436059245781507e-05 old loss 1.4828116036369465e-05 BETTER
I0328 06:35:35.401180 2242841 finetune.py:45] layer 9_q initial loss 1.281698678212706e-05
I0328 06:35:41.911528 2242034 finetune.py:68] layer 8_q @ epoch 1 new loss 1.2027658158331178e-05 old loss 1.2194923328934237e-05 BETTER
I0328 06:35:49.518507 2244327 finetune.py:68] layer 11_v @ epoch 1 new loss 1.3254773875814863e-05 old loss 1.3970397048979066e-05 BETTER
I0328 06:35:51.530494 2243602 finetune.py:68] layer 10_v @ epoch 3 new loss 1.414897087670397e-05 old loss 1.4436059245781507e-05 BETTER
I0328 06:36:08.514201 2242841 finetune.py:68] layer 9_q @ epoch 0 new loss 1.2571211300382856e-05 old loss 1.281698678212706e-05 BETTER
I0328 06:36:17.969690 2242034 finetune.py:68] layer 8_q @ epoch 2 new loss 1.1896532669197768e-05 old loss 1.2027658158331178e-05 BETTER
I0328 06:36:23.804533 2244327 finetune.py:68] layer 11_v @ epoch 2 new loss 1.2887088814750314e-05 old loss 1.3254773875814863e-05 BETTER
I0328 06:36:26.639595 2243602 finetune.py:68] layer 10_v @ epoch 4 new loss 1.3938580195826944e-05 old loss 1.414897087670397e-05 BETTER
I0328 06:36:42.528238 2242841 finetune.py:68] layer 9_q @ epoch 1 new loss 1.2405488632794004e-05 old loss 1.2571211300382856e-05 BETTER
I0328 06:36:46.004505 2243602 finetune.py:45] layer 10_q initial loss 1.6938587577897124e-05
I0328 06:36:54.115621 2242034 finetune.py:68] layer 8_q @ epoch 3 new loss 1.1777331565099303e-05 old loss 1.1896532669197768e-05 BETTER
I0328 06:36:58.274572 2244327 finetune.py:68] layer 11_v @ epoch 3 new loss 1.262754994968418e-05 old loss 1.2887088814750314e-05 BETTER
I0328 06:37:16.763431 2242841 finetune.py:68] layer 9_q @ epoch 2 new loss 1.227299162565032e-05 old loss 1.2405488632794004e-05 BETTER
I0328 06:37:19.388143 2243602 finetune.py:68] layer 10_q @ epoch 0 new loss 1.658490509726107e-05 old loss 1.6938587577897124e-05 BETTER
I0328 06:37:30.316065 2242034 finetune.py:68] layer 8_q @ epoch 4 new loss 1.1679483577609062e-05 old loss 1.1777331565099303e-05 BETTER
I0328 06:37:32.643372 2244327 finetune.py:68] layer 11_v @ epoch 4 new loss 1.2423403859429527e-05 old loss 1.262754994968418e-05 BETTER
I0328 06:37:47.804252 2242034 finetune.py:45] layer 8_k initial loss 1.2237604096299037e-05
I0328 06:37:51.043436 2242841 finetune.py:68] layer 9_q @ epoch 3 new loss 1.2155853255535476e-05 old loss 1.227299162565032e-05 BETTER
I0328 06:37:51.584426 2244327 finetune.py:45] layer 11_q initial loss 1.5935092960717157e-05
I0328 06:37:53.729405 2243602 finetune.py:68] layer 10_q @ epoch 1 new loss 1.6360192603315227e-05 old loss 1.658490509726107e-05 BETTER
I0328 06:38:22.802349 2242034 finetune.py:68] layer 8_k @ epoch 0 new loss 1.2130900358897634e-05 old loss 1.2237604096299037e-05 BETTER
I0328 06:38:24.422878 2244327 finetune.py:68] layer 11_q @ epoch 0 new loss 1.561339377076365e-05 old loss 1.5935092960717157e-05 BETTER
I0328 06:38:25.557719 2242841 finetune.py:68] layer 9_q @ epoch 4 new loss 1.2059263099217787e-05 old loss 1.2155853255535476e-05 BETTER
I0328 06:38:28.218144 2243602 finetune.py:68] layer 10_q @ epoch 2 new loss 1.6176556528080255e-05 old loss 1.6360192603315227e-05 BETTER
I0328 06:38:43.329900 2242841 finetune.py:45] layer 9_k initial loss 1.2736399185087066e-05
I0328 06:38:58.216649 2244327 finetune.py:68] layer 11_q @ epoch 1 new loss 1.5409943443955854e-05 old loss 1.561339377076365e-05 BETTER
I0328 06:38:58.672346 2242034 finetune.py:68] layer 8_k @ epoch 1 new loss 1.205413445859449e-05 old loss 1.2130900358897634e-05 BETTER
I0328 06:39:02.623724 2243602 finetune.py:68] layer 10_q @ epoch 3 new loss 1.602551492396742e-05 old loss 1.6176556528080255e-05 BETTER
I0328 06:39:16.604278 2242841 finetune.py:68] layer 9_k @ epoch 0 new loss 1.2616747881111223e-05 old loss 1.2736399185087066e-05 BETTER
I0328 06:39:32.107728 2244327 finetune.py:68] layer 11_q @ epoch 2 new loss 1.5234359125315677e-05 old loss 1.5409943443955854e-05 BETTER
I0328 06:39:35.005436 2242034 finetune.py:68] layer 8_k @ epoch 2 new loss 1.1985976016148925e-05 old loss 1.205413445859449e-05 BETTER
I0328 06:39:37.185671 2243602 finetune.py:68] layer 10_q @ epoch 4 new loss 1.5885287211858667e-05 old loss 1.602551492396742e-05 BETTER
I0328 06:39:50.810888 2242841 finetune.py:68] layer 9_k @ epoch 1 new loss 1.2534959751064889e-05 old loss 1.2616747881111223e-05 BETTER
I0328 06:39:55.147426 2243602 finetune.py:45] layer 10_k initial loss 1.654636798775755e-05
I0328 06:40:05.958986 2244327 finetune.py:68] layer 11_q @ epoch 3 new loss 1.5117005204956513e-05 old loss 1.5234359125315677e-05 BETTER
I0328 06:40:11.205521 2242034 finetune.py:68] layer 8_k @ epoch 3 new loss 1.1922873454750516e-05 old loss 1.1985976016148925e-05 BETTER
I0328 06:40:24.979916 2242841 finetune.py:68] layer 9_k @ epoch 2 new loss 1.2469279681681655e-05 old loss 1.2534959751064889e-05 BETTER
I0328 06:40:28.488367 2243602 finetune.py:68] layer 10_k @ epoch 0 new loss 1.6402329492848366e-05 old loss 1.654636798775755e-05 BETTER
I0328 06:40:40.008976 2244327 finetune.py:68] layer 11_q @ epoch 4 new loss 1.5006566172814928e-05 old loss 1.5117005204956513e-05 BETTER
I0328 06:40:47.495239 2242034 finetune.py:68] layer 8_k @ epoch 4 new loss 1.186965801025508e-05 old loss 1.1922873454750516e-05 BETTER
I0328 06:40:57.446422 2244327 finetune.py:45] layer 11_k initial loss 1.5726263882243074e-05
I0328 06:40:59.471586 2242841 finetune.py:68] layer 9_k @ epoch 3 new loss 1.240382971445797e-05 old loss 1.2469279681681655e-05 BETTER
I0328 06:41:02.828457 2243602 finetune.py:68] layer 10_k @ epoch 1 new loss 1.6295989553327672e-05 old loss 1.6402329492848366e-05 BETTER
I0328 06:41:06.881614 2242034 finetune.py:45] layer 8_o initial loss 2.7361846150597557e-05
I0328 06:41:30.286339 2244327 finetune.py:68] layer 11_k @ epoch 0 new loss 1.5577323210891336e-05 old loss 1.5726263882243074e-05 BETTER
I0328 06:41:33.763191 2242841 finetune.py:68] layer 9_k @ epoch 4 new loss 1.2343528396741021e-05 old loss 1.240382971445797e-05 BETTER
I0328 06:41:37.242635 2243602 finetune.py:68] layer 10_k @ epoch 2 new loss 1.619937211216893e-05 old loss 1.6295989553327672e-05 BETTER
I0328 06:41:41.294851 2242034 finetune.py:68] layer 8_o @ epoch 0 new loss 2.6687910576583818e-05 old loss 2.7361846150597557e-05 BETTER
I0328 06:41:53.125325 2242841 finetune.py:45] layer 9_o initial loss 2.8674217901425436e-05
I0328 06:42:03.880765 2244327 finetune.py:68] layer 11_k @ epoch 1 new loss 1.5474903193535283e-05 old loss 1.5577323210891336e-05 BETTER
I0328 06:42:11.701304 2243602 finetune.py:68] layer 10_k @ epoch 3 new loss 1.611097286513541e-05 old loss 1.619937211216893e-05 BETTER
I0328 06:42:16.873210 2242034 finetune.py:68] layer 8_o @ epoch 1 new loss 2.6364214136265218e-05 old loss 2.6687910576583818e-05 BETTER
I0328 06:42:25.639564 2242841 finetune.py:68] layer 9_o @ epoch 0 new loss 2.7883304937859066e-05 old loss 2.8674217901425436e-05 BETTER
I0328 06:42:37.712130 2244327 finetune.py:68] layer 11_k @ epoch 2 new loss 1.538804281153716e-05 old loss 1.5474903193535283e-05 BETTER
I0328 06:42:46.175945 2243602 finetune.py:68] layer 10_k @ epoch 4 new loss 1.6039024558267556e-05 old loss 1.611097286513541e-05 BETTER
I0328 06:42:52.574249 2242034 finetune.py:68] layer 8_o @ epoch 2 new loss 2.6113710191566497e-05 old loss 2.6364214136265218e-05 BETTER
I0328 06:42:59.095881 2242841 finetune.py:68] layer 9_o @ epoch 1 new loss 2.7484285965329036e-05 old loss 2.7883304937859066e-05 BETTER
I0328 06:43:05.628763 2243602 finetune.py:45] layer 10_o initial loss 3.5654007660923526e-05
I0328 06:43:11.682116 2244327 finetune.py:68] layer 11_k @ epoch 3 new loss 1.5299134247470647e-05 old loss 1.538804281153716e-05 BETTER
I0328 06:43:28.425693 2242034 finetune.py:68] layer 8_o @ epoch 3 new loss 2.59026674029883e-05 old loss 2.6113710191566497e-05 BETTER
I0328 06:43:32.828158 2242841 finetune.py:68] layer 9_o @ epoch 2 new loss 2.717880670388695e-05 old loss 2.7484285965329036e-05 BETTER
I0328 06:43:38.545409 2243602 finetune.py:68] layer 10_o @ epoch 0 new loss 3.45456282957457e-05 old loss 3.5654007660923526e-05 BETTER
I0328 06:43:45.490493 2244327 finetune.py:68] layer 11_k @ epoch 4 new loss 1.5219237866404e-05 old loss 1.5299134247470647e-05 BETTER
I0328 06:44:04.312177 2242034 finetune.py:68] layer 8_o @ epoch 4 new loss 2.57180945482105e-05 old loss 2.59026674029883e-05 BETTER
I0328 06:44:04.639764 2244327 finetune.py:45] layer 11_o initial loss 3.569778709788807e-05
I0328 06:44:06.547511 2242841 finetune.py:68] layer 9_o @ epoch 3 new loss 2.6928968509309925e-05 old loss 2.717880670388695e-05 BETTER
I0328 06:44:12.228029 2243602 finetune.py:68] layer 10_o @ epoch 1 new loss 3.4006585337920114e-05 old loss 3.45456282957457e-05 BETTER
I0328 06:44:35.440952 2242034 finetune.py:45] layer 8_up initial loss 5.1097282266709954e-05
I0328 06:44:36.631614 2244327 finetune.py:68] layer 11_o @ epoch 0 new loss 3.467717760941014e-05 old loss 3.569778709788807e-05 BETTER
I0328 06:44:40.259516 2242841 finetune.py:68] layer 9_o @ epoch 4 new loss 2.6710602469393052e-05 old loss 2.6928968509309925e-05 BETTER
I0328 06:44:45.980715 2243602 finetune.py:68] layer 10_o @ epoch 2 new loss 3.360225309734233e-05 old loss 3.4006585337920114e-05 BETTER
I0328 06:45:07.123007 2242034 finetune.py:68] layer 8_up @ epoch 0 new loss 5.05298376083374e-05 old loss 5.1097282266709954e-05 BETTER
I0328 06:45:09.503154 2244327 finetune.py:68] layer 11_o @ epoch 1 new loss 3.415494938963093e-05 old loss 3.467717760941014e-05 BETTER
I0328 06:45:11.459145 2242841 finetune.py:45] layer 9_up initial loss 5.445220449473709e-05
I0328 06:45:19.733021 2243602 finetune.py:68] layer 10_o @ epoch 3 new loss 3.326845762785524e-05 old loss 3.360225309734233e-05 BETTER
I0328 06:45:39.972198 2242034 finetune.py:68] layer 8_up @ epoch 1 new loss 5.0097274652216583e-05 old loss 5.05298376083374e-05 BETTER
I0328 06:45:41.894392 2242841 finetune.py:68] layer 9_up @ epoch 0 new loss 5.3793173719896004e-05 old loss 5.445220449473709e-05 BETTER
I0328 06:45:42.841222 2244327 finetune.py:68] layer 11_o @ epoch 2 new loss 3.3753214665921405e-05 old loss 3.415494938963093e-05 BETTER
I0328 06:45:53.434914 2243602 finetune.py:68] layer 10_o @ epoch 4 new loss 3.297986040706746e-05 old loss 3.326845762785524e-05 BETTER
I0328 06:46:13.243864 2242034 finetune.py:68] layer 8_up @ epoch 2 new loss 4.971455200575292e-05 old loss 5.0097274652216583e-05 BETTER
I0328 06:46:13.268143 2242841 finetune.py:68] layer 9_up @ epoch 1 new loss 5.329130362952128e-05 old loss 5.3793173719896004e-05 BETTER
I0328 06:46:16.118039 2244327 finetune.py:68] layer 11_o @ epoch 3 new loss 3.341973206261173e-05 old loss 3.3753214665921405e-05 BETTER
I0328 06:46:24.804695 2243602 finetune.py:45] layer 10_up initial loss 6.150460103526711e-05
I0328 06:46:44.911107 2242841 finetune.py:68] layer 9_up @ epoch 2 new loss 5.285043516778387e-05 old loss 5.329130362952128e-05 BETTER
I0328 06:46:46.544196 2242034 finetune.py:68] layer 8_up @ epoch 3 new loss 4.9367146857548505e-05 old loss 4.971455200575292e-05 BETTER
I0328 06:46:49.404863 2244327 finetune.py:68] layer 11_o @ epoch 4 new loss 3.313297565910034e-05 old loss 3.341973206261173e-05 BETTER
I0328 06:46:55.415463 2243602 finetune.py:68] layer 10_up @ epoch 0 new loss 6.078312799218111e-05 old loss 6.150460103526711e-05 BETTER
I0328 06:47:16.865188 2242841 finetune.py:68] layer 9_up @ epoch 3 new loss 5.244581188890152e-05 old loss 5.285043516778387e-05 BETTER
I0328 06:47:19.948119 2242034 finetune.py:68] layer 8_up @ epoch 4 new loss 4.9041362217394635e-05 old loss 4.9367146857548505e-05 BETTER
I0328 06:47:20.641748 2244327 finetune.py:45] layer 11_up initial loss 6.334092176984996e-05
I0328 06:47:26.999228 2243602 finetune.py:68] layer 10_up @ epoch 1 new loss 6.02315558353439e-05 old loss 6.078312799218111e-05 BETTER
I0328 06:47:49.024821 2242841 finetune.py:68] layer 9_up @ epoch 4 new loss 5.207681897445582e-05 old loss 5.244581188890152e-05 BETTER
I0328 06:47:50.691011 2244327 finetune.py:68] layer 11_up @ epoch 0 new loss 6.256408960325643e-05 old loss 6.334092176984996e-05 BETTER
I0328 06:47:50.895574 2242034 finetune.py:45] layer 8_gate initial loss 5.854676783201285e-05
I0328 06:47:59.042119 2243602 finetune.py:68] layer 10_up @ epoch 2 new loss 5.975219755782746e-05 old loss 6.02315558353439e-05 BETTER
I0328 06:48:20.023043 2242841 finetune.py:45] layer 9_gate initial loss 6.231381121324375e-05
I0328 06:48:20.680207 2242034 finetune.py:68] layer 8_gate @ epoch 0 new loss 5.819517537020147e-05 old loss 5.854676783201285e-05 BETTER
I0328 06:48:21.742716 2244327 finetune.py:68] layer 11_up @ epoch 1 new loss 6.195945024956018e-05 old loss 6.256408960325643e-05 BETTER
I0328 06:48:31.219871 2243602 finetune.py:68] layer 10_up @ epoch 3 new loss 5.9308793424861506e-05 old loss 5.975219755782746e-05 BETTER
I0328 06:48:48.280375 2242841 finetune.py:68] layer 9_gate @ epoch 0 new loss 6.19112397544086e-05 old loss 6.231381121324375e-05 BETTER
I0328 06:48:51.532126 2242034 finetune.py:68] layer 8_gate @ epoch 1 new loss 5.7901881518773735e-05 old loss 5.819517537020147e-05 BETTER
I0328 06:48:53.135931 2244327 finetune.py:68] layer 11_up @ epoch 2 new loss 6.142783240647987e-05 old loss 6.195945024956018e-05 BETTER
I0328 06:49:03.310329 2243602 finetune.py:68] layer 10_up @ epoch 4 new loss 5.8902540331473574e-05 old loss 5.9308793424861506e-05 BETTER
I0328 06:49:17.710219 2242841 finetune.py:68] layer 9_gate @ epoch 1 new loss 6.158330506877974e-05 old loss 6.19112397544086e-05 BETTER
I0328 06:49:22.490991 2242034 finetune.py:68] layer 8_gate @ epoch 2 new loss 5.763200533692725e-05 old loss 5.7901881518773735e-05 BETTER
I0328 06:49:24.583723 2244327 finetune.py:68] layer 11_up @ epoch 3 new loss 6.09401504334528e-05 old loss 6.142783240647987e-05 BETTER
I0328 06:49:34.585973 2243602 finetune.py:45] layer 10_gate initial loss 7.011977140791714e-05
I0328 06:49:47.041310 2242841 finetune.py:68] layer 9_gate @ epoch 2 new loss 6.127921369625255e-05 old loss 6.158330506877974e-05 BETTER
I0328 06:49:53.601008 2242034 finetune.py:68] layer 8_gate @ epoch 3 new loss 5.73820507270284e-05 old loss 5.763200533692725e-05 BETTER
I0328 06:49:56.016399 2244327 finetune.py:68] layer 11_up @ epoch 4 new loss 6.049228613846935e-05 old loss 6.09401504334528e-05 BETTER
I0328 06:50:02.962336 2243602 finetune.py:68] layer 10_gate @ epoch 0 new loss 6.968696106923744e-05 old loss 7.011977140791714e-05 BETTER
I0328 06:50:16.329176 2242841 finetune.py:68] layer 9_gate @ epoch 3 new loss 6.0994159866822883e-05 old loss 6.127921369625255e-05 BETTER
I0328 06:50:24.743040 2242034 finetune.py:68] layer 8_gate @ epoch 4 new loss 5.714183862437494e-05 old loss 5.73820507270284e-05 BETTER
I0328 06:50:27.226742 2244327 finetune.py:45] layer 11_gate initial loss 7.247070607263595e-05
I0328 06:50:32.241413 2243602 finetune.py:68] layer 10_gate @ epoch 1 new loss 6.93296969984658e-05 old loss 6.968696106923744e-05 BETTER
I0328 06:50:45.968577 2242841 finetune.py:68] layer 9_gate @ epoch 4 new loss 6.072715041227639e-05 old loss 6.0994159866822883e-05 BETTER
I0328 06:50:55.077682 2244327 finetune.py:68] layer 11_gate @ epoch 0 new loss 7.201152766356245e-05 old loss 7.247070607263595e-05 BETTER
I0328 06:51:01.786104 2243602 finetune.py:68] layer 10_gate @ epoch 2 new loss 6.899947766214609e-05 old loss 6.93296969984658e-05 BETTER
I0328 06:51:20.074195 2242034 finetune.py:45] layer 8_down initial loss 8.733322465559468e-05
I0328 06:51:23.961548 2244327 finetune.py:68] layer 11_gate @ epoch 1 new loss 7.16240901965648e-05 old loss 7.201152766356245e-05 BETTER
I0328 06:51:31.554044 2243602 finetune.py:68] layer 10_gate @ epoch 3 new loss 6.869433127576485e-05 old loss 6.899947766214609e-05 BETTER
I0328 06:51:43.729556 2242841 finetune.py:45] layer 9_down initial loss 9.372305066790432e-05
I0328 06:51:47.408019 2242034 finetune.py:68] layer 8_down @ epoch 0 new loss 8.732623246032745e-05 old loss 8.733322465559468e-05 BETTER
I0328 06:51:53.062185 2244327 finetune.py:68] layer 11_gate @ epoch 2 new loss 7.127074059098959e-05 old loss 7.16240901965648e-05 BETTER
I0328 06:52:01.090142 2243602 finetune.py:68] layer 10_gate @ epoch 4 new loss 6.84027691022493e-05 old loss 6.869433127576485e-05 BETTER
I0328 06:52:09.813958 2242841 finetune.py:68] layer 9_down @ epoch 0 new loss 9.3716625997331e-05 old loss 9.372305066790432e-05 BETTER
I0328 06:52:15.651241 2242034 finetune.py:68] layer 8_down @ epoch 1 new loss 8.732071000849828e-05 old loss 8.732623246032745e-05 BETTER
I0328 06:52:22.452711 2244327 finetune.py:68] layer 11_gate @ epoch 3 new loss 7.093520252965391e-05 old loss 7.127074059098959e-05 BETTER
I0328 06:52:36.816944 2242841 finetune.py:68] layer 9_down @ epoch 1 new loss 9.371258784085512e-05 old loss 9.3716625997331e-05 BETTER
I0328 06:52:44.506085 2242034 finetune.py:68] layer 8_down @ epoch 2 new loss 8.731600973987952e-05 old loss 8.732071000849828e-05 BETTER
I0328 06:52:51.650038 2244327 finetune.py:68] layer 11_gate @ epoch 4 new loss 7.06222781445831e-05 old loss 7.093520252965391e-05 BETTER
I0328 06:52:58.184603 2243602 finetune.py:45] layer 10_down initial loss 0.0001023478907882236
I0328 06:53:04.107556 2242841 finetune.py:68] layer 9_down @ epoch 2 new loss 9.370718180434778e-05 old loss 9.371258784085512e-05 BETTER
I0328 06:53:13.561294 2242034 finetune.py:68] layer 8_down @ epoch 3 new loss 8.731224806979299e-05 old loss 8.731600973987952e-05 BETTER
I0328 06:53:24.376847 2243602 finetune.py:68] layer 10_down @ epoch 0 new loss 0.00010233954526484013 old loss 0.0001023478907882236 BETTER
I0328 06:53:31.594238 2242841 finetune.py:68] layer 9_down @ epoch 3 new loss 9.370332554681227e-05 old loss 9.370718180434778e-05 BETTER
I0328 06:53:42.739950 2242034 finetune.py:68] layer 8_down @ epoch 4 new loss 8.730853005545214e-05 old loss 8.731224806979299e-05 BETTER
8_v proxy err 0.02178853191435337 tr(WHW.T) 257.7052307128906
bpp_loss 2.6965860499767587
8_q proxy err 0.0009831588249653578 tr(WHW.T) 26604.412109375
bpp_loss 3.510479933640454
8_k proxy err 0.000325922534102574 tr(WHW.T) 22497.15625
bpp_loss 4.388637381372973
8_o proxy err 0.020818723365664482 tr(WHW.T) 749.3833618164062
bpp_loss 2.823132567340508
8_up proxy err 0.014071078971028328 tr(WHW.T) 8494.0400390625
bpp_loss 2.9377620251450156
8_gate proxy err 0.003410818288102746 tr(WHW.T) 37202.484375
bpp_loss 3.233828209274049
8_down proxy err 0.017969364300370216 tr(WHW.T) 6512.6201171875
bpp_loss 2.945293963038629
I0328 06:53:47.890052 2244327 finetune.py:45] layer 11_down initial loss 0.00010578622459433973
I0328 06:53:52.252105 2243602 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0001023320946842432 old loss 0.00010233954526484013 BETTER
I0328 06:53:59.529495 2242841 finetune.py:68] layer 9_down @ epoch 4 new loss 9.370003681397066e-05 old loss 9.370332554681227e-05 BETTER
9_v proxy err 0.01647687703371048 tr(WHW.T) 351.4288024902344
bpp_loss 2.7942189517780207
9_q proxy err 0.0010302503360435367 tr(WHW.T) 25662.662109375
bpp_loss 3.5225077304057777
9_k proxy err 0.00035538277006708086 tr(WHW.T) 20926.025390625
bpp_loss 4.4088916659820825
9_o proxy err 0.018720731139183044 tr(WHW.T) 780.2742919921875
bpp_loss 2.875788494187873
9_up proxy err 0.013326317071914673 tr(WHW.T) 8968.5556640625
bpp_loss 2.9467269867392525
9_gate proxy err 0.003226321889087558 tr(WHW.T) 39371.2421875
bpp_loss 3.2466466701589525
9_down proxy err 0.017805706709623337 tr(WHW.T) 6298.6259765625
bpp_loss 2.9465753216396218
I0328 06:54:14.093237 2244327 finetune.py:68] layer 11_down @ epoch 0 new loss 0.00010577688226476312 old loss 0.00010578622459433973 BETTER
I0328 06:54:19.943993 2243602 finetune.py:68] layer 10_down @ epoch 2 new loss 0.00010232567728962749 old loss 0.0001023320946842432 BETTER
I0328 06:54:41.010464 2244327 finetune.py:68] layer 11_down @ epoch 1 new loss 0.00010576863132882863 old loss 0.00010577688226476312 BETTER
I0328 06:54:47.659945 2243602 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0001023199874907732 old loss 0.00010232567728962749 BETTER
I0328 06:55:08.122231 2244327 finetune.py:68] layer 11_down @ epoch 2 new loss 0.00010576079512247816 old loss 0.00010576863132882863 BETTER
I0328 06:55:12.421721 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 67.74658179283142s
I0328 06:55:15.422036 2243602 finetune.py:68] layer 10_down @ epoch 4 new loss 0.00010231569467578083 old loss 0.0001023199874907732 BETTER
I0328 06:55:16.373316 2257011 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:55:16.373410 2257011 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:55:16.373448 2257011 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:55:16.755613 2257011 config.py:54] PyTorch version 2.6.0 available.
W0328 06:55:16.969536 2257011 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

10_v proxy err 0.021359754726290703 tr(WHW.T) 251.83889770507812
bpp_loss 2.6869318945682608
10_q proxy err 0.0010739645222201943 tr(WHW.T) 23346.142578125
bpp_loss 3.5200723947491497
10_k proxy err 0.0003572926507331431 tr(WHW.T) 19721.98046875
bpp_loss 4.407092372421175
10_o proxy err 0.022573862224817276 tr(WHW.T) 684.6346435546875
bpp_loss 2.815555733104702
10_up proxy err 0.01316144596785307 tr(WHW.T) 9196.7177734375
bpp_loss 2.963467817420938
10_gate proxy err 0.003409089520573616 tr(WHW.T) 37375.57421875
bpp_loss 3.218282972395952
10_down proxy err 0.017321500927209854 tr(WHW.T) 6550.4150390625
bpp_loss 2.9628561971975222
W0328 06:55:17.547390 2257011 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:55:17.551219 2210624 quantize_finetune_llama.py:209] layer 13 gpu 1
I0328 06:55:17.564842 2257011 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:55:34.141620 2257011 finetune.py:45] layer 12_v initial loss 2.0645818949560635e-05
W0328 06:55:34.141893 2257011 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:55:35.317282 2244327 finetune.py:68] layer 11_down @ epoch 3 new loss 0.00010575473424978554 old loss 0.00010576079512247816 BETTER
I0328 06:56:02.425364 2244327 finetune.py:68] layer 11_down @ epoch 4 new loss 0.00010574980115052313 old loss 0.00010575473424978554 BETTER
11_v proxy err 0.017618469893932343 tr(WHW.T) 319.5691223144531
bpp_loss 2.69375230174046
11_q proxy err 0.0011781224748119712 tr(WHW.T) 22198.17578125
bpp_loss 3.465891526080668
11_k proxy err 0.0004124982515349984 tr(WHW.T) 17995.412109375
bpp_loss 4.407973540422972
11_o proxy err 0.023011093959212303 tr(WHW.T) 569.0487670898438
bpp_loss 2.834715715784114
11_up proxy err 0.012896341271698475 tr(WHW.T) 9192.31640625
bpp_loss 2.967490019210215
11_gate proxy err 0.003376389853656292 tr(WHW.T) 36788.70703125
bpp_loss 3.1975926673705026
11_down proxy err 0.01658570021390915 tr(WHW.T) 6682.998046875
bpp_loss 2.9697826033557897
I0328 06:56:08.966535 2257011 finetune.py:68] layer 12_v @ epoch 0 new loss 1.573934787302278e-05 old loss 2.0645818949560635e-05 BETTER
I0328 06:56:24.499265 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 64.28866529464722s
I0328 06:56:28.185267 2257789 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:56:28.185373 2257789 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:56:28.185414 2257789 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:56:28.517084 2257789 config.py:54] PyTorch version 2.6.0 available.
W0328 06:56:28.718663 2257789 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:56:29.290080 2257789 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:56:29.293735 2210624 quantize_finetune_llama.py:209] layer 14 gpu 2
I0328 06:56:29.313042 2257789 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:56:45.340020 2257011 finetune.py:68] layer 12_v @ epoch 1 new loss 1.497937864769483e-05 old loss 1.573934787302278e-05 BETTER
I0328 06:56:46.127518 2257789 finetune.py:45] layer 13_v initial loss 2.1513797037187032e-05
W0328 06:56:46.127736 2257789 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:57:19.264633 2257789 finetune.py:68] layer 13_v @ epoch 0 new loss 1.6647336451569572e-05 old loss 2.1513797037187032e-05 BETTER
I0328 06:57:22.260242 2257011 finetune.py:68] layer 12_v @ epoch 2 new loss 1.4505239050777163e-05 old loss 1.497937864769483e-05 BETTER
I0328 06:57:33.614721 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 63.84757328033447s
I0328 06:57:37.307691 2258536 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:57:37.307808 2258536 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:57:37.307860 2258536 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:57:37.674152 2258536 config.py:54] PyTorch version 2.6.0 available.
W0328 06:57:37.882168 2258536 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:57:38.463413 2258536 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:57:38.467413 2210624 quantize_finetune_llama.py:209] layer 15 gpu 3
I0328 06:57:38.481263 2258536 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:57:53.788163 2257789 finetune.py:68] layer 13_v @ epoch 1 new loss 1.5865622117416933e-05 old loss 1.6647336451569572e-05 BETTER
I0328 06:57:56.056804 2258536 finetune.py:45] layer 14_v initial loss 2.2981274014455266e-05
W0328 06:57:56.057090 2258536 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:57:59.238839 2257011 finetune.py:68] layer 12_v @ epoch 3 new loss 1.4178838682710193e-05 old loss 1.4505239050777163e-05 BETTER
I0328 06:58:28.523914 2257789 finetune.py:68] layer 13_v @ epoch 2 new loss 1.5420428098877892e-05 old loss 1.5865622117416933e-05 BETTER
I0328 06:58:29.422492 2258536 finetune.py:68] layer 14_v @ epoch 0 new loss 1.814178904169239e-05 old loss 2.2981274014455266e-05 BETTER
I0328 06:58:36.186347 2257011 finetune.py:68] layer 12_v @ epoch 4 new loss 1.3949234016763512e-05 old loss 1.4178838682710193e-05 BETTER
I0328 06:58:42.950369 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 63.70276737213135s
I0328 06:58:46.742737 2259305 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 06:58:46.742843 2259305 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 06:58:46.742886 2259305 utils.py:162] NumExpr defaulting to 16 threads.
I0328 06:58:47.078840 2259305 config.py:54] PyTorch version 2.6.0 available.
W0328 06:58:47.290343 2259305 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 06:58:47.877194 2259305 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 06:58:47.880696 2210624 quantize_finetune_llama.py:209] layer 16 gpu 0
I0328 06:58:47.897025 2259305 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 06:58:55.524244 2257011 finetune.py:45] layer 12_q initial loss 1.6704314475646242e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 06:59:03.587422 2257789 finetune.py:68] layer 13_v @ epoch 3 new loss 1.510048059572e-05 old loss 1.5420428098877892e-05 BETTER
I0328 06:59:04.013819 2258536 finetune.py:68] layer 14_v @ epoch 1 new loss 1.7337088138447143e-05 old loss 1.814178904169239e-05 BETTER
I0328 06:59:04.900727 2259305 finetune.py:45] layer 15_v initial loss 3.072338586207479e-05
W0328 06:59:04.901094 2259305 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 06:59:30.786341 2257011 finetune.py:68] layer 12_q @ epoch 0 new loss 1.633327337913215e-05 old loss 1.6704314475646242e-05 BETTER
I0328 06:59:38.095966 2259305 finetune.py:68] layer 15_v @ epoch 0 new loss 2.030881296377629e-05 old loss 3.072338586207479e-05 BETTER
I0328 06:59:38.952370 2257789 finetune.py:68] layer 13_v @ epoch 4 new loss 1.4862537682347465e-05 old loss 1.510048059572e-05 BETTER
I0328 06:59:39.089395 2258536 finetune.py:68] layer 14_v @ epoch 2 new loss 1.68561382452026e-05 old loss 1.7337088138447143e-05 BETTER
I0328 06:59:58.261502 2257789 finetune.py:45] layer 13_q initial loss 1.8122029359801672e-05
I0328 07:00:06.990377 2257011 finetune.py:68] layer 12_q @ epoch 1 new loss 1.606647128937766e-05 old loss 1.633327337913215e-05 BETTER
I0328 07:00:12.106142 2259305 finetune.py:68] layer 15_v @ epoch 1 new loss 1.9132168745272793e-05 old loss 2.030881296377629e-05 BETTER
I0328 07:00:14.113756 2258536 finetune.py:68] layer 14_v @ epoch 3 new loss 1.650574449740816e-05 old loss 1.68561382452026e-05 BETTER
I0328 07:00:31.943279 2257789 finetune.py:68] layer 13_q @ epoch 0 new loss 1.779167359927669e-05 old loss 1.8122029359801672e-05 BETTER
I0328 07:00:43.414348 2257011 finetune.py:68] layer 12_q @ epoch 2 new loss 1.584451092639938e-05 old loss 1.606647128937766e-05 BETTER
I0328 07:00:46.675401 2259305 finetune.py:68] layer 15_v @ epoch 2 new loss 1.8475593606126495e-05 old loss 1.9132168745272793e-05 BETTER
I0328 07:00:49.227250 2258536 finetune.py:68] layer 14_v @ epoch 4 new loss 1.6238684111158364e-05 old loss 1.650574449740816e-05 BETTER
I0328 07:01:06.278294 2257789 finetune.py:68] layer 13_q @ epoch 1 new loss 1.7559608750161715e-05 old loss 1.779167359927669e-05 BETTER
I0328 07:01:08.614485 2258536 finetune.py:45] layer 14_q initial loss 2.0650148144341074e-05
I0328 07:01:20.007533 2257011 finetune.py:68] layer 12_q @ epoch 3 new loss 1.5640778656234033e-05 old loss 1.584451092639938e-05 BETTER
I0328 07:01:21.159144 2259305 finetune.py:68] layer 15_v @ epoch 3 new loss 1.802952647267375e-05 old loss 1.8475593606126495e-05 BETTER
I0328 07:01:40.760373 2257789 finetune.py:68] layer 13_q @ epoch 2 new loss 1.7378953998559155e-05 old loss 1.7559608750161715e-05 BETTER
I0328 07:01:42.196224 2258536 finetune.py:68] layer 14_q @ epoch 0 new loss 2.0172647055005655e-05 old loss 2.0650148144341074e-05 BETTER
I0328 07:01:55.699494 2259305 finetune.py:68] layer 15_v @ epoch 4 new loss 1.769711343513336e-05 old loss 1.802952647267375e-05 BETTER
I0328 07:01:56.684923 2257011 finetune.py:68] layer 12_q @ epoch 4 new loss 1.5469538993784226e-05 old loss 1.5640778656234033e-05 BETTER
I0328 07:02:14.372997 2257011 finetune.py:45] layer 12_k initial loss 1.650393460295163e-05
I0328 07:02:15.096571 2259305 finetune.py:45] layer 15_q initial loss 2.0374174710013904e-05
I0328 07:02:15.547521 2257789 finetune.py:68] layer 13_q @ epoch 3 new loss 1.7216352716786787e-05 old loss 1.7378953998559155e-05 BETTER
I0328 07:02:16.830344 2258536 finetune.py:68] layer 14_q @ epoch 1 new loss 1.9866494767484255e-05 old loss 2.0172647055005655e-05 BETTER
I0328 07:02:48.163321 2259305 finetune.py:68] layer 15_q @ epoch 0 new loss 1.9987688574474305e-05 old loss 2.0374174710013904e-05 BETTER
I0328 07:02:49.976136 2257011 finetune.py:68] layer 12_k @ epoch 0 new loss 1.6297572074108757e-05 old loss 1.650393460295163e-05 BETTER
I0328 07:02:50.229555 2257789 finetune.py:68] layer 13_q @ epoch 4 new loss 1.7081434634746984e-05 old loss 1.7216352716786787e-05 BETTER
I0328 07:02:51.521006 2258536 finetune.py:68] layer 14_q @ epoch 2 new loss 1.963215254363604e-05 old loss 1.9866494767484255e-05 BETTER
I0328 07:03:08.412774 2257789 finetune.py:45] layer 13_k initial loss 1.7858224964584224e-05
I0328 07:03:22.345018 2259305 finetune.py:68] layer 15_q @ epoch 1 new loss 1.9704173610080034e-05 old loss 1.9987688574474305e-05 BETTER
I0328 07:03:26.233059 2258536 finetune.py:68] layer 14_q @ epoch 3 new loss 1.9420325770624913e-05 old loss 1.963215254363604e-05 BETTER
I0328 07:03:26.246561 2257011 finetune.py:68] layer 12_k @ epoch 1 new loss 1.6152825992321596e-05 old loss 1.6297572074108757e-05 BETTER
I0328 07:03:41.778150 2257789 finetune.py:68] layer 13_k @ epoch 0 new loss 1.7708287487039343e-05 old loss 1.7858224964584224e-05 BETTER
I0328 07:03:56.403470 2259305 finetune.py:68] layer 15_q @ epoch 2 new loss 1.9474719010759145e-05 old loss 1.9704173610080034e-05 BETTER
I0328 07:04:00.987708 2258536 finetune.py:68] layer 14_q @ epoch 4 new loss 1.9245146177127026e-05 old loss 1.9420325770624913e-05 BETTER
I0328 07:04:02.666268 2257011 finetune.py:68] layer 12_k @ epoch 2 new loss 1.6017622328945436e-05 old loss 1.6152825992321596e-05 BETTER
I0328 07:04:16.024087 2257789 finetune.py:68] layer 13_k @ epoch 1 new loss 1.7587313777767122e-05 old loss 1.7708287487039343e-05 BETTER
I0328 07:04:18.958905 2258536 finetune.py:45] layer 14_k initial loss 2.065255648631137e-05
I0328 07:04:30.630273 2259305 finetune.py:68] layer 15_q @ epoch 3 new loss 1.9279023035778664e-05 old loss 1.9474719010759145e-05 BETTER
I0328 07:04:39.203231 2257011 finetune.py:68] layer 12_k @ epoch 3 new loss 1.5904888641671278e-05 old loss 1.6017622328945436e-05 BETTER
I0328 07:04:50.542466 2257789 finetune.py:68] layer 13_k @ epoch 2 new loss 1.749592411215417e-05 old loss 1.7587313777767122e-05 BETTER
I0328 07:04:52.560913 2258536 finetune.py:68] layer 14_k @ epoch 0 new loss 2.0403411326697096e-05 old loss 2.065255648631137e-05 BETTER
I0328 07:05:04.799602 2259305 finetune.py:68] layer 15_q @ epoch 4 new loss 1.911237632157281e-05 old loss 1.9279023035778664e-05 BETTER
I0328 07:05:15.775422 2257011 finetune.py:68] layer 12_k @ epoch 4 new loss 1.5793733837199397e-05 old loss 1.5904888641671278e-05 BETTER
I0328 07:05:22.585960 2259305 finetune.py:45] layer 15_k initial loss 2.020935426116921e-05
I0328 07:05:25.039556 2257789 finetune.py:68] layer 13_k @ epoch 3 new loss 1.7410093278158456e-05 old loss 1.749592411215417e-05 BETTER
I0328 07:05:26.934811 2258536 finetune.py:68] layer 14_k @ epoch 1 new loss 2.0246452550054528e-05 old loss 2.0403411326697096e-05 BETTER
I0328 07:05:35.063041 2257011 finetune.py:45] layer 12_o initial loss 3.6206594813847914e-05
I0328 07:05:55.601476 2259305 finetune.py:68] layer 15_k @ epoch 0 new loss 1.9996901755803265e-05 old loss 2.020935426116921e-05 BETTER
I0328 07:05:59.518118 2257789 finetune.py:68] layer 13_k @ epoch 4 new loss 1.7329435650026426e-05 old loss 1.7410093278158456e-05 BETTER
I0328 07:06:01.428674 2258536 finetune.py:68] layer 14_k @ epoch 2 new loss 2.011625656450633e-05 old loss 2.0246452550054528e-05 BETTER
I0328 07:06:09.674236 2257011 finetune.py:68] layer 12_o @ epoch 0 new loss 3.518760058796033e-05 old loss 3.6206594813847914e-05 BETTER
I0328 07:06:19.194196 2257789 finetune.py:45] layer 13_o initial loss 4.3685555283445865e-05
I0328 07:06:29.472646 2259305 finetune.py:68] layer 15_k @ epoch 1 new loss 1.985445305763278e-05 old loss 1.9996901755803265e-05 BETTER
I0328 07:06:36.016789 2258536 finetune.py:68] layer 14_k @ epoch 3 new loss 1.9999832147732377e-05 old loss 2.011625656450633e-05 BETTER
I0328 07:06:45.368857 2257011 finetune.py:68] layer 12_o @ epoch 1 new loss 3.4625743865035474e-05 old loss 3.518760058796033e-05 BETTER
I0328 07:06:51.923535 2257789 finetune.py:68] layer 13_o @ epoch 0 new loss 4.234524749335833e-05 old loss 4.3685555283445865e-05 BETTER
I0328 07:07:03.547120 2259305 finetune.py:68] layer 15_k @ epoch 2 new loss 1.9724713638424873e-05 old loss 1.985445305763278e-05 BETTER
I0328 07:07:10.705702 2258536 finetune.py:68] layer 14_k @ epoch 4 new loss 1.990129385376349e-05 old loss 1.9999832147732377e-05 BETTER
I0328 07:07:21.270166 2257011 finetune.py:68] layer 12_o @ epoch 2 new loss 3.419138738536276e-05 old loss 3.4625743865035474e-05 BETTER
I0328 07:07:25.538233 2257789 finetune.py:68] layer 13_o @ epoch 1 new loss 4.1643721488071606e-05 old loss 4.234524749335833e-05 BETTER
I0328 07:07:30.078862 2258536 finetune.py:45] layer 14_o initial loss 4.796444045496173e-05
I0328 07:07:37.632087 2259305 finetune.py:68] layer 15_k @ epoch 3 new loss 1.9625336790340953e-05 old loss 1.9724713638424873e-05 BETTER
I0328 07:07:57.622150 2257011 finetune.py:68] layer 12_o @ epoch 3 new loss 3.382302384125069e-05 old loss 3.419138738536276e-05 BETTER
I0328 07:07:59.568132 2257789 finetune.py:68] layer 13_o @ epoch 2 new loss 4.1105908167082816e-05 old loss 4.1643721488071606e-05 BETTER
I0328 07:08:02.945806 2258536 finetune.py:68] layer 14_o @ epoch 0 new loss 4.641138730221428e-05 old loss 4.796444045496173e-05 BETTER
I0328 07:08:11.991200 2259305 finetune.py:68] layer 15_k @ epoch 4 new loss 1.9526865798979998e-05 old loss 1.9625336790340953e-05 BETTER
I0328 07:08:31.229235 2259305 finetune.py:45] layer 15_o initial loss 4.815698048332706e-05
I0328 07:08:33.437320 2257789 finetune.py:68] layer 13_o @ epoch 3 new loss 4.066204564878717e-05 old loss 4.1105908167082816e-05 BETTER
I0328 07:08:33.699261 2257011 finetune.py:68] layer 12_o @ epoch 4 new loss 3.3504322345834225e-05 old loss 3.382302384125069e-05 BETTER
I0328 07:08:36.927248 2258536 finetune.py:68] layer 14_o @ epoch 1 new loss 4.555738632916473e-05 old loss 4.641138730221428e-05 BETTER
I0328 07:09:03.703472 2259305 finetune.py:68] layer 15_o @ epoch 0 new loss 4.6487184590660036e-05 old loss 4.815698048332706e-05 BETTER
I0328 07:09:04.570697 2257011 finetune.py:45] layer 12_up initial loss 6.499616574728861e-05
I0328 07:09:07.452515 2257789 finetune.py:68] layer 13_o @ epoch 4 new loss 4.028152034152299e-05 old loss 4.066204564878717e-05 BETTER
I0328 07:09:10.791086 2258536 finetune.py:68] layer 14_o @ epoch 2 new loss 4.489745697355829e-05 old loss 4.555738632916473e-05 BETTER
I0328 07:09:36.734564 2257011 finetune.py:68] layer 12_up @ epoch 0 new loss 6.407242472050712e-05 old loss 6.499616574728861e-05 BETTER
I0328 07:09:37.008322 2259305 finetune.py:68] layer 15_o @ epoch 1 new loss 4.560585512081161e-05 old loss 4.6487184590660036e-05 BETTER
I0328 07:09:39.548378 2257789 finetune.py:45] layer 13_up initial loss 7.627992454217747e-05
I0328 07:09:44.731074 2258536 finetune.py:68] layer 14_o @ epoch 3 new loss 4.4360906031215563e-05 old loss 4.489745697355829e-05 BETTER
I0328 07:10:10.053875 2257011 finetune.py:68] layer 12_up @ epoch 1 new loss 6.335754733299837e-05 old loss 6.407242472050712e-05 BETTER
I0328 07:10:10.175382 2257789 finetune.py:68] layer 13_up @ epoch 0 new loss 7.516548066632822e-05 old loss 7.627992454217747e-05 BETTER
I0328 07:10:10.444098 2259305 finetune.py:68] layer 15_o @ epoch 2 new loss 4.492574589676224e-05 old loss 4.560585512081161e-05 BETTER
I0328 07:10:18.830861 2258536 finetune.py:68] layer 14_o @ epoch 4 new loss 4.390471804072149e-05 old loss 4.4360906031215563e-05 BETTER
I0328 07:10:41.975692 2257789 finetune.py:68] layer 13_up @ epoch 1 new loss 7.432520214933902e-05 old loss 7.516548066632822e-05 BETTER
I0328 07:10:43.451620 2257011 finetune.py:68] layer 12_up @ epoch 2 new loss 6.272912287386134e-05 old loss 6.335754733299837e-05 BETTER
I0328 07:10:43.914143 2259305 finetune.py:68] layer 15_o @ epoch 3 new loss 4.437334428075701e-05 old loss 4.492574589676224e-05 BETTER
I0328 07:10:50.413640 2258536 finetune.py:45] layer 14_up initial loss 8.737054304219782e-05
I0328 07:11:13.957977 2257789 finetune.py:68] layer 13_up @ epoch 2 new loss 7.359351002378389e-05 old loss 7.432520214933902e-05 BETTER
I0328 07:11:16.996750 2257011 finetune.py:68] layer 12_up @ epoch 3 new loss 6.216680048964918e-05 old loss 6.272912287386134e-05 BETTER
I0328 07:11:17.513575 2259305 finetune.py:68] layer 15_o @ epoch 4 new loss 4.390178219182417e-05 old loss 4.437334428075701e-05 BETTER
I0328 07:11:21.068348 2258536 finetune.py:68] layer 14_up @ epoch 0 new loss 8.594064274802804e-05 old loss 8.737054304219782e-05 BETTER
I0328 07:11:46.139367 2257789 finetune.py:68] layer 13_up @ epoch 3 new loss 7.293586531886831e-05 old loss 7.359351002378389e-05 BETTER
I0328 07:11:48.552031 2259305 finetune.py:45] layer 15_up initial loss 9.583115024724975e-05
I0328 07:11:50.602297 2257011 finetune.py:68] layer 12_up @ epoch 4 new loss 6.164500518934801e-05 old loss 6.216680048964918e-05 BETTER
I0328 07:11:52.938925 2258536 finetune.py:68] layer 14_up @ epoch 1 new loss 8.483129931846634e-05 old loss 8.594064274802804e-05 BETTER
I0328 07:12:18.374785 2257789 finetune.py:68] layer 13_up @ epoch 4 new loss 7.233582437038422e-05 old loss 7.293586531886831e-05 BETTER
I0328 07:12:18.644905 2259305 finetune.py:68] layer 15_up @ epoch 0 new loss 9.399043483426794e-05 old loss 9.583115024724975e-05 BETTER
I0328 07:12:21.723763 2257011 finetune.py:45] layer 12_gate initial loss 7.524758257204667e-05
I0328 07:12:25.182003 2258536 finetune.py:68] layer 14_up @ epoch 2 new loss 8.387621346628293e-05 old loss 8.483129931846634e-05 BETTER
I0328 07:12:49.665447 2259305 finetune.py:68] layer 15_up @ epoch 1 new loss 9.259532816940919e-05 old loss 9.399043483426794e-05 BETTER
I0328 07:12:49.807561 2257789 finetune.py:45] layer 13_gate initial loss 8.730914851184934e-05
I0328 07:12:51.761609 2257011 finetune.py:68] layer 12_gate @ epoch 0 new loss 7.471338176401332e-05 old loss 7.524758257204667e-05 BETTER
I0328 07:12:57.276917 2258536 finetune.py:68] layer 14_up @ epoch 3 new loss 8.302708738483489e-05 old loss 8.387621346628293e-05 BETTER
I0328 07:13:18.220823 2257789 finetune.py:68] layer 13_gate @ epoch 0 new loss 8.66627597133629e-05 old loss 8.730914851184934e-05 BETTER
I0328 07:13:20.819095 2259305 finetune.py:68] layer 15_up @ epoch 2 new loss 9.140421752817929e-05 old loss 9.259532816940919e-05 BETTER
I0328 07:13:22.769786 2257011 finetune.py:68] layer 12_gate @ epoch 1 new loss 7.42520423955284e-05 old loss 7.471338176401332e-05 BETTER
I0328 07:13:29.482927 2258536 finetune.py:68] layer 14_up @ epoch 4 new loss 8.225427154684439e-05 old loss 8.302708738483489e-05 BETTER
I0328 07:13:47.508304 2257789 finetune.py:68] layer 13_gate @ epoch 1 new loss 8.611856901552528e-05 old loss 8.66627597133629e-05 BETTER
I0328 07:13:52.256023 2259305 finetune.py:68] layer 15_up @ epoch 3 new loss 9.03469481272623e-05 old loss 9.140421752817929e-05 BETTER
I0328 07:13:54.231945 2257011 finetune.py:68] layer 12_gate @ epoch 2 new loss 7.38316957722418e-05 old loss 7.42520423955284e-05 BETTER
I0328 07:14:01.153623 2258536 finetune.py:45] layer 14_gate initial loss 9.843688894761726e-05
I0328 07:14:17.238494 2257789 finetune.py:68] layer 13_gate @ epoch 2 new loss 8.562781295040622e-05 old loss 8.611856901552528e-05 BETTER
I0328 07:14:23.546513 2259305 finetune.py:68] layer 15_up @ epoch 4 new loss 8.938221435528249e-05 old loss 9.03469481272623e-05 BETTER
I0328 07:14:25.528419 2257011 finetune.py:68] layer 12_gate @ epoch 3 new loss 7.343824836425483e-05 old loss 7.38316957722418e-05 BETTER
I0328 07:14:29.611058 2258536 finetune.py:68] layer 14_gate @ epoch 0 new loss 9.76291557890363e-05 old loss 9.843688894761726e-05 BETTER
I0328 07:14:46.762753 2257789 finetune.py:68] layer 13_gate @ epoch 3 new loss 8.516611705999821e-05 old loss 8.562781295040622e-05 BETTER
I0328 07:14:54.810562 2259305 finetune.py:45] layer 15_gate initial loss 0.00010718595876824111
I0328 07:14:56.783332 2257011 finetune.py:68] layer 12_gate @ epoch 4 new loss 7.307171472348273e-05 old loss 7.343824836425483e-05 BETTER
I0328 07:14:59.004196 2258536 finetune.py:68] layer 14_gate @ epoch 1 new loss 9.694531763670966e-05 old loss 9.76291557890363e-05 BETTER
I0328 07:15:16.296416 2257789 finetune.py:68] layer 13_gate @ epoch 4 new loss 8.474271453451365e-05 old loss 8.516611705999821e-05 BETTER
I0328 07:15:23.065111 2259305 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.00010617975931381807 old loss 0.00010718595876824111 BETTER
I0328 07:15:28.674724 2258536 finetune.py:68] layer 14_gate @ epoch 2 new loss 9.633065201342106e-05 old loss 9.694531763670966e-05 BETTER
I0328 07:15:51.998954 2259305 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.00010529918654356152 old loss 0.00010617975931381807 BETTER
I0328 07:15:52.498949 2257011 finetune.py:45] layer 12_down initial loss 0.00010990899318130687
I0328 07:15:58.264460 2258536 finetune.py:68] layer 14_gate @ epoch 3 new loss 9.575778676662594e-05 old loss 9.633065201342106e-05 BETTER
I0328 07:16:13.167385 2257789 finetune.py:45] layer 13_down initial loss 0.0001286126353079453
I0328 07:16:19.963388 2257011 finetune.py:68] layer 12_down @ epoch 0 new loss 0.00010990144801326096 old loss 0.00010990899318130687 BETTER
I0328 07:16:21.212867 2259305 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.00010450797708472237 old loss 0.00010529918654356152 BETTER
I0328 07:16:28.018300 2258536 finetune.py:68] layer 14_gate @ epoch 4 new loss 9.522648178972304e-05 old loss 9.575778676662594e-05 BETTER
I0328 07:16:39.232976 2257789 finetune.py:68] layer 13_down @ epoch 0 new loss 0.00012860306014772505 old loss 0.0001286126353079453 BETTER
I0328 07:16:48.363462 2257011 finetune.py:68] layer 12_down @ epoch 1 new loss 0.00010989460861310363 old loss 0.00010990144801326096 BETTER
I0328 07:16:50.422217 2259305 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.00010378624574514106 old loss 0.00010450797708472237 BETTER
I0328 07:17:06.354760 2257789 finetune.py:68] layer 13_down @ epoch 1 new loss 0.00012859505659434944 old loss 0.00012860306014772505 BETTER
I0328 07:17:16.902636 2257011 finetune.py:68] layer 12_down @ epoch 2 new loss 0.00010988988651661202 old loss 0.00010989460861310363 BETTER
I0328 07:17:19.715890 2259305 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0001031200445140712 old loss 0.00010378624574514106 BETTER
I0328 07:17:24.620140 2258536 finetune.py:45] layer 14_down initial loss 0.00014802449732087553
I0328 07:17:33.802298 2257789 finetune.py:68] layer 13_down @ epoch 2 new loss 0.00012858853733632714 old loss 0.00012859505659434944 BETTER
I0328 07:17:45.877983 2257011 finetune.py:68] layer 12_down @ epoch 3 new loss 0.00010988584836013615 old loss 0.00010988988651661202 BETTER
I0328 07:17:50.846840 2258536 finetune.py:68] layer 14_down @ epoch 0 new loss 0.00014801285578869283 old loss 0.00014802449732087553 BETTER
I0328 07:18:01.385048 2257789 finetune.py:68] layer 13_down @ epoch 3 new loss 0.00012858315312769264 old loss 0.00012858853733632714 BETTER
I0328 07:18:14.784922 2257011 finetune.py:68] layer 12_down @ epoch 4 new loss 0.00010988268331857398 old loss 0.00010988584836013615 BETTER
I0328 07:18:16.644922 2259305 finetune.py:45] layer 15_down initial loss 0.0001700201100902632
12_v proxy err 0.01664651744067669 tr(WHW.T) 363.6233825683594
bpp_loss 2.808650058635976
12_q proxy err 0.0008104664157144725 tr(WHW.T) 34120.05859375
bpp_loss 3.52260085393209
12_k proxy err 0.0003384500159882009 tr(WHW.T) 23034.623046875
bpp_loss 4.407548951159697
12_o proxy err 0.018908711150288582 tr(WHW.T) 783.4699096679688
bpp_loss 2.879361288738437
12_up proxy err 0.011603987775743008 tr(WHW.T) 10022.76171875
bpp_loss 2.9856026132058884
12_gate proxy err 0.0032423939555883408 tr(WHW.T) 37262.8828125
bpp_loss 3.1784487875577594
12_down proxy err 0.015679046511650085 tr(WHW.T) 6844.74365234375
bpp_loss 2.981490448549656
I0328 07:18:18.301416 2258536 finetune.py:68] layer 14_down @ epoch 1 new loss 0.00014800389180891216 old loss 0.00014801285578869283 BETTER
I0328 07:18:29.292463 2257789 finetune.py:68] layer 13_down @ epoch 4 new loss 0.00012857804540544748 old loss 0.00012858315312769264 BETTER
13_v proxy err 0.020179985091090202 tr(WHW.T) 280.153564453125
bpp_loss 2.750709260813892
13_q proxy err 0.0012436974793672562 tr(WHW.T) 20902.234375
bpp_loss 3.4960732412873767
13_k proxy err 0.0004142488760408014 tr(WHW.T) 17779.755859375
bpp_loss 4.421139298414346
13_o proxy err 0.01995537057518959 tr(WHW.T) 677.6998291015625
bpp_loss 2.862031716736965
13_up proxy err 0.011388863436877728 tr(WHW.T) 10005.79296875
bpp_loss 2.989244517404586
13_gate proxy err 0.0030435388907790184 tr(WHW.T) 38885.296875
bpp_loss 3.183018565976194
13_down proxy err 0.016035115346312523 tr(WHW.T) 6579.7099609375
bpp_loss 2.981265084235929
I0328 07:18:42.667992 2259305 finetune.py:68] layer 15_down @ epoch 0 new loss 0.00017000814841594547 old loss 0.0001700201100902632 BETTER
I0328 07:18:46.395351 2258536 finetune.py:68] layer 14_down @ epoch 2 new loss 0.00014799540804233402 old loss 0.00014800389180891216 BETTER
I0328 07:19:09.638629 2259305 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00016999781655613333 old loss 0.00017000814841594547 BETTER
I0328 07:19:14.200847 2258536 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0001479883649153635 old loss 0.00014799540804233402 BETTER
I0328 07:19:36.710289 2259305 finetune.py:68] layer 15_down @ epoch 2 new loss 0.00016998934734147042 old loss 0.00016999781655613333 BETTER
I0328 07:19:41.975384 2258536 finetune.py:68] layer 14_down @ epoch 4 new loss 0.00014798234042245895 old loss 0.0001479883649153635 BETTER
I0328 07:19:42.435161 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 67.89321374893188s
14_v proxy err 0.01895088329911232 tr(WHW.T) 281.3382873535156
bpp_loss 2.7427224707207642
14_q proxy err 0.0011737202294170856 tr(WHW.T) 20888.65625
bpp_loss 3.466829349868931
14_k proxy err 0.00037280580727383494 tr(WHW.T) 18592.208984375
bpp_loss 4.370796607166994
14_o proxy err 0.02127280831336975 tr(WHW.T) 689.81982421875
bpp_loss 2.853245195292402
14_up proxy err 0.012372012250125408 tr(WHW.T) 9163.9208984375
bpp_loss 2.9837469160556793
14_gate proxy err 0.002835668157786131 tr(WHW.T) 41778.2109375
bpp_loss 3.21134483321969
14_down proxy err 0.016933318227529526 tr(WHW.T) 6413.53857421875
bpp_loss 2.9772764742208113
I0328 07:19:46.060440 2271926 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 07:19:46.060561 2271926 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 07:19:46.060603 2271926 utils.py:162] NumExpr defaulting to 16 threads.
I0328 07:19:46.405700 2271926 config.py:54] PyTorch version 2.6.0 available.
W0328 07:19:46.612839 2271926 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 07:19:47.263884 2271926 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 07:19:47.270866 2210624 quantize_finetune_llama.py:209] layer 17 gpu 1
I0328 07:19:47.284184 2271926 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 07:20:03.843127 2259305 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0001699824060779065 old loss 0.00016998934734147042 BETTER
I0328 07:20:04.602163 2271926 finetune.py:45] layer 16_v initial loss 3.520365498843603e-05
W0328 07:20:04.602587 2271926 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 07:20:31.115243 2259305 finetune.py:68] layer 15_down @ epoch 4 new loss 0.0001699763524811715 old loss 0.0001699824060779065 BETTER
15_v proxy err 0.020996762439608574 tr(WHW.T) 284.0271301269531
bpp_loss 2.80367081053555
15_q proxy err 0.0009789594914764166 tr(WHW.T) 28082.892578125
bpp_loss 3.5914265530300327
15_k proxy err 0.0004076051991432905 tr(WHW.T) 18847.58984375
bpp_loss 4.416324666410219
15_o proxy err 0.021823212504386902 tr(WHW.T) 829.6182861328125
bpp_loss 2.8824056061566807
15_up proxy err 0.0125961285084486 tr(WHW.T) 8995.396484375
bpp_loss 2.9770248232941543
15_gate proxy err 0.0025882197078317404 tr(WHW.T) 46110.6953125
bpp_loss 3.2474169936696335
15_down proxy err 0.017002206295728683 tr(WHW.T) 6427.40576171875
bpp_loss 2.971972863821845
I0328 07:20:39.510763 2271926 finetune.py:68] layer 16_v @ epoch 0 new loss 2.2356887711794116e-05 old loss 3.520365498843603e-05 BETTER
I0328 07:20:51.548616 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 63.81569576263428s
I0328 07:20:55.177619 2272698 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 07:20:55.177722 2272698 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 07:20:55.177762 2272698 utils.py:162] NumExpr defaulting to 16 threads.
I0328 07:20:55.501753 2272698 config.py:54] PyTorch version 2.6.0 available.
W0328 07:20:55.695684 2272698 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 07:20:56.278053 2272698 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 07:20:56.281646 2210624 quantize_finetune_llama.py:209] layer 18 gpu 2
I0328 07:20:56.295000 2272698 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 07:21:12.788264 2272698 finetune.py:45] layer 17_v initial loss 3.941136674256995e-05
W0328 07:21:12.788460 2272698 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 07:21:15.817462 2271926 finetune.py:68] layer 16_v @ epoch 1 new loss 2.090856469294522e-05 old loss 2.2356887711794116e-05 BETTER
I0328 07:21:46.104001 2272698 finetune.py:68] layer 17_v @ epoch 0 new loss 2.092186696245335e-05 old loss 3.941136674256995e-05 BETTER
I0328 07:21:52.686350 2271926 finetune.py:68] layer 16_v @ epoch 2 new loss 2.0092917111469433e-05 old loss 2.090856469294522e-05 BETTER
I0328 07:21:59.714753 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 62.98609113693237s
I0328 07:22:03.467982 2273424 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 07:22:03.468088 2273424 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 07:22:03.468131 2273424 utils.py:162] NumExpr defaulting to 16 threads.
I0328 07:22:03.821783 2273424 config.py:54] PyTorch version 2.6.0 available.
W0328 07:22:04.020062 2273424 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 07:22:04.617735 2273424 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 07:22:04.621772 2210624 quantize_finetune_llama.py:209] layer 19 gpu 3
I0328 07:22:04.636802 2273424 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 07:22:20.578817 2272698 finetune.py:68] layer 17_v @ epoch 1 new loss 1.9286437236587517e-05 old loss 2.092186696245335e-05 BETTER
I0328 07:22:21.644180 2273424 finetune.py:45] layer 18_v initial loss 4.662333958549425e-05
W0328 07:22:21.644370 2273424 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 07:22:29.635685 2271926 finetune.py:68] layer 16_v @ epoch 3 new loss 1.953049468284007e-05 old loss 2.0092917111469433e-05 BETTER
I0328 07:22:55.270855 2272698 finetune.py:68] layer 17_v @ epoch 2 new loss 1.8477445337339304e-05 old loss 1.9286437236587517e-05 BETTER
I0328 07:22:55.310679 2273424 finetune.py:68] layer 18_v @ epoch 0 new loss 1.72773034137208e-05 old loss 4.662333958549425e-05 BETTER
I0328 07:23:06.596501 2271926 finetune.py:68] layer 16_v @ epoch 4 new loss 1.9097269614576362e-05 old loss 1.953049468284007e-05 BETTER
I0328 07:23:08.139462 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 63.025962114334106s
I0328 07:23:11.971860 2274216 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 07:23:11.971961 2274216 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 07:23:11.971998 2274216 utils.py:162] NumExpr defaulting to 16 threads.
I0328 07:23:12.345524 2274216 config.py:54] PyTorch version 2.6.0 available.
W0328 07:23:12.551885 2274216 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 07:23:13.214963 2274216 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 07:23:13.218736 2210624 quantize_finetune_llama.py:209] layer 20 gpu 0
I0328 07:23:13.232243 2274216 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 07:23:25.898443 2271926 finetune.py:45] layer 16_q initial loss 2.2293887013802305e-05
I0328 07:23:29.824176 2273424 finetune.py:68] layer 18_v @ epoch 1 new loss 1.5655403331038542e-05 old loss 1.72773034137208e-05 BETTER
I0328 07:23:30.241788 2272698 finetune.py:68] layer 17_v @ epoch 3 new loss 1.797319964680355e-05 old loss 1.8477445337339304e-05 BETTER
I0328 07:23:30.643484 2274216 finetune.py:45] layer 19_v initial loss 5.390085789258592e-05
W0328 07:23:30.643888 2274216 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 07:24:01.324797 2271926 finetune.py:68] layer 16_q @ epoch 0 new loss 2.175170084228739e-05 old loss 2.2293887013802305e-05 BETTER
I0328 07:24:03.977324 2274216 finetune.py:68] layer 19_v @ epoch 0 new loss 1.8052222003461793e-05 old loss 5.390085789258592e-05 BETTER
I0328 07:24:05.000065 2273424 finetune.py:68] layer 18_v @ epoch 2 new loss 1.4953046047594398e-05 old loss 1.5655403331038542e-05 BETTER
I0328 07:24:05.445580 2272698 finetune.py:68] layer 17_v @ epoch 4 new loss 1.7607506379135884e-05 old loss 1.797319964680355e-05 BETTER
I0328 07:24:24.929339 2272698 finetune.py:45] layer 17_q initial loss 2.0390894860611297e-05
I0328 07:24:38.115401 2271926 finetune.py:68] layer 16_q @ epoch 1 new loss 2.1345076675061136e-05 old loss 2.175170084228739e-05 BETTER
I0328 07:24:38.535909 2274216 finetune.py:68] layer 19_v @ epoch 1 new loss 1.6222114936681464e-05 old loss 1.8052222003461793e-05 BETTER
I0328 07:24:40.339960 2273424 finetune.py:68] layer 18_v @ epoch 3 new loss 1.4521923731081188e-05 old loss 1.4953046047594398e-05 BETTER
I0328 07:24:58.430926 2272698 finetune.py:68] layer 17_q @ epoch 0 new loss 1.9928935216739774e-05 old loss 2.0390894860611297e-05 BETTER
I0328 07:25:12.979719 2274216 finetune.py:68] layer 19_v @ epoch 2 new loss 1.5499252185691148e-05 old loss 1.6222114936681464e-05 BETTER
I0328 07:25:14.720928 2271926 finetune.py:68] layer 16_q @ epoch 2 new loss 2.1028001356171444e-05 old loss 2.1345076675061136e-05 BETTER
I0328 07:25:15.606038 2273424 finetune.py:68] layer 18_v @ epoch 4 new loss 1.4212322639650665e-05 old loss 1.4521923731081188e-05 BETTER
I0328 07:25:32.738125 2272698 finetune.py:68] layer 17_q @ epoch 1 new loss 1.959663859452121e-05 old loss 1.9928935216739774e-05 BETTER
I0328 07:25:34.803561 2273424 finetune.py:45] layer 18_q initial loss 1.698391315585468e-05
I0328 07:25:47.802284 2274216 finetune.py:68] layer 19_v @ epoch 3 new loss 1.5061479643918574e-05 old loss 1.5499252185691148e-05 BETTER
I0328 07:25:51.317985 2271926 finetune.py:68] layer 16_q @ epoch 3 new loss 2.0747957023559138e-05 old loss 2.1028001356171444e-05 BETTER
I0328 07:26:07.315074 2272698 finetune.py:68] layer 17_q @ epoch 2 new loss 1.932931991177611e-05 old loss 1.959663859452121e-05 BETTER
I0328 07:26:08.456962 2273424 finetune.py:68] layer 18_q @ epoch 0 new loss 1.656823769735638e-05 old loss 1.698391315585468e-05 BETTER
I0328 07:26:22.567462 2274216 finetune.py:68] layer 19_v @ epoch 4 new loss 1.4775863746763207e-05 old loss 1.5061479643918574e-05 BETTER
I0328 07:26:28.112520 2271926 finetune.py:68] layer 16_q @ epoch 4 new loss 2.0523239072645083e-05 old loss 2.0747957023559138e-05 BETTER
I0328 07:26:41.744106 2274216 finetune.py:45] layer 19_q initial loss 1.716163933451753e-05
I0328 07:26:42.207777 2272698 finetune.py:68] layer 17_q @ epoch 3 new loss 1.9105213141301647e-05 old loss 1.932931991177611e-05 BETTER
I0328 07:26:43.080162 2273424 finetune.py:68] layer 18_q @ epoch 1 new loss 1.628291829547379e-05 old loss 1.656823769735638e-05 BETTER
I0328 07:26:46.483390 2271926 finetune.py:45] layer 16_k initial loss 2.1601150365313515e-05
I0328 07:27:14.791188 2274216 finetune.py:68] layer 19_q @ epoch 0 new loss 1.6763526218710467e-05 old loss 1.716163933451753e-05 BETTER
I0328 07:27:17.087035 2272698 finetune.py:68] layer 17_q @ epoch 4 new loss 1.8908844140241854e-05 old loss 1.9105213141301647e-05 BETTER
I0328 07:27:18.009600 2273424 finetune.py:68] layer 18_q @ epoch 2 new loss 1.6077299733296968e-05 old loss 1.628291829547379e-05 BETTER
I0328 07:27:21.570831 2271926 finetune.py:68] layer 16_k @ epoch 0 new loss 2.1315521735232323e-05 old loss 2.1601150365313515e-05 BETTER
I0328 07:27:34.798299 2272698 finetune.py:45] layer 17_k initial loss 1.9903851352864876e-05
I0328 07:27:49.021369 2274216 finetune.py:68] layer 19_q @ epoch 1 new loss 1.649374644330237e-05 old loss 1.6763526218710467e-05 BETTER
I0328 07:27:52.682778 2273424 finetune.py:68] layer 18_q @ epoch 3 new loss 1.5900639482424594e-05 old loss 1.6077299733296968e-05 BETTER
I0328 07:27:57.732452 2271926 finetune.py:68] layer 16_k @ epoch 1 new loss 2.112562106049154e-05 old loss 2.1315521735232323e-05 BETTER
I0328 07:28:08.341596 2272698 finetune.py:68] layer 17_k @ epoch 0 new loss 1.9691595298354514e-05 old loss 1.9903851352864876e-05 BETTER
I0328 07:28:23.028876 2274216 finetune.py:68] layer 19_q @ epoch 2 new loss 1.628782592888456e-05 old loss 1.649374644330237e-05 BETTER
I0328 07:28:27.454684 2273424 finetune.py:68] layer 18_q @ epoch 4 new loss 1.575696478539612e-05 old loss 1.5900639482424594e-05 BETTER
I0328 07:28:34.017101 2271926 finetune.py:68] layer 16_k @ epoch 2 new loss 2.0965730072930455e-05 old loss 2.112562106049154e-05 BETTER
I0328 07:28:42.500598 2272698 finetune.py:68] layer 17_k @ epoch 1 new loss 1.9532111764419824e-05 old loss 1.9691595298354514e-05 BETTER
I0328 07:28:45.564682 2273424 finetune.py:45] layer 18_k initial loss 1.658381734159775e-05
I0328 07:28:57.078249 2274216 finetune.py:68] layer 19_q @ epoch 3 new loss 1.611074731044937e-05 old loss 1.628782592888456e-05 BETTER
I0328 07:29:10.393331 2271926 finetune.py:68] layer 16_k @ epoch 3 new loss 2.0821542420890182e-05 old loss 2.0965730072930455e-05 BETTER
I0328 07:29:16.684272 2272698 finetune.py:68] layer 17_k @ epoch 2 new loss 1.940799666044768e-05 old loss 1.9532111764419824e-05 BETTER
I0328 07:29:18.987694 2273424 finetune.py:68] layer 18_k @ epoch 0 new loss 1.6395048078265972e-05 old loss 1.658381734159775e-05 BETTER
I0328 07:29:31.111132 2274216 finetune.py:68] layer 19_q @ epoch 4 new loss 1.5961788449203596e-05 old loss 1.611074731044937e-05 BETTER
I0328 07:29:46.745517 2271926 finetune.py:68] layer 16_k @ epoch 4 new loss 2.068078174488619e-05 old loss 2.0821542420890182e-05 BETTER
I0328 07:29:49.034492 2274216 finetune.py:45] layer 19_k initial loss 1.684836024651304e-05
I0328 07:29:51.084960 2272698 finetune.py:68] layer 17_k @ epoch 3 new loss 1.9289576812298037e-05 old loss 1.940799666044768e-05 BETTER
I0328 07:29:53.439409 2273424 finetune.py:68] layer 18_k @ epoch 1 new loss 1.6272322682198137e-05 old loss 1.6395048078265972e-05 BETTER
I0328 07:30:06.578208 2271926 finetune.py:45] layer 16_o initial loss 4.8560315917711705e-05
I0328 07:30:21.831949 2274216 finetune.py:68] layer 19_k @ epoch 0 new loss 1.670025994826574e-05 old loss 1.684836024651304e-05 BETTER
I0328 07:30:25.502330 2272698 finetune.py:68] layer 17_k @ epoch 4 new loss 1.920241629704833e-05 old loss 1.9289576812298037e-05 BETTER
I0328 07:30:27.823489 2273424 finetune.py:68] layer 18_k @ epoch 2 new loss 1.6165418855962344e-05 old loss 1.6272322682198137e-05 BETTER
I0328 07:30:40.936067 2271926 finetune.py:68] layer 16_o @ epoch 0 new loss 4.69551669084467e-05 old loss 4.8560315917711705e-05 BETTER
I0328 07:30:44.976999 2272698 finetune.py:45] layer 17_o initial loss 4.623926361091435e-05
I0328 07:30:55.583284 2274216 finetune.py:68] layer 19_k @ epoch 1 new loss 1.6583957403781824e-05 old loss 1.670025994826574e-05 BETTER
I0328 07:31:02.227907 2273424 finetune.py:68] layer 18_k @ epoch 3 new loss 1.607522244739812e-05 old loss 1.6165418855962344e-05 BETTER
I0328 07:31:16.563653 2271926 finetune.py:68] layer 16_o @ epoch 1 new loss 4.6111839765217155e-05 old loss 4.69551669084467e-05 BETTER
I0328 07:31:17.568967 2272698 finetune.py:68] layer 17_o @ epoch 0 new loss 4.4554704800248146e-05 old loss 4.623926361091435e-05 BETTER
I0328 07:31:29.519155 2274216 finetune.py:68] layer 19_k @ epoch 2 new loss 1.6494224837515503e-05 old loss 1.6583957403781824e-05 BETTER
I0328 07:31:36.756393 2273424 finetune.py:68] layer 18_k @ epoch 4 new loss 1.5986253856681287e-05 old loss 1.607522244739812e-05 BETTER
I0328 07:31:51.124299 2272698 finetune.py:68] layer 17_o @ epoch 1 new loss 4.3727923184633255e-05 old loss 4.4554704800248146e-05 BETTER
I0328 07:31:52.319102 2271926 finetune.py:68] layer 16_o @ epoch 2 new loss 4.547169737634249e-05 old loss 4.6111839765217155e-05 BETTER
I0328 07:31:56.105480 2273424 finetune.py:45] layer 18_o initial loss 3.8235906686168164e-05
I0328 07:32:03.370494 2274216 finetune.py:68] layer 19_k @ epoch 3 new loss 1.6409645468229428e-05 old loss 1.6494224837515503e-05 BETTER
I0328 07:32:25.234255 2272698 finetune.py:68] layer 17_o @ epoch 2 new loss 4.31059961556457e-05 old loss 4.3727923184633255e-05 BETTER
I0328 07:32:28.155882 2271926 finetune.py:68] layer 16_o @ epoch 3 new loss 4.4935932237422094e-05 old loss 4.547169737634249e-05 BETTER
I0328 07:32:28.949484 2273424 finetune.py:68] layer 18_o @ epoch 0 new loss 3.6453882785281166e-05 old loss 3.8235906686168164e-05 BETTER
I0328 07:32:37.466562 2274216 finetune.py:68] layer 19_k @ epoch 4 new loss 1.6338512068614364e-05 old loss 1.6409645468229428e-05 BETTER
I0328 07:32:56.805544 2274216 finetune.py:45] layer 19_o initial loss 3.657193519757129e-05
I0328 07:32:59.262634 2272698 finetune.py:68] layer 17_o @ epoch 3 new loss 4.260469722794369e-05 old loss 4.31059961556457e-05 BETTER
I0328 07:33:02.724853 2273424 finetune.py:68] layer 18_o @ epoch 1 new loss 3.590113919926807e-05 old loss 3.6453882785281166e-05 BETTER
I0328 07:33:04.002957 2271926 finetune.py:68] layer 16_o @ epoch 4 new loss 4.44881989096757e-05 old loss 4.4935932237422094e-05 BETTER
I0328 07:33:29.054920 2274216 finetune.py:68] layer 19_o @ epoch 0 new loss 3.492968971841037e-05 old loss 3.657193519757129e-05 BETTER
I0328 07:33:33.291002 2272698 finetune.py:68] layer 17_o @ epoch 4 new loss 4.2175041016889736e-05 old loss 4.260469722794369e-05 BETTER
I0328 07:33:35.091386 2271926 finetune.py:45] layer 16_up initial loss 0.00010072463192045689
I0328 07:33:36.811411 2273424 finetune.py:68] layer 18_o @ epoch 2 new loss 3.5506400308804587e-05 old loss 3.590113919926807e-05 BETTER
I0328 07:34:02.342324 2274216 finetune.py:68] layer 19_o @ epoch 1 new loss 3.443313835305162e-05 old loss 3.492968971841037e-05 BETTER
I0328 07:34:04.676000 2272698 finetune.py:45] layer 17_up initial loss 0.00010601586109260097
I0328 07:34:07.383608 2271926 finetune.py:68] layer 16_up @ epoch 0 new loss 9.887580381473526e-05 old loss 0.00010072463192045689 BETTER
I0328 07:34:10.882151 2273424 finetune.py:68] layer 18_o @ epoch 3 new loss 3.518226003507152e-05 old loss 3.5506400308804587e-05 BETTER
I0328 07:34:35.228533 2272698 finetune.py:68] layer 17_up @ epoch 0 new loss 0.00010398453014204279 old loss 0.00010601586109260097 BETTER
I0328 07:34:35.800050 2274216 finetune.py:68] layer 19_o @ epoch 2 new loss 3.408363409107551e-05 old loss 3.443313835305162e-05 BETTER
I0328 07:34:40.515785 2271926 finetune.py:68] layer 16_up @ epoch 1 new loss 9.749962919158861e-05 old loss 9.887580381473526e-05 BETTER
I0328 07:34:44.855271 2273424 finetune.py:68] layer 18_o @ epoch 4 new loss 3.4926848456962034e-05 old loss 3.518226003507152e-05 BETTER
I0328 07:35:06.856069 2272698 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0001024575685732998 old loss 0.00010398453014204279 BETTER
I0328 07:35:09.256312 2274216 finetune.py:68] layer 19_o @ epoch 3 new loss 3.379632835276425e-05 old loss 3.408363409107551e-05 BETTER
I0328 07:35:13.833359 2271926 finetune.py:68] layer 16_up @ epoch 2 new loss 9.631244756747037e-05 old loss 9.749962919158861e-05 BETTER
I0328 07:35:16.162160 2273424 finetune.py:45] layer 18_up initial loss 0.00010088820272358134
I0328 07:35:38.796856 2272698 finetune.py:68] layer 17_up @ epoch 2 new loss 0.00010117587225977331 old loss 0.0001024575685732998 BETTER
I0328 07:35:42.696048 2274216 finetune.py:68] layer 19_o @ epoch 4 new loss 3.35652512148954e-05 old loss 3.379632835276425e-05 BETTER
I0328 07:35:46.806698 2273424 finetune.py:68] layer 18_up @ epoch 0 new loss 9.903436148306355e-05 old loss 0.00010088820272358134 BETTER
I0328 07:35:47.328917 2271926 finetune.py:68] layer 16_up @ epoch 3 new loss 9.526315261609852e-05 old loss 9.631244756747037e-05 BETTER
I0328 07:36:10.890582 2272698 finetune.py:68] layer 17_up @ epoch 3 new loss 0.0001000406700768508 old loss 0.00010117587225977331 BETTER
I0328 07:36:14.041912 2274216 finetune.py:45] layer 19_up initial loss 0.00010511446453165263
I0328 07:36:18.804226 2273424 finetune.py:68] layer 18_up @ epoch 1 new loss 9.769145253812894e-05 old loss 9.903436148306355e-05 BETTER
I0328 07:36:20.835996 2271926 finetune.py:68] layer 16_up @ epoch 4 new loss 9.431307262275368e-05 old loss 9.526315261609852e-05 BETTER
I0328 07:36:42.970136 2272698 finetune.py:68] layer 17_up @ epoch 4 new loss 9.902450983645394e-05 old loss 0.0001000406700768508 BETTER
I0328 07:36:44.192434 2274216 finetune.py:68] layer 19_up @ epoch 0 new loss 0.00010327380005037412 old loss 0.00010511446453165263 BETTER
I0328 07:36:50.935266 2273424 finetune.py:68] layer 18_up @ epoch 2 new loss 9.657708142185584e-05 old loss 9.769145253812894e-05 BETTER
I0328 07:36:51.925903 2271926 finetune.py:45] layer 16_gate initial loss 0.00011360363714629784
I0328 07:37:14.314059 2272698 finetune.py:45] layer 17_gate initial loss 0.0001212089991895482
I0328 07:37:15.329454 2274216 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0001019358096527867 old loss 0.00010327380005037412 BETTER
I0328 07:37:21.859728 2271926 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.00011260702012805268 old loss 0.00011360363714629784 BETTER
I0328 07:37:23.138573 2273424 finetune.py:68] layer 18_up @ epoch 3 new loss 9.557879820931703e-05 old loss 9.657708142185584e-05 BETTER
I0328 07:37:42.649329 2272698 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.00012009665806544945 old loss 0.0001212089991895482 BETTER
I0328 07:37:46.416207 2274216 finetune.py:68] layer 19_up @ epoch 2 new loss 0.00010079669300466776 old loss 0.0001019358096527867 BETTER
I0328 07:37:52.845446 2271926 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.00011175246618222445 old loss 0.00011260702012805268 BETTER
I0328 07:37:55.440416 2273424 finetune.py:68] layer 18_up @ epoch 4 new loss 9.469911310588941e-05 old loss 9.557879820931703e-05 BETTER
I0328 07:38:12.124453 2272698 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.00011916639050468802 old loss 0.00012009665806544945 BETTER
I0328 07:38:17.749200 2274216 finetune.py:68] layer 19_up @ epoch 3 new loss 9.980723552871495e-05 old loss 0.00010079669300466776 BETTER
I0328 07:38:24.126844 2271926 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.00011097944661742076 old loss 0.00011175246618222445 BETTER
I0328 07:38:26.961656 2273424 finetune.py:45] layer 18_gate initial loss 0.00011873811308760196
I0328 07:38:41.473261 2272698 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.00011832873133243993 old loss 0.00011916639050468802 BETTER
I0328 07:38:49.003271 2274216 finetune.py:68] layer 19_up @ epoch 4 new loss 9.892058733385056e-05 old loss 9.980723552871495e-05 BETTER
I0328 07:38:55.423265 2271926 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.00011027340224245563 old loss 0.00011097944661742076 BETTER
I0328 07:38:55.461288 2273424 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.00011776277096942067 old loss 0.00011873811308760196 BETTER
I0328 07:39:11.214248 2272698 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.00011755893501685932 old loss 0.00011832873133243993 BETTER
I0328 07:39:20.429106 2274216 finetune.py:45] layer 19_gate initial loss 0.00012530530511867255
I0328 07:39:24.925590 2273424 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.00011695585999405012 old loss 0.00011776277096942067 BETTER
I0328 07:39:26.560613 2271926 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.00010962570377159864 old loss 0.00011027340224245563 BETTER
I0328 07:39:40.836106 2272698 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.00011685927165672183 old loss 0.00011755893501685932 BETTER
I0328 07:39:48.394984 2274216 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.00012434879317879677 old loss 0.00012530530511867255 BETTER
I0328 07:39:54.687967 2273424 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.00011622579768300056 old loss 0.00011695585999405012 BETTER
I0328 07:40:17.450060 2274216 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.00012354367936495692 old loss 0.00012434879317879677 BETTER
I0328 07:40:22.522482 2271926 finetune.py:45] layer 16_down initial loss 0.00018368673045188189
I0328 07:40:24.815666 2273424 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.00011556782555999234 old loss 0.00011622579768300056 BETTER
I0328 07:40:37.806088 2272698 finetune.py:45] layer 17_down initial loss 0.00020674322149716318
I0328 07:40:46.992619 2274216 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.00012280605733394623 old loss 0.00012354367936495692 BETTER
I0328 07:40:50.046412 2271926 finetune.py:68] layer 16_down @ epoch 0 new loss 0.00018367353186476976 old loss 0.00018368673045188189 BETTER
I0328 07:40:54.664213 2273424 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.00011495882790768519 old loss 0.00011556782555999234 BETTER
I0328 07:41:03.989607 2272698 finetune.py:68] layer 17_down @ epoch 0 new loss 0.00020672875689342618 old loss 0.00020674322149716318 BETTER
I0328 07:41:16.685138 2274216 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.00012214994058012962 old loss 0.00012280605733394623 BETTER
I0328 07:41:18.388910 2271926 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0001836634473875165 old loss 0.00018367353186476976 BETTER
I0328 07:41:31.284262 2272698 finetune.py:68] layer 17_down @ epoch 1 new loss 0.00020671759557444602 old loss 0.00020672875689342618 BETTER
I0328 07:41:46.945031 2274216 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.00012154989235568792 old loss 0.00012214994058012962 BETTER
I0328 07:41:47.692663 2271926 finetune.py:68] layer 16_down @ epoch 2 new loss 0.00018365512369200587 old loss 0.0001836634473875165 BETTER
I0328 07:41:54.518051 2273424 finetune.py:45] layer 18_down initial loss 0.0002062845160253346
I0328 07:41:58.780278 2272698 finetune.py:68] layer 17_down @ epoch 2 new loss 0.000206708864425309 old loss 0.00020671759557444602 BETTER
I0328 07:42:16.332736 2271926 finetune.py:68] layer 16_down @ epoch 3 new loss 0.00018364829884376377 old loss 0.00018365512369200587 BETTER
I0328 07:42:20.762634 2273424 finetune.py:68] layer 18_down @ epoch 0 new loss 0.00020627175399567932 old loss 0.0002062845160253346 BETTER
I0328 07:42:26.506123 2272698 finetune.py:68] layer 17_down @ epoch 3 new loss 0.00020670129742939025 old loss 0.000206708864425309 BETTER
I0328 07:42:46.164151 2271926 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0001836423616623506 old loss 0.00018364829884376377 BETTER
I0328 07:42:47.829534 2274216 finetune.py:45] layer 19_down initial loss 0.00021760114759672433
16_v proxy err 0.019607577472925186 tr(WHW.T) 274.28167724609375
bpp_loss 2.762677094549872
16_q proxy err 0.0010197191732004285 tr(WHW.T) 24485.791015625
bpp_loss 3.5695474426611327
16_k proxy err 0.0003581165801733732 tr(WHW.T) 19478.259765625
bpp_loss 4.407738692127168
16_o proxy err 0.018071532249450684 tr(WHW.T) 971.7817993164062
bpp_loss 2.8649309155298397
16_up proxy err 0.01421105396002531 tr(WHW.T) 8332.07421875
bpp_loss 2.9649454118417844
16_gate proxy err 0.0030648803804069757 tr(WHW.T) 41100.5390625
bpp_loss 3.2794375136228546
16_down proxy err 0.017771342769265175 tr(WHW.T) 6316.8388671875
bpp_loss 2.9581414122367278
I0328 07:42:49.566051 2273424 finetune.py:68] layer 18_down @ epoch 1 new loss 0.00020626130572054535 old loss 0.00020627175399567932 BETTER
I0328 07:42:54.947567 2272698 finetune.py:68] layer 17_down @ epoch 4 new loss 0.00020669522928074002 old loss 0.00020670129742939025 BETTER
17_v proxy err 0.02154354192316532 tr(WHW.T) 283.9730224609375
bpp_loss 2.8298590604099445
17_q proxy err 0.001012901309877634 tr(WHW.T) 27575.482421875
bpp_loss 3.5834003139752895
17_k proxy err 0.00044993640040047467 tr(WHW.T) 17405.794921875
bpp_loss 4.434258907858748
17_o proxy err 0.01970778964459896 tr(WHW.T) 1109.4962158203125
bpp_loss 2.8926376137533225
17_up proxy err 0.014024706557393074 tr(WHW.T) 8453.1142578125
bpp_loss 2.962996006477624
17_gate proxy err 0.0030411656480282545 tr(WHW.T) 41625.5859375
bpp_loss 3.2926811818033457
17_down proxy err 0.01802627556025982 tr(WHW.T) 6241.455078125
bpp_loss 2.9542647586037805
I0328 07:43:14.107223 2274216 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0002175886183977127 old loss 0.00021760114759672433 BETTER
I0328 07:43:17.437520 2273424 finetune.py:68] layer 18_down @ epoch 2 new loss 0.00020625334582291543 old loss 0.00020626130572054535 BETTER
I0328 07:43:41.224565 2274216 finetune.py:68] layer 19_down @ epoch 1 new loss 0.00021757889771834016 old loss 0.0002175886183977127 BETTER
I0328 07:43:45.239052 2273424 finetune.py:68] layer 18_down @ epoch 3 new loss 0.00020624601165764034 old loss 0.00020625334582291543 BETTER
I0328 07:44:07.194816 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 67.0434467792511s
I0328 07:44:08.376215 2274216 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0002175711269956082 old loss 0.00021757889771834016 BETTER
I0328 07:44:11.231017 2286844 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 07:44:11.231147 2286844 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 07:44:11.231195 2286844 utils.py:162] NumExpr defaulting to 16 threads.
I0328 07:44:11.629817 2286844 config.py:54] PyTorch version 2.6.0 available.
W0328 07:44:11.847057 2286844 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 07:44:12.497614 2286844 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 07:44:12.501992 2210624 quantize_finetune_llama.py:209] layer 21 gpu 1
I0328 07:44:12.517297 2286844 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 07:44:13.079035 2273424 finetune.py:68] layer 18_down @ epoch 4 new loss 0.00020624062744900584 old loss 0.00020624601165764034 BETTER
18_v proxy err 0.021434366703033447 tr(WHW.T) 287.61376953125
bpp_loss 2.757447583542671
18_q proxy err 0.0012837655376642942 tr(WHW.T) 22407.822265625
bpp_loss 3.5826703253551386
18_k proxy err 0.000466583005618304 tr(WHW.T) 17379.05078125
bpp_loss 4.506620774162002
18_o proxy err 0.019429422914981842 tr(WHW.T) 1206.357666015625
bpp_loss 2.8731371891335584
18_up proxy err 0.015444200485944748 tr(WHW.T) 7986.13330078125
bpp_loss 2.9598093083394423
18_gate proxy err 0.0037551578134298325 tr(WHW.T) 35168.28125
bpp_loss 3.300946349810277
18_down proxy err 0.018554512411355972 tr(WHW.T) 6257.70556640625
bpp_loss 2.9539488496292114
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 07:44:29.580652 2286844 finetune.py:45] layer 20_v initial loss 5.0709862989606336e-05
W0328 07:44:29.580859 2286844 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 07:44:35.531710 2274216 finetune.py:68] layer 19_down @ epoch 3 new loss 0.00021756472415290773 old loss 0.0002175711269956082 BETTER
I0328 07:45:02.726264 2274216 finetune.py:68] layer 19_down @ epoch 4 new loss 0.00021755929628852755 old loss 0.00021756472415290773 BETTER
I0328 07:45:04.437053 2286844 finetune.py:68] layer 20_v @ epoch 0 new loss 1.898927985166665e-05 old loss 5.0709862989606336e-05 BETTER
19_v proxy err 0.018849609419703484 tr(WHW.T) 341.0596618652344
bpp_loss 2.8019597760867327
19_q proxy err 0.0012340667890384793 tr(WHW.T) 24048.443359375
bpp_loss 3.5854620840982534
19_k proxy err 0.0005323427612893283 tr(WHW.T) 15535.8857421875
bpp_loss 4.418021897959989
19_o proxy err 0.020134130492806435 tr(WHW.T) 1171.2052001953125
bpp_loss 2.885911384189967
19_up proxy err 0.01637580618262291 tr(WHW.T) 7651.44921875
bpp_loss 2.956605505131717
19_gate proxy err 0.004109227564185858 tr(WHW.T) 32743.283203125
bpp_loss 3.311377016561372
19_down proxy err 0.01877427287399769 tr(WHW.T) 6210.61279296875
bpp_loss 2.9521186703849316
I0328 07:45:21.850766 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 63.94080114364624s
I0328 07:45:25.464154 2287659 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 07:45:25.464261 2287659 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 07:45:25.464304 2287659 utils.py:162] NumExpr defaulting to 16 threads.
I0328 07:45:25.794478 2287659 config.py:54] PyTorch version 2.6.0 available.
W0328 07:45:25.994557 2287659 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 07:45:26.589969 2287659 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 07:45:26.593573 2210624 quantize_finetune_llama.py:209] layer 22 gpu 2
I0328 07:45:26.613795 2287659 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 07:45:40.765683 2286844 finetune.py:68] layer 20_v @ epoch 1 new loss 1.7284426576225087e-05 old loss 1.898927985166665e-05 BETTER
I0328 07:45:43.089078 2287659 finetune.py:45] layer 21_v initial loss 5.4378186177928e-05
W0328 07:45:43.089308 2287659 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 07:46:16.167511 2287659 finetune.py:68] layer 21_v @ epoch 0 new loss 2.287819006596692e-05 old loss 5.4378186177928e-05 BETTER
I0328 07:46:17.710568 2286844 finetune.py:68] layer 20_v @ epoch 2 new loss 1.6579131624894217e-05 old loss 1.7284426576225087e-05 BETTER
I0328 07:46:30.899917 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 63.829106092453s
I0328 07:46:34.650423 2288413 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 07:46:34.650534 2288413 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 07:46:34.650578 2288413 utils.py:162] NumExpr defaulting to 16 threads.
I0328 07:46:35.012633 2288413 config.py:54] PyTorch version 2.6.0 available.
W0328 07:46:35.227035 2288413 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 07:46:35.826459 2288413 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 07:46:35.830387 2210624 quantize_finetune_llama.py:209] layer 23 gpu 3
I0328 07:46:35.844006 2288413 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 07:46:50.615488 2287659 finetune.py:68] layer 21_v @ epoch 1 new loss 2.082938408420887e-05 old loss 2.287819006596692e-05 BETTER
I0328 07:46:52.815170 2288413 finetune.py:45] layer 22_v initial loss 5.8747151342686266e-05
W0328 07:46:52.815587 2288413 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 07:46:54.595994 2286844 finetune.py:68] layer 20_v @ epoch 3 new loss 1.6135312762344256e-05 old loss 1.6579131624894217e-05 BETTER
I0328 07:47:25.350743 2287659 finetune.py:68] layer 21_v @ epoch 2 new loss 1.9886778318323195e-05 old loss 2.082938408420887e-05 BETTER
I0328 07:47:26.396617 2288413 finetune.py:68] layer 22_v @ epoch 0 new loss 1.8807242668117397e-05 old loss 5.8747151342686266e-05 BETTER
I0328 07:47:31.643927 2286844 finetune.py:68] layer 20_v @ epoch 4 new loss 1.5810190234333277e-05 old loss 1.6135312762344256e-05 BETTER
I0328 07:47:39.082150 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 62.74582123756409s
I0328 07:47:42.836022 2289175 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 07:47:42.836156 2289175 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 07:47:42.836204 2289175 utils.py:162] NumExpr defaulting to 16 threads.
I0328 07:47:43.169939 2289175 config.py:54] PyTorch version 2.6.0 available.
W0328 07:47:43.381729 2289175 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 07:47:44.065509 2289175 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 07:47:44.069511 2210624 quantize_finetune_llama.py:209] layer 24 gpu 0
I0328 07:47:44.100414 2289175 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 07:47:50.960805 2286844 finetune.py:45] layer 20_q initial loss 1.86022025445709e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 07:48:00.375782 2287659 finetune.py:68] layer 21_v @ epoch 3 new loss 1.9276336388429627e-05 old loss 1.9886778318323195e-05 BETTER
I0328 07:48:00.988277 2288413 finetune.py:68] layer 22_v @ epoch 1 new loss 1.6761858205427416e-05 old loss 1.8807242668117397e-05 BETTER
I0328 07:48:01.119032 2289175 finetune.py:45] layer 23_v initial loss 7.140578236430883e-05
W0328 07:48:01.119467 2289175 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 07:48:26.359763 2286844 finetune.py:68] layer 20_q @ epoch 0 new loss 1.8162145352107473e-05 old loss 1.86022025445709e-05 BETTER
I0328 07:48:34.210181 2289175 finetune.py:68] layer 23_v @ epoch 0 new loss 1.975266968656797e-05 old loss 7.140578236430883e-05 BETTER
I0328 07:48:35.506091 2287659 finetune.py:68] layer 21_v @ epoch 4 new loss 1.881071148090996e-05 old loss 1.9276336388429627e-05 BETTER
I0328 07:48:35.938906 2288413 finetune.py:68] layer 22_v @ epoch 2 new loss 1.598253038537223e-05 old loss 1.6761858205427416e-05 BETTER
I0328 07:48:54.739347 2287659 finetune.py:45] layer 21_q initial loss 2.3288021111511625e-05
I0328 07:49:02.611530 2286844 finetune.py:68] layer 20_q @ epoch 1 new loss 1.785002496035304e-05 old loss 1.8162145352107473e-05 BETTER
I0328 07:49:08.548304 2289175 finetune.py:68] layer 23_v @ epoch 1 new loss 1.710143442323897e-05 old loss 1.975266968656797e-05 BETTER
I0328 07:49:10.834794 2288413 finetune.py:68] layer 22_v @ epoch 3 new loss 1.5512994650634937e-05 old loss 1.598253038537223e-05 BETTER
I0328 07:49:28.032852 2287659 finetune.py:68] layer 21_q @ epoch 0 new loss 2.258224594697822e-05 old loss 2.3288021111511625e-05 BETTER
I0328 07:49:38.875056 2286844 finetune.py:68] layer 20_q @ epoch 2 new loss 1.7605254470254295e-05 old loss 1.785002496035304e-05 BETTER
I0328 07:49:42.993997 2289175 finetune.py:68] layer 23_v @ epoch 2 new loss 1.6225490981014445e-05 old loss 1.710143442323897e-05 BETTER
I0328 07:49:45.789329 2288413 finetune.py:68] layer 22_v @ epoch 4 new loss 1.5200105735857505e-05 old loss 1.5512994650634937e-05 BETTER
I0328 07:50:02.211748 2287659 finetune.py:68] layer 21_q @ epoch 1 new loss 2.2109894416644238e-05 old loss 2.258224594697822e-05 BETTER
I0328 07:50:05.166171 2288413 finetune.py:45] layer 22_q initial loss 1.8944096154882573e-05
I0328 07:50:15.344931 2286844 finetune.py:68] layer 20_q @ epoch 3 new loss 1.7399715943611227e-05 old loss 1.7605254470254295e-05 BETTER
I0328 07:50:17.424196 2289175 finetune.py:68] layer 23_v @ epoch 3 new loss 1.5737210560473613e-05 old loss 1.6225490981014445e-05 BETTER
I0328 07:50:36.635926 2287659 finetune.py:68] layer 21_q @ epoch 2 new loss 2.1749245206592605e-05 old loss 2.2109894416644238e-05 BETTER
I0328 07:50:38.684975 2288413 finetune.py:68] layer 22_q @ epoch 0 new loss 1.8437323888065293e-05 old loss 1.8944096154882573e-05 BETTER
I0328 07:50:51.797160 2286844 finetune.py:68] layer 20_q @ epoch 4 new loss 1.7231586753041483e-05 old loss 1.7399715943611227e-05 BETTER
I0328 07:50:51.909691 2289175 finetune.py:68] layer 23_v @ epoch 4 new loss 1.5406321836053394e-05 old loss 1.5737210560473613e-05 BETTER
I0328 07:51:10.079302 2286844 finetune.py:45] layer 20_k initial loss 1.8157330487156287e-05
I0328 07:51:11.260308 2287659 finetune.py:68] layer 21_q @ epoch 3 new loss 2.141737968486268e-05 old loss 2.1749245206592605e-05 BETTER
I0328 07:51:11.801459 2289175 finetune.py:45] layer 23_q initial loss 1.883353979792446e-05
I0328 07:51:13.268182 2288413 finetune.py:68] layer 22_q @ epoch 1 new loss 1.8115326383849606e-05 old loss 1.8437323888065293e-05 BETTER
I0328 07:51:44.966834 2289175 finetune.py:68] layer 23_q @ epoch 0 new loss 1.8363765775575303e-05 old loss 1.883353979792446e-05 BETTER
I0328 07:51:45.130559 2286844 finetune.py:68] layer 20_k @ epoch 0 new loss 1.795499883883167e-05 old loss 1.8157330487156287e-05 BETTER
I0328 07:51:46.287432 2287659 finetune.py:68] layer 21_q @ epoch 4 new loss 2.115482129738666e-05 old loss 2.141737968486268e-05 BETTER
I0328 07:51:47.900423 2288413 finetune.py:68] layer 22_q @ epoch 2 new loss 1.7874333934742026e-05 old loss 1.8115326383849606e-05 BETTER
I0328 07:52:03.821371 2287659 finetune.py:45] layer 21_k initial loss 2.3189122657640837e-05
I0328 07:52:18.949978 2289175 finetune.py:68] layer 23_q @ epoch 1 new loss 1.805258943932131e-05 old loss 1.8363765775575303e-05 BETTER
I0328 07:52:21.451533 2286844 finetune.py:68] layer 20_k @ epoch 1 new loss 1.7821535948314704e-05 old loss 1.795499883883167e-05 BETTER
I0328 07:52:22.541479 2288413 finetune.py:68] layer 22_q @ epoch 3 new loss 1.767747016856447e-05 old loss 1.7874333934742026e-05 BETTER
I0328 07:52:37.120254 2287659 finetune.py:68] layer 21_k @ epoch 0 new loss 2.2585429178434424e-05 old loss 2.3189122657640837e-05 BETTER
I0328 07:52:52.977606 2289175 finetune.py:68] layer 23_q @ epoch 2 new loss 1.7815837054513395e-05 old loss 1.805258943932131e-05 BETTER
I0328 07:52:57.223604 2288413 finetune.py:68] layer 22_q @ epoch 4 new loss 1.7512968042865396e-05 old loss 1.767747016856447e-05 BETTER
I0328 07:52:57.904602 2286844 finetune.py:68] layer 20_k @ epoch 2 new loss 1.769945265550632e-05 old loss 1.7821535948314704e-05 BETTER
I0328 07:53:11.322124 2287659 finetune.py:68] layer 21_k @ epoch 1 new loss 2.2366335542756133e-05 old loss 2.2585429178434424e-05 BETTER
I0328 07:53:14.949889 2288413 finetune.py:45] layer 22_k initial loss 1.8785087377182208e-05
I0328 07:53:27.355096 2289175 finetune.py:68] layer 23_q @ epoch 3 new loss 1.763060390658211e-05 old loss 1.7815837054513395e-05 BETTER
I0328 07:53:34.367433 2286844 finetune.py:68] layer 20_k @ epoch 3 new loss 1.7601823856239207e-05 old loss 1.769945265550632e-05 BETTER
I0328 07:53:45.658216 2287659 finetune.py:68] layer 21_k @ epoch 2 new loss 2.2183701730682515e-05 old loss 2.2366335542756133e-05 BETTER
I0328 07:53:48.413172 2288413 finetune.py:68] layer 22_k @ epoch 0 new loss 1.8605791410664096e-05 old loss 1.8785087377182208e-05 BETTER
I0328 07:54:01.650867 2289175 finetune.py:68] layer 23_q @ epoch 4 new loss 1.747860551404301e-05 old loss 1.763060390658211e-05 BETTER
I0328 07:54:10.878941 2286844 finetune.py:68] layer 20_k @ epoch 4 new loss 1.7515305444248952e-05 old loss 1.7601823856239207e-05 BETTER
I0328 07:54:19.245138 2289175 finetune.py:45] layer 23_k initial loss 1.8978927982971072e-05
I0328 07:54:20.030382 2287659 finetune.py:68] layer 21_k @ epoch 3 new loss 2.2017240553395823e-05 old loss 2.2183701730682515e-05 BETTER
I0328 07:54:22.850776 2288413 finetune.py:68] layer 22_k @ epoch 1 new loss 1.848149076977279e-05 old loss 1.8605791410664096e-05 BETTER
I0328 07:54:30.388446 2286844 finetune.py:45] layer 20_o initial loss 3.948627636418678e-05
I0328 07:54:52.324901 2289175 finetune.py:68] layer 23_k @ epoch 0 new loss 1.881526804936584e-05 old loss 1.8978927982971072e-05 BETTER
I0328 07:54:54.549317 2287659 finetune.py:68] layer 21_k @ epoch 4 new loss 2.1898904378758743e-05 old loss 2.2017240553395823e-05 BETTER
I0328 07:54:57.311027 2288413 finetune.py:68] layer 22_k @ epoch 2 new loss 1.837557829276193e-05 old loss 1.848149076977279e-05 BETTER
I0328 07:55:05.032389 2286844 finetune.py:68] layer 20_o @ epoch 0 new loss 3.779758480959572e-05 old loss 3.948627636418678e-05 BETTER
I0328 07:55:13.922533 2287659 finetune.py:45] layer 21_o initial loss 4.9496913561597466e-05
I0328 07:55:26.469919 2289175 finetune.py:68] layer 23_k @ epoch 1 new loss 1.8689408534555696e-05 old loss 1.881526804936584e-05 BETTER
I0328 07:55:31.827412 2288413 finetune.py:68] layer 22_k @ epoch 3 new loss 1.8282613382325508e-05 old loss 1.837557829276193e-05 BETTER
I0328 07:55:40.765932 2286844 finetune.py:68] layer 20_o @ epoch 1 new loss 3.7231759051792324e-05 old loss 3.779758480959572e-05 BETTER
I0328 07:55:46.608844 2287659 finetune.py:68] layer 21_o @ epoch 0 new loss 4.739328142022714e-05 old loss 4.9496913561597466e-05 BETTER
I0328 07:56:00.457858 2289175 finetune.py:68] layer 23_k @ epoch 2 new loss 1.8578257368062623e-05 old loss 1.8689408534555696e-05 BETTER
I0328 07:56:06.420222 2288413 finetune.py:68] layer 22_k @ epoch 4 new loss 1.8203776562586427e-05 old loss 1.8282613382325508e-05 BETTER
I0328 07:56:16.559197 2286844 finetune.py:68] layer 20_o @ epoch 2 new loss 3.682534952531569e-05 old loss 3.7231759051792324e-05 BETTER
I0328 07:56:20.226282 2287659 finetune.py:68] layer 21_o @ epoch 1 new loss 4.654837539419532e-05 old loss 4.739328142022714e-05 BETTER
I0328 07:56:25.650790 2288413 finetune.py:45] layer 22_o initial loss 4.475914829527028e-05
I0328 07:56:34.907021 2289175 finetune.py:68] layer 23_k @ epoch 3 new loss 1.8487891793483868e-05 old loss 1.8578257368062623e-05 BETTER
I0328 07:56:52.517232 2286844 finetune.py:68] layer 20_o @ epoch 3 new loss 3.648717392934486e-05 old loss 3.682534952531569e-05 BETTER
I0328 07:56:53.995716 2287659 finetune.py:68] layer 21_o @ epoch 2 new loss 4.594392521539703e-05 old loss 4.654837539419532e-05 BETTER
I0328 07:56:58.622048 2288413 finetune.py:68] layer 22_o @ epoch 0 new loss 4.294267273508012e-05 old loss 4.475914829527028e-05 BETTER
I0328 07:57:08.956420 2289175 finetune.py:68] layer 23_k @ epoch 4 new loss 1.8407929019303992e-05 old loss 1.8487891793483868e-05 BETTER
I0328 07:57:28.004246 2287659 finetune.py:68] layer 21_o @ epoch 3 new loss 4.545615229289979e-05 old loss 4.594392521539703e-05 BETTER
I0328 07:57:28.046249 2289175 finetune.py:45] layer 23_o initial loss 4.644743967219256e-05
I0328 07:57:28.655116 2286844 finetune.py:68] layer 20_o @ epoch 4 new loss 3.621681025833823e-05 old loss 3.648717392934486e-05 BETTER
I0328 07:57:32.607418 2288413 finetune.py:68] layer 22_o @ epoch 1 new loss 4.238943074597046e-05 old loss 4.294267273508012e-05 BETTER
I0328 07:58:00.095798 2286844 finetune.py:45] layer 20_up initial loss 0.00011312129208818078
I0328 07:58:00.279628 2289175 finetune.py:68] layer 23_o @ epoch 0 new loss 4.4215186790097505e-05 old loss 4.644743967219256e-05 BETTER
I0328 07:58:01.876038 2287659 finetune.py:68] layer 21_o @ epoch 4 new loss 4.506330151343718e-05 old loss 4.545615229289979e-05 BETTER
I0328 07:58:06.553484 2288413 finetune.py:68] layer 22_o @ epoch 2 new loss 4.1992825572378933e-05 old loss 4.238943074597046e-05 BETTER
I0328 07:58:32.394283 2286844 finetune.py:68] layer 20_up @ epoch 0 new loss 0.00011122233991045505 old loss 0.00011312129208818078 BETTER
I0328 07:58:34.239938 2289175 finetune.py:68] layer 23_o @ epoch 1 new loss 4.364864435046911e-05 old loss 4.4215186790097505e-05 BETTER
I0328 07:58:34.372405 2287659 finetune.py:45] layer 21_up initial loss 0.0001327317295363173
I0328 07:58:40.647968 2288413 finetune.py:68] layer 22_o @ epoch 3 new loss 4.1690218495205045e-05 old loss 4.1992825572378933e-05 BETTER
I0328 07:59:04.948575 2287659 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0001304287725361064 old loss 0.0001327317295363173 BETTER
I0328 07:59:05.832796 2286844 finetune.py:68] layer 20_up @ epoch 1 new loss 0.00010982760431943461 old loss 0.00011122233991045505 BETTER
I0328 07:59:07.705600 2289175 finetune.py:68] layer 23_o @ epoch 2 new loss 4.32426568295341e-05 old loss 4.364864435046911e-05 BETTER
I0328 07:59:14.681734 2288413 finetune.py:68] layer 22_o @ epoch 4 new loss 4.14390851801727e-05 old loss 4.1690218495205045e-05 BETTER
I0328 07:59:36.977289 2287659 finetune.py:68] layer 21_up @ epoch 1 new loss 0.00012876075925305486 old loss 0.0001304287725361064 BETTER
I0328 07:59:39.261818 2286844 finetune.py:68] layer 20_up @ epoch 2 new loss 0.00010865989315789193 old loss 0.00010982760431943461 BETTER
I0328 07:59:41.172214 2289175 finetune.py:68] layer 23_o @ epoch 3 new loss 4.293197707738727e-05 old loss 4.32426568295341e-05 BETTER
I0328 07:59:47.808989 2288413 finetune.py:45] layer 22_up initial loss 0.00013366813072934747
I0328 08:00:08.653747 2287659 finetune.py:68] layer 21_up @ epoch 2 new loss 0.00012734268966596574 old loss 0.00012876075925305486 BETTER
I0328 08:00:12.724743 2286844 finetune.py:68] layer 20_up @ epoch 3 new loss 0.00010762670717667788 old loss 0.00010865989315789193 BETTER
I0328 08:00:14.440796 2289175 finetune.py:68] layer 23_o @ epoch 4 new loss 4.268909469828941e-05 old loss 4.293197707738727e-05 BETTER
I0328 08:00:18.606101 2288413 finetune.py:68] layer 22_up @ epoch 0 new loss 0.00013149174628779292 old loss 0.00013366813072934747 BETTER
I0328 08:00:40.697212 2287659 finetune.py:68] layer 21_up @ epoch 3 new loss 0.00012611485726665705 old loss 0.00012734268966596574 BETTER
I0328 08:00:46.138424 2286844 finetune.py:68] layer 20_up @ epoch 4 new loss 0.00010672177450032905 old loss 0.00010762670717667788 BETTER
I0328 08:00:47.442420 2289175 finetune.py:45] layer 23_up initial loss 0.00014276932051870972
I0328 08:00:50.370769 2288413 finetune.py:68] layer 22_up @ epoch 1 new loss 0.00012995988072361797 old loss 0.00013149174628779292 BETTER
I0328 08:01:13.240951 2287659 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0001250288769369945 old loss 0.00012611485726665705 BETTER
I0328 08:01:17.740758 2289175 finetune.py:68] layer 23_up @ epoch 0 new loss 0.00014072413614485413 old loss 0.00014276932051870972 BETTER
I0328 08:01:20.536620 2286844 finetune.py:45] layer 20_gate initial loss 0.00013671893975697458
I0328 08:01:22.273878 2288413 finetune.py:68] layer 22_up @ epoch 2 new loss 0.00012866816541645676 old loss 0.00012995988072361797 BETTER
I0328 08:01:47.804843 2287659 finetune.py:45] layer 21_gate initial loss 0.00015926314517855644
I0328 08:01:50.164029 2289175 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0001392450649291277 old loss 0.00014072413614485413 BETTER
I0328 08:01:51.077040 2286844 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.00013572504394687712 old loss 0.00013671893975697458 BETTER
I0328 08:01:54.481281 2288413 finetune.py:68] layer 22_up @ epoch 3 new loss 0.00012754765339195728 old loss 0.00012866816541645676 BETTER
I0328 08:02:16.133055 2287659 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.00015807588351890445 old loss 0.00015926314517855644 BETTER
I0328 08:02:21.811433 2289175 finetune.py:68] layer 23_up @ epoch 2 new loss 0.00013802052126266062 old loss 0.0001392450649291277 BETTER
I0328 08:02:22.291417 2286844 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.00013488589320331812 old loss 0.00013572504394687712 BETTER
I0328 08:02:26.924508 2288413 finetune.py:68] layer 22_up @ epoch 4 new loss 0.00012655078899115324 old loss 0.00012754765339195728 BETTER
I0328 08:02:45.743928 2287659 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.00015708952560089529 old loss 0.00015807588351890445 BETTER
I0328 08:02:53.285343 2289175 finetune.py:68] layer 23_up @ epoch 3 new loss 0.00013696495443582535 old loss 0.00013802052126266062 BETTER
I0328 08:02:53.737450 2286844 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.00013413328269962221 old loss 0.00013488589320331812 BETTER
I0328 08:02:59.989031 2288413 finetune.py:45] layer 22_gate initial loss 0.00016378459986299276
I0328 08:03:15.424155 2287659 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0001561942626722157 old loss 0.00015708952560089529 BETTER
I0328 08:03:25.502756 2289175 finetune.py:68] layer 23_up @ epoch 4 new loss 0.00013603645493276417 old loss 0.00013696495443582535 BETTER
I0328 08:03:25.622056 2286844 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0001334549451712519 old loss 0.00013413328269962221 BETTER
I0328 08:03:28.801695 2288413 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.00016268074978142977 old loss 0.00016378459986299276 BETTER
I0328 08:03:45.948316 2287659 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.00015539508603978902 old loss 0.0001561942626722157 BETTER
I0328 08:03:57.526894 2286844 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.00013283539738040417 old loss 0.0001334549451712519 BETTER
I0328 08:03:59.495604 2288413 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.00016175967175513506 old loss 0.00016268074978142977 BETTER
I0328 08:03:59.498243 2289175 finetune.py:45] layer 23_gate initial loss 0.0001779940794222057
I0328 08:04:15.843595 2287659 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.00015466095646843314 old loss 0.00015539508603978902 BETTER
I0328 08:04:27.299713 2289175 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0001769804657669738 old loss 0.0001779940794222057 BETTER
I0328 08:04:29.260554 2288413 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.00016094317834358662 old loss 0.00016175967175513506 BETTER
I0328 08:04:56.730788 2289175 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.0001761453750077635 old loss 0.0001769804657669738 BETTER
I0328 08:04:56.878444 2286844 finetune.py:45] layer 20_down initial loss 0.00023544131545349956
I0328 08:04:59.388417 2288413 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.00016020437760744244 old loss 0.00016094317834358662 BETTER
I0328 08:05:16.503459 2287659 finetune.py:45] layer 21_down initial loss 0.0002743423101492226
I0328 08:05:24.740779 2286844 finetune.py:68] layer 20_down @ epoch 0 new loss 0.00023543013958260417 old loss 0.00023544131545349956 BETTER
I0328 08:05:27.311550 2289175 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0001753977412590757 old loss 0.0001761453750077635 BETTER
I0328 08:05:30.092693 2288413 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0001595369103597477 old loss 0.00016020437760744244 BETTER
I0328 08:05:42.983238 2287659 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0002743270597420633 old loss 0.0002743423101492226 BETTER
I0328 08:05:53.395596 2286844 finetune.py:68] layer 20_down @ epoch 1 new loss 0.00023542108829133213 old loss 0.00023543013958260417 BETTER
I0328 08:05:57.336087 2289175 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.00017472566105425358 old loss 0.0001753977412590757 BETTER
I0328 08:06:10.677027 2287659 finetune.py:68] layer 21_down @ epoch 1 new loss 0.000274314486887306 old loss 0.0002743270597420633 BETTER
I0328 08:06:22.335173 2286844 finetune.py:68] layer 20_down @ epoch 2 new loss 0.00023541448172181845 old loss 0.00023542108829133213 BETTER
I0328 08:06:27.200057 2289175 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0001741274172673002 old loss 0.00017472566105425358 BETTER
I0328 08:06:31.056801 2288413 finetune.py:45] layer 22_down initial loss 0.00028628247673623264
I0328 08:06:38.736983 2287659 finetune.py:68] layer 21_down @ epoch 2 new loss 0.00027430435875430703 old loss 0.000274314486887306 BETTER
I0328 08:06:51.721690 2286844 finetune.py:68] layer 20_down @ epoch 3 new loss 0.00023540899564977735 old loss 0.00023541448172181845 BETTER
I0328 08:06:57.428568 2288413 finetune.py:68] layer 22_down @ epoch 0 new loss 0.00028626626590266824 old loss 0.00028628247673623264 BETTER
I0328 08:07:06.723733 2287659 finetune.py:68] layer 21_down @ epoch 3 new loss 0.00027429559850133955 old loss 0.00027430435875430703 BETTER
I0328 08:07:20.986912 2286844 finetune.py:68] layer 20_down @ epoch 4 new loss 0.00023540420806966722 old loss 0.00023540899564977735 BETTER
20_v proxy err 0.018992459401488304 tr(WHW.T) 330.192138671875
bpp_loss 2.8346663501579314
20_q proxy err 0.0013724626041948795 tr(WHW.T) 20752.150390625
bpp_loss 3.558734084595926
20_k proxy err 0.0005154350074008107 tr(WHW.T) 15387.703125
bpp_loss 4.377312820171937
20_o proxy err 0.021158525720238686 tr(WHW.T) 1206.9736328125
bpp_loss 2.8724926565773785
20_up proxy err 0.016655240207910538 tr(WHW.T) 7604.44580078125
bpp_loss 2.960227540361562
20_gate proxy err 0.004439385142177343 tr(WHW.T) 30604.205078125
bpp_loss 3.313671785938953
20_down proxy err 0.018528327345848083 tr(WHW.T) 6330.791015625
bpp_loss 2.9563545366483077
I0328 08:07:25.363974 2288413 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0002862541005015373 old loss 0.00028626626590266824 BETTER
I0328 08:07:28.884148 2289175 finetune.py:45] layer 23_down initial loss 0.00030897182296030223
I0328 08:07:34.871614 2287659 finetune.py:68] layer 21_down @ epoch 4 new loss 0.00027428881730884314 old loss 0.00027429559850133955 BETTER
21_v proxy err 0.01761346124112606 tr(WHW.T) 362.87310791015625
bpp_loss 2.8598313902621157
21_q proxy err 0.0011186028132215142 tr(WHW.T) 25857.505859375
bpp_loss 3.5585699088405818
21_k proxy err 0.0004830930847674608 tr(WHW.T) 16776.28125
bpp_loss 4.414938771049492
21_o proxy err 0.014262106269598007 tr(WHW.T) 1271.912353515625
bpp_loss 2.888529100222513
21_up proxy err 0.016010096296668053 tr(WHW.T) 7770.2998046875
bpp_loss 2.962932089277144
21_gate proxy err 0.004251987207680941 tr(WHW.T) 31454.744140625
bpp_loss 3.3242453514997448
21_down proxy err 0.01748189702630043 tr(WHW.T) 6376.40087890625
bpp_loss 2.956668168206566
I0328 08:07:53.277860 2288413 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0002862443507183343 old loss 0.0002862541005015373 BETTER
I0328 08:07:54.807876 2289175 finetune.py:68] layer 23_down @ epoch 0 new loss 0.00030895607778802514 old loss 0.00030897182296030223 BETTER
I0328 08:08:21.172172 2288413 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0002862366964109242 old loss 0.0002862443507183343 BETTER
I0328 08:08:21.724878 2289175 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0003089443489443511 old loss 0.00030895607778802514 BETTER
I0328 08:08:47.332881 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 67.38657140731812s
I0328 08:08:49.101700 2289175 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0003089349193032831 old loss 0.0003089443489443511 BETTER
I0328 08:08:49.265335 2288413 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0002862301771529019 old loss 0.0002862366964109242 BETTER
22_v proxy err 0.017602134495973587 tr(WHW.T) 346.0789489746094
bpp_loss 2.9084044653573073
22_q proxy err 0.001332606771029532 tr(WHW.T) 20338.37890625
bpp_loss 3.521488055062946
22_k proxy err 0.000515064864885062 tr(WHW.T) 14713.1630859375
bpp_loss 4.356941193051171
22_o proxy err 0.019903739914298058 tr(WHW.T) 1223.2801513671875
bpp_loss 2.918834712880198
22_up proxy err 0.016678253188729286 tr(WHW.T) 7549.02197265625
bpp_loss 2.967196887026408
22_gate proxy err 0.004589257761836052 tr(WHW.T) 29480.33984375
bpp_loss 3.329078450732465
22_down proxy err 0.01753840036690235 tr(WHW.T) 6557.763671875
bpp_loss 2.962749544963507
I0328 08:08:51.506255 2296563 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 08:08:51.506349 2296563 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 08:08:51.506387 2296563 utils.py:162] NumExpr defaulting to 16 threads.
I0328 08:08:51.863118 2296563 config.py:54] PyTorch version 2.6.0 available.
W0328 08:08:52.070355 2296563 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 08:08:52.688649 2296563 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 08:08:52.692501 2210624 quantize_finetune_llama.py:209] layer 25 gpu 1
I0328 08:08:52.706001 2296563 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 08:09:09.589929 2296563 finetune.py:45] layer 24_v initial loss 8.421737584285438e-05
W0328 08:09:09.590140 2296563 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 08:09:16.174511 2289175 finetune.py:68] layer 23_down @ epoch 3 new loss 0.00030892735230736434 old loss 0.0003089349193032831 BETTER
I0328 08:09:43.439744 2289175 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0003089211822953075 old loss 0.00030892735230736434 BETTER
I0328 08:09:44.369149 2296563 finetune.py:68] layer 24_v @ epoch 0 new loss 2.1206204110058025e-05 old loss 8.421737584285438e-05 BETTER
23_v proxy err 0.016648493707180023 tr(WHW.T) 397.90704345703125
bpp_loss 2.958052349567879
23_q proxy err 0.00129153102170676 tr(WHW.T) 22621.15625
bpp_loss 3.5324858391541056
23_k proxy err 0.0005502298008650541 tr(WHW.T) 14850.2109375
bpp_loss 4.359598794893827
23_o proxy err 0.016848981380462646 tr(WHW.T) 1745.870361328125
bpp_loss 2.9416785210487433
23_up proxy err 0.01696643978357315 tr(WHW.T) 7423.2412109375
bpp_loss 2.9709719638340175
23_gate proxy err 0.004964611027389765 tr(WHW.T) 27249.904296875
bpp_loss 3.3311559072296535
23_down proxy err 0.017558734863996506 tr(WHW.T) 6695.96044921875
bpp_loss 2.967891912774316
I0328 08:09:58.295835 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 64.24631404876709s
I0328 08:10:01.878223 2296702 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 08:10:01.878318 2296702 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 08:10:01.878355 2296702 utils.py:162] NumExpr defaulting to 16 threads.
I0328 08:10:02.237699 2296702 config.py:54] PyTorch version 2.6.0 available.
W0328 08:10:02.430498 2296702 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 08:10:03.022109 2296702 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 08:10:03.025944 2210624 quantize_finetune_llama.py:209] layer 26 gpu 2
I0328 08:10:03.039727 2296702 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 08:10:19.590585 2296702 finetune.py:45] layer 25_v initial loss 9.13052135729231e-05
W0328 08:10:19.590891 2296702 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 08:10:20.566891 2296563 finetune.py:68] layer 24_v @ epoch 1 new loss 1.812697882996872e-05 old loss 2.1206204110058025e-05 BETTER
I0328 08:10:52.688679 2296702 finetune.py:68] layer 25_v @ epoch 0 new loss 2.4460938220727257e-05 old loss 9.13052135729231e-05 BETTER
I0328 08:10:57.202437 2296563 finetune.py:68] layer 24_v @ epoch 2 new loss 1.716708720778115e-05 old loss 1.812697882996872e-05 BETTER
I0328 08:11:06.534212 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 63.047839641571045s
I0328 08:11:10.232035 2296835 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 08:11:10.232124 2296835 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 08:11:10.232165 2296835 utils.py:162] NumExpr defaulting to 16 threads.
I0328 08:11:10.575523 2296835 config.py:54] PyTorch version 2.6.0 available.
W0328 08:11:10.774064 2296835 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 08:11:11.370463 2296835 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 08:11:11.374310 2210624 quantize_finetune_llama.py:209] layer 27 gpu 3
I0328 08:11:11.387810 2296835 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 08:11:27.124079 2296702 finetune.py:68] layer 25_v @ epoch 1 new loss 2.1380574253271334e-05 old loss 2.4460938220727257e-05 BETTER
I0328 08:11:28.350271 2296835 finetune.py:45] layer 26_v initial loss 8.449402957921848e-05
W0328 08:11:28.350457 2296835 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 08:11:34.046914 2296563 finetune.py:68] layer 24_v @ epoch 3 new loss 1.665827221586369e-05 old loss 1.716708720778115e-05 BETTER
I0328 08:12:01.752460 2296702 finetune.py:68] layer 25_v @ epoch 2 new loss 2.0347719328128733e-05 old loss 2.1380574253271334e-05 BETTER
I0328 08:12:01.837944 2296835 finetune.py:68] layer 26_v @ epoch 0 new loss 3.369400292285718e-05 old loss 8.449402957921848e-05 BETTER
I0328 08:12:10.997183 2296563 finetune.py:68] layer 24_v @ epoch 4 new loss 1.6316114852088504e-05 old loss 1.665827221586369e-05 BETTER
I0328 08:12:15.461014 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 63.591108083724976s
I0328 08:12:19.134015 2296971 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 08:12:19.134169 2296971 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 08:12:19.134222 2296971 utils.py:162] NumExpr defaulting to 16 threads.
I0328 08:12:19.468439 2296971 config.py:54] PyTorch version 2.6.0 available.
W0328 08:12:19.677967 2296971 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 08:12:20.308319 2296971 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 08:12:20.311890 2210624 quantize_finetune_llama.py:209] layer 28 gpu 0
I0328 08:12:20.324317 2296971 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 08:12:30.129682 2296563 finetune.py:45] layer 24_q initial loss 2.0523346393019892e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 08:12:36.489310 2296835 finetune.py:68] layer 26_v @ epoch 1 new loss 3.0952298402553424e-05 old loss 3.369400292285718e-05 BETTER
I0328 08:12:36.543120 2296702 finetune.py:68] layer 25_v @ epoch 3 new loss 1.9910827177227475e-05 old loss 2.0347719328128733e-05 BETTER
I0328 08:12:37.424695 2296971 finetune.py:45] layer 27_v initial loss 9.280537051381543e-05
W0328 08:12:37.425021 2296971 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 08:13:05.350024 2296563 finetune.py:68] layer 24_q @ epoch 0 new loss 1.9956729374825954e-05 old loss 2.0523346393019892e-05 BETTER
I0328 08:13:10.963587 2296971 finetune.py:68] layer 27_v @ epoch 0 new loss 3.1390864023705944e-05 old loss 9.280537051381543e-05 BETTER
I0328 08:13:11.521266 2296835 finetune.py:68] layer 26_v @ epoch 2 new loss 2.9785336664645e-05 old loss 3.0952298402553424e-05 BETTER
I0328 08:13:11.944574 2296702 finetune.py:68] layer 25_v @ epoch 4 new loss 1.9420327589614317e-05 old loss 1.9910827177227475e-05 BETTER
I0328 08:13:30.940987 2296702 finetune.py:45] layer 25_q initial loss 2.761695213848725e-05
I0328 08:13:41.636271 2296563 finetune.py:68] layer 24_q @ epoch 1 new loss 1.9635495846159756e-05 old loss 1.9956729374825954e-05 BETTER
I0328 08:13:45.548337 2296971 finetune.py:68] layer 27_v @ epoch 1 new loss 2.9074961275910027e-05 old loss 3.1390864023705944e-05 BETTER
I0328 08:13:46.760854 2296835 finetune.py:68] layer 26_v @ epoch 3 new loss 2.9017439374001697e-05 old loss 2.9785336664645e-05 BETTER
I0328 08:14:04.406883 2296702 finetune.py:68] layer 25_q @ epoch 0 new loss 2.6470966986380517e-05 old loss 2.761695213848725e-05 BETTER
I0328 08:14:18.205249 2296563 finetune.py:68] layer 24_q @ epoch 2 new loss 1.9402885300223716e-05 old loss 1.9635495846159756e-05 BETTER
I0328 08:14:20.192419 2296971 finetune.py:68] layer 27_v @ epoch 2 new loss 2.8070931875845417e-05 old loss 2.9074961275910027e-05 BETTER
I0328 08:14:22.060459 2296835 finetune.py:68] layer 26_v @ epoch 4 new loss 2.8505504815257154e-05 old loss 2.9017439374001697e-05 BETTER
I0328 08:14:38.785374 2296702 finetune.py:68] layer 25_q @ epoch 1 new loss 2.5829434889601544e-05 old loss 2.6470966986380517e-05 BETTER
I0328 08:14:41.989081 2296835 finetune.py:45] layer 26_q initial loss 3.496676072245464e-05
I0328 08:14:54.727615 2296563 finetune.py:68] layer 24_q @ epoch 3 new loss 1.9220069589209743e-05 old loss 1.9402885300223716e-05 BETTER
I0328 08:14:55.089273 2296971 finetune.py:68] layer 27_v @ epoch 3 new loss 2.7507381673785858e-05 old loss 2.8070931875845417e-05 BETTER
I0328 08:15:13.383843 2296702 finetune.py:68] layer 25_q @ epoch 2 new loss 2.5390432710992172e-05 old loss 2.5829434889601544e-05 BETTER
I0328 08:15:15.724956 2296835 finetune.py:68] layer 26_q @ epoch 0 new loss 3.4039596357615665e-05 old loss 3.496676072245464e-05 BETTER
I0328 08:15:30.016747 2296971 finetune.py:68] layer 27_v @ epoch 4 new loss 2.7043079171562567e-05 old loss 2.7507381673785858e-05 BETTER
I0328 08:15:31.163270 2296563 finetune.py:68] layer 24_q @ epoch 4 new loss 1.9071881979471073e-05 old loss 1.9220069589209743e-05 BETTER
I0328 08:15:48.802798 2296702 finetune.py:68] layer 25_q @ epoch 3 new loss 2.5023186026373878e-05 old loss 2.5390432710992172e-05 BETTER
I0328 08:15:49.291496 2296563 finetune.py:45] layer 24_k initial loss 2.1163035853533074e-05
I0328 08:15:50.244494 2296971 finetune.py:45] layer 27_q initial loss 3.726527211256325e-05
I0328 08:15:50.711273 2296835 finetune.py:68] layer 26_q @ epoch 1 new loss 3.346509765833616e-05 old loss 3.4039596357615665e-05 BETTER
I0328 08:16:24.069829 2296702 finetune.py:68] layer 25_q @ epoch 4 new loss 2.470655454089865e-05 old loss 2.5023186026373878e-05 BETTER
I0328 08:16:24.075974 2296971 finetune.py:68] layer 27_q @ epoch 0 new loss 3.634088352555409e-05 old loss 3.726527211256325e-05 BETTER
I0328 08:16:24.628356 2296563 finetune.py:68] layer 24_k @ epoch 0 new loss 2.100584242725745e-05 old loss 2.1163035853533074e-05 BETTER
I0328 08:16:26.029040 2296835 finetune.py:68] layer 26_q @ epoch 2 new loss 3.3023537980625406e-05 old loss 3.346509765833616e-05 BETTER
I0328 08:16:42.015213 2296702 finetune.py:45] layer 25_k initial loss 2.8679634851869196e-05
I0328 08:16:58.404446 2296971 finetune.py:68] layer 27_q @ epoch 1 new loss 3.572878267732449e-05 old loss 3.634088352555409e-05 BETTER
I0328 08:17:01.135345 2296835 finetune.py:68] layer 26_q @ epoch 3 new loss 3.263389953644946e-05 old loss 3.3023537980625406e-05 BETTER
I0328 08:17:01.187874 2296563 finetune.py:68] layer 24_k @ epoch 1 new loss 2.089076406264212e-05 old loss 2.100584242725745e-05 BETTER
I0328 08:17:15.025376 2296702 finetune.py:68] layer 25_k @ epoch 0 new loss 2.8275069780647755e-05 old loss 2.8679634851869196e-05 BETTER
I0328 08:17:32.966461 2296971 finetune.py:68] layer 27_q @ epoch 2 new loss 3.535150972311385e-05 old loss 3.572878267732449e-05 BETTER
I0328 08:17:35.846264 2296835 finetune.py:68] layer 26_q @ epoch 4 new loss 3.235456097172573e-05 old loss 3.263389953644946e-05 BETTER
I0328 08:17:37.464793 2296563 finetune.py:68] layer 24_k @ epoch 2 new loss 2.0801331629627384e-05 old loss 2.089076406264212e-05 BETTER
I0328 08:17:49.254343 2296702 finetune.py:68] layer 25_k @ epoch 1 new loss 2.7997597499052063e-05 old loss 2.8275069780647755e-05 BETTER
I0328 08:17:54.189972 2296835 finetune.py:45] layer 26_k initial loss 3.5237408155808225e-05
I0328 08:18:07.112609 2296971 finetune.py:68] layer 27_q @ epoch 3 new loss 3.503682091832161e-05 old loss 3.535150972311385e-05 BETTER
I0328 08:18:13.673349 2296563 finetune.py:68] layer 24_k @ epoch 3 new loss 2.0729385141748935e-05 old loss 2.0801331629627384e-05 BETTER
I0328 08:18:23.767694 2296702 finetune.py:68] layer 25_k @ epoch 2 new loss 2.7770529413828626e-05 old loss 2.7997597499052063e-05 BETTER
I0328 08:18:27.802921 2296835 finetune.py:68] layer 26_k @ epoch 0 new loss 3.472651951597072e-05 old loss 3.5237408155808225e-05 BETTER
I0328 08:18:41.479959 2296971 finetune.py:68] layer 27_q @ epoch 4 new loss 3.478047074167989e-05 old loss 3.503682091832161e-05 BETTER
I0328 08:18:50.068731 2296563 finetune.py:68] layer 24_k @ epoch 4 new loss 2.064132240775507e-05 old loss 2.0729385141748935e-05 BETTER
I0328 08:18:58.172447 2296702 finetune.py:68] layer 25_k @ epoch 3 new loss 2.7598620363278314e-05 old loss 2.7770529413828626e-05 BETTER
I0328 08:18:59.513587 2296971 finetune.py:45] layer 27_k initial loss 3.992949496023357e-05
I0328 08:19:02.455630 2296835 finetune.py:68] layer 26_k @ epoch 1 new loss 3.446310438448563e-05 old loss 3.472651951597072e-05 BETTER
I0328 08:19:09.773818 2296563 finetune.py:45] layer 24_o initial loss 5.1168819481972605e-05
I0328 08:19:32.820818 2296971 finetune.py:68] layer 27_k @ epoch 0 new loss 3.938732697861269e-05 old loss 3.992949496023357e-05 BETTER
I0328 08:19:33.251605 2296702 finetune.py:68] layer 25_k @ epoch 4 new loss 2.7449150366010144e-05 old loss 2.7598620363278314e-05 BETTER
I0328 08:19:37.064019 2296835 finetune.py:68] layer 26_k @ epoch 2 new loss 3.426371404202655e-05 old loss 3.446310438448563e-05 BETTER
I0328 08:19:44.161170 2296563 finetune.py:68] layer 24_o @ epoch 0 new loss 4.919129787595011e-05 old loss 5.1168819481972605e-05 BETTER
I0328 08:19:53.142586 2296702 finetune.py:45] layer 25_o initial loss 6.230609869817272e-05
I0328 08:20:07.074519 2296971 finetune.py:68] layer 27_k @ epoch 1 new loss 3.9070218917913735e-05 old loss 3.938732697861269e-05 BETTER
I0328 08:20:11.951263 2296835 finetune.py:68] layer 26_k @ epoch 3 new loss 3.4069205867126584e-05 old loss 3.426371404202655e-05 BETTER
I0328 08:20:20.017160 2296563 finetune.py:68] layer 24_o @ epoch 1 new loss 4.8728161345934495e-05 old loss 4.919129787595011e-05 BETTER
I0328 08:20:25.885113 2296702 finetune.py:68] layer 25_o @ epoch 0 new loss 5.958377732895315e-05 old loss 6.230609869817272e-05 BETTER
I0328 08:20:41.445290 2296971 finetune.py:68] layer 27_k @ epoch 2 new loss 3.88807529816404e-05 old loss 3.9070218917913735e-05 BETTER
I0328 08:20:46.638879 2296835 finetune.py:68] layer 26_k @ epoch 4 new loss 3.392282815184444e-05 old loss 3.4069205867126584e-05 BETTER
I0328 08:20:55.967501 2296563 finetune.py:68] layer 24_o @ epoch 2 new loss 4.841201371164061e-05 old loss 4.8728161345934495e-05 BETTER
I0328 08:20:59.617420 2296702 finetune.py:68] layer 25_o @ epoch 1 new loss 5.8893794630421326e-05 old loss 5.958377732895315e-05 BETTER
I0328 08:21:07.403217 2296835 finetune.py:45] layer 26_o initial loss 8.193495887098834e-05
I0328 08:21:15.953575 2296971 finetune.py:68] layer 27_k @ epoch 3 new loss 3.874620233546011e-05 old loss 3.88807529816404e-05 BETTER
I0328 08:21:31.946913 2296563 finetune.py:68] layer 24_o @ epoch 3 new loss 4.815954889636487e-05 old loss 4.841201371164061e-05 BETTER
I0328 08:21:33.715344 2296702 finetune.py:68] layer 25_o @ epoch 2 new loss 5.840330049977638e-05 old loss 5.8893794630421326e-05 BETTER
I0328 08:21:40.276051 2296835 finetune.py:68] layer 26_o @ epoch 0 new loss 7.879900658736005e-05 old loss 8.193495887098834e-05 BETTER
I0328 08:21:50.189367 2296971 finetune.py:68] layer 27_k @ epoch 4 new loss 3.862309313262813e-05 old loss 3.874620233546011e-05 BETTER
I0328 08:22:08.428050 2296702 finetune.py:68] layer 25_o @ epoch 3 new loss 5.80352425458841e-05 old loss 5.840330049977638e-05 BETTER
I0328 08:22:08.438544 2296563 finetune.py:68] layer 24_o @ epoch 4 new loss 4.796482971869409e-05 old loss 4.815954889636487e-05 BETTER
I0328 08:22:10.972817 2296971 finetune.py:45] layer 27_o initial loss 9.623475489206612e-05
I0328 08:22:14.093196 2296835 finetune.py:68] layer 26_o @ epoch 1 new loss 7.782958709867671e-05 old loss 7.879900658736005e-05 BETTER
I0328 08:22:41.719255 2296563 finetune.py:45] layer 24_up initial loss 0.000156332302140072
I0328 08:22:42.947134 2296702 finetune.py:68] layer 25_o @ epoch 4 new loss 5.775410318165086e-05 old loss 5.80352425458841e-05 BETTER
I0328 08:22:43.817193 2296971 finetune.py:68] layer 27_o @ epoch 0 new loss 9.29172892938368e-05 old loss 9.623475489206612e-05 BETTER
I0328 08:22:48.087393 2296835 finetune.py:68] layer 26_o @ epoch 2 new loss 7.714403182035312e-05 old loss 7.782958709867671e-05 BETTER
I0328 08:23:14.051804 2296563 finetune.py:68] layer 24_up @ epoch 0 new loss 0.00015432029613293707 old loss 0.000156332302140072 BETTER
I0328 08:23:17.237707 2296702 finetune.py:45] layer 25_up initial loss 0.00017946526349987835
I0328 08:23:17.735482 2296971 finetune.py:68] layer 27_o @ epoch 1 new loss 9.173520084004849e-05 old loss 9.29172892938368e-05 BETTER
I0328 08:23:22.258046 2296835 finetune.py:68] layer 26_o @ epoch 3 new loss 7.660430856049061e-05 old loss 7.714403182035312e-05 BETTER
I0328 08:23:47.017235 2296563 finetune.py:68] layer 24_up @ epoch 1 new loss 0.00015291817544493824 old loss 0.00015432029613293707 BETTER
I0328 08:23:47.632530 2296702 finetune.py:68] layer 25_up @ epoch 0 new loss 0.00017722835764288902 old loss 0.00017946526349987835 BETTER
I0328 08:23:51.525479 2296971 finetune.py:68] layer 27_o @ epoch 2 new loss 9.094719280255958e-05 old loss 9.173520084004849e-05 BETTER
I0328 08:23:56.348107 2296835 finetune.py:68] layer 26_o @ epoch 4 new loss 7.617480150656775e-05 old loss 7.660430856049061e-05 BETTER
I0328 08:24:20.160548 2296702 finetune.py:68] layer 25_up @ epoch 1 new loss 0.00017567556642461568 old loss 0.00017722835764288902 BETTER
I0328 08:24:21.460484 2296563 finetune.py:68] layer 24_up @ epoch 2 new loss 0.00015177240129560232 old loss 0.00015291817544493824 BETTER
I0328 08:24:25.785331 2296971 finetune.py:68] layer 27_o @ epoch 3 new loss 9.028811473399401e-05 old loss 9.094719280255958e-05 BETTER
I0328 08:24:32.488399 2296835 finetune.py:45] layer 26_up initial loss 0.00021860108245164156
I0328 08:24:53.147663 2296702 finetune.py:68] layer 25_up @ epoch 2 new loss 0.00017440745432395488 old loss 0.00017567556642461568 BETTER
I0328 08:24:55.988656 2296563 finetune.py:68] layer 24_up @ epoch 3 new loss 0.00015076009731274098 old loss 0.00015177240129560232 BETTER
I0328 08:25:00.024165 2296971 finetune.py:68] layer 27_o @ epoch 4 new loss 8.979460108093917e-05 old loss 9.028811473399401e-05 BETTER
I0328 08:25:03.668642 2296835 finetune.py:68] layer 26_up @ epoch 0 new loss 0.00021593720884993672 old loss 0.00021860108245164156 BETTER
I0328 08:25:25.405697 2296702 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0001733038225211203 old loss 0.00017440745432395488 BETTER
I0328 08:25:29.877743 2296563 finetune.py:68] layer 24_up @ epoch 4 new loss 0.00014988613838795573 old loss 0.00015076009731274098 BETTER
I0328 08:25:34.427410 2296971 finetune.py:45] layer 27_up initial loss 0.000256572529906407
I0328 08:25:35.810950 2296835 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0002140621218131855 old loss 0.00021593720884993672 BETTER
I0328 08:25:57.733939 2296702 finetune.py:68] layer 25_up @ epoch 4 new loss 0.00017234192637261003 old loss 0.0001733038225211203 BETTER
I0328 08:26:05.347090 2296563 finetune.py:45] layer 24_gate initial loss 0.00019787647761404514
I0328 08:26:05.536346 2296971 finetune.py:68] layer 27_up @ epoch 0 new loss 0.000253270729444921 old loss 0.000256572529906407 BETTER
I0328 08:26:08.327915 2296835 finetune.py:68] layer 26_up @ epoch 2 new loss 0.00021253066370263696 old loss 0.0002140621218131855 BETTER
I0328 08:26:33.075841 2296702 finetune.py:45] layer 25_gate initial loss 0.00022786138288211077
I0328 08:26:35.937885 2296563 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.00019692153728101403 old loss 0.00019787647761404514 BETTER
I0328 08:26:37.993756 2296971 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0002509753976482898 old loss 0.000253270729444921 BETTER
I0328 08:26:41.060986 2296835 finetune.py:68] layer 26_up @ epoch 3 new loss 0.000211184291401878 old loss 0.00021253066370263696 BETTER
I0328 08:27:01.574764 2296702 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.00022683663701172918 old loss 0.00022786138288211077 BETTER
I0328 08:27:08.406599 2296563 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0001961157686309889 old loss 0.00019692153728101403 BETTER
I0328 08:27:11.086418 2296971 finetune.py:68] layer 27_up @ epoch 2 new loss 0.00024913137895055115 old loss 0.0002509753976482898 BETTER
I0328 08:27:14.437990 2296835 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0002100043056998402 old loss 0.000211184291401878 BETTER
I0328 08:27:31.101324 2296702 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.00022597640054300427 old loss 0.00022683663701172918 BETTER
I0328 08:27:40.030332 2296563 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.00019542047812137753 old loss 0.0001961157686309889 BETTER
I0328 08:27:42.952439 2296971 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0002475159417372197 old loss 0.00024913137895055115 BETTER
I0328 08:27:49.604299 2296835 finetune.py:45] layer 26_gate initial loss 0.0002758289920166135
I0328 08:28:01.209765 2296702 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.00022522453218698502 old loss 0.00022597640054300427 BETTER
I0328 08:28:12.391950 2296563 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.00019477713794913143 old loss 0.00019542047812137753 BETTER
I0328 08:28:15.868295 2296971 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0002461078984197229 old loss 0.0002475159417372197 BETTER
I0328 08:28:18.709076 2296835 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.00027460051933303475 old loss 0.0002758289920166135 BETTER
I0328 08:28:31.292095 2296702 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.00022455013822764158 old loss 0.00022522453218698502 BETTER
I0328 08:28:44.436475 2296563 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.00019421592878643423 old loss 0.00019477713794913143 BETTER
I0328 08:28:48.765877 2296835 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.0002735773450694978 old loss 0.00027460051933303475 BETTER
I0328 08:28:51.366180 2296971 finetune.py:45] layer 27_gate initial loss 0.00032785977236926556
I0328 08:29:01.219958 2296702 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0002239541499875486 old loss 0.00022455013822764158 BETTER
I0328 08:29:19.089892 2296835 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0002726945385802537 old loss 0.0002735773450694978 BETTER
I0328 08:29:19.846667 2296971 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.00032632332295179367 old loss 0.00032785977236926556 BETTER
I0328 08:29:44.331526 2296563 finetune.py:45] layer 24_down initial loss 0.0003361140552442521
I0328 08:29:49.506711 2296835 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.00027187931118533015 old loss 0.0002726945385802537 BETTER
I0328 08:29:49.949468 2296971 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.0003250506124459207 old loss 0.00032632332295179367 BETTER
I0328 08:30:01.830227 2296702 finetune.py:45] layer 25_down initial loss 0.00037711061304435134
I0328 08:30:12.110080 2296563 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0003360998525749892 old loss 0.0003361140552442521 BETTER
I0328 08:30:20.648607 2296835 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0002711694687604904 old loss 0.00027187931118533015 BETTER
I0328 08:30:20.781105 2296971 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.00032394775189459324 old loss 0.0003250506124459207 BETTER
I0328 08:30:28.427541 2296702 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0003770980692934245 old loss 0.00037711061304435134 BETTER
I0328 08:30:41.137349 2296563 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0003360872797202319 old loss 0.0003360998525749892 BETTER
I0328 08:30:50.752604 2296971 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.00032296677818521857 old loss 0.00032394775189459324 BETTER
I0328 08:30:56.073883 2296702 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0003770879120565951 old loss 0.0003770980692934245 BETTER
I0328 08:31:10.677694 2296563 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0003360779373906553 old loss 0.0003360872797202319 BETTER
I0328 08:31:21.538165 2296971 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.00032208432094193995 old loss 0.00032296677818521857 BETTER
I0328 08:31:23.693184 2296835 finetune.py:45] layer 26_down initial loss 0.0004474338493309915
I0328 08:31:24.686399 2296702 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0003770792973227799 old loss 0.0003770879120565951 BETTER
I0328 08:31:39.749664 2296563 finetune.py:68] layer 24_down @ epoch 3 new loss 0.00033606914803385735 old loss 0.0003360779373906553 BETTER
I0328 08:31:50.296485 2296835 finetune.py:68] layer 26_down @ epoch 0 new loss 0.00044742008321918547 old loss 0.0004474338493309915 BETTER
I0328 08:31:52.860641 2296702 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0003770719631575048 old loss 0.0003770792973227799 BETTER
I0328 08:32:09.317477 2296563 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0003360634727869183 old loss 0.00033606914803385735 BETTER
24_v proxy err 0.014025003649294376 tr(WHW.T) 467.2783508300781
bpp_loss 3.0500379027216695
24_q proxy err 0.0012590889818966389 tr(WHW.T) 22456.20703125
bpp_loss 3.5021779384696856
24_k proxy err 0.000548279145732522 tr(WHW.T) 14175.189453125
bpp_loss 4.205798610346392
24_o proxy err 0.016567451879382133 tr(WHW.T) 1591.575439453125
bpp_loss 2.9805988515145145
24_up proxy err 0.017418356612324715 tr(WHW.T) 7312.6875
bpp_loss 2.975262592812734
24_gate proxy err 0.005298161413520575 tr(WHW.T) 25841.31640625
bpp_loss 3.336554446656789
24_down proxy err 0.017422232776880264 tr(WHW.T) 6770.8056640625
bpp_loss 2.9729692986501113
I0328 08:32:18.578282 2296835 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0004474094312172383 old loss 0.00044742008321918547 BETTER
I0328 08:32:20.864984 2296702 finetune.py:68] layer 25_down @ epoch 4 new loss 0.00037706553121097386 old loss 0.0003770719631575048 BETTER
25_v proxy err 0.012086153030395508 tr(WHW.T) 557.8086547851562
bpp_loss 3.061677629360929
25_q proxy err 0.0011061823461204767 tr(WHW.T) 26141.64453125
bpp_loss 3.483990176115185
25_k proxy err 0.0005525812739506364 tr(WHW.T) 14406.3017578125
bpp_loss 4.186781696043909
25_o proxy err 0.013880135491490364 tr(WHW.T) 1993.0301513671875
bpp_loss 2.9789021861506626
25_up proxy err 0.017252059653401375 tr(WHW.T) 7385.3955078125
bpp_loss 2.9839909842370878
25_gate proxy err 0.0052123540081083775 tr(WHW.T) 26268.15625
bpp_loss 3.3445158966683914
25_down proxy err 0.017345312982797623 tr(WHW.T) 6646.96923828125
bpp_loss 2.9813258143175125
I0328 08:32:23.226516 2296971 finetune.py:45] layer 27_down initial loss 0.0005281619960442185
I0328 08:32:46.197994 2296835 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0004473997396416962 old loss 0.0004474094312172383 BETTER
I0328 08:32:49.271195 2296971 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0005281436606310308 old loss 0.0005281619960442185 BETTER
I0328 08:33:13.977143 2296835 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0004473922017496079 old loss 0.0004473997396416962 BETTER
I0328 08:33:16.350122 2296971 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0005281288758851588 old loss 0.0005281436606310308 BETTER
I0328 08:33:33.114324 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 66.67699670791626s
I0328 08:33:36.985329 2298265 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 08:33:36.985430 2298265 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 08:33:36.985474 2298265 utils.py:162] NumExpr defaulting to 16 threads.
I0328 08:33:37.379676 2298265 config.py:54] PyTorch version 2.6.0 available.
W0328 08:33:37.616303 2298265 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 08:33:38.275997 2298265 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 08:33:38.279844 2210624 quantize_finetune_llama.py:209] layer 29 gpu 1
I0328 08:33:38.293516 2298265 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 08:33:41.725538 2296835 finetune.py:68] layer 26_down @ epoch 4 new loss 0.00044738667202182114 old loss 0.0004473922017496079 BETTER
I0328 08:33:43.562048 2296971 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0005281155463308096 old loss 0.0005281288758851588 BETTER
26_v proxy err 0.014863744378089905 tr(WHW.T) 434.54583740234375
bpp_loss 3.1064327731146477
26_q proxy err 0.0012806272134184837 tr(WHW.T) 21412.666015625
bpp_loss 3.4931999188265763
26_k proxy err 0.0004931599833071232 tr(WHW.T) 15415.4921875
bpp_loss 4.262146794935688
26_o proxy err 0.010009330697357655 tr(WHW.T) 2391.728515625
bpp_loss 2.9962198534049094
26_up proxy err 0.016678256914019585 tr(WHW.T) 7651.875
bpp_loss 2.992944452046816
26_gate proxy err 0.004747668281197548 tr(WHW.T) 28880.806640625
bpp_loss 3.3491671381385197
26_down proxy err 0.01737058535218239 tr(WHW.T) 6651.23291015625
bpp_loss 2.9886904400939653
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 08:33:55.240046 2298265 finetune.py:45] layer 28_v initial loss 0.00010957763879559934
W0328 08:33:55.240476 2298265 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 08:34:11.073586 2296971 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0005281044286675751 old loss 0.0005281155463308096 BETTER
I0328 08:34:30.035982 2298265 finetune.py:68] layer 28_v @ epoch 0 new loss 4.3424628529464826e-05 old loss 0.00010957763879559934 BETTER
I0328 08:34:38.566717 2296971 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0005280958139337599 old loss 0.0005281044286675751 BETTER
27_v proxy err 0.010304097086191177 tr(WHW.T) 677.69384765625
bpp_loss 3.193077791540418
27_q proxy err 0.0013657533563673496 tr(WHW.T) 21317.205078125
bpp_loss 3.4643994119251147
27_k proxy err 0.0005754688172601163 tr(WHW.T) 14004.09375
bpp_loss 4.219147775496822
27_o proxy err 0.011992983520030975 tr(WHW.T) 2163.398193359375
bpp_loss 3.035322425130289
27_up proxy err 0.015218150801956654 tr(WHW.T) 8480.8603515625
bpp_loss 3.007758777721652
27_gate proxy err 0.004224550444632769 tr(WHW.T) 32784.34375
bpp_loss 3.3584996072043265
27_down proxy err 0.014446024782955647 tr(WHW.T) 6577.5654296875
bpp_loss 2.999159536124872
I0328 08:34:50.557245 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 63.923930168151855s
I0328 08:34:54.093729 2298410 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 08:34:54.093829 2298410 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 08:34:54.093867 2298410 utils.py:162] NumExpr defaulting to 16 threads.
I0328 08:34:54.427358 2298410 config.py:54] PyTorch version 2.6.0 available.
W0328 08:34:54.614310 2298410 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 08:34:55.163889 2298410 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 08:34:55.167488 2210624 quantize_finetune_llama.py:209] layer 30 gpu 2
I0328 08:34:55.187103 2298410 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 08:35:06.136075 2298265 finetune.py:68] layer 28_v @ epoch 1 new loss 4.1087158024311066e-05 old loss 4.3424628529464826e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 08:35:11.919704 2298410 finetune.py:45] layer 29_v initial loss 0.00013280173880048096
W0328 08:35:11.919972 2298410 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 08:35:42.858688 2298265 finetune.py:68] layer 28_v @ epoch 2 new loss 3.997718886239454e-05 old loss 4.1087158024311066e-05 BETTER
I0328 08:35:45.005955 2298410 finetune.py:68] layer 29_v @ epoch 0 new loss 5.0006194214802235e-05 old loss 0.00013280173880048096 BETTER
I0328 08:35:58.467835 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 62.81592011451721s
I0328 08:36:02.227929 2298546 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 08:36:02.228032 2298546 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 08:36:02.228077 2298546 utils.py:162] NumExpr defaulting to 16 threads.
I0328 08:36:02.594698 2298546 config.py:54] PyTorch version 2.6.0 available.
W0328 08:36:02.823135 2298546 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 08:36:03.457042 2298546 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 08:36:03.460848 2210624 quantize_finetune_llama.py:209] layer 31 gpu 3
I0328 08:36:03.474996 2298546 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 08:36:19.299152 2298410 finetune.py:68] layer 29_v @ epoch 1 new loss 4.771302701556124e-05 old loss 5.0006194214802235e-05 BETTER
I0328 08:36:19.796751 2298265 finetune.py:68] layer 28_v @ epoch 3 new loss 3.91009307350032e-05 old loss 3.997718886239454e-05 BETTER
I0328 08:36:20.825671 2298546 finetune.py:45] layer 30_v initial loss 0.00016056526510510594
W0328 08:36:20.826120 2298546 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 08:36:53.965534 2298410 finetune.py:68] layer 29_v @ epoch 2 new loss 4.6693647163920105e-05 old loss 4.771302701556124e-05 BETTER
I0328 08:36:54.275176 2298546 finetune.py:68] layer 30_v @ epoch 0 new loss 8.606915071140975e-05 old loss 0.00016056526510510594 BETTER
I0328 08:36:56.857652 2298265 finetune.py:68] layer 28_v @ epoch 4 new loss 3.870482032652944e-05 old loss 3.91009307350032e-05 BETTER
I0328 08:37:01.215858 2210624 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 57.23478055000305s
I0328 08:37:04.892905 2298676 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 08:37:04.893029 2298676 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 08:37:04.893075 2298676 utils.py:162] NumExpr defaulting to 16 threads.
I0328 08:37:05.242889 2298676 config.py:54] PyTorch version 2.6.0 available.
W0328 08:37:05.446538 2298676 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 08:37:06.060541 2298676 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 08:37:06.080883 2298676 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 08:37:15.886554 2298265 finetune.py:45] layer 28_q initial loss 5.069300459581427e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 08:37:25.503177 2298676 finetune.py:45] layer 31_v initial loss 0.0002681379555724561
W0328 08:37:25.503412 2298676 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 08:37:30.756565 2298410 finetune.py:68] layer 29_v @ epoch 3 new loss 4.654924487113021e-05 old loss 4.6693647163920105e-05 BETTER
I0328 08:37:31.082378 2298546 finetune.py:68] layer 30_v @ epoch 1 new loss 8.182031888281927e-05 old loss 8.606915071140975e-05 BETTER
I0328 08:37:51.531568 2298265 finetune.py:68] layer 28_q @ epoch 0 new loss 4.929108763462864e-05 old loss 5.069300459581427e-05 BETTER
I0328 08:37:59.944423 2298676 finetune.py:68] layer 31_v @ epoch 0 new loss 0.00015102839097380638 old loss 0.0002681379555724561 BETTER
I0328 08:38:06.346151 2298410 finetune.py:68] layer 29_v @ epoch 4 new loss 4.635675941244699e-05 old loss 4.654924487113021e-05 BETTER
I0328 08:38:06.677601 2298546 finetune.py:68] layer 30_v @ epoch 2 new loss 7.98508117441088e-05 old loss 8.182031888281927e-05 BETTER
I0328 08:38:27.257568 2298410 finetune.py:45] layer 29_q initial loss 7.843876664992422e-05
I0328 08:38:28.863833 2298265 finetune.py:68] layer 28_q @ epoch 1 new loss 4.847391028306447e-05 old loss 4.929108763462864e-05 BETTER
I0328 08:38:34.417115 2298676 finetune.py:76] layer 31_v @ epoch 1 new loss 0.00015769302262924612 old loss 0.00015102839097380638 WORSE
I0328 08:38:41.764844 2298546 finetune.py:68] layer 30_v @ epoch 3 new loss 7.937444024719298e-05 old loss 7.98508117441088e-05 BETTER
I0328 08:39:00.772082 2298410 finetune.py:68] layer 29_q @ epoch 0 new loss 7.417600863846019e-05 old loss 7.843876664992422e-05 BETTER
I0328 08:39:05.245349 2298265 finetune.py:68] layer 28_q @ epoch 2 new loss 4.782867108588107e-05 old loss 4.847391028306447e-05 BETTER
I0328 08:39:08.784737 2298676 finetune.py:76] layer 31_v @ epoch 2 new loss 0.00015449633065145463 old loss 0.00015102839097380638 WORSE
I0328 08:39:16.895987 2298546 finetune.py:68] layer 30_v @ epoch 4 new loss 7.764904876239598e-05 old loss 7.937444024719298e-05 BETTER
I0328 08:39:35.536620 2298410 finetune.py:68] layer 29_q @ epoch 1 new loss 7.184829155448824e-05 old loss 7.417600863846019e-05 BETTER
I0328 08:39:37.114356 2298546 finetune.py:45] layer 30_q initial loss 9.844409942161292e-05
I0328 08:39:42.148530 2298265 finetune.py:68] layer 28_q @ epoch 3 new loss 4.729550710180774e-05 old loss 4.782867108588107e-05 BETTER
I0328 08:39:44.035401 2298676 finetune.py:76] layer 31_v @ epoch 3 new loss 0.0001541195815661922 old loss 0.00015102839097380638 WORSE
I0328 08:40:10.265151 2298410 finetune.py:68] layer 29_q @ epoch 2 new loss 7.0357826189138e-05 old loss 7.184829155448824e-05 BETTER
I0328 08:40:10.960426 2298546 finetune.py:68] layer 30_q @ epoch 0 new loss 9.642272198107094e-05 old loss 9.844409942161292e-05 BETTER
I0328 08:40:18.661717 2298265 finetune.py:68] layer 28_q @ epoch 4 new loss 4.6920209570089355e-05 old loss 4.729550710180774e-05 BETTER
I0328 08:40:18.709426 2298676 finetune.py:76] layer 31_v @ epoch 4 new loss 0.0001746859634295106 old loss 0.00015102839097380638 WORSE
I0328 08:40:36.827156 2298265 finetune.py:45] layer 28_k initial loss 5.238335870672017e-05
I0328 08:40:39.239726 2298676 finetune.py:45] layer 31_q initial loss 0.00021170038962736726
I0328 08:40:45.361252 2298410 finetune.py:68] layer 29_q @ epoch 3 new loss 6.919652514625341e-05 old loss 7.0357826189138e-05 BETTER
I0328 08:40:46.248659 2298546 finetune.py:68] layer 30_q @ epoch 1 new loss 9.478065476287156e-05 old loss 9.642272198107094e-05 BETTER
I0328 08:41:12.914067 2298265 finetune.py:68] layer 28_k @ epoch 0 new loss 5.170657823327929e-05 old loss 5.238335870672017e-05 BETTER
I0328 08:41:13.806128 2298676 finetune.py:68] layer 31_q @ epoch 0 new loss 0.0002029234019573778 old loss 0.00021170038962736726 BETTER
I0328 08:41:20.064688 2298410 finetune.py:68] layer 29_q @ epoch 4 new loss 6.836225657025352e-05 old loss 6.919652514625341e-05 BETTER
I0328 08:41:20.883480 2298546 finetune.py:68] layer 30_q @ epoch 2 new loss 9.384586883243173e-05 old loss 9.478065476287156e-05 BETTER
I0328 08:41:38.617122 2298410 finetune.py:45] layer 29_k initial loss 7.636559894308448e-05
I0328 08:41:48.465209 2298676 finetune.py:68] layer 31_q @ epoch 1 new loss 0.0001818330929381773 old loss 0.0002029234019573778 BETTER
I0328 08:41:49.011618 2298265 finetune.py:68] layer 28_k @ epoch 1 new loss 5.133400918566622e-05 old loss 5.170657823327929e-05 BETTER
I0328 08:41:56.102544 2298546 finetune.py:68] layer 30_q @ epoch 3 new loss 9.304096602136269e-05 old loss 9.384586883243173e-05 BETTER
I0328 08:42:12.981366 2298410 finetune.py:68] layer 29_k @ epoch 0 new loss 7.535703480243683e-05 old loss 7.636559894308448e-05 BETTER
I0328 08:42:22.956451 2298676 finetune.py:68] layer 31_q @ epoch 2 new loss 0.00017163160373456776 old loss 0.0001818330929381773 BETTER
I0328 08:42:25.205137 2298265 finetune.py:68] layer 28_k @ epoch 2 new loss 5.1091868954245e-05 old loss 5.133400918566622e-05 BETTER
I0328 08:42:30.973115 2298546 finetune.py:68] layer 30_q @ epoch 4 new loss 9.262730600312352e-05 old loss 9.304096602136269e-05 BETTER
I0328 08:42:47.854974 2298410 finetune.py:68] layer 29_k @ epoch 1 new loss 7.45948200346902e-05 old loss 7.535703480243683e-05 BETTER
I0328 08:42:51.343981 2298546 finetune.py:45] layer 30_k initial loss 0.00010437831224408001
I0328 08:42:58.643014 2298676 finetune.py:68] layer 31_q @ epoch 3 new loss 0.00016901384515222162 old loss 0.00017163160373456776 BETTER
I0328 08:43:01.937980 2298265 finetune.py:68] layer 28_k @ epoch 3 new loss 5.078380127088167e-05 old loss 5.1091868954245e-05 BETTER
I0328 08:43:22.291078 2298410 finetune.py:68] layer 29_k @ epoch 2 new loss 7.422611815854907e-05 old loss 7.45948200346902e-05 BETTER
I0328 08:43:25.111733 2298546 finetune.py:68] layer 30_k @ epoch 0 new loss 0.00010313482925994322 old loss 0.00010437831224408001 BETTER
I0328 08:43:33.418330 2298676 finetune.py:76] layer 31_q @ epoch 4 new loss 0.00017463465337641537 old loss 0.00016901384515222162 WORSE
I0328 08:43:38.252477 2298265 finetune.py:68] layer 28_k @ epoch 4 new loss 5.065621007815935e-05 old loss 5.078380127088167e-05 BETTER
I0328 08:43:52.072795 2298676 finetune.py:45] layer 31_k initial loss 0.00020511158800218254
I0328 08:43:58.083076 2298410 finetune.py:68] layer 29_k @ epoch 3 new loss 7.375224959105253e-05 old loss 7.422611815854907e-05 BETTER
I0328 08:44:00.095607 2298265 finetune.py:45] layer 28_o initial loss 0.000125480568385683
I0328 08:44:00.609864 2298546 finetune.py:68] layer 30_k @ epoch 1 new loss 0.00010290529462508857 old loss 0.00010313482925994322 BETTER
I0328 08:44:26.120254 2298676 finetune.py:68] layer 31_k @ epoch 0 new loss 0.00019064730440732092 old loss 0.00020511158800218254 BETTER
I0328 08:44:32.917283 2298410 finetune.py:68] layer 29_k @ epoch 4 new loss 7.360645395237952e-05 old loss 7.375224959105253e-05 BETTER
I0328 08:44:34.712219 2298265 finetune.py:68] layer 28_o @ epoch 0 new loss 0.00012074579717591405 old loss 0.000125480568385683 BETTER
I0328 08:44:35.497166 2298546 finetune.py:76] layer 30_k @ epoch 2 new loss 0.00010336852574255317 old loss 0.00010290529462508857 WORSE
I0328 08:44:54.833095 2298410 finetune.py:45] layer 29_o initial loss 0.00014925154391676188
I0328 08:45:01.706962 2298676 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0001838119642343372 old loss 0.00019064730440732092 BETTER
I0328 08:45:09.709273 2298546 finetune.py:68] layer 30_k @ epoch 3 new loss 0.00010187616862822324 old loss 0.00010290529462508857 BETTER
I0328 08:45:10.858954 2298265 finetune.py:68] layer 28_o @ epoch 1 new loss 0.00011888606968568638 old loss 0.00012074579717591405 BETTER
I0328 08:45:27.575767 2298410 finetune.py:68] layer 29_o @ epoch 0 new loss 0.0001439098414266482 old loss 0.00014925154391676188 BETTER
I0328 08:45:36.164523 2298676 finetune.py:68] layer 31_k @ epoch 2 new loss 0.0001795293064787984 old loss 0.0001838119642343372 BETTER
I0328 08:45:46.042849 2298546 finetune.py:68] layer 30_k @ epoch 4 new loss 0.00010175675561185926 old loss 0.00010187616862822324 BETTER
I0328 08:45:47.301031 2298265 finetune.py:68] layer 28_o @ epoch 2 new loss 0.00011751184501918033 old loss 0.00011888606968568638 BETTER
I0328 08:46:01.117202 2298410 finetune.py:68] layer 29_o @ epoch 1 new loss 0.00014188705245032907 old loss 0.0001439098414266482 BETTER
I0328 08:46:08.313177 2298546 finetune.py:45] layer 30_o initial loss 0.00022872842964716256
I0328 08:46:12.430780 2298676 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0001784887135727331 old loss 0.0001795293064787984 BETTER
I0328 08:46:23.298221 2298265 finetune.py:68] layer 28_o @ epoch 3 new loss 0.00011647103383438662 old loss 0.00011751184501918033 BETTER
I0328 08:46:35.780851 2298410 finetune.py:68] layer 29_o @ epoch 2 new loss 0.00014053615450393409 old loss 0.00014188705245032907 BETTER
I0328 08:46:41.327208 2298546 finetune.py:68] layer 30_o @ epoch 0 new loss 0.0002223474148195237 old loss 0.00022872842964716256 BETTER
I0328 08:46:47.059091 2298676 finetune.py:76] layer 31_k @ epoch 4 new loss 0.00017912023758981377 old loss 0.0001784887135727331 WORSE
I0328 08:46:59.383037 2298265 finetune.py:68] layer 28_o @ epoch 4 new loss 0.00011559034464880824 old loss 0.00011647103383438662 BETTER
I0328 08:47:08.465729 2298676 finetune.py:45] layer 31_o initial loss 0.0004357603029347956
I0328 08:47:10.730559 2298410 finetune.py:68] layer 29_o @ epoch 3 new loss 0.00013946511899121106 old loss 0.00014053615450393409 BETTER
I0328 08:47:15.987496 2298546 finetune.py:68] layer 30_o @ epoch 1 new loss 0.00021934251708444208 old loss 0.0002223474148195237 BETTER
I0328 08:47:34.266120 2298265 finetune.py:45] layer 28_up initial loss 0.00032381864730268717
I0328 08:47:42.240590 2298676 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0003906543424818665 old loss 0.0004357603029347956 BETTER
I0328 08:47:45.546914 2298410 finetune.py:68] layer 29_o @ epoch 4 new loss 0.00013856483565177768 old loss 0.00013946511899121106 BETTER
I0328 08:47:50.291511 2298546 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0002173523826058954 old loss 0.00021934251708444208 BETTER
I0328 08:48:06.851311 2298265 finetune.py:68] layer 28_up @ epoch 0 new loss 0.00031897317967377603 old loss 0.00032381864730268717 BETTER
I0328 08:48:16.630240 2298676 finetune.py:68] layer 31_o @ epoch 1 new loss 0.00037425453774631023 old loss 0.0003906543424818665 BETTER
I0328 08:48:21.278995 2298410 finetune.py:45] layer 29_up initial loss 0.00042588490759953856
I0328 08:48:25.326286 2298546 finetune.py:68] layer 30_o @ epoch 3 new loss 0.00021593175188172609 old loss 0.0002173523826058954 BETTER
I0328 08:48:40.216927 2298265 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0003157106111757457 old loss 0.00031897317967377603 BETTER
I0328 08:48:51.685263 2298676 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0003637889167293906 old loss 0.00037425453774631023 BETTER
I0328 08:48:53.019895 2298410 finetune.py:68] layer 29_up @ epoch 0 new loss 0.00041835775482468307 old loss 0.00042588490759953856 BETTER
I0328 08:48:59.519040 2298546 finetune.py:68] layer 30_o @ epoch 4 new loss 0.00021464873861987144 old loss 0.00021593175188172609 BETTER
I0328 08:49:13.932899 2298265 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0003131856210529804 old loss 0.0003157106111757457 BETTER
I0328 08:49:25.987286 2298410 finetune.py:68] layer 29_up @ epoch 1 new loss 0.00041355888242833316 old loss 0.00041835775482468307 BETTER
I0328 08:49:27.202975 2298676 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0003553488350007683 old loss 0.0003637889167293906 BETTER
I0328 08:49:37.258139 2298546 finetune.py:45] layer 30_up initial loss 0.0007813885458745062
I0328 08:49:48.119894 2298265 finetune.py:68] layer 28_up @ epoch 3 new loss 0.0003109875542577356 old loss 0.0003131856210529804 BETTER
I0328 08:49:59.049590 2298410 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0004097408091183752 old loss 0.00041355888242833316 BETTER
I0328 08:50:02.228593 2298676 finetune.py:68] layer 31_o @ epoch 4 new loss 0.00034970257547684014 old loss 0.0003553488350007683 BETTER
I0328 08:50:08.303209 2298546 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0007613382767885923 old loss 0.0007813885458745062 BETTER
I0328 08:50:22.286530 2298265 finetune.py:68] layer 28_up @ epoch 4 new loss 0.00030908413464203477 old loss 0.0003109875542577356 BETTER
I0328 08:50:31.280765 2298410 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0004064007953274995 old loss 0.0004097408091183752 BETTER
I0328 08:50:39.123314 2298676 finetune.py:45] layer 31_up initial loss 0.0022988300770521164
I0328 08:50:41.414758 2298546 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0007481635548174381 old loss 0.0007613382767885923 BETTER
I0328 08:50:58.557874 2298265 finetune.py:45] layer 28_gate initial loss 0.0004147546424064785
I0328 08:51:04.045596 2298410 finetune.py:68] layer 29_up @ epoch 4 new loss 0.00040361794526688755 old loss 0.0004064007953274995 BETTER
I0328 08:51:10.319548 2298676 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0021720139775425196 old loss 0.0022988300770521164 BETTER
I0328 08:51:14.410752 2298546 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0007371471147052944 old loss 0.0007481635548174381 BETTER
I0328 08:51:29.172185 2298265 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.00041264179162681103 old loss 0.0004147546424064785 BETTER
I0328 08:51:40.272162 2298410 finetune.py:45] layer 29_gate initial loss 0.0005443820264190435
I0328 08:51:43.597984 2298676 finetune.py:68] layer 31_up @ epoch 1 new loss 0.002092757262289524 old loss 0.0021720139775425196 BETTER
I0328 08:51:47.660553 2298546 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0007279133424162865 old loss 0.0007371471147052944 BETTER
I0328 08:52:01.371063 2298265 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0004109055735170841 old loss 0.00041264179162681103 BETTER
I0328 08:52:09.318857 2298410 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0005414033657871187 old loss 0.0005443820264190435 BETTER
I0328 08:52:17.030600 2298676 finetune.py:68] layer 31_up @ epoch 2 new loss 0.002029257593676448 old loss 0.002092757262289524 BETTER
I0328 08:52:21.108159 2298546 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0007200586842373013 old loss 0.0007279133424162865 BETTER
I0328 08:52:33.083317 2298265 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.0004093162133358419 old loss 0.0004109055735170841 BETTER
I0328 08:52:39.819687 2298410 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.000539091881364584 old loss 0.0005414033657871187 BETTER
I0328 08:52:50.046794 2298676 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0019738469272851944 old loss 0.002029257593676448 BETTER
I0328 08:52:58.491334 2298546 finetune.py:45] layer 30_gate initial loss 0.0009251576266251504
I0328 08:53:05.392848 2298265 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.00040800494025461376 old loss 0.0004093162133358419 BETTER
I0328 08:53:10.202118 2298410 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0005370209692046046 old loss 0.000539091881364584 BETTER
I0328 08:53:23.382661 2298676 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0019247173331677914 old loss 0.0019738469272851944 BETTER
I0328 08:53:27.426885 2298546 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0009171719430014491 old loss 0.0009251576266251504 BETTER
I0328 08:53:37.348564 2298265 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.00040675236959941685 old loss 0.00040800494025461376 BETTER
I0328 08:53:41.051457 2298410 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0005352570442482829 old loss 0.0005370209692046046 BETTER
I0328 08:53:58.364542 2298546 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.0009110849350690842 old loss 0.0009171719430014491 BETTER
I0328 08:54:00.009823 2298676 finetune.py:45] layer 31_gate initial loss 0.002325834007933736
I0328 08:54:11.122318 2298410 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0005337262409739196 old loss 0.0005352570442482829 BETTER
I0328 08:54:29.269399 2298546 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0009057738352566957 old loss 0.0009110849350690842 BETTER
I0328 08:54:29.584208 2298676 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.002280675806105137 old loss 0.002325834007933736 BETTER
I0328 08:54:37.824197 2298265 finetune.py:45] layer 28_down initial loss 0.0006694499170407653
I0328 08:55:00.490344 2298546 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.000900871295016259 old loss 0.0009057738352566957 BETTER
I0328 08:55:00.577212 2298676 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.002247363096103072 old loss 0.002280675806105137 BETTER
I0328 08:55:05.646170 2298265 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0006694251205772161 old loss 0.0006694499170407653 BETTER
I0328 08:55:13.406617 2298410 finetune.py:45] layer 29_down initial loss 0.000883877684827894
I0328 08:55:32.244786 2298546 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0008966770255938172 old loss 0.000900871295016259 BETTER
I0328 08:55:32.457066 2298676 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.002220263471826911 old loss 0.002247363096103072 BETTER
I0328 08:55:35.382773 2298265 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0006694035255350173 old loss 0.0006694251205772161 BETTER
I0328 08:55:40.592056 2298410 finetune.py:68] layer 29_down @ epoch 0 new loss 0.000883838685695082 old loss 0.000883877684827894 BETTER
I0328 08:56:03.807484 2298676 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.0021942835301160812 old loss 0.002220263471826911 BETTER
I0328 08:56:04.837639 2298265 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0006693844916298985 old loss 0.0006694035255350173 BETTER
I0328 08:56:08.360255 2298410 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0008838018984533846 old loss 0.000883838685695082 BETTER
I0328 08:56:35.450199 2298265 finetune.py:68] layer 28_down @ epoch 3 new loss 0.0006693677860312164 old loss 0.0006693844916298985 BETTER
I0328 08:56:35.730767 2298546 finetune.py:45] layer 30_down initial loss 0.0014531132765114307
I0328 08:56:35.997115 2298676 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.0021707089617848396 old loss 0.0021942835301160812 BETTER
I0328 08:56:37.749921 2298410 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0008837755885906518 old loss 0.0008838018984533846 BETTER
I0328 08:57:02.798730 2298546 finetune.py:68] layer 30_down @ epoch 0 new loss 0.0014530615881085396 old loss 0.0014531132765114307 BETTER
I0328 08:57:04.967001 2298265 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0006693534087389708 old loss 0.0006693677860312164 BETTER
I0328 08:57:06.233348 2298410 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0008837526547722518 old loss 0.0008837755885906518 BETTER
28_v proxy err 0.011512141674757004 tr(WHW.T) 601.4844360351562
bpp_loss 3.240106967277825
28_q proxy err 0.0012328102020546794 tr(WHW.T) 23176.83203125
bpp_loss 3.4753784055355936
28_k proxy err 0.0005264454521238804 tr(WHW.T) 14985.248046875
bpp_loss 4.18504592590034
28_o proxy err 0.010420090518891811 tr(WHW.T) 2509.163330078125
bpp_loss 3.0618257126770914
28_up proxy err 0.012304071336984634 tr(WHW.T) 10250.8193359375
bpp_loss 3.0289034597309574
28_gate proxy err 0.0037439456209540367 tr(WHW.T) 35916.50390625
bpp_loss 3.3448714587305273
28_down proxy err 0.013682616874575615 tr(WHW.T) 7248.30419921875
bpp_loss 3.013299362052099
I0328 08:57:30.359982 2298546 finetune.py:68] layer 30_down @ epoch 1 new loss 0.0014530136249959469 old loss 0.0014530615881085396 BETTER
I0328 08:57:34.202329 2298410 finetune.py:68] layer 29_down @ epoch 4 new loss 0.0008837252389639616 old loss 0.0008837526547722518 BETTER
I0328 08:57:36.565936 2298676 finetune.py:45] layer 31_down initial loss 0.0037131505087018013
29_v proxy err 0.008859463967382908 tr(WHW.T) 850.4290161132812
bpp_loss 3.304861934739165
29_q proxy err 0.001485321088694036 tr(WHW.T) 20661.22265625
bpp_loss 3.466782957140822
29_k proxy err 0.0005250644753687084 tr(WHW.T) 16359.7998046875
bpp_loss 4.261561846418772
29_o proxy err 0.006586949806660414 tr(WHW.T) 3105.738525390625
bpp_loss 3.09699562296737
29_up proxy err 0.009581721387803555 tr(WHW.T) 12877.7490234375
bpp_loss 3.0610275022419438
29_gate proxy err 0.0034084783401340246 tr(WHW.T) 38320.26171875
bpp_loss 3.336148329012628
29_down proxy err 0.01122230477631092 tr(WHW.T) 7505.92822265625
bpp_loss 3.0264316509211704
I0328 08:57:58.088043 2298546 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0014529642648994923 old loss 0.0014530136249959469 BETTER
I0328 08:58:03.088927 2298676 finetune.py:68] layer 31_down @ epoch 0 new loss 0.0037128510884940624 old loss 0.0037131505087018013 BETTER
I0328 08:58:25.820197 2298546 finetune.py:68] layer 30_down @ epoch 3 new loss 0.0014529206091538072 old loss 0.0014529642648994923 BETTER
I0328 08:58:30.623445 2298676 finetune.py:68] layer 31_down @ epoch 1 new loss 0.0037125847302377224 old loss 0.0037128510884940624 BETTER
I0328 08:58:53.575594 2298546 finetune.py:68] layer 30_down @ epoch 4 new loss 0.0014528795145452023 old loss 0.0014529206091538072 BETTER
30_v proxy err 0.008092669770121574 tr(WHW.T) 863.060791015625
bpp_loss 3.560670319304336
30_q proxy err 0.0011131031205877662 tr(WHW.T) 24068.1171875
bpp_loss 3.3758743525249884
30_k proxy err 0.0005224320339038968 tr(WHW.T) 14023.71484375
bpp_loss 3.9617789620533586
30_o proxy err 0.004816149361431599 tr(WHW.T) 4874.6630859375
bpp_loss 3.177963017078582
30_up proxy err 0.00566996680572629 tr(WHW.T) 21695.40625
bpp_loss 3.091783242499722
30_gate proxy err 0.002523522125557065 tr(WHW.T) 51793.76171875
bpp_loss 3.3815283313659683
30_down proxy err 0.006385449785739183 tr(WHW.T) 8847.5615234375
bpp_loss 3.02575011963823
I0328 08:58:58.333509 2298676 finetune.py:68] layer 31_down @ epoch 2 new loss 0.0037123472429811954 old loss 0.0037125847302377224 BETTER
I0328 08:59:26.265555 2298676 finetune.py:68] layer 31_down @ epoch 3 new loss 0.003712114179506898 old loss 0.0037123472429811954 BETTER
I0328 08:59:54.012867 2298676 finetune.py:68] layer 31_down @ epoch 4 new loss 0.0037118971813470125 old loss 0.003712114179506898 BETTER
31_v proxy err 0.004055958706885576 tr(WHW.T) 1808.723876953125
bpp_loss 3.4014778035343625
31_q proxy err 0.000644434301648289 tr(WHW.T) 46185.078125
bpp_loss 3.514714469842147
31_k proxy err 0.00039920813287608325 tr(WHW.T) 20464.12109375
bpp_loss 4.16809860127978
31_o proxy err 0.003976029809564352 tr(WHW.T) 2213.546875
bpp_loss 3.1334287517820485
31_up proxy err 0.0017565302550792694 tr(WHW.T) 69012.203125
bpp_loss 3.272709697418447
31_gate proxy err 0.0008973903604783118 tr(WHW.T) 143579.828125
bpp_loss 3.577779886645398
31_down proxy err 0.002442697063088417 tr(WHW.T) 9998.0341796875
bpp_loss 3.049753579710211
I0328 09:00:21.095180 2300078 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:00:21.095390 2300078 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:00:21.095428 2300078 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:00:21.421347 2300078 config.py:54] PyTorch version 2.6.0 available.
W0328 09:00:21.635369 2300078 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 09:00:21.745196 2300078 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.60it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.46it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.79it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  9.02it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  9.20it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.13it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.37it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.06it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  9.19it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  9.36it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  9.34it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  9.33it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  9.33it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.26it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.39it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.34it/s]
I0328 09:00:24.639908 2300078 hfize_llama.py:153] loaded layer 0
I0328 09:00:25.290208 2300078 hfize_llama.py:153] loaded layer 1
I0328 09:00:25.904135 2300078 hfize_llama.py:153] loaded layer 2
I0328 09:00:26.559254 2300078 hfize_llama.py:153] loaded layer 3
I0328 09:00:27.188136 2300078 hfize_llama.py:153] loaded layer 4
I0328 09:00:27.792797 2300078 hfize_llama.py:153] loaded layer 5
I0328 09:00:28.412250 2300078 hfize_llama.py:153] loaded layer 6
I0328 09:00:29.062556 2300078 hfize_llama.py:153] loaded layer 7
I0328 09:00:29.705710 2300078 hfize_llama.py:153] loaded layer 8
I0328 09:00:30.294601 2300078 hfize_llama.py:153] loaded layer 9
I0328 09:00:30.920205 2300078 hfize_llama.py:153] loaded layer 10
I0328 09:00:31.592545 2300078 hfize_llama.py:153] loaded layer 11
I0328 09:00:32.244596 2300078 hfize_llama.py:153] loaded layer 12
I0328 09:00:32.897579 2300078 hfize_llama.py:153] loaded layer 13
I0328 09:00:33.611274 2300078 hfize_llama.py:153] loaded layer 14
I0328 09:00:34.272841 2300078 hfize_llama.py:153] loaded layer 15
I0328 09:00:35.021325 2300078 hfize_llama.py:153] loaded layer 16
I0328 09:00:35.682857 2300078 hfize_llama.py:153] loaded layer 17
I0328 09:00:36.340158 2300078 hfize_llama.py:153] loaded layer 18
I0328 09:00:37.005082 2300078 hfize_llama.py:153] loaded layer 19
I0328 09:00:37.696111 2300078 hfize_llama.py:153] loaded layer 20
I0328 09:00:38.344796 2300078 hfize_llama.py:153] loaded layer 21
I0328 09:00:38.993998 2300078 hfize_llama.py:153] loaded layer 22
I0328 09:00:39.612656 2300078 hfize_llama.py:153] loaded layer 23
I0328 09:00:40.223037 2300078 hfize_llama.py:153] loaded layer 24
I0328 09:00:40.850771 2300078 hfize_llama.py:153] loaded layer 25
I0328 09:00:41.512278 2300078 hfize_llama.py:153] loaded layer 26
I0328 09:00:42.196260 2300078 hfize_llama.py:153] loaded layer 27
I0328 09:00:42.804507 2300078 hfize_llama.py:153] loaded layer 28
I0328 09:00:43.453137 2300078 hfize_llama.py:153] loaded layer 29
I0328 09:00:44.063253 2300078 hfize_llama.py:153] loaded layer 30
I0328 09:00:44.747658 2300078 hfize_llama.py:153] loaded layer 31
I0328 09:00:44.747787 2300078 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.21s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.14s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.02s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:02,  1.03it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:04<00:01,  1.10it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:05<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.15it/s]
I0328 09:01:30.806644 2300078 hfize_llama.py:167] successfully loaded hfized model
I0328 09:01:35.638405 2300368 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:01:35.638587 2300368 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:01:35.638630 2300368 utils.py:162] NumExpr defaulting to 16 threads.
W0328 09:01:35.987443 2300368 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 09:01:36.362032 2300368 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.09s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.08s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.17s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.14s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.09s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.05s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.01s/it]
I0328 09:01:43.569809 2300368 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.7039461135864258:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.7039461135864258:   1%|          | 1/141 [00:01<04:29,  1.93s/it]avg_loss = 1.9924499988555908:   1%|          | 1/141 [00:03<04:29,  1.93s/it]avg_loss = 1.9924499988555908:   1%|▏         | 2/141 [00:03<03:51,  1.66s/it]avg_loss = 2.113277037938436:   1%|▏         | 2/141 [00:04<03:51,  1.66s/it] avg_loss = 2.113277037938436:   2%|▏         | 3/141 [00:04<03:38,  1.58s/it]avg_loss = 2.0598576068878174:   2%|▏         | 3/141 [00:06<03:38,  1.58s/it]avg_loss = 2.0598576068878174:   3%|▎         | 4/141 [00:06<03:31,  1.54s/it]avg_loss = 2.0056881666183473:   3%|▎         | 4/141 [00:07<03:31,  1.54s/it]avg_loss = 2.0056881666183473:   4%|▎         | 5/141 [00:07<03:27,  1.53s/it]avg_loss = 1.9075188636779785:   4%|▎         | 5/141 [00:09<03:27,  1.53s/it]avg_loss = 1.9075188636779785:   4%|▍         | 6/141 [00:09<03:24,  1.52s/it]avg_loss = 1.8473725318908691:   4%|▍         | 6/141 [00:10<03:24,  1.52s/it]avg_loss = 1.8473725318908691:   5%|▍         | 7/141 [00:10<03:22,  1.51s/it]avg_loss = 1.8428235203027725:   5%|▍         | 7/141 [00:12<03:22,  1.51s/it]avg_loss = 1.8428235203027725:   6%|▌         | 8/141 [00:12<03:20,  1.51s/it]avg_loss = 1.8751659790674846:   6%|▌         | 8/141 [00:13<03:20,  1.51s/it]avg_loss = 1.8751659790674846:   6%|▋         | 9/141 [00:13<03:19,  1.51s/it]avg_loss = 1.8742950558662415:   6%|▋         | 9/141 [00:15<03:19,  1.51s/it]avg_loss = 1.8742950558662415:   7%|▋         | 10/141 [00:15<03:18,  1.51s/it]avg_loss = 1.867973186753013:   7%|▋         | 10/141 [00:16<03:18,  1.51s/it] avg_loss = 1.867973186753013:   8%|▊         | 11/141 [00:16<03:16,  1.51s/it]avg_loss = 1.8904710511366527:   8%|▊         | 11/141 [00:18<03:16,  1.51s/it]avg_loss = 1.8904710511366527:   9%|▊         | 12/141 [00:18<03:15,  1.52s/it]avg_loss = 1.9018808970084558:   9%|▊         | 12/141 [00:19<03:15,  1.52s/it]avg_loss = 1.9018808970084558:   9%|▉         | 13/141 [00:19<03:14,  1.52s/it]avg_loss = 1.918815553188324:   9%|▉         | 13/141 [00:21<03:14,  1.52s/it] avg_loss = 1.918815553188324:  10%|▉         | 14/141 [00:21<03:13,  1.52s/it]avg_loss = 1.928199871381124:  10%|▉         | 14/141 [00:23<03:13,  1.52s/it]avg_loss = 1.928199871381124:  11%|█         | 15/141 [00:23<03:11,  1.52s/it]avg_loss = 1.951289676129818:  11%|█         | 15/141 [00:24<03:11,  1.52s/it]avg_loss = 1.951289676129818:  11%|█▏        | 16/141 [00:24<03:10,  1.52s/it]avg_loss = 1.9537197491701912:  11%|█▏        | 16/141 [00:26<03:10,  1.52s/it]avg_loss = 1.9537197491701912:  12%|█▏        | 17/141 [00:26<03:09,  1.53s/it]avg_loss = 1.9563414322005377:  12%|█▏        | 17/141 [00:27<03:09,  1.53s/it]avg_loss = 1.9563414322005377:  13%|█▎        | 18/141 [00:27<03:08,  1.53s/it]avg_loss = 1.9438657321427997:  13%|█▎        | 18/141 [00:29<03:08,  1.53s/it]avg_loss = 1.9438657321427997:  13%|█▎        | 19/141 [00:29<03:06,  1.53s/it]avg_loss = 1.9427814543247224:  13%|█▎        | 19/141 [00:30<03:06,  1.53s/it]avg_loss = 1.9427814543247224:  14%|█▍        | 20/141 [00:30<03:05,  1.53s/it]avg_loss = 1.9477157195409138:  14%|█▍        | 20/141 [00:32<03:05,  1.53s/it]avg_loss = 1.9477157195409138:  15%|█▍        | 21/141 [00:32<03:04,  1.53s/it]avg_loss = 1.9499004591595044:  15%|█▍        | 21/141 [00:33<03:04,  1.53s/it]avg_loss = 1.9499004591595044:  16%|█▌        | 22/141 [00:33<03:02,  1.54s/it]avg_loss = 1.9511997855227927:  16%|█▌        | 22/141 [00:35<03:02,  1.54s/it]avg_loss = 1.9511997855227927:  16%|█▋        | 23/141 [00:35<03:01,  1.54s/it]avg_loss = 1.955707515279452:  16%|█▋        | 23/141 [00:36<03:01,  1.54s/it] avg_loss = 1.955707515279452:  17%|█▋        | 24/141 [00:36<03:00,  1.54s/it]avg_loss = 1.9614969301223755:  17%|█▋        | 24/141 [00:38<03:00,  1.54s/it]avg_loss = 1.9614969301223755:  18%|█▊        | 25/141 [00:38<02:59,  1.54s/it]avg_loss = 1.9724497199058533:  18%|█▊        | 25/141 [00:39<02:59,  1.54s/it]avg_loss = 1.9724497199058533:  18%|█▊        | 26/141 [00:39<02:57,  1.55s/it]avg_loss = 1.9839326408174303:  18%|█▊        | 26/141 [00:41<02:57,  1.55s/it]avg_loss = 1.9839326408174303:  19%|█▉        | 27/141 [00:41<02:56,  1.55s/it]avg_loss = 1.9890302291938238:  19%|█▉        | 27/141 [00:43<02:56,  1.55s/it]avg_loss = 1.9890302291938238:  20%|█▉        | 28/141 [00:43<02:54,  1.55s/it]avg_loss = 1.9854920938097198:  20%|█▉        | 28/141 [00:44<02:54,  1.55s/it]avg_loss = 1.9854920938097198:  21%|██        | 29/141 [00:44<02:53,  1.55s/it]avg_loss = 1.9761952837308248:  21%|██        | 29/141 [00:46<02:53,  1.55s/it]avg_loss = 1.9761952837308248:  21%|██▏       | 30/141 [00:46<02:51,  1.55s/it]avg_loss = 1.9636437393003894:  21%|██▏       | 30/141 [00:47<02:51,  1.55s/it]avg_loss = 1.9636437393003894:  22%|██▏       | 31/141 [00:47<02:50,  1.55s/it]avg_loss = 1.9524513259530067:  22%|██▏       | 31/141 [00:49<02:50,  1.55s/it]avg_loss = 1.9524513259530067:  23%|██▎       | 32/141 [00:49<02:49,  1.55s/it]avg_loss = 1.9502631172989353:  23%|██▎       | 32/141 [00:50<02:49,  1.55s/it]avg_loss = 1.9502631172989353:  23%|██▎       | 33/141 [00:50<02:47,  1.55s/it]avg_loss = 1.9488495132502388:  23%|██▎       | 33/141 [00:52<02:47,  1.55s/it]avg_loss = 1.9488495132502388:  24%|██▍       | 34/141 [00:52<02:46,  1.55s/it]avg_loss = 1.9508006538663591:  24%|██▍       | 34/141 [00:53<02:46,  1.55s/it]avg_loss = 1.9508006538663591:  25%|██▍       | 35/141 [00:53<02:44,  1.55s/it]avg_loss = 1.934600270456738:  25%|██▍       | 35/141 [00:55<02:44,  1.55s/it] avg_loss = 1.934600270456738:  26%|██▌       | 36/141 [00:55<02:43,  1.55s/it]avg_loss = 1.9190491888974164:  26%|██▌       | 36/141 [00:57<02:43,  1.55s/it]avg_loss = 1.9190491888974164:  26%|██▌       | 37/141 [00:57<02:41,  1.55s/it]avg_loss = 1.9038339100385968:  26%|██▌       | 37/141 [00:58<02:41,  1.55s/it]avg_loss = 1.9038339100385968:  27%|██▋       | 38/141 [00:58<02:40,  1.56s/it]avg_loss = 1.88905258056445:  27%|██▋       | 38/141 [01:00<02:40,  1.56s/it]  avg_loss = 1.88905258056445:  28%|██▊       | 39/141 [01:00<02:38,  1.56s/it]avg_loss = 1.8802833646535873:  28%|██▊       | 39/141 [01:01<02:38,  1.56s/it]avg_loss = 1.8802833646535873:  28%|██▊       | 40/141 [01:01<02:37,  1.56s/it]avg_loss = 1.885309885187847:  28%|██▊       | 40/141 [01:03<02:37,  1.56s/it] avg_loss = 1.885309885187847:  29%|██▉       | 41/141 [01:03<02:35,  1.56s/it]avg_loss = 1.9017611089206876:  29%|██▉       | 41/141 [01:04<02:35,  1.56s/it]avg_loss = 1.9017611089206876:  30%|██▉       | 42/141 [01:04<02:34,  1.56s/it]avg_loss = 1.9182204950687498:  30%|██▉       | 42/141 [01:06<02:34,  1.56s/it]avg_loss = 1.9182204950687498:  30%|███       | 43/141 [01:06<02:32,  1.56s/it]avg_loss = 1.9225594049150294:  30%|███       | 43/141 [01:07<02:32,  1.56s/it]avg_loss = 1.9225594049150294:  31%|███       | 44/141 [01:07<02:31,  1.56s/it]avg_loss = 1.9275046004189385:  31%|███       | 44/141 [01:09<02:31,  1.56s/it]avg_loss = 1.9275046004189385:  32%|███▏      | 45/141 [01:09<02:30,  1.56s/it]avg_loss = 1.9320032052371814:  32%|███▏      | 45/141 [01:11<02:30,  1.56s/it]avg_loss = 1.9320032052371814:  33%|███▎      | 46/141 [01:11<02:28,  1.57s/it]avg_loss = 1.937932884439509:  33%|███▎      | 46/141 [01:12<02:28,  1.57s/it] avg_loss = 1.937932884439509:  33%|███▎      | 47/141 [01:12<02:27,  1.57s/it]avg_loss = 1.9405699198444684:  33%|███▎      | 47/141 [01:14<02:27,  1.57s/it]avg_loss = 1.9405699198444684:  34%|███▍      | 48/141 [01:14<02:25,  1.57s/it]avg_loss = 1.939955295348654:  34%|███▍      | 48/141 [01:15<02:25,  1.57s/it] avg_loss = 1.939955295348654:  35%|███▍      | 49/141 [01:15<02:24,  1.57s/it]avg_loss = 1.939538836479187:  35%|███▍      | 49/141 [01:17<02:24,  1.57s/it]avg_loss = 1.939538836479187:  35%|███▌      | 50/141 [01:17<02:22,  1.57s/it]avg_loss = 1.9337797375286327:  35%|███▌      | 50/141 [01:18<02:22,  1.57s/it]avg_loss = 1.9337797375286327:  36%|███▌      | 51/141 [01:18<02:21,  1.57s/it]avg_loss = 1.9293822371042693:  36%|███▌      | 51/141 [01:20<02:21,  1.57s/it]avg_loss = 1.9293822371042693:  37%|███▋      | 52/141 [01:20<02:19,  1.57s/it]avg_loss = 1.9223367425630677:  37%|███▋      | 52/141 [01:22<02:19,  1.57s/it]avg_loss = 1.9223367425630677:  38%|███▊      | 53/141 [01:22<02:18,  1.57s/it]avg_loss = 1.9192747584095708:  38%|███▊      | 53/141 [01:23<02:18,  1.57s/it]avg_loss = 1.9192747584095708:  38%|███▊      | 54/141 [01:23<02:16,  1.57s/it]avg_loss = 1.9114546407352795:  38%|███▊      | 54/141 [01:25<02:16,  1.57s/it]avg_loss = 1.9114546407352795:  39%|███▉      | 55/141 [01:25<02:15,  1.57s/it]avg_loss = 1.9034804957253593:  39%|███▉      | 55/141 [01:26<02:15,  1.57s/it]avg_loss = 1.9034804957253593:  40%|███▉      | 56/141 [01:26<02:13,  1.57s/it]avg_loss = 1.8992892022718464:  40%|███▉      | 56/141 [01:28<02:13,  1.57s/it]avg_loss = 1.8992892022718464:  40%|████      | 57/141 [01:28<02:12,  1.57s/it]avg_loss = 1.896327775100182:  40%|████      | 57/141 [01:29<02:12,  1.57s/it] avg_loss = 1.896327775100182:  41%|████      | 58/141 [01:29<02:10,  1.57s/it]avg_loss = 1.898340091866962:  41%|████      | 58/141 [01:31<02:10,  1.57s/it]avg_loss = 1.898340091866962:  42%|████▏     | 59/141 [01:31<02:09,  1.57s/it]avg_loss = 1.9036219437917075:  42%|████▏     | 59/141 [01:33<02:09,  1.57s/it]avg_loss = 1.9036219437917075:  43%|████▎     | 60/141 [01:33<02:07,  1.57s/it]avg_loss = 1.9089839692975654:  43%|████▎     | 60/141 [01:34<02:07,  1.57s/it]avg_loss = 1.9089839692975654:  43%|████▎     | 61/141 [01:34<02:05,  1.57s/it]avg_loss = 1.9162381925890524:  43%|████▎     | 61/141 [01:36<02:05,  1.57s/it]avg_loss = 1.9162381925890524:  44%|████▍     | 62/141 [01:36<02:04,  1.57s/it]avg_loss = 1.9074298900271218:  44%|████▍     | 62/141 [01:37<02:04,  1.57s/it]avg_loss = 1.9074298900271218:  45%|████▍     | 63/141 [01:37<02:02,  1.58s/it]avg_loss = 1.9050018098205328:  45%|████▍     | 63/141 [01:39<02:02,  1.58s/it]avg_loss = 1.9050018098205328:  45%|████▌     | 64/141 [01:39<02:01,  1.58s/it]avg_loss = 1.9024874081978431:  45%|████▌     | 64/141 [01:40<02:01,  1.58s/it]avg_loss = 1.9024874081978431:  46%|████▌     | 65/141 [01:40<01:59,  1.58s/it]avg_loss = 1.8965259031815962:  46%|████▌     | 65/141 [01:42<01:59,  1.58s/it]avg_loss = 1.8965259031815962:  47%|████▋     | 66/141 [01:42<01:58,  1.58s/it]avg_loss = 1.8937730682429983:  47%|████▋     | 66/141 [01:44<01:58,  1.58s/it]avg_loss = 1.8937730682429983:  48%|████▊     | 67/141 [01:44<01:56,  1.58s/it]avg_loss = 1.8905625343322754:  48%|████▊     | 67/141 [01:45<01:56,  1.58s/it]avg_loss = 1.8905625343322754:  48%|████▊     | 68/141 [01:45<01:55,  1.58s/it]avg_loss = 1.8877711175144583:  48%|████▊     | 68/141 [01:47<01:55,  1.58s/it]avg_loss = 1.8877711175144583:  49%|████▉     | 69/141 [01:47<01:53,  1.58s/it]avg_loss = 1.888616534641811:  49%|████▉     | 69/141 [01:48<01:53,  1.58s/it] avg_loss = 1.888616534641811:  50%|████▉     | 70/141 [01:48<01:52,  1.58s/it]avg_loss = 1.8921251901438538:  50%|████▉     | 70/141 [01:50<01:52,  1.58s/it]avg_loss = 1.8921251901438538:  50%|█████     | 71/141 [01:50<01:50,  1.58s/it]avg_loss = 1.894471397002538:  50%|█████     | 71/141 [01:52<01:50,  1.58s/it] avg_loss = 1.894471397002538:  51%|█████     | 72/141 [01:52<01:49,  1.58s/it]avg_loss = 1.8929780460383794:  51%|█████     | 72/141 [01:53<01:49,  1.58s/it]avg_loss = 1.8929780460383794:  52%|█████▏    | 73/141 [01:53<01:47,  1.58s/it]avg_loss = 1.8947367297636497:  52%|█████▏    | 73/141 [01:55<01:47,  1.58s/it]avg_loss = 1.8947367297636497:  52%|█████▏    | 74/141 [01:55<01:45,  1.58s/it]avg_loss = 1.8947173643112183:  52%|█████▏    | 74/141 [01:56<01:45,  1.58s/it]avg_loss = 1.8947173643112183:  53%|█████▎    | 75/141 [01:56<01:44,  1.58s/it]avg_loss = 1.8934028729012138:  53%|█████▎    | 75/141 [01:58<01:44,  1.58s/it]avg_loss = 1.8934028729012138:  54%|█████▍    | 76/141 [01:58<01:42,  1.58s/it]avg_loss = 1.894557234528777:  54%|█████▍    | 76/141 [01:59<01:42,  1.58s/it] avg_loss = 1.894557234528777:  55%|█████▍    | 77/141 [01:59<01:41,  1.58s/it]avg_loss = 1.8966828706936958:  55%|█████▍    | 77/141 [02:01<01:41,  1.58s/it]avg_loss = 1.8966828706936958:  55%|█████▌    | 78/141 [02:01<01:39,  1.58s/it]avg_loss = 1.900516645817817:  55%|█████▌    | 78/141 [02:03<01:39,  1.58s/it] avg_loss = 1.900516645817817:  56%|█████▌    | 79/141 [02:03<01:38,  1.58s/it]avg_loss = 1.8971516355872153:  56%|█████▌    | 79/141 [02:04<01:38,  1.58s/it]avg_loss = 1.8971516355872153:  57%|█████▋    | 80/141 [02:04<01:36,  1.58s/it]avg_loss = 1.8959589666790433:  57%|█████▋    | 80/141 [02:06<01:36,  1.58s/it]avg_loss = 1.8959589666790433:  57%|█████▋    | 81/141 [02:06<01:35,  1.58s/it]avg_loss = 1.8950824199653253:  57%|█████▋    | 81/141 [02:07<01:35,  1.58s/it]avg_loss = 1.8950824199653253:  58%|█████▊    | 82/141 [02:07<01:33,  1.58s/it]avg_loss = 1.893255433404302:  58%|█████▊    | 82/141 [02:09<01:33,  1.58s/it] avg_loss = 1.893255433404302:  59%|█████▉    | 83/141 [02:09<01:31,  1.58s/it]avg_loss = 1.8909891701879955:  59%|█████▉    | 83/141 [02:11<01:31,  1.58s/it]avg_loss = 1.8909891701879955:  60%|█████▉    | 84/141 [02:11<01:30,  1.58s/it]avg_loss = 1.8887080066344317:  60%|█████▉    | 84/141 [02:12<01:30,  1.58s/it]avg_loss = 1.8887080066344317:  60%|██████    | 85/141 [02:12<01:28,  1.58s/it]avg_loss = 1.8901835804761842:  60%|██████    | 85/141 [02:14<01:28,  1.58s/it]avg_loss = 1.8901835804761842:  61%|██████    | 86/141 [02:14<01:27,  1.59s/it]avg_loss = 1.891874616173492:  61%|██████    | 86/141 [02:15<01:27,  1.59s/it] avg_loss = 1.891874616173492:  62%|██████▏   | 87/141 [02:15<01:25,  1.59s/it]avg_loss = 1.8926574899391695:  62%|██████▏   | 87/141 [02:17<01:25,  1.59s/it]avg_loss = 1.8926574899391695:  62%|██████▏   | 88/141 [02:17<01:24,  1.59s/it]avg_loss = 1.9014331155948425:  62%|██████▏   | 88/141 [02:18<01:24,  1.59s/it]avg_loss = 1.9014331155948425:  63%|██████▎   | 89/141 [02:18<01:22,  1.59s/it]avg_loss = 1.9088918990559047:  63%|██████▎   | 89/141 [02:20<01:22,  1.59s/it]avg_loss = 1.9088918990559047:  64%|██████▍   | 90/141 [02:20<01:20,  1.59s/it]avg_loss = 1.9120761287081374:  64%|██████▍   | 90/141 [02:22<01:20,  1.59s/it]avg_loss = 1.9120761287081374:  65%|██████▍   | 91/141 [02:22<01:19,  1.59s/it]avg_loss = 1.9168623128662938:  65%|██████▍   | 91/141 [02:23<01:19,  1.59s/it]avg_loss = 1.9168623128662938:  65%|██████▌   | 92/141 [02:23<01:17,  1.59s/it]avg_loss = 1.9218249128710838:  65%|██████▌   | 92/141 [02:25<01:17,  1.59s/it]avg_loss = 1.9218249128710838:  66%|██████▌   | 93/141 [02:25<01:16,  1.59s/it]avg_loss = 1.9226556826145091:  66%|██████▌   | 93/141 [02:26<01:16,  1.59s/it]avg_loss = 1.9226556826145091:  67%|██████▋   | 94/141 [02:26<01:14,  1.59s/it]avg_loss = 1.9263772851542422:  67%|██████▋   | 94/141 [02:28<01:14,  1.59s/it]avg_loss = 1.9263772851542422:  67%|██████▋   | 95/141 [02:28<01:12,  1.59s/it]avg_loss = 1.9271754436194897:  67%|██████▋   | 95/141 [02:30<01:12,  1.59s/it]avg_loss = 1.9271754436194897:  68%|██████▊   | 96/141 [02:30<01:11,  1.59s/it]avg_loss = 1.9290670869276696:  68%|██████▊   | 96/141 [02:31<01:11,  1.59s/it]avg_loss = 1.9290670869276696:  69%|██████▉   | 97/141 [02:31<01:09,  1.59s/it]avg_loss = 1.9257474590320975:  69%|██████▉   | 97/141 [02:33<01:09,  1.59s/it]avg_loss = 1.9257474590320975:  70%|██████▉   | 98/141 [02:33<01:08,  1.59s/it]avg_loss = 1.9267638175174444:  70%|██████▉   | 98/141 [02:34<01:08,  1.59s/it]avg_loss = 1.9267638175174444:  70%|███████   | 99/141 [02:34<01:06,  1.59s/it]avg_loss = 1.9288459861278533:  70%|███████   | 99/141 [02:36<01:06,  1.59s/it]avg_loss = 1.9288459861278533:  71%|███████   | 100/141 [02:36<01:05,  1.59s/it]avg_loss = 1.9280442341719524:  71%|███████   | 100/141 [02:38<01:05,  1.59s/it]avg_loss = 1.9280442341719524:  72%|███████▏  | 101/141 [02:38<01:03,  1.59s/it]avg_loss = 1.928612790855707:  72%|███████▏  | 101/141 [02:39<01:03,  1.59s/it] avg_loss = 1.928612790855707:  72%|███████▏  | 102/141 [02:39<01:01,  1.59s/it]avg_loss = 1.927429870494361:  72%|███████▏  | 102/141 [02:41<01:01,  1.59s/it]avg_loss = 1.927429870494361:  73%|███████▎  | 103/141 [02:41<01:00,  1.59s/it]avg_loss = 1.9303309413102956:  73%|███████▎  | 103/141 [02:42<01:00,  1.59s/it]avg_loss = 1.9303309413102956:  74%|███████▍  | 104/141 [02:42<00:58,  1.59s/it]avg_loss = 1.9296737580072312:  74%|███████▍  | 104/141 [02:44<00:58,  1.59s/it]avg_loss = 1.9296737580072312:  74%|███████▍  | 105/141 [02:44<00:57,  1.59s/it]avg_loss = 1.929131396536557:  74%|███████▍  | 105/141 [02:45<00:57,  1.59s/it] avg_loss = 1.929131396536557:  75%|███████▌  | 106/141 [02:45<00:55,  1.59s/it]avg_loss = 1.9271680016383947:  75%|███████▌  | 106/141 [02:47<00:55,  1.59s/it]avg_loss = 1.9271680016383947:  76%|███████▌  | 107/141 [02:47<00:53,  1.59s/it]avg_loss = 1.9250717538374442:  76%|███████▌  | 107/141 [02:49<00:53,  1.59s/it]avg_loss = 1.9250717538374442:  77%|███████▋  | 108/141 [02:49<00:52,  1.59s/it]avg_loss = 1.922930565449076:  77%|███████▋  | 108/141 [02:50<00:52,  1.59s/it] avg_loss = 1.922930565449076:  77%|███████▋  | 109/141 [02:50<00:50,  1.59s/it]avg_loss = 1.9205497048117899:  77%|███████▋  | 109/141 [02:52<00:50,  1.59s/it]avg_loss = 1.9205497048117899:  78%|███████▊  | 110/141 [02:52<00:49,  1.59s/it]avg_loss = 1.9228928432808265:  78%|███████▊  | 110/141 [02:53<00:49,  1.59s/it]avg_loss = 1.9228928432808265:  79%|███████▊  | 111/141 [02:53<00:47,  1.59s/it]avg_loss = 1.9224797327603613:  79%|███████▊  | 111/141 [02:55<00:47,  1.59s/it]avg_loss = 1.9224797327603613:  79%|███████▉  | 112/141 [02:55<00:46,  1.59s/it]avg_loss = 1.9235641903581873:  79%|███████▉  | 112/141 [02:57<00:46,  1.59s/it]avg_loss = 1.9235641903581873:  80%|████████  | 113/141 [02:57<00:44,  1.59s/it]avg_loss = 1.924577720332564:  80%|████████  | 113/141 [02:58<00:44,  1.59s/it] avg_loss = 1.924577720332564:  81%|████████  | 114/141 [02:58<00:42,  1.59s/it]avg_loss = 1.9238745336947234:  81%|████████  | 114/141 [03:00<00:42,  1.59s/it]avg_loss = 1.9238745336947234:  82%|████████▏ | 115/141 [03:00<00:41,  1.59s/it]avg_loss = 1.9227089645533726:  82%|████████▏ | 115/141 [03:01<00:41,  1.59s/it]avg_loss = 1.9227089645533726:  82%|████████▏ | 116/141 [03:01<00:39,  1.59s/it]avg_loss = 1.9249379319003506:  82%|████████▏ | 116/141 [03:03<00:39,  1.59s/it]avg_loss = 1.9249379319003506:  83%|████████▎ | 117/141 [03:03<00:38,  1.59s/it]avg_loss = 1.9245129308458102:  83%|████████▎ | 117/141 [03:05<00:38,  1.59s/it]avg_loss = 1.9245129308458102:  84%|████████▎ | 118/141 [03:05<00:36,  1.59s/it]avg_loss = 1.9230058623963044:  84%|████████▎ | 118/141 [03:06<00:36,  1.59s/it]avg_loss = 1.9230058623963044:  84%|████████▍ | 119/141 [03:06<00:34,  1.59s/it]avg_loss = 1.9212579379479091:  84%|████████▍ | 119/141 [03:08<00:34,  1.59s/it]avg_loss = 1.9212579379479091:  85%|████████▌ | 120/141 [03:08<00:33,  1.59s/it]avg_loss = 1.921133355660872:  85%|████████▌ | 120/141 [03:09<00:33,  1.59s/it] avg_loss = 1.921133355660872:  86%|████████▌ | 121/141 [03:09<00:31,  1.59s/it]avg_loss = 1.9215624293342966:  86%|████████▌ | 121/141 [03:11<00:31,  1.59s/it]avg_loss = 1.9215624293342966:  87%|████████▋ | 122/141 [03:11<00:30,  1.59s/it]avg_loss = 1.9212354605760031:  87%|████████▋ | 122/141 [03:12<00:30,  1.59s/it]avg_loss = 1.9212354605760031:  87%|████████▋ | 123/141 [03:12<00:28,  1.59s/it]avg_loss = 1.9212916156937998:  87%|████████▋ | 123/141 [03:14<00:28,  1.59s/it]avg_loss = 1.9212916156937998:  88%|████████▊ | 124/141 [03:14<00:26,  1.59s/it]avg_loss = 1.9199298248291015:  88%|████████▊ | 124/141 [03:16<00:26,  1.59s/it]avg_loss = 1.9199298248291015:  89%|████████▊ | 125/141 [03:16<00:25,  1.59s/it]avg_loss = 1.9200834745452517:  89%|████████▊ | 125/141 [03:17<00:25,  1.59s/it]avg_loss = 1.9200834745452517:  89%|████████▉ | 126/141 [03:17<00:23,  1.59s/it]avg_loss = 1.9197238984070426:  89%|████████▉ | 126/141 [03:19<00:23,  1.59s/it]avg_loss = 1.9197238984070426:  90%|█████████ | 127/141 [03:19<00:22,  1.59s/it]avg_loss = 1.9185625612735748:  90%|█████████ | 127/141 [03:20<00:22,  1.59s/it]avg_loss = 1.9185625612735748:  91%|█████████ | 128/141 [03:20<00:20,  1.59s/it]avg_loss = 1.918561386507611:  91%|█████████ | 128/141 [03:22<00:20,  1.59s/it] avg_loss = 1.918561386507611:  91%|█████████▏| 129/141 [03:22<00:19,  1.59s/it]avg_loss = 1.9194236480272733:  91%|█████████▏| 129/141 [03:24<00:19,  1.59s/it]avg_loss = 1.9194236480272733:  92%|█████████▏| 130/141 [03:24<00:17,  1.59s/it]avg_loss = 1.9202971422035275:  92%|█████████▏| 130/141 [03:25<00:17,  1.59s/it]avg_loss = 1.9202971422035275:  93%|█████████▎| 131/141 [03:25<00:15,  1.59s/it]avg_loss = 1.9208979904651642:  93%|█████████▎| 131/141 [03:27<00:15,  1.59s/it]avg_loss = 1.9208979904651642:  94%|█████████▎| 132/141 [03:27<00:14,  1.59s/it]avg_loss = 1.917941395501445:  94%|█████████▎| 132/141 [03:28<00:14,  1.59s/it] avg_loss = 1.917941395501445:  94%|█████████▍| 133/141 [03:28<00:12,  1.59s/it]avg_loss = 1.9133163014454628:  94%|█████████▍| 133/141 [03:30<00:12,  1.59s/it]avg_loss = 1.9133163014454628:  95%|█████████▌| 134/141 [03:30<00:11,  1.59s/it]avg_loss = 1.9156680283723053:  95%|█████████▌| 134/141 [03:31<00:11,  1.59s/it]avg_loss = 1.9156680283723053:  96%|█████████▌| 135/141 [03:31<00:09,  1.59s/it]avg_loss = 1.9191115340765785:  96%|█████████▌| 135/141 [03:33<00:09,  1.59s/it]avg_loss = 1.9191115340765785:  96%|█████████▋| 136/141 [03:33<00:07,  1.59s/it]avg_loss = 1.9203903379231473:  96%|█████████▋| 136/141 [03:35<00:07,  1.59s/it]avg_loss = 1.9203903379231473:  97%|█████████▋| 137/141 [03:35<00:06,  1.59s/it]avg_loss = 1.9192947607109512:  97%|█████████▋| 137/141 [03:36<00:06,  1.59s/it]avg_loss = 1.9192947607109512:  98%|█████████▊| 138/141 [03:36<00:04,  1.59s/it]avg_loss = 1.9196881793385787:  98%|█████████▊| 138/141 [03:38<00:04,  1.59s/it]avg_loss = 1.9196881793385787:  99%|█████████▊| 139/141 [03:38<00:03,  1.59s/it]avg_loss = 1.920503923722676:  99%|█████████▊| 139/141 [03:39<00:03,  1.59s/it] avg_loss = 1.920503923722676:  99%|█████████▉| 140/141 [03:39<00:01,  1.59s/it]avg_loss = 1.9218144847991618:  99%|█████████▉| 140/141 [03:41<00:01,  1.59s/it]avg_loss = 1.9218144847991618: 100%|██████████| 141/141 [03:41<00:00,  1.59s/it]avg_loss = 1.9218144847991618: 100%|██████████| 141/141 [03:41<00:00,  1.57s/it]
I0328 09:05:48.772320 2300368 eval_ppl.py:107] wikitext2 perplexity: 6.833345890045166
wikitext2 perplexity: 6.833
