I0328 12:31:16.362232 2311068 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:31:16.362327 2311068 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:31:16.362369 2311068 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:31:16.707024 2311068 config.py:54] PyTorch version 2.6.0 available.
W0328 12:31:16.910603 2311068 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:31:17.509162 2311068 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.01it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.43it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.61it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.70it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.46it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.65it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.79it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.64it/s]
I0328 12:31:19.028742 2311068 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:18,  1.69it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:16,  1.81it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:15,  1.86it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:14,  1.88it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.88it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.88it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.88it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.88it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:12,  1.88it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.88it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:11,  1.88it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.89it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:10,  1.88it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.89it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:08,  1.90it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.92it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.93it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.95it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.93it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.94it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.94it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.92it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.92it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.92it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.92it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.92it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.93it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.94it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.96it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.96it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.96it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]
I0328 12:31:41.817372 2311068 quantize_finetune_llama.py:185] loaded compression model
I0328 12:32:00.816066 2311068 quantize_finetune_llama.py:189] loaded dataset and devset
I0328 12:32:05.782106 2311068 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:33:06.114852 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 60.17964482307434s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0328 12:33:28.856836 2311199 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:33:28.856934 2311199 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:33:28.856975 2311199 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:33:29.190704 2311199 config.py:54] PyTorch version 2.6.0 available.
W0328 12:33:29.377446 2311199 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:33:30.024429 2311199 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:33:30.028403 2311068 quantize_finetune_llama.py:209] layer 1 gpu 1
I0328 12:33:30.041539 2311199 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:33:46.647532 2311199 finetune.py:45] layer 0_v initial loss 3.8275288716249634e-07
W0328 12:33:46.647731 2311199 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 12:34:20.456462 2311199 finetune.py:68] layer 0_v @ epoch 0 new loss 3.4343710808570904e-07 old loss 3.8275288716249634e-07 BETTER
I0328 12:34:26.009146 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 55.79718518257141s
I0328 12:34:34.856253 2311271 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:34:34.856350 2311271 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:34:34.856400 2311271 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:34:35.210674 2311271 config.py:54] PyTorch version 2.6.0 available.
W0328 12:34:35.405427 2311271 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:34:35.996480 2311271 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:34:36.000146 2311068 quantize_finetune_llama.py:209] layer 2 gpu 2
I0328 12:34:36.013597 2311271 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:34:52.931451 2311271 finetune.py:45] layer 1_v initial loss 3.310817874080385e-06
W0328 12:34:52.931676 2311271 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 12:34:55.737278 2311199 finetune.py:68] layer 0_v @ epoch 1 new loss 3.296880208836228e-07 old loss 3.4343710808570904e-07 BETTER
I0328 12:35:25.644479 2311271 finetune.py:68] layer 1_v @ epoch 0 new loss 1.128186340793036e-06 old loss 3.310817874080385e-06 BETTER
I0328 12:35:31.894262 2311199 finetune.py:68] layer 0_v @ epoch 2 new loss 3.226816716050962e-07 old loss 3.296880208836228e-07 BETTER
I0328 12:35:42.175169 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 65.96576690673828s
I0328 12:35:53.622413 2311343 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:35:53.622509 2311343 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:35:53.622546 2311343 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:35:53.995526 2311343 config.py:54] PyTorch version 2.6.0 available.
W0328 12:35:54.204812 2311343 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:35:54.835556 2311343 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:35:54.840633 2311068 quantize_finetune_llama.py:209] layer 3 gpu 3
I0328 12:35:54.855309 2311343 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 12:35:59.892890 2311271 finetune.py:68] layer 1_v @ epoch 1 new loss 6.588563792320201e-07 old loss 1.128186340793036e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:36:08.196032 2311199 finetune.py:68] layer 0_v @ epoch 3 new loss 3.1820860613152036e-07 old loss 3.226816716050962e-07 BETTER
I0328 12:36:12.374748 2311343 finetune.py:45] layer 2_v initial loss 7.345451194851194e-06
W0328 12:36:12.375053 2311343 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 12:36:34.344248 2311271 finetune.py:68] layer 1_v @ epoch 2 new loss 5.276818342281331e-07 old loss 6.588563792320201e-07 BETTER
I0328 12:36:45.023337 2311199 finetune.py:68] layer 0_v @ epoch 4 new loss 3.1496063002123265e-07 old loss 3.1820860613152036e-07 BETTER
I0328 12:36:45.668712 2311343 finetune.py:68] layer 2_v @ epoch 0 new loss 1.642242523303139e-06 old loss 7.345451194851194e-06 BETTER
I0328 12:37:01.409979 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 66.33933520317078s
I0328 12:37:08.513716 2311199 finetune.py:45] layer 0_q initial loss 3.1560284696752205e-07
I0328 12:37:10.775875 2311271 finetune.py:68] layer 1_v @ epoch 3 new loss 4.77111598229385e-07 old loss 5.276818342281331e-07 BETTER
I0328 12:37:13.073397 2311415 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:37:13.073512 2311415 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:37:13.073559 2311415 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:37:13.425179 2311415 config.py:54] PyTorch version 2.6.0 available.
W0328 12:37:13.635561 2311415 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:37:14.453705 2311415 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:37:14.457621 2311068 quantize_finetune_llama.py:209] layer 4 gpu 0
I0328 12:37:14.470954 2311415 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 12:37:20.644646 2311343 finetune.py:68] layer 2_v @ epoch 1 new loss 9.097628321796947e-07 old loss 1.642242523303139e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:37:31.337087 2311415 finetune.py:45] layer 3_v initial loss 7.544490927102743e-06
W0328 12:37:31.337403 2311415 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 12:37:43.292916 2311199 finetune.py:68] layer 0_q @ epoch 0 new loss 3.1296698921323696e-07 old loss 3.1560284696752205e-07 BETTER
I0328 12:37:45.401144 2311271 finetune.py:68] layer 1_v @ epoch 4 new loss 4.498587031775969e-07 old loss 4.77111598229385e-07 BETTER
I0328 12:37:55.562042 2311343 finetune.py:68] layer 2_v @ epoch 2 new loss 7.618423865096702e-07 old loss 9.097628321796947e-07 BETTER
I0328 12:38:04.126222 2311415 finetune.py:68] layer 3_v @ epoch 0 new loss 2.0705003862531157e-06 old loss 7.544490927102743e-06 BETTER
I0328 12:38:04.913983 2311271 finetune.py:45] layer 1_q initial loss 4.573583964884165e-07
I0328 12:38:19.629054 2311199 finetune.py:68] layer 0_q @ epoch 1 new loss 3.107533075308311e-07 old loss 3.1296698921323696e-07 BETTER
I0328 12:38:30.490058 2311343 finetune.py:68] layer 2_v @ epoch 3 new loss 7.083651212269615e-07 old loss 7.618423865096702e-07 BETTER
I0328 12:38:38.167683 2311271 finetune.py:68] layer 1_q @ epoch 0 new loss 4.3638331703732547e-07 old loss 4.573583964884165e-07 BETTER
I0328 12:38:38.278610 2311415 finetune.py:68] layer 3_v @ epoch 1 new loss 1.3703884178539738e-06 old loss 2.0705003862531157e-06 BETTER
I0328 12:38:55.800896 2311199 finetune.py:68] layer 0_q @ epoch 2 new loss 3.0885888691045693e-07 old loss 3.107533075308311e-07 BETTER
I0328 12:39:05.670312 2311343 finetune.py:68] layer 2_v @ epoch 4 new loss 6.781867227800831e-07 old loss 7.083651212269615e-07 BETTER
I0328 12:39:12.396617 2311271 finetune.py:68] layer 1_q @ epoch 1 new loss 4.2179789261354017e-07 old loss 4.3638331703732547e-07 BETTER
I0328 12:39:12.690501 2311415 finetune.py:68] layer 3_v @ epoch 2 new loss 1.1741983598767547e-06 old loss 1.3703884178539738e-06 BETTER
I0328 12:39:25.231705 2311343 finetune.py:45] layer 2_q initial loss 7.61851026709337e-07
I0328 12:39:32.036815 2311199 finetune.py:68] layer 0_q @ epoch 3 new loss 3.071749858918338e-07 old loss 3.0885888691045693e-07 BETTER
I0328 12:39:46.705829 2311271 finetune.py:68] layer 1_q @ epoch 2 new loss 4.1016318164111e-07 old loss 4.2179789261354017e-07 BETTER
I0328 12:39:47.221153 2311415 finetune.py:68] layer 3_v @ epoch 3 new loss 1.0864381465580664e-06 old loss 1.1741983598767547e-06 BETTER
I0328 12:39:58.882507 2311343 finetune.py:68] layer 2_q @ epoch 0 new loss 7.317630661418661e-07 old loss 7.61851026709337e-07 BETTER
I0328 12:40:08.528155 2311199 finetune.py:68] layer 0_q @ epoch 4 new loss 3.055986610434047e-07 old loss 3.071749858918338e-07 BETTER
I0328 12:40:21.113896 2311271 finetune.py:68] layer 1_q @ epoch 3 new loss 4.0294537484442117e-07 old loss 4.1016318164111e-07 BETTER
I0328 12:40:21.892599 2311415 finetune.py:68] layer 3_v @ epoch 4 new loss 1.0361104614275973e-06 old loss 1.0864381465580664e-06 BETTER
I0328 12:40:26.548455 2311199 finetune.py:45] layer 0_k initial loss 3.0783812121626397e-07
I0328 12:40:33.321982 2311343 finetune.py:68] layer 2_q @ epoch 1 new loss 7.133727990549232e-07 old loss 7.317630661418661e-07 BETTER
I0328 12:40:41.584529 2311415 finetune.py:45] layer 3_q initial loss 1.1954473393416265e-06
I0328 12:40:55.710148 2311271 finetune.py:68] layer 1_q @ epoch 4 new loss 3.9275752783396456e-07 old loss 4.0294537484442117e-07 BETTER
I0328 12:41:01.530102 2311199 finetune.py:68] layer 0_k @ epoch 0 new loss 3.0603123946093547e-07 old loss 3.0783812121626397e-07 BETTER
I0328 12:41:08.015386 2311343 finetune.py:68] layer 2_q @ epoch 2 new loss 7.001385711191688e-07 old loss 7.133727990549232e-07 BETTER
I0328 12:41:13.758958 2311271 finetune.py:45] layer 1_k initial loss 4.13070040394814e-07
I0328 12:41:14.644117 2311415 finetune.py:68] layer 3_q @ epoch 0 new loss 1.1528507002367405e-06 old loss 1.1954473393416265e-06 BETTER
I0328 12:41:37.955775 2311199 finetune.py:68] layer 0_k @ epoch 1 new loss 3.0465895406450727e-07 old loss 3.0603123946093547e-07 BETTER
I0328 12:41:42.920749 2311343 finetune.py:68] layer 2_q @ epoch 3 new loss 6.900183393554471e-07 old loss 7.001385711191688e-07 BETTER
I0328 12:41:47.196351 2311271 finetune.py:68] layer 1_k @ epoch 0 new loss 4.0196567852035514e-07 old loss 4.13070040394814e-07 BETTER
I0328 12:41:48.746259 2311415 finetune.py:68] layer 3_q @ epoch 1 new loss 1.1259772918492672e-06 old loss 1.1528507002367405e-06 BETTER
I0328 12:42:14.413836 2311199 finetune.py:68] layer 0_k @ epoch 2 new loss 3.034356268472038e-07 old loss 3.0465895406450727e-07 BETTER
I0328 12:42:17.872692 2311343 finetune.py:68] layer 2_q @ epoch 4 new loss 6.817038524786767e-07 old loss 6.900183393554471e-07 BETTER
I0328 12:42:21.559685 2311271 finetune.py:68] layer 1_k @ epoch 1 new loss 3.9314193145401077e-07 old loss 4.0196567852035514e-07 BETTER
I0328 12:42:22.972063 2311415 finetune.py:68] layer 3_q @ epoch 2 new loss 1.1069964784837794e-06 old loss 1.1259772918492672e-06 BETTER
I0328 12:42:36.047399 2311343 finetune.py:45] layer 2_k initial loss 7.166923978729756e-07
I0328 12:42:50.887100 2311199 finetune.py:68] layer 0_k @ epoch 3 new loss 3.0228642344809487e-07 old loss 3.034356268472038e-07 BETTER
I0328 12:42:55.859344 2311271 finetune.py:68] layer 1_k @ epoch 2 new loss 3.874323795116652e-07 old loss 3.9314193145401077e-07 BETTER
I0328 12:42:57.104065 2311415 finetune.py:68] layer 3_q @ epoch 3 new loss 1.0915839538938599e-06 old loss 1.1069964784837794e-06 BETTER
I0328 12:43:09.596707 2311343 finetune.py:68] layer 2_k @ epoch 0 new loss 7.07336596406094e-07 old loss 7.166923978729756e-07 BETTER
I0328 12:43:27.534928 2311199 finetune.py:68] layer 0_k @ epoch 4 new loss 3.0120438054836995e-07 old loss 3.0228642344809487e-07 BETTER
I0328 12:43:30.427798 2311271 finetune.py:68] layer 1_k @ epoch 3 new loss 3.824212626568624e-07 old loss 3.874323795116652e-07 BETTER
I0328 12:43:31.396963 2311415 finetune.py:68] layer 3_q @ epoch 4 new loss 1.079484377441986e-06 old loss 1.0915839538938599e-06 BETTER
I0328 12:43:44.097739 2311343 finetune.py:68] layer 2_k @ epoch 1 new loss 7.013466643002175e-07 old loss 7.07336596406094e-07 BETTER
I0328 12:43:47.668094 2311199 finetune.py:45] layer 0_o initial loss 4.806168476534367e-07
I0328 12:43:49.481606 2311415 finetune.py:45] layer 3_k initial loss 1.1441518381616333e-06
I0328 12:44:04.851130 2311271 finetune.py:68] layer 1_k @ epoch 4 new loss 3.779066730658087e-07 old loss 3.824212626568624e-07 BETTER
I0328 12:44:18.760380 2311343 finetune.py:68] layer 2_k @ epoch 2 new loss 6.961616350054101e-07 old loss 7.013466643002175e-07 BETTER
I0328 12:44:22.148970 2311199 finetune.py:68] layer 0_o @ epoch 0 new loss 4.792217396243359e-07 old loss 4.806168476534367e-07 BETTER
I0328 12:44:22.436637 2311415 finetune.py:68] layer 3_k @ epoch 0 new loss 1.1329998415021691e-06 old loss 1.1441518381616333e-06 BETTER
I0328 12:44:24.657327 2311271 finetune.py:45] layer 1_o initial loss 9.27831536046142e-07
I0328 12:44:53.714410 2311343 finetune.py:68] layer 2_k @ epoch 3 new loss 6.916438906046096e-07 old loss 6.961616350054101e-07 BETTER
I0328 12:44:56.694191 2311415 finetune.py:68] layer 3_k @ epoch 1 new loss 1.1239417290198617e-06 old loss 1.1329998415021691e-06 BETTER
I0328 12:44:57.406410 2311271 finetune.py:68] layer 1_o @ epoch 0 new loss 9.063812171916652e-07 old loss 9.27831536046142e-07 BETTER
I0328 12:44:57.828535 2311199 finetune.py:68] layer 0_o @ epoch 1 new loss 4.781098255079996e-07 old loss 4.792217396243359e-07 BETTER
I0328 12:45:28.656815 2311343 finetune.py:68] layer 2_k @ epoch 4 new loss 6.875645794934826e-07 old loss 6.916438906046096e-07 BETTER
I0328 12:45:30.954528 2311415 finetune.py:68] layer 3_k @ epoch 2 new loss 1.1163090221089078e-06 old loss 1.1239417290198617e-06 BETTER
I0328 12:45:31.084718 2311271 finetune.py:68] layer 1_o @ epoch 1 new loss 8.906073389880476e-07 old loss 9.063812171916652e-07 BETTER
I0328 12:45:33.820656 2311199 finetune.py:68] layer 0_o @ epoch 2 new loss 4.771321755470126e-07 old loss 4.781098255079996e-07 BETTER
I0328 12:45:48.606153 2311343 finetune.py:45] layer 2_o initial loss 1.543381927149312e-06
I0328 12:46:05.145953 2311415 finetune.py:68] layer 3_k @ epoch 3 new loss 1.1092805607404443e-06 old loss 1.1163090221089078e-06 BETTER
I0328 12:46:05.228631 2311271 finetune.py:68] layer 1_o @ epoch 2 new loss 8.788811101112515e-07 old loss 8.906073389880476e-07 BETTER
I0328 12:46:09.855199 2311199 finetune.py:68] layer 0_o @ epoch 3 new loss 4.762466403462895e-07 old loss 4.771321755470126e-07 BETTER
I0328 12:46:21.471228 2311343 finetune.py:68] layer 2_o @ epoch 0 new loss 1.4951457387724076e-06 old loss 1.543381927149312e-06 BETTER
I0328 12:46:39.206221 2311415 finetune.py:68] layer 3_k @ epoch 4 new loss 1.1029160305042751e-06 old loss 1.1092805607404443e-06 BETTER
I0328 12:46:39.477830 2311271 finetune.py:68] layer 1_o @ epoch 3 new loss 8.69609834808216e-07 old loss 8.788811101112515e-07 BETTER
I0328 12:46:46.150853 2311199 finetune.py:68] layer 0_o @ epoch 4 new loss 4.754339215651271e-07 old loss 4.762466403462895e-07 BETTER
I0328 12:46:55.263589 2311343 finetune.py:68] layer 2_o @ epoch 1 new loss 1.4696435073346947e-06 old loss 1.4951457387724076e-06 BETTER
I0328 12:46:58.704137 2311415 finetune.py:45] layer 3_o initial loss 2.4851219677657355e-06
I0328 12:47:13.573618 2311271 finetune.py:68] layer 1_o @ epoch 4 new loss 8.622490668130922e-07 old loss 8.69609834808216e-07 BETTER
I0328 12:47:17.443745 2311199 finetune.py:45] layer 0_up initial loss 6.410458581740386e-07
I0328 12:47:29.261930 2311343 finetune.py:68] layer 2_o @ epoch 2 new loss 1.4545208841809654e-06 old loss 1.4696435073346947e-06 BETTER
I0328 12:47:31.037812 2311415 finetune.py:68] layer 3_o @ epoch 0 new loss 2.429271489745588e-06 old loss 2.4851219677657355e-06 BETTER
I0328 12:47:45.155057 2311271 finetune.py:45] layer 1_up initial loss 1.2497033594627283e-06
I0328 12:47:49.167011 2311199 finetune.py:68] layer 0_up @ epoch 0 new loss 6.398855703082518e-07 old loss 6.410458581740386e-07 BETTER
I0328 12:48:03.386055 2311343 finetune.py:68] layer 2_o @ epoch 3 new loss 1.4442701967709581e-06 old loss 1.4545208841809654e-06 BETTER
I0328 12:48:04.350223 2311415 finetune.py:68] layer 3_o @ epoch 1 new loss 2.4085011318675242e-06 old loss 2.429271489745588e-06 BETTER
I0328 12:48:15.524756 2311271 finetune.py:68] layer 1_up @ epoch 0 new loss 1.242152507074934e-06 old loss 1.2497033594627283e-06 BETTER
I0328 12:48:22.551498 2311199 finetune.py:68] layer 0_up @ epoch 1 new loss 6.390307589754229e-07 old loss 6.398855703082518e-07 BETTER
I0328 12:48:37.695028 2311343 finetune.py:68] layer 2_o @ epoch 4 new loss 1.4364793514687335e-06 old loss 1.4442701967709581e-06 BETTER
I0328 12:48:37.817255 2311415 finetune.py:68] layer 3_o @ epoch 2 new loss 2.39457631323603e-06 old loss 2.4085011318675242e-06 BETTER
I0328 12:48:47.119139 2311271 finetune.py:68] layer 1_up @ epoch 1 new loss 1.2378300198179204e-06 old loss 1.242152507074934e-06 BETTER
I0328 12:48:55.922769 2311199 finetune.py:68] layer 0_up @ epoch 2 new loss 6.382963420037413e-07 old loss 6.390307589754229e-07 BETTER
I0328 12:49:09.202827 2311343 finetune.py:45] layer 2_up initial loss 2.3756115297146607e-06
I0328 12:49:11.414700 2311415 finetune.py:68] layer 3_o @ epoch 3 new loss 2.3830355075915577e-06 old loss 2.39457631323603e-06 BETTER
I0328 12:49:18.900658 2311271 finetune.py:68] layer 1_up @ epoch 2 new loss 1.234294813912129e-06 old loss 1.2378300198179204e-06 BETTER
I0328 12:49:29.513088 2311199 finetune.py:68] layer 0_up @ epoch 3 new loss 6.376264991558855e-07 old loss 6.382963420037413e-07 BETTER
I0328 12:49:40.064488 2311343 finetune.py:68] layer 2_up @ epoch 0 new loss 2.3680108824919444e-06 old loss 2.3756115297146607e-06 BETTER
I0328 12:49:45.040382 2311415 finetune.py:68] layer 3_o @ epoch 4 new loss 2.372862127231201e-06 old loss 2.3830355075915577e-06 BETTER
I0328 12:49:50.902039 2311271 finetune.py:68] layer 1_up @ epoch 3 new loss 1.2311467116887798e-06 old loss 1.234294813912129e-06 BETTER
I0328 12:50:03.143101 2311199 finetune.py:68] layer 0_up @ epoch 4 new loss 6.370058827087632e-07 old loss 6.376264991558855e-07 BETTER
I0328 12:50:12.300846 2311343 finetune.py:68] layer 2_up @ epoch 1 new loss 2.3621273612661753e-06 old loss 2.3680108824919444e-06 BETTER
I0328 12:50:16.681012 2311415 finetune.py:45] layer 3_up initial loss 4.50792140327394e-06
I0328 12:50:23.051422 2311271 finetune.py:68] layer 1_up @ epoch 4 new loss 1.2284110653126845e-06 old loss 1.2311467116887798e-06 BETTER
I0328 12:50:34.463223 2311199 finetune.py:45] layer 0_gate initial loss 7.423147394547414e-07
I0328 12:50:44.557935 2311343 finetune.py:68] layer 2_up @ epoch 2 new loss 2.3570225948787993e-06 old loss 2.3621273612661753e-06 BETTER
I0328 12:50:46.960313 2311415 finetune.py:68] layer 3_up @ epoch 0 new loss 4.492826974455966e-06 old loss 4.50792140327394e-06 BETTER
I0328 12:50:54.474260 2311271 finetune.py:45] layer 1_gate initial loss 1.5022983461676631e-06
I0328 12:51:04.414947 2311199 finetune.py:68] layer 0_gate @ epoch 0 new loss 7.411658202727267e-07 old loss 7.423147394547414e-07 BETTER
I0328 12:51:16.833915 2311343 finetune.py:68] layer 2_up @ epoch 3 new loss 2.3524187326984247e-06 old loss 2.3570225948787993e-06 BETTER
I0328 12:51:18.209435 2311415 finetune.py:68] layer 3_up @ epoch 1 new loss 4.480771167436615e-06 old loss 4.492826974455966e-06 BETTER
I0328 12:51:22.951299 2311271 finetune.py:68] layer 1_gate @ epoch 0 new loss 1.4874296994094038e-06 old loss 1.5022983461676631e-06 BETTER
I0328 12:51:35.697624 2311199 finetune.py:68] layer 0_gate @ epoch 1 new loss 7.403623385471292e-07 old loss 7.411658202727267e-07 BETTER
I0328 12:51:49.231004 2311343 finetune.py:68] layer 2_up @ epoch 4 new loss 2.3481479729525745e-06 old loss 2.3524187326984247e-06 BETTER
I0328 12:51:49.659832 2311415 finetune.py:68] layer 3_up @ epoch 2 new loss 4.469664418138564e-06 old loss 4.480771167436615e-06 BETTER
I0328 12:51:52.353969 2311271 finetune.py:76] layer 1_gate @ epoch 1 new loss 1.4880040453135734e-06 old loss 1.4874296994094038e-06 WORSE
I0328 12:52:06.990681 2311199 finetune.py:68] layer 0_gate @ epoch 2 new loss 7.397156878141686e-07 old loss 7.403623385471292e-07 BETTER
I0328 12:52:21.324430 2311415 finetune.py:68] layer 3_up @ epoch 3 new loss 4.459272986423457e-06 old loss 4.469664418138564e-06 BETTER
I0328 12:52:21.488668 2311271 finetune.py:68] layer 1_gate @ epoch 2 new loss 1.4840328503851197e-06 old loss 1.4874296994094038e-06 BETTER
I0328 12:52:21.761141 2311343 finetune.py:45] layer 2_gate initial loss 2.9359248401306104e-06
I0328 12:52:38.295800 2311199 finetune.py:68] layer 0_gate @ epoch 3 new loss 7.39147878903168e-07 old loss 7.397156878141686e-07 BETTER
I0328 12:52:50.435624 2311343 finetune.py:68] layer 2_gate @ epoch 0 new loss 2.9314289804460714e-06 old loss 2.9359248401306104e-06 BETTER
I0328 12:52:51.165652 2311271 finetune.py:68] layer 1_gate @ epoch 3 new loss 1.4826488268226967e-06 old loss 1.4840328503851197e-06 BETTER
I0328 12:52:52.718980 2311415 finetune.py:68] layer 3_up @ epoch 4 new loss 4.4495182009995915e-06 old loss 4.459272986423457e-06 BETTER
I0328 12:53:09.621294 2311199 finetune.py:68] layer 0_gate @ epoch 4 new loss 7.386381071228243e-07 old loss 7.39147878903168e-07 BETTER
I0328 12:53:20.156011 2311343 finetune.py:68] layer 2_gate @ epoch 1 new loss 2.9277305202413118e-06 old loss 2.9314289804460714e-06 BETTER
I0328 12:53:20.936045 2311271 finetune.py:68] layer 1_gate @ epoch 4 new loss 1.4813400639468455e-06 old loss 1.4826488268226967e-06 BETTER
I0328 12:53:24.329120 2311415 finetune.py:45] layer 3_gate initial loss 5.4504471336258575e-06
I0328 12:53:49.807047 2311343 finetune.py:68] layer 2_gate @ epoch 2 new loss 2.9244561119412538e-06 old loss 2.9277305202413118e-06 BETTER
I0328 12:53:52.419532 2311415 finetune.py:68] layer 3_gate @ epoch 0 new loss 5.439734650281025e-06 old loss 5.4504471336258575e-06 BETTER
I0328 12:54:06.352225 2311199 finetune.py:45] layer 0_down initial loss 1.1376478141755797e-06
I0328 12:54:17.507365 2311271 finetune.py:45] layer 1_down initial loss 2.2300596356217284e-06
I0328 12:54:19.751786 2311343 finetune.py:68] layer 2_gate @ epoch 3 new loss 2.921441819125903e-06 old loss 2.9244561119412538e-06 BETTER
I0328 12:54:21.523348 2311415 finetune.py:68] layer 3_gate @ epoch 1 new loss 5.431063073046971e-06 old loss 5.439734650281025e-06 BETTER
I0328 12:54:33.655976 2311199 finetune.py:68] layer 0_down @ epoch 0 new loss 1.1374791029084008e-06 old loss 1.1376478141755797e-06 BETTER
I0328 12:54:43.660669 2311271 finetune.py:68] layer 1_down @ epoch 0 new loss 2.2270580757322023e-06 old loss 2.2300596356217284e-06 BETTER
I0328 12:54:49.740926 2311343 finetune.py:68] layer 2_gate @ epoch 4 new loss 2.9185828225308796e-06 old loss 2.921441819125903e-06 BETTER
I0328 12:54:50.684629 2311415 finetune.py:68] layer 3_gate @ epoch 2 new loss 5.4229990382737014e-06 old loss 5.431063073046971e-06 BETTER
I0328 12:55:02.699745 2311199 finetune.py:68] layer 0_down @ epoch 1 new loss 1.1373431334504858e-06 old loss 1.1374791029084008e-06 BETTER
I0328 12:55:10.941458 2311271 finetune.py:68] layer 1_down @ epoch 1 new loss 2.226714741482283e-06 old loss 2.2270580757322023e-06 BETTER
I0328 12:55:20.279047 2311415 finetune.py:68] layer 3_gate @ epoch 3 new loss 5.415291980170878e-06 old loss 5.4229990382737014e-06 BETTER
I0328 12:55:31.136323 2311199 finetune.py:68] layer 0_down @ epoch 2 new loss 1.1372354720151634e-06 old loss 1.1373431334504858e-06 BETTER
I0328 12:55:38.313707 2311271 finetune.py:68] layer 1_down @ epoch 2 new loss 2.226415745099075e-06 old loss 2.226714741482283e-06 BETTER
I0328 12:55:47.096384 2311343 finetune.py:45] layer 2_down initial loss 4.476190952118486e-06
I0328 12:55:49.669074 2311415 finetune.py:68] layer 3_gate @ epoch 4 new loss 5.4079305300547276e-06 old loss 5.415291980170878e-06 BETTER
I0328 12:56:00.194258 2311199 finetune.py:68] layer 0_down @ epoch 3 new loss 1.1371417940608808e-06 old loss 1.1372354720151634e-06 BETTER
I0328 12:56:06.042210 2311271 finetune.py:68] layer 1_down @ epoch 3 new loss 2.226085371148656e-06 old loss 2.226415745099075e-06 BETTER
I0328 12:56:13.452950 2311343 finetune.py:68] layer 2_down @ epoch 0 new loss 4.476044523471501e-06 old loss 4.476190952118486e-06 BETTER
I0328 12:56:29.273585 2311199 finetune.py:68] layer 0_down @ epoch 4 new loss 1.1370590300430194e-06 old loss 1.1371417940608808e-06 BETTER
0_v proxy err 0.014895766973495483 tr(WHW.T) 60.88684844970703
bpp_loss 3.3617614617105573
0_q proxy err 2.400005905656144e-05 tr(WHW.T) 288076.875
bpp_loss 4.238121094123926
0_k proxy err 2.7125313863507472e-05 tr(WHW.T) 100161.078125
bpp_loss 4.854944872087799
0_o proxy err 0.0021667317487299442 tr(WHW.T) 3120.658203125
bpp_loss 3.453862933674827
0_up proxy err 0.004239746835082769 tr(WHW.T) 8924.5537109375
bpp_loss 3.74161984332438
0_gate proxy err 0.0024136316496878862 tr(WHW.T) 15778.767578125
bpp_loss 3.856376505523388
0_down proxy err 0.003387527074664831 tr(WHW.T) 10824.89453125
bpp_loss 3.735118704748207
I0328 12:56:33.763641 2311271 finetune.py:68] layer 1_down @ epoch 4 new loss 2.225878006356652e-06 old loss 2.226085371148656e-06 BETTER
1_v proxy err 0.006402443163096905 tr(WHW.T) 109.07096099853516
bpp_loss 3.4634377619368024
1_q proxy err 2.2769465431338176e-05 tr(WHW.T) 144762.6875
bpp_loss 4.497848371393047
1_k proxy err 1.3490851415554062e-05 tr(WHW.T) 75463.9921875
bpp_loss 5.281004086311441
1_o proxy err 0.004065783228725195 tr(WHW.T) 1989.2791748046875
bpp_loss 3.5358464138116688
1_up proxy err 0.004855785984545946 tr(WHW.T) 8230.4697265625
bpp_loss 3.7559895057763373
1_gate proxy err 0.0028836701530963182 tr(WHW.T) 13948.400390625
bpp_loss 3.8668925048384284
1_down proxy err 9.399204282090068e-05 tr(WHW.T) 13976.638671875
bpp_loss 3.7492957248385728
I0328 12:56:41.203790 2311343 finetune.py:68] layer 2_down @ epoch 1 new loss 4.475942205317551e-06 old loss 4.476044523471501e-06 BETTER
I0328 12:56:47.078568 2311415 finetune.py:45] layer 3_down initial loss 8.575634637963958e-06
I0328 12:57:08.978369 2311343 finetune.py:68] layer 2_down @ epoch 2 new loss 4.475835339690093e-06 old loss 4.475942205317551e-06 BETTER
I0328 12:57:13.076582 2311415 finetune.py:68] layer 3_down @ epoch 0 new loss 8.575492756790482e-06 old loss 8.575634637963958e-06 BETTER
I0328 12:57:36.669788 2311343 finetune.py:68] layer 2_down @ epoch 3 new loss 4.47575166617753e-06 old loss 4.475835339690093e-06 BETTER
I0328 12:57:40.032209 2311415 finetune.py:68] layer 3_down @ epoch 1 new loss 8.575388164899778e-06 old loss 8.575492756790482e-06 BETTER
I0328 12:57:49.617833 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 68.91032576560974s
I0328 12:57:53.515974 2311485 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:57:53.516074 2311485 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:57:53.516115 2311485 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:57:53.874960 2311485 config.py:54] PyTorch version 2.6.0 available.
W0328 12:57:54.100630 2311485 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:57:54.802072 2311485 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:57:54.805833 2311068 quantize_finetune_llama.py:209] layer 5 gpu 1
I0328 12:57:54.821325 2311485 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 12:58:04.577782 2311343 finetune.py:68] layer 2_down @ epoch 4 new loss 4.47566981165437e-06 old loss 4.47575166617753e-06 BETTER
2_v proxy err 0.00988039094954729 tr(WHW.T) 155.95950317382812
bpp_loss 3.3727842771913856
2_q proxy err 0.00016384627087973058 tr(WHW.T) 41460.265625
bpp_loss 4.422798314073589
2_k proxy err 8.741286001168191e-05 tr(WHW.T) 22600.3828125
bpp_loss 5.344259175239131
2_o proxy err 0.00399643974378705 tr(WHW.T) 1966.325927734375
bpp_loss 3.4861818223726004
2_up proxy err 0.005733618512749672 tr(WHW.T) 7601.4521484375
bpp_loss 3.7472337479037896
2_gate proxy err 0.0029093080665916204 tr(WHW.T) 15115.7998046875
bpp_loss 3.9003331030585935
2_down proxy err 0.005399046465754509 tr(WHW.T) 7737.6572265625
bpp_loss 3.751905756121102
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:58:07.185302 2311415 finetune.py:68] layer 3_down @ epoch 2 new loss 8.575251740694512e-06 old loss 8.575388164899778e-06 BETTER
I0328 12:58:12.101037 2311485 finetune.py:45] layer 4_v initial loss 6.990959718677914e-06
W0328 12:58:12.101226 2311485 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 12:58:34.534170 2311415 finetune.py:68] layer 3_down @ epoch 3 new loss 8.575170795666054e-06 old loss 8.575251740694512e-06 BETTER
I0328 12:58:47.169795 2311485 finetune.py:68] layer 4_v @ epoch 0 new loss 2.1658670448232442e-06 old loss 6.990959718677914e-06 BETTER
I0328 12:59:01.875447 2311415 finetune.py:68] layer 3_down @ epoch 4 new loss 8.575080755690578e-06 old loss 8.575170795666054e-06 BETTER
3_v proxy err 0.007500324863940477 tr(WHW.T) 289.3331604003906
bpp_loss 3.469767422240693
3_q proxy err 0.00019523699302226305 tr(WHW.T) 47575.58203125
bpp_loss 4.462866425223183
3_k proxy err 9.64101345743984e-05 tr(WHW.T) 26166.62890625
bpp_loss 5.426779303525109
3_o proxy err 0.004694856237620115 tr(WHW.T) 1858.8299560546875
bpp_loss 3.582911998091731
3_up proxy err 0.005700649693608284 tr(WHW.T) 7535.6943359375
bpp_loss 3.729012719954231
3_gate proxy err 0.002087501809000969 tr(WHW.T) 20879.517578125
bpp_loss 3.9786325339227915
3_down proxy err 0.0062203481793403625 tr(WHW.T) 7009.1455078125
bpp_loss 3.7276864492367685
I0328 12:59:16.049382 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 65.27920937538147s
I0328 12:59:19.747221 2311555 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:59:19.747331 2311555 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:59:19.747374 2311555 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:59:20.118369 2311555 config.py:54] PyTorch version 2.6.0 available.
W0328 12:59:20.332153 2311555 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:59:21.023692 2311555 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:59:21.027642 2311068 quantize_finetune_llama.py:209] layer 6 gpu 2
I0328 12:59:21.041780 2311555 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 12:59:23.736901 2311485 finetune.py:68] layer 4_v @ epoch 1 new loss 1.6857642322065658e-06 old loss 2.1658670448232442e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:59:37.725804 2311555 finetune.py:45] layer 5_v initial loss 7.67426536185667e-06
W0328 12:59:37.726270 2311555 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:00:00.579869 2311485 finetune.py:68] layer 4_v @ epoch 2 new loss 1.52202119352296e-06 old loss 1.6857642322065658e-06 BETTER
I0328 13:00:10.863257 2311555 finetune.py:68] layer 5_v @ epoch 0 new loss 2.804047198878834e-06 old loss 7.67426536185667e-06 BETTER
I0328 13:00:25.634346 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 64.09451675415039s
I0328 13:00:29.523041 2311625 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:00:29.523145 2311625 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:00:29.523190 2311625 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:00:29.876467 2311625 config.py:54] PyTorch version 2.6.0 available.
W0328 13:00:30.090415 2311625 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 13:00:30.738186 2311625 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:00:30.742512 2311068 quantize_finetune_llama.py:209] layer 7 gpu 3
I0328 13:00:30.757663 2311625 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 13:00:37.502632 2311485 finetune.py:68] layer 4_v @ epoch 3 new loss 1.440593223378528e-06 old loss 1.52202119352296e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:00:45.258933 2311555 finetune.py:68] layer 5_v @ epoch 1 new loss 2.370981974308961e-06 old loss 2.804047198878834e-06 BETTER
I0328 13:00:48.169944 2311625 finetune.py:45] layer 6_v initial loss 6.131445388746215e-06
W0328 13:00:48.170172 2311625 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:01:14.545093 2311485 finetune.py:68] layer 4_v @ epoch 4 new loss 1.390554189129034e-06 old loss 1.440593223378528e-06 BETTER
I0328 13:01:20.048974 2311555 finetune.py:68] layer 5_v @ epoch 2 new loss 2.2131828245619545e-06 old loss 2.370981974308961e-06 BETTER
I0328 13:01:21.636588 2311625 finetune.py:68] layer 6_v @ epoch 0 new loss 3.0627559226559242e-06 old loss 6.131445388746215e-06 BETTER
I0328 13:01:34.011277 2311485 finetune.py:45] layer 4_q initial loss 1.6432850316050462e-06
I0328 13:01:36.289241 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 64.9905173778534s
I0328 13:01:40.291885 2311695 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:01:40.291989 2311695 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:01:40.292032 2311695 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:01:40.666533 2311695 config.py:54] PyTorch version 2.6.0 available.
W0328 13:01:40.903698 2311695 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 13:01:41.532670 2311695 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:01:41.536448 2311068 quantize_finetune_llama.py:209] layer 8 gpu 0
I0328 13:01:41.553931 2311695 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:01:54.887834 2311555 finetune.py:68] layer 5_v @ epoch 3 new loss 2.130555685653235e-06 old loss 2.2131828245619545e-06 BETTER
I0328 13:01:56.359160 2311625 finetune.py:68] layer 6_v @ epoch 1 new loss 2.775667553578387e-06 old loss 3.0627559226559242e-06 BETTER
I0328 13:01:58.850259 2311695 finetune.py:45] layer 7_v initial loss 5.852003596373834e-06
W0328 13:01:58.850454 2311695 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:02:09.170526 2311485 finetune.py:68] layer 4_q @ epoch 0 new loss 1.5889355609033373e-06 old loss 1.6432850316050462e-06 BETTER
I0328 13:02:30.266612 2311555 finetune.py:68] layer 5_v @ epoch 4 new loss 2.0782188130397117e-06 old loss 2.130555685653235e-06 BETTER
I0328 13:02:31.547977 2311625 finetune.py:68] layer 6_v @ epoch 2 new loss 2.6567520308162784e-06 old loss 2.775667553578387e-06 BETTER
I0328 13:02:31.897175 2311695 finetune.py:68] layer 7_v @ epoch 0 new loss 3.632601647041156e-06 old loss 5.852003596373834e-06 BETTER
I0328 13:02:45.467329 2311485 finetune.py:68] layer 4_q @ epoch 1 new loss 1.553929678266286e-06 old loss 1.5889355609033373e-06 BETTER
I0328 13:02:49.876219 2311555 finetune.py:45] layer 5_q initial loss 2.47732418756641e-06
I0328 13:03:06.210876 2311695 finetune.py:68] layer 7_v @ epoch 1 new loss 3.4062034046655754e-06 old loss 3.632601647041156e-06 BETTER
I0328 13:03:06.631160 2311625 finetune.py:68] layer 6_v @ epoch 3 new loss 2.588138386272476e-06 old loss 2.6567520308162784e-06 BETTER
I0328 13:03:22.193993 2311485 finetune.py:68] layer 4_q @ epoch 2 new loss 1.5279523495337344e-06 old loss 1.553929678266286e-06 BETTER
I0328 13:03:23.347860 2311555 finetune.py:68] layer 5_q @ epoch 0 new loss 2.4114920051943045e-06 old loss 2.47732418756641e-06 BETTER
I0328 13:03:40.605310 2311695 finetune.py:68] layer 7_v @ epoch 2 new loss 3.3022163279383676e-06 old loss 3.4062034046655754e-06 BETTER
I0328 13:03:41.889291 2311625 finetune.py:68] layer 6_v @ epoch 4 new loss 2.5400277081644163e-06 old loss 2.588138386272476e-06 BETTER
I0328 13:03:57.847125 2311555 finetune.py:68] layer 5_q @ epoch 1 new loss 2.3705763396719703e-06 old loss 2.4114920051943045e-06 BETTER
I0328 13:03:58.957976 2311485 finetune.py:68] layer 4_q @ epoch 3 new loss 1.5072571386554046e-06 old loss 1.5279523495337344e-06 BETTER
I0328 13:04:01.808147 2311625 finetune.py:45] layer 6_q initial loss 3.010082082255394e-06
I0328 13:04:15.233778 2311695 finetune.py:68] layer 7_v @ epoch 3 new loss 3.2349523735319963e-06 old loss 3.3022163279383676e-06 BETTER
I0328 13:04:32.553646 2311555 finetune.py:68] layer 5_q @ epoch 2 new loss 2.33894252232858e-06 old loss 2.3705763396719703e-06 BETTER
I0328 13:04:35.512444 2311625 finetune.py:68] layer 6_q @ epoch 0 new loss 2.9479533623089083e-06 old loss 3.010082082255394e-06 BETTER
I0328 13:04:35.850056 2311485 finetune.py:68] layer 4_q @ epoch 4 new loss 1.4897540268066223e-06 old loss 1.5072571386554046e-06 BETTER
I0328 13:04:49.852243 2311695 finetune.py:68] layer 7_v @ epoch 4 new loss 3.1887173008726677e-06 old loss 3.2349523735319963e-06 BETTER
I0328 13:04:54.097420 2311485 finetune.py:45] layer 4_k initial loss 1.5841656022530515e-06
I0328 13:05:07.213674 2311555 finetune.py:68] layer 5_q @ epoch 3 new loss 2.313794766450883e-06 old loss 2.33894252232858e-06 BETTER
I0328 13:05:09.925134 2311695 finetune.py:45] layer 7_q initial loss 3.877305061905645e-06
I0328 13:05:10.247996 2311625 finetune.py:68] layer 6_q @ epoch 1 new loss 2.9039435958111426e-06 old loss 2.9479533623089083e-06 BETTER
I0328 13:05:29.322272 2311485 finetune.py:68] layer 4_k @ epoch 0 new loss 1.5633966086170403e-06 old loss 1.5841656022530515e-06 BETTER
I0328 13:05:41.892086 2311555 finetune.py:68] layer 5_q @ epoch 4 new loss 2.292368662892841e-06 old loss 2.313794766450883e-06 BETTER
I0328 13:05:43.124121 2311695 finetune.py:68] layer 7_q @ epoch 0 new loss 3.807748271356104e-06 old loss 3.877305061905645e-06 BETTER
I0328 13:05:45.004210 2311625 finetune.py:68] layer 6_q @ epoch 2 new loss 2.8706290322588757e-06 old loss 2.9039435958111426e-06 BETTER
I0328 13:05:59.880312 2311555 finetune.py:45] layer 5_k initial loss 2.4063529053819366e-06
I0328 13:06:05.719724 2311485 finetune.py:68] layer 4_k @ epoch 1 new loss 1.549507260278915e-06 old loss 1.5633966086170403e-06 BETTER
I0328 13:06:17.238553 2311695 finetune.py:68] layer 7_q @ epoch 1 new loss 3.7606484966090648e-06 old loss 3.807748271356104e-06 BETTER
I0328 13:06:19.827361 2311625 finetune.py:68] layer 6_q @ epoch 3 new loss 2.842069534381153e-06 old loss 2.8706290322588757e-06 BETTER
I0328 13:06:33.454768 2311555 finetune.py:68] layer 5_k @ epoch 0 new loss 2.380837941018399e-06 old loss 2.4063529053819366e-06 BETTER
I0328 13:06:42.205978 2311485 finetune.py:68] layer 4_k @ epoch 2 new loss 1.5379916931124171e-06 old loss 1.549507260278915e-06 BETTER
I0328 13:06:51.485153 2311695 finetune.py:68] layer 7_q @ epoch 2 new loss 3.720122322192765e-06 old loss 3.7606484966090648e-06 BETTER
I0328 13:06:54.780380 2311625 finetune.py:68] layer 6_q @ epoch 4 new loss 2.817945642163977e-06 old loss 2.842069534381153e-06 BETTER
I0328 13:07:07.826470 2311555 finetune.py:68] layer 5_k @ epoch 1 new loss 2.364185320402612e-06 old loss 2.380837941018399e-06 BETTER
I0328 13:07:12.432007 2311625 finetune.py:45] layer 6_k initial loss 2.974010612888378e-06
I0328 13:07:18.862218 2311485 finetune.py:68] layer 4_k @ epoch 3 new loss 1.5279654235200724e-06 old loss 1.5379916931124171e-06 BETTER
I0328 13:07:25.816223 2311695 finetune.py:68] layer 7_q @ epoch 3 new loss 3.6860176351183327e-06 old loss 3.720122322192765e-06 BETTER
I0328 13:07:42.269437 2311555 finetune.py:68] layer 5_k @ epoch 2 new loss 2.3498187147197314e-06 old loss 2.364185320402612e-06 BETTER
I0328 13:07:45.925896 2311625 finetune.py:68] layer 6_k @ epoch 0 new loss 2.936753617177601e-06 old loss 2.974010612888378e-06 BETTER
I0328 13:07:55.685160 2311485 finetune.py:68] layer 4_k @ epoch 4 new loss 1.5182328070295625e-06 old loss 1.5279654235200724e-06 BETTER
I0328 13:08:00.157494 2311695 finetune.py:68] layer 7_q @ epoch 4 new loss 3.657345814644941e-06 old loss 3.6860176351183327e-06 BETTER
I0328 13:08:15.360772 2311485 finetune.py:45] layer 4_o initial loss 3.3890821669047e-06
I0328 13:08:16.821184 2311555 finetune.py:68] layer 5_k @ epoch 3 new loss 2.336936859137495e-06 old loss 2.3498187147197314e-06 BETTER
I0328 13:08:18.194772 2311695 finetune.py:45] layer 7_k initial loss 3.839657892967807e-06
I0328 13:08:20.510098 2311625 finetune.py:68] layer 6_k @ epoch 1 new loss 2.9173741040722234e-06 old loss 2.936753617177601e-06 BETTER
I0328 13:08:50.107417 2311485 finetune.py:68] layer 4_o @ epoch 0 new loss 3.3052392609533854e-06 old loss 3.3890821669047e-06 BETTER
I0328 13:08:51.542746 2311695 finetune.py:68] layer 7_k @ epoch 0 new loss 3.8009884519851767e-06 old loss 3.839657892967807e-06 BETTER
I0328 13:08:51.694977 2311555 finetune.py:68] layer 5_k @ epoch 4 new loss 2.3247987428476335e-06 old loss 2.336936859137495e-06 BETTER
I0328 13:08:55.179266 2311625 finetune.py:68] layer 6_k @ epoch 2 new loss 2.8998401830904186e-06 old loss 2.9173741040722234e-06 BETTER
I0328 13:09:11.058952 2311555 finetune.py:45] layer 5_o initial loss 4.7208227442752104e-06
I0328 13:09:25.633333 2311695 finetune.py:68] layer 7_k @ epoch 1 new loss 3.785599346883828e-06 old loss 3.8009884519851767e-06 BETTER
I0328 13:09:26.107051 2311485 finetune.py:68] layer 4_o @ epoch 1 new loss 3.273233005529619e-06 old loss 3.3052392609533854e-06 BETTER
I0328 13:09:29.822800 2311625 finetune.py:68] layer 6_k @ epoch 3 new loss 2.8858175937784836e-06 old loss 2.8998401830904186e-06 BETTER
I0328 13:09:43.819263 2311555 finetune.py:68] layer 5_o @ epoch 0 new loss 4.6222035052778665e-06 old loss 4.7208227442752104e-06 BETTER
I0328 13:09:59.886073 2311695 finetune.py:68] layer 7_k @ epoch 2 new loss 3.7589206840493716e-06 old loss 3.785599346883828e-06 BETTER
I0328 13:10:01.968127 2311485 finetune.py:68] layer 4_o @ epoch 2 new loss 3.248148914281046e-06 old loss 3.273233005529619e-06 BETTER
I0328 13:10:04.534856 2311625 finetune.py:68] layer 6_k @ epoch 4 new loss 2.870627668016823e-06 old loss 2.8858175937784836e-06 BETTER
I0328 13:10:17.450424 2311555 finetune.py:68] layer 5_o @ epoch 1 new loss 4.580489530781051e-06 old loss 4.6222035052778665e-06 BETTER
I0328 13:10:23.855931 2311625 finetune.py:45] layer 6_o initial loss 6.442574886023067e-06
I0328 13:10:34.017236 2311695 finetune.py:68] layer 7_k @ epoch 3 new loss 3.7424642869154923e-06 old loss 3.7589206840493716e-06 BETTER
I0328 13:10:38.082988 2311485 finetune.py:68] layer 4_o @ epoch 3 new loss 3.227530214644503e-06 old loss 3.248148914281046e-06 BETTER
I0328 13:10:51.285133 2311555 finetune.py:68] layer 5_o @ epoch 2 new loss 4.548357082967414e-06 old loss 4.580489530781051e-06 BETTER
I0328 13:10:56.747447 2311625 finetune.py:68] layer 6_o @ epoch 0 new loss 6.318864961940562e-06 old loss 6.442574886023067e-06 BETTER
I0328 13:11:08.080897 2311695 finetune.py:68] layer 7_k @ epoch 4 new loss 3.7213221730780788e-06 old loss 3.7424642869154923e-06 BETTER
I0328 13:11:13.893440 2311485 finetune.py:68] layer 4_o @ epoch 4 new loss 3.2090592867461964e-06 old loss 3.227530214644503e-06 BETTER
I0328 13:11:25.126446 2311555 finetune.py:68] layer 5_o @ epoch 3 new loss 4.521211849350948e-06 old loss 4.548357082967414e-06 BETTER
I0328 13:11:27.972302 2311695 finetune.py:45] layer 7_o initial loss 8.231942047132179e-06
I0328 13:11:30.755667 2311625 finetune.py:68] layer 6_o @ epoch 1 new loss 6.252725142985582e-06 old loss 6.318864961940562e-06 BETTER
I0328 13:11:45.401564 2311485 finetune.py:45] layer 4_up initial loss 7.202988854260184e-06
I0328 13:11:58.864192 2311555 finetune.py:68] layer 5_o @ epoch 4 new loss 4.497900135902455e-06 old loss 4.521211849350948e-06 BETTER
I0328 13:12:00.168979 2311695 finetune.py:68] layer 7_o @ epoch 0 new loss 8.04388582764659e-06 old loss 8.231942047132179e-06 BETTER
I0328 13:12:04.655535 2311625 finetune.py:68] layer 6_o @ epoch 2 new loss 6.200620646268362e-06 old loss 6.252725142985582e-06 BETTER
I0328 13:12:17.282649 2311485 finetune.py:68] layer 4_up @ epoch 0 new loss 7.157173058658373e-06 old loss 7.202988854260184e-06 BETTER
I0328 13:12:30.355716 2311555 finetune.py:45] layer 5_up initial loss 1.028983297146624e-05
I0328 13:12:33.602134 2311695 finetune.py:68] layer 7_o @ epoch 1 new loss 7.942289812490344e-06 old loss 8.04388582764659e-06 BETTER
I0328 13:12:38.771044 2311625 finetune.py:68] layer 6_o @ epoch 3 new loss 6.156014023872558e-06 old loss 6.200620646268362e-06 BETTER
I0328 13:12:50.588840 2311485 finetune.py:68] layer 4_up @ epoch 1 new loss 7.120871032384457e-06 old loss 7.157173058658373e-06 BETTER
I0328 13:13:00.999691 2311555 finetune.py:68] layer 5_up @ epoch 0 new loss 1.0203003512287978e-05 old loss 1.028983297146624e-05 BETTER
I0328 13:13:07.069471 2311695 finetune.py:68] layer 7_o @ epoch 2 new loss 7.864383405831177e-06 old loss 7.942289812490344e-06 BETTER
I0328 13:13:12.954409 2311625 finetune.py:68] layer 6_o @ epoch 4 new loss 6.116497843322577e-06 old loss 6.156014023872558e-06 BETTER
I0328 13:13:24.075075 2311485 finetune.py:68] layer 4_up @ epoch 2 new loss 7.0883302214497235e-06 old loss 7.120871032384457e-06 BETTER
I0328 13:13:32.693123 2311555 finetune.py:68] layer 5_up @ epoch 1 new loss 1.0136571290786378e-05 old loss 1.0203003512287978e-05 BETTER
I0328 13:13:40.504596 2311695 finetune.py:68] layer 7_o @ epoch 3 new loss 7.798472324793693e-06 old loss 7.864383405831177e-06 BETTER
I0328 13:13:44.747029 2311625 finetune.py:45] layer 6_up initial loss 1.346785302303033e-05
I0328 13:13:57.642433 2311485 finetune.py:68] layer 4_up @ epoch 3 new loss 7.057483799144393e-06 old loss 7.0883302214497235e-06 BETTER
I0328 13:14:04.609149 2311555 finetune.py:68] layer 5_up @ epoch 2 new loss 1.0076046237372793e-05 old loss 1.0136571290786378e-05 BETTER
I0328 13:14:14.043421 2311695 finetune.py:68] layer 7_o @ epoch 4 new loss 7.742058187432121e-06 old loss 7.798472324793693e-06 BETTER
I0328 13:14:15.517999 2311625 finetune.py:68] layer 6_up @ epoch 0 new loss 1.3327792657946702e-05 old loss 1.346785302303033e-05 BETTER
I0328 13:14:31.235308 2311485 finetune.py:68] layer 4_up @ epoch 4 new loss 7.0286105255945586e-06 old loss 7.057483799144393e-06 BETTER
I0328 13:14:36.544817 2311555 finetune.py:68] layer 5_up @ epoch 3 new loss 1.0020648915087804e-05 old loss 1.0076046237372793e-05 BETTER
I0328 13:14:45.433486 2311695 finetune.py:45] layer 7_up initial loss 1.5688112398493104e-05
I0328 13:14:47.668175 2311625 finetune.py:68] layer 6_up @ epoch 1 new loss 1.3216444131103344e-05 old loss 1.3327792657946702e-05 BETTER
I0328 13:15:02.624032 2311485 finetune.py:45] layer 4_gate initial loss 8.440105375484563e-06
I0328 13:15:08.725018 2311555 finetune.py:68] layer 5_up @ epoch 4 new loss 9.968787708203308e-06 old loss 1.0020648915087804e-05 BETTER
I0328 13:15:15.792309 2311695 finetune.py:68] layer 7_up @ epoch 0 new loss 1.5513844118686393e-05 old loss 1.5688112398493104e-05 BETTER
I0328 13:15:20.154019 2311625 finetune.py:68] layer 6_up @ epoch 2 new loss 1.3116847185301594e-05 old loss 1.3216444131103344e-05 BETTER
I0328 13:15:32.648503 2311485 finetune.py:68] layer 4_gate @ epoch 0 new loss 8.409239853790496e-06 old loss 8.440105375484563e-06 BETTER
I0328 13:15:40.239591 2311555 finetune.py:45] layer 5_gate initial loss 1.1993999578407966e-05
I0328 13:15:47.099688 2311695 finetune.py:68] layer 7_up @ epoch 1 new loss 1.5376934243249707e-05 old loss 1.5513844118686393e-05 BETTER
I0328 13:15:52.413540 2311625 finetune.py:68] layer 6_up @ epoch 3 new loss 1.3026316082687117e-05 old loss 1.3116847185301594e-05 BETTER
I0328 13:16:04.009107 2311485 finetune.py:68] layer 4_gate @ epoch 1 new loss 8.383030035474803e-06 old loss 8.409239853790496e-06 BETTER
I0328 13:16:08.671525 2311555 finetune.py:68] layer 5_gate @ epoch 0 new loss 1.1939441719732713e-05 old loss 1.1993999578407966e-05 BETTER
I0328 13:16:18.295826 2311695 finetune.py:68] layer 7_up @ epoch 2 new loss 1.5255323887686245e-05 old loss 1.5376934243249707e-05 BETTER
I0328 13:16:24.604090 2311625 finetune.py:68] layer 6_up @ epoch 4 new loss 1.2941671229782514e-05 old loss 1.3026316082687117e-05 BETTER
I0328 13:16:35.457640 2311485 finetune.py:68] layer 4_gate @ epoch 2 new loss 8.358566446986515e-06 old loss 8.383030035474803e-06 BETTER
I0328 13:16:38.123214 2311555 finetune.py:68] layer 5_gate @ epoch 1 new loss 1.189204522233922e-05 old loss 1.1939441719732713e-05 BETTER
I0328 13:16:49.582841 2311695 finetune.py:68] layer 7_up @ epoch 3 new loss 1.5144702047109604e-05 old loss 1.5255323887686245e-05 BETTER
I0328 13:16:56.009341 2311625 finetune.py:45] layer 6_gate initial loss 1.535342016723007e-05
I0328 13:17:06.901498 2311485 finetune.py:68] layer 4_gate @ epoch 3 new loss 8.335610800713766e-06 old loss 8.358566446986515e-06 BETTER
I0328 13:17:08.007895 2311555 finetune.py:68] layer 5_gate @ epoch 2 new loss 1.1847850146295968e-05 old loss 1.189204522233922e-05 BETTER
I0328 13:17:21.967776 2311695 finetune.py:68] layer 7_up @ epoch 4 new loss 1.5042419363453519e-05 old loss 1.5144702047109604e-05 BETTER
I0328 13:17:25.279655 2311625 finetune.py:68] layer 6_gate @ epoch 0 new loss 1.5267532944562845e-05 old loss 1.535342016723007e-05 BETTER
I0328 13:17:37.922028 2311555 finetune.py:68] layer 5_gate @ epoch 3 new loss 1.1807133887486998e-05 old loss 1.1847850146295968e-05 BETTER
I0328 13:17:38.563639 2311485 finetune.py:68] layer 4_gate @ epoch 4 new loss 8.313534635817632e-06 old loss 8.335610800713766e-06 BETTER
I0328 13:17:54.070184 2311695 finetune.py:45] layer 7_gate initial loss 1.799162055249326e-05
I0328 13:17:54.987880 2311625 finetune.py:68] layer 6_gate @ epoch 1 new loss 1.5192352293524891e-05 old loss 1.5267532944562845e-05 BETTER
I0328 13:18:07.720602 2311555 finetune.py:68] layer 5_gate @ epoch 4 new loss 1.1767579053412192e-05 old loss 1.1807133887486998e-05 BETTER
I0328 13:18:22.208935 2311695 finetune.py:68] layer 7_gate @ epoch 0 new loss 1.788788904377725e-05 old loss 1.799162055249326e-05 BETTER
I0328 13:18:25.206554 2311625 finetune.py:68] layer 6_gate @ epoch 2 new loss 1.5123285265872255e-05 old loss 1.5192352293524891e-05 BETTER
I0328 13:18:35.693667 2311485 finetune.py:45] layer 4_down initial loss 1.3808497897116467e-05
I0328 13:18:51.803239 2311695 finetune.py:68] layer 7_gate @ epoch 1 new loss 1.7798642147681676e-05 old loss 1.788788904377725e-05 BETTER
I0328 13:18:55.317939 2311625 finetune.py:68] layer 6_gate @ epoch 3 new loss 1.505799446022138e-05 old loss 1.5123285265872255e-05 BETTER
I0328 13:19:03.020938 2311485 finetune.py:68] layer 4_down @ epoch 0 new loss 1.3807997675030492e-05 old loss 1.3808497897116467e-05 BETTER
I0328 13:19:04.613055 2311555 finetune.py:45] layer 5_down initial loss 1.9328015696373768e-05
I0328 13:19:21.446227 2311695 finetune.py:68] layer 7_gate @ epoch 2 new loss 1.7716443835524842e-05 old loss 1.7798642147681676e-05 BETTER
I0328 13:19:25.401738 2311625 finetune.py:68] layer 6_gate @ epoch 4 new loss 1.4996154277469032e-05 old loss 1.505799446022138e-05 BETTER
I0328 13:19:30.844075 2311555 finetune.py:68] layer 5_down @ epoch 0 new loss 1.9327313566464e-05 old loss 1.9328015696373768e-05 BETTER
I0328 13:19:31.553755 2311485 finetune.py:68] layer 4_down @ epoch 1 new loss 1.380770936521003e-05 old loss 1.3807997675030492e-05 BETTER
I0328 13:19:50.965960 2311695 finetune.py:68] layer 7_gate @ epoch 3 new loss 1.7638820281717926e-05 old loss 1.7716443835524842e-05 BETTER
I0328 13:19:58.064557 2311555 finetune.py:68] layer 5_down @ epoch 1 new loss 1.9326860638102517e-05 old loss 1.9327313566464e-05 BETTER
I0328 13:20:00.318694 2311485 finetune.py:68] layer 4_down @ epoch 2 new loss 1.3807454706693534e-05 old loss 1.380770936521003e-05 BETTER
I0328 13:20:20.461874 2311695 finetune.py:68] layer 7_gate @ epoch 4 new loss 1.7566098904353566e-05 old loss 1.7638820281717926e-05 BETTER
I0328 13:20:22.964911 2311625 finetune.py:45] layer 6_down initial loss 2.4136952561093494e-05
I0328 13:20:25.722061 2311555 finetune.py:68] layer 5_down @ epoch 2 new loss 1.932655504788272e-05 old loss 1.9326860638102517e-05 BETTER
I0328 13:20:29.287040 2311485 finetune.py:68] layer 4_down @ epoch 3 new loss 1.3807254617859144e-05 old loss 1.3807454706693534e-05 BETTER
I0328 13:20:49.359612 2311625 finetune.py:68] layer 6_down @ epoch 0 new loss 2.4135968487826176e-05 old loss 2.4136952561093494e-05 BETTER
I0328 13:20:53.447355 2311555 finetune.py:68] layer 5_down @ epoch 3 new loss 1.932619488798082e-05 old loss 1.932655504788272e-05 BETTER
I0328 13:20:58.321929 2311485 finetune.py:68] layer 4_down @ epoch 4 new loss 1.3807064533466473e-05 old loss 1.3807254617859144e-05 BETTER
4_v proxy err 0.00680414168164134 tr(WHW.T) 285.30712890625
bpp_loss 3.515440603659954
4_q proxy err 0.0001661979767959565 tr(WHW.T) 50119.05859375
bpp_loss 4.427144396235235
4_k proxy err 7.872104470152408e-05 tr(WHW.T) 29289.794921875
bpp_loss 5.402156394266058
4_o proxy err 0.0053957924246788025 tr(WHW.T) 1302.2686767578125
bpp_loss 3.5935858689481393
4_up proxy err 0.0058012306690216064 tr(WHW.T) 7380.45068359375
bpp_loss 3.7019626421055625
4_gate proxy err 0.0015009454218670726 tr(WHW.T) 29116.0703125
bpp_loss 4.053962214118136
4_down proxy err 0.006700871046632528 tr(WHW.T) 6402.68505859375
bpp_loss 3.702646644519908
I0328 13:21:17.192946 2311625 finetune.py:68] layer 6_down @ epoch 1 new loss 2.4135188141372055e-05 old loss 2.4135968487826176e-05 BETTER
I0328 13:21:18.443235 2311695 finetune.py:45] layer 7_down initial loss 2.7519979994394816e-05
I0328 13:21:21.342543 2311555 finetune.py:68] layer 5_down @ epoch 4 new loss 1.9326014808029868e-05 old loss 1.932619488798082e-05 BETTER
5_v proxy err 0.00964779406785965 tr(WHW.T) 208.81988525390625
bpp_loss 3.402652598801069
5_q proxy err 0.00024118587316479534 tr(WHW.T) 36002.34765625
bpp_loss 4.40056790120434
5_k proxy err 0.00010723837476689368 tr(WHW.T) 22994.98828125
bpp_loss 5.370283065713011
5_o proxy err 0.0058805388398468494 tr(WHW.T) 1059.2498779296875
bpp_loss 3.5404410308692604
5_up proxy err 0.0055813235230743885 tr(WHW.T) 7655.76708984375
bpp_loss 3.7070704113825093
5_gate proxy err 0.0014364076778292656 tr(WHW.T) 30369.279296875
bpp_loss 4.055715939761805
5_down proxy err 0.006459888070821762 tr(WHW.T) 6420.900390625
bpp_loss 3.707938141023208
I0328 13:21:44.444208 2311695 finetune.py:68] layer 7_down @ epoch 0 new loss 2.7518644856172614e-05 old loss 2.7519979994394816e-05 BETTER
I0328 13:21:45.133674 2311625 finetune.py:68] layer 6_down @ epoch 2 new loss 2.4134747945936397e-05 old loss 2.4135188141372055e-05 BETTER
I0328 13:22:11.557569 2311695 finetune.py:68] layer 7_down @ epoch 1 new loss 2.751773536147084e-05 old loss 2.7518644856172614e-05 BETTER
I0328 13:22:12.856405 2311625 finetune.py:68] layer 6_down @ epoch 3 new loss 2.413429319858551e-05 old loss 2.4134747945936397e-05 BETTER
I0328 13:22:33.242928 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 66.81621384620667s
I0328 13:22:37.219683 2311765 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:22:37.219802 2311765 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:22:37.219862 2311765 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:22:37.624290 2311765 config.py:54] PyTorch version 2.6.0 available.
W0328 13:22:37.857574 2311765 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 13:22:38.535476 2311765 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:22:38.539366 2311068 quantize_finetune_llama.py:209] layer 9 gpu 1
I0328 13:22:38.554590 2311765 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 13:22:38.833274 2311695 finetune.py:68] layer 7_down @ epoch 2 new loss 2.751706779235974e-05 old loss 2.751773536147084e-05 BETTER
I0328 13:22:40.661598 2311625 finetune.py:68] layer 6_down @ epoch 4 new loss 2.4134014893206768e-05 old loss 2.413429319858551e-05 BETTER
6_v proxy err 0.00810823030769825 tr(WHW.T) 253.63377380371094
bpp_loss 3.447755796718411
6_q proxy err 0.0002473326167091727 tr(WHW.T) 35647.09765625
bpp_loss 4.448497055913322
6_k proxy err 9.225100075127557e-05 tr(WHW.T) 26157.298828125
bpp_loss 5.439659635885619
6_o proxy err 0.006760701071470976 tr(WHW.T) 1012.2965087890625
bpp_loss 3.5724317972781137
6_up proxy err 0.005260092671960592 tr(WHW.T) 7923.8291015625
bpp_loss 3.7061190768810257
6_gate proxy err 0.0011913905618712306 tr(WHW.T) 35733.28125
bpp_loss 4.061706049567355
6_down proxy err 0.006196525879204273 tr(WHW.T) 6494.615234375
bpp_loss 3.7080963691696525
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:22:55.673450 2311765 finetune.py:45] layer 8_v initial loss 6.049131570762256e-06
W0328 13:22:55.673673 2311765 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:23:06.164032 2311695 finetune.py:68] layer 7_down @ epoch 3 new loss 2.75165857601678e-05 old loss 2.751706779235974e-05 BETTER
I0328 13:23:30.467775 2311765 finetune.py:68] layer 8_v @ epoch 0 new loss 4.156686827627709e-06 old loss 6.049131570762256e-06 BETTER
I0328 13:23:33.355781 2311695 finetune.py:68] layer 7_down @ epoch 4 new loss 2.751619649643544e-05 old loss 2.75165857601678e-05 BETTER
7_v proxy err 0.006730250082910061 tr(WHW.T) 309.4270935058594
bpp_loss 3.4459194038645364
7_q proxy err 0.0002520780835766345 tr(WHW.T) 35148.8984375
bpp_loss 4.369622903526761
7_k proxy err 9.01922830962576e-05 tr(WHW.T) 26840.486328125
bpp_loss 5.468953475705348
7_o proxy err 0.005945341195911169 tr(WHW.T) 962.465576171875
bpp_loss 3.5840898725437
7_up proxy err 0.004761877469718456 tr(WHW.T) 8625.5517578125
bpp_loss 3.7198599042104825
7_gate proxy err 0.0011996657121926546 tr(WHW.T) 34887.671875
bpp_loss 4.032237739634833
7_down proxy err 0.006187861785292625 tr(WHW.T) 6543.84619140625
bpp_loss 3.722848673833401
I0328 13:23:49.751998 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 64.20433354377747s
I0328 13:23:53.473828 2311835 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:23:53.473984 2311835 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:23:53.474050 2311835 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:23:53.805529 2311835 config.py:54] PyTorch version 2.6.0 available.
W0328 13:23:54.005571 2311835 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 13:23:54.581980 2311835 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:23:54.586014 2311068 quantize_finetune_llama.py:209] layer 10 gpu 2
I0328 13:23:54.602329 2311835 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:24:06.859172 2311765 finetune.py:68] layer 8_v @ epoch 1 new loss 3.921598363376688e-06 old loss 4.156686827627709e-06 BETTER
I0328 13:24:11.800149 2311835 finetune.py:45] layer 9_v initial loss 6.604003829124849e-06
W0328 13:24:11.800385 2311835 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:24:43.826010 2311765 finetune.py:68] layer 8_v @ epoch 2 new loss 3.803828576565138e-06 old loss 3.921598363376688e-06 BETTER
I0328 13:24:44.885723 2311835 finetune.py:68] layer 9_v @ epoch 0 new loss 4.2130122892558575e-06 old loss 6.604003829124849e-06 BETTER
I0328 13:24:59.136909 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 64.05969452857971s
I0328 13:25:02.881656 2311905 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:25:02.881780 2311905 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:25:02.881833 2311905 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:25:03.238111 2311905 config.py:54] PyTorch version 2.6.0 available.
W0328 13:25:03.449839 2311905 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 13:25:04.032733 2311905 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:25:04.036454 2311068 quantize_finetune_llama.py:209] layer 11 gpu 3
I0328 13:25:04.049395 2311905 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:25:19.396916 2311835 finetune.py:68] layer 9_v @ epoch 1 new loss 3.956412001571152e-06 old loss 4.2130122892558575e-06 BETTER
I0328 13:25:21.113324 2311765 finetune.py:68] layer 8_v @ epoch 3 new loss 3.724215503098094e-06 old loss 3.803828576565138e-06 BETTER
I0328 13:25:21.998414 2311905 finetune.py:45] layer 10_v initial loss 8.20654986455338e-06
W0328 13:25:21.998608 2311905 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:25:54.630092 2311835 finetune.py:68] layer 9_v @ epoch 2 new loss 3.831338290183339e-06 old loss 3.956412001571152e-06 BETTER
I0328 13:25:55.689169 2311905 finetune.py:68] layer 10_v @ epoch 0 new loss 5.717247404390946e-06 old loss 8.20654986455338e-06 BETTER
I0328 13:25:58.312491 2311765 finetune.py:68] layer 8_v @ epoch 4 new loss 3.672524599096505e-06 old loss 3.724215503098094e-06 BETTER
I0328 13:26:09.843348 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 65.33255171775818s
I0328 13:26:13.808153 2311975 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:26:13.808257 2311975 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:26:13.808298 2311975 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:26:14.225065 2311975 config.py:54] PyTorch version 2.6.0 available.
W0328 13:26:14.450120 2311975 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 13:26:15.138409 2311975 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:26:15.142222 2311068 quantize_finetune_llama.py:209] layer 12 gpu 0
I0328 13:26:15.155629 2311975 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 13:26:17.649808 2311765 finetune.py:45] layer 8_q initial loss 4.458259809325682e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:26:29.828530 2311835 finetune.py:68] layer 9_v @ epoch 3 new loss 3.7485012853721855e-06 old loss 3.831338290183339e-06 BETTER
I0328 13:26:30.567923 2311905 finetune.py:68] layer 10_v @ epoch 1 new loss 5.4256465773505624e-06 old loss 5.717247404390946e-06 BETTER
I0328 13:26:33.280931 2311975 finetune.py:45] layer 11_v initial loss 6.957541245355969e-06
W0328 13:26:33.281176 2311975 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:26:53.200622 2311765 finetune.py:68] layer 8_q @ epoch 0 new loss 4.361551418696763e-06 old loss 4.458259809325682e-06 BETTER
I0328 13:27:05.751892 2311835 finetune.py:68] layer 9_v @ epoch 4 new loss 3.693263352033682e-06 old loss 3.7485012853721855e-06 BETTER
I0328 13:27:06.059623 2311905 finetune.py:68] layer 10_v @ epoch 2 new loss 5.285795396048343e-06 old loss 5.4256465773505624e-06 BETTER
I0328 13:27:06.862845 2311975 finetune.py:68] layer 11_v @ epoch 0 new loss 5.055760084360372e-06 old loss 6.957541245355969e-06 BETTER
I0328 13:27:25.440263 2311835 finetune.py:45] layer 9_q initial loss 4.5416554712574e-06
I0328 13:27:29.973798 2311765 finetune.py:68] layer 8_q @ epoch 1 new loss 4.30318186772638e-06 old loss 4.361551418696763e-06 BETTER
I0328 13:27:41.184602 2311975 finetune.py:68] layer 11_v @ epoch 1 new loss 4.792736945091747e-06 old loss 5.055760084360372e-06 BETTER
I0328 13:27:41.410316 2311905 finetune.py:68] layer 10_v @ epoch 3 new loss 5.174365014681825e-06 old loss 5.285795396048343e-06 BETTER
I0328 13:27:59.072861 2311835 finetune.py:68] layer 9_q @ epoch 0 new loss 4.457053819351131e-06 old loss 4.5416554712574e-06 BETTER
I0328 13:28:06.988866 2311765 finetune.py:68] layer 8_q @ epoch 2 new loss 4.260267814970575e-06 old loss 4.30318186772638e-06 BETTER
I0328 13:28:15.717011 2311975 finetune.py:68] layer 11_v @ epoch 2 new loss 4.671718215831788e-06 old loss 4.792736945091747e-06 BETTER
I0328 13:28:16.897200 2311905 finetune.py:68] layer 10_v @ epoch 4 new loss 5.1001538849959616e-06 old loss 5.174365014681825e-06 BETTER
I0328 13:28:33.708112 2311835 finetune.py:68] layer 9_q @ epoch 1 new loss 4.398026248964015e-06 old loss 4.457053819351131e-06 BETTER
I0328 13:28:36.678543 2311905 finetune.py:45] layer 10_q initial loss 6.0931529333174694e-06
I0328 13:28:44.146877 2311765 finetune.py:68] layer 8_q @ epoch 3 new loss 4.215698481857544e-06 old loss 4.260267814970575e-06 BETTER
I0328 13:28:50.485056 2311975 finetune.py:68] layer 11_v @ epoch 3 new loss 4.584843281918438e-06 old loss 4.671718215831788e-06 BETTER
I0328 13:29:08.359254 2311835 finetune.py:68] layer 9_q @ epoch 2 new loss 4.349996288510738e-06 old loss 4.398026248964015e-06 BETTER
I0328 13:29:10.607064 2311905 finetune.py:68] layer 10_q @ epoch 0 new loss 5.980329660815187e-06 old loss 6.0931529333174694e-06 BETTER
I0328 13:29:21.335150 2311765 finetune.py:68] layer 8_q @ epoch 4 new loss 4.187993454252137e-06 old loss 4.215698481857544e-06 BETTER
I0328 13:29:25.224148 2311975 finetune.py:68] layer 11_v @ epoch 4 new loss 4.519873073149938e-06 old loss 4.584843281918438e-06 BETTER
I0328 13:29:39.240209 2311765 finetune.py:45] layer 8_k initial loss 4.40481016994454e-06
I0328 13:29:43.216731 2311835 finetune.py:68] layer 9_q @ epoch 3 new loss 4.317926595831523e-06 old loss 4.349996288510738e-06 BETTER
I0328 13:29:45.280164 2311975 finetune.py:45] layer 11_q initial loss 5.66996186535107e-06
I0328 13:29:45.576166 2311905 finetune.py:68] layer 10_q @ epoch 1 new loss 5.8995674407924525e-06 old loss 5.980329660815187e-06 BETTER
I0328 13:30:14.800151 2311765 finetune.py:68] layer 8_k @ epoch 0 new loss 4.341313797340263e-06 old loss 4.40481016994454e-06 BETTER
I0328 13:30:18.331731 2311835 finetune.py:68] layer 9_q @ epoch 4 new loss 4.282502686692169e-06 old loss 4.317926595831523e-06 BETTER
I0328 13:30:18.731436 2311975 finetune.py:68] layer 11_q @ epoch 0 new loss 5.588188741967315e-06 old loss 5.66996186535107e-06 BETTER
I0328 13:30:20.659868 2311905 finetune.py:68] layer 10_q @ epoch 2 new loss 5.82957363803871e-06 old loss 5.8995674407924525e-06 BETTER
I0328 13:30:36.561525 2311835 finetune.py:45] layer 9_k initial loss 4.542664555629017e-06
I0328 13:30:51.536051 2311765 finetune.py:68] layer 8_k @ epoch 1 new loss 4.313847512094071e-06 old loss 4.341313797340263e-06 BETTER
I0328 13:30:53.100331 2311975 finetune.py:68] layer 11_q @ epoch 1 new loss 5.494731340149883e-06 old loss 5.588188741967315e-06 BETTER
I0328 13:30:55.551403 2311905 finetune.py:68] layer 10_q @ epoch 3 new loss 5.780748324468732e-06 old loss 5.82957363803871e-06 BETTER
I0328 13:31:10.090788 2311835 finetune.py:68] layer 9_k @ epoch 0 new loss 4.487866135605145e-06 old loss 4.542664555629017e-06 BETTER
I0328 13:31:27.796026 2311975 finetune.py:68] layer 11_q @ epoch 2 new loss 5.431475074146874e-06 old loss 5.494731340149883e-06 BETTER
I0328 13:31:28.635079 2311765 finetune.py:68] layer 8_k @ epoch 2 new loss 4.2897258936136495e-06 old loss 4.313847512094071e-06 BETTER
I0328 13:31:30.611124 2311905 finetune.py:68] layer 10_q @ epoch 4 new loss 5.727864390792092e-06 old loss 5.780748324468732e-06 BETTER
I0328 13:31:44.839062 2311835 finetune.py:68] layer 9_k @ epoch 1 new loss 4.448067102202913e-06 old loss 4.487866135605145e-06 BETTER
I0328 13:31:49.008390 2311905 finetune.py:45] layer 10_k initial loss 5.958747806289466e-06
I0328 13:32:02.231647 2311975 finetune.py:68] layer 11_q @ epoch 3 new loss 5.38591939402977e-06 old loss 5.431475074146874e-06 BETTER
I0328 13:32:05.483008 2311765 finetune.py:68] layer 8_k @ epoch 3 new loss 4.266287305654259e-06 old loss 4.2897258936136495e-06 BETTER
I0328 13:32:19.480509 2311835 finetune.py:68] layer 9_k @ epoch 2 new loss 4.421962330525275e-06 old loss 4.448067102202913e-06 BETTER
I0328 13:32:22.738931 2311905 finetune.py:68] layer 10_k @ epoch 0 new loss 5.912675078434404e-06 old loss 5.958747806289466e-06 BETTER
I0328 13:32:36.609192 2311975 finetune.py:68] layer 11_q @ epoch 4 new loss 5.352311291062506e-06 old loss 5.38591939402977e-06 BETTER
I0328 13:32:42.457900 2311765 finetune.py:68] layer 8_k @ epoch 4 new loss 4.251439804647816e-06 old loss 4.266287305654259e-06 BETTER
I0328 13:32:54.086390 2311835 finetune.py:68] layer 9_k @ epoch 3 new loss 4.403937055030838e-06 old loss 4.421962330525275e-06 BETTER
I0328 13:32:54.477960 2311975 finetune.py:45] layer 11_k initial loss 5.600107215286698e-06
I0328 13:32:57.433621 2311905 finetune.py:68] layer 10_k @ epoch 1 new loss 5.873385816812515e-06 old loss 5.912675078434404e-06 BETTER
I0328 13:33:02.287503 2311765 finetune.py:45] layer 8_o initial loss 9.759473869053181e-06
I0328 13:33:27.627348 2311975 finetune.py:68] layer 11_k @ epoch 0 new loss 5.54437156097265e-06 old loss 5.600107215286698e-06 BETTER
I0328 13:33:28.819115 2311835 finetune.py:68] layer 9_k @ epoch 4 new loss 4.374126547190826e-06 old loss 4.403937055030838e-06 BETTER
I0328 13:33:32.157855 2311905 finetune.py:68] layer 10_k @ epoch 2 new loss 5.842671725986293e-06 old loss 5.873385816812515e-06 BETTER
I0328 13:33:36.939432 2311765 finetune.py:68] layer 8_o @ epoch 0 new loss 9.55174800765235e-06 old loss 9.759473869053181e-06 BETTER
I0328 13:33:48.501572 2311835 finetune.py:45] layer 9_o initial loss 1.0125238077307586e-05
I0328 13:34:01.762603 2311975 finetune.py:68] layer 11_k @ epoch 1 new loss 5.51357197764446e-06 old loss 5.54437156097265e-06 BETTER
I0328 13:34:06.869535 2311905 finetune.py:68] layer 10_k @ epoch 3 new loss 5.813003099319758e-06 old loss 5.842671725986293e-06 BETTER
I0328 13:34:13.114916 2311765 finetune.py:68] layer 8_o @ epoch 1 new loss 9.439411769562867e-06 old loss 9.55174800765235e-06 BETTER
I0328 13:34:21.384576 2311835 finetune.py:68] layer 9_o @ epoch 0 new loss 9.871228940028232e-06 old loss 1.0125238077307586e-05 BETTER
I0328 13:34:35.801543 2311975 finetune.py:68] layer 11_k @ epoch 2 new loss 5.501984105649171e-06 old loss 5.51357197764446e-06 BETTER
I0328 13:34:41.581248 2311905 finetune.py:68] layer 10_k @ epoch 4 new loss 5.789046099380357e-06 old loss 5.813003099319758e-06 BETTER
I0328 13:34:49.181706 2311765 finetune.py:68] layer 8_o @ epoch 2 new loss 9.351360859000124e-06 old loss 9.439411769562867e-06 BETTER
I0328 13:34:55.357306 2311835 finetune.py:68] layer 9_o @ epoch 1 new loss 9.729102202982176e-06 old loss 9.871228940028232e-06 BETTER
I0328 13:35:01.473295 2311905 finetune.py:45] layer 10_o initial loss 1.2869026249973103e-05
I0328 13:35:10.031923 2311975 finetune.py:68] layer 11_k @ epoch 3 new loss 5.4593751883658115e-06 old loss 5.501984105649171e-06 BETTER
I0328 13:35:25.193018 2311765 finetune.py:68] layer 8_o @ epoch 3 new loss 9.276622222387232e-06 old loss 9.351360859000124e-06 BETTER
I0328 13:35:29.616137 2311835 finetune.py:68] layer 9_o @ epoch 2 new loss 9.620469427318312e-06 old loss 9.729102202982176e-06 BETTER
I0328 13:35:34.473815 2311905 finetune.py:68] layer 10_o @ epoch 0 new loss 1.2480825716920663e-05 old loss 1.2869026249973103e-05 BETTER
I0328 13:35:44.070789 2311975 finetune.py:68] layer 11_k @ epoch 4 new loss 5.427242285804823e-06 old loss 5.4593751883658115e-06 BETTER
I0328 13:36:01.607566 2311765 finetune.py:68] layer 8_o @ epoch 4 new loss 9.210888492816593e-06 old loss 9.276622222387232e-06 BETTER
I0328 13:36:03.919562 2311835 finetune.py:68] layer 9_o @ epoch 3 new loss 9.531747309665661e-06 old loss 9.620469427318312e-06 BETTER
I0328 13:36:04.109965 2311975 finetune.py:45] layer 11_o initial loss 1.2774182323482819e-05
I0328 13:36:08.422170 2311905 finetune.py:68] layer 10_o @ epoch 1 new loss 1.2281218005227856e-05 old loss 1.2480825716920663e-05 BETTER
I0328 13:36:34.291440 2311765 finetune.py:45] layer 8_up initial loss 1.799668461899273e-05
I0328 13:36:36.584199 2311975 finetune.py:68] layer 11_o @ epoch 0 new loss 1.2428261470631696e-05 old loss 1.2774182323482819e-05 BETTER
I0328 13:36:38.127687 2311835 finetune.py:68] layer 9_o @ epoch 4 new loss 9.4552287919214e-06 old loss 9.531747309665661e-06 BETTER
I0328 13:36:42.464536 2311905 finetune.py:68] layer 10_o @ epoch 2 new loss 1.2130921277275775e-05 old loss 1.2281218005227856e-05 BETTER
I0328 13:37:06.517317 2311765 finetune.py:68] layer 8_up @ epoch 0 new loss 1.7795497115002945e-05 old loss 1.799668461899273e-05 BETTER
I0328 13:37:10.203663 2311835 finetune.py:45] layer 9_up initial loss 1.9061613784288056e-05
I0328 13:37:10.342467 2311975 finetune.py:68] layer 11_o @ epoch 1 new loss 1.2235381291247904e-05 old loss 1.2428261470631696e-05 BETTER
I0328 13:37:16.644114 2311905 finetune.py:68] layer 10_o @ epoch 3 new loss 1.2008705198240932e-05 old loss 1.2130921277275775e-05 BETTER
I0328 13:37:39.968548 2311765 finetune.py:68] layer 8_up @ epoch 1 new loss 1.7635960830375552e-05 old loss 1.7795497115002945e-05 BETTER
I0328 13:37:41.075682 2311835 finetune.py:68] layer 9_up @ epoch 0 new loss 1.882358264992945e-05 old loss 1.9061613784288056e-05 BETTER
I0328 13:37:43.711242 2311975 finetune.py:68] layer 11_o @ epoch 2 new loss 1.2088384210073855e-05 old loss 1.2235381291247904e-05 BETTER
I0328 13:37:50.827656 2311905 finetune.py:68] layer 10_o @ epoch 4 new loss 1.190291550301481e-05 old loss 1.2008705198240932e-05 BETTER
I0328 13:38:12.876473 2311835 finetune.py:68] layer 9_up @ epoch 1 new loss 1.8634764273883775e-05 old loss 1.882358264992945e-05 BETTER
I0328 13:38:13.755200 2311765 finetune.py:68] layer 8_up @ epoch 2 new loss 1.7494083294877782e-05 old loss 1.7635960830375552e-05 BETTER
I0328 13:38:17.101111 2311975 finetune.py:68] layer 11_o @ epoch 3 new loss 1.1967391401412897e-05 old loss 1.2088384210073855e-05 BETTER
I0328 13:38:22.825915 2311905 finetune.py:45] layer 10_up initial loss 2.1744350306107663e-05
I0328 13:38:44.753067 2311835 finetune.py:68] layer 9_up @ epoch 2 new loss 1.846991290221922e-05 old loss 1.8634764273883775e-05 BETTER
I0328 13:38:47.506346 2311765 finetune.py:68] layer 8_up @ epoch 3 new loss 1.736612648528535e-05 old loss 1.7494083294877782e-05 BETTER
I0328 13:38:50.377754 2311975 finetune.py:68] layer 11_o @ epoch 4 new loss 1.1864014595630579e-05 old loss 1.1967391401412897e-05 BETTER
I0328 13:38:53.503153 2311905 finetune.py:68] layer 10_up @ epoch 0 new loss 2.1483660020749085e-05 old loss 2.1744350306107663e-05 BETTER
I0328 13:39:16.796032 2311835 finetune.py:68] layer 9_up @ epoch 3 new loss 1.8319644368602894e-05 old loss 1.846991290221922e-05 BETTER
I0328 13:39:21.115887 2311765 finetune.py:68] layer 8_up @ epoch 4 new loss 1.7246597053599544e-05 old loss 1.736612648528535e-05 BETTER
I0328 13:39:22.491469 2311975 finetune.py:45] layer 11_up initial loss 2.229000165243633e-05
I0328 13:39:25.389983 2311905 finetune.py:68] layer 10_up @ epoch 1 new loss 2.127594780176878e-05 old loss 2.1483660020749085e-05 BETTER
I0328 13:39:48.916295 2311835 finetune.py:68] layer 9_up @ epoch 4 new loss 1.8183147403760813e-05 old loss 1.8319644368602894e-05 BETTER
I0328 13:39:52.795304 2311975 finetune.py:68] layer 11_up @ epoch 0 new loss 2.2003590856911615e-05 old loss 2.229000165243633e-05 BETTER
I0328 13:39:53.261435 2311765 finetune.py:45] layer 8_gate initial loss 2.0418283384060487e-05
I0328 13:39:57.597368 2311905 finetune.py:68] layer 10_up @ epoch 2 new loss 2.109652450599242e-05 old loss 2.127594780176878e-05 BETTER
I0328 13:40:21.044073 2311835 finetune.py:45] layer 9_gate initial loss 2.1598963940050453e-05
I0328 13:40:23.543127 2311765 finetune.py:68] layer 8_gate @ epoch 0 new loss 2.0300827600294724e-05 old loss 2.0418283384060487e-05 BETTER
I0328 13:40:24.318648 2311975 finetune.py:68] layer 11_up @ epoch 1 new loss 2.177513852075208e-05 old loss 2.2003590856911615e-05 BETTER
I0328 13:40:29.997401 2311905 finetune.py:68] layer 10_up @ epoch 3 new loss 2.0931309336447157e-05 old loss 2.109652450599242e-05 BETTER
I0328 13:40:49.563290 2311835 finetune.py:68] layer 9_gate @ epoch 0 new loss 2.1461271899170242e-05 old loss 2.1598963940050453e-05 BETTER
I0328 13:40:54.860752 2311765 finetune.py:68] layer 8_gate @ epoch 1 new loss 2.0197177946101874e-05 old loss 2.0300827600294724e-05 BETTER
I0328 13:40:55.772168 2311975 finetune.py:68] layer 11_up @ epoch 2 new loss 2.1576473955065012e-05 old loss 2.177513852075208e-05 BETTER
I0328 13:41:02.306326 2311905 finetune.py:68] layer 10_up @ epoch 4 new loss 2.078126090054866e-05 old loss 2.0931309336447157e-05 BETTER
I0328 13:41:19.021030 2311835 finetune.py:68] layer 9_gate @ epoch 1 new loss 2.134355054295156e-05 old loss 2.1461271899170242e-05 BETTER
I0328 13:41:26.340068 2311765 finetune.py:68] layer 8_gate @ epoch 2 new loss 2.0101007976336405e-05 old loss 2.0197177946101874e-05 BETTER
I0328 13:41:27.219244 2311975 finetune.py:68] layer 11_up @ epoch 3 new loss 2.1395657313405536e-05 old loss 2.1576473955065012e-05 BETTER
I0328 13:41:34.626089 2311905 finetune.py:45] layer 10_gate initial loss 2.4532633688068017e-05
I0328 13:41:48.590883 2311835 finetune.py:68] layer 9_gate @ epoch 2 new loss 2.1234809537418187e-05 old loss 2.134355054295156e-05 BETTER
I0328 13:41:57.911302 2311765 finetune.py:68] layer 8_gate @ epoch 3 new loss 2.001186294364743e-05 old loss 2.0101007976336405e-05 BETTER
I0328 13:41:58.711487 2311975 finetune.py:68] layer 11_up @ epoch 4 new loss 2.1230649508652277e-05 old loss 2.1395657313405536e-05 BETTER
I0328 13:42:03.190951 2311905 finetune.py:68] layer 10_gate @ epoch 0 new loss 2.43843187490711e-05 old loss 2.4532633688068017e-05 BETTER
I0328 13:42:18.368927 2311835 finetune.py:68] layer 9_gate @ epoch 3 new loss 2.1132040274096653e-05 old loss 2.1234809537418187e-05 BETTER
I0328 13:42:29.486614 2311765 finetune.py:68] layer 8_gate @ epoch 4 new loss 1.9926170352846384e-05 old loss 2.001186294364743e-05 BETTER
I0328 13:42:31.009436 2311975 finetune.py:45] layer 11_gate initial loss 2.5258221285184845e-05
I0328 13:42:32.951873 2311905 finetune.py:68] layer 10_gate @ epoch 1 new loss 2.4255672542494722e-05 old loss 2.43843187490711e-05 BETTER
I0328 13:42:48.173585 2311835 finetune.py:68] layer 9_gate @ epoch 4 new loss 2.1036605176050216e-05 old loss 2.1132040274096653e-05 BETTER
I0328 13:42:58.911690 2311975 finetune.py:68] layer 11_gate @ epoch 0 new loss 2.5098552214330994e-05 old loss 2.5258221285184845e-05 BETTER
I0328 13:43:02.779483 2311905 finetune.py:68] layer 10_gate @ epoch 2 new loss 2.4136403226293623e-05 old loss 2.4255672542494722e-05 BETTER
I0328 13:43:26.436407 2311765 finetune.py:45] layer 8_down initial loss 3.0655985028715804e-05
I0328 13:43:28.161249 2311975 finetune.py:68] layer 11_gate @ epoch 1 new loss 2.4958435460575856e-05 old loss 2.5098552214330994e-05 BETTER
I0328 13:43:32.555051 2311905 finetune.py:68] layer 10_gate @ epoch 3 new loss 2.4026750907069072e-05 old loss 2.4136403226293623e-05 BETTER
I0328 13:43:46.171418 2311835 finetune.py:45] layer 9_down initial loss 3.27463167195674e-05
I0328 13:43:53.965723 2311765 finetune.py:68] layer 8_down @ epoch 0 new loss 3.065416603931226e-05 old loss 3.0655985028715804e-05 BETTER
I0328 13:43:58.109256 2311975 finetune.py:68] layer 11_gate @ epoch 2 new loss 2.4830904294503853e-05 old loss 2.4958435460575856e-05 BETTER
I0328 13:44:02.609806 2311905 finetune.py:68] layer 10_gate @ epoch 4 new loss 2.3921727915876545e-05 old loss 2.4026750907069072e-05 BETTER
I0328 13:44:12.308957 2311835 finetune.py:68] layer 9_down @ epoch 0 new loss 3.274478149251081e-05 old loss 3.27463167195674e-05 BETTER
I0328 13:44:22.453379 2311765 finetune.py:68] layer 8_down @ epoch 1 new loss 3.0653030989924446e-05 old loss 3.065416603931226e-05 BETTER
I0328 13:44:27.511177 2311975 finetune.py:68] layer 11_gate @ epoch 3 new loss 2.470988874847535e-05 old loss 2.4830904294503853e-05 BETTER
I0328 13:44:39.575787 2311835 finetune.py:68] layer 9_down @ epoch 1 new loss 3.27438028762117e-05 old loss 3.274478149251081e-05 BETTER
I0328 13:44:51.462315 2311765 finetune.py:68] layer 8_down @ epoch 2 new loss 3.06522379105445e-05 old loss 3.0653030989924446e-05 BETTER
I0328 13:44:57.328856 2311975 finetune.py:68] layer 11_gate @ epoch 4 new loss 2.4597715309937485e-05 old loss 2.470988874847535e-05 BETTER
I0328 13:45:00.196917 2311905 finetune.py:45] layer 10_down initial loss 3.5959565138909966e-05
I0328 13:45:07.073500 2311835 finetune.py:68] layer 9_down @ epoch 2 new loss 3.274274422437884e-05 old loss 3.27438028762117e-05 BETTER
I0328 13:45:20.435747 2311765 finetune.py:68] layer 8_down @ epoch 3 new loss 3.065168857574463e-05 old loss 3.06522379105445e-05 BETTER
I0328 13:45:26.458750 2311905 finetune.py:68] layer 10_down @ epoch 0 new loss 3.595768066588789e-05 old loss 3.5959565138909966e-05 BETTER
I0328 13:45:34.807893 2311835 finetune.py:68] layer 9_down @ epoch 3 new loss 3.274218033766374e-05 old loss 3.274274422437884e-05 BETTER
I0328 13:45:49.534825 2311765 finetune.py:68] layer 8_down @ epoch 4 new loss 3.0651266570203006e-05 old loss 3.065168857574463e-05 BETTER
8_v proxy err 0.007890093140304089 tr(WHW.T) 257.7052307128906
bpp_loss 3.4683211958617903
8_q proxy err 0.0003269006556365639 tr(WHW.T) 26602.21875
bpp_loss 4.354554217716213
8_k proxy err 0.0001072230952559039 tr(WHW.T) 22514.248046875
bpp_loss 5.371841107145883
8_o proxy err 0.007447015028446913 tr(WHW.T) 747.7518310546875
bpp_loss 3.5958256295416504
8_up proxy err 0.0048870062455534935 tr(WHW.T) 8495.2470703125
bpp_loss 3.7153718012518118
8_gate proxy err 0.001137229846790433 tr(WHW.T) 37231.0234375
bpp_loss 4.037907732584115
8_down proxy err 0.006276730913668871 tr(WHW.T) 6497.8740234375
bpp_loss 3.7211413097102195
I0328 13:45:54.055355 2311905 finetune.py:68] layer 10_down @ epoch 1 new loss 3.595639282139018e-05 old loss 3.595768066588789e-05 BETTER
I0328 13:45:54.951672 2311975 finetune.py:45] layer 11_down initial loss 3.704824484884739e-05
I0328 13:46:02.636054 2311835 finetune.py:68] layer 9_down @ epoch 4 new loss 3.2741911127232015e-05 old loss 3.274218033766374e-05 BETTER
9_v proxy err 0.005864504259079695 tr(WHW.T) 351.4288024902344
bpp_loss 3.5703512911568396
9_q proxy err 0.0003417102852836251 tr(WHW.T) 25657.380859375
bpp_loss 4.367280971433502
9_k proxy err 0.00011674543929984793 tr(WHW.T) 20937.44140625
bpp_loss 5.393861882505007
9_o proxy err 0.006619826890528202 tr(WHW.T) 778.2363891601562
bpp_loss 3.6523098272737116
9_up proxy err 0.004623073153197765 tr(WHW.T) 8969.671875
bpp_loss 3.7251878272342895
9_gate proxy err 0.0010743810562416911 tr(WHW.T) 39395.5078125
bpp_loss 4.053428317553231
9_down proxy err 0.006216232664883137 tr(WHW.T) 6283.01708984375
bpp_loss 3.722787765852575
I0328 13:46:21.066673 2311975 finetune.py:68] layer 11_down @ epoch 0 new loss 3.704598930198699e-05 old loss 3.704824484884739e-05 BETTER
I0328 13:46:22.479007 2311905 finetune.py:68] layer 10_down @ epoch 2 new loss 3.595548332668841e-05 old loss 3.595639282139018e-05 BETTER
I0328 13:46:48.006146 2311975 finetune.py:68] layer 11_down @ epoch 1 new loss 3.7044555938337e-05 old loss 3.704598930198699e-05 BETTER
I0328 13:46:50.202251 2311905 finetune.py:68] layer 10_down @ epoch 3 new loss 3.59545556420926e-05 old loss 3.595548332668841e-05 BETTER
I0328 13:47:14.997742 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 67.2328794002533s
I0328 13:47:15.015631 2311975 finetune.py:68] layer 11_down @ epoch 2 new loss 3.704365008161403e-05 old loss 3.7044555938337e-05 BETTER
I0328 13:47:18.047614 2311905 finetune.py:68] layer 10_down @ epoch 4 new loss 3.595400630729273e-05 old loss 3.59545556420926e-05 BETTER
I0328 13:47:18.955466 2312045 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:47:18.955563 2312045 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:47:18.955604 2312045 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:47:19.341879 2312045 config.py:54] PyTorch version 2.6.0 available.
W0328 13:47:19.557270 2312045 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

10_v proxy err 0.007751310244202614 tr(WHW.T) 251.83889770507812
bpp_loss 3.4585876609780826
10_q proxy err 0.00035832164576277137 tr(WHW.T) 23344.578125
bpp_loss 4.3698999780463055
10_k proxy err 0.00012007572513539344 tr(WHW.T) 19735.16015625
bpp_loss 5.396097969962284
10_o proxy err 0.008107205852866173 tr(WHW.T) 682.7929077148438
bpp_loss 3.5882373247877695
10_up proxy err 0.004553766455501318 tr(WHW.T) 9196.470703125
bpp_loss 3.7418894287464872
10_gate proxy err 0.001139939296990633 tr(WHW.T) 37377.82421875
bpp_loss 4.020924937312624
10_down proxy err 0.006035543512552977 tr(WHW.T) 6534.431640625
bpp_loss 3.7395023504338627
W0328 13:47:20.132538 2312045 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:47:20.136345 2311068 quantize_finetune_llama.py:209] layer 13 gpu 1
I0328 13:47:20.152333 2312045 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:47:36.868361 2312045 finetune.py:45] layer 12_v initial loss 7.4750359999598e-06
W0328 13:47:36.868708 2312045 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:47:42.308334 2311975 finetune.py:68] layer 11_down @ epoch 3 new loss 3.7042984331492335e-05 old loss 3.704365008161403e-05 BETTER
I0328 13:48:09.643049 2311975 finetune.py:68] layer 11_down @ epoch 4 new loss 3.704238042701036e-05 old loss 3.7042984331492335e-05 BETTER
11_v proxy err 0.006386489607393742 tr(WHW.T) 319.5691223144531
bpp_loss 3.4631955435033888
11_q proxy err 0.0003910528321284801 tr(WHW.T) 22193.1796875
bpp_loss 4.303337338322308
11_k proxy err 0.00013411651889327914 tr(WHW.T) 18013.193359375
bpp_loss 5.394663315848447
11_o proxy err 0.00825820118188858 tr(WHW.T) 567.80224609375
bpp_loss 3.607899900758639
11_up proxy err 0.004456609021872282 tr(WHW.T) 9194.2294921875
bpp_loss 3.747036474025143
11_gate proxy err 0.0011303451610729098 tr(WHW.T) 36851.72265625
bpp_loss 4.000117759952055
11_down proxy err 0.00577553128823638 tr(WHW.T) 6666.533203125
bpp_loss 3.746995138569868
I0328 13:48:11.844861 2312045 finetune.py:68] layer 12_v @ epoch 0 new loss 5.602269993687514e-06 old loss 7.4750359999598e-06 BETTER
I0328 13:48:25.981211 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 63.59062385559082s
I0328 13:48:29.748154 2312115 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:48:29.748249 2312115 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:48:29.748286 2312115 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:48:30.106880 2312115 config.py:54] PyTorch version 2.6.0 available.
W0328 13:48:30.292702 2312115 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 13:48:30.876531 2312115 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:48:30.880441 2311068 quantize_finetune_llama.py:209] layer 14 gpu 2
I0328 13:48:30.894186 2312115 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:48:48.068390 2312115 finetune.py:45] layer 13_v initial loss 7.683724106755108e-06
W0328 13:48:48.068581 2312115 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:48:48.192064 2312045 finetune.py:68] layer 12_v @ epoch 1 new loss 5.321950993675273e-06 old loss 5.602269993687514e-06 BETTER
I0328 13:49:21.186360 2312115 finetune.py:68] layer 13_v @ epoch 0 new loss 5.924906872678548e-06 old loss 7.683724106755108e-06 BETTER
I0328 13:49:25.081651 2312045 finetune.py:68] layer 12_v @ epoch 2 new loss 5.146838702785317e-06 old loss 5.321950993675273e-06 BETTER
I0328 13:49:35.152294 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 63.788904905319214s
I0328 13:49:38.914102 2312185 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:49:38.914216 2312185 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:49:38.914259 2312185 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:49:39.269646 2312185 config.py:54] PyTorch version 2.6.0 available.
W0328 13:49:39.480071 2312185 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 13:49:40.090657 2312185 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:49:40.094564 2311068 quantize_finetune_llama.py:209] layer 15 gpu 3
I0328 13:49:40.108837 2312185 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:49:55.717538 2312115 finetune.py:68] layer 13_v @ epoch 1 new loss 5.6375397434749175e-06 old loss 5.924906872678548e-06 BETTER
I0328 13:49:57.759840 2312185 finetune.py:45] layer 14_v initial loss 8.38887171994429e-06
W0328 13:49:57.760205 2312185 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:50:01.970200 2312045 finetune.py:68] layer 12_v @ epoch 3 new loss 5.053647782915505e-06 old loss 5.146838702785317e-06 BETTER
I0328 13:50:30.582922 2312115 finetune.py:68] layer 13_v @ epoch 2 new loss 5.475547368405387e-06 old loss 5.6375397434749175e-06 BETTER
I0328 13:50:31.388953 2312185 finetune.py:68] layer 14_v @ epoch 0 new loss 6.529534857691033e-06 old loss 8.38887171994429e-06 BETTER
I0328 13:50:39.038426 2312045 finetune.py:68] layer 12_v @ epoch 4 new loss 5.024775873607723e-06 old loss 5.053647782915505e-06 BETTER
I0328 13:50:45.298458 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 64.70020127296448s
I0328 13:50:49.010159 2312255 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 13:50:49.010256 2312255 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 13:50:49.010294 2312255 utils.py:162] NumExpr defaulting to 16 threads.
I0328 13:50:49.389286 2312255 config.py:54] PyTorch version 2.6.0 available.
W0328 13:50:49.588780 2312255 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 13:50:50.253347 2312255 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 13:50:50.256865 2311068 quantize_finetune_llama.py:209] layer 16 gpu 0
I0328 13:50:50.269716 2312255 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 13:50:58.165623 2312045 finetune.py:45] layer 12_q initial loss 5.931286978011485e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 13:51:06.083518 2312115 finetune.py:68] layer 13_v @ epoch 3 new loss 5.363273885450326e-06 old loss 5.475547368405387e-06 BETTER
I0328 13:51:06.552129 2312185 finetune.py:68] layer 14_v @ epoch 1 new loss 6.22454535914585e-06 old loss 6.529534857691033e-06 BETTER
I0328 13:51:08.646310 2312255 finetune.py:45] layer 15_v initial loss 1.1247261682001408e-05
W0328 13:51:08.646570 2312255 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 13:51:33.349853 2312045 finetune.py:68] layer 12_q @ epoch 0 new loss 5.728825271944515e-06 old loss 5.931286978011485e-06 BETTER
I0328 13:51:41.761929 2312115 finetune.py:68] layer 13_v @ epoch 4 new loss 5.2921132009942085e-06 old loss 5.363273885450326e-06 BETTER
I0328 13:51:42.206023 2312185 finetune.py:68] layer 14_v @ epoch 2 new loss 6.0491342992463615e-06 old loss 6.22454535914585e-06 BETTER
I0328 13:51:42.306454 2312255 finetune.py:68] layer 15_v @ epoch 0 new loss 7.262317922140937e-06 old loss 1.1247261682001408e-05 BETTER
I0328 13:52:01.161550 2312115 finetune.py:45] layer 13_q initial loss 6.3651978052803315e-06
I0328 13:52:09.666066 2312045 finetune.py:68] layer 12_q @ epoch 1 new loss 5.6412764024571516e-06 old loss 5.728825271944515e-06 BETTER
I0328 13:52:16.581548 2312255 finetune.py:68] layer 15_v @ epoch 1 new loss 6.82760628478718e-06 old loss 7.262317922140937e-06 BETTER
I0328 13:52:17.344406 2312185 finetune.py:68] layer 14_v @ epoch 3 new loss 5.930621227889787e-06 old loss 6.0491342992463615e-06 BETTER
I0328 13:52:34.596698 2312115 finetune.py:68] layer 13_q @ epoch 0 new loss 6.248149929888314e-06 old loss 6.3651978052803315e-06 BETTER
I0328 13:52:46.121723 2312045 finetune.py:68] layer 12_q @ epoch 2 new loss 5.570417670242023e-06 old loss 5.6412764024571516e-06 BETTER
I0328 13:52:51.231432 2312255 finetune.py:68] layer 15_v @ epoch 2 new loss 6.5957078732026275e-06 old loss 6.82760628478718e-06 BETTER
I0328 13:52:52.779658 2312185 finetune.py:68] layer 14_v @ epoch 4 new loss 5.863602382305544e-06 old loss 5.930621227889787e-06 BETTER
I0328 13:53:09.012308 2312115 finetune.py:68] layer 13_q @ epoch 1 new loss 6.16437682765536e-06 old loss 6.248149929888314e-06 BETTER
I0328 13:53:12.467428 2312185 finetune.py:45] layer 14_q initial loss 7.329754225793295e-06
I0328 13:53:22.720485 2312045 finetune.py:68] layer 12_q @ epoch 3 new loss 5.489151590154506e-06 old loss 5.570417670242023e-06 BETTER
I0328 13:53:25.865206 2312255 finetune.py:68] layer 15_v @ epoch 3 new loss 6.446954103012104e-06 old loss 6.5957078732026275e-06 BETTER
I0328 13:53:43.574384 2312115 finetune.py:68] layer 13_q @ epoch 2 new loss 6.101326562202303e-06 old loss 6.16437682765536e-06 BETTER
I0328 13:53:46.217449 2312185 finetune.py:68] layer 14_q @ epoch 0 new loss 7.1505605774291325e-06 old loss 7.329754225793295e-06 BETTER
I0328 13:53:59.460772 2312045 finetune.py:68] layer 12_q @ epoch 4 new loss 5.437319032353116e-06 old loss 5.489151590154506e-06 BETTER
I0328 13:54:00.681683 2312255 finetune.py:68] layer 15_v @ epoch 4 new loss 6.337341346807079e-06 old loss 6.446954103012104e-06 BETTER
I0328 13:54:17.326654 2312045 finetune.py:45] layer 12_k initial loss 5.798812253487995e-06
I0328 13:54:18.479553 2312115 finetune.py:68] layer 13_q @ epoch 3 new loss 6.0475522332126275e-06 old loss 6.101326562202303e-06 BETTER
I0328 13:54:21.152298 2312185 finetune.py:68] layer 14_q @ epoch 1 new loss 7.03269461155287e-06 old loss 7.1505605774291325e-06 BETTER
I0328 13:54:21.362915 2312255 finetune.py:45] layer 15_q initial loss 7.216053290903801e-06
I0328 13:54:53.378908 2312045 finetune.py:68] layer 12_k @ epoch 0 new loss 5.6999465414264705e-06 old loss 5.798812253487995e-06 BETTER
I0328 13:54:53.829060 2312115 finetune.py:68] layer 13_q @ epoch 4 new loss 6.003612725180574e-06 old loss 6.0475522332126275e-06 BETTER
I0328 13:54:55.054957 2312255 finetune.py:68] layer 15_q @ epoch 0 new loss 7.082976026140386e-06 old loss 7.216053290903801e-06 BETTER
I0328 13:54:56.390663 2312185 finetune.py:68] layer 14_q @ epoch 2 new loss 6.962229235796258e-06 old loss 7.03269461155287e-06 BETTER
I0328 13:55:11.963145 2312115 finetune.py:45] layer 13_k initial loss 6.27965573585243e-06
I0328 13:55:29.698550 2312255 finetune.py:68] layer 15_q @ epoch 1 new loss 6.985452273511328e-06 old loss 7.082976026140386e-06 BETTER
I0328 13:55:30.120712 2312045 finetune.py:68] layer 12_k @ epoch 1 new loss 5.663324373017531e-06 old loss 5.6999465414264705e-06 BETTER
I0328 13:55:31.399147 2312185 finetune.py:68] layer 14_q @ epoch 3 new loss 6.8788644966844e-06 old loss 6.962229235796258e-06 BETTER
I0328 13:55:45.644810 2312115 finetune.py:68] layer 13_k @ epoch 0 new loss 6.23191181148286e-06 old loss 6.27965573585243e-06 BETTER
I0328 13:56:04.431218 2312255 finetune.py:68] layer 15_q @ epoch 2 new loss 6.904959263920318e-06 old loss 6.985452273511328e-06 BETTER
I0328 13:56:06.660699 2312185 finetune.py:68] layer 14_q @ epoch 4 new loss 6.817805115133524e-06 old loss 6.8788644966844e-06 BETTER
I0328 13:56:06.987304 2312045 finetune.py:68] layer 12_k @ epoch 2 new loss 5.617287115455838e-06 old loss 5.663324373017531e-06 BETTER
I0328 13:56:19.793498 2312115 finetune.py:68] layer 13_k @ epoch 1 new loss 6.193304670887301e-06 old loss 6.23191181148286e-06 BETTER
I0328 13:56:24.980095 2312185 finetune.py:45] layer 14_k initial loss 7.3107853495457675e-06
I0328 13:56:38.665589 2312255 finetune.py:68] layer 15_q @ epoch 3 new loss 6.842361017334042e-06 old loss 6.904959263920318e-06 BETTER
I0328 13:56:43.458499 2312045 finetune.py:68] layer 12_k @ epoch 3 new loss 5.578155651164707e-06 old loss 5.617287115455838e-06 BETTER
I0328 13:56:54.226467 2312115 finetune.py:68] layer 13_k @ epoch 2 new loss 6.167565970827127e-06 old loss 6.193304670887301e-06 BETTER
I0328 13:56:58.583592 2312185 finetune.py:68] layer 14_k @ epoch 0 new loss 7.219268809421919e-06 old loss 7.3107853495457675e-06 BETTER
I0328 13:57:13.011031 2312255 finetune.py:68] layer 15_q @ epoch 4 new loss 6.797250534873456e-06 old loss 6.842361017334042e-06 BETTER
I0328 13:57:20.045688 2312045 finetune.py:68] layer 12_k @ epoch 4 new loss 5.5475443332397845e-06 old loss 5.578155651164707e-06 BETTER
I0328 13:57:28.967472 2312115 finetune.py:68] layer 13_k @ epoch 3 new loss 6.139358902146341e-06 old loss 6.167565970827127e-06 BETTER
I0328 13:57:31.258800 2312255 finetune.py:45] layer 15_k initial loss 7.172941423050361e-06
I0328 13:57:33.046912 2312185 finetune.py:68] layer 14_k @ epoch 1 new loss 7.163579994085012e-06 old loss 7.219268809421919e-06 BETTER
I0328 13:57:40.256363 2312045 finetune.py:45] layer 12_o initial loss 1.2767229236487765e-05
I0328 13:58:03.576772 2312115 finetune.py:68] layer 13_k @ epoch 4 new loss 6.121768365119351e-06 old loss 6.139358902146341e-06 BETTER
I0328 13:58:04.320780 2312255 finetune.py:68] layer 15_k @ epoch 0 new loss 7.089936843840405e-06 old loss 7.172941423050361e-06 BETTER
I0328 13:58:07.605651 2312185 finetune.py:68] layer 14_k @ epoch 2 new loss 7.121044745872496e-06 old loss 7.163579994085012e-06 BETTER
I0328 13:58:14.704959 2312045 finetune.py:68] layer 12_o @ epoch 0 new loss 1.2408952898113057e-05 old loss 1.2767229236487765e-05 BETTER
I0328 13:58:23.605875 2312115 finetune.py:45] layer 13_o initial loss 1.5482331946259364e-05
I0328 13:58:38.586107 2312255 finetune.py:68] layer 15_k @ epoch 1 new loss 7.041706339805387e-06 old loss 7.089936843840405e-06 BETTER
I0328 13:58:42.188132 2312185 finetune.py:68] layer 14_k @ epoch 3 new loss 7.081587227730779e-06 old loss 7.121044745872496e-06 BETTER
I0328 13:58:50.421491 2312045 finetune.py:68] layer 12_o @ epoch 1 new loss 1.2201392564747948e-05 old loss 1.2408952898113057e-05 BETTER
I0328 13:58:56.266509 2312115 finetune.py:68] layer 13_o @ epoch 0 new loss 1.4994769117038231e-05 old loss 1.5482331946259364e-05 BETTER
I0328 13:59:12.840394 2312255 finetune.py:68] layer 15_k @ epoch 2 new loss 6.985276286286535e-06 old loss 7.041706339805387e-06 BETTER
I0328 13:59:16.792259 2312185 finetune.py:68] layer 14_k @ epoch 4 new loss 7.0595401666651014e-06 old loss 7.081587227730779e-06 BETTER
I0328 13:59:26.098980 2312045 finetune.py:68] layer 12_o @ epoch 2 new loss 1.2043328752042726e-05 old loss 1.2201392564747948e-05 BETTER
I0328 13:59:30.112930 2312115 finetune.py:68] layer 13_o @ epoch 1 new loss 1.4736966477357782e-05 old loss 1.4994769117038231e-05 BETTER
I0328 13:59:36.637469 2312185 finetune.py:45] layer 14_o initial loss 1.705530667095445e-05
I0328 13:59:47.024007 2312255 finetune.py:68] layer 15_k @ epoch 3 new loss 6.973848030611407e-06 old loss 6.985276286286535e-06 BETTER
I0328 14:00:02.474147 2312045 finetune.py:68] layer 12_o @ epoch 3 new loss 1.1910755347344093e-05 old loss 1.2043328752042726e-05 BETTER
I0328 14:00:03.918980 2312115 finetune.py:68] layer 13_o @ epoch 2 new loss 1.454013090551598e-05 old loss 1.4736966477357782e-05 BETTER
I0328 14:00:09.692311 2312185 finetune.py:68] layer 14_o @ epoch 0 new loss 1.649133446335327e-05 old loss 1.705530667095445e-05 BETTER
I0328 14:00:21.317597 2312255 finetune.py:68] layer 15_k @ epoch 4 new loss 6.929312348802341e-06 old loss 6.973848030611407e-06 BETTER
I0328 14:00:38.028736 2312115 finetune.py:68] layer 13_o @ epoch 3 new loss 1.4379917047335766e-05 old loss 1.454013090551598e-05 BETTER
I0328 14:00:38.766011 2312045 finetune.py:68] layer 12_o @ epoch 4 new loss 1.1795840691775084e-05 old loss 1.1910755347344093e-05 BETTER
I0328 14:00:41.447239 2312255 finetune.py:45] layer 15_o initial loss 1.701946894172579e-05
I0328 14:00:43.818980 2312185 finetune.py:68] layer 14_o @ epoch 1 new loss 1.6173698895727284e-05 old loss 1.649133446335327e-05 BETTER
I0328 14:01:10.938270 2312045 finetune.py:45] layer 12_up initial loss 2.2632933905697428e-05
I0328 14:01:12.243804 2312115 finetune.py:68] layer 13_o @ epoch 4 new loss 1.4243873010855168e-05 old loss 1.4379917047335766e-05 BETTER
I0328 14:01:13.920263 2312255 finetune.py:68] layer 15_o @ epoch 0 new loss 1.644489384489134e-05 old loss 1.701946894172579e-05 BETTER
I0328 14:01:17.941270 2312185 finetune.py:68] layer 14_o @ epoch 2 new loss 1.593170964042656e-05 old loss 1.6173698895727284e-05 BETTER
I0328 14:01:42.994452 2312045 finetune.py:68] layer 12_up @ epoch 0 new loss 2.2285767045104876e-05 old loss 2.2632933905697428e-05 BETTER
I0328 14:01:44.420734 2312115 finetune.py:45] layer 13_up initial loss 2.6623898520483635e-05
I0328 14:01:47.312488 2312255 finetune.py:68] layer 15_o @ epoch 1 new loss 1.6110399883473292e-05 old loss 1.644489384489134e-05 BETTER
I0328 14:01:52.110705 2312185 finetune.py:68] layer 14_o @ epoch 3 new loss 1.5736051864223555e-05 old loss 1.593170964042656e-05 BETTER
I0328 14:02:15.017895 2312115 finetune.py:68] layer 13_up @ epoch 0 new loss 2.6206020265817642e-05 old loss 2.6623898520483635e-05 BETTER
I0328 14:02:16.822786 2312045 finetune.py:68] layer 12_up @ epoch 1 new loss 2.2016089133103378e-05 old loss 2.2285767045104876e-05 BETTER
I0328 14:02:20.784893 2312255 finetune.py:68] layer 15_o @ epoch 2 new loss 1.5862566215218976e-05 old loss 1.6110399883473292e-05 BETTER
I0328 14:02:26.606654 2312185 finetune.py:68] layer 14_o @ epoch 4 new loss 1.5572757547488436e-05 old loss 1.5736051864223555e-05 BETTER
I0328 14:02:46.784359 2312115 finetune.py:68] layer 13_up @ epoch 1 new loss 2.5886365619953722e-05 old loss 2.6206020265817642e-05 BETTER
I0328 14:02:50.503549 2312045 finetune.py:68] layer 12_up @ epoch 2 new loss 2.1781794202979654e-05 old loss 2.2016089133103378e-05 BETTER
I0328 14:02:54.338363 2312255 finetune.py:68] layer 15_o @ epoch 3 new loss 1.566130231367424e-05 old loss 1.5862566215218976e-05 BETTER
I0328 14:02:58.854763 2312185 finetune.py:45] layer 14_up initial loss 3.052513056900352e-05
I0328 14:03:18.749664 2312115 finetune.py:68] layer 13_up @ epoch 2 new loss 2.561146902735345e-05 old loss 2.5886365619953722e-05 BETTER
I0328 14:03:24.487341 2312045 finetune.py:68] layer 12_up @ epoch 3 new loss 2.1574034690274857e-05 old loss 2.1781794202979654e-05 BETTER
I0328 14:03:27.863856 2312255 finetune.py:68] layer 15_o @ epoch 4 new loss 1.5491883459617384e-05 old loss 1.566130231367424e-05 BETTER
I0328 14:03:29.587855 2312185 finetune.py:68] layer 14_up @ epoch 0 new loss 2.9983229978824966e-05 old loss 3.052513056900352e-05 BETTER
I0328 14:03:50.750358 2312115 finetune.py:68] layer 13_up @ epoch 3 new loss 2.5366820409544744e-05 old loss 2.561146902735345e-05 BETTER
I0328 14:03:58.234719 2312045 finetune.py:68] layer 12_up @ epoch 4 new loss 2.1382929844548926e-05 old loss 2.1574034690274857e-05 BETTER
I0328 14:03:59.841935 2312255 finetune.py:45] layer 15_up initial loss 3.3348009310429916e-05
I0328 14:04:01.637324 2312185 finetune.py:68] layer 14_up @ epoch 1 new loss 2.9559330869233236e-05 old loss 2.9983229978824966e-05 BETTER
I0328 14:04:22.964647 2312115 finetune.py:68] layer 13_up @ epoch 4 new loss 2.5145613108179532e-05 old loss 2.5366820409544744e-05 BETTER
I0328 14:04:29.937411 2312255 finetune.py:68] layer 15_up @ epoch 0 new loss 3.263790495111607e-05 old loss 3.3348009310429916e-05 BETTER
I0328 14:04:30.360139 2312045 finetune.py:45] layer 12_gate initial loss 2.5974248273996636e-05
I0328 14:04:33.862976 2312185 finetune.py:68] layer 14_up @ epoch 2 new loss 2.9199800337664783e-05 old loss 2.9559330869233236e-05 BETTER
I0328 14:04:54.319588 2312115 finetune.py:45] layer 13_gate initial loss 3.0187457014108077e-05
I0328 14:05:00.461944 2312045 finetune.py:68] layer 12_gate @ epoch 0 new loss 2.578196654212661e-05 old loss 2.5974248273996636e-05 BETTER
I0328 14:05:01.232027 2312255 finetune.py:68] layer 15_up @ epoch 1 new loss 3.20990257023368e-05 old loss 3.263790495111607e-05 BETTER
I0328 14:05:06.067842 2312185 finetune.py:68] layer 14_up @ epoch 3 new loss 2.888400194933638e-05 old loss 2.9199800337664783e-05 BETTER
I0328 14:05:22.836653 2312115 finetune.py:68] layer 13_gate @ epoch 0 new loss 2.9958533559693024e-05 old loss 3.0187457014108077e-05 BETTER
I0328 14:05:31.852736 2312045 finetune.py:68] layer 12_gate @ epoch 1 new loss 2.5615494450903498e-05 old loss 2.578196654212661e-05 BETTER
I0328 14:05:32.606886 2312255 finetune.py:68] layer 15_up @ epoch 2 new loss 3.1645220587961376e-05 old loss 3.20990257023368e-05 BETTER
I0328 14:05:38.391629 2312185 finetune.py:68] layer 14_up @ epoch 4 new loss 2.8599299184861593e-05 old loss 2.888400194933638e-05 BETTER
I0328 14:05:52.244888 2312115 finetune.py:68] layer 13_gate @ epoch 1 new loss 2.9761244149995036e-05 old loss 2.9958533559693024e-05 BETTER
I0328 14:06:03.256310 2312045 finetune.py:68] layer 12_gate @ epoch 2 new loss 2.5464447389822453e-05 old loss 2.5615494450903498e-05 BETTER
I0328 14:06:03.953491 2312255 finetune.py:68] layer 15_up @ epoch 3 new loss 3.1247433071257547e-05 old loss 3.1645220587961376e-05 BETTER
I0328 14:06:10.376909 2312185 finetune.py:45] layer 14_gate initial loss 3.402978472877294e-05
I0328 14:06:22.014336 2312115 finetune.py:68] layer 13_gate @ epoch 2 new loss 2.9584252843051217e-05 old loss 2.9761244149995036e-05 BETTER
I0328 14:06:34.887962 2312045 finetune.py:68] layer 12_gate @ epoch 3 new loss 2.5323617592221126e-05 old loss 2.5464447389822453e-05 BETTER
I0328 14:06:35.448869 2312255 finetune.py:68] layer 15_up @ epoch 4 new loss 3.0889303161529824e-05 old loss 3.1247433071257547e-05 BETTER
I0328 14:06:38.994751 2312185 finetune.py:68] layer 14_gate @ epoch 0 new loss 3.3743144740583375e-05 old loss 3.402978472877294e-05 BETTER
I0328 14:06:51.754631 2312115 finetune.py:68] layer 13_gate @ epoch 3 new loss 2.941780439869035e-05 old loss 2.9584252843051217e-05 BETTER
I0328 14:07:06.525225 2312045 finetune.py:68] layer 12_gate @ epoch 4 new loss 2.519273584766779e-05 old loss 2.5323617592221126e-05 BETTER
I0328 14:07:07.509449 2312255 finetune.py:45] layer 15_gate initial loss 3.677368295029737e-05
I0328 14:07:08.860294 2312185 finetune.py:68] layer 14_gate @ epoch 1 new loss 3.349596227053553e-05 old loss 3.3743144740583375e-05 BETTER
I0328 14:07:21.638925 2312115 finetune.py:68] layer 13_gate @ epoch 4 new loss 2.9267121135490015e-05 old loss 2.941780439869035e-05 BETTER
I0328 14:07:35.782694 2312255 finetune.py:68] layer 15_gate @ epoch 0 new loss 3.641397051978856e-05 old loss 3.677368295029737e-05 BETTER
I0328 14:07:38.592846 2312185 finetune.py:68] layer 14_gate @ epoch 2 new loss 3.327645026729442e-05 old loss 3.349596227053553e-05 BETTER
I0328 14:08:02.662436 2312045 finetune.py:45] layer 12_down initial loss 3.818342156591825e-05
I0328 14:08:05.020788 2312255 finetune.py:68] layer 15_gate @ epoch 1 new loss 3.6098826967645437e-05 old loss 3.641397051978856e-05 BETTER
I0328 14:08:08.522667 2312185 finetune.py:68] layer 14_gate @ epoch 3 new loss 3.3072286896640435e-05 old loss 3.327645026729442e-05 BETTER
I0328 14:08:18.197299 2312115 finetune.py:45] layer 13_down initial loss 4.472164800972678e-05
I0328 14:08:30.128124 2312045 finetune.py:68] layer 12_down @ epoch 0 new loss 3.818179538939148e-05 old loss 3.818342156591825e-05 BETTER
I0328 14:08:34.415095 2312255 finetune.py:68] layer 15_gate @ epoch 2 new loss 3.581957571441308e-05 old loss 3.6098826967645437e-05 BETTER
I0328 14:08:38.431943 2312185 finetune.py:68] layer 14_gate @ epoch 4 new loss 3.2884639949770644e-05 old loss 3.3072286896640435e-05 BETTER
I0328 14:08:44.371771 2312115 finetune.py:68] layer 13_down @ epoch 0 new loss 4.4719687139149755e-05 old loss 4.472164800972678e-05 BETTER
I0328 14:08:58.858008 2312045 finetune.py:68] layer 12_down @ epoch 1 new loss 3.8180678529897705e-05 old loss 3.818179538939148e-05 BETTER
I0328 14:09:03.841125 2312255 finetune.py:68] layer 15_gate @ epoch 3 new loss 3.556581577868201e-05 old loss 3.581957571441308e-05 BETTER
I0328 14:09:11.786755 2312115 finetune.py:68] layer 13_down @ epoch 1 new loss 4.471825741347857e-05 old loss 4.4719687139149755e-05 BETTER
I0328 14:09:27.632376 2312045 finetune.py:68] layer 12_down @ epoch 2 new loss 3.8179940020199865e-05 old loss 3.8180678529897705e-05 BETTER
I0328 14:09:33.179302 2312255 finetune.py:68] layer 15_gate @ epoch 4 new loss 3.533408380462788e-05 old loss 3.556581577868201e-05 BETTER
I0328 14:09:36.291346 2312185 finetune.py:45] layer 14_down initial loss 5.14678395120427e-05
I0328 14:09:39.325516 2312115 finetune.py:68] layer 13_down @ epoch 2 new loss 4.471757711144164e-05 old loss 4.471825741347857e-05 BETTER
I0328 14:09:56.625592 2312045 finetune.py:68] layer 12_down @ epoch 3 new loss 3.817942342720926e-05 old loss 3.8179940020199865e-05 BETTER
I0328 14:10:02.618542 2312185 finetune.py:68] layer 14_down @ epoch 0 new loss 5.1465282012941316e-05 old loss 5.14678395120427e-05 BETTER
I0328 14:10:07.036860 2312115 finetune.py:68] layer 13_down @ epoch 3 new loss 4.471687498153187e-05 old loss 4.471757711144164e-05 BETTER
I0328 14:10:25.876353 2312045 finetune.py:68] layer 12_down @ epoch 4 new loss 3.817899414571002e-05 old loss 3.817942342720926e-05 BETTER
12_v proxy err 0.005903099197894335 tr(WHW.T) 363.6233825683594
bpp_loss 3.581486033042893
12_q proxy err 0.0002668457163963467 tr(WHW.T) 34111.62890625
bpp_loss 4.362888661737088
12_k proxy err 0.00010770455992314965 tr(WHW.T) 23047.11328125
bpp_loss 5.388676424277946
12_o proxy err 0.0066784825176000595 tr(WHW.T) 781.61376953125
bpp_loss 3.6537376945489086
12_up proxy err 0.0040011219680309296 tr(WHW.T) 10025.671875
bpp_loss 3.7673285844336664
12_gate proxy err 0.0010908443946391344 tr(WHW.T) 37302.77734375
bpp_loss 3.9815853965867842
12_down proxy err 0.0054469117894768715 tr(WHW.T) 6831.32275390625
bpp_loss 3.759996511324841
I0328 14:10:30.163845 2312185 finetune.py:68] layer 14_down @ epoch 1 new loss 5.146379044163041e-05 old loss 5.1465282012941316e-05 BETTER
I0328 14:10:30.945921 2312255 finetune.py:45] layer 15_down initial loss 5.883972335141152e-05
I0328 14:10:35.262517 2312115 finetune.py:68] layer 13_down @ epoch 4 new loss 4.4716336560668424e-05 old loss 4.471687498153187e-05 BETTER
13_v proxy err 0.007246026769280434 tr(WHW.T) 280.153564453125
bpp_loss 3.5216482132091187
13_q proxy err 0.0004133910406380892 tr(WHW.T) 20900.37890625
bpp_loss 4.3387437966885045
13_k proxy err 0.00013997456699144095 tr(WHW.T) 17791.443359375
bpp_loss 5.409336195210926
13_o proxy err 0.007081249728798866 tr(WHW.T) 676.2918701171875
bpp_loss 3.635697181278374
13_up proxy err 0.0039254482835531235 tr(WHW.T) 10008.4033203125
bpp_loss 3.770800693160189
13_gate proxy err 0.0010244905715808272 tr(WHW.T) 38906.62109375
bpp_loss 3.986161355261824
13_down proxy err 0.005565328523516655 tr(WHW.T) 6568.28173828125
bpp_loss 3.7595141803446626
I0328 14:10:57.354181 2312255 finetune.py:68] layer 15_down @ epoch 0 new loss 5.883737321710214e-05 old loss 5.883972335141152e-05 BETTER
I0328 14:10:58.607987 2312185 finetune.py:68] layer 14_down @ epoch 2 new loss 5.146264084032737e-05 old loss 5.146379044163041e-05 BETTER
I0328 14:11:24.458927 2312255 finetune.py:68] layer 15_down @ epoch 1 new loss 5.883595804334618e-05 old loss 5.883737321710214e-05 BETTER
I0328 14:11:26.347035 2312185 finetune.py:68] layer 14_down @ epoch 3 new loss 5.146184048498981e-05 old loss 5.146264084032737e-05 BETTER
I0328 14:11:40.211446 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 60.10369062423706s
I0328 14:11:43.992038 2312325 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 14:11:43.992138 2312325 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 14:11:43.992182 2312325 utils.py:162] NumExpr defaulting to 16 threads.
I0328 14:11:44.364145 2312325 config.py:54] PyTorch version 2.6.0 available.
W0328 14:11:44.582809 2312325 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 14:11:45.216846 2312325 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 14:11:45.221000 2311068 quantize_finetune_llama.py:209] layer 17 gpu 1
I0328 14:11:45.236155 2312325 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 14:11:51.784431 2312255 finetune.py:68] layer 15_down @ epoch 2 new loss 5.883499761694111e-05 old loss 5.883595804334618e-05 BETTER
I0328 14:11:54.313511 2312185 finetune.py:68] layer 14_down @ epoch 4 new loss 5.1460992835927755e-05 old loss 5.146184048498981e-05 BETTER
14_v proxy err 0.006829316262155771 tr(WHW.T) 281.3382873535156
bpp_loss 3.514235822716728
14_q proxy err 0.00039189381641335785 tr(WHW.T) 20884.5390625
bpp_loss 4.304601234733127
14_k proxy err 0.00012871302897110581 tr(WHW.T) 18606.984375
bpp_loss 5.353686486603692
14_o proxy err 0.007565133739262819 tr(WHW.T) 687.5159912109375
bpp_loss 3.627538678352721
14_up proxy err 0.004269642289727926 tr(WHW.T) 9163.24609375
bpp_loss 3.7651062703558376
14_gate proxy err 0.0009522183099761605 tr(WHW.T) 41800.34765625
bpp_loss 4.019794829588916
14_down proxy err 0.005882966332137585 tr(WHW.T) 6399.0673828125
bpp_loss 3.7552204317513054
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 14:12:02.654631 2312325 finetune.py:45] layer 16_v initial loss 1.2854561646236107e-05
W0328 14:12:02.654839 2312325 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 14:12:19.126356 2312255 finetune.py:68] layer 15_down @ epoch 3 new loss 5.883416815777309e-05 old loss 5.883499761694111e-05 BETTER
I0328 14:12:37.843175 2312325 finetune.py:68] layer 16_v @ epoch 0 new loss 8.005473318917211e-06 old loss 1.2854561646236107e-05 BETTER
I0328 14:12:46.431152 2312255 finetune.py:68] layer 15_down @ epoch 4 new loss 5.8833487855736166e-05 old loss 5.883416815777309e-05 BETTER
15_v proxy err 0.007494423538446426 tr(WHW.T) 284.0271301269531
bpp_loss 3.5779997899080627
15_q proxy err 0.0003273577312938869 tr(WHW.T) 28081.470703125
bpp_loss 4.452835511416197
15_k proxy err 0.00013808661606162786 tr(WHW.T) 18861.763671875
bpp_loss 5.40067341865506
15_o proxy err 0.0077073294669389725 tr(WHW.T) 829.0851440429688
bpp_loss 3.65806026768405
15_up proxy err 0.004350964445620775 tr(WHW.T) 8994.4833984375
bpp_loss 3.7570353081183776
15_gate proxy err 0.0008647884824313223 tr(WHW.T) 46147.5078125
bpp_loss 4.057661422860941
15_down proxy err 0.005908614955842495 tr(WHW.T) 6410.669921875
bpp_loss 3.749551248853095
I0328 14:13:04.041719 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 64.91348958015442s
I0328 14:13:07.767841 2312395 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 14:13:07.767959 2312395 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 14:13:07.768000 2312395 utils.py:162] NumExpr defaulting to 16 threads.
I0328 14:13:08.122538 2312395 config.py:54] PyTorch version 2.6.0 available.
W0328 14:13:08.334121 2312395 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 14:13:08.980998 2312395 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 14:13:08.984694 2311068 quantize_finetune_llama.py:209] layer 18 gpu 2
I0328 14:13:09.002080 2312395 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 14:13:14.476427 2312325 finetune.py:68] layer 16_v @ epoch 1 new loss 7.472426659660414e-06 old loss 8.005473318917211e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 14:13:26.289418 2312395 finetune.py:45] layer 17_v initial loss 1.3746182048635092e-05
W0328 14:13:26.289628 2312395 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 14:13:51.538795 2312325 finetune.py:68] layer 16_v @ epoch 2 new loss 7.182345143519342e-06 old loss 7.472426659660414e-06 BETTER
I0328 14:13:59.532949 2312395 finetune.py:68] layer 17_v @ epoch 0 new loss 7.367181297013303e-06 old loss 1.3746182048635092e-05 BETTER
I0328 14:14:14.098119 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 64.66399574279785s
I0328 14:14:17.967367 2312465 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 14:14:17.967478 2312465 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 14:14:17.967523 2312465 utils.py:162] NumExpr defaulting to 16 threads.
I0328 14:14:18.322867 2312465 config.py:54] PyTorch version 2.6.0 available.
W0328 14:14:18.540554 2312465 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 14:14:19.161817 2312465 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 14:14:19.165886 2311068 quantize_finetune_llama.py:209] layer 19 gpu 3
I0328 14:14:19.181453 2312465 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 14:14:28.501846 2312325 finetune.py:68] layer 16_v @ epoch 3 new loss 6.985829259065213e-06 old loss 7.182345143519342e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 14:14:34.044668 2312395 finetune.py:68] layer 17_v @ epoch 1 new loss 6.816746008553309e-06 old loss 7.367181297013303e-06 BETTER
I0328 14:14:36.894180 2312465 finetune.py:45] layer 18_v initial loss 1.7969945474760607e-05
W0328 14:14:36.894408 2312465 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 14:15:05.518222 2312325 finetune.py:68] layer 16_v @ epoch 4 new loss 6.84596125211101e-06 old loss 6.985829259065213e-06 BETTER
I0328 14:15:08.839291 2312395 finetune.py:68] layer 17_v @ epoch 2 new loss 6.5476442614453845e-06 old loss 6.816746008553309e-06 BETTER
I0328 14:15:10.371167 2312465 finetune.py:68] layer 18_v @ epoch 0 new loss 6.3071111071622e-06 old loss 1.7969945474760607e-05 BETTER
I0328 14:15:24.425997 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 64.77464962005615s
I0328 14:15:25.939433 2312325 finetune.py:45] layer 16_q initial loss 7.92458467913093e-06
I0328 14:15:28.418004 2312535 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 14:15:28.418121 2312535 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 14:15:28.418174 2312535 utils.py:162] NumExpr defaulting to 16 threads.
I0328 14:15:28.798418 2312535 config.py:54] PyTorch version 2.6.0 available.
W0328 14:15:29.015172 2312535 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 14:15:29.664536 2312535 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 14:15:29.668401 2311068 quantize_finetune_llama.py:209] layer 20 gpu 0
I0328 14:15:29.682272 2312535 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 14:15:43.738454 2312395 finetune.py:68] layer 17_v @ epoch 3 new loss 6.398543064278783e-06 old loss 6.5476442614453845e-06 BETTER
I0328 14:15:45.120513 2312465 finetune.py:68] layer 18_v @ epoch 1 new loss 5.707658601750154e-06 old loss 6.3071111071622e-06 BETTER
I0328 14:15:48.155236 2312535 finetune.py:45] layer 19_v initial loss 2.0266877982066944e-05
W0328 14:15:48.155433 2312535 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 14:16:01.229971 2312325 finetune.py:68] layer 16_q @ epoch 0 new loss 7.74098043621052e-06 old loss 7.92458467913093e-06 BETTER
I0328 14:16:19.015036 2312395 finetune.py:68] layer 17_v @ epoch 4 new loss 6.299444066826254e-06 old loss 6.398543064278783e-06 BETTER
I0328 14:16:20.446203 2312465 finetune.py:68] layer 18_v @ epoch 2 new loss 5.441108442028053e-06 old loss 5.707658601750154e-06 BETTER
I0328 14:16:21.246740 2312535 finetune.py:68] layer 19_v @ epoch 0 new loss 6.565280273207463e-06 old loss 2.0266877982066944e-05 BETTER
I0328 14:16:37.738674 2312325 finetune.py:68] layer 16_q @ epoch 1 new loss 7.5826787906407844e-06 old loss 7.74098043621052e-06 BETTER
I0328 14:16:39.278203 2312395 finetune.py:45] layer 17_q initial loss 7.224807632155716e-06
I0328 14:16:55.571022 2312535 finetune.py:68] layer 19_v @ epoch 1 new loss 5.899670213693753e-06 old loss 6.565280273207463e-06 BETTER
I0328 14:16:55.592205 2312465 finetune.py:68] layer 18_v @ epoch 3 new loss 5.28212103745318e-06 old loss 5.441108442028053e-06 BETTER
I0328 14:17:12.820706 2312395 finetune.py:68] layer 17_q @ epoch 0 new loss 7.011781235632952e-06 old loss 7.224807632155716e-06 BETTER
I0328 14:17:14.337534 2312325 finetune.py:68] layer 16_q @ epoch 2 new loss 7.470149739674525e-06 old loss 7.5826787906407844e-06 BETTER
I0328 14:17:30.060121 2312535 finetune.py:68] layer 19_v @ epoch 2 new loss 5.627793143503368e-06 old loss 5.899670213693753e-06 BETTER
I0328 14:17:30.776820 2312465 finetune.py:68] layer 18_v @ epoch 4 new loss 5.17188982485095e-06 old loss 5.28212103745318e-06 BETTER
I0328 14:17:47.167006 2312395 finetune.py:68] layer 17_q @ epoch 1 new loss 6.890474651299883e-06 old loss 7.011781235632952e-06 BETTER
I0328 14:17:50.591184 2312465 finetune.py:45] layer 18_q initial loss 6.0767829381802585e-06
I0328 14:17:51.301643 2312325 finetune.py:68] layer 16_q @ epoch 3 new loss 7.370971616182942e-06 old loss 7.470149739674525e-06 BETTER
I0328 14:18:04.995601 2312535 finetune.py:68] layer 19_v @ epoch 3 new loss 5.465163667395245e-06 old loss 5.627793143503368e-06 BETTER
I0328 14:18:21.840898 2312395 finetune.py:68] layer 17_q @ epoch 2 new loss 6.799819402658613e-06 old loss 6.890474651299883e-06 BETTER
I0328 14:18:24.285351 2312465 finetune.py:68] layer 18_q @ epoch 0 new loss 5.9515327848203015e-06 old loss 6.0767829381802585e-06 BETTER
I0328 14:18:28.283092 2312325 finetune.py:68] layer 16_q @ epoch 4 new loss 7.293797807506053e-06 old loss 7.370971616182942e-06 BETTER
I0328 14:18:39.702715 2312535 finetune.py:68] layer 19_v @ epoch 4 new loss 5.351193976821378e-06 old loss 5.465163667395245e-06 BETTER
I0328 14:18:46.143944 2312325 finetune.py:45] layer 16_k initial loss 7.681095667066984e-06
I0328 14:18:56.748445 2312395 finetune.py:68] layer 17_q @ epoch 3 new loss 6.723420028720284e-06 old loss 6.799819402658613e-06 BETTER
I0328 14:18:59.090692 2312465 finetune.py:68] layer 18_q @ epoch 1 new loss 5.83901783102192e-06 old loss 5.9515327848203015e-06 BETTER
I0328 14:19:00.008345 2312535 finetune.py:45] layer 19_q initial loss 6.142048277979484e-06
I0328 14:19:21.696360 2312325 finetune.py:68] layer 16_k @ epoch 0 new loss 7.5811203714692965e-06 old loss 7.681095667066984e-06 BETTER
I0328 14:19:31.533584 2312395 finetune.py:68] layer 17_q @ epoch 4 new loss 6.65597508486826e-06 old loss 6.723420028720284e-06 BETTER
I0328 14:19:33.205412 2312535 finetune.py:68] layer 19_q @ epoch 0 new loss 5.998857886879705e-06 old loss 6.142048277979484e-06 BETTER
I0328 14:19:33.968734 2312465 finetune.py:68] layer 18_q @ epoch 2 new loss 5.767798938904889e-06 old loss 5.83901783102192e-06 BETTER
I0328 14:19:49.734441 2312395 finetune.py:45] layer 17_k initial loss 7.0329288064385764e-06
I0328 14:19:58.032752 2312325 finetune.py:68] layer 16_k @ epoch 1 new loss 7.520097824453842e-06 old loss 7.5811203714692965e-06 BETTER
I0328 14:20:07.337153 2312535 finetune.py:68] layer 19_q @ epoch 1 new loss 5.89744240642176e-06 old loss 5.998857886879705e-06 BETTER
I0328 14:20:08.702794 2312465 finetune.py:68] layer 18_q @ epoch 3 new loss 5.698660061170813e-06 old loss 5.767798938904889e-06 BETTER
I0328 14:20:23.019186 2312395 finetune.py:68] layer 17_k @ epoch 0 new loss 6.962325187487295e-06 old loss 7.0329288064385764e-06 BETTER
I0328 14:20:34.433968 2312325 finetune.py:68] layer 16_k @ epoch 2 new loss 7.46298564990866e-06 old loss 7.520097824453842e-06 BETTER
I0328 14:20:41.525806 2312535 finetune.py:68] layer 19_q @ epoch 2 new loss 5.8273435570299625e-06 old loss 5.89744240642176e-06 BETTER
I0328 14:20:43.471985 2312465 finetune.py:68] layer 18_q @ epoch 4 new loss 5.6513176787120756e-06 old loss 5.698660061170813e-06 BETTER
I0328 14:20:57.193614 2312395 finetune.py:68] layer 17_k @ epoch 1 new loss 6.903521807544166e-06 old loss 6.962325187487295e-06 BETTER
I0328 14:21:01.381092 2312465 finetune.py:45] layer 18_k initial loss 5.949752903688932e-06
I0328 14:21:11.318487 2312325 finetune.py:68] layer 16_k @ epoch 3 new loss 7.426269348798087e-06 old loss 7.46298564990866e-06 BETTER
I0328 14:21:15.749402 2312535 finetune.py:68] layer 19_q @ epoch 3 new loss 5.76678894503857e-06 old loss 5.8273435570299625e-06 BETTER
I0328 14:21:31.656867 2312395 finetune.py:68] layer 17_k @ epoch 2 new loss 6.8630788518930785e-06 old loss 6.903521807544166e-06 BETTER
I0328 14:21:35.127999 2312465 finetune.py:68] layer 18_k @ epoch 0 new loss 5.885314294573618e-06 old loss 5.949752903688932e-06 BETTER
I0328 14:21:48.193651 2312325 finetune.py:68] layer 16_k @ epoch 4 new loss 7.371203537331894e-06 old loss 7.426269348798087e-06 BETTER
I0328 14:21:50.031087 2312535 finetune.py:68] layer 19_q @ epoch 4 new loss 5.733728812629124e-06 old loss 5.76678894503857e-06 BETTER
I0328 14:22:06.487904 2312395 finetune.py:68] layer 17_k @ epoch 3 new loss 6.823300282121636e-06 old loss 6.8630788518930785e-06 BETTER
I0328 14:22:08.673904 2312325 finetune.py:45] layer 16_o initial loss 1.7205749827553518e-05
I0328 14:22:08.761346 2312535 finetune.py:45] layer 19_k initial loss 6.080394086893648e-06
I0328 14:22:09.973235 2312465 finetune.py:68] layer 18_k @ epoch 1 new loss 5.843274266226217e-06 old loss 5.885314294573618e-06 BETTER
I0328 14:22:41.895481 2312395 finetune.py:68] layer 17_k @ epoch 4 new loss 6.799878065066878e-06 old loss 6.823300282121636e-06 BETTER
I0328 14:22:41.904691 2312535 finetune.py:68] layer 19_k @ epoch 0 new loss 6.0150928220537025e-06 old loss 6.080394086893648e-06 BETTER
I0328 14:22:43.416654 2312325 finetune.py:68] layer 16_o @ epoch 0 new loss 1.6660758774378337e-05 old loss 1.7205749827553518e-05 BETTER
I0328 14:22:44.651645 2312465 finetune.py:68] layer 18_k @ epoch 2 new loss 5.8090631682716776e-06 old loss 5.843274266226217e-06 BETTER
I0328 14:23:02.004462 2312395 finetune.py:45] layer 17_o initial loss 1.6328187484759837e-05
I0328 14:23:15.855875 2312535 finetune.py:68] layer 19_k @ epoch 1 new loss 5.9701415011659265e-06 old loss 6.0150928220537025e-06 BETTER
I0328 14:23:19.256751 2312325 finetune.py:68] layer 16_o @ epoch 1 new loss 1.6343705283361487e-05 old loss 1.6660758774378337e-05 BETTER
I0328 14:23:19.263776 2312465 finetune.py:68] layer 18_k @ epoch 3 new loss 5.777660589956213e-06 old loss 5.8090631682716776e-06 BETTER
I0328 14:23:34.787907 2312395 finetune.py:68] layer 17_o @ epoch 0 new loss 1.5775882275193e-05 old loss 1.6328187484759837e-05 BETTER
I0328 14:23:50.038897 2312535 finetune.py:68] layer 19_k @ epoch 2 new loss 5.9379945014370605e-06 old loss 5.9701415011659265e-06 BETTER
I0328 14:23:53.973967 2312465 finetune.py:68] layer 18_k @ epoch 4 new loss 5.744464488088852e-06 old loss 5.777660589956213e-06 BETTER
I0328 14:23:55.218105 2312325 finetune.py:68] layer 16_o @ epoch 2 new loss 1.6110918295453303e-05 old loss 1.6343705283361487e-05 BETTER
I0328 14:24:08.449145 2312395 finetune.py:68] layer 17_o @ epoch 1 new loss 1.546219937154092e-05 old loss 1.5775882275193e-05 BETTER
I0328 14:24:13.584155 2312465 finetune.py:45] layer 18_o initial loss 1.3453552128339652e-05
I0328 14:24:24.298038 2312535 finetune.py:68] layer 19_k @ epoch 3 new loss 5.908565526624443e-06 old loss 5.9379945014370605e-06 BETTER
I0328 14:24:31.281378 2312325 finetune.py:68] layer 16_o @ epoch 3 new loss 1.5917466953396797e-05 old loss 1.6110918295453303e-05 BETTER
I0328 14:24:42.469213 2312395 finetune.py:68] layer 17_o @ epoch 2 new loss 1.5230390999931842e-05 old loss 1.546219937154092e-05 BETTER
I0328 14:24:46.576083 2312465 finetune.py:68] layer 18_o @ epoch 0 new loss 1.2992017218493856e-05 old loss 1.3453552128339652e-05 BETTER
I0328 14:24:58.674696 2312535 finetune.py:68] layer 19_k @ epoch 4 new loss 5.883088306291029e-06 old loss 5.908565526624443e-06 BETTER
I0328 14:25:07.568825 2312325 finetune.py:68] layer 16_o @ epoch 4 new loss 1.575833448441699e-05 old loss 1.5917466953396797e-05 BETTER
I0328 14:25:16.648055 2312395 finetune.py:68] layer 17_o @ epoch 3 new loss 1.5047762644826435e-05 old loss 1.5230390999931842e-05 BETTER
I0328 14:25:18.391887 2312535 finetune.py:45] layer 19_o initial loss 1.2871757462562528e-05
I0328 14:25:20.568875 2312465 finetune.py:68] layer 18_o @ epoch 1 new loss 1.279168645851314e-05 old loss 1.2992017218493856e-05 BETTER
I0328 14:25:38.961420 2312325 finetune.py:45] layer 16_up initial loss 3.512955299811438e-05
I0328 14:25:51.022078 2312535 finetune.py:68] layer 19_o @ epoch 0 new loss 1.2471384252421558e-05 old loss 1.2871757462562528e-05 BETTER
I0328 14:25:51.026588 2312395 finetune.py:68] layer 17_o @ epoch 4 new loss 1.4889594240230508e-05 old loss 1.5047762644826435e-05 BETTER
I0328 14:25:54.612035 2312465 finetune.py:68] layer 18_o @ epoch 2 new loss 1.265201444766717e-05 old loss 1.279168645851314e-05 BETTER
I0328 14:26:10.809302 2312325 finetune.py:68] layer 16_up @ epoch 0 new loss 3.442202432779595e-05 old loss 3.512955299811438e-05 BETTER
I0328 14:26:22.841753 2312395 finetune.py:45] layer 17_up initial loss 3.687424032250419e-05
I0328 14:26:24.255857 2312535 finetune.py:68] layer 19_o @ epoch 1 new loss 1.2287110621400643e-05 old loss 1.2471384252421558e-05 BETTER
I0328 14:26:28.764432 2312465 finetune.py:68] layer 18_o @ epoch 3 new loss 1.2535553651105147e-05 old loss 1.265201444766717e-05 BETTER
I0328 14:26:44.218726 2312325 finetune.py:68] layer 16_up @ epoch 1 new loss 3.388883123989217e-05 old loss 3.442202432779595e-05 BETTER
I0328 14:26:53.486068 2312395 finetune.py:68] layer 17_up @ epoch 0 new loss 3.608643964980729e-05 old loss 3.687424032250419e-05 BETTER
I0328 14:26:57.909137 2312535 finetune.py:68] layer 19_o @ epoch 2 new loss 1.2163556675659493e-05 old loss 1.2287110621400643e-05 BETTER
I0328 14:27:03.016171 2312465 finetune.py:68] layer 18_o @ epoch 4 new loss 1.2444485037121922e-05 old loss 1.2535553651105147e-05 BETTER
I0328 14:27:17.849365 2312325 finetune.py:68] layer 16_up @ epoch 2 new loss 3.343493153806776e-05 old loss 3.388883123989217e-05 BETTER
I0328 14:27:25.070344 2312395 finetune.py:68] layer 17_up @ epoch 1 new loss 3.548803215380758e-05 old loss 3.608643964980729e-05 BETTER
I0328 14:27:31.399562 2312535 finetune.py:68] layer 19_o @ epoch 3 new loss 1.2061737834301312e-05 old loss 1.2163556675659493e-05 BETTER
I0328 14:27:34.859331 2312465 finetune.py:45] layer 18_up initial loss 3.519883466651663e-05
I0328 14:27:51.483035 2312325 finetune.py:68] layer 16_up @ epoch 3 new loss 3.303933044662699e-05 old loss 3.343493153806776e-05 BETTER
I0328 14:27:56.898808 2312395 finetune.py:68] layer 17_up @ epoch 2 new loss 3.499034573906101e-05 old loss 3.548803215380758e-05 BETTER
I0328 14:28:04.969663 2312535 finetune.py:68] layer 19_o @ epoch 4 new loss 1.1980922863585874e-05 old loss 1.2061737834301312e-05 BETTER
I0328 14:28:05.674910 2312465 finetune.py:68] layer 18_up @ epoch 0 new loss 3.4487031371099874e-05 old loss 3.519883466651663e-05 BETTER
I0328 14:28:25.512640 2312325 finetune.py:68] layer 16_up @ epoch 4 new loss 3.268475120421499e-05 old loss 3.303933044662699e-05 BETTER
I0328 14:28:29.295475 2312395 finetune.py:68] layer 17_up @ epoch 3 new loss 3.4557720937300473e-05 old loss 3.499034573906101e-05 BETTER
I0328 14:28:37.574128 2312535 finetune.py:45] layer 19_up initial loss 3.672293678391725e-05
I0328 14:28:37.762356 2312465 finetune.py:68] layer 18_up @ epoch 1 new loss 3.396254396648146e-05 old loss 3.4487031371099874e-05 BETTER
I0328 14:28:57.877175 2312325 finetune.py:45] layer 16_gate initial loss 3.901735908584669e-05
I0328 14:29:01.646218 2312395 finetune.py:68] layer 17_up @ epoch 4 new loss 3.417587504372932e-05 old loss 3.4557720937300473e-05 BETTER
I0328 14:29:07.607138 2312535 finetune.py:68] layer 19_up @ epoch 0 new loss 3.599971387302503e-05 old loss 3.672293678391725e-05 BETTER
I0328 14:29:09.948342 2312465 finetune.py:68] layer 18_up @ epoch 2 new loss 3.3531156077515334e-05 old loss 3.396254396648146e-05 BETTER
I0328 14:29:27.686043 2312325 finetune.py:68] layer 16_gate @ epoch 0 new loss 3.8660797144984826e-05 old loss 3.901735908584669e-05 BETTER
I0328 14:29:34.544506 2312395 finetune.py:45] layer 17_gate initial loss 4.1438150219619274e-05
I0328 14:29:38.919413 2312535 finetune.py:68] layer 19_up @ epoch 1 new loss 3.5466109693516046e-05 old loss 3.599971387302503e-05 BETTER
I0328 14:29:42.355165 2312465 finetune.py:68] layer 18_up @ epoch 3 new loss 3.315078720333986e-05 old loss 3.3531156077515334e-05 BETTER
I0328 14:29:58.865777 2312325 finetune.py:68] layer 16_gate @ epoch 1 new loss 3.835725146927871e-05 old loss 3.8660797144984826e-05 BETTER
I0328 14:30:03.018420 2312395 finetune.py:68] layer 17_gate @ epoch 0 new loss 4.1037335904547945e-05 old loss 4.1438150219619274e-05 BETTER
I0328 14:30:10.264463 2312535 finetune.py:68] layer 19_up @ epoch 2 new loss 3.502164690871723e-05 old loss 3.5466109693516046e-05 BETTER
I0328 14:30:14.668561 2312465 finetune.py:68] layer 18_up @ epoch 4 new loss 3.2819742045830935e-05 old loss 3.315078720333986e-05 BETTER
I0328 14:30:30.319185 2312325 finetune.py:68] layer 16_gate @ epoch 2 new loss 3.808466863119975e-05 old loss 3.835725146927871e-05 BETTER
I0328 14:30:32.318414 2312395 finetune.py:68] layer 17_gate @ epoch 1 new loss 4.0706472645979375e-05 old loss 4.1037335904547945e-05 BETTER
I0328 14:30:41.827339 2312535 finetune.py:68] layer 19_up @ epoch 3 new loss 3.4642092941794544e-05 old loss 3.502164690871723e-05 BETTER
I0328 14:30:47.753501 2312465 finetune.py:45] layer 18_gate initial loss 4.0684044506633654e-05
I0328 14:31:01.939969 2312325 finetune.py:68] layer 16_gate @ epoch 3 new loss 3.783812280744314e-05 old loss 3.808466863119975e-05 BETTER
I0328 14:31:02.082131 2312395 finetune.py:68] layer 17_gate @ epoch 2 new loss 4.0411461668554693e-05 old loss 4.0706472645979375e-05 BETTER
I0328 14:31:13.488812 2312535 finetune.py:68] layer 19_up @ epoch 4 new loss 3.430786819080822e-05 old loss 3.4642092941794544e-05 BETTER
I0328 14:31:16.379774 2312465 finetune.py:68] layer 18_gate @ epoch 0 new loss 4.033631557831541e-05 old loss 4.0684044506633654e-05 BETTER
I0328 14:31:32.013423 2312395 finetune.py:68] layer 17_gate @ epoch 3 new loss 4.0143881051335484e-05 old loss 4.0411461668554693e-05 BETTER
I0328 14:31:33.471067 2312325 finetune.py:68] layer 16_gate @ epoch 4 new loss 3.761430343729444e-05 old loss 3.783812280744314e-05 BETTER
I0328 14:31:45.829927 2312465 finetune.py:68] layer 18_gate @ epoch 1 new loss 4.0049486415227875e-05 old loss 4.033631557831541e-05 BETTER
I0328 14:31:46.253016 2312535 finetune.py:45] layer 19_gate initial loss 4.2952702642651275e-05
I0328 14:32:01.982187 2312395 finetune.py:68] layer 17_gate @ epoch 4 new loss 3.990364712080918e-05 old loss 4.0143881051335484e-05 BETTER
I0328 14:32:14.327088 2312535 finetune.py:68] layer 19_gate @ epoch 0 new loss 4.260745117790066e-05 old loss 4.2952702642651275e-05 BETTER
I0328 14:32:15.746835 2312465 finetune.py:68] layer 18_gate @ epoch 2 new loss 3.97942858398892e-05 old loss 4.0049486415227875e-05 BETTER
I0328 14:32:30.150965 2312325 finetune.py:45] layer 16_down initial loss 6.37275370536372e-05
I0328 14:32:43.835689 2312535 finetune.py:68] layer 19_gate @ epoch 1 new loss 4.23200435761828e-05 old loss 4.260745117790066e-05 BETTER
I0328 14:32:45.767977 2312465 finetune.py:68] layer 18_gate @ epoch 3 new loss 3.956737418775447e-05 old loss 3.97942858398892e-05 BETTER
I0328 14:32:57.431621 2312325 finetune.py:68] layer 16_down @ epoch 0 new loss 6.372562347678468e-05 old loss 6.37275370536372e-05 BETTER
I0328 14:32:59.738704 2312395 finetune.py:45] layer 17_down initial loss 7.159083907026798e-05
I0328 14:33:13.291609 2312535 finetune.py:68] layer 19_gate @ epoch 2 new loss 4.206161975162104e-05 old loss 4.23200435761828e-05 BETTER
I0328 14:33:15.811614 2312465 finetune.py:68] layer 18_gate @ epoch 4 new loss 3.935924905817956e-05 old loss 3.956737418775447e-05 BETTER
I0328 14:33:25.919384 2312325 finetune.py:68] layer 16_down @ epoch 1 new loss 6.372433563228697e-05 old loss 6.372562347678468e-05 BETTER
I0328 14:33:25.932036 2312395 finetune.py:68] layer 17_down @ epoch 0 new loss 7.158877997426316e-05 old loss 7.159083907026798e-05 BETTER
I0328 14:33:42.784928 2312535 finetune.py:68] layer 19_gate @ epoch 3 new loss 4.183485361863859e-05 old loss 4.206161975162104e-05 BETTER
I0328 14:33:53.246985 2312395 finetune.py:68] layer 17_down @ epoch 1 new loss 7.158723019529134e-05 old loss 7.158877997426316e-05 BETTER
I0328 14:33:54.817325 2312325 finetune.py:68] layer 16_down @ epoch 2 new loss 6.372325879056007e-05 old loss 6.372433563228697e-05 BETTER
I0328 14:34:12.220956 2312535 finetune.py:68] layer 19_gate @ epoch 4 new loss 4.162901677773334e-05 old loss 4.183485361863859e-05 BETTER
I0328 14:34:13.449449 2312465 finetune.py:45] layer 18_down initial loss 7.15937785571441e-05
I0328 14:34:20.878946 2312395 finetune.py:68] layer 17_down @ epoch 2 new loss 7.158616062952206e-05 old loss 7.158723019529134e-05 BETTER
I0328 14:34:23.913608 2312325 finetune.py:68] layer 16_down @ epoch 3 new loss 6.3722298364155e-05 old loss 6.372325879056007e-05 BETTER
I0328 14:34:39.811094 2312465 finetune.py:68] layer 18_down @ epoch 0 new loss 7.159232336562127e-05 old loss 7.15937785571441e-05 BETTER
I0328 14:34:48.666464 2312395 finetune.py:68] layer 17_down @ epoch 3 new loss 7.158533844631165e-05 old loss 7.158616062952206e-05 BETTER
I0328 14:34:53.169731 2312325 finetune.py:68] layer 16_down @ epoch 4 new loss 6.372154166456312e-05 old loss 6.3722298364155e-05 BETTER
16_v proxy err 0.0070669506676495075 tr(WHW.T) 274.28167724609375
bpp_loss 3.534696653485298
16_q proxy err 0.0003420081629883498 tr(WHW.T) 24485.736328125
bpp_loss 4.423605756834149
16_k proxy err 0.00012606150994542986 tr(WHW.T) 19492.232421875
bpp_loss 5.392019805731252
16_o proxy err 0.006386586930602789 tr(WHW.T) 970.340576171875
bpp_loss 3.6388467339565977
16_up proxy err 0.0049171666614711285 tr(WHW.T) 8332.3310546875
bpp_loss 3.7436676554914032
16_gate proxy err 0.0010178724769502878 tr(WHW.T) 41152.4921875
bpp_loss 4.091999181878886
16_down proxy err 0.006190522573888302 tr(WHW.T) 6299.3232421875
bpp_loss 3.7350271795044785
I0328 14:35:07.835358 2312465 finetune.py:68] layer 18_down @ epoch 1 new loss 7.159132655942813e-05 old loss 7.159232336562127e-05 BETTER
I0328 14:35:09.781089 2312535 finetune.py:45] layer 19_down initial loss 7.561127858934924e-05
I0328 14:35:16.656075 2312395 finetune.py:68] layer 17_down @ epoch 4 new loss 7.158458174671978e-05 old loss 7.158533844631165e-05 BETTER
17_v proxy err 0.007643864955753088 tr(WHW.T) 283.9730224609375
bpp_loss 3.604199012974277
17_q proxy err 0.00033901503775268793 tr(WHW.T) 27574.369140625
bpp_loss 4.4407372993882746
17_k proxy err 0.00015916768461465836 tr(WHW.T) 17419.130859375
bpp_loss 5.4222289067693055
17_o proxy err 0.00698094954714179 tr(WHW.T) 1107.483642578125
bpp_loss 3.668123120733071
17_up proxy err 0.00485624372959137 tr(WHW.T) 8453.23828125
bpp_loss 3.741108368217413
17_gate proxy err 0.0010079223429784179 tr(WHW.T) 41685.9140625
bpp_loss 4.106589984680925
17_down proxy err 0.006281246431171894 tr(WHW.T) 6222.36767578125
bpp_loss 3.731480742721552
I0328 14:35:35.664013 2312535 finetune.py:68] layer 19_down @ epoch 0 new loss 7.560962694697082e-05 old loss 7.561127858934924e-05 BETTER
I0328 14:35:35.697291 2312465 finetune.py:68] layer 18_down @ epoch 2 new loss 7.15904898243025e-05 old loss 7.159132655942813e-05 BETTER
I0328 14:36:02.797406 2312535 finetune.py:68] layer 19_down @ epoch 1 new loss 7.560852100141346e-05 old loss 7.560962694697082e-05 BETTER
I0328 14:36:03.565232 2312465 finetune.py:68] layer 18_down @ epoch 3 new loss 7.158984954003245e-05 old loss 7.15904898243025e-05 BETTER
I0328 14:36:28.443150 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 66.59483098983765s
I0328 14:36:29.912852 2312535 finetune.py:68] layer 19_down @ epoch 2 new loss 7.560769154224545e-05 old loss 7.560852100141346e-05 BETTER
I0328 14:36:31.414874 2312465 finetune.py:68] layer 18_down @ epoch 4 new loss 7.158928201533854e-05 old loss 7.158984954003245e-05 BETTER
I0328 14:36:32.344569 2312605 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 14:36:32.344668 2312605 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 14:36:32.344707 2312605 utils.py:162] NumExpr defaulting to 16 threads.
I0328 14:36:32.717203 2312605 config.py:54] PyTorch version 2.6.0 available.
W0328 14:36:32.929890 2312605 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

18_v proxy err 0.007729849312454462 tr(WHW.T) 287.61376953125
bpp_loss 3.529033595463261
18_q proxy err 0.00042770395521074533 tr(WHW.T) 22404.212890625
bpp_loss 4.434973236289807
18_k proxy err 0.00016271979256998748 tr(WHW.T) 17386.9140625
bpp_loss 5.506467443308793
18_o proxy err 0.006886246148496866 tr(WHW.T) 1205.0513916015625
bpp_loss 3.6468004601774737
18_up proxy err 0.005351200234144926 tr(WHW.T) 7983.7392578125
bpp_loss 3.7370035622401963
18_gate proxy err 0.0012417921097949147 tr(WHW.T) 35218.9296875
bpp_loss 4.113344752628889
18_down proxy err 0.006470395717769861 tr(WHW.T) 6238.6025390625
bpp_loss 3.7302423979687904
W0328 14:36:33.580397 2312605 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 14:36:33.585532 2311068 quantize_finetune_llama.py:209] layer 21 gpu 1
I0328 14:36:33.600124 2312605 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 14:36:50.593933 2312605 finetune.py:45] layer 20_v initial loss 1.9881696061929688e-05
W0328 14:36:50.594179 2312605 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 14:36:57.227157 2312535 finetune.py:68] layer 19_down @ epoch 3 new loss 7.560691301478073e-05 old loss 7.560769154224545e-05 BETTER
I0328 14:37:24.701137 2312535 finetune.py:68] layer 19_down @ epoch 4 new loss 7.560619269497693e-05 old loss 7.560691301478073e-05 BETTER
I0328 14:37:25.659320 2312605 finetune.py:68] layer 20_v @ epoch 0 new loss 6.898633273522137e-06 old loss 1.9881696061929688e-05 BETTER
19_v proxy err 0.006743720732629299 tr(WHW.T) 341.0596618652344
bpp_loss 3.574704827507958
19_q proxy err 0.0004112487658858299 tr(WHW.T) 24040.142578125
bpp_loss 4.437616804265417
19_k proxy err 0.00018193582945968956 tr(WHW.T) 15545.921875
bpp_loss 5.4009477163199335
19_o proxy err 0.007112826686352491 tr(WHW.T) 1170.017822265625
bpp_loss 3.6599728751461953
19_up proxy err 0.005675911903381348 tr(WHW.T) 7651.47509765625
bpp_loss 3.733337267501546
19_gate proxy err 0.001356181688606739 tr(WHW.T) 32798.56640625
bpp_loss 4.123829720541835
19_down proxy err 0.006552135571837425 tr(WHW.T) 6194.96923828125
bpp_loss 3.7285369115249654
I0328 14:37:40.811477 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 64.93023180961609s
I0328 14:37:44.410655 2312675 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 14:37:44.410761 2312675 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 14:37:44.410805 2312675 utils.py:162] NumExpr defaulting to 16 threads.
I0328 14:37:44.756502 2312675 config.py:54] PyTorch version 2.6.0 available.
W0328 14:37:44.970035 2312675 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 14:37:45.785512 2312675 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 14:37:45.789488 2311068 quantize_finetune_llama.py:209] layer 22 gpu 2
I0328 14:37:45.803494 2312675 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 14:38:02.201654 2312605 finetune.py:68] layer 20_v @ epoch 1 new loss 6.248066711123101e-06 old loss 6.898633273522137e-06 BETTER
I0328 14:38:03.174986 2312675 finetune.py:45] layer 21_v initial loss 2.003599001909606e-05
W0328 14:38:03.175202 2312675 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 14:38:36.473965 2312675 finetune.py:68] layer 21_v @ epoch 0 new loss 8.074216566456016e-06 old loss 2.003599001909606e-05 BETTER
I0328 14:38:39.344464 2312605 finetune.py:68] layer 20_v @ epoch 2 new loss 5.9696249081753194e-06 old loss 6.248066711123101e-06 BETTER
I0328 14:38:50.767363 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 64.51091408729553s
I0328 14:38:54.451925 2312745 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 14:38:54.452020 2312745 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 14:38:54.452063 2312745 utils.py:162] NumExpr defaulting to 16 threads.
I0328 14:38:54.792744 2312745 config.py:54] PyTorch version 2.6.0 available.
W0328 14:38:55.000368 2312745 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 14:38:55.600699 2312745 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 14:38:55.604693 2311068 quantize_finetune_llama.py:209] layer 23 gpu 3
I0328 14:38:55.619584 2312745 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 14:39:11.125956 2312675 finetune.py:68] layer 21_v @ epoch 1 new loss 7.342592653003521e-06 old loss 8.074216566456016e-06 BETTER
I0328 14:39:13.163450 2312745 finetune.py:45] layer 22_v initial loss 2.1760099116363563e-05
W0328 14:39:13.163674 2312745 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 14:39:16.375084 2312605 finetune.py:68] layer 20_v @ epoch 3 new loss 5.8130690376856364e-06 old loss 5.9696249081753194e-06 BETTER
I0328 14:39:46.181427 2312675 finetune.py:68] layer 21_v @ epoch 2 new loss 7.011603429418756e-06 old loss 7.342592653003521e-06 BETTER
I0328 14:39:46.793531 2312745 finetune.py:68] layer 22_v @ epoch 0 new loss 6.640238552790834e-06 old loss 2.1760099116363563e-05 BETTER
I0328 14:39:53.563889 2312605 finetune.py:68] layer 20_v @ epoch 4 new loss 5.689079443982337e-06 old loss 5.8130690376856364e-06 BETTER
I0328 14:40:01.490308 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 65.38742971420288s
I0328 14:40:05.374614 2312815 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 14:40:05.374719 2312815 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 14:40:05.374761 2312815 utils.py:162] NumExpr defaulting to 16 threads.
I0328 14:40:05.729485 2312815 config.py:54] PyTorch version 2.6.0 available.
W0328 14:40:05.952687 2312815 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 14:40:06.594271 2312815 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 14:40:06.598248 2311068 quantize_finetune_llama.py:209] layer 24 gpu 0
I0328 14:40:06.613042 2312815 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 14:40:13.350219 2312605 finetune.py:45] layer 20_q initial loss 6.625306468777126e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 14:40:21.369715 2312675 finetune.py:68] layer 21_v @ epoch 3 new loss 6.788673545088386e-06 old loss 7.011603429418756e-06 BETTER
I0328 14:40:21.779389 2312745 finetune.py:68] layer 22_v @ epoch 1 new loss 5.9276126194163226e-06 old loss 6.640238552790834e-06 BETTER
I0328 14:40:24.423099 2312815 finetune.py:45] layer 23_v initial loss 2.710259286686778e-05
W0328 14:40:24.423353 2312815 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 14:40:48.652140 2312605 finetune.py:68] layer 20_q @ epoch 0 new loss 6.4598916651448235e-06 old loss 6.625306468777126e-06 BETTER
I0328 14:40:56.924221 2312675 finetune.py:68] layer 21_v @ epoch 4 new loss 6.621997272304725e-06 old loss 6.788673545088386e-06 BETTER
I0328 14:40:57.198920 2312745 finetune.py:68] layer 22_v @ epoch 2 new loss 5.634187800751533e-06 old loss 5.9276126194163226e-06 BETTER
I0328 14:40:57.899987 2312815 finetune.py:68] layer 23_v @ epoch 0 new loss 6.932363703526789e-06 old loss 2.710259286686778e-05 BETTER
I0328 14:41:16.719186 2312675 finetune.py:45] layer 21_q initial loss 8.127062756102532e-06
I0328 14:41:25.038694 2312605 finetune.py:68] layer 20_q @ epoch 1 new loss 6.343688710330753e-06 old loss 6.4598916651448235e-06 BETTER
I0328 14:41:32.094208 2312815 finetune.py:68] layer 23_v @ epoch 1 new loss 6.03386160946684e-06 old loss 6.932363703526789e-06 BETTER
I0328 14:41:32.587805 2312745 finetune.py:68] layer 22_v @ epoch 3 new loss 5.459435669763479e-06 old loss 5.634187800751533e-06 BETTER
I0328 14:41:50.145992 2312675 finetune.py:68] layer 21_q @ epoch 0 new loss 7.862620805099141e-06 old loss 8.127062756102532e-06 BETTER
I0328 14:42:01.637554 2312605 finetune.py:68] layer 20_q @ epoch 2 new loss 6.256215783650987e-06 old loss 6.343688710330753e-06 BETTER
I0328 14:42:06.662020 2312815 finetune.py:68] layer 23_v @ epoch 2 new loss 5.708972366846865e-06 old loss 6.03386160946684e-06 BETTER
I0328 14:42:07.774922 2312745 finetune.py:68] layer 22_v @ epoch 4 new loss 5.3512712838710286e-06 old loss 5.459435669763479e-06 BETTER
I0328 14:42:24.554951 2312675 finetune.py:68] layer 21_q @ epoch 1 new loss 7.705806638114154e-06 old loss 7.862620805099141e-06 BETTER
I0328 14:42:27.459494 2312745 finetune.py:45] layer 22_q initial loss 6.616641258005984e-06
I0328 14:42:38.340691 2312605 finetune.py:68] layer 20_q @ epoch 3 new loss 6.186684004205745e-06 old loss 6.256215783650987e-06 BETTER
I0328 14:42:41.397632 2312815 finetune.py:68] layer 23_v @ epoch 3 new loss 5.52973597223172e-06 old loss 5.708972366846865e-06 BETTER
I0328 14:42:59.086004 2312675 finetune.py:68] layer 21_q @ epoch 2 new loss 7.56090503273299e-06 old loss 7.705806638114154e-06 BETTER
I0328 14:43:01.106254 2312745 finetune.py:68] layer 22_q @ epoch 0 new loss 6.424501407309435e-06 old loss 6.616641258005984e-06 BETTER
I0328 14:43:15.167469 2312605 finetune.py:68] layer 20_q @ epoch 4 new loss 6.123772436694708e-06 old loss 6.186684004205745e-06 BETTER
I0328 14:43:16.230272 2312815 finetune.py:68] layer 23_v @ epoch 4 new loss 5.410447101894533e-06 old loss 5.52973597223172e-06 BETTER
I0328 14:43:33.166743 2312605 finetune.py:45] layer 20_k initial loss 6.4987430050678086e-06
I0328 14:43:34.103447 2312675 finetune.py:68] layer 21_q @ epoch 3 new loss 7.455186278093606e-06 old loss 7.56090503273299e-06 BETTER
I0328 14:43:35.964342 2312745 finetune.py:68] layer 22_q @ epoch 1 new loss 6.307872354227584e-06 old loss 6.424501407309435e-06 BETTER
I0328 14:43:36.754418 2312815 finetune.py:45] layer 23_q initial loss 6.556915195687907e-06
I0328 14:44:08.541734 2312605 finetune.py:68] layer 20_k @ epoch 0 new loss 6.429743280023104e-06 old loss 6.4987430050678086e-06 BETTER
I0328 14:44:09.583117 2312675 finetune.py:68] layer 21_q @ epoch 4 new loss 7.3657342909427825e-06 old loss 7.455186278093606e-06 BETTER
I0328 14:44:10.134170 2312815 finetune.py:68] layer 23_q @ epoch 0 new loss 6.384997959685279e-06 old loss 6.556915195687907e-06 BETTER
I0328 14:44:11.043942 2312745 finetune.py:68] layer 22_q @ epoch 2 new loss 6.217479949555127e-06 old loss 6.307872354227584e-06 BETTER
I0328 14:44:27.926240 2312675 finetune.py:45] layer 21_k initial loss 8.057354534685146e-06
I0328 14:44:44.367130 2312815 finetune.py:68] layer 23_q @ epoch 1 new loss 6.263949671847513e-06 old loss 6.384997959685279e-06 BETTER
I0328 14:44:45.198465 2312605 finetune.py:68] layer 20_k @ epoch 1 new loss 6.379215392371407e-06 old loss 6.429743280023104e-06 BETTER
I0328 14:44:45.809198 2312745 finetune.py:68] layer 22_q @ epoch 3 new loss 6.151104571472388e-06 old loss 6.217479949555127e-06 BETTER
I0328 14:45:01.242945 2312675 finetune.py:68] layer 21_k @ epoch 0 new loss 7.885856575740036e-06 old loss 8.057354534685146e-06 BETTER
I0328 14:45:18.795764 2312815 finetune.py:68] layer 23_q @ epoch 2 new loss 6.179466708999826e-06 old loss 6.263949671847513e-06 BETTER
I0328 14:45:20.828592 2312745 finetune.py:68] layer 22_q @ epoch 4 new loss 6.098956873756833e-06 old loss 6.151104571472388e-06 BETTER
I0328 14:45:21.904098 2312605 finetune.py:68] layer 20_k @ epoch 2 new loss 6.337714239634806e-06 old loss 6.379215392371407e-06 BETTER
I0328 14:45:35.512398 2312675 finetune.py:68] layer 21_k @ epoch 1 new loss 7.814951459295116e-06 old loss 7.885856575740036e-06 BETTER
I0328 14:45:38.874908 2312745 finetune.py:45] layer 22_k initial loss 6.667462457699003e-06
I0328 14:45:52.998532 2312815 finetune.py:68] layer 23_q @ epoch 3 new loss 6.115918495197548e-06 old loss 6.179466708999826e-06 BETTER
I0328 14:45:58.367017 2312605 finetune.py:68] layer 20_k @ epoch 3 new loss 6.3006323216541205e-06 old loss 6.337714239634806e-06 BETTER
I0328 14:46:09.887224 2312675 finetune.py:68] layer 21_k @ epoch 2 new loss 7.75354328652611e-06 old loss 7.814951459295116e-06 BETTER
I0328 14:46:12.436976 2312745 finetune.py:68] layer 22_k @ epoch 0 new loss 6.5855506363732275e-06 old loss 6.667462457699003e-06 BETTER
I0328 14:46:27.093621 2312815 finetune.py:68] layer 23_q @ epoch 4 new loss 6.060658961359877e-06 old loss 6.115918495197548e-06 BETTER
I0328 14:46:34.879930 2312605 finetune.py:68] layer 20_k @ epoch 4 new loss 6.275441592151765e-06 old loss 6.3006323216541205e-06 BETTER
I0328 14:46:44.408871 2312675 finetune.py:68] layer 21_k @ epoch 3 new loss 7.706146789132617e-06 old loss 7.75354328652611e-06 BETTER
I0328 14:46:45.396508 2312815 finetune.py:45] layer 23_k initial loss 6.800626579206437e-06
I0328 14:46:47.000847 2312745 finetune.py:68] layer 22_k @ epoch 1 new loss 6.538371053466108e-06 old loss 6.5855506363732275e-06 BETTER
I0328 14:46:54.342566 2312605 finetune.py:45] layer 20_o initial loss 1.3918517652200535e-05
I0328 14:47:18.255096 2312815 finetune.py:68] layer 23_k @ epoch 0 new loss 6.651950116065564e-06 old loss 6.800626579206437e-06 BETTER
I0328 14:47:19.216758 2312675 finetune.py:68] layer 21_k @ epoch 4 new loss 7.669460501347203e-06 old loss 7.706146789132617e-06 BETTER
I0328 14:47:21.709460 2312745 finetune.py:68] layer 22_k @ epoch 2 new loss 6.500730250991182e-06 old loss 6.538371053466108e-06 BETTER
I0328 14:47:29.029311 2312605 finetune.py:68] layer 20_o @ epoch 0 new loss 1.3461039088724647e-05 old loss 1.3918517652200535e-05 BETTER
I0328 14:47:39.007812 2312675 finetune.py:45] layer 21_o initial loss 1.74044216691982e-05
I0328 14:47:52.171521 2312815 finetune.py:68] layer 23_k @ epoch 1 new loss 6.592775207536761e-06 old loss 6.651950116065564e-06 BETTER
I0328 14:47:56.125128 2312745 finetune.py:68] layer 22_k @ epoch 3 new loss 6.469786967500113e-06 old loss 6.500730250991182e-06 BETTER
I0328 14:48:04.912480 2312605 finetune.py:68] layer 20_o @ epoch 1 new loss 1.3257167665869929e-05 old loss 1.3461039088724647e-05 BETTER
I0328 14:48:11.705506 2312675 finetune.py:68] layer 21_o @ epoch 0 new loss 1.6735075405449606e-05 old loss 1.74044216691982e-05 BETTER
I0328 14:48:26.272820 2312815 finetune.py:68] layer 23_k @ epoch 2 new loss 6.546168151544407e-06 old loss 6.592775207536761e-06 BETTER
I0328 14:48:30.570082 2312745 finetune.py:68] layer 22_k @ epoch 4 new loss 6.445036433433415e-06 old loss 6.469786967500113e-06 BETTER
I0328 14:48:40.560570 2312605 finetune.py:68] layer 20_o @ epoch 2 new loss 1.311085816269042e-05 old loss 1.3257167665869929e-05 BETTER
I0328 14:48:45.327095 2312675 finetune.py:68] layer 21_o @ epoch 1 new loss 1.6414111087215133e-05 old loss 1.6735075405449606e-05 BETTER
I0328 14:48:50.660880 2312745 finetune.py:45] layer 22_o initial loss 1.558562507852912e-05
I0328 14:49:00.228574 2312815 finetune.py:68] layer 23_k @ epoch 3 new loss 6.511833817057777e-06 old loss 6.546168151544407e-06 BETTER
I0328 14:49:16.543627 2312605 finetune.py:68] layer 20_o @ epoch 3 new loss 1.2990832146897446e-05 old loss 1.311085816269042e-05 BETTER
I0328 14:49:19.110321 2312675 finetune.py:68] layer 21_o @ epoch 2 new loss 1.6192652765312232e-05 old loss 1.6414111087215133e-05 BETTER
I0328 14:49:23.559166 2312745 finetune.py:68] layer 22_o @ epoch 0 new loss 1.5106848877621815e-05 old loss 1.558562507852912e-05 BETTER
I0328 14:49:34.312819 2312815 finetune.py:68] layer 23_k @ epoch 4 new loss 6.480281626863871e-06 old loss 6.511833817057777e-06 BETTER
I0328 14:49:52.752946 2312605 finetune.py:68] layer 20_o @ epoch 4 new loss 1.2899125977128278e-05 old loss 1.2990832146897446e-05 BETTER
I0328 14:49:53.115170 2312675 finetune.py:68] layer 21_o @ epoch 3 new loss 1.601619806024246e-05 old loss 1.6192652765312232e-05 BETTER
I0328 14:49:54.211295 2312815 finetune.py:45] layer 23_o initial loss 1.610077561053913e-05
I0328 14:49:57.421673 2312745 finetune.py:68] layer 22_o @ epoch 1 new loss 1.4908544471836649e-05 old loss 1.5106848877621815e-05 BETTER
I0328 14:50:24.611819 2312605 finetune.py:45] layer 20_up initial loss 3.946808283217251e-05
I0328 14:50:26.592144 2312815 finetune.py:68] layer 23_o @ epoch 0 new loss 1.5529192751273513e-05 old loss 1.610077561053913e-05 BETTER
I0328 14:50:27.437262 2312675 finetune.py:68] layer 21_o @ epoch 4 new loss 1.5876401448622346e-05 old loss 1.601619806024246e-05 BETTER
I0328 14:50:31.469939 2312745 finetune.py:68] layer 22_o @ epoch 2 new loss 1.4767625543754548e-05 old loss 1.4908544471836649e-05 BETTER
I0328 14:50:56.513096 2312605 finetune.py:68] layer 20_up @ epoch 0 new loss 3.872477827826515e-05 old loss 3.946808283217251e-05 BETTER
I0328 14:50:58.865000 2312675 finetune.py:45] layer 21_up initial loss 4.612865814124234e-05
I0328 14:50:59.925180 2312815 finetune.py:68] layer 23_o @ epoch 1 new loss 1.5322171748266555e-05 old loss 1.5529192751273513e-05 BETTER
I0328 14:51:05.451405 2312745 finetune.py:68] layer 22_o @ epoch 3 new loss 1.4663393812952563e-05 old loss 1.4767625543754548e-05 BETTER
I0328 14:51:29.214195 2312675 finetune.py:68] layer 21_up @ epoch 0 new loss 4.520514266914688e-05 old loss 4.612865814124234e-05 BETTER
I0328 14:51:29.880167 2312605 finetune.py:68] layer 20_up @ epoch 1 new loss 3.817674223682843e-05 old loss 3.872477827826515e-05 BETTER
I0328 14:51:33.316282 2312815 finetune.py:68] layer 23_o @ epoch 2 new loss 1.5176994565990753e-05 old loss 1.5322171748266555e-05 BETTER
I0328 14:51:39.436361 2312745 finetune.py:68] layer 22_o @ epoch 4 new loss 1.4576896319340449e-05 old loss 1.4663393812952563e-05 BETTER
I0328 14:52:00.757851 2312675 finetune.py:68] layer 21_up @ epoch 1 new loss 4.453363726497628e-05 old loss 4.520514266914688e-05 BETTER
I0328 14:52:03.414070 2312605 finetune.py:68] layer 20_up @ epoch 2 new loss 3.7723184505011886e-05 old loss 3.817674223682843e-05 BETTER
I0328 14:52:06.951272 2312815 finetune.py:68] layer 23_o @ epoch 3 new loss 1.506542776041897e-05 old loss 1.5176994565990753e-05 BETTER
I0328 14:52:11.641116 2312745 finetune.py:45] layer 22_up initial loss 4.637956226360984e-05
I0328 14:52:32.516163 2312675 finetune.py:68] layer 21_up @ epoch 2 new loss 4.398067176225595e-05 old loss 4.453363726497628e-05 BETTER
I0328 14:52:37.173139 2312605 finetune.py:68] layer 20_up @ epoch 3 new loss 3.732883487828076e-05 old loss 3.7723184505011886e-05 BETTER
I0328 14:52:40.505557 2312815 finetune.py:68] layer 23_o @ epoch 4 new loss 1.4981234926381148e-05 old loss 1.506542776041897e-05 BETTER
I0328 14:52:42.201088 2312745 finetune.py:68] layer 22_up @ epoch 0 new loss 4.5494794903788716e-05 old loss 4.637956226360984e-05 BETTER
I0328 14:53:04.400309 2312675 finetune.py:68] layer 21_up @ epoch 3 new loss 4.3510954128578305e-05 old loss 4.398067176225595e-05 BETTER
I0328 14:53:10.885941 2312605 finetune.py:68] layer 20_up @ epoch 4 new loss 3.699040098581463e-05 old loss 3.732883487828076e-05 BETTER
I0328 14:53:12.330496 2312815 finetune.py:45] layer 23_up initial loss 4.9528836825629696e-05
I0328 14:53:13.987176 2312745 finetune.py:68] layer 22_up @ epoch 1 new loss 4.4881227950099856e-05 old loss 4.5494794903788716e-05 BETTER
I0328 14:53:36.559673 2312675 finetune.py:68] layer 21_up @ epoch 4 new loss 4.310533404350281e-05 old loss 4.3510954128578305e-05 BETTER
I0328 14:53:42.532420 2312815 finetune.py:68] layer 23_up @ epoch 0 new loss 4.8694513679947704e-05 old loss 4.9528836825629696e-05 BETTER
I0328 14:53:42.557355 2312605 finetune.py:45] layer 20_gate initial loss 4.684128725784831e-05
I0328 14:53:46.016470 2312745 finetune.py:68] layer 22_up @ epoch 2 new loss 4.437469760887325e-05 old loss 4.4881227950099856e-05 BETTER
I0328 14:54:08.384440 2312675 finetune.py:45] layer 21_gate initial loss 5.4311782150762156e-05
I0328 14:54:12.706831 2312605 finetune.py:68] layer 20_gate @ epoch 0 new loss 4.648351750802249e-05 old loss 4.684128725784831e-05 BETTER
I0328 14:54:13.764544 2312815 finetune.py:68] layer 23_up @ epoch 1 new loss 4.8101584980031475e-05 old loss 4.8694513679947704e-05 BETTER
I0328 14:54:18.169031 2312745 finetune.py:68] layer 22_up @ epoch 3 new loss 4.394636562210508e-05 old loss 4.437469760887325e-05 BETTER
I0328 14:54:36.799436 2312675 finetune.py:68] layer 21_gate @ epoch 0 new loss 5.387761484598741e-05 old loss 5.4311782150762156e-05 BETTER
I0328 14:54:43.811711 2312605 finetune.py:68] layer 20_gate @ epoch 1 new loss 4.6186192776076496e-05 old loss 4.648351750802249e-05 BETTER
I0328 14:54:45.036976 2312815 finetune.py:68] layer 23_up @ epoch 2 new loss 4.762074968311936e-05 old loss 4.8101584980031475e-05 BETTER
I0328 14:54:50.358392 2312745 finetune.py:68] layer 22_up @ epoch 4 new loss 4.3572847062023357e-05 old loss 4.394636562210508e-05 BETTER
I0328 14:55:06.288935 2312675 finetune.py:68] layer 21_gate @ epoch 1 new loss 5.3525505791185424e-05 old loss 5.387761484598741e-05 BETTER
I0328 14:55:15.002561 2312605 finetune.py:68] layer 20_gate @ epoch 2 new loss 4.5922239223727956e-05 old loss 4.6186192776076496e-05 BETTER
I0328 14:55:16.357437 2312815 finetune.py:68] layer 23_up @ epoch 3 new loss 4.7215220547514036e-05 old loss 4.762074968311936e-05 BETTER
I0328 14:55:22.441078 2312745 finetune.py:45] layer 22_gate initial loss 5.579201751970686e-05
I0328 14:55:35.836170 2312675 finetune.py:68] layer 21_gate @ epoch 2 new loss 5.32140584255103e-05 old loss 5.3525505791185424e-05 BETTER
I0328 14:55:46.476868 2312605 finetune.py:68] layer 20_gate @ epoch 3 new loss 4.5690936531173065e-05 old loss 4.5922239223727956e-05 BETTER
I0328 14:55:47.694234 2312815 finetune.py:68] layer 23_up @ epoch 4 new loss 4.6867022319929674e-05 old loss 4.7215220547514036e-05 BETTER
I0328 14:55:51.096998 2312745 finetune.py:68] layer 22_gate @ epoch 0 new loss 5.539175617741421e-05 old loss 5.579201751970686e-05 BETTER
I0328 14:56:05.490974 2312675 finetune.py:68] layer 21_gate @ epoch 3 new loss 5.2942621550755575e-05 old loss 5.32140584255103e-05 BETTER
I0328 14:56:17.868264 2312605 finetune.py:68] layer 20_gate @ epoch 4 new loss 4.547942808130756e-05 old loss 4.5690936531173065e-05 BETTER
I0328 14:56:20.158695 2312815 finetune.py:45] layer 23_gate initial loss 6.0667196521535516e-05
I0328 14:56:20.548116 2312745 finetune.py:68] layer 22_gate @ epoch 1 new loss 5.506546585820615e-05 old loss 5.539175617741421e-05 BETTER
I0328 14:56:35.081398 2312675 finetune.py:68] layer 21_gate @ epoch 4 new loss 5.2696308557642624e-05 old loss 5.2942621550755575e-05 BETTER
I0328 14:56:48.073270 2312815 finetune.py:68] layer 23_gate @ epoch 0 new loss 6.029962605680339e-05 old loss 6.0667196521535516e-05 BETTER
I0328 14:56:50.398837 2312745 finetune.py:68] layer 22_gate @ epoch 2 new loss 5.478541788761504e-05 old loss 5.506546585820615e-05 BETTER
I0328 14:57:13.454592 2312605 finetune.py:45] layer 20_down initial loss 8.174042886821553e-05
I0328 14:57:17.226165 2312815 finetune.py:68] layer 23_gate @ epoch 1 new loss 6.0003469116054475e-05 old loss 6.029962605680339e-05 BETTER
I0328 14:57:20.116170 2312745 finetune.py:68] layer 22_gate @ epoch 3 new loss 5.453688572742976e-05 old loss 5.478541788761504e-05 BETTER
I0328 14:57:31.962146 2312675 finetune.py:45] layer 21_down initial loss 9.49335953919217e-05
I0328 14:57:40.749931 2312605 finetune.py:68] layer 20_down @ epoch 0 new loss 8.173893002094701e-05 old loss 8.174042886821553e-05 BETTER
I0328 14:57:46.740486 2312815 finetune.py:68] layer 23_gate @ epoch 2 new loss 5.974720261292532e-05 old loss 6.0003469116054475e-05 BETTER
I0328 14:57:50.101435 2312745 finetune.py:68] layer 22_gate @ epoch 4 new loss 5.431448880699463e-05 old loss 5.453688572742976e-05 BETTER
I0328 14:57:58.075358 2312675 finetune.py:68] layer 21_down @ epoch 0 new loss 9.49322638916783e-05 old loss 9.49335953919217e-05 BETTER
I0328 14:58:09.055284 2312605 finetune.py:68] layer 20_down @ epoch 1 new loss 8.173780224751681e-05 old loss 8.173893002094701e-05 BETTER
I0328 14:58:16.232944 2312815 finetune.py:68] layer 23_gate @ epoch 3 new loss 5.95191158936359e-05 old loss 5.974720261292532e-05 BETTER
I0328 14:58:25.155472 2312675 finetune.py:68] layer 21_down @ epoch 1 new loss 9.493121615378186e-05 old loss 9.49322638916783e-05 BETTER
I0328 14:58:38.012042 2312605 finetune.py:68] layer 20_down @ epoch 2 new loss 8.17370237200521e-05 old loss 8.173780224751681e-05 BETTER
I0328 14:58:45.537677 2312815 finetune.py:68] layer 23_gate @ epoch 4 new loss 5.9322366723790765e-05 old loss 5.95191158936359e-05 BETTER
I0328 14:58:47.387698 2312745 finetune.py:45] layer 22_down initial loss 9.903912723530084e-05
I0328 14:58:52.583247 2312675 finetune.py:68] layer 21_down @ epoch 2 new loss 9.493018296780065e-05 old loss 9.493121615378186e-05 BETTER
I0328 14:59:07.228015 2312605 finetune.py:68] layer 20_down @ epoch 3 new loss 8.17361069493927e-05 old loss 8.17370237200521e-05 BETTER
I0328 14:59:13.573657 2312745 finetune.py:68] layer 22_down @ epoch 0 new loss 9.90377229754813e-05 old loss 9.903912723530084e-05 BETTER
I0328 14:59:20.049559 2312675 finetune.py:68] layer 21_down @ epoch 3 new loss 9.492946264799684e-05 old loss 9.493018296780065e-05 BETTER
I0328 14:59:36.593565 2312605 finetune.py:68] layer 20_down @ epoch 4 new loss 8.17356922198087e-05 old loss 8.17361069493927e-05 BETTER
20_v proxy err 0.006745405960828066 tr(WHW.T) 330.192138671875
bpp_loss 3.6089831505087204
20_q proxy err 0.0004612554039340466 tr(WHW.T) 20742.044921875
bpp_loss 4.408401930239052
20_k proxy err 0.00018219648336526006 tr(WHW.T) 15390.7412109375
bpp_loss 5.349392196396366
20_o proxy err 0.007462187670171261 tr(WHW.T) 1206.2579345703125
bpp_loss 3.647300444135908
20_up proxy err 0.005770571995526552 tr(WHW.T) 7602.451171875
bpp_loss 3.7370697491403138
20_gate proxy err 0.001464869361370802 tr(WHW.T) 30656.3828125
bpp_loss 4.1255158785996695
20_down proxy err 0.006463497411459684 tr(WHW.T) 6312.9794921875
bpp_loss 3.732740713832235
I0328 14:59:41.135190 2312745 finetune.py:68] layer 22_down @ epoch 1 new loss 9.903675527311862e-05 old loss 9.90377229754813e-05 BETTER
I0328 14:59:45.714442 2312815 finetune.py:45] layer 23_down initial loss 0.0001069453137461096
I0328 14:59:48.145838 2312675 finetune.py:68] layer 21_down @ epoch 4 new loss 9.492899698670954e-05 old loss 9.492946264799684e-05 BETTER
21_v proxy err 0.006231190171092749 tr(WHW.T) 362.87310791015625
bpp_loss 3.6345796978566796
21_q proxy err 0.0003749222378246486 tr(WHW.T) 25845.17578125
bpp_loss 4.404458460514434
21_k proxy err 0.00016740287537686527 tr(WHW.T) 16781.7265625
bpp_loss 5.394161853357218
21_o proxy err 0.005060584284365177 tr(WHW.T) 1268.4730224609375
bpp_loss 3.666865906561725
21_up proxy err 0.005541905295103788 tr(WHW.T) 7772.787109375
bpp_loss 3.7399764115523015
21_gate proxy err 0.0013998609501868486 tr(WHW.T) 31535.693359375
bpp_loss 4.137524236525808
21_down proxy err 0.006093364208936691 tr(WHW.T) 6363.75537109375
bpp_loss 3.7338251442083026
I0328 15:00:09.359731 2312745 finetune.py:68] layer 22_down @ epoch 2 new loss 9.903573663905263e-05 old loss 9.903675527311862e-05 BETTER
I0328 15:00:11.708989 2312815 finetune.py:68] layer 23_down @ epoch 0 new loss 0.00010694374941522256 old loss 0.0001069453137461096 BETTER
I0328 15:00:37.142594 2312745 finetune.py:68] layer 22_down @ epoch 3 new loss 9.903492173179984e-05 old loss 9.903573663905263e-05 BETTER
I0328 15:00:38.585422 2312815 finetune.py:68] layer 23_down @ epoch 1 new loss 0.00010694232332753018 old loss 0.00010694374941522256 BETTER
I0328 15:00:52.674977 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 59.526700019836426s
I0328 15:00:56.452599 2312885 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:00:56.452758 2312885 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:00:56.452799 2312885 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:00:56.813312 2312885 config.py:54] PyTorch version 2.6.0 available.
W0328 15:00:57.030246 2312885 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:00:57.637088 2312885 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 15:00:57.641222 2311068 quantize_finetune_llama.py:209] layer 25 gpu 1
I0328 15:00:57.660439 2312885 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 15:01:04.959718 2312745 finetune.py:68] layer 22_down @ epoch 4 new loss 9.903419413603842e-05 old loss 9.903492173179984e-05 BETTER
I0328 15:01:05.639310 2312815 finetune.py:68] layer 23_down @ epoch 2 new loss 0.00010694144293665886 old loss 0.00010694232332753018 BETTER
22_v proxy err 0.006179550662636757 tr(WHW.T) 346.0789489746094
bpp_loss 3.6849318064632826
22_q proxy err 0.0004491245490498841 tr(WHW.T) 20326.59765625
bpp_loss 4.36279835482128
22_k proxy err 0.00018323783297091722 tr(WHW.T) 14721.00390625
bpp_loss 5.325556579511613
22_o proxy err 0.006964760832488537 tr(WHW.T) 1222.267333984375
bpp_loss 3.6940764919272624
22_up proxy err 0.005771930795162916 tr(WHW.T) 7548.646484375
bpp_loss 3.7441105845916485
22_gate proxy err 0.0015117530710995197 tr(WHW.T) 29524.259765625
bpp_loss 4.141195899341255
22_down proxy err 0.006111132446676493 tr(WHW.T) 6541.99169921875
bpp_loss 3.73939192153713
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:01:15.043361 2312885 finetune.py:45] layer 24_v initial loss 3.0096045520622283e-05
W0328 15:01:15.043585 2312885 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 15:01:32.789546 2312815 finetune.py:68] layer 23_down @ epoch 3 new loss 0.00010694063530536368 old loss 0.00010694144293665886 BETTER
I0328 15:01:50.039550 2312885 finetune.py:68] layer 24_v @ epoch 0 new loss 7.239844762807479e-06 old loss 3.0096045520622283e-05 BETTER
I0328 15:02:00.051848 2312815 finetune.py:68] layer 23_down @ epoch 4 new loss 0.00010693974763853475 old loss 0.00010694063530536368 BETTER
23_v proxy err 0.005791909992694855 tr(WHW.T) 397.90704345703125
bpp_loss 3.7374452096410096
23_q proxy err 0.00043328534229658544 tr(WHW.T) 22617.953125
bpp_loss 4.375116075621918
23_k proxy err 0.00019021541811525822 tr(WHW.T) 14856.1669921875
bpp_loss 5.328166205435991
23_o proxy err 0.0058890110813081264 tr(WHW.T) 1744.0814208984375
bpp_loss 3.7169787303428166
23_up proxy err 0.005868909880518913 tr(WHW.T) 7419.701171875
bpp_loss 3.7480804060718844
23_gate proxy err 0.0016353405080735683 tr(WHW.T) 27263.359375
bpp_loss 4.143552054251943
23_down proxy err 0.006117255426943302 tr(WHW.T) 6679.88232421875
bpp_loss 3.744684297242202
I0328 15:02:14.631360 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 65.01565408706665s
I0328 15:02:18.248386 2312955 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:02:18.248483 2312955 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:02:18.248526 2312955 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:02:18.582246 2312955 config.py:54] PyTorch version 2.6.0 available.
W0328 15:02:18.788559 2312955 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:02:19.467143 2312955 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 15:02:19.471316 2311068 quantize_finetune_llama.py:209] layer 26 gpu 2
I0328 15:02:19.484754 2312955 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 15:02:26.751135 2312885 finetune.py:68] layer 24_v @ epoch 1 new loss 6.304991984507069e-06 old loss 7.239844762807479e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:02:36.940723 2312955 finetune.py:45] layer 25_v initial loss 3.391340214875527e-05
W0328 15:02:36.941192 2312955 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 15:03:03.799775 2312885 finetune.py:68] layer 24_v @ epoch 2 new loss 6.001904239383293e-06 old loss 6.304991984507069e-06 BETTER
I0328 15:03:10.200536 2312955 finetune.py:68] layer 25_v @ epoch 0 new loss 8.341134162037633e-06 old loss 3.391340214875527e-05 BETTER
I0328 15:03:24.272420 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 64.33896470069885s
I0328 15:03:28.119367 2313025 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:03:28.119464 2313025 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:03:28.119502 2313025 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:03:28.479102 2313025 config.py:54] PyTorch version 2.6.0 available.
W0328 15:03:28.676109 2313025 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:03:29.271700 2313025 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 15:03:29.276456 2311068 quantize_finetune_llama.py:209] layer 27 gpu 3
I0328 15:03:29.296957 2313025 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 15:03:41.043089 2312885 finetune.py:68] layer 24_v @ epoch 3 new loss 5.83201062909211e-06 old loss 6.001904239383293e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:03:44.696246 2312955 finetune.py:68] layer 25_v @ epoch 1 new loss 7.4381182457727846e-06 old loss 8.341134162037633e-06 BETTER
I0328 15:03:47.457894 2313025 finetune.py:45] layer 26_v initial loss 3.202493462595157e-05
W0328 15:03:47.458295 2313025 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 15:04:18.435616 2312885 finetune.py:68] layer 24_v @ epoch 4 new loss 5.715598945243983e-06 old loss 5.83201062909211e-06 BETTER
I0328 15:04:20.018205 2312955 finetune.py:68] layer 25_v @ epoch 2 new loss 7.019297299848404e-06 old loss 7.4381182457727846e-06 BETTER
I0328 15:04:21.282506 2313025 finetune.py:68] layer 26_v @ epoch 0 new loss 1.148656247096369e-05 old loss 3.202493462595157e-05 BETTER
I0328 15:04:35.250556 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 65.52569031715393s
I0328 15:04:38.824828 2312885 finetune.py:45] layer 24_q initial loss 7.132966402423335e-06
I0328 15:04:39.276277 2313095 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:04:39.276369 2313095 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:04:39.276409 2313095 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:04:39.648737 2313095 config.py:54] PyTorch version 2.6.0 available.
W0328 15:04:39.847743 2313095 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:04:40.467003 2313095 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 15:04:40.470818 2311068 quantize_finetune_llama.py:209] layer 28 gpu 0
I0328 15:04:40.484321 2313095 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:04:55.086129 2312955 finetune.py:68] layer 25_v @ epoch 3 new loss 6.980257239774801e-06 old loss 7.019297299848404e-06 BETTER
I0328 15:04:55.927897 2313025 finetune.py:68] layer 26_v @ epoch 1 new loss 1.0539981303736567e-05 old loss 1.148656247096369e-05 BETTER
I0328 15:04:58.168098 2313095 finetune.py:45] layer 27_v initial loss 3.353611100465059e-05
W0328 15:04:58.168282 2313095 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 15:05:14.116845 2312885 finetune.py:68] layer 24_q @ epoch 0 new loss 6.919718543940689e-06 old loss 7.132966402423335e-06 BETTER
I0328 15:05:30.552654 2312955 finetune.py:68] layer 25_v @ epoch 4 new loss 6.763963028788567e-06 old loss 6.980257239774801e-06 BETTER
I0328 15:05:31.284187 2313025 finetune.py:68] layer 26_v @ epoch 2 new loss 1.0207440936937928e-05 old loss 1.0539981303736567e-05 BETTER
I0328 15:05:31.531851 2313095 finetune.py:68] layer 27_v @ epoch 0 new loss 1.0526805453991983e-05 old loss 3.353611100465059e-05 BETTER
I0328 15:05:50.256857 2312955 finetune.py:45] layer 25_q initial loss 9.47951411944814e-06
I0328 15:05:50.721719 2312885 finetune.py:68] layer 24_q @ epoch 1 new loss 6.8016292971151415e-06 old loss 6.919718543940689e-06 BETTER
I0328 15:06:05.909985 2313095 finetune.py:68] layer 27_v @ epoch 1 new loss 9.80304685072042e-06 old loss 1.0526805453991983e-05 BETTER
I0328 15:06:06.499746 2313025 finetune.py:68] layer 26_v @ epoch 3 new loss 9.881966434477363e-06 old loss 1.0207440936937928e-05 BETTER
I0328 15:06:23.865631 2312955 finetune.py:68] layer 25_q @ epoch 0 new loss 9.051204870047513e-06 old loss 9.47951411944814e-06 BETTER
I0328 15:06:27.535456 2312885 finetune.py:68] layer 24_q @ epoch 2 new loss 6.724091235810192e-06 old loss 6.8016292971151415e-06 BETTER
I0328 15:06:40.383169 2313095 finetune.py:68] layer 27_v @ epoch 2 new loss 9.53849757934222e-06 old loss 9.80304685072042e-06 BETTER
I0328 15:06:41.726884 2313025 finetune.py:68] layer 26_v @ epoch 4 new loss 9.777847481018398e-06 old loss 9.881966434477363e-06 BETTER
I0328 15:06:58.359929 2312955 finetune.py:68] layer 25_q @ epoch 1 new loss 8.795885150902905e-06 old loss 9.051204870047513e-06 BETTER
I0328 15:07:01.764135 2313025 finetune.py:45] layer 26_q initial loss 1.2023885574308224e-05
I0328 15:07:04.401515 2312885 finetune.py:68] layer 24_q @ epoch 3 new loss 6.672410563623998e-06 old loss 6.724091235810192e-06 BETTER
I0328 15:07:15.073382 2313095 finetune.py:68] layer 27_v @ epoch 3 new loss 9.426458746020216e-06 old loss 9.53849757934222e-06 BETTER
I0328 15:07:32.899685 2312955 finetune.py:68] layer 25_q @ epoch 2 new loss 8.663492735649925e-06 old loss 8.795885150902905e-06 BETTER
I0328 15:07:35.402912 2313025 finetune.py:68] layer 26_q @ epoch 0 new loss 1.1602252925513312e-05 old loss 1.2023885574308224e-05 BETTER
I0328 15:07:41.479916 2312885 finetune.py:68] layer 24_q @ epoch 4 new loss 6.620377916988218e-06 old loss 6.672410563623998e-06 BETTER
I0328 15:07:49.667659 2313095 finetune.py:68] layer 27_v @ epoch 4 new loss 9.221205800713506e-06 old loss 9.426458746020216e-06 BETTER
I0328 15:07:59.385738 2312885 finetune.py:45] layer 24_k initial loss 7.660192750336137e-06
I0328 15:08:07.626993 2312955 finetune.py:68] layer 25_q @ epoch 3 new loss 8.539766895410139e-06 old loss 8.663492735649925e-06 BETTER
I0328 15:08:09.708157 2313095 finetune.py:45] layer 27_q initial loss 1.268208052351838e-05
I0328 15:08:09.969447 2313025 finetune.py:68] layer 26_q @ epoch 1 new loss 1.1394839020795189e-05 old loss 1.1602252925513312e-05 BETTER
I0328 15:08:34.591232 2312885 finetune.py:68] layer 24_k @ epoch 0 new loss 7.565387932118028e-06 old loss 7.660192750336137e-06 BETTER
I0328 15:08:42.808814 2312955 finetune.py:68] layer 25_q @ epoch 4 new loss 8.416180207859725e-06 old loss 8.539766895410139e-06 BETTER
I0328 15:08:43.099019 2313095 finetune.py:68] layer 27_q @ epoch 0 new loss 1.2328057891863864e-05 old loss 1.268208052351838e-05 BETTER
I0328 15:08:44.985098 2313025 finetune.py:68] layer 26_q @ epoch 2 new loss 1.124100435845321e-05 old loss 1.1394839020795189e-05 BETTER
I0328 15:09:00.819967 2312955 finetune.py:45] layer 25_k initial loss 1.0080028914671857e-05
I0328 15:09:11.165850 2312885 finetune.py:68] layer 24_k @ epoch 1 new loss 7.504770564992214e-06 old loss 7.565387932118028e-06 BETTER
I0328 15:09:17.083343 2313095 finetune.py:68] layer 27_q @ epoch 1 new loss 1.204986165248556e-05 old loss 1.2328057891863864e-05 BETTER
I0328 15:09:19.732100 2313025 finetune.py:68] layer 26_q @ epoch 3 new loss 1.1116975656477734e-05 old loss 1.124100435845321e-05 BETTER
I0328 15:09:34.205375 2312955 finetune.py:68] layer 25_k @ epoch 0 new loss 9.944197699951474e-06 old loss 1.0080028914671857e-05 BETTER
I0328 15:09:48.312089 2312885 finetune.py:68] layer 24_k @ epoch 2 new loss 7.467079740308691e-06 old loss 7.504770564992214e-06 BETTER
I0328 15:09:51.361012 2313095 finetune.py:68] layer 27_q @ epoch 2 new loss 1.1930764230783097e-05 old loss 1.204986165248556e-05 BETTER
I0328 15:09:54.941465 2313025 finetune.py:68] layer 26_q @ epoch 4 new loss 1.1024041668861173e-05 old loss 1.1116975656477734e-05 BETTER
I0328 15:10:08.782359 2312955 finetune.py:68] layer 25_k @ epoch 1 new loss 9.858481462288182e-06 old loss 9.944197699951474e-06 BETTER
I0328 15:10:13.244913 2313025 finetune.py:45] layer 26_k initial loss 1.2234469068062026e-05
I0328 15:10:25.252336 2312885 finetune.py:68] layer 24_k @ epoch 3 new loss 7.4399986260687e-06 old loss 7.467079740308691e-06 BETTER
I0328 15:10:25.701983 2313095 finetune.py:68] layer 27_q @ epoch 3 new loss 1.1840306797239464e-05 old loss 1.1930764230783097e-05 BETTER
I0328 15:10:43.175750 2312955 finetune.py:68] layer 25_k @ epoch 2 new loss 9.791572665562853e-06 old loss 9.858481462288182e-06 BETTER
I0328 15:10:46.744105 2313025 finetune.py:68] layer 26_k @ epoch 0 new loss 1.2062638234056067e-05 old loss 1.2234469068062026e-05 BETTER
I0328 15:11:00.087153 2313095 finetune.py:68] layer 27_q @ epoch 4 new loss 1.178819547931198e-05 old loss 1.1840306797239464e-05 BETTER
I0328 15:11:02.155642 2312885 finetune.py:68] layer 24_k @ epoch 4 new loss 7.400322374451207e-06 old loss 7.4399986260687e-06 BETTER
I0328 15:11:17.717493 2312955 finetune.py:68] layer 25_k @ epoch 3 new loss 9.736806532600895e-06 old loss 9.791572665562853e-06 BETTER
I0328 15:11:17.918257 2313095 finetune.py:45] layer 27_k initial loss 1.4175707292451989e-05
I0328 15:11:21.247200 2313025 finetune.py:68] layer 26_k @ epoch 1 new loss 1.198383415612625e-05 old loss 1.2062638234056067e-05 BETTER
I0328 15:11:22.606789 2312885 finetune.py:45] layer 24_o initial loss 1.7814005332184024e-05
I0328 15:11:51.025642 2313095 finetune.py:68] layer 27_k @ epoch 0 new loss 1.398950280417921e-05 old loss 1.4175707292451989e-05 BETTER
I0328 15:11:52.274464 2312955 finetune.py:68] layer 25_k @ epoch 4 new loss 9.702117495180573e-06 old loss 9.736806532600895e-06 BETTER
I0328 15:11:55.889167 2313025 finetune.py:68] layer 26_k @ epoch 2 new loss 1.1967928912781645e-05 old loss 1.198383415612625e-05 BETTER
I0328 15:11:57.366964 2312885 finetune.py:68] layer 24_o @ epoch 0 new loss 1.7367836335324682e-05 old loss 1.7814005332184024e-05 BETTER
I0328 15:12:12.097574 2312955 finetune.py:45] layer 25_o initial loss 2.157725793949794e-05
I0328 15:12:24.852952 2313095 finetune.py:68] layer 27_k @ epoch 1 new loss 1.3816667888022494e-05 old loss 1.398950280417921e-05 BETTER
I0328 15:12:30.330772 2313025 finetune.py:68] layer 26_k @ epoch 3 new loss 1.189361501019448e-05 old loss 1.1967928912781645e-05 BETTER
I0328 15:12:33.065407 2312885 finetune.py:68] layer 24_o @ epoch 1 new loss 1.720063119137194e-05 old loss 1.7367836335324682e-05 BETTER
I0328 15:12:44.933185 2312955 finetune.py:68] layer 25_o @ epoch 0 new loss 2.092795148200821e-05 old loss 2.157725793949794e-05 BETTER
I0328 15:12:58.588371 2313095 finetune.py:68] layer 27_k @ epoch 2 new loss 1.3753777238889597e-05 old loss 1.3816667888022494e-05 BETTER
I0328 15:13:04.983311 2313025 finetune.py:68] layer 26_k @ epoch 4 new loss 1.1866511158586945e-05 old loss 1.189361501019448e-05 BETTER
I0328 15:13:08.847696 2312885 finetune.py:68] layer 24_o @ epoch 2 new loss 1.7087950254790485e-05 old loss 1.720063119137194e-05 BETTER
I0328 15:13:18.624545 2312955 finetune.py:68] layer 25_o @ epoch 1 new loss 2.0650722944992594e-05 old loss 2.092795148200821e-05 BETTER
I0328 15:13:24.757127 2313025 finetune.py:45] layer 26_o initial loss 2.8374737667036243e-05
I0328 15:13:32.558014 2313095 finetune.py:68] layer 27_k @ epoch 3 new loss 1.3722740732191596e-05 old loss 1.3753777238889597e-05 BETTER
I0328 15:13:44.792817 2312885 finetune.py:68] layer 24_o @ epoch 3 new loss 1.6996318663586862e-05 old loss 1.7087950254790485e-05 BETTER
I0328 15:13:52.313703 2312955 finetune.py:68] layer 25_o @ epoch 2 new loss 2.0476840290939435e-05 old loss 2.0650722944992594e-05 BETTER
I0328 15:13:57.614045 2313025 finetune.py:68] layer 26_o @ epoch 0 new loss 2.7417228920967318e-05 old loss 2.8374737667036243e-05 BETTER
I0328 15:14:06.601395 2313095 finetune.py:68] layer 27_k @ epoch 4 new loss 1.3690893865714315e-05 old loss 1.3722740732191596e-05 BETTER
I0328 15:14:20.861094 2312885 finetune.py:68] layer 24_o @ epoch 4 new loss 1.6929590856307186e-05 old loss 1.6996318663586862e-05 BETTER
I0328 15:14:25.984916 2312955 finetune.py:68] layer 25_o @ epoch 3 new loss 2.0352566934889182e-05 old loss 2.0476840290939435e-05 BETTER
I0328 15:14:26.454178 2313095 finetune.py:45] layer 27_o initial loss 3.3397882361896336e-05
I0328 15:14:31.572365 2313025 finetune.py:68] layer 26_o @ epoch 1 new loss 2.702533674892038e-05 old loss 2.7417228920967318e-05 BETTER
I0328 15:14:52.387012 2312885 finetune.py:45] layer 24_up initial loss 5.4185304179554805e-05
I0328 15:14:58.603745 2313095 finetune.py:68] layer 27_o @ epoch 0 new loss 3.244982872274704e-05 old loss 3.3397882361896336e-05 BETTER
I0328 15:14:59.833548 2312955 finetune.py:68] layer 25_o @ epoch 4 new loss 2.0286028302507475e-05 old loss 2.0352566934889182e-05 BETTER
I0328 15:15:05.461108 2313025 finetune.py:68] layer 26_o @ epoch 2 new loss 2.677477095858194e-05 old loss 2.702533674892038e-05 BETTER
I0328 15:15:24.260972 2312885 finetune.py:68] layer 24_up @ epoch 0 new loss 5.337462789611891e-05 old loss 5.4185304179554805e-05 BETTER
I0328 15:15:31.717143 2312955 finetune.py:45] layer 25_up initial loss 6.214529275894165e-05
I0328 15:15:31.865485 2313095 finetune.py:68] layer 27_o @ epoch 1 new loss 3.199787170160562e-05 old loss 3.244982872274704e-05 BETTER
I0328 15:15:39.623161 2313025 finetune.py:68] layer 26_o @ epoch 3 new loss 2.6584155421005562e-05 old loss 2.677477095858194e-05 BETTER
I0328 15:15:57.785485 2312885 finetune.py:68] layer 24_up @ epoch 1 new loss 5.2811028581345454e-05 old loss 5.337462789611891e-05 BETTER
I0328 15:16:02.047253 2312955 finetune.py:68] layer 25_up @ epoch 0 new loss 6.120435864431784e-05 old loss 6.214529275894165e-05 BETTER
I0328 15:16:05.393566 2313095 finetune.py:68] layer 27_o @ epoch 2 new loss 3.172431388520636e-05 old loss 3.199787170160562e-05 BETTER
I0328 15:16:13.828989 2313025 finetune.py:68] layer 26_o @ epoch 4 new loss 2.64380760199856e-05 old loss 2.6584155421005562e-05 BETTER
I0328 15:16:31.229132 2312885 finetune.py:68] layer 24_up @ epoch 2 new loss 5.235819480731152e-05 old loss 5.2811028581345454e-05 BETTER
I0328 15:16:33.737714 2312955 finetune.py:68] layer 25_up @ epoch 1 new loss 6.057842620066367e-05 old loss 6.120435864431784e-05 BETTER
I0328 15:16:38.901634 2313095 finetune.py:68] layer 27_o @ epoch 3 new loss 3.148716496070847e-05 old loss 3.172431388520636e-05 BETTER
I0328 15:16:46.153048 2313025 finetune.py:45] layer 26_up initial loss 7.541646482422948e-05
I0328 15:17:04.854013 2312885 finetune.py:68] layer 24_up @ epoch 3 new loss 5.196885467739776e-05 old loss 5.235819480731152e-05 BETTER
I0328 15:17:05.496480 2312955 finetune.py:68] layer 25_up @ epoch 2 new loss 6.008238415233791e-05 old loss 6.057842620066367e-05 BETTER
I0328 15:17:12.302562 2313095 finetune.py:68] layer 27_o @ epoch 4 new loss 3.132499477942474e-05 old loss 3.148716496070847e-05 BETTER
I0328 15:17:16.887071 2313025 finetune.py:68] layer 26_up @ epoch 0 new loss 7.434336293954402e-05 old loss 7.541646482422948e-05 BETTER
I0328 15:17:37.578295 2312955 finetune.py:68] layer 25_up @ epoch 3 new loss 5.965978925814852e-05 old loss 6.008238415233791e-05 BETTER
I0328 15:17:38.822560 2312885 finetune.py:68] layer 24_up @ epoch 4 new loss 5.1640872698044404e-05 old loss 5.196885467739776e-05 BETTER
I0328 15:17:44.643110 2313095 finetune.py:45] layer 27_up initial loss 8.849040023051202e-05
I0328 15:17:48.791435 2313025 finetune.py:68] layer 26_up @ epoch 1 new loss 7.358199945883825e-05 old loss 7.434336293954402e-05 BETTER
I0328 15:18:09.783671 2312955 finetune.py:68] layer 25_up @ epoch 4 new loss 5.9298050473444164e-05 old loss 5.965978925814852e-05 BETTER
I0328 15:18:10.962842 2312885 finetune.py:45] layer 24_gate initial loss 6.743283302057534e-05
I0328 15:18:14.768853 2313095 finetune.py:68] layer 27_up @ epoch 0 new loss 8.715516742086038e-05 old loss 8.849040023051202e-05 BETTER
I0328 15:18:20.957643 2313025 finetune.py:68] layer 26_up @ epoch 2 new loss 7.297404226846993e-05 old loss 7.358199945883825e-05 BETTER
I0328 15:18:40.841644 2312885 finetune.py:68] layer 24_gate @ epoch 0 new loss 6.708825094392523e-05 old loss 6.743283302057534e-05 BETTER
I0328 15:18:41.630143 2312955 finetune.py:45] layer 25_gate initial loss 7.759829168207943e-05
I0328 15:18:46.135649 2313095 finetune.py:68] layer 27_up @ epoch 1 new loss 8.62091692397371e-05 old loss 8.715516742086038e-05 BETTER
I0328 15:18:53.280248 2313025 finetune.py:68] layer 26_up @ epoch 3 new loss 7.245571759995073e-05 old loss 7.297404226846993e-05 BETTER
I0328 15:19:10.175711 2312955 finetune.py:68] layer 25_gate @ epoch 0 new loss 7.722526788711548e-05 old loss 7.759829168207943e-05 BETTER
I0328 15:19:12.246915 2312885 finetune.py:68] layer 24_gate @ epoch 1 new loss 6.68033171677962e-05 old loss 6.708825094392523e-05 BETTER
I0328 15:19:17.531319 2313095 finetune.py:68] layer 27_up @ epoch 2 new loss 8.548173354938626e-05 old loss 8.62091692397371e-05 BETTER
I0328 15:19:25.542459 2313025 finetune.py:68] layer 26_up @ epoch 4 new loss 7.20163225196302e-05 old loss 7.245571759995073e-05 BETTER
I0328 15:19:39.844691 2312955 finetune.py:68] layer 25_gate @ epoch 1 new loss 7.69187172409147e-05 old loss 7.722526788711548e-05 BETTER
I0328 15:19:43.617316 2312885 finetune.py:68] layer 24_gate @ epoch 2 new loss 6.656572077190503e-05 old loss 6.68033171677962e-05 BETTER
I0328 15:19:48.839920 2313095 finetune.py:68] layer 27_up @ epoch 3 new loss 8.48510826472193e-05 old loss 8.548173354938626e-05 BETTER
I0328 15:19:57.414386 2313025 finetune.py:45] layer 26_gate initial loss 9.372994827572256e-05
I0328 15:20:09.451445 2312955 finetune.py:68] layer 25_gate @ epoch 2 new loss 7.666075543966144e-05 old loss 7.69187172409147e-05 BETTER
I0328 15:20:15.064082 2312885 finetune.py:68] layer 24_gate @ epoch 3 new loss 6.635008321609348e-05 old loss 6.656572077190503e-05 BETTER
I0328 15:20:20.373544 2313095 finetune.py:68] layer 27_up @ epoch 4 new loss 8.432896720478311e-05 old loss 8.48510826472193e-05 BETTER
I0328 15:20:25.954566 2313025 finetune.py:68] layer 26_gate @ epoch 0 new loss 9.327903535449877e-05 old loss 9.372994827572256e-05 BETTER
I0328 15:20:39.247467 2312955 finetune.py:68] layer 25_gate @ epoch 3 new loss 7.643653952982277e-05 old loss 7.666075543966144e-05 BETTER
I0328 15:20:46.504988 2312885 finetune.py:68] layer 24_gate @ epoch 4 new loss 6.616830069106072e-05 old loss 6.635008321609348e-05 BETTER
I0328 15:20:52.484607 2313095 finetune.py:45] layer 27_gate initial loss 0.00011123785225208849
I0328 15:20:55.522645 2313025 finetune.py:68] layer 26_gate @ epoch 1 new loss 9.291860624216497e-05 old loss 9.327903535449877e-05 BETTER
I0328 15:21:08.990528 2312955 finetune.py:68] layer 25_gate @ epoch 4 new loss 7.624162390129641e-05 old loss 7.643653952982277e-05 BETTER
I0328 15:21:20.629563 2313095 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.00011068040475947782 old loss 0.00011123785225208849 BETTER
I0328 15:21:25.310241 2313025 finetune.py:68] layer 26_gate @ epoch 2 new loss 9.261678496841341e-05 old loss 9.291860624216497e-05 BETTER
I0328 15:21:42.781078 2312885 finetune.py:45] layer 24_down initial loss 0.0001163506749435328
I0328 15:21:49.804999 2313095 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.00011021761019947007 old loss 0.00011068040475947782 BETTER
I0328 15:21:55.188691 2313025 finetune.py:68] layer 26_gate @ epoch 3 new loss 9.234096069121733e-05 old loss 9.261678496841341e-05 BETTER
I0328 15:22:06.350095 2312955 finetune.py:45] layer 25_down initial loss 0.00013047605170868337
I0328 15:22:10.082769 2312885 finetune.py:68] layer 24_down @ epoch 0 new loss 0.00011634869588306174 old loss 0.0001163506749435328 BETTER
I0328 15:22:19.194918 2313095 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.00010984396794810891 old loss 0.00011021761019947007 BETTER
I0328 15:22:25.208692 2313025 finetune.py:68] layer 26_gate @ epoch 4 new loss 9.211218275595456e-05 old loss 9.234096069121733e-05 BETTER
I0328 15:22:32.529039 2312955 finetune.py:68] layer 25_down @ epoch 0 new loss 0.00013047004176769406 old loss 0.00013047605170868337 BETTER
I0328 15:22:38.756645 2312885 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0001163468841696158 old loss 0.00011634869588306174 BETTER
I0328 15:22:48.647407 2313095 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.00010951259901048616 old loss 0.00010984396794810891 BETTER
I0328 15:22:59.661911 2312955 finetune.py:68] layer 25_down @ epoch 1 new loss 0.000130466214613989 old loss 0.00013047004176769406 BETTER
I0328 15:23:07.530037 2312885 finetune.py:68] layer 24_down @ epoch 2 new loss 0.00011634589463938028 old loss 0.0001163468841696158 BETTER
I0328 15:23:18.279367 2313095 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.00010922748333541676 old loss 0.00010951259901048616 BETTER
I0328 15:23:22.613445 2313025 finetune.py:45] layer 26_down initial loss 0.00015438934497069567
I0328 15:23:27.174882 2312955 finetune.py:68] layer 25_down @ epoch 2 new loss 0.00013046360982116312 old loss 0.000130466214613989 BETTER
I0328 15:23:36.565549 2312885 finetune.py:68] layer 24_down @ epoch 3 new loss 0.00011634448310360312 old loss 0.00011634589463938028 BETTER
I0328 15:23:48.826944 2313025 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0001543879770906642 old loss 0.00015438934497069567 BETTER
I0328 15:23:54.811502 2312955 finetune.py:68] layer 25_down @ epoch 3 new loss 0.00013046135427430272 old loss 0.00013046360982116312 BETTER
I0328 15:24:05.870046 2312885 finetune.py:68] layer 24_down @ epoch 4 new loss 0.00011634341353783384 old loss 0.00011634448310360312 BETTER
24_v proxy err 0.004832085222005844 tr(WHW.T) 467.2783508300781
bpp_loss 3.8371447435347363
24_q proxy err 0.0004249176708981395 tr(WHW.T) 22446.91796875
bpp_loss 4.340841807308607
24_k proxy err 0.0001931328879436478 tr(WHW.T) 14181.3046875
bpp_loss 5.149743964313529
24_o proxy err 0.005755122285336256 tr(WHW.T) 1592.092041015625
bpp_loss 3.7593415048904717
24_up proxy err 0.006018816493451595 tr(WHW.T) 7311.78515625
bpp_loss 3.7528072276950946
24_gate proxy err 0.0017423996469005942 tr(WHW.T) 25863.423828125
bpp_loss 4.1497188678144346
24_down proxy err 0.006063112989068031 tr(WHW.T) 6756.56396484375
bpp_loss 3.750133102743088
I0328 15:24:16.156433 2313095 finetune.py:45] layer 27_down initial loss 0.00018197558529209346
I0328 15:24:16.623780 2313025 finetune.py:68] layer 26_down @ epoch 1 new loss 0.00015438701666425914 old loss 0.0001543879770906642 BETTER
I0328 15:24:22.863462 2312955 finetune.py:68] layer 25_down @ epoch 4 new loss 0.00013045953528489918 old loss 0.00013046135427430272 BETTER
25_v proxy err 0.004151288885623217 tr(WHW.T) 557.8086547851562
bpp_loss 3.8477118847658858
25_q proxy err 0.0003722110704984516 tr(WHW.T) 26120.865234375
bpp_loss 4.318128317128867
25_k proxy err 0.00018929109501186758 tr(WHW.T) 14411.9033203125
bpp_loss 5.131288651842624
25_o proxy err 0.004822381306439638 tr(WHW.T) 1990.302734375
bpp_loss 3.7593866752577014
25_up proxy err 0.005951377097517252 tr(WHW.T) 7386.3759765625
bpp_loss 3.7622951571164385
25_gate proxy err 0.0017111642519012094 tr(WHW.T) 26303.267578125
bpp_loss 4.1594309240047425
25_down proxy err 0.0060427808202803135 tr(WHW.T) 6630.7255859375
bpp_loss 3.759272104611487
I0328 15:24:42.146240 2313095 finetune.py:68] layer 27_down @ epoch 0 new loss 0.000181970841367729 old loss 0.00018197558529209346 BETTER
I0328 15:24:44.207738 2313025 finetune.py:68] layer 26_down @ epoch 2 new loss 0.00015438560512848198 old loss 0.00015438701666425914 BETTER
I0328 15:25:09.041472 2313095 finetune.py:68] layer 27_down @ epoch 1 new loss 0.00018196803284808993 old loss 0.000181970841367729 BETTER
I0328 15:25:11.973624 2313025 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0001543844846310094 old loss 0.00015438560512848198 BETTER
I0328 15:25:33.898196 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 66.16164302825928s
I0328 15:25:36.314007 2313095 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0001819659082684666 old loss 0.00018196803284808993 BETTER
I0328 15:25:37.880800 2313165 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:25:37.880915 2313165 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:25:37.880959 2313165 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:25:38.233258 2313165 config.py:54] PyTorch version 2.6.0 available.
W0328 15:25:38.434400 2313165 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:25:39.044872 2313165 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 15:25:39.048925 2311068 quantize_finetune_llama.py:209] layer 29 gpu 1
I0328 15:25:39.062487 2313165 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 15:25:39.819631 2313025 finetune.py:68] layer 26_down @ epoch 4 new loss 0.00015438349510077387 old loss 0.0001543844846310094 BETTER
26_v proxy err 0.005061013624072075 tr(WHW.T) 434.54583740234375
bpp_loss 3.8994429325684905
26_q proxy err 0.0004337962600402534 tr(WHW.T) 21407.33203125
bpp_loss 4.328754570800811
26_k proxy err 0.00017539483087603003 tr(WHW.T) 15420.7900390625
bpp_loss 5.21631745446939
26_o proxy err 0.003462847089394927 tr(WHW.T) 2388.557861328125
bpp_loss 3.776832697505597
26_up proxy err 0.005748309660702944 tr(WHW.T) 7649.64501953125
bpp_loss 3.771681724049683
26_gate proxy err 0.00155834446195513 tr(WHW.T) 28898.408203125
bpp_loss 4.166989270065512
26_down proxy err 0.006042078137397766 tr(WHW.T) 6633.2724609375
bpp_loss 3.7671820872430026
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:25:55.672728 2313165 finetune.py:45] layer 28_v initial loss 4.048019400215708e-05
W0328 15:25:55.673128 2313165 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 15:26:03.546761 2313095 finetune.py:68] layer 27_down @ epoch 3 new loss 0.00018196362361777574 old loss 0.0001819659082684666 BETTER
I0328 15:26:30.689551 2313165 finetune.py:68] layer 28_v @ epoch 0 new loss 1.4391373952094e-05 old loss 4.048019400215708e-05 BETTER
I0328 15:26:30.860095 2313095 finetune.py:68] layer 27_down @ epoch 4 new loss 0.000181961600901559 old loss 0.00018196362361777574 BETTER
27_v proxy err 0.0034907013177871704 tr(WHW.T) 677.69384765625
bpp_loss 3.9927002968615852
27_q proxy err 0.0004601315886247903 tr(WHW.T) 21309.80078125
bpp_loss 4.297273161122575
27_k proxy err 0.00020147865870967507 tr(WHW.T) 14010.33203125
bpp_loss 5.168578892014921
27_o proxy err 0.004133392591029406 tr(WHW.T) 2160.751708984375
bpp_loss 3.8177425833418965
27_up proxy err 0.005233301315456629 tr(WHW.T) 8481.896484375
bpp_loss 3.787467904987612
27_gate proxy err 0.0013846446527168155 tr(WHW.T) 32819.5
bpp_loss 4.17879185818934
27_down proxy err 0.005017580930143595 tr(WHW.T) 6561.8525390625
bpp_loss 3.7784793513128534
I0328 15:26:47.477289 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 63.20621418952942s
I0328 15:26:51.228847 2313235 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:26:51.228946 2313235 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:26:51.228990 2313235 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:26:51.561900 2313235 config.py:54] PyTorch version 2.6.0 available.
W0328 15:26:51.762440 2313235 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:26:52.402894 2313235 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 15:26:52.406793 2311068 quantize_finetune_llama.py:209] layer 30 gpu 2
I0328 15:26:52.419937 2313235 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:27:07.142647 2313165 finetune.py:68] layer 28_v @ epoch 1 new loss 1.36092176035163e-05 old loss 1.4391373952094e-05 BETTER
I0328 15:27:09.350413 2313235 finetune.py:45] layer 29_v initial loss 4.946070475853048e-05
W0328 15:27:09.350612 2313235 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 15:27:42.690728 2313235 finetune.py:68] layer 29_v @ epoch 0 new loss 1.657852226344403e-05 old loss 4.946070475853048e-05 BETTER
I0328 15:27:44.058217 2313165 finetune.py:68] layer 28_v @ epoch 2 new loss 1.3389616469794419e-05 old loss 1.36092176035163e-05 BETTER
I0328 15:27:56.149992 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 63.284170150756836s
I0328 15:27:59.935966 2313305 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:27:59.936067 2313305 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:27:59.936110 2313305 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:28:00.304126 2313305 config.py:54] PyTorch version 2.6.0 available.
W0328 15:28:00.530566 2313305 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:28:02.492352 2313305 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 15:28:02.496316 2311068 quantize_finetune_llama.py:209] layer 31 gpu 3
I0328 15:28:02.510480 2313305 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:28:17.453721 2313235 finetune.py:68] layer 29_v @ epoch 1 new loss 1.5984665878931992e-05 old loss 1.657852226344403e-05 BETTER
I0328 15:28:20.365052 2313305 finetune.py:45] layer 30_v initial loss 5.428344229585491e-05
W0328 15:28:20.365403 2313305 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 15:28:21.130162 2313165 finetune.py:68] layer 28_v @ epoch 3 new loss 1.3136516827216838e-05 old loss 1.3389616469794419e-05 BETTER
I0328 15:28:52.353965 2313235 finetune.py:68] layer 29_v @ epoch 2 new loss 1.589923704159446e-05 old loss 1.5984665878931992e-05 BETTER
I0328 15:28:54.012849 2313305 finetune.py:68] layer 30_v @ epoch 0 new loss 2.80620988633018e-05 old loss 5.428344229585491e-05 BETTER
I0328 15:28:58.219706 2313165 finetune.py:76] layer 28_v @ epoch 4 new loss 1.3203492017055396e-05 old loss 1.3136516827216838e-05 WORSE
I0328 15:28:59.614385 2311068 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 56.64853835105896s
I0328 15:29:03.319872 2313375 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:29:03.319967 2313375 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:29:03.320004 2313375 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:29:03.697731 2313375 config.py:54] PyTorch version 2.6.0 available.
W0328 15:29:03.914912 2313375 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:29:04.729382 2313375 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 15:29:04.747561 2313375 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 15:29:16.898058 2313165 finetune.py:45] layer 28_q initial loss 1.712755511107389e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:29:22.411778 2313375 finetune.py:45] layer 31_v initial loss 8.057078957790509e-05
W0328 15:29:22.411990 2313375 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 15:29:27.402448 2313235 finetune.py:76] layer 29_v @ epoch 3 new loss 1.648506076890044e-05 old loss 1.589923704159446e-05 WORSE
I0328 15:29:28.695437 2313305 finetune.py:68] layer 30_v @ epoch 1 new loss 2.6645704565453343e-05 old loss 2.80620988633018e-05 BETTER
I0328 15:29:52.368983 2313165 finetune.py:68] layer 28_q @ epoch 0 new loss 1.6559353753109463e-05 old loss 1.712755511107389e-05 BETTER
I0328 15:29:55.329773 2313375 finetune.py:68] layer 31_v @ epoch 0 new loss 5.43884962098673e-05 old loss 8.057078957790509e-05 BETTER
I0328 15:30:01.873438 2313235 finetune.py:68] layer 29_v @ epoch 4 new loss 1.5726191122666933e-05 old loss 1.589923704159446e-05 BETTER
I0328 15:30:03.577148 2313305 finetune.py:76] layer 30_v @ epoch 2 new loss 2.6889278160524555e-05 old loss 2.6645704565453343e-05 WORSE
I0328 15:30:21.462681 2313235 finetune.py:45] layer 29_q initial loss 2.6484784029889852e-05
I0328 15:30:28.885780 2313165 finetune.py:68] layer 28_q @ epoch 1 new loss 1.6241461707977578e-05 old loss 1.6559353753109463e-05 BETTER
I0328 15:30:29.665226 2313375 finetune.py:76] layer 31_v @ epoch 1 new loss 0.00011715231084963307 old loss 5.43884962098673e-05 WORSE
I0328 15:30:38.011737 2313305 finetune.py:76] layer 30_v @ epoch 3 new loss 2.7736450647353195e-05 old loss 2.6645704565453343e-05 WORSE
I0328 15:30:54.907369 2313235 finetune.py:68] layer 29_q @ epoch 0 new loss 2.466356636432465e-05 old loss 2.6484784029889852e-05 BETTER
I0328 15:31:03.500153 2313375 finetune.py:76] layer 31_v @ epoch 2 new loss 8.89725997694768e-05 old loss 5.43884962098673e-05 WORSE
I0328 15:31:05.561289 2313165 finetune.py:68] layer 28_q @ epoch 2 new loss 1.60185281856684e-05 old loss 1.6241461707977578e-05 BETTER
I0328 15:31:12.459663 2313305 finetune.py:68] layer 30_v @ epoch 4 new loss 2.6083571356139146e-05 old loss 2.6645704565453343e-05 BETTER
I0328 15:31:29.415957 2313235 finetune.py:68] layer 29_q @ epoch 1 new loss 2.3777502065058798e-05 old loss 2.466356636432465e-05 BETTER
I0328 15:31:32.339172 2313305 finetune.py:45] layer 30_q initial loss 3.322631528135389e-05
I0328 15:31:37.269131 2313375 finetune.py:76] layer 31_v @ epoch 3 new loss 8.30503850011155e-05 old loss 5.43884962098673e-05 WORSE
I0328 15:31:42.287640 2313165 finetune.py:68] layer 28_q @ epoch 3 new loss 1.5824552974663675e-05 old loss 1.60185281856684e-05 BETTER
I0328 15:32:04.016270 2313235 finetune.py:68] layer 29_q @ epoch 2 new loss 2.332741678401362e-05 old loss 2.3777502065058798e-05 BETTER
I0328 15:32:06.316708 2313305 finetune.py:68] layer 30_q @ epoch 0 new loss 3.213485615560785e-05 old loss 3.322631528135389e-05 BETTER
I0328 15:32:11.416386 2313375 finetune.py:76] layer 31_v @ epoch 4 new loss 6.887885683681816e-05 old loss 5.43884962098673e-05 WORSE
I0328 15:32:19.125444 2313165 finetune.py:68] layer 28_q @ epoch 4 new loss 1.5791778423590586e-05 old loss 1.5824552974663675e-05 BETTER
I0328 15:32:30.377528 2313375 finetune.py:45] layer 31_q initial loss 7.562113751191646e-05
I0328 15:32:37.038689 2313165 finetune.py:45] layer 28_k initial loss 1.7673337424639612e-05
I0328 15:32:38.591309 2313235 finetune.py:68] layer 29_q @ epoch 3 new loss 2.2832962713437155e-05 old loss 2.332741678401362e-05 BETTER
I0328 15:32:41.127145 2313305 finetune.py:68] layer 30_q @ epoch 1 new loss 3.099957393715158e-05 old loss 3.213485615560785e-05 BETTER
I0328 15:33:03.614223 2313375 finetune.py:76] layer 31_q @ epoch 0 new loss 8.196009730454534e-05 old loss 7.562113751191646e-05 WORSE
I0328 15:33:12.628703 2313165 finetune.py:68] layer 28_k @ epoch 0 new loss 1.7395841496181674e-05 old loss 1.7673337424639612e-05 BETTER
I0328 15:33:13.367805 2313235 finetune.py:68] layer 29_q @ epoch 4 new loss 2.269999822601676e-05 old loss 2.2832962713437155e-05 BETTER
I0328 15:33:15.869023 2313305 finetune.py:68] layer 30_q @ epoch 2 new loss 3.067662692046724e-05 old loss 3.099957393715158e-05 BETTER
I0328 15:33:31.483907 2313235 finetune.py:45] layer 29_k initial loss 2.573206802480854e-05
I0328 15:33:37.102238 2313375 finetune.py:68] layer 31_q @ epoch 1 new loss 7.12547916918993e-05 old loss 7.562113751191646e-05 BETTER
I0328 15:33:49.122596 2313165 finetune.py:68] layer 28_k @ epoch 1 new loss 1.72264644788811e-05 old loss 1.7395841496181674e-05 BETTER
I0328 15:33:50.702619 2313305 finetune.py:68] layer 30_q @ epoch 3 new loss 3.0478846383630298e-05 old loss 3.067662692046724e-05 BETTER
I0328 15:34:05.037934 2313235 finetune.py:68] layer 29_k @ epoch 0 new loss 2.506011151126586e-05 old loss 2.573206802480854e-05 BETTER
I0328 15:34:11.360263 2313375 finetune.py:68] layer 31_q @ epoch 2 new loss 6.142124038888142e-05 old loss 7.12547916918993e-05 BETTER
I0328 15:34:25.538833 2313305 finetune.py:76] layer 30_q @ epoch 4 new loss 3.062918767682277e-05 old loss 3.0478846383630298e-05 WORSE
I0328 15:34:25.741937 2313165 finetune.py:68] layer 28_k @ epoch 2 new loss 1.7175245375256054e-05 old loss 1.72264644788811e-05 BETTER
I0328 15:34:39.190509 2313235 finetune.py:68] layer 29_k @ epoch 1 new loss 2.4788942027953453e-05 old loss 2.506011151126586e-05 BETTER
I0328 15:34:43.032655 2313305 finetune.py:45] layer 30_k initial loss 3.520520476740785e-05
I0328 15:34:45.585678 2313375 finetune.py:76] layer 31_q @ epoch 3 new loss 6.472768291132525e-05 old loss 6.142124038888142e-05 WORSE
I0328 15:35:02.383760 2313165 finetune.py:68] layer 28_k @ epoch 3 new loss 1.7046835637302138e-05 old loss 1.7175245375256054e-05 BETTER
I0328 15:35:13.475536 2313235 finetune.py:76] layer 29_k @ epoch 2 new loss 2.479098657204304e-05 old loss 2.4788942027953453e-05 WORSE
I0328 15:35:16.684669 2313305 finetune.py:76] layer 30_k @ epoch 0 new loss 3.5274431866128e-05 old loss 3.520520476740785e-05 WORSE
I0328 15:35:19.277534 2313375 finetune.py:76] layer 31_q @ epoch 4 new loss 6.522498006233945e-05 old loss 6.142124038888142e-05 WORSE
I0328 15:35:36.488620 2313375 finetune.py:45] layer 31_k initial loss 7.288429333129898e-05
I0328 15:35:39.199834 2313165 finetune.py:76] layer 28_k @ epoch 4 new loss 1.705849717836827e-05 old loss 1.7046835637302138e-05 WORSE
I0328 15:35:47.268922 2313235 finetune.py:68] layer 29_k @ epoch 3 new loss 2.465439320076257e-05 old loss 2.4788942027953453e-05 BETTER
I0328 15:35:50.391434 2313305 finetune.py:76] layer 30_k @ epoch 1 new loss 3.6166500649414957e-05 old loss 3.520520476740785e-05 WORSE
I0328 15:35:58.348722 2313165 finetune.py:45] layer 28_o initial loss 4.2821757233468816e-05
I0328 15:36:09.501857 2313375 finetune.py:68] layer 31_k @ epoch 0 new loss 6.783559365430847e-05 old loss 7.288429333129898e-05 BETTER
I0328 15:36:21.779882 2313235 finetune.py:76] layer 29_k @ epoch 4 new loss 2.4830629627103917e-05 old loss 2.465439320076257e-05 WORSE
I0328 15:36:24.596846 2313305 finetune.py:76] layer 30_k @ epoch 2 new loss 3.690456651384011e-05 old loss 3.520520476740785e-05 WORSE
I0328 15:36:33.205283 2313165 finetune.py:68] layer 28_o @ epoch 0 new loss 4.146323772147298e-05 old loss 4.2821757233468816e-05 BETTER
I0328 15:36:41.266078 2313235 finetune.py:45] layer 29_o initial loss 5.0506441766628996e-05
I0328 15:36:43.396274 2313375 finetune.py:68] layer 31_k @ epoch 1 new loss 6.72656751703471e-05 old loss 6.783559365430847e-05 BETTER
I0328 15:36:58.510494 2313305 finetune.py:68] layer 30_k @ epoch 3 new loss 3.5049823054578155e-05 old loss 3.520520476740785e-05 BETTER
I0328 15:37:08.799324 2313165 finetune.py:68] layer 28_o @ epoch 1 new loss 4.063600499648601e-05 old loss 4.146323772147298e-05 BETTER
I0328 15:37:14.178558 2313235 finetune.py:68] layer 29_o @ epoch 0 new loss 4.879628249909729e-05 old loss 5.0506441766628996e-05 BETTER
I0328 15:37:17.439856 2313375 finetune.py:68] layer 31_k @ epoch 2 new loss 6.440407014451921e-05 old loss 6.72656751703471e-05 BETTER
I0328 15:37:33.102112 2313305 finetune.py:68] layer 30_k @ epoch 4 new loss 3.491873212624341e-05 old loss 3.5049823054578155e-05 BETTER
I0328 15:37:44.744269 2313165 finetune.py:68] layer 28_o @ epoch 2 new loss 4.01435827370733e-05 old loss 4.063600499648601e-05 BETTER
I0328 15:37:47.795021 2313235 finetune.py:68] layer 29_o @ epoch 1 new loss 4.7971705498639494e-05 old loss 4.879628249909729e-05 BETTER
I0328 15:37:51.712587 2313375 finetune.py:76] layer 31_k @ epoch 3 new loss 6.592658610315993e-05 old loss 6.440407014451921e-05 WORSE
I0328 15:37:52.992131 2313305 finetune.py:45] layer 30_o initial loss 7.767769420752302e-05
I0328 15:38:20.785948 2313165 finetune.py:68] layer 28_o @ epoch 3 new loss 3.982458292739466e-05 old loss 4.01435827370733e-05 BETTER
I0328 15:38:21.516830 2313235 finetune.py:68] layer 29_o @ epoch 2 new loss 4.744690522784367e-05 old loss 4.7971705498639494e-05 BETTER
I0328 15:38:25.122651 2313375 finetune.py:76] layer 31_k @ epoch 4 new loss 6.683280662400648e-05 old loss 6.440407014451921e-05 WORSE
I0328 15:38:25.846495 2313305 finetune.py:68] layer 30_o @ epoch 0 new loss 7.580684177810326e-05 old loss 7.767769420752302e-05 BETTER
I0328 15:38:44.048914 2313375 finetune.py:45] layer 31_o initial loss 0.0001527014683233574
I0328 15:38:55.509875 2313235 finetune.py:68] layer 29_o @ epoch 3 new loss 4.7139266825979576e-05 old loss 4.744690522784367e-05 BETTER
I0328 15:38:56.943770 2313165 finetune.py:68] layer 28_o @ epoch 4 new loss 3.952943370677531e-05 old loss 3.982458292739466e-05 BETTER
I0328 15:38:59.747862 2313305 finetune.py:68] layer 30_o @ epoch 1 new loss 7.40166287869215e-05 old loss 7.580684177810326e-05 BETTER
I0328 15:39:16.243201 2313375 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0001351867977064103 old loss 0.0001527014683233574 BETTER
I0328 15:39:28.828482 2313165 finetune.py:45] layer 28_up initial loss 0.00011070878099417314
I0328 15:39:29.381833 2313235 finetune.py:68] layer 29_o @ epoch 4 new loss 4.681272184825502e-05 old loss 4.7139266825979576e-05 BETTER
I0328 15:39:33.923083 2313305 finetune.py:68] layer 30_o @ epoch 2 new loss 7.334745896514505e-05 old loss 7.40166287869215e-05 BETTER
I0328 15:39:49.715301 2313375 finetune.py:68] layer 31_o @ epoch 1 new loss 0.00012879357382189482 old loss 0.0001351867977064103 BETTER
I0328 15:40:00.664521 2313165 finetune.py:68] layer 28_up @ epoch 0 new loss 0.00010884725634241477 old loss 0.00011070878099417314 BETTER
I0328 15:40:01.520983 2313235 finetune.py:45] layer 29_up initial loss 0.00014501236728392541
I0328 15:40:07.916839 2313305 finetune.py:68] layer 30_o @ epoch 3 new loss 7.298332639038563e-05 old loss 7.334745896514505e-05 BETTER
I0328 15:40:23.104266 2313375 finetune.py:68] layer 31_o @ epoch 2 new loss 0.00012554138083942235 old loss 0.00012879357382189482 BETTER
I0328 15:40:32.097478 2313235 finetune.py:68] layer 29_up @ epoch 0 new loss 0.00014188975910656154 old loss 0.00014501236728392541 BETTER
I0328 15:40:33.931608 2313165 finetune.py:68] layer 28_up @ epoch 1 new loss 0.00010746100451797247 old loss 0.00010884725634241477 BETTER
I0328 15:40:41.808149 2313305 finetune.py:68] layer 30_o @ epoch 4 new loss 7.243191066663712e-05 old loss 7.298332639038563e-05 BETTER
I0328 15:40:56.395546 2313375 finetune.py:68] layer 31_o @ epoch 3 new loss 0.00012473479728214443 old loss 0.00012554138083942235 BETTER
I0328 15:41:03.791194 2313235 finetune.py:68] layer 29_up @ epoch 1 new loss 0.00013998350186739117 old loss 0.00014188975910656154 BETTER
I0328 15:41:07.270254 2313165 finetune.py:68] layer 28_up @ epoch 2 new loss 0.00010647871386026964 old loss 0.00010746100451797247 BETTER
I0328 15:41:13.383844 2313305 finetune.py:45] layer 30_up initial loss 0.0002661092148628086
I0328 15:41:29.848315 2313375 finetune.py:68] layer 31_o @ epoch 4 new loss 0.00012260733637958765 old loss 0.00012473479728214443 BETTER
I0328 15:41:35.579668 2313235 finetune.py:68] layer 29_up @ epoch 2 new loss 0.000138490300741978 old loss 0.00013998350186739117 BETTER
I0328 15:41:40.855792 2313165 finetune.py:68] layer 28_up @ epoch 3 new loss 0.00010563969408394769 old loss 0.00010647871386026964 BETTER
I0328 15:41:44.018482 2313305 finetune.py:68] layer 30_up @ epoch 0 new loss 0.00025754724629223347 old loss 0.0002661092148628086 BETTER
I0328 15:42:01.506975 2313375 finetune.py:45] layer 31_up initial loss 0.0007756950217299163
I0328 15:42:07.554826 2313235 finetune.py:68] layer 29_up @ epoch 3 new loss 0.00013720804417971522 old loss 0.000138490300741978 BETTER
I0328 15:42:14.443663 2313165 finetune.py:68] layer 28_up @ epoch 4 new loss 0.0001049338752636686 old loss 0.00010563969408394769 BETTER
I0328 15:42:16.005258 2313305 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0002520493289921433 old loss 0.00025754724629223347 BETTER
I0328 15:42:31.663590 2313375 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0007183753768913448 old loss 0.0007756950217299163 BETTER
I0328 15:42:39.519781 2313235 finetune.py:68] layer 29_up @ epoch 4 new loss 0.00013619210221804678 old loss 0.00013720804417971522 BETTER
I0328 15:42:46.131389 2313165 finetune.py:45] layer 28_gate initial loss 0.0001397703745169565
I0328 15:42:47.988412 2313305 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0002477033995091915 old loss 0.0002520493289921433 BETTER
I0328 15:43:02.957680 2313375 finetune.py:68] layer 31_up @ epoch 1 new loss 0.0006862590671516955 old loss 0.0007183753768913448 BETTER
I0328 15:43:10.942384 2313235 finetune.py:45] layer 29_gate initial loss 0.00018267503764946014
I0328 15:43:16.260990 2313165 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0001389917597407475 old loss 0.0001397703745169565 BETTER
I0328 15:43:20.192207 2313305 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0002440125826979056 old loss 0.0002477033995091915 BETTER
I0328 15:43:34.380882 2313375 finetune.py:68] layer 31_up @ epoch 2 new loss 0.0006600443739444017 old loss 0.0006862590671516955 BETTER
I0328 15:43:39.263627 2313235 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0001815631694626063 old loss 0.00018267503764946014 BETTER
I0328 15:43:47.628434 2313165 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.00013837104779668152 old loss 0.0001389917597407475 BETTER
I0328 15:43:52.442447 2313305 finetune.py:68] layer 30_up @ epoch 4 new loss 0.00024090275110211223 old loss 0.0002440125826979056 BETTER
I0328 15:44:05.948712 2313375 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0006375286029651761 old loss 0.0006600443739444017 BETTER
I0328 15:44:08.792670 2313235 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.000180718008778058 old loss 0.0001815631694626063 BETTER
I0328 15:44:19.028069 2313165 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.00013781979214400053 old loss 0.00013837104779668152 BETTER
I0328 15:44:24.005558 2313305 finetune.py:45] layer 30_gate initial loss 0.00030801541288383305
I0328 15:44:37.667794 2313375 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0006175613962113857 old loss 0.0006375286029651761 BETTER
I0328 15:44:38.388845 2313235 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.00018000027921516448 old loss 0.000180718008778058 BETTER
I0328 15:44:50.586431 2313165 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.00013738745474256575 old loss 0.00013781979214400053 BETTER
I0328 15:44:52.651301 2313305 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0003048923972528428 old loss 0.00030801541288383305 BETTER
I0328 15:45:08.024832 2313235 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.00017939519602805376 old loss 0.00018000027921516448 BETTER
I0328 15:45:09.168210 2313375 finetune.py:45] layer 31_gate initial loss 0.0007439674809575081
I0328 15:45:22.166379 2313165 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.0001369841629639268 old loss 0.00013738745474256575 BETTER
I0328 15:45:22.293927 2313305 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.000302539934637025 old loss 0.0003048923972528428 BETTER
I0328 15:45:37.507924 2313375 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.0007268699118867517 old loss 0.0007439674809575081 BETTER
I0328 15:45:37.725133 2313235 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.00017890229355543852 old loss 0.00017939519602805376 BETTER
I0328 15:45:52.121802 2313305 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.00030050508212298155 old loss 0.000302539934637025 BETTER
I0328 15:46:06.679889 2313375 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.0007138944929465652 old loss 0.0007268699118867517 BETTER
I0328 15:46:17.349819 2313165 finetune.py:45] layer 28_down initial loss 0.00022952583094593138
I0328 15:46:21.962350 2313305 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.00029878722853027284 old loss 0.00030050508212298155 BETTER
I0328 15:46:33.737393 2313235 finetune.py:45] layer 29_down initial loss 0.00030205328948795795
I0328 15:46:35.970575 2313375 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.0007029032567515969 old loss 0.0007138944929465652 BETTER
I0328 15:46:44.769201 2313165 finetune.py:68] layer 28_down @ epoch 0 new loss 0.00022952165454626083 old loss 0.00022952583094593138 BETTER
I0328 15:46:51.763513 2313305 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0002972094516735524 old loss 0.00029878722853027284 BETTER
I0328 15:46:59.895498 2313235 finetune.py:68] layer 29_down @ epoch 0 new loss 0.0003020449075847864 old loss 0.00030205328948795795 BETTER
I0328 15:47:05.422763 2313375 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.0006928804214112461 old loss 0.0007029032567515969 BETTER
I0328 15:47:13.551812 2313165 finetune.py:68] layer 28_down @ epoch 1 new loss 0.00022951867140363902 old loss 0.00022952165454626083 BETTER
I0328 15:47:27.074328 2313235 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0003020379226654768 old loss 0.0003020449075847864 BETTER
I0328 15:47:34.760431 2313375 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.0006838455446995795 old loss 0.0006928804214112461 BETTER
I0328 15:47:42.615206 2313165 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0002295152225997299 old loss 0.00022951867140363902 BETTER
I0328 15:47:48.605485 2313305 finetune.py:45] layer 30_down initial loss 0.000494659470859915
I0328 15:47:54.525407 2313235 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0003020312578883022 old loss 0.0003020379226654768 BETTER
I0328 15:48:11.481904 2313165 finetune.py:68] layer 28_down @ epoch 3 new loss 0.00022951321443542838 old loss 0.0002295152225997299 BETTER
I0328 15:48:14.861303 2313305 finetune.py:68] layer 30_down @ epoch 0 new loss 0.0004946378176100552 old loss 0.000494659470859915 BETTER
I0328 15:48:22.210170 2313235 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0003020259609911591 old loss 0.0003020312578883022 BETTER
I0328 15:48:31.678404 2313375 finetune.py:45] layer 31_down initial loss 0.0012409259798005223
I0328 15:48:40.630908 2313165 finetune.py:68] layer 28_down @ epoch 4 new loss 0.00022951082792133093 old loss 0.00022951321443542838 BETTER
I0328 15:48:42.496511 2313305 finetune.py:68] layer 30_down @ epoch 1 new loss 0.000494615756906569 old loss 0.0004946378176100552 BETTER
28_v proxy err 0.003847681451588869 tr(WHW.T) 601.4844360351562
bpp_loss 4.047001019760501
28_q proxy err 0.0004143593250773847 tr(WHW.T) 23168.701171875
bpp_loss 4.3067927388474345
28_k proxy err 0.00017869719886220992 tr(WHW.T) 14992.6044921875
bpp_loss 5.119062904966995
28_o proxy err 0.0036011235788464546 tr(WHW.T) 2511.931396484375
bpp_loss 3.847467905259691
28_up proxy err 0.004220475908368826 tr(WHW.T) 10244.041015625
bpp_loss 3.8101533141785433
28_gate proxy err 0.0012298945803195238 tr(WHW.T) 35926.22265625
bpp_loss 4.165252126420715
28_down proxy err 0.004744461737573147 tr(WHW.T) 7225.84716796875
bpp_loss 3.7939566278198202
I0328 15:48:49.888346 2313235 finetune.py:68] layer 29_down @ epoch 4 new loss 0.0003020212461706251 old loss 0.0003020259609911591 BETTER
29_v proxy err 0.0029381222557276487 tr(WHW.T) 850.4290161132812
bpp_loss 4.116949769668281
29_q proxy err 0.0004954937030561268 tr(WHW.T) 20656.9296875
bpp_loss 4.29489380994346
29_k proxy err 0.00017284456407651305 tr(WHW.T) 16358.451171875
bpp_loss 5.213030721177347
29_o proxy err 0.0022686782758682966 tr(WHW.T) 3102.1748046875
bpp_loss 3.884343975223601
29_up proxy err 0.003269998822361231 tr(WHW.T) 12883.1396484375
bpp_loss 3.844146227331034
29_gate proxy err 0.0011193057289347053 tr(WHW.T) 38382.54296875
bpp_loss 4.157193931551384
29_down proxy err 0.003889393759891391 tr(WHW.T) 7490.56298828125
bpp_loss 3.8091556405748372
I0328 15:48:57.743280 2313375 finetune.py:68] layer 31_down @ epoch 0 new loss 0.0012407279573380947 old loss 0.0012409259798005223 BETTER
I0328 15:49:10.161660 2313305 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0004945994005538523 old loss 0.000494615756906569 BETTER
I0328 15:49:24.798289 2313375 finetune.py:68] layer 31_down @ epoch 1 new loss 0.0012405983870849013 old loss 0.0012407279573380947 BETTER
I0328 15:49:37.847261 2313305 finetune.py:68] layer 30_down @ epoch 3 new loss 0.0004945837426930666 old loss 0.0004945994005538523 BETTER
I0328 15:49:51.871330 2313375 finetune.py:68] layer 31_down @ epoch 2 new loss 0.0012404870940372348 old loss 0.0012405983870849013 BETTER
I0328 15:50:05.546429 2313305 finetune.py:68] layer 30_down @ epoch 4 new loss 0.0004945674445480108 old loss 0.0004945837426930666 BETTER
30_v proxy err 0.0026350212283432484 tr(WHW.T) 863.060791015625
bpp_loss 4.408901377930306
30_q proxy err 0.0003821249702014029 tr(WHW.T) 24056.669921875
bpp_loss 4.198484200169332
30_k proxy err 0.00019005044305231422 tr(WHW.T) 14030.478515625
bpp_loss 4.857788401073776
30_o proxy err 0.001621144125238061 tr(WHW.T) 4872.35009765625
bpp_loss 3.972514807479456
30_up proxy err 0.0019296251703053713 tr(WHW.T) 21695.015625
bpp_loss 3.8770768023761257
30_gate proxy err 0.0008237185538746417 tr(WHW.T) 51892.65234375
bpp_loss 4.210173136488136
30_down proxy err 0.002239516004920006 tr(WHW.T) 8822.623046875
bpp_loss 3.810237953655555
I0328 15:50:19.204115 2313375 finetune.py:68] layer 31_down @ epoch 3 new loss 0.0012403883738443255 old loss 0.0012404870940372348 BETTER
I0328 15:50:46.414896 2313375 finetune.py:68] layer 31_down @ epoch 4 new loss 0.0012402976863086224 old loss 0.0012403883738443255 BETTER
31_v proxy err 0.001346766366623342 tr(WHW.T) 1808.723876953125
bpp_loss 4.2226204256294295
31_q proxy err 0.00021482030570041388 tr(WHW.T) 46178.765625
bpp_loss 4.3543213594239205
31_k proxy err 0.000133016292238608 tr(WHW.T) 20468.88671875
bpp_loss 5.102805036818609
31_o proxy err 0.001367418677546084 tr(WHW.T) 2212.325927734375
bpp_loss 3.9329277123324573
31_up proxy err 0.0005848376895301044 tr(WHW.T) 69010.859375
bpp_loss 4.076920081462179
31_gate proxy err 0.0002867263974621892 tr(WHW.T) 143985.078125
bpp_loss 4.434300917001175
31_down proxy err 0.0008786945254541934 tr(WHW.T) 9971.6337890625
bpp_loss 3.837777582113631
I0328 15:51:11.703102 2313445 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:51:11.703241 2313445 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:51:11.703287 2313445 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:51:12.040439 2313445 config.py:54] PyTorch version 2.6.0 available.
W0328 15:51:12.257195 2313445 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 15:51:12.371083 2313445 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.86it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.53it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.61it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.81it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.96it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.79it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  9.21it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.88it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.99it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.66it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.69it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.83it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.08it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.93it/s]
I0328 15:51:15.227318 2313445 hfize_llama.py:153] loaded layer 0
I0328 15:51:15.742064 2313445 hfize_llama.py:153] loaded layer 1
I0328 15:51:16.144670 2313445 hfize_llama.py:153] loaded layer 2
I0328 15:51:16.502825 2313445 hfize_llama.py:153] loaded layer 3
I0328 15:51:17.072499 2313445 hfize_llama.py:153] loaded layer 4
I0328 15:51:17.479709 2313445 hfize_llama.py:153] loaded layer 5
I0328 15:51:17.818656 2313445 hfize_llama.py:153] loaded layer 6
I0328 15:51:18.190824 2313445 hfize_llama.py:153] loaded layer 7
I0328 15:51:18.557712 2313445 hfize_llama.py:153] loaded layer 8
I0328 15:51:18.965156 2313445 hfize_llama.py:153] loaded layer 9
I0328 15:51:19.334933 2313445 hfize_llama.py:153] loaded layer 10
I0328 15:51:19.688569 2313445 hfize_llama.py:153] loaded layer 11
I0328 15:51:20.023867 2313445 hfize_llama.py:153] loaded layer 12
I0328 15:51:20.387905 2313445 hfize_llama.py:153] loaded layer 13
I0328 15:51:20.689191 2313445 hfize_llama.py:153] loaded layer 14
I0328 15:51:21.011147 2313445 hfize_llama.py:153] loaded layer 15
I0328 15:51:21.363685 2313445 hfize_llama.py:153] loaded layer 16
I0328 15:51:21.734789 2313445 hfize_llama.py:153] loaded layer 17
I0328 15:51:22.086840 2313445 hfize_llama.py:153] loaded layer 18
I0328 15:51:22.455838 2313445 hfize_llama.py:153] loaded layer 19
I0328 15:51:22.794495 2313445 hfize_llama.py:153] loaded layer 20
I0328 15:51:23.142369 2313445 hfize_llama.py:153] loaded layer 21
I0328 15:51:23.485048 2313445 hfize_llama.py:153] loaded layer 22
I0328 15:51:23.859897 2313445 hfize_llama.py:153] loaded layer 23
I0328 15:51:24.236195 2313445 hfize_llama.py:153] loaded layer 24
I0328 15:51:24.598342 2313445 hfize_llama.py:153] loaded layer 25
I0328 15:51:24.965689 2313445 hfize_llama.py:153] loaded layer 26
I0328 15:51:25.583271 2313445 hfize_llama.py:153] loaded layer 27
I0328 15:51:25.929759 2313445 hfize_llama.py:153] loaded layer 28
I0328 15:51:26.293897 2313445 hfize_llama.py:153] loaded layer 29
I0328 15:51:26.631543 2313445 hfize_llama.py:153] loaded layer 30
I0328 15:51:26.959813 2313445 hfize_llama.py:153] loaded layer 31
I0328 15:51:26.959976 2313445 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.83s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:08,  1.61s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:06,  1.56s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:04,  1.53s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:07<00:02,  1.46s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:08<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.39s/it]
I0328 15:52:28.143861 2313445 hfize_llama.py:167] successfully loaded hfized model
I0328 15:52:33.576699 2313664 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:52:33.576845 2313664 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:52:33.576888 2313664 utils.py:162] NumExpr defaulting to 16 threads.
W0328 15:52:33.938768 2313664 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 15:52:34.480801 2313664 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:09,  1.62s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:08,  1.63s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:06,  1.67s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:04,  1.59s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:07<00:03,  1.51s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:09<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.43s/it]
I0328 15:52:44.623744 2313664 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.6148630380630493:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.6148630380630493:   1%|          | 1/141 [00:01<04:24,  1.89s/it]avg_loss = 1.9059097170829773:   1%|          | 1/141 [00:03<04:24,  1.89s/it]avg_loss = 1.9059097170829773:   1%|▏         | 2/141 [00:03<03:47,  1.64s/it]avg_loss = 2.03363565603892:   1%|▏         | 2/141 [00:04<03:47,  1.64s/it]  avg_loss = 2.03363565603892:   2%|▏         | 3/141 [00:04<03:35,  1.56s/it]avg_loss = 1.9817928671836853:   2%|▏         | 3/141 [00:06<03:35,  1.56s/it]avg_loss = 1.9817928671836853:   3%|▎         | 4/141 [00:06<03:29,  1.53s/it]avg_loss = 1.929897952079773:   3%|▎         | 4/141 [00:07<03:29,  1.53s/it] avg_loss = 1.929897952079773:   4%|▎         | 5/141 [00:07<03:25,  1.51s/it]avg_loss = 1.8326464494069417:   4%|▎         | 5/141 [00:09<03:25,  1.51s/it]avg_loss = 1.8326464494069417:   4%|▍         | 6/141 [00:09<03:22,  1.50s/it]avg_loss = 1.7689091307776315:   4%|▍         | 6/141 [00:10<03:22,  1.50s/it]avg_loss = 1.7689091307776315:   5%|▍         | 7/141 [00:10<03:20,  1.49s/it]avg_loss = 1.7649422138929367:   5%|▍         | 7/141 [00:12<03:20,  1.49s/it]avg_loss = 1.7649422138929367:   6%|▌         | 8/141 [00:12<03:18,  1.49s/it]avg_loss = 1.7995487451553345:   6%|▌         | 8/141 [00:13<03:18,  1.49s/it]avg_loss = 1.7995487451553345:   6%|▋         | 9/141 [00:13<03:17,  1.49s/it]avg_loss = 1.8036588311195374:   6%|▋         | 9/141 [00:15<03:17,  1.49s/it]avg_loss = 1.8036588311195374:   7%|▋         | 10/141 [00:15<03:15,  1.49s/it]avg_loss = 1.7995997017080134:   7%|▋         | 10/141 [00:16<03:15,  1.49s/it]avg_loss = 1.7995997017080134:   8%|▊         | 11/141 [00:16<03:14,  1.49s/it]avg_loss = 1.822254369656245:   8%|▊         | 11/141 [00:18<03:14,  1.49s/it] avg_loss = 1.822254369656245:   9%|▊         | 12/141 [00:18<03:12,  1.50s/it]avg_loss = 1.834557056427002:   9%|▊         | 12/141 [00:19<03:12,  1.50s/it]avg_loss = 1.834557056427002:   9%|▉         | 13/141 [00:19<03:11,  1.50s/it]avg_loss = 1.8530630043574743:   9%|▉         | 13/141 [00:21<03:11,  1.50s/it]avg_loss = 1.8530630043574743:  10%|▉         | 14/141 [00:21<03:10,  1.50s/it]avg_loss = 1.8634724299112955:  10%|▉         | 14/141 [00:22<03:10,  1.50s/it]avg_loss = 1.8634724299112955:  11%|█         | 15/141 [00:22<03:09,  1.50s/it]avg_loss = 1.8877470046281815:  11%|█         | 15/141 [00:24<03:09,  1.50s/it]avg_loss = 1.8877470046281815:  11%|█▏        | 16/141 [00:24<03:08,  1.50s/it]avg_loss = 1.8911109882242538:  11%|█▏        | 16/141 [00:25<03:08,  1.50s/it]avg_loss = 1.8911109882242538:  12%|█▏        | 17/141 [00:25<03:06,  1.51s/it]avg_loss = 1.892687565750546:  12%|█▏        | 17/141 [00:27<03:06,  1.51s/it] avg_loss = 1.892687565750546:  13%|█▎        | 18/141 [00:27<03:05,  1.51s/it]avg_loss = 1.8743928231691058:  13%|█▎        | 18/141 [00:28<03:05,  1.51s/it]avg_loss = 1.8743928231691058:  13%|█▎        | 19/141 [00:28<03:04,  1.51s/it]avg_loss = 1.8735437095165253:  13%|█▎        | 19/141 [00:30<03:04,  1.51s/it]avg_loss = 1.8735437095165253:  14%|█▍        | 20/141 [00:30<03:03,  1.52s/it]avg_loss = 1.8782400233404977:  14%|█▍        | 20/141 [00:31<03:03,  1.52s/it]avg_loss = 1.8782400233404977:  15%|█▍        | 21/141 [00:31<03:02,  1.52s/it]avg_loss = 1.8803433775901794:  15%|█▍        | 21/141 [00:33<03:02,  1.52s/it]avg_loss = 1.8803433775901794:  16%|█▌        | 22/141 [00:33<03:00,  1.52s/it]avg_loss = 1.881842649501303:  16%|█▌        | 22/141 [00:34<03:00,  1.52s/it] avg_loss = 1.881842649501303:  16%|█▋        | 23/141 [00:34<02:59,  1.52s/it]avg_loss = 1.8866717517375946:  16%|█▋        | 23/141 [00:36<02:59,  1.52s/it]avg_loss = 1.8866717517375946:  17%|█▋        | 24/141 [00:36<02:58,  1.52s/it]avg_loss = 1.892433614730835:  17%|█▋        | 24/141 [00:37<02:58,  1.52s/it] avg_loss = 1.892433614730835:  18%|█▊        | 25/141 [00:37<02:57,  1.53s/it]avg_loss = 1.9039534605466402:  18%|█▊        | 25/141 [00:39<02:57,  1.53s/it]avg_loss = 1.9039534605466402:  18%|█▊        | 26/141 [00:39<02:55,  1.53s/it]avg_loss = 1.9165934633325648:  18%|█▊        | 26/141 [00:41<02:55,  1.53s/it]avg_loss = 1.9165934633325648:  19%|█▉        | 27/141 [00:41<02:54,  1.53s/it]avg_loss = 1.9230254207338606:  19%|█▉        | 27/141 [00:42<02:54,  1.53s/it]avg_loss = 1.9230254207338606:  20%|█▉        | 28/141 [00:42<02:53,  1.54s/it]avg_loss = 1.9196844840871876:  20%|█▉        | 28/141 [00:44<02:53,  1.54s/it]avg_loss = 1.9196844840871876:  21%|██        | 29/141 [00:44<02:52,  1.54s/it]avg_loss = 1.9100257396697997:  21%|██        | 29/141 [00:45<02:52,  1.54s/it]avg_loss = 1.9100257396697997:  21%|██▏       | 30/141 [00:45<02:50,  1.54s/it]avg_loss = 1.8961108769139936:  21%|██▏       | 30/141 [00:47<02:50,  1.54s/it]avg_loss = 1.8961108769139936:  22%|██▏       | 31/141 [00:47<02:49,  1.54s/it]avg_loss = 1.8840479403734207:  22%|██▏       | 31/141 [00:48<02:49,  1.54s/it]avg_loss = 1.8840479403734207:  23%|██▎       | 32/141 [00:48<02:48,  1.54s/it]avg_loss = 1.8825298656116833:  23%|██▎       | 32/141 [00:50<02:48,  1.54s/it]avg_loss = 1.8825298656116833:  23%|██▎       | 33/141 [00:50<02:46,  1.54s/it]avg_loss = 1.8808872173814213:  23%|██▎       | 33/141 [00:51<02:46,  1.54s/it]avg_loss = 1.8808872173814213:  24%|██▍       | 34/141 [00:51<02:45,  1.55s/it]avg_loss = 1.8834502083914622:  24%|██▍       | 34/141 [00:53<02:45,  1.55s/it]avg_loss = 1.8834502083914622:  25%|██▍       | 35/141 [00:53<02:43,  1.55s/it]avg_loss = 1.8671161664856806:  25%|██▍       | 35/141 [00:54<02:43,  1.55s/it]avg_loss = 1.8671161664856806:  26%|██▌       | 36/141 [00:54<02:42,  1.55s/it]avg_loss = 1.8515707125534882:  26%|██▌       | 36/141 [00:56<02:42,  1.55s/it]avg_loss = 1.8515707125534882:  26%|██▌       | 37/141 [00:56<02:40,  1.55s/it]avg_loss = 1.8366794021506059:  26%|██▌       | 37/141 [00:58<02:40,  1.55s/it]avg_loss = 1.8366794021506059:  27%|██▋       | 38/141 [00:58<02:39,  1.55s/it]avg_loss = 1.8222981599661021:  27%|██▋       | 38/141 [00:59<02:39,  1.55s/it]avg_loss = 1.8222981599661021:  28%|██▊       | 39/141 [00:59<02:38,  1.55s/it]avg_loss = 1.8138905495405198:  28%|██▊       | 39/141 [01:01<02:38,  1.55s/it]avg_loss = 1.8138905495405198:  28%|██▊       | 40/141 [01:01<02:36,  1.55s/it]avg_loss = 1.8190834376870133:  28%|██▊       | 40/141 [01:02<02:36,  1.55s/it]avg_loss = 1.8190834376870133:  29%|██▉       | 41/141 [01:02<02:35,  1.55s/it]avg_loss = 1.8360524092401778:  29%|██▉       | 41/141 [01:04<02:35,  1.55s/it]avg_loss = 1.8360524092401778:  30%|██▉       | 42/141 [01:04<02:33,  1.55s/it]avg_loss = 1.8525667218274848:  30%|██▉       | 42/141 [01:05<02:33,  1.55s/it]avg_loss = 1.8525667218274848:  30%|███       | 43/141 [01:05<02:32,  1.55s/it]avg_loss = 1.8557332374832847:  30%|███       | 43/141 [01:07<02:32,  1.55s/it]avg_loss = 1.8557332374832847:  31%|███       | 44/141 [01:07<02:30,  1.55s/it]avg_loss = 1.8598487854003907:  31%|███       | 44/141 [01:08<02:30,  1.55s/it]avg_loss = 1.8598487854003907:  32%|███▏      | 45/141 [01:08<02:29,  1.56s/it]avg_loss = 1.8651923096698264:  32%|███▏      | 45/141 [01:10<02:29,  1.56s/it]avg_loss = 1.8651923096698264:  33%|███▎      | 46/141 [01:10<02:27,  1.56s/it]avg_loss = 1.8716440403715093:  33%|███▎      | 46/141 [01:12<02:27,  1.56s/it]avg_loss = 1.8716440403715093:  33%|███▎      | 47/141 [01:12<02:26,  1.56s/it]avg_loss = 1.8747575730085373:  33%|███▎      | 47/141 [01:13<02:26,  1.56s/it]avg_loss = 1.8747575730085373:  34%|███▍      | 48/141 [01:13<02:24,  1.56s/it]avg_loss = 1.8735000527634913:  34%|███▍      | 48/141 [01:15<02:24,  1.56s/it]avg_loss = 1.8735000527634913:  35%|███▍      | 49/141 [01:15<02:23,  1.56s/it]avg_loss = 1.8731679940223693:  35%|███▍      | 49/141 [01:16<02:23,  1.56s/it]avg_loss = 1.8731679940223693:  35%|███▌      | 50/141 [01:16<02:22,  1.56s/it]avg_loss = 1.8669410055758906:  35%|███▌      | 50/141 [01:18<02:22,  1.56s/it]avg_loss = 1.8669410055758906:  36%|███▌      | 51/141 [01:18<02:20,  1.56s/it]avg_loss = 1.8629284822023833:  36%|███▌      | 51/141 [01:19<02:20,  1.56s/it]avg_loss = 1.8629284822023833:  37%|███▋      | 52/141 [01:19<02:19,  1.56s/it]avg_loss = 1.856376715426175:  37%|███▋      | 52/141 [01:21<02:19,  1.56s/it] avg_loss = 1.856376715426175:  38%|███▊      | 53/141 [01:21<02:17,  1.56s/it]avg_loss = 1.8533114503931116:  38%|███▊      | 53/141 [01:22<02:17,  1.56s/it]avg_loss = 1.8533114503931116:  38%|███▊      | 54/141 [01:22<02:16,  1.56s/it]avg_loss = 1.8454644788395276:  38%|███▊      | 54/141 [01:24<02:16,  1.56s/it]avg_loss = 1.8454644788395276:  39%|███▉      | 55/141 [01:24<02:14,  1.56s/it]avg_loss = 1.8378419003316335:  39%|███▉      | 55/141 [01:26<02:14,  1.56s/it]avg_loss = 1.8378419003316335:  40%|███▉      | 56/141 [01:26<02:13,  1.57s/it]avg_loss = 1.8315924238740353:  40%|███▉      | 56/141 [01:27<02:13,  1.57s/it]avg_loss = 1.8315924238740353:  40%|████      | 57/141 [01:27<02:11,  1.57s/it]avg_loss = 1.8286538802344223:  40%|████      | 57/141 [01:29<02:11,  1.57s/it]avg_loss = 1.8286538802344223:  41%|████      | 58/141 [01:29<02:10,  1.57s/it]avg_loss = 1.8308755826141874:  41%|████      | 58/141 [01:30<02:10,  1.57s/it]avg_loss = 1.8308755826141874:  42%|████▏     | 59/141 [01:30<02:08,  1.57s/it]avg_loss = 1.8364329854647319:  42%|████▏     | 59/141 [01:32<02:08,  1.57s/it]avg_loss = 1.8364329854647319:  43%|████▎     | 60/141 [01:32<02:07,  1.57s/it]avg_loss = 1.8422472203364137:  43%|████▎     | 60/141 [01:33<02:07,  1.57s/it]avg_loss = 1.8422472203364137:  43%|████▎     | 61/141 [01:33<02:05,  1.57s/it]avg_loss = 1.8495933663460515:  43%|████▎     | 61/141 [01:35<02:05,  1.57s/it]avg_loss = 1.8495933663460515:  44%|████▍     | 62/141 [01:35<02:04,  1.57s/it]avg_loss = 1.840476739974249:  44%|████▍     | 62/141 [01:37<02:04,  1.57s/it] avg_loss = 1.840476739974249:  45%|████▍     | 63/141 [01:37<02:02,  1.57s/it]avg_loss = 1.8383297733962536:  45%|████▍     | 63/141 [01:38<02:02,  1.57s/it]avg_loss = 1.8383297733962536:  45%|████▌     | 64/141 [01:38<02:01,  1.57s/it]avg_loss = 1.8357989879754872:  45%|████▌     | 64/141 [01:40<02:01,  1.57s/it]avg_loss = 1.8357989879754872:  46%|████▌     | 65/141 [01:40<01:59,  1.58s/it]avg_loss = 1.8296678282997825:  46%|████▌     | 65/141 [01:41<01:59,  1.58s/it]avg_loss = 1.8296678282997825:  47%|████▋     | 66/141 [01:41<01:58,  1.58s/it]avg_loss = 1.8270369003068154:  47%|████▋     | 66/141 [01:43<01:58,  1.58s/it]avg_loss = 1.8270369003068154:  48%|████▊     | 67/141 [01:43<01:56,  1.58s/it]avg_loss = 1.8236706414643455:  48%|████▊     | 67/141 [01:45<01:56,  1.58s/it]avg_loss = 1.8236706414643455:  48%|████▊     | 68/141 [01:45<01:55,  1.58s/it]avg_loss = 1.8208419741063877:  48%|████▊     | 68/141 [01:46<01:55,  1.58s/it]avg_loss = 1.8208419741063877:  49%|████▉     | 69/141 [01:46<01:53,  1.58s/it]avg_loss = 1.8216683183397566:  49%|████▉     | 69/141 [01:48<01:53,  1.58s/it]avg_loss = 1.8216683183397566:  50%|████▉     | 70/141 [01:48<01:52,  1.58s/it]avg_loss = 1.825350475982881:  50%|████▉     | 70/141 [01:49<01:52,  1.58s/it] avg_loss = 1.825350475982881:  50%|█████     | 71/141 [01:49<01:50,  1.58s/it]avg_loss = 1.8275908132394154:  50%|█████     | 71/141 [01:51<01:50,  1.58s/it]avg_loss = 1.8275908132394154:  51%|█████     | 72/141 [01:51<01:48,  1.58s/it]avg_loss = 1.8261365220971304:  51%|█████     | 72/141 [01:52<01:48,  1.58s/it]avg_loss = 1.8261365220971304:  52%|█████▏    | 73/141 [01:52<01:47,  1.58s/it]avg_loss = 1.827962053788675:  52%|█████▏    | 73/141 [01:54<01:47,  1.58s/it] avg_loss = 1.827962053788675:  52%|█████▏    | 74/141 [01:54<01:45,  1.58s/it]avg_loss = 1.8282265424728394:  52%|█████▏    | 74/141 [01:56<01:45,  1.58s/it]avg_loss = 1.8282265424728394:  53%|█████▎    | 75/141 [01:56<01:44,  1.58s/it]avg_loss = 1.8271523008221073:  53%|█████▎    | 75/141 [01:57<01:44,  1.58s/it]avg_loss = 1.8271523008221073:  54%|█████▍    | 76/141 [01:57<01:42,  1.58s/it]avg_loss = 1.8283986302165243:  54%|█████▍    | 76/141 [01:59<01:42,  1.58s/it]avg_loss = 1.8283986302165243:  55%|█████▍    | 77/141 [01:59<01:41,  1.58s/it]avg_loss = 1.8307482309830494:  55%|█████▍    | 77/141 [02:00<01:41,  1.58s/it]avg_loss = 1.8307482309830494:  55%|█████▌    | 78/141 [02:00<01:39,  1.58s/it]avg_loss = 1.8349002343189866:  55%|█████▌    | 78/141 [02:02<01:39,  1.58s/it]avg_loss = 1.8349002343189866:  56%|█████▌    | 79/141 [02:02<01:37,  1.58s/it]avg_loss = 1.8320176392793655:  56%|█████▌    | 79/141 [02:03<01:37,  1.58s/it]avg_loss = 1.8320176392793655:  57%|█████▋    | 80/141 [02:03<01:36,  1.58s/it]avg_loss = 1.830924884772595:  57%|█████▋    | 80/141 [02:05<01:36,  1.58s/it] avg_loss = 1.830924884772595:  57%|█████▋    | 81/141 [02:05<01:34,  1.58s/it]avg_loss = 1.830154786749584:  57%|█████▋    | 81/141 [02:07<01:34,  1.58s/it]avg_loss = 1.830154786749584:  58%|█████▊    | 82/141 [02:07<01:33,  1.58s/it]avg_loss = 1.828392660761454:  58%|█████▊    | 82/141 [02:08<01:33,  1.58s/it]avg_loss = 1.828392660761454:  59%|█████▉    | 83/141 [02:08<01:31,  1.58s/it]avg_loss = 1.8263075791654133:  59%|█████▉    | 83/141 [02:10<01:31,  1.58s/it]avg_loss = 1.8263075791654133:  60%|█████▉    | 84/141 [02:10<01:30,  1.58s/it]avg_loss = 1.8239705506493062:  60%|█████▉    | 84/141 [02:11<01:30,  1.58s/it]avg_loss = 1.8239705506493062:  60%|██████    | 85/141 [02:11<01:28,  1.58s/it]avg_loss = 1.8255775335223177:  60%|██████    | 85/141 [02:13<01:28,  1.58s/it]avg_loss = 1.8255775335223177:  61%|██████    | 86/141 [02:13<01:26,  1.58s/it]avg_loss = 1.8274265116658703:  61%|██████    | 86/141 [02:15<01:26,  1.58s/it]avg_loss = 1.8274265116658703:  62%|██████▏   | 87/141 [02:15<01:25,  1.58s/it]avg_loss = 1.827819969166409:  62%|██████▏   | 87/141 [02:16<01:25,  1.58s/it] avg_loss = 1.827819969166409:  62%|██████▏   | 88/141 [02:16<01:23,  1.58s/it]avg_loss = 1.8365806153651034:  62%|██████▏   | 88/141 [02:18<01:23,  1.58s/it]avg_loss = 1.8365806153651034:  63%|██████▎   | 89/141 [02:18<01:22,  1.58s/it]avg_loss = 1.8441265198919508:  63%|██████▎   | 89/141 [02:19<01:22,  1.58s/it]avg_loss = 1.8441265198919508:  64%|██████▍   | 90/141 [02:19<01:20,  1.58s/it]avg_loss = 1.8474018298662627:  64%|██████▍   | 90/141 [02:21<01:20,  1.58s/it]avg_loss = 1.8474018298662627:  65%|██████▍   | 91/141 [02:21<01:19,  1.58s/it]avg_loss = 1.8523951444936835:  65%|██████▍   | 91/141 [02:22<01:19,  1.58s/it]avg_loss = 1.8523951444936835:  65%|██████▌   | 92/141 [02:22<01:17,  1.58s/it]avg_loss = 1.857397867787269:  65%|██████▌   | 92/141 [02:24<01:17,  1.58s/it] avg_loss = 1.857397867787269:  66%|██████▌   | 93/141 [02:24<01:16,  1.58s/it]avg_loss = 1.8584344513872837:  66%|██████▌   | 93/141 [02:26<01:16,  1.58s/it]avg_loss = 1.8584344513872837:  67%|██████▋   | 94/141 [02:26<01:14,  1.59s/it]avg_loss = 1.8622163044778923:  67%|██████▋   | 94/141 [02:27<01:14,  1.59s/it]avg_loss = 1.8622163044778923:  67%|██████▋   | 95/141 [02:27<01:12,  1.59s/it]avg_loss = 1.8630528189241886:  67%|██████▋   | 95/141 [02:29<01:12,  1.59s/it]avg_loss = 1.8630528189241886:  68%|██████▊   | 96/141 [02:29<01:11,  1.58s/it]avg_loss = 1.865029562379896:  68%|██████▊   | 96/141 [02:30<01:11,  1.58s/it] avg_loss = 1.865029562379896:  69%|██████▉   | 97/141 [02:30<01:09,  1.58s/it]avg_loss = 1.8604047018654493:  69%|██████▉   | 97/141 [02:32<01:09,  1.58s/it]avg_loss = 1.8604047018654493:  70%|██████▉   | 98/141 [02:32<01:08,  1.59s/it]avg_loss = 1.8611274367631083:  70%|██████▉   | 98/141 [02:34<01:08,  1.59s/it]avg_loss = 1.8611274367631083:  70%|███████   | 99/141 [02:34<01:06,  1.59s/it]avg_loss = 1.8631737303733826:  70%|███████   | 99/141 [02:35<01:06,  1.59s/it]avg_loss = 1.8631737303733826:  71%|███████   | 100/141 [02:35<01:05,  1.59s/it]avg_loss = 1.8617181317641003:  71%|███████   | 100/141 [02:37<01:05,  1.59s/it]avg_loss = 1.8617181317641003:  72%|███████▏  | 101/141 [02:37<01:03,  1.59s/it]avg_loss = 1.8618973750694126:  72%|███████▏  | 101/141 [02:38<01:03,  1.59s/it]avg_loss = 1.8618973750694126:  72%|███████▏  | 102/141 [02:38<01:01,  1.59s/it]avg_loss = 1.8600023151601401:  72%|███████▏  | 102/141 [02:40<01:01,  1.59s/it]avg_loss = 1.8600023151601401:  73%|███████▎  | 103/141 [02:40<01:00,  1.59s/it]avg_loss = 1.862375212403444:  73%|███████▎  | 103/141 [02:41<01:00,  1.59s/it] avg_loss = 1.862375212403444:  74%|███████▍  | 104/141 [02:41<00:58,  1.59s/it]avg_loss = 1.8606067907242547:  74%|███████▍  | 104/141 [02:43<00:58,  1.59s/it]avg_loss = 1.8606067907242547:  74%|███████▍  | 105/141 [02:43<00:57,  1.59s/it]avg_loss = 1.8594933095967994:  74%|███████▍  | 105/141 [02:45<00:57,  1.59s/it]avg_loss = 1.8594933095967994:  75%|███████▌  | 106/141 [02:45<00:55,  1.59s/it]avg_loss = 1.8570901801652997:  75%|███████▌  | 106/141 [02:46<00:55,  1.59s/it]avg_loss = 1.8570901801652997:  76%|███████▌  | 107/141 [02:46<00:53,  1.59s/it]avg_loss = 1.8547388006139685:  76%|███████▌  | 107/141 [02:48<00:53,  1.59s/it]avg_loss = 1.8547388006139685:  77%|███████▋  | 108/141 [02:48<00:52,  1.59s/it]avg_loss = 1.8521807073453151:  77%|███████▋  | 108/141 [02:49<00:52,  1.59s/it]avg_loss = 1.8521807073453151:  77%|███████▋  | 109/141 [02:49<00:50,  1.59s/it]avg_loss = 1.8496891336007553:  77%|███████▋  | 109/141 [02:51<00:50,  1.59s/it]avg_loss = 1.8496891336007553:  78%|███████▊  | 110/141 [02:51<00:49,  1.59s/it]avg_loss = 1.8520208253516808:  78%|███████▊  | 110/141 [02:53<00:49,  1.59s/it]avg_loss = 1.8520208253516808:  79%|███████▊  | 111/141 [02:53<00:47,  1.59s/it]avg_loss = 1.8518045331750597:  79%|███████▊  | 111/141 [02:54<00:47,  1.59s/it]avg_loss = 1.8518045331750597:  79%|███████▉  | 112/141 [02:54<00:45,  1.59s/it]avg_loss = 1.852918496174095:  79%|███████▉  | 112/141 [02:56<00:45,  1.59s/it] avg_loss = 1.852918496174095:  80%|████████  | 113/141 [02:56<00:44,  1.59s/it]avg_loss = 1.8538532047940974:  80%|████████  | 113/141 [02:57<00:44,  1.59s/it]avg_loss = 1.8538532047940974:  81%|████████  | 114/141 [02:57<00:42,  1.59s/it]avg_loss = 1.8532461632852968:  81%|████████  | 114/141 [02:59<00:42,  1.59s/it]avg_loss = 1.8532461632852968:  82%|████████▏ | 115/141 [02:59<00:41,  1.58s/it]avg_loss = 1.851764191841257:  82%|████████▏ | 115/141 [03:01<00:41,  1.58s/it] avg_loss = 1.851764191841257:  82%|████████▏ | 116/141 [03:01<00:39,  1.58s/it]avg_loss = 1.853956538387853:  82%|████████▏ | 116/141 [03:02<00:39,  1.58s/it]avg_loss = 1.853956538387853:  83%|████████▎ | 117/141 [03:02<00:38,  1.59s/it]avg_loss = 1.8536965988450131:  83%|████████▎ | 117/141 [03:04<00:38,  1.59s/it]avg_loss = 1.8536965988450131:  84%|████████▎ | 118/141 [03:04<00:36,  1.59s/it]avg_loss = 1.8523173853128898:  84%|████████▎ | 118/141 [03:05<00:36,  1.59s/it]avg_loss = 1.8523173853128898:  84%|████████▍ | 119/141 [03:05<00:34,  1.59s/it]avg_loss = 1.8506430943806966:  84%|████████▍ | 119/141 [03:07<00:34,  1.59s/it]avg_loss = 1.8506430943806966:  85%|████████▌ | 120/141 [03:07<00:33,  1.59s/it]avg_loss = 1.850475716196801:  85%|████████▌ | 120/141 [03:08<00:33,  1.59s/it] avg_loss = 1.850475716196801:  86%|████████▌ | 121/141 [03:08<00:31,  1.59s/it]avg_loss = 1.850778426303238:  86%|████████▌ | 121/141 [03:10<00:31,  1.59s/it]avg_loss = 1.850778426303238:  87%|████████▋ | 122/141 [03:10<00:30,  1.59s/it]avg_loss = 1.8506379592709425:  87%|████████▋ | 122/141 [03:12<00:30,  1.59s/it]avg_loss = 1.8506379592709425:  87%|████████▋ | 123/141 [03:12<00:28,  1.59s/it]avg_loss = 1.8508820591434356:  87%|████████▋ | 123/141 [03:13<00:28,  1.59s/it]avg_loss = 1.8508820591434356:  88%|████████▊ | 124/141 [03:13<00:26,  1.59s/it]avg_loss = 1.8496135969161986:  88%|████████▊ | 124/141 [03:15<00:26,  1.59s/it]avg_loss = 1.8496135969161986:  89%|████████▊ | 125/141 [03:15<00:25,  1.59s/it]avg_loss = 1.8500312453224546:  89%|████████▊ | 125/141 [03:16<00:25,  1.59s/it]avg_loss = 1.8500312453224546:  89%|████████▉ | 126/141 [03:16<00:23,  1.59s/it]avg_loss = 1.8497991411704717:  89%|████████▉ | 126/141 [03:18<00:23,  1.59s/it]avg_loss = 1.8497991411704717:  90%|█████████ | 127/141 [03:18<00:22,  1.59s/it]avg_loss = 1.848514516837895:  90%|█████████ | 127/141 [03:20<00:22,  1.59s/it] avg_loss = 1.848514516837895:  91%|█████████ | 128/141 [03:20<00:20,  1.59s/it]avg_loss = 1.848691653835681:  91%|█████████ | 128/141 [03:21<00:20,  1.59s/it]avg_loss = 1.848691653835681:  91%|█████████▏| 129/141 [03:21<00:19,  1.59s/it]avg_loss = 1.849395950940939:  91%|█████████▏| 129/141 [03:23<00:19,  1.59s/it]avg_loss = 1.849395950940939:  92%|█████████▏| 130/141 [03:23<00:17,  1.59s/it]avg_loss = 1.8503561629593828:  92%|█████████▏| 130/141 [03:24<00:17,  1.59s/it]avg_loss = 1.8503561629593828:  93%|█████████▎| 131/141 [03:24<00:15,  1.59s/it]avg_loss = 1.8509756407954476:  93%|█████████▎| 131/141 [03:26<00:15,  1.59s/it]avg_loss = 1.8509756407954476:  94%|█████████▎| 132/141 [03:26<00:14,  1.59s/it]avg_loss = 1.8481354229432299:  94%|█████████▎| 132/141 [03:28<00:14,  1.59s/it]avg_loss = 1.8481354229432299:  94%|█████████▍| 133/141 [03:28<00:12,  1.59s/it]avg_loss = 1.843732865888681:  94%|█████████▍| 133/141 [03:29<00:12,  1.59s/it] avg_loss = 1.843732865888681:  95%|█████████▌| 134/141 [03:29<00:11,  1.59s/it]avg_loss = 1.8462409725895634:  95%|█████████▌| 134/141 [03:31<00:11,  1.59s/it]avg_loss = 1.8462409725895634:  96%|█████████▌| 135/141 [03:31<00:09,  1.59s/it]avg_loss = 1.8497379141695358:  96%|█████████▌| 135/141 [03:32<00:09,  1.59s/it]avg_loss = 1.8497379141695358:  96%|█████████▋| 136/141 [03:32<00:07,  1.59s/it]avg_loss = 1.8508546700442794:  96%|█████████▋| 136/141 [03:34<00:07,  1.59s/it]avg_loss = 1.8508546700442794:  97%|█████████▋| 137/141 [03:34<00:06,  1.59s/it]avg_loss = 1.849523007005885:  97%|█████████▋| 137/141 [03:35<00:06,  1.59s/it] avg_loss = 1.849523007005885:  98%|█████████▊| 138/141 [03:35<00:04,  1.59s/it]avg_loss = 1.8497575435707037:  98%|█████████▊| 138/141 [03:37<00:04,  1.59s/it]avg_loss = 1.8497575435707037:  99%|█████████▊| 139/141 [03:37<00:03,  1.59s/it]avg_loss = 1.8503068370478495:  99%|█████████▊| 139/141 [03:39<00:03,  1.59s/it]avg_loss = 1.8503068370478495:  99%|█████████▉| 140/141 [03:39<00:01,  1.59s/it]avg_loss = 1.8516448381099295:  99%|█████████▉| 140/141 [03:40<00:01,  1.59s/it]avg_loss = 1.8516448381099295: 100%|██████████| 141/141 [03:40<00:00,  1.59s/it]avg_loss = 1.8516448381099295: 100%|██████████| 141/141 [03:40<00:00,  1.57s/it]
I0328 15:56:51.924907 2313664 eval_ppl.py:107] wikitext2 perplexity: 6.370289325714111
wikitext2 perplexity: 6.370
