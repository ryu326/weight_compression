I0328 09:05:57.838061 2300848 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:05:57.838159 2300848 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:05:57.838198 2300848 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:05:58.160947 2300848 config.py:54] PyTorch version 2.6.0 available.
W0328 09:05:58.349843 2300848 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:05:58.901718 2300848 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.25it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.74it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.90it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.00it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.07it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.72it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.95it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.89it/s]
I0328 09:06:00.353284 2300848 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.41it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:18,  1.66it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.78it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.83it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.85it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.87it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.89it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.89it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:12,  1.90it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.89it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.89it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:10,  1.90it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.89it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  1.89it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.90it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.92it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.93it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.95it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.93it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.95it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.93it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.95it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.94it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.94it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.94it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.94it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.94it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.95it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.95it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.94it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.94it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.90it/s]
I0328 09:06:22.891076 2300848 quantize_finetune_llama.py:185] loaded compression model
I0328 09:06:41.493600 2300848 quantize_finetune_llama.py:189] loaded dataset and devset
I0328 09:06:46.548363 2300848 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:07:49.013213 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 62.315223932266235s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0328 09:08:08.904936 2301103 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:08:08.905037 2301103 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:08:08.905076 2301103 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:08:09.227252 2301103 config.py:54] PyTorch version 2.6.0 available.
W0328 09:08:09.414128 2301103 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:08:09.950418 2301103 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:08:09.954160 2300848 quantize_finetune_llama.py:209] layer 1 gpu 1
I0328 09:08:09.967743 2301103 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:08:26.160488 2301103 finetune.py:45] layer 0_v initial loss 5.653593007082236e-07
W0328 09:08:26.160822 2301103 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:09:00.394269 2301103 finetune.py:68] layer 0_v @ epoch 0 new loss 4.990399702364812e-07 old loss 5.653593007082236e-07 BETTER
I0328 09:09:05.328539 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 55.20185375213623s
I0328 09:09:14.127338 2301238 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:09:14.127440 2301238 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:09:14.127479 2301238 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:09:14.463793 2301238 config.py:54] PyTorch version 2.6.0 available.
W0328 09:09:14.659382 2301238 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:09:15.300977 2301238 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:09:15.304416 2300848 quantize_finetune_llama.py:209] layer 2 gpu 2
I0328 09:09:15.318500 2301238 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:09:31.778112 2301238 finetune.py:45] layer 1_v initial loss 4.946552962792339e-06
W0328 09:09:31.778337 2301238 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:09:36.080684 2301103 finetune.py:68] layer 0_v @ epoch 1 new loss 4.784439511240635e-07 old loss 4.990399702364812e-07 BETTER
I0328 09:10:04.648615 2301238 finetune.py:68] layer 1_v @ epoch 0 new loss 1.4891397768224124e-06 old loss 4.946552962792339e-06 BETTER
I0328 09:10:12.220567 2301103 finetune.py:68] layer 0_v @ epoch 2 new loss 4.6787067731202114e-07 old loss 4.784439511240635e-07 BETTER
I0328 09:10:19.270852 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 63.79592180252075s
I0328 09:10:27.886273 2301379 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:10:27.886367 2301379 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:10:27.886409 2301379 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:10:28.223843 2301379 config.py:54] PyTorch version 2.6.0 available.
W0328 09:10:28.423998 2301379 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:10:28.993123 2301379 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:10:28.997029 2300848 quantize_finetune_llama.py:209] layer 3 gpu 3
I0328 09:10:29.012081 2301379 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 09:10:38.892725 2301238 finetune.py:68] layer 1_v @ epoch 1 new loss 8.887594731277204e-07 old loss 1.4891397768224124e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:10:46.131450 2301379 finetune.py:45] layer 2_v initial loss 1.0696883691707626e-05
W0328 09:10:46.131843 2301379 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:10:48.421894 2301103 finetune.py:68] layer 0_v @ epoch 3 new loss 4.611719361946598e-07 old loss 4.6787067731202114e-07 BETTER
I0328 09:11:13.365369 2301238 finetune.py:68] layer 1_v @ epoch 2 new loss 7.344249297602801e-07 old loss 8.887594731277204e-07 BETTER
I0328 09:11:19.435058 2301379 finetune.py:68] layer 2_v @ epoch 0 new loss 2.187708560086321e-06 old loss 1.0696883691707626e-05 BETTER
I0328 09:11:25.026765 2301103 finetune.py:68] layer 0_v @ epoch 4 new loss 4.5636309664587316e-07 old loss 4.611719361946598e-07 BETTER
I0328 09:11:35.391326 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 66.21008157730103s
I0328 09:11:44.715226 2301523 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:11:44.715345 2301523 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:11:44.715392 2301523 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:11:45.128525 2301523 config.py:54] PyTorch version 2.6.0 available.
W0328 09:11:45.357319 2301523 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 09:11:45.671365 2301103 finetune.py:45] layer 0_q initial loss 4.570736962250521e-07
W0328 09:11:46.025379 2301523 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:11:46.029348 2300848 quantize_finetune_llama.py:209] layer 4 gpu 0
I0328 09:11:46.043057 2301523 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 09:11:48.015105 2301238 finetune.py:68] layer 1_v @ epoch 3 new loss 6.745575547029148e-07 old loss 7.344249297602801e-07 BETTER
I0328 09:11:53.889777 2301379 finetune.py:68] layer 2_v @ epoch 1 new loss 1.238984282281308e-06 old loss 2.187708560086321e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:12:02.973685 2301523 finetune.py:45] layer 3_v initial loss 1.1190429177077021e-05
W0328 09:12:02.974104 2301523 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:12:20.352380 2301103 finetune.py:68] layer 0_q @ epoch 0 new loss 4.5313728946894116e-07 old loss 4.570736962250521e-07 BETTER
I0328 09:12:22.693841 2301238 finetune.py:68] layer 1_v @ epoch 4 new loss 6.408695298887324e-07 old loss 6.745575547029148e-07 BETTER
I0328 09:12:28.460363 2301379 finetune.py:68] layer 2_v @ epoch 2 new loss 1.0646421060300781e-06 old loss 1.238984282281308e-06 BETTER
I0328 09:12:35.811886 2301523 finetune.py:68] layer 3_v @ epoch 0 new loss 2.901871766880504e-06 old loss 1.1190429177077021e-05 BETTER
I0328 09:12:42.005356 2301238 finetune.py:45] layer 1_q initial loss 6.508770979962719e-07
I0328 09:12:56.174058 2301103 finetune.py:68] layer 0_q @ epoch 1 new loss 4.4985591785007273e-07 old loss 4.5313728946894116e-07 BETTER
I0328 09:13:03.324994 2301379 finetune.py:68] layer 2_v @ epoch 3 new loss 1.0031747024186188e-06 old loss 1.0646421060300781e-06 BETTER
I0328 09:13:09.686410 2301523 finetune.py:68] layer 3_v @ epoch 1 new loss 1.924705202327459e-06 old loss 2.901871766880504e-06 BETTER
I0328 09:13:15.062487 2301238 finetune.py:68] layer 1_q @ epoch 0 new loss 6.231035740711377e-07 old loss 6.508770979962719e-07 BETTER
I0328 09:13:31.989062 2301103 finetune.py:68] layer 0_q @ epoch 2 new loss 4.4709619828608993e-07 old loss 4.4985591785007273e-07 BETTER
I0328 09:13:38.175507 2301379 finetune.py:68] layer 2_v @ epoch 4 new loss 9.681830306362826e-07 old loss 1.0031747024186188e-06 BETTER
I0328 09:13:43.977148 2301523 finetune.py:68] layer 3_v @ epoch 2 new loss 1.6712345995983924e-06 old loss 1.924705202327459e-06 BETTER
I0328 09:13:49.149678 2301238 finetune.py:68] layer 1_q @ epoch 1 new loss 6.031406201145728e-07 old loss 6.231035740711377e-07 BETTER
I0328 09:13:57.627826 2301379 finetune.py:45] layer 2_q initial loss 1.0867032642636332e-06
I0328 09:14:07.974708 2301103 finetune.py:68] layer 0_q @ epoch 3 new loss 4.446050070328056e-07 old loss 4.4709619828608993e-07 BETTER
I0328 09:14:18.368525 2301523 finetune.py:68] layer 3_v @ epoch 3 new loss 1.5603402516717324e-06 old loss 1.6712345995983924e-06 BETTER
I0328 09:14:23.360224 2301238 finetune.py:68] layer 1_q @ epoch 2 new loss 5.872874453416443e-07 old loss 6.031406201145728e-07 BETTER
I0328 09:14:31.040828 2301379 finetune.py:68] layer 2_q @ epoch 0 new loss 1.0507341130505665e-06 old loss 1.0867032642636332e-06 BETTER
I0328 09:14:44.099497 2301103 finetune.py:68] layer 0_q @ epoch 4 new loss 4.423225163918687e-07 old loss 4.446050070328056e-07 BETTER
I0328 09:14:52.820324 2301523 finetune.py:68] layer 3_v @ epoch 4 new loss 1.4964460888222675e-06 old loss 1.5603402516717324e-06 BETTER
I0328 09:14:57.624211 2301238 finetune.py:68] layer 1_q @ epoch 3 new loss 5.743295332649723e-07 old loss 5.872874453416443e-07 BETTER
I0328 09:15:01.762424 2301103 finetune.py:45] layer 0_k initial loss 4.4521820541376655e-07
I0328 09:15:05.471126 2301379 finetune.py:68] layer 2_q @ epoch 1 new loss 1.0278839681632235e-06 old loss 1.0507341130505665e-06 BETTER
I0328 09:15:12.338913 2301523 finetune.py:45] layer 3_q initial loss 1.7323121710433043e-06
I0328 09:15:32.023928 2301238 finetune.py:68] layer 1_q @ epoch 4 new loss 5.633762043544266e-07 old loss 5.743295332649723e-07 BETTER
I0328 09:15:36.578648 2301103 finetune.py:68] layer 0_k @ epoch 0 new loss 4.4276566768530756e-07 old loss 4.4521820541376655e-07 BETTER
I0328 09:15:40.078439 2301379 finetune.py:68] layer 2_q @ epoch 2 new loss 1.0112441941600991e-06 old loss 1.0278839681632235e-06 BETTER
I0328 09:15:45.378886 2301523 finetune.py:68] layer 3_q @ epoch 0 new loss 1.6763134453867679e-06 old loss 1.7323121710433043e-06 BETTER
I0328 09:15:50.116405 2301238 finetune.py:45] layer 1_k initial loss 5.75267279145919e-07
I0328 09:16:12.559085 2301103 finetune.py:68] layer 0_k @ epoch 1 new loss 4.4082321437599603e-07 old loss 4.4276566768530756e-07 BETTER
I0328 09:16:14.574253 2301379 finetune.py:68] layer 2_q @ epoch 3 new loss 9.98048221845238e-07 old loss 1.0112441941600991e-06 BETTER
I0328 09:16:19.373680 2301523 finetune.py:68] layer 3_q @ epoch 1 new loss 1.6409337604272878e-06 old loss 1.6763134453867679e-06 BETTER
I0328 09:16:23.366278 2301238 finetune.py:68] layer 1_k @ epoch 0 new loss 5.647179364132171e-07 old loss 5.75267279145919e-07 BETTER
I0328 09:16:48.723734 2301103 finetune.py:68] layer 0_k @ epoch 2 new loss 4.3907414237764897e-07 old loss 4.4082321437599603e-07 BETTER
I0328 09:16:49.130167 2301379 finetune.py:68] layer 2_q @ epoch 4 new loss 9.871907877823105e-07 old loss 9.98048221845238e-07 BETTER
I0328 09:16:53.403760 2301523 finetune.py:68] layer 3_q @ epoch 2 new loss 1.6152033595062676e-06 old loss 1.6409337604272878e-06 BETTER
I0328 09:16:57.459189 2301238 finetune.py:68] layer 1_k @ epoch 1 new loss 5.564084517573065e-07 old loss 5.647179364132171e-07 BETTER
I0328 09:17:07.029814 2301379 finetune.py:45] layer 2_k initial loss 1.032771706377389e-06
I0328 09:17:24.927216 2301103 finetune.py:68] layer 0_k @ epoch 3 new loss 4.373999331619416e-07 old loss 4.3907414237764897e-07 BETTER
I0328 09:17:27.534012 2301523 finetune.py:68] layer 3_q @ epoch 3 new loss 1.5942044910843833e-06 old loss 1.6152033595062676e-06 BETTER
I0328 09:17:31.538358 2301238 finetune.py:68] layer 1_k @ epoch 2 new loss 5.492750574376259e-07 old loss 5.564084517573065e-07 BETTER
I0328 09:17:40.432808 2301379 finetune.py:68] layer 2_k @ epoch 0 new loss 1.0205080798186827e-06 old loss 1.032771706377389e-06 BETTER
I0328 09:18:01.204380 2301103 finetune.py:68] layer 0_k @ epoch 4 new loss 4.358279568350554e-07 old loss 4.373999331619416e-07 BETTER
I0328 09:18:01.645528 2301523 finetune.py:68] layer 3_q @ epoch 4 new loss 1.5774937764945207e-06 old loss 1.5942044910843833e-06 BETTER
I0328 09:18:05.821607 2301238 finetune.py:68] layer 1_k @ epoch 3 new loss 5.427556857284799e-07 old loss 5.492750574376259e-07 BETTER
I0328 09:18:14.842702 2301379 finetune.py:68] layer 2_k @ epoch 1 new loss 1.012372649711324e-06 old loss 1.0205080798186827e-06 BETTER
I0328 09:18:19.498190 2301523 finetune.py:45] layer 3_k initial loss 1.672770281402336e-06
I0328 09:18:20.343618 2301103 finetune.py:45] layer 0_o initial loss 6.995605303927732e-07
I0328 09:18:40.117322 2301238 finetune.py:68] layer 1_k @ epoch 4 new loss 5.368608526623575e-07 old loss 5.427556857284799e-07 BETTER
I0328 09:18:49.251187 2301379 finetune.py:68] layer 2_k @ epoch 2 new loss 1.0053879577753833e-06 old loss 1.012372649711324e-06 BETTER
I0328 09:18:52.444829 2301523 finetune.py:68] layer 3_k @ epoch 0 new loss 1.6570343177590985e-06 old loss 1.672770281402336e-06 BETTER
I0328 09:18:54.660832 2301103 finetune.py:68] layer 0_o @ epoch 0 new loss 6.97268774274562e-07 old loss 6.995605303927732e-07 BETTER
I0328 09:18:59.767219 2301238 finetune.py:45] layer 1_o initial loss 1.4071302985030343e-06
I0328 09:19:23.743553 2301379 finetune.py:68] layer 2_k @ epoch 3 new loss 9.992443210649071e-07 old loss 1.0053879577753833e-06 BETTER
I0328 09:19:26.490304 2301523 finetune.py:68] layer 3_k @ epoch 1 new loss 1.6443792674181168e-06 old loss 1.6570343177590985e-06 BETTER
I0328 09:19:30.264661 2301103 finetune.py:68] layer 0_o @ epoch 1 new loss 6.954138598302961e-07 old loss 6.97268774274562e-07 BETTER
I0328 09:19:32.310719 2301238 finetune.py:68] layer 1_o @ epoch 0 new loss 1.3645502576764557e-06 old loss 1.4071302985030343e-06 BETTER
I0328 09:19:58.311746 2301379 finetune.py:68] layer 2_k @ epoch 4 new loss 9.936617288985872e-07 old loss 9.992443210649071e-07 BETTER
I0328 09:20:00.366464 2301523 finetune.py:68] layer 3_k @ epoch 2 new loss 1.633347210372449e-06 old loss 1.6443792674181168e-06 BETTER
I0328 09:20:05.843432 2301238 finetune.py:68] layer 1_o @ epoch 1 new loss 1.332912802354258e-06 old loss 1.3645502576764557e-06 BETTER
I0328 09:20:05.993234 2301103 finetune.py:68] layer 0_o @ epoch 2 new loss 6.938340675333166e-07 old loss 6.954138598302961e-07 BETTER
I0328 09:20:18.133252 2301379 finetune.py:45] layer 2_o initial loss 2.311997832293855e-06
I0328 09:20:34.415930 2301523 finetune.py:68] layer 3_k @ epoch 3 new loss 1.623579464649083e-06 old loss 1.633347210372449e-06 BETTER
I0328 09:20:39.563202 2301238 finetune.py:68] layer 1_o @ epoch 2 new loss 1.3090391348669073e-06 old loss 1.332912802354258e-06 BETTER
I0328 09:20:41.922011 2301103 finetune.py:68] layer 0_o @ epoch 3 new loss 6.923937121428025e-07 old loss 6.938340675333166e-07 BETTER
I0328 09:20:50.919005 2301379 finetune.py:68] layer 2_o @ epoch 0 new loss 2.2215947410586523e-06 old loss 2.311997832293855e-06 BETTER
I0328 09:21:08.208302 2301523 finetune.py:68] layer 3_k @ epoch 4 new loss 1.6145542076628772e-06 old loss 1.623579464649083e-06 BETTER
I0328 09:21:13.465955 2301238 finetune.py:68] layer 1_o @ epoch 3 new loss 1.2908536746181198e-06 old loss 1.3090391348669073e-06 BETTER
I0328 09:21:17.887810 2301103 finetune.py:68] layer 0_o @ epoch 4 new loss 6.911298555678513e-07 old loss 6.923937121428025e-07 BETTER
I0328 09:21:24.717609 2301379 finetune.py:68] layer 2_o @ epoch 1 new loss 2.1718367406720063e-06 old loss 2.2215947410586523e-06 BETTER
I0328 09:21:27.612838 2301523 finetune.py:45] layer 3_o initial loss 3.6965805065847235e-06
I0328 09:21:47.385084 2301238 finetune.py:68] layer 1_o @ epoch 4 new loss 1.2779652251992957e-06 old loss 1.2908536746181198e-06 BETTER
I0328 09:21:48.681493 2301103 finetune.py:45] layer 0_up initial loss 9.370016300636053e-07
I0328 09:21:58.561542 2301379 finetune.py:68] layer 2_o @ epoch 2 new loss 2.141849790859851e-06 old loss 2.1718367406720063e-06 BETTER
I0328 09:21:59.806352 2301523 finetune.py:68] layer 3_o @ epoch 0 new loss 3.5861949072568677e-06 old loss 3.6965805065847235e-06 BETTER
I0328 09:22:18.673532 2301238 finetune.py:45] layer 1_up initial loss 1.8782694723995519e-06
I0328 09:22:20.488903 2301103 finetune.py:68] layer 0_up @ epoch 0 new loss 9.348387379759515e-07 old loss 9.370016300636053e-07 BETTER
I0328 09:22:32.322803 2301379 finetune.py:68] layer 2_o @ epoch 3 new loss 2.121909801644506e-06 old loss 2.141849790859851e-06 BETTER
I0328 09:22:32.846948 2301523 finetune.py:68] layer 3_o @ epoch 1 new loss 3.547030246409122e-06 old loss 3.5861949072568677e-06 BETTER
I0328 09:22:48.908148 2301238 finetune.py:68] layer 1_up @ epoch 0 new loss 1.8366375797995715e-06 old loss 1.8782694723995519e-06 BETTER
I0328 09:22:53.376327 2301103 finetune.py:68] layer 0_up @ epoch 1 new loss 9.333828074886696e-07 old loss 9.348387379759515e-07 BETTER
I0328 09:23:05.967200 2301523 finetune.py:68] layer 3_o @ epoch 2 new loss 3.5227835724072065e-06 old loss 3.547030246409122e-06 BETTER
I0328 09:23:06.184233 2301379 finetune.py:68] layer 2_o @ epoch 4 new loss 2.1071846276754513e-06 old loss 2.121909801644506e-06 BETTER
I0328 09:23:20.240556 2301238 finetune.py:68] layer 1_up @ epoch 1 new loss 1.828004656090343e-06 old loss 1.8366375797995715e-06 BETTER
I0328 09:23:26.511944 2301103 finetune.py:68] layer 0_up @ epoch 2 new loss 9.321658467342786e-07 old loss 9.333828074886696e-07 BETTER
I0328 09:23:37.467352 2301379 finetune.py:45] layer 2_up initial loss 3.4989643609151244e-06
I0328 09:23:39.139413 2301523 finetune.py:68] layer 3_o @ epoch 3 new loss 3.5033547192142578e-06 old loss 3.5227835724072065e-06 BETTER
I0328 09:23:51.660676 2301238 finetune.py:68] layer 1_up @ epoch 2 new loss 1.8210430425824597e-06 old loss 1.828004656090343e-06 BETTER
I0328 09:23:59.446691 2301103 finetune.py:68] layer 0_up @ epoch 3 new loss 9.310856512456667e-07 old loss 9.321658467342786e-07 BETTER
I0328 09:24:07.905090 2301379 finetune.py:68] layer 2_up @ epoch 0 new loss 3.4850622796511743e-06 old loss 3.4989643609151244e-06 BETTER
I0328 09:24:12.412745 2301523 finetune.py:68] layer 3_o @ epoch 4 new loss 3.4866386613430222e-06 old loss 3.5033547192142578e-06 BETTER
I0328 09:24:23.253761 2301238 finetune.py:68] layer 1_up @ epoch 3 new loss 1.8152178427044419e-06 old loss 1.8210430425824597e-06 BETTER
I0328 09:24:33.021188 2301103 finetune.py:68] layer 0_up @ epoch 4 new loss 9.300903229814139e-07 old loss 9.310856512456667e-07 BETTER
I0328 09:24:39.667704 2301379 finetune.py:68] layer 2_up @ epoch 1 new loss 3.474716095297481e-06 old loss 3.4850622796511743e-06 BETTER
I0328 09:24:43.413089 2301523 finetune.py:45] layer 3_up initial loss 6.6434431573725305e-06
I0328 09:24:54.930727 2301238 finetune.py:68] layer 1_up @ epoch 4 new loss 1.810329081308737e-06 old loss 1.8152178427044419e-06 BETTER
I0328 09:25:03.473960 2301103 finetune.py:45] layer 0_gate initial loss 1.0881256002903683e-06
I0328 09:25:11.380920 2301379 finetune.py:68] layer 2_up @ epoch 2 new loss 3.4660463370528305e-06 old loss 3.474716095297481e-06 BETTER
I0328 09:25:13.446312 2301523 finetune.py:68] layer 3_up @ epoch 0 new loss 6.6195798353874125e-06 old loss 6.6434431573725305e-06 BETTER
I0328 09:25:26.315781 2301238 finetune.py:45] layer 1_gate initial loss 2.203147005275241e-06
I0328 09:25:33.068957 2301103 finetune.py:68] layer 0_gate @ epoch 0 new loss 1.0859313306355034e-06 old loss 1.0881256002903683e-06 BETTER
I0328 09:25:43.524660 2301379 finetune.py:68] layer 2_up @ epoch 3 new loss 3.4584222703415435e-06 old loss 3.4660463370528305e-06 BETTER
I0328 09:25:44.599256 2301523 finetune.py:68] layer 3_up @ epoch 1 new loss 6.60087516735075e-06 old loss 6.6195798353874125e-06 BETTER
I0328 09:25:54.530272 2301238 finetune.py:68] layer 1_gate @ epoch 0 new loss 2.1941318664175924e-06 old loss 2.203147005275241e-06 BETTER
I0328 09:26:04.097979 2301103 finetune.py:68] layer 0_gate @ epoch 1 new loss 1.0844507869478548e-06 old loss 1.0859313306355034e-06 BETTER
I0328 09:26:15.714157 2301523 finetune.py:68] layer 3_up @ epoch 2 new loss 6.584086349903373e-06 old loss 6.60087516735075e-06 BETTER
I0328 09:26:15.802305 2301379 finetune.py:68] layer 2_up @ epoch 4 new loss 3.4514544040575856e-06 old loss 3.4584222703415435e-06 BETTER
I0328 09:26:23.814316 2301238 finetune.py:68] layer 1_gate @ epoch 1 new loss 2.1908826965955086e-06 old loss 2.1941318664175924e-06 BETTER
I0328 09:26:35.247043 2301103 finetune.py:68] layer 0_gate @ epoch 2 new loss 1.0832829957507784e-06 old loss 1.0844507869478548e-06 BETTER
I0328 09:26:46.884099 2301523 finetune.py:68] layer 3_up @ epoch 3 new loss 6.568337539647473e-06 old loss 6.584086349903373e-06 BETTER
I0328 09:26:46.949419 2301379 finetune.py:45] layer 2_gate initial loss 4.325242571212584e-06
I0328 09:26:53.336884 2301238 finetune.py:68] layer 1_gate @ epoch 2 new loss 2.1881269276491366e-06 old loss 2.1908826965955086e-06 BETTER
I0328 09:27:06.395663 2301103 finetune.py:68] layer 0_gate @ epoch 3 new loss 1.082305061572697e-06 old loss 1.0832829957507784e-06 BETTER
I0328 09:27:15.447772 2301379 finetune.py:68] layer 2_gate @ epoch 0 new loss 4.317801085562678e-06 old loss 4.325242571212584e-06 BETTER
I0328 09:27:18.109693 2301523 finetune.py:68] layer 3_up @ epoch 4 new loss 6.553600996994646e-06 old loss 6.568337539647473e-06 BETTER
I0328 09:27:22.951685 2301238 finetune.py:68] layer 1_gate @ epoch 3 new loss 2.185766788898036e-06 old loss 2.1881269276491366e-06 BETTER
I0328 09:27:37.508264 2301103 finetune.py:68] layer 0_gate @ epoch 4 new loss 1.081447067008412e-06 old loss 1.082305061572697e-06 BETTER
I0328 09:27:44.970007 2301379 finetune.py:68] layer 2_gate @ epoch 1 new loss 4.311700195103185e-06 old loss 4.317801085562678e-06 BETTER
I0328 09:27:49.230041 2301523 finetune.py:45] layer 3_gate initial loss 8.039435670070816e-06
I0328 09:27:52.350684 2301238 finetune.py:68] layer 1_gate @ epoch 4 new loss 2.183521019105683e-06 old loss 2.185766788898036e-06 BETTER
I0328 09:28:14.581769 2301379 finetune.py:68] layer 2_gate @ epoch 2 new loss 4.306351456762059e-06 old loss 4.311700195103185e-06 BETTER
I0328 09:28:17.347733 2301523 finetune.py:68] layer 3_gate @ epoch 0 new loss 8.022641850402579e-06 old loss 8.039435670070816e-06 BETTER
I0328 09:28:32.534507 2301103 finetune.py:45] layer 0_down initial loss 1.6660384289934882e-06
I0328 09:28:44.520461 2301379 finetune.py:68] layer 2_gate @ epoch 3 new loss 4.301534318074118e-06 old loss 4.306351456762059e-06 BETTER
I0328 09:28:46.882244 2301523 finetune.py:68] layer 3_gate @ epoch 1 new loss 8.009329576452728e-06 old loss 8.022641850402579e-06 BETTER
I0328 09:28:49.103929 2301238 finetune.py:45] layer 1_down initial loss 3.2538798677705927e-06
I0328 09:28:59.918942 2301103 finetune.py:68] layer 0_down @ epoch 0 new loss 1.6657832020428032e-06 old loss 1.6660384289934882e-06 BETTER
I0328 09:29:14.484507 2301379 finetune.py:68] layer 2_gate @ epoch 4 new loss 4.2970114009222016e-06 old loss 4.301534318074118e-06 BETTER
I0328 09:29:15.174476 2301238 finetune.py:68] layer 1_down @ epoch 0 new loss 3.2532366276427638e-06 old loss 3.2538798677705927e-06 BETTER
I0328 09:29:16.181832 2301523 finetune.py:68] layer 3_gate @ epoch 2 new loss 7.997016837180126e-06 old loss 8.009329576452728e-06 BETTER
I0328 09:29:28.161592 2301103 finetune.py:68] layer 0_down @ epoch 1 new loss 1.665560716901382e-06 old loss 1.6657832020428032e-06 BETTER
I0328 09:29:42.320667 2301238 finetune.py:68] layer 1_down @ epoch 1 new loss 3.2524942525924416e-06 old loss 3.2532366276427638e-06 BETTER
I0328 09:29:45.406879 2301523 finetune.py:68] layer 3_gate @ epoch 3 new loss 7.985560841916595e-06 old loss 7.997016837180126e-06 BETTER
I0328 09:29:56.715999 2301103 finetune.py:68] layer 0_down @ epoch 2 new loss 1.6653652892273385e-06 old loss 1.665560716901382e-06 BETTER
I0328 09:30:09.915348 2301238 finetune.py:68] layer 1_down @ epoch 2 new loss 3.252180022172979e-06 old loss 3.2524942525924416e-06 BETTER
I0328 09:30:11.762794 2301379 finetune.py:45] layer 2_down initial loss 6.581859452126082e-06
I0328 09:30:14.777325 2301523 finetune.py:68] layer 3_gate @ epoch 4 new loss 7.974520485731773e-06 old loss 7.985560841916595e-06 BETTER
I0328 09:30:25.483963 2301103 finetune.py:68] layer 0_down @ epoch 3 new loss 1.6651850955895497e-06 old loss 1.6653652892273385e-06 BETTER
I0328 09:30:37.789233 2301238 finetune.py:68] layer 1_down @ epoch 3 new loss 3.251697307860013e-06 old loss 3.252180022172979e-06 BETTER
I0328 09:30:38.193907 2301379 finetune.py:68] layer 2_down @ epoch 0 new loss 6.581567959074164e-06 old loss 6.581859452126082e-06 BETTER
I0328 09:30:54.406375 2301103 finetune.py:68] layer 0_down @ epoch 4 new loss 1.6650363932058099e-06 old loss 1.6651850955895497e-06 BETTER
0_v proxy err 0.021707510575652122 tr(WHW.T) 60.88684844970703
bpp_loss 3.0727425417280756
0_q proxy err 2.7542946554603986e-05 tr(WHW.T) 288067.5625
bpp_loss 3.9314949852996506
0_k proxy err 2.9359986001509242e-05 tr(WHW.T) 100150.609375
bpp_loss 4.517891734431032
0_o proxy err 0.00315806083381176 tr(WHW.T) 3117.794677734375
bpp_loss 3.1637327531934716
0_up proxy err 0.006264439318329096 tr(WHW.T) 8924.6025390625
bpp_loss 3.4536690968088806
0_gate proxy err 0.003570565255358815 tr(WHW.T) 15778.7138671875
bpp_loss 3.566499313339591
0_down proxy err 0.004972447641193867 tr(WHW.T) 10828.6533203125
bpp_loss 3.446499291541321
I0328 09:31:06.458873 2301238 finetune.py:68] layer 1_down @ epoch 4 new loss 3.2510918117623078e-06 old loss 3.251697307860013e-06 BETTER
I0328 09:31:06.601575 2301379 finetune.py:68] layer 2_down @ epoch 1 new loss 6.581338766409317e-06 old loss 6.581567959074164e-06 BETTER
1_v proxy err 0.009347118437290192 tr(WHW.T) 109.07096099853516
bpp_loss 3.175668287440203
1_q proxy err 3.0920873541617766e-05 tr(WHW.T) 144755.875
bpp_loss 4.176620372280013
1_k proxy err 1.6655203580739908e-05 tr(WHW.T) 75454.078125
bpp_loss 4.9066690998733975
1_o proxy err 0.0059441132470965385 tr(WHW.T) 1991.975830078125
bpp_loss 3.2449490763247013
1_up proxy err 0.007178224623203278 tr(WHW.T) 8229.2978515625
bpp_loss 3.4681153635361364
1_gate proxy err 0.004269933793693781 tr(WHW.T) 13945.3623046875
bpp_loss 3.577649322138833
1_down proxy err 0.00013567546557169408 tr(WHW.T) 13982.3212890625
bpp_loss 3.46073489684412
I0328 09:31:12.750843 2301523 finetune.py:45] layer 3_down initial loss 1.2623129805433564e-05
I0328 09:31:34.274339 2301379 finetune.py:68] layer 2_down @ epoch 2 new loss 6.581123670912348e-06 old loss 6.581338766409317e-06 BETTER
I0328 09:31:38.665107 2301523 finetune.py:68] layer 3_down @ epoch 0 new loss 1.2622806934814434e-05 old loss 1.2623129805433564e-05 BETTER
I0328 09:32:02.076011 2301379 finetune.py:68] layer 2_down @ epoch 3 new loss 6.580944045708748e-06 old loss 6.581123670912348e-06 BETTER
I0328 09:32:05.580166 2301523 finetune.py:68] layer 3_down @ epoch 1 new loss 1.2622687791008502e-05 old loss 1.2622806934814434e-05 BETTER
I0328 09:32:20.495545 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 67.54803776741028s
I0328 09:32:24.416928 2302772 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:32:24.417041 2302772 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:32:24.417085 2302772 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:32:24.787456 2302772 config.py:54] PyTorch version 2.6.0 available.
W0328 09:32:25.010895 2302772 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:32:25.675535 2302772 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:32:25.679487 2300848 quantize_finetune_llama.py:209] layer 5 gpu 1
I0328 09:32:25.693135 2302772 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 09:32:29.869252 2301379 finetune.py:68] layer 2_down @ epoch 4 new loss 6.580787612620043e-06 old loss 6.580944045708748e-06 BETTER
2_v proxy err 0.014439165592193604 tr(WHW.T) 155.95950317382812
bpp_loss 3.0838636256521568
2_q proxy err 0.00023448480351362377 tr(WHW.T) 41460.796875
bpp_loss 4.118764659797307
2_k proxy err 0.00011921383702429011 tr(WHW.T) 22602.73828125
bpp_loss 5.005527193425223
2_o proxy err 0.005844748578965664 tr(WHW.T) 1965.3353271484375
bpp_loss 3.1972469851607457
2_up proxy err 0.008479014039039612 tr(WHW.T) 7601.98095703125
bpp_loss 3.4593315177730153
2_gate proxy err 0.004312511999160051 tr(WHW.T) 15115.3466796875
bpp_loss 3.610616924374231
2_down proxy err 0.007951239123940468 tr(WHW.T) 7743.34228515625
bpp_loss 3.4638859819388017
I0328 09:32:32.692107 2301523 finetune.py:68] layer 3_down @ epoch 2 new loss 1.2622451322386041e-05 old loss 1.2622687791008502e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:32:43.093027 2302772 finetune.py:45] layer 4_v initial loss 1.0437463970447425e-05
W0328 09:32:43.093394 2302772 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:32:59.743295 2301523 finetune.py:68] layer 3_down @ epoch 3 new loss 1.262231762666488e-05 old loss 1.2622451322386041e-05 BETTER
I0328 09:33:17.970888 2302772 finetune.py:68] layer 4_v @ epoch 0 new loss 3.133538712063455e-06 old loss 1.0437463970447425e-05 BETTER
I0328 09:33:26.819908 2301523 finetune.py:68] layer 3_down @ epoch 4 new loss 1.2622173017007299e-05 old loss 1.262231762666488e-05 BETTER
3_v proxy err 0.011007454246282578 tr(WHW.T) 289.3331604003906
bpp_loss 3.1814688175800256
3_q proxy err 0.0002871109754778445 tr(WHW.T) 47570.09375
bpp_loss 4.1590849357889965
3_k proxy err 0.00013982344535179436 tr(WHW.T) 26164.58984375
bpp_loss 5.084764573955908
3_o proxy err 0.006896547973155975 tr(WHW.T) 1858.0908203125
bpp_loss 3.2951848916127346
3_up proxy err 0.008423884399235249 tr(WHW.T) 7536.18994140625
bpp_loss 3.441160658773567
3_gate proxy err 0.0030968054197728634 tr(WHW.T) 20878.1640625
bpp_loss 3.6870768884462968
3_down proxy err 0.009165477938950062 tr(WHW.T) 7014.642578125
bpp_loss 3.4397698571972017
I0328 09:33:40.325710 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 64.3339774608612s
I0328 09:33:44.047786 2302917 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:33:44.047907 2302917 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:33:44.047945 2302917 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:33:44.387737 2302917 config.py:54] PyTorch version 2.6.0 available.
W0328 09:33:44.577326 2302917 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:33:45.152184 2302917 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:33:45.156079 2300848 quantize_finetune_llama.py:209] layer 6 gpu 2
I0328 09:33:45.169788 2302917 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 09:33:54.081579 2302772 finetune.py:68] layer 4_v @ epoch 1 new loss 2.430577524137334e-06 old loss 3.133538712063455e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:34:02.129978 2302917 finetune.py:45] layer 5_v initial loss 1.0717093573475722e-05
W0328 09:34:02.130441 2302917 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:34:30.733300 2302772 finetune.py:68] layer 4_v @ epoch 2 new loss 2.203249550802866e-06 old loss 2.430577524137334e-06 BETTER
I0328 09:34:35.271362 2302917 finetune.py:68] layer 5_v @ epoch 0 new loss 3.984654995292658e-06 old loss 1.0717093573475722e-05 BETTER
I0328 09:34:49.086814 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 63.48260426521301s
I0328 09:34:52.870782 2303053 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:34:52.870883 2303053 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:34:52.870924 2303053 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:34:53.242153 2303053 config.py:54] PyTorch version 2.6.0 available.
W0328 09:34:53.461619 2303053 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:34:54.085412 2303053 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:34:54.089188 2300848 quantize_finetune_llama.py:209] layer 7 gpu 3
I0328 09:34:54.102494 2303053 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:35:07.771398 2302772 finetune.py:68] layer 4_v @ epoch 3 new loss 2.0913676053169183e-06 old loss 2.203249550802866e-06 BETTER
I0328 09:35:09.801954 2302917 finetune.py:68] layer 5_v @ epoch 1 new loss 3.4054878597089555e-06 old loss 3.984654995292658e-06 BETTER
I0328 09:35:12.020192 2303053 finetune.py:45] layer 6_v initial loss 8.741722012928221e-06
W0328 09:35:12.020422 2303053 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:35:45.408986 2302917 finetune.py:68] layer 5_v @ epoch 2 new loss 3.1968556868378073e-06 old loss 3.4054878597089555e-06 BETTER
I0328 09:35:45.722501 2302772 finetune.py:68] layer 4_v @ epoch 4 new loss 2.0231532289471943e-06 old loss 2.0913676053169183e-06 BETTER
I0328 09:35:46.243987 2303053 finetune.py:68] layer 6_v @ epoch 0 new loss 4.4498146962723695e-06 old loss 8.741722012928221e-06 BETTER
I0328 09:36:00.410368 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 65.82455825805664s
I0328 09:36:04.495025 2303192 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:36:04.495136 2303192 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:36:04.495179 2303192 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:36:04.912297 2303192 config.py:54] PyTorch version 2.6.0 available.
W0328 09:36:05.129969 2303192 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 09:36:05.385798 2302772 finetune.py:45] layer 4_q initial loss 2.398421202087775e-06
W0328 09:36:05.938300 2303192 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:36:05.942247 2300848 quantize_finetune_llama.py:209] layer 8 gpu 0
I0328 09:36:05.955866 2303192 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:36:20.498858 2302917 finetune.py:68] layer 5_v @ epoch 3 new loss 3.087276809310424e-06 old loss 3.1968556868378073e-06 BETTER
I0328 09:36:21.078243 2303053 finetune.py:68] layer 6_v @ epoch 1 new loss 4.049908966408111e-06 old loss 4.4498146962723695e-06 BETTER
I0328 09:36:23.855163 2303192 finetune.py:45] layer 7_v initial loss 8.379451173823327e-06
W0328 09:36:23.855392 2303192 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:36:40.603773 2302772 finetune.py:68] layer 4_q @ epoch 0 new loss 2.322407453902997e-06 old loss 2.398421202087775e-06 BETTER
I0328 09:36:56.305584 2302917 finetune.py:68] layer 5_v @ epoch 4 new loss 3.017203880517627e-06 old loss 3.087276809310424e-06 BETTER
I0328 09:36:56.615750 2303053 finetune.py:68] layer 6_v @ epoch 2 new loss 3.884421403199667e-06 old loss 4.049908966408111e-06 BETTER
I0328 09:36:57.426110 2303192 finetune.py:68] layer 7_v @ epoch 0 new loss 5.311367658578092e-06 old loss 8.379451173823327e-06 BETTER
I0328 09:37:15.937295 2302917 finetune.py:45] layer 5_q initial loss 3.5978562209493248e-06
I0328 09:37:17.026533 2302772 finetune.py:68] layer 4_q @ epoch 1 new loss 2.2734795948053943e-06 old loss 2.322407453902997e-06 BETTER
I0328 09:37:31.810180 2303053 finetune.py:68] layer 6_v @ epoch 3 new loss 3.788656840697513e-06 old loss 3.884421403199667e-06 BETTER
I0328 09:37:31.839344 2303192 finetune.py:68] layer 7_v @ epoch 1 new loss 4.991769401385682e-06 old loss 5.311367658578092e-06 BETTER
I0328 09:37:49.369070 2302917 finetune.py:68] layer 5_q @ epoch 0 new loss 3.511026534397388e-06 old loss 3.5978562209493248e-06 BETTER
I0328 09:37:53.673607 2302772 finetune.py:68] layer 4_q @ epoch 2 new loss 2.236784212072962e-06 old loss 2.2734795948053943e-06 BETTER
I0328 09:38:06.445030 2303192 finetune.py:68] layer 7_v @ epoch 2 new loss 4.842229373025475e-06 old loss 4.991769401385682e-06 BETTER
I0328 09:38:07.031552 2303053 finetune.py:68] layer 6_v @ epoch 4 new loss 3.7212466850178316e-06 old loss 3.788656840697513e-06 BETTER
I0328 09:38:23.802957 2302917 finetune.py:68] layer 5_q @ epoch 1 new loss 3.4550780583231244e-06 old loss 3.511026534397388e-06 BETTER
I0328 09:38:26.665252 2303053 finetune.py:45] layer 6_q initial loss 4.415800958668115e-06
I0328 09:38:30.539486 2302772 finetune.py:68] layer 4_q @ epoch 3 new loss 2.2076424102124292e-06 old loss 2.236784212072962e-06 BETTER
I0328 09:38:41.097808 2303192 finetune.py:68] layer 7_v @ epoch 3 new loss 4.745357728097588e-06 old loss 4.842229373025475e-06 BETTER
I0328 09:38:58.510517 2302917 finetune.py:68] layer 5_q @ epoch 2 new loss 3.411437774047954e-06 old loss 3.4550780583231244e-06 BETTER
I0328 09:39:00.381932 2303053 finetune.py:68] layer 6_q @ epoch 0 new loss 4.325755526224384e-06 old loss 4.415800958668115e-06 BETTER
I0328 09:39:07.281765 2302772 finetune.py:68] layer 4_q @ epoch 4 new loss 2.1829112029081443e-06 old loss 2.2076424102124292e-06 BETTER
I0328 09:39:15.683430 2303192 finetune.py:68] layer 7_v @ epoch 4 new loss 4.6736195145058446e-06 old loss 4.745357728097588e-06 BETTER
I0328 09:39:25.088216 2302772 finetune.py:45] layer 4_k initial loss 2.317971166121424e-06
I0328 09:39:33.418913 2302917 finetune.py:68] layer 5_q @ epoch 3 new loss 3.3762028124328936e-06 old loss 3.411437774047954e-06 BETTER
I0328 09:39:35.564036 2303053 finetune.py:68] layer 6_q @ epoch 1 new loss 4.263206392352004e-06 old loss 4.325755526224384e-06 BETTER
I0328 09:39:35.673825 2303192 finetune.py:45] layer 7_q initial loss 5.6894446061050985e-06
I0328 09:40:00.244078 2302772 finetune.py:68] layer 4_k @ epoch 0 new loss 2.28777958000137e-06 old loss 2.317971166121424e-06 BETTER
I0328 09:40:08.455068 2302917 finetune.py:68] layer 5_q @ epoch 4 new loss 3.345725190229132e-06 old loss 3.3762028124328936e-06 BETTER
I0328 09:40:09.066847 2303192 finetune.py:68] layer 7_q @ epoch 0 new loss 5.5891114243422635e-06 old loss 5.6894446061050985e-06 BETTER
I0328 09:40:10.370352 2303053 finetune.py:68] layer 6_q @ epoch 2 new loss 4.214241471345304e-06 old loss 4.263206392352004e-06 BETTER
I0328 09:40:26.559247 2302917 finetune.py:45] layer 5_k initial loss 3.5084628962067654e-06
I0328 09:40:36.780848 2302772 finetune.py:68] layer 4_k @ epoch 1 new loss 2.267893478347105e-06 old loss 2.28777958000137e-06 BETTER
I0328 09:40:43.684761 2303192 finetune.py:68] layer 7_q @ epoch 1 new loss 5.517842964763986e-06 old loss 5.5891114243422635e-06 BETTER
I0328 09:40:45.467245 2303053 finetune.py:68] layer 6_q @ epoch 3 new loss 4.173172328592045e-06 old loss 4.214241471345304e-06 BETTER
I0328 09:41:00.035500 2302917 finetune.py:68] layer 5_k @ epoch 0 new loss 3.469873945505242e-06 old loss 3.5084628962067654e-06 BETTER
I0328 09:41:13.163829 2302772 finetune.py:68] layer 4_k @ epoch 2 new loss 2.251195610369905e-06 old loss 2.267893478347105e-06 BETTER
I0328 09:41:17.975280 2303192 finetune.py:68] layer 7_q @ epoch 2 new loss 5.459596650325693e-06 old loss 5.517842964763986e-06 BETTER
I0328 09:41:20.379401 2303053 finetune.py:68] layer 6_q @ epoch 4 new loss 4.137180440011434e-06 old loss 4.173172328592045e-06 BETTER
I0328 09:41:34.470560 2302917 finetune.py:68] layer 5_k @ epoch 1 new loss 3.4459906146366848e-06 old loss 3.469873945505242e-06 BETTER
I0328 09:41:38.433860 2303053 finetune.py:45] layer 6_k initial loss 4.349008122517262e-06
I0328 09:41:49.835101 2302772 finetune.py:68] layer 4_k @ epoch 3 new loss 2.2368651571014198e-06 old loss 2.251195610369905e-06 BETTER
I0328 09:41:52.345294 2303192 finetune.py:68] layer 7_q @ epoch 3 new loss 5.411970505520003e-06 old loss 5.459596650325693e-06 BETTER
I0328 09:42:09.011734 2302917 finetune.py:68] layer 5_k @ epoch 2 new loss 3.425768227316439e-06 old loss 3.4459906146366848e-06 BETTER
I0328 09:42:11.893268 2303053 finetune.py:68] layer 6_k @ epoch 0 new loss 4.306078608351527e-06 old loss 4.349008122517262e-06 BETTER
I0328 09:42:26.490342 2302772 finetune.py:68] layer 4_k @ epoch 4 new loss 2.2230608465179102e-06 old loss 2.2368651571014198e-06 BETTER
I0328 09:42:26.672300 2303192 finetune.py:68] layer 7_q @ epoch 4 new loss 5.368202437239233e-06 old loss 5.411970505520003e-06 BETTER
I0328 09:42:44.426450 2302917 finetune.py:68] layer 5_k @ epoch 3 new loss 3.4070890251314268e-06 old loss 3.425768227316439e-06 BETTER
I0328 09:42:45.222083 2303192 finetune.py:45] layer 7_k initial loss 5.630072337226011e-06
I0328 09:42:46.997896 2303053 finetune.py:68] layer 6_k @ epoch 1 new loss 4.277964308130322e-06 old loss 4.306078608351527e-06 BETTER
I0328 09:42:47.023266 2302772 finetune.py:45] layer 4_o initial loss 5.000254077458521e-06
I0328 09:43:18.668203 2303192 finetune.py:68] layer 7_k @ epoch 0 new loss 5.576841886067996e-06 old loss 5.630072337226011e-06 BETTER
I0328 09:43:19.656868 2302917 finetune.py:68] layer 5_k @ epoch 4 new loss 3.389817720744759e-06 old loss 3.4070890251314268e-06 BETTER
I0328 09:43:21.993854 2303053 finetune.py:68] layer 6_k @ epoch 2 new loss 4.253163297107676e-06 old loss 4.277964308130322e-06 BETTER
I0328 09:43:22.088073 2302772 finetune.py:68] layer 4_o @ epoch 0 new loss 4.855424776906148e-06 old loss 5.000254077458521e-06 BETTER
I0328 09:43:39.597419 2302917 finetune.py:45] layer 5_o initial loss 6.92543335389928e-06
I0328 09:43:52.780577 2303192 finetune.py:68] layer 7_k @ epoch 1 new loss 5.551200501940912e-06 old loss 5.576841886067996e-06 BETTER
I0328 09:43:56.656942 2303053 finetune.py:68] layer 6_k @ epoch 3 new loss 4.231035291013541e-06 old loss 4.253163297107676e-06 BETTER
I0328 09:43:57.736305 2302772 finetune.py:68] layer 4_o @ epoch 1 new loss 4.804454420082038e-06 old loss 4.855424776906148e-06 BETTER
I0328 09:44:12.547722 2302917 finetune.py:68] layer 5_o @ epoch 0 new loss 6.757832125003915e-06 old loss 6.92543335389928e-06 BETTER
I0328 09:44:27.112966 2303192 finetune.py:68] layer 7_k @ epoch 2 new loss 5.5147966122603975e-06 old loss 5.551200501940912e-06 BETTER
I0328 09:44:31.406344 2303053 finetune.py:68] layer 6_k @ epoch 4 new loss 4.21020740759559e-06 old loss 4.231035291013541e-06 BETTER
I0328 09:44:33.649225 2302772 finetune.py:68] layer 4_o @ epoch 2 new loss 4.76597097076592e-06 old loss 4.804454420082038e-06 BETTER
I0328 09:44:46.315795 2302917 finetune.py:68] layer 5_o @ epoch 1 new loss 6.693159321002895e-06 old loss 6.757832125003915e-06 BETTER
I0328 09:44:51.323398 2303053 finetune.py:45] layer 6_o initial loss 9.484061592957005e-06
I0328 09:45:01.257534 2303192 finetune.py:68] layer 7_k @ epoch 3 new loss 5.489561317517655e-06 old loss 5.5147966122603975e-06 BETTER
I0328 09:45:09.402369 2302772 finetune.py:68] layer 4_o @ epoch 3 new loss 4.734537014883244e-06 old loss 4.76597097076592e-06 BETTER
I0328 09:45:20.485982 2302917 finetune.py:68] layer 5_o @ epoch 2 new loss 6.644359928031918e-06 old loss 6.693159321002895e-06 BETTER
I0328 09:45:24.186951 2303053 finetune.py:68] layer 6_o @ epoch 0 new loss 9.2787604444311e-06 old loss 9.484061592957005e-06 BETTER
I0328 09:45:35.282439 2303192 finetune.py:68] layer 7_k @ epoch 4 new loss 5.461130058392882e-06 old loss 5.489561317517655e-06 BETTER
I0328 09:45:45.477416 2302772 finetune.py:68] layer 4_o @ epoch 4 new loss 4.7067760533536784e-06 old loss 4.734537014883244e-06 BETTER
I0328 09:45:54.814736 2302917 finetune.py:68] layer 5_o @ epoch 3 new loss 6.603839665331179e-06 old loss 6.644359928031918e-06 BETTER
I0328 09:45:55.070242 2303192 finetune.py:45] layer 7_o initial loss 1.2086194146831986e-05
I0328 09:45:58.233703 2303053 finetune.py:68] layer 6_o @ epoch 1 new loss 9.179547305393498e-06 old loss 9.2787604444311e-06 BETTER
I0328 09:46:16.798169 2302772 finetune.py:45] layer 4_up initial loss 1.0606296200421639e-05
I0328 09:46:27.382326 2303192 finetune.py:68] layer 7_o @ epoch 0 new loss 1.1801100299635436e-05 old loss 1.2086194146831986e-05 BETTER
I0328 09:46:29.021836 2302917 finetune.py:68] layer 5_o @ epoch 4 new loss 6.569041033799294e-06 old loss 6.603839665331179e-06 BETTER
I0328 09:46:32.327997 2303053 finetune.py:68] layer 6_o @ epoch 2 new loss 9.101930118049495e-06 old loss 9.179547305393498e-06 BETTER
I0328 09:46:48.733436 2302772 finetune.py:68] layer 4_up @ epoch 0 new loss 1.0536747140577063e-05 old loss 1.0606296200421639e-05 BETTER
I0328 09:47:00.689335 2303192 finetune.py:68] layer 7_o @ epoch 1 new loss 1.1652471584966406e-05 old loss 1.1801100299635436e-05 BETTER
I0328 09:47:01.014436 2302917 finetune.py:45] layer 5_up initial loss 1.5126779544516467e-05
I0328 09:47:06.267905 2303053 finetune.py:68] layer 6_o @ epoch 3 new loss 9.035824405145831e-06 old loss 9.101930118049495e-06 BETTER
I0328 09:47:21.766919 2302772 finetune.py:68] layer 4_up @ epoch 1 new loss 1.0483218829904217e-05 old loss 1.0536747140577063e-05 BETTER
I0328 09:47:31.474639 2302917 finetune.py:68] layer 5_up @ epoch 0 new loss 1.4997470316302497e-05 old loss 1.5126779544516467e-05 BETTER
I0328 09:47:33.878056 2303192 finetune.py:68] layer 7_o @ epoch 2 new loss 1.153867196990177e-05 old loss 1.1652471584966406e-05 BETTER
I0328 09:47:40.264555 2303053 finetune.py:68] layer 6_o @ epoch 4 new loss 8.977531251730397e-06 old loss 9.035824405145831e-06 BETTER
I0328 09:47:55.333197 2302772 finetune.py:68] layer 4_up @ epoch 2 new loss 1.0435390322527383e-05 old loss 1.0483218829904217e-05 BETTER
I0328 09:48:03.107700 2302917 finetune.py:68] layer 5_up @ epoch 1 new loss 1.490070644649677e-05 old loss 1.4997470316302497e-05 BETTER
I0328 09:48:07.247586 2303192 finetune.py:68] layer 7_o @ epoch 3 new loss 1.1442355571489315e-05 old loss 1.153867196990177e-05 BETTER
I0328 09:48:11.911109 2303053 finetune.py:45] layer 6_up initial loss 1.9839897504425608e-05
I0328 09:48:29.044430 2302772 finetune.py:68] layer 4_up @ epoch 3 new loss 1.0390110219304916e-05 old loss 1.0435390322527383e-05 BETTER
I0328 09:48:34.971054 2302917 finetune.py:68] layer 5_up @ epoch 2 new loss 1.4812751942372415e-05 old loss 1.490070644649677e-05 BETTER
I0328 09:48:40.953254 2303192 finetune.py:68] layer 7_o @ epoch 4 new loss 1.1359837117197458e-05 old loss 1.1442355571489315e-05 BETTER
I0328 09:48:42.675243 2303053 finetune.py:68] layer 6_up @ epoch 0 new loss 1.963458998943679e-05 old loss 1.9839897504425608e-05 BETTER
I0328 09:49:02.650306 2302772 finetune.py:68] layer 4_up @ epoch 4 new loss 1.03479342214996e-05 old loss 1.0390110219304916e-05 BETTER
I0328 09:49:06.843233 2302917 finetune.py:68] layer 5_up @ epoch 3 new loss 1.4732021554664243e-05 old loss 1.4812751942372415e-05 BETTER
I0328 09:49:12.617854 2303192 finetune.py:45] layer 7_up initial loss 2.3105836589820683e-05
I0328 09:49:14.666193 2303053 finetune.py:68] layer 6_up @ epoch 1 new loss 1.9473538486636244e-05 old loss 1.963458998943679e-05 BETTER
I0328 09:49:34.897121 2302772 finetune.py:45] layer 4_gate initial loss 1.2443619198165834e-05
I0328 09:49:39.031748 2302917 finetune.py:68] layer 5_up @ epoch 4 new loss 1.4656679923064075e-05 old loss 1.4732021554664243e-05 BETTER
I0328 09:49:42.794150 2303192 finetune.py:68] layer 7_up @ epoch 0 new loss 2.2850259483675472e-05 old loss 2.3105836589820683e-05 BETTER
I0328 09:49:46.912896 2303053 finetune.py:68] layer 6_up @ epoch 2 new loss 1.9329656424815767e-05 old loss 1.9473538486636244e-05 BETTER
I0328 09:50:04.875662 2302772 finetune.py:68] layer 4_gate @ epoch 0 new loss 1.2396838428685442e-05 old loss 1.2443619198165834e-05 BETTER
I0328 09:50:11.002943 2302917 finetune.py:45] layer 5_gate initial loss 1.7664555343799293e-05
I0328 09:50:14.099129 2303192 finetune.py:68] layer 7_up @ epoch 1 new loss 2.2652548068435863e-05 old loss 2.2850259483675472e-05 BETTER
I0328 09:50:19.285609 2303053 finetune.py:68] layer 6_up @ epoch 3 new loss 1.9198603695258498e-05 old loss 1.9329656424815767e-05 BETTER
I0328 09:50:36.195214 2302772 finetune.py:68] layer 4_gate @ epoch 1 new loss 1.2357400009932462e-05 old loss 1.2396838428685442e-05 BETTER
I0328 09:50:39.532875 2302917 finetune.py:68] layer 5_gate @ epoch 0 new loss 1.758291728037875e-05 old loss 1.7664555343799293e-05 BETTER
I0328 09:50:45.453738 2303192 finetune.py:68] layer 7_up @ epoch 2 new loss 2.2476979211205617e-05 old loss 2.2652548068435863e-05 BETTER
I0328 09:50:51.701593 2303053 finetune.py:68] layer 6_up @ epoch 4 new loss 1.9075798263656907e-05 old loss 1.9198603695258498e-05 BETTER
I0328 09:51:07.582695 2302772 finetune.py:68] layer 4_gate @ epoch 2 new loss 1.2321061149123125e-05 old loss 1.2357400009932462e-05 BETTER
I0328 09:51:09.101455 2302917 finetune.py:68] layer 5_gate @ epoch 1 new loss 1.7513053535367362e-05 old loss 1.758291728037875e-05 BETTER
I0328 09:51:16.868182 2303192 finetune.py:68] layer 7_up @ epoch 3 new loss 2.2316904505714774e-05 old loss 2.2476979211205617e-05 BETTER
I0328 09:51:23.899974 2303053 finetune.py:45] layer 6_gate initial loss 2.2655385691905394e-05
I0328 09:51:38.877027 2302917 finetune.py:68] layer 5_gate @ epoch 2 new loss 1.7447880964027718e-05 old loss 1.7513053535367362e-05 BETTER
I0328 09:51:39.197567 2302772 finetune.py:68] layer 4_gate @ epoch 3 new loss 1.228706241818145e-05 old loss 1.2321061149123125e-05 BETTER
I0328 09:51:48.256247 2303192 finetune.py:68] layer 7_up @ epoch 4 new loss 2.2168618670548312e-05 old loss 2.2316904505714774e-05 BETTER
I0328 09:51:52.389596 2303053 finetune.py:68] layer 6_gate @ epoch 0 new loss 2.2527794499183074e-05 old loss 2.2655385691905394e-05 BETTER
I0328 09:52:08.626517 2302917 finetune.py:68] layer 5_gate @ epoch 3 new loss 1.7388363630743697e-05 old loss 1.7447880964027718e-05 BETTER
I0328 09:52:10.526590 2302772 finetune.py:68] layer 4_gate @ epoch 4 new loss 1.2254449757165276e-05 old loss 1.228706241818145e-05 BETTER
I0328 09:52:20.017007 2303192 finetune.py:45] layer 7_gate initial loss 2.653602496138774e-05
I0328 09:52:21.876126 2303053 finetune.py:68] layer 6_gate @ epoch 1 new loss 2.241807487735059e-05 old loss 2.2527794499183074e-05 BETTER
I0328 09:52:38.221256 2302917 finetune.py:68] layer 5_gate @ epoch 4 new loss 1.7330199625575915e-05 old loss 1.7388363630743697e-05 BETTER
I0328 09:52:47.977679 2303192 finetune.py:68] layer 7_gate @ epoch 0 new loss 2.6382522264611907e-05 old loss 2.653602496138774e-05 BETTER
I0328 09:52:51.477522 2303053 finetune.py:68] layer 6_gate @ epoch 2 new loss 2.2317130060400814e-05 old loss 2.241807487735059e-05 BETTER
I0328 09:53:07.034118 2302772 finetune.py:45] layer 4_down initial loss 2.032353586400859e-05
I0328 09:53:17.158499 2303192 finetune.py:68] layer 7_gate @ epoch 1 new loss 2.6252657335135154e-05 old loss 2.6382522264611907e-05 BETTER
I0328 09:53:21.230386 2303053 finetune.py:68] layer 6_gate @ epoch 3 new loss 2.2221905965125188e-05 old loss 2.2317130060400814e-05 BETTER
I0328 09:53:34.369640 2302772 finetune.py:68] layer 4_down @ epoch 0 new loss 2.032265365414787e-05 old loss 2.032353586400859e-05 BETTER
I0328 09:53:35.029934 2302917 finetune.py:45] layer 5_down initial loss 2.8427126380847767e-05
I0328 09:53:46.581145 2303192 finetune.py:68] layer 7_gate @ epoch 2 new loss 2.6133007850148715e-05 old loss 2.6252657335135154e-05 BETTER
I0328 09:53:51.094360 2303053 finetune.py:68] layer 6_gate @ epoch 4 new loss 2.2131613150122575e-05 old loss 2.2221905965125188e-05 BETTER
I0328 09:54:01.173408 2302917 finetune.py:68] layer 5_down @ epoch 0 new loss 2.8425825803424232e-05 old loss 2.8427126380847767e-05 BETTER
I0328 09:54:02.940239 2302772 finetune.py:68] layer 4_down @ epoch 1 new loss 2.0322080672485754e-05 old loss 2.032265365414787e-05 BETTER
I0328 09:54:15.993953 2303192 finetune.py:68] layer 7_gate @ epoch 3 new loss 2.602004497020971e-05 old loss 2.6133007850148715e-05 BETTER
I0328 09:54:28.409898 2302917 finetune.py:68] layer 5_down @ epoch 1 new loss 2.8424961783457547e-05 old loss 2.8425825803424232e-05 BETTER
I0328 09:54:31.435083 2302772 finetune.py:68] layer 4_down @ epoch 2 new loss 2.0321604097262025e-05 old loss 2.0322080672485754e-05 BETTER
I0328 09:54:45.439834 2303192 finetune.py:68] layer 7_gate @ epoch 4 new loss 2.5914026991813444e-05 old loss 2.602004497020971e-05 BETTER
I0328 09:54:48.513621 2303053 finetune.py:45] layer 6_down initial loss 3.55531919922214e-05
I0328 09:54:55.907368 2302917 finetune.py:68] layer 5_down @ epoch 2 new loss 2.8424303309293464e-05 old loss 2.8424961783457547e-05 BETTER
I0328 09:55:00.455099 2302772 finetune.py:68] layer 4_down @ epoch 3 new loss 2.032118572969921e-05 old loss 2.0321604097262025e-05 BETTER
I0328 09:55:14.866186 2303053 finetune.py:68] layer 6_down @ epoch 0 new loss 3.555136936483905e-05 old loss 3.55531919922214e-05 BETTER
I0328 09:55:23.572513 2302917 finetune.py:68] layer 5_down @ epoch 3 new loss 2.8423643016139977e-05 old loss 2.8424303309293464e-05 BETTER
I0328 09:55:29.332989 2302772 finetune.py:68] layer 4_down @ epoch 4 new loss 2.03209237952251e-05 old loss 2.032118572969921e-05 BETTER
4_v proxy err 0.009985777549445629 tr(WHW.T) 285.30712890625
bpp_loss 3.2270970851532184
4_q proxy err 0.00024274700263049453 tr(WHW.T) 50121.71484375
bpp_loss 4.122461688588373
4_k proxy err 0.00011284677020739764 tr(WHW.T) 29286.984375
bpp_loss 5.06341970001813
4_o proxy err 0.007918448187410831 tr(WHW.T) 1302.2215576171875
bpp_loss 3.3047518954845145
4_up proxy err 0.008567177690565586 tr(WHW.T) 7379.72119140625
bpp_loss 3.413665648150657
4_gate proxy err 0.0022282691206783056 tr(WHW.T) 29105.384765625
bpp_loss 3.760330719474171
4_down proxy err 0.009875042364001274 tr(WHW.T) 6406.79638671875
bpp_loss 3.414796209395198
I0328 09:55:43.070879 2303053 finetune.py:68] layer 6_down @ epoch 1 new loss 3.554997601895593e-05 old loss 3.555136936483905e-05 BETTER
I0328 09:55:43.824137 2303192 finetune.py:45] layer 7_down initial loss 4.053095472045243e-05
I0328 09:55:52.118073 2302917 finetune.py:68] layer 5_down @ epoch 4 new loss 2.8423235562513582e-05 old loss 2.8423643016139977e-05 BETTER
5_v proxy err 0.01412282045930624 tr(WHW.T) 208.81988525390625
bpp_loss 3.1138088123407215
5_q proxy err 0.00035120226675644517 tr(WHW.T) 36003.4765625
bpp_loss 4.097817900532391
5_k proxy err 0.0001534957846160978 tr(WHW.T) 22994.52734375
bpp_loss 5.03643178823404
5_o proxy err 0.00860020611435175 tr(WHW.T) 1059.8447265625
bpp_loss 3.25271420419449
5_up proxy err 0.008243613876402378 tr(WHW.T) 7655.91796875
bpp_loss 3.4189070743907775
5_gate proxy err 0.0021325901616364717 tr(WHW.T) 30358.6640625
bpp_loss 3.7626487308714007
5_down proxy err 0.009519357234239578 tr(WHW.T) 6425.3662109375
bpp_loss 3.420197733029324
I0328 09:56:09.793708 2303192 finetune.py:68] layer 7_down @ epoch 0 new loss 4.0528517274651676e-05 old loss 4.053095472045243e-05 BETTER
I0328 09:56:10.969019 2303053 finetune.py:68] layer 6_down @ epoch 2 new loss 3.554896466084756e-05 old loss 3.554997601895593e-05 BETTER
I0328 09:56:36.888297 2303192 finetune.py:68] layer 7_down @ epoch 1 new loss 4.052658550790511e-05 old loss 4.0528517274651676e-05 BETTER
I0328 09:56:38.773307 2303053 finetune.py:68] layer 6_down @ epoch 3 new loss 3.554809154593386e-05 old loss 3.554896466084756e-05 BETTER
I0328 09:57:04.099074 2303192 finetune.py:68] layer 7_down @ epoch 2 new loss 4.052531949128024e-05 old loss 4.052658550790511e-05 BETTER
I0328 09:57:05.208760 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 67.83923697471619s
I0328 09:57:06.645090 2303053 finetune.py:68] layer 6_down @ epoch 4 new loss 3.554742579581216e-05 old loss 3.554809154593386e-05 BETTER
6_v proxy err 0.011892334558069706 tr(WHW.T) 253.63377380371094
bpp_loss 3.159280013642274
6_q proxy err 0.00036280517815612257 tr(WHW.T) 35647.43359375
bpp_loss 4.1453550679725595
6_k proxy err 0.0001326272904407233 tr(WHW.T) 26153.07421875
bpp_loss 5.107808374916203
6_o proxy err 0.009929091669619083 tr(WHW.T) 1012.3342895507812
bpp_loss 3.2846456462866627
6_up proxy err 0.007768460549414158 tr(WHW.T) 7923.8388671875
bpp_loss 3.4180333855162774
6_gate proxy err 0.0017680261516943574 tr(WHW.T) 35727.94921875
bpp_loss 3.7679351958712295
6_down proxy err 0.009133233688771725 tr(WHW.T) 6499.365234375
bpp_loss 3.42035475195319
I0328 09:57:08.837598 2304468 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:57:08.837700 2304468 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:57:08.837742 2304468 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:57:09.179292 2304468 config.py:54] PyTorch version 2.6.0 available.
W0328 09:57:09.380901 2304468 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:57:09.974847 2304468 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:57:09.978626 2300848 quantize_finetune_llama.py:209] layer 9 gpu 1
I0328 09:57:09.992237 2304468 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:57:27.317927 2304468 finetune.py:45] layer 8_v initial loss 8.874870218278375e-06
W0328 09:57:27.318186 2304468 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:57:31.283872 2303192 finetune.py:68] layer 7_down @ epoch 3 new loss 4.0524377254769206e-05 old loss 4.052531949128024e-05 BETTER
I0328 09:57:58.602829 2303192 finetune.py:68] layer 7_down @ epoch 4 new loss 4.052356962347403e-05 old loss 4.0524377254769206e-05 BETTER
7_v proxy err 0.009869711473584175 tr(WHW.T) 309.4270935058594
bpp_loss 3.1574525365722366
7_q proxy err 0.00037109022377990186 tr(WHW.T) 35150.21484375
bpp_loss 4.068210041150451
7_k proxy err 0.00013082487566862255 tr(WHW.T) 26836.509765625
bpp_loss 5.134989231126383
7_o proxy err 0.00871550478041172 tr(WHW.T) 963.3968505859375
bpp_loss 3.296067611547187
7_up proxy err 0.0070333038456737995 tr(WHW.T) 8625.3505859375
bpp_loss 3.4313829625690624
7_gate proxy err 0.0017792201833799481 tr(WHW.T) 34879.12890625
bpp_loss 3.7383061763830483
7_down proxy err 0.009123497642576694 tr(WHW.T) 6547.7333984375
bpp_loss 3.434852661448531
I0328 09:58:02.139175 2304468 finetune.py:68] layer 8_v @ epoch 0 new loss 6.098817266320111e-06 old loss 8.874870218278375e-06 BETTER
I0328 09:58:15.676850 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 64.40829277038574s
I0328 09:58:19.326050 2304604 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:58:19.326150 2304604 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:58:19.326193 2304604 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:58:19.658736 2304604 config.py:54] PyTorch version 2.6.0 available.
W0328 09:58:19.870193 2304604 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:58:20.480188 2304604 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:58:20.484007 2300848 quantize_finetune_llama.py:209] layer 10 gpu 2
I0328 09:58:20.497367 2304604 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:58:37.556335 2304604 finetune.py:45] layer 9_v initial loss 9.822245374380145e-06
W0328 09:58:37.556581 2304604 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:58:38.486832 2304468 finetune.py:68] layer 8_v @ epoch 1 new loss 5.7564898270356935e-06 old loss 6.098817266320111e-06 BETTER
I0328 09:59:10.749939 2304604 finetune.py:68] layer 9_v @ epoch 0 new loss 6.203519660630263e-06 old loss 9.822245374380145e-06 BETTER
I0328 09:59:15.437415 2304468 finetune.py:68] layer 8_v @ epoch 2 new loss 5.585094640991883e-06 old loss 5.7564898270356935e-06 BETTER
I0328 09:59:25.039466 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 64.09244227409363s
I0328 09:59:28.705085 2304740 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 09:59:28.705190 2304740 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 09:59:28.705231 2304740 utils.py:162] NumExpr defaulting to 16 threads.
I0328 09:59:29.039746 2304740 config.py:54] PyTorch version 2.6.0 available.
W0328 09:59:29.242504 2304740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 09:59:29.816216 2304740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 09:59:29.819928 2300848 quantize_finetune_llama.py:209] layer 11 gpu 3
I0328 09:59:29.833335 2304740 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 09:59:45.271264 2304604 finetune.py:68] layer 9_v @ epoch 1 new loss 5.8205905588692985e-06 old loss 6.203519660630263e-06 BETTER
I0328 09:59:47.678920 2304740 finetune.py:45] layer 10_v initial loss 1.1930802429560572e-05
W0328 09:59:47.679318 2304740 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 09:59:52.569045 2304468 finetune.py:68] layer 8_v @ epoch 3 new loss 5.466801667353138e-06 old loss 5.585094640991883e-06 BETTER
I0328 10:00:20.112233 2304604 finetune.py:68] layer 9_v @ epoch 2 new loss 5.634689387079561e-06 old loss 5.8205905588692985e-06 BETTER
I0328 10:00:21.234368 2304740 finetune.py:68] layer 10_v @ epoch 0 new loss 8.389059985347558e-06 old loss 1.1930802429560572e-05 BETTER
I0328 10:00:29.882966 2304468 finetune.py:68] layer 8_v @ epoch 4 new loss 5.38732501809136e-06 old loss 5.466801667353138e-06 BETTER
I0328 10:00:34.572993 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 64.27976369857788s
I0328 10:00:38.373412 2304879 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 10:00:38.373510 2304879 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 10:00:38.373550 2304879 utils.py:162] NumExpr defaulting to 16 threads.
I0328 10:00:38.764881 2304879 config.py:54] PyTorch version 2.6.0 available.
W0328 10:00:38.984962 2304879 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 10:00:39.625730 2304879 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 10:00:39.629461 2300848 quantize_finetune_llama.py:209] layer 12 gpu 0
I0328 10:00:39.643228 2304879 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 10:00:49.248095 2304468 finetune.py:45] layer 8_q initial loss 6.53968527331017e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 10:00:55.135268 2304604 finetune.py:68] layer 9_v @ epoch 3 new loss 5.5097980293794535e-06 old loss 5.634689387079561e-06 BETTER
I0328 10:00:55.948404 2304740 finetune.py:68] layer 10_v @ epoch 1 new loss 7.96572112449212e-06 old loss 8.389059985347558e-06 BETTER
I0328 10:00:57.637266 2304879 finetune.py:45] layer 11_v initial loss 1.0211891094513703e-05
W0328 10:00:57.637490 2304879 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 10:01:24.416931 2304468 finetune.py:68] layer 8_q @ epoch 0 new loss 6.405139629350742e-06 old loss 6.53968527331017e-06 BETTER
I0328 10:01:30.621661 2304604 finetune.py:68] layer 9_v @ epoch 4 new loss 5.421313744591316e-06 old loss 5.5097980293794535e-06 BETTER
I0328 10:01:30.901594 2304879 finetune.py:68] layer 11_v @ epoch 0 new loss 7.423125680361409e-06 old loss 1.0211891094513703e-05 BETTER
I0328 10:01:31.331812 2304740 finetune.py:68] layer 10_v @ epoch 2 new loss 7.751654266030528e-06 old loss 7.96572112449212e-06 BETTER
I0328 10:01:50.380907 2304604 finetune.py:45] layer 9_q initial loss 6.673629741271725e-06
I0328 10:02:00.676207 2304468 finetune.py:68] layer 8_q @ epoch 1 new loss 6.3195293478202075e-06 old loss 6.405139629350742e-06 BETTER
I0328 10:02:04.842363 2304879 finetune.py:68] layer 11_v @ epoch 1 new loss 7.046081464068266e-06 old loss 7.423125680361409e-06 BETTER
I0328 10:02:06.406991 2304740 finetune.py:68] layer 10_v @ epoch 3 new loss 7.59315707909991e-06 old loss 7.751654266030528e-06 BETTER
I0328 10:02:23.857805 2304604 finetune.py:68] layer 9_q @ epoch 0 new loss 6.549598310812144e-06 old loss 6.673629741271725e-06 BETTER
I0328 10:02:37.171689 2304468 finetune.py:68] layer 8_q @ epoch 2 new loss 6.253192168514943e-06 old loss 6.3195293478202075e-06 BETTER
I0328 10:02:39.370972 2304879 finetune.py:68] layer 11_v @ epoch 2 new loss 6.860624125692993e-06 old loss 7.046081464068266e-06 BETTER
I0328 10:02:41.668355 2304740 finetune.py:68] layer 10_v @ epoch 4 new loss 7.481303327949718e-06 old loss 7.59315707909991e-06 BETTER
I0328 10:02:58.159657 2304604 finetune.py:68] layer 9_q @ epoch 1 new loss 6.461272278102115e-06 old loss 6.549598310812144e-06 BETTER
I0328 10:03:01.438571 2304740 finetune.py:45] layer 10_q initial loss 8.937096026784275e-06
I0328 10:03:13.833656 2304468 finetune.py:68] layer 8_q @ epoch 3 new loss 6.189977739268215e-06 old loss 6.253192168514943e-06 BETTER
I0328 10:03:13.926137 2304879 finetune.py:68] layer 11_v @ epoch 3 new loss 6.722866601194255e-06 old loss 6.860624125692993e-06 BETTER
I0328 10:03:32.729966 2304604 finetune.py:68] layer 9_q @ epoch 2 new loss 6.3918373598426115e-06 old loss 6.461272278102115e-06 BETTER
I0328 10:03:34.992482 2304740 finetune.py:68] layer 10_q @ epoch 0 new loss 8.771711691224482e-06 old loss 8.937096026784275e-06 BETTER
I0328 10:03:48.438438 2304879 finetune.py:68] layer 11_v @ epoch 4 new loss 6.6242300817975774e-06 old loss 6.722866601194255e-06 BETTER
I0328 10:03:50.327331 2304468 finetune.py:68] layer 8_q @ epoch 4 new loss 6.143893642729381e-06 old loss 6.189977739268215e-06 BETTER
I0328 10:04:07.760776 2304604 finetune.py:68] layer 9_q @ epoch 3 new loss 6.340987056319136e-06 old loss 6.3918373598426115e-06 BETTER
I0328 10:04:08.542239 2304468 finetune.py:45] layer 8_k initial loss 6.432790087274043e-06
I0328 10:04:08.798406 2304879 finetune.py:45] layer 11_q initial loss 8.314968908962328e-06
I0328 10:04:09.936637 2304740 finetune.py:68] layer 10_q @ epoch 1 new loss 8.654965313326102e-06 old loss 8.771711691224482e-06 BETTER
I0328 10:04:42.130004 2304879 finetune.py:68] layer 11_q @ epoch 0 new loss 8.182007150026038e-06 old loss 8.314968908962328e-06 BETTER
I0328 10:04:42.825500 2304604 finetune.py:68] layer 9_q @ epoch 4 new loss 6.289740667853039e-06 old loss 6.340987056319136e-06 BETTER
I0328 10:04:43.916701 2304468 finetune.py:68] layer 8_k @ epoch 0 new loss 6.357093752740184e-06 old loss 6.432790087274043e-06 BETTER
I0328 10:04:44.835301 2304740 finetune.py:68] layer 10_q @ epoch 2 new loss 8.55556208989583e-06 old loss 8.654965313326102e-06 BETTER
I0328 10:05:00.862435 2304604 finetune.py:45] layer 9_k initial loss 6.617158760491293e-06
I0328 10:05:16.324949 2304879 finetune.py:68] layer 11_q @ epoch 1 new loss 8.064257599471603e-06 old loss 8.182007150026038e-06 BETTER
I0328 10:05:19.520533 2304740 finetune.py:68] layer 10_q @ epoch 3 new loss 8.479451025777962e-06 old loss 8.55556208989583e-06 BETTER
I0328 10:05:20.063449 2304468 finetune.py:68] layer 8_k @ epoch 1 new loss 6.317015959211858e-06 old loss 6.357093752740184e-06 BETTER
I0328 10:05:34.163394 2304604 finetune.py:68] layer 9_k @ epoch 0 new loss 6.567596756212879e-06 old loss 6.617158760491293e-06 BETTER
I0328 10:05:50.535171 2304879 finetune.py:68] layer 11_q @ epoch 2 new loss 7.974789696163498e-06 old loss 8.064257599471603e-06 BETTER
I0328 10:05:54.334862 2304740 finetune.py:68] layer 10_q @ epoch 4 new loss 8.406430424656719e-06 old loss 8.479451025777962e-06 BETTER
I0328 10:05:56.709184 2304468 finetune.py:68] layer 8_k @ epoch 2 new loss 6.282118647504831e-06 old loss 6.317015959211858e-06 BETTER
I0328 10:06:08.493802 2304604 finetune.py:68] layer 9_k @ epoch 1 new loss 6.511086667160271e-06 old loss 6.567596756212879e-06 BETTER
I0328 10:06:12.930972 2304740 finetune.py:45] layer 10_k initial loss 8.717936907487456e-06
I0328 10:06:24.807825 2304879 finetune.py:68] layer 11_q @ epoch 3 new loss 7.90747617429588e-06 old loss 7.974789696163498e-06 BETTER
I0328 10:06:33.327315 2304468 finetune.py:68] layer 8_k @ epoch 3 new loss 6.248165391298244e-06 old loss 6.282118647504831e-06 BETTER
I0328 10:06:43.114242 2304604 finetune.py:68] layer 9_k @ epoch 2 new loss 6.474369456554996e-06 old loss 6.511086667160271e-06 BETTER
I0328 10:06:46.469388 2304740 finetune.py:68] layer 10_k @ epoch 0 new loss 8.652996257296763e-06 old loss 8.717936907487456e-06 BETTER
I0328 10:06:59.240128 2304879 finetune.py:68] layer 11_q @ epoch 4 new loss 7.854435352783184e-06 old loss 7.90747617429588e-06 BETTER
I0328 10:07:10.080463 2304468 finetune.py:68] layer 8_k @ epoch 4 new loss 6.22356037638383e-06 old loss 6.248165391298244e-06 BETTER
I0328 10:07:17.553879 2304879 finetune.py:45] layer 11_k initial loss 8.196092494472396e-06
I0328 10:07:18.152658 2304604 finetune.py:68] layer 9_k @ epoch 3 new loss 6.445932285714662e-06 old loss 6.474369456554996e-06 BETTER
I0328 10:07:21.053149 2304740 finetune.py:68] layer 10_k @ epoch 1 new loss 8.597561645729002e-06 old loss 8.652996257296763e-06 BETTER
I0328 10:07:29.758749 2304468 finetune.py:45] layer 8_o initial loss 1.4324814401334152e-05
I0328 10:07:50.518705 2304879 finetune.py:68] layer 11_k @ epoch 0 new loss 8.120855454762932e-06 old loss 8.196092494472396e-06 BETTER
I0328 10:07:53.053810 2304604 finetune.py:68] layer 9_k @ epoch 4 new loss 6.407997716451064e-06 old loss 6.445932285714662e-06 BETTER
I0328 10:07:55.684738 2304740 finetune.py:68] layer 10_k @ epoch 2 new loss 8.550493475922849e-06 old loss 8.597561645729002e-06 BETTER
I0328 10:08:04.637305 2304468 finetune.py:68] layer 8_o @ epoch 0 new loss 1.4008184734848328e-05 old loss 1.4324814401334152e-05 BETTER
I0328 10:08:12.783429 2304604 finetune.py:45] layer 9_o initial loss 1.4871929124637973e-05
I0328 10:08:24.458101 2304879 finetune.py:68] layer 11_k @ epoch 1 new loss 8.075263394857757e-06 old loss 8.120855454762932e-06 BETTER
I0328 10:08:30.368272 2304740 finetune.py:68] layer 10_k @ epoch 3 new loss 8.506594895152375e-06 old loss 8.550493475922849e-06 BETTER
I0328 10:08:40.234076 2304468 finetune.py:68] layer 8_o @ epoch 1 new loss 1.3843427950632758e-05 old loss 1.4008184734848328e-05 BETTER
I0328 10:08:45.523720 2304604 finetune.py:68] layer 9_o @ epoch 0 new loss 1.4493842172669247e-05 old loss 1.4871929124637973e-05 BETTER
I0328 10:08:58.405538 2304879 finetune.py:68] layer 11_k @ epoch 2 new loss 8.041745786613319e-06 old loss 8.075263394857757e-06 BETTER
I0328 10:09:04.972771 2304740 finetune.py:68] layer 10_k @ epoch 4 new loss 8.470539796689991e-06 old loss 8.506594895152375e-06 BETTER
I0328 10:09:15.909983 2304468 finetune.py:68] layer 8_o @ epoch 2 new loss 1.371425787510816e-05 old loss 1.3843427950632758e-05 BETTER
I0328 10:09:19.091082 2304604 finetune.py:68] layer 9_o @ epoch 1 new loss 1.4287574231275357e-05 old loss 1.4493842172669247e-05 BETTER
I0328 10:09:24.923240 2304740 finetune.py:45] layer 10_o initial loss 1.8832563000614755e-05
I0328 10:09:32.506430 2304879 finetune.py:68] layer 11_k @ epoch 3 new loss 7.989715413714293e-06 old loss 8.041745786613319e-06 BETTER
I0328 10:09:51.755156 2304468 finetune.py:68] layer 8_o @ epoch 3 new loss 1.3604674677480944e-05 old loss 1.371425787510816e-05 BETTER
I0328 10:09:52.824580 2304604 finetune.py:68] layer 9_o @ epoch 2 new loss 1.4129394003248308e-05 old loss 1.4287574231275357e-05 BETTER
I0328 10:09:57.675042 2304740 finetune.py:68] layer 10_o @ epoch 0 new loss 1.8269027350470424e-05 old loss 1.8832563000614755e-05 BETTER
I0328 10:10:06.383339 2304879 finetune.py:68] layer 11_k @ epoch 4 new loss 7.944188837427646e-06 old loss 7.989715413714293e-06 BETTER
I0328 10:10:25.805088 2304879 finetune.py:45] layer 11_o initial loss 1.8715269106905907e-05
I0328 10:10:26.687875 2304604 finetune.py:68] layer 9_o @ epoch 3 new loss 1.3999890143168159e-05 old loss 1.4129394003248308e-05 BETTER
I0328 10:10:27.551483 2304468 finetune.py:68] layer 8_o @ epoch 4 new loss 1.3508300071407575e-05 old loss 1.3604674677480944e-05 BETTER
I0328 10:10:31.485377 2304740 finetune.py:68] layer 10_o @ epoch 1 new loss 1.79813305294374e-05 old loss 1.8269027350470424e-05 BETTER
I0328 10:10:57.928930 2304879 finetune.py:68] layer 11_o @ epoch 0 new loss 1.8209622794529423e-05 old loss 1.8715269106905907e-05 BETTER
I0328 10:10:59.099800 2304468 finetune.py:45] layer 8_up initial loss 2.6493671612115577e-05
I0328 10:11:00.659400 2304604 finetune.py:68] layer 9_o @ epoch 4 new loss 1.3888059584132861e-05 old loss 1.3999890143168159e-05 BETTER
I0328 10:11:05.328376 2304740 finetune.py:68] layer 10_o @ epoch 2 new loss 1.776486897142604e-05 old loss 1.79813305294374e-05 BETTER
I0328 10:11:31.091947 2304468 finetune.py:68] layer 8_up @ epoch 0 new loss 2.6201312721241266e-05 old loss 2.6493671612115577e-05 BETTER
I0328 10:11:31.491005 2304879 finetune.py:68] layer 11_o @ epoch 1 new loss 1.7932743503479287e-05 old loss 1.8209622794529423e-05 BETTER
I0328 10:11:32.860894 2304604 finetune.py:45] layer 9_up initial loss 2.808708813972771e-05
I0328 10:11:39.180779 2304740 finetune.py:68] layer 10_o @ epoch 3 new loss 1.7587281035957858e-05 old loss 1.776486897142604e-05 BETTER
I0328 10:12:03.477852 2304604 finetune.py:68] layer 9_up @ epoch 0 new loss 2.7743422833736986e-05 old loss 2.808708813972771e-05 BETTER
I0328 10:12:04.256293 2304468 finetune.py:68] layer 8_up @ epoch 1 new loss 2.5971614377340302e-05 old loss 2.6201312721241266e-05 BETTER
I0328 10:12:05.046477 2304879 finetune.py:68] layer 11_o @ epoch 2 new loss 1.7718502931529656e-05 old loss 1.7932743503479287e-05 BETTER
I0328 10:12:13.161050 2304740 finetune.py:68] layer 10_o @ epoch 4 new loss 1.7433609173167497e-05 old loss 1.7587281035957858e-05 BETTER
I0328 10:12:35.312135 2304604 finetune.py:68] layer 9_up @ epoch 1 new loss 2.7472775400383398e-05 old loss 2.7743422833736986e-05 BETTER
I0328 10:12:37.926328 2304468 finetune.py:68] layer 8_up @ epoch 2 new loss 2.5766998078324832e-05 old loss 2.5971614377340302e-05 BETTER
I0328 10:12:38.630358 2304879 finetune.py:68] layer 11_o @ epoch 3 new loss 1.7543141439091414e-05 old loss 1.7718502931529656e-05 BETTER
I0328 10:12:46.359794 2304740 finetune.py:45] layer 10_up initial loss 3.1984716770239174e-05
I0328 10:13:07.249804 2304604 finetune.py:68] layer 9_up @ epoch 2 new loss 2.7236008463660255e-05 old loss 2.7472775400383398e-05 BETTER
I0328 10:13:11.645229 2304468 finetune.py:68] layer 8_up @ epoch 3 new loss 2.558196683821734e-05 old loss 2.5766998078324832e-05 BETTER
I0328 10:13:12.246808 2304879 finetune.py:68] layer 11_o @ epoch 4 new loss 1.739258186717052e-05 old loss 1.7543141439091414e-05 BETTER
I0328 10:13:17.011864 2304740 finetune.py:68] layer 10_up @ epoch 0 new loss 3.1609382858732715e-05 old loss 3.1984716770239174e-05 BETTER
I0328 10:13:39.172159 2304604 finetune.py:68] layer 9_up @ epoch 3 new loss 2.7019363187719136e-05 old loss 2.7236008463660255e-05 BETTER
I0328 10:13:44.659963 2304879 finetune.py:45] layer 11_up initial loss 3.28051028191112e-05
I0328 10:13:45.375221 2304468 finetune.py:68] layer 8_up @ epoch 4 new loss 2.5408764486201108e-05 old loss 2.558196683821734e-05 BETTER
I0328 10:13:48.825074 2304740 finetune.py:68] layer 10_up @ epoch 1 new loss 3.131206540274434e-05 old loss 3.1609382858732715e-05 BETTER
I0328 10:14:11.173221 2304604 finetune.py:68] layer 9_up @ epoch 4 new loss 2.6822477593668737e-05 old loss 2.7019363187719136e-05 BETTER
I0328 10:14:14.781691 2304879 finetune.py:68] layer 11_up @ epoch 0 new loss 3.239401848986745e-05 old loss 3.28051028191112e-05 BETTER
I0328 10:14:17.309657 2304468 finetune.py:45] layer 8_gate initial loss 3.0106439226074144e-05
I0328 10:14:21.047314 2304740 finetune.py:68] layer 10_up @ epoch 2 new loss 3.105384894297458e-05 old loss 3.131206540274434e-05 BETTER
I0328 10:14:43.495628 2304604 finetune.py:45] layer 9_gate initial loss 3.188628033967689e-05
I0328 10:14:46.322468 2304879 finetune.py:68] layer 11_up @ epoch 1 new loss 3.2066975109046325e-05 old loss 3.239401848986745e-05 BETTER
I0328 10:14:47.436454 2304468 finetune.py:68] layer 8_gate @ epoch 0 new loss 2.9934029953437857e-05 old loss 3.0106439226074144e-05 BETTER
I0328 10:14:53.417190 2304740 finetune.py:68] layer 10_up @ epoch 3 new loss 3.081586692132987e-05 old loss 3.105384894297458e-05 BETTER
I0328 10:15:11.976625 2304604 finetune.py:68] layer 9_gate @ epoch 0 new loss 3.168440525769256e-05 old loss 3.188628033967689e-05 BETTER
I0328 10:15:18.118343 2304879 finetune.py:68] layer 11_up @ epoch 2 new loss 3.178097904310562e-05 old loss 3.2066975109046325e-05 BETTER
I0328 10:15:18.595754 2304468 finetune.py:68] layer 8_gate @ epoch 1 new loss 2.9783164791297168e-05 old loss 2.9934029953437857e-05 BETTER
I0328 10:15:25.833554 2304740 finetune.py:68] layer 10_up @ epoch 4 new loss 3.059925074921921e-05 old loss 3.081586692132987e-05 BETTER
I0328 10:15:41.445625 2304604 finetune.py:68] layer 9_gate @ epoch 1 new loss 3.151365672238171e-05 old loss 3.168440525769256e-05 BETTER
I0328 10:15:49.694135 2304879 finetune.py:68] layer 11_up @ epoch 3 new loss 3.1520379707217216e-05 old loss 3.178097904310562e-05 BETTER
I0328 10:15:49.839178 2304468 finetune.py:68] layer 8_gate @ epoch 2 new loss 2.9643255402334034e-05 old loss 2.9783164791297168e-05 BETTER
I0328 10:15:58.317267 2304740 finetune.py:45] layer 10_gate initial loss 3.615665264078416e-05
I0328 10:16:11.197225 2304604 finetune.py:68] layer 9_gate @ epoch 2 new loss 3.135505903628655e-05 old loss 3.151365672238171e-05 BETTER
I0328 10:16:21.313204 2304879 finetune.py:68] layer 11_up @ epoch 4 new loss 3.1282022973755375e-05 old loss 3.1520379707217216e-05 BETTER
I0328 10:16:21.334125 2304468 finetune.py:68] layer 8_gate @ epoch 3 new loss 2.9513388653867878e-05 old loss 2.9643255402334034e-05 BETTER
I0328 10:16:26.937160 2304740 finetune.py:68] layer 10_gate @ epoch 0 new loss 3.594007284846157e-05 old loss 3.615665264078416e-05 BETTER
I0328 10:16:40.956236 2304604 finetune.py:68] layer 9_gate @ epoch 3 new loss 3.120613473583944e-05 old loss 3.135505903628655e-05 BETTER
I0328 10:16:52.852459 2304468 finetune.py:68] layer 8_gate @ epoch 4 new loss 2.938861507573165e-05 old loss 2.9513388653867878e-05 BETTER
I0328 10:16:54.328784 2304879 finetune.py:45] layer 11_gate initial loss 3.724560156115331e-05
I0328 10:16:56.843479 2304740 finetune.py:68] layer 10_gate @ epoch 1 new loss 3.5753419069806114e-05 old loss 3.594007284846157e-05 BETTER
I0328 10:17:10.802357 2304604 finetune.py:68] layer 9_gate @ epoch 4 new loss 3.106700751231983e-05 old loss 3.120613473583944e-05 BETTER
I0328 10:17:22.331489 2304879 finetune.py:68] layer 11_gate @ epoch 0 new loss 3.7013123801443726e-05 old loss 3.724560156115331e-05 BETTER
I0328 10:17:26.460927 2304740 finetune.py:68] layer 10_gate @ epoch 2 new loss 3.558045136742294e-05 old loss 3.5753419069806114e-05 BETTER
I0328 10:17:49.623759 2304468 finetune.py:45] layer 8_down initial loss 4.514360989560373e-05
I0328 10:17:51.533941 2304879 finetune.py:68] layer 11_gate @ epoch 1 new loss 3.681044836412184e-05 old loss 3.7013123801443726e-05 BETTER
I0328 10:17:56.346829 2304740 finetune.py:68] layer 10_gate @ epoch 3 new loss 3.542116246535443e-05 old loss 3.558045136742294e-05 BETTER
I0328 10:18:08.197249 2304604 finetune.py:45] layer 9_down initial loss 4.8261663323501125e-05
I0328 10:18:17.064508 2304468 finetune.py:68] layer 8_down @ epoch 0 new loss 4.514054671744816e-05 old loss 4.514360989560373e-05 BETTER
I0328 10:18:20.970853 2304879 finetune.py:68] layer 11_gate @ epoch 2 new loss 3.66254935215693e-05 old loss 3.681044836412184e-05 BETTER
I0328 10:18:26.457482 2304740 finetune.py:68] layer 10_gate @ epoch 4 new loss 3.5268694773549214e-05 old loss 3.542116246535443e-05 BETTER
I0328 10:18:34.390475 2304604 finetune.py:68] layer 9_down @ epoch 0 new loss 4.825887663173489e-05 old loss 4.8261663323501125e-05 BETTER
I0328 10:18:45.962003 2304468 finetune.py:68] layer 8_down @ epoch 1 new loss 4.513848398346454e-05 old loss 4.514054671744816e-05 BETTER
I0328 10:18:50.188689 2304879 finetune.py:68] layer 11_gate @ epoch 3 new loss 3.6449695471674204e-05 old loss 3.66254935215693e-05 BETTER
I0328 10:19:01.646446 2304604 finetune.py:68] layer 9_down @ epoch 1 new loss 4.825692667509429e-05 old loss 4.825887663173489e-05 BETTER
I0328 10:19:14.767095 2304468 finetune.py:68] layer 8_down @ epoch 2 new loss 4.513693420449272e-05 old loss 4.513848398346454e-05 BETTER
I0328 10:19:19.740505 2304879 finetune.py:68] layer 11_gate @ epoch 4 new loss 3.628637932706624e-05 old loss 3.6449695471674204e-05 BETTER
I0328 10:19:25.072819 2304740 finetune.py:45] layer 10_down initial loss 5.29440694663208e-05
I0328 10:19:29.073891 2304604 finetune.py:68] layer 9_down @ epoch 2 new loss 4.825507858186029e-05 old loss 4.825692667509429e-05 BETTER
I0328 10:19:43.656640 2304468 finetune.py:68] layer 8_down @ epoch 3 new loss 4.513577732723206e-05 old loss 4.513693420449272e-05 BETTER
I0328 10:19:51.328309 2304740 finetune.py:68] layer 10_down @ epoch 0 new loss 5.2940606110496446e-05 old loss 5.29440694663208e-05 BETTER
I0328 10:19:56.794335 2304604 finetune.py:68] layer 9_down @ epoch 3 new loss 4.8253994464175776e-05 old loss 4.825507858186029e-05 BETTER
I0328 10:20:12.694370 2304468 finetune.py:68] layer 8_down @ epoch 4 new loss 4.513474050327204e-05 old loss 4.513577732723206e-05 BETTER
8_v proxy err 0.011572693474590778 tr(WHW.T) 257.7052307128906
bpp_loss 3.179427703260444
8_q proxy err 0.00047939291107468307 tr(WHW.T) 26603.623046875
bpp_loss 4.053915317577776
8_k proxy err 0.00015343414270319045 tr(WHW.T) 22506.951171875
bpp_loss 5.041623784345575
8_o proxy err 0.010926718823611736 tr(WHW.T) 748.5548706054688
bpp_loss 3.3075832533068024
8_up proxy err 0.007217681501060724 tr(WHW.T) 8495.005859375
bpp_loss 3.4270469679364135
8_gate proxy err 0.0016863062046468258 tr(WHW.T) 37222.96484375
bpp_loss 3.74341319348397
8_down proxy err 0.009252532385289669 tr(WHW.T) 6501.333984375
bpp_loss 3.433128924624595
I0328 10:20:18.016008 2304879 finetune.py:45] layer 11_down initial loss 5.456897270050831e-05
I0328 10:20:18.838305 2304740 finetune.py:68] layer 10_down @ epoch 1 new loss 5.2937681175535545e-05 old loss 5.2940606110496446e-05 BETTER
I0328 10:20:25.343924 2304604 finetune.py:68] layer 9_down @ epoch 4 new loss 4.825317591894418e-05 old loss 4.8253994464175776e-05 BETTER
9_v proxy err 0.008622881025075912 tr(WHW.T) 351.4288024902344
bpp_loss 3.2811292451806366
9_q proxy err 0.0005011066677980125 tr(WHW.T) 25658.314453125
bpp_loss 4.066469801298808
9_k proxy err 0.00016683699504937977 tr(WHW.T) 20932.625
bpp_loss 5.06278706563171
9_o proxy err 0.009726584888994694 tr(WHW.T) 778.6640625
bpp_loss 3.363569044333417
9_up proxy err 0.006828351877629757 tr(WHW.T) 8969.5673828125
bpp_loss 3.4366076696398005
9_gate proxy err 0.0015935245901346207 tr(WHW.T) 39385.359375
bpp_loss 3.7577879077621867
9_down proxy err 0.009162831120193005 tr(WHW.T) 6287.267578125
bpp_loss 3.434698372463962
I0328 10:20:44.490699 2304879 finetune.py:68] layer 11_down @ epoch 0 new loss 5.456477083498612e-05 old loss 5.456897270050831e-05 BETTER
I0328 10:20:47.289531 2304740 finetune.py:68] layer 10_down @ epoch 2 new loss 5.293542199069634e-05 old loss 5.2937681175535545e-05 BETTER
I0328 10:21:11.515447 2304879 finetune.py:68] layer 11_down @ epoch 1 new loss 5.45619914191775e-05 old loss 5.456477083498612e-05 BETTER
I0328 10:21:15.082640 2304740 finetune.py:68] layer 10_down @ epoch 3 new loss 5.2933879487682134e-05 old loss 5.293542199069634e-05 BETTER
I0328 10:21:37.318854 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 66.60012412071228s
I0328 10:21:38.555891 2304879 finetune.py:68] layer 11_down @ epoch 2 new loss 5.455992868519388e-05 old loss 5.45619914191775e-05 BETTER
I0328 10:21:41.294075 2306161 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 10:21:41.294178 2306161 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 10:21:41.294222 2306161 utils.py:162] NumExpr defaulting to 16 threads.
I0328 10:21:41.702794 2306161 config.py:54] PyTorch version 2.6.0 available.
W0328 10:21:41.934488 2306161 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 10:21:42.615198 2306161 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 10:21:42.619169 2300848 quantize_finetune_llama.py:209] layer 13 gpu 1
I0328 10:21:42.634713 2306161 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 10:21:42.739634 2304740 finetune.py:68] layer 10_down @ epoch 4 new loss 5.293259528116323e-05 old loss 5.2933879487682134e-05 BETTER
10_v proxy err 0.011365756392478943 tr(WHW.T) 251.83889770507812
bpp_loss 3.169681415252853
10_q proxy err 0.000524268951267004 tr(WHW.T) 23345.431640625
bpp_loss 4.067924465809483
10_k proxy err 0.00017024492262862623 tr(WHW.T) 19731.740234375
bpp_loss 5.064042113022879
10_o proxy err 0.011871255934238434 tr(WHW.T) 683.4375610351562
bpp_loss 3.3000115276081488
10_up proxy err 0.006729413755238056 tr(WHW.T) 9197.115234375
bpp_loss 3.4536074310142015
10_gate proxy err 0.0016898849280551076 tr(WHW.T) 37381.84375
bpp_loss 3.7266089615545104
10_down proxy err 0.008898354135453701 tr(WHW.T) 6538.7802734375
bpp_loss 3.4514348949638327
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 10:21:59.967872 2306161 finetune.py:45] layer 12_v initial loss 1.0960524377878755e-05
W0328 10:21:59.968099 2306161 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 10:22:05.741523 2304879 finetune.py:68] layer 11_down @ epoch 3 new loss 5.4557945986744016e-05 old loss 5.455992868519388e-05 BETTER
I0328 10:22:33.051669 2304879 finetune.py:68] layer 11_down @ epoch 4 new loss 5.455671634990722e-05 old loss 5.4557945986744016e-05 BETTER
I0328 10:22:34.863802 2306161 finetune.py:68] layer 12_v @ epoch 0 new loss 8.247692676377483e-06 old loss 1.0960524377878755e-05 BETTER
11_v proxy err 0.009366346523165703 tr(WHW.T) 319.5691223144531
bpp_loss 3.1748270426760428
11_q proxy err 0.0005747716058976948 tr(WHW.T) 22194.162109375
bpp_loss 4.004713065223768
11_k proxy err 0.0001928408892126754 tr(WHW.T) 18006.927734375
bpp_loss 5.0637204386875965
11_o proxy err 0.012101967819035053 tr(WHW.T) 568.19140625
bpp_loss 3.3196419219020754
11_up proxy err 0.006586291361600161 tr(WHW.T) 9193.3427734375
bpp_loss 3.4583404532500674
11_gate proxy err 0.0016754132229834795 tr(WHW.T) 36836.8515625
bpp_loss 3.705399662721902
11_down proxy err 0.00851361732929945 tr(WHW.T) 6670.50537109375
bpp_loss 3.45881366831184
I0328 10:22:51.311972 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 64.00988268852234s
I0328 10:22:55.086067 2306303 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 10:22:55.086172 2306303 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 10:22:55.086211 2306303 utils.py:162] NumExpr defaulting to 16 threads.
I0328 10:22:55.434928 2306303 config.py:54] PyTorch version 2.6.0 available.
W0328 10:22:55.635187 2306303 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 10:22:56.234258 2306303 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 10:22:56.238267 2300848 quantize_finetune_llama.py:209] layer 14 gpu 2
I0328 10:22:56.253003 2306303 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 10:23:11.117830 2306161 finetune.py:68] layer 12_v @ epoch 1 new loss 7.837630619178526e-06 old loss 8.247692676377483e-06 BETTER
I0328 10:23:13.998964 2306303 finetune.py:45] layer 13_v initial loss 1.131989210989559e-05
W0328 10:23:13.999703 2306303 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 10:23:47.172829 2306303 finetune.py:68] layer 13_v @ epoch 0 new loss 8.7174976215465e-06 old loss 1.131989210989559e-05 BETTER
I0328 10:23:48.058371 2306161 finetune.py:68] layer 12_v @ epoch 2 new loss 7.580333658552263e-06 old loss 7.837630619178526e-06 BETTER
I0328 10:24:01.446678 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 64.78760170936584s
I0328 10:24:05.246402 2306439 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 10:24:05.246493 2306439 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 10:24:05.246531 2306439 utils.py:162] NumExpr defaulting to 16 threads.
I0328 10:24:05.617228 2306439 config.py:54] PyTorch version 2.6.0 available.
W0328 10:24:05.830174 2306439 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 10:24:06.510625 2306439 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 10:24:06.514744 2300848 quantize_finetune_llama.py:209] layer 15 gpu 3
I0328 10:24:06.530190 2306439 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 10:24:21.738271 2306303 finetune.py:68] layer 13_v @ epoch 1 new loss 8.301314665004611e-06 old loss 8.7174976215465e-06 BETTER
I0328 10:24:24.445221 2306439 finetune.py:45] layer 14_v initial loss 1.2263159078429453e-05
W0328 10:24:24.445445 2306439 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 10:24:25.137103 2306161 finetune.py:68] layer 12_v @ epoch 3 new loss 7.417400865961099e-06 old loss 7.580333658552263e-06 BETTER
I0328 10:24:56.587061 2306303 finetune.py:68] layer 13_v @ epoch 2 new loss 8.062393135332968e-06 old loss 8.301314665004611e-06 BETTER
I0328 10:24:58.232285 2306439 finetune.py:68] layer 14_v @ epoch 0 new loss 9.578235221852083e-06 old loss 1.2263159078429453e-05 BETTER
I0328 10:25:02.149880 2306161 finetune.py:68] layer 12_v @ epoch 4 new loss 7.341128821281018e-06 old loss 7.417400865961099e-06 BETTER
I0328 10:25:12.218419 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 65.24872136116028s
I0328 10:25:16.143307 2306578 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 10:25:16.143410 2306578 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 10:25:16.143450 2306578 utils.py:162] NumExpr defaulting to 16 threads.
I0328 10:25:16.527866 2306578 config.py:54] PyTorch version 2.6.0 available.
W0328 10:25:16.745880 2306578 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 10:25:17.467586 2306578 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 10:25:17.471909 2300848 quantize_finetune_llama.py:209] layer 16 gpu 0
I0328 10:25:17.501144 2306578 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 10:25:21.977524 2306161 finetune.py:45] layer 12_q initial loss 8.680187420395669e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 10:25:31.661130 2306303 finetune.py:68] layer 13_v @ epoch 3 new loss 7.895563612692058e-06 old loss 8.062393135332968e-06 BETTER
I0328 10:25:33.137450 2306439 finetune.py:68] layer 14_v @ epoch 1 new loss 9.141023838310502e-06 old loss 9.578235221852083e-06 BETTER
I0328 10:25:35.862062 2306578 finetune.py:45] layer 15_v initial loss 1.6440204490209e-05
W0328 10:25:35.862276 2306578 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 10:25:57.209695 2306161 finetune.py:68] layer 12_q @ epoch 0 new loss 8.435376912530046e-06 old loss 8.680187420395669e-06 BETTER
I0328 10:26:07.264188 2306303 finetune.py:68] layer 13_v @ epoch 4 new loss 7.776934580761008e-06 old loss 7.895563612692058e-06 BETTER
I0328 10:26:08.611072 2306439 finetune.py:68] layer 14_v @ epoch 2 new loss 8.884099770511966e-06 old loss 9.141023838310502e-06 BETTER
I0328 10:26:09.172095 2306578 finetune.py:68] layer 15_v @ epoch 0 new loss 1.067971061274875e-05 old loss 1.6440204490209e-05 BETTER
I0328 10:26:26.921251 2306303 finetune.py:45] layer 13_q initial loss 9.354886969958898e-06
I0328 10:26:33.645042 2306161 finetune.py:68] layer 12_q @ epoch 1 new loss 8.302089554490522e-06 old loss 8.435376912530046e-06 BETTER
I0328 10:26:43.571275 2306578 finetune.py:68] layer 15_v @ epoch 1 new loss 1.0043340807897039e-05 old loss 1.067971061274875e-05 BETTER
I0328 10:26:43.864839 2306439 finetune.py:68] layer 14_v @ epoch 3 new loss 8.706490916665643e-06 old loss 8.884099770511966e-06 BETTER
I0328 10:27:00.373612 2306303 finetune.py:68] layer 13_q @ epoch 0 new loss 9.195720849675126e-06 old loss 9.354886969958898e-06 BETTER
I0328 10:27:10.244130 2306161 finetune.py:68] layer 12_q @ epoch 2 new loss 8.193784196919296e-06 old loss 8.302089554490522e-06 BETTER
I0328 10:27:18.318741 2306578 finetune.py:68] layer 15_v @ epoch 2 new loss 9.70067958405707e-06 old loss 1.0043340807897039e-05 BETTER
I0328 10:27:19.196997 2306439 finetune.py:68] layer 14_v @ epoch 4 new loss 8.584539500589017e-06 old loss 8.706490916665643e-06 BETTER
I0328 10:27:34.727884 2306303 finetune.py:68] layer 13_q @ epoch 1 new loss 9.07658068172168e-06 old loss 9.195720849675126e-06 BETTER
I0328 10:27:39.736284 2306439 finetune.py:45] layer 14_q initial loss 1.0750567525974475e-05
I0328 10:27:46.946922 2306161 finetune.py:68] layer 12_q @ epoch 3 new loss 8.083776265266351e-06 old loss 8.193784196919296e-06 BETTER
I0328 10:27:53.051600 2306578 finetune.py:68] layer 15_v @ epoch 3 new loss 9.474990292801522e-06 old loss 9.70067958405707e-06 BETTER
I0328 10:28:09.763175 2306303 finetune.py:68] layer 13_q @ epoch 2 new loss 8.987845831143204e-06 old loss 9.07658068172168e-06 BETTER
I0328 10:28:13.269148 2306439 finetune.py:68] layer 14_q @ epoch 0 new loss 1.050231367116794e-05 old loss 1.0750567525974475e-05 BETTER
I0328 10:28:23.758692 2306161 finetune.py:68] layer 12_q @ epoch 4 new loss 8.005601557670161e-06 old loss 8.083776265266351e-06 BETTER
I0328 10:28:27.607663 2306578 finetune.py:68] layer 15_v @ epoch 4 new loss 9.307020263804588e-06 old loss 9.474990292801522e-06 BETTER
I0328 10:28:42.093446 2306161 finetune.py:45] layer 12_k initial loss 8.489633728459012e-06
I0328 10:28:44.499728 2306303 finetune.py:68] layer 13_q @ epoch 3 new loss 8.906678885978181e-06 old loss 8.987845831143204e-06 BETTER
I0328 10:28:47.835621 2306578 finetune.py:45] layer 15_q initial loss 1.0599621418805327e-05
I0328 10:28:47.870763 2306439 finetune.py:68] layer 14_q @ epoch 1 new loss 1.033717217069352e-05 old loss 1.050231367116794e-05 BETTER
I0328 10:29:17.232031 2306161 finetune.py:68] layer 12_k @ epoch 0 new loss 8.378336133318953e-06 old loss 8.489633728459012e-06 BETTER
I0328 10:29:19.437742 2306303 finetune.py:68] layer 13_q @ epoch 4 new loss 8.841295311867725e-06 old loss 8.906678885978181e-06 BETTER
I0328 10:29:21.161383 2306578 finetune.py:68] layer 15_q @ epoch 0 new loss 1.0411091352580115e-05 old loss 1.0599621418805327e-05 BETTER
I0328 10:29:22.812370 2306439 finetune.py:68] layer 14_q @ epoch 2 new loss 1.0225233381788712e-05 old loss 1.033717217069352e-05 BETTER
I0328 10:29:37.883827 2306303 finetune.py:45] layer 13_k initial loss 9.235260222340003e-06
I0328 10:29:53.599079 2306161 finetune.py:68] layer 12_k @ epoch 1 new loss 8.316286766785197e-06 old loss 8.378336133318953e-06 BETTER
I0328 10:29:55.168585 2306578 finetune.py:68] layer 15_q @ epoch 1 new loss 1.026415793603519e-05 old loss 1.0411091352580115e-05 BETTER
I0328 10:29:57.663383 2306439 finetune.py:68] layer 14_q @ epoch 3 new loss 1.0109285540238488e-05 old loss 1.0225233381788712e-05 BETTER
I0328 10:30:11.481392 2306303 finetune.py:68] layer 13_k @ epoch 0 new loss 9.158691682387143e-06 old loss 9.235260222340003e-06 BETTER
I0328 10:30:29.701730 2306578 finetune.py:68] layer 15_q @ epoch 2 new loss 1.0149861736863386e-05 old loss 1.026415793603519e-05 BETTER
I0328 10:30:30.506316 2306161 finetune.py:68] layer 12_k @ epoch 2 new loss 8.253057785623241e-06 old loss 8.316286766785197e-06 BETTER
I0328 10:30:32.714663 2306439 finetune.py:68] layer 14_q @ epoch 4 new loss 1.0019147339335177e-05 old loss 1.0109285540238488e-05 BETTER
I0328 10:30:45.822561 2306303 finetune.py:68] layer 13_k @ epoch 1 new loss 9.093183507502545e-06 old loss 9.158691682387143e-06 BETTER
I0328 10:30:50.866327 2306439 finetune.py:45] layer 14_k initial loss 1.0706392458814662e-05
I0328 10:31:04.077982 2306578 finetune.py:68] layer 15_q @ epoch 3 new loss 1.0050271157524548e-05 old loss 1.0149861736863386e-05 BETTER
I0328 10:31:07.119549 2306161 finetune.py:68] layer 12_k @ epoch 3 new loss 8.192757377400994e-06 old loss 8.253057785623241e-06 BETTER
I0328 10:31:20.210169 2306303 finetune.py:68] layer 13_k @ epoch 2 new loss 9.053534995473456e-06 old loss 9.093183507502545e-06 BETTER
I0328 10:31:24.397291 2306439 finetune.py:68] layer 14_k @ epoch 0 new loss 1.0573863619356416e-05 old loss 1.0706392458814662e-05 BETTER
I0328 10:31:38.450425 2306578 finetune.py:68] layer 15_q @ epoch 4 new loss 9.968997801479418e-06 old loss 1.0050271157524548e-05 BETTER
I0328 10:31:43.762919 2306161 finetune.py:68] layer 12_k @ epoch 4 new loss 8.145126230374444e-06 old loss 8.192757377400994e-06 BETTER
I0328 10:31:54.743657 2306303 finetune.py:68] layer 13_k @ epoch 3 new loss 9.008806046040263e-06 old loss 9.053534995473456e-06 BETTER
I0328 10:31:56.677503 2306578 finetune.py:45] layer 15_k initial loss 1.0487168765394017e-05
I0328 10:31:58.834686 2306439 finetune.py:68] layer 14_k @ epoch 1 new loss 1.0495293281564955e-05 old loss 1.0573863619356416e-05 BETTER
I0328 10:32:03.973121 2306161 finetune.py:45] layer 12_o initial loss 1.873759538284503e-05
I0328 10:32:29.875921 2306303 finetune.py:68] layer 13_k @ epoch 4 new loss 8.973316653282382e-06 old loss 9.008806046040263e-06 BETTER
I0328 10:32:29.885242 2306578 finetune.py:68] layer 15_k @ epoch 0 new loss 1.0377153557783458e-05 old loss 1.0487168765394017e-05 BETTER
I0328 10:32:33.483027 2306439 finetune.py:68] layer 14_k @ epoch 2 new loss 1.0429180292703677e-05 old loss 1.0495293281564955e-05 BETTER
I0328 10:32:38.498364 2306161 finetune.py:68] layer 12_o @ epoch 0 new loss 1.8219074263470247e-05 old loss 1.873759538284503e-05 BETTER
I0328 10:32:49.926230 2306303 finetune.py:45] layer 13_o initial loss 2.2708823962602764e-05
I0328 10:33:03.821483 2306578 finetune.py:68] layer 15_k @ epoch 1 new loss 1.0304497664037626e-05 old loss 1.0377153557783458e-05 BETTER
I0328 10:33:08.111984 2306439 finetune.py:68] layer 14_k @ epoch 3 new loss 1.036964931699913e-05 old loss 1.0429180292703677e-05 BETTER
I0328 10:33:14.096865 2306161 finetune.py:68] layer 12_o @ epoch 1 new loss 1.7922517145052552e-05 old loss 1.8219074263470247e-05 BETTER
I0328 10:33:22.477625 2306303 finetune.py:68] layer 13_o @ epoch 0 new loss 2.201786992372945e-05 old loss 2.2708823962602764e-05 BETTER
I0328 10:33:37.741757 2306578 finetune.py:68] layer 15_k @ epoch 2 new loss 1.0233053217234556e-05 old loss 1.0304497664037626e-05 BETTER
I0328 10:33:42.632585 2306439 finetune.py:68] layer 14_k @ epoch 4 new loss 1.0329255019314587e-05 old loss 1.036964931699913e-05 BETTER
I0328 10:33:49.863041 2306161 finetune.py:68] layer 12_o @ epoch 2 new loss 1.76934408955276e-05 old loss 1.7922517145052552e-05 BETTER
I0328 10:33:56.017218 2306303 finetune.py:68] layer 13_o @ epoch 1 new loss 2.1647712856065482e-05 old loss 2.201786992372945e-05 BETTER
I0328 10:34:02.663337 2306439 finetune.py:45] layer 14_o initial loss 2.4994245904963464e-05
I0328 10:34:11.859678 2306578 finetune.py:68] layer 15_k @ epoch 3 new loss 1.0201119948760606e-05 old loss 1.0233053217234556e-05 BETTER
I0328 10:34:25.607139 2306161 finetune.py:68] layer 12_o @ epoch 3 new loss 1.75009663507808e-05 old loss 1.76934408955276e-05 BETTER
I0328 10:34:29.883589 2306303 finetune.py:68] layer 13_o @ epoch 2 new loss 2.1364763597375713e-05 old loss 2.1647712856065482e-05 BETTER
I0328 10:34:35.442725 2306439 finetune.py:68] layer 14_o @ epoch 0 new loss 2.4183691493817605e-05 old loss 2.4994245904963464e-05 BETTER
I0328 10:34:45.894059 2306578 finetune.py:68] layer 15_k @ epoch 4 new loss 1.015076122712344e-05 old loss 1.0201119948760606e-05 BETTER
I0328 10:35:01.477077 2306161 finetune.py:68] layer 12_o @ epoch 4 new loss 1.733389035507571e-05 old loss 1.75009663507808e-05 BETTER
I0328 10:35:03.753289 2306303 finetune.py:68] layer 13_o @ epoch 3 new loss 2.1131934772711247e-05 old loss 2.1364763597375713e-05 BETTER
I0328 10:35:05.913650 2306578 finetune.py:45] layer 15_o initial loss 2.5012443074956536e-05
I0328 10:35:09.314101 2306439 finetune.py:68] layer 14_o @ epoch 1 new loss 2.3725939172436483e-05 old loss 2.4183691493817605e-05 BETTER
I0328 10:35:33.566519 2306161 finetune.py:45] layer 12_up initial loss 3.33561583829578e-05
I0328 10:35:37.789014 2306303 finetune.py:68] layer 13_o @ epoch 4 new loss 2.093339389830362e-05 old loss 2.1131934772711247e-05 BETTER
I0328 10:35:38.050100 2306578 finetune.py:68] layer 15_o @ epoch 0 new loss 2.416074858047068e-05 old loss 2.5012443074956536e-05 BETTER
I0328 10:35:43.180976 2306439 finetune.py:68] layer 14_o @ epoch 2 new loss 2.337592377443798e-05 old loss 2.3725939172436483e-05 BETTER
I0328 10:36:05.507101 2306161 finetune.py:68] layer 12_up @ epoch 0 new loss 3.285863931523636e-05 old loss 3.33561583829578e-05 BETTER
I0328 10:36:09.647913 2306303 finetune.py:45] layer 13_up initial loss 3.9231574191944674e-05
I0328 10:36:11.526622 2306578 finetune.py:68] layer 15_o @ epoch 1 new loss 2.3683358449488878e-05 old loss 2.416074858047068e-05 BETTER
I0328 10:36:17.256024 2306439 finetune.py:68] layer 14_o @ epoch 3 new loss 2.3091733964974992e-05 old loss 2.337592377443798e-05 BETTER
I0328 10:36:38.714811 2306161 finetune.py:68] layer 12_up @ epoch 1 new loss 3.247315180487931e-05 old loss 3.285863931523636e-05 BETTER
I0328 10:36:40.242636 2306303 finetune.py:68] layer 13_up @ epoch 0 new loss 3.863607707899064e-05 old loss 3.9231574191944674e-05 BETTER
I0328 10:36:45.035391 2306578 finetune.py:68] layer 15_o @ epoch 2 new loss 2.3322530978475697e-05 old loss 2.3683358449488878e-05 BETTER
I0328 10:36:51.329371 2306439 finetune.py:68] layer 14_o @ epoch 4 new loss 2.285327536810655e-05 old loss 2.3091733964974992e-05 BETTER
I0328 10:37:11.900067 2306303 finetune.py:68] layer 13_up @ epoch 1 new loss 3.81806748919189e-05 old loss 3.863607707899064e-05 BETTER
I0328 10:37:12.224555 2306161 finetune.py:68] layer 12_up @ epoch 2 new loss 3.213650416000746e-05 old loss 3.247315180487931e-05 BETTER
I0328 10:37:18.844047 2306578 finetune.py:68] layer 15_o @ epoch 3 new loss 2.3031054297462106e-05 old loss 2.3322530978475697e-05 BETTER
I0328 10:37:23.364448 2306439 finetune.py:45] layer 14_up initial loss 4.494888707995415e-05
I0328 10:37:44.043606 2306303 finetune.py:68] layer 13_up @ epoch 2 new loss 3.778697282541543e-05 old loss 3.81806748919189e-05 BETTER
I0328 10:37:46.259071 2306161 finetune.py:68] layer 12_up @ epoch 3 new loss 3.1836898415349424e-05 old loss 3.213650416000746e-05 BETTER
I0328 10:37:52.625179 2306578 finetune.py:68] layer 15_o @ epoch 4 new loss 2.2782376618124545e-05 old loss 2.3031054297462106e-05 BETTER
I0328 10:37:54.301815 2306439 finetune.py:68] layer 14_up @ epoch 0 new loss 4.417672971612774e-05 old loss 4.494888707995415e-05 BETTER
I0328 10:38:16.043463 2306303 finetune.py:68] layer 13_up @ epoch 3 new loss 3.743527486221865e-05 old loss 3.778697282541543e-05 BETTER
I0328 10:38:20.205688 2306161 finetune.py:68] layer 12_up @ epoch 4 new loss 3.156038292218e-05 old loss 3.1836898415349424e-05 BETTER
I0328 10:38:24.417174 2306578 finetune.py:45] layer 15_up initial loss 4.91751816298347e-05
I0328 10:38:26.170824 2306439 finetune.py:68] layer 14_up @ epoch 1 new loss 4.3572810682235286e-05 old loss 4.417672971612774e-05 BETTER
I0328 10:38:47.937266 2306303 finetune.py:68] layer 13_up @ epoch 4 new loss 3.711604222189635e-05 old loss 3.743527486221865e-05 BETTER
I0328 10:38:52.560801 2306161 finetune.py:45] layer 12_gate initial loss 3.83427286578808e-05
I0328 10:38:54.640091 2306578 finetune.py:68] layer 15_up @ epoch 0 new loss 4.8171321395784616e-05 old loss 4.91751816298347e-05 BETTER
I0328 10:38:58.398155 2306439 finetune.py:68] layer 14_up @ epoch 2 new loss 4.305775655666366e-05 old loss 4.3572810682235286e-05 BETTER
I0328 10:39:19.805172 2306303 finetune.py:45] layer 13_gate initial loss 4.456803799257614e-05
I0328 10:39:22.583949 2306161 finetune.py:68] layer 12_gate @ epoch 0 new loss 3.806560562225059e-05 old loss 3.83427286578808e-05 BETTER
I0328 10:39:25.886549 2306578 finetune.py:68] layer 15_up @ epoch 1 new loss 4.740806616609916e-05 old loss 4.8171321395784616e-05 BETTER
I0328 10:39:30.691150 2306439 finetune.py:68] layer 14_up @ epoch 3 new loss 4.2603714973665774e-05 old loss 4.305775655666366e-05 BETTER
I0328 10:39:48.440281 2306303 finetune.py:68] layer 13_gate @ epoch 0 new loss 4.423701830091886e-05 old loss 4.456803799257614e-05 BETTER
I0328 10:39:53.993042 2306161 finetune.py:68] layer 12_gate @ epoch 1 new loss 3.782478961511515e-05 old loss 3.806560562225059e-05 BETTER
I0328 10:39:57.358814 2306578 finetune.py:68] layer 15_up @ epoch 2 new loss 4.676035314332694e-05 old loss 4.740806616609916e-05 BETTER
I0328 10:40:03.058837 2306439 finetune.py:68] layer 14_up @ epoch 4 new loss 4.2192667024210095e-05 old loss 4.2603714973665774e-05 BETTER
I0328 10:40:17.917772 2306303 finetune.py:68] layer 13_gate @ epoch 1 new loss 4.395232463139109e-05 old loss 4.423701830091886e-05 BETTER
I0328 10:40:25.357024 2306161 finetune.py:68] layer 12_gate @ epoch 2 new loss 3.760583058465272e-05 old loss 3.782478961511515e-05 BETTER
I0328 10:40:28.806111 2306578 finetune.py:68] layer 15_up @ epoch 3 new loss 4.619027458829805e-05 old loss 4.676035314332694e-05 BETTER
I0328 10:40:35.620618 2306439 finetune.py:45] layer 14_gate initial loss 5.021538527216762e-05
I0328 10:40:47.622129 2306303 finetune.py:68] layer 13_gate @ epoch 2 new loss 4.369607268017717e-05 old loss 4.395232463139109e-05 BETTER
I0328 10:40:57.002587 2306161 finetune.py:68] layer 12_gate @ epoch 3 new loss 3.740129977813922e-05 old loss 3.760583058465272e-05 BETTER
I0328 10:41:00.347928 2306578 finetune.py:68] layer 15_up @ epoch 4 new loss 4.567415453493595e-05 old loss 4.619027458829805e-05 BETTER
I0328 10:41:04.160888 2306439 finetune.py:68] layer 14_gate @ epoch 0 new loss 4.9800033593783155e-05 old loss 5.021538527216762e-05 BETTER
I0328 10:41:17.384403 2306303 finetune.py:68] layer 13_gate @ epoch 3 new loss 4.3454550905153155e-05 old loss 4.369607268017717e-05 BETTER
I0328 10:41:28.525166 2306161 finetune.py:68] layer 12_gate @ epoch 4 new loss 3.721087705343962e-05 old loss 3.740129977813922e-05 BETTER
I0328 10:41:32.491635 2306578 finetune.py:45] layer 15_gate initial loss 5.4392865422414616e-05
I0328 10:41:33.954819 2306439 finetune.py:68] layer 14_gate @ epoch 1 new loss 4.944283136865124e-05 old loss 4.9800033593783155e-05 BETTER
I0328 10:41:47.124582 2306303 finetune.py:68] layer 13_gate @ epoch 4 new loss 4.3235169869149104e-05 old loss 4.3454550905153155e-05 BETTER
I0328 10:42:00.737840 2306578 finetune.py:68] layer 15_gate @ epoch 0 new loss 5.3872277931077406e-05 old loss 5.4392865422414616e-05 BETTER
I0328 10:42:03.802657 2306439 finetune.py:68] layer 14_gate @ epoch 2 new loss 4.912391159450635e-05 old loss 4.944283136865124e-05 BETTER
I0328 10:42:25.451063 2306161 finetune.py:45] layer 12_down initial loss 5.629696534015238e-05
I0328 10:42:29.831722 2306578 finetune.py:68] layer 15_gate @ epoch 1 new loss 5.3415500588016585e-05 old loss 5.3872277931077406e-05 BETTER
I0328 10:42:33.622581 2306439 finetune.py:68] layer 14_gate @ epoch 3 new loss 4.8827270802576095e-05 old loss 4.912391159450635e-05 BETTER
I0328 10:42:44.112567 2306303 finetune.py:45] layer 13_down initial loss 6.594449223484844e-05
I0328 10:42:52.859655 2306161 finetune.py:68] layer 12_down @ epoch 0 new loss 5.6293760280823335e-05 old loss 5.629696534015238e-05 BETTER
I0328 10:42:59.198100 2306578 finetune.py:68] layer 15_gate @ epoch 2 new loss 5.3007745009381324e-05 old loss 5.3415500588016585e-05 BETTER
I0328 10:43:03.681834 2306439 finetune.py:68] layer 14_gate @ epoch 4 new loss 4.8552934458712116e-05 old loss 4.8827270802576095e-05 BETTER
I0328 10:43:10.256392 2306303 finetune.py:68] layer 13_down @ epoch 0 new loss 6.59405704936944e-05 old loss 6.594449223484844e-05 BETTER
I0328 10:43:21.325381 2306161 finetune.py:68] layer 12_down @ epoch 1 new loss 5.629157385556027e-05 old loss 5.6293760280823335e-05 BETTER
I0328 10:43:28.749640 2306578 finetune.py:68] layer 15_gate @ epoch 3 new loss 5.2637074986705557e-05 old loss 5.3007745009381324e-05 BETTER
I0328 10:43:37.609989 2306303 finetune.py:68] layer 13_down @ epoch 1 new loss 6.593784200958908e-05 old loss 6.59405704936944e-05 BETTER
I0328 10:43:50.271389 2306161 finetune.py:68] layer 12_down @ epoch 2 new loss 5.628996223094873e-05 old loss 5.629157385556027e-05 BETTER
I0328 10:43:58.146388 2306578 finetune.py:68] layer 15_gate @ epoch 4 new loss 5.2297156798886135e-05 old loss 5.2637074986705557e-05 BETTER
I0328 10:44:01.060119 2306439 finetune.py:45] layer 14_down initial loss 7.586249557789415e-05
I0328 10:44:05.174435 2306303 finetune.py:68] layer 13_down @ epoch 2 new loss 6.593602302018553e-05 old loss 6.593784200958908e-05 BETTER
I0328 10:44:19.244873 2306161 finetune.py:68] layer 12_down @ epoch 3 new loss 5.6288758059963584e-05 old loss 5.628996223094873e-05 BETTER
I0328 10:44:27.376348 2306439 finetune.py:68] layer 14_down @ epoch 0 new loss 7.585826824652031e-05 old loss 7.586249557789415e-05 BETTER
I0328 10:44:32.813927 2306303 finetune.py:68] layer 13_down @ epoch 3 new loss 6.593456055270508e-05 old loss 6.593602302018553e-05 BETTER
I0328 10:44:48.341627 2306161 finetune.py:68] layer 12_down @ epoch 4 new loss 5.62877394258976e-05 old loss 5.6288758059963584e-05 BETTER
12_v proxy err 0.008684176951646805 tr(WHW.T) 363.6233825683594
bpp_loss 3.293276348442305
12_q proxy err 0.00039266515523195267 tr(WHW.T) 34114.3515625
bpp_loss 4.063495520676952
12_k proxy err 0.0001563704281579703 tr(WHW.T) 23042.0625
bpp_loss 5.060121152200736
12_o proxy err 0.009802849031984806 tr(WHW.T) 782.3468017578125
bpp_loss 3.365383946336806
12_up proxy err 0.005911618005484343 tr(WHW.T) 10024.68359375
bpp_loss 3.4781708241706446
12_gate proxy err 0.0016145018162205815 tr(WHW.T) 37288.4375
bpp_loss 3.685738375876099
12_down proxy err 0.00802989024668932 tr(WHW.T) 6834.15869140625
bpp_loss 3.4716073538376286
I0328 10:44:54.974956 2306578 finetune.py:45] layer 15_down initial loss 8.686413639225066e-05
I0328 10:44:55.003432 2306439 finetune.py:68] layer 14_down @ epoch 1 new loss 7.585516141261905e-05 old loss 7.585826824652031e-05 BETTER
I0328 10:45:01.177873 2306303 finetune.py:68] layer 13_down @ epoch 4 new loss 6.593329453608021e-05 old loss 6.593456055270508e-05 BETTER
13_v proxy err 0.010634833946824074 tr(WHW.T) 280.153564453125
bpp_loss 3.233366481726989
13_q proxy err 0.0006064030458219349 tr(WHW.T) 20900.90625
bpp_loss 4.039014332171064
13_k proxy err 0.00019697587413247675 tr(WHW.T) 17786.009765625
bpp_loss 5.077930788625963
13_o proxy err 0.010388770140707493 tr(WHW.T) 676.6400146484375
bpp_loss 3.3476308797253296
13_up proxy err 0.005799137521535158 tr(WHW.T) 10008.056640625
bpp_loss 3.4816472576265887
13_gate proxy err 0.0015156182926148176 tr(WHW.T) 38897.40234375
bpp_loss 3.6905570247077515
13_down proxy err 0.008206943050026894 tr(WHW.T) 6570.95458984375
bpp_loss 3.4711897278071513
I0328 10:45:21.631182 2306578 finetune.py:68] layer 15_down @ epoch 0 new loss 8.686057844897732e-05 old loss 8.686413639225066e-05 BETTER
I0328 10:45:23.761881 2306439 finetune.py:68] layer 14_down @ epoch 2 new loss 7.585284038214013e-05 old loss 7.585516141261905e-05 BETTER
I0328 10:45:48.688255 2306578 finetune.py:68] layer 15_down @ epoch 1 new loss 8.685776992933825e-05 old loss 8.686057844897732e-05 BETTER
I0328 10:45:51.520265 2306439 finetune.py:68] layer 14_down @ epoch 3 new loss 7.585102139273658e-05 old loss 7.585284038214013e-05 BETTER
I0328 10:46:04.564664 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 58.28819727897644s
I0328 10:46:08.583142 2307854 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 10:46:08.583241 2307854 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 10:46:08.583284 2307854 utils.py:162] NumExpr defaulting to 16 threads.
I0328 10:46:08.981331 2307854 config.py:54] PyTorch version 2.6.0 available.
W0328 10:46:09.213601 2307854 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 10:46:09.837028 2307854 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 10:46:09.841015 2300848 quantize_finetune_llama.py:209] layer 17 gpu 1
I0328 10:46:09.855112 2307854 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 10:46:15.856377 2306578 finetune.py:68] layer 15_down @ epoch 2 new loss 8.685565990163013e-05 old loss 8.685776992933825e-05 BETTER
I0328 10:46:19.232464 2306439 finetune.py:68] layer 14_down @ epoch 4 new loss 7.584957347717136e-05 old loss 7.585102139273658e-05 BETTER
14_v proxy err 0.010009454563260078 tr(WHW.T) 281.3382873535156
bpp_loss 3.2257266112137586
14_q proxy err 0.0005739693879149854 tr(WHW.T) 20885.916015625
bpp_loss 4.0056952888844535
14_k proxy err 0.00017989010666497052 tr(WHW.T) 18602.337890625
bpp_loss 5.022866535000503
14_o proxy err 0.011099955067038536 tr(WHW.T) 688.2974243164062
bpp_loss 3.339128566964064
14_up proxy err 0.006305999588221312 tr(WHW.T) 9163.15625
bpp_loss 3.4759705550968647
14_gate proxy err 0.0014077115338295698 tr(WHW.T) 41798.51953125
bpp_loss 3.7222827206631854
14_down proxy err 0.008671164512634277 tr(WHW.T) 6402.5185546875
bpp_loss 3.4669979009278387
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 10:46:28.105504 2307854 finetune.py:45] layer 16_v initial loss 1.871111453510821e-05
W0328 10:46:28.105716 2307854 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 10:46:43.071381 2306578 finetune.py:68] layer 15_down @ epoch 3 new loss 8.685403008712456e-05 old loss 8.685565990163013e-05 BETTER
I0328 10:47:03.081797 2307854 finetune.py:68] layer 16_v @ epoch 0 new loss 1.175914985651616e-05 old loss 1.871111453510821e-05 BETTER
I0328 10:47:10.288523 2306578 finetune.py:68] layer 15_down @ epoch 4 new loss 8.685261127538979e-05 old loss 8.685403008712456e-05 BETTER
15_v proxy err 0.011001852340996265 tr(WHW.T) 284.0271301269531
bpp_loss 3.2893134954501875
15_q proxy err 0.00047724676551297307 tr(WHW.T) 28081.640625
bpp_loss 4.148072845011484
15_k proxy err 0.00019300688290968537 tr(WHW.T) 18856.9296875
bpp_loss 5.069432386895642
15_o proxy err 0.011343149468302727 tr(WHW.T) 829.1097412109375
bpp_loss 3.369703841628507
15_up proxy err 0.006426317151635885 tr(WHW.T) 8994.345703125
bpp_loss 3.468283490743488
15_gate proxy err 0.001279592514038086 tr(WHW.T) 46137.25390625
bpp_loss 3.7601970988325775
15_down proxy err 0.008711179718375206 tr(WHW.T) 6414.8447265625
bpp_loss 3.4614082417018444
I0328 10:47:28.365020 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 64.07828116416931s
I0328 10:47:32.015064 2308005 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 10:47:32.015157 2308005 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 10:47:32.015199 2308005 utils.py:162] NumExpr defaulting to 16 threads.
I0328 10:47:32.348203 2308005 config.py:54] PyTorch version 2.6.0 available.
W0328 10:47:32.551530 2308005 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 10:47:33.147322 2308005 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 10:47:33.151074 2300848 quantize_finetune_llama.py:209] layer 18 gpu 2
I0328 10:47:33.166184 2308005 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 10:47:39.597837 2307854 finetune.py:68] layer 16_v @ epoch 1 new loss 1.0982474123011343e-05 old loss 1.175914985651616e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 10:47:50.393392 2308005 finetune.py:45] layer 17_v initial loss 2.0467647118493915e-05
W0328 10:47:50.393582 2308005 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 10:48:16.507699 2307854 finetune.py:68] layer 16_v @ epoch 2 new loss 1.0556876986811403e-05 old loss 1.0982474123011343e-05 BETTER
I0328 10:48:23.477066 2308005 finetune.py:68] layer 17_v @ epoch 0 new loss 1.0869789548451081e-05 old loss 2.0467647118493915e-05 BETTER
I0328 10:48:37.685712 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 64.07538080215454s
I0328 10:48:41.517153 2308141 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 10:48:41.517254 2308141 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 10:48:41.517297 2308141 utils.py:162] NumExpr defaulting to 16 threads.
I0328 10:48:41.899434 2308141 config.py:54] PyTorch version 2.6.0 available.
W0328 10:48:42.123750 2308141 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 10:48:42.775010 2308141 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 10:48:42.778446 2300848 quantize_finetune_llama.py:209] layer 19 gpu 3
I0328 10:48:42.791802 2308141 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 10:48:53.556905 2307854 finetune.py:68] layer 16_v @ epoch 3 new loss 1.0260728231514804e-05 old loss 1.0556876986811403e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 10:48:58.141583 2308005 finetune.py:68] layer 17_v @ epoch 1 new loss 1.0036089406639803e-05 old loss 1.0869789548451081e-05 BETTER
I0328 10:49:00.950172 2308141 finetune.py:45] layer 18_v initial loss 2.611833406263031e-05
W0328 10:49:00.951029 2308141 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 10:49:30.726915 2307854 finetune.py:68] layer 16_v @ epoch 4 new loss 1.0046230272564571e-05 old loss 1.0260728231514804e-05 BETTER
I0328 10:49:33.023672 2308005 finetune.py:68] layer 17_v @ epoch 2 new loss 9.62334070209181e-06 old loss 1.0036089406639803e-05 BETTER
I0328 10:49:34.559159 2308141 finetune.py:68] layer 18_v @ epoch 0 new loss 9.263444553653244e-06 old loss 2.611833406263031e-05 BETTER
I0328 10:49:49.033618 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 65.80980587005615s
I0328 10:49:50.965436 2307854 finetune.py:45] layer 16_q initial loss 1.1616406482062303e-05
I0328 10:49:53.041935 2308280 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 10:49:53.042033 2308280 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 10:49:53.042078 2308280 utils.py:162] NumExpr defaulting to 16 threads.
I0328 10:49:53.401314 2308280 config.py:54] PyTorch version 2.6.0 available.
W0328 10:49:53.619612 2308280 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 10:49:54.297155 2308280 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 10:49:54.301135 2300848 quantize_finetune_llama.py:209] layer 20 gpu 0
I0328 10:49:54.314992 2308280 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 10:50:08.028013 2308005 finetune.py:68] layer 17_v @ epoch 3 new loss 9.380531082570087e-06 old loss 9.62334070209181e-06 BETTER
I0328 10:50:09.133702 2308141 finetune.py:68] layer 18_v @ epoch 1 new loss 8.36852632346563e-06 old loss 9.263444553653244e-06 BETTER
I0328 10:50:12.091555 2308280 finetune.py:45] layer 19_v initial loss 2.9290606107679196e-05
W0328 10:50:12.091738 2308280 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 10:50:26.108170 2307854 finetune.py:68] layer 16_q @ epoch 0 new loss 1.1339974662405439e-05 old loss 1.1616406482062303e-05 BETTER
I0328 10:50:43.812635 2308005 finetune.py:68] layer 17_v @ epoch 4 new loss 9.236634468834382e-06 old loss 9.380531082570087e-06 BETTER
I0328 10:50:44.721330 2308141 finetune.py:68] layer 18_v @ epoch 2 new loss 7.977428140293341e-06 old loss 8.36852632346563e-06 BETTER
I0328 10:50:45.546009 2308280 finetune.py:68] layer 19_v @ epoch 0 new loss 9.587736713001505e-06 old loss 2.9290606107679196e-05 BETTER
I0328 10:51:02.488664 2307854 finetune.py:68] layer 16_q @ epoch 1 new loss 1.1125522178190295e-05 old loss 1.1339974662405439e-05 BETTER
I0328 10:51:03.847718 2308005 finetune.py:45] layer 17_q initial loss 1.0586112693999894e-05
I0328 10:51:20.045013 2308141 finetune.py:68] layer 18_v @ epoch 3 new loss 7.741943591099698e-06 old loss 7.977428140293341e-06 BETTER
I0328 10:51:20.163423 2308280 finetune.py:68] layer 19_v @ epoch 1 new loss 8.608824828115758e-06 old loss 9.587736713001505e-06 BETTER
I0328 10:51:37.671993 2308005 finetune.py:68] layer 17_q @ epoch 0 new loss 1.0299962013959885e-05 old loss 1.0586112693999894e-05 BETTER
I0328 10:51:39.358462 2307854 finetune.py:68] layer 16_q @ epoch 2 new loss 1.0968047718051821e-05 old loss 1.1125522178190295e-05 BETTER
I0328 10:51:54.670349 2308280 finetune.py:68] layer 19_v @ epoch 2 new loss 8.211167369154282e-06 old loss 8.608824828115758e-06 BETTER
I0328 10:51:55.267255 2308141 finetune.py:68] layer 18_v @ epoch 4 new loss 7.591200301249046e-06 old loss 7.741943591099698e-06 BETTER
I0328 10:52:12.186581 2308005 finetune.py:68] layer 17_q @ epoch 1 new loss 1.0127563655260019e-05 old loss 1.0299962013959885e-05 BETTER
I0328 10:52:15.186970 2308141 finetune.py:45] layer 18_q initial loss 8.923909263103269e-06
I0328 10:52:16.347811 2307854 finetune.py:68] layer 16_q @ epoch 3 new loss 1.0818176633620169e-05 old loss 1.0968047718051821e-05 BETTER
I0328 10:52:29.205583 2308280 finetune.py:68] layer 19_v @ epoch 3 new loss 7.972257662913762e-06 old loss 8.211167369154282e-06 BETTER
I0328 10:52:46.899778 2308005 finetune.py:68] layer 17_q @ epoch 2 new loss 9.993926141760312e-06 old loss 1.0127563655260019e-05 BETTER
I0328 10:52:48.893935 2308141 finetune.py:68] layer 18_q @ epoch 0 new loss 8.720808182260953e-06 old loss 8.923909263103269e-06 BETTER
I0328 10:52:53.380659 2307854 finetune.py:68] layer 16_q @ epoch 4 new loss 1.0704476153478026e-05 old loss 1.0818176633620169e-05 BETTER
I0328 10:53:03.826465 2308280 finetune.py:68] layer 19_v @ epoch 4 new loss 7.811441719240975e-06 old loss 7.972257662913762e-06 BETTER
I0328 10:53:11.738108 2307854 finetune.py:45] layer 16_k initial loss 1.1217371138627641e-05
I0328 10:53:21.553005 2308005 finetune.py:68] layer 17_q @ epoch 3 new loss 9.88186275208136e-06 old loss 9.993926141760312e-06 BETTER
I0328 10:53:23.584204 2308141 finetune.py:68] layer 18_q @ epoch 1 new loss 8.56527822179487e-06 old loss 8.720808182260953e-06 BETTER
I0328 10:53:23.801364 2308280 finetune.py:45] layer 19_q initial loss 8.968274414655752e-06
I0328 10:53:46.936450 2307854 finetune.py:68] layer 16_k @ epoch 0 new loss 1.1081122465839144e-05 old loss 1.1217371138627641e-05 BETTER
I0328 10:53:56.465902 2308005 finetune.py:68] layer 17_q @ epoch 4 new loss 9.779035281098913e-06 old loss 9.88186275208136e-06 BETTER
I0328 10:53:57.196124 2308280 finetune.py:68] layer 19_q @ epoch 0 new loss 8.768211046117358e-06 old loss 8.968274414655752e-06 BETTER
I0328 10:53:58.504320 2308141 finetune.py:68] layer 18_q @ epoch 2 new loss 8.456056093564257e-06 old loss 8.56527822179487e-06 BETTER
I0328 10:54:14.709619 2308005 finetune.py:45] layer 17_k initial loss 1.026449172059074e-05
I0328 10:54:23.372450 2307854 finetune.py:68] layer 16_k @ epoch 1 new loss 1.0986605047946796e-05 old loss 1.1081122465839144e-05 BETTER
I0328 10:54:31.312943 2308280 finetune.py:68] layer 19_q @ epoch 1 new loss 8.623193025414366e-06 old loss 8.768211046117358e-06 BETTER
I0328 10:54:33.366662 2308141 finetune.py:68] layer 18_q @ epoch 3 new loss 8.360369065485429e-06 old loss 8.456056093564257e-06 BETTER
I0328 10:54:47.943949 2308005 finetune.py:68] layer 17_k @ epoch 0 new loss 1.0159009434573818e-05 old loss 1.026449172059074e-05 BETTER
I0328 10:54:59.955231 2307854 finetune.py:68] layer 16_k @ epoch 2 new loss 1.0904685041168705e-05 old loss 1.0986605047946796e-05 BETTER
I0328 10:55:05.569293 2308280 finetune.py:68] layer 19_q @ epoch 2 new loss 8.522632924723439e-06 old loss 8.623193025414366e-06 BETTER
I0328 10:55:08.373705 2308141 finetune.py:68] layer 18_q @ epoch 4 new loss 8.283248462248594e-06 old loss 8.360369065485429e-06 BETTER
I0328 10:55:22.145990 2308005 finetune.py:68] layer 17_k @ epoch 1 new loss 1.0077651495521422e-05 old loss 1.0159009434573818e-05 BETTER
I0328 10:55:26.397696 2308141 finetune.py:45] layer 18_k initial loss 8.66473692440195e-06
I0328 10:55:36.519688 2307854 finetune.py:68] layer 16_k @ epoch 3 new loss 1.0842625670193229e-05 old loss 1.0904685041168705e-05 BETTER
I0328 10:55:39.885796 2308280 finetune.py:68] layer 19_q @ epoch 3 new loss 8.429841727775056e-06 old loss 8.522632924723439e-06 BETTER
I0328 10:55:56.493050 2308005 finetune.py:68] layer 17_k @ epoch 2 new loss 1.0014497092925012e-05 old loss 1.0077651495521422e-05 BETTER
I0328 10:55:59.823321 2308141 finetune.py:68] layer 18_k @ epoch 0 new loss 8.605084985902067e-06 old loss 8.66473692440195e-06 BETTER
I0328 10:56:13.080176 2307854 finetune.py:68] layer 16_k @ epoch 4 new loss 1.0768481843115296e-05 old loss 1.0842625670193229e-05 BETTER
I0328 10:56:14.011728 2308280 finetune.py:68] layer 19_q @ epoch 4 new loss 8.365952453459613e-06 old loss 8.429841727775056e-06 BETTER
I0328 10:56:31.384220 2308005 finetune.py:68] layer 17_k @ epoch 3 new loss 9.957541806215886e-06 old loss 1.0014497092925012e-05 BETTER
I0328 10:56:32.583644 2308280 finetune.py:45] layer 19_k initial loss 8.828402314975392e-06
I0328 10:56:32.923845 2307854 finetune.py:45] layer 16_o initial loss 2.5230920073227026e-05
I0328 10:56:34.637109 2308141 finetune.py:68] layer 18_k @ epoch 1 new loss 8.521621566615067e-06 old loss 8.605084985902067e-06 BETTER
I0328 10:57:05.978935 2308280 finetune.py:68] layer 19_k @ epoch 0 new loss 8.735358278499916e-06 old loss 8.828402314975392e-06 BETTER
I0328 10:57:06.296411 2308005 finetune.py:68] layer 17_k @ epoch 4 new loss 9.913513167703059e-06 old loss 9.957541806215886e-06 BETTER
I0328 10:57:07.638671 2307854 finetune.py:68] layer 16_o @ epoch 0 new loss 2.443195080559235e-05 old loss 2.5230920073227026e-05 BETTER
I0328 10:57:09.319597 2308141 finetune.py:68] layer 18_k @ epoch 2 new loss 8.46707189339213e-06 old loss 8.521621566615067e-06 BETTER
I0328 10:57:25.970508 2308005 finetune.py:45] layer 17_o initial loss 2.394012699369341e-05
I0328 10:57:40.083978 2308280 finetune.py:68] layer 19_k @ epoch 1 new loss 8.677008736412972e-06 old loss 8.735358278499916e-06 BETTER
I0328 10:57:43.378546 2307854 finetune.py:68] layer 16_o @ epoch 1 new loss 2.3981423510122113e-05 old loss 2.443195080559235e-05 BETTER
I0328 10:57:43.885693 2308141 finetune.py:68] layer 18_k @ epoch 3 new loss 8.432756658294238e-06 old loss 8.46707189339213e-06 BETTER
I0328 10:57:58.625107 2308005 finetune.py:68] layer 17_o @ epoch 0 new loss 2.3123593564378098e-05 old loss 2.394012699369341e-05 BETTER
I0328 10:58:14.060827 2308280 finetune.py:68] layer 19_k @ epoch 2 new loss 8.627464012533892e-06 old loss 8.677008736412972e-06 BETTER
I0328 10:58:18.520549 2308141 finetune.py:68] layer 18_k @ epoch 4 new loss 8.378718121093698e-06 old loss 8.432756658294238e-06 BETTER
I0328 10:58:19.469919 2307854 finetune.py:68] layer 16_o @ epoch 2 new loss 2.3642651285626926e-05 old loss 2.3981423510122113e-05 BETTER
I0328 10:58:32.310582 2308005 finetune.py:68] layer 17_o @ epoch 1 new loss 2.2675181753584184e-05 old loss 2.3123593564378098e-05 BETTER
I0328 10:58:38.714773 2308141 finetune.py:45] layer 18_o initial loss 1.978129512281157e-05
I0328 10:58:48.296068 2308280 finetune.py:68] layer 19_k @ epoch 3 new loss 8.58631119626807e-06 old loss 8.627464012533892e-06 BETTER
I0328 10:58:55.354884 2307854 finetune.py:68] layer 16_o @ epoch 3 new loss 2.336033503524959e-05 old loss 2.3642651285626926e-05 BETTER
I0328 10:59:06.049264 2308005 finetune.py:68] layer 17_o @ epoch 2 new loss 2.2342395823216066e-05 old loss 2.2675181753584184e-05 BETTER
I0328 10:59:11.629708 2308141 finetune.py:68] layer 18_o @ epoch 0 new loss 1.9033936041523702e-05 old loss 1.978129512281157e-05 BETTER
I0328 10:59:22.470274 2308280 finetune.py:68] layer 19_k @ epoch 4 new loss 8.548792720830534e-06 old loss 8.58631119626807e-06 BETTER
I0328 10:59:31.260757 2307854 finetune.py:68] layer 16_o @ epoch 4 new loss 2.3127491658669896e-05 old loss 2.336033503524959e-05 BETTER
I0328 10:59:39.876563 2308005 finetune.py:68] layer 17_o @ epoch 3 new loss 2.2076594177633524e-05 old loss 2.2342395823216066e-05 BETTER
I0328 10:59:42.085547 2308280 finetune.py:45] layer 19_o initial loss 1.8888320482801646e-05
I0328 10:59:45.524338 2308141 finetune.py:68] layer 18_o @ epoch 1 new loss 1.8750162780634128e-05 old loss 1.9033936041523702e-05 BETTER
I0328 11:00:02.808547 2307854 finetune.py:45] layer 16_up initial loss 5.178344144951552e-05
I0328 11:00:13.630710 2308005 finetune.py:68] layer 17_o @ epoch 4 new loss 2.1845831724931486e-05 old loss 2.2076594177633524e-05 BETTER
I0328 11:00:14.278007 2308280 finetune.py:68] layer 19_o @ epoch 0 new loss 1.8231850845040753e-05 old loss 1.8888320482801646e-05 BETTER
I0328 11:00:19.437907 2308141 finetune.py:68] layer 18_o @ epoch 2 new loss 1.8543574697105214e-05 old loss 1.8750162780634128e-05 BETTER
I0328 11:00:34.658379 2307854 finetune.py:68] layer 16_up @ epoch 0 new loss 5.077869718661532e-05 old loss 5.178344144951552e-05 BETTER
I0328 11:00:45.315422 2308005 finetune.py:45] layer 17_up initial loss 5.4352523875422776e-05
I0328 11:00:47.494590 2308280 finetune.py:68] layer 19_o @ epoch 1 new loss 1.7968395695788786e-05 old loss 1.8231850845040753e-05 BETTER
I0328 11:00:53.417082 2308141 finetune.py:68] layer 18_o @ epoch 3 new loss 1.8372626072959974e-05 old loss 1.8543574697105214e-05 BETTER
I0328 11:01:08.242253 2307854 finetune.py:68] layer 16_up @ epoch 1 new loss 5.002269972465001e-05 old loss 5.077869718661532e-05 BETTER
I0328 11:01:15.785908 2308005 finetune.py:68] layer 17_up @ epoch 0 new loss 5.324299854692072e-05 old loss 5.4352523875422776e-05 BETTER
I0328 11:01:20.817931 2308280 finetune.py:68] layer 19_o @ epoch 2 new loss 1.7788583136280067e-05 old loss 1.7968395695788786e-05 BETTER
I0328 11:01:27.580023 2308141 finetune.py:68] layer 18_o @ epoch 4 new loss 1.824256469262764e-05 old loss 1.8372626072959974e-05 BETTER
I0328 11:01:41.947144 2307854 finetune.py:68] layer 16_up @ epoch 2 new loss 4.93745319545269e-05 old loss 5.002269972465001e-05 BETTER
I0328 11:01:47.306656 2308005 finetune.py:68] layer 17_up @ epoch 1 new loss 5.239980237092823e-05 old loss 5.324299854692072e-05 BETTER
I0328 11:01:54.249973 2308280 finetune.py:68] layer 19_o @ epoch 3 new loss 1.76387547980994e-05 old loss 1.7788583136280067e-05 BETTER
I0328 11:01:59.708755 2308141 finetune.py:45] layer 18_up initial loss 5.1896244258387014e-05
I0328 11:02:15.814372 2307854 finetune.py:68] layer 16_up @ epoch 3 new loss 4.880766209680587e-05 old loss 4.93745319545269e-05 BETTER
I0328 11:02:18.974652 2308005 finetune.py:68] layer 17_up @ epoch 2 new loss 5.169406358618289e-05 old loss 5.239980237092823e-05 BETTER
I0328 11:02:27.773570 2308280 finetune.py:68] layer 19_o @ epoch 4 new loss 1.7520096662337892e-05 old loss 1.76387547980994e-05 BETTER
I0328 11:02:30.344119 2308141 finetune.py:68] layer 18_up @ epoch 0 new loss 5.0889280828414485e-05 old loss 5.1896244258387014e-05 BETTER
I0328 11:02:49.347575 2307854 finetune.py:68] layer 16_up @ epoch 4 new loss 4.8296893510269e-05 old loss 4.880766209680587e-05 BETTER
I0328 11:02:51.106323 2308005 finetune.py:68] layer 17_up @ epoch 3 new loss 5.107717515784316e-05 old loss 5.169406358618289e-05 BETTER
I0328 11:02:59.718464 2308280 finetune.py:45] layer 19_up initial loss 5.408930155681446e-05
I0328 11:03:02.256309 2308141 finetune.py:68] layer 18_up @ epoch 1 new loss 5.014853741158731e-05 old loss 5.0889280828414485e-05 BETTER
I0328 11:03:21.447807 2307854 finetune.py:45] layer 16_gate initial loss 5.769665585830808e-05
I0328 11:03:23.253941 2308005 finetune.py:68] layer 17_up @ epoch 4 new loss 5.052866254118271e-05 old loss 5.107717515784316e-05 BETTER
I0328 11:03:29.916804 2308280 finetune.py:68] layer 19_up @ epoch 0 new loss 5.3075978939887136e-05 old loss 5.408930155681446e-05 BETTER
I0328 11:03:34.562482 2308141 finetune.py:68] layer 18_up @ epoch 2 new loss 4.953493771608919e-05 old loss 5.014853741158731e-05 BETTER
I0328 11:03:51.414945 2307854 finetune.py:68] layer 16_gate @ epoch 0 new loss 5.718032116419636e-05 old loss 5.769665585830808e-05 BETTER
I0328 11:03:55.285514 2308005 finetune.py:45] layer 17_gate initial loss 6.1319864471443e-05
I0328 11:04:01.224591 2308280 finetune.py:68] layer 19_up @ epoch 1 new loss 5.232781040831469e-05 old loss 5.3075978939887136e-05 BETTER
I0328 11:04:06.799676 2308141 finetune.py:68] layer 18_up @ epoch 3 new loss 4.899188570561819e-05 old loss 4.953493771608919e-05 BETTER
I0328 11:04:22.612580 2307854 finetune.py:68] layer 16_gate @ epoch 1 new loss 5.6738055718597025e-05 old loss 5.718032116419636e-05 BETTER
I0328 11:04:23.942627 2308005 finetune.py:68] layer 17_gate @ epoch 0 new loss 6.0737336752936244e-05 old loss 6.1319864471443e-05 BETTER
I0328 11:04:32.551895 2308280 finetune.py:68] layer 19_up @ epoch 2 new loss 5.169674113858491e-05 old loss 5.232781040831469e-05 BETTER
I0328 11:04:38.935166 2308141 finetune.py:68] layer 18_up @ epoch 4 new loss 4.851576522924006e-05 old loss 4.899188570561819e-05 BETTER
I0328 11:04:53.379495 2308005 finetune.py:68] layer 17_gate @ epoch 1 new loss 6.0255464632064104e-05 old loss 6.0737336752936244e-05 BETTER
I0328 11:04:54.009393 2307854 finetune.py:68] layer 16_gate @ epoch 2 new loss 5.633934051729739e-05 old loss 5.6738055718597025e-05 BETTER
I0328 11:05:03.918404 2308280 finetune.py:68] layer 19_up @ epoch 3 new loss 5.1154995162505656e-05 old loss 5.169674113858491e-05 BETTER
I0328 11:05:10.724780 2308141 finetune.py:45] layer 18_gate initial loss 6.020479850121774e-05
I0328 11:05:23.128874 2308005 finetune.py:68] layer 17_gate @ epoch 2 new loss 5.9823825722560287e-05 old loss 6.0255464632064104e-05 BETTER
I0328 11:05:25.417824 2307854 finetune.py:68] layer 16_gate @ epoch 3 new loss 5.5977408919716254e-05 old loss 5.633934051729739e-05 BETTER
I0328 11:05:35.335956 2308280 finetune.py:68] layer 19_up @ epoch 4 new loss 5.06756296090316e-05 old loss 5.1154995162505656e-05 BETTER
I0328 11:05:39.333325 2308141 finetune.py:68] layer 18_gate @ epoch 0 new loss 5.9699639678001404e-05 old loss 6.020479850121774e-05 BETTER
I0328 11:05:52.905649 2308005 finetune.py:68] layer 17_gate @ epoch 3 new loss 5.942999632679857e-05 old loss 5.9823825722560287e-05 BETTER
I0328 11:05:56.748717 2307854 finetune.py:68] layer 16_gate @ epoch 4 new loss 5.564847378991544e-05 old loss 5.5977408919716254e-05 BETTER
I0328 11:06:07.182249 2308280 finetune.py:45] layer 19_gate initial loss 6.352433410938829e-05
I0328 11:06:08.908389 2308141 finetune.py:68] layer 18_gate @ epoch 1 new loss 5.92825781495776e-05 old loss 5.9699639678001404e-05 BETTER
I0328 11:06:22.697645 2308005 finetune.py:68] layer 17_gate @ epoch 4 new loss 5.907498416490853e-05 old loss 5.942999632679857e-05 BETTER
I0328 11:06:35.188993 2308280 finetune.py:68] layer 19_gate @ epoch 0 new loss 6.302549445535988e-05 old loss 6.352433410938829e-05 BETTER
I0328 11:06:38.699821 2308141 finetune.py:68] layer 18_gate @ epoch 2 new loss 5.890964894206263e-05 old loss 5.92825781495776e-05 BETTER
I0328 11:06:53.127341 2307854 finetune.py:45] layer 16_down initial loss 9.402405703440309e-05
I0328 11:07:04.493752 2308280 finetune.py:68] layer 19_gate @ epoch 1 new loss 6.260826921788976e-05 old loss 6.302549445535988e-05 BETTER
I0328 11:07:08.511406 2308141 finetune.py:68] layer 18_gate @ epoch 3 new loss 5.857585711055435e-05 old loss 5.890964894206263e-05 BETTER
I0328 11:07:19.634372 2308005 finetune.py:45] layer 17_down initial loss 0.00010564340482233092
I0328 11:07:20.370223 2307854 finetune.py:68] layer 16_down @ epoch 0 new loss 9.40209865802899e-05 old loss 9.402405703440309e-05 BETTER
I0328 11:07:34.022879 2308280 finetune.py:68] layer 19_gate @ epoch 2 new loss 6.222962838364765e-05 old loss 6.260826921788976e-05 BETTER
I0328 11:07:38.428056 2308141 finetune.py:68] layer 18_gate @ epoch 4 new loss 5.8269208238925785e-05 old loss 5.857585711055435e-05 BETTER
I0328 11:07:45.818320 2308005 finetune.py:68] layer 17_down @ epoch 0 new loss 0.00010564021795289591 old loss 0.00010564340482233092 BETTER
I0328 11:07:48.775146 2307854 finetune.py:68] layer 16_down @ epoch 1 new loss 9.401841089129448e-05 old loss 9.40209865802899e-05 BETTER
I0328 11:08:03.336774 2308280 finetune.py:68] layer 19_gate @ epoch 3 new loss 6.18964186287485e-05 old loss 6.222962838364765e-05 BETTER
I0328 11:08:13.077564 2308005 finetune.py:68] layer 17_down @ epoch 1 new loss 0.00010563801333773881 old loss 0.00010564021795289591 BETTER
I0328 11:08:17.850855 2307854 finetune.py:68] layer 16_down @ epoch 2 new loss 9.401635179528967e-05 old loss 9.401841089129448e-05 BETTER
I0328 11:08:32.685198 2308280 finetune.py:68] layer 19_gate @ epoch 4 new loss 6.15934914094396e-05 old loss 6.18964186287485e-05 BETTER
I0328 11:08:35.560124 2308141 finetune.py:45] layer 18_down initial loss 0.00010563481191638857
I0328 11:08:40.990040 2308005 finetune.py:68] layer 17_down @ epoch 2 new loss 0.00010563599789747968 old loss 0.00010563801333773881 BETTER
I0328 11:08:46.881013 2307854 finetune.py:68] layer 16_down @ epoch 3 new loss 9.401458373758942e-05 old loss 9.401635179528967e-05 BETTER
I0328 11:09:01.879181 2308141 finetune.py:68] layer 18_down @ epoch 0 new loss 0.00010563281102804467 old loss 0.00010563481191638857 BETTER
I0328 11:09:08.585768 2308005 finetune.py:68] layer 17_down @ epoch 3 new loss 0.00010563444811850786 old loss 0.00010563599789747968 BETTER
I0328 11:09:15.916698 2307854 finetune.py:68] layer 16_down @ epoch 4 new loss 9.401315764989704e-05 old loss 9.401458373758942e-05 BETTER
16_v proxy err 0.01033890899270773 tr(WHW.T) 274.28167724609375
bpp_loss 3.246264085872099
16_q proxy err 0.0004976728814654052 tr(WHW.T) 24486.3125
bpp_loss 4.121303567371797
16_k proxy err 0.00017306390509475023 tr(WHW.T) 19488.1875
bpp_loss 5.060913493158296
16_o proxy err 0.009384382516145706 tr(WHW.T) 970.676513671875
bpp_loss 3.3505426957271993
16_up proxy err 0.007263462524861097 tr(WHW.T) 8331.9443359375
bpp_loss 3.4552331790328026
16_gate proxy err 0.001508726621977985 tr(WHW.T) 41127.6171875
bpp_loss 3.7944934999437203
16_down proxy err 0.009123560041189194 tr(WHW.T) 6303.66455078125
bpp_loss 3.446981355897151
I0328 11:09:29.872672 2308141 finetune.py:68] layer 18_down @ epoch 1 new loss 0.00010563115938566625 old loss 0.00010563281102804467 BETTER
I0328 11:09:31.302250 2308280 finetune.py:45] layer 19_down initial loss 0.00011147826444357634
I0328 11:09:36.942180 2308005 finetune.py:68] layer 17_down @ epoch 4 new loss 0.00010563280375208706 old loss 0.00010563444811850786 BETTER
17_v proxy err 0.0112236887216568 tr(WHW.T) 283.9730224609375
bpp_loss 3.3156873005791567
17_q proxy err 0.0004930543946102262 tr(WHW.T) 27573.458984375
bpp_loss 4.137329720018897
17_k proxy err 0.0002166589692933485 tr(WHW.T) 17412.837890625
bpp_loss 5.090246757608838
17_o proxy err 0.010258406400680542 tr(WHW.T) 1107.3743896484375
bpp_loss 3.380037171416916
17_up proxy err 0.007171515841037035 tr(WHW.T) 8453.962890625
bpp_loss 3.452870612897511
17_gate proxy err 0.0014940615510568023 tr(WHW.T) 41671.53125
bpp_loss 3.808854236267507
17_down proxy err 0.009255615063011646 tr(WHW.T) 6227.32373046875
bpp_loss 3.4434015087982908
I0328 11:09:57.052765 2308280 finetune.py:68] layer 19_down @ epoch 0 new loss 0.00011147600889671594 old loss 0.00011147826444357634 BETTER
I0328 11:09:57.724859 2308141 finetune.py:68] layer 18_down @ epoch 2 new loss 0.00010562947136349976 old loss 0.00010563115938566625 BETTER
I0328 11:10:24.029206 2308280 finetune.py:68] layer 19_down @ epoch 1 new loss 0.00011147432815050706 old loss 0.00011147600889671594 BETTER
I0328 11:10:25.496247 2308141 finetune.py:68] layer 18_down @ epoch 3 new loss 0.00010562820534687489 old loss 0.00010562947136349976 BETTER
I0328 11:10:48.558858 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 66.4322726726532s
I0328 11:10:51.124323 2308280 finetune.py:68] layer 19_down @ epoch 2 new loss 0.00011147283657919616 old loss 0.00011147432815050706 BETTER
I0328 11:10:52.778690 2309565 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 11:10:52.778805 2309565 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 11:10:52.778847 2309565 utils.py:162] NumExpr defaulting to 16 threads.
I0328 11:10:53.177621 2309565 config.py:54] PyTorch version 2.6.0 available.
I0328 11:10:53.365568 2308141 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0001056270775734447 old loss 0.00010562820534687489 BETTER
W0328 11:10:53.387043 2309565 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 11:10:53.995485 2309565 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 11:10:53.999275 2300848 quantize_finetune_llama.py:209] layer 21 gpu 1
I0328 11:10:54.020277 2309565 data_utils.py:336] using 256 training seqs, 128 validation seqs
18_v proxy err 0.011316923424601555 tr(WHW.T) 287.61376953125
bpp_loss 3.240508708287962
18_q proxy err 0.0006234506727196276 tr(WHW.T) 22403.517578125
bpp_loss 4.13326087204041
18_k proxy err 0.00022334263485390693 tr(WHW.T) 17382.265625
bpp_loss 5.1713836275739595
18_o proxy err 0.010117592290043831 tr(WHW.T) 1204.9593505859375
bpp_loss 3.358856215432752
18_up proxy err 0.007903844118118286 tr(WHW.T) 7985.0849609375
bpp_loss 3.44912044956748
18_gate proxy err 0.0018419631524011493 tr(WHW.T) 35222.3828125
bpp_loss 3.816666470308389
18_down proxy err 0.009534137323498726 tr(WHW.T) 6243.0205078125
bpp_loss 3.442450597100625
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 11:11:11.088955 2309565 finetune.py:45] layer 20_v initial loss 2.728624895098619e-05
W0328 11:11:11.089227 2309565 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 11:11:18.356260 2308280 finetune.py:68] layer 19_down @ epoch 3 new loss 0.00011147153418278322 old loss 0.00011147283657919616 BETTER
I0328 11:11:45.433173 2308280 finetune.py:68] layer 19_down @ epoch 4 new loss 0.00011147045734105632 old loss 0.00011147153418278322 BETTER
I0328 11:11:45.981547 2309565 finetune.py:68] layer 20_v @ epoch 0 new loss 9.971616236725822e-06 old loss 2.728624895098619e-05 BETTER
19_v proxy err 0.00988213811069727 tr(WHW.T) 341.0596618652344
bpp_loss 3.2864878994296305
19_q proxy err 0.0005998429260216653 tr(WHW.T) 24041.95703125
bpp_loss 4.135485096427146
19_k proxy err 0.0002534664236009121 tr(WHW.T) 15541.9072265625
bpp_loss 5.069439793005586
19_o proxy err 0.010445060208439827 tr(WHW.T) 1170.39794921875
bpp_loss 3.3721498626982793
19_up proxy err 0.008384998887777328 tr(WHW.T) 7651.591796875
bpp_loss 3.4455623485014906
19_gate proxy err 0.0020136109087616205 tr(WHW.T) 32782.78515625
bpp_loss 3.827918403316289
19_down proxy err 0.009647799655795097 tr(WHW.T) 6199.38525390625
bpp_loss 3.440741091750429
I0328 11:12:02.236401 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 64.27115321159363s
I0328 11:12:06.035564 2309707 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 11:12:06.035660 2309707 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 11:12:06.035701 2309707 utils.py:162] NumExpr defaulting to 16 threads.
I0328 11:12:06.376310 2309707 config.py:54] PyTorch version 2.6.0 available.
W0328 11:12:06.578140 2309707 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 11:12:07.226871 2309707 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 11:12:07.230628 2300848 quantize_finetune_llama.py:209] layer 22 gpu 2
I0328 11:12:07.244124 2309707 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 11:12:22.318295 2309565 finetune.py:68] layer 20_v @ epoch 1 new loss 9.067312021215912e-06 old loss 9.971616236725822e-06 BETTER
I0328 11:12:24.402046 2309707 finetune.py:45] layer 21_v initial loss 2.9493387046386488e-05
W0328 11:12:24.402232 2309707 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 11:12:57.509125 2309707 finetune.py:68] layer 21_v @ epoch 0 new loss 1.192428135254886e-05 old loss 2.9493387046386488e-05 BETTER
I0328 11:12:59.200858 2309565 finetune.py:68] layer 20_v @ epoch 2 new loss 8.681592589709908e-06 old loss 9.067312021215912e-06 BETTER
I0328 11:13:12.036205 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 64.35673785209656s
I0328 11:13:15.825061 2309843 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 11:13:15.825162 2309843 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 11:13:15.825202 2309843 utils.py:162] NumExpr defaulting to 16 threads.
I0328 11:13:16.166798 2309843 config.py:54] PyTorch version 2.6.0 available.
W0328 11:13:16.381298 2309843 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 11:13:17.002917 2309843 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 11:13:17.006677 2300848 quantize_finetune_llama.py:209] layer 23 gpu 3
I0328 11:13:17.020260 2309843 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 11:13:32.022198 2309707 finetune.py:68] layer 21_v @ epoch 1 new loss 1.0828433005372062e-05 old loss 1.192428135254886e-05 BETTER
I0328 11:13:34.567275 2309843 finetune.py:45] layer 22_v initial loss 3.239627039874904e-05
W0328 11:13:34.567494 2309843 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 11:13:36.109854 2309565 finetune.py:68] layer 20_v @ epoch 3 new loss 8.447120308119338e-06 old loss 8.681592589709908e-06 BETTER
I0328 11:14:06.921442 2309707 finetune.py:68] layer 21_v @ epoch 2 new loss 1.032816908264067e-05 old loss 1.0828433005372062e-05 BETTER
I0328 11:14:08.063276 2309843 finetune.py:68] layer 22_v @ epoch 0 new loss 9.803920875128824e-06 old loss 3.239627039874904e-05 BETTER
I0328 11:14:13.195060 2309565 finetune.py:68] layer 20_v @ epoch 4 new loss 8.287849595944863e-06 old loss 8.447120308119338e-06 BETTER
I0328 11:14:22.231872 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 64.74713063240051s
I0328 11:14:26.090658 2309982 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 11:14:26.090770 2309982 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 11:14:26.090813 2309982 utils.py:162] NumExpr defaulting to 16 threads.
I0328 11:14:26.511932 2309982 config.py:54] PyTorch version 2.6.0 available.
W0328 11:14:26.732063 2309982 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 11:14:27.345702 2309982 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 11:14:27.349470 2300848 quantize_finetune_llama.py:209] layer 24 gpu 0
I0328 11:14:27.365775 2309982 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 11:14:32.727046 2309565 finetune.py:45] layer 20_q initial loss 9.623568985261954e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 11:14:41.925695 2309707 finetune.py:68] layer 21_v @ epoch 3 new loss 9.99882922769757e-06 old loss 1.032816908264067e-05 BETTER
I0328 11:14:42.774713 2309843 finetune.py:68] layer 22_v @ epoch 1 new loss 8.716843694855925e-06 old loss 9.803920875128824e-06 BETTER
I0328 11:14:45.387623 2309982 finetune.py:45] layer 23_v initial loss 3.896749694831669e-05
W0328 11:14:45.387812 2309982 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 11:15:07.713808 2309565 finetune.py:68] layer 20_q @ epoch 0 new loss 9.404090633324813e-06 old loss 9.623568985261954e-06 BETTER
I0328 11:15:17.539427 2309707 finetune.py:68] layer 21_v @ epoch 4 new loss 9.752815458341502e-06 old loss 9.99882922769757e-06 BETTER
I0328 11:15:18.139618 2309843 finetune.py:68] layer 22_v @ epoch 2 new loss 8.282860108010937e-06 old loss 8.716843694855925e-06 BETTER
I0328 11:15:18.664864 2309982 finetune.py:68] layer 23_v @ epoch 0 new loss 1.0244817531201988e-05 old loss 3.896749694831669e-05 BETTER
I0328 11:15:37.250129 2309707 finetune.py:45] layer 21_q initial loss 1.1936574082938023e-05
I0328 11:15:43.999286 2309565 finetune.py:68] layer 20_q @ epoch 1 new loss 9.248925380234141e-06 old loss 9.404090633324813e-06 BETTER
I0328 11:15:53.082248 2309982 finetune.py:68] layer 23_v @ epoch 1 new loss 8.869315934134647e-06 old loss 1.0244817531201988e-05 BETTER
I0328 11:15:53.243929 2309843 finetune.py:68] layer 22_v @ epoch 3 new loss 8.024085218494292e-06 old loss 8.282860108010937e-06 BETTER
I0328 11:16:10.803183 2309707 finetune.py:68] layer 21_q @ epoch 0 new loss 1.1564316992007662e-05 old loss 1.1936574082938023e-05 BETTER
I0328 11:16:20.421110 2309565 finetune.py:68] layer 20_q @ epoch 2 new loss 9.121791663346812e-06 old loss 9.248925380234141e-06 BETTER
I0328 11:16:27.565392 2309982 finetune.py:68] layer 23_v @ epoch 2 new loss 8.393089956371114e-06 old loss 8.869315934134647e-06 BETTER
I0328 11:16:28.434310 2309843 finetune.py:68] layer 22_v @ epoch 4 new loss 7.85891643317882e-06 old loss 8.024085218494292e-06 BETTER
I0328 11:16:45.332950 2309707 finetune.py:68] layer 21_q @ epoch 1 new loss 1.1325650120852515e-05 old loss 1.1564316992007662e-05 BETTER
I0328 11:16:48.073068 2309843 finetune.py:45] layer 22_q initial loss 9.676155059423763e-06
I0328 11:16:57.189752 2309565 finetune.py:68] layer 20_q @ epoch 3 new loss 9.019012395583559e-06 old loss 9.121791663346812e-06 BETTER
I0328 11:17:02.111166 2309982 finetune.py:68] layer 23_v @ epoch 3 new loss 8.128326953737997e-06 old loss 8.393089956371114e-06 BETTER
I0328 11:17:19.880381 2309707 finetune.py:68] layer 21_q @ epoch 2 new loss 1.1125988748972304e-05 old loss 1.1325650120852515e-05 BETTER
I0328 11:17:21.764143 2309843 finetune.py:68] layer 22_q @ epoch 0 new loss 9.408830010215752e-06 old loss 9.676155059423763e-06 BETTER
I0328 11:17:34.015357 2309565 finetune.py:68] layer 20_q @ epoch 4 new loss 8.931694537750445e-06 old loss 9.019012395583559e-06 BETTER
I0328 11:17:36.814993 2309982 finetune.py:68] layer 23_v @ epoch 4 new loss 7.953744898259174e-06 old loss 8.128326953737997e-06 BETTER
I0328 11:17:52.219221 2309565 finetune.py:45] layer 20_k initial loss 9.387676072947215e-06
I0328 11:17:54.817401 2309707 finetune.py:68] layer 21_q @ epoch 3 new loss 1.0969933100568596e-05 old loss 1.1125988748972304e-05 BETTER
I0328 11:17:56.545989 2309843 finetune.py:68] layer 22_q @ epoch 1 new loss 9.242330634151585e-06 old loss 9.408830010215752e-06 BETTER
I0328 11:17:57.365479 2309982 finetune.py:45] layer 23_q initial loss 9.615559974918142e-06
I0328 11:18:27.517443 2309565 finetune.py:68] layer 20_k @ epoch 0 new loss 9.295785275753587e-06 old loss 9.387676072947215e-06 BETTER
I0328 11:18:29.937075 2309707 finetune.py:68] layer 21_q @ epoch 4 new loss 1.0836477486009244e-05 old loss 1.0969933100568596e-05 BETTER
I0328 11:18:30.646726 2309982 finetune.py:68] layer 23_q @ epoch 0 new loss 9.374854926136322e-06 old loss 9.615559974918142e-06 BETTER
I0328 11:18:31.509647 2309843 finetune.py:68] layer 22_q @ epoch 2 new loss 9.116397450270597e-06 old loss 9.242330634151585e-06 BETTER
I0328 11:18:47.913333 2309707 finetune.py:45] layer 21_k initial loss 1.1828214155684691e-05
I0328 11:19:04.212837 2309565 finetune.py:68] layer 20_k @ epoch 1 new loss 9.22673280001618e-06 old loss 9.295785275753587e-06 BETTER
I0328 11:19:05.061084 2309982 finetune.py:68] layer 23_q @ epoch 1 new loss 9.210252756020054e-06 old loss 9.374854926136322e-06 BETTER
I0328 11:19:06.380725 2309843 finetune.py:68] layer 22_q @ epoch 3 new loss 9.018152013595682e-06 old loss 9.116397450270597e-06 BETTER
I0328 11:19:21.262302 2309707 finetune.py:68] layer 21_k @ epoch 0 new loss 1.1549046575964894e-05 old loss 1.1828214155684691e-05 BETTER
I0328 11:19:40.076310 2309982 finetune.py:68] layer 23_q @ epoch 2 new loss 9.085895726457238e-06 old loss 9.210252756020054e-06 BETTER
I0328 11:19:41.051667 2309565 finetune.py:68] layer 20_k @ epoch 2 new loss 9.167940334009472e-06 old loss 9.22673280001618e-06 BETTER
I0328 11:19:41.658723 2309843 finetune.py:68] layer 22_q @ epoch 4 new loss 8.934514880820643e-06 old loss 9.018152013595682e-06 BETTER
I0328 11:19:55.614278 2309707 finetune.py:68] layer 21_k @ epoch 1 new loss 1.1438795809226576e-05 old loss 1.1549046575964894e-05 BETTER
I0328 11:19:59.878524 2309843 finetune.py:45] layer 22_k initial loss 9.607257197785657e-06
I0328 11:20:14.368677 2309982 finetune.py:68] layer 23_q @ epoch 3 new loss 8.991495633381419e-06 old loss 9.085895726457238e-06 BETTER
I0328 11:20:17.637968 2309565 finetune.py:68] layer 20_k @ epoch 3 new loss 9.115612556342967e-06 old loss 9.167940334009472e-06 BETTER
I0328 11:20:29.946739 2309707 finetune.py:68] layer 21_k @ epoch 2 new loss 1.1342635843902826e-05 old loss 1.1438795809226576e-05 BETTER
I0328 11:20:33.372549 2309843 finetune.py:68] layer 22_k @ epoch 0 new loss 9.51516176428413e-06 old loss 9.607257197785657e-06 BETTER
I0328 11:20:48.698622 2309982 finetune.py:68] layer 23_q @ epoch 4 new loss 8.91548097570194e-06 old loss 8.991495633381419e-06 BETTER
I0328 11:20:54.409335 2309565 finetune.py:68] layer 20_k @ epoch 4 new loss 9.074999979929999e-06 old loss 9.115612556342967e-06 BETTER
I0328 11:21:04.431100 2309707 finetune.py:68] layer 21_k @ epoch 3 new loss 1.126871029555332e-05 old loss 1.1342635843902826e-05 BETTER
I0328 11:21:07.310059 2309982 finetune.py:45] layer 23_k initial loss 9.67459891398903e-06
I0328 11:21:07.933642 2309843 finetune.py:68] layer 22_k @ epoch 1 new loss 9.450362995266914e-06 old loss 9.51516176428413e-06 BETTER
I0328 11:21:14.378688 2309565 finetune.py:45] layer 20_o initial loss 2.0354013031465e-05
I0328 11:21:39.074871 2309707 finetune.py:68] layer 21_k @ epoch 4 new loss 1.1209632248210255e-05 old loss 1.126871029555332e-05 BETTER
I0328 11:21:40.342405 2309982 finetune.py:68] layer 23_k @ epoch 0 new loss 9.584657163941301e-06 old loss 9.67459891398903e-06 BETTER
I0328 11:21:42.555550 2309843 finetune.py:68] layer 22_k @ epoch 2 new loss 9.399137525178958e-06 old loss 9.450362995266914e-06 BETTER
I0328 11:21:49.174115 2309565 finetune.py:68] layer 20_o @ epoch 0 new loss 1.9638937374111265e-05 old loss 2.0354013031465e-05 BETTER
I0328 11:21:59.136782 2309707 finetune.py:45] layer 21_o initial loss 2.5519922928651795e-05
I0328 11:22:14.337048 2309982 finetune.py:68] layer 23_k @ epoch 1 new loss 9.517087164567783e-06 old loss 9.584657163941301e-06 BETTER
I0328 11:22:17.174375 2309843 finetune.py:68] layer 22_k @ epoch 3 new loss 9.352140295959543e-06 old loss 9.399137525178958e-06 BETTER
I0328 11:22:24.856554 2309565 finetune.py:68] layer 20_o @ epoch 1 new loss 1.9343277017469518e-05 old loss 1.9638937374111265e-05 BETTER
I0328 11:22:31.737799 2309707 finetune.py:68] layer 21_o @ epoch 0 new loss 2.4508917704224586e-05 old loss 2.5519922928651795e-05 BETTER
I0328 11:22:48.460677 2309982 finetune.py:68] layer 23_k @ epoch 2 new loss 9.459420653001871e-06 old loss 9.517087164567783e-06 BETTER
I0328 11:22:51.837487 2309843 finetune.py:68] layer 22_k @ epoch 4 new loss 9.31505474000005e-06 old loss 9.352140295959543e-06 BETTER
I0328 11:23:00.759996 2309565 finetune.py:68] layer 20_o @ epoch 2 new loss 1.913068990688771e-05 old loss 1.9343277017469518e-05 BETTER
I0328 11:23:05.463423 2309707 finetune.py:68] layer 21_o @ epoch 1 new loss 2.4052647859207354e-05 old loss 2.4508917704224586e-05 BETTER
I0328 11:23:11.478660 2309843 finetune.py:45] layer 22_o initial loss 2.2852211259305477e-05
I0328 11:23:22.644264 2309982 finetune.py:68] layer 23_k @ epoch 3 new loss 9.413993211637717e-06 old loss 9.459420653001871e-06 BETTER
I0328 11:23:37.077318 2309565 finetune.py:68] layer 20_o @ epoch 3 new loss 1.8954979168483987e-05 old loss 1.913068990688771e-05 BETTER
I0328 11:23:39.281748 2309707 finetune.py:68] layer 21_o @ epoch 2 new loss 2.3731234250590205e-05 old loss 2.4052647859207354e-05 BETTER
I0328 11:23:44.395920 2309843 finetune.py:68] layer 22_o @ epoch 0 new loss 2.2087464458309114e-05 old loss 2.2852211259305477e-05 BETTER
I0328 11:23:56.800752 2309982 finetune.py:68] layer 23_k @ epoch 4 new loss 9.37358163355384e-06 old loss 9.413993211637717e-06 BETTER
I0328 11:24:13.135616 2309707 finetune.py:68] layer 21_o @ epoch 3 new loss 2.347466943319887e-05 old loss 2.3731234250590205e-05 BETTER
I0328 11:24:13.154957 2309565 finetune.py:68] layer 20_o @ epoch 4 new loss 1.8818393073161133e-05 old loss 1.8954979168483987e-05 BETTER
I0328 11:24:17.069810 2309982 finetune.py:45] layer 23_o initial loss 2.359294558118563e-05
I0328 11:24:18.316174 2309843 finetune.py:68] layer 22_o @ epoch 1 new loss 2.1797388399136253e-05 old loss 2.2087464458309114e-05 BETTER
I0328 11:24:45.509750 2309565 finetune.py:45] layer 20_up initial loss 5.8088353398488835e-05
I0328 11:24:47.533667 2309707 finetune.py:68] layer 21_o @ epoch 4 new loss 2.3267086362466216e-05 old loss 2.347466943319887e-05 BETTER
I0328 11:24:49.525130 2309982 finetune.py:68] layer 23_o @ epoch 0 new loss 2.268259777338244e-05 old loss 2.359294558118563e-05 BETTER
I0328 11:24:52.260896 2309843 finetune.py:68] layer 22_o @ epoch 2 new loss 2.159266841772478e-05 old loss 2.1797388399136253e-05 BETTER
I0328 11:25:17.437798 2309565 finetune.py:68] layer 20_up @ epoch 0 new loss 5.7046388974413276e-05 old loss 5.8088353398488835e-05 BETTER
I0328 11:25:19.430797 2309707 finetune.py:45] layer 21_up initial loss 6.800628761993721e-05
I0328 11:25:23.055305 2309982 finetune.py:68] layer 23_o @ epoch 1 new loss 2.239048626506701e-05 old loss 2.268259777338244e-05 BETTER
I0328 11:25:26.389847 2309843 finetune.py:68] layer 22_o @ epoch 3 new loss 2.1438854673760943e-05 old loss 2.159266841772478e-05 BETTER
I0328 11:25:50.055997 2309707 finetune.py:68] layer 21_up @ epoch 0 new loss 6.672210292890668e-05 old loss 6.800628761993721e-05 BETTER
I0328 11:25:50.911988 2309565 finetune.py:68] layer 20_up @ epoch 1 new loss 5.6273656809935346e-05 old loss 5.7046388974413276e-05 BETTER
I0328 11:25:56.911953 2309982 finetune.py:68] layer 23_o @ epoch 2 new loss 2.218016015831381e-05 old loss 2.239048626506701e-05 BETTER
I0328 11:26:00.548402 2309843 finetune.py:68] layer 22_o @ epoch 4 new loss 2.1311550881364383e-05 old loss 2.1438854673760943e-05 BETTER
I0328 11:26:22.065360 2309707 finetune.py:68] layer 21_up @ epoch 1 new loss 6.578236207133159e-05 old loss 6.672210292890668e-05 BETTER
I0328 11:26:24.576437 2309565 finetune.py:68] layer 20_up @ epoch 2 new loss 5.563014929066412e-05 old loss 5.6273656809935346e-05 BETTER
I0328 11:26:30.596048 2309982 finetune.py:68] layer 23_o @ epoch 3 new loss 2.2021631593815982e-05 old loss 2.218016015831381e-05 BETTER
I0328 11:26:32.351586 2309843 finetune.py:45] layer 22_up initial loss 6.834571831859648e-05
I0328 11:26:54.221556 2309707 finetune.py:68] layer 21_up @ epoch 2 new loss 6.499804294435307e-05 old loss 6.578236207133159e-05 BETTER
I0328 11:26:58.490292 2309565 finetune.py:68] layer 20_up @ epoch 3 new loss 5.506689922185615e-05 old loss 5.563014929066412e-05 BETTER
I0328 11:27:03.149430 2309843 finetune.py:68] layer 22_up @ epoch 0 new loss 6.711923197144642e-05 old loss 6.834571831859648e-05 BETTER
I0328 11:27:04.156137 2309982 finetune.py:68] layer 23_o @ epoch 4 new loss 2.1898949853493832e-05 old loss 2.2021631593815982e-05 BETTER
I0328 11:27:26.557724 2309707 finetune.py:68] layer 21_up @ epoch 3 new loss 6.432678492274135e-05 old loss 6.499804294435307e-05 BETTER
I0328 11:27:32.382835 2309565 finetune.py:68] layer 20_up @ epoch 4 new loss 5.458018495119177e-05 old loss 5.506689922185615e-05 BETTER
I0328 11:27:35.382290 2309843 finetune.py:68] layer 22_up @ epoch 1 new loss 6.625973037444055e-05 old loss 6.711923197144642e-05 BETTER
I0328 11:27:36.493278 2309982 finetune.py:45] layer 23_up initial loss 7.297651609405875e-05
I0328 11:27:58.882591 2309707 finetune.py:68] layer 21_up @ epoch 4 new loss 6.374187796609476e-05 old loss 6.432678492274135e-05 BETTER
I0328 11:28:04.223106 2309565 finetune.py:45] layer 20_gate initial loss 6.92011890350841e-05
I0328 11:28:06.724519 2309982 finetune.py:68] layer 23_up @ epoch 0 new loss 7.182233093772084e-05 old loss 7.297651609405875e-05 BETTER
I0328 11:28:07.695583 2309843 finetune.py:68] layer 22_up @ epoch 2 new loss 6.55425974400714e-05 old loss 6.625973037444055e-05 BETTER
I0328 11:28:30.462092 2309707 finetune.py:45] layer 21_gate initial loss 8.038343366933987e-05
I0328 11:28:34.373770 2309565 finetune.py:68] layer 20_gate @ epoch 0 new loss 6.86850180500187e-05 old loss 6.92011890350841e-05 BETTER
I0328 11:28:37.932915 2309982 finetune.py:68] layer 23_up @ epoch 1 new loss 7.099258073139936e-05 old loss 7.182233093772084e-05 BETTER
I0328 11:28:39.835321 2309843 finetune.py:68] layer 22_up @ epoch 3 new loss 6.493108230642974e-05 old loss 6.55425974400714e-05 BETTER
I0328 11:28:58.890137 2309707 finetune.py:68] layer 21_gate @ epoch 0 new loss 7.975639164214954e-05 old loss 8.038343366933987e-05 BETTER
I0328 11:29:05.675795 2309565 finetune.py:68] layer 20_gate @ epoch 1 new loss 6.825260061305016e-05 old loss 6.86850180500187e-05 BETTER
I0328 11:29:09.332108 2309982 finetune.py:68] layer 23_up @ epoch 2 new loss 7.031446148175746e-05 old loss 7.099258073139936e-05 BETTER
I0328 11:29:12.120972 2309843 finetune.py:68] layer 22_up @ epoch 4 new loss 6.43937528366223e-05 old loss 6.493108230642974e-05 BETTER
I0328 11:29:28.201433 2309707 finetune.py:68] layer 21_gate @ epoch 1 new loss 7.924368401290849e-05 old loss 7.975639164214954e-05 BETTER
I0328 11:29:37.395605 2309565 finetune.py:68] layer 20_gate @ epoch 2 new loss 6.786733865737915e-05 old loss 6.825260061305016e-05 BETTER
I0328 11:29:40.907653 2309982 finetune.py:68] layer 23_up @ epoch 3 new loss 6.973600829951465e-05 old loss 7.031446148175746e-05 BETTER
I0328 11:29:44.651858 2309843 finetune.py:45] layer 22_gate initial loss 8.254372369265184e-05
I0328 11:29:57.982258 2309707 finetune.py:68] layer 21_gate @ epoch 2 new loss 7.878642645664513e-05 old loss 7.924368401290849e-05 BETTER
I0328 11:30:09.101527 2309565 finetune.py:68] layer 20_gate @ epoch 3 new loss 6.752618355676532e-05 old loss 6.786733865737915e-05 BETTER
I0328 11:30:12.408054 2309982 finetune.py:68] layer 23_up @ epoch 4 new loss 6.923508044565096e-05 old loss 6.973600829951465e-05 BETTER
I0328 11:30:13.255766 2309843 finetune.py:68] layer 22_gate @ epoch 0 new loss 8.196454291464761e-05 old loss 8.254372369265184e-05 BETTER
I0328 11:30:27.632026 2309707 finetune.py:68] layer 21_gate @ epoch 3 new loss 7.838521560188383e-05 old loss 7.878642645664513e-05 BETTER
I0328 11:30:40.820727 2309565 finetune.py:68] layer 20_gate @ epoch 4 new loss 6.721428508171812e-05 old loss 6.752618355676532e-05 BETTER
I0328 11:30:43.063209 2309843 finetune.py:68] layer 22_gate @ epoch 1 new loss 8.1489713920746e-05 old loss 8.196454291464761e-05 BETTER
I0328 11:30:44.492000 2309982 finetune.py:45] layer 23_gate initial loss 8.972997602541e-05
I0328 11:30:57.469080 2309707 finetune.py:68] layer 21_gate @ epoch 4 new loss 7.80189293436706e-05 old loss 7.838521560188383e-05 BETTER
I0328 11:31:12.664010 2309982 finetune.py:68] layer 23_gate @ epoch 0 new loss 8.920003165258095e-05 old loss 8.972997602541e-05 BETTER
I0328 11:31:13.034481 2309843 finetune.py:68] layer 22_gate @ epoch 2 new loss 8.107658504741266e-05 old loss 8.1489713920746e-05 BETTER
I0328 11:31:36.640548 2309565 finetune.py:45] layer 20_down initial loss 0.00012046089977957308
I0328 11:31:41.851404 2309982 finetune.py:68] layer 23_gate @ epoch 1 new loss 8.876946958480403e-05 old loss 8.920003165258095e-05 BETTER
I0328 11:31:43.017373 2309843 finetune.py:68] layer 22_gate @ epoch 3 new loss 8.070688636507839e-05 old loss 8.107658504741266e-05 BETTER
I0328 11:31:54.060050 2309707 finetune.py:45] layer 21_down initial loss 0.0001400188048137352
I0328 11:32:03.892609 2309565 finetune.py:68] layer 20_down @ epoch 0 new loss 0.00012045902985846624 old loss 0.00012046089977957308 BETTER
I0328 11:32:11.413443 2309982 finetune.py:68] layer 23_gate @ epoch 2 new loss 8.839167276164517e-05 old loss 8.876946958480403e-05 BETTER
I0328 11:32:13.153509 2309843 finetune.py:68] layer 22_gate @ epoch 4 new loss 8.03755028755404e-05 old loss 8.070688636507839e-05 BETTER
I0328 11:32:20.139178 2309707 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0001400166074745357 old loss 0.0001400188048137352 BETTER
I0328 11:32:32.152910 2309565 finetune.py:68] layer 20_down @ epoch 1 new loss 0.00012045738549204543 old loss 0.00012045902985846624 BETTER
I0328 11:32:40.720829 2309982 finetune.py:68] layer 23_gate @ epoch 3 new loss 8.805395918898284e-05 old loss 8.839167276164517e-05 BETTER
I0328 11:32:47.334041 2309707 finetune.py:68] layer 21_down @ epoch 1 new loss 0.00014001457020640373 old loss 0.0001400166074745357 BETTER
I0328 11:33:00.825980 2309565 finetune.py:68] layer 20_down @ epoch 2 new loss 0.00012045611219946295 old loss 0.00012045738549204543 BETTER
I0328 11:33:09.445140 2309843 finetune.py:45] layer 22_down initial loss 0.00014603452291339636
I0328 11:33:10.137203 2309982 finetune.py:68] layer 23_gate @ epoch 4 new loss 8.776078902883455e-05 old loss 8.805395918898284e-05 BETTER
I0328 11:33:14.730124 2309707 finetune.py:68] layer 21_down @ epoch 2 new loss 0.00014001304225530475 old loss 0.00014001457020640373 BETTER
I0328 11:33:29.862850 2309565 finetune.py:68] layer 20_down @ epoch 3 new loss 0.00012045478797517717 old loss 0.00012045611219946295 BETTER
I0328 11:33:35.762039 2309843 finetune.py:68] layer 22_down @ epoch 0 new loss 0.00014603231102228165 old loss 0.00014603452291339636 BETTER
I0328 11:33:42.351391 2309707 finetune.py:68] layer 21_down @ epoch 3 new loss 0.00014001164527144283 old loss 0.00014001304225530475 BETTER
I0328 11:33:59.359798 2309565 finetune.py:68] layer 20_down @ epoch 4 new loss 0.00012045391486026347 old loss 0.00012045478797517717 BETTER
20_v proxy err 0.009891284629702568 tr(WHW.T) 330.192138671875
bpp_loss 3.32058716902975
20_q proxy err 0.0006703473045490682 tr(WHW.T) 20746.31640625
bpp_loss 4.106936304306146
20_k proxy err 0.00024891149951145053 tr(WHW.T) 15388.685546875
bpp_loss 5.022697334992699
20_o proxy err 0.010974062606692314 tr(WHW.T) 1206.1986083984375
bpp_loss 3.3588023231714033
20_up proxy err 0.008525906130671501 tr(WHW.T) 7602.03173828125
bpp_loss 3.449313427321613
20_gate proxy err 0.0021759537048637867 tr(WHW.T) 30630.9375
bpp_loss 3.829967097179698
20_down proxy err 0.009519137442111969 tr(WHW.T) 6316.87109375
bpp_loss 3.44497596608874
I0328 11:34:03.193045 2309843 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0001460305938962847 old loss 0.00014603231102228165 BETTER
I0328 11:34:09.025818 2309982 finetune.py:45] layer 23_down initial loss 0.000157635411596857
I0328 11:34:11.229332 2309707 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0001400100445607677 old loss 0.00014001164527144283 BETTER
21_v proxy err 0.009146014228463173 tr(WHW.T) 362.87310791015625
bpp_loss 3.3461508937180042
21_q proxy err 0.0005449769669212401 tr(WHW.T) 25847.666015625
bpp_loss 4.10418302367907
21_k proxy err 0.00023083061387296766 tr(WHW.T) 16777.796875
bpp_loss 5.065097332000732
21_o proxy err 0.00742004718631506 tr(WHW.T) 1269.3822021484375
bpp_loss 3.377179404720664
21_up proxy err 0.008188193663954735 tr(WHW.T) 7772.310546875
bpp_loss 3.4521835651913926
21_gate proxy err 0.0020790123380720615 tr(WHW.T) 31522.408203125
bpp_loss 3.8417750082111786
21_down proxy err 0.008971973322331905 tr(WHW.T) 6367.076171875
bpp_loss 3.4459355950654884
I0328 11:34:31.809037 2309843 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0001460290077375248 old loss 0.0001460305938962847 BETTER
I0328 11:34:34.978079 2309982 finetune.py:68] layer 23_down @ epoch 0 new loss 0.00015763359260745347 old loss 0.000157635411596857 BETTER
I0328 11:34:59.555156 2309843 finetune.py:68] layer 22_down @ epoch 3 new loss 0.00014602774172089994 old loss 0.0001460290077375248 BETTER
I0328 11:35:01.875572 2309982 finetune.py:68] layer 23_down @ epoch 1 new loss 0.00015763158444315195 old loss 0.00015763359260745347 BETTER
I0328 11:35:23.267813 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 66.77730655670166s
I0328 11:35:27.285346 2310059 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 11:35:27.285447 2310059 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 11:35:27.285490 2310059 utils.py:162] NumExpr defaulting to 16 threads.
I0328 11:35:27.693645 2310059 config.py:54] PyTorch version 2.6.0 available.
I0328 11:35:27.717568 2309843 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0001460266939830035 old loss 0.00014602774172089994 BETTER
W0328 11:35:27.909190 2310059 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 11:35:28.551683 2310059 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 11:35:28.555726 2300848 quantize_finetune_llama.py:209] layer 25 gpu 1
I0328 11:35:28.575557 2310059 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 11:35:28.988131 2309982 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0001576303329784423 old loss 0.00015763158444315195 BETTER
22_v proxy err 0.009075111709535122 tr(WHW.T) 346.0789489746094
bpp_loss 3.396570733282715
22_q proxy err 0.0006521373288705945 tr(WHW.T) 20328.619140625
bpp_loss 4.063340200402308
22_k proxy err 0.00024935606052167714 tr(WHW.T) 14716.2060546875
bpp_loss 4.999459545942955
22_o proxy err 0.010258039459586143 tr(WHW.T) 1222.539794921875
bpp_loss 3.40627298085019
22_up proxy err 0.008529779501259327 tr(WHW.T) 7548.5380859375
bpp_loss 3.456477675719985
22_gate proxy err 0.0022453840356320143 tr(WHW.T) 29516.43359375
bpp_loss 3.846290490656559
22_down proxy err 0.008998831734061241 tr(WHW.T) 6546.11279296875
bpp_loss 3.4516797630848095
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 11:35:46.409662 2310059 finetune.py:45] layer 24_v initial loss 4.607646042131819e-05
W0328 11:35:46.410034 2310059 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 11:35:56.164222 2309982 finetune.py:68] layer 23_down @ epoch 3 new loss 0.00015762915427330881 old loss 0.0001576303329784423 BETTER
I0328 11:36:21.340137 2310059 finetune.py:68] layer 24_v @ epoch 0 new loss 1.086129941541003e-05 old loss 4.607646042131819e-05 BETTER
I0328 11:36:23.439176 2309982 finetune.py:68] layer 23_down @ epoch 4 new loss 0.00015762800467200577 old loss 0.00015762915427330881 BETTER
23_v proxy err 0.008523829281330109 tr(WHW.T) 397.90704345703125
bpp_loss 3.4489405612112023
23_q proxy err 0.0006301190587691963 tr(WHW.T) 22617.640625
bpp_loss 4.075493737240322
23_k proxy err 0.00026321838959120214 tr(WHW.T) 14851.5439453125
bpp_loss 5.0015744644915685
23_o proxy err 0.008663219399750233 tr(WHW.T) 1745.08154296875
bpp_loss 3.4295110790408216
23_up proxy err 0.008673382923007011 tr(WHW.T) 7421.376953125
bpp_loss 3.460447805068855
23_gate proxy err 0.002428824547678232 tr(WHW.T) 27266.3046875
bpp_loss 3.848639148087906
23_down proxy err 0.009005933068692684 tr(WHW.T) 6683.54345703125
bpp_loss 3.4569987149110863
I0328 11:36:37.006931 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 64.61326813697815s
I0328 11:36:40.678647 2310129 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 11:36:40.678746 2310129 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 11:36:40.678786 2310129 utils.py:162] NumExpr defaulting to 16 threads.
I0328 11:36:41.015202 2310129 config.py:54] PyTorch version 2.6.0 available.
W0328 11:36:41.215961 2310129 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 11:36:41.840338 2310129 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 11:36:41.844191 2300848 quantize_finetune_llama.py:209] layer 26 gpu 2
I0328 11:36:41.857510 2310129 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 11:36:57.775304 2310059 finetune.py:68] layer 24_v @ epoch 1 new loss 9.331102774012834e-06 old loss 1.086129941541003e-05 BETTER
I0328 11:36:58.997540 2310129 finetune.py:45] layer 25_v initial loss 4.775697743752971e-05
W0328 11:36:58.997911 2310129 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 11:37:32.161546 2310129 finetune.py:68] layer 25_v @ epoch 0 new loss 1.2244172467035241e-05 old loss 4.775697743752971e-05 BETTER
I0328 11:37:34.582882 2310059 finetune.py:68] layer 24_v @ epoch 2 new loss 8.844295734888874e-06 old loss 9.331102774012834e-06 BETTER
I0328 11:37:46.077138 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 63.77530097961426s
I0328 11:37:49.926913 2310199 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 11:37:49.927022 2310199 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 11:37:49.927065 2310199 utils.py:162] NumExpr defaulting to 16 threads.
I0328 11:37:50.303678 2310199 config.py:54] PyTorch version 2.6.0 available.
W0328 11:37:50.516593 2310199 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 11:37:51.151899 2310199 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 11:37:51.155696 2300848 quantize_finetune_llama.py:209] layer 27 gpu 3
I0328 11:37:51.168973 2310199 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 11:38:06.632956 2310129 finetune.py:68] layer 25_v @ epoch 1 new loss 1.08394306153059e-05 old loss 1.2244172467035241e-05 BETTER
I0328 11:38:09.365599 2310199 finetune.py:45] layer 26_v initial loss 4.605811045621522e-05
W0328 11:38:09.365974 2310199 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 11:38:11.545696 2310059 finetune.py:68] layer 24_v @ epoch 3 new loss 8.57681061461335e-06 old loss 8.844295734888874e-06 BETTER
I0328 11:38:41.393346 2310129 finetune.py:68] layer 25_v @ epoch 2 new loss 1.0274094165652059e-05 old loss 1.08394306153059e-05 BETTER
I0328 11:38:42.923176 2310199 finetune.py:68] layer 26_v @ epoch 0 new loss 1.7010688679874875e-05 old loss 4.605811045621522e-05 BETTER
I0328 11:38:48.519722 2310059 finetune.py:68] layer 24_v @ epoch 4 new loss 8.38875348563306e-06 old loss 8.57681061461335e-06 BETTER
I0328 11:38:55.762470 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 64.11479091644287s
I0328 11:38:59.588646 2310269 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 11:38:59.588743 2310269 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 11:38:59.588785 2310269 utils.py:162] NumExpr defaulting to 16 threads.
I0328 11:38:59.946599 2310269 config.py:54] PyTorch version 2.6.0 available.
W0328 11:39:00.165008 2310269 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 11:39:01.060132 2310269 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 11:39:01.064009 2300848 quantize_finetune_llama.py:209] layer 28 gpu 0
I0328 11:39:01.078372 2310269 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 11:39:08.383546 2310059 finetune.py:45] layer 24_q initial loss 1.040531242324505e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 11:39:16.275631 2310129 finetune.py:68] layer 25_v @ epoch 3 new loss 1.0164943887502886e-05 old loss 1.0274094165652059e-05 BETTER
I0328 11:39:17.878415 2310199 finetune.py:68] layer 26_v @ epoch 1 new loss 1.5580011677229777e-05 old loss 1.7010688679874875e-05 BETTER
I0328 11:39:19.099200 2310269 finetune.py:45] layer 27_v initial loss 4.942485975334421e-05
W0328 11:39:19.099591 2310269 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 11:39:43.689814 2310059 finetune.py:68] layer 24_q @ epoch 0 new loss 1.0136192031495739e-05 old loss 1.040531242324505e-05 BETTER
I0328 11:39:51.596096 2310129 finetune.py:68] layer 25_v @ epoch 4 new loss 9.866004802461248e-06 old loss 1.0164943887502886e-05 BETTER
I0328 11:39:52.291270 2310269 finetune.py:68] layer 27_v @ epoch 0 new loss 1.562462784931995e-05 old loss 4.942485975334421e-05 BETTER
I0328 11:39:53.308929 2310199 finetune.py:68] layer 26_v @ epoch 2 new loss 1.5019165402918588e-05 old loss 1.5580011677229777e-05 BETTER
I0328 11:40:11.512974 2310129 finetune.py:45] layer 25_q initial loss 1.383711423841305e-05
I0328 11:40:19.963460 2310059 finetune.py:68] layer 24_q @ epoch 1 new loss 9.962664080376271e-06 old loss 1.0136192031495739e-05 BETTER
I0328 11:40:26.692612 2310269 finetune.py:68] layer 27_v @ epoch 1 new loss 1.4492796253762208e-05 old loss 1.562462784931995e-05 BETTER
I0328 11:40:28.299891 2310199 finetune.py:68] layer 26_v @ epoch 3 new loss 1.4569923223461956e-05 old loss 1.5019165402918588e-05 BETTER
I0328 11:40:44.870573 2310129 finetune.py:68] layer 25_q @ epoch 0 new loss 1.3254896657599602e-05 old loss 1.383711423841305e-05 BETTER
I0328 11:40:56.725783 2310059 finetune.py:68] layer 24_q @ epoch 2 new loss 9.852587027125992e-06 old loss 9.962664080376271e-06 BETTER
I0328 11:41:01.286950 2310269 finetune.py:68] layer 27_v @ epoch 2 new loss 1.4032020771992393e-05 old loss 1.4492796253762208e-05 BETTER
I0328 11:41:03.411654 2310199 finetune.py:68] layer 26_v @ epoch 4 new loss 1.437763603462372e-05 old loss 1.4569923223461956e-05 BETTER
I0328 11:41:19.248082 2310129 finetune.py:68] layer 25_q @ epoch 1 new loss 1.2910336408822332e-05 old loss 1.3254896657599602e-05 BETTER
I0328 11:41:23.179007 2310199 finetune.py:45] layer 26_q initial loss 1.756484925863333e-05
I0328 11:41:33.362175 2310059 finetune.py:68] layer 24_q @ epoch 3 new loss 9.764871720108204e-06 old loss 9.852587027125992e-06 BETTER
I0328 11:41:35.815303 2310269 finetune.py:68] layer 27_v @ epoch 3 new loss 1.3796574421576224e-05 old loss 1.4032020771992393e-05 BETTER
I0328 11:41:53.816054 2310129 finetune.py:68] layer 25_q @ epoch 2 new loss 1.2690297808148898e-05 old loss 1.2910336408822332e-05 BETTER
I0328 11:41:56.780551 2310199 finetune.py:68] layer 26_q @ epoch 0 new loss 1.7051932445610873e-05 old loss 1.756484925863333e-05 BETTER
I0328 11:42:10.306158 2310059 finetune.py:68] layer 24_q @ epoch 4 new loss 9.693490028439555e-06 old loss 9.764871720108204e-06 BETTER
I0328 11:42:10.518410 2310269 finetune.py:68] layer 27_v @ epoch 4 new loss 1.3516516446543392e-05 old loss 1.3796574421576224e-05 BETTER
I0328 11:42:28.607139 2310059 finetune.py:45] layer 24_k initial loss 1.0841757102753036e-05
I0328 11:42:28.653030 2310129 finetune.py:68] layer 25_q @ epoch 3 new loss 1.2510672604548745e-05 old loss 1.2690297808148898e-05 BETTER
I0328 11:42:30.870549 2310269 finetune.py:45] layer 27_q initial loss 1.850893022492528e-05
I0328 11:42:31.517487 2310199 finetune.py:68] layer 26_q @ epoch 1 new loss 1.675416024227161e-05 old loss 1.7051932445610873e-05 BETTER
I0328 11:43:03.614773 2310129 finetune.py:68] layer 25_q @ epoch 4 new loss 1.2357596460788045e-05 old loss 1.2510672604548745e-05 BETTER
I0328 11:43:03.952966 2310059 finetune.py:68] layer 24_k @ epoch 0 new loss 1.073630937753478e-05 old loss 1.0841757102753036e-05 BETTER
I0328 11:43:04.221681 2310269 finetune.py:68] layer 27_q @ epoch 0 new loss 1.806379805202596e-05 old loss 1.850893022492528e-05 BETTER
I0328 11:43:06.391470 2310199 finetune.py:68] layer 26_q @ epoch 2 new loss 1.6537662304472178e-05 old loss 1.675416024227161e-05 BETTER
I0328 11:43:21.855145 2310129 finetune.py:45] layer 25_k initial loss 1.43258648677147e-05
I0328 11:43:38.559317 2310269 finetune.py:68] layer 27_q @ epoch 1 new loss 1.7722653865348548e-05 old loss 1.806379805202596e-05 BETTER
I0328 11:43:40.306932 2310059 finetune.py:68] layer 24_k @ epoch 1 new loss 1.0666610251064412e-05 old loss 1.073630937753478e-05 BETTER
I0328 11:43:41.178089 2310199 finetune.py:68] layer 26_q @ epoch 3 new loss 1.6349635188817047e-05 old loss 1.6537662304472178e-05 BETTER
I0328 11:43:55.140726 2310129 finetune.py:68] layer 25_k @ epoch 0 new loss 1.4115130397840403e-05 old loss 1.43258648677147e-05 BETTER
I0328 11:44:12.749029 2310269 finetune.py:68] layer 27_q @ epoch 2 new loss 1.753690048644785e-05 old loss 1.7722653865348548e-05 BETTER
I0328 11:44:16.009399 2310199 finetune.py:68] layer 26_q @ epoch 4 new loss 1.6232606867561117e-05 old loss 1.6349635188817047e-05 BETTER
I0328 11:44:16.872295 2310059 finetune.py:68] layer 24_k @ epoch 2 new loss 1.062463070411468e-05 old loss 1.0666610251064412e-05 BETTER
I0328 11:44:29.314445 2310129 finetune.py:68] layer 25_k @ epoch 1 new loss 1.400271230522776e-05 old loss 1.4115130397840403e-05 BETTER
I0328 11:44:34.065902 2310199 finetune.py:45] layer 26_k initial loss 1.7648588254814968e-05
I0328 11:44:47.018990 2310269 finetune.py:68] layer 27_q @ epoch 3 new loss 1.739116123644635e-05 old loss 1.753690048644785e-05 BETTER
I0328 11:44:53.640121 2310059 finetune.py:68] layer 24_k @ epoch 3 new loss 1.0587985343590844e-05 old loss 1.062463070411468e-05 BETTER
I0328 11:45:03.695412 2310129 finetune.py:68] layer 25_k @ epoch 2 new loss 1.3911070709582418e-05 old loss 1.400271230522776e-05 BETTER
I0328 11:45:07.808808 2310199 finetune.py:68] layer 26_k @ epoch 0 new loss 1.7405434846295975e-05 old loss 1.7648588254814968e-05 BETTER
I0328 11:45:21.222223 2310269 finetune.py:68] layer 27_q @ epoch 4 new loss 1.7285770809394307e-05 old loss 1.739116123644635e-05 BETTER
I0328 11:45:30.584329 2310059 finetune.py:68] layer 24_k @ epoch 4 new loss 1.0540338735154364e-05 old loss 1.0587985343590844e-05 BETTER
I0328 11:45:38.095260 2310129 finetune.py:68] layer 25_k @ epoch 3 new loss 1.3836142898071557e-05 old loss 1.3911070709582418e-05 BETTER
I0328 11:45:39.373086 2310269 finetune.py:45] layer 27_k initial loss 2.019467319769319e-05
I0328 11:45:42.245951 2310199 finetune.py:68] layer 26_k @ epoch 1 new loss 1.730620533635374e-05 old loss 1.7405434846295975e-05 BETTER
I0328 11:45:50.604239 2310059 finetune.py:45] layer 24_o initial loss 2.5960187485907227e-05
I0328 11:46:12.193707 2310269 finetune.py:68] layer 27_k @ epoch 0 new loss 1.987206815101672e-05 old loss 2.019467319769319e-05 BETTER
I0328 11:46:12.654222 2310129 finetune.py:68] layer 25_k @ epoch 4 new loss 1.3785605005978141e-05 old loss 1.3836142898071557e-05 BETTER
I0328 11:46:16.815324 2310199 finetune.py:68] layer 26_k @ epoch 2 new loss 1.724386675050482e-05 old loss 1.730620533635374e-05 BETTER
I0328 11:46:25.196851 2310059 finetune.py:68] layer 24_o @ epoch 0 new loss 2.520028647268191e-05 old loss 2.5960187485907227e-05 BETTER
I0328 11:46:32.614169 2310129 finetune.py:45] layer 25_o initial loss 3.132892743451521e-05
I0328 11:46:46.039010 2310269 finetune.py:68] layer 27_k @ epoch 1 new loss 1.9680126570165157e-05 old loss 1.987206815101672e-05 BETTER
I0328 11:46:51.339497 2310199 finetune.py:68] layer 26_k @ epoch 3 new loss 1.7157301044790074e-05 old loss 1.724386675050482e-05 BETTER
I0328 11:47:01.010806 2310059 finetune.py:68] layer 24_o @ epoch 1 new loss 2.4961465896922164e-05 old loss 2.520028647268191e-05 BETTER
I0328 11:47:05.251138 2310129 finetune.py:68] layer 25_o @ epoch 0 new loss 3.0267419788287953e-05 old loss 3.132892743451521e-05 BETTER
I0328 11:47:19.874628 2310269 finetune.py:68] layer 27_k @ epoch 2 new loss 1.9586716007324867e-05 old loss 1.9680126570165157e-05 BETTER
I0328 11:47:25.906547 2310199 finetune.py:68] layer 26_k @ epoch 4 new loss 1.7100563127314672e-05 old loss 1.7157301044790074e-05 BETTER
I0328 11:47:37.024682 2310059 finetune.py:68] layer 24_o @ epoch 2 new loss 2.4802420739433728e-05 old loss 2.4961465896922164e-05 BETTER
I0328 11:47:38.792063 2310129 finetune.py:68] layer 25_o @ epoch 1 new loss 2.9903931135777384e-05 old loss 3.0267419788287953e-05 BETTER
I0328 11:47:45.749405 2310199 finetune.py:45] layer 26_o initial loss 4.150386666879058e-05
I0328 11:47:53.999932 2310269 finetune.py:68] layer 27_k @ epoch 3 new loss 1.9530547433532774e-05 old loss 1.9586716007324867e-05 BETTER
I0328 11:48:12.694425 2310129 finetune.py:68] layer 25_o @ epoch 2 new loss 2.9649077987414785e-05 old loss 2.9903931135777384e-05 BETTER
I0328 11:48:12.816815 2310059 finetune.py:68] layer 24_o @ epoch 3 new loss 2.466913610987831e-05 old loss 2.4802420739433728e-05 BETTER
I0328 11:48:18.607733 2310199 finetune.py:68] layer 26_o @ epoch 0 new loss 4.0045018977252766e-05 old loss 4.150386666879058e-05 BETTER
I0328 11:48:28.132128 2310269 finetune.py:68] layer 27_k @ epoch 4 new loss 1.9486195014906116e-05 old loss 1.9530547433532774e-05 BETTER
I0328 11:48:46.534111 2310129 finetune.py:68] layer 25_o @ epoch 3 new loss 2.946230051747989e-05 old loss 2.9649077987414785e-05 BETTER
I0328 11:48:48.046237 2310269 finetune.py:45] layer 27_o initial loss 4.8553672968409956e-05
I0328 11:48:48.805957 2310059 finetune.py:68] layer 24_o @ epoch 4 new loss 2.457431037328206e-05 old loss 2.466913610987831e-05 BETTER
I0328 11:48:52.589725 2310199 finetune.py:68] layer 26_o @ epoch 1 new loss 3.950581594835967e-05 old loss 4.0045018977252766e-05 BETTER
I0328 11:49:20.424104 2310059 finetune.py:45] layer 24_up initial loss 7.969202852109447e-05
I0328 11:49:20.842421 2310269 finetune.py:68] layer 27_o @ epoch 0 new loss 4.710918801720254e-05 old loss 4.8553672968409956e-05 BETTER
I0328 11:49:21.284509 2310129 finetune.py:68] layer 25_o @ epoch 4 new loss 2.932684583356604e-05 old loss 2.946230051747989e-05 BETTER
I0328 11:49:26.592694 2310199 finetune.py:68] layer 26_o @ epoch 2 new loss 3.915289926226251e-05 old loss 3.950581594835967e-05 BETTER
I0328 11:49:52.477949 2310059 finetune.py:68] layer 24_up @ epoch 0 new loss 7.8569159086328e-05 old loss 7.969202852109447e-05 BETTER
I0328 11:49:53.291452 2310129 finetune.py:45] layer 25_up initial loss 9.127477824222296e-05
I0328 11:49:54.307909 2310269 finetune.py:68] layer 27_o @ epoch 1 new loss 4.647338209906593e-05 old loss 4.710918801720254e-05 BETTER
I0328 11:50:00.655429 2310199 finetune.py:68] layer 26_o @ epoch 3 new loss 3.886502963723615e-05 old loss 3.915289926226251e-05 BETTER
I0328 11:50:23.912688 2310129 finetune.py:68] layer 25_up @ epoch 0 new loss 9.001880971482024e-05 old loss 9.127477824222296e-05 BETTER
I0328 11:50:25.971097 2310059 finetune.py:68] layer 24_up @ epoch 1 new loss 7.778477447573096e-05 old loss 7.8569159086328e-05 BETTER
I0328 11:50:27.815879 2310269 finetune.py:68] layer 27_o @ epoch 2 new loss 4.6067896619206294e-05 old loss 4.647338209906593e-05 BETTER
I0328 11:50:34.814739 2310199 finetune.py:68] layer 26_o @ epoch 4 new loss 3.864496466121636e-05 old loss 3.886502963723615e-05 BETTER
I0328 11:50:55.777388 2310129 finetune.py:68] layer 25_up @ epoch 1 new loss 8.914696081774309e-05 old loss 9.001880971482024e-05 BETTER
I0328 11:50:59.624273 2310059 finetune.py:68] layer 24_up @ epoch 2 new loss 7.714716048212722e-05 old loss 7.778477447573096e-05 BETTER
I0328 11:51:01.404754 2310269 finetune.py:68] layer 27_o @ epoch 3 new loss 4.572619218379259e-05 old loss 4.6067896619206294e-05 BETTER
I0328 11:51:06.915963 2310199 finetune.py:45] layer 26_up initial loss 0.00011113753134850413
I0328 11:51:27.715190 2310129 finetune.py:68] layer 25_up @ epoch 2 new loss 8.844737749313936e-05 old loss 8.914696081774309e-05 BETTER
I0328 11:51:33.483523 2310059 finetune.py:68] layer 24_up @ epoch 3 new loss 7.659442053409293e-05 old loss 7.714716048212722e-05 BETTER
I0328 11:51:34.972036 2310269 finetune.py:68] layer 27_o @ epoch 4 new loss 4.547725256998092e-05 old loss 4.572619218379259e-05 BETTER
I0328 11:51:37.549297 2310199 finetune.py:68] layer 26_up @ epoch 0 new loss 0.00010964169632643461 old loss 0.00011113753134850413 BETTER
I0328 11:51:59.773793 2310129 finetune.py:68] layer 25_up @ epoch 3 new loss 8.78476639627479e-05 old loss 8.844737749313936e-05 BETTER
I0328 11:52:06.758547 2310269 finetune.py:45] layer 27_up initial loss 0.00013007975940126926
I0328 11:52:07.425588 2310059 finetune.py:68] layer 24_up @ epoch 4 new loss 7.612322951899841e-05 old loss 7.659442053409293e-05 BETTER
I0328 11:52:09.660995 2310199 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0001085822659661062 old loss 0.00010964169632643461 BETTER
I0328 11:52:31.767373 2310129 finetune.py:68] layer 25_up @ epoch 4 new loss 8.732819696888328e-05 old loss 8.78476639627479e-05 BETTER
I0328 11:52:36.991119 2310269 finetune.py:68] layer 27_up @ epoch 0 new loss 0.00012823689030483365 old loss 0.00013007975940126926 BETTER
I0328 11:52:39.038664 2310059 finetune.py:45] layer 24_gate initial loss 9.956768190022558e-05
I0328 11:52:41.954029 2310199 finetune.py:68] layer 26_up @ epoch 2 new loss 0.00010772395762614906 old loss 0.0001085822659661062 BETTER
I0328 11:53:03.374208 2310129 finetune.py:45] layer 25_gate initial loss 0.00011448043369455263
I0328 11:53:08.229402 2310269 finetune.py:68] layer 27_up @ epoch 1 new loss 0.00012692213931586593 old loss 0.00012823689030483365 BETTER
I0328 11:53:09.009588 2310059 finetune.py:68] layer 24_gate @ epoch 0 new loss 9.907317871693522e-05 old loss 9.956768190022558e-05 BETTER
I0328 11:53:14.255930 2310199 finetune.py:68] layer 26_up @ epoch 3 new loss 0.00010699005360947922 old loss 0.00010772395762614906 BETTER
I0328 11:53:32.026831 2310129 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.00011394371540518478 old loss 0.00011448043369455263 BETTER
I0328 11:53:39.615026 2310269 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0001258993288502097 old loss 0.00012692213931586593 BETTER
I0328 11:53:40.288808 2310059 finetune.py:68] layer 24_gate @ epoch 1 new loss 9.865770698525012e-05 old loss 9.907317871693522e-05 BETTER
I0328 11:53:46.515373 2310199 finetune.py:68] layer 26_up @ epoch 4 new loss 0.00010635679063852876 old loss 0.00010699005360947922 BETTER
I0328 11:54:01.561852 2310129 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.00011349919077474624 old loss 0.00011394371540518478 BETTER
I0328 11:54:11.117855 2310269 finetune.py:68] layer 27_up @ epoch 3 new loss 0.00012500376033131033 old loss 0.0001258993288502097 BETTER
I0328 11:54:11.826168 2310059 finetune.py:68] layer 24_gate @ epoch 2 new loss 9.830851922743022e-05 old loss 9.865770698525012e-05 BETTER
I0328 11:54:18.218686 2310199 finetune.py:45] layer 26_gate initial loss 0.00013854927965439856
I0328 11:54:31.243418 2310129 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.00011312039714539424 old loss 0.00011349919077474624 BETTER
I0328 11:54:42.829213 2310269 finetune.py:68] layer 27_up @ epoch 4 new loss 0.00012425065506249666 old loss 0.00012500376033131033 BETTER
I0328 11:54:43.561718 2310059 finetune.py:68] layer 24_gate @ epoch 3 new loss 9.798643441172317e-05 old loss 9.830851922743022e-05 BETTER
I0328 11:54:46.886942 2310199 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.00013790238881483674 old loss 0.00013854927965439856 BETTER
I0328 11:55:00.929038 2310129 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.00011278734018560499 old loss 0.00011312039714539424 BETTER
I0328 11:55:15.158504 2310269 finetune.py:45] layer 27_gate initial loss 0.0001641719281906262
I0328 11:55:15.244683 2310059 finetune.py:68] layer 24_gate @ epoch 4 new loss 9.771486656973138e-05 old loss 9.798643441172317e-05 BETTER
I0328 11:55:16.798489 2310199 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.00013737878180108964 old loss 0.00013790238881483674 BETTER
I0328 11:55:30.732201 2310129 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.00011249553790548816 old loss 0.00011278734018560499 BETTER
I0328 11:55:43.414159 2310269 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.00016337475972250104 old loss 0.0001641719281906262 BETTER
I0328 11:55:46.501540 2310199 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0001369302481180057 old loss 0.00013737878180108964 BETTER
I0328 11:56:11.494083 2310059 finetune.py:45] layer 24_down initial loss 0.00017133241635747254
I0328 11:56:13.035733 2310269 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.00016270886408165097 old loss 0.00016337475972250104 BETTER
I0328 11:56:16.537506 2310199 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.00013652240158990026 old loss 0.0001369302481180057 BETTER
I0328 11:56:27.728039 2310129 finetune.py:45] layer 25_down initial loss 0.00019191758474335074
I0328 11:56:38.820943 2310059 finetune.py:68] layer 24_down @ epoch 0 new loss 0.00017132973880507052 old loss 0.00017133241635747254 BETTER
I0328 11:56:42.287378 2310269 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.00016215848154388368 old loss 0.00016270886408165097 BETTER
I0328 11:56:46.524893 2310199 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.00013617784134112298 old loss 0.00013652240158990026 BETTER
I0328 11:56:53.863674 2310129 finetune.py:68] layer 25_down @ epoch 0 new loss 0.00019191308820154518 old loss 0.00019191758474335074 BETTER
I0328 11:57:07.214881 2310059 finetune.py:68] layer 24_down @ epoch 1 new loss 0.00017132805078290403 old loss 0.00017132973880507052 BETTER
I0328 11:57:11.512240 2310269 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.00016166552086360753 old loss 0.00016215848154388368 BETTER
I0328 11:57:20.999133 2310129 finetune.py:68] layer 25_down @ epoch 1 new loss 0.00019191019237041473 old loss 0.00019191308820154518 BETTER
I0328 11:57:36.146604 2310059 finetune.py:68] layer 24_down @ epoch 2 new loss 0.00017132663924712688 old loss 0.00017132805078290403 BETTER
I0328 11:57:40.860975 2310269 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.00016123979003168643 old loss 0.00016166552086360753 BETTER
I0328 11:57:44.379991 2310199 finetune.py:45] layer 26_down initial loss 0.00022745691239833832
I0328 11:57:48.421078 2310129 finetune.py:68] layer 25_down @ epoch 2 new loss 0.00019190757302567363 old loss 0.00019191019237041473 BETTER
I0328 11:58:05.171673 2310059 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0001713251695036888 old loss 0.00017132663924712688 BETTER
I0328 11:58:10.649709 2310199 finetune.py:68] layer 26_down @ epoch 0 new loss 0.00022745475871488452 old loss 0.00022745691239833832 BETTER
I0328 11:58:15.954328 2310129 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0001919059141073376 old loss 0.00019190757302567363 BETTER
I0328 11:58:34.275886 2310059 finetune.py:68] layer 24_down @ epoch 4 new loss 0.000171324543771334 old loss 0.0001713251695036888 BETTER
24_v proxy err 0.007107988931238651 tr(WHW.T) 467.2783508300781
bpp_loss 3.5475641596131027
24_q proxy err 0.0006173966103233397 tr(WHW.T) 22448.5234375
bpp_loss 4.0416961255832575
24_k proxy err 0.0002657524892129004 tr(WHW.T) 14178.0908203125
bpp_loss 4.829096959438175
24_o proxy err 0.008475069887936115 tr(WHW.T) 1592.1722412109375
bpp_loss 3.471291476627812
24_up proxy err 0.00889709871262312 tr(WHW.T) 7312.2392578125
bpp_loss 3.4651124285800114
24_gate proxy err 0.0025893463753163815 tr(WHW.T) 25858.779296875
bpp_loss 3.8549253934595202
24_down proxy err 0.008927338756620884 tr(WHW.T) 6759.64404296875
bpp_loss 3.462360399708684
I0328 11:58:37.987956 2310199 finetune.py:68] layer 26_down @ epoch 1 new loss 0.00022745355090592057 old loss 0.00022745475871488452 BETTER
I0328 11:58:38.947177 2310269 finetune.py:45] layer 27_down initial loss 0.00026783905923366547
I0328 11:58:44.637542 2310129 finetune.py:68] layer 25_down @ epoch 4 new loss 0.00019190409511793405 old loss 0.0001919059141073376 BETTER
25_v proxy err 0.00611440185457468 tr(WHW.T) 557.8086547851562
bpp_loss 3.558321427088231
25_q proxy err 0.0005418133223429322 tr(WHW.T) 26124.314453125
bpp_loss 4.020327643549535
25_k proxy err 0.0002653711417224258 tr(WHW.T) 14408.5146484375
bpp_loss 4.811356370686553
25_o proxy err 0.00709132244810462 tr(WHW.T) 1991.2464599609375
bpp_loss 3.470371162344236
25_up proxy err 0.008799049071967602 tr(WHW.T) 7387.19921875
bpp_loss 3.4745171196492657
25_gate proxy err 0.0025437406729906797 tr(WHW.T) 26295.818359375
bpp_loss 3.864165743280734
25_down proxy err 0.008885565213859081 tr(WHW.T) 6635.5341796875
bpp_loss 3.471386946571459
I0328 11:59:05.508839 2310269 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0002678345190361142 old loss 0.00026783905923366547 BETTER
I0328 11:59:06.410715 2310199 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0002274519792990759 old loss 0.00022745355090592057 BETTER
I0328 11:59:32.520853 2310269 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0002678309101611376 old loss 0.0002678345190361142 BETTER
I0328 11:59:34.222429 2310199 finetune.py:68] layer 26_down @ epoch 3 new loss 0.00022745045134797692 old loss 0.0002274519792990759 BETTER
I0328 11:59:56.524968 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 66.68993878364563s
I0328 11:59:59.778017 2310269 finetune.py:68] layer 27_down @ epoch 2 new loss 0.00026782782515510917 old loss 0.0002678309101611376 BETTER
I0328 12:00:00.602650 2310339 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:00:00.602759 2310339 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:00:00.602800 2310339 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:00:01.001679 2310339 config.py:54] PyTorch version 2.6.0 available.
W0328 12:00:01.222259 2310339 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:00:01.848322 2310339 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:00:01.852549 2300848 quantize_finetune_llama.py:209] layer 29 gpu 1
I0328 12:00:01.866437 2310339 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 12:00:02.058092 2310199 finetune.py:68] layer 26_down @ epoch 4 new loss 0.00022744893794879317 old loss 0.00022745045134797692 BETTER
26_v proxy err 0.007466109469532967 tr(WHW.T) 434.54583740234375
bpp_loss 3.6088974172598682
26_q proxy err 0.0006290617166087031 tr(WHW.T) 21408.474609375
bpp_loss 4.030237838800531
26_k proxy err 0.00024052834487520158 tr(WHW.T) 15419.806640625
bpp_loss 4.892095794668421
26_o proxy err 0.0051094843074679375 tr(WHW.T) 2388.19873046875
bpp_loss 3.488337653572671
26_up proxy err 0.008500325493514538 tr(WHW.T) 7650.94140625
bpp_loss 3.48386806894892
26_gate proxy err 0.0023158499971032143 tr(WHW.T) 28900.125
bpp_loss 3.8703185334535584
26_down proxy err 0.008887221105396748 tr(WHW.T) 6639.2177734375
bpp_loss 3.4792277327173258
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:00:18.775524 2310339 finetune.py:45] layer 28_v initial loss 5.8286368584958836e-05
W0328 12:00:18.775775 2310339 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 12:00:27.198354 2310269 finetune.py:68] layer 27_down @ epoch 3 new loss 0.00026782508939504623 old loss 0.00026782782515510917 BETTER
I0328 12:00:53.902431 2310339 finetune.py:68] layer 28_v @ epoch 0 new loss 2.1367384761106223e-05 old loss 5.8286368584958836e-05 BETTER
I0328 12:00:54.601136 2310269 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0002678229648154229 old loss 0.00026782508939504623 BETTER
27_v proxy err 0.005141167435795069 tr(WHW.T) 677.69384765625
bpp_loss 3.701207883830648
27_q proxy err 0.0006701301317662001 tr(WHW.T) 21311.203125
bpp_loss 3.9995050397119485
27_k proxy err 0.0002784967655315995 tr(WHW.T) 14007.7353515625
bpp_loss 4.845462889759801
27_o proxy err 0.006081471219658852 tr(WHW.T) 2163.099853515625
bpp_loss 3.5291927724611014
27_up proxy err 0.007739932741969824 tr(WHW.T) 8483.9931640625
bpp_loss 3.499507107904979
27_gate proxy err 0.002058371901512146 tr(WHW.T) 32815.01953125
bpp_loss 3.8808499701720263
27_down proxy err 0.00738113047555089 tr(WHW.T) 6566.458984375
bpp_loss 3.49039304514216
I0328 12:01:10.592421 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 63.85538172721863s
I0328 12:01:14.367954 2310409 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:01:14.368039 2310409 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:01:14.368076 2310409 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:01:14.717055 2310409 config.py:54] PyTorch version 2.6.0 available.
W0328 12:01:14.921183 2310409 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:01:15.520749 2310409 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:01:15.524494 2300848 quantize_finetune_llama.py:209] layer 30 gpu 2
I0328 12:01:15.539580 2310409 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:01:30.551272 2310339 finetune.py:68] layer 28_v @ epoch 1 new loss 2.0198322090436704e-05 old loss 2.1367384761106223e-05 BETTER
I0328 12:01:32.708399 2310409 finetune.py:45] layer 29_v initial loss 6.901028973516077e-05
W0328 12:01:32.708595 2310409 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 12:02:05.945586 2310409 finetune.py:68] layer 29_v @ epoch 0 new loss 2.445857535349205e-05 old loss 6.901028973516077e-05 BETTER
I0328 12:02:07.524652 2310339 finetune.py:68] layer 28_v @ epoch 2 new loss 1.9861457985825837e-05 old loss 2.0198322090436704e-05 BETTER
I0328 12:02:20.240158 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 64.25422072410583s
I0328 12:02:24.087715 2310479 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:02:24.087830 2310479 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:02:24.087876 2310479 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:02:24.462105 2310479 config.py:54] PyTorch version 2.6.0 available.
W0328 12:02:24.693983 2310479 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:02:25.447907 2310479 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:02:25.451739 2300848 quantize_finetune_llama.py:209] layer 31 gpu 3
I0328 12:02:25.465441 2310479 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:02:40.414495 2310409 finetune.py:68] layer 29_v @ epoch 1 new loss 2.3476772184949368e-05 old loss 2.445857535349205e-05 BETTER
I0328 12:02:43.433744 2310479 finetune.py:45] layer 30_v initial loss 8.29358832561411e-05
W0328 12:02:43.434169 2310479 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 12:02:44.443603 2310339 finetune.py:68] layer 28_v @ epoch 3 new loss 1.9413515474298038e-05 old loss 1.9861457985825837e-05 BETTER
I0328 12:03:15.165306 2310409 finetune.py:68] layer 29_v @ epoch 2 new loss 2.3156357201514766e-05 old loss 2.3476772184949368e-05 BETTER
I0328 12:03:17.011174 2310479 finetune.py:68] layer 30_v @ epoch 0 new loss 4.1556635551387444e-05 old loss 8.29358832561411e-05 BETTER
I0328 12:03:21.527497 2310339 finetune.py:68] layer 28_v @ epoch 4 new loss 1.9366418200661428e-05 old loss 1.9413515474298038e-05 BETTER
I0328 12:03:29.720321 2300848 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 63.81477451324463s
I0328 12:03:33.455517 2310549 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:03:33.455625 2310549 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:03:33.455666 2310549 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:03:33.845590 2310549 config.py:54] PyTorch version 2.6.0 available.
W0328 12:03:34.067807 2310549 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 12:03:34.978861 2310549 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 12:03:34.996249 2310549 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 12:03:40.738885 2310339 finetune.py:45] layer 28_q initial loss 2.5179691874654964e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 12:03:50.121890 2310409 finetune.py:76] layer 29_v @ epoch 3 new loss 2.3647879061172716e-05 old loss 2.3156357201514766e-05 WORSE
I0328 12:03:51.704297 2310479 finetune.py:68] layer 30_v @ epoch 1 new loss 3.9439961255993694e-05 old loss 4.1556635551387444e-05 BETTER
I0328 12:03:52.503059 2310549 finetune.py:45] layer 31_v initial loss 0.00011886912398040295
W0328 12:03:52.503349 2310549 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 12:04:15.781421 2310339 finetune.py:68] layer 28_q @ epoch 0 new loss 2.4278819182654843e-05 old loss 2.5179691874654964e-05 BETTER
I0328 12:04:24.736612 2310409 finetune.py:68] layer 29_v @ epoch 4 new loss 2.2774986064177938e-05 old loss 2.3156357201514766e-05 BETTER
I0328 12:04:25.726637 2310549 finetune.py:68] layer 31_v @ epoch 0 new loss 7.508115959353745e-05 old loss 0.00011886912398040295 BETTER
I0328 12:04:26.717637 2310479 finetune.py:68] layer 30_v @ epoch 2 new loss 3.905417543137446e-05 old loss 3.9439961255993694e-05 BETTER
I0328 12:04:44.319688 2310409 finetune.py:45] layer 29_q initial loss 3.872033630614169e-05
I0328 12:04:52.113038 2310339 finetune.py:68] layer 28_q @ epoch 1 new loss 2.3842865630285814e-05 old loss 2.4278819182654843e-05 BETTER
I0328 12:05:00.165058 2310549 finetune.py:76] layer 31_v @ epoch 1 new loss 0.00012543678167276084 old loss 7.508115959353745e-05 WORSE
I0328 12:05:01.745150 2310479 finetune.py:76] layer 30_v @ epoch 3 new loss 3.9281545468838885e-05 old loss 3.905417543137446e-05 WORSE
I0328 12:05:17.883980 2310409 finetune.py:68] layer 29_q @ epoch 0 new loss 3.6453122447710484e-05 old loss 3.872033630614169e-05 BETTER
I0328 12:05:28.860828 2310339 finetune.py:68] layer 28_q @ epoch 2 new loss 2.3513463020208292e-05 old loss 2.3842865630285814e-05 BETTER
I0328 12:05:33.951203 2310549 finetune.py:76] layer 31_v @ epoch 2 new loss 0.0001119524022215046 old loss 7.508115959353745e-05 WORSE
I0328 12:05:36.131028 2310479 finetune.py:68] layer 30_v @ epoch 4 new loss 3.7947622331557795e-05 old loss 3.905417543137446e-05 BETTER
I0328 12:05:52.310918 2310409 finetune.py:68] layer 29_q @ epoch 1 new loss 3.526134241838008e-05 old loss 3.6453122447710484e-05 BETTER
I0328 12:05:56.385578 2310479 finetune.py:45] layer 30_q initial loss 4.804005584446713e-05
I0328 12:06:06.046438 2310339 finetune.py:68] layer 28_q @ epoch 3 new loss 2.3241665985551663e-05 old loss 2.3513463020208292e-05 BETTER
I0328 12:06:08.043961 2310549 finetune.py:76] layer 31_v @ epoch 3 new loss 8.84987020981498e-05 old loss 7.508115959353745e-05 WORSE
I0328 12:06:27.291849 2310409 finetune.py:68] layer 29_q @ epoch 2 new loss 3.4536315069999546e-05 old loss 3.526134241838008e-05 BETTER
I0328 12:06:30.137746 2310479 finetune.py:68] layer 30_q @ epoch 0 new loss 4.686140164267272e-05 old loss 4.804005584446713e-05 BETTER
I0328 12:06:42.132880 2310549 finetune.py:76] layer 31_v @ epoch 4 new loss 8.325593080371618e-05 old loss 7.508115959353745e-05 WORSE
I0328 12:06:42.993730 2310339 finetune.py:68] layer 28_q @ epoch 4 new loss 2.3135817173169926e-05 old loss 2.3241665985551663e-05 BETTER
I0328 12:07:00.948633 2310339 finetune.py:45] layer 28_k initial loss 2.561567089287564e-05
I0328 12:07:01.487468 2310549 finetune.py:45] layer 31_q initial loss 0.00010487476538401097
I0328 12:07:02.022093 2310409 finetune.py:68] layer 29_q @ epoch 3 new loss 3.3877440728247166e-05 old loss 3.4536315069999546e-05 BETTER
I0328 12:07:04.862045 2310479 finetune.py:68] layer 30_q @ epoch 1 new loss 4.567061478155665e-05 old loss 4.686140164267272e-05 BETTER
I0328 12:07:34.697633 2310549 finetune.py:76] layer 31_q @ epoch 0 new loss 0.00010832316183950752 old loss 0.00010487476538401097 WORSE
I0328 12:07:36.250560 2310339 finetune.py:68] layer 28_k @ epoch 0 new loss 2.5306258976343088e-05 old loss 2.561567089287564e-05 BETTER
I0328 12:07:36.778773 2310409 finetune.py:68] layer 29_q @ epoch 4 new loss 3.354904765728861e-05 old loss 3.3877440728247166e-05 BETTER
I0328 12:07:39.410748 2310479 finetune.py:68] layer 30_q @ epoch 2 new loss 4.522486051428132e-05 old loss 4.567061478155665e-05 BETTER
I0328 12:07:54.858873 2310409 finetune.py:45] layer 29_k initial loss 3.733858102350496e-05
I0328 12:08:08.114300 2310549 finetune.py:68] layer 31_q @ epoch 1 new loss 9.636666072765365e-05 old loss 0.00010487476538401097 BETTER
I0328 12:08:12.746696 2310339 finetune.py:68] layer 28_k @ epoch 1 new loss 2.5121151338680647e-05 old loss 2.5306258976343088e-05 BETTER
I0328 12:08:14.183697 2310479 finetune.py:68] layer 30_q @ epoch 3 new loss 4.488589911488816e-05 old loss 4.522486051428132e-05 BETTER
I0328 12:08:28.105586 2310409 finetune.py:68] layer 29_k @ epoch 0 new loss 3.671443846542388e-05 old loss 3.733858102350496e-05 BETTER
I0328 12:08:42.422850 2310549 finetune.py:68] layer 31_q @ epoch 2 new loss 9.176039748126641e-05 old loss 9.636666072765365e-05 BETTER
I0328 12:08:48.863864 2310479 finetune.py:68] layer 30_q @ epoch 4 new loss 4.487698242883198e-05 old loss 4.488589911488816e-05 BETTER
I0328 12:08:49.345994 2310339 finetune.py:68] layer 28_k @ epoch 2 new loss 2.5061332053155638e-05 old loss 2.5121151338680647e-05 BETTER
I0328 12:09:02.343215 2310409 finetune.py:68] layer 29_k @ epoch 1 new loss 3.635220855358057e-05 old loss 3.671443846542388e-05 BETTER
I0328 12:09:07.046471 2310479 finetune.py:45] layer 30_k initial loss 5.0832866691052914e-05
I0328 12:09:16.545984 2310549 finetune.py:76] layer 31_q @ epoch 3 new loss 9.362294076709077e-05 old loss 9.176039748126641e-05 WORSE
I0328 12:09:26.121253 2310339 finetune.py:68] layer 28_k @ epoch 3 new loss 2.4873948859749362e-05 old loss 2.5061332053155638e-05 BETTER
I0328 12:09:36.845661 2310409 finetune.py:68] layer 29_k @ epoch 2 new loss 3.6292556615080684e-05 old loss 3.635220855358057e-05 BETTER
I0328 12:09:40.559082 2310479 finetune.py:68] layer 30_k @ epoch 0 new loss 5.0295129767619073e-05 old loss 5.0832866691052914e-05 BETTER
I0328 12:09:50.218635 2310549 finetune.py:76] layer 31_q @ epoch 4 new loss 9.446660988032818e-05 old loss 9.176039748126641e-05 WORSE
I0328 12:10:02.831987 2310339 finetune.py:76] layer 28_k @ epoch 4 new loss 2.4897379262256436e-05 old loss 2.4873948859749362e-05 WORSE
I0328 12:10:08.009470 2310549 finetune.py:45] layer 31_k initial loss 0.0001077578344848007
I0328 12:10:11.283076 2310409 finetune.py:68] layer 29_k @ epoch 3 new loss 3.6109435313846916e-05 old loss 3.6292556615080684e-05 BETTER
I0328 12:10:14.989111 2310479 finetune.py:76] layer 30_k @ epoch 1 new loss 5.094409789307974e-05 old loss 5.0295129767619073e-05 WORSE
I0328 12:10:22.257703 2310339 finetune.py:45] layer 28_o initial loss 6.283613765845075e-05
I0328 12:10:41.011804 2310549 finetune.py:68] layer 31_k @ epoch 0 new loss 9.70060791587457e-05 old loss 0.0001077578344848007 BETTER
I0328 12:10:45.872438 2310409 finetune.py:76] layer 29_k @ epoch 4 new loss 3.623545489972457e-05 old loss 3.6109435313846916e-05 WORSE
I0328 12:10:48.920443 2310479 finetune.py:76] layer 30_k @ epoch 2 new loss 5.154564860276878e-05 old loss 5.0295129767619073e-05 WORSE
I0328 12:10:56.897984 2310339 finetune.py:68] layer 28_o @ epoch 0 new loss 6.0703299823217094e-05 old loss 6.283613765845075e-05 BETTER
I0328 12:11:05.040937 2310409 finetune.py:45] layer 29_o initial loss 7.419980829581618e-05
I0328 12:11:14.846576 2310549 finetune.py:68] layer 31_k @ epoch 1 new loss 9.432846127310768e-05 old loss 9.70060791587457e-05 BETTER
I0328 12:11:22.893483 2310479 finetune.py:68] layer 30_k @ epoch 3 new loss 4.99197376484517e-05 old loss 5.0295129767619073e-05 BETTER
I0328 12:11:32.595424 2310339 finetune.py:68] layer 28_o @ epoch 1 new loss 5.9627287555485964e-05 old loss 6.0703299823217094e-05 BETTER
I0328 12:11:37.754255 2310409 finetune.py:68] layer 29_o @ epoch 0 new loss 7.169861055444926e-05 old loss 7.419980829581618e-05 BETTER
I0328 12:11:49.052446 2310549 finetune.py:68] layer 31_k @ epoch 2 new loss 9.283972758566961e-05 old loss 9.432846127310768e-05 BETTER
I0328 12:11:57.551121 2310479 finetune.py:68] layer 30_k @ epoch 4 new loss 4.990506204194389e-05 old loss 4.99197376484517e-05 BETTER
I0328 12:12:08.488871 2310339 finetune.py:68] layer 28_o @ epoch 2 new loss 5.891300679650158e-05 old loss 5.9627287555485964e-05 BETTER
I0328 12:12:11.496818 2310409 finetune.py:68] layer 29_o @ epoch 1 new loss 7.055961759760976e-05 old loss 7.169861055444926e-05 BETTER
I0328 12:12:17.229137 2310479 finetune.py:45] layer 30_o initial loss 0.0001131315075326711
I0328 12:12:23.123035 2310549 finetune.py:76] layer 31_k @ epoch 3 new loss 9.592749120201916e-05 old loss 9.283972758566961e-05 WORSE
I0328 12:12:44.393470 2310339 finetune.py:68] layer 28_o @ epoch 3 new loss 5.839381628902629e-05 old loss 5.891300679650158e-05 BETTER
I0328 12:12:45.282002 2310409 finetune.py:68] layer 29_o @ epoch 2 new loss 6.982398917898536e-05 old loss 7.055961759760976e-05 BETTER
I0328 12:12:50.336314 2310479 finetune.py:68] layer 30_o @ epoch 0 new loss 0.00011016777716577053 old loss 0.0001131315075326711 BETTER
I0328 12:12:56.656863 2310549 finetune.py:76] layer 31_k @ epoch 4 new loss 9.468598000239581e-05 old loss 9.283972758566961e-05 WORSE
I0328 12:13:15.627417 2310549 finetune.py:45] layer 31_o initial loss 0.0002221240138169378
I0328 12:13:19.340703 2310409 finetune.py:68] layer 29_o @ epoch 3 new loss 6.933130498509854e-05 old loss 6.982398917898536e-05 BETTER
I0328 12:13:20.549990 2310339 finetune.py:68] layer 28_o @ epoch 4 new loss 5.7964687584899366e-05 old loss 5.839381628902629e-05 BETTER
I0328 12:13:24.272046 2310479 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0001082441012840718 old loss 0.00011016777716577053 BETTER
I0328 12:13:47.948847 2310549 finetune.py:68] layer 31_o @ epoch 0 new loss 0.00019766595505643636 old loss 0.0002221240138169378 BETTER
I0328 12:13:52.418210 2310339 finetune.py:45] layer 28_up initial loss 0.00016333306848537177
I0328 12:13:53.537474 2310409 finetune.py:68] layer 29_o @ epoch 4 new loss 6.885542825330049e-05 old loss 6.933130498509854e-05 BETTER
I0328 12:13:58.371773 2310479 finetune.py:68] layer 30_o @ epoch 2 new loss 0.00010725491301855072 old loss 0.0001082441012840718 BETTER
I0328 12:14:21.330174 2310549 finetune.py:68] layer 31_o @ epoch 1 new loss 0.00018877835827879608 old loss 0.00019766595505643636 BETTER
I0328 12:14:24.382755 2310339 finetune.py:68] layer 28_up @ epoch 0 new loss 0.00016073569713626057 old loss 0.00016333306848537177 BETTER
I0328 12:14:25.375551 2310409 finetune.py:45] layer 29_up initial loss 0.00021434373047668487
I0328 12:14:32.404907 2310479 finetune.py:68] layer 30_o @ epoch 3 new loss 0.00010655863297870383 old loss 0.00010725491301855072 BETTER
I0328 12:14:54.954318 2310549 finetune.py:68] layer 31_o @ epoch 2 new loss 0.00018316776549909264 old loss 0.00018877835827879608 BETTER
I0328 12:14:55.930873 2310409 finetune.py:68] layer 29_up @ epoch 0 new loss 0.00021009426563978195 old loss 0.00021434373047668487 BETTER
I0328 12:14:57.843971 2310339 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0001588601735420525 old loss 0.00016073569713626057 BETTER
I0328 12:15:06.516911 2310479 finetune.py:68] layer 30_o @ epoch 4 new loss 0.00010584347182884812 old loss 0.00010655863297870383 BETTER
I0328 12:15:27.656584 2310409 finetune.py:68] layer 29_up @ epoch 1 new loss 0.00020744476933032274 old loss 0.00021009426563978195 BETTER
I0328 12:15:28.665337 2310549 finetune.py:68] layer 31_o @ epoch 3 new loss 0.00018029229249805212 old loss 0.00018316776549909264 BETTER
I0328 12:15:31.214238 2310339 finetune.py:68] layer 28_up @ epoch 2 new loss 0.00015746937424410135 old loss 0.0001588601735420525 BETTER
I0328 12:15:38.302152 2310479 finetune.py:45] layer 30_up initial loss 0.00039255767478607595
I0328 12:15:59.570717 2310409 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0002053307107416913 old loss 0.00020744476933032274 BETTER
I0328 12:16:02.242035 2310549 finetune.py:68] layer 31_o @ epoch 4 new loss 0.00017751415725797415 old loss 0.00018029229249805212 BETTER
I0328 12:16:04.979765 2310339 finetune.py:68] layer 28_up @ epoch 3 new loss 0.00015627729590050876 old loss 0.00015746937424410135 BETTER
I0328 12:16:09.099527 2310479 finetune.py:68] layer 30_up @ epoch 0 new loss 0.000380994810257107 old loss 0.00039255767478607595 BETTER
I0328 12:16:31.818256 2310409 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0002034987701335922 old loss 0.0002053307107416913 BETTER
I0328 12:16:34.467211 2310549 finetune.py:45] layer 31_up initial loss 0.0011451630853116512
I0328 12:16:39.154709 2310339 finetune.py:68] layer 28_up @ epoch 4 new loss 0.0001552667818032205 old loss 0.00015627729590050876 BETTER
I0328 12:16:41.391808 2310479 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0003732234181370586 old loss 0.000380994810257107 BETTER
I0328 12:17:03.895528 2310409 finetune.py:68] layer 29_up @ epoch 4 new loss 0.00020203343592584133 old loss 0.0002034987701335922 BETTER
I0328 12:17:04.780228 2310549 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0010711626382544637 old loss 0.0011451630853116512 BETTER
I0328 12:17:10.818840 2310339 finetune.py:45] layer 28_gate initial loss 0.00020699271408375353
I0328 12:17:13.598149 2310479 finetune.py:68] layer 30_up @ epoch 2 new loss 0.00036717491457238793 old loss 0.0003732234181370586 BETTER
I0328 12:17:34.893156 2310409 finetune.py:45] layer 29_gate initial loss 0.000270980381174013
I0328 12:17:36.427171 2310549 finetune.py:68] layer 31_up @ epoch 1 new loss 0.001025400822982192 old loss 0.0010711626382544637 BETTER
I0328 12:17:40.935473 2310339 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.00020587218750733882 old loss 0.00020699271408375353 BETTER
I0328 12:17:45.877102 2310479 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0003619145427364856 old loss 0.00036717491457238793 BETTER
I0328 12:18:03.398520 2310409 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.00026939870440401137 old loss 0.000270980381174013 BETTER
I0328 12:18:07.989199 2310549 finetune.py:68] layer 31_up @ epoch 2 new loss 0.0009892501402646303 old loss 0.001025400822982192 BETTER
I0328 12:18:12.111481 2310339 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.00020497717196121812 old loss 0.00020587218750733882 BETTER
I0328 12:18:18.140174 2310479 finetune.py:68] layer 30_up @ epoch 4 new loss 0.00035749434027820826 old loss 0.0003619145427364856 BETTER
I0328 12:18:33.098864 2310409 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.00026819281629286706 old loss 0.00026939870440401137 BETTER
I0328 12:18:39.599651 2310549 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0009581756894476712 old loss 0.0009892501402646303 BETTER
I0328 12:18:43.494729 2310339 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.00020416839106474072 old loss 0.00020497717196121812 BETTER
I0328 12:18:50.043534 2310479 finetune.py:45] layer 30_gate initial loss 0.0004572416946757585
I0328 12:19:02.639930 2310409 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.00026713081751950085 old loss 0.00026819281629286706 BETTER
I0328 12:19:11.244643 2310549 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0009303793194703758 old loss 0.0009581756894476712 BETTER
I0328 12:19:15.131126 2310339 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0002035245852312073 old loss 0.00020416839106474072 BETTER
I0328 12:19:18.576066 2310479 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0004528888675849885 old loss 0.0004572416946757585 BETTER
I0328 12:19:32.247429 2310409 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.00026624667225405574 old loss 0.00026713081751950085 BETTER
I0328 12:19:42.879348 2310549 finetune.py:45] layer 31_gate initial loss 0.00111882365308702
I0328 12:19:46.728361 2310339 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.00020290575048420578 old loss 0.0002035245852312073 BETTER
I0328 12:19:48.333119 2310479 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.000449482788098976 old loss 0.0004528888675849885 BETTER
I0328 12:20:01.860644 2310409 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.00026549017638899386 old loss 0.00026624667225405574 BETTER
I0328 12:20:11.062050 2310549 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.0010951088042929769 old loss 0.00111882365308702 BETTER
I0328 12:20:18.368263 2310479 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.000446660298621282 old loss 0.000449482788098976 BETTER
I0328 12:20:40.193411 2310549 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.0010772134410217404 old loss 0.0010951088042929769 BETTER
I0328 12:20:41.811967 2310339 finetune.py:45] layer 28_down initial loss 0.00033852263004519045
I0328 12:20:48.346008 2310479 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.00044410620466805995 old loss 0.000446660298621282 BETTER
I0328 12:20:57.882782 2310409 finetune.py:45] layer 29_down initial loss 0.0004459204210434109
I0328 12:21:09.203548 2310339 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0003385167510714382 old loss 0.00033852263004519045 BETTER
I0328 12:21:09.684773 2310549 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.0010619318345561624 old loss 0.0010772134410217404 BETTER
I0328 12:21:18.238917 2310479 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0004418465541675687 old loss 0.00044410620466805995 BETTER
I0328 12:21:23.967464 2310409 finetune.py:68] layer 29_down @ epoch 0 new loss 0.00044590915786102414 old loss 0.0004459204210434109 BETTER
I0328 12:21:37.488975 2310339 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0003385117743164301 old loss 0.0003385167510714382 BETTER
I0328 12:21:39.175768 2310549 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.0010479566408321261 old loss 0.0010619318345561624 BETTER
I0328 12:21:51.250389 2310409 finetune.py:68] layer 29_down @ epoch 1 new loss 0.00044589920435100794 old loss 0.00044590915786102414 BETTER
I0328 12:22:06.385194 2310339 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0003385072632227093 old loss 0.0003385117743164301 BETTER
I0328 12:22:08.701392 2310549 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.001034867251291871 old loss 0.0010479566408321261 BETTER
I0328 12:22:16.818016 2310479 finetune.py:45] layer 30_down initial loss 0.0007296937401406467
I0328 12:22:19.059410 2310409 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0004458898620214313 old loss 0.00044589920435100794 BETTER
I0328 12:22:35.469887 2310339 finetune.py:68] layer 28_down @ epoch 3 new loss 0.00033850420732051134 old loss 0.0003385072632227093 BETTER
I0328 12:22:43.225628 2310479 finetune.py:68] layer 30_down @ epoch 0 new loss 0.0007296636467799544 old loss 0.0007296937401406467 BETTER
I0328 12:22:46.615099 2310409 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0004458815383259207 old loss 0.0004458898620214313 BETTER
I0328 12:23:04.754379 2310339 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0003385004820302129 old loss 0.00033850420732051134 BETTER
I0328 12:23:06.120861 2310549 finetune.py:45] layer 31_down initial loss 0.0018380371620878577
28_v proxy err 0.005693120416253805 tr(WHW.T) 601.4844360351562
bpp_loss 3.754300523782149
28_q proxy err 0.0006041021551936865 tr(WHW.T) 23170.21484375
bpp_loss 4.009413474937901
28_k proxy err 0.0002512084611225873 tr(WHW.T) 14988.2470703125
bpp_loss 4.8004661137238145
28_o proxy err 0.005292219575494528 tr(WHW.T) 2510.5634765625
bpp_loss 3.5583633441710845
28_up proxy err 0.006242587696760893 tr(WHW.T) 10248.8828125
bpp_loss 3.521950468953167
28_gate proxy err 0.0018268629210069776 tr(WHW.T) 35937.21484375
bpp_loss 3.8667915268535062
28_down proxy err 0.006977829150855541 tr(WHW.T) 7233.1328125
bpp_loss 3.505684108922391
I0328 12:23:10.983989 2310479 finetune.py:68] layer 30_down @ epoch 1 new loss 0.0007296364055946469 old loss 0.0007296636467799544 BETTER
I0328 12:23:14.332279 2310409 finetune.py:68] layer 29_down @ epoch 4 new loss 0.00044587551383301616 old loss 0.0004458815383259207 BETTER
29_v proxy err 0.004350635688751936 tr(WHW.T) 850.4290161132812
bpp_loss 3.8235706510022283
29_q proxy err 0.0007260942365974188 tr(WHW.T) 20658.0546875
bpp_loss 3.9985209394362755
29_k proxy err 0.0002469240571372211 tr(WHW.T) 16355.8505859375
bpp_loss 4.890502401802223
29_o proxy err 0.0033337105996906757 tr(WHW.T) 3102.6953125
bpp_loss 3.5951227981131524
29_up proxy err 0.00484178913757205 tr(WHW.T) 12881.1494140625
bpp_loss 3.555668550942625
29_gate proxy err 0.0016631979960948229 tr(WHW.T) 38368.5390625
bpp_loss 3.857510259641068
29_down proxy err 0.005719497334212065 tr(WHW.T) 7493.8583984375
bpp_loss 3.5204722176006595
I0328 12:23:32.118813 2310549 finetune.py:68] layer 31_down @ epoch 0 new loss 0.0018378488020971417 old loss 0.0018380371620878577 BETTER
I0328 12:23:38.845598 2310479 finetune.py:68] layer 30_down @ epoch 2 new loss 0.000729611492715776 old loss 0.0007296364055946469 BETTER
I0328 12:23:59.132327 2310549 finetune.py:68] layer 31_down @ epoch 1 new loss 0.0018376768566668034 old loss 0.0018378488020971417 BETTER
I0328 12:24:06.623281 2310479 finetune.py:68] layer 30_down @ epoch 3 new loss 0.0007295873947441578 old loss 0.000729611492715776 BETTER
I0328 12:24:26.306259 2310549 finetune.py:68] layer 31_down @ epoch 2 new loss 0.0018375308718532324 old loss 0.0018376768566668034 BETTER
I0328 12:24:34.391396 2310479 finetune.py:68] layer 30_down @ epoch 4 new loss 0.0007295664399862289 old loss 0.0007295873947441578 BETTER
30_v proxy err 0.003889869898557663 tr(WHW.T) 863.060791015625
bpp_loss 4.1092608664766885
30_q proxy err 0.0005535586387850344 tr(WHW.T) 24060.14453125
bpp_loss 3.901653053064365
30_k proxy err 0.0002585012698546052 tr(WHW.T) 14026.236328125
bpp_loss 4.546833271917421
30_o proxy err 0.0023959872778505087 tr(WHW.T) 4875.609375
bpp_loss 3.6821156349033117
30_up proxy err 0.002859693020582199 tr(WHW.T) 21678.369140625
bpp_loss 3.5879927588227605
30_gate proxy err 0.001226047403179109 tr(WHW.T) 51824.18359375
bpp_loss 3.907694314939103
30_down proxy err 0.003275708295404911 tr(WHW.T) 8824.28125
bpp_loss 3.521223397875604
I0328 12:24:53.659998 2310549 finetune.py:68] layer 31_down @ epoch 3 new loss 0.0018374011851847172 old loss 0.0018375308718532324 BETTER
I0328 12:25:21.124680 2310549 finetune.py:68] layer 31_down @ epoch 4 new loss 0.0018372677732259035 old loss 0.0018374011851847172 BETTER
31_v proxy err 0.0019846612121909857 tr(WHW.T) 1808.723876953125
bpp_loss 3.9278728154604323
31_q proxy err 0.00031455577118322253 tr(WHW.T) 46179.32421875
bpp_loss 4.053963833546732
31_k proxy err 0.0001892972068162635 tr(WHW.T) 20466.53515625
bpp_loss 4.782687638828065
31_o proxy err 0.002004997106269002 tr(WHW.T) 2212.88330078125
bpp_loss 3.639283010328654
31_up proxy err 0.0008684162166900933 tr(WHW.T) 68990.9609375
bpp_loss 3.783928903351937
31_gate proxy err 0.00042748020496219397 tr(WHW.T) 143825.5
bpp_loss 4.125780492355781
31_down proxy err 0.0012680977815762162 tr(WHW.T) 9980.0869140625
bpp_loss 3.547735815219182
I0328 12:25:50.100373 2310619 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:25:50.100483 2310619 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:25:50.100527 2310619 utils.py:162] NumExpr defaulting to 16 threads.
I0328 12:25:50.421137 2310619 config.py:54] PyTorch version 2.6.0 available.
W0328 12:25:50.635926 2310619 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 12:25:50.747638 2310619 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.12it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.59it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.86it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.18it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.34it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.49it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.84it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.61it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.28it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.63it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.68it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.71it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.61it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.68it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.73it/s]
I0328 12:25:53.648543 2310619 hfize_llama.py:153] loaded layer 0
I0328 12:25:54.005200 2310619 hfize_llama.py:153] loaded layer 1
I0328 12:25:54.324955 2310619 hfize_llama.py:153] loaded layer 2
I0328 12:25:54.668856 2310619 hfize_llama.py:153] loaded layer 3
I0328 12:25:54.988350 2310619 hfize_llama.py:153] loaded layer 4
I0328 12:25:55.358609 2310619 hfize_llama.py:153] loaded layer 5
I0328 12:25:55.708299 2310619 hfize_llama.py:153] loaded layer 6
I0328 12:25:56.051908 2310619 hfize_llama.py:153] loaded layer 7
I0328 12:25:56.413348 2310619 hfize_llama.py:153] loaded layer 8
I0328 12:25:56.745464 2310619 hfize_llama.py:153] loaded layer 9
I0328 12:25:57.114061 2310619 hfize_llama.py:153] loaded layer 10
I0328 12:25:57.470397 2310619 hfize_llama.py:153] loaded layer 11
I0328 12:25:57.926313 2310619 hfize_llama.py:153] loaded layer 12
I0328 12:25:58.275271 2310619 hfize_llama.py:153] loaded layer 13
I0328 12:25:58.610426 2310619 hfize_llama.py:153] loaded layer 14
I0328 12:25:58.968919 2310619 hfize_llama.py:153] loaded layer 15
I0328 12:25:59.328074 2310619 hfize_llama.py:153] loaded layer 16
I0328 12:25:59.671903 2310619 hfize_llama.py:153] loaded layer 17
I0328 12:25:59.986882 2310619 hfize_llama.py:153] loaded layer 18
I0328 12:26:00.308005 2310619 hfize_llama.py:153] loaded layer 19
I0328 12:26:00.623446 2310619 hfize_llama.py:153] loaded layer 20
I0328 12:26:00.978594 2310619 hfize_llama.py:153] loaded layer 21
I0328 12:26:01.374734 2310619 hfize_llama.py:153] loaded layer 22
I0328 12:26:01.729296 2310619 hfize_llama.py:153] loaded layer 23
I0328 12:26:02.045015 2310619 hfize_llama.py:153] loaded layer 24
I0328 12:26:02.375119 2310619 hfize_llama.py:153] loaded layer 25
I0328 12:26:02.696065 2310619 hfize_llama.py:153] loaded layer 26
I0328 12:26:03.020486 2310619 hfize_llama.py:153] loaded layer 27
I0328 12:26:03.365184 2310619 hfize_llama.py:153] loaded layer 28
I0328 12:26:03.689142 2310619 hfize_llama.py:153] loaded layer 29
I0328 12:26:03.999260 2310619 hfize_llama.py:153] loaded layer 30
I0328 12:26:04.331849 2310619 hfize_llama.py:153] loaded layer 31
I0328 12:26:04.331966 2310619 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.22s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.28s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.25s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.12s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.13s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.15s/it]
I0328 12:26:49.275117 2310619 hfize_llama.py:167] successfully loaded hfized model
I0328 12:26:54.248763 2310837 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 12:26:54.248908 2310837 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 12:26:54.248951 2310837 utils.py:162] NumExpr defaulting to 16 threads.
W0328 12:26:54.608983 2310837 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 12:26:55.012057 2310837 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.06s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.08s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.06s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.07s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.16s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.16s/it]
I0328 12:27:03.257190 2310837 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.648997187614441:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.648997187614441:   1%|          | 1/141 [00:01<04:23,  1.88s/it]avg_loss = 1.9330289959907532:   1%|          | 1/141 [00:03<04:23,  1.88s/it]avg_loss = 1.9330289959907532:   1%|▏         | 2/141 [00:03<03:48,  1.64s/it]avg_loss = 2.055898388226827:   1%|▏         | 2/141 [00:04<03:48,  1.64s/it] avg_loss = 2.055898388226827:   2%|▏         | 3/141 [00:04<03:35,  1.57s/it]avg_loss = 2.002847731113434:   2%|▏         | 3/141 [00:06<03:35,  1.57s/it]avg_loss = 2.002847731113434:   3%|▎         | 4/141 [00:06<03:29,  1.53s/it]avg_loss = 1.9505704641342163:   3%|▎         | 4/141 [00:07<03:29,  1.53s/it]avg_loss = 1.9505704641342163:   4%|▎         | 5/141 [00:07<03:25,  1.51s/it]avg_loss = 1.8528791666030884:   4%|▎         | 5/141 [00:09<03:25,  1.51s/it]avg_loss = 1.8528791666030884:   4%|▍         | 6/141 [00:09<03:22,  1.50s/it]avg_loss = 1.7890739270618983:   4%|▍         | 6/141 [00:10<03:22,  1.50s/it]avg_loss = 1.7890739270618983:   5%|▍         | 7/141 [00:10<03:20,  1.50s/it]avg_loss = 1.7842630594968796:   5%|▍         | 7/141 [00:12<03:20,  1.50s/it]avg_loss = 1.7842630594968796:   6%|▌         | 8/141 [00:12<03:18,  1.49s/it]avg_loss = 1.818241344557868:   6%|▌         | 8/141 [00:13<03:18,  1.49s/it] avg_loss = 1.818241344557868:   6%|▋         | 9/141 [00:13<03:17,  1.49s/it]avg_loss = 1.8211236119270324:   6%|▋         | 9/141 [00:15<03:17,  1.49s/it]avg_loss = 1.8211236119270324:   7%|▋         | 10/141 [00:15<03:15,  1.49s/it]avg_loss = 1.8164072903719815:   7%|▋         | 10/141 [00:16<03:15,  1.49s/it]avg_loss = 1.8164072903719815:   8%|▊         | 11/141 [00:16<03:14,  1.50s/it]avg_loss = 1.8390925129254658:   8%|▊         | 11/141 [00:18<03:14,  1.50s/it]avg_loss = 1.8390925129254658:   9%|▊         | 12/141 [00:18<03:13,  1.50s/it]avg_loss = 1.850795003084036:   9%|▊         | 12/141 [00:19<03:13,  1.50s/it] avg_loss = 1.850795003084036:   9%|▉         | 13/141 [00:19<03:12,  1.50s/it]avg_loss = 1.868751585483551:   9%|▉         | 13/141 [00:21<03:12,  1.50s/it]avg_loss = 1.868751585483551:  10%|▉         | 14/141 [00:21<03:10,  1.50s/it]avg_loss = 1.8789257605870564:  10%|▉         | 14/141 [00:22<03:10,  1.50s/it]avg_loss = 1.8789257605870564:  11%|█         | 15/141 [00:22<03:09,  1.51s/it]avg_loss = 1.9027129784226418:  11%|█         | 15/141 [00:24<03:09,  1.51s/it]avg_loss = 1.9027129784226418:  11%|█▏        | 16/141 [00:24<03:08,  1.51s/it]avg_loss = 1.9057778190164005:  11%|█▏        | 16/141 [00:25<03:08,  1.51s/it]avg_loss = 1.9057778190164005:  12%|█▏        | 17/141 [00:25<03:07,  1.51s/it]avg_loss = 1.9073759979671903:  12%|█▏        | 17/141 [00:27<03:07,  1.51s/it]avg_loss = 1.9073759979671903:  13%|█▎        | 18/141 [00:27<03:06,  1.51s/it]avg_loss = 1.8918365302838778:  13%|█▎        | 18/141 [00:28<03:06,  1.51s/it]avg_loss = 1.8918365302838778:  13%|█▎        | 19/141 [00:28<03:05,  1.52s/it]avg_loss = 1.8906574308872224:  13%|█▎        | 19/141 [00:30<03:05,  1.52s/it]avg_loss = 1.8906574308872224:  14%|█▍        | 20/141 [00:30<03:03,  1.52s/it]avg_loss = 1.8952518417721702:  14%|█▍        | 20/141 [00:31<03:03,  1.52s/it]avg_loss = 1.8952518417721702:  15%|█▍        | 21/141 [00:31<03:02,  1.52s/it]avg_loss = 1.8973442857915706:  15%|█▍        | 21/141 [00:33<03:02,  1.52s/it]avg_loss = 1.8973442857915706:  16%|█▌        | 22/141 [00:33<03:01,  1.52s/it]avg_loss = 1.8988079713738484:  16%|█▌        | 22/141 [00:34<03:01,  1.52s/it]avg_loss = 1.8988079713738484:  16%|█▋        | 23/141 [00:34<02:59,  1.53s/it]avg_loss = 1.90366397301356:  16%|█▋        | 23/141 [00:36<02:59,  1.53s/it]  avg_loss = 1.90366397301356:  17%|█▋        | 24/141 [00:36<02:58,  1.53s/it]avg_loss = 1.909360752105713:  17%|█▋        | 24/141 [00:38<02:58,  1.53s/it]avg_loss = 1.909360752105713:  18%|█▊        | 25/141 [00:38<02:57,  1.53s/it]avg_loss = 1.9206723066476674:  18%|█▊        | 25/141 [00:39<02:57,  1.53s/it]avg_loss = 1.9206723066476674:  18%|█▊        | 26/141 [00:39<02:56,  1.53s/it]avg_loss = 1.9330704035582367:  18%|█▊        | 26/141 [00:41<02:56,  1.53s/it]avg_loss = 1.9330704035582367:  19%|█▉        | 27/141 [00:41<02:55,  1.54s/it]avg_loss = 1.939138148512159:  19%|█▉        | 27/141 [00:42<02:55,  1.54s/it] avg_loss = 1.939138148512159:  20%|█▉        | 28/141 [00:42<02:53,  1.54s/it]avg_loss = 1.9356736725774304:  20%|█▉        | 28/141 [00:44<02:53,  1.54s/it]avg_loss = 1.9356736725774304:  21%|██        | 29/141 [00:44<02:52,  1.54s/it]avg_loss = 1.9261527101198832:  21%|██        | 29/141 [00:45<02:52,  1.54s/it]avg_loss = 1.9261527101198832:  21%|██▏       | 30/141 [00:45<02:50,  1.54s/it]avg_loss = 1.9125164401146673:  21%|██▏       | 30/141 [00:47<02:50,  1.54s/it]avg_loss = 1.9125164401146673:  22%|██▏       | 31/141 [00:47<02:49,  1.54s/it]avg_loss = 1.9008474126458168:  22%|██▏       | 31/141 [00:48<02:49,  1.54s/it]avg_loss = 1.9008474126458168:  23%|██▎       | 32/141 [00:48<02:48,  1.55s/it]avg_loss = 1.8991052707036336:  23%|██▎       | 32/141 [00:50<02:48,  1.55s/it]avg_loss = 1.8991052707036336:  23%|██▎       | 33/141 [00:50<02:47,  1.55s/it]avg_loss = 1.8975595165701473:  23%|██▎       | 33/141 [00:51<02:47,  1.55s/it]avg_loss = 1.8975595165701473:  24%|██▍       | 34/141 [00:51<02:45,  1.55s/it]avg_loss = 1.900041811806815:  24%|██▍       | 34/141 [00:53<02:45,  1.55s/it] avg_loss = 1.900041811806815:  25%|██▍       | 35/141 [00:53<02:44,  1.55s/it]avg_loss = 1.8837594985961914:  25%|██▍       | 35/141 [00:55<02:44,  1.55s/it]avg_loss = 1.8837594985961914:  26%|██▌       | 36/141 [00:55<02:42,  1.55s/it]avg_loss = 1.8682122101654877:  26%|██▌       | 36/141 [00:56<02:42,  1.55s/it]avg_loss = 1.8682122101654877:  26%|██▌       | 37/141 [00:56<02:41,  1.55s/it]avg_loss = 1.853434797964598:  26%|██▌       | 37/141 [00:58<02:41,  1.55s/it] avg_loss = 1.853434797964598:  27%|██▋       | 38/141 [00:58<02:40,  1.55s/it]avg_loss = 1.839018974548731:  27%|██▋       | 38/141 [00:59<02:40,  1.55s/it]avg_loss = 1.839018974548731:  28%|██▊       | 39/141 [00:59<02:38,  1.56s/it]avg_loss = 1.8305286794900895:  28%|██▊       | 39/141 [01:01<02:38,  1.56s/it]avg_loss = 1.8305286794900895:  28%|██▊       | 40/141 [01:01<02:37,  1.56s/it]avg_loss = 1.8358472411225482:  28%|██▊       | 40/141 [01:02<02:37,  1.56s/it]avg_loss = 1.8358472411225482:  29%|██▉       | 41/141 [01:02<02:36,  1.56s/it]avg_loss = 1.8526782450221835:  29%|██▉       | 41/141 [01:04<02:36,  1.56s/it]avg_loss = 1.8526782450221835:  30%|██▉       | 42/141 [01:04<02:34,  1.56s/it]avg_loss = 1.8690691909124686:  30%|██▉       | 42/141 [01:05<02:34,  1.56s/it]avg_loss = 1.8690691909124686:  30%|███       | 43/141 [01:05<02:33,  1.56s/it]avg_loss = 1.8726279654286124:  30%|███       | 43/141 [01:07<02:33,  1.56s/it]avg_loss = 1.8726279654286124:  31%|███       | 44/141 [01:07<02:31,  1.56s/it]avg_loss = 1.8766484869851006:  31%|███       | 44/141 [01:09<02:31,  1.56s/it]avg_loss = 1.8766484869851006:  32%|███▏      | 45/141 [01:09<02:29,  1.56s/it]avg_loss = 1.8818018255026445:  32%|███▏      | 45/141 [01:10<02:29,  1.56s/it]avg_loss = 1.8818018255026445:  33%|███▎      | 46/141 [01:10<02:28,  1.56s/it]avg_loss = 1.8881146527351218:  33%|███▎      | 46/141 [01:12<02:28,  1.56s/it]avg_loss = 1.8881146527351218:  33%|███▎      | 47/141 [01:12<02:26,  1.56s/it]avg_loss = 1.8911464139819145:  33%|███▎      | 47/141 [01:13<02:26,  1.56s/it]avg_loss = 1.8911464139819145:  34%|███▍      | 48/141 [01:13<02:25,  1.56s/it]avg_loss = 1.8900614344343847:  34%|███▍      | 48/141 [01:15<02:25,  1.56s/it]avg_loss = 1.8900614344343847:  35%|███▍      | 49/141 [01:15<02:23,  1.56s/it]avg_loss = 1.889669461250305:  35%|███▍      | 49/141 [01:16<02:23,  1.56s/it] avg_loss = 1.889669461250305:  35%|███▌      | 50/141 [01:16<02:22,  1.56s/it]avg_loss = 1.8835825709735645:  35%|███▌      | 50/141 [01:18<02:22,  1.56s/it]avg_loss = 1.8835825709735645:  36%|███▌      | 51/141 [01:18<02:20,  1.56s/it]avg_loss = 1.8794798759313731:  36%|███▌      | 51/141 [01:20<02:20,  1.56s/it]avg_loss = 1.8794798759313731:  37%|███▋      | 52/141 [01:20<02:19,  1.57s/it]avg_loss = 1.8727265641374409:  37%|███▋      | 52/141 [01:21<02:19,  1.57s/it]avg_loss = 1.8727265641374409:  38%|███▊      | 53/141 [01:21<02:18,  1.57s/it]avg_loss = 1.8695440358585782:  38%|███▊      | 53/141 [01:23<02:18,  1.57s/it]avg_loss = 1.8695440358585782:  38%|███▊      | 54/141 [01:23<02:16,  1.57s/it]avg_loss = 1.8616722432049837:  38%|███▊      | 54/141 [01:24<02:16,  1.57s/it]avg_loss = 1.8616722432049837:  39%|███▉      | 55/141 [01:24<02:15,  1.57s/it]avg_loss = 1.8539946249553136:  39%|███▉      | 55/141 [01:26<02:15,  1.57s/it]avg_loss = 1.8539946249553136:  40%|███▉      | 56/141 [01:26<02:13,  1.57s/it]avg_loss = 1.8483656038317764:  40%|███▉      | 56/141 [01:27<02:13,  1.57s/it]avg_loss = 1.8483656038317764:  40%|████      | 57/141 [01:27<02:12,  1.57s/it]avg_loss = 1.845314802794621:  40%|████      | 57/141 [01:29<02:12,  1.57s/it] avg_loss = 1.845314802794621:  41%|████      | 58/141 [01:29<02:10,  1.57s/it]avg_loss = 1.847595610861051:  41%|████      | 58/141 [01:31<02:10,  1.57s/it]avg_loss = 1.847595610861051:  42%|████▏     | 59/141 [01:31<02:09,  1.58s/it]avg_loss = 1.8530332446098328:  42%|████▏     | 59/141 [01:32<02:09,  1.58s/it]avg_loss = 1.8530332446098328:  43%|████▎     | 60/141 [01:32<02:07,  1.58s/it]avg_loss = 1.8587892719956696:  43%|████▎     | 60/141 [01:34<02:07,  1.58s/it]avg_loss = 1.8587892719956696:  43%|████▎     | 61/141 [01:34<02:06,  1.58s/it]avg_loss = 1.8660690938272784:  43%|████▎     | 61/141 [01:35<02:06,  1.58s/it]avg_loss = 1.8660690938272784:  44%|████▍     | 62/141 [01:35<02:04,  1.58s/it]avg_loss = 1.8569685239640494:  44%|████▍     | 62/141 [01:37<02:04,  1.58s/it]avg_loss = 1.8569685239640494:  45%|████▍     | 63/141 [01:37<02:02,  1.57s/it]avg_loss = 1.8548182044178247:  45%|████▍     | 63/141 [01:38<02:02,  1.57s/it]avg_loss = 1.8548182044178247:  45%|████▌     | 64/141 [01:38<02:01,  1.57s/it]avg_loss = 1.852306527357835:  45%|████▌     | 64/141 [01:40<02:01,  1.57s/it] avg_loss = 1.852306527357835:  46%|████▌     | 65/141 [01:40<01:59,  1.58s/it]avg_loss = 1.8462248101378933:  46%|████▌     | 65/141 [01:42<01:59,  1.58s/it]avg_loss = 1.8462248101378933:  47%|████▋     | 66/141 [01:42<01:58,  1.58s/it]avg_loss = 1.8435912292395065:  47%|████▋     | 66/141 [01:43<01:58,  1.58s/it]avg_loss = 1.8435912292395065:  48%|████▊     | 67/141 [01:43<01:56,  1.58s/it]avg_loss = 1.8402359888834112:  48%|████▊     | 67/141 [01:45<01:56,  1.58s/it]avg_loss = 1.8402359888834112:  48%|████▊     | 68/141 [01:45<01:55,  1.58s/it]avg_loss = 1.8375741105148757:  48%|████▊     | 68/141 [01:46<01:55,  1.58s/it]avg_loss = 1.8375741105148757:  49%|████▉     | 69/141 [01:46<01:53,  1.58s/it]avg_loss = 1.8384413072041104:  49%|████▉     | 69/141 [01:48<01:53,  1.58s/it]avg_loss = 1.8384413072041104:  50%|████▉     | 70/141 [01:48<01:52,  1.58s/it]avg_loss = 1.8420680140105772:  50%|████▉     | 70/141 [01:50<01:52,  1.58s/it]avg_loss = 1.8420680140105772:  50%|█████     | 71/141 [01:50<01:50,  1.58s/it]avg_loss = 1.8442903227276273:  50%|█████     | 71/141 [01:51<01:50,  1.58s/it]avg_loss = 1.8442903227276273:  51%|█████     | 72/141 [01:51<01:48,  1.58s/it]avg_loss = 1.84272467110255:  51%|█████     | 72/141 [01:53<01:48,  1.58s/it]  avg_loss = 1.84272467110255:  52%|█████▏    | 73/141 [01:53<01:47,  1.58s/it]avg_loss = 1.844549812175132:  52%|█████▏    | 73/141 [01:54<01:47,  1.58s/it]avg_loss = 1.844549812175132:  52%|█████▏    | 74/141 [01:54<01:45,  1.58s/it]avg_loss = 1.8447703043619792:  52%|█████▏    | 74/141 [01:56<01:45,  1.58s/it]avg_loss = 1.8447703043619792:  53%|█████▎    | 75/141 [01:56<01:44,  1.58s/it]avg_loss = 1.8436310008952492:  53%|█████▎    | 75/141 [01:57<01:44,  1.58s/it]avg_loss = 1.8436310008952492:  54%|█████▍    | 76/141 [01:57<01:42,  1.58s/it]avg_loss = 1.8449225162530873:  54%|█████▍    | 76/141 [01:59<01:42,  1.58s/it]avg_loss = 1.8449225162530873:  55%|█████▍    | 77/141 [01:59<01:41,  1.58s/it]avg_loss = 1.8471227532778032:  55%|█████▍    | 77/141 [02:01<01:41,  1.58s/it]avg_loss = 1.8471227532778032:  55%|█████▌    | 78/141 [02:01<01:39,  1.58s/it]avg_loss = 1.8511606847183615:  55%|█████▌    | 78/141 [02:02<01:39,  1.58s/it]avg_loss = 1.8511606847183615:  56%|█████▌    | 79/141 [02:02<01:38,  1.59s/it]avg_loss = 1.84816173017025:  56%|█████▌    | 79/141 [02:04<01:38,  1.59s/it]  avg_loss = 1.84816173017025:  57%|█████▋    | 80/141 [02:04<01:36,  1.59s/it]avg_loss = 1.8470135662290785:  57%|█████▋    | 80/141 [02:05<01:36,  1.59s/it]avg_loss = 1.8470135662290785:  57%|█████▋    | 81/141 [02:05<01:35,  1.59s/it]avg_loss = 1.8461644387826688:  57%|█████▋    | 81/141 [02:07<01:35,  1.59s/it]avg_loss = 1.8461644387826688:  58%|█████▊    | 82/141 [02:07<01:33,  1.59s/it]avg_loss = 1.8443803040378064:  58%|█████▊    | 82/141 [02:09<01:33,  1.59s/it]avg_loss = 1.8443803040378064:  59%|█████▉    | 83/141 [02:09<01:31,  1.59s/it]avg_loss = 1.8422618167740958:  59%|█████▉    | 83/141 [02:10<01:31,  1.59s/it]avg_loss = 1.8422618167740958:  60%|█████▉    | 84/141 [02:10<01:30,  1.59s/it]avg_loss = 1.8399018862668206:  60%|█████▉    | 84/141 [02:12<01:30,  1.59s/it]avg_loss = 1.8399018862668206:  60%|██████    | 85/141 [02:12<01:28,  1.59s/it]avg_loss = 1.8414532486782518:  60%|██████    | 85/141 [02:13<01:28,  1.59s/it]avg_loss = 1.8414532486782518:  61%|██████    | 86/141 [02:13<01:27,  1.59s/it]avg_loss = 1.843231868469852:  61%|██████    | 86/141 [02:15<01:27,  1.59s/it] avg_loss = 1.843231868469852:  62%|██████▏   | 87/141 [02:15<01:25,  1.59s/it]avg_loss = 1.8437445895238356:  62%|██████▏   | 87/141 [02:16<01:25,  1.59s/it]avg_loss = 1.8437445895238356:  62%|██████▏   | 88/141 [02:16<01:24,  1.59s/it]avg_loss = 1.8524651286307345:  62%|██████▏   | 88/141 [02:18<01:24,  1.59s/it]avg_loss = 1.8524651286307345:  63%|██████▎   | 89/141 [02:18<01:22,  1.59s/it]avg_loss = 1.8599565108617147:  63%|██████▎   | 89/141 [02:20<01:22,  1.59s/it]avg_loss = 1.8599565108617147:  64%|██████▍   | 90/141 [02:20<01:20,  1.59s/it]avg_loss = 1.8632311585185293:  64%|██████▍   | 90/141 [02:21<01:20,  1.59s/it]avg_loss = 1.8632311585185293:  65%|██████▍   | 91/141 [02:21<01:19,  1.59s/it]avg_loss = 1.8682130678840305:  65%|██████▍   | 91/141 [02:23<01:19,  1.59s/it]avg_loss = 1.8682130678840305:  65%|██████▌   | 92/141 [02:23<01:17,  1.59s/it]avg_loss = 1.8732280013381795:  65%|██████▌   | 92/141 [02:24<01:17,  1.59s/it]avg_loss = 1.8732280013381795:  66%|██████▌   | 93/141 [02:24<01:16,  1.59s/it]avg_loss = 1.8742102524067492:  66%|██████▌   | 93/141 [02:26<01:16,  1.59s/it]avg_loss = 1.8742102524067492:  67%|██████▋   | 94/141 [02:26<01:14,  1.59s/it]avg_loss = 1.8779767350146646:  67%|██████▋   | 94/141 [02:28<01:14,  1.59s/it]avg_loss = 1.8779767350146646:  67%|██████▋   | 95/141 [02:28<01:13,  1.59s/it]avg_loss = 1.8787206932902336:  67%|██████▋   | 95/141 [02:29<01:13,  1.59s/it]avg_loss = 1.8787206932902336:  68%|██████▊   | 96/141 [02:29<01:11,  1.59s/it]avg_loss = 1.8806138726853834:  68%|██████▊   | 96/141 [02:31<01:11,  1.59s/it]avg_loss = 1.8806138726853834:  69%|██████▉   | 97/141 [02:31<01:09,  1.59s/it]avg_loss = 1.8764905528146394:  69%|██████▉   | 97/141 [02:32<01:09,  1.59s/it]avg_loss = 1.8764905528146394:  70%|██████▉   | 98/141 [02:32<01:08,  1.59s/it]avg_loss = 1.877236540871437:  70%|██████▉   | 98/141 [02:34<01:08,  1.59s/it] avg_loss = 1.877236540871437:  70%|███████   | 99/141 [02:34<01:06,  1.59s/it]avg_loss = 1.8793142664432525:  70%|███████   | 99/141 [02:36<01:06,  1.59s/it]avg_loss = 1.8793142664432525:  71%|███████   | 100/141 [02:36<01:05,  1.59s/it]avg_loss = 1.8780115158251016:  71%|███████   | 100/141 [02:37<01:05,  1.59s/it]avg_loss = 1.8780115158251016:  72%|███████▏  | 101/141 [02:37<01:03,  1.59s/it]avg_loss = 1.8782365964908225:  72%|███████▏  | 101/141 [02:39<01:03,  1.59s/it]avg_loss = 1.8782365964908225:  72%|███████▏  | 102/141 [02:39<01:02,  1.59s/it]avg_loss = 1.876578034706486:  72%|███████▏  | 102/141 [02:40<01:02,  1.59s/it] avg_loss = 1.876578034706486:  73%|███████▎  | 103/141 [02:40<01:00,  1.59s/it]avg_loss = 1.8791152147146373:  73%|███████▎  | 103/141 [02:42<01:00,  1.59s/it]avg_loss = 1.8791152147146373:  74%|███████▍  | 104/141 [02:42<00:58,  1.59s/it]avg_loss = 1.877626203355335:  74%|███████▍  | 104/141 [02:44<00:58,  1.59s/it] avg_loss = 1.877626203355335:  74%|███████▍  | 105/141 [02:44<00:57,  1.59s/it]avg_loss = 1.8766197764648582:  74%|███████▍  | 105/141 [02:45<00:57,  1.59s/it]avg_loss = 1.8766197764648582:  75%|███████▌  | 106/141 [02:45<00:55,  1.59s/it]avg_loss = 1.8742764965396062:  75%|███████▌  | 106/141 [02:47<00:55,  1.59s/it]avg_loss = 1.8742764965396062:  76%|███████▌  | 107/141 [02:47<00:54,  1.59s/it]avg_loss = 1.8720115069989804:  76%|███████▌  | 107/141 [02:48<00:54,  1.59s/it]avg_loss = 1.8720115069989804:  77%|███████▋  | 108/141 [02:48<00:52,  1.59s/it]avg_loss = 1.8695662131003283:  77%|███████▋  | 108/141 [02:50<00:52,  1.59s/it]avg_loss = 1.8695662131003283:  77%|███████▋  | 109/141 [02:50<00:50,  1.59s/it]avg_loss = 1.867101910981265:  77%|███████▋  | 109/141 [02:51<00:50,  1.59s/it] avg_loss = 1.867101910981265:  78%|███████▊  | 110/141 [02:51<00:49,  1.59s/it]avg_loss = 1.8694007944416355:  78%|███████▊  | 110/141 [02:53<00:49,  1.59s/it]avg_loss = 1.8694007944416355:  79%|███████▊  | 111/141 [02:53<00:47,  1.59s/it]avg_loss = 1.8691450261643954:  79%|███████▊  | 111/141 [02:55<00:47,  1.59s/it]avg_loss = 1.8691450261643954:  79%|███████▉  | 112/141 [02:55<00:46,  1.59s/it]avg_loss = 1.8702208710982737:  79%|███████▉  | 112/141 [02:56<00:46,  1.59s/it]avg_loss = 1.8702208710982737:  80%|████████  | 113/141 [02:56<00:44,  1.59s/it]avg_loss = 1.871173368211378:  80%|████████  | 113/141 [02:58<00:44,  1.59s/it] avg_loss = 1.871173368211378:  81%|████████  | 114/141 [02:58<00:43,  1.59s/it]avg_loss = 1.87050650741743:  81%|████████  | 114/141 [02:59<00:43,  1.59s/it] avg_loss = 1.87050650741743:  82%|████████▏ | 115/141 [02:59<00:41,  1.59s/it]avg_loss = 1.8690957951134648:  82%|████████▏ | 115/141 [03:01<00:41,  1.59s/it]avg_loss = 1.8690957951134648:  82%|████████▏ | 116/141 [03:01<00:39,  1.59s/it]avg_loss = 1.8713089431452954:  82%|████████▏ | 116/141 [03:03<00:39,  1.59s/it]avg_loss = 1.8713089431452954:  83%|████████▎ | 117/141 [03:03<00:38,  1.59s/it]avg_loss = 1.8710134706254733:  83%|████████▎ | 117/141 [03:04<00:38,  1.59s/it]avg_loss = 1.8710134706254733:  84%|████████▎ | 118/141 [03:04<00:36,  1.59s/it]avg_loss = 1.86957444763985:  84%|████████▎ | 118/141 [03:06<00:36,  1.59s/it]  avg_loss = 1.86957444763985:  84%|████████▍ | 119/141 [03:06<00:35,  1.59s/it]avg_loss = 1.8678766906261444:  84%|████████▍ | 119/141 [03:07<00:35,  1.59s/it]avg_loss = 1.8678766906261444:  85%|████████▌ | 120/141 [03:07<00:33,  1.59s/it]avg_loss = 1.8677116415717385:  85%|████████▌ | 120/141 [03:09<00:33,  1.59s/it]avg_loss = 1.8677116415717385:  86%|████████▌ | 121/141 [03:09<00:31,  1.59s/it]avg_loss = 1.8680304347491654:  86%|████████▌ | 121/141 [03:11<00:31,  1.59s/it]avg_loss = 1.8680304347491654:  87%|████████▋ | 122/141 [03:11<00:30,  1.59s/it]avg_loss = 1.8678129329914:  87%|████████▋ | 122/141 [03:12<00:30,  1.59s/it]   avg_loss = 1.8678129329914:  87%|████████▋ | 123/141 [03:12<00:28,  1.59s/it]avg_loss = 1.8680344022089435:  87%|████████▋ | 123/141 [03:14<00:28,  1.59s/it]avg_loss = 1.8680344022089435:  88%|████████▊ | 124/141 [03:14<00:27,  1.60s/it]avg_loss = 1.8666875200271607:  88%|████████▊ | 124/141 [03:15<00:27,  1.60s/it]avg_loss = 1.8666875200271607:  89%|████████▊ | 125/141 [03:15<00:25,  1.60s/it]avg_loss = 1.8670716228939237:  89%|████████▊ | 125/141 [03:17<00:25,  1.60s/it]avg_loss = 1.8670716228939237:  89%|████████▉ | 126/141 [03:17<00:23,  1.60s/it]avg_loss = 1.866810084327938:  89%|████████▉ | 126/141 [03:19<00:23,  1.60s/it] avg_loss = 1.866810084327938:  90%|█████████ | 127/141 [03:19<00:22,  1.60s/it]avg_loss = 1.8655403470620513:  90%|█████████ | 127/141 [03:20<00:22,  1.60s/it]avg_loss = 1.8655403470620513:  91%|█████████ | 128/141 [03:20<00:20,  1.60s/it]avg_loss = 1.8656909558200097:  91%|█████████ | 128/141 [03:22<00:20,  1.60s/it]avg_loss = 1.8656909558200097:  91%|█████████▏| 129/141 [03:22<00:19,  1.60s/it]avg_loss = 1.866424887913924:  91%|█████████▏| 129/141 [03:23<00:19,  1.60s/it] avg_loss = 1.866424887913924:  92%|█████████▏| 130/141 [03:23<00:17,  1.60s/it]avg_loss = 1.8673704671495743:  92%|█████████▏| 130/141 [03:25<00:17,  1.60s/it]avg_loss = 1.8673704671495743:  93%|█████████▎| 131/141 [03:25<00:15,  1.60s/it]avg_loss = 1.8679526646931965:  93%|█████████▎| 131/141 [03:27<00:15,  1.60s/it]avg_loss = 1.8679526646931965:  94%|█████████▎| 132/141 [03:27<00:14,  1.60s/it]avg_loss = 1.8650641226230706:  94%|█████████▎| 132/141 [03:28<00:14,  1.60s/it]avg_loss = 1.8650641226230706:  94%|█████████▍| 133/141 [03:28<00:12,  1.60s/it]avg_loss = 1.8605819122115177:  94%|█████████▍| 133/141 [03:30<00:12,  1.60s/it]avg_loss = 1.8605819122115177:  95%|█████████▌| 134/141 [03:30<00:11,  1.60s/it]avg_loss = 1.8630475715354637:  95%|█████████▌| 134/141 [03:31<00:11,  1.60s/it]avg_loss = 1.8630475715354637:  96%|█████████▌| 135/141 [03:31<00:09,  1.60s/it]avg_loss = 1.8665514185148127:  96%|█████████▌| 135/141 [03:33<00:09,  1.60s/it]avg_loss = 1.8665514185148127:  96%|█████████▋| 136/141 [03:33<00:07,  1.60s/it]avg_loss = 1.8677524998240227:  96%|█████████▋| 136/141 [03:35<00:07,  1.60s/it]avg_loss = 1.8677524998240227:  97%|█████████▋| 137/141 [03:35<00:06,  1.60s/it]avg_loss = 1.8664546643478284:  97%|█████████▋| 137/141 [03:36<00:06,  1.60s/it]avg_loss = 1.8664546643478284:  98%|█████████▊| 138/141 [03:36<00:04,  1.60s/it]avg_loss = 1.8667298478188274:  98%|█████████▊| 138/141 [03:38<00:04,  1.60s/it]avg_loss = 1.8667298478188274:  99%|█████████▊| 139/141 [03:38<00:03,  1.60s/it]avg_loss = 1.8673184701374599:  99%|█████████▊| 139/141 [03:39<00:03,  1.60s/it]avg_loss = 1.8673184701374599:  99%|█████████▉| 140/141 [03:39<00:01,  1.60s/it]avg_loss = 1.8686561364654108:  99%|█████████▉| 140/141 [03:41<00:01,  1.60s/it]avg_loss = 1.8686561364654108: 100%|██████████| 141/141 [03:41<00:00,  1.60s/it]avg_loss = 1.8686561364654108: 100%|██████████| 141/141 [03:41<00:00,  1.57s/it]
I0328 12:31:08.005482 2310837 eval_ppl.py:107] wikitext2 perplexity: 6.479582786560059
wikitext2 perplexity: 6.480
