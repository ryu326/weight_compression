I0328 15:57:00.570021 2313906 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:57:00.570125 2313906 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:57:00.570168 2313906 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:57:00.909392 2313906 config.py:54] PyTorch version 2.6.0 available.
W0328 15:57:01.117667 2313906 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:57:01.703319 2313906 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.57it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.13it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.17it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.32it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.40it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.36it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.55it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.37it/s]
I0328 15:57:03.257546 2313906 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.43it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:17,  1.69it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.78it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.84it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.85it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.87it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.88it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.88it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:12,  1.89it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.89it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:11,  1.90it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.91it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:09,  1.91it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.91it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  1.92it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.94it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.94it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.94it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.93it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.93it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.93it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.92it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.94it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.93it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.93it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.92it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.92it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.94it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.95it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.95it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]
I0328 15:57:26.114751 2313906 quantize_finetune_llama.py:185] loaded compression model
I0328 15:57:45.790841 2313906 quantize_finetune_llama.py:189] loaded dataset and devset
I0328 15:57:50.888385 2313906 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:58:43.242165 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 52.21191668510437s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0328 15:59:06.254899 2314036 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 15:59:06.255002 2314036 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 15:59:06.255041 2314036 utils.py:162] NumExpr defaulting to 16 threads.
I0328 15:59:06.585435 2314036 config.py:54] PyTorch version 2.6.0 available.
W0328 15:59:06.776842 2314036 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 15:59:07.401573 2314036 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 15:59:07.405337 2313906 quantize_finetune_llama.py:209] layer 1 gpu 1
I0328 15:59:07.418299 2314036 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 15:59:23.911077 2314036 finetune.py:45] layer 0_v initial loss 1.3420506661532272e-07
W0328 15:59:23.911291 2314036 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 15:59:57.567819 2314036 finetune.py:68] layer 0_v @ epoch 0 new loss 1.1583335890463786e-07 old loss 1.3420506661532272e-07 BETTER
I0328 16:00:11.208197 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 63.61838126182556s
I0328 16:00:20.166171 2314108 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:00:20.166272 2314108 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:00:20.166310 2314108 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:00:20.523615 2314108 config.py:54] PyTorch version 2.6.0 available.
W0328 16:00:20.740632 2314108 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:00:21.350288 2314108 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:00:21.353799 2313906 quantize_finetune_llama.py:209] layer 2 gpu 2
I0328 16:00:21.367733 2314108 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 16:00:32.711454 2314036 finetune.py:68] layer 0_v @ epoch 1 new loss 1.0987543674900735e-07 old loss 1.1583335890463786e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:00:38.352092 2314108 finetune.py:45] layer 1_v initial loss 1.1320780686219223e-06
W0328 16:00:38.352519 2314108 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:01:08.471610 2314036 finetune.py:68] layer 0_v @ epoch 2 new loss 1.0702986230626266e-07 old loss 1.0987543674900735e-07 BETTER
I0328 16:01:10.945120 2314108 finetune.py:68] layer 1_v @ epoch 0 new loss 4.899465011476423e-07 old loss 1.1320780686219223e-06 BETTER
I0328 16:01:25.406767 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 63.888731479644775s
I0328 16:01:34.840863 2314180 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:01:34.840961 2314180 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:01:34.841004 2314180 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:01:35.203805 2314180 config.py:54] PyTorch version 2.6.0 available.
W0328 16:01:35.407549 2314180 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:01:36.005660 2314180 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:01:36.009733 2313906 quantize_finetune_llama.py:209] layer 3 gpu 3
I0328 16:01:36.025688 2314180 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 16:01:44.643067 2314036 finetune.py:68] layer 0_v @ epoch 3 new loss 1.0529134897296899e-07 old loss 1.0702986230626266e-07 BETTER
I0328 16:01:45.000596 2314108 finetune.py:68] layer 1_v @ epoch 1 new loss 2.9305971338544623e-07 old loss 4.899465011476423e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:01:52.975476 2314180 finetune.py:45] layer 2_v initial loss 2.351690909563331e-06
W0328 16:01:52.975755 2314180 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:02:19.295409 2314108 finetune.py:68] layer 1_v @ epoch 2 new loss 2.1691634799481108e-07 old loss 2.9305971338544623e-07 BETTER
I0328 16:02:21.106188 2314036 finetune.py:68] layer 0_v @ epoch 4 new loss 1.040363883930695e-07 old loss 1.0529134897296899e-07 BETTER
I0328 16:02:26.337985 2314180 finetune.py:68] layer 2_v @ epoch 0 new loss 6.891027624078561e-07 old loss 2.351690909563331e-06 BETTER
I0328 16:02:41.065842 2314036 finetune.py:45] layer 0_q initial loss 1.0432245289848652e-07
I0328 16:02:41.687167 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 65.45106530189514s
I0328 16:02:50.807507 2314252 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:02:50.807620 2314252 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:02:50.807663 2314252 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:02:51.195363 2314252 config.py:54] PyTorch version 2.6.0 available.
W0328 16:02:51.408415 2314252 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:02:52.037358 2314252 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:02:52.041411 2313906 quantize_finetune_llama.py:209] layer 4 gpu 0
I0328 16:02:52.056005 2314252 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 16:02:53.962943 2314108 finetune.py:68] layer 1_v @ epoch 3 new loss 1.8506047183564078e-07 old loss 2.1691634799481108e-07 BETTER
I0328 16:03:00.802397 2314180 finetune.py:68] layer 2_v @ epoch 1 new loss 3.837600104361627e-07 old loss 6.891027624078561e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:03:09.035829 2314252 finetune.py:45] layer 3_v initial loss 2.377498958594515e-06
W0328 16:03:09.036049 2314252 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:03:15.813641 2314036 finetune.py:68] layer 0_q @ epoch 0 new loss 1.0330604283126377e-07 old loss 1.0432245289848652e-07 BETTER
I0328 16:03:28.635946 2314108 finetune.py:68] layer 1_v @ epoch 4 new loss 1.6821485360196675e-07 old loss 1.8506047183564078e-07 BETTER
I0328 16:03:35.297588 2314180 finetune.py:68] layer 2_v @ epoch 2 new loss 3.020608119186363e-07 old loss 3.837600104361627e-07 BETTER
I0328 16:03:41.624820 2314252 finetune.py:68] layer 3_v @ epoch 0 new loss 7.585616685901186e-07 old loss 2.377498958594515e-06 BETTER
I0328 16:03:48.115314 2314108 finetune.py:45] layer 1_q initial loss 1.7061681489849434e-07
I0328 16:03:51.812145 2314036 finetune.py:68] layer 0_q @ epoch 1 new loss 1.0247576653910073e-07 old loss 1.0330604283126377e-07 BETTER
I0328 16:04:10.030266 2314180 finetune.py:68] layer 2_v @ epoch 3 new loss 2.676706571946852e-07 old loss 3.020608119186363e-07 BETTER
I0328 16:04:15.699845 2314252 finetune.py:68] layer 3_v @ epoch 1 new loss 5.050272875450901e-07 old loss 7.585616685901186e-07 BETTER
I0328 16:04:21.295394 2314108 finetune.py:68] layer 1_q @ epoch 0 new loss 1.6088407051029208e-07 old loss 1.7061681489849434e-07 BETTER
I0328 16:04:28.104484 2314036 finetune.py:68] layer 0_q @ epoch 2 new loss 1.0177667064681373e-07 old loss 1.0247576653910073e-07 BETTER
I0328 16:04:45.062913 2314180 finetune.py:68] layer 2_v @ epoch 4 new loss 2.476552367625118e-07 old loss 2.676706571946852e-07 BETTER
I0328 16:04:50.151664 2314252 finetune.py:68] layer 3_v @ epoch 2 new loss 4.181191854968347e-07 old loss 5.050272875450901e-07 BETTER
I0328 16:04:55.427540 2314108 finetune.py:68] layer 1_q @ epoch 1 new loss 1.5086813220932527e-07 old loss 1.6088407051029208e-07 BETTER
I0328 16:05:04.282318 2314036 finetune.py:68] layer 0_q @ epoch 3 new loss 1.011447636756202e-07 old loss 1.0177667064681373e-07 BETTER
I0328 16:05:04.443689 2314180 finetune.py:45] layer 2_q initial loss 2.743848597219767e-07
I0328 16:05:24.540871 2314252 finetune.py:68] layer 3_v @ epoch 3 new loss 3.755196189558774e-07 old loss 4.181191854968347e-07 BETTER
I0328 16:05:29.912872 2314108 finetune.py:68] layer 1_q @ epoch 2 new loss 1.4497737765850616e-07 old loss 1.5086813220932527e-07 BETTER
I0328 16:05:37.811810 2314180 finetune.py:68] layer 2_q @ epoch 0 new loss 2.572681978563196e-07 old loss 2.743848597219767e-07 BETTER
I0328 16:05:40.548532 2314036 finetune.py:68] layer 0_q @ epoch 4 new loss 1.0057836874466375e-07 old loss 1.011447636756202e-07 BETTER
I0328 16:05:58.247603 2314036 finetune.py:45] layer 0_k initial loss 1.0305091535656175e-07
I0328 16:05:58.997015 2314252 finetune.py:68] layer 3_v @ epoch 4 new loss 3.5050376823164697e-07 old loss 3.755196189558774e-07 BETTER
I0328 16:06:04.326193 2314108 finetune.py:68] layer 1_q @ epoch 3 new loss 1.4021253491591779e-07 old loss 1.4497737765850616e-07 BETTER
I0328 16:06:12.265843 2314180 finetune.py:68] layer 2_q @ epoch 1 new loss 2.463808357333619e-07 old loss 2.572681978563196e-07 BETTER
I0328 16:06:18.339577 2314252 finetune.py:45] layer 3_q initial loss 4.008277869615995e-07
I0328 16:06:33.324235 2314036 finetune.py:68] layer 0_k @ epoch 0 new loss 1.0195937960588708e-07 old loss 1.0305091535656175e-07 BETTER
I0328 16:06:39.038787 2314108 finetune.py:68] layer 1_q @ epoch 4 new loss 1.3631780859668652e-07 old loss 1.4021253491591779e-07 BETTER
I0328 16:06:46.679898 2314180 finetune.py:68] layer 2_q @ epoch 2 new loss 2.3871737653280434e-07 old loss 2.463808357333619e-07 BETTER
I0328 16:06:51.253302 2314252 finetune.py:68] layer 3_q @ epoch 0 new loss 3.808137876148976e-07 old loss 4.008277869615995e-07 BETTER
I0328 16:06:56.781952 2314108 finetune.py:45] layer 1_k initial loss 1.429939544550507e-07
I0328 16:07:09.292689 2314036 finetune.py:68] layer 0_k @ epoch 1 new loss 1.0136958650264205e-07 old loss 1.0195937960588708e-07 BETTER
I0328 16:07:21.209554 2314180 finetune.py:68] layer 2_q @ epoch 3 new loss 2.329780386389757e-07 old loss 2.3871737653280434e-07 BETTER
I0328 16:07:25.007688 2314252 finetune.py:68] layer 3_q @ epoch 1 new loss 3.681039402181341e-07 old loss 3.808137876148976e-07 BETTER
I0328 16:07:29.819475 2314108 finetune.py:68] layer 1_k @ epoch 0 new loss 1.3832617185016716e-07 old loss 1.429939544550507e-07 BETTER
I0328 16:07:45.495454 2314036 finetune.py:68] layer 0_k @ epoch 2 new loss 1.0088202628821819e-07 old loss 1.0136958650264205e-07 BETTER
I0328 16:07:56.018956 2314180 finetune.py:68] layer 2_q @ epoch 4 new loss 2.2853222958474362e-07 old loss 2.329780386389757e-07 BETTER
I0328 16:07:59.053978 2314252 finetune.py:68] layer 3_q @ epoch 2 new loss 3.5951919130639e-07 old loss 3.681039402181341e-07 BETTER
I0328 16:08:03.872194 2314108 finetune.py:68] layer 1_k @ epoch 1 new loss 1.351432104002015e-07 old loss 1.3832617185016716e-07 BETTER
I0328 16:08:13.825629 2314180 finetune.py:45] layer 2_k initial loss 2.4119179897752474e-07
I0328 16:08:21.545433 2314036 finetune.py:68] layer 0_k @ epoch 3 new loss 1.0042502651685936e-07 old loss 1.0088202628821819e-07 BETTER
I0328 16:08:32.984059 2314252 finetune.py:68] layer 3_q @ epoch 3 new loss 3.526973273437761e-07 old loss 3.5951919130639e-07 BETTER
I0328 16:08:37.934804 2314108 finetune.py:68] layer 1_k @ epoch 2 new loss 1.3255426267733128e-07 old loss 1.351432104002015e-07 BETTER
I0328 16:08:47.277830 2314180 finetune.py:68] layer 2_k @ epoch 0 new loss 2.362605897587855e-07 old loss 2.4119179897752474e-07 BETTER
I0328 16:08:57.881483 2314036 finetune.py:68] layer 0_k @ epoch 4 new loss 1.0001484440635977e-07 old loss 1.0042502651685936e-07 BETTER
I0328 16:09:07.012630 2314252 finetune.py:68] layer 3_q @ epoch 4 new loss 3.4757997013912245e-07 old loss 3.526973273437761e-07 BETTER
I0328 16:09:12.052749 2314108 finetune.py:68] layer 1_k @ epoch 3 new loss 1.3033475454449217e-07 old loss 1.3255426267733128e-07 BETTER
I0328 16:09:17.592920 2314036 finetune.py:45] layer 0_o initial loss 1.5617978021964518e-07
I0328 16:09:21.816865 2314180 finetune.py:68] layer 2_k @ epoch 1 new loss 2.3316506769788248e-07 old loss 2.362605897587855e-07 BETTER
I0328 16:09:24.977989 2314252 finetune.py:45] layer 3_k initial loss 3.707453402057581e-07
I0328 16:09:46.290053 2314108 finetune.py:68] layer 1_k @ epoch 4 new loss 1.2838624741107196e-07 old loss 1.3033475454449217e-07 BETTER
I0328 16:09:51.928719 2314036 finetune.py:68] layer 0_o @ epoch 0 new loss 1.5581883872073377e-07 old loss 1.5617978021964518e-07 BETTER
I0328 16:09:56.411409 2314180 finetune.py:68] layer 2_k @ epoch 2 new loss 2.3065305754244037e-07 old loss 2.3316506769788248e-07 BETTER
I0328 16:09:57.896043 2314252 finetune.py:68] layer 3_k @ epoch 0 new loss 3.650926601039828e-07 old loss 3.707453402057581e-07 BETTER
I0328 16:10:05.821545 2314108 finetune.py:45] layer 1_o initial loss 2.8077607794330106e-07
I0328 16:10:27.378850 2314036 finetune.py:68] layer 0_o @ epoch 1 new loss 1.5550601517588802e-07 old loss 1.5581883872073377e-07 BETTER
I0328 16:10:30.896484 2314180 finetune.py:68] layer 2_k @ epoch 3 new loss 2.2847179081963986e-07 old loss 2.3065305754244037e-07 BETTER
I0328 16:10:31.627055 2314252 finetune.py:68] layer 3_k @ epoch 1 new loss 3.6133016578787647e-07 old loss 3.650926601039828e-07 BETTER
I0328 16:10:38.382055 2314108 finetune.py:68] layer 1_o @ epoch 0 new loss 2.7775786293204874e-07 old loss 2.8077607794330106e-07 BETTER
I0328 16:11:03.142089 2314036 finetune.py:68] layer 0_o @ epoch 2 new loss 1.552273403149229e-07 old loss 1.5550601517588802e-07 BETTER
I0328 16:11:05.416116 2314180 finetune.py:68] layer 2_k @ epoch 4 new loss 2.265531406919763e-07 old loss 2.2847179081963986e-07 BETTER
I0328 16:11:05.541543 2314252 finetune.py:68] layer 3_k @ epoch 2 new loss 3.582212571018317e-07 old loss 3.6133016578787647e-07 BETTER
I0328 16:11:11.863779 2314108 finetune.py:68] layer 1_o @ epoch 1 new loss 2.775401810595213e-07 old loss 2.7775786293204874e-07 BETTER
I0328 16:11:24.623250 2314180 finetune.py:45] layer 2_o initial loss 4.703597085153888e-07
I0328 16:11:38.895445 2314036 finetune.py:68] layer 0_o @ epoch 3 new loss 1.5497319338919624e-07 old loss 1.552273403149229e-07 BETTER
I0328 16:11:39.482097 2314252 finetune.py:68] layer 3_k @ epoch 3 new loss 3.5549214771890547e-07 old loss 3.582212571018317e-07 BETTER
I0328 16:11:45.401520 2314108 finetune.py:68] layer 1_o @ epoch 2 new loss 2.737465081281698e-07 old loss 2.775401810595213e-07 BETTER
I0328 16:11:57.443369 2314180 finetune.py:68] layer 2_o @ epoch 0 new loss 4.639038309051102e-07 old loss 4.703597085153888e-07 BETTER
I0328 16:12:13.507640 2314252 finetune.py:68] layer 3_k @ epoch 4 new loss 3.530149399466609e-07 old loss 3.5549214771890547e-07 BETTER
I0328 16:12:14.640891 2314036 finetune.py:68] layer 0_o @ epoch 4 new loss 1.54728056145359e-07 old loss 1.5497319338919624e-07 BETTER
I0328 16:12:19.287661 2314108 finetune.py:68] layer 1_o @ epoch 3 new loss 2.7233079435973195e-07 old loss 2.737465081281698e-07 BETTER
I0328 16:12:31.182031 2314180 finetune.py:68] layer 2_o @ epoch 1 new loss 4.6058809743954043e-07 old loss 4.639038309051102e-07 BETTER
I0328 16:12:32.648014 2314252 finetune.py:45] layer 3_o initial loss 7.71720806369558e-07
I0328 16:12:45.898861 2314036 finetune.py:45] layer 0_up initial loss 2.061140946807427e-07
I0328 16:12:53.318031 2314108 finetune.py:68] layer 1_o @ epoch 4 new loss 2.7114901968161575e-07 old loss 2.7233079435973195e-07 BETTER
I0328 16:13:04.671866 2314252 finetune.py:68] layer 3_o @ epoch 0 new loss 7.624037152709207e-07 old loss 7.71720806369558e-07 BETTER
I0328 16:13:04.940072 2314180 finetune.py:68] layer 2_o @ epoch 2 new loss 4.584230737236794e-07 old loss 4.6058809743954043e-07 BETTER
I0328 16:13:17.675474 2314036 finetune.py:68] layer 0_up @ epoch 0 new loss 2.0584623428021587e-07 old loss 2.061140946807427e-07 BETTER
I0328 16:13:24.214949 2314108 finetune.py:45] layer 1_up initial loss 4.1466412881163706e-07
I0328 16:13:37.796818 2314252 finetune.py:68] layer 3_o @ epoch 1 new loss 7.58001647227502e-07 old loss 7.624037152709207e-07 BETTER
I0328 16:13:38.819590 2314180 finetune.py:68] layer 2_o @ epoch 3 new loss 4.567208691241831e-07 old loss 4.584230737236794e-07 BETTER
I0328 16:13:50.601646 2314036 finetune.py:68] layer 0_up @ epoch 1 new loss 2.0563963687436626e-07 old loss 2.0584623428021587e-07 BETTER
I0328 16:13:54.547955 2314108 finetune.py:68] layer 1_up @ epoch 0 new loss 3.9144896391007933e-07 old loss 4.1466412881163706e-07 BETTER
I0328 16:14:11.056441 2314252 finetune.py:68] layer 3_o @ epoch 2 new loss 7.545060611846566e-07 old loss 7.58001647227502e-07 BETTER
I0328 16:14:12.840310 2314180 finetune.py:68] layer 2_o @ epoch 4 new loss 4.5522494929173263e-07 old loss 4.567208691241831e-07 BETTER
I0328 16:14:23.822494 2314036 finetune.py:68] layer 0_up @ epoch 2 new loss 2.0544034384784027e-07 old loss 2.0563963687436626e-07 BETTER
I0328 16:14:25.839202 2314108 finetune.py:68] layer 1_up @ epoch 1 new loss 3.89777596865315e-07 old loss 3.9144896391007933e-07 BETTER
I0328 16:14:43.635479 2314180 finetune.py:45] layer 2_up initial loss 7.474959602404851e-07
I0328 16:14:44.305804 2314252 finetune.py:68] layer 3_o @ epoch 3 new loss 7.513871196351829e-07 old loss 7.545060611846566e-07 BETTER
I0328 16:14:57.159288 2314036 finetune.py:68] layer 0_up @ epoch 3 new loss 2.0524980470781884e-07 old loss 2.0544034384784027e-07 BETTER
I0328 16:14:57.554221 2314108 finetune.py:68] layer 1_up @ epoch 2 new loss 3.890934010541969e-07 old loss 3.89777596865315e-07 BETTER
I0328 16:15:14.235987 2314180 finetune.py:68] layer 2_up @ epoch 0 new loss 7.458214099642646e-07 old loss 7.474959602404851e-07 BETTER
I0328 16:15:17.350805 2314252 finetune.py:68] layer 3_o @ epoch 4 new loss 7.485342052859778e-07 old loss 7.513871196351829e-07 BETTER
I0328 16:15:29.377456 2314108 finetune.py:76] layer 1_up @ epoch 3 new loss 3.89248526744268e-07 old loss 3.890934010541969e-07 WORSE
I0328 16:15:30.510439 2314036 finetune.py:68] layer 0_up @ epoch 4 new loss 2.0506520570506837e-07 old loss 2.0524980470781884e-07 BETTER
I0328 16:15:45.992918 2314180 finetune.py:68] layer 2_up @ epoch 1 new loss 7.44400892926933e-07 old loss 7.458214099642646e-07 BETTER
I0328 16:15:48.423020 2314252 finetune.py:45] layer 3_up initial loss 1.4141628525976557e-06
I0328 16:16:00.447422 2314108 finetune.py:68] layer 1_up @ epoch 4 new loss 3.887630839471967e-07 old loss 3.890934010541969e-07 BETTER
I0328 16:16:02.160615 2314036 finetune.py:45] layer 0_gate initial loss 2.3721187858427584e-07
I0328 16:16:18.238399 2314180 finetune.py:68] layer 2_up @ epoch 2 new loss 7.430811024278228e-07 old loss 7.44400892926933e-07 BETTER
I0328 16:16:18.548878 2314252 finetune.py:68] layer 3_up @ epoch 0 new loss 1.4098337715040543e-06 old loss 1.4141628525976557e-06 BETTER
I0328 16:16:32.144919 2314036 finetune.py:68] layer 0_gate @ epoch 0 new loss 2.369902034615734e-07 old loss 2.3721187858427584e-07 BETTER
I0328 16:16:32.279344 2314108 finetune.py:45] layer 1_gate initial loss 4.749990409891325e-07
I0328 16:16:49.615935 2314252 finetune.py:68] layer 3_up @ epoch 1 new loss 1.4061871524972958e-06 old loss 1.4098337715040543e-06 BETTER
I0328 16:16:50.375599 2314180 finetune.py:68] layer 2_up @ epoch 3 new loss 7.418478276122187e-07 old loss 7.430811024278228e-07 BETTER
I0328 16:17:00.729228 2314108 finetune.py:68] layer 1_gate @ epoch 0 new loss 4.6866151137692214e-07 old loss 4.749990409891325e-07 BETTER
I0328 16:17:03.209397 2314036 finetune.py:68] layer 0_gate @ epoch 1 new loss 2.3681658944951778e-07 old loss 2.369902034615734e-07 BETTER
I0328 16:17:20.977886 2314252 finetune.py:68] layer 3_up @ epoch 2 new loss 1.402792577209766e-06 old loss 1.4061871524972958e-06 BETTER
I0328 16:17:22.569206 2314180 finetune.py:68] layer 2_up @ epoch 4 new loss 7.406696340694907e-07 old loss 7.418478276122187e-07 BETTER
I0328 16:17:30.120796 2314108 finetune.py:68] layer 1_gate @ epoch 1 new loss 4.6822086119391315e-07 old loss 4.6866151137692214e-07 BETTER
I0328 16:17:34.467054 2314036 finetune.py:68] layer 0_gate @ epoch 2 new loss 2.3665607784550957e-07 old loss 2.3681658944951778e-07 BETTER
I0328 16:17:52.350197 2314252 finetune.py:68] layer 3_up @ epoch 3 new loss 1.3995702374813845e-06 old loss 1.402792577209766e-06 BETTER
I0328 16:17:53.494335 2314180 finetune.py:45] layer 2_gate initial loss 9.229653414877248e-07
I0328 16:17:59.611636 2314108 finetune.py:68] layer 1_gate @ epoch 2 new loss 4.680150595959276e-07 old loss 4.6822086119391315e-07 BETTER
I0328 16:18:05.849138 2314036 finetune.py:68] layer 0_gate @ epoch 3 new loss 2.365156035466498e-07 old loss 2.3665607784550957e-07 BETTER
I0328 16:18:22.212339 2314180 finetune.py:68] layer 2_gate @ epoch 0 new loss 9.218699119628582e-07 old loss 9.229653414877248e-07 BETTER
I0328 16:18:23.719401 2314252 finetune.py:68] layer 3_up @ epoch 4 new loss 1.396541506437643e-06 old loss 1.3995702374813845e-06 BETTER
I0328 16:18:29.250165 2314108 finetune.py:68] layer 1_gate @ epoch 3 new loss 4.6721845592401223e-07 old loss 4.680150595959276e-07 BETTER
I0328 16:18:37.359479 2314036 finetune.py:68] layer 0_gate @ epoch 4 new loss 2.3637927881736687e-07 old loss 2.365156035466498e-07 BETTER
I0328 16:18:51.961977 2314180 finetune.py:68] layer 2_gate @ epoch 1 new loss 9.209006179844437e-07 old loss 9.218699119628582e-07 BETTER
I0328 16:18:54.513971 2314252 finetune.py:45] layer 3_gate initial loss 1.7082212480090675e-06
I0328 16:18:58.960286 2314108 finetune.py:68] layer 1_gate @ epoch 4 new loss 4.668720805511839e-07 old loss 4.6721845592401223e-07 BETTER
I0328 16:19:21.871436 2314180 finetune.py:68] layer 2_gate @ epoch 2 new loss 9.199954433825042e-07 old loss 9.209006179844437e-07 BETTER
I0328 16:19:22.911486 2314252 finetune.py:68] layer 3_gate @ epoch 0 new loss 1.7052450402843533e-06 old loss 1.7082212480090675e-06 BETTER
I0328 16:19:34.233566 2314036 finetune.py:45] layer 0_down initial loss 3.6246694889996434e-07
I0328 16:19:51.605799 2314180 finetune.py:68] layer 2_gate @ epoch 3 new loss 9.191223284688022e-07 old loss 9.199954433825042e-07 BETTER
I0328 16:19:52.040081 2314252 finetune.py:68] layer 3_gate @ epoch 1 new loss 1.7026887917381828e-06 old loss 1.7052450402843533e-06 BETTER
I0328 16:19:56.597079 2314108 finetune.py:45] layer 1_down initial loss 7.181218961704872e-07
I0328 16:20:01.546259 2314036 finetune.py:68] layer 0_down @ epoch 0 new loss 3.624451210271218e-07 old loss 3.6246694889996434e-07 BETTER
I0328 16:20:21.615645 2314252 finetune.py:68] layer 3_gate @ epoch 2 new loss 1.700210646049527e-06 old loss 1.7026887917381828e-06 BETTER
I0328 16:20:21.631680 2314180 finetune.py:68] layer 2_gate @ epoch 4 new loss 9.1827865844607e-07 old loss 9.191223284688022e-07 BETTER
I0328 16:20:22.880205 2314108 finetune.py:68] layer 1_down @ epoch 0 new loss 7.177911243161361e-07 old loss 7.181218961704872e-07 BETTER
I0328 16:20:30.100464 2314036 finetune.py:68] layer 0_down @ epoch 1 new loss 3.6242332157598867e-07 old loss 3.624451210271218e-07 BETTER
I0328 16:20:49.978713 2314108 finetune.py:68] layer 1_down @ epoch 1 new loss 7.173347853495216e-07 old loss 7.177911243161361e-07 BETTER
I0328 16:20:50.926557 2314252 finetune.py:68] layer 3_gate @ epoch 3 new loss 1.697865741334681e-06 old loss 1.700210646049527e-06 BETTER
I0328 16:20:58.788057 2314036 finetune.py:68] layer 0_down @ epoch 2 new loss 3.624028579451988e-07 old loss 3.6242332157598867e-07 BETTER
I0328 16:21:17.424931 2314108 finetune.py:76] layer 1_down @ epoch 2 new loss 7.17568184427364e-07 old loss 7.173347853495216e-07 WORSE
I0328 16:21:17.926818 2314180 finetune.py:45] layer 2_down initial loss 1.4094989637669642e-06
I0328 16:21:20.235931 2314252 finetune.py:68] layer 3_gate @ epoch 4 new loss 1.6955647197391954e-06 old loss 1.697865741334681e-06 BETTER
I0328 16:21:27.972084 2314036 finetune.py:68] layer 0_down @ epoch 3 new loss 3.623824227361183e-07 old loss 3.624028579451988e-07 BETTER
I0328 16:21:44.204237 2314180 finetune.py:68] layer 2_down @ epoch 0 new loss 1.4094749758442049e-06 old loss 1.4094989637669642e-06 BETTER
I0328 16:21:44.406125 2314108 finetune.py:76] layer 1_down @ epoch 3 new loss 7.174684242272633e-07 old loss 7.173347853495216e-07 WORSE
I0328 16:21:57.069547 2314036 finetune.py:68] layer 0_down @ epoch 4 new loss 3.6237031508790096e-07 old loss 3.623824227361183e-07 BETTER
0_v proxy err 0.0047753844410181046 tr(WHW.T) 60.88684844970703
bpp_loss 4.200907702150289
0_q proxy err 1.0169225788558833e-05 tr(WHW.T) 288090.46875
bpp_loss 5.103120757674333
0_k proxy err 1.148777300841175e-05 tr(WHW.T) 100178.234375
bpp_loss 5.780488599964883
0_o proxy err 0.0006874721730127931 tr(WHW.T) 3118.390625
bpp_loss 4.292888306546956
0_up proxy err 0.0013228686293587089 tr(WHW.T) 8924.583984375
bpp_loss 4.575050992891192
0_gate proxy err 0.0007532857125625014 tr(WHW.T) 15778.90625
bpp_loss 4.690323270790811
0_down proxy err 0.0010707515757530928 tr(WHW.T) 10821.7001953125
bpp_loss 4.5701110459465
I0328 16:22:11.542831 2314108 finetune.py:68] layer 1_down @ epoch 4 new loss 7.170903018050012e-07 old loss 7.173347853495216e-07 BETTER
I0328 16:22:11.832574 2314180 finetune.py:68] layer 2_down @ epoch 1 new loss 1.4094612197368406e-06 old loss 1.4094749758442049e-06 BETTER
1_v proxy err 0.002023800276219845 tr(WHW.T) 109.07096099853516
bpp_loss 4.3022453234298155
1_q proxy err 7.319170890696114e-06 tr(WHW.T) 144791.34375
bpp_loss 5.388421263487544
1_k proxy err 4.4132380025985185e-06 tr(WHW.T) 75499.34375
bpp_loss 6.274472615506966
1_o proxy err 0.0012747487053275108 tr(WHW.T) 1988.54638671875
bpp_loss 4.3755816196789965
1_up proxy err 0.001515716197900474 tr(WHW.T) 8230.537109375
bpp_loss 4.589076614206923
1_gate proxy err 0.0009001971920952201 tr(WHW.T) 13950.03515625
bpp_loss 4.700162766700877
1_down proxy err 3.125765942968428e-05 tr(WHW.T) 13986.8701171875
bpp_loss 4.58393131462591
I0328 16:22:17.196708 2314252 finetune.py:45] layer 3_down initial loss 2.6933328172162874e-06
I0328 16:22:39.615936 2314180 finetune.py:68] layer 2_down @ epoch 2 new loss 1.4094432572164806e-06 old loss 1.4094612197368406e-06 BETTER
I0328 16:22:43.082090 2314252 finetune.py:68] layer 3_down @ epoch 0 new loss 2.6932939363177866e-06 old loss 2.6933328172162874e-06 BETTER
I0328 16:23:07.432050 2314180 finetune.py:68] layer 2_down @ epoch 3 new loss 1.409431661159033e-06 old loss 1.4094432572164806e-06 BETTER
I0328 16:23:10.158746 2314252 finetune.py:68] layer 3_down @ epoch 1 new loss 2.693264377739979e-06 old loss 2.6932939363177866e-06 BETTER
I0328 16:23:25.425661 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 67.47660708427429s
I0328 16:23:29.248440 2314322 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:23:29.248555 2314322 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:23:29.248599 2314322 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:23:29.617270 2314322 config.py:54] PyTorch version 2.6.0 available.
W0328 16:23:29.832767 2314322 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:23:30.447994 2314322 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:23:30.451794 2313906 quantize_finetune_llama.py:209] layer 5 gpu 1
I0328 16:23:30.465687 2314322 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 16:23:35.253268 2314180 finetune.py:68] layer 2_down @ epoch 4 new loss 1.409416540809616e-06 old loss 1.409431661159033e-06 BETTER
2_v proxy err 0.0031118036713451147 tr(WHW.T) 155.95950317382812
bpp_loss 4.21287785685854
2_q proxy err 5.2146486268611625e-05 tr(WHW.T) 41466.0234375
bpp_loss 5.283413515833672
2_k proxy err 2.810194382618647e-05 tr(WHW.T) 22607.220703125
bpp_loss 6.288102694321424
2_o proxy err 0.0012597888708114624 tr(WHW.T) 1965.6915283203125
bpp_loss 4.323888432001695
2_up proxy err 0.001788509194739163 tr(WHW.T) 7601.19384765625
bpp_loss 4.581228825529771
2_gate proxy err 0.0009076225105673075 tr(WHW.T) 15116.060546875
bpp_loss 4.7342784223811964
2_down proxy err 0.001693214988335967 tr(WHW.T) 7731.037109375
bpp_loss 4.585827216905143
I0328 16:23:37.493803 2314252 finetune.py:76] layer 3_down @ epoch 2 new loss 2.6932661967293825e-06 old loss 2.693264377739979e-06 WORSE
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:23:47.410637 2314322 finetune.py:45] layer 4_v initial loss 2.17702245208784e-06
W0328 16:23:47.410919 2314322 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:24:04.086199 2314252 finetune.py:68] layer 3_down @ epoch 3 new loss 2.693216629268136e-06 old loss 2.693264377739979e-06 BETTER
I0328 16:24:22.360604 2314322 finetune.py:68] layer 4_v @ epoch 0 new loss 7.103229222593654e-07 old loss 2.17702245208784e-06 BETTER
I0328 16:24:31.352724 2314252 finetune.py:68] layer 3_down @ epoch 4 new loss 2.6931934371532407e-06 old loss 2.693216629268136e-06 BETTER
3_v proxy err 0.002344361739233136 tr(WHW.T) 289.3331604003906
bpp_loss 4.30883882305352
3_q proxy err 6.105643842602149e-05 tr(WHW.T) 47572.25
bpp_loss 5.320776136824861
3_k proxy err 3.021849988726899e-05 tr(WHW.T) 26168.77734375
bpp_loss 6.367026045569219
3_o proxy err 0.0014729609247297049 tr(WHW.T) 1857.251708984375
bpp_loss 4.418760439381003
3_up proxy err 0.001778257661499083 tr(WHW.T) 7536.345703125
bpp_loss 4.563006124924868
3_gate proxy err 0.0006511692772619426 tr(WHW.T) 20886.501953125
bpp_loss 4.814516248647124
3_down proxy err 0.0019472538260743022 tr(WHW.T) 7003.7919921875
bpp_loss 4.5616166449617594
I0328 16:24:45.330589 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 64.38206505775452s
I0328 16:24:48.982418 2314392 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:24:48.982507 2314392 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:24:48.982545 2314392 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:24:49.312037 2314392 config.py:54] PyTorch version 2.6.0 available.
W0328 16:24:49.509452 2314392 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:24:50.066930 2314392 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:24:50.070430 2313906 quantize_finetune_llama.py:209] layer 6 gpu 2
I0328 16:24:50.083286 2314392 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 16:24:58.865564 2314322 finetune.py:68] layer 4_v @ epoch 1 new loss 5.54697919596947e-07 old loss 7.103229222593654e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:25:06.835155 2314392 finetune.py:45] layer 5_v initial loss 2.3829195470170816e-06
W0328 16:25:06.835569 2314392 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:25:35.909688 2314322 finetune.py:68] layer 4_v @ epoch 2 new loss 4.959287593919726e-07 old loss 5.54697919596947e-07 BETTER
I0328 16:25:40.098305 2314392 finetune.py:68] layer 5_v @ epoch 0 new loss 9.014893294079229e-07 old loss 2.3829195470170816e-06 BETTER
I0328 16:25:53.962839 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 63.42204761505127s
I0328 16:25:57.726106 2314462 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:25:57.726201 2314462 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:25:57.726243 2314462 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:25:58.068226 2314462 config.py:54] PyTorch version 2.6.0 available.
W0328 16:25:58.275515 2314462 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:25:59.002807 2314462 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:25:59.006774 2313906 quantize_finetune_llama.py:209] layer 7 gpu 3
I0328 16:25:59.021188 2314462 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:26:13.109552 2314322 finetune.py:68] layer 4_v @ epoch 3 new loss 4.6590611191277276e-07 old loss 4.959287593919726e-07 BETTER
I0328 16:26:14.350112 2314392 finetune.py:68] layer 5_v @ epoch 1 new loss 7.589868005197786e-07 old loss 9.014893294079229e-07 BETTER
I0328 16:26:16.594391 2314462 finetune.py:45] layer 6_v initial loss 1.890902694867691e-06
W0328 16:26:16.594658 2314462 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:26:49.161981 2314392 finetune.py:68] layer 5_v @ epoch 2 new loss 7.036042006802745e-07 old loss 7.589868005197786e-07 BETTER
I0328 16:26:50.286484 2314462 finetune.py:68] layer 6_v @ epoch 0 new loss 9.487321790402348e-07 old loss 1.890902694867691e-06 BETTER
I0328 16:26:50.464825 2314322 finetune.py:68] layer 4_v @ epoch 4 new loss 4.4765243956135237e-07 old loss 4.6590611191277276e-07 BETTER
I0328 16:27:02.668907 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 63.13290357589722s
I0328 16:27:06.569814 2314532 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:27:06.569947 2314532 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:27:06.570007 2314532 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:27:06.972168 2314532 config.py:54] PyTorch version 2.6.0 available.
W0328 16:27:07.208486 2314532 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:27:07.980261 2314532 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:27:07.984183 2313906 quantize_finetune_llama.py:209] layer 8 gpu 0
I0328 16:27:07.997940 2314532 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 16:27:09.768284 2314322 finetune.py:45] layer 4_q initial loss 5.285730821924517e-07
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:27:23.951175 2314392 finetune.py:68] layer 5_v @ epoch 3 new loss 6.745108294126112e-07 old loss 7.036042006802745e-07 BETTER
I0328 16:27:24.782141 2314462 finetune.py:68] layer 6_v @ epoch 1 new loss 8.632495109850424e-07 old loss 9.487321790402348e-07 BETTER
I0328 16:27:25.436141 2314532 finetune.py:45] layer 7_v initial loss 1.8988667989106034e-06
W0328 16:27:25.436365 2314532 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:27:44.814717 2314322 finetune.py:68] layer 4_q @ epoch 0 new loss 5.067014399173786e-07 old loss 5.285730821924517e-07 BETTER
I0328 16:27:58.226511 2314532 finetune.py:68] layer 7_v @ epoch 0 new loss 1.129917450271023e-06 old loss 1.8988667989106034e-06 BETTER
I0328 16:27:58.943260 2314392 finetune.py:68] layer 5_v @ epoch 4 new loss 6.564115437868168e-07 old loss 6.745108294126112e-07 BETTER
I0328 16:27:59.817529 2314462 finetune.py:68] layer 6_v @ epoch 2 new loss 8.267895168501127e-07 old loss 8.632495109850424e-07 BETTER
I0328 16:28:18.210101 2314392 finetune.py:45] layer 5_q initial loss 7.813106321918895e-07
I0328 16:28:21.054752 2314322 finetune.py:68] layer 4_q @ epoch 1 new loss 4.938532924825267e-07 old loss 5.067014399173786e-07 BETTER
I0328 16:28:32.439514 2314532 finetune.py:68] layer 7_v @ epoch 1 new loss 1.060473778125015e-06 old loss 1.129917450271023e-06 BETTER
I0328 16:28:34.865449 2314462 finetune.py:68] layer 6_v @ epoch 3 new loss 8.068438432928815e-07 old loss 8.267895168501127e-07 BETTER
I0328 16:28:51.825259 2314392 finetune.py:68] layer 5_q @ epoch 0 new loss 7.590983841510024e-07 old loss 7.813106321918895e-07 BETTER
I0328 16:28:57.658241 2314322 finetune.py:68] layer 4_q @ epoch 2 new loss 4.845614967052825e-07 old loss 4.938532924825267e-07 BETTER
I0328 16:29:06.849267 2314532 finetune.py:68] layer 7_v @ epoch 2 new loss 1.0308315268048318e-06 old loss 1.060473778125015e-06 BETTER
I0328 16:29:09.844852 2314462 finetune.py:68] layer 6_v @ epoch 4 new loss 7.910862791504769e-07 old loss 8.068438432928815e-07 BETTER
I0328 16:29:26.238651 2314392 finetune.py:68] layer 5_q @ epoch 1 new loss 7.450165071531956e-07 old loss 7.590983841510024e-07 BETTER
I0328 16:29:29.635740 2314462 finetune.py:45] layer 6_q initial loss 9.393286291015102e-07
I0328 16:29:34.559025 2314322 finetune.py:68] layer 4_q @ epoch 3 new loss 4.772304009748041e-07 old loss 4.845614967052825e-07 BETTER
I0328 16:29:41.341809 2314532 finetune.py:68] layer 7_v @ epoch 3 new loss 1.0098984830619884e-06 old loss 1.0308315268048318e-06 BETTER
I0328 16:30:00.740255 2314392 finetune.py:68] layer 5_q @ epoch 2 new loss 7.344812615883711e-07 old loss 7.450165071531956e-07 BETTER
I0328 16:30:03.506931 2314462 finetune.py:68] layer 6_q @ epoch 0 new loss 9.217530418936803e-07 old loss 9.393286291015102e-07 BETTER
I0328 16:30:11.418684 2314322 finetune.py:68] layer 4_q @ epoch 4 new loss 4.7101144673433737e-07 old loss 4.772304009748041e-07 BETTER
I0328 16:30:16.009348 2314532 finetune.py:68] layer 7_v @ epoch 4 new loss 1.0046417173725786e-06 old loss 1.0098984830619884e-06 BETTER
I0328 16:30:29.128093 2314322 finetune.py:45] layer 4_k initial loss 5.046425712862401e-07
I0328 16:30:35.185700 2314532 finetune.py:45] layer 7_q initial loss 1.2234266932864557e-06
I0328 16:30:35.212983 2314392 finetune.py:68] layer 5_q @ epoch 3 new loss 7.262067356350599e-07 old loss 7.344812615883711e-07 BETTER
I0328 16:30:38.131297 2314462 finetune.py:68] layer 6_q @ epoch 1 new loss 9.078868856704503e-07 old loss 9.217530418936803e-07 BETTER
I0328 16:31:04.316574 2314322 finetune.py:68] layer 4_k @ epoch 0 new loss 4.961206059306278e-07 old loss 5.046425712862401e-07 BETTER
I0328 16:31:08.154444 2314532 finetune.py:68] layer 7_q @ epoch 0 new loss 1.1954092542509898e-06 old loss 1.2234266932864557e-06 BETTER
I0328 16:31:09.948675 2314392 finetune.py:68] layer 5_q @ epoch 4 new loss 7.193407327577006e-07 old loss 7.262067356350599e-07 BETTER
I0328 16:31:12.814753 2314462 finetune.py:68] layer 6_q @ epoch 2 new loss 8.979454833024647e-07 old loss 9.078868856704503e-07 BETTER
I0328 16:31:27.949243 2314392 finetune.py:45] layer 5_k initial loss 7.612153467562166e-07
I0328 16:31:40.679975 2314322 finetune.py:68] layer 4_k @ epoch 1 new loss 4.911895530312904e-07 old loss 4.961206059306278e-07 BETTER
I0328 16:31:42.218780 2314532 finetune.py:68] layer 7_q @ epoch 1 new loss 1.1813218634415534e-06 old loss 1.1954092542509898e-06 BETTER
I0328 16:31:47.464476 2314462 finetune.py:68] layer 6_q @ epoch 3 new loss 8.884823046173551e-07 old loss 8.979454833024647e-07 BETTER
I0328 16:32:01.401679 2314392 finetune.py:68] layer 5_k @ epoch 0 new loss 7.498235845559975e-07 old loss 7.612153467562166e-07 BETTER
I0328 16:32:16.251272 2314532 finetune.py:68] layer 7_q @ epoch 2 new loss 1.1658833045657957e-06 old loss 1.1813218634415534e-06 BETTER
I0328 16:32:17.263090 2314322 finetune.py:68] layer 4_k @ epoch 2 new loss 4.871208147960715e-07 old loss 4.911895530312904e-07 BETTER
I0328 16:32:22.109521 2314462 finetune.py:68] layer 6_q @ epoch 4 new loss 8.814916441224341e-07 old loss 8.884823046173551e-07 BETTER
I0328 16:32:35.740479 2314392 finetune.py:68] layer 5_k @ epoch 1 new loss 7.44006399600039e-07 old loss 7.498235845559975e-07 BETTER
I0328 16:32:40.098650 2314462 finetune.py:45] layer 6_k initial loss 9.259231887881469e-07
I0328 16:32:50.499152 2314532 finetune.py:68] layer 7_q @ epoch 3 new loss 1.153466769210354e-06 old loss 1.1658833045657957e-06 BETTER
I0328 16:32:53.871110 2314322 finetune.py:68] layer 4_k @ epoch 3 new loss 4.836666107621568e-07 old loss 4.871208147960715e-07 BETTER
I0328 16:33:10.211806 2314392 finetune.py:68] layer 5_k @ epoch 2 new loss 7.392872021227959e-07 old loss 7.44006399600039e-07 BETTER
I0328 16:33:13.696929 2314462 finetune.py:68] layer 6_k @ epoch 0 new loss 9.170687462756177e-07 old loss 9.259231887881469e-07 BETTER
I0328 16:33:24.731568 2314532 finetune.py:68] layer 7_q @ epoch 4 new loss 1.1445612244642689e-06 old loss 1.153466769210354e-06 BETTER
I0328 16:33:30.632057 2314322 finetune.py:68] layer 4_k @ epoch 4 new loss 4.80391690871329e-07 old loss 4.836666107621568e-07 BETTER
I0328 16:33:42.326543 2314532 finetune.py:45] layer 7_k initial loss 1.203770352731226e-06
I0328 16:33:44.624572 2314392 finetune.py:68] layer 5_k @ epoch 3 new loss 7.351694648605189e-07 old loss 7.392872021227959e-07 BETTER
I0328 16:33:48.162050 2314462 finetune.py:68] layer 6_k @ epoch 1 new loss 9.116297405853402e-07 old loss 9.170687462756177e-07 BETTER
I0328 16:33:50.098676 2314322 finetune.py:45] layer 4_o initial loss 1.0510809715924552e-06
I0328 16:34:15.190658 2314532 finetune.py:68] layer 7_k @ epoch 0 new loss 1.1926694014618988e-06 old loss 1.203770352731226e-06 BETTER
I0328 16:34:19.099220 2314392 finetune.py:68] layer 5_k @ epoch 4 new loss 7.311425065381627e-07 old loss 7.351694648605189e-07 BETTER
I0328 16:34:22.703003 2314462 finetune.py:68] layer 6_k @ epoch 2 new loss 9.061612331606739e-07 old loss 9.116297405853402e-07 BETTER
I0328 16:34:24.756912 2314322 finetune.py:68] layer 4_o @ epoch 0 new loss 1.0352393928769743e-06 old loss 1.0510809715924552e-06 BETTER
I0328 16:34:38.896675 2314392 finetune.py:45] layer 5_o initial loss 1.4698878203489585e-06
I0328 16:34:49.023680 2314532 finetune.py:76] layer 7_k @ epoch 1 new loss 1.1930120535907918e-06 old loss 1.1926694014618988e-06 WORSE
I0328 16:34:57.312283 2314462 finetune.py:68] layer 6_k @ epoch 3 new loss 9.029869829646486e-07 old loss 9.061612331606739e-07 BETTER
I0328 16:35:00.744014 2314322 finetune.py:68] layer 4_o @ epoch 1 new loss 1.0268531696056016e-06 old loss 1.0352393928769743e-06 BETTER
I0328 16:35:11.611856 2314392 finetune.py:68] layer 5_o @ epoch 0 new loss 1.4491690762952203e-06 old loss 1.4698878203489585e-06 BETTER
I0328 16:35:22.715683 2314532 finetune.py:68] layer 7_k @ epoch 2 new loss 1.1828793731183396e-06 old loss 1.1926694014618988e-06 BETTER
I0328 16:35:31.863915 2314462 finetune.py:68] layer 6_k @ epoch 4 new loss 8.98056441656081e-07 old loss 9.029869829646486e-07 BETTER
I0328 16:35:36.556965 2314322 finetune.py:68] layer 4_o @ epoch 2 new loss 1.019770024868194e-06 old loss 1.0268531696056016e-06 BETTER
I0328 16:35:45.304631 2314392 finetune.py:68] layer 5_o @ epoch 1 new loss 1.4374414831763715e-06 old loss 1.4491690762952203e-06 BETTER
I0328 16:35:51.355888 2314462 finetune.py:45] layer 6_o initial loss 2.00885665435635e-06
I0328 16:35:56.585462 2314532 finetune.py:68] layer 7_k @ epoch 3 new loss 1.1805630037997616e-06 old loss 1.1828793731183396e-06 BETTER
I0328 16:36:12.474387 2314322 finetune.py:68] layer 4_o @ epoch 3 new loss 1.0137542858501547e-06 old loss 1.019770024868194e-06 BETTER
I0328 16:36:19.120412 2314392 finetune.py:68] layer 5_o @ epoch 2 new loss 1.4280797131505096e-06 old loss 1.4374414831763715e-06 BETTER
I0328 16:36:24.165698 2314462 finetune.py:68] layer 6_o @ epoch 0 new loss 1.9767753656196874e-06 old loss 2.00885665435635e-06 BETTER
I0328 16:36:30.430090 2314532 finetune.py:68] layer 7_k @ epoch 4 new loss 1.168716494248656e-06 old loss 1.1805630037997616e-06 BETTER
I0328 16:36:48.544787 2314322 finetune.py:68] layer 4_o @ epoch 4 new loss 1.008230356092099e-06 old loss 1.0137542858501547e-06 BETTER
I0328 16:36:49.698928 2314532 finetune.py:45] layer 7_o initial loss 2.581495209597051e-06
I0328 16:36:52.834922 2314392 finetune.py:68] layer 5_o @ epoch 3 new loss 1.4198717508406844e-06 old loss 1.4280797131505096e-06 BETTER
I0328 16:36:57.981049 2314462 finetune.py:68] layer 6_o @ epoch 1 new loss 1.9559915926947724e-06 old loss 1.9767753656196874e-06 BETTER
I0328 16:37:19.689061 2314322 finetune.py:45] layer 4_up initial loss 2.252733111163252e-06
I0328 16:37:21.655262 2314532 finetune.py:68] layer 7_o @ epoch 0 new loss 2.524802539483062e-06 old loss 2.581495209597051e-06 BETTER
I0328 16:37:26.601799 2314392 finetune.py:68] layer 5_o @ epoch 4 new loss 1.4127094800642226e-06 old loss 1.4198717508406844e-06 BETTER
I0328 16:37:31.821541 2314462 finetune.py:68] layer 6_o @ epoch 2 new loss 1.939891262736637e-06 old loss 1.9559915926947724e-06 BETTER
I0328 16:37:51.697926 2314322 finetune.py:68] layer 4_up @ epoch 0 new loss 2.2389272089640144e-06 old loss 2.252733111163252e-06 BETTER
I0328 16:37:54.865785 2314532 finetune.py:68] layer 7_o @ epoch 1 new loss 2.4915789254009724e-06 old loss 2.524802539483062e-06 BETTER
I0328 16:37:58.123331 2314392 finetune.py:45] layer 5_up initial loss 3.217815901734866e-06
I0328 16:38:05.756124 2314462 finetune.py:68] layer 6_o @ epoch 3 new loss 1.925995320561924e-06 old loss 1.939891262736637e-06 BETTER
I0328 16:38:24.921995 2314322 finetune.py:68] layer 4_up @ epoch 1 new loss 2.2275239643931855e-06 old loss 2.2389272089640144e-06 BETTER
I0328 16:38:28.108219 2314532 finetune.py:68] layer 7_o @ epoch 2 new loss 2.466722207827843e-06 old loss 2.4915789254009724e-06 BETTER
I0328 16:38:28.659728 2314392 finetune.py:68] layer 5_up @ epoch 0 new loss 3.190892584825633e-06 old loss 3.217815901734866e-06 BETTER
I0328 16:38:39.689722 2314462 finetune.py:68] layer 6_o @ epoch 4 new loss 1.913701908051735e-06 old loss 1.925995320561924e-06 BETTER
I0328 16:38:58.426488 2314322 finetune.py:68] layer 4_up @ epoch 2 new loss 2.217288056272082e-06 old loss 2.2275239643931855e-06 BETTER
I0328 16:39:00.189264 2314392 finetune.py:68] layer 5_up @ epoch 1 new loss 3.169507408529171e-06 old loss 3.190892584825633e-06 BETTER
I0328 16:39:01.384785 2314532 finetune.py:68] layer 7_o @ epoch 3 new loss 2.445709242238081e-06 old loss 2.466722207827843e-06 BETTER
I0328 16:39:11.271486 2314462 finetune.py:45] layer 6_up initial loss 4.206789526506327e-06
I0328 16:39:32.039286 2314392 finetune.py:68] layer 5_up @ epoch 2 new loss 3.150002157781273e-06 old loss 3.169507408529171e-06 BETTER
I0328 16:39:32.067671 2314322 finetune.py:68] layer 4_up @ epoch 3 new loss 2.207392981290468e-06 old loss 2.217288056272082e-06 BETTER
I0328 16:39:34.777186 2314532 finetune.py:68] layer 7_o @ epoch 4 new loss 2.427827894280199e-06 old loss 2.445709242238081e-06 BETTER
I0328 16:39:42.041630 2314462 finetune.py:68] layer 6_up @ epoch 0 new loss 4.162014192843344e-06 old loss 4.206789526506327e-06 BETTER
I0328 16:40:04.026008 2314392 finetune.py:68] layer 5_up @ epoch 3 new loss 3.1321524147642776e-06 old loss 3.150002157781273e-06 BETTER
I0328 16:40:06.032764 2314532 finetune.py:45] layer 7_up initial loss 4.9083150770457e-06
I0328 16:40:06.040175 2314322 finetune.py:68] layer 4_up @ epoch 4 new loss 2.1982066300552106e-06 old loss 2.207392981290468e-06 BETTER
I0328 16:40:13.722265 2314462 finetune.py:68] layer 6_up @ epoch 1 new loss 4.125645318708848e-06 old loss 4.162014192843344e-06 BETTER
I0328 16:40:36.365871 2314532 finetune.py:68] layer 7_up @ epoch 0 new loss 4.851631729252404e-06 old loss 4.9083150770457e-06 BETTER
I0328 16:40:36.387213 2314392 finetune.py:68] layer 5_up @ epoch 4 new loss 3.115544586762553e-06 old loss 3.1321524147642776e-06 BETTER
I0328 16:40:37.107152 2314322 finetune.py:45] layer 4_gate initial loss 2.638127170939697e-06
I0328 16:40:45.888837 2314462 finetune.py:68] layer 6_up @ epoch 2 new loss 4.093245479452889e-06 old loss 4.125645318708848e-06 BETTER
I0328 16:41:07.360689 2314322 finetune.py:68] layer 4_gate @ epoch 0 new loss 2.629104301377083e-06 old loss 2.638127170939697e-06 BETTER
I0328 16:41:07.387056 2314392 finetune.py:45] layer 5_gate initial loss 3.7457907637872268e-06
I0328 16:41:07.504745 2314532 finetune.py:68] layer 7_up @ epoch 1 new loss 4.8065721784951165e-06 old loss 4.851631729252404e-06 BETTER
I0328 16:41:18.077976 2314462 finetune.py:68] layer 6_up @ epoch 3 new loss 4.063853339175694e-06 old loss 4.093245479452889e-06 BETTER
I0328 16:41:35.915838 2314392 finetune.py:68] layer 5_gate @ epoch 0 new loss 3.7294471439963672e-06 old loss 3.7457907637872268e-06 BETTER
I0328 16:41:38.660148 2314322 finetune.py:68] layer 4_gate @ epoch 1 new loss 2.62108983406506e-06 old loss 2.629104301377083e-06 BETTER
I0328 16:41:38.742798 2314532 finetune.py:68] layer 7_up @ epoch 2 new loss 4.766726306115743e-06 old loss 4.8065721784951165e-06 BETTER
I0328 16:41:50.368125 2314462 finetune.py:68] layer 6_up @ epoch 4 new loss 4.036552127217874e-06 old loss 4.063853339175694e-06 BETTER
I0328 16:42:05.516675 2314392 finetune.py:68] layer 5_gate @ epoch 1 new loss 3.7146076010685647e-06 old loss 3.7294471439963672e-06 BETTER
I0328 16:42:09.929946 2314322 finetune.py:68] layer 4_gate @ epoch 2 new loss 2.6134500785701675e-06 old loss 2.62108983406506e-06 BETTER
I0328 16:42:10.045311 2314532 finetune.py:68] layer 7_up @ epoch 3 new loss 4.730606178782182e-06 old loss 4.766726306115743e-06 BETTER
I0328 16:42:22.072295 2314462 finetune.py:45] layer 6_gate initial loss 4.788823844137369e-06
I0328 16:42:35.113896 2314392 finetune.py:68] layer 5_gate @ epoch 2 new loss 3.700613888213411e-06 old loss 3.7146076010685647e-06 BETTER
I0328 16:42:41.369289 2314532 finetune.py:68] layer 7_up @ epoch 4 new loss 4.697388249041978e-06 old loss 4.730606178782182e-06 BETTER
I0328 16:42:41.451900 2314322 finetune.py:68] layer 4_gate @ epoch 3 new loss 2.6062214146804763e-06 old loss 2.6134500785701675e-06 BETTER
I0328 16:42:50.654581 2314462 finetune.py:68] layer 6_gate @ epoch 0 new loss 4.762096523336368e-06 old loss 4.788823844137369e-06 BETTER
I0328 16:43:04.638941 2314392 finetune.py:68] layer 5_gate @ epoch 3 new loss 3.687699290821911e-06 old loss 3.700613888213411e-06 BETTER
I0328 16:43:12.405791 2314532 finetune.py:45] layer 7_gate initial loss 5.618264822260244e-06
I0328 16:43:12.919857 2314322 finetune.py:68] layer 4_gate @ epoch 4 new loss 2.5992442260758253e-06 old loss 2.6062214146804763e-06 BETTER
I0328 16:43:20.279305 2314462 finetune.py:68] layer 6_gate @ epoch 1 new loss 4.738188636110863e-06 old loss 4.762096523336368e-06 BETTER
I0328 16:43:34.267289 2314392 finetune.py:68] layer 5_gate @ epoch 4 new loss 3.6750539038621355e-06 old loss 3.687699290821911e-06 BETTER
I0328 16:43:40.519543 2314532 finetune.py:68] layer 7_gate @ epoch 0 new loss 5.585902272287058e-06 old loss 5.618264822260244e-06 BETTER
I0328 16:43:49.863138 2314462 finetune.py:68] layer 6_gate @ epoch 2 new loss 4.715981049230322e-06 old loss 4.738188636110863e-06 BETTER
I0328 16:44:08.086443 2314322 finetune.py:45] layer 4_down initial loss 4.3273362280160654e-06
I0328 16:44:09.570130 2314532 finetune.py:68] layer 7_gate @ epoch 1 new loss 5.557174517889507e-06 old loss 5.585902272287058e-06 BETTER
I0328 16:44:19.706869 2314462 finetune.py:68] layer 6_gate @ epoch 3 new loss 4.6951322474342305e-06 old loss 4.715981049230322e-06 BETTER
I0328 16:44:30.258519 2314392 finetune.py:45] layer 5_down initial loss 6.054906407371163e-06
I0328 16:44:35.712134 2314322 finetune.py:68] layer 4_down @ epoch 0 new loss 4.327247097535292e-06 old loss 4.3273362280160654e-06 BETTER
I0328 16:44:38.701120 2314532 finetune.py:68] layer 7_gate @ epoch 2 new loss 5.530744601855986e-06 old loss 5.557174517889507e-06 BETTER
I0328 16:44:49.642535 2314462 finetune.py:68] layer 6_gate @ epoch 4 new loss 4.675354830396827e-06 old loss 4.6951322474342305e-06 BETTER
I0328 16:44:56.398623 2314392 finetune.py:68] layer 5_down @ epoch 0 new loss 6.054823188605951e-06 old loss 6.054906407371163e-06 BETTER
I0328 16:45:04.295392 2314322 finetune.py:68] layer 4_down @ epoch 1 new loss 4.327218903199537e-06 old loss 4.327247097535292e-06 BETTER
I0328 16:45:07.994065 2314532 finetune.py:68] layer 7_gate @ epoch 3 new loss 5.5058130783436354e-06 old loss 5.530744601855986e-06 BETTER
I0328 16:45:23.514701 2314392 finetune.py:68] layer 5_down @ epoch 1 new loss 6.054743607819546e-06 old loss 6.054823188605951e-06 BETTER
I0328 16:45:32.923679 2314322 finetune.py:68] layer 4_down @ epoch 2 new loss 4.327180249674711e-06 old loss 4.327218903199537e-06 BETTER
I0328 16:45:37.286549 2314532 finetune.py:68] layer 7_gate @ epoch 4 new loss 5.482468623085879e-06 old loss 5.5058130783436354e-06 BETTER
I0328 16:45:46.407617 2314462 finetune.py:45] layer 6_down initial loss 7.549385372840334e-06
I0328 16:45:50.925657 2314392 finetune.py:68] layer 5_down @ epoch 2 new loss 6.0546935856109485e-06 old loss 6.054743607819546e-06 BETTER
I0328 16:46:02.056277 2314322 finetune.py:68] layer 4_down @ epoch 3 new loss 4.327152510086307e-06 old loss 4.327180249674711e-06 BETTER
I0328 16:46:12.684768 2314462 finetune.py:68] layer 6_down @ epoch 0 new loss 7.549213478341699e-06 old loss 7.549385372840334e-06 BETTER
I0328 16:46:18.439656 2314392 finetune.py:68] layer 5_down @ epoch 3 new loss 6.054624918760965e-06 old loss 6.0546935856109485e-06 BETTER
I0328 16:46:31.144394 2314322 finetune.py:68] layer 4_down @ epoch 4 new loss 4.327127044234658e-06 old loss 4.327152510086307e-06 BETTER
4_v proxy err 0.0021292525343596935 tr(WHW.T) 285.30712890625
bpp_loss 4.354048443085048
4_q proxy err 5.223128755460493e-05 tr(WHW.T) 50117.68359375
bpp_loss 5.285450828669127
4_k proxy err 2.4769609808572568e-05 tr(WHW.T) 29295.20703125
bpp_loss 6.33340595359914
4_o proxy err 0.0016978546045720577 tr(WHW.T) 1300.0889892578125
bpp_loss 4.4309881870867684
4_up proxy err 0.001809763489291072 tr(WHW.T) 7382.4287109375
bpp_loss 4.536679031992597
4_gate proxy err 0.00046842353185638785 tr(WHW.T) 29123.05078125
bpp_loss 4.892391305017684
4_down proxy err 0.002095626899972558 tr(WHW.T) 6397.57421875
bpp_loss 4.536834421467835
I0328 16:46:34.101301 2314532 finetune.py:45] layer 7_down initial loss 8.610250006313436e-06
I0328 16:46:40.778321 2314462 finetune.py:68] layer 6_down @ epoch 1 new loss 7.549110250693047e-06 old loss 7.549213478341699e-06 BETTER
I0328 16:46:46.481592 2314392 finetune.py:68] layer 5_down @ epoch 4 new loss 6.054595814930508e-06 old loss 6.054624918760965e-06 BETTER
5_v proxy err 0.0030204765498638153 tr(WHW.T) 208.81988525390625
bpp_loss 4.242639881151263
5_q proxy err 7.597250078106299e-05 tr(WHW.T) 35997.09375
bpp_loss 5.257717470172793
5_k proxy err 3.3953758247662336e-05 tr(WHW.T) 22998.11328125
bpp_loss 6.298962521017529
5_o proxy err 0.001851300010457635 tr(WHW.T) 1057.834716796875
bpp_loss 4.376221764483489
5_up proxy err 0.0017418633215129375 tr(WHW.T) 7655.8271484375
bpp_loss 4.541723877657205
5_gate proxy err 0.00044815210276283324 tr(WHW.T) 30379.51171875
bpp_loss 4.892989183842603
5_down proxy err 0.0020224670879542828 tr(WHW.T) 6414.30126953125
bpp_loss 4.542006123179037
I0328 16:47:00.326138 2314532 finetune.py:68] layer 7_down @ epoch 0 new loss 8.609971700934693e-06 old loss 8.610250006313436e-06 BETTER
I0328 16:47:08.483576 2314462 finetune.py:68] layer 6_down @ epoch 2 new loss 7.5490820563572925e-06 old loss 7.549110250693047e-06 BETTER
I0328 16:47:27.290866 2314532 finetune.py:68] layer 7_down @ epoch 1 new loss 8.609870747022796e-06 old loss 8.609971700934693e-06 BETTER
I0328 16:47:36.241754 2314462 finetune.py:68] layer 6_down @ epoch 3 new loss 7.548996563855326e-06 old loss 7.5490820563572925e-06 BETTER
I0328 16:47:54.403804 2314532 finetune.py:68] layer 7_down @ epoch 2 new loss 8.60981072037248e-06 old loss 8.609870747022796e-06 BETTER
I0328 16:47:58.931078 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 67.46492719650269s
I0328 16:48:02.979454 2314602 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:48:02.979557 2314602 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:48:02.979598 2314602 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:48:03.396144 2314602 config.py:54] PyTorch version 2.6.0 available.
W0328 16:48:03.620360 2314602 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 16:48:04.173751 2314462 finetune.py:68] layer 6_down @ epoch 4 new loss 7.5489538176043425e-06 old loss 7.548996563855326e-06 BETTER
W0328 16:48:04.256207 2314602 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:48:04.259982 2313906 quantize_finetune_llama.py:209] layer 9 gpu 1
I0328 16:48:04.272939 2314602 data_utils.py:336] using 256 training seqs, 128 validation seqs
6_v proxy err 0.0025359634310007095 tr(WHW.T) 253.63377380371094
bpp_loss 4.287631609826349
6_q proxy err 7.750785152893513e-05 tr(WHW.T) 35644.0078125
bpp_loss 5.305802724091336
6_k proxy err 2.905210931203328e-05 tr(WHW.T) 26160.9921875
bpp_loss 6.360515970387496
6_o proxy err 0.0021207635290920734 tr(WHW.T) 1010.5958251953125
bpp_loss 4.408163615968078
6_up proxy err 0.001642261748202145 tr(WHW.T) 7923.51611328125
bpp_loss 4.540856596415064
6_gate proxy err 0.00037204159889370203 tr(WHW.T) 35738.6640625
bpp_loss 4.900260561345411
6_down proxy err 0.0019384791376069188 tr(WHW.T) 6486.86474609375
bpp_loss 4.542141264869964
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:48:21.086116 2314602 finetune.py:45] layer 8_v initial loss 1.8877541378969909e-06
W0328 16:48:21.086572 2314602 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:48:21.575391 2314532 finetune.py:68] layer 7_down @ epoch 3 new loss 8.609751603216864e-06 old loss 8.60981072037248e-06 BETTER
I0328 16:48:48.732912 2314532 finetune.py:68] layer 7_down @ epoch 4 new loss 8.609705218987074e-06 old loss 8.609751603216864e-06 BETTER
7_v proxy err 0.0021065478213131428 tr(WHW.T) 309.4270935058594
bpp_loss 4.2854754640720785
7_q proxy err 7.912964792922139e-05 tr(WHW.T) 35147.63671875
bpp_loss 5.221942842006683
7_k proxy err 2.8374057364999317e-05 tr(WHW.T) 26846.1953125
bpp_loss 6.390162473428063
7_o proxy err 0.001868446241132915 tr(WHW.T) 960.8865966796875
bpp_loss 4.420274436590262
7_up proxy err 0.001487194444052875 tr(WHW.T) 8625.923828125
bpp_loss 4.554762788316501
7_gate proxy err 0.0003746870788745582 tr(WHW.T) 34896.1953125
bpp_loss 4.871329950567867
7_down proxy err 0.001934744417667389 tr(WHW.T) 6537.400390625
bpp_loss 4.55731538922659
I0328 16:48:56.174594 2314602 finetune.py:68] layer 8_v @ epoch 0 new loss 1.2930714774483931e-06 old loss 1.8877541378969909e-06 BETTER
I0328 16:49:13.054870 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 64.40443253517151s
I0328 16:49:16.712561 2314672 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:49:16.712660 2314672 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:49:16.712702 2314672 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:49:17.041218 2314672 config.py:54] PyTorch version 2.6.0 available.
W0328 16:49:17.245862 2314672 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:49:17.845812 2314672 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:49:17.849321 2313906 quantize_finetune_llama.py:209] layer 10 gpu 2
I0328 16:49:17.866796 2314672 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:49:32.599600 2314602 finetune.py:68] layer 8_v @ epoch 1 new loss 1.2222642453707522e-06 old loss 1.2930714774483931e-06 BETTER
I0328 16:49:34.895274 2314672 finetune.py:45] layer 9_v initial loss 2.092710928991437e-06
W0328 16:49:34.895660 2314672 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:50:07.943753 2314672 finetune.py:68] layer 9_v @ epoch 0 new loss 1.3191161087888759e-06 old loss 2.092710928991437e-06 BETTER
I0328 16:50:09.793625 2314602 finetune.py:68] layer 8_v @ epoch 2 new loss 1.1858381867568824e-06 old loss 1.2222642453707522e-06 BETTER
I0328 16:50:22.770843 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 64.44089603424072s
I0328 16:50:26.541509 2314742 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:50:26.541602 2314742 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:50:26.541641 2314742 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:50:26.902945 2314742 config.py:54] PyTorch version 2.6.0 available.
W0328 16:50:27.111998 2314742 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:50:27.826550 2314742 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:50:27.831023 2313906 quantize_finetune_llama.py:209] layer 11 gpu 3
I0328 16:50:27.845585 2314742 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:50:42.397104 2314672 finetune.py:68] layer 9_v @ epoch 1 new loss 1.2390547681206954e-06 old loss 1.3191161087888759e-06 BETTER
I0328 16:50:45.412849 2314742 finetune.py:45] layer 10_v initial loss 2.559630502219079e-06
W0328 16:50:45.413074 2314742 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:50:46.799145 2314602 finetune.py:68] layer 8_v @ epoch 3 new loss 1.162844000646146e-06 old loss 1.1858381867568824e-06 BETTER
I0328 16:51:17.250241 2314672 finetune.py:68] layer 9_v @ epoch 2 new loss 1.201294708153e-06 old loss 1.2390547681206954e-06 BETTER
I0328 16:51:18.930184 2314742 finetune.py:68] layer 10_v @ epoch 0 new loss 1.7828499494498828e-06 old loss 2.559630502219079e-06 BETTER
I0328 16:51:23.812149 2314602 finetune.py:68] layer 8_v @ epoch 4 new loss 1.149518084275769e-06 old loss 1.162844000646146e-06 BETTER
I0328 16:51:32.400089 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 64.07907843589783s
I0328 16:51:36.152871 2314812 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 16:51:36.152983 2314812 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 16:51:36.153026 2314812 utils.py:162] NumExpr defaulting to 16 threads.
I0328 16:51:36.553139 2314812 config.py:54] PyTorch version 2.6.0 available.
W0328 16:51:36.783550 2314812 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 16:51:37.441043 2314812 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 16:51:37.445084 2313906 quantize_finetune_llama.py:209] layer 12 gpu 0
I0328 16:51:37.461134 2314812 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 16:51:42.996588 2314602 finetune.py:45] layer 8_q initial loss 1.3960999467599322e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 16:51:52.323685 2314672 finetune.py:68] layer 9_v @ epoch 3 new loss 1.1770372339015012e-06 old loss 1.201294708153e-06 BETTER
I0328 16:51:53.597746 2314742 finetune.py:68] layer 10_v @ epoch 1 new loss 1.6931214759097202e-06 old loss 1.7828499494498828e-06 BETTER
I0328 16:51:54.670607 2314812 finetune.py:45] layer 11_v initial loss 2.2196602458279813e-06
W0328 16:51:54.670971 2314812 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 16:52:18.273076 2314602 finetune.py:68] layer 8_q @ epoch 0 new loss 1.3669075542566134e-06 old loss 1.3960999467599322e-06 BETTER
I0328 16:52:27.764477 2314672 finetune.py:68] layer 9_v @ epoch 4 new loss 1.1707503517754958e-06 old loss 1.1770372339015012e-06 BETTER
I0328 16:52:27.890032 2314812 finetune.py:68] layer 11_v @ epoch 0 new loss 1.5861928659433033e-06 old loss 2.2196602458279813e-06 BETTER
I0328 16:52:28.757521 2314742 finetune.py:68] layer 10_v @ epoch 2 new loss 1.6530033235540031e-06 old loss 1.6931214759097202e-06 BETTER
I0328 16:52:47.234745 2314672 finetune.py:45] layer 9_q initial loss 1.4405216006707633e-06
I0328 16:52:54.736859 2314602 finetune.py:68] layer 8_q @ epoch 1 new loss 1.3455128282657824e-06 old loss 1.3669075542566134e-06 BETTER
I0328 16:53:02.088941 2314812 finetune.py:68] layer 11_v @ epoch 1 new loss 1.5031495195216849e-06 old loss 1.5861928659433033e-06 BETTER
I0328 16:53:03.825954 2314742 finetune.py:68] layer 10_v @ epoch 3 new loss 1.6277913346129935e-06 old loss 1.6530033235540031e-06 BETTER
I0328 16:53:20.550168 2314672 finetune.py:68] layer 9_q @ epoch 0 new loss 1.408106413691712e-06 old loss 1.4405216006707633e-06 BETTER
I0328 16:53:31.331943 2314602 finetune.py:68] layer 8_q @ epoch 2 new loss 1.3373203273658874e-06 old loss 1.3455128282657824e-06 BETTER
I0328 16:53:36.478838 2314812 finetune.py:68] layer 11_v @ epoch 2 new loss 1.4723046888320823e-06 old loss 1.5031495195216849e-06 BETTER
I0328 16:53:38.912455 2314742 finetune.py:68] layer 10_v @ epoch 4 new loss 1.6108950831039692e-06 old loss 1.6277913346129935e-06 BETTER
I0328 16:53:54.876687 2314672 finetune.py:68] layer 9_q @ epoch 1 new loss 1.3837756114298827e-06 old loss 1.408106413691712e-06 BETTER
I0328 16:53:58.462973 2314742 finetune.py:45] layer 10_q initial loss 1.924042408063542e-06
I0328 16:54:08.032562 2314602 finetune.py:68] layer 8_q @ epoch 3 new loss 1.3232559012976708e-06 old loss 1.3373203273658874e-06 BETTER
I0328 16:54:11.070664 2314812 finetune.py:68] layer 11_v @ epoch 3 new loss 1.4648720707555185e-06 old loss 1.4723046888320823e-06 BETTER
I0328 16:54:29.378844 2314672 finetune.py:68] layer 9_q @ epoch 2 new loss 1.371813596051652e-06 old loss 1.3837756114298827e-06 BETTER
I0328 16:54:32.144712 2314742 finetune.py:68] layer 10_q @ epoch 0 new loss 1.884050107037183e-06 old loss 1.924042408063542e-06 BETTER
I0328 16:54:45.076502 2314602 finetune.py:68] layer 8_q @ epoch 4 new loss 1.320190108344832e-06 old loss 1.3232559012976708e-06 BETTER
I0328 16:54:45.698346 2314812 finetune.py:68] layer 11_v @ epoch 4 new loss 1.4516417650156654e-06 old loss 1.4648720707555185e-06 BETTER
I0328 16:55:02.697106 2314602 finetune.py:45] layer 8_k initial loss 1.3959984244138468e-06
I0328 16:55:03.941204 2314672 finetune.py:68] layer 9_q @ epoch 3 new loss 1.3698919474336435e-06 old loss 1.371813596051652e-06 BETTER
I0328 16:55:05.548022 2314812 finetune.py:45] layer 11_q initial loss 1.816770009099855e-06
I0328 16:55:06.881533 2314742 finetune.py:68] layer 10_q @ epoch 1 new loss 1.854372158049955e-06 old loss 1.884050107037183e-06 BETTER
I0328 16:55:38.071063 2314602 finetune.py:68] layer 8_k @ epoch 0 new loss 1.3655107977683656e-06 old loss 1.3959984244138468e-06 BETTER
I0328 16:55:38.666788 2314672 finetune.py:68] layer 9_q @ epoch 4 new loss 1.3606151014755596e-06 old loss 1.3698919474336435e-06 BETTER
I0328 16:55:38.779620 2314812 finetune.py:68] layer 11_q @ epoch 0 new loss 1.794345848793455e-06 old loss 1.816770009099855e-06 BETTER
I0328 16:55:41.528903 2314742 finetune.py:68] layer 10_q @ epoch 2 new loss 1.8307393929717364e-06 old loss 1.854372158049955e-06 BETTER
I0328 16:55:56.698117 2314672 finetune.py:45] layer 9_k initial loss 1.4431724366659182e-06
I0328 16:56:12.835216 2314812 finetune.py:68] layer 11_q @ epoch 1 new loss 1.7287476339333807e-06 old loss 1.794345848793455e-06 BETTER
I0328 16:56:14.662545 2314602 finetune.py:68] layer 8_k @ epoch 1 new loss 1.358241888738121e-06 old loss 1.3655107977683656e-06 BETTER
I0328 16:56:16.474964 2314742 finetune.py:68] layer 10_q @ epoch 3 new loss 1.8134346646547783e-06 old loss 1.8307393929717364e-06 BETTER
I0328 16:56:30.118147 2314672 finetune.py:68] layer 9_k @ epoch 0 new loss 1.4405815136342426e-06 old loss 1.4431724366659182e-06 BETTER
I0328 16:56:47.119847 2314812 finetune.py:68] layer 11_q @ epoch 2 new loss 1.7100096556532662e-06 old loss 1.7287476339333807e-06 BETTER
I0328 16:56:51.376806 2314602 finetune.py:68] layer 8_k @ epoch 2 new loss 1.3496255633071996e-06 old loss 1.358241888738121e-06 BETTER
I0328 16:56:51.410154 2314742 finetune.py:68] layer 10_q @ epoch 4 new loss 1.7984638134294073e-06 old loss 1.8134346646547783e-06 BETTER
I0328 16:57:04.464616 2314672 finetune.py:68] layer 9_k @ epoch 1 new loss 1.412804863321071e-06 old loss 1.4405815136342426e-06 BETTER
I0328 16:57:09.447152 2314742 finetune.py:45] layer 10_k initial loss 1.8810907249644515e-06
I0328 16:57:21.572150 2314812 finetune.py:68] layer 11_q @ epoch 3 new loss 1.6982249917418812e-06 old loss 1.7100096556532662e-06 BETTER
I0328 16:57:28.264787 2314602 finetune.py:68] layer 8_k @ epoch 3 new loss 1.3434909078569035e-06 old loss 1.3496255633071996e-06 BETTER
I0328 16:57:38.811029 2314672 finetune.py:68] layer 9_k @ epoch 2 new loss 1.4002421266923193e-06 old loss 1.412804863321071e-06 BETTER
I0328 16:57:43.257932 2314742 finetune.py:68] layer 10_k @ epoch 0 new loss 1.8616083252709359e-06 old loss 1.8810907249644515e-06 BETTER
I0328 16:57:55.758742 2314812 finetune.py:68] layer 11_q @ epoch 4 new loss 1.6950627923506545e-06 old loss 1.6982249917418812e-06 BETTER
I0328 16:58:04.810245 2314602 finetune.py:68] layer 8_k @ epoch 4 new loss 1.3422650226857513e-06 old loss 1.3434909078569035e-06 BETTER
I0328 16:58:13.293349 2314672 finetune.py:68] layer 9_k @ epoch 3 new loss 1.3943285921413917e-06 old loss 1.4002421266923193e-06 BETTER
I0328 16:58:13.678061 2314812 finetune.py:45] layer 11_k initial loss 1.7721710037221783e-06
I0328 16:58:18.018023 2314742 finetune.py:68] layer 10_k @ epoch 1 new loss 1.8545538296166342e-06 old loss 1.8616083252709359e-06 BETTER
I0328 16:58:23.976537 2314602 finetune.py:45] layer 8_o initial loss 3.064851398448809e-06
I0328 16:58:46.568435 2314812 finetune.py:68] layer 11_k @ epoch 0 new loss 1.756611823111598e-06 old loss 1.7721710037221783e-06 BETTER
I0328 16:58:48.132693 2314672 finetune.py:68] layer 9_k @ epoch 4 new loss 1.3805296248392551e-06 old loss 1.3943285921413917e-06 BETTER
I0328 16:58:52.633748 2314742 finetune.py:68] layer 10_k @ epoch 2 new loss 1.8458205204296974e-06 old loss 1.8545538296166342e-06 BETTER
I0328 16:58:58.466736 2314602 finetune.py:68] layer 8_o @ epoch 0 new loss 2.9976397399877897e-06 old loss 3.064851398448809e-06 BETTER
I0328 16:59:07.579089 2314672 finetune.py:45] layer 9_o initial loss 3.1842089356359793e-06
I0328 16:59:20.476909 2314812 finetune.py:68] layer 11_k @ epoch 1 new loss 1.7559060552230221e-06 old loss 1.756611823111598e-06 BETTER
I0328 16:59:27.169126 2314742 finetune.py:68] layer 10_k @ epoch 3 new loss 1.841771563704242e-06 old loss 1.8458205204296974e-06 BETTER
I0328 16:59:34.242895 2314602 finetune.py:68] layer 8_o @ epoch 1 new loss 2.9609982448164374e-06 old loss 2.9976397399877897e-06 BETTER
I0328 16:59:40.319726 2314672 finetune.py:68] layer 9_o @ epoch 0 new loss 3.103492417722009e-06 old loss 3.1842089356359793e-06 BETTER
I0328 16:59:54.210581 2314812 finetune.py:76] layer 11_k @ epoch 2 new loss 1.774972588464152e-06 old loss 1.7559060552230221e-06 WORSE
I0328 17:00:01.797533 2314742 finetune.py:68] layer 10_k @ epoch 4 new loss 1.8287170178155066e-06 old loss 1.841771563704242e-06 BETTER
I0328 17:00:10.052399 2314602 finetune.py:68] layer 8_o @ epoch 2 new loss 2.932907818831154e-06 old loss 2.9609982448164374e-06 BETTER
I0328 17:00:13.860458 2314672 finetune.py:68] layer 9_o @ epoch 1 new loss 3.05688263324555e-06 old loss 3.103492417722009e-06 BETTER
I0328 17:00:21.327994 2314742 finetune.py:45] layer 10_o initial loss 4.057631485920865e-06
I0328 17:00:27.512316 2314812 finetune.py:68] layer 11_k @ epoch 3 new loss 1.753906303747499e-06 old loss 1.7559060552230221e-06 BETTER
I0328 17:00:46.105897 2314602 finetune.py:68] layer 8_o @ epoch 3 new loss 2.908939904955332e-06 old loss 2.932907818831154e-06 BETTER
I0328 17:00:47.510025 2314672 finetune.py:68] layer 9_o @ epoch 2 new loss 3.0215815058909357e-06 old loss 3.05688263324555e-06 BETTER
I0328 17:00:54.220548 2314742 finetune.py:68] layer 10_o @ epoch 0 new loss 3.925418695871485e-06 old loss 4.057631485920865e-06 BETTER
I0328 17:01:01.484988 2314812 finetune.py:68] layer 11_k @ epoch 4 new loss 1.7334625681542093e-06 old loss 1.753906303747499e-06 BETTER
I0328 17:01:21.002544 2314812 finetune.py:45] layer 11_o initial loss 4.036620339320507e-06
I0328 17:01:21.351183 2314672 finetune.py:68] layer 9_o @ epoch 3 new loss 2.993118869198952e-06 old loss 3.0215815058909357e-06 BETTER
I0328 17:01:22.086527 2314602 finetune.py:68] layer 8_o @ epoch 4 new loss 2.8881593152618734e-06 old loss 2.908939904955332e-06 BETTER
I0328 17:01:28.048218 2314742 finetune.py:68] layer 10_o @ epoch 1 new loss 3.860846845782362e-06 old loss 3.925418695871485e-06 BETTER
I0328 17:01:53.084770 2314602 finetune.py:45] layer 8_up initial loss 5.630676241707988e-06
I0328 17:01:53.150728 2314812 finetune.py:68] layer 11_o @ epoch 0 new loss 3.916948571713874e-06 old loss 4.036620339320507e-06 BETTER
I0328 17:01:55.312700 2314672 finetune.py:68] layer 9_o @ epoch 4 new loss 2.9685782010346884e-06 old loss 2.993118869198952e-06 BETTER
I0328 17:02:02.016349 2314742 finetune.py:68] layer 10_o @ epoch 2 new loss 3.8115749703138135e-06 old loss 3.860846845782362e-06 BETTER
I0328 17:02:24.967828 2314602 finetune.py:68] layer 8_up @ epoch 0 new loss 5.56441409571562e-06 old loss 5.630676241707988e-06 BETTER
I0328 17:02:26.515592 2314812 finetune.py:68] layer 11_o @ epoch 1 new loss 3.850660505122505e-06 old loss 3.916948571713874e-06 BETTER
I0328 17:02:26.634966 2314672 finetune.py:45] layer 9_up initial loss 5.968598088657018e-06
I0328 17:02:35.840415 2314742 finetune.py:68] layer 10_o @ epoch 3 new loss 3.772332547669066e-06 old loss 3.8115749703138135e-06 BETTER
I0328 17:02:57.244083 2314672 finetune.py:68] layer 9_up @ epoch 0 new loss 5.889479325560387e-06 old loss 5.968598088657018e-06 BETTER
I0328 17:02:58.720107 2314602 finetune.py:68] layer 8_up @ epoch 1 new loss 5.51145330973668e-06 old loss 5.56441409571562e-06 BETTER
I0328 17:02:59.946462 2314812 finetune.py:68] layer 11_o @ epoch 2 new loss 3.8017660699551925e-06 old loss 3.850660505122505e-06 BETTER
I0328 17:03:09.999274 2314742 finetune.py:68] layer 10_o @ epoch 4 new loss 3.738613713721861e-06 old loss 3.772332547669066e-06 BETTER
I0328 17:03:28.682096 2314672 finetune.py:68] layer 9_up @ epoch 1 new loss 5.826423148391768e-06 old loss 5.889479325560387e-06 BETTER
I0328 17:03:32.473179 2314602 finetune.py:68] layer 8_up @ epoch 2 new loss 5.464675268740393e-06 old loss 5.51145330973668e-06 BETTER
I0328 17:03:33.318989 2314812 finetune.py:68] layer 11_o @ epoch 3 new loss 3.7625611639668932e-06 old loss 3.8017660699551925e-06 BETTER
I0328 17:03:41.601722 2314742 finetune.py:45] layer 10_up initial loss 6.81169831295847e-06
I0328 17:04:00.640259 2314672 finetune.py:68] layer 9_up @ epoch 2 new loss 5.771848009317182e-06 old loss 5.826423148391768e-06 BETTER
I0328 17:04:06.100857 2314602 finetune.py:68] layer 8_up @ epoch 3 new loss 5.422807589638978e-06 old loss 5.464675268740393e-06 BETTER
I0328 17:04:06.793957 2314812 finetune.py:68] layer 11_o @ epoch 4 new loss 3.729454192580306e-06 old loss 3.7625611639668932e-06 BETTER
I0328 17:04:12.291966 2314742 finetune.py:68] layer 10_up @ epoch 0 new loss 6.724601917085238e-06 old loss 6.81169831295847e-06 BETTER
I0328 17:04:32.634238 2314672 finetune.py:68] layer 9_up @ epoch 3 new loss 5.722180503653362e-06 old loss 5.771848009317182e-06 BETTER
I0328 17:04:38.398613 2314812 finetune.py:45] layer 11_up initial loss 6.986211246839957e-06
I0328 17:04:39.714286 2314602 finetune.py:68] layer 8_up @ epoch 4 new loss 5.383819370763376e-06 old loss 5.422807589638978e-06 BETTER
I0328 17:04:44.107119 2314742 finetune.py:68] layer 10_up @ epoch 1 new loss 6.655496690655127e-06 old loss 6.724601917085238e-06 BETTER
I0328 17:05:04.720478 2314672 finetune.py:68] layer 9_up @ epoch 4 new loss 5.677531589753926e-06 old loss 5.722180503653362e-06 BETTER
I0328 17:05:08.484890 2314812 finetune.py:68] layer 11_up @ epoch 0 new loss 6.889973519719206e-06 old loss 6.986211246839957e-06 BETTER
I0328 17:05:10.785255 2314602 finetune.py:45] layer 8_gate initial loss 6.37653920421144e-06
I0328 17:05:16.238424 2314742 finetune.py:68] layer 10_up @ epoch 2 new loss 6.5962208282144275e-06 old loss 6.655496690655127e-06 BETTER
I0328 17:05:35.992864 2314672 finetune.py:45] layer 9_gate initial loss 6.746296548953978e-06
I0328 17:05:39.599246 2314812 finetune.py:68] layer 11_up @ epoch 1 new loss 6.813736490585143e-06 old loss 6.889973519719206e-06 BETTER
I0328 17:05:40.867858 2314602 finetune.py:68] layer 8_gate @ epoch 0 new loss 6.3391512412636075e-06 old loss 6.37653920421144e-06 BETTER
I0328 17:05:48.469519 2314742 finetune.py:68] layer 10_up @ epoch 3 new loss 6.542007668031147e-06 old loss 6.5962208282144275e-06 BETTER
I0328 17:06:04.395609 2314672 finetune.py:68] layer 9_gate @ epoch 0 new loss 6.702697646687739e-06 old loss 6.746296548953978e-06 BETTER
I0328 17:06:10.848125 2314812 finetune.py:68] layer 11_up @ epoch 2 new loss 6.747743100277148e-06 old loss 6.813736490585143e-06 BETTER
I0328 17:06:12.041935 2314602 finetune.py:68] layer 8_gate @ epoch 1 new loss 6.305710030574119e-06 old loss 6.3391512412636075e-06 BETTER
I0328 17:06:20.604078 2314742 finetune.py:68] layer 10_up @ epoch 4 new loss 6.493064120149938e-06 old loss 6.542007668031147e-06 BETTER
I0328 17:06:33.667546 2314672 finetune.py:68] layer 9_gate @ epoch 1 new loss 6.664414740953362e-06 old loss 6.702697646687739e-06 BETTER
I0328 17:06:42.082847 2314812 finetune.py:68] layer 11_up @ epoch 3 new loss 6.688509529340081e-06 old loss 6.747743100277148e-06 BETTER
I0328 17:06:43.305990 2314602 finetune.py:68] layer 8_gate @ epoch 2 new loss 6.2746644289291e-06 old loss 6.305710030574119e-06 BETTER
I0328 17:06:52.422443 2314742 finetune.py:45] layer 10_gate initial loss 7.667154932278208e-06
I0328 17:07:03.253805 2314672 finetune.py:68] layer 9_gate @ epoch 2 new loss 6.628991286561359e-06 old loss 6.664414740953362e-06 BETTER
I0328 17:07:13.807848 2314812 finetune.py:68] layer 11_up @ epoch 4 new loss 6.634509190917015e-06 old loss 6.688509529340081e-06 BETTER
I0328 17:07:15.042694 2314602 finetune.py:68] layer 8_gate @ epoch 3 new loss 6.245990789466305e-06 old loss 6.2746644289291e-06 BETTER
I0328 17:07:21.135772 2314742 finetune.py:68] layer 10_gate @ epoch 0 new loss 7.619820735271787e-06 old loss 7.667154932278208e-06 BETTER
I0328 17:07:33.038899 2314672 finetune.py:68] layer 9_gate @ epoch 3 new loss 6.595949798793299e-06 old loss 6.628991286561359e-06 BETTER
I0328 17:07:45.140551 2314812 finetune.py:45] layer 11_gate initial loss 7.89489240560215e-06
I0328 17:07:46.599662 2314602 finetune.py:68] layer 8_gate @ epoch 4 new loss 6.218474027264165e-06 old loss 6.245990789466305e-06 BETTER
I0328 17:07:50.753554 2314742 finetune.py:68] layer 10_gate @ epoch 1 new loss 7.578129043395165e-06 old loss 7.619820735271787e-06 BETTER
I0328 17:08:02.690424 2314672 finetune.py:68] layer 9_gate @ epoch 4 new loss 6.565025159943616e-06 old loss 6.595949798793299e-06 BETTER
I0328 17:08:13.067096 2314812 finetune.py:68] layer 11_gate @ epoch 0 new loss 7.843328603485133e-06 old loss 7.89489240560215e-06 BETTER
I0328 17:08:20.438987 2314742 finetune.py:68] layer 10_gate @ epoch 2 new loss 7.539604212070117e-06 old loss 7.578129043395165e-06 BETTER
I0328 17:08:41.629536 2314602 finetune.py:45] layer 8_down initial loss 9.591495654603932e-06
I0328 17:08:42.022905 2314812 finetune.py:68] layer 11_gate @ epoch 1 new loss 7.797787475283258e-06 old loss 7.843328603485133e-06 BETTER
I0328 17:08:50.275645 2314742 finetune.py:68] layer 10_gate @ epoch 3 new loss 7.504323093598941e-06 old loss 7.539604212070117e-06 BETTER
I0328 17:08:59.326999 2314672 finetune.py:45] layer 9_down initial loss 1.0245686098642182e-05
I0328 17:09:08.922234 2314602 finetune.py:68] layer 8_down @ epoch 0 new loss 9.591250091034453e-06 old loss 9.591495654603932e-06 BETTER
I0328 17:09:11.393925 2314812 finetune.py:68] layer 11_gate @ epoch 2 new loss 7.756311788398307e-06 old loss 7.797787475283258e-06 BETTER
I0328 17:09:20.130779 2314742 finetune.py:68] layer 10_gate @ epoch 4 new loss 7.4705094448290765e-06 old loss 7.504323093598941e-06 BETTER
I0328 17:09:25.406260 2314672 finetune.py:68] layer 9_down @ epoch 0 new loss 1.0245426892652176e-05 old loss 1.0245686098642182e-05 BETTER
I0328 17:09:37.318770 2314602 finetune.py:68] layer 8_down @ epoch 1 new loss 9.591074558557011e-06 old loss 9.591250091034453e-06 BETTER
I0328 17:09:40.722898 2314812 finetune.py:68] layer 11_gate @ epoch 3 new loss 7.717270818830002e-06 old loss 7.756311788398307e-06 BETTER
I0328 17:09:52.473541 2314672 finetune.py:68] layer 9_down @ epoch 1 new loss 1.0245303201372735e-05 old loss 1.0245426892652176e-05 BETTER
I0328 17:10:06.032345 2314602 finetune.py:68] layer 8_down @ epoch 2 new loss 9.590977242623921e-06 old loss 9.591074558557011e-06 BETTER
I0328 17:10:09.901967 2314812 finetune.py:68] layer 11_gate @ epoch 4 new loss 7.681286660954356e-06 old loss 7.717270818830002e-06 BETTER
I0328 17:10:17.256826 2314742 finetune.py:45] layer 10_down initial loss 1.1255691788392141e-05
I0328 17:10:19.826625 2314672 finetune.py:68] layer 9_down @ epoch 2 new loss 1.024521861836547e-05 old loss 1.0245303201372735e-05 BETTER
I0328 17:10:34.897366 2314602 finetune.py:68] layer 8_down @ epoch 3 new loss 9.59089993557427e-06 old loss 9.590977242623921e-06 BETTER
I0328 17:10:43.456943 2314742 finetune.py:68] layer 10_down @ epoch 0 new loss 1.1255466233706102e-05 old loss 1.1255691788392141e-05 BETTER
I0328 17:10:47.285432 2314672 finetune.py:68] layer 9_down @ epoch 3 new loss 1.024515677272575e-05 old loss 1.024521861836547e-05 BETTER
I0328 17:11:03.667484 2314602 finetune.py:68] layer 8_down @ epoch 4 new loss 9.590849913365673e-06 old loss 9.59089993557427e-06 BETTER
8_v proxy err 0.002474329899996519 tr(WHW.T) 257.7052307128906
bpp_loss 4.308177842292935
8_q proxy err 0.0001029288541758433 tr(WHW.T) 26603.361328125
bpp_loss 5.207951346994378
8_k proxy err 3.415344326640479e-05 tr(WHW.T) 22523.720703125
bpp_loss 6.290508383186534
8_o proxy err 0.002335046185180545 tr(WHW.T) 746.6843872070312
bpp_loss 4.432402208680287
8_up proxy err 0.0015271601732820272 tr(WHW.T) 8494.78125
bpp_loss 4.550198572288666
8_gate proxy err 0.00035543148987926543 tr(WHW.T) 37232.41796875
bpp_loss 4.877932964890663
8_down proxy err 0.0019628810696303844 tr(WHW.T) 6491.89892578125
bpp_loss 4.555450307804027
I0328 17:11:07.363082 2314812 finetune.py:45] layer 11_down initial loss 1.1597446246014442e-05
I0328 17:11:11.487844 2314742 finetune.py:68] layer 10_down @ epoch 1 new loss 1.1255303434154484e-05 old loss 1.1255466233706102e-05 BETTER
I0328 17:11:15.422931 2314672 finetune.py:68] layer 9_down @ epoch 4 new loss 1.0245085832139011e-05 old loss 1.024515677272575e-05 BETTER
9_v proxy err 0.0018439892446622252 tr(WHW.T) 351.4288024902344
bpp_loss 4.410176564706489
9_q proxy err 0.00010804149496834725 tr(WHW.T) 25656.904296875
bpp_loss 5.219947813078761
9_k proxy err 3.750847827177495e-05 tr(WHW.T) 20943.169921875
bpp_loss 6.312096499605104
9_o proxy err 0.0020792351569980383 tr(WHW.T) 777.1723022460938
bpp_loss 4.488775055273436
9_up proxy err 0.0014446554705500603 tr(WHW.T) 8969.431640625
bpp_loss 4.560063810819494
9_gate proxy err 0.0003359187103342265 tr(WHW.T) 39396.53125
bpp_loss 4.89530083324228
9_down proxy err 0.00194425112567842 tr(WHW.T) 6276.451171875
bpp_loss 4.557498976754557
I0328 17:11:33.432670 2314812 finetune.py:68] layer 11_down @ epoch 0 new loss 1.1597087905101944e-05 old loss 1.1597446246014442e-05 BETTER
I0328 17:11:39.197920 2314742 finetune.py:68] layer 10_down @ epoch 2 new loss 1.1255217032157816e-05 old loss 1.1255303434154484e-05 BETTER
I0328 17:12:00.238989 2314812 finetune.py:68] layer 11_down @ epoch 1 new loss 1.1596883268794045e-05 old loss 1.1597087905101944e-05 BETTER
I0328 17:12:06.927004 2314742 finetune.py:68] layer 10_down @ epoch 3 new loss 1.1255122444708832e-05 old loss 1.1255217032157816e-05 BETTER
I0328 17:12:27.208141 2314812 finetune.py:68] layer 11_down @ epoch 2 new loss 1.159677321993513e-05 old loss 1.1596883268794045e-05 BETTER
I0328 17:12:28.209131 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 67.72356724739075s
I0328 17:12:32.117602 2314882 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 17:12:32.117730 2314882 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 17:12:32.117777 2314882 utils.py:162] NumExpr defaulting to 16 threads.
I0328 17:12:32.526010 2314882 config.py:54] PyTorch version 2.6.0 available.
W0328 17:12:32.752536 2314882 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 17:12:33.418168 2314882 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 17:12:33.422162 2313906 quantize_finetune_llama.py:209] layer 13 gpu 1
I0328 17:12:33.436148 2314882 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 17:12:34.635108 2314742 finetune.py:76] layer 10_down @ epoch 4 new loss 1.125512699218234e-05 old loss 1.1255122444708832e-05 WORSE
10_v proxy err 0.002436369890347123 tr(WHW.T) 251.83889770507812
bpp_loss 4.298700176761486
10_q proxy err 0.00011324213846819475 tr(WHW.T) 23341.375
bpp_loss 5.226361204870045
10_k proxy err 3.865781400236301e-05 tr(WHW.T) 19742.53125
bpp_loss 6.317439530626871
10_o proxy err 0.0025535174645483494 tr(WHW.T) 681.4904174804688
bpp_loss 4.4245714822318405
10_up proxy err 0.0014227371430024505 tr(WHW.T) 9197.376953125
bpp_loss 4.576375863581363
10_gate proxy err 0.00035630003549158573 tr(WHW.T) 37384.00390625
bpp_loss 4.860856931523553
10_down proxy err 0.0018880608258768916 tr(WHW.T) 6528.16650390625
bpp_loss 4.573764441235523
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 17:12:50.609778 2314882 finetune.py:45] layer 12_v initial loss 2.384480694672675e-06
W0328 17:12:50.610170 2314882 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 17:12:54.285343 2314812 finetune.py:68] layer 11_down @ epoch 3 new loss 1.1596698641369585e-05 old loss 1.159677321993513e-05 BETTER
I0328 17:13:21.436579 2314812 finetune.py:68] layer 11_down @ epoch 4 new loss 1.1596635886235163e-05 old loss 1.1596698641369585e-05 BETTER
11_v proxy err 0.0020103936549276114 tr(WHW.T) 319.5691223144531
bpp_loss 4.302607957855798
11_q proxy err 0.0001235074596479535 tr(WHW.T) 22192.517578125
bpp_loss 5.152750860550441
11_k proxy err 4.2934414523188025e-05 tr(WHW.T) 18022.982421875
bpp_loss 6.310270801652223
11_o proxy err 0.002596920356154442 tr(WHW.T) 566.5255126953125
bpp_loss 4.4438584432937205
11_up proxy err 0.0013930454151704907 tr(WHW.T) 9193.64453125
bpp_loss 4.581995852225061
11_gate proxy err 0.0003533478593453765 tr(WHW.T) 36872.5234375
bpp_loss 4.840953430426972
11_down proxy err 0.0018065235344693065 tr(WHW.T) 6659.92724609375
bpp_loss 4.581564656225964
I0328 17:13:25.348981 2314882 finetune.py:68] layer 12_v @ epoch 0 new loss 1.7491801145297359e-06 old loss 2.384480694672675e-06 BETTER
I0328 17:13:42.514035 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 64.07697129249573s
I0328 17:13:46.189936 2314952 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 17:13:46.190037 2314952 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 17:13:46.190078 2314952 utils.py:162] NumExpr defaulting to 16 threads.
I0328 17:13:46.553421 2314952 config.py:54] PyTorch version 2.6.0 available.
W0328 17:13:46.744221 2314952 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 17:13:47.367754 2314952 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 17:13:47.371649 2313906 quantize_finetune_llama.py:209] layer 14 gpu 2
I0328 17:13:47.384944 2314952 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 17:14:01.665218 2314882 finetune.py:68] layer 12_v @ epoch 1 new loss 1.6622554994683014e-06 old loss 1.7491801145297359e-06 BETTER
I0328 17:14:04.247739 2314952 finetune.py:45] layer 13_v initial loss 2.412986077615642e-06
W0328 17:14:04.247922 2314952 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 17:14:37.368593 2314952 finetune.py:68] layer 13_v @ epoch 0 new loss 1.8555252836449654e-06 old loss 2.412986077615642e-06 BETTER
I0328 17:14:38.388852 2314882 finetune.py:68] layer 12_v @ epoch 2 new loss 1.610900881132693e-06 old loss 1.6622554994683014e-06 BETTER
I0328 17:14:51.721902 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 63.88088822364807s
I0328 17:14:55.470502 2315022 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 17:14:55.470612 2315022 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 17:14:55.470657 2315022 utils.py:162] NumExpr defaulting to 16 threads.
I0328 17:14:55.827429 2315022 config.py:54] PyTorch version 2.6.0 available.
W0328 17:14:56.036105 2315022 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 17:14:56.655636 2315022 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 17:14:56.660036 2313906 quantize_finetune_llama.py:209] layer 15 gpu 3
I0328 17:14:56.673461 2315022 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 17:15:11.860988 2314952 finetune.py:68] layer 13_v @ epoch 1 new loss 1.7663236349108047e-06 old loss 1.8555252836449654e-06 BETTER
I0328 17:15:14.191403 2315022 finetune.py:45] layer 14_v initial loss 2.679969156815787e-06
W0328 17:15:14.191657 2315022 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 17:15:15.195590 2314882 finetune.py:68] layer 12_v @ epoch 3 new loss 1.5990224255801877e-06 old loss 1.610900881132693e-06 BETTER
I0328 17:15:46.543228 2314952 finetune.py:68] layer 13_v @ epoch 2 new loss 1.7202685285155894e-06 old loss 1.7663236349108047e-06 BETTER
I0328 17:15:47.520934 2315022 finetune.py:68] layer 14_v @ epoch 0 new loss 2.0513923573162174e-06 old loss 2.679969156815787e-06 BETTER
I0328 17:15:52.097084 2314882 finetune.py:76] layer 12_v @ epoch 4 new loss 1.6182947319975938e-06 old loss 1.5990224255801877e-06 WORSE
I0328 17:16:00.063900 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 62.958045959472656s
I0328 17:16:03.920532 2315092 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 17:16:03.920636 2315092 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 17:16:03.920679 2315092 utils.py:162] NumExpr defaulting to 16 threads.
I0328 17:16:04.305991 2315092 config.py:54] PyTorch version 2.6.0 available.
W0328 17:16:04.535619 2315092 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 17:16:05.166332 2315092 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 17:16:05.170114 2313906 quantize_finetune_llama.py:209] layer 16 gpu 0
I0328 17:16:05.183773 2315092 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 17:16:10.898120 2314882 finetune.py:45] layer 12_q initial loss 1.8852042558137327e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 17:16:21.419767 2314952 finetune.py:68] layer 13_v @ epoch 3 new loss 1.6863515384102357e-06 old loss 1.7202685285155894e-06 BETTER
I0328 17:16:22.078148 2315022 finetune.py:68] layer 14_v @ epoch 1 new loss 1.9509554931573803e-06 old loss 2.0513923573162174e-06 BETTER
I0328 17:16:22.290006 2315092 finetune.py:45] layer 15_v initial loss 3.4887075344158802e-06
W0328 17:16:22.290432 2315092 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 17:16:46.112107 2314882 finetune.py:68] layer 12_q @ epoch 0 new loss 1.820654119910614e-06 old loss 1.8852042558137327e-06 BETTER
I0328 17:16:55.436088 2315092 finetune.py:68] layer 15_v @ epoch 0 new loss 2.2654278382105986e-06 old loss 3.4887075344158802e-06 BETTER
I0328 17:16:56.606200 2314952 finetune.py:68] layer 13_v @ epoch 4 new loss 1.6779267753008753e-06 old loss 1.6863515384102357e-06 BETTER
I0328 17:16:56.906497 2315022 finetune.py:68] layer 14_v @ epoch 2 new loss 1.8953835478896508e-06 old loss 1.9509554931573803e-06 BETTER
I0328 17:17:16.224197 2314952 finetune.py:45] layer 13_q initial loss 2.0198415313643636e-06
I0328 17:17:22.355295 2314882 finetune.py:68] layer 12_q @ epoch 1 new loss 1.7969597365663503e-06 old loss 1.820654119910614e-06 BETTER
I0328 17:17:29.629016 2315092 finetune.py:68] layer 15_v @ epoch 1 new loss 2.135722070306656e-06 old loss 2.2654278382105986e-06 BETTER
I0328 17:17:31.830090 2315022 finetune.py:68] layer 14_v @ epoch 3 new loss 1.8606724552228115e-06 old loss 1.8953835478896508e-06 BETTER
I0328 17:17:49.637045 2314952 finetune.py:68] layer 13_q @ epoch 0 new loss 1.982623871299438e-06 old loss 2.0198415313643636e-06 BETTER
I0328 17:17:58.814306 2314882 finetune.py:68] layer 12_q @ epoch 2 new loss 1.7733405002218205e-06 old loss 1.7969597365663503e-06 BETTER
I0328 17:18:03.948220 2315092 finetune.py:68] layer 15_v @ epoch 2 new loss 2.068821459033643e-06 old loss 2.135722070306656e-06 BETTER
I0328 17:18:06.893477 2315022 finetune.py:76] layer 14_v @ epoch 4 new loss 1.8688665477384347e-06 old loss 1.8606724552228115e-06 WORSE
I0328 17:18:23.947868 2314952 finetune.py:68] layer 13_q @ epoch 1 new loss 1.9534738839865895e-06 old loss 1.982623871299438e-06 BETTER
I0328 17:18:26.110610 2315022 finetune.py:45] layer 14_q initial loss 2.3288471311389003e-06
I0328 17:18:35.285693 2314882 finetune.py:68] layer 12_q @ epoch 3 new loss 1.7433571883884724e-06 old loss 1.7733405002218205e-06 BETTER
I0328 17:18:38.440744 2315092 finetune.py:68] layer 15_v @ epoch 3 new loss 2.04428374672716e-06 old loss 2.068821459033643e-06 BETTER
I0328 17:18:58.514607 2314952 finetune.py:68] layer 13_q @ epoch 2 new loss 1.9299347968626535e-06 old loss 1.9534738839865895e-06 BETTER
I0328 17:18:59.717758 2315022 finetune.py:68] layer 14_q @ epoch 0 new loss 2.2728604562871624e-06 old loss 2.3288471311389003e-06 BETTER
I0328 17:19:11.924155 2314882 finetune.py:68] layer 12_q @ epoch 4 new loss 1.7331244634988252e-06 old loss 1.7433571883884724e-06 BETTER
I0328 17:19:12.991443 2315092 finetune.py:68] layer 15_v @ epoch 4 new loss 2.0371687696751906e-06 old loss 2.04428374672716e-06 BETTER
I0328 17:19:29.734729 2314882 finetune.py:45] layer 12_k initial loss 1.838742150539474e-06
I0328 17:19:32.407272 2315092 finetune.py:45] layer 15_q initial loss 2.318897941222531e-06
I0328 17:19:33.018264 2314952 finetune.py:68] layer 13_q @ epoch 3 new loss 1.911378603836056e-06 old loss 1.9299347968626535e-06 BETTER
I0328 17:19:34.304528 2315022 finetune.py:68] layer 14_q @ epoch 1 new loss 2.2278552478383062e-06 old loss 2.2728604562871624e-06 BETTER
I0328 17:20:05.039689 2314882 finetune.py:68] layer 12_k @ epoch 0 new loss 1.81220309514174e-06 old loss 1.838742150539474e-06 BETTER
I0328 17:20:05.436133 2315092 finetune.py:68] layer 15_q @ epoch 0 new loss 2.2384876956493827e-06 old loss 2.318897941222531e-06 BETTER
I0328 17:20:07.650558 2314952 finetune.py:68] layer 13_q @ epoch 4 new loss 1.9011787344425102e-06 old loss 1.911378603836056e-06 BETTER
I0328 17:20:09.055440 2315022 finetune.py:68] layer 14_q @ epoch 2 new loss 2.22195012611337e-06 old loss 2.2278552478383062e-06 BETTER
I0328 17:20:25.442919 2314952 finetune.py:45] layer 13_k initial loss 1.9946453448937973e-06
I0328 17:20:39.646584 2315092 finetune.py:68] layer 15_q @ epoch 1 new loss 2.205296823376557e-06 old loss 2.2384876956493827e-06 BETTER
I0328 17:20:41.628741 2314882 finetune.py:68] layer 12_k @ epoch 1 new loss 1.798105358830071e-06 old loss 1.81220309514174e-06 BETTER
I0328 17:20:43.839481 2315022 finetune.py:68] layer 14_q @ epoch 3 new loss 2.1810187718074303e-06 old loss 2.22195012611337e-06 BETTER
I0328 17:20:59.109740 2314952 finetune.py:76] layer 13_k @ epoch 0 new loss 1.994768808799563e-06 old loss 1.9946453448937973e-06 WORSE
I0328 17:21:13.878155 2315092 finetune.py:68] layer 15_q @ epoch 2 new loss 2.177995611418737e-06 old loss 2.205296823376557e-06 BETTER
I0328 17:21:18.294294 2314882 finetune.py:68] layer 12_k @ epoch 2 new loss 1.786622078725486e-06 old loss 1.798105358830071e-06 BETTER
I0328 17:21:18.622413 2315022 finetune.py:68] layer 14_q @ epoch 4 new loss 2.1657272100128466e-06 old loss 2.1810187718074303e-06 BETTER
I0328 17:21:32.825851 2314952 finetune.py:68] layer 13_k @ epoch 1 new loss 1.97388862943626e-06 old loss 1.9946453448937973e-06 BETTER
I0328 17:21:36.914264 2315022 finetune.py:45] layer 14_k initial loss 2.3269508346857037e-06
I0328 17:21:48.263004 2315092 finetune.py:68] layer 15_q @ epoch 3 new loss 2.159881887564552e-06 old loss 2.177995611418737e-06 BETTER
I0328 17:21:54.878376 2314882 finetune.py:68] layer 12_k @ epoch 3 new loss 1.7781642327463487e-06 old loss 1.786622078725486e-06 BETTER
I0328 17:22:07.341861 2314952 finetune.py:68] layer 13_k @ epoch 2 new loss 1.9620065359049477e-06 old loss 1.97388862943626e-06 BETTER
I0328 17:22:10.559129 2315022 finetune.py:68] layer 14_k @ epoch 0 new loss 2.294810656167101e-06 old loss 2.3269508346857037e-06 BETTER
I0328 17:22:22.432467 2315092 finetune.py:68] layer 15_q @ epoch 4 new loss 2.1540879515669076e-06 old loss 2.159881887564552e-06 BETTER
I0328 17:22:31.892284 2314882 finetune.py:68] layer 12_k @ epoch 4 new loss 1.7731983916746685e-06 old loss 1.7781642327463487e-06 BETTER
I0328 17:22:40.099917 2315092 finetune.py:45] layer 15_k initial loss 2.2802953481004806e-06
I0328 17:22:42.077879 2314952 finetune.py:68] layer 13_k @ epoch 3 new loss 1.9544690985640045e-06 old loss 1.9620065359049477e-06 BETTER
I0328 17:22:44.975441 2315022 finetune.py:68] layer 14_k @ epoch 1 new loss 2.274393182233325e-06 old loss 2.294810656167101e-06 BETTER
I0328 17:22:51.623168 2314882 finetune.py:45] layer 12_o initial loss 4.0384002204518765e-06
I0328 17:23:13.118018 2315092 finetune.py:68] layer 15_k @ epoch 0 new loss 2.2505023480334785e-06 old loss 2.2802953481004806e-06 BETTER
I0328 17:23:16.653728 2314952 finetune.py:68] layer 13_k @ epoch 4 new loss 1.9513049664965365e-06 old loss 1.9544690985640045e-06 BETTER
I0328 17:23:19.641899 2315022 finetune.py:68] layer 14_k @ epoch 2 new loss 2.2631593310507014e-06 old loss 2.274393182233325e-06 BETTER
I0328 17:23:26.396622 2314882 finetune.py:68] layer 12_o @ epoch 0 new loss 3.909643055521883e-06 old loss 4.0384002204518765e-06 BETTER
I0328 17:23:36.486393 2314952 finetune.py:45] layer 13_o initial loss 4.885570433543762e-06
I0328 17:23:47.028783 2315092 finetune.py:68] layer 15_k @ epoch 1 new loss 2.24542964133434e-06 old loss 2.2505023480334785e-06 BETTER
I0328 17:23:54.270909 2315022 finetune.py:68] layer 14_k @ epoch 3 new loss 2.2464137146016583e-06 old loss 2.2631593310507014e-06 BETTER
I0328 17:24:02.232776 2314882 finetune.py:68] layer 12_o @ epoch 1 new loss 3.836944415525068e-06 old loss 3.909643055521883e-06 BETTER
I0328 17:24:09.242287 2314952 finetune.py:68] layer 13_o @ epoch 0 new loss 4.7164003262878396e-06 old loss 4.885570433543762e-06 BETTER
I0328 17:24:21.273605 2315092 finetune.py:68] layer 15_k @ epoch 2 new loss 2.2246595108299516e-06 old loss 2.24542964133434e-06 BETTER
I0328 17:24:28.934741 2315022 finetune.py:76] layer 14_k @ epoch 4 new loss 2.2576191440748516e-06 old loss 2.2464137146016583e-06 WORSE
I0328 17:24:38.331414 2314882 finetune.py:68] layer 12_o @ epoch 2 new loss 3.785906756093027e-06 old loss 3.836944415525068e-06 BETTER
I0328 17:24:42.866833 2314952 finetune.py:68] layer 13_o @ epoch 1 new loss 4.62969501313637e-06 old loss 4.7164003262878396e-06 BETTER
I0328 17:24:48.075741 2315022 finetune.py:45] layer 14_o initial loss 5.379512458603131e-06
I0328 17:24:55.346895 2315092 finetune.py:76] layer 15_k @ epoch 3 new loss 2.237205990240909e-06 old loss 2.2246595108299516e-06 WORSE
I0328 17:25:14.456014 2314882 finetune.py:68] layer 12_o @ epoch 3 new loss 3.7425822938530473e-06 old loss 3.785906756093027e-06 BETTER
I0328 17:25:16.662757 2314952 finetune.py:68] layer 13_o @ epoch 2 new loss 4.5658207454835065e-06 old loss 4.62969501313637e-06 BETTER
I0328 17:25:21.070769 2315022 finetune.py:68] layer 14_o @ epoch 0 new loss 5.197509381105192e-06 old loss 5.379512458603131e-06 BETTER
I0328 17:25:28.671523 2315092 finetune.py:68] layer 15_k @ epoch 4 new loss 2.221882823505439e-06 old loss 2.2246595108299516e-06 BETTER
I0328 17:25:48.022212 2315092 finetune.py:45] layer 15_o initial loss 5.3796529755345546e-06
I0328 17:25:50.456151 2314882 finetune.py:68] layer 12_o @ epoch 4 new loss 3.7057357076264452e-06 old loss 3.7425822938530473e-06 BETTER
I0328 17:25:50.640875 2314952 finetune.py:68] layer 13_o @ epoch 3 new loss 4.514253760135034e-06 old loss 4.5658207454835065e-06 BETTER
I0328 17:25:54.832767 2315022 finetune.py:68] layer 14_o @ epoch 1 new loss 5.094113021186786e-06 old loss 5.197509381105192e-06 BETTER
I0328 17:26:20.272274 2315092 finetune.py:68] layer 15_o @ epoch 0 new loss 5.189309831621358e-06 old loss 5.3796529755345546e-06 BETTER
I0328 17:26:22.329475 2314882 finetune.py:45] layer 12_up initial loss 7.094119155226508e-06
I0328 17:26:24.355677 2314952 finetune.py:68] layer 13_o @ epoch 4 new loss 4.470471594686387e-06 old loss 4.514253760135034e-06 BETTER
I0328 17:26:28.804345 2315022 finetune.py:68] layer 14_o @ epoch 2 new loss 5.015665465180064e-06 old loss 5.094113021186786e-06 BETTER
I0328 17:26:53.480234 2315092 finetune.py:68] layer 15_o @ epoch 1 new loss 5.068912287242711e-06 old loss 5.189309831621358e-06 BETTER
I0328 17:26:54.168095 2314882 finetune.py:68] layer 12_up @ epoch 0 new loss 6.976482382015092e-06 old loss 7.094119155226508e-06 BETTER
I0328 17:26:55.521386 2314952 finetune.py:45] layer 13_up initial loss 8.338066436408553e-06
I0328 17:27:02.686554 2315022 finetune.py:68] layer 14_o @ epoch 3 new loss 4.953178631694755e-06 old loss 5.015665465180064e-06 BETTER
I0328 17:27:26.151313 2314952 finetune.py:68] layer 13_up @ epoch 0 new loss 8.195806003641337e-06 old loss 8.338066436408553e-06 BETTER
I0328 17:27:26.977849 2315092 finetune.py:68] layer 15_o @ epoch 2 new loss 4.989357876183931e-06 old loss 5.068912287242711e-06 BETTER
I0328 17:27:27.268592 2314882 finetune.py:68] layer 12_up @ epoch 1 new loss 6.8856616053381e-06 old loss 6.976482382015092e-06 BETTER
I0328 17:27:36.725322 2315022 finetune.py:68] layer 14_o @ epoch 4 new loss 4.900954991171602e-06 old loss 4.953178631694755e-06 BETTER
I0328 17:27:57.755857 2314952 finetune.py:68] layer 13_up @ epoch 1 new loss 8.087817150226329e-06 old loss 8.195806003641337e-06 BETTER
I0328 17:28:00.277313 2315092 finetune.py:68] layer 15_o @ epoch 3 new loss 4.923294000036549e-06 old loss 4.989357876183931e-06 BETTER
I0328 17:28:00.610441 2314882 finetune.py:68] layer 12_up @ epoch 2 new loss 6.807814315834548e-06 old loss 6.8856616053381e-06 BETTER
I0328 17:28:08.735159 2315022 finetune.py:45] layer 14_up initial loss 9.574117029842455e-06
I0328 17:28:29.680792 2314952 finetune.py:68] layer 13_up @ epoch 2 new loss 7.996319254743867e-06 old loss 8.087817150226329e-06 BETTER
I0328 17:28:33.578468 2315092 finetune.py:68] layer 15_o @ epoch 4 new loss 4.86990256831632e-06 old loss 4.923294000036549e-06 BETTER
I0328 17:28:34.281475 2314882 finetune.py:68] layer 12_up @ epoch 3 new loss 6.73935164741124e-06 old loss 6.807814315834548e-06 BETTER
I0328 17:28:39.386827 2315022 finetune.py:68] layer 14_up @ epoch 0 new loss 9.387943464389537e-06 old loss 9.574117029842455e-06 BETTER
I0328 17:29:01.925538 2314952 finetune.py:68] layer 13_up @ epoch 3 new loss 7.915464266261552e-06 old loss 7.996319254743867e-06 BETTER
I0328 17:29:04.864391 2315092 finetune.py:45] layer 15_up initial loss 1.0446447959111538e-05
I0328 17:29:07.841009 2314882 finetune.py:68] layer 12_up @ epoch 4 new loss 6.676954399154056e-06 old loss 6.73935164741124e-06 BETTER
I0328 17:29:11.217481 2315022 finetune.py:68] layer 14_up @ epoch 1 new loss 9.244365173799451e-06 old loss 9.387943464389537e-06 BETTER
I0328 17:29:34.090476 2314952 finetune.py:68] layer 13_up @ epoch 4 new loss 7.843117600714322e-06 old loss 7.915464266261552e-06 BETTER
I0328 17:29:35.015924 2315092 finetune.py:68] layer 15_up @ epoch 0 new loss 1.0199335520155728e-05 old loss 1.0446447959111538e-05 BETTER
I0328 17:29:39.573076 2314882 finetune.py:45] layer 12_gate initial loss 8.114763659250457e-06
I0328 17:29:43.503149 2315022 finetune.py:68] layer 14_up @ epoch 2 new loss 9.124343705479987e-06 old loss 9.244365173799451e-06 BETTER
I0328 17:30:05.592780 2314952 finetune.py:45] layer 13_gate initial loss 9.417819455848075e-06
I0328 17:30:06.202343 2315092 finetune.py:68] layer 15_up @ epoch 1 new loss 1.0014440704253502e-05 old loss 1.0199335520155728e-05 BETTER
I0328 17:30:09.708867 2314882 finetune.py:68] layer 12_gate @ epoch 0 new loss 8.05293439043453e-06 old loss 8.114763659250457e-06 BETTER
I0328 17:30:15.789740 2315022 finetune.py:68] layer 14_up @ epoch 3 new loss 9.020140169013757e-06 old loss 9.124343705479987e-06 BETTER
I0328 17:30:34.038850 2314952 finetune.py:68] layer 13_gate @ epoch 0 new loss 9.34434137889184e-06 old loss 9.417819455848075e-06 BETTER
I0328 17:30:37.539401 2315092 finetune.py:68] layer 15_up @ epoch 2 new loss 9.861417311185505e-06 old loss 1.0014440704253502e-05 BETTER
I0328 17:30:40.839325 2314882 finetune.py:68] layer 12_gate @ epoch 1 new loss 7.998827641131356e-06 old loss 8.05293439043453e-06 BETTER
I0328 17:30:48.012512 2315022 finetune.py:68] layer 14_up @ epoch 4 new loss 8.926615009841044e-06 old loss 9.020140169013757e-06 BETTER
I0328 17:31:03.652508 2314952 finetune.py:68] layer 13_gate @ epoch 1 new loss 9.27998644328909e-06 old loss 9.34434137889184e-06 BETTER
I0328 17:31:08.825562 2315092 finetune.py:68] layer 15_up @ epoch 3 new loss 9.728993973112665e-06 old loss 9.861417311185505e-06 BETTER
I0328 17:31:12.195413 2314882 finetune.py:68] layer 12_gate @ epoch 2 new loss 7.949872269819025e-06 old loss 7.998827641131356e-06 BETTER
I0328 17:31:20.090630 2315022 finetune.py:45] layer 14_gate initial loss 1.0623576599755324e-05
I0328 17:31:33.081944 2314952 finetune.py:68] layer 13_gate @ epoch 2 new loss 9.222627340932377e-06 old loss 9.27998644328909e-06 BETTER
I0328 17:31:40.143900 2315092 finetune.py:68] layer 15_up @ epoch 4 new loss 9.611519999452867e-06 old loss 9.728993973112665e-06 BETTER
I0328 17:31:43.724721 2314882 finetune.py:68] layer 12_gate @ epoch 3 new loss 7.904363883426413e-06 old loss 7.949872269819025e-06 BETTER
I0328 17:31:48.846402 2315022 finetune.py:68] layer 14_gate @ epoch 0 new loss 1.0531766747590154e-05 old loss 1.0623576599755324e-05 BETTER
I0328 17:32:02.863362 2314952 finetune.py:68] layer 13_gate @ epoch 3 new loss 9.168957149086054e-06 old loss 9.222627340932377e-06 BETTER
I0328 17:32:11.696753 2315092 finetune.py:45] layer 15_gate initial loss 1.1446506505308207e-05
I0328 17:32:15.285894 2314882 finetune.py:68] layer 12_gate @ epoch 4 new loss 7.86230248195352e-06 old loss 7.904363883426413e-06 BETTER
I0328 17:32:18.527646 2315022 finetune.py:68] layer 14_gate @ epoch 1 new loss 1.0452104106661864e-05 old loss 1.0531766747590154e-05 BETTER
I0328 17:32:32.542096 2314952 finetune.py:68] layer 13_gate @ epoch 4 new loss 9.12078485271195e-06 old loss 9.168957149086054e-06 BETTER
I0328 17:32:39.749590 2315092 finetune.py:68] layer 15_gate @ epoch 0 new loss 1.1330770576023497e-05 old loss 1.1446506505308207e-05 BETTER
I0328 17:32:48.330099 2315022 finetune.py:68] layer 14_gate @ epoch 2 new loss 1.0381569154560566e-05 old loss 1.0452104106661864e-05 BETTER
I0328 17:33:08.731960 2315092 finetune.py:68] layer 15_gate @ epoch 1 new loss 1.1229589290451258e-05 old loss 1.1330770576023497e-05 BETTER
I0328 17:33:11.306361 2314882 finetune.py:45] layer 12_down initial loss 1.1950040970987175e-05
I0328 17:33:18.173105 2315022 finetune.py:68] layer 14_gate @ epoch 3 new loss 1.0316227417206392e-05 old loss 1.0381569154560566e-05 BETTER
I0328 17:33:30.271938 2314952 finetune.py:45] layer 13_down initial loss 1.3984560609969776e-05
I0328 17:33:37.928483 2315092 finetune.py:68] layer 15_gate @ epoch 2 new loss 1.1140562492073514e-05 old loss 1.1229589290451258e-05 BETTER
I0328 17:33:38.683681 2314882 finetune.py:68] layer 12_down @ epoch 0 new loss 1.1949784493481275e-05 old loss 1.1950040970987175e-05 BETTER
I0328 17:33:48.115454 2315022 finetune.py:68] layer 14_gate @ epoch 4 new loss 1.0256508176098578e-05 old loss 1.0316227417206392e-05 BETTER
I0328 17:33:56.295877 2314952 finetune.py:68] layer 13_down @ epoch 0 new loss 1.3984125871502329e-05 old loss 1.3984560609969776e-05 BETTER
I0328 17:34:06.856301 2314882 finetune.py:68] layer 12_down @ epoch 1 new loss 1.1949604413530324e-05 old loss 1.1949784493481275e-05 BETTER
I0328 17:34:07.336455 2315092 finetune.py:68] layer 15_gate @ epoch 3 new loss 1.1059981261496432e-05 old loss 1.1140562492073514e-05 BETTER
I0328 17:34:23.334074 2314952 finetune.py:68] layer 13_down @ epoch 1 new loss 1.3983963071950711e-05 old loss 1.3984125871502329e-05 BETTER
I0328 17:34:35.661165 2314882 finetune.py:68] layer 12_down @ epoch 2 new loss 1.1949490726692602e-05 old loss 1.1949604413530324e-05 BETTER
I0328 17:34:36.869632 2315092 finetune.py:68] layer 15_gate @ epoch 4 new loss 1.0987004316120874e-05 old loss 1.1059981261496432e-05 BETTER
I0328 17:34:45.640679 2315022 finetune.py:45] layer 14_down initial loss 1.6105908798635937e-05
I0328 17:34:50.723936 2314952 finetune.py:68] layer 13_down @ epoch 2 new loss 1.398382391926134e-05 old loss 1.3983963071950711e-05 BETTER
I0328 17:35:04.703830 2314882 finetune.py:68] layer 12_down @ epoch 3 new loss 1.1949387953791302e-05 old loss 1.1949490726692602e-05 BETTER
I0328 17:35:11.902938 2315022 finetune.py:68] layer 14_down @ epoch 0 new loss 1.6105532267829403e-05 old loss 1.6105908798635937e-05 BETTER
I0328 17:35:18.340503 2314952 finetune.py:68] layer 13_down @ epoch 3 new loss 1.3983719327370636e-05 old loss 1.398382391926134e-05 BETTER
I0328 17:35:33.173399 2315092 finetune.py:45] layer 15_down initial loss 1.8372395061305724e-05
I0328 17:35:33.766445 2314882 finetune.py:68] layer 12_down @ epoch 4 new loss 1.1949324289162178e-05 old loss 1.1949387953791302e-05 BETTER
12_v proxy err 0.001855991780757904 tr(WHW.T) 363.6233825683594
bpp_loss 4.419650851632468
12_q proxy err 8.425769192399457e-05 tr(WHW.T) 34105.62890625
bpp_loss 5.2111971186241135
12_k proxy err 3.4196422348031774e-05 tr(WHW.T) 23053.732421875
bpp_loss 6.296369801391847
12_o proxy err 0.0020960262045264244 tr(WHW.T) 780.5471801757812
bpp_loss 4.489795292727649
12_up proxy err 0.0012514855479821563 tr(WHW.T) 10027.2177734375
bpp_loss 4.602494036047054
12_gate proxy err 0.0003413758531678468 tr(WHW.T) 37312.61328125
bpp_loss 4.823449933901429
12_down proxy err 0.0017058522207662463 tr(WHW.T) 6825.703125
bpp_loss 4.59488744814215
I0328 17:35:39.433837 2315022 finetune.py:68] layer 14_down @ epoch 1 new loss 1.610525396245066e-05 old loss 1.6105532267829403e-05 BETTER
I0328 17:35:46.289238 2314952 finetune.py:68] layer 13_down @ epoch 4 new loss 1.398362928739516e-05 old loss 1.3983719327370636e-05 BETTER
13_v proxy err 0.0022853852715343237 tr(WHW.T) 280.153564453125
bpp_loss 4.360516218119301
13_q proxy err 0.0001310208608629182 tr(WHW.T) 20899.30078125
bpp_loss 5.190206807106733
13_k proxy err 4.617958620656282e-05 tr(WHW.T) 17797.5390625
bpp_loss 6.331792083685286
13_o proxy err 0.0022209137678146362 tr(WHW.T) 675.6954956054688
bpp_loss 4.47127405484207
13_up proxy err 0.0012272716267034411 tr(WHW.T) 10009.7998046875
bpp_loss 4.605752420678202
13_gate proxy err 0.00032043876126408577 tr(WHW.T) 38924.296875
bpp_loss 4.827492141963115
13_down proxy err 0.0017428253777325153 tr(WHW.T) 6563.0908203125
bpp_loss 4.594352634656908
I0328 17:35:59.134742 2315092 finetune.py:68] layer 15_down @ epoch 0 new loss 1.837191666709259e-05 old loss 1.8372395061305724e-05 BETTER
I0328 17:36:07.585852 2315022 finetune.py:68] layer 14_down @ epoch 2 new loss 1.6105112081277184e-05 old loss 1.610525396245066e-05 BETTER
I0328 17:36:26.119038 2315092 finetune.py:68] layer 15_down @ epoch 1 new loss 1.8371749320067465e-05 old loss 1.837191666709259e-05 BETTER
I0328 17:36:35.407860 2315022 finetune.py:68] layer 14_down @ epoch 3 new loss 1.6104999303934164e-05 old loss 1.6105112081277184e-05 BETTER
I0328 17:36:50.924835 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 59.68269062042236s
I0328 17:36:53.127677 2315092 finetune.py:68] layer 15_down @ epoch 2 new loss 1.837162380979862e-05 old loss 1.8371749320067465e-05 BETTER
I0328 17:36:54.922709 2315162 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 17:36:54.922820 2315162 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 17:36:54.922868 2315162 utils.py:162] NumExpr defaulting to 16 threads.
I0328 17:36:55.293588 2315162 config.py:54] PyTorch version 2.6.0 available.
W0328 17:36:55.498816 2315162 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 17:36:56.223247 2315162 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 17:36:56.227225 2313906 quantize_finetune_llama.py:209] layer 17 gpu 1
I0328 17:36:56.243653 2315162 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 17:37:03.229890 2315022 finetune.py:68] layer 14_down @ epoch 4 new loss 1.6104937458294444e-05 old loss 1.6104999303934164e-05 BETTER
14_v proxy err 0.0021551107056438923 tr(WHW.T) 281.3382873535156
bpp_loss 4.353213366004638
14_q proxy err 0.00012428173795342445 tr(WHW.T) 20883.20703125
bpp_loss 5.154205575003289
14_k proxy err 4.22889570472762e-05 tr(WHW.T) 18614.75
bpp_loss 6.272621858632192
14_o proxy err 0.0023721768520772457 tr(WHW.T) 686.9353637695312
bpp_loss 4.464042913285084
14_up proxy err 0.0013352654641494155 tr(WHW.T) 9162.9267578125
bpp_loss 4.6001967360664695
14_gate proxy err 0.0002981770085170865 tr(WHW.T) 41786.66796875
bpp_loss 4.862907635846308
14_down proxy err 0.0018440759740769863 tr(WHW.T) 6393.4072265625
bpp_loss 4.590130323377837
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 17:37:13.485841 2315162 finetune.py:45] layer 16_v initial loss 4.1309558582725e-06
W0328 17:37:13.486030 2315162 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 17:37:20.177795 2315092 finetune.py:68] layer 15_down @ epoch 3 new loss 1.8371481928625144e-05 old loss 1.837162380979862e-05 BETTER
I0328 17:37:47.341018 2315092 finetune.py:68] layer 15_down @ epoch 4 new loss 1.8371378246229142e-05 old loss 1.8371481928625144e-05 BETTER
I0328 17:37:48.432349 2315162 finetune.py:68] layer 16_v @ epoch 0 new loss 2.5074421046156203e-06 old loss 4.1309558582725e-06 BETTER
15_v proxy err 0.0023648166097700596 tr(WHW.T) 284.0271301269531
bpp_loss 4.416905992780812
15_q proxy err 0.00010408491652924567 tr(WHW.T) 28082.21875
bpp_loss 5.314353496301919
15_k proxy err 4.4985030399402604e-05 tr(WHW.T) 18869.388671875
bpp_loss 6.315119479666464
15_o proxy err 0.0024141024332493544 tr(WHW.T) 828.5297241210938
bpp_loss 4.4940980488900095
15_up proxy err 0.001360514434054494 tr(WHW.T) 8993.9248046875
bpp_loss 4.591806015997593
15_gate proxy err 0.00027059632702730596 tr(WHW.T) 46174.75
bpp_loss 4.900613739155233
15_down proxy err 0.0018505898769944906 tr(WHW.T) 6403.47900390625
bpp_loss 4.584220445847937
I0328 17:38:11.242625 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 63.444973945617676s
I0328 17:38:14.752644 2315232 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 17:38:14.752737 2315232 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 17:38:14.752778 2315232 utils.py:162] NumExpr defaulting to 16 threads.
I0328 17:38:15.085283 2315232 config.py:54] PyTorch version 2.6.0 available.
W0328 17:38:15.275308 2315232 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 17:38:15.832323 2315232 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 17:38:15.835969 2313906 quantize_finetune_llama.py:209] layer 18 gpu 2
I0328 17:38:15.848746 2315232 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 17:38:24.861579 2315162 finetune.py:68] layer 16_v @ epoch 1 new loss 2.3355064513452817e-06 old loss 2.5074421046156203e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 17:38:32.516048 2315232 finetune.py:45] layer 17_v initial loss 4.337055543146562e-06
W0328 17:38:32.516234 2315232 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 17:39:01.747196 2315162 finetune.py:68] layer 16_v @ epoch 2 new loss 2.2653498490399215e-06 old loss 2.3355064513452817e-06 BETTER
I0328 17:39:05.513677 2315232 finetune.py:68] layer 17_v @ epoch 0 new loss 2.296271532031824e-06 old loss 4.337055543146562e-06 BETTER
I0328 17:39:18.315915 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 62.02136206626892s
I0328 17:39:22.079898 2315302 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 17:39:22.079995 2315302 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 17:39:22.080038 2315302 utils.py:162] NumExpr defaulting to 16 threads.
I0328 17:39:22.459883 2315302 config.py:54] PyTorch version 2.6.0 available.
W0328 17:39:22.680612 2315302 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 17:39:23.459899 2315302 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 17:39:23.463808 2313906 quantize_finetune_llama.py:209] layer 19 gpu 3
I0328 17:39:23.477872 2315302 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 17:39:38.766395 2315162 finetune.py:68] layer 16_v @ epoch 3 new loss 2.2268159227678552e-06 old loss 2.2653498490399215e-06 BETTER
I0328 17:39:39.845509 2315232 finetune.py:68] layer 17_v @ epoch 1 new loss 2.126507297361968e-06 old loss 2.296271532031824e-06 BETTER
I0328 17:39:40.952963 2315302 finetune.py:45] layer 18_v initial loss 5.7313482102472335e-06
W0328 17:39:40.953211 2315302 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 17:40:14.449016 2315302 finetune.py:68] layer 18_v @ epoch 0 new loss 1.966564695976558e-06 old loss 5.7313482102472335e-06 BETTER
I0328 17:40:14.595753 2315232 finetune.py:68] layer 17_v @ epoch 2 new loss 2.0557995412673336e-06 old loss 2.126507297361968e-06 BETTER
I0328 17:40:15.892747 2315162 finetune.py:68] layer 16_v @ epoch 4 new loss 2.1827256659889827e-06 old loss 2.2268159227678552e-06 BETTER
I0328 17:40:26.918322 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 63.001689434051514s
I0328 17:40:30.807573 2315372 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 17:40:30.807691 2315372 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 17:40:30.807732 2315372 utils.py:162] NumExpr defaulting to 16 threads.
I0328 17:40:31.202269 2315372 config.py:54] PyTorch version 2.6.0 available.
W0328 17:40:31.419274 2315372 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 17:40:32.049088 2315372 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 17:40:32.053191 2313906 quantize_finetune_llama.py:209] layer 20 gpu 0
I0328 17:40:32.069104 2315372 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 17:40:35.256084 2315162 finetune.py:45] layer 16_q initial loss 2.5266167540394235e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 17:40:49.036327 2315302 finetune.py:68] layer 18_v @ epoch 1 new loss 1.7918523553817067e-06 old loss 1.966564695976558e-06 BETTER
I0328 17:40:49.239087 2315372 finetune.py:45] layer 19_v initial loss 6.48323111818172e-06
W0328 17:40:49.239316 2315372 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 17:40:49.493578 2315232 finetune.py:76] layer 17_v @ epoch 3 new loss 2.076431201203377e-06 old loss 2.0557995412673336e-06 WORSE
I0328 17:41:10.290849 2315162 finetune.py:68] layer 16_q @ epoch 0 new loss 2.46858803620853e-06 old loss 2.5266167540394235e-06 BETTER
I0328 17:41:22.153303 2315372 finetune.py:68] layer 19_v @ epoch 0 new loss 2.0682666672655614e-06 old loss 6.48323111818172e-06 BETTER
I0328 17:41:23.743538 2315232 finetune.py:76] layer 17_v @ epoch 4 new loss 2.0625016077246983e-06 old loss 2.0557995412673336e-06 WORSE
I0328 17:41:23.888178 2315302 finetune.py:68] layer 18_v @ epoch 2 new loss 1.7132766743088723e-06 old loss 1.7918523553817067e-06 BETTER
I0328 17:41:42.367677 2315232 finetune.py:45] layer 17_q initial loss 2.348649331906927e-06
I0328 17:41:46.779219 2315162 finetune.py:68] layer 16_q @ epoch 1 new loss 2.394102921243757e-06 old loss 2.46858803620853e-06 BETTER
I0328 17:41:56.386197 2315372 finetune.py:68] layer 19_v @ epoch 1 new loss 1.867025503088371e-06 old loss 2.0682666672655614e-06 BETTER
I0328 17:41:58.794714 2315302 finetune.py:68] layer 18_v @ epoch 3 new loss 1.672204689384671e-06 old loss 1.7132766743088723e-06 BETTER
I0328 17:42:15.851571 2315232 finetune.py:68] layer 17_q @ epoch 0 new loss 2.2787824036640814e-06 old loss 2.348649331906927e-06 BETTER
I0328 17:42:23.476989 2315162 finetune.py:68] layer 16_q @ epoch 2 new loss 2.3545599106000736e-06 old loss 2.394102921243757e-06 BETTER
I0328 17:42:30.717345 2315372 finetune.py:68] layer 19_v @ epoch 2 new loss 1.7883982081912109e-06 old loss 1.867025503088371e-06 BETTER
I0328 17:42:33.956324 2315302 finetune.py:68] layer 18_v @ epoch 4 new loss 1.6673002392053604e-06 old loss 1.672204689384671e-06 BETTER
I0328 17:42:50.161478 2315232 finetune.py:68] layer 17_q @ epoch 1 new loss 2.232855649708654e-06 old loss 2.2787824036640814e-06 BETTER
I0328 17:42:53.498770 2315302 finetune.py:45] layer 18_q initial loss 1.956745563802542e-06
I0328 17:43:00.241861 2315162 finetune.py:68] layer 16_q @ epoch 3 new loss 2.321199872312718e-06 old loss 2.3545599106000736e-06 BETTER
I0328 17:43:05.248958 2315372 finetune.py:68] layer 19_v @ epoch 3 new loss 1.7579242239662562e-06 old loss 1.7883982081912109e-06 BETTER
I0328 17:43:24.731884 2315232 finetune.py:68] layer 17_q @ epoch 2 new loss 2.198863967350917e-06 old loss 2.232855649708654e-06 BETTER
I0328 17:43:27.314455 2315302 finetune.py:68] layer 18_q @ epoch 0 new loss 1.9039106291529606e-06 old loss 1.956745563802542e-06 BETTER
I0328 17:43:37.108717 2315162 finetune.py:68] layer 16_q @ epoch 4 new loss 2.2985095711192116e-06 old loss 2.321199872312718e-06 BETTER
I0328 17:43:39.618721 2315372 finetune.py:68] layer 19_v @ epoch 4 new loss 1.7233312519238098e-06 old loss 1.7579242239662562e-06 BETTER
I0328 17:43:55.191525 2315162 finetune.py:45] layer 16_k initial loss 2.43400222643686e-06
I0328 17:43:58.760795 2315372 finetune.py:45] layer 19_q initial loss 1.9765896013268502e-06
I0328 17:43:59.319805 2315232 finetune.py:68] layer 17_q @ epoch 3 new loss 2.1655162072420353e-06 old loss 2.198863967350917e-06 BETTER
I0328 17:44:01.903717 2315302 finetune.py:68] layer 18_q @ epoch 1 new loss 1.8499838461139007e-06 old loss 1.9039106291529606e-06 BETTER
I0328 17:44:30.473723 2315162 finetune.py:68] layer 16_k @ epoch 0 new loss 2.4029143332882086e-06 old loss 2.43400222643686e-06 BETTER
I0328 17:44:31.715319 2315372 finetune.py:68] layer 19_q @ epoch 0 new loss 1.920406702993205e-06 old loss 1.9765896013268502e-06 BETTER
I0328 17:44:33.785615 2315232 finetune.py:68] layer 17_q @ epoch 4 new loss 2.1503492462215945e-06 old loss 2.1655162072420353e-06 BETTER
I0328 17:44:36.483781 2315302 finetune.py:68] layer 18_q @ epoch 2 new loss 1.8262868479723693e-06 old loss 1.8499838461139007e-06 BETTER
I0328 17:44:51.389408 2315232 finetune.py:45] layer 17_k initial loss 2.2856929717818275e-06
I0328 17:45:05.773366 2315372 finetune.py:68] layer 19_q @ epoch 1 new loss 1.8779987840389367e-06 old loss 1.920406702993205e-06 BETTER
I0328 17:45:06.755332 2315162 finetune.py:68] layer 16_k @ epoch 1 new loss 2.3903501187305665e-06 old loss 2.4029143332882086e-06 BETTER
I0328 17:45:11.221455 2315302 finetune.py:68] layer 18_q @ epoch 3 new loss 1.803183181436907e-06 old loss 1.8262868479723693e-06 BETTER
I0328 17:45:25.041022 2315232 finetune.py:68] layer 17_k @ epoch 0 new loss 2.2548083506990224e-06 old loss 2.2856929717818275e-06 BETTER
I0328 17:45:39.887837 2315372 finetune.py:68] layer 19_q @ epoch 2 new loss 1.8492854678697768e-06 old loss 1.8779987840389367e-06 BETTER
I0328 17:45:43.120847 2315162 finetune.py:68] layer 16_k @ epoch 2 new loss 2.378982571826782e-06 old loss 2.3903501187305665e-06 BETTER
I0328 17:45:46.145421 2315302 finetune.py:68] layer 18_q @ epoch 4 new loss 1.7878661537906737e-06 old loss 1.803183181436907e-06 BETTER
I0328 17:45:59.200781 2315232 finetune.py:68] layer 17_k @ epoch 1 new loss 2.2213177999219624e-06 old loss 2.2548083506990224e-06 BETTER
I0328 17:46:04.071387 2315302 finetune.py:45] layer 18_k initial loss 1.9072067516390234e-06
I0328 17:46:13.991163 2315372 finetune.py:68] layer 19_q @ epoch 3 new loss 1.8318072534384555e-06 old loss 1.8492854678697768e-06 BETTER
I0328 17:46:19.701855 2315162 finetune.py:68] layer 16_k @ epoch 3 new loss 2.3736163257126464e-06 old loss 2.378982571826782e-06 BETTER
I0328 17:46:33.533029 2315232 finetune.py:68] layer 17_k @ epoch 2 new loss 2.2063909455027897e-06 old loss 2.2213177999219624e-06 BETTER
I0328 17:46:37.591372 2315302 finetune.py:68] layer 18_k @ epoch 0 new loss 1.8784257918014191e-06 old loss 1.9072067516390234e-06 BETTER
I0328 17:46:48.079849 2315372 finetune.py:68] layer 19_q @ epoch 4 new loss 1.8135054915546789e-06 old loss 1.8318072534384555e-06 BETTER
I0328 17:46:56.392472 2315162 finetune.py:68] layer 16_k @ epoch 4 new loss 2.346275323361624e-06 old loss 2.3736163257126464e-06 BETTER
I0328 17:47:05.486667 2315372 finetune.py:45] layer 19_k initial loss 1.9377862372493837e-06
I0328 17:47:07.811532 2315232 finetune.py:68] layer 17_k @ epoch 3 new loss 2.194418357248651e-06 old loss 2.2063909455027897e-06 BETTER
I0328 17:47:12.114163 2315302 finetune.py:68] layer 18_k @ epoch 1 new loss 1.860186330304714e-06 old loss 1.8784257918014191e-06 BETTER
I0328 17:47:16.172717 2315162 finetune.py:45] layer 16_o initial loss 5.423394668468973e-06
I0328 17:47:38.524950 2315372 finetune.py:68] layer 19_k @ epoch 0 new loss 1.9161352611263283e-06 old loss 1.9377862372493837e-06 BETTER
I0328 17:47:42.127540 2315232 finetune.py:68] layer 17_k @ epoch 4 new loss 2.187937298003817e-06 old loss 2.194418357248651e-06 BETTER
I0328 17:47:46.610511 2315302 finetune.py:68] layer 18_k @ epoch 2 new loss 1.8511923371988814e-06 old loss 1.860186330304714e-06 BETTER
I0328 17:47:50.852199 2315162 finetune.py:68] layer 16_o @ epoch 0 new loss 5.244803105597384e-06 old loss 5.423394668468973e-06 BETTER
I0328 17:48:01.410680 2315232 finetune.py:45] layer 17_o initial loss 5.16577483722358e-06
I0328 17:48:12.376108 2315372 finetune.py:68] layer 19_k @ epoch 1 new loss 1.9031693909710157e-06 old loss 1.9161352611263283e-06 BETTER
I0328 17:48:21.219455 2315302 finetune.py:76] layer 18_k @ epoch 3 new loss 1.8579067955215578e-06 old loss 1.8511923371988814e-06 WORSE
I0328 17:48:26.501358 2315162 finetune.py:68] layer 16_o @ epoch 1 new loss 5.135103947395692e-06 old loss 5.244803105597384e-06 BETTER
I0328 17:48:34.023048 2315232 finetune.py:68] layer 17_o @ epoch 0 new loss 4.9820814638223965e-06 old loss 5.16577483722358e-06 BETTER
I0328 17:48:46.173313 2315372 finetune.py:68] layer 19_k @ epoch 2 new loss 1.902772282846854e-06 old loss 1.9031693909710157e-06 BETTER
I0328 17:48:55.392538 2315302 finetune.py:68] layer 18_k @ epoch 4 new loss 1.8360065041633788e-06 old loss 1.8511923371988814e-06 BETTER
I0328 17:49:02.507939 2315162 finetune.py:68] layer 16_o @ epoch 2 new loss 5.060330749984132e-06 old loss 5.135103947395692e-06 BETTER
I0328 17:49:07.648593 2315232 finetune.py:68] layer 17_o @ epoch 1 new loss 4.875803824688774e-06 old loss 4.9820814638223965e-06 BETTER
I0328 17:49:14.904827 2315302 finetune.py:45] layer 18_o initial loss 4.2378128455311526e-06
I0328 17:49:20.217116 2315372 finetune.py:68] layer 19_k @ epoch 3 new loss 1.894786009870586e-06 old loss 1.902772282846854e-06 BETTER
I0328 17:49:38.458700 2315162 finetune.py:68] layer 16_o @ epoch 3 new loss 4.998640179110225e-06 old loss 5.060330749984132e-06 BETTER
I0328 17:49:41.402580 2315232 finetune.py:68] layer 17_o @ epoch 2 new loss 4.797469955519773e-06 old loss 4.875803824688774e-06 BETTER
I0328 17:49:47.721101 2315302 finetune.py:68] layer 18_o @ epoch 0 new loss 4.1010634959093295e-06 old loss 4.2378128455311526e-06 BETTER
I0328 17:49:54.546619 2315372 finetune.py:68] layer 19_k @ epoch 4 new loss 1.882906985883892e-06 old loss 1.894786009870586e-06 BETTER
I0328 17:50:13.851767 2315372 finetune.py:45] layer 19_o initial loss 4.0639142753207125e-06
I0328 17:50:14.599923 2315162 finetune.py:68] layer 16_o @ epoch 4 new loss 4.9481845962873194e-06 old loss 4.998640179110225e-06 BETTER
I0328 17:50:15.072953 2315232 finetune.py:68] layer 17_o @ epoch 3 new loss 4.7383059609273914e-06 old loss 4.797469955519773e-06 BETTER
I0328 17:50:21.546573 2315302 finetune.py:68] layer 18_o @ epoch 1 new loss 4.033776349388063e-06 old loss 4.1010634959093295e-06 BETTER
I0328 17:50:45.852721 2315162 finetune.py:45] layer 16_up initial loss 1.0987741916324012e-05
I0328 17:50:46.187406 2315372 finetune.py:68] layer 19_o @ epoch 0 new loss 3.947880486521171e-06 old loss 4.0639142753207125e-06 BETTER
I0328 17:50:48.814129 2315232 finetune.py:68] layer 17_o @ epoch 4 new loss 4.687022283178521e-06 old loss 4.7383059609273914e-06 BETTER
I0328 17:50:55.590670 2315302 finetune.py:68] layer 18_o @ epoch 2 new loss 3.987100626545725e-06 old loss 4.033776349388063e-06 BETTER
I0328 17:51:17.818919 2315162 finetune.py:68] layer 16_up @ epoch 0 new loss 1.074275769497035e-05 old loss 1.0987741916324012e-05 BETTER
I0328 17:51:19.547685 2315232 finetune.py:45] layer 17_up initial loss 1.1551536772458348e-05
I0328 17:51:19.650830 2315372 finetune.py:68] layer 19_o @ epoch 1 new loss 3.885234491463052e-06 old loss 3.947880486521171e-06 BETTER
I0328 17:51:29.550988 2315302 finetune.py:68] layer 18_o @ epoch 3 new loss 3.949090114474529e-06 old loss 3.987100626545725e-06 BETTER
I0328 17:51:50.022524 2315232 finetune.py:68] layer 17_up @ epoch 0 new loss 1.127170617110096e-05 old loss 1.1551536772458348e-05 BETTER
I0328 17:51:51.315092 2315162 finetune.py:68] layer 16_up @ epoch 1 new loss 1.0559648217167705e-05 old loss 1.074275769497035e-05 BETTER
I0328 17:51:53.049265 2315372 finetune.py:68] layer 19_o @ epoch 2 new loss 3.8448074519692454e-06 old loss 3.885234491463052e-06 BETTER
I0328 17:52:03.632177 2315302 finetune.py:68] layer 18_o @ epoch 4 new loss 3.9211004150274675e-06 old loss 3.949090114474529e-06 BETTER
I0328 17:52:21.878437 2315232 finetune.py:68] layer 17_up @ epoch 1 new loss 1.1062338671763428e-05 old loss 1.127170617110096e-05 BETTER
I0328 17:52:25.228970 2315162 finetune.py:68] layer 16_up @ epoch 2 new loss 1.0406518413219601e-05 old loss 1.0559648217167705e-05 BETTER
I0328 17:52:26.625909 2315372 finetune.py:68] layer 19_o @ epoch 3 new loss 3.8123112062748987e-06 old loss 3.8448074519692454e-06 BETTER
I0328 17:52:35.417390 2315302 finetune.py:45] layer 18_up initial loss 1.102017722587334e-05
I0328 17:52:53.936270 2315232 finetune.py:68] layer 17_up @ epoch 2 new loss 1.0892162208619993e-05 old loss 1.1062338671763428e-05 BETTER
I0328 17:52:59.012280 2315162 finetune.py:68] layer 16_up @ epoch 3 new loss 1.0274950000166427e-05 old loss 1.0406518413219601e-05 BETTER
I0328 17:53:00.109609 2315372 finetune.py:68] layer 19_o @ epoch 4 new loss 3.7868153413000982e-06 old loss 3.8123112062748987e-06 BETTER
I0328 17:53:06.108929 2315302 finetune.py:68] layer 18_up @ epoch 0 new loss 1.0769701475510374e-05 old loss 1.102017722587334e-05 BETTER
I0328 17:53:26.109386 2315232 finetune.py:68] layer 17_up @ epoch 3 new loss 1.074703595804749e-05 old loss 1.0892162208619993e-05 BETTER
I0328 17:53:31.127199 2315372 finetune.py:45] layer 19_up initial loss 1.1510740478115622e-05
I0328 17:53:32.741692 2315162 finetune.py:68] layer 16_up @ epoch 4 new loss 1.0158732948184479e-05 old loss 1.0274950000166427e-05 BETTER
I0328 17:53:38.144781 2315302 finetune.py:68] layer 18_up @ epoch 1 new loss 1.0586883036012296e-05 old loss 1.0769701475510374e-05 BETTER
I0328 17:53:58.266778 2315232 finetune.py:68] layer 17_up @ epoch 4 new loss 1.0621553883538581e-05 old loss 1.074703595804749e-05 BETTER
I0328 17:54:01.285284 2315372 finetune.py:68] layer 19_up @ epoch 0 new loss 1.1251720934524201e-05 old loss 1.1510740478115622e-05 BETTER
I0328 17:54:04.214321 2315162 finetune.py:45] layer 16_gate initial loss 1.2129054994147737e-05
I0328 17:54:10.300280 2315302 finetune.py:68] layer 18_up @ epoch 2 new loss 1.043962674884824e-05 old loss 1.0586883036012296e-05 BETTER
I0328 17:54:29.088694 2315232 finetune.py:45] layer 17_gate initial loss 1.2878351299150381e-05
I0328 17:54:32.319040 2315372 finetune.py:68] layer 19_up @ epoch 1 new loss 1.1065395483456086e-05 old loss 1.1251720934524201e-05 BETTER
I0328 17:54:34.249573 2315162 finetune.py:68] layer 16_gate @ epoch 0 new loss 1.2015802894893568e-05 old loss 1.2129054994147737e-05 BETTER
I0328 17:54:42.465111 2315302 finetune.py:68] layer 18_up @ epoch 3 new loss 1.031223564496031e-05 old loss 1.043962674884824e-05 BETTER
I0328 17:54:57.405693 2315232 finetune.py:68] layer 17_gate @ epoch 0 new loss 1.274989244848257e-05 old loss 1.2878351299150381e-05 BETTER
I0328 17:55:03.702885 2315372 finetune.py:68] layer 19_up @ epoch 2 new loss 1.0913123333011754e-05 old loss 1.1065395483456086e-05 BETTER
I0328 17:55:05.400516 2315162 finetune.py:68] layer 16_gate @ epoch 1 new loss 1.1919078133360017e-05 old loss 1.2015802894893568e-05 BETTER
I0328 17:55:14.721488 2315302 finetune.py:68] layer 18_up @ epoch 4 new loss 1.0203316378465388e-05 old loss 1.031223564496031e-05 BETTER
I0328 17:55:26.796186 2315232 finetune.py:68] layer 17_gate @ epoch 1 new loss 1.2644865819311235e-05 old loss 1.274989244848257e-05 BETTER
I0328 17:55:35.143936 2315372 finetune.py:68] layer 19_up @ epoch 3 new loss 1.0785950507852249e-05 old loss 1.0913123333011754e-05 BETTER
I0328 17:55:36.671082 2315162 finetune.py:68] layer 16_gate @ epoch 2 new loss 1.1832868040073663e-05 old loss 1.1919078133360017e-05 BETTER
I0328 17:55:46.117773 2315302 finetune.py:45] layer 18_gate initial loss 1.2644095477298833e-05
I0328 17:55:56.153874 2315232 finetune.py:68] layer 17_gate @ epoch 2 new loss 1.2551870895549655e-05 old loss 1.2644865819311235e-05 BETTER
I0328 17:56:06.483798 2315372 finetune.py:68] layer 19_up @ epoch 4 new loss 1.0674972145352513e-05 old loss 1.0785950507852249e-05 BETTER
I0328 17:56:08.167212 2315162 finetune.py:68] layer 16_gate @ epoch 3 new loss 1.1755294508475345e-05 old loss 1.1832868040073663e-05 BETTER
I0328 17:56:14.712890 2315302 finetune.py:68] layer 18_gate @ epoch 0 new loss 1.2533318113128189e-05 old loss 1.2644095477298833e-05 BETTER
I0328 17:56:25.726365 2315232 finetune.py:68] layer 17_gate @ epoch 3 new loss 1.246820465894416e-05 old loss 1.2551870895549655e-05 BETTER
I0328 17:56:37.573779 2315372 finetune.py:45] layer 19_gate initial loss 1.3366073289944325e-05
I0328 17:56:39.819988 2315162 finetune.py:68] layer 16_gate @ epoch 4 new loss 1.1685237041092478e-05 old loss 1.1755294508475345e-05 BETTER
I0328 17:56:44.341418 2315302 finetune.py:68] layer 18_gate @ epoch 1 new loss 1.244296072400175e-05 old loss 1.2533318113128189e-05 BETTER
I0328 17:56:55.247275 2315232 finetune.py:68] layer 17_gate @ epoch 4 new loss 1.2393423276080284e-05 old loss 1.246820465894416e-05 BETTER
I0328 17:57:05.708913 2315372 finetune.py:68] layer 19_gate @ epoch 0 new loss 1.3255438716441859e-05 old loss 1.3366073289944325e-05 BETTER
I0328 17:57:14.224716 2315302 finetune.py:68] layer 18_gate @ epoch 2 new loss 1.236205207533203e-05 old loss 1.244296072400175e-05 BETTER
I0328 17:57:35.045311 2315372 finetune.py:68] layer 19_gate @ epoch 1 new loss 1.3164377378416248e-05 old loss 1.3255438716441859e-05 BETTER
I0328 17:57:36.337841 2315162 finetune.py:45] layer 16_down initial loss 1.9899960534530692e-05
I0328 17:57:44.259251 2315302 finetune.py:68] layer 18_gate @ epoch 3 new loss 1.2291379789530765e-05 old loss 1.236205207533203e-05 BETTER
I0328 17:57:51.011140 2315232 finetune.py:45] layer 17_down initial loss 2.2348674974637106e-05
I0328 17:58:03.752217 2315162 finetune.py:68] layer 16_down @ epoch 0 new loss 1.9899387552868575e-05 old loss 1.9899960534530692e-05 BETTER
I0328 17:58:04.455646 2315372 finetune.py:68] layer 19_gate @ epoch 2 new loss 1.3083183148410171e-05 old loss 1.3164377378416248e-05 BETTER
I0328 17:58:14.205719 2315302 finetune.py:68] layer 18_gate @ epoch 4 new loss 1.2226570106577128e-05 old loss 1.2291379789530765e-05 BETTER
I0328 17:58:17.147940 2315232 finetune.py:68] layer 17_down @ epoch 0 new loss 2.2348029233398847e-05 old loss 2.2348674974637106e-05 BETTER
I0328 17:58:32.233440 2315162 finetune.py:68] layer 16_down @ epoch 1 new loss 1.989912925637327e-05 old loss 1.9899387552868575e-05 BETTER
I0328 17:58:33.905848 2315372 finetune.py:68] layer 19_gate @ epoch 3 new loss 1.3012622730457224e-05 old loss 1.3083183148410171e-05 BETTER
I0328 17:58:44.325166 2315232 finetune.py:68] layer 17_down @ epoch 1 new loss 2.2347654521581717e-05 old loss 2.2348029233398847e-05 BETTER
I0328 17:59:01.125275 2315162 finetune.py:68] layer 16_down @ epoch 2 new loss 1.989894917642232e-05 old loss 1.989912925637327e-05 BETTER
I0328 17:59:03.121200 2315372 finetune.py:68] layer 19_gate @ epoch 4 new loss 1.2948875337315258e-05 old loss 1.3012622730457224e-05 BETTER
I0328 17:59:11.312154 2315302 finetune.py:45] layer 18_down initial loss 2.2368587451637723e-05
I0328 17:59:11.713292 2315232 finetune.py:68] layer 17_down @ epoch 2 new loss 2.2347443518810906e-05 old loss 2.2347654521581717e-05 BETTER
I0328 17:59:30.146418 2315162 finetune.py:68] layer 16_down @ epoch 3 new loss 1.9898803657270037e-05 old loss 1.989894917642232e-05 BETTER
I0328 17:59:37.511394 2315302 finetune.py:68] layer 18_down @ epoch 0 new loss 2.236817090306431e-05 old loss 2.2368587451637723e-05 BETTER
I0328 17:59:39.229133 2315232 finetune.py:68] layer 17_down @ epoch 3 new loss 2.234726707683876e-05 old loss 2.2347443518810906e-05 BETTER
I0328 17:59:59.041571 2315162 finetune.py:68] layer 16_down @ epoch 4 new loss 1.9898678147001192e-05 old loss 1.9898803657270037e-05 BETTER
I0328 17:59:59.855658 2315372 finetune.py:45] layer 19_down initial loss 2.365823820582591e-05
16_v proxy err 0.002237382112070918 tr(WHW.T) 274.28167724609375
bpp_loss 4.373315981123596
16_q proxy err 0.00010914960148511454 tr(WHW.T) 24487.083984375
bpp_loss 5.278554635471664
16_k proxy err 4.245728996465914e-05 tr(WHW.T) 19500.68359375
bpp_loss 6.309335860074498
16_o proxy err 0.0020006936974823475 tr(WHW.T) 969.6748046875
bpp_loss 4.47473367373459
16_up proxy err 0.0015372791094705462 tr(WHW.T) 8331.2841796875
bpp_loss 4.578173548981015
16_gate proxy err 0.0003182951477356255 tr(WHW.T) 41179.07421875
bpp_loss 4.935744860608663
16_down proxy err 0.0019407750805839896 tr(WHW.T) 6292.091796875
bpp_loss 4.5696721504375875
I0328 18:00:04.927868 2315302 finetune.py:68] layer 18_down @ epoch 1 new loss 2.2367947167367674e-05 old loss 2.236817090306431e-05 BETTER
I0328 18:00:07.279989 2315232 finetune.py:68] layer 17_down @ epoch 4 new loss 2.234717430837918e-05 old loss 2.234726707683876e-05 BETTER
17_v proxy err 0.002413337118923664 tr(WHW.T) 283.9730224609375
bpp_loss 4.442840555508155
17_q proxy err 0.0001079664725693874 tr(WHW.T) 27573.177734375
bpp_loss 5.297870023408905
17_k proxy err 5.307633546181023e-05 tr(WHW.T) 17427.521484375
bpp_loss 6.342927040997893
17_o proxy err 0.0021882737055420876 tr(WHW.T) 1107.070556640625
bpp_loss 4.503533648792654
17_up proxy err 0.0015180232003331184 tr(WHW.T) 8451.8994140625
bpp_loss 4.57523908539276
17_gate proxy err 0.0003151765849906951 tr(WHW.T) 41706.80859375
bpp_loss 4.95110591793699
17_down proxy err 0.0019678883254528046 tr(WHW.T) 6215.4619140625
bpp_loss 4.566283610649407
I0328 18:00:25.885639 2315372 finetune.py:68] layer 19_down @ epoch 0 new loss 2.3657563360757194e-05 old loss 2.365823820582591e-05 BETTER
I0328 18:00:33.055802 2315302 finetune.py:68] layer 18_down @ epoch 2 new loss 2.2367725250660442e-05 old loss 2.2367947167367674e-05 BETTER
I0328 18:00:52.856562 2315372 finetune.py:68] layer 19_down @ epoch 1 new loss 2.3657250494579785e-05 old loss 2.3657563360757194e-05 BETTER
I0328 18:01:00.893202 2315302 finetune.py:68] layer 18_down @ epoch 3 new loss 2.23676524910843e-05 old loss 2.2367725250660442e-05 BETTER
I0328 18:01:19.506919 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 67.11771059036255s
I0328 18:01:19.971594 2315372 finetune.py:68] layer 19_down @ epoch 2 new loss 2.3656994017073885e-05 old loss 2.3657250494579785e-05 BETTER
I0328 18:01:23.419198 2315442 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:01:23.419309 2315442 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:01:23.419351 2315442 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:01:23.807732 2315442 config.py:54] PyTorch version 2.6.0 available.
W0328 18:01:24.033592 2315442 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:01:24.773741 2315442 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:01:24.777935 2313906 quantize_finetune_llama.py:209] layer 21 gpu 1
I0328 18:01:24.797887 2315442 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 18:01:28.626557 2315302 finetune.py:68] layer 18_down @ epoch 4 new loss 2.2367490601027384e-05 old loss 2.23676524910843e-05 BETTER
18_v proxy err 0.0024400397669523954 tr(WHW.T) 287.61376953125
bpp_loss 4.368332161393482
18_q proxy err 0.0001362139155389741 tr(WHW.T) 22397.796875
bpp_loss 5.287198875332251
18_k proxy err 5.384453834267333e-05 tr(WHW.T) 17391.37890625
bpp_loss 6.434179358999245
18_o proxy err 0.002167260041460395 tr(WHW.T) 1203.284912109375
bpp_loss 4.482619388494641
18_up proxy err 0.0016719955019652843 tr(WHW.T) 7983.80810546875
bpp_loss 4.570844540971199
18_gate proxy err 0.00038778901216574013 tr(WHW.T) 35253.5859375
bpp_loss 4.955757336358407
18_down proxy err 0.002028370276093483 tr(WHW.T) 6231.19970703125
bpp_loss 4.5645047406932076
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:01:42.046402 2315442 finetune.py:45] layer 20_v initial loss 6.63928267385927e-06
W0328 18:01:42.046630 2315442 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:01:47.093136 2315372 finetune.py:68] layer 19_down @ epoch 3 new loss 2.365682303206995e-05 old loss 2.3656994017073885e-05 BETTER
I0328 18:02:14.380127 2315372 finetune.py:68] layer 19_down @ epoch 4 new loss 2.3656664779991843e-05 old loss 2.365682303206995e-05 BETTER
19_v proxy err 0.0021268820855766535 tr(WHW.T) 341.0596618652344
bpp_loss 4.412605926569086
19_q proxy err 0.0001307254860876128 tr(WHW.T) 24042.935546875
bpp_loss 5.291024818783626
19_k proxy err 5.9785674238810316e-05 tr(WHW.T) 15552.8115234375
bpp_loss 6.3172758606961
19_o proxy err 0.0022318551782518625 tr(WHW.T) 1169.813232421875
bpp_loss 4.495429529226385
19_up proxy err 0.0017734338762238622 tr(WHW.T) 7649.26318359375
bpp_loss 4.566978761327586
19_gate proxy err 0.00042394277988933027 tr(WHW.T) 32794.40234375
bpp_loss 4.965709402957665
19_down proxy err 0.0020564086735248566 tr(WHW.T) 6187.3623046875
bpp_loss 4.562628757940339
I0328 18:02:16.891754 2315442 finetune.py:68] layer 20_v @ epoch 0 new loss 2.2055198769521667e-06 old loss 6.63928267385927e-06 BETTER
I0328 18:02:35.531824 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 62.53103041648865s
I0328 18:02:39.204429 2315512 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:02:39.204519 2315512 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:02:39.204556 2315512 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:02:39.555719 2315512 config.py:54] PyTorch version 2.6.0 available.
W0328 18:02:39.765034 2315512 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:02:40.347285 2315512 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:02:40.351083 2313906 quantize_finetune_llama.py:209] layer 22 gpu 2
I0328 18:02:40.364655 2315512 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:02:53.118013 2315442 finetune.py:68] layer 20_v @ epoch 1 new loss 1.9947967757616425e-06 old loss 2.2055198769521667e-06 BETTER
I0328 18:02:57.352723 2315512 finetune.py:45] layer 21_v initial loss 6.459869837271981e-06
W0328 18:02:57.352909 2315512 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:03:29.888322 2315442 finetune.py:68] layer 20_v @ epoch 2 new loss 1.9054704125665012e-06 old loss 1.9947967757616425e-06 BETTER
I0328 18:03:30.481311 2315512 finetune.py:68] layer 21_v @ epoch 0 new loss 2.5355950583616504e-06 old loss 6.459869837271981e-06 BETTER
I0328 18:03:43.197435 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 62.390058755874634s
I0328 18:03:46.817416 2315582 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:03:46.817515 2315582 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:03:46.817556 2315582 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:03:47.181780 2315582 config.py:54] PyTorch version 2.6.0 available.
W0328 18:03:47.386607 2315582 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:03:47.971349 2315582 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:03:47.975123 2313906 quantize_finetune_llama.py:209] layer 23 gpu 3
I0328 18:03:47.988619 2315582 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:04:04.799385 2315512 finetune.py:68] layer 21_v @ epoch 1 new loss 2.3094758034858387e-06 old loss 2.5355950583616504e-06 BETTER
I0328 18:04:04.955779 2315582 finetune.py:45] layer 22_v initial loss 7.022873887763126e-06
W0328 18:04:04.956114 2315582 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:04:06.842529 2315442 finetune.py:68] layer 20_v @ epoch 3 new loss 1.8711671145865694e-06 old loss 1.9054704125665012e-06 BETTER
I0328 18:04:38.419942 2315582 finetune.py:68] layer 22_v @ epoch 0 new loss 2.109678007400362e-06 old loss 7.022873887763126e-06 BETTER
I0328 18:04:39.441076 2315512 finetune.py:68] layer 21_v @ epoch 2 new loss 2.22050857701106e-06 old loss 2.3094758034858387e-06 BETTER
I0328 18:04:43.723948 2315442 finetune.py:68] layer 20_v @ epoch 4 new loss 1.822066337808792e-06 old loss 1.8711671145865694e-06 BETTER
I0328 18:04:51.395539 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 62.948495864868164s
I0328 18:04:55.215350 2315652 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:04:55.215450 2315652 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:04:55.215493 2315652 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:04:55.577544 2315652 config.py:54] PyTorch version 2.6.0 available.
W0328 18:04:55.798169 2315652 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:04:56.497104 2315652 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:04:56.501096 2313906 quantize_finetune_llama.py:209] layer 24 gpu 0
I0328 18:04:56.515660 2315652 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 18:05:03.059966 2315442 finetune.py:45] layer 20_q initial loss 2.1239968646113994e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:05:13.329335 2315582 finetune.py:68] layer 22_v @ epoch 1 new loss 1.894146862468915e-06 old loss 2.109678007400362e-06 BETTER
I0328 18:05:13.870185 2315652 finetune.py:45] layer 23_v initial loss 9.653348570282105e-06
W0328 18:05:13.870413 2315652 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:05:14.153623 2315512 finetune.py:68] layer 21_v @ epoch 3 new loss 2.156521759388852e-06 old loss 2.22050857701106e-06 BETTER
I0328 18:05:38.347680 2315442 finetune.py:68] layer 20_q @ epoch 0 new loss 2.06623440135445e-06 old loss 2.1239968646113994e-06 BETTER
I0328 18:05:46.884760 2315652 finetune.py:68] layer 23_v @ epoch 0 new loss 2.23771689888963e-06 old loss 9.653348570282105e-06 BETTER
I0328 18:05:48.202570 2315582 finetune.py:68] layer 22_v @ epoch 2 new loss 1.799754841158574e-06 old loss 1.894146862468915e-06 BETTER
I0328 18:05:49.351880 2315512 finetune.py:68] layer 21_v @ epoch 4 new loss 2.113987193297362e-06 old loss 2.156521759388852e-06 BETTER
I0328 18:06:08.554255 2315512 finetune.py:45] layer 21_q initial loss 2.6023285499832127e-06
I0328 18:06:14.741406 2315442 finetune.py:68] layer 20_q @ epoch 1 new loss 2.021380169026088e-06 old loss 2.06623440135445e-06 BETTER
I0328 18:06:21.260905 2315652 finetune.py:68] layer 23_v @ epoch 1 new loss 1.9539627373887924e-06 old loss 2.23771689888963e-06 BETTER
I0328 18:06:23.310560 2315582 finetune.py:68] layer 22_v @ epoch 3 new loss 1.7573528339198674e-06 old loss 1.799754841158574e-06 BETTER
I0328 18:06:41.871093 2315512 finetune.py:68] layer 21_q @ epoch 0 new loss 2.488944119249936e-06 old loss 2.6023285499832127e-06 BETTER
I0328 18:06:51.215685 2315442 finetune.py:68] layer 20_q @ epoch 2 new loss 1.9905842236767057e-06 old loss 2.021380169026088e-06 BETTER
I0328 18:06:55.670076 2315652 finetune.py:68] layer 23_v @ epoch 2 new loss 1.8507075765228365e-06 old loss 1.9539627373887924e-06 BETTER
I0328 18:06:58.330027 2315582 finetune.py:68] layer 22_v @ epoch 4 new loss 1.7273160892727901e-06 old loss 1.7573528339198674e-06 BETTER
I0328 18:07:16.237645 2315512 finetune.py:68] layer 21_q @ epoch 1 new loss 2.4610837954242015e-06 old loss 2.488944119249936e-06 BETTER
I0328 18:07:17.686925 2315582 finetune.py:45] layer 22_q initial loss 2.1318826384231215e-06
I0328 18:07:27.958053 2315442 finetune.py:68] layer 20_q @ epoch 3 new loss 1.967323896678863e-06 old loss 1.9905842236767057e-06 BETTER
I0328 18:07:30.200288 2315652 finetune.py:68] layer 23_v @ epoch 3 new loss 1.8077945469485712e-06 old loss 1.8507075765228365e-06 BETTER
I0328 18:07:50.805927 2315512 finetune.py:68] layer 21_q @ epoch 2 new loss 2.3963166313478723e-06 old loss 2.4610837954242015e-06 BETTER
I0328 18:07:51.226288 2315582 finetune.py:68] layer 22_q @ epoch 0 new loss 2.075417569358251e-06 old loss 2.1318826384231215e-06 BETTER
I0328 18:08:04.977133 2315442 finetune.py:68] layer 20_q @ epoch 4 new loss 1.946684278664179e-06 old loss 1.967323896678863e-06 BETTER
I0328 18:08:05.045147 2315652 finetune.py:68] layer 23_v @ epoch 4 new loss 1.8054944348477875e-06 old loss 1.8077945469485712e-06 BETTER
I0328 18:08:22.779253 2315442 finetune.py:45] layer 20_k initial loss 2.0988695723644923e-06
I0328 18:08:24.199720 2315652 finetune.py:45] layer 23_q initial loss 2.1712824036512757e-06
I0328 18:08:25.344414 2315512 finetune.py:68] layer 21_q @ epoch 3 new loss 2.3529419195256196e-06 old loss 2.3963166313478723e-06 BETTER
I0328 18:08:25.933439 2315582 finetune.py:68] layer 22_q @ epoch 1 new loss 2.024207333306549e-06 old loss 2.075417569358251e-06 BETTER
I0328 18:08:57.367659 2315652 finetune.py:68] layer 23_q @ epoch 0 new loss 2.070998561976012e-06 old loss 2.1712824036512757e-06 BETTER
I0328 18:08:58.104583 2315442 finetune.py:68] layer 20_k @ epoch 0 new loss 2.0654790660046274e-06 old loss 2.0988695723644923e-06 BETTER
I0328 18:08:59.946561 2315512 finetune.py:68] layer 21_q @ epoch 4 new loss 2.3266879907168914e-06 old loss 2.3529419195256196e-06 BETTER
I0328 18:09:00.520162 2315582 finetune.py:68] layer 22_q @ epoch 2 new loss 1.989407564906287e-06 old loss 2.024207333306549e-06 BETTER
I0328 18:09:17.855967 2315512 finetune.py:45] layer 21_k initial loss 2.5713495688250987e-06
I0328 18:09:31.858698 2315652 finetune.py:68] layer 23_q @ epoch 1 new loss 2.0192187548673246e-06 old loss 2.070998561976012e-06 BETTER
I0328 18:09:34.495352 2315442 finetune.py:68] layer 20_k @ epoch 1 new loss 2.0519894405879313e-06 old loss 2.0654790660046274e-06 BETTER
I0328 18:09:35.146075 2315582 finetune.py:68] layer 22_q @ epoch 3 new loss 1.9677165710163536e-06 old loss 1.989407564906287e-06 BETTER
I0328 18:09:50.988893 2315512 finetune.py:68] layer 21_k @ epoch 0 new loss 2.5081676540139597e-06 old loss 2.5713495688250987e-06 BETTER
I0328 18:10:06.197654 2315652 finetune.py:68] layer 23_q @ epoch 2 new loss 1.9860904103552457e-06 old loss 2.0192187548673246e-06 BETTER
I0328 18:10:09.814676 2315582 finetune.py:68] layer 22_q @ epoch 4 new loss 1.9519088709785137e-06 old loss 1.9677165710163536e-06 BETTER
I0328 18:10:10.854719 2315442 finetune.py:68] layer 20_k @ epoch 2 new loss 2.0361956103442935e-06 old loss 2.0519894405879313e-06 BETTER
I0328 18:10:25.141468 2315512 finetune.py:68] layer 21_k @ epoch 1 new loss 2.4850403406162513e-06 old loss 2.5081676540139597e-06 BETTER
I0328 18:10:27.868199 2315582 finetune.py:45] layer 22_k initial loss 2.145290864064009e-06
I0328 18:10:40.382488 2315652 finetune.py:68] layer 23_q @ epoch 3 new loss 1.964798457265715e-06 old loss 1.9860904103552457e-06 BETTER
I0328 18:10:47.346352 2315442 finetune.py:68] layer 20_k @ epoch 3 new loss 2.023280785579118e-06 old loss 2.0361956103442935e-06 BETTER
I0328 18:10:59.416271 2315512 finetune.py:68] layer 21_k @ epoch 2 new loss 2.470908839313779e-06 old loss 2.4850403406162513e-06 BETTER
I0328 18:11:01.326389 2315582 finetune.py:68] layer 22_k @ epoch 0 new loss 2.1192288386373548e-06 old loss 2.145290864064009e-06 BETTER
I0328 18:11:14.531231 2315652 finetune.py:68] layer 23_q @ epoch 4 new loss 1.9494636944727972e-06 old loss 1.964798457265715e-06 BETTER
I0328 18:11:23.968539 2315442 finetune.py:68] layer 20_k @ epoch 4 new loss 2.014961637542001e-06 old loss 2.023280785579118e-06 BETTER
I0328 18:11:32.057770 2315652 finetune.py:45] layer 23_k initial loss 2.1839177861693315e-06
I0328 18:11:33.893309 2315512 finetune.py:68] layer 21_k @ epoch 3 new loss 2.458262997606653e-06 old loss 2.470908839313779e-06 BETTER
I0328 18:11:36.249905 2315582 finetune.py:68] layer 22_k @ epoch 1 new loss 2.103876113324077e-06 old loss 2.1192288386373548e-06 BETTER
I0328 18:11:43.429202 2315442 finetune.py:45] layer 20_o initial loss 4.395036285131937e-06
I0328 18:12:04.892869 2315652 finetune.py:68] layer 23_k @ epoch 0 new loss 2.13153043659986e-06 old loss 2.1839177861693315e-06 BETTER
I0328 18:12:08.245555 2315512 finetune.py:68] layer 21_k @ epoch 4 new loss 2.455654112054617e-06 old loss 2.458262997606653e-06 BETTER
I0328 18:12:10.677397 2315582 finetune.py:68] layer 22_k @ epoch 2 new loss 2.0928055164404213e-06 old loss 2.103876113324077e-06 BETTER
I0328 18:12:18.189846 2315442 finetune.py:68] layer 20_o @ epoch 0 new loss 4.26276346843224e-06 old loss 4.395036285131937e-06 BETTER
I0328 18:12:27.637559 2315512 finetune.py:45] layer 21_o initial loss 5.506490651896456e-06
I0328 18:12:38.723577 2315652 finetune.py:68] layer 23_k @ epoch 1 new loss 2.115223423970747e-06 old loss 2.13153043659986e-06 BETTER
I0328 18:12:45.261422 2315582 finetune.py:68] layer 22_k @ epoch 3 new loss 2.0836691874137614e-06 old loss 2.0928055164404213e-06 BETTER
I0328 18:12:53.781425 2315442 finetune.py:68] layer 20_o @ epoch 1 new loss 4.195419023744762e-06 old loss 4.26276346843224e-06 BETTER
I0328 18:13:00.220409 2315512 finetune.py:68] layer 21_o @ epoch 0 new loss 5.284327471599681e-06 old loss 5.506490651896456e-06 BETTER
I0328 18:13:12.581679 2315652 finetune.py:68] layer 23_k @ epoch 2 new loss 2.097237484122161e-06 old loss 2.115223423970747e-06 BETTER
I0328 18:13:19.799548 2315582 finetune.py:68] layer 22_k @ epoch 4 new loss 2.076227019642829e-06 old loss 2.0836691874137614e-06 BETTER
I0328 18:13:29.490615 2315442 finetune.py:68] layer 20_o @ epoch 2 new loss 4.1490711737424135e-06 old loss 4.195419023744762e-06 BETTER
I0328 18:13:34.199828 2315512 finetune.py:68] layer 21_o @ epoch 1 new loss 5.174287252884824e-06 old loss 5.284327471599681e-06 BETTER
I0328 18:13:39.110675 2315582 finetune.py:45] layer 22_o initial loss 4.921610525343567e-06
I0328 18:13:46.445913 2315652 finetune.py:68] layer 23_k @ epoch 3 new loss 2.087217808366404e-06 old loss 2.097237484122161e-06 BETTER
I0328 18:14:05.485529 2315442 finetune.py:68] layer 20_o @ epoch 3 new loss 4.112274837098084e-06 old loss 4.1490711737424135e-06 BETTER
I0328 18:14:08.269635 2315512 finetune.py:68] layer 21_o @ epoch 2 new loss 5.1035144679190125e-06 old loss 5.174287252884824e-06 BETTER
I0328 18:14:12.038989 2315582 finetune.py:68] layer 22_o @ epoch 0 new loss 4.78730589748011e-06 old loss 4.921610525343567e-06 BETTER
I0328 18:14:20.318951 2315652 finetune.py:68] layer 23_k @ epoch 4 new loss 2.076282498819637e-06 old loss 2.087217808366404e-06 BETTER
I0328 18:14:39.548668 2315652 finetune.py:45] layer 23_o initial loss 5.059194791101618e-06
I0328 18:14:41.384930 2315442 finetune.py:68] layer 20_o @ epoch 4 new loss 4.08341702495818e-06 old loss 4.112274837098084e-06 BETTER
I0328 18:14:42.327849 2315512 finetune.py:68] layer 21_o @ epoch 3 new loss 5.047224476584233e-06 old loss 5.1035144679190125e-06 BETTER
I0328 18:14:45.731662 2315582 finetune.py:68] layer 22_o @ epoch 1 new loss 4.7218595682352316e-06 old loss 4.78730589748011e-06 BETTER
I0328 18:15:11.538093 2315652 finetune.py:68] layer 23_o @ epoch 0 new loss 4.905014066025615e-06 old loss 5.059194791101618e-06 BETTER
I0328 18:15:12.906609 2315442 finetune.py:45] layer 20_up initial loss 1.2379513464111369e-05
I0328 18:15:16.153637 2315512 finetune.py:68] layer 21_o @ epoch 4 new loss 5.003665137337521e-06 old loss 5.047224476584233e-06 BETTER
I0328 18:15:19.640935 2315582 finetune.py:68] layer 22_o @ epoch 2 new loss 4.677978267864091e-06 old loss 4.7218595682352316e-06 BETTER
I0328 18:15:44.804688 2315652 finetune.py:68] layer 23_o @ epoch 1 new loss 4.835507752432022e-06 old loss 4.905014066025615e-06 BETTER
I0328 18:15:44.856538 2315442 finetune.py:68] layer 20_up @ epoch 0 new loss 1.2114028322685044e-05 old loss 1.2379513464111369e-05 BETTER
I0328 18:15:47.873393 2315512 finetune.py:45] layer 21_up initial loss 1.4437171557801776e-05
I0328 18:15:53.555532 2315582 finetune.py:68] layer 22_o @ epoch 3 new loss 4.6452341848635115e-06 old loss 4.677978267864091e-06 BETTER
I0328 18:16:18.186747 2315652 finetune.py:68] layer 23_o @ epoch 2 new loss 4.790311777469469e-06 old loss 4.835507752432022e-06 BETTER
I0328 18:16:18.309342 2315442 finetune.py:68] layer 20_up @ epoch 1 new loss 1.1922245903406292e-05 old loss 1.2114028322685044e-05 BETTER
I0328 18:16:18.370886 2315512 finetune.py:68] layer 21_up @ epoch 0 new loss 1.4101972737989854e-05 old loss 1.4437171557801776e-05 BETTER
I0328 18:16:27.594971 2315582 finetune.py:68] layer 22_o @ epoch 4 new loss 4.618459115590667e-06 old loss 4.6452341848635115e-06 BETTER
I0328 18:16:50.020420 2315512 finetune.py:68] layer 21_up @ epoch 1 new loss 1.386533858749317e-05 old loss 1.4101972737989854e-05 BETTER
I0328 18:16:51.504718 2315652 finetune.py:68] layer 23_o @ epoch 3 new loss 4.755132977152243e-06 old loss 4.790311777469469e-06 BETTER
I0328 18:16:51.659487 2315442 finetune.py:68] layer 20_up @ epoch 2 new loss 1.1767478099500295e-05 old loss 1.1922245903406292e-05 BETTER
I0328 18:16:58.867025 2315582 finetune.py:45] layer 22_up initial loss 1.4543444194714539e-05
I0328 18:17:22.046590 2315512 finetune.py:68] layer 21_up @ epoch 2 new loss 1.3676495655090548e-05 old loss 1.386533858749317e-05 BETTER
I0328 18:17:25.115424 2315652 finetune.py:68] layer 23_o @ epoch 4 new loss 4.731891294795787e-06 old loss 4.755132977152243e-06 BETTER
I0328 18:17:25.173043 2315442 finetune.py:68] layer 20_up @ epoch 3 new loss 1.1635474947979674e-05 old loss 1.1767478099500295e-05 BETTER
I0328 18:17:29.702063 2315582 finetune.py:68] layer 22_up @ epoch 0 new loss 1.4220956472854596e-05 old loss 1.4543444194714539e-05 BETTER
I0328 18:17:54.410783 2315512 finetune.py:68] layer 21_up @ epoch 3 new loss 1.352024537482066e-05 old loss 1.3676495655090548e-05 BETTER
I0328 18:17:56.703381 2315652 finetune.py:45] layer 23_up initial loss 1.5511766832787544e-05
I0328 18:17:58.819618 2315442 finetune.py:68] layer 20_up @ epoch 4 new loss 1.1524078217917122e-05 old loss 1.1635474947979674e-05 BETTER
I0328 18:18:01.742273 2315582 finetune.py:68] layer 22_up @ epoch 1 new loss 1.4003473552293144e-05 old loss 1.4220956472854596e-05 BETTER
I0328 18:18:26.736446 2315512 finetune.py:68] layer 21_up @ epoch 4 new loss 1.3388368643063586e-05 old loss 1.352024537482066e-05 BETTER
I0328 18:18:26.917342 2315652 finetune.py:68] layer 23_up @ epoch 0 new loss 1.5206186617433559e-05 old loss 1.5511766832787544e-05 BETTER
I0328 18:18:30.129069 2315442 finetune.py:45] layer 20_gate initial loss 1.4593448213418014e-05
I0328 18:18:33.907645 2315582 finetune.py:68] layer 22_up @ epoch 2 new loss 1.3831018804921769e-05 old loss 1.4003473552293144e-05 BETTER
I0328 18:18:58.221705 2315512 finetune.py:45] layer 21_gate initial loss 1.688101292529609e-05
I0328 18:18:58.411143 2315652 finetune.py:68] layer 23_up @ epoch 1 new loss 1.4995995115896221e-05 old loss 1.5206186617433559e-05 BETTER
I0328 18:19:00.280958 2315442 finetune.py:68] layer 20_gate @ epoch 0 new loss 1.4479412129730918e-05 old loss 1.4593448213418014e-05 BETTER
I0328 18:19:06.052025 2315582 finetune.py:68] layer 22_up @ epoch 3 new loss 1.3687845239473972e-05 old loss 1.3831018804921769e-05 BETTER
I0328 18:19:26.628721 2315512 finetune.py:68] layer 21_gate @ epoch 0 new loss 1.6742214938858524e-05 old loss 1.688101292529609e-05 BETTER
I0328 18:19:29.739706 2315652 finetune.py:68] layer 23_up @ epoch 2 new loss 1.4830867257842328e-05 old loss 1.4995995115896221e-05 BETTER
I0328 18:19:31.397543 2315442 finetune.py:68] layer 20_gate @ epoch 1 new loss 1.4384502719622105e-05 old loss 1.4479412129730918e-05 BETTER
I0328 18:19:38.315402 2315582 finetune.py:68] layer 22_up @ epoch 4 new loss 1.3565752851718571e-05 old loss 1.3687845239473972e-05 BETTER
I0328 18:19:56.023529 2315512 finetune.py:68] layer 21_gate @ epoch 1 new loss 1.6630194295430556e-05 old loss 1.6742214938858524e-05 BETTER
I0328 18:20:01.024792 2315652 finetune.py:68] layer 23_up @ epoch 3 new loss 1.46954744195682e-05 old loss 1.4830867257842328e-05 BETTER
I0328 18:20:02.859742 2315442 finetune.py:68] layer 20_gate @ epoch 2 new loss 1.4301694136520382e-05 old loss 1.4384502719622105e-05 BETTER
I0328 18:20:10.449635 2315582 finetune.py:45] layer 22_gate initial loss 1.7376121832057834e-05
I0328 18:20:25.607080 2315512 finetune.py:68] layer 21_gate @ epoch 2 new loss 1.6533835150767118e-05 old loss 1.6630194295430556e-05 BETTER
I0328 18:20:32.527953 2315652 finetune.py:68] layer 23_up @ epoch 4 new loss 1.4582417861674912e-05 old loss 1.46954744195682e-05 BETTER
I0328 18:20:34.406219 2315442 finetune.py:68] layer 20_gate @ epoch 3 new loss 1.4229409316612873e-05 old loss 1.4301694136520382e-05 BETTER
I0328 18:20:39.000131 2315582 finetune.py:68] layer 22_gate @ epoch 0 new loss 1.7248090443899855e-05 old loss 1.7376121832057834e-05 BETTER
I0328 18:20:55.144743 2315512 finetune.py:68] layer 21_gate @ epoch 3 new loss 1.645060365262907e-05 old loss 1.6533835150767118e-05 BETTER
I0328 18:21:03.838591 2315652 finetune.py:45] layer 23_gate initial loss 1.8885370081989095e-05
I0328 18:21:05.772387 2315442 finetune.py:68] layer 20_gate @ epoch 4 new loss 1.416409850207856e-05 old loss 1.4229409316612873e-05 BETTER
I0328 18:21:08.527502 2315582 finetune.py:68] layer 22_gate @ epoch 1 new loss 1.7145657693617977e-05 old loss 1.7248090443899855e-05 BETTER
I0328 18:21:24.689554 2315512 finetune.py:68] layer 21_gate @ epoch 4 new loss 1.6375668565160595e-05 old loss 1.645060365262907e-05 BETTER
I0328 18:21:32.038496 2315652 finetune.py:68] layer 23_gate @ epoch 0 new loss 1.8768454538076185e-05 old loss 1.8885370081989095e-05 BETTER
I0328 18:21:38.232167 2315582 finetune.py:68] layer 22_gate @ epoch 2 new loss 1.7059905076166615e-05 old loss 1.7145657693617977e-05 BETTER
I0328 18:22:01.051065 2315652 finetune.py:68] layer 23_gate @ epoch 1 new loss 1.867588798631914e-05 old loss 1.8768454538076185e-05 BETTER
I0328 18:22:02.503862 2315442 finetune.py:45] layer 20_down initial loss 2.5585135517758317e-05
I0328 18:22:07.985304 2315582 finetune.py:68] layer 22_gate @ epoch 3 new loss 1.6984286048682407e-05 old loss 1.7059905076166615e-05 BETTER
I0328 18:22:21.847206 2315512 finetune.py:45] layer 21_down initial loss 2.9691133022424765e-05
I0328 18:22:29.879172 2315442 finetune.py:68] layer 20_down @ epoch 0 new loss 2.5584604372852482e-05 old loss 2.5585135517758317e-05 BETTER
I0328 18:22:30.204745 2315652 finetune.py:68] layer 23_gate @ epoch 2 new loss 1.8597891539684497e-05 old loss 1.867588798631914e-05 BETTER
I0328 18:22:38.189576 2315582 finetune.py:68] layer 22_gate @ epoch 4 new loss 1.691707075224258e-05 old loss 1.6984286048682407e-05 BETTER
I0328 18:22:48.106019 2315512 finetune.py:68] layer 21_down @ epoch 0 new loss 2.9690660085179843e-05 old loss 2.9691133022424765e-05 BETTER
I0328 18:22:58.228821 2315442 finetune.py:68] layer 20_down @ epoch 1 new loss 2.558431151555851e-05 old loss 2.5584604372852482e-05 BETTER
I0328 18:22:59.616941 2315652 finetune.py:68] layer 23_gate @ epoch 3 new loss 1.85289991350146e-05 old loss 1.8597891539684497e-05 BETTER
I0328 18:23:15.301062 2315512 finetune.py:68] layer 21_down @ epoch 1 new loss 2.9690349037991837e-05 old loss 2.9690660085179843e-05 BETTER
I0328 18:23:27.332500 2315442 finetune.py:68] layer 20_down @ epoch 2 new loss 2.558408959885128e-05 old loss 2.558431151555851e-05 BETTER
I0328 18:23:28.809667 2315652 finetune.py:68] layer 23_gate @ epoch 4 new loss 1.847011299105361e-05 old loss 1.85289991350146e-05 BETTER
I0328 18:23:34.803846 2315582 finetune.py:45] layer 22_down initial loss 3.100846515735611e-05
I0328 18:23:42.735785 2315512 finetune.py:68] layer 21_down @ epoch 2 new loss 2.969015622511506e-05 old loss 2.9690349037991837e-05 BETTER
I0328 18:23:56.663951 2315442 finetune.py:68] layer 20_down @ epoch 3 new loss 2.5583913156879134e-05 old loss 2.558408959885128e-05 BETTER
I0328 18:24:01.019546 2315582 finetune.py:68] layer 22_down @ epoch 0 new loss 3.1007974030217156e-05 old loss 3.100846515735611e-05 BETTER
I0328 18:24:10.469860 2315512 finetune.py:68] layer 21_down @ epoch 3 new loss 2.9689983421121724e-05 old loss 2.969015622511506e-05 BETTER
I0328 18:24:26.126734 2315442 finetune.py:68] layer 20_down @ epoch 4 new loss 2.558378764661029e-05 old loss 2.5583913156879134e-05 BETTER
I0328 18:24:27.087652 2315652 finetune.py:45] layer 23_down initial loss 3.348810787429102e-05
20_v proxy err 0.0021306262351572514 tr(WHW.T) 330.192138671875
bpp_loss 4.447260332177393
20_q proxy err 0.00014700330211780965 tr(WHW.T) 20738.263671875
bpp_loss 5.2614087532274425
20_k proxy err 6.0308841057121754e-05 tr(WHW.T) 15393.0439453125
bpp_loss 6.259391036815941
20_o proxy err 0.0023393782321363688 tr(WHW.T) 1205.8402099609375
bpp_loss 4.483693222748116
20_up proxy err 0.0018029562197625637 tr(WHW.T) 7601.587890625
bpp_loss 4.570660241879523
20_gate proxy err 0.0004577917861752212 tr(WHW.T) 30664.861328125
bpp_loss 4.966653715952167
20_down proxy err 0.0020271448884159327 tr(WHW.T) 6305.5234375
bpp_loss 4.566658155793058
I0328 18:24:28.872241 2315582 finetune.py:68] layer 22_down @ epoch 1 new loss 3.100764297414571e-05 old loss 3.1007974030217156e-05 BETTER
I0328 18:24:38.366396 2315512 finetune.py:68] layer 21_down @ epoch 4 new loss 2.968990520457737e-05 old loss 2.9689983421121724e-05 BETTER
21_v proxy err 0.001965330448001623 tr(WHW.T) 362.87310791015625
bpp_loss 4.4729978298419155
21_q proxy err 0.00011926479783141986 tr(WHW.T) 25841.197265625
bpp_loss 5.255666424985975
21_k proxy err 5.504713408299722e-05 tr(WHW.T) 16784.95703125
bpp_loss 6.306794405565597
21_o proxy err 0.0015933335525915027 tr(WHW.T) 1266.883544921875
bpp_loss 4.50346652767621
21_up proxy err 0.0017311392584815621 tr(WHW.T) 7772.4013671875
bpp_loss 4.5736052381273895
21_gate proxy err 0.0004373461997602135 tr(WHW.T) 31548.42578125
bpp_loss 4.978741458949766
21_down proxy err 0.0019129920983687043 tr(WHW.T) 6356.27978515625
bpp_loss 4.568094594204532
I0328 18:24:53.244060 2315652 finetune.py:68] layer 23_down @ epoch 0 new loss 3.3487423934275284e-05 old loss 3.348810787429102e-05 BETTER
I0328 18:24:56.940795 2315582 finetune.py:68] layer 22_down @ epoch 2 new loss 3.100736648775637e-05 old loss 3.100764297414571e-05 BETTER
I0328 18:25:20.209810 2315652 finetune.py:68] layer 23_down @ epoch 1 new loss 3.34871765517164e-05 old loss 3.3487423934275284e-05 BETTER
I0328 18:25:24.712098 2315582 finetune.py:68] layer 22_down @ epoch 3 new loss 3.1007188226794824e-05 old loss 3.100736648775637e-05 BETTER
I0328 18:25:47.163142 2315652 finetune.py:68] layer 23_down @ epoch 2 new loss 3.348689278936945e-05 old loss 3.34871765517164e-05 BETTER
I0328 18:25:49.860619 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 66.43803262710571s
I0328 18:25:52.448674 2315582 finetune.py:68] layer 22_down @ epoch 4 new loss 3.100699541391805e-05 old loss 3.1007188226794824e-05 BETTER
I0328 18:25:53.723999 2315722 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:25:53.724102 2315722 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:25:53.724143 2315722 utils.py:162] NumExpr defaulting to 16 threads.
22_v proxy err 0.001953914761543274 tr(WHW.T) 346.0789489746094
bpp_loss 4.523344975954387
22_q proxy err 0.00014359995839186013 tr(WHW.T) 20322.529296875
bpp_loss 5.211681797285564
22_k proxy err 6.104460771894082e-05 tr(WHW.T) 14722.091796875
bpp_loss 6.233501786366105
22_o proxy err 0.0021819504909217358 tr(WHW.T) 1222.2720947265625
bpp_loss 4.529279937851243
22_up proxy err 0.0018030210630968213 tr(WHW.T) 7548.26513671875
bpp_loss 4.5775068222678135
22_gate proxy err 0.00047227286268025637 tr(WHW.T) 29533.31640625
bpp_loss 4.980743154617293
22_down proxy err 0.0019171504536643624 tr(WHW.T) 6534.15771484375
bpp_loss 4.57318569972579
I0328 18:25:54.078489 2315722 config.py:54] PyTorch version 2.6.0 available.
W0328 18:25:54.289782 2315722 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:25:54.866769 2315722 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:25:54.870576 2313906 quantize_finetune_llama.py:209] layer 25 gpu 1
I0328 18:25:54.884302 2315722 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:26:11.762579 2315722 finetune.py:45] layer 24_v initial loss 9.573489478498232e-06
W0328 18:26:11.762805 2315722 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:26:14.313303 2315652 finetune.py:68] layer 23_down @ epoch 3 new loss 3.3486649044789374e-05 old loss 3.348689278936945e-05 BETTER
I0328 18:26:41.462076 2315652 finetune.py:68] layer 23_down @ epoch 4 new loss 3.348659083712846e-05 old loss 3.3486649044789374e-05 BETTER
23_v proxy err 0.001825840910896659 tr(WHW.T) 397.90704345703125
bpp_loss 4.575637846777681
23_q proxy err 0.00013769797806162387 tr(WHW.T) 22615.671875
bpp_loss 5.223592030000873
23_k proxy err 6.296679930528626e-05 tr(WHW.T) 14857.98046875
bpp_loss 6.234039414674044
23_o proxy err 0.0018480628496035933 tr(WHW.T) 1744.2869873046875
bpp_loss 4.55112520698458
23_up proxy err 0.0018334941705688834 tr(WHW.T) 7418.71435546875
bpp_loss 4.5814619889882
23_gate proxy err 0.0005108888726681471 tr(WHW.T) 27266.7734375
bpp_loss 4.983035921651338
23_down proxy err 0.0019194677006453276 tr(WHW.T) 6673.2802734375
bpp_loss 4.5782845254122675
I0328 18:26:46.644686 2315722 finetune.py:68] layer 24_v @ epoch 0 new loss 2.257916321468656e-06 old loss 9.573489478498232e-06 BETTER
I0328 18:27:00.125020 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 63.34070014953613s
I0328 18:27:03.676466 2315792 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:27:03.676571 2315792 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:27:03.676617 2315792 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:27:04.010793 2315792 config.py:54] PyTorch version 2.6.0 available.
W0328 18:27:04.205408 2315792 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:27:04.781481 2315792 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:27:04.785632 2313906 quantize_finetune_llama.py:209] layer 26 gpu 2
I0328 18:27:04.799444 2315792 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:27:21.774889 2315792 finetune.py:45] layer 25_v initial loss 1.1095259651483502e-05
W0328 18:27:21.775111 2315792 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:27:22.893839 2315722 finetune.py:68] layer 24_v @ epoch 1 new loss 1.9946717202401487e-06 old loss 2.257916321468656e-06 BETTER
I0328 18:27:54.929109 2315792 finetune.py:68] layer 25_v @ epoch 0 new loss 2.6070740659633884e-06 old loss 1.1095259651483502e-05 BETTER
I0328 18:27:59.715467 2315722 finetune.py:68] layer 24_v @ epoch 2 new loss 1.926930963236373e-06 old loss 1.9946717202401487e-06 BETTER
I0328 18:28:08.249746 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 63.0104763507843s
I0328 18:28:11.986069 2315862 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:28:11.986167 2315862 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:28:11.986209 2315862 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:28:12.335299 2315862 config.py:54] PyTorch version 2.6.0 available.
W0328 18:28:12.535449 2315862 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:28:13.121107 2315862 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:28:13.125642 2313906 quantize_finetune_llama.py:209] layer 27 gpu 3
I0328 18:28:13.140029 2315862 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:28:29.442246 2315792 finetune.py:68] layer 25_v @ epoch 1 new loss 2.377400960540399e-06 old loss 2.6070740659633884e-06 BETTER
I0328 18:28:30.397144 2315862 finetune.py:45] layer 26_v initial loss 1.019283172354335e-05
W0328 18:28:30.397381 2315862 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:28:36.880752 2315722 finetune.py:68] layer 24_v @ epoch 3 new loss 1.9001732880497002e-06 old loss 1.926930963236373e-06 BETTER
I0328 18:29:04.049400 2315862 finetune.py:68] layer 26_v @ epoch 0 new loss 3.558785920176888e-06 old loss 1.019283172354335e-05 BETTER
I0328 18:29:04.434049 2315792 finetune.py:68] layer 25_v @ epoch 2 new loss 2.2755343707103748e-06 old loss 2.377400960540399e-06 BETTER
I0328 18:29:13.894479 2315722 finetune.py:68] layer 24_v @ epoch 4 new loss 1.8712879636950674e-06 old loss 1.9001732880497002e-06 BETTER
I0328 18:29:17.673853 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 64.1067111492157s
I0328 18:29:21.521406 2315932 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:29:21.521508 2315932 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:29:21.521550 2315932 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:29:21.888003 2315932 config.py:54] PyTorch version 2.6.0 available.
W0328 18:29:22.089310 2315932 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:29:22.891745 2315932 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:29:22.895462 2313906 quantize_finetune_llama.py:209] layer 28 gpu 0
I0328 18:29:22.908813 2315932 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 18:29:33.067728 2315722 finetune.py:45] layer 24_q initial loss 2.3317952582146972e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:29:38.735623 2315862 finetune.py:68] layer 26_v @ epoch 1 new loss 3.3085414088418474e-06 old loss 3.558785920176888e-06 BETTER
I0328 18:29:39.141141 2315792 finetune.py:76] layer 25_v @ epoch 3 new loss 2.469448418196407e-06 old loss 2.2755343707103748e-06 WORSE
I0328 18:29:40.833216 2315932 finetune.py:45] layer 27_v initial loss 1.0723470950324554e-05
W0328 18:29:40.833440 2315932 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:30:08.193381 2315722 finetune.py:68] layer 24_q @ epoch 0 new loss 2.2350916424329625e-06 old loss 2.3317952582146972e-06 BETTER
I0328 18:30:13.585689 2315792 finetune.py:68] layer 25_v @ epoch 4 new loss 2.2238955352804624e-06 old loss 2.2755343707103748e-06 BETTER
I0328 18:30:14.023936 2315932 finetune.py:68] layer 27_v @ epoch 0 new loss 3.2771192763902945e-06 old loss 1.0723470950324554e-05 BETTER
I0328 18:30:14.038182 2315862 finetune.py:76] layer 26_v @ epoch 2 new loss 3.3361864097969374e-06 old loss 3.3085414088418474e-06 WORSE
I0328 18:30:32.820083 2315792 finetune.py:45] layer 25_q initial loss 3.097695980613935e-06
I0328 18:30:44.437552 2315722 finetune.py:68] layer 24_q @ epoch 1 new loss 2.172492258978309e-06 old loss 2.2350916424329625e-06 BETTER
I0328 18:30:48.185499 2315932 finetune.py:68] layer 27_v @ epoch 1 new loss 3.1264241897588363e-06 old loss 3.2771192763902945e-06 BETTER
I0328 18:30:48.514510 2315862 finetune.py:68] layer 26_v @ epoch 3 new loss 3.1889901492831996e-06 old loss 3.3085414088418474e-06 BETTER
I0328 18:31:06.237927 2315792 finetune.py:68] layer 25_q @ epoch 0 new loss 2.9140660444682e-06 old loss 3.097695980613935e-06 BETTER
I0328 18:31:20.943298 2315722 finetune.py:68] layer 24_q @ epoch 2 new loss 2.1521309463423677e-06 old loss 2.172492258978309e-06 BETTER
I0328 18:31:22.917230 2315932 finetune.py:76] layer 27_v @ epoch 2 new loss 3.2356056181015447e-06 old loss 3.1264241897588363e-06 WORSE
I0328 18:31:23.846408 2315862 finetune.py:68] layer 26_v @ epoch 4 new loss 3.178631004630006e-06 old loss 3.1889901492831996e-06 BETTER
I0328 18:31:40.490189 2315792 finetune.py:68] layer 25_q @ epoch 1 new loss 2.80321160062158e-06 old loss 2.9140660444682e-06 BETTER
I0328 18:31:43.594440 2315862 finetune.py:45] layer 26_q initial loss 3.890077096002642e-06
I0328 18:31:56.781965 2315932 finetune.py:76] layer 27_v @ epoch 3 new loss 3.279693601143663e-06 old loss 3.1264241897588363e-06 WORSE
I0328 18:31:57.633126 2315722 finetune.py:68] layer 24_q @ epoch 3 new loss 2.1304883830453036e-06 old loss 2.1521309463423677e-06 BETTER
I0328 18:32:15.014583 2315792 finetune.py:68] layer 25_q @ epoch 2 new loss 2.7895268885913538e-06 old loss 2.80321160062158e-06 BETTER
I0328 18:32:17.159998 2315862 finetune.py:68] layer 26_q @ epoch 0 new loss 3.687731123136473e-06 old loss 3.890077096002642e-06 BETTER
I0328 18:32:30.648741 2315932 finetune.py:68] layer 27_v @ epoch 4 new loss 3.0954572594055207e-06 old loss 3.1264241897588363e-06 BETTER
I0328 18:32:34.193611 2315722 finetune.py:68] layer 24_q @ epoch 4 new loss 2.1167418253753567e-06 old loss 2.1304883830453036e-06 BETTER
I0328 18:32:49.826567 2315792 finetune.py:68] layer 25_q @ epoch 3 new loss 2.745714482443873e-06 old loss 2.7895268885913538e-06 BETTER
I0328 18:32:50.808451 2315932 finetune.py:45] layer 27_q initial loss 4.197378984827083e-06
I0328 18:32:52.149738 2315862 finetune.py:68] layer 26_q @ epoch 1 new loss 3.595478574425215e-06 old loss 3.687731123136473e-06 BETTER
I0328 18:32:52.358968 2315722 finetune.py:45] layer 24_k initial loss 2.4859321001713397e-06
I0328 18:33:24.038011 2315932 finetune.py:68] layer 27_q @ epoch 0 new loss 4.019151674583554e-06 old loss 4.197378984827083e-06 BETTER
I0328 18:33:24.809737 2315792 finetune.py:68] layer 25_q @ epoch 4 new loss 2.6848499601328513e-06 old loss 2.745714482443873e-06 BETTER
I0328 18:33:26.814808 2315862 finetune.py:68] layer 26_q @ epoch 2 new loss 3.570065018720925e-06 old loss 3.595478574425215e-06 BETTER
I0328 18:33:27.840272 2315722 finetune.py:68] layer 24_k @ epoch 0 new loss 2.445783593429951e-06 old loss 2.4859321001713397e-06 BETTER
I0328 18:33:42.882560 2315792 finetune.py:45] layer 25_k initial loss 3.3060048281186027e-06
I0328 18:33:58.334266 2315932 finetune.py:68] layer 27_q @ epoch 1 new loss 3.824885880021611e-06 old loss 4.019151674583554e-06 BETTER
I0328 18:34:01.611249 2315862 finetune.py:68] layer 26_q @ epoch 3 new loss 3.542509830367635e-06 old loss 3.570065018720925e-06 BETTER
I0328 18:34:04.033198 2315722 finetune.py:68] layer 24_k @ epoch 1 new loss 2.42477358369797e-06 old loss 2.445783593429951e-06 BETTER
I0328 18:34:15.999211 2315792 finetune.py:68] layer 25_k @ epoch 0 new loss 3.2454775009682635e-06 old loss 3.3060048281186027e-06 BETTER
I0328 18:34:32.683516 2315932 finetune.py:68] layer 27_q @ epoch 2 new loss 3.7915235679975012e-06 old loss 3.824885880021611e-06 BETTER
I0328 18:34:36.324683 2315862 finetune.py:76] layer 26_q @ epoch 4 new loss 3.5552882309275446e-06 old loss 3.542509830367635e-06 WORSE
I0328 18:34:40.544578 2315722 finetune.py:68] layer 24_k @ epoch 2 new loss 2.418992153252475e-06 old loss 2.42477358369797e-06 BETTER
I0328 18:34:50.089366 2315792 finetune.py:68] layer 25_k @ epoch 1 new loss 3.214549678887124e-06 old loss 3.2454775009682635e-06 BETTER
I0328 18:34:54.217457 2315862 finetune.py:45] layer 26_k initial loss 4.0588151932752226e-06
I0328 18:35:06.944588 2315932 finetune.py:68] layer 27_q @ epoch 3 new loss 3.7695276660087984e-06 old loss 3.7915235679975012e-06 BETTER
I0328 18:35:16.919338 2315722 finetune.py:68] layer 24_k @ epoch 3 new loss 2.4175571979867527e-06 old loss 2.418992153252475e-06 BETTER
I0328 18:35:24.374722 2315792 finetune.py:68] layer 25_k @ epoch 2 new loss 3.190981715306407e-06 old loss 3.214549678887124e-06 BETTER
I0328 18:35:27.807198 2315862 finetune.py:68] layer 26_k @ epoch 0 new loss 3.976685093221022e-06 old loss 4.0588151932752226e-06 BETTER
I0328 18:35:41.001906 2315932 finetune.py:76] layer 27_q @ epoch 4 new loss 3.782947715080809e-06 old loss 3.7695276660087984e-06 WORSE
I0328 18:35:53.300139 2315722 finetune.py:68] layer 24_k @ epoch 4 new loss 2.393640670561581e-06 old loss 2.4175571979867527e-06 BETTER
I0328 18:35:58.473232 2315932 finetune.py:45] layer 27_k initial loss 4.6736440708627924e-06
I0328 18:35:58.816738 2315792 finetune.py:68] layer 25_k @ epoch 3 new loss 3.179570740030613e-06 old loss 3.190981715306407e-06 BETTER
I0328 18:36:02.209707 2315862 finetune.py:68] layer 26_k @ epoch 1 new loss 3.941956947528524e-06 old loss 3.976685093221022e-06 BETTER
I0328 18:36:13.028995 2315722 finetune.py:45] layer 24_o initial loss 5.62959485250758e-06
I0328 18:36:31.304994 2315932 finetune.py:68] layer 27_k @ epoch 0 new loss 4.5630131353391334e-06 old loss 4.6736440708627924e-06 BETTER
I0328 18:36:33.356973 2315792 finetune.py:68] layer 25_k @ epoch 4 new loss 3.174517360093887e-06 old loss 3.179570740030613e-06 BETTER
I0328 18:36:36.711866 2315862 finetune.py:76] layer 26_k @ epoch 2 new loss 3.957444278057665e-06 old loss 3.941956947528524e-06 WORSE
I0328 18:36:47.506590 2315722 finetune.py:68] layer 24_o @ epoch 0 new loss 5.519684236787725e-06 old loss 5.62959485250758e-06 BETTER
I0328 18:36:53.030148 2315792 finetune.py:45] layer 25_o initial loss 6.874653536215192e-06
I0328 18:37:05.435516 2315932 finetune.py:68] layer 27_k @ epoch 1 new loss 4.491630534175783e-06 old loss 4.5630131353391334e-06 BETTER
I0328 18:37:10.536192 2315862 finetune.py:68] layer 26_k @ epoch 3 new loss 3.928452315449249e-06 old loss 3.941956947528524e-06 BETTER
I0328 18:37:23.052302 2315722 finetune.py:68] layer 24_o @ epoch 1 new loss 5.460996362671722e-06 old loss 5.519684236787725e-06 BETTER
I0328 18:37:25.619092 2315792 finetune.py:68] layer 25_o @ epoch 0 new loss 6.7102278080710676e-06 old loss 6.874653536215192e-06 BETTER
I0328 18:37:39.323874 2315932 finetune.py:68] layer 27_k @ epoch 2 new loss 4.490459559747251e-06 old loss 4.491630534175783e-06 BETTER
I0328 18:37:45.185460 2315862 finetune.py:76] layer 26_k @ epoch 4 new loss 3.930775619664928e-06 old loss 3.928452315449249e-06 WORSE
I0328 18:37:59.071184 2315722 finetune.py:68] layer 24_o @ epoch 2 new loss 5.423731181508629e-06 old loss 5.460996362671722e-06 BETTER
I0328 18:37:59.569308 2315792 finetune.py:68] layer 25_o @ epoch 1 new loss 6.586130439245608e-06 old loss 6.7102278080710676e-06 BETTER
I0328 18:38:04.659034 2315862 finetune.py:45] layer 26_o initial loss 9.113711712416261e-06
I0328 18:38:13.437493 2315932 finetune.py:76] layer 27_k @ epoch 3 new loss 4.499290753301466e-06 old loss 4.490459559747251e-06 WORSE
I0328 18:38:33.469712 2315792 finetune.py:68] layer 25_o @ epoch 2 new loss 6.531382041430334e-06 old loss 6.586130439245608e-06 BETTER
I0328 18:38:34.852931 2315722 finetune.py:68] layer 24_o @ epoch 3 new loss 5.393037099565845e-06 old loss 5.423731181508629e-06 BETTER
I0328 18:38:37.635724 2315862 finetune.py:68] layer 26_o @ epoch 0 new loss 8.788261766312644e-06 old loss 9.113711712416261e-06 BETTER
I0328 18:38:46.711014 2315932 finetune.py:76] layer 27_k @ epoch 4 new loss 4.4928956413059495e-06 old loss 4.490459559747251e-06 WORSE
I0328 18:39:05.710531 2315932 finetune.py:45] layer 27_o initial loss 1.0655536243575625e-05
I0328 18:39:07.339239 2315792 finetune.py:68] layer 25_o @ epoch 3 new loss 6.495833531516837e-06 old loss 6.531382041430334e-06 BETTER
I0328 18:39:10.721003 2315722 finetune.py:68] layer 24_o @ epoch 4 new loss 5.3769258556712884e-06 old loss 5.393037099565845e-06 BETTER
I0328 18:39:11.549392 2315862 finetune.py:68] layer 26_o @ epoch 1 new loss 8.629457624920178e-06 old loss 8.788261766312644e-06 BETTER
I0328 18:39:37.545094 2315932 finetune.py:68] layer 27_o @ epoch 0 new loss 1.0353099241910968e-05 old loss 1.0655536243575625e-05 BETTER
I0328 18:39:41.425516 2315792 finetune.py:76] layer 25_o @ epoch 4 new loss 6.504642442450859e-06 old loss 6.495833531516837e-06 WORSE
I0328 18:39:42.145382 2315722 finetune.py:45] layer 24_up initial loss 1.7012616808642633e-05
I0328 18:39:45.595290 2315862 finetune.py:68] layer 26_o @ epoch 2 new loss 8.5617029981222e-06 old loss 8.629457624920178e-06 BETTER
I0328 18:40:10.848578 2315932 finetune.py:68] layer 27_o @ epoch 1 new loss 1.0192529771302361e-05 old loss 1.0353099241910968e-05 BETTER
I0328 18:40:12.452226 2315792 finetune.py:45] layer 25_up initial loss 1.9567196432035416e-05
I0328 18:40:14.028893 2315722 finetune.py:68] layer 24_up @ epoch 0 new loss 1.6712559954612516e-05 old loss 1.7012616808642633e-05 BETTER
I0328 18:40:19.471026 2315862 finetune.py:68] layer 26_o @ epoch 3 new loss 8.48468243930256e-06 old loss 8.5617029981222e-06 BETTER
I0328 18:40:42.889187 2315792 finetune.py:68] layer 25_up @ epoch 0 new loss 1.9221908587496728e-05 old loss 1.9567196432035416e-05 BETTER
I0328 18:40:44.270087 2315932 finetune.py:68] layer 27_o @ epoch 2 new loss 1.0096439837070648e-05 old loss 1.0192529771302361e-05 BETTER
I0328 18:40:47.214038 2315722 finetune.py:68] layer 24_up @ epoch 1 new loss 1.650998274271842e-05 old loss 1.6712559954612516e-05 BETTER
I0328 18:40:53.840920 2315862 finetune.py:68] layer 26_o @ epoch 4 new loss 8.435215931967832e-06 old loss 8.48468243930256e-06 BETTER
I0328 18:41:14.384536 2315792 finetune.py:68] layer 25_up @ epoch 1 new loss 1.8992928744410165e-05 old loss 1.9221908587496728e-05 BETTER
I0328 18:41:17.484512 2315932 finetune.py:68] layer 27_o @ epoch 3 new loss 1.0019626643043011e-05 old loss 1.0096439837070648e-05 BETTER
I0328 18:41:20.553396 2315722 finetune.py:68] layer 24_up @ epoch 2 new loss 1.6353755199816078e-05 old loss 1.650998274271842e-05 BETTER
I0328 18:41:25.873250 2315862 finetune.py:45] layer 26_up initial loss 2.3727427105768584e-05
I0328 18:41:46.127505 2315792 finetune.py:68] layer 25_up @ epoch 2 new loss 1.8818775060935877e-05 old loss 1.8992928744410165e-05 BETTER
I0328 18:41:50.693649 2315932 finetune.py:68] layer 27_o @ epoch 4 new loss 9.976255569199566e-06 old loss 1.0019626643043011e-05 BETTER
I0328 18:41:54.133308 2315722 finetune.py:68] layer 24_up @ epoch 3 new loss 1.6223304555751383e-05 old loss 1.6353755199816078e-05 BETTER
I0328 18:41:56.601486 2315862 finetune.py:68] layer 26_up @ epoch 0 new loss 2.332402436877601e-05 old loss 2.3727427105768584e-05 BETTER
I0328 18:42:17.975514 2315792 finetune.py:68] layer 25_up @ epoch 3 new loss 1.8675624232855625e-05 old loss 1.8818775060935877e-05 BETTER
I0328 18:42:22.389346 2315932 finetune.py:45] layer 27_up initial loss 2.782128103717696e-05
I0328 18:42:27.923866 2315722 finetune.py:68] layer 24_up @ epoch 4 new loss 1.611678999324795e-05 old loss 1.6223304555751383e-05 BETTER
I0328 18:42:28.472571 2315862 finetune.py:68] layer 26_up @ epoch 1 new loss 2.3045033231028356e-05 old loss 2.332402436877601e-05 BETTER
I0328 18:42:49.806190 2315792 finetune.py:68] layer 25_up @ epoch 4 new loss 1.8557266230345704e-05 old loss 1.8675624232855625e-05 BETTER
I0328 18:42:52.403924 2315932 finetune.py:68] layer 27_up @ epoch 0 new loss 2.7314423277857713e-05 old loss 2.782128103717696e-05 BETTER
I0328 18:42:59.148485 2315722 finetune.py:45] layer 24_gate initial loss 2.105039129673969e-05
I0328 18:43:00.462175 2315862 finetune.py:68] layer 26_up @ epoch 2 new loss 2.283424328197725e-05 old loss 2.3045033231028356e-05 BETTER
I0328 18:43:21.274875 2315792 finetune.py:45] layer 25_gate initial loss 2.4278982891701162e-05
I0328 18:43:23.443453 2315932 finetune.py:68] layer 27_up @ epoch 1 new loss 2.6972558771376498e-05 old loss 2.7314423277857713e-05 BETTER
I0328 18:43:28.883034 2315722 finetune.py:68] layer 24_gate @ epoch 0 new loss 2.0939614842063747e-05 old loss 2.105039129673969e-05 BETTER
I0328 18:43:32.523119 2315862 finetune.py:68] layer 26_up @ epoch 3 new loss 2.265845614601858e-05 old loss 2.283424328197725e-05 BETTER
I0328 18:43:49.573878 2315792 finetune.py:68] layer 25_gate @ epoch 0 new loss 2.4156544895959087e-05 old loss 2.4278982891701162e-05 BETTER
I0328 18:43:54.746811 2315932 finetune.py:68] layer 27_up @ epoch 2 new loss 2.6716565116657875e-05 old loss 2.6972558771376498e-05 BETTER
I0328 18:43:59.949993 2315722 finetune.py:68] layer 24_gate @ epoch 1 new loss 2.085024243569933e-05 old loss 2.0939614842063747e-05 BETTER
I0328 18:44:04.725284 2315862 finetune.py:68] layer 26_up @ epoch 4 new loss 2.2512709620059468e-05 old loss 2.265845614601858e-05 BETTER
I0328 18:44:19.007576 2315792 finetune.py:68] layer 25_gate @ epoch 1 new loss 2.4059372663032264e-05 old loss 2.4156544895959087e-05 BETTER
I0328 18:44:25.899867 2315932 finetune.py:68] layer 27_up @ epoch 3 new loss 2.650329224707093e-05 old loss 2.6716565116657875e-05 BETTER
I0328 18:44:31.072611 2315722 finetune.py:68] layer 24_gate @ epoch 2 new loss 2.0777066310984083e-05 old loss 2.085024243569933e-05 BETTER
I0328 18:44:36.660879 2315862 finetune.py:45] layer 26_gate initial loss 2.9304857889655977e-05
I0328 18:44:48.419874 2315792 finetune.py:68] layer 25_gate @ epoch 2 new loss 2.3978145691216923e-05 old loss 2.4059372663032264e-05 BETTER
I0328 18:44:57.150665 2315932 finetune.py:68] layer 27_up @ epoch 4 new loss 2.6333864298067056e-05 old loss 2.650329224707093e-05 BETTER
I0328 18:45:02.410397 2315722 finetune.py:68] layer 24_gate @ epoch 3 new loss 2.0713092453661375e-05 old loss 2.0777066310984083e-05 BETTER
I0328 18:45:05.174472 2315862 finetune.py:68] layer 26_gate @ epoch 0 new loss 2.9157428798498586e-05 old loss 2.9304857889655977e-05 BETTER
I0328 18:45:17.877405 2315792 finetune.py:68] layer 25_gate @ epoch 3 new loss 2.3908483854029328e-05 old loss 2.3978145691216923e-05 BETTER
I0328 18:45:28.753666 2315932 finetune.py:45] layer 27_gate initial loss 3.4752334613585845e-05
I0328 18:45:33.792187 2315722 finetune.py:68] layer 24_gate @ epoch 4 new loss 2.065869739453774e-05 old loss 2.0713092453661375e-05 BETTER
I0328 18:45:34.722783 2315862 finetune.py:68] layer 26_gate @ epoch 1 new loss 2.9043174436083063e-05 old loss 2.9157428798498586e-05 BETTER
I0328 18:45:47.405668 2315792 finetune.py:68] layer 25_gate @ epoch 4 new loss 2.3850883735576645e-05 old loss 2.3908483854029328e-05 BETTER
I0328 18:45:56.627851 2315932 finetune.py:68] layer 27_gate @ epoch 0 new loss 3.45657899742946e-05 old loss 3.4752334613585845e-05 BETTER
I0328 18:46:04.409712 2315862 finetune.py:68] layer 26_gate @ epoch 2 new loss 2.89508043351816e-05 old loss 2.9043174436083063e-05 BETTER
I0328 18:46:25.721498 2315932 finetune.py:68] layer 27_gate @ epoch 1 new loss 3.441423177719116e-05 old loss 3.45657899742946e-05 BETTER
I0328 18:46:28.997611 2315722 finetune.py:45] layer 24_down initial loss 3.647550693131052e-05
I0328 18:46:34.320410 2315862 finetune.py:68] layer 26_gate @ epoch 3 new loss 2.8868800654890947e-05 old loss 2.89508043351816e-05 BETTER
I0328 18:46:43.759891 2315792 finetune.py:45] layer 25_down initial loss 4.098501449334435e-05
I0328 18:46:54.847914 2315932 finetune.py:68] layer 27_gate @ epoch 2 new loss 3.429892240092158e-05 old loss 3.441423177719116e-05 BETTER
I0328 18:46:56.218112 2315722 finetune.py:68] layer 24_down @ epoch 0 new loss 3.6474863009061664e-05 old loss 3.647550693131052e-05 BETTER
I0328 18:47:04.216931 2315862 finetune.py:68] layer 26_gate @ epoch 4 new loss 2.8802276574424468e-05 old loss 2.8868800654890947e-05 BETTER
I0328 18:47:09.821906 2315792 finetune.py:68] layer 25_down @ epoch 0 new loss 4.0982769860420376e-05 old loss 4.098501449334435e-05 BETTER
I0328 18:47:23.991971 2315932 finetune.py:68] layer 27_gate @ epoch 3 new loss 3.41997692885343e-05 old loss 3.429892240092158e-05 BETTER
I0328 18:47:24.812935 2315722 finetune.py:68] layer 24_down @ epoch 1 new loss 3.6474335502134636e-05 old loss 3.6474863009061664e-05 BETTER
I0328 18:47:36.846630 2315792 finetune.py:68] layer 25_down @ epoch 1 new loss 4.098148929188028e-05 old loss 4.0982769860420376e-05 BETTER
I0328 18:47:53.338472 2315932 finetune.py:68] layer 27_gate @ epoch 4 new loss 3.411809666431509e-05 old loss 3.41997692885343e-05 BETTER
I0328 18:47:53.823057 2315722 finetune.py:68] layer 24_down @ epoch 2 new loss 3.647406992968172e-05 old loss 3.6474335502134636e-05 BETTER
I0328 18:48:01.269387 2315862 finetune.py:45] layer 26_down initial loss 4.8472404159838334e-05
I0328 18:48:04.139040 2315792 finetune.py:68] layer 25_down @ epoch 2 new loss 4.098059798707254e-05 old loss 4.098148929188028e-05 BETTER
I0328 18:48:22.424372 2315722 finetune.py:68] layer 24_down @ epoch 3 new loss 3.647361882030964e-05 old loss 3.647406992968172e-05 BETTER
I0328 18:48:27.547546 2315862 finetune.py:68] layer 26_down @ epoch 0 new loss 4.847127274842933e-05 old loss 4.8472404159838334e-05 BETTER
I0328 18:48:31.666317 2315792 finetune.py:68] layer 25_down @ epoch 3 new loss 4.0979819459607825e-05 old loss 4.098059798707254e-05 BETTER
I0328 18:48:51.207999 2315932 finetune.py:45] layer 27_down initial loss 5.7121018471661955e-05
I0328 18:48:51.310582 2315722 finetune.py:68] layer 24_down @ epoch 4 new loss 3.647325866040774e-05 old loss 3.647361882030964e-05 BETTER
24_v proxy err 0.0015239444328472018 tr(WHW.T) 467.2783508300781
bpp_loss 4.675452624331228
24_q proxy err 0.0001353915431536734 tr(WHW.T) 22440.966796875
bpp_loss 5.189144950243644
24_k proxy err 6.38833298580721e-05 tr(WHW.T) 14183.8994140625
bpp_loss 6.044786274433136
24_o proxy err 0.0018063437892124057 tr(WHW.T) 1591.25634765625
bpp_loss 4.593744055368006
24_up proxy err 0.0018798831151798368 tr(WHW.T) 7311.13037109375
bpp_loss 4.586188137464758
24_gate proxy err 0.000544163747690618 tr(WHW.T) 25871.65625
bpp_loss 4.989331053969051
24_down proxy err 0.0019029674585908651 tr(WHW.T) 6747.3974609375
bpp_loss 4.583744183149455
I0328 18:48:55.140251 2315862 finetune.py:68] layer 26_down @ epoch 1 new loss 4.847076706937514e-05 old loss 4.847127274842933e-05 BETTER
I0328 18:48:59.880358 2315792 finetune.py:68] layer 25_down @ epoch 4 new loss 4.0979131881613284e-05 old loss 4.0979819459607825e-05 BETTER
25_v proxy err 0.0013063453370705247 tr(WHW.T) 557.8086547851562
bpp_loss 4.685693298233673
25_q proxy err 0.00011813841410912573 tr(WHW.T) 26116.20703125
bpp_loss 5.1650118679972365
25_k proxy err 6.173231668071821e-05 tr(WHW.T) 14413.8994140625
bpp_loss 6.025240028742701
25_o proxy err 0.0015164322685450315 tr(WHW.T) 1991.3702392578125
bpp_loss 4.594401835813187
25_up proxy err 0.0018592325504869223 tr(WHW.T) 7383.88134765625
bpp_loss 4.595632030296007
25_gate proxy err 0.000534331367816776 tr(WHW.T) 26310.9140625
bpp_loss 4.999951370393059
25_down proxy err 0.001899166265502572 tr(WHW.T) 6621.29541015625
bpp_loss 4.592830000678077
I0328 18:49:17.381890 2315932 finetune.py:68] layer 27_down @ epoch 0 new loss 5.7119505072478205e-05 old loss 5.7121018471661955e-05 BETTER
I0328 18:49:23.011318 2315862 finetune.py:68] layer 26_down @ epoch 2 new loss 4.8470326873939484e-05 old loss 4.847076706937514e-05 BETTER
I0328 18:49:44.238376 2315932 finetune.py:68] layer 27_down @ epoch 1 new loss 5.7118690165225416e-05 old loss 5.7119505072478205e-05 BETTER
I0328 18:49:50.774724 2315862 finetune.py:68] layer 26_down @ epoch 3 new loss 4.846979572903365e-05 old loss 4.8470326873939484e-05 BETTER
I0328 18:50:11.234762 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 66.41108846664429s
I0328 18:50:11.291409 2315932 finetune.py:68] layer 27_down @ epoch 2 new loss 5.7117955293506384e-05 old loss 5.7118690165225416e-05 BETTER
I0328 18:50:15.121102 2316002 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:50:15.121205 2316002 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:50:15.121247 2316002 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:50:15.518533 2316002 config.py:54] PyTorch version 2.6.0 available.
W0328 18:50:15.733121 2316002 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:50:16.366318 2316002 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:50:16.370844 2313906 quantize_finetune_llama.py:209] layer 29 gpu 1
I0328 18:50:16.385539 2316002 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 18:50:18.613459 2315862 finetune.py:68] layer 26_down @ epoch 4 new loss 4.8469399189343676e-05 old loss 4.846979572903365e-05 BETTER
26_v proxy err 0.0015933813992887735 tr(WHW.T) 434.54583740234375
bpp_loss 4.738523366162553
26_q proxy err 0.00013810594100505114 tr(WHW.T) 21405.375
bpp_loss 5.175423080450855
26_k proxy err 5.80058986088261e-05 tr(WHW.T) 15423.7626953125
bpp_loss 6.117677121888846
26_o proxy err 0.0010900306515395641 tr(WHW.T) 2387.672607421875
bpp_loss 4.611650678212754
26_up proxy err 0.001795380376279354 tr(WHW.T) 7648.22216796875
bpp_loss 4.604899058451077
26_gate proxy err 0.00048659188905730844 tr(WHW.T) 28907.494140625
bpp_loss 5.009824424449887
26_down proxy err 0.0018985312199220061 tr(WHW.T) 6625.30908203125
bpp_loss 4.600810675050265
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:50:33.307981 2316002 finetune.py:45] layer 28_v initial loss 1.3026986380282324e-05
W0328 18:50:33.308205 2316002 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:50:38.506372 2315932 finetune.py:68] layer 27_down @ epoch 3 new loss 5.7117340475087985e-05 old loss 5.7117955293506384e-05 BETTER
I0328 18:51:05.787373 2315932 finetune.py:68] layer 27_down @ epoch 4 new loss 5.711669291486032e-05 old loss 5.7117340475087985e-05 BETTER
27_v proxy err 0.0010974656324833632 tr(WHW.T) 677.69384765625
bpp_loss 4.833270658098627
27_q proxy err 0.00014593146624974906 tr(WHW.T) 21307.462890625
bpp_loss 5.143095601059031
27_k proxy err 6.60215737298131e-05 tr(WHW.T) 14010.4609375
bpp_loss 6.069465680164285
27_o proxy err 0.001296990318223834 tr(WHW.T) 2160.527099609375
bpp_loss 4.652966482797638
27_up proxy err 0.0016345364274457097 tr(WHW.T) 8478.0048828125
bpp_loss 4.620479902319078
27_gate proxy err 0.00043237340287305415 tr(WHW.T) 32819.25
bpp_loss 5.024006843966033
27_down proxy err 0.001578569645062089 tr(WHW.T) 6550.37109375
bpp_loss 4.611998642828049
I0328 18:51:08.286424 2316002 finetune.py:68] layer 28_v @ epoch 0 new loss 4.486850684770616e-06 old loss 1.3026986380282324e-05 BETTER
I0328 18:51:25.889343 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 62.83392834663391s
I0328 18:51:29.719511 2316072 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:51:29.719603 2316072 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:51:29.719640 2316072 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:51:30.084486 2316072 config.py:54] PyTorch version 2.6.0 available.
W0328 18:51:30.279187 2316072 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:51:30.871476 2316072 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:51:30.875230 2313906 quantize_finetune_llama.py:209] layer 30 gpu 2
I0328 18:51:30.889512 2316072 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:51:44.804248 2316002 finetune.py:68] layer 28_v @ epoch 1 new loss 4.30693171438179e-06 old loss 4.486850684770616e-06 BETTER
I0328 18:51:48.391582 2316072 finetune.py:45] layer 29_v initial loss 1.6266887541860342e-05
W0328 18:51:48.391770 2316072 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:52:21.673867 2316072 finetune.py:68] layer 29_v @ epoch 0 new loss 5.259177669358905e-06 old loss 1.6266887541860342e-05 BETTER
I0328 18:52:21.746958 2316002 finetune.py:76] layer 28_v @ epoch 2 new loss 4.606043148669414e-06 old loss 4.30693171438179e-06 WORSE
I0328 18:52:34.794711 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 63.43957042694092s
I0328 18:52:38.557170 2316142 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:52:38.557372 2316142 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:52:38.557454 2316142 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:52:38.900754 2316142 config.py:54] PyTorch version 2.6.0 available.
W0328 18:52:39.113542 2316142 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:52:39.738573 2316142 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:52:39.742029 2313906 quantize_finetune_llama.py:209] layer 31 gpu 3
I0328 18:52:39.754578 2316142 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:52:56.251203 2316072 finetune.py:76] layer 29_v @ epoch 1 new loss 5.351436811906751e-06 old loss 5.259177669358905e-06 WORSE
I0328 18:52:57.075982 2316142 finetune.py:45] layer 30_v initial loss 1.6984757166937925e-05
W0328 18:52:57.076208 2316142 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:52:57.994908 2316002 finetune.py:76] layer 28_v @ epoch 3 new loss 4.497566806094255e-06 old loss 4.30693171438179e-06 WORSE
I0328 18:53:30.136764 2316072 finetune.py:76] layer 29_v @ epoch 2 new loss 5.649854301736923e-06 old loss 5.259177669358905e-06 WORSE
I0328 18:53:30.650254 2316142 finetune.py:68] layer 30_v @ epoch 0 new loss 9.026592124428134e-06 old loss 1.6984757166937925e-05 BETTER
I0328 18:53:34.505303 2316002 finetune.py:76] layer 28_v @ epoch 4 new loss 4.489918410399696e-06 old loss 4.30693171438179e-06 WORSE
I0328 18:53:35.760649 2313906 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 55.53178572654724s
I0328 18:53:39.526064 2316212 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 18:53:39.526168 2316212 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 18:53:39.526211 2316212 utils.py:162] NumExpr defaulting to 16 threads.
I0328 18:53:39.870583 2316212 config.py:54] PyTorch version 2.6.0 available.
W0328 18:53:40.082616 2316212 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 18:53:40.687395 2316212 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 18:53:40.705184 2316212 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 18:53:53.003682 2316002 finetune.py:45] layer 28_q initial loss 5.582913217949681e-06
I0328 18:53:57.501176 2316212 finetune.py:45] layer 31_v initial loss 2.654075615282636e-05
W0328 18:53:57.501626 2316212 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 18:54:04.391513 2316072 finetune.py:76] layer 29_v @ epoch 3 new loss 5.926271569478558e-06 old loss 5.259177669358905e-06 WORSE
I0328 18:54:05.424338 2316142 finetune.py:68] layer 30_v @ epoch 1 new loss 8.856211934471503e-06 old loss 9.026592124428134e-06 BETTER
I0328 18:54:28.187690 2316002 finetune.py:68] layer 28_q @ epoch 0 new loss 5.425068593467586e-06 old loss 5.582913217949681e-06 BETTER
I0328 18:54:30.350511 2316212 finetune.py:68] layer 31_v @ epoch 0 new loss 2.287187635374721e-05 old loss 2.654075615282636e-05 BETTER
I0328 18:54:38.610564 2316072 finetune.py:76] layer 29_v @ epoch 4 new loss 5.819391390105011e-06 old loss 5.259177669358905e-06 WORSE
I0328 18:54:40.293419 2316142 finetune.py:76] layer 30_v @ epoch 2 new loss 1.0238536560791545e-05 old loss 8.856211934471503e-06 WORSE
I0328 18:54:57.786685 2316072 finetune.py:45] layer 29_q initial loss 8.810058716335334e-06
I0328 18:55:04.519577 2316212 finetune.py:76] layer 31_v @ epoch 1 new loss 0.00016313607920892537 old loss 2.287187635374721e-05 WORSE
I0328 18:55:04.638355 2316002 finetune.py:68] layer 28_q @ epoch 1 new loss 5.308906565915095e-06 old loss 5.425068593467586e-06 BETTER
I0328 18:55:14.853501 2316142 finetune.py:76] layer 30_v @ epoch 3 new loss 1.115481154556619e-05 old loss 8.856211934471503e-06 WORSE
I0328 18:55:31.725919 2316072 finetune.py:68] layer 29_q @ epoch 0 new loss 8.133247320074588e-06 old loss 8.810058716335334e-06 BETTER
I0328 18:55:38.394699 2316212 finetune.py:76] layer 31_v @ epoch 2 new loss 4.2095845856238157e-05 old loss 2.287187635374721e-05 WORSE
I0328 18:55:41.327439 2316002 finetune.py:68] layer 28_q @ epoch 2 new loss 5.214273187448271e-06 old loss 5.308906565915095e-06 BETTER
I0328 18:55:49.442637 2316142 finetune.py:76] layer 30_v @ epoch 4 new loss 9.667172889749054e-06 old loss 8.856211934471503e-06 WORSE
I0328 18:56:06.100315 2316072 finetune.py:68] layer 29_q @ epoch 1 new loss 7.854005161789246e-06 old loss 8.133247320074588e-06 BETTER
I0328 18:56:08.701934 2316142 finetune.py:45] layer 30_q initial loss 1.1128142432426102e-05
I0328 18:56:12.223943 2316212 finetune.py:76] layer 31_v @ epoch 3 new loss 5.3281492000678554e-05 old loss 2.287187635374721e-05 WORSE
I0328 18:56:18.133962 2316002 finetune.py:68] layer 28_q @ epoch 3 new loss 5.134255843586288e-06 old loss 5.214273187448271e-06 BETTER
I0328 18:56:40.749977 2316072 finetune.py:68] layer 29_q @ epoch 2 new loss 7.835694304958452e-06 old loss 7.854005161789246e-06 BETTER
I0328 18:56:42.473739 2316142 finetune.py:68] layer 30_q @ epoch 0 new loss 1.0970390576403588e-05 old loss 1.1128142432426102e-05 BETTER
I0328 18:56:46.054559 2316212 finetune.py:76] layer 31_v @ epoch 4 new loss 3.901460149791092e-05 old loss 2.287187635374721e-05 WORSE
I0328 18:56:54.834602 2316002 finetune.py:76] layer 28_q @ epoch 4 new loss 5.25356790603837e-06 old loss 5.134255843586288e-06 WORSE
I0328 18:57:04.504384 2316212 finetune.py:45] layer 31_q initial loss 2.9469460059772246e-05
I0328 18:57:12.035502 2316002 finetune.py:45] layer 28_k initial loss 5.9410826906969305e-06
I0328 18:57:15.338093 2316072 finetune.py:68] layer 29_q @ epoch 3 new loss 7.511306193919154e-06 old loss 7.835694304958452e-06 BETTER
I0328 18:57:17.044512 2316142 finetune.py:68] layer 30_q @ epoch 1 new loss 1.0604663657431956e-05 old loss 1.0970390576403588e-05 BETTER
I0328 18:57:37.497263 2316212 finetune.py:76] layer 31_q @ epoch 0 new loss 3.590827327570878e-05 old loss 2.9469460059772246e-05 WORSE
I0328 18:57:47.224284 2316002 finetune.py:68] layer 28_k @ epoch 0 new loss 5.780956144008087e-06 old loss 5.9410826906969305e-06 BETTER
I0328 18:57:49.913667 2316072 finetune.py:76] layer 29_q @ epoch 4 new loss 7.5750881478597876e-06 old loss 7.511306193919154e-06 WORSE
I0328 18:57:51.650641 2316142 finetune.py:68] layer 30_q @ epoch 2 new loss 1.0476708666828927e-05 old loss 1.0604663657431956e-05 BETTER
I0328 18:58:07.647445 2316072 finetune.py:45] layer 29_k initial loss 8.551223800168373e-06
I0328 18:58:10.904334 2316212 finetune.py:76] layer 31_q @ epoch 1 new loss 3.423497037147172e-05 old loss 2.9469460059772246e-05 WORSE
I0328 18:58:23.522789 2316002 finetune.py:68] layer 28_k @ epoch 1 new loss 5.659941962221637e-06 old loss 5.780956144008087e-06 BETTER
I0328 18:58:26.291000 2316142 finetune.py:76] layer 30_q @ epoch 3 new loss 1.10480123112211e-05 old loss 1.0476708666828927e-05 WORSE
I0328 18:58:40.846827 2316072 finetune.py:68] layer 29_k @ epoch 0 new loss 8.496622285747435e-06 old loss 8.551223800168373e-06 BETTER
I0328 18:58:44.483182 2316212 finetune.py:68] layer 31_q @ epoch 2 new loss 2.4218994440161623e-05 old loss 2.9469460059772246e-05 BETTER
I0328 18:59:00.079267 2316002 finetune.py:68] layer 28_k @ epoch 2 new loss 5.659846920025302e-06 old loss 5.659941962221637e-06 BETTER
I0328 18:59:00.388826 2316142 finetune.py:76] layer 30_q @ epoch 4 new loss 1.1747202734113671e-05 old loss 1.0476708666828927e-05 WORSE
I0328 18:59:15.110677 2316072 finetune.py:68] layer 29_k @ epoch 1 new loss 8.037427505769301e-06 old loss 8.496622285747435e-06 BETTER
I0328 18:59:17.751781 2316142 finetune.py:45] layer 30_k initial loss 1.2222536497574765e-05
I0328 18:59:18.889827 2316212 finetune.py:76] layer 31_q @ epoch 3 new loss 2.4832977942423895e-05 old loss 2.4218994440161623e-05 WORSE
I0328 18:59:36.682211 2316002 finetune.py:68] layer 28_k @ epoch 3 new loss 5.564579169003991e-06 old loss 5.659846920025302e-06 BETTER
I0328 18:59:49.775161 2316072 finetune.py:68] layer 29_k @ epoch 2 new loss 8.02609974925872e-06 old loss 8.037427505769301e-06 BETTER
I0328 18:59:51.661289 2316142 finetune.py:68] layer 30_k @ epoch 0 new loss 1.2148884707130492e-05 old loss 1.2222536497574765e-05 BETTER
I0328 18:59:52.476324 2316212 finetune.py:68] layer 31_q @ epoch 4 new loss 2.2264419385464862e-05 old loss 2.4218994440161623e-05 BETTER
I0328 19:00:10.203058 2316212 finetune.py:45] layer 31_k initial loss 2.6307587177143432e-05
I0328 19:00:13.341752 2316002 finetune.py:76] layer 28_k @ epoch 4 new loss 5.631593012367375e-06 old loss 5.564579169003991e-06 WORSE
I0328 19:00:24.333157 2316072 finetune.py:68] layer 29_k @ epoch 3 new loss 7.896655006334186e-06 old loss 8.02609974925872e-06 BETTER
I0328 19:00:26.134436 2316142 finetune.py:76] layer 30_k @ epoch 1 new loss 1.3053509974270128e-05 old loss 1.2148884707130492e-05 WORSE
I0328 19:00:32.030501 2316002 finetune.py:45] layer 28_o initial loss 1.363360024697613e-05
I0328 19:00:43.040491 2316212 finetune.py:68] layer 31_k @ epoch 0 new loss 2.3764372599544004e-05 old loss 2.6307587177143432e-05 BETTER
I0328 19:00:58.845755 2316072 finetune.py:76] layer 29_k @ epoch 4 new loss 8.036851795623079e-06 old loss 7.896655006334186e-06 WORSE
I0328 19:01:00.028864 2316142 finetune.py:76] layer 30_k @ epoch 2 new loss 1.3122026757628191e-05 old loss 1.2148884707130492e-05 WORSE
I0328 19:01:06.782138 2316002 finetune.py:68] layer 28_o @ epoch 0 new loss 1.3238099199952558e-05 old loss 1.363360024697613e-05 BETTER
I0328 19:01:16.744813 2316212 finetune.py:76] layer 31_k @ epoch 1 new loss 2.5313245714642107e-05 old loss 2.3764372599544004e-05 WORSE
I0328 19:01:18.289840 2316072 finetune.py:45] layer 29_o initial loss 1.604495082574431e-05
I0328 19:01:33.790515 2316142 finetune.py:68] layer 30_k @ epoch 3 new loss 1.2036334737786092e-05 old loss 1.2148884707130492e-05 BETTER
I0328 19:01:42.755761 2316002 finetune.py:68] layer 28_o @ epoch 1 new loss 1.2887016055174172e-05 old loss 1.3238099199952558e-05 BETTER
I0328 19:01:49.842601 2316212 finetune.py:76] layer 31_k @ epoch 2 new loss 2.503544237697497e-05 old loss 2.3764372599544004e-05 WORSE
I0328 19:01:50.805360 2316072 finetune.py:68] layer 29_o @ epoch 0 new loss 1.557434552523773e-05 old loss 1.604495082574431e-05 BETTER
I0328 19:02:08.512730 2316142 finetune.py:76] layer 30_k @ epoch 4 new loss 1.2271679224795662e-05 old loss 1.2036334737786092e-05 WORSE
I0328 19:02:18.727012 2316002 finetune.py:68] layer 28_o @ epoch 2 new loss 1.274474197998643e-05 old loss 1.2887016055174172e-05 BETTER
I0328 19:02:23.049374 2316212 finetune.py:76] layer 31_k @ epoch 3 new loss 2.6773728677653708e-05 old loss 2.3764372599544004e-05 WORSE
I0328 19:02:24.277857 2316072 finetune.py:68] layer 29_o @ epoch 1 new loss 1.5270867152139544e-05 old loss 1.557434552523773e-05 BETTER
I0328 19:02:27.784547 2316142 finetune.py:45] layer 30_o initial loss 2.5411634851479903e-05
I0328 19:02:54.689337 2316002 finetune.py:68] layer 28_o @ epoch 3 new loss 1.2686776244663633e-05 old loss 1.274474197998643e-05 BETTER
I0328 19:02:56.361474 2316212 finetune.py:76] layer 31_k @ epoch 4 new loss 2.8706996090477332e-05 old loss 2.3764372599544004e-05 WORSE
I0328 19:02:57.928662 2316072 finetune.py:68] layer 29_o @ epoch 2 new loss 1.5092459761945065e-05 old loss 1.5270867152139544e-05 BETTER
I0328 19:03:00.635360 2316142 finetune.py:68] layer 30_o @ epoch 0 new loss 2.4418388420599513e-05 old loss 2.5411634851479903e-05 BETTER
I0328 19:03:15.221963 2316212 finetune.py:45] layer 31_o initial loss 5.156922998139635e-05
I0328 19:03:30.478867 2316002 finetune.py:68] layer 28_o @ epoch 4 new loss 1.2617740139830858e-05 old loss 1.2686776244663633e-05 BETTER
I0328 19:03:31.768436 2316072 finetune.py:68] layer 29_o @ epoch 3 new loss 1.5069392247823998e-05 old loss 1.5092459761945065e-05 BETTER
I0328 19:03:34.428047 2316142 finetune.py:68] layer 30_o @ epoch 1 new loss 2.3730217435513623e-05 old loss 2.4418388420599513e-05 BETTER
I0328 19:03:47.119923 2316212 finetune.py:68] layer 31_o @ epoch 0 new loss 4.519448702922091e-05 old loss 5.156922998139635e-05 BETTER
I0328 19:04:01.660330 2316002 finetune.py:45] layer 28_up initial loss 3.482322790659964e-05
I0328 19:04:05.688508 2316072 finetune.py:68] layer 29_o @ epoch 4 new loss 1.5007452020654455e-05 old loss 1.5069392247823998e-05 BETTER
I0328 19:04:08.533716 2316142 finetune.py:68] layer 30_o @ epoch 2 new loss 2.3547579985461198e-05 old loss 2.3730217435513623e-05 BETTER
I0328 19:04:20.218258 2316212 finetune.py:68] layer 31_o @ epoch 1 new loss 4.220824484946206e-05 old loss 4.519448702922091e-05 BETTER
I0328 19:04:33.671553 2316002 finetune.py:68] layer 28_up @ epoch 0 new loss 3.4188004065072164e-05 old loss 3.482322790659964e-05 BETTER
I0328 19:04:37.617944 2316072 finetune.py:45] layer 29_up initial loss 4.563093898468651e-05
I0328 19:04:42.530585 2316142 finetune.py:76] layer 30_o @ epoch 3 new loss 2.36139621847542e-05 old loss 2.3547579985461198e-05 WORSE
I0328 19:04:53.261344 2316212 finetune.py:68] layer 31_o @ epoch 2 new loss 4.196316876914352e-05 old loss 4.220824484946206e-05 BETTER
I0328 19:05:06.460003 2316002 finetune.py:68] layer 28_up @ epoch 1 new loss 3.359596666996367e-05 old loss 3.4188004065072164e-05 BETTER
I0328 19:05:07.960558 2316072 finetune.py:68] layer 29_up @ epoch 0 new loss 4.4292817619862035e-05 old loss 4.563093898468651e-05 BETTER
I0328 19:05:15.910495 2316142 finetune.py:68] layer 30_o @ epoch 4 new loss 2.3244159820023924e-05 old loss 2.3547579985461198e-05 BETTER
I0328 19:05:26.653780 2316212 finetune.py:76] layer 31_o @ epoch 3 new loss 4.208899918012321e-05 old loss 4.196316876914352e-05 WORSE
I0328 19:05:39.303526 2316072 finetune.py:68] layer 29_up @ epoch 1 new loss 4.3592575821094215e-05 old loss 4.4292817619862035e-05 BETTER
I0328 19:05:40.167900 2316002 finetune.py:68] layer 28_up @ epoch 2 new loss 3.3267227991018444e-05 old loss 3.359596666996367e-05 BETTER
I0328 19:05:47.341305 2316142 finetune.py:45] layer 30_up initial loss 8.366961992578581e-05
I0328 19:05:59.165009 2316212 finetune.py:68] layer 31_o @ epoch 4 new loss 4.1077993955696e-05 old loss 4.196316876914352e-05 BETTER
I0328 19:06:10.862224 2316072 finetune.py:68] layer 29_up @ epoch 2 new loss 4.307852213969454e-05 old loss 4.3592575821094215e-05 BETTER
I0328 19:06:13.458760 2316002 finetune.py:68] layer 28_up @ epoch 3 new loss 3.2982636184897274e-05 old loss 3.3267227991018444e-05 BETTER
I0328 19:06:17.897232 2316142 finetune.py:68] layer 30_up @ epoch 0 new loss 8.024444105103612e-05 old loss 8.366961992578581e-05 BETTER
I0328 19:06:30.186049 2316212 finetune.py:45] layer 31_up initial loss 0.0002454799832776189
I0328 19:06:42.579923 2316072 finetune.py:68] layer 29_up @ epoch 3 new loss 4.263856681063771e-05 old loss 4.307852213969454e-05 BETTER
I0328 19:06:46.986381 2316002 finetune.py:68] layer 28_up @ epoch 4 new loss 3.275213748565875e-05 old loss 3.2982636184897274e-05 BETTER
I0328 19:06:49.890385 2316142 finetune.py:68] layer 30_up @ epoch 1 new loss 7.811110117472708e-05 old loss 8.024444105103612e-05 BETTER
I0328 19:07:00.319766 2316212 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0002233267150586471 old loss 0.0002454799832776189 BETTER
I0328 19:07:14.365074 2316072 finetune.py:68] layer 29_up @ epoch 4 new loss 4.229252226650715e-05 old loss 4.263856681063771e-05 BETTER
I0328 19:07:18.196451 2316002 finetune.py:45] layer 28_gate initial loss 4.3653944885591045e-05
I0328 19:07:21.980788 2316142 finetune.py:68] layer 30_up @ epoch 2 new loss 7.648460450582206e-05 old loss 7.811110117472708e-05 BETTER
I0328 19:07:31.356906 2316212 finetune.py:68] layer 31_up @ epoch 1 new loss 0.00020864793623331934 old loss 0.0002233267150586471 BETTER
I0328 19:07:45.826692 2316072 finetune.py:45] layer 29_gate initial loss 5.6821354519343004e-05
I0328 19:07:48.133510 2316002 finetune.py:68] layer 28_gate @ epoch 0 new loss 4.339113729656674e-05 old loss 4.3653944885591045e-05 BETTER
I0328 19:07:54.097717 2316142 finetune.py:68] layer 30_up @ epoch 3 new loss 7.518535858253017e-05 old loss 7.648460450582206e-05 BETTER
I0328 19:08:02.607481 2316212 finetune.py:68] layer 31_up @ epoch 2 new loss 0.00019838627486024052 old loss 0.00020864793623331934 BETTER
I0328 19:08:14.021592 2316072 finetune.py:68] layer 29_gate @ epoch 0 new loss 5.6452125136274844e-05 old loss 5.6821354519343004e-05 BETTER
I0328 19:08:19.182108 2316002 finetune.py:68] layer 28_gate @ epoch 1 new loss 4.3184380047023296e-05 old loss 4.339113729656674e-05 BETTER
I0328 19:08:26.349917 2316142 finetune.py:68] layer 30_up @ epoch 4 new loss 7.41446710890159e-05 old loss 7.518535858253017e-05 BETTER
I0328 19:08:33.898319 2316212 finetune.py:68] layer 31_up @ epoch 3 new loss 0.00018980898312292993 old loss 0.00019838627486024052 BETTER
I0328 19:08:43.390414 2316072 finetune.py:68] layer 29_gate @ epoch 1 new loss 5.617289571091533e-05 old loss 5.6452125136274844e-05 BETTER
I0328 19:08:50.496997 2316002 finetune.py:68] layer 28_gate @ epoch 2 new loss 4.301068838685751e-05 old loss 4.3184380047023296e-05 BETTER
I0328 19:08:57.922115 2316142 finetune.py:45] layer 30_gate initial loss 9.506223432254046e-05
I0328 19:09:05.106034 2316212 finetune.py:68] layer 31_up @ epoch 4 new loss 0.00018242547230329365 old loss 0.00018980898312292993 BETTER
I0328 19:09:12.702497 2316072 finetune.py:68] layer 29_gate @ epoch 2 new loss 5.596780465566553e-05 old loss 5.617289571091533e-05 BETTER
I0328 19:09:21.754472 2316002 finetune.py:68] layer 28_gate @ epoch 3 new loss 4.288070340408012e-05 old loss 4.301068838685751e-05 BETTER
I0328 19:09:26.440864 2316142 finetune.py:68] layer 30_gate @ epoch 0 new loss 9.395574306836352e-05 old loss 9.506223432254046e-05 BETTER
I0328 19:09:36.124347 2316212 finetune.py:45] layer 31_gate initial loss 0.00022149128199089319
I0328 19:09:42.081542 2316072 finetune.py:68] layer 29_gate @ epoch 3 new loss 5.576759940595366e-05 old loss 5.596780465566553e-05 BETTER
I0328 19:09:53.409108 2316002 finetune.py:68] layer 28_gate @ epoch 4 new loss 4.2761403165059164e-05 old loss 4.288070340408012e-05 BETTER
I0328 19:09:56.207223 2316142 finetune.py:68] layer 30_gate @ epoch 1 new loss 9.308325388701633e-05 old loss 9.395574306836352e-05 BETTER
I0328 19:10:04.384509 2316212 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.0002154324174625799 old loss 0.00022149128199089319 BETTER
I0328 19:10:12.132731 2316072 finetune.py:68] layer 29_gate @ epoch 4 new loss 5.563649392570369e-05 old loss 5.576759940595366e-05 BETTER
I0328 19:10:25.933497 2316142 finetune.py:68] layer 30_gate @ epoch 2 new loss 9.24278429010883e-05 old loss 9.308325388701633e-05 BETTER
I0328 19:10:33.418115 2316212 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.0002105419262079522 old loss 0.0002154324174625799 BETTER
I0328 19:10:48.307567 2316002 finetune.py:45] layer 28_down initial loss 7.199132232926786e-05
I0328 19:10:55.815367 2316142 finetune.py:68] layer 30_gate @ epoch 3 new loss 9.187289833789691e-05 old loss 9.24278429010883e-05 BETTER
I0328 19:11:02.528804 2316212 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.00020665826741605997 old loss 0.0002105419262079522 BETTER
I0328 19:11:09.037141 2316072 finetune.py:45] layer 29_down initial loss 9.473817772231996e-05
I0328 19:11:15.533398 2316002 finetune.py:68] layer 28_down @ epoch 0 new loss 7.19897507224232e-05 old loss 7.199132232926786e-05 BETTER
I0328 19:11:25.680307 2316142 finetune.py:68] layer 30_gate @ epoch 4 new loss 9.14238189579919e-05 old loss 9.187289833789691e-05 BETTER
I0328 19:11:31.921084 2316212 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.00020315898291300982 old loss 0.00020665826741605997 BETTER
I0328 19:11:35.117112 2316072 finetune.py:68] layer 29_down @ epoch 0 new loss 9.473288810113445e-05 old loss 9.473817772231996e-05 BETTER
I0328 19:11:44.231223 2316002 finetune.py:68] layer 28_down @ epoch 1 new loss 7.19888776075095e-05 old loss 7.19897507224232e-05 BETTER
I0328 19:12:01.235406 2316212 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.00020023813704028726 old loss 0.00020315898291300982 BETTER
I0328 19:12:02.153040 2316072 finetune.py:68] layer 29_down @ epoch 1 new loss 9.472945384914055e-05 old loss 9.473288810113445e-05 BETTER
I0328 19:12:13.164587 2316002 finetune.py:68] layer 28_down @ epoch 2 new loss 7.198773528216407e-05 old loss 7.19888776075095e-05 BETTER
I0328 19:12:22.075701 2316142 finetune.py:45] layer 30_down initial loss 0.0001548157015349716
I0328 19:12:29.391794 2316072 finetune.py:68] layer 29_down @ epoch 2 new loss 9.472711099078879e-05 old loss 9.472945384914055e-05 BETTER
I0328 19:12:42.005926 2316002 finetune.py:68] layer 28_down @ epoch 3 new loss 7.198718230938539e-05 old loss 7.198773528216407e-05 BETTER
I0328 19:12:48.416130 2316142 finetune.py:68] layer 30_down @ epoch 0 new loss 0.00015480542788282037 old loss 0.0001548157015349716 BETTER
I0328 19:12:56.934931 2316072 finetune.py:68] layer 29_down @ epoch 3 new loss 9.472478996030986e-05 old loss 9.472711099078879e-05 BETTER
I0328 19:12:58.388627 2316212 finetune.py:45] layer 31_down initial loss 0.0003838716365862638
I0328 19:13:10.826911 2316002 finetune.py:68] layer 28_down @ epoch 4 new loss 7.198628736659884e-05 old loss 7.198718230938539e-05 BETTER
28_v proxy err 0.0012088031508028507 tr(WHW.T) 601.4844360351562
bpp_loss 4.88757175381761
28_q proxy err 0.00013131540617905557 tr(WHW.T) 23166.10546875
bpp_loss 5.152090815012343
28_k proxy err 5.789249189547263e-05 tr(WHW.T) 14993.859375
bpp_loss 6.006988259148784
28_o proxy err 0.0011300976620987058 tr(WHW.T) 2510.900146484375
bpp_loss 4.682962385355495
28_up proxy err 0.0013179205125197768 tr(WHW.T) 10244.0205078125
bpp_loss 4.643359864396708
28_gate proxy err 0.00038424358353950083 tr(WHW.T) 35919.97265625
bpp_loss 5.012030916820679
28_down proxy err 0.0014916477957740426 tr(WHW.T) 7215.20263671875
bpp_loss 4.627735926104443
I0328 19:13:15.947811 2316142 finetune.py:68] layer 30_down @ epoch 1 new loss 0.00015479701687581837 old loss 0.00015480542788282037 BETTER
I0328 19:13:24.261168 2316212 finetune.py:68] layer 31_down @ epoch 0 new loss 0.0003837903786916286 old loss 0.0003838716365862638 BETTER
I0328 19:13:24.713725 2316072 finetune.py:68] layer 29_down @ epoch 4 new loss 9.472363308304921e-05 old loss 9.472478996030986e-05 BETTER
29_v proxy err 0.0009208702831529081 tr(WHW.T) 850.4290161132812
bpp_loss 4.957597260130569
29_q proxy err 0.00015612294373568147 tr(WHW.T) 20653.353515625
bpp_loss 5.138714078639168
29_k proxy err 5.5284421250689775e-05 tr(WHW.T) 16364.873046875
bpp_loss 6.108843041001819
29_o proxy err 0.0007163842092268169 tr(WHW.T) 3100.552978515625
bpp_loss 4.7190966591006145
29_up proxy err 0.0010219335090368986 tr(WHW.T) 12872.537109375
bpp_loss 4.677246460957186
29_gate proxy err 0.0003495702112559229 tr(WHW.T) 38377.17578125
bpp_loss 5.00634259065347
29_down proxy err 0.0012283034157007933 tr(WHW.T) 7482.60400390625
bpp_loss 4.643092468979636
I0328 19:13:43.750057 2316142 finetune.py:68] layer 30_down @ epoch 2 new loss 0.00015479062858503312 old loss 0.00015479701687581837 BETTER
I0328 19:13:51.333905 2316212 finetune.py:68] layer 31_down @ epoch 1 new loss 0.00038373752613551915 old loss 0.0003837903786916286 BETTER
I0328 19:14:11.392706 2316142 finetune.py:68] layer 30_down @ epoch 3 new loss 0.00015478571003768593 old loss 0.00015479062858503312 BETTER
I0328 19:14:18.468778 2316212 finetune.py:68] layer 31_down @ epoch 2 new loss 0.00038369299727492034 old loss 0.00038373752613551915 BETTER
I0328 19:14:39.341501 2316142 finetune.py:68] layer 30_down @ epoch 4 new loss 0.00015478114073630422 old loss 0.00015478571003768593 BETTER
30_v proxy err 0.0008275468135252595 tr(WHW.T) 863.060791015625
bpp_loss 5.261278082034551
30_q proxy err 0.00012131590483477339 tr(WHW.T) 24052.998046875
bpp_loss 5.0438345884904265
30_k proxy err 6.35827163932845e-05 tr(WHW.T) 14030.494140625
bpp_loss 5.732648036675528
30_o proxy err 0.0005071835475973785 tr(WHW.T) 4869.0654296875
bpp_loss 4.8081450772006065
30_up proxy err 0.0006019733264110982 tr(WHW.T) 21706.5546875
bpp_loss 4.710532745346427
30_gate proxy err 0.00025698755052872 tr(WHW.T) 51924.89453125
bpp_loss 5.06409994153572
30_down proxy err 0.0007156342035159469 tr(WHW.T) 8814.6220703125
bpp_loss 4.644474339505125
I0328 19:14:45.579162 2316212 finetune.py:68] layer 31_down @ epoch 3 new loss 0.0003836559480987489 old loss 0.00038369299727492034 BETTER
I0328 19:15:12.652158 2316212 finetune.py:68] layer 31_down @ epoch 4 new loss 0.0003836267860606313 old loss 0.0003836559480987489 BETTER
31_v proxy err 0.0004224778967909515 tr(WHW.T) 1808.723876953125
bpp_loss 5.0658157635480165
31_q proxy err 6.777049566153437e-05 tr(WHW.T) 46175.4453125
bpp_loss 5.205497501301579
31_k proxy err 4.276722756912932e-05 tr(WHW.T) 20469.5
bpp_loss 5.993151358678006
31_o proxy err 0.0004301424487493932 tr(WHW.T) 2212.80322265625
bpp_loss 4.77308985311538
31_up proxy err 0.000182568168384023 tr(WHW.T) 69011.390625
bpp_loss 4.914421290957502
31_gate proxy err 8.92236057552509e-05 tr(WHW.T) 144254.640625
bpp_loss 5.29894790106586
31_down proxy err 0.0002882207336369902 tr(WHW.T) 9967.6865234375
bpp_loss 4.673216653322535
I0328 19:15:38.568950 2316282 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:15:38.569080 2316282 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:15:38.569128 2316282 utils.py:162] NumExpr defaulting to 16 threads.
I0328 19:15:38.902245 2316282 config.py:54] PyTorch version 2.6.0 available.
W0328 19:15:39.122732 2316282 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 19:15:39.236256 2316282 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.01it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.54it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.48it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.86it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.12it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.40it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.76it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.45it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.36it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.70it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.14it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.43it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.58it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.47it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.57it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.37it/s]
I0328 19:15:42.350000 2316282 hfize_llama.py:153] loaded layer 0
I0328 19:15:43.016907 2316282 hfize_llama.py:153] loaded layer 1
I0328 19:15:43.785933 2316282 hfize_llama.py:153] loaded layer 2
I0328 19:15:44.478893 2316282 hfize_llama.py:153] loaded layer 3
I0328 19:15:45.151031 2316282 hfize_llama.py:153] loaded layer 4
I0328 19:15:45.848884 2316282 hfize_llama.py:153] loaded layer 5
I0328 19:15:46.429239 2316282 hfize_llama.py:153] loaded layer 6
I0328 19:15:47.142859 2316282 hfize_llama.py:153] loaded layer 7
I0328 19:15:47.824786 2316282 hfize_llama.py:153] loaded layer 8
I0328 19:15:48.511872 2316282 hfize_llama.py:153] loaded layer 9
I0328 19:15:49.224193 2316282 hfize_llama.py:153] loaded layer 10
I0328 19:15:49.906367 2316282 hfize_llama.py:153] loaded layer 11
I0328 19:15:50.669951 2316282 hfize_llama.py:153] loaded layer 12
I0328 19:15:51.384142 2316282 hfize_llama.py:153] loaded layer 13
I0328 19:15:52.095482 2316282 hfize_llama.py:153] loaded layer 14
I0328 19:15:52.752274 2316282 hfize_llama.py:153] loaded layer 15
I0328 19:15:53.441719 2316282 hfize_llama.py:153] loaded layer 16
I0328 19:15:54.105064 2316282 hfize_llama.py:153] loaded layer 17
I0328 19:15:54.744489 2316282 hfize_llama.py:153] loaded layer 18
I0328 19:15:55.458055 2316282 hfize_llama.py:153] loaded layer 19
I0328 19:15:56.114569 2316282 hfize_llama.py:153] loaded layer 20
I0328 19:15:56.814092 2316282 hfize_llama.py:153] loaded layer 21
I0328 19:15:57.434622 2316282 hfize_llama.py:153] loaded layer 22
I0328 19:15:58.059551 2316282 hfize_llama.py:153] loaded layer 23
I0328 19:15:58.707257 2316282 hfize_llama.py:153] loaded layer 24
I0328 19:15:59.323512 2316282 hfize_llama.py:153] loaded layer 25
I0328 19:15:59.952429 2316282 hfize_llama.py:153] loaded layer 26
I0328 19:16:00.620300 2316282 hfize_llama.py:153] loaded layer 27
I0328 19:16:01.333940 2316282 hfize_llama.py:153] loaded layer 28
I0328 19:16:02.008996 2316282 hfize_llama.py:153] loaded layer 29
I0328 19:16:02.639338 2316282 hfize_llama.py:153] loaded layer 30
I0328 19:16:03.276109 2316282 hfize_llama.py:153] loaded layer 31
I0328 19:16:03.276253 2316282 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.76s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:08,  1.64s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:06,  1.65s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:04,  1.60s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.59s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:09<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.33s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:10<00:00,  1.49s/it]
I0328 19:17:08.925139 2316282 hfize_llama.py:167] successfully loaded hfized model
I0328 19:17:14.090411 2316500 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 19:17:14.090538 2316500 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 19:17:14.090582 2316500 utils.py:162] NumExpr defaulting to 16 threads.
W0328 19:17:14.446673 2316500 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 19:17:14.848250 2316500 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:09,  1.59s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:08,  1.63s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:05<00:06,  1.68s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:06<00:05,  1.71s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:08<00:03,  1.69s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:10<00:01,  1.72s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.58s/it]
I0328 19:17:26.021799 2316500 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.5385336875915527:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.5385336875915527:   1%|          | 1/141 [00:01<04:21,  1.87s/it]avg_loss = 1.8557721376419067:   1%|          | 1/141 [00:03<04:21,  1.87s/it]avg_loss = 1.8557721376419067:   1%|▏         | 2/141 [00:03<03:46,  1.63s/it]avg_loss = 1.9925381342569988:   1%|▏         | 2/141 [00:04<03:46,  1.63s/it]avg_loss = 1.9925381342569988:   2%|▏         | 3/141 [00:04<03:34,  1.56s/it]avg_loss = 1.9465607106685638:   2%|▏         | 3/141 [00:06<03:34,  1.56s/it]avg_loss = 1.9465607106685638:   3%|▎         | 4/141 [00:06<03:28,  1.52s/it]avg_loss = 1.8973743438720703:   3%|▎         | 4/141 [00:07<03:28,  1.52s/it]avg_loss = 1.8973743438720703:   4%|▎         | 5/141 [00:07<03:24,  1.51s/it]avg_loss = 1.8008450269699097:   4%|▎         | 5/141 [00:09<03:24,  1.51s/it]avg_loss = 1.8008450269699097:   4%|▍         | 6/141 [00:09<03:22,  1.50s/it]avg_loss = 1.7370226553508215:   4%|▍         | 6/141 [00:10<03:22,  1.50s/it]avg_loss = 1.7370226553508215:   5%|▍         | 7/141 [00:10<03:20,  1.49s/it]avg_loss = 1.734297126531601:   5%|▍         | 7/141 [00:12<03:20,  1.49s/it] avg_loss = 1.734297126531601:   6%|▌         | 8/141 [00:12<03:18,  1.49s/it]avg_loss = 1.769047154320611:   6%|▌         | 8/141 [00:13<03:18,  1.49s/it]avg_loss = 1.769047154320611:   6%|▋         | 9/141 [00:13<03:16,  1.49s/it]avg_loss = 1.7744274735450745:   6%|▋         | 9/141 [00:15<03:16,  1.49s/it]avg_loss = 1.7744274735450745:   7%|▋         | 10/141 [00:15<03:15,  1.49s/it]avg_loss = 1.7715627280148594:   7%|▋         | 10/141 [00:16<03:15,  1.49s/it]avg_loss = 1.7715627280148594:   8%|▊         | 11/141 [00:16<03:14,  1.49s/it]avg_loss = 1.7944942116737366:   8%|▊         | 11/141 [00:18<03:14,  1.49s/it]avg_loss = 1.7944942116737366:   9%|▊         | 12/141 [00:18<03:13,  1.50s/it]avg_loss = 1.807477776820843:   9%|▊         | 12/141 [00:19<03:13,  1.50s/it] avg_loss = 1.807477776820843:   9%|▉         | 13/141 [00:19<03:11,  1.50s/it]avg_loss = 1.826354239668165:   9%|▉         | 13/141 [00:21<03:11,  1.50s/it]avg_loss = 1.826354239668165:  10%|▉         | 14/141 [00:21<03:10,  1.50s/it]avg_loss = 1.836770757039388:  10%|▉         | 14/141 [00:22<03:10,  1.50s/it]avg_loss = 1.836770757039388:  11%|█         | 15/141 [00:22<03:09,  1.50s/it]avg_loss = 1.8615234792232513:  11%|█         | 15/141 [00:24<03:09,  1.50s/it]avg_loss = 1.8615234792232513:  11%|█▏        | 16/141 [00:24<03:08,  1.51s/it]avg_loss = 1.8651387691497803:  11%|█▏        | 16/141 [00:25<03:08,  1.51s/it]avg_loss = 1.8651387691497803:  12%|█▏        | 17/141 [00:25<03:07,  1.51s/it]avg_loss = 1.8670657475789387:  12%|█▏        | 17/141 [00:27<03:07,  1.51s/it]avg_loss = 1.8670657475789387:  13%|█▎        | 18/141 [00:27<03:06,  1.51s/it]avg_loss = 1.8460058350312083:  13%|█▎        | 18/141 [00:28<03:06,  1.51s/it]avg_loss = 1.8460058350312083:  13%|█▎        | 19/141 [00:28<03:05,  1.52s/it]avg_loss = 1.8449892282485962:  13%|█▎        | 19/141 [00:30<03:05,  1.52s/it]avg_loss = 1.8449892282485962:  14%|█▍        | 20/141 [00:30<03:03,  1.52s/it]avg_loss = 1.8501895836421423:  14%|█▍        | 20/141 [00:31<03:03,  1.52s/it]avg_loss = 1.8501895836421423:  15%|█▍        | 21/141 [00:31<03:02,  1.52s/it]avg_loss = 1.8524712053212253:  15%|█▍        | 21/141 [00:33<03:02,  1.52s/it]avg_loss = 1.8524712053212253:  16%|█▌        | 22/141 [00:33<03:01,  1.52s/it]avg_loss = 1.8541247326394785:  16%|█▌        | 22/141 [00:34<03:01,  1.52s/it]avg_loss = 1.8541247326394785:  16%|█▋        | 23/141 [00:34<02:59,  1.53s/it]avg_loss = 1.8590959161520004:  16%|█▋        | 23/141 [00:36<02:59,  1.53s/it]avg_loss = 1.8590959161520004:  17%|█▋        | 24/141 [00:36<02:58,  1.53s/it]avg_loss = 1.8649883699417114:  17%|█▋        | 24/141 [00:37<02:58,  1.53s/it]avg_loss = 1.8649883699417114:  18%|█▊        | 25/141 [00:37<02:57,  1.53s/it]avg_loss = 1.8769395213860731:  18%|█▊        | 25/141 [00:39<02:57,  1.53s/it]avg_loss = 1.8769395213860731:  18%|█▊        | 26/141 [00:39<02:56,  1.53s/it]avg_loss = 1.8900646677723638:  18%|█▊        | 26/141 [00:41<02:56,  1.53s/it]avg_loss = 1.8900646677723638:  19%|█▉        | 27/141 [00:41<02:54,  1.53s/it]avg_loss = 1.8969463918890273:  19%|█▉        | 27/141 [00:42<02:54,  1.53s/it]avg_loss = 1.8969463918890273:  20%|█▉        | 28/141 [00:42<02:53,  1.54s/it]avg_loss = 1.8935149990279099:  20%|█▉        | 28/141 [00:44<02:53,  1.54s/it]avg_loss = 1.8935149990279099:  21%|██        | 29/141 [00:44<02:52,  1.54s/it]avg_loss = 1.8838904579480489:  21%|██        | 29/141 [00:45<02:52,  1.54s/it]avg_loss = 1.8838904579480489:  21%|██▏       | 30/141 [00:45<02:51,  1.54s/it]avg_loss = 1.8694837131807882:  21%|██▏       | 30/141 [00:47<02:51,  1.54s/it]avg_loss = 1.8694837131807882:  22%|██▏       | 31/141 [00:47<02:49,  1.54s/it]avg_loss = 1.85715926066041:  22%|██▏       | 31/141 [00:48<02:49,  1.54s/it]  avg_loss = 1.85715926066041:  23%|██▎       | 32/141 [00:48<02:48,  1.55s/it]avg_loss = 1.8560806043220288:  23%|██▎       | 32/141 [00:50<02:48,  1.55s/it]avg_loss = 1.8560806043220288:  23%|██▎       | 33/141 [00:50<02:47,  1.55s/it]avg_loss = 1.854745913954342:  23%|██▎       | 33/141 [00:51<02:47,  1.55s/it] avg_loss = 1.854745913954342:  24%|██▍       | 34/141 [00:51<02:45,  1.55s/it]avg_loss = 1.8575917175837926:  24%|██▍       | 34/141 [00:53<02:45,  1.55s/it]avg_loss = 1.8575917175837926:  25%|██▍       | 35/141 [00:53<02:44,  1.55s/it]avg_loss = 1.841293106476466:  25%|██▍       | 35/141 [00:54<02:44,  1.55s/it] avg_loss = 1.841293106476466:  26%|██▌       | 36/141 [00:54<02:42,  1.55s/it]avg_loss = 1.8257702782347396:  26%|██▌       | 36/141 [00:56<02:42,  1.55s/it]avg_loss = 1.8257702782347396:  26%|██▌       | 37/141 [00:56<02:41,  1.55s/it]avg_loss = 1.8109229834456193:  26%|██▌       | 37/141 [00:58<02:41,  1.55s/it]avg_loss = 1.8109229834456193:  27%|██▋       | 38/141 [00:58<02:40,  1.55s/it]avg_loss = 1.7965667706269484:  27%|██▋       | 38/141 [00:59<02:40,  1.55s/it]avg_loss = 1.7965667706269484:  28%|██▊       | 39/141 [00:59<02:38,  1.56s/it]avg_loss = 1.788349264860153:  28%|██▊       | 39/141 [01:01<02:38,  1.56s/it] avg_loss = 1.788349264860153:  28%|██▊       | 40/141 [01:01<02:37,  1.56s/it]avg_loss = 1.7930664056684913:  28%|██▊       | 40/141 [01:02<02:37,  1.56s/it]avg_loss = 1.7930664056684913:  29%|██▉       | 41/141 [01:02<02:35,  1.56s/it]avg_loss = 1.8103437849453516:  29%|██▉       | 41/141 [01:04<02:35,  1.56s/it]avg_loss = 1.8103437849453516:  30%|██▉       | 42/141 [01:04<02:34,  1.56s/it]avg_loss = 1.8269758418548938:  30%|██▉       | 42/141 [01:05<02:34,  1.56s/it]avg_loss = 1.8269758418548938:  30%|███       | 43/141 [01:05<02:32,  1.56s/it]avg_loss = 1.8298035507852382:  30%|███       | 43/141 [01:07<02:32,  1.56s/it]avg_loss = 1.8298035507852382:  31%|███       | 44/141 [01:07<02:31,  1.56s/it]avg_loss = 1.834060745769077:  31%|███       | 44/141 [01:09<02:31,  1.56s/it] avg_loss = 1.834060745769077:  32%|███▏      | 45/141 [01:09<02:30,  1.56s/it]avg_loss = 1.8396294298379317:  32%|███▏      | 45/141 [01:10<02:30,  1.56s/it]avg_loss = 1.8396294298379317:  33%|███▎      | 46/141 [01:10<02:28,  1.57s/it]avg_loss = 1.846365748567784:  33%|███▎      | 46/141 [01:12<02:28,  1.57s/it] avg_loss = 1.846365748567784:  33%|███▎      | 47/141 [01:12<02:27,  1.57s/it]avg_loss = 1.849749820927779:  33%|███▎      | 47/141 [01:13<02:27,  1.57s/it]avg_loss = 1.849749820927779:  34%|███▍      | 48/141 [01:13<02:25,  1.57s/it]avg_loss = 1.8484325822518797:  34%|███▍      | 48/141 [01:15<02:25,  1.57s/it]avg_loss = 1.8484325822518797:  35%|███▍      | 49/141 [01:15<02:24,  1.57s/it]avg_loss = 1.8481576609611512:  35%|███▍      | 49/141 [01:16<02:24,  1.57s/it]avg_loss = 1.8481576609611512:  35%|███▌      | 50/141 [01:16<02:22,  1.57s/it]avg_loss = 1.841741358532625:  35%|███▌      | 50/141 [01:18<02:22,  1.57s/it] avg_loss = 1.841741358532625:  36%|███▌      | 51/141 [01:18<02:21,  1.57s/it]avg_loss = 1.8380207190146813:  36%|███▌      | 51/141 [01:20<02:21,  1.57s/it]avg_loss = 1.8380207190146813:  37%|███▋      | 52/141 [01:20<02:19,  1.57s/it]avg_loss = 1.8316420181742255:  37%|███▋      | 52/141 [01:21<02:19,  1.57s/it]avg_loss = 1.8316420181742255:  38%|███▊      | 53/141 [01:21<02:18,  1.57s/it]avg_loss = 1.8287188013394673:  38%|███▊      | 53/141 [01:23<02:18,  1.57s/it]avg_loss = 1.8287188013394673:  38%|███▊      | 54/141 [01:23<02:16,  1.57s/it]avg_loss = 1.8210329229181463:  38%|███▊      | 54/141 [01:24<02:16,  1.57s/it]avg_loss = 1.8210329229181463:  39%|███▉      | 55/141 [01:24<02:15,  1.57s/it]avg_loss = 1.8134288787841797:  39%|███▉      | 55/141 [01:26<02:15,  1.57s/it]avg_loss = 1.8134288787841797:  40%|███▉      | 56/141 [01:26<02:13,  1.58s/it]avg_loss = 1.8066914060659576:  40%|███▉      | 56/141 [01:27<02:13,  1.58s/it]avg_loss = 1.8066914060659576:  40%|████      | 57/141 [01:27<02:12,  1.58s/it]avg_loss = 1.8039116941649338:  40%|████      | 57/141 [01:29<02:12,  1.58s/it]avg_loss = 1.8039116941649338:  41%|████      | 58/141 [01:29<02:10,  1.58s/it]avg_loss = 1.8061249680438285:  41%|████      | 58/141 [01:31<02:10,  1.58s/it]avg_loss = 1.8061249680438285:  42%|████▏     | 59/141 [01:31<02:09,  1.58s/it]avg_loss = 1.8118740538756053:  42%|████▏     | 59/141 [01:32<02:09,  1.58s/it]avg_loss = 1.8118740538756053:  43%|████▎     | 60/141 [01:32<02:08,  1.58s/it]avg_loss = 1.8178271797836805:  43%|████▎     | 60/141 [01:34<02:08,  1.58s/it]avg_loss = 1.8178271797836805:  43%|████▎     | 61/141 [01:34<02:06,  1.58s/it]avg_loss = 1.8252352303074253:  43%|████▎     | 61/141 [01:35<02:06,  1.58s/it]avg_loss = 1.8252352303074253:  44%|████▍     | 62/141 [01:35<02:04,  1.58s/it]avg_loss = 1.8159066760350788:  44%|████▍     | 62/141 [01:37<02:04,  1.58s/it]avg_loss = 1.8159066760350788:  45%|████▍     | 63/141 [01:37<02:03,  1.58s/it]avg_loss = 1.8138087932020426:  45%|████▍     | 63/141 [01:38<02:03,  1.58s/it]avg_loss = 1.8138087932020426:  45%|████▌     | 64/141 [01:38<02:01,  1.58s/it]avg_loss = 1.8112197362459623:  45%|████▌     | 64/141 [01:40<02:01,  1.58s/it]avg_loss = 1.8112197362459623:  46%|████▌     | 65/141 [01:40<02:00,  1.58s/it]avg_loss = 1.8051635948094455:  46%|████▌     | 65/141 [01:42<02:00,  1.58s/it]avg_loss = 1.8051635948094455:  47%|████▋     | 66/141 [01:42<01:58,  1.58s/it]avg_loss = 1.8026293017970982:  47%|████▋     | 66/141 [01:43<01:58,  1.58s/it]avg_loss = 1.8026293017970982:  48%|████▊     | 67/141 [01:43<01:57,  1.58s/it]avg_loss = 1.7992399194661308:  48%|████▊     | 67/141 [01:45<01:57,  1.58s/it]avg_loss = 1.7992399194661308:  48%|████▊     | 68/141 [01:45<01:55,  1.58s/it]avg_loss = 1.7962710010832634:  48%|████▊     | 68/141 [01:46<01:55,  1.58s/it]avg_loss = 1.7962710010832634:  49%|████▉     | 69/141 [01:46<01:54,  1.59s/it]avg_loss = 1.7971497075898306:  49%|████▉     | 69/141 [01:48<01:54,  1.59s/it]avg_loss = 1.7971497075898306:  50%|████▉     | 70/141 [01:48<01:52,  1.58s/it]avg_loss = 1.8009094570724058:  50%|████▉     | 70/141 [01:50<01:52,  1.58s/it]avg_loss = 1.8009094570724058:  50%|█████     | 71/141 [01:50<01:50,  1.58s/it]avg_loss = 1.8032429102394316:  50%|█████     | 71/141 [01:51<01:50,  1.58s/it]avg_loss = 1.8032429102394316:  51%|█████     | 72/141 [01:51<01:49,  1.58s/it]avg_loss = 1.8019199828579002:  51%|█████     | 72/141 [01:53<01:49,  1.58s/it]avg_loss = 1.8019199828579002:  52%|█████▏    | 73/141 [01:53<01:47,  1.58s/it]avg_loss = 1.8037318281225256:  52%|█████▏    | 73/141 [01:54<01:47,  1.58s/it]avg_loss = 1.8037318281225256:  52%|█████▏    | 74/141 [01:54<01:46,  1.58s/it]avg_loss = 1.804059697786967:  52%|█████▏    | 74/141 [01:56<01:46,  1.58s/it] avg_loss = 1.804059697786967:  53%|█████▎    | 75/141 [01:56<01:44,  1.58s/it]avg_loss = 1.8030565804556797:  53%|█████▎    | 75/141 [01:58<01:44,  1.58s/it]avg_loss = 1.8030565804556797:  54%|█████▍    | 76/141 [01:58<01:42,  1.58s/it]avg_loss = 1.804312613103297:  54%|█████▍    | 76/141 [01:59<01:42,  1.58s/it] avg_loss = 1.804312613103297:  55%|█████▍    | 77/141 [01:59<01:41,  1.58s/it]avg_loss = 1.8068558527873113:  55%|█████▍    | 77/141 [02:01<01:41,  1.58s/it]avg_loss = 1.8068558527873113:  55%|█████▌    | 78/141 [02:01<01:39,  1.59s/it]avg_loss = 1.8111329682265656:  55%|█████▌    | 78/141 [02:02<01:39,  1.59s/it]avg_loss = 1.8111329682265656:  56%|█████▌    | 79/141 [02:02<01:38,  1.59s/it]avg_loss = 1.8084778547286988:  56%|█████▌    | 79/141 [02:04<01:38,  1.59s/it]avg_loss = 1.8084778547286988:  57%|█████▋    | 80/141 [02:04<01:37,  1.59s/it]avg_loss = 1.807531377415598:  57%|█████▋    | 80/141 [02:05<01:37,  1.59s/it] avg_loss = 1.807531377415598:  57%|█████▋    | 81/141 [02:05<01:35,  1.59s/it]avg_loss = 1.8069069109311917:  57%|█████▋    | 81/141 [02:07<01:35,  1.59s/it]avg_loss = 1.8069069109311917:  58%|█████▊    | 82/141 [02:07<01:33,  1.59s/it]avg_loss = 1.8051771571837276:  58%|█████▊    | 82/141 [02:09<01:33,  1.59s/it]avg_loss = 1.8051771571837276:  59%|█████▉    | 83/141 [02:09<01:32,  1.59s/it]avg_loss = 1.8031269794418698:  59%|█████▉    | 83/141 [02:10<01:32,  1.59s/it]avg_loss = 1.8031269794418698:  60%|█████▉    | 84/141 [02:10<01:30,  1.59s/it]avg_loss = 1.800876802556655:  60%|█████▉    | 84/141 [02:12<01:30,  1.59s/it] avg_loss = 1.800876802556655:  60%|██████    | 85/141 [02:12<01:28,  1.59s/it]avg_loss = 1.8026256131571392:  60%|██████    | 85/141 [02:13<01:28,  1.59s/it]avg_loss = 1.8026256131571392:  61%|██████    | 86/141 [02:13<01:27,  1.59s/it]avg_loss = 1.804612241942307:  61%|██████    | 86/141 [02:15<01:27,  1.59s/it] avg_loss = 1.804612241942307:  62%|██████▏   | 87/141 [02:15<01:25,  1.59s/it]avg_loss = 1.804819568991661:  62%|██████▏   | 87/141 [02:17<01:25,  1.59s/it]avg_loss = 1.804819568991661:  62%|██████▏   | 88/141 [02:17<01:24,  1.59s/it]avg_loss = 1.8136477617735274:  62%|██████▏   | 88/141 [02:18<01:24,  1.59s/it]avg_loss = 1.8136477617735274:  63%|██████▎   | 89/141 [02:18<01:22,  1.59s/it]avg_loss = 1.821247947216034:  63%|██████▎   | 89/141 [02:20<01:22,  1.59s/it] avg_loss = 1.821247947216034:  64%|██████▍   | 90/141 [02:20<01:21,  1.59s/it]avg_loss = 1.8244704610698825:  64%|██████▍   | 90/141 [02:21<01:21,  1.59s/it]avg_loss = 1.8244704610698825:  65%|██████▍   | 91/141 [02:21<01:19,  1.59s/it]avg_loss = 1.829447610222775:  65%|██████▍   | 91/141 [02:23<01:19,  1.59s/it] avg_loss = 1.829447610222775:  65%|██████▌   | 92/141 [02:23<01:18,  1.59s/it]avg_loss = 1.83449286671095:  65%|██████▌   | 92/141 [02:25<01:18,  1.59s/it] avg_loss = 1.83449286671095:  66%|██████▌   | 93/141 [02:25<01:16,  1.59s/it]avg_loss = 1.835613231709663:  66%|██████▌   | 93/141 [02:26<01:16,  1.59s/it]avg_loss = 1.835613231709663:  67%|██████▋   | 94/141 [02:26<01:15,  1.60s/it]avg_loss = 1.839412986604791:  67%|██████▋   | 94/141 [02:28<01:15,  1.60s/it]avg_loss = 1.839412986604791:  67%|██████▋   | 95/141 [02:28<01:13,  1.60s/it]avg_loss = 1.8402990413208802:  67%|██████▋   | 95/141 [02:29<01:13,  1.60s/it]avg_loss = 1.8402990413208802:  68%|██████▊   | 96/141 [02:29<01:11,  1.60s/it]avg_loss = 1.8423329591751099:  68%|██████▊   | 96/141 [02:31<01:11,  1.60s/it]avg_loss = 1.8423329591751099:  69%|██████▉   | 97/141 [02:31<01:10,  1.60s/it]avg_loss = 1.8369616914768607:  69%|██████▉   | 97/141 [02:33<01:10,  1.60s/it]avg_loss = 1.8369616914768607:  70%|██████▉   | 98/141 [02:33<01:08,  1.60s/it]avg_loss = 1.837631588030343:  70%|██████▉   | 98/141 [02:34<01:08,  1.60s/it] avg_loss = 1.837631588030343:  70%|███████   | 99/141 [02:34<01:07,  1.60s/it]avg_loss = 1.8393615639209748:  70%|███████   | 99/141 [02:36<01:07,  1.60s/it]avg_loss = 1.8393615639209748:  71%|███████   | 100/141 [02:36<01:05,  1.60s/it]avg_loss = 1.837736430734691:  71%|███████   | 100/141 [02:37<01:05,  1.60s/it] avg_loss = 1.837736430734691:  72%|███████▏  | 101/141 [02:37<01:04,  1.60s/it]avg_loss = 1.837738201898687:  72%|███████▏  | 101/141 [02:39<01:04,  1.60s/it]avg_loss = 1.837738201898687:  72%|███████▏  | 102/141 [02:39<01:02,  1.60s/it]avg_loss = 1.8354256338286168:  72%|███████▏  | 102/141 [02:41<01:02,  1.60s/it]avg_loss = 1.8354256338286168:  73%|███████▎  | 103/141 [02:41<01:00,  1.60s/it]avg_loss = 1.8375568091869354:  73%|███████▎  | 103/141 [02:42<01:00,  1.60s/it]avg_loss = 1.8375568091869354:  74%|███████▍  | 104/141 [02:42<00:59,  1.60s/it]avg_loss = 1.8352324338186354:  74%|███████▍  | 104/141 [02:44<00:59,  1.60s/it]avg_loss = 1.8352324338186354:  74%|███████▍  | 105/141 [02:44<00:57,  1.60s/it]avg_loss = 1.8337935006843422:  74%|███████▍  | 105/141 [02:45<00:57,  1.60s/it]avg_loss = 1.8337935006843422:  75%|███████▌  | 106/141 [02:45<00:56,  1.60s/it]avg_loss = 1.8313419362094914:  75%|███████▌  | 106/141 [02:47<00:56,  1.60s/it]avg_loss = 1.8313419362094914:  76%|███████▌  | 107/141 [02:47<00:54,  1.60s/it]avg_loss = 1.8288500143422022:  76%|███████▌  | 107/141 [02:49<00:54,  1.60s/it]avg_loss = 1.8288500143422022:  77%|███████▋  | 108/141 [02:49<00:52,  1.60s/it]avg_loss = 1.8261198483475851:  77%|███████▋  | 108/141 [02:50<00:52,  1.60s/it]avg_loss = 1.8261198483475851:  77%|███████▋  | 109/141 [02:50<00:51,  1.60s/it]avg_loss = 1.8236007755452937:  77%|███████▋  | 109/141 [02:52<00:51,  1.60s/it]avg_loss = 1.8236007755452937:  78%|███████▊  | 110/141 [02:52<00:49,  1.60s/it]avg_loss = 1.8259535523148271:  78%|███████▊  | 110/141 [02:53<00:49,  1.60s/it]avg_loss = 1.8259535523148271:  79%|███████▊  | 111/141 [02:53<00:48,  1.60s/it]avg_loss = 1.825767035995211:  79%|███████▊  | 111/141 [02:55<00:48,  1.60s/it] avg_loss = 1.825767035995211:  79%|███████▉  | 112/141 [02:55<00:46,  1.60s/it]avg_loss = 1.8269773078175773:  79%|███████▉  | 112/141 [02:57<00:46,  1.60s/it]avg_loss = 1.8269773078175773:  80%|████████  | 113/141 [02:57<00:44,  1.60s/it]avg_loss = 1.8279289258153815:  80%|████████  | 113/141 [02:58<00:44,  1.60s/it]avg_loss = 1.8279289258153815:  81%|████████  | 114/141 [02:58<00:43,  1.60s/it]avg_loss = 1.8273715081422226:  81%|████████  | 114/141 [03:00<00:43,  1.60s/it]avg_loss = 1.8273715081422226:  82%|████████▏ | 115/141 [03:00<00:41,  1.60s/it]avg_loss = 1.8258777055247077:  82%|████████▏ | 115/141 [03:01<00:41,  1.60s/it]avg_loss = 1.8258777055247077:  82%|████████▏ | 116/141 [03:01<00:40,  1.60s/it]avg_loss = 1.8280079935350988:  82%|████████▏ | 116/141 [03:03<00:40,  1.60s/it]avg_loss = 1.8280079935350988:  83%|████████▎ | 117/141 [03:03<00:38,  1.60s/it]avg_loss = 1.8277691875473927:  83%|████████▎ | 117/141 [03:05<00:38,  1.60s/it]avg_loss = 1.8277691875473927:  84%|████████▎ | 118/141 [03:05<00:36,  1.60s/it]avg_loss = 1.8264620815004622:  84%|████████▎ | 118/141 [03:06<00:36,  1.60s/it]avg_loss = 1.8264620815004622:  84%|████████▍ | 119/141 [03:06<00:35,  1.60s/it]avg_loss = 1.8248075117667517:  84%|████████▍ | 119/141 [03:08<00:35,  1.60s/it]avg_loss = 1.8248075117667517:  85%|████████▌ | 120/141 [03:08<00:33,  1.60s/it]avg_loss = 1.8246630085401299:  85%|████████▌ | 120/141 [03:09<00:33,  1.60s/it]avg_loss = 1.8246630085401299:  86%|████████▌ | 121/141 [03:09<00:32,  1.60s/it]avg_loss = 1.8249419663773208:  86%|████████▌ | 121/141 [03:11<00:32,  1.60s/it]avg_loss = 1.8249419663773208:  87%|████████▋ | 122/141 [03:11<00:30,  1.60s/it]avg_loss = 1.824873591826214:  87%|████████▋ | 122/141 [03:13<00:30,  1.60s/it] avg_loss = 1.824873591826214:  87%|████████▋ | 123/141 [03:13<00:28,  1.60s/it]avg_loss = 1.8251586037297403:  87%|████████▋ | 123/141 [03:14<00:28,  1.60s/it]avg_loss = 1.8251586037297403:  88%|████████▊ | 124/141 [03:14<00:27,  1.60s/it]avg_loss = 1.8239938097000121:  88%|████████▊ | 124/141 [03:16<00:27,  1.60s/it]avg_loss = 1.8239938097000121:  89%|████████▊ | 125/141 [03:16<00:25,  1.60s/it]avg_loss = 1.8244555554692707:  89%|████████▊ | 125/141 [03:17<00:25,  1.60s/it]avg_loss = 1.8244555554692707:  89%|████████▉ | 126/141 [03:17<00:23,  1.60s/it]avg_loss = 1.824266674011711:  89%|████████▉ | 126/141 [03:19<00:23,  1.60s/it] avg_loss = 1.824266674011711:  90%|█████████ | 127/141 [03:19<00:22,  1.60s/it]avg_loss = 1.8229076685383916:  90%|█████████ | 127/141 [03:21<00:22,  1.60s/it]avg_loss = 1.8229076685383916:  91%|█████████ | 128/141 [03:21<00:20,  1.60s/it]avg_loss = 1.8231387332428333:  91%|█████████ | 128/141 [03:22<00:20,  1.60s/it]avg_loss = 1.8231387332428333:  91%|█████████▏| 129/141 [03:22<00:19,  1.60s/it]avg_loss = 1.8238246092429529:  91%|█████████▏| 129/141 [03:24<00:19,  1.60s/it]avg_loss = 1.8238246092429529:  92%|█████████▏| 130/141 [03:24<00:17,  1.60s/it]avg_loss = 1.8248021020234086:  92%|█████████▏| 130/141 [03:25<00:17,  1.60s/it]avg_loss = 1.8248021020234086:  93%|█████████▎| 131/141 [03:25<00:15,  1.60s/it]avg_loss = 1.8254562977588538:  93%|█████████▎| 131/141 [03:27<00:15,  1.60s/it]avg_loss = 1.8254562977588538:  94%|█████████▎| 132/141 [03:27<00:14,  1.60s/it]avg_loss = 1.8226640520239235:  94%|█████████▎| 132/141 [03:29<00:14,  1.60s/it]avg_loss = 1.8226640520239235:  94%|█████████▍| 133/141 [03:29<00:12,  1.60s/it]avg_loss = 1.8183787865425223:  94%|█████████▍| 133/141 [03:30<00:12,  1.60s/it]avg_loss = 1.8183787865425223:  95%|█████████▌| 134/141 [03:30<00:11,  1.60s/it]avg_loss = 1.8209270812846996:  95%|█████████▌| 134/141 [03:32<00:11,  1.60s/it]avg_loss = 1.8209270812846996:  96%|█████████▌| 135/141 [03:32<00:09,  1.60s/it]avg_loss = 1.8244452827117021:  96%|█████████▌| 135/141 [03:33<00:09,  1.60s/it]avg_loss = 1.8244452827117021:  96%|█████████▋| 136/141 [03:33<00:07,  1.60s/it]avg_loss = 1.8253535467342739:  96%|█████████▋| 136/141 [03:35<00:07,  1.60s/it]avg_loss = 1.8253535467342739:  97%|█████████▋| 137/141 [03:35<00:06,  1.60s/it]avg_loss = 1.8239732851152834:  97%|█████████▋| 137/141 [03:37<00:06,  1.60s/it]avg_loss = 1.8239732851152834:  98%|█████████▊| 138/141 [03:37<00:04,  1.60s/it]avg_loss = 1.8241517552368933:  98%|█████████▊| 138/141 [03:38<00:04,  1.60s/it]avg_loss = 1.8241517552368933:  99%|█████████▊| 139/141 [03:38<00:03,  1.60s/it]avg_loss = 1.8246327085154397:  99%|█████████▊| 139/141 [03:40<00:03,  1.60s/it]avg_loss = 1.8246327085154397:  99%|█████████▉| 140/141 [03:40<00:01,  1.60s/it]avg_loss = 1.8259695024355083:  99%|█████████▉| 140/141 [03:41<00:01,  1.60s/it]avg_loss = 1.8259695024355083: 100%|██████████| 141/141 [03:41<00:00,  1.60s/it]avg_loss = 1.8259695024355083: 100%|██████████| 141/141 [03:41<00:00,  1.57s/it]
I0328 19:21:34.510513 2316500 eval_ppl.py:107] wikitext2 perplexity: 6.208811283111572
wikitext2 perplexity: 6.209
