I0328 02:14:41.519582 2083830 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:14:41.519679 2083830 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:14:41.519719 2083830 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:14:41.841167 2083830 config.py:54] PyTorch version 2.6.0 available.
W0328 02:14:42.029483 2083830 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 02:14:42.584344 2083830 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.29it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.67it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.83it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.93it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.61it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.77it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.96it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.82it/s]
I0328 02:14:44.051388 2083830 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.41it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:17,  1.69it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.79it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.85it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.88it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.90it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.92it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.92it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:11,  1.92it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.93it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:10,  1.93it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.93it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:09,  1.92it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.92it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:08,  1.93it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.94it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:08<00:07,  1.95it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.97it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:09<00:06,  1.96it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.96it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:10<00:05,  1.96it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.96it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.96it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.99it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:12<00:03,  2.05it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:02,  2.10it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:13<00:02,  2.14it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:01,  2.18it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:14<00:01,  2.18it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:00,  2.19it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:15<00:00,  2.20it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  2.21it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.99it/s]
I0328 02:15:05.657418 2083830 quantize_finetune_llama.py:185] loaded compression model
I0328 02:15:23.679254 2083830 quantize_finetune_llama.py:189] loaded dataset and devset
I0328 02:15:29.085625 2083830 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 02:16:28.669685 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 59.43602800369263s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0328 02:16:50.293050 2085215 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:16:50.293150 2085215 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:16:50.293188 2085215 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:16:50.612344 2085215 config.py:54] PyTorch version 2.6.0 available.
W0328 02:16:50.798907 2085215 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 02:16:51.339990 2085215 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 02:16:51.346401 2083830 quantize_finetune_llama.py:209] layer 1 gpu 1
I0328 02:16:51.357737 2085215 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 02:17:07.579493 2085215 finetune.py:45] layer 0_v initial loss 2.059545522570261e-06
W0328 02:17:07.579677 2085215 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 02:17:42.064114 2085215 finetune.py:68] layer 0_v @ epoch 0 new loss 1.8523826383898268e-06 old loss 2.059545522570261e-06 BETTER
I0328 02:17:45.895402 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 54.37494111061096s
I0328 02:17:55.066592 2085924 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:17:55.066691 2085924 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:17:55.066727 2085924 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:17:55.413821 2085924 config.py:54] PyTorch version 2.6.0 available.
W0328 02:17:55.603764 2085924 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 02:17:56.184103 2085924 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 02:17:56.188518 2083830 quantize_finetune_llama.py:209] layer 2 gpu 2
I0328 02:17:56.202008 2085924 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 02:18:12.744286 2085924 finetune.py:45] layer 1_v initial loss 1.789438283594791e-05
W0328 02:18:12.744709 2085924 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 02:18:18.009582 2085215 finetune.py:68] layer 0_v @ epoch 1 new loss 1.7765756865628646e-06 old loss 1.8523826383898268e-06 BETTER
I0328 02:18:45.833836 2085924 finetune.py:68] layer 1_v @ epoch 0 new loss 4.084497959411237e-06 old loss 1.789438283594791e-05 BETTER
I0328 02:18:54.427639 2085215 finetune.py:68] layer 0_v @ epoch 2 new loss 1.7355207546643214e-06 old loss 1.7765756865628646e-06 BETTER
I0328 02:19:02.842245 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 66.46329617500305s
I0328 02:19:11.408163 2086757 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:19:11.408303 2086757 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:19:11.408351 2086757 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:19:11.757553 2086757 config.py:54] PyTorch version 2.6.0 available.
W0328 02:19:11.959580 2086757 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 02:19:12.557176 2086757 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 02:19:12.561165 2083830 quantize_finetune_llama.py:209] layer 3 gpu 3
I0328 02:19:12.575766 2086757 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 02:19:20.310765 2085924 finetune.py:68] layer 1_v @ epoch 1 new loss 2.819241899487679e-06 old loss 4.084497959411237e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 02:19:30.075119 2086757 finetune.py:45] layer 2_v initial loss 3.777799065574072e-05
W0328 02:19:30.075536 2086757 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 02:19:31.418104 2085215 finetune.py:68] layer 0_v @ epoch 3 new loss 1.7089962511818158e-06 old loss 1.7355207546643214e-06 BETTER
I0328 02:19:54.992801 2085924 finetune.py:68] layer 1_v @ epoch 2 new loss 2.5533572625136003e-06 old loss 2.819241899487679e-06 BETTER
I0328 02:20:03.758168 2086757 finetune.py:68] layer 2_v @ epoch 0 new loss 6.604177542612888e-06 old loss 3.777799065574072e-05 BETTER
I0328 02:20:08.415697 2085215 finetune.py:68] layer 0_v @ epoch 4 new loss 1.688905399532814e-06 old loss 1.7089962511818158e-06 BETTER
I0328 02:20:25.025048 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 72.28132891654968s
I0328 02:20:32.268688 2085215 finetune.py:45] layer 0_q initial loss 1.6913705849219696e-06
I0328 02:20:33.696829 2085924 finetune.py:68] layer 1_v @ epoch 3 new loss 2.426938408461865e-06 old loss 2.5533572625136003e-06 BETTER
I0328 02:20:36.756937 2087681 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:20:36.757050 2087681 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:20:36.757093 2087681 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:20:37.133540 2087681 config.py:54] PyTorch version 2.6.0 available.
W0328 02:20:37.338660 2087681 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 02:20:37.942315 2087681 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 02:20:37.946300 2083830 quantize_finetune_llama.py:209] layer 4 gpu 0
I0328 02:20:37.959830 2087681 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 02:20:39.316112 2086757 finetune.py:68] layer 2_v @ epoch 1 new loss 4.0697782424103934e-06 old loss 6.604177542612888e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 02:20:54.763144 2087681 finetune.py:45] layer 3_v initial loss 3.9855109207564965e-05
W0328 02:20:54.763456 2087681 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 02:21:07.318597 2085215 finetune.py:68] layer 0_q @ epoch 0 new loss 1.6746421351854224e-06 old loss 1.6913705849219696e-06 BETTER
I0328 02:21:08.577054 2085924 finetune.py:68] layer 1_v @ epoch 4 new loss 2.3392285584122874e-06 old loss 2.426938408461865e-06 BETTER
I0328 02:21:14.143007 2086757 finetune.py:68] layer 2_v @ epoch 2 new loss 3.6946373711543856e-06 old loss 4.0697782424103934e-06 BETTER
I0328 02:21:27.829492 2085924 finetune.py:45] layer 1_q initial loss 2.378167437200318e-06
I0328 02:21:27.880810 2087681 finetune.py:68] layer 3_v @ epoch 0 new loss 9.349571882921737e-06 old loss 3.9855109207564965e-05 BETTER
I0328 02:21:43.690421 2085215 finetune.py:68] layer 0_q @ epoch 1 new loss 1.6606460349066765e-06 old loss 1.6746421351854224e-06 BETTER
I0328 02:21:49.336895 2086757 finetune.py:68] layer 2_v @ epoch 3 new loss 3.567350859157159e-06 old loss 3.6946373711543856e-06 BETTER
I0328 02:22:01.352996 2085924 finetune.py:68] layer 1_q @ epoch 0 new loss 2.293123316121637e-06 old loss 2.378167437200318e-06 BETTER
I0328 02:22:02.234503 2087681 finetune.py:68] layer 3_v @ epoch 1 new loss 6.544863936142065e-06 old loss 9.349571882921737e-06 BETTER
I0328 02:22:20.262155 2085215 finetune.py:68] layer 0_q @ epoch 2 new loss 1.6484708567077178e-06 old loss 1.6606460349066765e-06 BETTER
I0328 02:22:24.579925 2086757 finetune.py:68] layer 2_v @ epoch 4 new loss 3.4862880511354888e-06 old loss 3.567350859157159e-06 BETTER
I0328 02:22:35.832809 2085924 finetune.py:68] layer 1_q @ epoch 1 new loss 2.2290162178251194e-06 old loss 2.293123316121637e-06 BETTER
I0328 02:22:36.766169 2087681 finetune.py:68] layer 3_v @ epoch 2 new loss 5.932859494350851e-06 old loss 6.544863936142065e-06 BETTER
I0328 02:22:43.872524 2086757 finetune.py:45] layer 2_q initial loss 3.9789611037122086e-06
I0328 02:22:57.157451 2085215 finetune.py:68] layer 0_q @ epoch 3 new loss 1.6375828408854431e-06 old loss 1.6484708567077178e-06 BETTER
I0328 02:23:10.580975 2085924 finetune.py:68] layer 1_q @ epoch 2 new loss 2.1764783468825044e-06 old loss 2.2290162178251194e-06 BETTER
I0328 02:23:11.476096 2087681 finetune.py:68] layer 3_v @ epoch 3 new loss 5.669523034157464e-06 old loss 5.932859494350851e-06 BETTER
I0328 02:23:17.724808 2086757 finetune.py:68] layer 2_q @ epoch 0 new loss 3.8744105950172525e-06 old loss 3.9789611037122086e-06 BETTER
I0328 02:23:34.106760 2085215 finetune.py:68] layer 0_q @ epoch 4 new loss 1.6274451581921312e-06 old loss 1.6375828408854431e-06 BETTER
I0328 02:23:45.156719 2085924 finetune.py:68] layer 1_q @ epoch 3 new loss 2.1349333110265434e-06 old loss 2.1764783468825044e-06 BETTER
I0328 02:23:46.295986 2087681 finetune.py:68] layer 3_v @ epoch 4 new loss 5.509403308678884e-06 old loss 5.669523034157464e-06 BETTER
I0328 02:23:52.128457 2085215 finetune.py:45] layer 0_k initial loss 1.6313160813297145e-06
I0328 02:23:52.712044 2086757 finetune.py:68] layer 2_q @ epoch 1 new loss 3.8066968954808544e-06 old loss 3.8744105950172525e-06 BETTER
I0328 02:24:05.973167 2087681 finetune.py:45] layer 3_q initial loss 6.490174655482406e-06
I0328 02:24:20.099776 2085924 finetune.py:68] layer 1_q @ epoch 4 new loss 2.0955048967152834e-06 old loss 2.1349333110265434e-06 BETTER
I0328 02:24:27.391128 2086757 finetune.py:68] layer 2_q @ epoch 2 new loss 3.7543104554060847e-06 old loss 3.8066968954808544e-06 BETTER
I0328 02:24:27.409998 2085215 finetune.py:68] layer 0_k @ epoch 0 new loss 1.6223618786170846e-06 old loss 1.6313160813297145e-06 BETTER
I0328 02:24:37.672996 2085924 finetune.py:45] layer 1_k initial loss 2.144353175026481e-06
I0328 02:24:39.409144 2087681 finetune.py:68] layer 3_q @ epoch 0 new loss 6.3155162024486344e-06 old loss 6.490174655482406e-06 BETTER
I0328 02:25:02.372585 2086757 finetune.py:68] layer 2_q @ epoch 3 new loss 3.71099122276064e-06 old loss 3.7543104554060847e-06 BETTER
I0328 02:25:03.951669 2085215 finetune.py:68] layer 0_k @ epoch 1 new loss 1.6141535752467462e-06 old loss 1.6223618786170846e-06 BETTER
I0328 02:25:11.650056 2085924 finetune.py:68] layer 1_k @ epoch 0 new loss 2.1049779661552748e-06 old loss 2.144353175026481e-06 BETTER
I0328 02:25:13.729924 2087681 finetune.py:68] layer 3_q @ epoch 1 new loss 6.200421466928674e-06 old loss 6.3155162024486344e-06 BETTER
I0328 02:25:37.403298 2086757 finetune.py:68] layer 2_q @ epoch 4 new loss 3.674515028251335e-06 old loss 3.71099122276064e-06 BETTER
I0328 02:25:40.529112 2085215 finetune.py:68] layer 0_k @ epoch 2 new loss 1.6066013586168992e-06 old loss 1.6141535752467462e-06 BETTER
I0328 02:25:46.008033 2085924 finetune.py:68] layer 1_k @ epoch 1 new loss 2.0749146187881706e-06 old loss 2.1049779661552748e-06 BETTER
I0328 02:25:47.988099 2087681 finetune.py:68] layer 3_q @ epoch 2 new loss 6.113886229286436e-06 old loss 6.200421466928674e-06 BETTER
I0328 02:25:55.620002 2086757 finetune.py:45] layer 2_k initial loss 3.848238520731684e-06
I0328 02:26:17.364430 2085215 finetune.py:68] layer 0_k @ epoch 3 new loss 1.599436814103683e-06 old loss 1.6066013586168992e-06 BETTER
I0328 02:26:20.695280 2085924 finetune.py:68] layer 1_k @ epoch 2 new loss 2.0482925719989e-06 old loss 2.0749146187881706e-06 BETTER
I0328 02:26:22.375947 2087681 finetune.py:68] layer 3_q @ epoch 3 new loss 6.043238045094768e-06 old loss 6.113886229286436e-06 BETTER
I0328 02:26:29.271903 2086757 finetune.py:68] layer 2_k @ epoch 0 new loss 3.8078305806266144e-06 old loss 3.848238520731684e-06 BETTER
I0328 02:26:54.194067 2085215 finetune.py:68] layer 0_k @ epoch 4 new loss 1.5926750620565144e-06 old loss 1.599436814103683e-06 BETTER
I0328 02:26:55.321047 2085924 finetune.py:68] layer 1_k @ epoch 3 new loss 2.024830109803588e-06 old loss 2.0482925719989e-06 BETTER
I0328 02:26:56.749341 2087681 finetune.py:68] layer 3_q @ epoch 4 new loss 5.979784873488825e-06 old loss 6.043238045094768e-06 BETTER
I0328 02:27:03.912226 2086757 finetune.py:68] layer 2_k @ epoch 1 new loss 3.779161488637328e-06 old loss 3.8078305806266144e-06 BETTER
I0328 02:27:13.864969 2085215 finetune.py:45] layer 0_o initial loss 2.673754806892248e-06
I0328 02:27:14.654888 2087681 finetune.py:45] layer 3_k initial loss 6.394810043275356e-06
I0328 02:27:29.848131 2085924 finetune.py:68] layer 1_k @ epoch 4 new loss 2.0030333871545736e-06 old loss 2.024830109803588e-06 BETTER
I0328 02:27:38.468975 2086757 finetune.py:68] layer 2_k @ epoch 2 new loss 3.754257704713382e-06 old loss 3.779161488637328e-06 BETTER
I0328 02:27:47.945375 2087681 finetune.py:68] layer 3_k @ epoch 0 new loss 6.340592790365918e-06 old loss 6.394810043275356e-06 BETTER
I0328 02:27:48.616228 2085215 finetune.py:68] layer 0_o @ epoch 0 new loss 2.6560842343315016e-06 old loss 2.673754806892248e-06 BETTER
I0328 02:27:49.512231 2085924 finetune.py:45] layer 1_o initial loss 7.117113455024082e-06
I0328 02:28:13.269989 2086757 finetune.py:68] layer 2_k @ epoch 3 new loss 3.7322338357625995e-06 old loss 3.754257704713382e-06 BETTER
I0328 02:28:22.511359 2087681 finetune.py:68] layer 3_k @ epoch 1 new loss 6.294950708252145e-06 old loss 6.340592790365918e-06 BETTER
I0328 02:28:22.662945 2085924 finetune.py:68] layer 1_o @ epoch 0 new loss 6.643428605457302e-06 old loss 7.117113455024082e-06 BETTER
I0328 02:28:24.648096 2085215 finetune.py:68] layer 0_o @ epoch 1 new loss 2.642249000928132e-06 old loss 2.6560842343315016e-06 BETTER
I0328 02:28:48.164869 2086757 finetune.py:68] layer 2_k @ epoch 4 new loss 3.7108056858414784e-06 old loss 3.7322338357625995e-06 BETTER
I0328 02:28:56.591718 2085924 finetune.py:68] layer 1_o @ epoch 1 new loss 6.282425601966679e-06 old loss 6.643428605457302e-06 BETTER
I0328 02:28:56.698980 2087681 finetune.py:68] layer 3_k @ epoch 2 new loss 6.253656010812847e-06 old loss 6.294950708252145e-06 BETTER
I0328 02:29:00.907992 2085215 finetune.py:68] layer 0_o @ epoch 2 new loss 2.6307711777917575e-06 old loss 2.642249000928132e-06 BETTER
I0328 02:29:08.140748 2086757 finetune.py:45] layer 2_o initial loss 1.0709442904044408e-05
I0328 02:29:30.708805 2085924 finetune.py:68] layer 1_o @ epoch 2 new loss 6.002681402605958e-06 old loss 6.282425601966679e-06 BETTER
I0328 02:29:31.483447 2087681 finetune.py:68] layer 3_k @ epoch 3 new loss 6.217877398739802e-06 old loss 6.253656010812847e-06 BETTER
I0328 02:29:37.130815 2085215 finetune.py:68] layer 0_o @ epoch 3 new loss 2.6208049348497298e-06 old loss 2.6307711777917575e-06 BETTER
I0328 02:29:41.235663 2086757 finetune.py:68] layer 2_o @ epoch 0 new loss 9.832756404648535e-06 old loss 1.0709442904044408e-05 BETTER
I0328 02:30:04.974691 2085924 finetune.py:68] layer 1_o @ epoch 3 new loss 5.782539574283874e-06 old loss 6.002681402605958e-06 BETTER
I0328 02:30:05.689724 2087681 finetune.py:68] layer 3_k @ epoch 4 new loss 6.184720859891968e-06 old loss 6.217877398739802e-06 BETTER
I0328 02:30:13.342929 2085215 finetune.py:68] layer 0_o @ epoch 4 new loss 2.6122265808226075e-06 old loss 2.6208049348497298e-06 BETTER
I0328 02:30:15.335626 2086757 finetune.py:68] layer 2_o @ epoch 1 new loss 9.286156455345917e-06 old loss 9.832756404648535e-06 BETTER
I0328 02:30:25.075784 2087681 finetune.py:45] layer 3_o initial loss 1.574208181409631e-05
I0328 02:30:39.206904 2085924 finetune.py:68] layer 1_o @ epoch 4 new loss 5.611182132270187e-06 old loss 5.782539574283874e-06 BETTER
I0328 02:30:44.831356 2085215 finetune.py:45] layer 0_up initial loss 3.613588205553242e-06
I0328 02:30:49.447569 2086757 finetune.py:68] layer 2_o @ epoch 2 new loss 8.926444934331812e-06 old loss 9.286156455345917e-06 BETTER
I0328 02:30:57.572549 2087681 finetune.py:68] layer 3_o @ epoch 0 new loss 1.4569332051905803e-05 old loss 1.574208181409631e-05 BETTER
I0328 02:31:10.888592 2085924 finetune.py:45] layer 1_up initial loss 7.8951743489597e-06
I0328 02:31:17.161411 2085215 finetune.py:68] layer 0_up @ epoch 0 new loss 3.595139105527778e-06 old loss 3.613588205553242e-06 BETTER
I0328 02:31:23.761821 2086757 finetune.py:68] layer 2_o @ epoch 3 new loss 8.679086022311822e-06 old loss 8.926444934331812e-06 BETTER
I0328 02:31:31.185469 2087681 finetune.py:68] layer 3_o @ epoch 1 new loss 1.4116260899754707e-05 old loss 1.4569332051905803e-05 BETTER
I0328 02:31:41.602817 2085924 finetune.py:68] layer 1_up @ epoch 0 new loss 7.767303031869233e-06 old loss 7.8951743489597e-06 BETTER
I0328 02:31:50.562851 2085215 finetune.py:68] layer 0_up @ epoch 1 new loss 3.5817322441289434e-06 old loss 3.595139105527778e-06 BETTER
I0328 02:31:57.869445 2086757 finetune.py:68] layer 2_o @ epoch 4 new loss 8.500345757056493e-06 old loss 8.679086022311822e-06 BETTER
I0328 02:32:04.692741 2087681 finetune.py:68] layer 3_o @ epoch 2 new loss 1.3877828678232618e-05 old loss 1.4116260899754707e-05 BETTER
I0328 02:32:13.459706 2085924 finetune.py:68] layer 1_up @ epoch 1 new loss 7.666712008358445e-06 old loss 7.767303031869233e-06 BETTER
I0328 02:32:24.226737 2085215 finetune.py:68] layer 0_up @ epoch 2 new loss 3.5710379506781464e-06 old loss 3.5817322441289434e-06 BETTER
I0328 02:32:29.401267 2086757 finetune.py:45] layer 2_up initial loss 1.4070458746573422e-05
I0328 02:32:38.254774 2087681 finetune.py:68] layer 3_o @ epoch 3 new loss 1.371434518659953e-05 old loss 1.3877828678232618e-05 BETTER
I0328 02:32:45.731088 2085924 finetune.py:68] layer 1_up @ epoch 2 new loss 7.583881142636528e-06 old loss 7.666712008358445e-06 BETTER
I0328 02:32:58.129447 2085215 finetune.py:68] layer 0_up @ epoch 3 new loss 3.562319534466951e-06 old loss 3.5710379506781464e-06 BETTER
I0328 02:33:00.419036 2086757 finetune.py:68] layer 2_up @ epoch 0 new loss 1.39194235089235e-05 old loss 1.4070458746573422e-05 BETTER
I0328 02:33:11.741048 2087681 finetune.py:68] layer 3_o @ epoch 4 new loss 1.3588008187070955e-05 old loss 1.371434518659953e-05 BETTER
I0328 02:33:17.916750 2085924 finetune.py:68] layer 1_up @ epoch 3 new loss 7.516611731261946e-06 old loss 7.583881142636528e-06 BETTER
I0328 02:33:31.876551 2085215 finetune.py:68] layer 0_up @ epoch 4 new loss 3.5549835502024507e-06 old loss 3.562319534466951e-06 BETTER
I0328 02:33:32.549592 2086757 finetune.py:68] layer 2_up @ epoch 1 new loss 1.3808238691126462e-05 old loss 1.39194235089235e-05 BETTER
I0328 02:33:43.012682 2087681 finetune.py:45] layer 3_up initial loss 2.6029476430267096e-05
I0328 02:33:50.051939 2085924 finetune.py:68] layer 1_up @ epoch 4 new loss 7.460509095835732e-06 old loss 7.516611731261946e-06 BETTER
I0328 02:34:03.274924 2085215 finetune.py:45] layer 0_gate initial loss 4.27175109507516e-06
I0328 02:34:04.848557 2086757 finetune.py:68] layer 2_up @ epoch 2 new loss 1.3722076801059302e-05 old loss 1.3808238691126462e-05 BETTER
I0328 02:34:13.309167 2087681 finetune.py:68] layer 3_up @ epoch 0 new loss 2.58700420090463e-05 old loss 2.6029476430267096e-05 BETTER
I0328 02:34:21.788977 2085924 finetune.py:45] layer 1_gate initial loss 9.136556400335394e-06
I0328 02:34:33.618143 2085215 finetune.py:68] layer 0_gate @ epoch 0 new loss 4.252110556990374e-06 old loss 4.27175109507516e-06 BETTER
I0328 02:34:37.130459 2086757 finetune.py:68] layer 2_up @ epoch 3 new loss 1.3651502740685828e-05 old loss 1.3722076801059302e-05 BETTER
I0328 02:34:44.936755 2087681 finetune.py:68] layer 3_up @ epoch 1 new loss 2.5756535251275636e-05 old loss 2.58700420090463e-05 BETTER
I0328 02:34:50.468784 2085924 finetune.py:68] layer 1_gate @ epoch 0 new loss 9.026172847370617e-06 old loss 9.136556400335394e-06 BETTER
I0328 02:35:05.033612 2085215 finetune.py:68] layer 0_gate @ epoch 1 new loss 4.236458607920213e-06 old loss 4.252110556990374e-06 BETTER
I0328 02:35:09.394529 2086757 finetune.py:68] layer 2_up @ epoch 4 new loss 1.35913142003119e-05 old loss 1.3651502740685828e-05 BETTER
I0328 02:35:16.317956 2087681 finetune.py:68] layer 3_up @ epoch 2 new loss 2.5662468033260666e-05 old loss 2.5756535251275636e-05 BETTER
I0328 02:35:20.174417 2085924 finetune.py:68] layer 1_gate @ epoch 1 new loss 8.969021109805908e-06 old loss 9.026172847370617e-06 BETTER
I0328 02:35:36.603907 2085215 finetune.py:68] layer 0_gate @ epoch 2 new loss 4.223599717079196e-06 old loss 4.236458607920213e-06 BETTER
I0328 02:35:41.005012 2086757 finetune.py:45] layer 2_gate initial loss 1.7221420421265066e-05
I0328 02:35:47.922852 2087681 finetune.py:68] layer 3_up @ epoch 3 new loss 2.5579865905456245e-05 old loss 2.5662468033260666e-05 BETTER
I0328 02:35:49.968746 2085924 finetune.py:68] layer 1_gate @ epoch 2 new loss 8.940729458117858e-06 old loss 8.969021109805908e-06 BETTER
I0328 02:36:08.366357 2085215 finetune.py:68] layer 0_gate @ epoch 3 new loss 4.213114152662456e-06 old loss 4.223599717079196e-06 BETTER
I0328 02:36:09.702434 2086757 finetune.py:68] layer 2_gate @ epoch 0 new loss 1.7165346434921958e-05 old loss 1.7221420421265066e-05 BETTER
I0328 02:36:19.878906 2087681 finetune.py:68] layer 3_up @ epoch 4 new loss 2.5504958102828823e-05 old loss 2.5579865905456245e-05 BETTER
I0328 02:36:19.889217 2085924 finetune.py:68] layer 1_gate @ epoch 3 new loss 8.915131729736459e-06 old loss 8.940729458117858e-06 BETTER
I0328 02:36:39.624810 2086757 finetune.py:68] layer 2_gate @ epoch 1 new loss 1.7118036339525133e-05 old loss 1.7165346434921958e-05 BETTER
I0328 02:36:40.177872 2085215 finetune.py:68] layer 0_gate @ epoch 4 new loss 4.203911430522567e-06 old loss 4.213114152662456e-06 BETTER
I0328 02:36:49.885934 2085924 finetune.py:68] layer 1_gate @ epoch 4 new loss 8.896373401512392e-06 old loss 8.915131729736459e-06 BETTER
I0328 02:36:51.199631 2087681 finetune.py:45] layer 3_gate initial loss 3.158268009428866e-05
I0328 02:37:09.601088 2086757 finetune.py:68] layer 2_gate @ epoch 2 new loss 1.7075675714295357e-05 old loss 1.7118036339525133e-05 BETTER
I0328 02:37:19.493267 2087681 finetune.py:68] layer 3_gate @ epoch 0 new loss 3.1490486435359344e-05 old loss 3.158268009428866e-05 BETTER
I0328 02:37:35.889675 2085215 finetune.py:45] layer 0_down initial loss 6.392436262103729e-06
I0328 02:37:39.582012 2086757 finetune.py:68] layer 2_gate @ epoch 3 new loss 1.7038286387105472e-05 old loss 1.7075675714295357e-05 BETTER
I0328 02:37:46.586614 2085924 finetune.py:45] layer 1_down initial loss 1.2796324881492183e-05
I0328 02:37:48.879150 2087681 finetune.py:68] layer 3_gate @ epoch 1 new loss 3.1415518606081605e-05 old loss 3.1490486435359344e-05 BETTER
I0328 02:38:03.492052 2085215 finetune.py:68] layer 0_down @ epoch 0 new loss 6.386963832483161e-06 old loss 6.392436262103729e-06 BETTER
I0328 02:38:09.550990 2086757 finetune.py:68] layer 2_gate @ epoch 4 new loss 1.7003210814436898e-05 old loss 1.7038286387105472e-05 BETTER
I0328 02:38:12.860714 2085924 finetune.py:68] layer 1_down @ epoch 0 new loss 1.278739728149958e-05 old loss 1.2796324881492183e-05 BETTER
I0328 02:38:18.158195 2087681 finetune.py:68] layer 3_gate @ epoch 2 new loss 3.134781582048163e-05 old loss 3.1415518606081605e-05 BETTER
I0328 02:38:32.250838 2085215 finetune.py:68] layer 0_down @ epoch 1 new loss 6.3825732468103524e-06 old loss 6.386963832483161e-06 BETTER
I0328 02:38:40.151973 2085924 finetune.py:68] layer 1_down @ epoch 1 new loss 1.277969204238616e-05 old loss 1.278739728149958e-05 BETTER
I0328 02:38:47.630781 2087681 finetune.py:68] layer 3_gate @ epoch 3 new loss 3.128652315353975e-05 old loss 3.134781582048163e-05 BETTER
I0328 02:39:01.430426 2085215 finetune.py:68] layer 0_down @ epoch 2 new loss 6.378990292432718e-06 old loss 6.3825732468103524e-06 BETTER
I0328 02:39:06.979351 2086757 finetune.py:45] layer 2_down initial loss 2.554310231062118e-05
I0328 02:39:08.151492 2085924 finetune.py:68] layer 1_down @ epoch 2 new loss 1.2773009075317532e-05 old loss 1.277969204238616e-05 BETTER
I0328 02:39:17.239900 2087681 finetune.py:68] layer 3_gate @ epoch 4 new loss 3.1229610613081604e-05 old loss 3.128652315353975e-05 BETTER
I0328 02:39:30.679852 2085215 finetune.py:68] layer 0_down @ epoch 3 new loss 6.375945304171182e-06 old loss 6.378990292432718e-06 BETTER
I0328 02:39:33.462563 2086757 finetune.py:68] layer 2_down @ epoch 0 new loss 2.5531295250402763e-05 old loss 2.554310231062118e-05 BETTER
I0328 02:39:35.940753 2085924 finetune.py:68] layer 1_down @ epoch 3 new loss 1.2766638064931612e-05 old loss 1.2773009075317532e-05 BETTER
I0328 02:40:00.000624 2085215 finetune.py:68] layer 0_down @ epoch 4 new loss 6.37324592389632e-06 old loss 6.375945304171182e-06 BETTER
I0328 02:40:01.209274 2086757 finetune.py:68] layer 2_down @ epoch 1 new loss 2.5520697818137705e-05 old loss 2.5531295250402763e-05 BETTER
0_v proxy err 0.08119596540927887 tr(WHW.T) 60.88684844970703
bpp_loss 2.1023015622922685
0_q proxy err 8.934548532124609e-05 tr(WHW.T) 288005.8125
bpp_loss 2.8969120397232473
0_k proxy err 7.735100371064618e-05 tr(WHW.T) 100080.375
bpp_loss 3.408320155693218
0_o proxy err 0.011993040330708027 tr(WHW.T) 3121.8779296875
bpp_loss 2.181938296678709
0_up proxy err 0.024516209959983826 tr(WHW.T) 8924.7607421875
bpp_loss 2.471223724673369
0_gate proxy err 0.014144292101264 tr(WHW.T) 15779.3369140625
bpp_loss 2.578875479587753
0_down proxy err 0.019264860078692436 tr(WHW.T) 10861.783203125
bpp_loss 2.4624618910020217
I0328 02:40:03.767749 2085924 finetune.py:68] layer 1_down @ epoch 4 new loss 1.2761061952915043e-05 old loss 1.2766638064931612e-05 BETTER
1_v proxy err 0.03550063073635101 tr(WHW.T) 109.07096099853516
bpp_loss 2.2075055494497065
1_q proxy err 0.00012394243094604462 tr(WHW.T) 144731.03125
bpp_loss 3.1247830900538247
1_k proxy err 6.617397593799978e-05 tr(WHW.T) 75410.4921875
bpp_loss 3.7457746871223208
1_o proxy err 0.022613506764173508 tr(WHW.T) 1993.0496826171875
bpp_loss 2.2673542369739152
1_up proxy err 0.028150921687483788 tr(WHW.T) 8230.330078125
bpp_loss 2.485359728136765
1_gate proxy err 0.01697198674082756 tr(WHW.T) 13942.9931640625
bpp_loss 2.589476357945906
1_down proxy err 0.00051503861322999 tr(WHW.T) 13995.8203125
bpp_loss 2.4721583538588936
I0328 02:40:14.978686 2087681 finetune.py:45] layer 3_down initial loss 4.847820309805684e-05
I0328 02:40:29.841003 2086757 finetune.py:68] layer 2_down @ epoch 2 new loss 2.551166107878089e-05 old loss 2.5520697818137705e-05 BETTER
I0328 02:40:40.993816 2087681 finetune.py:68] layer 3_down @ epoch 0 new loss 4.847062882618047e-05 old loss 4.847820309805684e-05 BETTER
I0328 02:40:57.607614 2086757 finetune.py:68] layer 2_down @ epoch 3 new loss 2.550437238824088e-05 old loss 2.551166107878089e-05 BETTER
I0328 02:41:08.130158 2087681 finetune.py:68] layer 3_down @ epoch 1 new loss 4.8465408326592296e-05 old loss 4.847062882618047e-05 BETTER
I0328 02:41:18.925770 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 67.87044787406921s
I0328 02:41:22.772617 2100301 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:41:22.772729 2100301 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:41:22.772772 2100301 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:41:23.156430 2100301 config.py:54] PyTorch version 2.6.0 available.
W0328 02:41:23.375535 2100301 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 02:41:24.041341 2100301 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 02:41:24.045186 2083830 quantize_finetune_llama.py:209] layer 5 gpu 1
I0328 02:41:24.058875 2100301 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 02:41:25.391403 2086757 finetune.py:68] layer 2_down @ epoch 4 new loss 2.549799501139205e-05 old loss 2.550437238824088e-05 BETTER
2_v proxy err 0.053904663771390915 tr(WHW.T) 155.95950317382812
bpp_loss 2.1092207594774663
2_q proxy err 0.0009547056979499757 tr(WHW.T) 41467.15625
bpp_loss 3.0725356237089727
2_k proxy err 0.0004909076960757375 tr(WHW.T) 22591.1953125
bpp_loss 3.8435800901206676
2_o proxy err 0.02195098251104355 tr(WHW.T) 1975.2957763671875
bpp_loss 2.218054223252693
2_up proxy err 0.03322232514619827 tr(WHW.T) 7602.90087890625
bpp_loss 2.474435011856258
2_gate proxy err 0.01722436398267746 tr(WHW.T) 15097.8173828125
bpp_loss 2.61723851309424
2_down proxy err 0.030893467366695404 tr(WHW.T) 7790.5263671875
bpp_loss 2.47715224678229
I0328 02:41:35.398486 2087681 finetune.py:68] layer 3_down @ epoch 2 new loss 4.846107549383305e-05 old loss 4.8465408326592296e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 02:41:40.878447 2100301 finetune.py:45] layer 4_v initial loss 3.695111445267685e-05
W0328 02:41:40.878644 2100301 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 02:42:02.731746 2087681 finetune.py:68] layer 3_down @ epoch 3 new loss 4.845725925406441e-05 old loss 4.846107549383305e-05 BETTER
I0328 02:42:16.109998 2100301 finetune.py:68] layer 4_v @ epoch 0 new loss 1.0618132364470512e-05 old loss 3.695111445267685e-05 BETTER
I0328 02:42:30.185822 2087681 finetune.py:68] layer 3_down @ epoch 4 new loss 4.845403600484133e-05 old loss 4.845725925406441e-05 BETTER
3_v proxy err 0.041563089936971664 tr(WHW.T) 289.3331604003906
bpp_loss 2.205618715233868
3_q proxy err 0.0011967374011874199 tr(WHW.T) 47559.66015625
bpp_loss 3.1107212985516526
3_k proxy err 0.0006147968233563006 tr(WHW.T) 26144.50390625
bpp_loss 3.9153421856171917
3_o proxy err 0.026392962783575058 tr(WHW.T) 1857.7415771484375
bpp_loss 2.3159450065577403
3_up proxy err 0.032920338213443756 tr(WHW.T) 7535.11083984375
bpp_loss 2.457052669554417
3_gate proxy err 0.012472018599510193 tr(WHW.T) 20861.3125
bpp_loss 2.688552276563964
3_down proxy err 0.035460565239191055 tr(WHW.T) 7065.60546875
bpp_loss 2.4544328843143637
I0328 02:42:34.522141 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 63.544305086135864s
I0328 02:42:38.188171 2101133 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:42:38.188280 2101133 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:42:38.188319 2101133 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:42:38.555332 2101133 config.py:54] PyTorch version 2.6.0 available.
W0328 02:42:38.742040 2101133 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 02:42:39.348824 2101133 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 02:42:39.352630 2083830 quantize_finetune_llama.py:209] layer 6 gpu 2
I0328 02:42:39.366508 2101133 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 02:42:52.903590 2100301 finetune.py:68] layer 4_v @ epoch 1 new loss 8.513498869433533e-06 old loss 1.0618132364470512e-05 BETTER
I0328 02:42:56.230307 2101133 finetune.py:45] layer 5_v initial loss 3.7031462852610275e-05
W0328 02:42:56.230531 2101133 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 02:43:29.641008 2101133 finetune.py:68] layer 5_v @ epoch 0 new loss 1.4077207197260577e-05 old loss 3.7031462852610275e-05 BETTER
I0328 02:43:29.939827 2100301 finetune.py:68] layer 4_v @ epoch 2 new loss 7.923714292701334e-06 old loss 8.513498869433533e-06 BETTER
I0328 02:43:43.604889 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 63.800174713134766s
I0328 02:43:47.382476 2101890 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:43:47.382598 2101890 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:43:47.382639 2101890 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:43:47.762439 2101890 config.py:54] PyTorch version 2.6.0 available.
W0328 02:43:47.979479 2101890 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 02:43:48.659560 2101890 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 02:43:48.663446 2083830 quantize_finetune_llama.py:209] layer 7 gpu 3
I0328 02:43:48.677517 2101890 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 02:44:04.379874 2101133 finetune.py:68] layer 5_v @ epoch 1 new loss 1.2375540791254025e-05 old loss 1.4077207197260577e-05 BETTER
I0328 02:44:06.000347 2101890 finetune.py:45] layer 6_v initial loss 3.2518499210709706e-05
W0328 02:44:06.000574 2101890 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 02:44:07.056599 2100301 finetune.py:68] layer 4_v @ epoch 3 new loss 7.632648703292944e-06 old loss 7.923714292701334e-06 BETTER
I0328 02:44:39.443302 2101133 finetune.py:68] layer 5_v @ epoch 2 new loss 1.1774881386372726e-05 old loss 1.2375540791254025e-05 BETTER
I0328 02:44:39.747744 2101890 finetune.py:68] layer 6_v @ epoch 0 new loss 1.649597834330052e-05 old loss 3.2518499210709706e-05 BETTER
I0328 02:44:44.430645 2100301 finetune.py:68] layer 4_v @ epoch 4 new loss 7.445537448802497e-06 old loss 7.632648703292944e-06 BETTER
I0328 02:44:53.403449 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 64.19232511520386s
I0328 02:44:57.241681 2102647 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 02:44:57.241785 2102647 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 02:44:57.241828 2102647 utils.py:162] NumExpr defaulting to 16 threads.
I0328 02:44:57.606938 2102647 config.py:54] PyTorch version 2.6.0 available.
W0328 02:44:57.835268 2102647 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 02:44:58.458664 2102647 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 02:44:58.463047 2083830 quantize_finetune_llama.py:209] layer 8 gpu 0
I0328 02:44:58.478076 2102647 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 02:45:03.663999 2100301 finetune.py:45] layer 4_q initial loss 8.991915819933638e-06
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 02:45:14.634457 2101890 finetune.py:68] layer 6_v @ epoch 1 new loss 1.51614231072017e-05 old loss 1.649597834330052e-05 BETTER
I0328 02:45:14.781177 2101133 finetune.py:68] layer 5_v @ epoch 3 new loss 1.1440298294473905e-05 old loss 1.1774881386372726e-05 BETTER
I0328 02:45:16.315907 2102647 finetune.py:45] layer 7_v initial loss 3.172752258251421e-05
W0328 02:45:16.316099 2102647 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 02:45:39.257532 2100301 finetune.py:68] layer 4_q @ epoch 0 new loss 8.700289072294254e-06 old loss 8.991915819933638e-06 BETTER
I0328 02:45:49.519678 2102647 finetune.py:68] layer 7_v @ epoch 0 new loss 1.9977382180513814e-05 old loss 3.172752258251421e-05 BETTER
I0328 02:45:49.956384 2101890 finetune.py:68] layer 6_v @ epoch 2 new loss 1.4619824469264131e-05 old loss 1.51614231072017e-05 BETTER
I0328 02:45:50.245158 2101133 finetune.py:68] layer 5_v @ epoch 4 new loss 1.1216628081456292e-05 old loss 1.1440298294473905e-05 BETTER
I0328 02:46:09.391659 2101133 finetune.py:45] layer 5_q initial loss 1.3588407455245033e-05
I0328 02:46:15.882271 2100301 finetune.py:68] layer 4_q @ epoch 1 new loss 8.542007890355308e-06 old loss 8.700289072294254e-06 BETTER
I0328 02:46:23.835947 2102647 finetune.py:68] layer 7_v @ epoch 1 new loss 1.8804139472194947e-05 old loss 1.9977382180513814e-05 BETTER
I0328 02:46:25.340347 2101890 finetune.py:68] layer 6_v @ epoch 3 new loss 1.4286623809312005e-05 old loss 1.4619824469264131e-05 BETTER
I0328 02:46:43.263589 2101133 finetune.py:68] layer 5_q @ epoch 0 new loss 1.3248958566691726e-05 old loss 1.3588407455245033e-05 BETTER
I0328 02:46:52.991702 2100301 finetune.py:68] layer 4_q @ epoch 2 new loss 8.418499419349246e-06 old loss 8.542007890355308e-06 BETTER
I0328 02:46:58.516418 2102647 finetune.py:68] layer 7_v @ epoch 2 new loss 1.8236298274132423e-05 old loss 1.8804139472194947e-05 BETTER
I0328 02:47:00.727960 2101890 finetune.py:68] layer 6_v @ epoch 4 new loss 1.4041877875570208e-05 old loss 1.4286623809312005e-05 BETTER
I0328 02:47:17.796263 2101133 finetune.py:68] layer 5_q @ epoch 1 new loss 1.3045793821220286e-05 old loss 1.3248958566691726e-05 BETTER
I0328 02:47:20.269172 2101890 finetune.py:45] layer 6_q initial loss 1.6890848201001063e-05
I0328 02:47:30.181274 2100301 finetune.py:68] layer 4_q @ epoch 3 new loss 8.317153515235987e-06 old loss 8.418499419349246e-06 BETTER
I0328 02:47:33.268987 2102647 finetune.py:68] layer 7_v @ epoch 3 new loss 1.7867809219751507e-05 old loss 1.8236298274132423e-05 BETTER
I0328 02:47:52.496043 2101133 finetune.py:68] layer 5_q @ epoch 2 new loss 1.2883460840384942e-05 old loss 1.3045793821220286e-05 BETTER
I0328 02:47:54.011394 2101890 finetune.py:68] layer 6_q @ epoch 0 new loss 1.654260813666042e-05 old loss 1.6890848201001063e-05 BETTER
I0328 02:48:07.434943 2100301 finetune.py:68] layer 4_q @ epoch 4 new loss 8.228969818446785e-06 old loss 8.317153515235987e-06 BETTER
I0328 02:48:08.123764 2102647 finetune.py:68] layer 7_v @ epoch 4 new loss 1.758916005201172e-05 old loss 1.7867809219751507e-05 BETTER
I0328 02:48:25.464152 2100301 finetune.py:45] layer 4_k initial loss 8.769300620770082e-06
I0328 02:48:27.669848 2101133 finetune.py:68] layer 5_q @ epoch 3 new loss 1.2749259440170135e-05 old loss 1.2883460840384942e-05 BETTER
I0328 02:48:28.372105 2102647 finetune.py:45] layer 7_q initial loss 2.1808946257806383e-05
I0328 02:48:29.255146 2101890 finetune.py:68] layer 6_q @ epoch 1 new loss 1.6305573808494955e-05 old loss 1.654260813666042e-05 BETTER
I0328 02:49:00.836978 2100301 finetune.py:68] layer 4_k @ epoch 0 new loss 8.674921446072403e-06 old loss 8.769300620770082e-06 BETTER
I0328 02:49:01.850845 2102647 finetune.py:68] layer 7_q @ epoch 0 new loss 2.1351781470002607e-05 old loss 2.1808946257806383e-05 BETTER
I0328 02:49:02.721328 2101133 finetune.py:68] layer 5_q @ epoch 4 new loss 1.2633010555873625e-05 old loss 1.2749259440170135e-05 BETTER
I0328 02:49:04.336859 2101890 finetune.py:68] layer 6_q @ epoch 2 new loss 1.6113437595777214e-05 old loss 1.6305573808494955e-05 BETTER
I0328 02:49:20.653989 2101133 finetune.py:45] layer 5_k initial loss 1.330626673734514e-05
I0328 02:49:36.143010 2102647 finetune.py:68] layer 7_q @ epoch 1 new loss 2.1062936866655946e-05 old loss 2.1351781470002607e-05 BETTER
I0328 02:49:37.706027 2100301 finetune.py:68] layer 4_k @ epoch 1 new loss 8.605102266301401e-06 old loss 8.674921446072403e-06 BETTER
I0328 02:49:39.343475 2101890 finetune.py:68] layer 6_q @ epoch 3 new loss 1.5950316083035432e-05 old loss 1.6113437595777214e-05 BETTER
I0328 02:49:54.801344 2101133 finetune.py:68] layer 5_k @ epoch 0 new loss 1.3158650290279184e-05 old loss 1.330626673734514e-05 BETTER
I0328 02:50:10.806029 2102647 finetune.py:68] layer 7_q @ epoch 2 new loss 2.0818539269384928e-05 old loss 2.1062936866655946e-05 BETTER
I0328 02:50:14.633918 2101890 finetune.py:68] layer 6_q @ epoch 4 new loss 1.5803821952431463e-05 old loss 1.5950316083035432e-05 BETTER
I0328 02:50:14.719005 2100301 finetune.py:68] layer 4_k @ epoch 2 new loss 8.544949196220841e-06 old loss 8.605102266301401e-06 BETTER
I0328 02:50:29.193078 2101133 finetune.py:68] layer 5_k @ epoch 1 new loss 1.3060956007393543e-05 old loss 1.3158650290279184e-05 BETTER
I0328 02:50:32.643571 2101890 finetune.py:45] layer 6_k initial loss 1.671581412665546e-05
I0328 02:50:45.238746 2102647 finetune.py:68] layer 7_q @ epoch 3 new loss 2.0626659534173086e-05 old loss 2.0818539269384928e-05 BETTER
I0328 02:50:51.441323 2100301 finetune.py:68] layer 4_k @ epoch 3 new loss 8.49185380502604e-06 old loss 8.544949196220841e-06 BETTER
I0328 02:51:03.760957 2101133 finetune.py:68] layer 5_k @ epoch 2 new loss 1.2977428013982717e-05 old loss 1.3060956007393543e-05 BETTER
I0328 02:51:06.351078 2101890 finetune.py:68] layer 6_k @ epoch 0 new loss 1.6511954527231865e-05 old loss 1.671581412665546e-05 BETTER
I0328 02:51:19.540350 2102647 finetune.py:68] layer 7_q @ epoch 4 new loss 2.044434404524509e-05 old loss 2.0626659534173086e-05 BETTER
I0328 02:51:28.436165 2100301 finetune.py:68] layer 4_k @ epoch 4 new loss 8.44165242597228e-06 old loss 8.49185380502604e-06 BETTER
I0328 02:51:37.468045 2102647 finetune.py:45] layer 7_k initial loss 2.1578793166554533e-05
I0328 02:51:38.619550 2101133 finetune.py:68] layer 5_k @ epoch 3 new loss 1.2904130926472135e-05 old loss 1.2977428013982717e-05 BETTER
I0328 02:51:41.107722 2101890 finetune.py:68] layer 6_k @ epoch 1 new loss 1.6399204469053075e-05 old loss 1.6511954527231865e-05 BETTER
I0328 02:51:47.934251 2100301 finetune.py:45] layer 4_o initial loss 2.043757376668509e-05
I0328 02:52:10.501800 2102647 finetune.py:68] layer 7_k @ epoch 0 new loss 2.1356403522077017e-05 old loss 2.1578793166554533e-05 BETTER
I0328 02:52:13.335988 2101133 finetune.py:68] layer 5_k @ epoch 4 new loss 1.2835186680604238e-05 old loss 1.2904130926472135e-05 BETTER
I0328 02:52:16.025839 2101890 finetune.py:68] layer 6_k @ epoch 2 new loss 1.6293524822685868e-05 old loss 1.6399204469053075e-05 BETTER
I0328 02:52:22.972769 2100301 finetune.py:68] layer 4_o @ epoch 0 new loss 1.9033665012102574e-05 old loss 2.043757376668509e-05 BETTER
I0328 02:52:32.853133 2101133 finetune.py:45] layer 5_o initial loss 2.739956835284829e-05
I0328 02:52:44.656082 2102647 finetune.py:68] layer 7_k @ epoch 1 new loss 2.1214562366367318e-05 old loss 2.1356403522077017e-05 BETTER
I0328 02:52:50.941642 2101890 finetune.py:68] layer 6_k @ epoch 3 new loss 1.620082912268117e-05 old loss 1.6293524822685868e-05 BETTER
I0328 02:52:59.030366 2100301 finetune.py:68] layer 4_o @ epoch 1 new loss 1.8645789168658666e-05 old loss 1.9033665012102574e-05 BETTER
I0328 02:53:05.712144 2101133 finetune.py:68] layer 5_o @ epoch 0 new loss 2.5960072889574803e-05 old loss 2.739956835284829e-05 BETTER
I0328 02:53:18.768420 2102647 finetune.py:68] layer 7_k @ epoch 2 new loss 2.108753142238129e-05 old loss 2.1214562366367318e-05 BETTER
I0328 02:53:25.839301 2101890 finetune.py:68] layer 6_k @ epoch 4 new loss 1.612005326023791e-05 old loss 1.620082912268117e-05 BETTER
I0328 02:53:35.000647 2100301 finetune.py:68] layer 4_o @ epoch 2 new loss 1.8400800399831496e-05 old loss 1.8645789168658666e-05 BETTER
I0328 02:53:39.571167 2101133 finetune.py:68] layer 5_o @ epoch 1 new loss 2.5539107809890993e-05 old loss 2.5960072889574803e-05 BETTER
I0328 02:53:45.462979 2101890 finetune.py:45] layer 6_o initial loss 3.709709562826902e-05
I0328 02:53:52.817392 2102647 finetune.py:68] layer 7_k @ epoch 3 new loss 2.097472497553099e-05 old loss 2.108753142238129e-05 BETTER
I0328 02:54:11.047528 2100301 finetune.py:68] layer 4_o @ epoch 3 new loss 1.821788464440033e-05 old loss 1.8400800399831496e-05 BETTER
I0328 02:54:13.649380 2101133 finetune.py:68] layer 5_o @ epoch 2 new loss 2.526334355934523e-05 old loss 2.5539107809890993e-05 BETTER
I0328 02:54:18.471584 2101890 finetune.py:68] layer 6_o @ epoch 0 new loss 3.5564120480557904e-05 old loss 3.709709562826902e-05 BETTER
I0328 02:54:26.840382 2102647 finetune.py:68] layer 7_k @ epoch 4 new loss 2.0869503714493476e-05 old loss 2.097472497553099e-05 BETTER
I0328 02:54:46.524098 2102647 finetune.py:45] layer 7_o initial loss 4.633871867554262e-05
I0328 02:54:47.157181 2100301 finetune.py:68] layer 4_o @ epoch 4 new loss 1.806794352887664e-05 old loss 1.821788464440033e-05 BETTER
I0328 02:54:47.836442 2101133 finetune.py:68] layer 5_o @ epoch 3 new loss 2.505238080630079e-05 old loss 2.526334355934523e-05 BETTER
I0328 02:54:52.695616 2101890 finetune.py:68] layer 6_o @ epoch 1 new loss 3.5057881177635863e-05 old loss 3.5564120480557904e-05 BETTER
I0328 02:55:18.439154 2100301 finetune.py:45] layer 4_up initial loss 4.1255403630202636e-05
I0328 02:55:18.877652 2102647 finetune.py:68] layer 7_o @ epoch 0 new loss 4.4851683924207464e-05 old loss 4.633871867554262e-05 BETTER
I0328 02:55:22.060827 2101133 finetune.py:68] layer 5_o @ epoch 4 new loss 2.4880066121113487e-05 old loss 2.505238080630079e-05 BETTER
I0328 02:55:27.133874 2101890 finetune.py:68] layer 6_o @ epoch 2 new loss 3.469868897809647e-05 old loss 3.5057881177635863e-05 BETTER
I0328 02:55:50.677672 2100301 finetune.py:68] layer 4_up @ epoch 0 new loss 4.089180947630666e-05 old loss 4.1255403630202636e-05 BETTER
I0328 02:55:52.450119 2102647 finetune.py:68] layer 7_o @ epoch 1 new loss 4.423446807777509e-05 old loss 4.4851683924207464e-05 BETTER
I0328 02:55:53.564424 2101133 finetune.py:45] layer 5_up initial loss 5.853546463185921e-05
I0328 02:56:01.341072 2101890 finetune.py:68] layer 6_o @ epoch 3 new loss 3.441136868786998e-05 old loss 3.469868897809647e-05 BETTER
I0328 02:56:24.311393 2100301 finetune.py:68] layer 4_up @ epoch 1 new loss 4.064696258865297e-05 old loss 4.089180947630666e-05 BETTER
I0328 02:56:24.442572 2101133 finetune.py:68] layer 5_up @ epoch 0 new loss 5.7926725276047364e-05 old loss 5.853546463185921e-05 BETTER
I0328 02:56:26.143542 2102647 finetune.py:68] layer 7_o @ epoch 2 new loss 4.377562800073065e-05 old loss 4.423446807777509e-05 BETTER
I0328 02:56:35.575422 2101890 finetune.py:68] layer 6_o @ epoch 4 new loss 3.4162545489380136e-05 old loss 3.441136868786998e-05 BETTER
I0328 02:56:56.417507 2101133 finetune.py:68] layer 5_up @ epoch 1 new loss 5.751967546530068e-05 old loss 5.7926725276047364e-05 BETTER
I0328 02:56:58.171694 2100301 finetune.py:68] layer 4_up @ epoch 2 new loss 4.0440831071464345e-05 old loss 4.064696258865297e-05 BETTER
I0328 02:56:59.789018 2102647 finetune.py:68] layer 7_o @ epoch 3 new loss 4.339448787504807e-05 old loss 4.377562800073065e-05 BETTER
I0328 02:57:07.054281 2101890 finetune.py:45] layer 6_up initial loss 7.680593262193725e-05
I0328 02:57:28.781598 2101133 finetune.py:68] layer 5_up @ epoch 2 new loss 5.716943633160554e-05 old loss 5.751967546530068e-05 BETTER
I0328 02:57:32.257391 2100301 finetune.py:68] layer 4_up @ epoch 3 new loss 4.0252507460536435e-05 old loss 4.0440831071464345e-05 BETTER
I0328 02:57:33.324884 2102647 finetune.py:68] layer 7_o @ epoch 4 new loss 4.3069147068308666e-05 old loss 4.339448787504807e-05 BETTER
I0328 02:57:38.150276 2101890 finetune.py:68] layer 6_up @ epoch 0 new loss 7.591340545332059e-05 old loss 7.680593262193725e-05 BETTER
I0328 02:58:00.972418 2101133 finetune.py:68] layer 5_up @ epoch 3 new loss 5.6854889407986775e-05 old loss 5.716943633160554e-05 BETTER
I0328 02:58:05.023390 2102647 finetune.py:45] layer 7_up initial loss 8.91157251317054e-05
I0328 02:58:06.278960 2100301 finetune.py:68] layer 4_up @ epoch 4 new loss 4.00795615860261e-05 old loss 4.0252507460536435e-05 BETTER
I0328 02:58:10.379730 2101890 finetune.py:68] layer 6_up @ epoch 1 new loss 7.529590948252007e-05 old loss 7.591340545332059e-05 BETTER
I0328 02:58:33.300026 2101133 finetune.py:68] layer 5_up @ epoch 4 new loss 5.656283610733226e-05 old loss 5.6854889407986775e-05 BETTER
I0328 02:58:35.377803 2102647 finetune.py:68] layer 7_up @ epoch 0 new loss 8.806646656012163e-05 old loss 8.91157251317054e-05 BETTER
I0328 02:58:37.546481 2100301 finetune.py:45] layer 4_gate initial loss 4.876167804468423e-05
I0328 02:58:42.543577 2101890 finetune.py:68] layer 6_up @ epoch 2 new loss 7.475847087334841e-05 old loss 7.529590948252007e-05 BETTER
I0328 02:59:04.650054 2101133 finetune.py:45] layer 5_gate initial loss 6.90021479385905e-05
I0328 02:59:06.793476 2102647 finetune.py:68] layer 7_up @ epoch 1 new loss 8.733960567042232e-05 old loss 8.806646656012163e-05 BETTER
I0328 02:59:07.697433 2100301 finetune.py:68] layer 4_gate @ epoch 0 new loss 4.854059443459846e-05 old loss 4.876167804468423e-05 BETTER
I0328 02:59:14.884771 2101890 finetune.py:68] layer 6_up @ epoch 3 new loss 7.427246600855142e-05 old loss 7.475847087334841e-05 BETTER
I0328 02:59:33.209126 2101133 finetune.py:68] layer 5_gate @ epoch 0 new loss 6.863531598355621e-05 old loss 6.90021479385905e-05 BETTER
I0328 02:59:38.269849 2102647 finetune.py:68] layer 7_up @ epoch 2 new loss 8.669865201227367e-05 old loss 8.733960567042232e-05 BETTER
I0328 02:59:39.030923 2100301 finetune.py:68] layer 4_gate @ epoch 1 new loss 4.836032530874945e-05 old loss 4.854059443459846e-05 BETTER
I0328 02:59:47.208647 2101890 finetune.py:68] layer 6_up @ epoch 4 new loss 7.381883187917992e-05 old loss 7.427246600855142e-05 BETTER
I0328 03:00:02.916329 2101133 finetune.py:68] layer 5_gate @ epoch 1 new loss 6.833922088844702e-05 old loss 6.863531598355621e-05 BETTER
I0328 03:00:09.713840 2102647 finetune.py:68] layer 7_up @ epoch 3 new loss 8.611561497673392e-05 old loss 8.669865201227367e-05 BETTER
I0328 03:00:10.375777 2100301 finetune.py:68] layer 4_gate @ epoch 2 new loss 4.8195604904321954e-05 old loss 4.836032530874945e-05 BETTER
I0328 03:00:19.054826 2101890 finetune.py:45] layer 6_gate initial loss 8.84791516000405e-05
I0328 03:00:32.698231 2101133 finetune.py:68] layer 5_gate @ epoch 2 new loss 6.806811143178493e-05 old loss 6.833922088844702e-05 BETTER
I0328 03:00:41.207717 2102647 finetune.py:68] layer 7_up @ epoch 4 new loss 8.557259570807219e-05 old loss 8.611561497673392e-05 BETTER
I0328 03:00:41.965744 2100301 finetune.py:68] layer 4_gate @ epoch 3 new loss 4.804213676834479e-05 old loss 4.8195604904321954e-05 BETTER
I0328 03:00:47.624775 2101890 finetune.py:68] layer 6_gate @ epoch 0 new loss 8.793062443146482e-05 old loss 8.84791516000405e-05 BETTER
I0328 03:01:02.543355 2101133 finetune.py:68] layer 5_gate @ epoch 3 new loss 6.782162381568924e-05 old loss 6.806811143178493e-05 BETTER
I0328 03:01:12.763485 2102647 finetune.py:45] layer 7_gate initial loss 0.00010324314644094557
I0328 03:01:13.549227 2100301 finetune.py:68] layer 4_gate @ epoch 4 new loss 4.789750528289005e-05 old loss 4.804213676834479e-05 BETTER
I0328 03:01:17.261945 2101890 finetune.py:68] layer 6_gate @ epoch 1 new loss 8.749166590860114e-05 old loss 8.793062443146482e-05 BETTER
I0328 03:01:32.401310 2101133 finetune.py:68] layer 5_gate @ epoch 4 new loss 6.758312520105392e-05 old loss 6.782162381568924e-05 BETTER
I0328 03:01:40.880652 2102647 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.0001025994133669883 old loss 0.00010324314644094557 BETTER
I0328 03:01:47.237858 2101890 finetune.py:68] layer 6_gate @ epoch 2 new loss 8.709637040738016e-05 old loss 8.749166590860114e-05 BETTER
I0328 03:02:08.590012 2100301 finetune.py:45] layer 4_down initial loss 7.788205402903259e-05
I0328 03:02:10.237341 2102647 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.0001020928830257617 old loss 0.0001025994133669883 BETTER
I0328 03:02:17.237863 2101890 finetune.py:68] layer 6_gate @ epoch 3 new loss 8.672690455568954e-05 old loss 8.709637040738016e-05 BETTER
I0328 03:02:29.501836 2101133 finetune.py:45] layer 5_down initial loss 0.00010879701585508883
I0328 03:02:35.924021 2100301 finetune.py:68] layer 4_down @ epoch 0 new loss 7.787035428918898e-05 old loss 7.788205402903259e-05 BETTER
I0328 03:02:39.488637 2102647 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.00010163503611693159 old loss 0.0001020928830257617 BETTER
I0328 03:02:47.142444 2101890 finetune.py:68] layer 6_gate @ epoch 4 new loss 8.637843711767346e-05 old loss 8.672690455568954e-05 BETTER
I0328 03:02:55.638721 2101133 finetune.py:68] layer 5_down @ epoch 0 new loss 0.00010878387547563761 old loss 0.00010879701585508883 BETTER
I0328 03:03:04.625009 2100301 finetune.py:68] layer 4_down @ epoch 1 new loss 7.786123751429841e-05 old loss 7.787035428918898e-05 BETTER
I0328 03:03:08.792666 2102647 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.00010120440856553614 old loss 0.00010163503611693159 BETTER
I0328 03:03:22.914380 2101133 finetune.py:68] layer 5_down @ epoch 1 new loss 0.00010877296153921634 old loss 0.00010878387547563761 BETTER
I0328 03:03:33.549805 2100301 finetune.py:68] layer 4_down @ epoch 2 new loss 7.785397610859945e-05 old loss 7.786123751429841e-05 BETTER
I0328 03:03:38.000696 2102647 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.00010080197534989566 old loss 0.00010120440856553614 BETTER
I0328 03:03:43.688055 2101890 finetune.py:45] layer 6_down initial loss 0.00013633407070301473
I0328 03:03:50.474451 2101133 finetune.py:68] layer 5_down @ epoch 2 new loss 0.00010876347369048744 old loss 0.00010877296153921634 BETTER
I0328 03:04:02.715910 2100301 finetune.py:68] layer 4_down @ epoch 3 new loss 7.784742774674669e-05 old loss 7.785397610859945e-05 BETTER
I0328 03:04:09.986201 2101890 finetune.py:68] layer 6_down @ epoch 0 new loss 0.00013632238551508635 old loss 0.00013633407070301473 BETTER
I0328 03:04:18.166867 2101133 finetune.py:68] layer 5_down @ epoch 3 new loss 0.00010875490261241794 old loss 0.00010876347369048744 BETTER
I0328 03:04:31.904252 2100301 finetune.py:68] layer 4_down @ epoch 4 new loss 7.784213084960356e-05 old loss 7.784742774674669e-05 BETTER
4_v proxy err 0.037967219948768616 tr(WHW.T) 285.30712890625
bpp_loss 2.2494104863435496
4_q proxy err 0.0010014300933107734 tr(WHW.T) 50128.9375
bpp_loss 3.0764342166949064
4_k proxy err 0.00048405895358882844 tr(WHW.T) 29277.3046875
bpp_loss 3.899239263642812
4_o proxy err 0.03031439520418644 tr(WHW.T) 1304.1240234375
bpp_loss 2.3239653643104248
4_up proxy err 0.033356811851263046 tr(WHW.T) 7376.02392578125
bpp_loss 2.4295182349160314
4_gate proxy err 0.009070989675819874 tr(WHW.T) 28986.310546875
bpp_loss 2.755420239609001
4_down proxy err 0.03810384124517441 tr(WHW.T) 6451.43115234375
bpp_loss 2.4310196907193
I0328 03:04:35.400264 2102647 finetune.py:45] layer 7_down initial loss 0.0001552467729197815
I0328 03:04:37.623101 2101890 finetune.py:68] layer 6_down @ epoch 1 new loss 0.0001363116316497326 old loss 0.00013632238551508635 BETTER
I0328 03:04:46.867520 2101133 finetune.py:68] layer 5_down @ epoch 4 new loss 0.00010874768486246467 old loss 0.00010875490261241794 BETTER
5_v proxy err 0.05282389745116234 tr(WHW.T) 208.81988525390625
bpp_loss 2.136990925617283
5_q proxy err 0.0014442632673308253 tr(WHW.T) 36008.87109375
bpp_loss 3.0528146064607427
5_k proxy err 0.0006381860584951937 tr(WHW.T) 22979.525390625
bpp_loss 3.8723730288329534
5_o proxy err 0.03252197057008743 tr(WHW.T) 1064.9248046875
bpp_loss 2.2754522605100647
5_up proxy err 0.0321197584271431 tr(WHW.T) 7653.98681640625
bpp_loss 2.434279500473557
5_gate proxy err 0.008672443218529224 tr(WHW.T) 30269.673828125
bpp_loss 2.7577895194824253
5_down proxy err 0.036746859550476074 tr(WHW.T) 6469.93115234375
bpp_loss 2.436118019744754
I0328 03:05:02.198590 2102647 finetune.py:68] layer 7_down @ epoch 0 new loss 0.0001552291796542704 old loss 0.0001552467729197815 BETTER
I0328 03:05:06.382096 2101890 finetune.py:68] layer 6_down @ epoch 2 new loss 0.00013630129978992045 old loss 0.0001363116316497326 BETTER
I0328 03:05:29.232891 2102647 finetune.py:68] layer 7_down @ epoch 1 new loss 0.00015521323075518012 old loss 0.0001552291796542704 BETTER
I0328 03:05:34.542126 2101890 finetune.py:68] layer 6_down @ epoch 3 new loss 0.00013629264140035957 old loss 0.00013630129978992045 BETTER
I0328 03:05:56.408823 2102647 finetune.py:68] layer 7_down @ epoch 2 new loss 0.0001551996247144416 old loss 0.00015521323075518012 BETTER
I0328 03:05:57.928881 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 66.18835711479187s
I0328 03:06:01.934646 2115320 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:06:01.934754 2115320 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:06:01.934798 2115320 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:06:02.330533 2115320 config.py:54] PyTorch version 2.6.0 available.
I0328 03:06:02.372356 2101890 finetune.py:68] layer 6_down @ epoch 4 new loss 0.00013628391025122255 old loss 0.00013629264140035957 BETTER
W0328 03:06:02.542442 2115320 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:06:03.117247 2115320 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:06:03.121156 2083830 quantize_finetune_llama.py:209] layer 9 gpu 1
I0328 03:06:03.135034 2115320 data_utils.py:336] using 256 training seqs, 128 validation seqs
6_v proxy err 0.044806789606809616 tr(WHW.T) 253.63377380371094
bpp_loss 2.182654587260913
6_q proxy err 0.0015074649127200246 tr(WHW.T) 35669.4375
bpp_loss 3.096887736406643
6_k proxy err 0.0005797623889520764 tr(WHW.T) 26136.654296875
bpp_loss 3.9408473677467555
6_o proxy err 0.037705257534980774 tr(WHW.T) 1018.563720703125
bpp_loss 2.304818849777803
6_up proxy err 0.030291210860013962 tr(WHW.T) 7915.3779296875
bpp_loss 2.433443372171106
6_gate proxy err 0.007204696536064148 tr(WHW.T) 35559.7578125
bpp_loss 2.762075497303158
6_down proxy err 0.03529488295316696 tr(WHW.T) 6537.56787109375
bpp_loss 2.4367472501637946
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:06:20.100908 2115320 finetune.py:45] layer 8_v initial loss 3.3161100873257965e-05
W0328 03:06:20.101165 2115320 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:06:23.660175 2102647 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00015518764848820865 old loss 0.0001551996247144416 BETTER
I0328 03:06:50.955737 2102647 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0001551768509671092 old loss 0.00015518764848820865 BETTER
7_v proxy err 0.03711007907986641 tr(WHW.T) 309.4270935058594
bpp_loss 2.1808424551272765
7_q proxy err 0.0015360291581600904 tr(WHW.T) 35160.80078125
bpp_loss 3.0266187083907425
7_k proxy err 0.0005769011913798749 tr(WHW.T) 26809.849609375
bpp_loss 3.9619686943478882
7_o proxy err 0.0331055112183094 tr(WHW.T) 969.447021484375
bpp_loss 2.3158684634254314
7_up proxy err 0.027470696717500687 tr(WHW.T) 8617.0869140625
bpp_loss 2.4460508791463718
7_gate proxy err 0.007206449750810862 tr(WHW.T) 34819.43359375
bpp_loss 2.7344354480904127
7_down proxy err 0.03534949943423271 tr(WHW.T) 6578.34521484375
bpp_loss 2.450342466389494
I0328 03:06:55.231347 2115320 finetune.py:68] layer 8_v @ epoch 0 new loss 2.2909962353878655e-05 old loss 3.3161100873257965e-05 BETTER
I0328 03:07:10.235518 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 62.91358661651611s
I0328 03:07:13.823749 2116106 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:07:13.823860 2116106 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:07:13.823898 2116106 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:07:14.155724 2116106 config.py:54] PyTorch version 2.6.0 available.
W0328 03:07:14.344671 2116106 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:07:14.917682 2116106 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:07:14.921507 2083830 quantize_finetune_llama.py:209] layer 10 gpu 2
I0328 03:07:14.936810 2116106 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:07:31.656830 2116106 finetune.py:45] layer 9_v initial loss 3.6485205782810226e-05
W0328 03:07:31.657036 2116106 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:07:32.004840 2115320 finetune.py:68] layer 8_v @ epoch 1 new loss 2.1666628526872955e-05 old loss 2.2909962353878655e-05 BETTER
I0328 03:08:05.022529 2116106 finetune.py:68] layer 9_v @ epoch 0 new loss 2.347385998291429e-05 old loss 3.6485205782810226e-05 BETTER
I0328 03:08:09.109709 2115320 finetune.py:68] layer 8_v @ epoch 2 new loss 2.103638689732179e-05 old loss 2.1666628526872955e-05 BETTER
I0328 03:08:18.379055 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 62.998051404953s
I0328 03:08:22.163304 2116860 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:08:22.163414 2116860 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:08:22.163458 2116860 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:08:22.512826 2116860 config.py:54] PyTorch version 2.6.0 available.
W0328 03:08:22.722691 2116860 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:08:23.347739 2116860 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:08:23.351551 2083830 quantize_finetune_llama.py:209] layer 11 gpu 3
I0328 03:08:23.365428 2116860 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:08:39.617554 2116106 finetune.py:68] layer 9_v @ epoch 1 new loss 2.2064379663788714e-05 old loss 2.347385998291429e-05 BETTER
I0328 03:08:40.757709 2116860 finetune.py:45] layer 10_v initial loss 4.326664929976687e-05
W0328 03:08:40.757936 2116860 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:08:46.070684 2115320 finetune.py:68] layer 8_v @ epoch 3 new loss 2.0605368263204582e-05 old loss 2.103638689732179e-05 BETTER
I0328 03:09:14.356126 2116860 finetune.py:68] layer 10_v @ epoch 0 new loss 3.118481981800869e-05 old loss 4.326664929976687e-05 BETTER
I0328 03:09:14.558823 2116106 finetune.py:68] layer 9_v @ epoch 2 new loss 2.1360194295994006e-05 old loss 2.2064379663788714e-05 BETTER
I0328 03:09:23.024609 2115320 finetune.py:68] layer 8_v @ epoch 4 new loss 2.027910522883758e-05 old loss 2.0605368263204582e-05 BETTER
I0328 03:09:27.701915 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 63.86700177192688s
I0328 03:09:31.510672 2117651 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:09:31.510771 2117651 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:09:31.510809 2117651 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:09:31.863362 2117651 config.py:54] PyTorch version 2.6.0 available.
W0328 03:09:32.051566 2117651 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:09:32.652033 2117651 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:09:32.658391 2083830 quantize_finetune_llama.py:209] layer 12 gpu 0
I0328 03:09:32.672576 2117651 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 03:09:42.066608 2115320 finetune.py:45] layer 8_q initial loss 2.5007948352140374e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:09:49.147131 2116860 finetune.py:68] layer 10_v @ epoch 1 new loss 2.9653961973963305e-05 old loss 3.118481981800869e-05 BETTER
I0328 03:09:49.711768 2116106 finetune.py:68] layer 9_v @ epoch 3 new loss 2.089972622343339e-05 old loss 2.1360194295994006e-05 BETTER
I0328 03:09:50.422981 2117651 finetune.py:45] layer 11_v initial loss 3.8140598917379975e-05
W0328 03:09:50.423202 2117651 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:10:17.396352 2115320 finetune.py:68] layer 8_q @ epoch 0 new loss 2.441921787976753e-05 old loss 2.5007948352140374e-05 BETTER
I0328 03:10:23.645514 2117651 finetune.py:68] layer 11_v @ epoch 0 new loss 2.7936870537814684e-05 old loss 3.8140598917379975e-05 BETTER
I0328 03:10:24.387239 2116860 finetune.py:68] layer 10_v @ epoch 2 new loss 2.884488094423432e-05 old loss 2.9653961973963305e-05 BETTER
I0328 03:10:25.055306 2116106 finetune.py:68] layer 9_v @ epoch 4 new loss 2.056034645647742e-05 old loss 2.089972622343339e-05 BETTER
I0328 03:10:44.674289 2116106 finetune.py:45] layer 9_q initial loss 2.570675133028999e-05
I0328 03:10:53.982924 2115320 finetune.py:68] layer 8_q @ epoch 1 new loss 2.4080150978988968e-05 old loss 2.441921787976753e-05 BETTER
I0328 03:10:58.087047 2117651 finetune.py:68] layer 11_v @ epoch 1 new loss 2.6515830541029572e-05 old loss 2.7936870537814684e-05 BETTER
I0328 03:10:59.769776 2116860 finetune.py:68] layer 10_v @ epoch 3 new loss 2.827207208611071e-05 old loss 2.884488094423432e-05 BETTER
I0328 03:11:18.144899 2116106 finetune.py:68] layer 9_q @ epoch 0 new loss 2.5162087695207447e-05 old loss 2.570675133028999e-05 BETTER
I0328 03:11:30.749240 2115320 finetune.py:68] layer 8_q @ epoch 2 new loss 2.3810451239114627e-05 old loss 2.4080150978988968e-05 BETTER
I0328 03:11:32.757737 2117651 finetune.py:68] layer 11_v @ epoch 2 new loss 2.576547922217287e-05 old loss 2.6515830541029572e-05 BETTER
I0328 03:11:35.053713 2116860 finetune.py:68] layer 10_v @ epoch 4 new loss 2.7826517907669768e-05 old loss 2.827207208611071e-05 BETTER
I0328 03:11:52.860907 2116106 finetune.py:68] layer 9_q @ epoch 1 new loss 2.4814669814077206e-05 old loss 2.5162087695207447e-05 BETTER
I0328 03:11:54.584450 2116860 finetune.py:45] layer 10_q initial loss 3.3825217542471364e-05
I0328 03:12:07.632368 2117651 finetune.py:68] layer 11_v @ epoch 3 new loss 2.5251341867260635e-05 old loss 2.576547922217287e-05 BETTER
I0328 03:12:07.935778 2115320 finetune.py:68] layer 8_q @ epoch 3 new loss 2.356939876335673e-05 old loss 2.3810451239114627e-05 BETTER
I0328 03:12:27.451648 2116106 finetune.py:68] layer 9_q @ epoch 2 new loss 2.4533777832402848e-05 old loss 2.4814669814077206e-05 BETTER
I0328 03:12:28.442208 2116860 finetune.py:68] layer 10_q @ epoch 0 new loss 3.3042113500414416e-05 old loss 3.3825217542471364e-05 BETTER
I0328 03:12:42.418536 2117651 finetune.py:68] layer 11_v @ epoch 4 new loss 2.484179822204169e-05 old loss 2.5251341867260635e-05 BETTER
I0328 03:12:44.945353 2115320 finetune.py:68] layer 8_q @ epoch 4 new loss 2.3367405447061174e-05 old loss 2.356939876335673e-05 BETTER
I0328 03:13:02.423609 2117651 finetune.py:45] layer 11_q initial loss 3.184324305038899e-05
I0328 03:13:02.715972 2116106 finetune.py:68] layer 9_q @ epoch 3 new loss 2.4289884095196612e-05 old loss 2.4533777832402848e-05 BETTER
I0328 03:13:02.741767 2115320 finetune.py:45] layer 8_k initial loss 2.4558215955039486e-05
I0328 03:13:03.591594 2116860 finetune.py:68] layer 10_q @ epoch 1 new loss 3.259212098782882e-05 old loss 3.3042113500414416e-05 BETTER
I0328 03:13:36.013746 2117651 finetune.py:68] layer 11_q @ epoch 0 new loss 3.111904879915528e-05 old loss 3.184324305038899e-05 BETTER
I0328 03:13:37.931845 2116106 finetune.py:68] layer 9_q @ epoch 4 new loss 2.4087910787784494e-05 old loss 2.4289884095196612e-05 BETTER
I0328 03:13:38.394303 2115320 finetune.py:68] layer 8_k @ epoch 0 new loss 2.429186315566767e-05 old loss 2.4558215955039486e-05 BETTER
I0328 03:13:38.956505 2116860 finetune.py:68] layer 10_q @ epoch 2 new loss 3.221076258341782e-05 old loss 3.259212098782882e-05 BETTER
I0328 03:13:56.172206 2116106 finetune.py:45] layer 9_k initial loss 2.5573208404239267e-05
I0328 03:14:10.414913 2117651 finetune.py:68] layer 11_q @ epoch 1 new loss 3.069714875891805e-05 old loss 3.111904879915528e-05 BETTER
I0328 03:14:14.069090 2116860 finetune.py:68] layer 10_q @ epoch 3 new loss 3.189145354554057e-05 old loss 3.221076258341782e-05 BETTER
I0328 03:14:15.114873 2115320 finetune.py:68] layer 8_k @ epoch 1 new loss 2.412312460364774e-05 old loss 2.429186315566767e-05 BETTER
I0328 03:14:29.730193 2116106 finetune.py:68] layer 9_k @ epoch 0 new loss 2.5254070351365954e-05 old loss 2.5573208404239267e-05 BETTER
I0328 03:14:44.866846 2117651 finetune.py:68] layer 11_q @ epoch 2 new loss 3.0342825994011946e-05 old loss 3.069714875891805e-05 BETTER
I0328 03:14:49.216568 2116860 finetune.py:68] layer 10_q @ epoch 4 new loss 3.160407504765317e-05 old loss 3.189145354554057e-05 BETTER
I0328 03:14:52.175668 2115320 finetune.py:68] layer 8_k @ epoch 2 new loss 2.3979897378012538e-05 old loss 2.412312460364774e-05 BETTER
I0328 03:15:04.282485 2116106 finetune.py:68] layer 9_k @ epoch 1 new loss 2.5074534278246574e-05 old loss 2.5254070351365954e-05 BETTER
I0328 03:15:07.522763 2116860 finetune.py:45] layer 10_k initial loss 3.300901153124869e-05
I0328 03:15:19.354904 2117651 finetune.py:68] layer 11_q @ epoch 3 new loss 3.006975748576224e-05 old loss 3.0342825994011946e-05 BETTER
I0328 03:15:29.099460 2115320 finetune.py:68] layer 8_k @ epoch 3 new loss 2.3848124328651465e-05 old loss 2.3979897378012538e-05 BETTER
I0328 03:15:39.083604 2116106 finetune.py:68] layer 9_k @ epoch 2 new loss 2.4919381758081727e-05 old loss 2.5074534278246574e-05 BETTER
I0328 03:15:41.484962 2116860 finetune.py:68] layer 10_k @ epoch 0 new loss 3.2664294849382713e-05 old loss 3.300901153124869e-05 BETTER
I0328 03:15:53.940666 2117651 finetune.py:68] layer 11_q @ epoch 4 new loss 2.9815619200235233e-05 old loss 3.006975748576224e-05 BETTER
I0328 03:16:06.148477 2115320 finetune.py:68] layer 8_k @ epoch 4 new loss 2.3728323867544532e-05 old loss 2.3848124328651465e-05 BETTER
I0328 03:16:11.571835 2117651 finetune.py:45] layer 11_k initial loss 3.127194941043854e-05
I0328 03:16:14.010881 2116106 finetune.py:68] layer 9_k @ epoch 3 new loss 2.4779985324130394e-05 old loss 2.4919381758081727e-05 BETTER
I0328 03:16:16.379045 2116860 finetune.py:68] layer 10_k @ epoch 1 new loss 3.243906030547805e-05 old loss 3.2664294849382713e-05 BETTER
I0328 03:16:25.607522 2115320 finetune.py:45] layer 8_o initial loss 5.487361704581417e-05
I0328 03:16:44.869943 2117651 finetune.py:68] layer 11_k @ epoch 0 new loss 3.094389467150904e-05 old loss 3.127194941043854e-05 BETTER
I0328 03:16:48.618319 2116106 finetune.py:68] layer 9_k @ epoch 4 new loss 2.4650420527905226e-05 old loss 2.4779985324130394e-05 BETTER
I0328 03:16:51.050904 2116860 finetune.py:68] layer 10_k @ epoch 2 new loss 3.2232008379651234e-05 old loss 3.243906030547805e-05 BETTER
I0328 03:17:00.414543 2115320 finetune.py:68] layer 8_o @ epoch 0 new loss 5.3189687605481595e-05 old loss 5.487361704581417e-05 BETTER
I0328 03:17:08.030715 2116106 finetune.py:45] layer 9_o initial loss 5.7360015489393845e-05
I0328 03:17:19.083626 2117651 finetune.py:68] layer 11_k @ epoch 1 new loss 3.071907485718839e-05 old loss 3.094389467150904e-05 BETTER
I0328 03:17:25.829555 2116860 finetune.py:68] layer 10_k @ epoch 3 new loss 3.2048512366600335e-05 old loss 3.2232008379651234e-05 BETTER
I0328 03:17:36.352850 2115320 finetune.py:68] layer 8_o @ epoch 1 new loss 5.248927482170984e-05 old loss 5.3189687605481595e-05 BETTER
I0328 03:17:41.033704 2116106 finetune.py:68] layer 9_o @ epoch 0 new loss 5.5486703786300495e-05 old loss 5.7360015489393845e-05 BETTER
I0328 03:17:53.122206 2117651 finetune.py:68] layer 11_k @ epoch 2 new loss 3.053058389923535e-05 old loss 3.071907485718839e-05 BETTER
I0328 03:18:00.779073 2116860 finetune.py:68] layer 10_k @ epoch 4 new loss 3.188686241628602e-05 old loss 3.2048512366600335e-05 BETTER
I0328 03:18:12.412421 2115320 finetune.py:68] layer 8_o @ epoch 2 new loss 5.1966042519779876e-05 old loss 5.248927482170984e-05 BETTER
I0328 03:18:14.818278 2116106 finetune.py:68] layer 9_o @ epoch 1 new loss 5.467033770401031e-05 old loss 5.5486703786300495e-05 BETTER
I0328 03:18:20.243163 2116860 finetune.py:45] layer 10_o initial loss 7.079137867549434e-05
I0328 03:18:27.257200 2117651 finetune.py:68] layer 11_k @ epoch 3 new loss 3.034680958080571e-05 old loss 3.053058389923535e-05 BETTER
I0328 03:18:48.562867 2115320 finetune.py:68] layer 8_o @ epoch 3 new loss 5.152828452992253e-05 old loss 5.1966042519779876e-05 BETTER
I0328 03:18:48.744847 2116106 finetune.py:68] layer 9_o @ epoch 2 new loss 5.405099727795459e-05 old loss 5.467033770401031e-05 BETTER
I0328 03:18:53.464012 2116860 finetune.py:68] layer 10_o @ epoch 0 new loss 6.830850179539993e-05 old loss 7.079137867549434e-05 BETTER
I0328 03:19:01.350632 2117651 finetune.py:68] layer 11_k @ epoch 4 new loss 3.018703864654526e-05 old loss 3.034680958080571e-05 BETTER
I0328 03:19:20.694760 2117651 finetune.py:45] layer 11_o initial loss 7.10261519998312e-05
I0328 03:19:22.908192 2116106 finetune.py:68] layer 9_o @ epoch 3 new loss 5.354930181056261e-05 old loss 5.405099727795459e-05 BETTER
I0328 03:19:24.736129 2115320 finetune.py:68] layer 8_o @ epoch 4 new loss 5.1147078920621425e-05 old loss 5.152828452992253e-05 BETTER
I0328 03:19:27.602035 2116860 finetune.py:68] layer 10_o @ epoch 1 new loss 6.722390389768407e-05 old loss 6.830850179539993e-05 BETTER
I0328 03:19:53.260538 2117651 finetune.py:68] layer 11_o @ epoch 0 new loss 6.863173621241003e-05 old loss 7.10261519998312e-05 BETTER
I0328 03:19:55.961641 2115320 finetune.py:45] layer 8_up initial loss 0.00010209788888460025
I0328 03:19:56.952147 2116106 finetune.py:68] layer 9_o @ epoch 4 new loss 5.311020140652545e-05 old loss 5.354930181056261e-05 BETTER
I0328 03:20:01.993967 2116860 finetune.py:68] layer 10_o @ epoch 2 new loss 6.641919753747061e-05 old loss 6.722390389768407e-05 BETTER
I0328 03:20:26.933591 2117651 finetune.py:68] layer 11_o @ epoch 1 new loss 6.757734809070826e-05 old loss 6.863173621241003e-05 BETTER
I0328 03:20:28.207269 2115320 finetune.py:68] layer 8_up @ epoch 0 new loss 0.00010093345917994156 old loss 0.00010209788888460025 BETTER
I0328 03:20:28.236232 2116106 finetune.py:45] layer 9_up initial loss 0.00010884201765293255
I0328 03:20:36.393328 2116860 finetune.py:68] layer 10_o @ epoch 3 new loss 6.57523050904274e-05 old loss 6.641919753747061e-05 BETTER
I0328 03:20:59.268049 2116106 finetune.py:68] layer 9_up @ epoch 0 new loss 0.00010748739441623911 old loss 0.00010884201765293255 BETTER
I0328 03:21:00.845140 2117651 finetune.py:68] layer 11_o @ epoch 2 new loss 6.677439523627982e-05 old loss 6.757734809070826e-05 BETTER
I0328 03:21:01.942914 2115320 finetune.py:68] layer 8_up @ epoch 1 new loss 0.00010009694960899651 old loss 0.00010093345917994156 BETTER
I0328 03:21:10.824482 2116860 finetune.py:68] layer 10_o @ epoch 4 new loss 6.517619476653636e-05 old loss 6.57523050904274e-05 BETTER
I0328 03:21:31.049691 2116106 finetune.py:68] layer 9_up @ epoch 1 new loss 0.00010652429045876488 old loss 0.00010748739441623911 BETTER
I0328 03:21:34.718561 2117651 finetune.py:68] layer 11_o @ epoch 3 new loss 6.611281423829496e-05 old loss 6.677439523627982e-05 BETTER
I0328 03:21:35.498111 2115320 finetune.py:68] layer 8_up @ epoch 2 new loss 9.935758134815842e-05 old loss 0.00010009694960899651 BETTER
I0328 03:21:42.198166 2116860 finetune.py:45] layer 10_up initial loss 0.0001223642029799521
I0328 03:22:03.365820 2116106 finetune.py:68] layer 9_up @ epoch 2 new loss 0.00010567896242719144 old loss 0.00010652429045876488 BETTER
I0328 03:22:08.268865 2117651 finetune.py:68] layer 11_o @ epoch 4 new loss 6.554288120241836e-05 old loss 6.611281423829496e-05 BETTER
I0328 03:22:09.243948 2115320 finetune.py:68] layer 8_up @ epoch 3 new loss 9.868561755865812e-05 old loss 9.935758134815842e-05 BETTER
I0328 03:22:12.968504 2116860 finetune.py:68] layer 10_up @ epoch 0 new loss 0.00012091689131921157 old loss 0.0001223642029799521 BETTER
I0328 03:22:35.636121 2116106 finetune.py:68] layer 9_up @ epoch 3 new loss 0.00010490653949091211 old loss 0.00010567896242719144 BETTER
I0328 03:22:39.490494 2117651 finetune.py:45] layer 11_up initial loss 0.00012607847747858614
I0328 03:22:43.068094 2115320 finetune.py:68] layer 8_up @ epoch 4 new loss 9.805388981476426e-05 old loss 9.868561755865812e-05 BETTER
I0328 03:22:45.164189 2116860 finetune.py:68] layer 10_up @ epoch 1 new loss 0.00011986141907982528 old loss 0.00012091689131921157 BETTER
I0328 03:23:07.930416 2116106 finetune.py:68] layer 9_up @ epoch 4 new loss 0.00010419882164569572 old loss 0.00010490653949091211 BETTER
I0328 03:23:09.807842 2117651 finetune.py:68] layer 11_up @ epoch 0 new loss 0.00012453562521841377 old loss 0.00012607847747858614 BETTER
I0328 03:23:14.493768 2115320 finetune.py:45] layer 8_gate initial loss 0.00011705005454132333
I0328 03:23:17.374000 2116860 finetune.py:68] layer 10_up @ epoch 2 new loss 0.00011894567433046177 old loss 0.00011986141907982528 BETTER
I0328 03:23:38.961780 2116106 finetune.py:45] layer 9_gate initial loss 0.00012473137758206576
I0328 03:23:41.097791 2117651 finetune.py:68] layer 11_up @ epoch 1 new loss 0.00012339076783973724 old loss 0.00012453562521841377 BETTER
I0328 03:23:44.771228 2115320 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.00011633776739472523 old loss 0.00011705005454132333 BETTER
I0328 03:23:49.662671 2116860 finetune.py:68] layer 10_up @ epoch 3 new loss 0.00011809575516963378 old loss 0.00011894567433046177 BETTER
I0328 03:24:07.518008 2116106 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.00012389309995342046 old loss 0.00012473137758206576 BETTER
I0328 03:24:12.558234 2117651 finetune.py:68] layer 11_up @ epoch 2 new loss 0.0001223826257046312 old loss 0.00012339076783973724 BETTER
I0328 03:24:16.067813 2115320 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.00011575451208045706 old loss 0.00011633776739472523 BETTER
I0328 03:24:21.911546 2116860 finetune.py:68] layer 10_up @ epoch 4 new loss 0.00011731701670214534 old loss 0.00011809575516963378 BETTER
I0328 03:24:37.099075 2116106 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.00012324663111940026 old loss 0.00012389309995342046 BETTER
I0328 03:24:44.141384 2117651 finetune.py:68] layer 11_up @ epoch 3 new loss 0.00012145620712544769 old loss 0.0001223826257046312 BETTER
I0328 03:24:47.425900 2115320 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.00011522068962221965 old loss 0.00011575451208045706 BETTER
I0328 03:24:53.657753 2116860 finetune.py:45] layer 10_gate initial loss 0.0001397270680172369
I0328 03:25:06.837183 2116106 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.00012264847464393824 old loss 0.00012324663111940026 BETTER
I0328 03:25:15.849389 2117651 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00012059763685101643 old loss 0.00012145620712544769 BETTER
I0328 03:25:19.008672 2115320 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.00011472771439002827 old loss 0.00011522068962221965 BETTER
I0328 03:25:22.282846 2116860 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.00013882358325645328 old loss 0.0001397270680172369 BETTER
I0328 03:25:36.610443 2116106 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.00012209234409965575 old loss 0.00012264847464393824 BETTER
I0328 03:25:47.670902 2117651 finetune.py:45] layer 11_gate initial loss 0.00014454188931267709
I0328 03:25:50.608007 2115320 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.00011425372213125229 old loss 0.00011472771439002827 BETTER
I0328 03:25:52.141778 2116860 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.00013811691314913332 old loss 0.00013882358325645328 BETTER
I0328 03:26:06.324776 2116106 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.00012156740558566526 old loss 0.00012209234409965575 BETTER
I0328 03:26:15.986893 2117651 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.0001436214952263981 old loss 0.00014454188931267709 BETTER
I0328 03:26:21.969487 2116860 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.00013747082266490906 old loss 0.00013811691314913332 BETTER
I0328 03:26:45.102652 2117651 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.00014285967336036265 old loss 0.0001436214952263981 BETTER
I0328 03:26:45.803550 2115320 finetune.py:45] layer 8_down initial loss 0.0001728258066577837
I0328 03:26:51.754338 2116860 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.00013687118189409375 old loss 0.00013747082266490906 BETTER
I0328 03:27:03.005518 2116106 finetune.py:45] layer 9_down initial loss 0.00018557165458332747
I0328 03:27:13.104194 2115320 finetune.py:68] layer 8_down @ epoch 0 new loss 0.00017280345491599292 old loss 0.0001728258066577837 BETTER
I0328 03:27:14.461484 2117651 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.0001421706983819604 old loss 0.00014285967336036265 BETTER
I0328 03:27:21.726448 2116860 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.00013629972818307579 old loss 0.00013687118189409375 BETTER
I0328 03:27:29.242611 2116106 finetune.py:68] layer 9_down @ epoch 0 new loss 0.00018555241695139557 old loss 0.00018557165458332747 BETTER
I0328 03:27:41.609195 2115320 finetune.py:68] layer 8_down @ epoch 1 new loss 0.00017278392624575645 old loss 0.00017280345491599292 BETTER
I0328 03:27:43.926817 2117651 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.00014151324285194278 old loss 0.0001421706983819604 BETTER
I0328 03:27:56.658209 2116106 finetune.py:68] layer 9_down @ epoch 1 new loss 0.00018553559493739158 old loss 0.00018555241695139557 BETTER
I0328 03:28:10.375349 2115320 finetune.py:68] layer 8_down @ epoch 2 new loss 0.00017276692960876971 old loss 0.00017278392624575645 BETTER
I0328 03:28:13.507318 2117651 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.00014090102922637016 old loss 0.00014151324285194278 BETTER
I0328 03:28:18.253408 2116860 finetune.py:45] layer 10_down initial loss 0.00020215574477333575
I0328 03:28:24.333165 2116106 finetune.py:68] layer 9_down @ epoch 2 new loss 0.0001855208829510957 old loss 0.00018553559493739158 BETTER
I0328 03:28:39.532391 2115320 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0001727511698845774 old loss 0.00017276692960876971 BETTER
I0328 03:28:44.714295 2116860 finetune.py:68] layer 10_down @ epoch 0 new loss 0.00020212731033097953 old loss 0.00020215574477333575 BETTER
I0328 03:28:52.013677 2116106 finetune.py:68] layer 9_down @ epoch 3 new loss 0.00018550494860392064 old loss 0.0001855208829510957 BETTER
I0328 03:29:09.030488 2115320 finetune.py:68] layer 8_down @ epoch 4 new loss 0.00017273605044465512 old loss 0.0001727511698845774 BETTER
I0328 03:29:10.827556 2117651 finetune.py:45] layer 11_down initial loss 0.00020911036699544638
8_v proxy err 0.04365367069840431 tr(WHW.T) 257.7052307128906
bpp_loss 2.200348670652602
8_q proxy err 0.0019726951140910387 tr(WHW.T) 26610.224609375
bpp_loss 3.013066513231024
8_k proxy err 0.0006631982978433371 tr(WHW.T) 22479.103515625
bpp_loss 3.878458831750322
8_o proxy err 0.041486382484436035 tr(WHW.T) 752.9906616210938
bpp_loss 2.326595747785177
8_up proxy err 0.028158659115433693 tr(WHW.T) 8491.82421875
bpp_loss 2.4420944280656323
8_gate proxy err 0.006832596380263567 tr(WHW.T) 37166.40625
bpp_loss 2.7388086096450155
8_down proxy err 0.035828884690999985 tr(WHW.T) 6531.07373046875
bpp_loss 2.448672918918809
I0328 03:29:12.406212 2116860 finetune.py:68] layer 10_down @ epoch 1 new loss 0.000202101597096771 old loss 0.00020212731033097953 BETTER
I0328 03:29:20.652252 2116106 finetune.py:68] layer 9_down @ epoch 4 new loss 0.0001854925212683156 old loss 0.00018550494860392064 BETTER
9_v proxy err 0.032990749925374985 tr(WHW.T) 351.4288024902344
bpp_loss 2.2986884281854145
9_q proxy err 0.002066653687506914 tr(WHW.T) 25667.333984375
bpp_loss 3.0250057672383264
9_k proxy err 0.0007231796043924987 tr(WHW.T) 20914.560546875
bpp_loss 3.8982836506329477
9_o proxy err 0.037316855043172836 tr(WHW.T) 784.048095703125
bpp_loss 2.378377360990271
9_up proxy err 0.02666599489748478 tr(WHW.T) 8966.2900390625
bpp_loss 2.451175064779818
9_gate proxy err 0.006467081140726805 tr(WHW.T) 39310.08984375
bpp_loss 2.751521459115403
9_down proxy err 0.03549148887395859 tr(WHW.T) 6318.26318359375
bpp_loss 2.4499389000868956
I0328 03:29:37.801921 2117651 finetune.py:68] layer 11_down @ epoch 0 new loss 0.00020908239821437746 old loss 0.00020911036699544638 BETTER
I0328 03:29:40.880289 2116860 finetune.py:68] layer 10_down @ epoch 2 new loss 0.00020207904162816703 old loss 0.000202101597096771 BETTER
I0328 03:30:04.935551 2117651 finetune.py:68] layer 11_down @ epoch 1 new loss 0.00020905498240608722 old loss 0.00020908239821437746 BETTER
I0328 03:30:08.662994 2116860 finetune.py:68] layer 10_down @ epoch 3 new loss 0.000202057373826392 old loss 0.00020207904162816703 BETTER
I0328 03:30:32.213085 2117651 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0002090313791995868 old loss 0.00020905498240608722 BETTER
I0328 03:30:33.127542 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 67.6624367237091s
I0328 03:30:36.517502 2116860 finetune.py:68] layer 10_down @ epoch 4 new loss 0.00020203730673529208 old loss 0.000202057373826392 BETTER
I0328 03:30:36.973956 2130334 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:30:36.974055 2130334 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:30:36.974096 2130334 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:30:37.314081 2130334 config.py:54] PyTorch version 2.6.0 available.
W0328 03:30:37.523059 2130334 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

10_v proxy err 0.04279424250125885 tr(WHW.T) 251.83889770507812
bpp_loss 2.1906784937891643
10_q proxy err 0.0021557582076638937 tr(WHW.T) 23352.724609375
bpp_loss 3.0218357906560414
10_k proxy err 0.0007279060082510114 tr(WHW.T) 19711.828125
bpp_loss 3.8958122663898394
10_o proxy err 0.04492781683802605 tr(WHW.T) 688.4729614257812
bpp_loss 2.318376652779989
10_up proxy err 0.026338228955864906 tr(WHW.T) 9195.0458984375
bpp_loss 2.467988635612918
10_gate proxy err 0.006826707627624273 tr(WHW.T) 37349.671875
bpp_loss 2.7234619652985463
10_down proxy err 0.034532953053712845 tr(WHW.T) 6569.8525390625
bpp_loss 2.466249144868925
W0328 03:30:38.133260 2130334 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:30:38.136915 2083830 quantize_finetune_llama.py:209] layer 13 gpu 1
I0328 03:30:38.149741 2130334 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:30:55.045408 2130334 finetune.py:45] layer 12_v initial loss 4.141403769608587e-05
W0328 03:30:55.045624 2130334 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:30:59.585144 2117651 finetune.py:68] layer 11_down @ epoch 3 new loss 0.00020900908566545695 old loss 0.0002090313791995868 BETTER
I0328 03:31:27.192836 2117651 finetune.py:68] layer 11_down @ epoch 4 new loss 0.00020898657385259867 old loss 0.00020900908566545695 BETTER
11_v proxy err 0.03527618572115898 tr(WHW.T) 319.5691223144531
bpp_loss 2.197880479914602
11_q proxy err 0.0023626922629773617 tr(WHW.T) 22203.390625
bpp_loss 2.9684293114114553
11_k proxy err 0.0008395844488404691 tr(WHW.T) 17976.1171875
bpp_loss 3.8964974287664518
11_o proxy err 0.04575971141457558 tr(WHW.T) 573.2348022460938
bpp_loss 2.3380989254219458
11_up proxy err 0.025807447731494904 tr(WHW.T) 9190.201171875
bpp_loss 2.471953936287069
11_gate proxy err 0.006771046668291092 tr(WHW.T) 36705.79296875
bpp_loss 2.7026080858361508
11_down proxy err 0.033054426312446594 tr(WHW.T) 6705.00830078125
bpp_loss 2.4732282116144364
I0328 03:31:30.439429 2130334 finetune.py:68] layer 12_v @ epoch 0 new loss 3.164209192618728e-05 old loss 4.141403769608587e-05 BETTER
I0328 03:31:44.889125 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 63.56385350227356s
I0328 03:31:48.494067 2131120 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:31:48.494178 2131120 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:31:48.494216 2131120 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:31:48.866065 2131120 config.py:54] PyTorch version 2.6.0 available.
W0328 03:31:49.063423 2131120 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:31:49.697153 2131120 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:31:49.700979 2083830 quantize_finetune_llama.py:209] layer 14 gpu 2
I0328 03:31:49.714765 2131120 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:32:06.387264 2131120 finetune.py:45] layer 13_v initial loss 4.3472187826409936e-05
W0328 03:32:06.387487 2131120 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:32:07.021474 2130334 finetune.py:68] layer 12_v @ epoch 1 new loss 3.0081529985181987e-05 old loss 3.164209192618728e-05 BETTER
I0328 03:32:39.879081 2131120 finetune.py:68] layer 13_v @ epoch 0 new loss 3.3614604035392404e-05 old loss 4.3472187826409936e-05 BETTER
I0328 03:32:44.063288 2130334 finetune.py:68] layer 12_v @ epoch 2 new loss 2.9138082027202472e-05 old loss 3.0081529985181987e-05 BETTER
I0328 03:32:53.303439 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 63.147600173950195s
I0328 03:32:56.968709 2131855 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:32:56.968805 2131855 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:32:56.968842 2131855 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:32:57.315053 2131855 config.py:54] PyTorch version 2.6.0 available.
W0328 03:32:57.505177 2131855 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:32:58.127018 2131855 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:32:58.130730 2083830 quantize_finetune_llama.py:209] layer 15 gpu 3
I0328 03:32:58.144799 2131855 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:33:14.434181 2131120 finetune.py:68] layer 13_v @ epoch 1 new loss 3.2007443223847076e-05 old loss 3.3614604035392404e-05 BETTER
I0328 03:33:14.973948 2131855 finetune.py:45] layer 14_v initial loss 4.606631409842521e-05
W0328 03:33:14.974144 2131855 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:33:21.303719 2130334 finetune.py:68] layer 12_v @ epoch 3 new loss 2.842702087946236e-05 old loss 2.9138082027202472e-05 BETTER
I0328 03:33:48.616154 2131855 finetune.py:68] layer 14_v @ epoch 0 new loss 3.636064502643421e-05 old loss 4.606631409842521e-05 BETTER
I0328 03:33:49.452028 2131120 finetune.py:68] layer 13_v @ epoch 2 new loss 3.108911187155172e-05 old loss 3.2007443223847076e-05 BETTER
I0328 03:33:58.769836 2130334 finetune.py:68] layer 12_v @ epoch 4 new loss 2.790039434330538e-05 old loss 2.842702087946236e-05 BETTER
I0328 03:34:02.361593 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 63.770517110824585s
I0328 03:34:06.109364 2132639 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:34:06.109468 2132639 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:34:06.109509 2132639 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:34:06.456717 2132639 config.py:54] PyTorch version 2.6.0 available.
W0328 03:34:06.667738 2132639 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:34:07.353254 2132639 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:34:07.357127 2083830 quantize_finetune_llama.py:209] layer 16 gpu 0
I0328 03:34:07.371639 2132639 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 03:34:18.285021 2130334 finetune.py:45] layer 12_q initial loss 3.340432158438489e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:34:23.473889 2131855 finetune.py:68] layer 14_v @ epoch 1 new loss 3.4760003472911194e-05 old loss 3.636064502643421e-05 BETTER
I0328 03:34:24.748210 2131120 finetune.py:68] layer 13_v @ epoch 3 new loss 3.0439361580647528e-05 old loss 3.108911187155172e-05 BETTER
I0328 03:34:24.911971 2132639 finetune.py:45] layer 15_v initial loss 6.080642924644053e-05
W0328 03:34:24.912213 2132639 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:34:53.784926 2130334 finetune.py:68] layer 12_q @ epoch 0 new loss 3.26493609463796e-05 old loss 3.340432158438489e-05 BETTER
I0328 03:34:58.013497 2132639 finetune.py:68] layer 15_v @ epoch 0 new loss 4.074818571098149e-05 old loss 6.080642924644053e-05 BETTER
I0328 03:34:58.899006 2131855 finetune.py:68] layer 14_v @ epoch 2 new loss 3.3801588870119303e-05 old loss 3.4760003472911194e-05 BETTER
I0328 03:35:00.035591 2131120 finetune.py:68] layer 13_v @ epoch 4 new loss 2.9936752980574965e-05 old loss 3.0439361580647528e-05 BETTER
I0328 03:35:19.155851 2131120 finetune.py:45] layer 13_q initial loss 3.645543256425299e-05
I0328 03:35:30.662159 2130334 finetune.py:68] layer 12_q @ epoch 1 new loss 3.2101350370794535e-05 old loss 3.26493609463796e-05 BETTER
I0328 03:35:32.611920 2132639 finetune.py:68] layer 15_v @ epoch 1 new loss 3.842268415610306e-05 old loss 4.074818571098149e-05 BETTER
I0328 03:35:34.243319 2131855 finetune.py:68] layer 14_v @ epoch 3 new loss 3.309172097942792e-05 old loss 3.3801588870119303e-05 BETTER
I0328 03:35:52.790221 2131120 finetune.py:68] layer 13_q @ epoch 0 new loss 3.5753830161411315e-05 old loss 3.645543256425299e-05 BETTER
I0328 03:36:07.245450 2132639 finetune.py:68] layer 15_v @ epoch 2 new loss 3.712371471920051e-05 old loss 3.842268415610306e-05 BETTER
I0328 03:36:07.773854 2130334 finetune.py:68] layer 12_q @ epoch 2 new loss 3.164168811053969e-05 old loss 3.2101350370794535e-05 BETTER
I0328 03:36:09.704284 2131855 finetune.py:68] layer 14_v @ epoch 4 new loss 3.2531672331970185e-05 old loss 3.309172097942792e-05 BETTER
I0328 03:36:27.791646 2131120 finetune.py:68] layer 13_q @ epoch 1 new loss 3.5282751923659816e-05 old loss 3.5753830161411315e-05 BETTER
I0328 03:36:29.312920 2131855 finetune.py:45] layer 14_q initial loss 4.136089773965068e-05
I0328 03:36:42.276636 2132639 finetune.py:68] layer 15_v @ epoch 3 new loss 3.62288010364864e-05 old loss 3.712371471920051e-05 BETTER
I0328 03:36:44.921511 2130334 finetune.py:68] layer 12_q @ epoch 3 new loss 3.123415808659047e-05 old loss 3.164168811053969e-05 BETTER
I0328 03:37:02.563127 2131120 finetune.py:68] layer 13_q @ epoch 2 new loss 3.490324525046162e-05 old loss 3.5282751923659816e-05 BETTER
I0328 03:37:03.190763 2131855 finetune.py:68] layer 14_q @ epoch 0 new loss 4.0349215851165354e-05 old loss 4.136089773965068e-05 BETTER
I0328 03:37:16.885711 2132639 finetune.py:68] layer 15_v @ epoch 4 new loss 3.5544191632652655e-05 old loss 3.62288010364864e-05 BETTER
I0328 03:37:21.676821 2130334 finetune.py:68] layer 12_q @ epoch 4 new loss 3.089382516918704e-05 old loss 3.123415808659047e-05 BETTER
I0328 03:37:36.612179 2132639 finetune.py:45] layer 15_q initial loss 4.087613706360571e-05
I0328 03:37:37.376448 2131120 finetune.py:68] layer 13_q @ epoch 3 new loss 3.456441481830552e-05 old loss 3.490324525046162e-05 BETTER
I0328 03:37:38.076732 2131855 finetune.py:68] layer 14_q @ epoch 1 new loss 3.972063132096082e-05 old loss 4.0349215851165354e-05 BETTER
I0328 03:37:40.051285 2130334 finetune.py:45] layer 12_k initial loss 3.2974628993542865e-05
I0328 03:38:09.743748 2132639 finetune.py:68] layer 15_q @ epoch 0 new loss 4.004438960691914e-05 old loss 4.087613706360571e-05 BETTER
I0328 03:38:12.212814 2131120 finetune.py:68] layer 13_q @ epoch 4 new loss 3.427987758186646e-05 old loss 3.456441481830552e-05 BETTER
I0328 03:38:12.940544 2131855 finetune.py:68] layer 14_q @ epoch 2 new loss 3.9215297874761745e-05 old loss 3.972063132096082e-05 BETTER
I0328 03:38:15.557281 2130334 finetune.py:68] layer 12_k @ epoch 0 new loss 3.255262708989903e-05 old loss 3.2974628993542865e-05 BETTER
I0328 03:38:30.084526 2131120 finetune.py:45] layer 13_k initial loss 3.587861647247337e-05
I0328 03:38:44.224494 2132639 finetune.py:68] layer 15_q @ epoch 1 new loss 3.944345735362731e-05 old loss 4.004438960691914e-05 BETTER
I0328 03:38:47.770265 2131855 finetune.py:68] layer 14_q @ epoch 3 new loss 3.8785288779763505e-05 old loss 3.9215297874761745e-05 BETTER
I0328 03:38:51.873325 2130334 finetune.py:68] layer 12_k @ epoch 1 new loss 3.225115506211296e-05 old loss 3.255262708989903e-05 BETTER
I0328 03:39:03.834647 2131120 finetune.py:68] layer 13_k @ epoch 0 new loss 3.552503767423332e-05 old loss 3.587861647247337e-05 BETTER
I0328 03:39:18.420016 2132639 finetune.py:68] layer 15_q @ epoch 2 new loss 3.8966380088822916e-05 old loss 3.944345735362731e-05 BETTER
I0328 03:39:22.688261 2131855 finetune.py:68] layer 14_q @ epoch 4 new loss 3.8416110328398645e-05 old loss 3.8785288779763505e-05 BETTER
I0328 03:39:28.686448 2130334 finetune.py:68] layer 12_k @ epoch 2 new loss 3.198238118784502e-05 old loss 3.225115506211296e-05 BETTER
I0328 03:39:38.213877 2131120 finetune.py:68] layer 13_k @ epoch 1 new loss 3.528219531290233e-05 old loss 3.552503767423332e-05 BETTER
I0328 03:39:40.550834 2131855 finetune.py:45] layer 14_k initial loss 4.1315059206681326e-05
I0328 03:39:52.822251 2132639 finetune.py:68] layer 15_q @ epoch 3 new loss 3.856156035908498e-05 old loss 3.8966380088822916e-05 BETTER
I0328 03:40:05.711945 2130334 finetune.py:68] layer 12_k @ epoch 3 new loss 3.175092570018023e-05 old loss 3.198238118784502e-05 BETTER
I0328 03:40:12.796166 2131120 finetune.py:68] layer 13_k @ epoch 2 new loss 3.50788795913104e-05 old loss 3.528219531290233e-05 BETTER
I0328 03:40:14.188610 2131855 finetune.py:68] layer 14_k @ epoch 0 new loss 4.073645322932862e-05 old loss 4.1315059206681326e-05 BETTER
I0328 03:40:27.134528 2132639 finetune.py:68] layer 15_q @ epoch 4 new loss 3.820804340648465e-05 old loss 3.856156035908498e-05 BETTER
I0328 03:40:42.718117 2130334 finetune.py:68] layer 12_k @ epoch 4 new loss 3.153302532155067e-05 old loss 3.175092570018023e-05 BETTER
I0328 03:40:45.219728 2132639 finetune.py:45] layer 15_k initial loss 4.038975021103397e-05
I0328 03:40:47.659842 2131120 finetune.py:68] layer 13_k @ epoch 3 new loss 3.488781658234075e-05 old loss 3.50788795913104e-05 BETTER
I0328 03:40:48.826496 2131855 finetune.py:68] layer 14_k @ epoch 1 new loss 4.0404491301160306e-05 old loss 4.073645322932862e-05 BETTER
I0328 03:41:02.711218 2130334 finetune.py:45] layer 12_o initial loss 7.218040263978764e-05
I0328 03:41:18.200333 2132639 finetune.py:68] layer 15_k @ epoch 0 new loss 3.997935709776357e-05 old loss 4.038975021103397e-05 BETTER
I0328 03:41:22.410952 2131120 finetune.py:68] layer 13_k @ epoch 4 new loss 3.4711592888925225e-05 old loss 3.488781658234075e-05 BETTER
I0328 03:41:23.557750 2131855 finetune.py:68] layer 14_k @ epoch 2 new loss 4.012354838778265e-05 old loss 4.0404491301160306e-05 BETTER
I0328 03:41:37.761337 2130334 finetune.py:68] layer 12_o @ epoch 0 new loss 6.994506111368537e-05 old loss 7.218040263978764e-05 BETTER
I0328 03:41:42.223524 2131120 finetune.py:45] layer 13_o initial loss 8.722530765226111e-05
I0328 03:41:52.155541 2132639 finetune.py:68] layer 15_k @ epoch 1 new loss 3.9691614801995456e-05 old loss 3.997935709776357e-05 BETTER
I0328 03:41:58.393789 2131855 finetune.py:68] layer 14_k @ epoch 3 new loss 3.987348827649839e-05 old loss 4.012354838778265e-05 BETTER
I0328 03:42:13.760127 2130334 finetune.py:68] layer 12_o @ epoch 1 new loss 6.884166214149445e-05 old loss 6.994506111368537e-05 BETTER
I0328 03:42:15.163157 2131120 finetune.py:68] layer 13_o @ epoch 0 new loss 8.434387564193457e-05 old loss 8.722530765226111e-05 BETTER
I0328 03:42:26.294430 2132639 finetune.py:68] layer 15_k @ epoch 2 new loss 3.942448893212713e-05 old loss 3.9691614801995456e-05 BETTER
I0328 03:42:33.183058 2131855 finetune.py:68] layer 14_k @ epoch 4 new loss 3.964650386478752e-05 old loss 3.987348827649839e-05 BETTER
I0328 03:42:48.956005 2131120 finetune.py:68] layer 13_o @ epoch 1 new loss 8.295141014968976e-05 old loss 8.434387564193457e-05 BETTER
I0328 03:42:50.066432 2130334 finetune.py:68] layer 12_o @ epoch 2 new loss 6.798246613470837e-05 old loss 6.884166214149445e-05 BETTER
I0328 03:42:52.859333 2131855 finetune.py:45] layer 14_o initial loss 9.538442827761173e-05
I0328 03:43:00.463142 2132639 finetune.py:68] layer 15_k @ epoch 3 new loss 3.9204514905577525e-05 old loss 3.942448893212713e-05 BETTER
I0328 03:43:23.370898 2131120 finetune.py:68] layer 13_o @ epoch 2 new loss 8.189099025912583e-05 old loss 8.295141014968976e-05 BETTER
I0328 03:43:25.974819 2131855 finetune.py:68] layer 14_o @ epoch 0 new loss 9.214804595103487e-05 old loss 9.538442827761173e-05 BETTER
I0328 03:43:26.472281 2130334 finetune.py:68] layer 12_o @ epoch 3 new loss 6.725800631102175e-05 old loss 6.798246613470837e-05 BETTER
I0328 03:43:34.647298 2132639 finetune.py:68] layer 15_k @ epoch 4 new loss 3.899926014128141e-05 old loss 3.9204514905577525e-05 BETTER
I0328 03:43:53.941089 2132639 finetune.py:45] layer 15_o initial loss 9.635700553189963e-05
I0328 03:43:57.659590 2131120 finetune.py:68] layer 13_o @ epoch 3 new loss 8.100610284600407e-05 old loss 8.189099025912583e-05 BETTER
I0328 03:44:00.142678 2131855 finetune.py:68] layer 14_o @ epoch 1 new loss 9.048733045347035e-05 old loss 9.214804595103487e-05 BETTER
I0328 03:44:02.552709 2130334 finetune.py:68] layer 12_o @ epoch 4 new loss 6.662232044618577e-05 old loss 6.725800631102175e-05 BETTER
I0328 03:44:26.155558 2132639 finetune.py:68] layer 15_o @ epoch 0 new loss 9.255481563741341e-05 old loss 9.635700553189963e-05 BETTER
I0328 03:44:32.089598 2131120 finetune.py:68] layer 13_o @ epoch 4 new loss 8.024815906537697e-05 old loss 8.100610284600407e-05 BETTER
I0328 03:44:33.881395 2130334 finetune.py:45] layer 12_up initial loss 0.0001298333372687921
I0328 03:44:34.334057 2131855 finetune.py:68] layer 14_o @ epoch 2 new loss 8.919984247768298e-05 old loss 9.048733045347035e-05 BETTER
I0328 03:44:59.385132 2132639 finetune.py:68] layer 15_o @ epoch 1 new loss 9.081339521799237e-05 old loss 9.255481563741341e-05 BETTER
I0328 03:45:03.534452 2131120 finetune.py:45] layer 13_up initial loss 0.00015250286378432065
I0328 03:45:05.973611 2130334 finetune.py:68] layer 12_up @ epoch 0 new loss 0.00012804512516595423 old loss 0.0001298333372687921 BETTER
I0328 03:45:08.567140 2131855 finetune.py:68] layer 14_o @ epoch 3 new loss 8.814091415842995e-05 old loss 8.919984247768298e-05 BETTER
I0328 03:45:32.679451 2132639 finetune.py:68] layer 15_o @ epoch 2 new loss 8.948034519562498e-05 old loss 9.081339521799237e-05 BETTER
I0328 03:45:34.124088 2131120 finetune.py:68] layer 13_up @ epoch 0 new loss 0.0001503461244283244 old loss 0.00015250286378432065 BETTER
I0328 03:45:39.318942 2130334 finetune.py:68] layer 12_up @ epoch 1 new loss 0.0001266889157705009 old loss 0.00012804512516595423 BETTER
I0328 03:45:43.025944 2131855 finetune.py:68] layer 14_o @ epoch 4 new loss 8.723505743546411e-05 old loss 8.814091415842995e-05 BETTER
I0328 03:46:05.927878 2131120 finetune.py:68] layer 13_up @ epoch 1 new loss 0.00014876386558171362 old loss 0.0001503461244283244 BETTER
I0328 03:46:06.226252 2132639 finetune.py:68] layer 15_o @ epoch 3 new loss 8.839372458169237e-05 old loss 8.948034519562498e-05 BETTER
I0328 03:46:12.926862 2130334 finetune.py:68] layer 12_up @ epoch 2 new loss 0.0001254946691915393 old loss 0.0001266889157705009 BETTER
I0328 03:46:14.544840 2131855 finetune.py:45] layer 14_up initial loss 0.00017461217066738755
I0328 03:46:38.169318 2131120 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0001473788870498538 old loss 0.00014876386558171362 BETTER
I0328 03:46:39.768102 2132639 finetune.py:68] layer 15_o @ epoch 4 new loss 8.745880768401548e-05 old loss 8.839372458169237e-05 BETTER
I0328 03:46:45.638077 2131855 finetune.py:68] layer 14_up @ epoch 0 new loss 0.0001718541025184095 old loss 0.00017461217066738755 BETTER
I0328 03:46:46.937621 2130334 finetune.py:68] layer 12_up @ epoch 3 new loss 0.00012442040315363556 old loss 0.0001254946691915393 BETTER
I0328 03:47:10.468903 2131120 finetune.py:68] layer 13_up @ epoch 3 new loss 0.0001461283245589584 old loss 0.0001473788870498538 BETTER
I0328 03:47:11.360946 2132639 finetune.py:45] layer 15_up initial loss 0.00019204562704544514
I0328 03:47:17.993996 2131855 finetune.py:68] layer 14_up @ epoch 1 new loss 0.00016976645565591753 old loss 0.0001718541025184095 BETTER
I0328 03:47:20.819830 2130334 finetune.py:68] layer 12_up @ epoch 4 new loss 0.0001234175724675879 old loss 0.00012442040315363556 BETTER
I0328 03:47:41.617920 2132639 finetune.py:68] layer 15_up @ epoch 0 new loss 0.00018852786161005497 old loss 0.00019204562704544514 BETTER
I0328 03:47:42.749761 2131120 finetune.py:68] layer 13_up @ epoch 4 new loss 0.00014498087693937123 old loss 0.0001461283245589584 BETTER
I0328 03:47:50.258492 2131855 finetune.py:68] layer 14_up @ epoch 2 new loss 0.000167959849932231 old loss 0.00016976645565591753 BETTER
I0328 03:47:52.063511 2130334 finetune.py:45] layer 12_gate initial loss 0.00015061926387716085
I0328 03:48:12.909461 2132639 finetune.py:68] layer 15_up @ epoch 1 new loss 0.00018592987908050418 old loss 0.00018852786161005497 BETTER
I0328 03:48:13.717109 2131120 finetune.py:45] layer 13_gate initial loss 0.0001749419025145471
I0328 03:48:22.318517 2130334 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.0001495474425610155 old loss 0.00015061926387716085 BETTER
I0328 03:48:22.519689 2131855 finetune.py:68] layer 14_up @ epoch 3 new loss 0.00016634732310194522 old loss 0.000167959849932231 BETTER
I0328 03:48:42.479582 2131120 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.00017367130203638226 old loss 0.0001749419025145471 BETTER
I0328 03:48:44.345681 2132639 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0001836994051700458 old loss 0.00018592987908050418 BETTER
I0328 03:48:53.620487 2130334 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.00014864624245092273 old loss 0.0001495474425610155 BETTER
I0328 03:48:54.696116 2131855 finetune.py:68] layer 14_up @ epoch 4 new loss 0.00016486727690789849 old loss 0.00016634732310194522 BETTER
I0328 03:49:12.099976 2131120 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0001726223563309759 old loss 0.00017367130203638226 BETTER
I0328 03:49:15.815497 2132639 finetune.py:68] layer 15_up @ epoch 3 new loss 0.00018171040574088693 old loss 0.0001836994051700458 BETTER
I0328 03:49:25.217025 2130334 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.0001478249323554337 old loss 0.00014864624245092273 BETTER
I0328 03:49:26.012584 2131855 finetune.py:45] layer 14_gate initial loss 0.0001974312763195485
I0328 03:49:41.979408 2131120 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.00017167272744700313 old loss 0.0001726223563309759 BETTER
I0328 03:49:47.469955 2132639 finetune.py:68] layer 15_up @ epoch 4 new loss 0.00017988159379456192 old loss 0.00018171040574088693 BETTER
I0328 03:49:54.708089 2131855 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.00019582381355576217 old loss 0.0001974312763195485 BETTER
I0328 03:49:56.889626 2130334 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.00014705474313814193 old loss 0.0001478249323554337 BETTER
I0328 03:50:11.911856 2131120 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.0001707782648736611 old loss 0.00017167272744700313 BETTER
I0328 03:50:18.726878 2132639 finetune.py:45] layer 15_gate initial loss 0.00021609512623399496
I0328 03:50:24.524828 2131855 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.00019449392857495695 old loss 0.00019582381355576217 BETTER
I0328 03:50:28.724625 2130334 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.00014633404498454183 old loss 0.00014705474313814193 BETTER
I0328 03:50:41.862533 2131120 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0001699509157333523 old loss 0.0001707782648736611 BETTER
I0328 03:50:47.139980 2132639 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.00021413729700725526 old loss 0.00021609512623399496 BETTER
I0328 03:50:54.340402 2131855 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.00019328684720676392 old loss 0.00019449392857495695 BETTER
I0328 03:51:16.293555 2132639 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.00021244336676318198 old loss 0.00021413729700725526 BETTER
I0328 03:51:24.168312 2131855 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.0001921588700497523 old loss 0.00019328684720676392 BETTER
I0328 03:51:24.527070 2130334 finetune.py:45] layer 12_down initial loss 0.00021804637799505144
I0328 03:51:37.834815 2131120 finetune.py:45] layer 13_down initial loss 0.0002553142549004406
I0328 03:51:45.635643 2132639 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.00021090720838401467 old loss 0.00021244336676318198 BETTER
I0328 03:51:52.373691 2130334 finetune.py:68] layer 12_down @ epoch 0 new loss 0.00021802382252644747 old loss 0.00021804637799505144 BETTER
I0328 03:51:54.376863 2131855 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.00019110568973701447 old loss 0.0001921588700497523 BETTER
I0328 03:52:04.358147 2131120 finetune.py:68] layer 13_down @ epoch 0 new loss 0.00025528183323331177 old loss 0.0002553142549004406 BETTER
I0328 03:52:15.103188 2132639 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.00020948934252373874 old loss 0.00021090720838401467 BETTER
I0328 03:52:20.956021 2130334 finetune.py:68] layer 12_down @ epoch 1 new loss 0.00021800232934765518 old loss 0.00021802382252644747 BETTER
I0328 03:52:31.620728 2131120 finetune.py:68] layer 13_down @ epoch 1 new loss 0.00025525284581817687 old loss 0.00025528183323331177 BETTER
I0328 03:52:44.376180 2132639 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.00020817056065425277 old loss 0.00020948934252373874 BETTER
I0328 03:52:50.113433 2130334 finetune.py:68] layer 12_down @ epoch 2 new loss 0.00021798390662297606 old loss 0.00021800232934765518 BETTER
I0328 03:52:50.761269 2131855 finetune.py:45] layer 14_down initial loss 0.0002941981074400246
I0328 03:52:59.340321 2131120 finetune.py:68] layer 13_down @ epoch 2 new loss 0.0002552263904362917 old loss 0.00025525284581817687 BETTER
I0328 03:53:17.220798 2131855 finetune.py:68] layer 14_down @ epoch 0 new loss 0.00029415611061267555 old loss 0.0002941981074400246 BETTER
I0328 03:53:19.244210 2130334 finetune.py:68] layer 12_down @ epoch 3 new loss 0.00021796772489324212 old loss 0.00021798390662297606 BETTER
I0328 03:53:27.080440 2131120 finetune.py:68] layer 13_down @ epoch 3 new loss 0.00025520226336084306 old loss 0.0002552263904362917 BETTER
I0328 03:53:41.460573 2132639 finetune.py:45] layer 15_down initial loss 0.00033910106867551804
I0328 03:53:44.961815 2131855 finetune.py:68] layer 14_down @ epoch 1 new loss 0.00029412086587399244 old loss 0.00029415611061267555 BETTER
I0328 03:53:48.522652 2130334 finetune.py:68] layer 12_down @ epoch 4 new loss 0.00021795163047499955 old loss 0.00021796772489324212 BETTER
12_v proxy err 0.033308640122413635 tr(WHW.T) 363.6233825683594
bpp_loss 2.3131977725133765
12_q proxy err 0.0016240054974332452 tr(WHW.T) 34136.98046875
bpp_loss 3.0257772127515636
12_k proxy err 0.0006869726930744946 tr(WHW.T) 23019.1640625
bpp_loss 3.897635843779426
12_o proxy err 0.03769204020500183 tr(WHW.T) 787.201416015625
bpp_loss 2.383161870355252
12_up proxy err 0.02322954498231411 tr(WHW.T) 10016.7109375
bpp_loss 2.489970310823992
12_gate proxy err 0.006496751680970192 tr(WHW.T) 37203.6953125
bpp_loss 2.68324510234275
12_down proxy err 0.0312790647149086 tr(WHW.T) 6860.65771484375
bpp_loss 2.485279814207128
I0328 03:53:56.095890 2131120 finetune.py:68] layer 13_down @ epoch 4 new loss 0.0002551807265263051 old loss 0.00025520226336084306 BETTER
13_v proxy err 0.040389616042375565 tr(WHW.T) 280.153564453125
bpp_loss 2.2545598953438457
13_q proxy err 0.002495128894224763 tr(WHW.T) 20905.201171875
bpp_loss 2.9980429923161864
13_k proxy err 0.0008434057235717773 tr(WHW.T) 17767.341796875
bpp_loss 3.9100751407677308
13_o proxy err 0.03976882994174957 tr(WHW.T) 680.4962158203125
bpp_loss 2.364981999853626
13_up proxy err 0.022794101387262344 tr(WHW.T) 10001.880859375
bpp_loss 2.4938623149147525
13_gate proxy err 0.006100846454501152 tr(WHW.T) 38811.82421875
bpp_loss 2.688235371972301
13_down proxy err 0.031982798129320145 tr(WHW.T) 6597.2333984375
bpp_loss 2.485074356314726
I0328 03:54:08.231438 2132639 finetune.py:68] layer 15_down @ epoch 0 new loss 0.00033905351301655173 old loss 0.00033910106867551804 BETTER
I0328 03:54:13.777782 2131855 finetune.py:68] layer 14_down @ epoch 2 new loss 0.00029408850241452456 old loss 0.00029412086587399244 BETTER
I0328 03:54:35.214462 2132639 finetune.py:68] layer 15_down @ epoch 1 new loss 0.0003390111669432372 old loss 0.00033905351301655173 BETTER
I0328 03:54:41.542823 2131855 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0002940571866929531 old loss 0.00029408850241452456 BETTER
I0328 03:55:00.666489 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 59.9521541595459s
I0328 03:55:02.323458 2132639 finetune.py:68] layer 15_down @ epoch 2 new loss 0.00033897452522069216 old loss 0.0003390111669432372 BETTER
I0328 03:55:04.516575 2145392 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:55:04.516681 2145392 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:55:04.516724 2145392 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:55:04.864565 2145392 config.py:54] PyTorch version 2.6.0 available.
W0328 03:55:05.079576 2145392 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:55:05.753563 2145392 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:55:05.757366 2083830 quantize_finetune_llama.py:209] layer 17 gpu 1
I0328 03:55:05.770646 2145392 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 03:55:09.391485 2131855 finetune.py:68] layer 14_down @ epoch 4 new loss 0.0002940320409834385 old loss 0.0002940571866929531 BETTER
14_v proxy err 0.03792686387896538 tr(WHW.T) 281.3382873535156
bpp_loss 2.2465839897631668
14_q proxy err 0.0023537736851722 tr(WHW.T) 20892.505859375
bpp_loss 2.9695534571656026
14_k proxy err 0.00075995281804353 tr(WHW.T) 18575.197265625
bpp_loss 3.8603539697942324
14_o proxy err 0.04239882901310921 tr(WHW.T) 693.2098999023438
bpp_loss 2.3560074607958086
14_up proxy err 0.024751029908657074 tr(WHW.T) 9163.92578125
bpp_loss 2.4879752907768955
14_gate proxy err 0.005667989142239094 tr(WHW.T) 41811.37890625
bpp_loss 2.7162330596308624
14_down proxy err 0.0337691493332386 tr(WHW.T) 6431.61669921875
bpp_loss 2.4811841247969175
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:55:22.596203 2145392 finetune.py:45] layer 16_v initial loss 7.452743011526763e-05
W0328 03:55:22.596421 2145392 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:55:29.576380 2132639 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0003389425401110202 old loss 0.00033897452522069216 BETTER
I0328 03:55:57.033669 2132639 finetune.py:68] layer 15_down @ epoch 4 new loss 0.00033891346538439393 old loss 0.0003389425401110202 BETTER
I0328 03:55:57.975414 2145392 finetune.py:68] layer 16_v @ epoch 0 new loss 4.565418930724263e-05 old loss 7.452743011526763e-05 BETTER
15_v proxy err 0.042010027915239334 tr(WHW.T) 284.0271301269531
bpp_loss 2.307210756436689
15_q proxy err 0.0019657863304018974 tr(WHW.T) 28088.419921875
bpp_loss 3.0911768993828446
15_k proxy err 0.0008280075271613896 tr(WHW.T) 18830.220703125
bpp_loss 3.906170212721918
15_o proxy err 0.0435652919113636 tr(WHW.T) 831.9530639648438
bpp_loss 2.3853225603234023
15_up proxy err 0.02521238848567009 tr(WHW.T) 8990.365234375
bpp_loss 2.481347732112876
15_gate proxy err 0.005197233520448208 tr(WHW.T) 45950.0625
bpp_loss 2.752486956638417
15_down proxy err 0.03388982638716698 tr(WHW.T) 6450.044921875
bpp_loss 2.476203219898577
I0328 03:56:16.620813 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 62.15595364570618s
I0328 03:56:20.247766 2146227 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:56:20.247866 2146227 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:56:20.247904 2146227 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:56:20.592602 2146227 config.py:54] PyTorch version 2.6.0 available.
W0328 03:56:20.780918 2146227 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:56:21.394946 2146227 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:56:21.398943 2083830 quantize_finetune_llama.py:209] layer 18 gpu 2
I0328 03:56:21.413206 2146227 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:56:34.880704 2145392 finetune.py:68] layer 16_v @ epoch 1 new loss 4.2565461626509205e-05 old loss 4.565418930724263e-05 BETTER
I0328 03:56:38.170659 2146227 finetune.py:45] layer 17_v initial loss 7.692787039559335e-05
W0328 03:56:38.170898 2146227 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:57:11.631696 2146227 finetune.py:68] layer 17_v @ epoch 0 new loss 4.20399010181427e-05 old loss 7.692787039559335e-05 BETTER
I0328 03:57:11.937043 2145392 finetune.py:68] layer 16_v @ epoch 2 new loss 4.088833156856708e-05 old loss 4.2565461626509205e-05 BETTER
I0328 03:57:24.176222 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 62.317697286605835s
I0328 03:57:27.933813 2146971 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:57:27.933928 2146971 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:57:27.933974 2146971 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:57:28.299496 2146971 config.py:54] PyTorch version 2.6.0 available.
W0328 03:57:28.520977 2146971 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:57:29.178218 2146971 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:57:29.181895 2083830 quantize_finetune_llama.py:209] layer 19 gpu 3
I0328 03:57:29.196104 2146971 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:57:46.328262 2146227 finetune.py:68] layer 17_v @ epoch 1 new loss 3.879187352140434e-05 old loss 4.20399010181427e-05 BETTER
I0328 03:57:46.451659 2146971 finetune.py:45] layer 18_v initial loss 9.473829413764179e-05
W0328 03:57:46.451883 2146971 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:57:48.998460 2145392 finetune.py:68] layer 16_v @ epoch 3 new loss 3.9712482248432934e-05 old loss 4.088833156856708e-05 BETTER
I0328 03:58:20.089075 2146971 finetune.py:68] layer 18_v @ epoch 0 new loss 3.502856634440832e-05 old loss 9.473829413764179e-05 BETTER
I0328 03:58:21.527325 2146227 finetune.py:68] layer 17_v @ epoch 2 new loss 3.721717439475469e-05 old loss 3.879187352140434e-05 BETTER
I0328 03:58:26.264041 2145392 finetune.py:68] layer 16_v @ epoch 4 new loss 3.881310476572253e-05 old loss 3.9712482248432934e-05 BETTER
I0328 03:58:33.678987 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 64.0202248096466s
I0328 03:58:37.428174 2147758 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 03:58:37.428281 2147758 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 03:58:37.428324 2147758 utils.py:162] NumExpr defaulting to 16 threads.
I0328 03:58:37.805678 2147758 config.py:54] PyTorch version 2.6.0 available.
W0328 03:58:38.016859 2147758 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 03:58:38.661419 2147758 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 03:58:38.665327 2083830 quantize_finetune_llama.py:209] layer 20 gpu 0
I0328 03:58:38.679711 2147758 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 03:58:45.645881 2145392 finetune.py:45] layer 16_q initial loss 4.522742165136151e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 03:58:54.902231 2146971 finetune.py:68] layer 18_v @ epoch 1 new loss 3.1607356504537165e-05 old loss 3.502856634440832e-05 BETTER
I0328 03:58:56.074795 2147758 finetune.py:45] layer 19_v initial loss 0.00010667350579751655
W0328 03:58:56.075022 2147758 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 03:58:56.652947 2146227 finetune.py:68] layer 17_v @ epoch 3 new loss 3.620184361352585e-05 old loss 3.721717439475469e-05 BETTER
I0328 03:59:21.137185 2145392 finetune.py:68] layer 16_q @ epoch 0 new loss 4.403577622724697e-05 old loss 4.522742165136151e-05 BETTER
I0328 03:59:29.156542 2147758 finetune.py:68] layer 19_v @ epoch 0 new loss 3.6441826523514464e-05 old loss 0.00010667350579751655 BETTER
I0328 03:59:30.321782 2146971 finetune.py:68] layer 18_v @ epoch 2 new loss 3.0148215955705382e-05 old loss 3.1607356504537165e-05 BETTER
I0328 03:59:32.047642 2146227 finetune.py:68] layer 17_v @ epoch 4 new loss 3.543922503013164e-05 old loss 3.620184361352585e-05 BETTER
I0328 03:59:51.461698 2146227 finetune.py:45] layer 17_q initial loss 4.097591227036901e-05
I0328 03:59:57.614792 2145392 finetune.py:68] layer 16_q @ epoch 1 new loss 4.318711580708623e-05 old loss 4.403577622724697e-05 BETTER
I0328 04:00:03.607772 2147758 finetune.py:68] layer 19_v @ epoch 1 new loss 3.262365498812869e-05 old loss 3.6441826523514464e-05 BETTER
I0328 04:00:05.477223 2146971 finetune.py:68] layer 18_v @ epoch 3 new loss 2.9269902370288037e-05 old loss 3.0148215955705382e-05 BETTER
I0328 04:00:25.068755 2146227 finetune.py:68] layer 17_q @ epoch 0 new loss 4.002727655461058e-05 old loss 4.097591227036901e-05 BETTER
I0328 04:00:34.461831 2145392 finetune.py:68] layer 16_q @ epoch 2 new loss 4.251358041074127e-05 old loss 4.318711580708623e-05 BETTER
I0328 04:00:38.124227 2147758 finetune.py:68] layer 19_v @ epoch 2 new loss 3.113900311291218e-05 old loss 3.262365498812869e-05 BETTER
I0328 04:00:40.672614 2146971 finetune.py:68] layer 18_v @ epoch 4 new loss 2.8641765311476775e-05 old loss 2.9269902370288037e-05 BETTER
I0328 04:00:59.628625 2146227 finetune.py:68] layer 17_q @ epoch 1 new loss 3.9342790842056274e-05 old loss 4.002727655461058e-05 BETTER
I0328 04:00:59.882012 2146971 finetune.py:45] layer 18_q initial loss 3.425732211326249e-05
I0328 04:01:11.545074 2145392 finetune.py:68] layer 16_q @ epoch 3 new loss 4.192315827822313e-05 old loss 4.251358041074127e-05 BETTER
I0328 04:01:12.893753 2147758 finetune.py:68] layer 19_v @ epoch 3 new loss 3.0249015253502876e-05 old loss 3.113900311291218e-05 BETTER
I0328 04:01:33.610959 2146971 finetune.py:68] layer 18_q @ epoch 0 new loss 3.333915810799226e-05 old loss 3.425732211326249e-05 BETTER
I0328 04:01:34.444436 2146227 finetune.py:68] layer 17_q @ epoch 2 new loss 3.8788992242189124e-05 old loss 3.9342790842056274e-05 BETTER
I0328 04:01:47.835211 2147758 finetune.py:68] layer 19_v @ epoch 4 new loss 2.96640646411106e-05 old loss 3.0249015253502876e-05 BETTER
I0328 04:01:48.654120 2145392 finetune.py:68] layer 16_q @ epoch 4 new loss 4.143625483266078e-05 old loss 4.192315827822313e-05 BETTER
I0328 04:02:06.258366 2145392 finetune.py:45] layer 16_k initial loss 4.3684380216291174e-05
I0328 04:02:07.446440 2147758 finetune.py:45] layer 19_q initial loss 3.451954398769885e-05
I0328 04:02:08.641042 2146971 finetune.py:68] layer 18_q @ epoch 1 new loss 3.275245398981497e-05 old loss 3.333915810799226e-05 BETTER
I0328 04:02:09.502863 2146227 finetune.py:68] layer 17_q @ epoch 3 new loss 3.833687151200138e-05 old loss 3.8788992242189124e-05 BETTER
I0328 04:02:40.880800 2147758 finetune.py:68] layer 19_q @ epoch 0 new loss 3.362305506016128e-05 old loss 3.451954398769885e-05 BETTER
I0328 04:02:42.288412 2145392 finetune.py:68] layer 16_k @ epoch 0 new loss 4.305259062675759e-05 old loss 4.3684380216291174e-05 BETTER
I0328 04:02:44.042407 2146971 finetune.py:68] layer 18_q @ epoch 2 new loss 3.2296316931024194e-05 old loss 3.275245398981497e-05 BETTER
I0328 04:02:44.473188 2146227 finetune.py:68] layer 17_q @ epoch 4 new loss 3.792208372033201e-05 old loss 3.833687151200138e-05 BETTER
I0328 04:03:02.189683 2146227 finetune.py:45] layer 17_k initial loss 3.999087857664563e-05
I0328 04:03:15.295381 2147758 finetune.py:68] layer 19_q @ epoch 1 new loss 3.306946746306494e-05 old loss 3.362305506016128e-05 BETTER
I0328 04:03:19.157503 2145392 finetune.py:68] layer 16_k @ epoch 1 new loss 4.263480150257237e-05 old loss 4.305259062675759e-05 BETTER
I0328 04:03:19.182173 2146971 finetune.py:68] layer 18_q @ epoch 3 new loss 3.1936720915837213e-05 old loss 3.2296316931024194e-05 BETTER
I0328 04:03:35.749672 2146227 finetune.py:68] layer 17_k @ epoch 0 new loss 3.954830390284769e-05 old loss 3.999087857664563e-05 BETTER
I0328 04:03:49.509156 2147758 finetune.py:68] layer 19_q @ epoch 2 new loss 3.265244959038682e-05 old loss 3.306946746306494e-05 BETTER
I0328 04:03:54.188923 2146971 finetune.py:68] layer 18_q @ epoch 4 new loss 3.161088170600124e-05 old loss 3.1936720915837213e-05 BETTER
I0328 04:03:56.117401 2145392 finetune.py:68] layer 16_k @ epoch 2 new loss 4.228524267091416e-05 old loss 4.263480150257237e-05 BETTER
I0328 04:04:10.239120 2146227 finetune.py:68] layer 17_k @ epoch 1 new loss 3.9216323784785345e-05 old loss 3.954830390284769e-05 BETTER
I0328 04:04:12.136598 2146971 finetune.py:45] layer 18_k initial loss 3.3237192837987095e-05
I0328 04:04:23.832764 2147758 finetune.py:68] layer 19_q @ epoch 3 new loss 3.2275489502353594e-05 old loss 3.265244959038682e-05 BETTER
I0328 04:04:33.215445 2145392 finetune.py:68] layer 16_k @ epoch 3 new loss 4.1967603465309367e-05 old loss 4.228524267091416e-05 BETTER
I0328 04:04:44.897566 2146227 finetune.py:68] layer 17_k @ epoch 2 new loss 3.894254405167885e-05 old loss 3.9216323784785345e-05 BETTER
I0328 04:04:45.769471 2146971 finetune.py:68] layer 18_k @ epoch 0 new loss 3.293927147751674e-05 old loss 3.3237192837987095e-05 BETTER
I0328 04:04:58.048782 2147758 finetune.py:68] layer 19_q @ epoch 4 new loss 3.1976858736015856e-05 old loss 3.2275489502353594e-05 BETTER
I0328 04:05:10.170361 2145392 finetune.py:68] layer 16_k @ epoch 4 new loss 4.167329825577326e-05 old loss 4.1967603465309367e-05 BETTER
I0328 04:05:15.892714 2147758 finetune.py:45] layer 19_k initial loss 3.3836586226243526e-05
I0328 04:05:19.557932 2146227 finetune.py:68] layer 17_k @ epoch 3 new loss 3.869244028464891e-05 old loss 3.894254405167885e-05 BETTER
I0328 04:05:20.336060 2146971 finetune.py:68] layer 18_k @ epoch 1 new loss 3.2651860237820074e-05 old loss 3.293927147751674e-05 BETTER
I0328 04:05:29.872818 2145392 finetune.py:45] layer 16_o initial loss 9.769170719664544e-05
I0328 04:05:48.909583 2147758 finetune.py:68] layer 19_k @ epoch 0 new loss 3.350746555952355e-05 old loss 3.3836586226243526e-05 BETTER
I0328 04:05:54.166710 2146227 finetune.py:68] layer 17_k @ epoch 4 new loss 3.848252890747972e-05 old loss 3.869244028464891e-05 BETTER
I0328 04:05:55.219523 2146971 finetune.py:68] layer 18_k @ epoch 2 new loss 3.241993181291036e-05 old loss 3.2651860237820074e-05 BETTER
I0328 04:06:04.757950 2145392 finetune.py:68] layer 16_o @ epoch 0 new loss 9.396098903380334e-05 old loss 9.769170719664544e-05 BETTER
I0328 04:06:13.769257 2146227 finetune.py:45] layer 17_o initial loss 9.296576172346249e-05
I0328 04:06:22.862492 2147758 finetune.py:68] layer 19_k @ epoch 1 new loss 3.327292142785154e-05 old loss 3.350746555952355e-05 BETTER
I0328 04:06:29.974333 2146971 finetune.py:68] layer 18_k @ epoch 3 new loss 3.2228541385848075e-05 old loss 3.241993181291036e-05 BETTER
I0328 04:06:40.692769 2145392 finetune.py:68] layer 16_o @ epoch 1 new loss 9.228727139998227e-05 old loss 9.396098903380334e-05 BETTER
I0328 04:06:46.601540 2146227 finetune.py:68] layer 17_o @ epoch 0 new loss 8.895409700926393e-05 old loss 9.296576172346249e-05 BETTER
I0328 04:06:56.888037 2147758 finetune.py:68] layer 19_k @ epoch 2 new loss 3.307284350739792e-05 old loss 3.327292142785154e-05 BETTER
I0328 04:07:05.045130 2146971 finetune.py:68] layer 18_k @ epoch 4 new loss 3.2042120437836275e-05 old loss 3.2228541385848075e-05 BETTER
I0328 04:07:16.816419 2145392 finetune.py:68] layer 16_o @ epoch 2 new loss 9.102156036533415e-05 old loss 9.228727139998227e-05 BETTER
I0328 04:07:20.585530 2146227 finetune.py:68] layer 17_o @ epoch 1 new loss 8.733214053791016e-05 old loss 8.895409700926393e-05 BETTER
I0328 04:07:24.359375 2146971 finetune.py:45] layer 18_o initial loss 7.790659583406523e-05
I0328 04:07:31.230637 2147758 finetune.py:68] layer 19_k @ epoch 3 new loss 3.288654988864437e-05 old loss 3.307284350739792e-05 BETTER
I0328 04:07:53.045032 2145392 finetune.py:68] layer 16_o @ epoch 3 new loss 8.995739335659891e-05 old loss 9.102156036533415e-05 BETTER
I0328 04:07:54.624718 2146227 finetune.py:68] layer 17_o @ epoch 2 new loss 8.612342935521156e-05 old loss 8.733214053791016e-05 BETTER
I0328 04:07:57.404810 2146971 finetune.py:68] layer 18_o @ epoch 0 new loss 7.276060205185786e-05 old loss 7.790659583406523e-05 BETTER
I0328 04:08:05.421776 2147758 finetune.py:68] layer 19_k @ epoch 4 new loss 3.273121183156036e-05 old loss 3.288654988864437e-05 BETTER
I0328 04:08:24.828783 2147758 finetune.py:45] layer 19_o initial loss 7.424032082781196e-05
I0328 04:08:28.660959 2146227 finetune.py:68] layer 17_o @ epoch 3 new loss 8.514354703947902e-05 old loss 8.612342935521156e-05 BETTER
I0328 04:08:29.195851 2145392 finetune.py:68] layer 16_o @ epoch 4 new loss 8.906153379939497e-05 old loss 8.995739335659891e-05 BETTER
I0328 04:08:31.448490 2146971 finetune.py:68] layer 18_o @ epoch 1 new loss 7.159036613302305e-05 old loss 7.276060205185786e-05 BETTER
I0328 04:08:57.201097 2147758 finetune.py:68] layer 19_o @ epoch 0 new loss 6.962392217246816e-05 old loss 7.424032082781196e-05 BETTER
I0328 04:09:00.615005 2145392 finetune.py:45] layer 16_up initial loss 0.00020233760005794466
I0328 04:09:02.797033 2146227 finetune.py:68] layer 17_o @ epoch 4 new loss 8.429656008956954e-05 old loss 8.514354703947902e-05 BETTER
I0328 04:09:05.463644 2146971 finetune.py:68] layer 18_o @ epoch 2 new loss 7.077767077134922e-05 old loss 7.159036613302305e-05 BETTER
I0328 04:09:30.661210 2147758 finetune.py:68] layer 19_o @ epoch 1 new loss 6.85923732817173e-05 old loss 6.962392217246816e-05 BETTER
I0328 04:09:32.704738 2145392 finetune.py:68] layer 16_up @ epoch 0 new loss 0.00019880653417203575 old loss 0.00020233760005794466 BETTER
I0328 04:09:34.322051 2146227 finetune.py:45] layer 17_up initial loss 0.0002128483320120722
I0328 04:09:39.761469 2146971 finetune.py:68] layer 18_o @ epoch 3 new loss 7.012017158558592e-05 old loss 7.077767077134922e-05 BETTER
I0328 04:10:04.422514 2147758 finetune.py:68] layer 19_o @ epoch 2 new loss 6.788678729208186e-05 old loss 6.85923732817173e-05 BETTER
I0328 04:10:05.134531 2146227 finetune.py:68] layer 17_up @ epoch 0 new loss 0.00020898779621347785 old loss 0.0002128483320120722 BETTER
I0328 04:10:06.126690 2145392 finetune.py:68] layer 16_up @ epoch 1 new loss 0.00019622988475020975 old loss 0.00019880653417203575 BETTER
I0328 04:10:13.966547 2146971 finetune.py:68] layer 18_o @ epoch 4 new loss 6.959478923818097e-05 old loss 7.012017158558592e-05 BETTER
I0328 04:10:36.865419 2146227 finetune.py:68] layer 17_up @ epoch 1 new loss 0.00020616932306438684 old loss 0.00020898779621347785 BETTER
I0328 04:10:37.841321 2147758 finetune.py:68] layer 19_o @ epoch 3 new loss 6.730607856297866e-05 old loss 6.788678729208186e-05 BETTER
I0328 04:10:39.802824 2145392 finetune.py:68] layer 16_up @ epoch 2 new loss 0.00019400073506403714 old loss 0.00019622988475020975 BETTER
I0328 04:10:45.332395 2146971 finetune.py:45] layer 18_up initial loss 0.00020239684090483934
I0328 04:11:08.967967 2146227 finetune.py:68] layer 17_up @ epoch 2 new loss 0.00020379028865136206 old loss 0.00020616932306438684 BETTER
I0328 04:11:11.435014 2147758 finetune.py:68] layer 19_o @ epoch 4 new loss 6.683372339466587e-05 old loss 6.730607856297866e-05 BETTER
I0328 04:11:13.585005 2145392 finetune.py:68] layer 16_up @ epoch 3 new loss 0.00019202266412321478 old loss 0.00019400073506403714 BETTER
I0328 04:11:16.168292 2146971 finetune.py:68] layer 18_up @ epoch 0 new loss 0.00019883459026459605 old loss 0.00020239684090483934 BETTER
I0328 04:11:41.204707 2146227 finetune.py:68] layer 17_up @ epoch 3 new loss 0.00020167659386061132 old loss 0.00020379028865136206 BETTER
I0328 04:11:42.900569 2147758 finetune.py:45] layer 19_up initial loss 0.0002107915497617796
I0328 04:11:47.416830 2145392 finetune.py:68] layer 16_up @ epoch 4 new loss 0.0001902156654978171 old loss 0.00019202266412321478 BETTER
I0328 04:11:48.333584 2146971 finetune.py:68] layer 18_up @ epoch 1 new loss 0.00019633944611996412 old loss 0.00019883459026459605 BETTER
I0328 04:12:13.183740 2147758 finetune.py:68] layer 19_up @ epoch 0 new loss 0.00020728624076582491 old loss 0.0002107915497617796 BETTER
I0328 04:12:13.516546 2146227 finetune.py:68] layer 17_up @ epoch 4 new loss 0.00019976828480139375 old loss 0.00020167659386061132 BETTER
I0328 04:12:18.938463 2145392 finetune.py:45] layer 16_gate initial loss 0.00022960660862736404
I0328 04:12:20.563197 2146971 finetune.py:68] layer 18_up @ epoch 2 new loss 0.0001942636154126376 old loss 0.00019633944611996412 BETTER
I0328 04:12:44.810269 2146227 finetune.py:45] layer 17_gate initial loss 0.0002452255575917661
I0328 04:12:44.821651 2147758 finetune.py:68] layer 19_up @ epoch 1 new loss 0.00020481123647186905 old loss 0.00020728624076582491 BETTER
I0328 04:12:49.189404 2145392 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.00022768182680010796 old loss 0.00022960660862736404 BETTER
I0328 04:12:52.927929 2146971 finetune.py:68] layer 18_up @ epoch 3 new loss 0.0001923983945744112 old loss 0.0001942636154126376 BETTER
I0328 04:13:13.355307 2146227 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0002430758613627404 old loss 0.0002452255575917661 BETTER
I0328 04:13:16.304162 2147758 finetune.py:68] layer 19_up @ epoch 2 new loss 0.00020271085668355227 old loss 0.00020481123647186905 BETTER
I0328 04:13:20.477514 2145392 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0002260320179630071 old loss 0.00022768182680010796 BETTER
I0328 04:13:25.201051 2146971 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0001907416881294921 old loss 0.0001923983945744112 BETTER
I0328 04:13:42.886620 2146227 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0002412938338238746 old loss 0.0002430758613627404 BETTER
I0328 04:13:47.882189 2147758 finetune.py:68] layer 19_up @ epoch 3 new loss 0.00020087613665964454 old loss 0.00020271085668355227 BETTER
I0328 04:13:51.925888 2145392 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.00022452077246271074 old loss 0.0002260320179630071 BETTER
I0328 04:13:56.771937 2146971 finetune.py:45] layer 18_gate initial loss 0.00023984871222637594
I0328 04:14:12.628472 2146227 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.00023966871958691627 old loss 0.0002412938338238746 BETTER
I0328 04:14:19.554602 2147758 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0001992212637560442 old loss 0.00020087613665964454 BETTER
I0328 04:14:23.554858 2145392 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.00022312426881399006 old loss 0.00022452077246271074 BETTER
I0328 04:14:25.404577 2146971 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0002379641227889806 old loss 0.00023984871222637594 BETTER
I0328 04:14:42.318710 2146227 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.00023815396707504988 old loss 0.00023966871958691627 BETTER
I0328 04:14:51.069833 2147758 finetune.py:45] layer 19_gate initial loss 0.0002529210178181529
I0328 04:14:55.122714 2146971 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.00023641045845579356 old loss 0.0002379641227889806 BETTER
I0328 04:14:55.182883 2145392 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.000221832116949372 old loss 0.00022312426881399006 BETTER
I0328 04:15:12.103299 2146227 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.00023676268756389618 old loss 0.00023815396707504988 BETTER
I0328 04:15:19.287330 2147758 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.00025107068358920515 old loss 0.0002529210178181529 BETTER
I0328 04:15:25.052796 2146971 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.00023498720838688314 old loss 0.00023641045845579356 BETTER
I0328 04:15:48.360083 2147758 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0002495297812856734 old loss 0.00025107068358920515 BETTER
I0328 04:15:50.534657 2145392 finetune.py:45] layer 16_down initial loss 0.00036653963616117835
I0328 04:15:54.939311 2146971 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.00023369201517198235 old loss 0.00023498720838688314 BETTER
I0328 04:16:08.512714 2146227 finetune.py:45] layer 17_down initial loss 0.00041270864312537014
I0328 04:16:17.788308 2147758 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0002480950206518173 old loss 0.0002495297812856734 BETTER
I0328 04:16:17.986176 2145392 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0003664828836917877 old loss 0.00036653963616117835 BETTER
I0328 04:16:24.852150 2146971 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.00023247295757755637 old loss 0.00023369201517198235 BETTER
I0328 04:16:34.717123 2146227 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0004126447602175176 old loss 0.00041270864312537014 BETTER
I0328 04:16:46.423279 2145392 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0003664327959995717 old loss 0.0003664828836917877 BETTER
I0328 04:16:47.081439 2147758 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0002468008315190673 old loss 0.0002480950206518173 BETTER
I0328 04:17:01.903751 2146227 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0004125894629396498 old loss 0.0004126447602175176 BETTER
I0328 04:17:15.540294 2145392 finetune.py:68] layer 16_down @ epoch 2 new loss 0.00036638975143432617 old loss 0.0003664327959995717 BETTER
I0328 04:17:16.433915 2147758 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.0002456038200762123 old loss 0.0002468008315190673 BETTER
I0328 04:17:21.881198 2146971 finetune.py:45] layer 18_down initial loss 0.0004108836001250893
I0328 04:17:29.420187 2146227 finetune.py:68] layer 17_down @ epoch 2 new loss 0.00041254397365264595 old loss 0.0004125894629396498 BETTER
I0328 04:17:44.496796 2145392 finetune.py:68] layer 16_down @ epoch 3 new loss 0.00036635296419262886 old loss 0.00036638975143432617 BETTER
I0328 04:17:48.303069 2146971 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0004108297871425748 old loss 0.0004108836001250893 BETTER
I0328 04:17:57.091652 2146227 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0004125028208363801 old loss 0.00041254397365264595 BETTER
I0328 04:18:14.291160 2145392 finetune.py:68] layer 16_down @ epoch 4 new loss 0.00036631972761824727 old loss 0.00036635296419262886 BETTER
I0328 04:18:14.410632 2147758 finetune.py:45] layer 19_down initial loss 0.0004330227675382048
16_v proxy err 0.03920785337686539 tr(WHW.T) 274.28167724609375
bpp_loss 2.265575544181047
16_q proxy err 0.002045578323304653 tr(WHW.T) 24482.72265625
bpp_loss 3.070396671653725
16_k proxy err 0.0007280720164999366 tr(WHW.T) 19454.9765625
bpp_loss 3.896701754129026
16_o proxy err 0.036148060113191605 tr(WHW.T) 974.1946411132812
bpp_loss 2.368466547864955
16_up proxy err 0.02842905931174755 tr(WHW.T) 8332.5
bpp_loss 2.4690681601808007
16_gate proxy err 0.006146878004074097 tr(WHW.T) 41007.8203125
bpp_loss 2.7842365334342634
16_down proxy err 0.03541681542992592 tr(WHW.T) 6339.2744140625
bpp_loss 2.462171150554371
I0328 04:18:16.415280 2146971 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0004107818240299821 old loss 0.0004108297871425748 BETTER
I0328 04:18:26.064302 2146227 finetune.py:68] layer 17_down @ epoch 4 new loss 0.000412468274589628 old loss 0.0004125028208363801 BETTER
17_v proxy err 0.04308577999472618 tr(WHW.T) 283.9730224609375
bpp_loss 2.333179443521658
17_q proxy err 0.002031253883615136 tr(WHW.T) 27578.349609375
bpp_loss 3.0831496662576683
17_k proxy err 0.0009149773977696896 tr(WHW.T) 17386.86328125
bpp_loss 3.9234697929932736
17_o proxy err 0.03943587467074394 tr(WHW.T) 1110.9658203125
bpp_loss 2.3962794994586147
17_up proxy err 0.028058303520083427 tr(WHW.T) 8452.37890625
bpp_loss 2.467183110703315
17_gate proxy err 0.006107031833380461 tr(WHW.T) 41486.6171875
bpp_loss 2.7974877187849154
17_down proxy err 0.03592391312122345 tr(WHW.T) 6264.51318359375
bpp_loss 2.458421643623816
I0328 04:18:41.277526 2147758 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0004329698858782649 old loss 0.0004330227675382048 BETTER
I0328 04:18:45.129849 2146971 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0004107401764485985 old loss 0.0004107818240299821 BETTER
I0328 04:19:08.396667 2147758 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0004329255607444793 old loss 0.0004329698858782649 BETTER
I0328 04:19:12.956018 2146971 finetune.py:68] layer 18_down @ epoch 3 new loss 0.00041070336010307074 old loss 0.0004107401764485985 BETTER
I0328 04:19:35.530056 2147758 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0004328861250542104 old loss 0.0004329255607444793 BETTER
I0328 04:19:37.911550 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 67.16895890235901s
I0328 04:19:40.822721 2146971 finetune.py:68] layer 18_down @ epoch 4 new loss 0.00041067131678573787 old loss 0.00041070336010307074 BETTER
I0328 04:19:41.765941 2160516 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 04:19:41.766034 2160516 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 04:19:41.766071 2160516 utils.py:162] NumExpr defaulting to 16 threads.
I0328 04:19:42.147268 2160516 config.py:54] PyTorch version 2.6.0 available.
W0328 04:19:42.363667 2160516 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

18_v proxy err 0.042866382747888565 tr(WHW.T) 287.61376953125
bpp_loss 2.2604291163152084
18_q proxy err 0.0025723224971443415 tr(WHW.T) 22413.810546875
bpp_loss 3.082861586648505
18_k proxy err 0.0009487346978858113 tr(WHW.T) 17369.248046875
bpp_loss 3.993997448065784
18_o proxy err 0.038841165602207184 tr(WHW.T) 1209.2923583984375
bpp_loss 2.3768212952418253
18_up proxy err 0.030888080596923828 tr(WHW.T) 7987.53271484375
bpp_loss 2.4638103197461794
18_gate proxy err 0.007539520040154457 tr(WHW.T) 35061.25390625
bpp_loss 2.8053819527849555
18_down proxy err 0.03696588799357414 tr(WHW.T) 6282.1494140625
bpp_loss 2.457686773118829
W0328 04:19:42.959013 2160516 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 04:19:42.962791 2083830 quantize_finetune_llama.py:209] layer 21 gpu 1
I0328 04:19:42.976209 2160516 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 04:19:59.642195 2160516 finetune.py:45] layer 20_v initial loss 9.434831736143678e-05
W0328 04:19:59.642419 2160516 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 04:20:02.924197 2147758 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0004328511713538319 old loss 0.0004328861250542104 BETTER
I0328 04:20:30.386082 2147758 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0004328204377088696 old loss 0.0004328511713538319 BETTER
19_v proxy err 0.037694621831178665 tr(WHW.T) 341.0596618652344
bpp_loss 2.305162391421618
19_q proxy err 0.0024730232544243336 tr(WHW.T) 24049.32421875
bpp_loss 3.0859781794133596
19_k proxy err 0.0010797027498483658 tr(WHW.T) 15525.2490234375
bpp_loss 3.9068907176260836
19_o proxy err 0.04028953239321709 tr(WHW.T) 1172.16943359375
bpp_loss 2.3892146688885987
19_up proxy err 0.03275744989514351 tr(WHW.T) 7652.0224609375
bpp_loss 2.4604544674179385
19_gate proxy err 0.008236123248934746 tr(WHW.T) 32700.93359375
bpp_loss 2.815695292249854
19_down proxy err 0.03742072731256485 tr(WHW.T) 6231.53857421875
bpp_loss 2.4557602919604897
I0328 04:20:34.871984 2160516 finetune.py:68] layer 20_v @ epoch 0 new loss 3.7648220313712955e-05 old loss 9.434831736143678e-05 BETTER
I0328 04:20:48.708145 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 63.016571283340454s
I0328 04:20:52.382222 2161298 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 04:20:52.382327 2161298 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 04:20:52.382364 2161298 utils.py:162] NumExpr defaulting to 16 threads.
I0328 04:20:52.730807 2161298 config.py:54] PyTorch version 2.6.0 available.
W0328 04:20:52.928488 2161298 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 04:20:53.513525 2161298 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 04:20:53.517285 2083830 quantize_finetune_llama.py:209] layer 22 gpu 2
I0328 04:20:53.533139 2161298 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 04:21:10.697162 2161298 finetune.py:45] layer 21_v initial loss 0.00010272513463860378
W0328 04:21:10.697425 2161298 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 04:21:11.470437 2160516 finetune.py:68] layer 20_v @ epoch 1 new loss 3.44228174071759e-05 old loss 3.7648220313712955e-05 BETTER
I0328 04:21:43.873391 2161298 finetune.py:68] layer 21_v @ epoch 0 new loss 4.559812805382535e-05 old loss 0.00010272513463860378 BETTER
I0328 04:21:48.497078 2160516 finetune.py:68] layer 20_v @ epoch 2 new loss 3.310570536996238e-05 old loss 3.44228174071759e-05 BETTER
I0328 04:21:57.657660 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 63.696905851364136s
I0328 04:22:01.399252 2162030 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 04:22:01.399355 2162030 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 04:22:01.399393 2162030 utils.py:162] NumExpr defaulting to 16 threads.
I0328 04:22:01.746589 2162030 config.py:54] PyTorch version 2.6.0 available.
W0328 04:22:01.943339 2162030 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 04:22:02.530248 2162030 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 04:22:02.535398 2083830 quantize_finetune_llama.py:209] layer 23 gpu 3
I0328 04:22:02.549430 2162030 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 04:22:18.466372 2161298 finetune.py:68] layer 21_v @ epoch 1 new loss 4.16190450778231e-05 old loss 4.559812805382535e-05 BETTER
I0328 04:22:19.739856 2162030 finetune.py:45] layer 22_v initial loss 0.00012098587467335165
W0328 04:22:19.740065 2162030 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 04:22:25.746638 2160516 finetune.py:68] layer 20_v @ epoch 3 new loss 3.2243002351606265e-05 old loss 3.310570536996238e-05 BETTER
I0328 04:22:53.427671 2162030 finetune.py:68] layer 22_v @ epoch 0 new loss 3.841461148113012e-05 old loss 0.00012098587467335165 BETTER
I0328 04:22:53.438029 2161298 finetune.py:68] layer 21_v @ epoch 2 new loss 3.9827715227147564e-05 old loss 4.16190450778231e-05 BETTER
I0328 04:23:03.146461 2160516 finetune.py:68] layer 20_v @ epoch 4 new loss 3.162843859172426e-05 old loss 3.2243002351606265e-05 BETTER
I0328 04:23:08.094855 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 65.07423448562622s
I0328 04:23:11.865596 2162831 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 04:23:11.865877 2162831 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 04:23:11.865985 2162831 utils.py:162] NumExpr defaulting to 16 threads.
I0328 04:23:12.211486 2162831 config.py:54] PyTorch version 2.6.0 available.
W0328 04:23:12.428561 2162831 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 04:23:13.037392 2162831 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 04:23:13.041292 2083830 quantize_finetune_llama.py:209] layer 24 gpu 0
I0328 04:23:13.055714 2162831 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 04:23:22.421577 2160516 finetune.py:45] layer 20_q initial loss 3.726671275217086e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 04:23:28.809038 2162030 finetune.py:68] layer 22_v @ epoch 1 new loss 3.391328573343344e-05 old loss 3.841461148113012e-05 BETTER
I0328 04:23:28.952089 2161298 finetune.py:68] layer 21_v @ epoch 3 new loss 3.865955659421161e-05 old loss 3.9827715227147564e-05 BETTER
I0328 04:23:31.572871 2162831 finetune.py:45] layer 23_v initial loss 0.0001360687310807407
W0328 04:23:31.573077 2162831 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 04:23:57.910996 2160516 finetune.py:68] layer 20_q @ epoch 0 new loss 3.634148379205726e-05 old loss 3.726671275217086e-05 BETTER
I0328 04:24:04.541402 2162030 finetune.py:68] layer 22_v @ epoch 2 new loss 3.2288822694681585e-05 old loss 3.391328573343344e-05 BETTER
I0328 04:24:04.684208 2161298 finetune.py:68] layer 21_v @ epoch 4 new loss 3.77763390133623e-05 old loss 3.865955659421161e-05 BETTER
I0328 04:24:05.114293 2162831 finetune.py:68] layer 23_v @ epoch 0 new loss 3.9590129745192826e-05 old loss 0.0001360687310807407 BETTER
I0328 04:24:24.362997 2161298 finetune.py:45] layer 21_q initial loss 4.667082612286322e-05
I0328 04:24:34.246047 2160516 finetune.py:68] layer 20_q @ epoch 1 new loss 3.570655098883435e-05 old loss 3.634148379205726e-05 BETTER
I0328 04:24:39.510869 2162831 finetune.py:68] layer 23_v @ epoch 1 new loss 3.4203301765955985e-05 old loss 3.9590129745192826e-05 BETTER
I0328 04:24:39.805421 2162030 finetune.py:68] layer 22_v @ epoch 3 new loss 3.133436621283181e-05 old loss 3.2288822694681585e-05 BETTER
I0328 04:24:57.820156 2161298 finetune.py:68] layer 21_q @ epoch 0 new loss 4.5283955842023715e-05 old loss 4.667082612286322e-05 BETTER
I0328 04:25:10.707848 2160516 finetune.py:68] layer 20_q @ epoch 2 new loss 3.5216216929256916e-05 old loss 3.570655098883435e-05 BETTER
I0328 04:25:14.238417 2162831 finetune.py:68] layer 23_v @ epoch 2 new loss 3.245993866585195e-05 old loss 3.4203301765955985e-05 BETTER
I0328 04:25:15.112339 2162030 finetune.py:68] layer 22_v @ epoch 4 new loss 3.068110891035758e-05 old loss 3.133436621283181e-05 BETTER
I0328 04:25:32.059964 2161298 finetune.py:68] layer 21_q @ epoch 1 new loss 4.436352173797786e-05 old loss 4.5283955842023715e-05 BETTER
I0328 04:25:34.655433 2162030 finetune.py:45] layer 22_q initial loss 3.8195201341295615e-05
I0328 04:25:47.354637 2160516 finetune.py:68] layer 20_q @ epoch 3 new loss 3.479680162854493e-05 old loss 3.5216216929256916e-05 BETTER
I0328 04:25:48.900977 2162831 finetune.py:68] layer 23_v @ epoch 3 new loss 3.150714474031702e-05 old loss 3.245993866585195e-05 BETTER
I0328 04:26:06.459701 2161298 finetune.py:68] layer 21_q @ epoch 2 new loss 4.363029802334495e-05 old loss 4.436352173797786e-05 BETTER
I0328 04:26:08.370524 2162030 finetune.py:68] layer 22_q @ epoch 0 new loss 3.712229590746574e-05 old loss 3.8195201341295615e-05 BETTER
I0328 04:26:23.840447 2162831 finetune.py:68] layer 23_v @ epoch 4 new loss 3.0848095775581896e-05 old loss 3.150714474031702e-05 BETTER
I0328 04:26:23.880988 2160516 finetune.py:68] layer 20_q @ epoch 4 new loss 3.44613945344463e-05 old loss 3.479680162854493e-05 BETTER
I0328 04:26:41.831544 2161298 finetune.py:68] layer 21_q @ epoch 3 new loss 4.298190833651461e-05 old loss 4.363029802334495e-05 BETTER
I0328 04:26:42.367959 2160516 finetune.py:45] layer 20_k initial loss 3.645961987785995e-05
I0328 04:26:43.863799 2162030 finetune.py:68] layer 22_q @ epoch 1 new loss 3.645467586466111e-05 old loss 3.712229590746574e-05 BETTER
I0328 04:26:44.433994 2162831 finetune.py:45] layer 23_q initial loss 3.7716479710070416e-05
I0328 04:27:17.467578 2161298 finetune.py:68] layer 21_q @ epoch 4 new loss 4.2446587031008676e-05 old loss 4.298190833651461e-05 BETTER
I0328 04:27:18.268309 2160516 finetune.py:68] layer 20_k @ epoch 0 new loss 3.602390643209219e-05 old loss 3.645961987785995e-05 BETTER
I0328 04:27:18.531090 2162831 finetune.py:68] layer 23_q @ epoch 0 new loss 3.674054823932238e-05 old loss 3.7716479710070416e-05 BETTER
I0328 04:27:19.583402 2162030 finetune.py:68] layer 22_q @ epoch 2 new loss 3.595527959987521e-05 old loss 3.645467586466111e-05 BETTER
I0328 04:27:35.824005 2161298 finetune.py:45] layer 21_k initial loss 4.6205030230339617e-05
I0328 04:27:53.168069 2162831 finetune.py:68] layer 23_q @ epoch 1 new loss 3.613746230257675e-05 old loss 3.674054823932238e-05 BETTER
I0328 04:27:54.935319 2162030 finetune.py:68] layer 22_q @ epoch 3 new loss 3.554431168595329e-05 old loss 3.595527959987521e-05 BETTER
I0328 04:27:55.012145 2160516 finetune.py:68] layer 20_k @ epoch 1 new loss 3.5723434848478064e-05 old loss 3.602390643209219e-05 BETTER
I0328 04:28:09.443427 2161298 finetune.py:68] layer 21_k @ epoch 0 new loss 4.517829802352935e-05 old loss 4.6205030230339617e-05 BETTER
I0328 04:28:28.121417 2162831 finetune.py:68] layer 23_q @ epoch 2 new loss 3.566429586498998e-05 old loss 3.613746230257675e-05 BETTER
I0328 04:28:30.365724 2162030 finetune.py:68] layer 22_q @ epoch 4 new loss 3.5194265365134925e-05 old loss 3.554431168595329e-05 BETTER
I0328 04:28:31.641300 2160516 finetune.py:68] layer 20_k @ epoch 2 new loss 3.546163497958332e-05 old loss 3.5723434848478064e-05 BETTER
I0328 04:28:43.778801 2161298 finetune.py:68] layer 21_k @ epoch 1 new loss 4.4730521040037274e-05 old loss 4.517829802352935e-05 BETTER
I0328 04:28:48.407788 2162030 finetune.py:45] layer 22_k initial loss 3.7919813621556386e-05
I0328 04:29:02.480509 2162831 finetune.py:68] layer 23_q @ epoch 3 new loss 3.528260640450753e-05 old loss 3.566429586498998e-05 BETTER
I0328 04:29:08.188242 2160516 finetune.py:68] layer 20_k @ epoch 3 new loss 3.524888961692341e-05 old loss 3.546163497958332e-05 BETTER
I0328 04:29:18.179421 2161298 finetune.py:68] layer 21_k @ epoch 2 new loss 4.434868606040254e-05 old loss 4.4730521040037274e-05 BETTER
I0328 04:29:22.048609 2162030 finetune.py:68] layer 22_k @ epoch 0 new loss 3.7486290239030495e-05 old loss 3.7919813621556386e-05 BETTER
I0328 04:29:36.843030 2162831 finetune.py:68] layer 23_q @ epoch 4 new loss 3.496250792522915e-05 old loss 3.528260640450753e-05 BETTER
I0328 04:29:44.925789 2160516 finetune.py:68] layer 20_k @ epoch 4 new loss 3.5060795198660344e-05 old loss 3.524888961692341e-05 BETTER
I0328 04:29:52.512926 2161298 finetune.py:68] layer 21_k @ epoch 3 new loss 4.401334444992244e-05 old loss 4.434868606040254e-05 BETTER
I0328 04:29:55.032553 2162831 finetune.py:45] layer 23_k initial loss 3.7927307857898995e-05
I0328 04:29:56.616580 2162030 finetune.py:68] layer 22_k @ epoch 1 new loss 3.721592293004505e-05 old loss 3.7486290239030495e-05 BETTER
I0328 04:30:04.616878 2160516 finetune.py:45] layer 20_o initial loss 7.963478856254369e-05
I0328 04:30:27.155110 2161298 finetune.py:68] layer 21_k @ epoch 4 new loss 4.373008414404467e-05 old loss 4.401334444992244e-05 BETTER
I0328 04:30:28.550537 2162831 finetune.py:68] layer 23_k @ epoch 0 new loss 3.761208790820092e-05 old loss 3.7927307857898995e-05 BETTER
I0328 04:30:31.360805 2162030 finetune.py:68] layer 22_k @ epoch 2 new loss 3.698343061842024e-05 old loss 3.721592293004505e-05 BETTER
I0328 04:30:39.456839 2160516 finetune.py:68] layer 20_o @ epoch 0 new loss 7.507004920626059e-05 old loss 7.963478856254369e-05 BETTER
I0328 04:30:47.444909 2161298 finetune.py:45] layer 21_o initial loss 9.951327228918672e-05
I0328 04:31:02.902051 2162831 finetune.py:68] layer 23_k @ epoch 1 new loss 3.73662514903117e-05 old loss 3.761208790820092e-05 BETTER
I0328 04:31:06.123167 2162030 finetune.py:68] layer 22_k @ epoch 3 new loss 3.677941276691854e-05 old loss 3.698343061842024e-05 BETTER
I0328 04:31:15.381185 2160516 finetune.py:68] layer 20_o @ epoch 1 new loss 7.391270628431812e-05 old loss 7.507004920626059e-05 BETTER
I0328 04:31:20.008190 2161298 finetune.py:68] layer 21_o @ epoch 0 new loss 9.425858297618106e-05 old loss 9.951327228918672e-05 BETTER
I0328 04:31:37.068510 2162831 finetune.py:68] layer 23_k @ epoch 2 new loss 3.7145684473216534e-05 old loss 3.73662514903117e-05 BETTER
I0328 04:31:40.870309 2162030 finetune.py:68] layer 22_k @ epoch 4 new loss 3.660620131995529e-05 old loss 3.677941276691854e-05 BETTER
I0328 04:31:51.529623 2160516 finetune.py:68] layer 20_o @ epoch 2 new loss 7.309475040528923e-05 old loss 7.391270628431812e-05 BETTER
I0328 04:31:53.721959 2161298 finetune.py:68] layer 21_o @ epoch 1 new loss 9.259828948415816e-05 old loss 9.425858297618106e-05 BETTER
I0328 04:32:00.427982 2162030 finetune.py:45] layer 22_o initial loss 9.099025919567794e-05
I0328 04:32:11.416560 2162831 finetune.py:68] layer 23_k @ epoch 3 new loss 3.696090425364673e-05 old loss 3.7145684473216534e-05 BETTER
I0328 04:32:27.519787 2160516 finetune.py:68] layer 20_o @ epoch 3 new loss 7.242448918987066e-05 old loss 7.309475040528923e-05 BETTER
I0328 04:32:27.716879 2161298 finetune.py:68] layer 21_o @ epoch 2 new loss 9.139525354839861e-05 old loss 9.259828948415816e-05 BETTER
I0328 04:32:33.628658 2162030 finetune.py:68] layer 22_o @ epoch 0 new loss 8.564265590393916e-05 old loss 9.099025919567794e-05 BETTER
I0328 04:32:45.863555 2162831 finetune.py:68] layer 23_k @ epoch 4 new loss 3.679952715174295e-05 old loss 3.696090425364673e-05 BETTER
I0328 04:33:01.574619 2161298 finetune.py:68] layer 21_o @ epoch 3 new loss 9.042633610079065e-05 old loss 9.139525354839861e-05 BETTER
I0328 04:33:03.770800 2160516 finetune.py:68] layer 20_o @ epoch 4 new loss 7.18731535016559e-05 old loss 7.242448918987066e-05 BETTER
I0328 04:33:06.091104 2162831 finetune.py:45] layer 23_o initial loss 9.474768012296408e-05
I0328 04:33:07.684242 2162030 finetune.py:68] layer 22_o @ epoch 1 new loss 8.448254084214568e-05 old loss 8.564265590393916e-05 BETTER
I0328 04:33:35.624445 2160516 finetune.py:45] layer 20_up initial loss 0.00022640571114607155
I0328 04:33:36.095499 2161298 finetune.py:68] layer 21_o @ epoch 4 new loss 8.962475112639368e-05 old loss 9.042633610079065e-05 BETTER
I0328 04:33:38.964380 2162831 finetune.py:68] layer 23_o @ epoch 0 new loss 8.821638039080426e-05 old loss 9.474768012296408e-05 BETTER
I0328 04:33:41.986344 2162030 finetune.py:68] layer 22_o @ epoch 2 new loss 8.366435940843076e-05 old loss 8.448254084214568e-05 BETTER
I0328 04:34:07.696493 2160516 finetune.py:68] layer 20_up @ epoch 0 new loss 0.00022279965924099088 old loss 0.00022640571114607155 BETTER
I0328 04:34:08.379569 2161298 finetune.py:45] layer 21_up initial loss 0.0002660300815477967
I0328 04:34:12.575167 2162831 finetune.py:68] layer 23_o @ epoch 1 new loss 8.697155863046646e-05 old loss 8.821638039080426e-05 BETTER
I0328 04:34:16.179014 2162030 finetune.py:68] layer 22_o @ epoch 3 new loss 8.304380025947466e-05 old loss 8.366435940843076e-05 BETTER
I0328 04:34:38.854443 2161298 finetune.py:68] layer 21_up @ epoch 0 new loss 0.00026178162079304457 old loss 0.0002660300815477967 BETTER
I0328 04:34:41.001608 2160516 finetune.py:68] layer 20_up @ epoch 1 new loss 0.00022022555640432984 old loss 0.00022279965924099088 BETTER
I0328 04:34:46.108599 2162831 finetune.py:68] layer 23_o @ epoch 2 new loss 8.612980309408158e-05 old loss 8.697155863046646e-05 BETTER
I0328 04:34:50.705812 2162030 finetune.py:68] layer 22_o @ epoch 4 new loss 8.2521335571073e-05 old loss 8.304380025947466e-05 BETTER
I0328 04:35:10.588453 2161298 finetune.py:68] layer 21_up @ epoch 1 new loss 0.0002587246708571911 old loss 0.00026178162079304457 BETTER
I0328 04:35:14.265598 2160516 finetune.py:68] layer 20_up @ epoch 2 new loss 0.0002180668816436082 old loss 0.00022022555640432984 BETTER
I0328 04:35:19.812259 2162831 finetune.py:68] layer 23_o @ epoch 3 new loss 8.548183541279286e-05 old loss 8.612980309408158e-05 BETTER
I0328 04:35:22.435119 2162030 finetune.py:45] layer 22_up initial loss 0.0002678541641216725
I0328 04:35:42.353736 2161298 finetune.py:68] layer 21_up @ epoch 2 new loss 0.00025612334138713777 old loss 0.0002587246708571911 BETTER
I0328 04:35:47.639249 2160516 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0002161392621928826 old loss 0.0002180668816436082 BETTER
I0328 04:35:53.392118 2162030 finetune.py:68] layer 22_up @ epoch 0 new loss 0.00026382473879493773 old loss 0.0002678541641216725 BETTER
I0328 04:35:53.405923 2162831 finetune.py:68] layer 23_o @ epoch 4 new loss 8.497061935486272e-05 old loss 8.548183541279286e-05 BETTER
I0328 04:36:14.105626 2161298 finetune.py:68] layer 21_up @ epoch 3 new loss 0.00025384622858837247 old loss 0.00025612334138713777 BETTER
I0328 04:36:21.099190 2160516 finetune.py:68] layer 20_up @ epoch 4 new loss 0.00021444681624416262 old loss 0.0002161392621928826 BETTER
I0328 04:36:25.167651 2162831 finetune.py:45] layer 23_up initial loss 0.00028621198725886643
I0328 04:36:25.479923 2162030 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0002610196825116873 old loss 0.00026382473879493773 BETTER
I0328 04:36:46.004053 2161298 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0002518094552215189 old loss 0.00025384622858837247 BETTER
I0328 04:36:52.882772 2160516 finetune.py:45] layer 20_gate initial loss 0.0002752143773250282
I0328 04:36:55.245032 2162831 finetune.py:68] layer 23_up @ epoch 0 new loss 0.00028242755797691643 old loss 0.00028621198725886643 BETTER
I0328 04:36:57.685025 2162030 finetune.py:68] layer 22_up @ epoch 2 new loss 0.00025864201597869396 old loss 0.0002610196825116873 BETTER
I0328 04:37:17.662282 2161298 finetune.py:45] layer 21_gate initial loss 0.000321238418109715
I0328 04:37:22.725903 2160516 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.00027330368175171316 old loss 0.0002752143773250282 BETTER
I0328 04:37:26.564928 2162831 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0002797171473503113 old loss 0.00028242755797691643 BETTER
I0328 04:37:30.076249 2162030 finetune.py:68] layer 22_up @ epoch 3 new loss 0.00025657351943664253 old loss 0.00025864201597869396 BETTER
I0328 04:37:45.941507 2161298 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.00031898371526040137 old loss 0.000321238418109715 BETTER
I0328 04:37:53.910709 2160516 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.00027169205714017153 old loss 0.00027330368175171316 BETTER
I0328 04:37:57.994418 2162831 finetune.py:68] layer 23_up @ epoch 2 new loss 0.00027746710111387074 old loss 0.0002797171473503113 BETTER
I0328 04:38:02.330736 2162030 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0002547089825384319 old loss 0.00025657351943664253 BETTER
I0328 04:38:15.308433 2161298 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0003170988638885319 old loss 0.00031898371526040137 BETTER
I0328 04:38:25.293916 2160516 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.00027022752328775823 old loss 0.00027169205714017153 BETTER
I0328 04:38:29.592162 2162831 finetune.py:68] layer 23_up @ epoch 3 new loss 0.00027550815138965845 old loss 0.00027746710111387074 BETTER
I0328 04:38:34.229258 2162030 finetune.py:45] layer 22_gate initial loss 0.000329993519699201
I0328 04:38:45.039758 2161298 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0003153624420519918 old loss 0.0003170988638885319 BETTER
I0328 04:38:57.122670 2160516 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0002688882523216307 old loss 0.00027022752328775823 BETTER
I0328 04:39:01.643122 2162831 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0002737653558142483 old loss 0.00027550815138965845 BETTER
I0328 04:39:03.337405 2162030 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0003279044176451862 old loss 0.000329993519699201 BETTER
I0328 04:39:14.732623 2161298 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.0003137880121357739 old loss 0.0003153624420519918 BETTER
I0328 04:39:28.545508 2160516 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.0002676482545211911 old loss 0.0002688882523216307 BETTER
I0328 04:39:33.163616 2162030 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.00032613566145300865 old loss 0.0003279044176451862 BETTER
I0328 04:39:33.710390 2162831 finetune.py:45] layer 23_gate initial loss 0.0003583324432838708
I0328 04:39:44.487415 2161298 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.0003123147180303931 old loss 0.0003137880121357739 BETTER
I0328 04:40:01.688982 2162831 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.00035640475107356906 old loss 0.0003583324432838708 BETTER
I0328 04:40:02.968214 2162030 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0003245433035772294 old loss 0.00032613566145300865 BETTER
I0328 04:40:24.883771 2160516 finetune.py:45] layer 20_down initial loss 0.000467870180727914
I0328 04:40:31.255627 2162831 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.0003548017411958426 old loss 0.00035640475107356906 BETTER
I0328 04:40:32.870657 2162030 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.00032308007939718664 old loss 0.0003245433035772294 BETTER
I0328 04:40:41.854044 2161298 finetune.py:45] layer 21_down initial loss 0.0005459915846586227
I0328 04:40:52.239269 2160516 finetune.py:68] layer 20_down @ epoch 0 new loss 0.00046782230492681265 old loss 0.000467870180727914 BETTER
I0328 04:41:00.929875 2162831 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0003533379640430212 old loss 0.0003548017411958426 BETTER
I0328 04:41:03.208193 2162030 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.00032173076760955155 old loss 0.00032308007939718664 BETTER
I0328 04:41:08.157096 2161298 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0005459297681227326 old loss 0.0005459915846586227 BETTER
I0328 04:41:20.793661 2160516 finetune.py:68] layer 20_down @ epoch 1 new loss 0.0004677810939028859 old loss 0.00046782230492681265 BETTER
I0328 04:41:30.722673 2162831 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.00035199406556785107 old loss 0.0003533379640430212 BETTER
I0328 04:41:35.401189 2161298 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0005458781379275024 old loss 0.0005459297681227326 BETTER
I0328 04:41:49.561750 2160516 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0004677461984101683 old loss 0.0004677810939028859 BETTER
I0328 04:42:00.618631 2162831 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0003507740329951048 old loss 0.00035199406556785107 BETTER
I0328 04:42:01.180935 2162030 finetune.py:45] layer 22_down initial loss 0.000568996649235487
I0328 04:42:03.038031 2161298 finetune.py:68] layer 21_down @ epoch 2 new loss 0.0005458330269902945 old loss 0.0005458781379275024 BETTER
I0328 04:42:18.588533 2160516 finetune.py:68] layer 20_down @ epoch 3 new loss 0.00046771520283073187 old loss 0.0004677461984101683 BETTER
I0328 04:42:27.550967 2162030 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0005689285462722182 old loss 0.000568996649235487 BETTER
I0328 04:42:30.677855 2161298 finetune.py:68] layer 21_down @ epoch 3 new loss 0.000545792980119586 old loss 0.0005458330269902945 BETTER
I0328 04:42:47.670029 2160516 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0004676890675909817 old loss 0.00046771520283073187 BETTER
20_v proxy err 0.0379800982773304 tr(WHW.T) 330.192138671875
bpp_loss 2.337761470465921
20_q proxy err 0.0027473452500998974 tr(WHW.T) 20766.931640625
bpp_loss 3.0590927285375074
20_k proxy err 0.001046032295562327 tr(WHW.T) 15386.091796875
bpp_loss 3.867292062321212
20_o proxy err 0.042251020669937134 tr(WHW.T) 1210.6224365234375
bpp_loss 2.3758511759806424
20_up proxy err 0.033315639942884445 tr(WHW.T) 7604.82080078125
bpp_loss 2.463954490908821
20_gate proxy err 0.008912334218621254 tr(WHW.T) 30514.8515625
bpp_loss 2.8178116832859814
20_down proxy err 0.0369378961622715 tr(WHW.T) 6351.5380859375
bpp_loss 2.4599379065080678
I0328 04:42:56.638718 2162030 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0005688699893653393 old loss 0.0005689285462722182 BETTER
I0328 04:42:59.370009 2161298 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0005457568913698196 old loss 0.000545792980119586 BETTER
I0328 04:42:59.583493 2162831 finetune.py:45] layer 23_down initial loss 0.0006135029252618551
21_v proxy err 0.035221658647060394 tr(WHW.T) 362.87310791015625
bpp_loss 2.3635031576268375
21_q proxy err 0.002239069202914834 tr(WHW.T) 25870.140625
bpp_loss 3.059617115126457
21_k proxy err 0.0009798617102205753 tr(WHW.T) 16765.572265625
bpp_loss 3.904133011412341
21_o proxy err 0.028494831174612045 tr(WHW.T) 1275.754638671875
bpp_loss 2.392934863572009
21_up proxy err 0.03200862184166908 tr(WHW.T) 7775.158203125
bpp_loss 2.4666724708596512
21_gate proxy err 0.008531559258699417 tr(WHW.T) 31382.087890625
bpp_loss 2.828351689768689
21_down proxy err 0.03484305366873741 tr(WHW.T) 6398.283203125
bpp_loss 2.460486762441828
I0328 04:43:24.579255 2162030 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0005688189994543791 old loss 0.0005688699893653393 BETTER
I0328 04:43:25.708092 2162831 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0006134415743872523 old loss 0.0006135029252618551 BETTER
I0328 04:43:52.442912 2162030 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0005687717348337173 old loss 0.0005688189994543791 BETTER
I0328 04:43:52.800939 2162831 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0006133884307928383 old loss 0.0006134415743872523 BETTER
I0328 04:44:10.417009 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 66.13955879211426s
I0328 04:44:14.244131 2175502 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 04:44:14.244241 2175502 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 04:44:14.244286 2175502 utils.py:162] NumExpr defaulting to 16 threads.
I0328 04:44:14.607290 2175502 config.py:54] PyTorch version 2.6.0 available.
W0328 04:44:14.827975 2175502 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 04:44:15.460195 2175502 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 04:44:15.464621 2083830 quantize_finetune_llama.py:209] layer 25 gpu 1
I0328 04:44:15.484121 2175502 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 04:44:20.238752 2162831 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0006133433198556304 old loss 0.0006133884307928383 BETTER
I0328 04:44:20.374946 2162030 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0005687337834388018 old loss 0.0005687717348337173 BETTER
22_v proxy err 0.03518694266676903 tr(WHW.T) 346.0789489746094
bpp_loss 2.4124558434123173
22_q proxy err 0.0026659804861992598 tr(WHW.T) 20352.396484375
bpp_loss 3.0228242419543676
22_k proxy err 0.0010435048025101423 tr(WHW.T) 14708.1279296875
bpp_loss 3.8474444773746654
22_o proxy err 0.03978478163480759 tr(WHW.T) 1225.5975341796875
bpp_loss 2.4217144799767993
22_up proxy err 0.03335541486740112 tr(WHW.T) 7551.1044921875
bpp_loss 2.470880324619689
22_gate proxy err 0.009209886193275452 tr(WHW.T) 29412.173828125
bpp_loss 2.8330485257320106
22_down proxy err 0.03495365381240845 tr(WHW.T) 6580.55322265625
bpp_loss 2.4664382930080007
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 04:44:32.482804 2175502 finetune.py:45] layer 24_v initial loss 0.00016125530237331986
W0328 04:44:32.483005 2175502 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 04:44:47.421908 2162831 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0006133030983619392 old loss 0.0006133433198556304 BETTER
I0328 04:45:07.637930 2175502 finetune.py:68] layer 24_v @ epoch 0 new loss 4.2438306991243735e-05 old loss 0.00016125530237331986 BETTER
I0328 04:45:14.713060 2162831 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0006132673588581383 old loss 0.0006133030983619392 BETTER
23_v proxy err 0.03329873085021973 tr(WHW.T) 397.90704345703125
bpp_loss 2.4620501337049063
23_q proxy err 0.0025843919720500708 tr(WHW.T) 22633.40625
bpp_loss 3.0336963621666655
23_k proxy err 0.0011137438705191016 tr(WHW.T) 14844.349609375
bpp_loss 3.8501321979565546
23_o proxy err 0.03370901942253113 tr(WHW.T) 1747.06640625
bpp_loss 2.4446844789781608
23_up proxy err 0.03393246605992317 tr(WHW.T) 7425.27880859375
bpp_loss 2.474584473862446
23_gate proxy err 0.009966352954506874 tr(WHW.T) 27176.966796875
bpp_loss 2.835028410157455
23_down proxy err 0.0350024588406086 tr(WHW.T) 6716.63037109375
bpp_loss 2.471355681557075
I0328 04:45:28.297493 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 62.9813768863678s
I0328 04:45:31.904617 2176383 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 04:45:31.904720 2176383 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 04:45:31.904759 2176383 utils.py:162] NumExpr defaulting to 16 threads.
I0328 04:45:32.247266 2176383 config.py:54] PyTorch version 2.6.0 available.
W0328 04:45:32.435590 2176383 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 04:45:33.003923 2176383 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 04:45:33.008113 2083830 quantize_finetune_llama.py:209] layer 26 gpu 2
I0328 04:45:33.024641 2176383 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 04:45:44.042231 2175502 finetune.py:68] layer 24_v @ epoch 1 new loss 3.6336037737783045e-05 old loss 4.2438306991243735e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 04:45:49.874914 2176383 finetune.py:45] layer 25_v initial loss 0.00017974278307519853
W0328 04:45:49.875142 2176383 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 04:46:20.851727 2175502 finetune.py:68] layer 24_v @ epoch 2 new loss 3.43727006111294e-05 old loss 3.6336037737783045e-05 BETTER
I0328 04:46:22.969067 2176383 finetune.py:68] layer 25_v @ epoch 0 new loss 4.9766633310355246e-05 old loss 0.00017974278307519853 BETTER
I0328 04:46:36.777946 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 63.341920137405396s
I0328 04:46:40.550171 2177133 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 04:46:40.550287 2177133 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 04:46:40.550330 2177133 utils.py:162] NumExpr defaulting to 16 threads.
I0328 04:46:40.920737 2177133 config.py:54] PyTorch version 2.6.0 available.
W0328 04:46:41.137141 2177133 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 04:46:41.758228 2177133 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 04:46:41.761670 2083830 quantize_finetune_llama.py:209] layer 27 gpu 3
I0328 04:46:41.780343 2177133 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 04:46:57.644435 2176383 finetune.py:68] layer 25_v @ epoch 1 new loss 4.30511063314043e-05 old loss 4.9766633310355246e-05 BETTER
I0328 04:46:57.949259 2175502 finetune.py:68] layer 24_v @ epoch 3 new loss 3.3365449780831113e-05 old loss 3.43727006111294e-05 BETTER
I0328 04:47:00.099484 2177133 finetune.py:45] layer 26_v initial loss 0.00016320410941261798
W0328 04:47:00.099776 2177133 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 04:47:32.808376 2176383 finetune.py:68] layer 25_v @ epoch 2 new loss 4.083280509803444e-05 old loss 4.30511063314043e-05 BETTER
I0328 04:47:33.927358 2177133 finetune.py:68] layer 26_v @ epoch 0 new loss 6.824197043897584e-05 old loss 0.00016320410941261798 BETTER
I0328 04:47:35.249339 2175502 finetune.py:68] layer 24_v @ epoch 4 new loss 3.267847569077276e-05 old loss 3.3365449780831113e-05 BETTER
I0328 04:47:46.898874 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 64.62810564041138s
I0328 04:47:50.890151 2177908 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 04:47:50.890320 2177908 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 04:47:50.890382 2177908 utils.py:162] NumExpr defaulting to 16 threads.
I0328 04:47:51.268115 2177908 config.py:54] PyTorch version 2.6.0 available.
W0328 04:47:51.493497 2177908 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 04:47:52.159463 2177908 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 04:47:52.163841 2083830 quantize_finetune_llama.py:209] layer 28 gpu 0
I0328 04:47:52.179464 2177908 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 04:47:54.510475 2175502 finetune.py:45] layer 24_q initial loss 4.1155733924824744e-05
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 04:48:08.271852 2176383 finetune.py:68] layer 25_v @ epoch 3 new loss 3.975980871473439e-05 old loss 4.083280509803444e-05 BETTER
I0328 04:48:08.993755 2177133 finetune.py:68] layer 26_v @ epoch 1 new loss 6.252972525544465e-05 old loss 6.824197043897584e-05 BETTER
I0328 04:48:10.339323 2177908 finetune.py:45] layer 27_v initial loss 0.0001752032112563029
W0328 04:48:10.339534 2177908 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 04:48:29.496276 2175502 finetune.py:68] layer 24_q @ epoch 0 new loss 3.99624441342894e-05 old loss 4.1155733924824744e-05 BETTER
I0328 04:48:43.805082 2176383 finetune.py:68] layer 25_v @ epoch 4 new loss 3.8874364690855145e-05 old loss 3.975980871473439e-05 BETTER
I0328 04:48:43.947937 2177908 finetune.py:68] layer 27_v @ epoch 0 new loss 6.290568126132712e-05 old loss 0.0001752032112563029 BETTER
I0328 04:48:44.532341 2177133 finetune.py:68] layer 26_v @ epoch 2 new loss 6.0145132010802627e-05 old loss 6.252972525544465e-05 BETTER
I0328 04:49:03.507702 2176383 finetune.py:45] layer 25_q initial loss 5.50381482753437e-05
I0328 04:49:06.174846 2175502 finetune.py:68] layer 24_q @ epoch 1 new loss 3.9315069443546236e-05 old loss 3.99624441342894e-05 BETTER
I0328 04:49:18.714607 2177908 finetune.py:68] layer 27_v @ epoch 1 new loss 5.820851220050827e-05 old loss 6.290568126132712e-05 BETTER
I0328 04:49:19.857482 2177133 finetune.py:68] layer 26_v @ epoch 3 new loss 5.865523053216748e-05 old loss 6.0145132010802627e-05 BETTER
I0328 04:49:37.125797 2176383 finetune.py:68] layer 25_q @ epoch 0 new loss 5.294341826811433e-05 old loss 5.50381482753437e-05 BETTER
I0328 04:49:42.814221 2175502 finetune.py:68] layer 24_q @ epoch 2 new loss 3.8812195271020755e-05 old loss 3.9315069443546236e-05 BETTER
I0328 04:49:53.408194 2177908 finetune.py:68] layer 27_v @ epoch 2 new loss 5.6202457926701754e-05 old loss 5.820851220050827e-05 BETTER
I0328 04:49:55.053002 2177133 finetune.py:68] layer 26_v @ epoch 4 new loss 5.753814184572548e-05 old loss 5.865523053216748e-05 BETTER
I0328 04:50:11.572006 2176383 finetune.py:68] layer 25_q @ epoch 1 new loss 5.1660550525411963e-05 old loss 5.294341826811433e-05 BETTER
I0328 04:50:14.857106 2177133 finetune.py:45] layer 26_q initial loss 7.05259371898137e-05
I0328 04:50:19.410128 2175502 finetune.py:68] layer 24_q @ epoch 3 new loss 3.84534832846839e-05 old loss 3.8812195271020755e-05 BETTER
I0328 04:50:28.186729 2177908 finetune.py:68] layer 27_v @ epoch 3 new loss 5.507916648639366e-05 old loss 5.6202457926701754e-05 BETTER
I0328 04:50:46.207530 2176383 finetune.py:68] layer 25_q @ epoch 2 new loss 5.074815635452978e-05 old loss 5.1660550525411963e-05 BETTER
I0328 04:50:48.547102 2177133 finetune.py:68] layer 26_q @ epoch 0 new loss 6.872584344819188e-05 old loss 7.05259371898137e-05 BETTER
I0328 04:50:55.981265 2175502 finetune.py:68] layer 24_q @ epoch 4 new loss 3.809944246313535e-05 old loss 3.84534832846839e-05 BETTER
I0328 04:51:02.863380 2177908 finetune.py:68] layer 27_v @ epoch 4 new loss 5.410386438597925e-05 old loss 5.507916648639366e-05 BETTER
I0328 04:51:13.900426 2175502 finetune.py:45] layer 24_k initial loss 4.2466301238164306e-05
I0328 04:51:21.102844 2176383 finetune.py:68] layer 25_q @ epoch 3 new loss 5.0012185965897515e-05 old loss 5.074815635452978e-05 BETTER
I0328 04:51:22.784461 2177908 finetune.py:45] layer 27_q initial loss 7.444075890816748e-05
I0328 04:51:23.284195 2177133 finetune.py:68] layer 26_q @ epoch 1 new loss 6.755610229447484e-05 old loss 6.872584344819188e-05 BETTER
I0328 04:51:48.981376 2175502 finetune.py:68] layer 24_k @ epoch 0 new loss 4.211681516608223e-05 old loss 4.2466301238164306e-05 BETTER
I0328 04:51:56.361856 2177908 finetune.py:68] layer 27_q @ epoch 0 new loss 7.253688818309456e-05 old loss 7.444075890816748e-05 BETTER
I0328 04:51:56.372691 2176383 finetune.py:68] layer 25_q @ epoch 4 new loss 4.9379079428035766e-05 old loss 5.0012185965897515e-05 BETTER
I0328 04:51:58.411024 2177133 finetune.py:68] layer 26_q @ epoch 2 new loss 6.66509076836519e-05 old loss 6.755610229447484e-05 BETTER
I0328 04:52:14.738782 2176383 finetune.py:45] layer 25_k initial loss 5.705303919967264e-05
I0328 04:52:25.404693 2175502 finetune.py:68] layer 24_k @ epoch 1 new loss 4.1852272261166945e-05 old loss 4.211681516608223e-05 BETTER
I0328 04:52:30.689908 2177908 finetune.py:68] layer 27_q @ epoch 1 new loss 7.141339301597327e-05 old loss 7.253688818309456e-05 BETTER
I0328 04:52:33.344696 2177133 finetune.py:68] layer 26_q @ epoch 3 new loss 6.585222581634298e-05 old loss 6.66509076836519e-05 BETTER
I0328 04:52:48.098501 2176383 finetune.py:68] layer 25_k @ epoch 0 new loss 5.6311386288143694e-05 old loss 5.705303919967264e-05 BETTER
I0328 04:53:02.185056 2175502 finetune.py:68] layer 24_k @ epoch 2 new loss 4.166174403508194e-05 old loss 4.1852272261166945e-05 BETTER
I0328 04:53:05.107373 2177908 finetune.py:68] layer 27_q @ epoch 2 new loss 7.062764052534476e-05 old loss 7.141339301597327e-05 BETTER
I0328 04:53:08.369480 2177133 finetune.py:68] layer 26_q @ epoch 4 new loss 6.524703348986804e-05 old loss 6.585222581634298e-05 BETTER
I0328 04:53:22.386445 2176383 finetune.py:68] layer 25_k @ epoch 1 new loss 5.5818920372985303e-05 old loss 5.6311386288143694e-05 BETTER
I0328 04:53:26.528441 2177133 finetune.py:45] layer 26_k initial loss 7.065583486109972e-05
I0328 04:53:39.199216 2175502 finetune.py:68] layer 24_k @ epoch 3 new loss 4.1469684219919145e-05 old loss 4.166174403508194e-05 BETTER
I0328 04:53:39.683222 2177908 finetune.py:68] layer 27_q @ epoch 3 new loss 6.995715375524014e-05 old loss 7.062764052534476e-05 BETTER
I0328 04:53:56.964988 2176383 finetune.py:68] layer 25_k @ epoch 2 new loss 5.539238918572664e-05 old loss 5.5818920372985303e-05 BETTER
I0328 04:54:00.161295 2177133 finetune.py:68] layer 26_k @ epoch 0 new loss 6.982950435485691e-05 old loss 7.065583486109972e-05 BETTER
I0328 04:54:14.470599 2177908 finetune.py:68] layer 27_q @ epoch 4 new loss 6.940140156075358e-05 old loss 6.995715375524014e-05 BETTER
I0328 04:54:16.022564 2175502 finetune.py:68] layer 24_k @ epoch 4 new loss 4.132028334424831e-05 old loss 4.1469684219919145e-05 BETTER
I0328 04:54:31.726360 2176383 finetune.py:68] layer 25_k @ epoch 3 new loss 5.5037016863934696e-05 old loss 5.539238918572664e-05 BETTER
I0328 04:54:32.683059 2177908 finetune.py:45] layer 27_k initial loss 8.003553375601768e-05
I0328 04:54:35.177616 2177133 finetune.py:68] layer 26_k @ epoch 1 new loss 6.932648102520034e-05 old loss 6.982950435485691e-05 BETTER
I0328 04:54:36.379047 2175502 finetune.py:45] layer 24_o initial loss 0.0001037612819345668
I0328 04:55:06.045001 2177908 finetune.py:68] layer 27_k @ epoch 0 new loss 7.889998232712969e-05 old loss 8.003553375601768e-05 BETTER
I0328 04:55:06.529667 2176383 finetune.py:68] layer 25_k @ epoch 4 new loss 5.47496929357294e-05 old loss 5.5037016863934696e-05 BETTER
I0328 04:55:09.895347 2177133 finetune.py:68] layer 26_k @ epoch 2 new loss 6.886956543894485e-05 old loss 6.932648102520034e-05 BETTER
I0328 04:55:11.039980 2175502 finetune.py:68] layer 24_o @ epoch 0 new loss 9.788668830879033e-05 old loss 0.0001037612819345668 BETTER
I0328 04:55:26.527796 2176383 finetune.py:45] layer 25_o initial loss 0.0001266893814317882
I0328 04:55:40.149939 2177908 finetune.py:68] layer 27_k @ epoch 1 new loss 7.831925177015364e-05 old loss 7.889998232712969e-05 BETTER
I0328 04:55:44.511591 2177133 finetune.py:68] layer 26_k @ epoch 3 new loss 6.851070065749809e-05 old loss 6.886956543894485e-05 BETTER
I0328 04:55:46.683383 2175502 finetune.py:68] layer 24_o @ epoch 1 new loss 9.689559374237433e-05 old loss 9.788668830879033e-05 BETTER
I0328 04:55:59.096695 2176383 finetune.py:68] layer 25_o @ epoch 0 new loss 0.00011871983588207513 old loss 0.0001266893814317882 BETTER
I0328 04:56:14.413970 2177908 finetune.py:68] layer 27_k @ epoch 2 new loss 7.79118126956746e-05 old loss 7.831925177015364e-05 BETTER
I0328 04:56:19.158636 2177133 finetune.py:68] layer 26_k @ epoch 4 new loss 6.818303518230096e-05 old loss 6.851070065749809e-05 BETTER
I0328 04:56:22.358806 2175502 finetune.py:68] layer 24_o @ epoch 2 new loss 9.622394281905144e-05 old loss 9.689559374237433e-05 BETTER
I0328 04:56:32.654561 2176383 finetune.py:68] layer 25_o @ epoch 1 new loss 0.00011726088996510953 old loss 0.00011871983588207513 BETTER
I0328 04:56:38.983415 2177133 finetune.py:45] layer 26_o initial loss 0.00016499118646606803
I0328 04:56:48.686486 2177908 finetune.py:68] layer 27_k @ epoch 3 new loss 7.757120329188183e-05 old loss 7.79118126956746e-05 BETTER
I0328 04:56:58.094905 2175502 finetune.py:68] layer 24_o @ epoch 3 new loss 9.571812552167103e-05 old loss 9.622394281905144e-05 BETTER
I0328 04:57:06.304825 2176383 finetune.py:68] layer 25_o @ epoch 2 new loss 0.00011625926708802581 old loss 0.00011726088996510953 BETTER
I0328 04:57:11.872952 2177133 finetune.py:68] layer 26_o @ epoch 0 new loss 0.00015706759586464614 old loss 0.00016499118646606803 BETTER
I0328 04:57:22.709468 2177908 finetune.py:68] layer 27_k @ epoch 4 new loss 7.729195203864947e-05 old loss 7.757120329188183e-05 BETTER
I0328 04:57:33.770836 2175502 finetune.py:68] layer 24_o @ epoch 4 new loss 9.530589886708185e-05 old loss 9.571812552167103e-05 BETTER
I0328 04:57:40.046992 2176383 finetune.py:68] layer 25_o @ epoch 3 new loss 0.0001155100399046205 old loss 0.00011625926708802581 BETTER
I0328 04:57:42.136367 2177908 finetune.py:45] layer 27_o initial loss 0.00019346788758412004
I0328 04:57:45.861129 2177133 finetune.py:68] layer 26_o @ epoch 1 new loss 0.00015518687723670155 old loss 0.00015706759586464614 BETTER
I0328 04:58:04.999917 2175502 finetune.py:45] layer 24_up initial loss 0.00031307386234402657
I0328 04:58:14.013362 2176383 finetune.py:68] layer 25_o @ epoch 4 new loss 0.00011490085307741538 old loss 0.0001155100399046205 BETTER
I0328 04:58:14.576800 2177908 finetune.py:68] layer 27_o @ epoch 0 new loss 0.00018481766164768487 old loss 0.00019346788758412004 BETTER
I0328 04:58:19.833389 2177133 finetune.py:68] layer 26_o @ epoch 2 new loss 0.00015384360449388623 old loss 0.00015518687723670155 BETTER
I0328 04:58:37.037787 2175502 finetune.py:68] layer 24_up @ epoch 0 new loss 0.00030937703559175134 old loss 0.00031307386234402657 BETTER
I0328 04:58:45.878618 2176383 finetune.py:45] layer 25_up initial loss 0.0003592733119148761
I0328 04:58:48.047656 2177908 finetune.py:68] layer 27_o @ epoch 1 new loss 0.00018260501383338124 old loss 0.00018481766164768487 BETTER
I0328 04:58:53.874440 2177133 finetune.py:68] layer 26_o @ epoch 3 new loss 0.00015276685007847846 old loss 0.00015384360449388623 BETTER
I0328 04:59:10.483269 2175502 finetune.py:68] layer 24_up @ epoch 1 new loss 0.00030683487420901656 old loss 0.00030937703559175134 BETTER
I0328 04:59:16.402017 2176383 finetune.py:68] layer 25_up @ epoch 0 new loss 0.000355113937985152 old loss 0.0003592733119148761 BETTER
I0328 04:59:21.459925 2177908 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0001810526300687343 old loss 0.00018260501383338124 BETTER
I0328 04:59:28.310143 2177133 finetune.py:68] layer 26_o @ epoch 4 new loss 0.00015189469559118152 old loss 0.00015276685007847846 BETTER
I0328 04:59:44.004226 2175502 finetune.py:68] layer 24_up @ epoch 2 new loss 0.00030473852530121803 old loss 0.00030683487420901656 BETTER
I0328 04:59:47.969866 2176383 finetune.py:68] layer 25_up @ epoch 1 new loss 0.00035226927138864994 old loss 0.000355113937985152 BETTER
I0328 04:59:55.106135 2177908 finetune.py:68] layer 27_o @ epoch 3 new loss 0.0001797547156456858 old loss 0.0001810526300687343 BETTER
I0328 05:00:00.019889 2177133 finetune.py:45] layer 26_up initial loss 0.0004379245510790497
I0328 05:00:17.492840 2175502 finetune.py:68] layer 24_up @ epoch 3 new loss 0.00030287596746347845 old loss 0.00030473852530121803 BETTER
I0328 05:00:20.018726 2176383 finetune.py:68] layer 25_up @ epoch 2 new loss 0.00034993578447028995 old loss 0.00035226927138864994 BETTER
I0328 05:00:28.707884 2177908 finetune.py:68] layer 27_o @ epoch 4 new loss 0.00017874701006803662 old loss 0.0001797547156456858 BETTER
I0328 05:00:30.972193 2177133 finetune.py:68] layer 26_up @ epoch 0 new loss 0.0004330278898123652 old loss 0.0004379245510790497 BETTER
I0328 05:00:51.217066 2175502 finetune.py:68] layer 24_up @ epoch 4 new loss 0.00030124126351438463 old loss 0.00030287596746347845 BETTER
I0328 05:00:52.172137 2176383 finetune.py:68] layer 25_up @ epoch 3 new loss 0.00034789295750670135 old loss 0.00034993578447028995 BETTER
I0328 05:01:00.253029 2177908 finetune.py:45] layer 27_up initial loss 0.0005134591483511031
I0328 05:01:03.004583 2177133 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0004296123515814543 old loss 0.0004330278898123652 BETTER
I0328 05:01:22.817685 2175502 finetune.py:45] layer 24_gate initial loss 0.0003975152794737369
I0328 05:01:24.256533 2176383 finetune.py:68] layer 25_up @ epoch 4 new loss 0.00034607676207087934 old loss 0.00034789295750670135 BETTER
I0328 05:01:30.575137 2177908 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0005073638749308884 old loss 0.0005134591483511031 BETTER
I0328 05:01:35.278163 2177133 finetune.py:68] layer 26_up @ epoch 2 new loss 0.00042678596219047904 old loss 0.0004296123515814543 BETTER
I0328 05:01:52.947229 2175502 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0003957105800509453 old loss 0.0003975152794737369 BETTER
I0328 05:01:55.854069 2176383 finetune.py:45] layer 25_gate initial loss 0.00045698625035583973
I0328 05:02:02.070304 2177908 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0005032451008446515 old loss 0.0005073638749308884 BETTER
I0328 05:02:07.617221 2177133 finetune.py:68] layer 26_up @ epoch 3 new loss 0.000424287689384073 old loss 0.00042678596219047904 BETTER
I0328 05:02:24.153845 2175502 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.00039416260551661253 old loss 0.0003957105800509453 BETTER
I0328 05:02:24.686503 2176383 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.00045502811553888023 old loss 0.00045698625035583973 BETTER
I0328 05:02:33.431140 2177908 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0004998691729269922 old loss 0.0005032451008446515 BETTER
I0328 05:02:39.989419 2177133 finetune.py:68] layer 26_up @ epoch 4 new loss 0.00042208313243463635 old loss 0.000424287689384073 BETTER
I0328 05:02:54.126677 2176383 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.00045336424955166876 old loss 0.00045502811553888023 BETTER
I0328 05:02:55.523348 2175502 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.00039281017961911857 old loss 0.00039416260551661253 BETTER
I0328 05:03:04.948507 2177908 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0004969281726516783 old loss 0.0004998691729269922 BETTER
I0328 05:03:12.060818 2177133 finetune.py:45] layer 26_gate initial loss 0.000553392106667161
I0328 05:03:23.906194 2176383 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.0004518784407991916 old loss 0.00045336424955166876 BETTER
I0328 05:03:27.101953 2175502 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0003915263805538416 old loss 0.00039281017961911857 BETTER
I0328 05:03:36.526942 2177908 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0004942614468745887 old loss 0.0004969281726516783 BETTER
I0328 05:03:40.748569 2177133 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0005510603077709675 old loss 0.000553392106667161 BETTER
I0328 05:03:53.548444 2176383 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.0004505316319409758 old loss 0.0004518784407991916 BETTER
I0328 05:03:58.554899 2175502 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.00039038454997353256 old loss 0.0003915263805538416 BETTER
I0328 05:04:07.980987 2177908 finetune.py:45] layer 27_gate initial loss 0.0006573047139681876
I0328 05:04:10.334409 2177133 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.0005490999319590628 old loss 0.0005510603077709675 BETTER
I0328 05:04:23.267158 2176383 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0004493012966122478 old loss 0.0004505316319409758 BETTER
I0328 05:04:36.117760 2177908 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.000654415343888104 old loss 0.0006573047139681876 BETTER
I0328 05:04:40.171566 2177133 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0005473640048876405 old loss 0.0005490999319590628 BETTER
I0328 05:04:54.347494 2175502 finetune.py:45] layer 24_down initial loss 0.0006664309184998274
I0328 05:05:05.448356 2177908 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.0006520116585306823 old loss 0.000654415343888104 BETTER
I0328 05:05:10.184386 2177133 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0005457376828417182 old loss 0.0005473640048876405 BETTER
I0328 05:05:20.638729 2176383 finetune.py:45] layer 25_down initial loss 0.0007469886331818998
I0328 05:05:21.709626 2175502 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0006663770182058215 old loss 0.0006664309184998274 BETTER
I0328 05:05:34.865203 2177908 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.0006498731672763824 old loss 0.0006520116585306823 BETTER
I0328 05:05:40.006244 2177133 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0005442901747301221 old loss 0.0005457376828417182 BETTER
I0328 05:05:46.774964 2176383 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0007469429983757436 old loss 0.0007469886331818998 BETTER
I0328 05:05:50.163020 2175502 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0006663278327323496 old loss 0.0006663770182058215 BETTER
I0328 05:06:04.153337 2177908 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0006479137809947133 old loss 0.0006498731672763824 BETTER
I0328 05:06:13.889106 2176383 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0007469006814062595 old loss 0.0007469429983757436 BETTER
I0328 05:06:18.532919 2175502 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0006662868545390666 old loss 0.0006663278327323496 BETTER
I0328 05:06:33.566318 2177908 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.0006461449665948749 old loss 0.0006479137809947133 BETTER
I0328 05:06:37.609459 2177133 finetune.py:45] layer 26_down initial loss 0.0008867263095453382
I0328 05:06:41.362892 2176383 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0007468643016181886 old loss 0.0007469006814062595 BETTER
I0328 05:06:47.276799 2175502 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0006662498926743865 old loss 0.0006662868545390666 BETTER
I0328 05:07:03.888904 2177133 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0008866728167049587 old loss 0.0008867263095453382 BETTER
I0328 05:07:08.881324 2176383 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0007468302501365542 old loss 0.0007468643016181886 BETTER
I0328 05:07:16.440574 2175502 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0006662181694991887 old loss 0.0006662498926743865 BETTER
24_v proxy err 0.028048153966665268 tr(WHW.T) 467.2783508300781
bpp_loss 2.5536573791177943
24_q proxy err 0.0025183262769132853 tr(WHW.T) 22473.2421875
bpp_loss 3.003551141882781
24_k proxy err 0.0011093448847532272 tr(WHW.T) 14174.9345703125
bpp_loss 3.6981509676552378
24_o proxy err 0.033078934997320175 tr(WHW.T) 1596.4342041015625
bpp_loss 2.484042319934815
24_up proxy err 0.03484700247645378 tr(WHW.T) 7311.4765625
bpp_loss 2.478695401722299
24_gate proxy err 0.010631888173520565 tr(WHW.T) 25781.6328125
bpp_loss 2.8401438130198846
24_down proxy err 0.034717392176389694 tr(WHW.T) 6794.1591796875
bpp_loss 2.476084099300871
I0328 05:07:31.506381 2177908 finetune.py:45] layer 27_down initial loss 0.0010467104148119688
I0328 05:07:32.242351 2177133 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0008866260177455842 old loss 0.0008866728167049587 BETTER
I0328 05:07:37.383691 2176383 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0007468002731911838 old loss 0.0007468302501365542 BETTER
25_v proxy err 0.024179471656680107 tr(WHW.T) 557.8086547851562
bpp_loss 2.5659265321446583
25_q proxy err 0.0022124480456113815 tr(WHW.T) 26157.671875
bpp_loss 2.9858509581536055
25_k proxy err 0.0011177638079971075 tr(WHW.T) 14404.0791015625
bpp_loss 3.678199397341814
25_o proxy err 0.027825167402625084 tr(WHW.T) 1990.2618408203125
bpp_loss 2.482237267657183
25_up proxy err 0.034498441964387894 tr(WHW.T) 7387.9931640625
bpp_loss 2.4874467596611276
25_gate proxy err 0.010457727126777172 tr(WHW.T) 26213.212890625
bpp_loss 2.8480308719777634
25_down proxy err 0.03457275405526161 tr(WHW.T) 6668.1669921875
bpp_loss 2.484375148363012
I0328 05:07:57.629438 2177908 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0010466474341228604 old loss 0.0010467104148119688 BETTER
I0328 05:08:00.072879 2177133 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0008865838753990829 old loss 0.0008866260177455842 BETTER
I0328 05:08:24.692494 2177908 finetune.py:68] layer 27_down @ epoch 1 new loss 0.001046593300998211 old loss 0.0010466474341228604 BETTER
I0328 05:08:27.961740 2177133 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0008865481941029429 old loss 0.0008865838753990829 BETTER
I0328 05:08:48.326454 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 66.14056968688965s
I0328 05:08:51.980200 2177908 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0010465430095791817 old loss 0.001046593300998211 BETTER
I0328 05:08:52.495779 2190740 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:08:52.495987 2190740 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:08:52.496029 2190740 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:08:52.889192 2190740 config.py:54] PyTorch version 2.6.0 available.
W0328 05:08:53.110971 2190740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 05:08:53.789352 2190740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 05:08:53.793334 2083830 quantize_finetune_llama.py:209] layer 29 gpu 1
I0328 05:08:53.809262 2190740 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 05:08:55.727014 2177133 finetune.py:68] layer 26_down @ epoch 4 new loss 0.0008865161798894405 old loss 0.0008865481941029429 BETTER
26_v proxy err 0.029727648943662643 tr(WHW.T) 434.54583740234375
bpp_loss 2.609557293355465
26_q proxy err 0.0025623426772654057 tr(WHW.T) 21420.298828125
bpp_loss 2.9948058614390902
26_k proxy err 0.000999339041300118 tr(WHW.T) 15411.693359375
bpp_loss 3.753318755712826
26_o proxy err 0.020069951191544533 tr(WHW.T) 2389.75390625
bpp_loss 2.500005606911145
26_up proxy err 0.03333612531423569 tr(WHW.T) 7658.09033203125
bpp_loss 2.496477388943146
26_gate proxy err 0.009511783719062805 tr(WHW.T) 28860.26171875
bpp_loss 2.8527108248589292
26_down proxy err 0.0346250981092453 tr(WHW.T) 6671.68701171875
bpp_loss 2.4916927186672444
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 05:09:10.579445 2190740 finetune.py:45] layer 28_v initial loss 0.00021405526786111295
W0328 05:09:10.579718 2190740 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 05:09:19.217280 2177908 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0010464995866641402 old loss 0.0010465430095791817 BETTER
I0328 05:09:45.492784 2190740 finetune.py:68] layer 28_v @ epoch 0 new loss 8.767331746639684e-05 old loss 0.00021405526786111295 BETTER
I0328 05:09:46.715072 2177908 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0010464619845151901 old loss 0.0010464995866641402 BETTER
27_v proxy err 0.020621974021196365 tr(WHW.T) 677.69384765625
bpp_loss 2.696163777814945
27_q proxy err 0.002732471562922001 tr(WHW.T) 21326.17578125
bpp_loss 2.965799780358793
27_k proxy err 0.001165880123153329 tr(WHW.T) 14002.986328125
bpp_loss 3.7108390685461927
27_o proxy err 0.023968243971467018 tr(WHW.T) 2166.907958984375
bpp_loss 2.5384738696157
27_up proxy err 0.030438242480158806 tr(WHW.T) 8481.2158203125
bpp_loss 2.511169384333438
27_gate proxy err 0.00847252830862999 tr(WHW.T) 32726.8984375
bpp_loss 2.8617725603814637
27_down proxy err 0.028803003951907158 tr(WHW.T) 6595.646484375
bpp_loss 2.5020814275069694
I0328 05:10:03.643009 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 63.041117429733276s
I0328 05:10:07.216261 2191560 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:10:07.216361 2191560 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:10:07.216399 2191560 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:10:07.567441 2191560 config.py:54] PyTorch version 2.6.0 available.
W0328 05:10:07.755531 2191560 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 05:10:08.339048 2191560 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 05:10:08.343147 2083830 quantize_finetune_llama.py:209] layer 30 gpu 2
I0328 05:10:08.359011 2191560 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 05:10:22.243141 2190740 finetune.py:68] layer 28_v @ epoch 1 new loss 8.303966751554981e-05 old loss 8.767331746639684e-05 BETTER
I0328 05:10:25.090014 2191560 finetune.py:45] layer 29_v initial loss 0.00027807336300611496
W0328 05:10:25.090239 2191560 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 05:10:58.197154 2191560 finetune.py:68] layer 29_v @ epoch 0 new loss 0.00010204751743003726 old loss 0.00027807336300611496 BETTER
I0328 05:10:59.213846 2190740 finetune.py:68] layer 28_v @ epoch 2 new loss 8.070450712693855e-05 old loss 8.303966751554981e-05 BETTER
I0328 05:11:11.705461 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 62.895028591156006s
I0328 05:11:15.406869 2192315 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:11:15.406967 2192315 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:11:15.407004 2192315 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:11:15.755497 2192315 config.py:54] PyTorch version 2.6.0 available.
W0328 05:11:15.955035 2192315 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 05:11:16.549482 2192315 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 05:11:16.553374 2083830 quantize_finetune_llama.py:209] layer 31 gpu 3
I0328 05:11:16.569065 2192315 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 05:11:33.104427 2191560 finetune.py:68] layer 29_v @ epoch 1 new loss 9.689839498605579e-05 old loss 0.00010204751743003726 BETTER
I0328 05:11:34.349770 2192315 finetune.py:45] layer 30_v initial loss 0.00031552527798339725
W0328 05:11:34.349982 2192315 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 05:11:36.199912 2190740 finetune.py:68] layer 28_v @ epoch 3 new loss 7.898723561083898e-05 old loss 8.070450712693855e-05 BETTER
I0328 05:12:08.017208 2191560 finetune.py:68] layer 29_v @ epoch 2 new loss 9.410120401298627e-05 old loss 9.689839498605579e-05 BETTER
I0328 05:12:08.138714 2192315 finetune.py:68] layer 30_v @ epoch 0 new loss 0.0001737480633892119 old loss 0.00031552527798339725 BETTER
I0328 05:12:13.256324 2190740 finetune.py:68] layer 28_v @ epoch 4 new loss 7.800227467669174e-05 old loss 7.898723561083898e-05 BETTER
I0328 05:12:16.139612 2083830 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 59.106993198394775s
I0328 05:12:19.889606 2193014 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:12:19.889707 2193014 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:12:19.889747 2193014 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:12:20.234093 2193014 config.py:54] PyTorch version 2.6.0 available.
W0328 05:12:20.454967 2193014 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0328 05:12:21.104513 2193014 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0328 05:12:21.121853 2193014 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0328 05:12:32.540547 2190740 finetune.py:45] layer 28_q initial loss 0.000101787198218517
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0328 05:12:38.566849 2193014 finetune.py:45] layer 31_v initial loss 0.0005537901306524873
W0328 05:12:38.567303 2193014 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0328 05:12:42.886528 2191560 finetune.py:68] layer 29_v @ epoch 3 new loss 9.288446744903922e-05 old loss 9.410120401298627e-05 BETTER
I0328 05:12:43.027738 2192315 finetune.py:68] layer 30_v @ epoch 1 new loss 0.0001655835221754387 old loss 0.0001737480633892119 BETTER
I0328 05:13:07.926198 2190740 finetune.py:68] layer 28_q @ epoch 0 new loss 9.910992230288684e-05 old loss 0.000101787198218517 BETTER
I0328 05:13:11.858008 2193014 finetune.py:68] layer 31_v @ epoch 0 new loss 0.00030377236544154584 old loss 0.0005537901306524873 BETTER
I0328 05:13:17.923432 2191560 finetune.py:68] layer 29_v @ epoch 4 new loss 9.196428436553106e-05 old loss 9.288446744903922e-05 BETTER
I0328 05:13:18.201479 2192315 finetune.py:68] layer 30_v @ epoch 2 new loss 0.0001614742068341002 old loss 0.0001655835221754387 BETTER
I0328 05:13:37.208157 2191560 finetune.py:45] layer 29_q initial loss 0.00015581730986014009
I0328 05:13:44.241791 2190740 finetune.py:68] layer 28_q @ epoch 1 new loss 9.74901849986054e-05 old loss 9.910992230288684e-05 BETTER
I0328 05:13:46.226375 2193014 finetune.py:76] layer 31_v @ epoch 1 new loss 0.0003119035391137004 old loss 0.00030377236544154584 WORSE
I0328 05:13:53.576759 2192315 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0001588074374012649 old loss 0.0001614742068341002 BETTER
I0328 05:14:10.624732 2191560 finetune.py:68] layer 29_q @ epoch 0 new loss 0.0001480346982134506 old loss 0.00015581730986014009 BETTER
I0328 05:14:20.102472 2193014 finetune.py:68] layer 31_v @ epoch 2 new loss 0.00029954256024211645 old loss 0.00030377236544154584 BETTER
I0328 05:14:20.775108 2190740 finetune.py:68] layer 28_q @ epoch 2 new loss 9.619046613806859e-05 old loss 9.74901849986054e-05 BETTER
I0328 05:14:28.739599 2192315 finetune.py:68] layer 30_v @ epoch 4 new loss 0.00015643981168977916 old loss 0.0001588074374012649 BETTER
I0328 05:14:44.844802 2191560 finetune.py:68] layer 29_q @ epoch 1 new loss 0.0001437338360119611 old loss 0.0001480346982134506 BETTER
I0328 05:14:48.683263 2192315 finetune.py:45] layer 30_q initial loss 0.00019816469284705818
I0328 05:14:54.743297 2193014 finetune.py:68] layer 31_v @ epoch 3 new loss 0.00027527136262506247 old loss 0.00029954256024211645 BETTER
I0328 05:14:57.297606 2190740 finetune.py:68] layer 28_q @ epoch 3 new loss 9.5119044999592e-05 old loss 9.619046613806859e-05 BETTER
I0328 05:15:19.382458 2191560 finetune.py:68] layer 29_q @ epoch 2 new loss 0.0001407828531228006 old loss 0.0001437338360119611 BETTER
I0328 05:15:22.434017 2192315 finetune.py:68] layer 30_q @ epoch 0 new loss 0.00019428919767960906 old loss 0.00019816469284705818 BETTER
I0328 05:15:29.536554 2193014 finetune.py:76] layer 31_v @ epoch 4 new loss 0.00028280101832933724 old loss 0.00027527136262506247 WORSE
I0328 05:15:33.860250 2190740 finetune.py:68] layer 28_q @ epoch 4 new loss 9.422418952453882e-05 old loss 9.5119044999592e-05 BETTER
I0328 05:15:48.343096 2193014 finetune.py:45] layer 31_q initial loss 0.00039719618507660925
I0328 05:15:51.823981 2190740 finetune.py:45] layer 28_k initial loss 0.0001054096210282296
I0328 05:15:54.078539 2191560 finetune.py:68] layer 29_q @ epoch 3 new loss 0.0001385075884172693 old loss 0.0001407828531228006 BETTER
I0328 05:15:57.117461 2192315 finetune.py:68] layer 30_q @ epoch 1 new loss 0.0001913836458697915 old loss 0.00019428919767960906 BETTER
I0328 05:16:21.538136 2193014 finetune.py:68] layer 31_q @ epoch 0 new loss 0.0003636999463196844 old loss 0.00039719618507660925 BETTER
I0328 05:16:27.304599 2190740 finetune.py:68] layer 28_k @ epoch 0 new loss 0.00010407157969893888 old loss 0.0001054096210282296 BETTER
I0328 05:16:28.718493 2191560 finetune.py:68] layer 29_q @ epoch 4 new loss 0.00013665578444488347 old loss 0.0001385075884172693 BETTER
I0328 05:16:31.925796 2192315 finetune.py:68] layer 30_q @ epoch 2 new loss 0.00018941093003377318 old loss 0.0001913836458697915 BETTER
I0328 05:16:46.810194 2191560 finetune.py:45] layer 29_k initial loss 0.00015453287051059306
I0328 05:16:55.694360 2193014 finetune.py:68] layer 31_q @ epoch 1 new loss 0.0003400582354515791 old loss 0.0003636999463196844 BETTER
I0328 05:17:03.607527 2190740 finetune.py:68] layer 28_k @ epoch 1 new loss 0.00010329973883926868 old loss 0.00010407157969893888 BETTER
I0328 05:17:06.746899 2192315 finetune.py:68] layer 30_q @ epoch 3 new loss 0.00018758149235509336 old loss 0.00018941093003377318 BETTER
I0328 05:17:20.119900 2191560 finetune.py:68] layer 29_k @ epoch 0 new loss 0.00015218296903185546 old loss 0.00015453287051059306 BETTER
I0328 05:17:29.852820 2193014 finetune.py:68] layer 31_q @ epoch 2 new loss 0.00032408745028078556 old loss 0.0003400582354515791 BETTER
I0328 05:17:40.011226 2190740 finetune.py:68] layer 28_k @ epoch 2 new loss 0.00010269979247823358 old loss 0.00010329973883926868 BETTER
I0328 05:17:41.555230 2192315 finetune.py:68] layer 30_q @ epoch 4 new loss 0.00018631209968589246 old loss 0.00018758149235509336 BETTER
I0328 05:17:54.272110 2191560 finetune.py:68] layer 29_k @ epoch 1 new loss 0.00015070338849909604 old loss 0.00015218296903185546 BETTER
I0328 05:17:59.699916 2192315 finetune.py:45] layer 30_k initial loss 0.00021043620654381812
I0328 05:18:04.172089 2193014 finetune.py:68] layer 31_q @ epoch 3 new loss 0.00031536127789877355 old loss 0.00032408745028078556 BETTER
I0328 05:18:16.588219 2190740 finetune.py:68] layer 28_k @ epoch 3 new loss 0.00010211786138825119 old loss 0.00010269979247823358 BETTER
I0328 05:18:28.799679 2191560 finetune.py:68] layer 29_k @ epoch 2 new loss 0.0001497171469964087 old loss 0.00015070338849909604 BETTER
I0328 05:18:33.302207 2192315 finetune.py:68] layer 30_k @ epoch 0 new loss 0.00020818626217078418 old loss 0.00021043620654381812 BETTER
I0328 05:18:38.462738 2193014 finetune.py:68] layer 31_q @ epoch 4 new loss 0.0003073840052820742 old loss 0.00031536127789877355 BETTER
I0328 05:18:53.185479 2190740 finetune.py:68] layer 28_k @ epoch 4 new loss 0.00010172597831115127 old loss 0.00010211786138825119 BETTER
I0328 05:18:56.191614 2193014 finetune.py:45] layer 31_k initial loss 0.0003755984653253108
I0328 05:19:03.268260 2191560 finetune.py:68] layer 29_k @ epoch 3 new loss 0.00014855035988148302 old loss 0.0001497171469964087 BETTER
I0328 05:19:07.909915 2192315 finetune.py:76] layer 30_k @ epoch 1 new loss 0.0002083575091091916 old loss 0.00020818626217078418 WORSE
I0328 05:19:13.100283 2190740 finetune.py:45] layer 28_o initial loss 0.0002524463925510645
I0328 05:19:29.185250 2193014 finetune.py:68] layer 31_k @ epoch 0 new loss 0.00035570055479183793 old loss 0.0003755984653253108 BETTER
I0328 05:19:37.894747 2191560 finetune.py:68] layer 29_k @ epoch 4 new loss 0.00014800972712691873 old loss 0.00014855035988148302 BETTER
I0328 05:19:41.778328 2192315 finetune.py:68] layer 30_k @ epoch 2 new loss 0.00020625388424377888 old loss 0.00020818626217078418 BETTER
I0328 05:19:47.637970 2190740 finetune.py:68] layer 28_o @ epoch 0 new loss 0.00024035735987126827 old loss 0.0002524463925510645 BETTER
I0328 05:19:57.699371 2191560 finetune.py:45] layer 29_o initial loss 0.0003005435282830149
I0328 05:20:03.295674 2193014 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0003471803793217987 old loss 0.00035570055479183793 BETTER
I0328 05:20:16.391517 2192315 finetune.py:68] layer 30_k @ epoch 3 new loss 0.00020571137429215014 old loss 0.00020625388424377888 BETTER
I0328 05:20:23.525298 2190740 finetune.py:68] layer 28_o @ epoch 1 new loss 0.00023694841365795583 old loss 0.00024035735987126827 BETTER
I0328 05:20:30.574779 2191560 finetune.py:68] layer 29_o @ epoch 0 new loss 0.00028733565704897046 old loss 0.0003005435282830149 BETTER
I0328 05:20:37.428925 2193014 finetune.py:68] layer 31_k @ epoch 2 new loss 0.0003421224537305534 old loss 0.0003471803793217987 BETTER
I0328 05:20:51.158574 2192315 finetune.py:68] layer 30_k @ epoch 4 new loss 0.0002043300773948431 old loss 0.00020571137429215014 BETTER
I0328 05:20:59.390857 2190740 finetune.py:68] layer 28_o @ epoch 2 new loss 0.00023430037254001945 old loss 0.00023694841365795583 BETTER
I0328 05:21:04.278523 2191560 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0002836277708411217 old loss 0.00028733565704897046 BETTER
I0328 05:21:11.202305 2192315 finetune.py:45] layer 30_o initial loss 0.000457911315606907
I0328 05:21:11.632625 2193014 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0003368656325619668 old loss 0.0003421224537305534 BETTER
I0328 05:21:35.284064 2190740 finetune.py:68] layer 28_o @ epoch 3 new loss 0.00023222938762046397 old loss 0.00023430037254001945 BETTER
I0328 05:21:38.215222 2191560 finetune.py:68] layer 29_o @ epoch 2 new loss 0.00028113165171816945 old loss 0.0002836277708411217 BETTER
I0328 05:21:44.228279 2192315 finetune.py:68] layer 30_o @ epoch 0 new loss 0.00044408263056539 old loss 0.000457911315606907 BETTER
I0328 05:21:45.788402 2193014 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0003366418241057545 old loss 0.0003368656325619668 BETTER
I0328 05:22:05.031500 2193014 finetune.py:45] layer 31_o initial loss 0.0008530833874829113
I0328 05:22:11.392493 2190740 finetune.py:68] layer 28_o @ epoch 4 new loss 0.00023046352725941688 old loss 0.00023222938762046397 BETTER
I0328 05:22:12.391379 2191560 finetune.py:68] layer 29_o @ epoch 3 new loss 0.0002789768041111529 old loss 0.00028113165171816945 BETTER
I0328 05:22:18.280222 2192315 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0004391328548081219 old loss 0.00044408263056539 BETTER
I0328 05:22:37.325623 2193014 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0007666694582439959 old loss 0.0008530833874829113 BETTER
I0328 05:22:43.081503 2190740 finetune.py:45] layer 28_up initial loss 0.0006487630889751017
I0328 05:22:46.212924 2191560 finetune.py:68] layer 29_o @ epoch 4 new loss 0.0002771845902316272 old loss 0.0002789768041111529 BETTER
I0328 05:22:52.193795 2192315 finetune.py:68] layer 30_o @ epoch 2 new loss 0.00043536900193430483 old loss 0.0004391328548081219 BETTER
I0328 05:23:10.641196 2193014 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0007374556153081357 old loss 0.0007666694582439959 BETTER
I0328 05:23:14.980731 2190740 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0006396180833689868 old loss 0.0006487630889751017 BETTER
I0328 05:23:17.944922 2191560 finetune.py:45] layer 29_up initial loss 0.0008526764577254653
I0328 05:23:26.263911 2192315 finetune.py:68] layer 30_o @ epoch 3 new loss 0.00043242229730822146 old loss 0.00043536900193430483 BETTER
I0328 05:23:44.011008 2193014 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0007184160640463233 old loss 0.0007374556153081357 BETTER
I0328 05:23:48.112514 2190740 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0006337185623124242 old loss 0.0006396180833689868 BETTER
I0328 05:23:48.402274 2191560 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0008386878180317581 old loss 0.0008526764577254653 BETTER
I0328 05:24:00.428535 2192315 finetune.py:68] layer 30_o @ epoch 4 new loss 0.00042999093420803547 old loss 0.00043242229730822146 BETTER
I0328 05:24:17.728163 2193014 finetune.py:68] layer 31_o @ epoch 3 new loss 0.000703197147231549 old loss 0.0007184160640463233 BETTER
I0328 05:24:20.072226 2191560 finetune.py:68] layer 29_up @ epoch 1 new loss 0.0008300390327349305 old loss 0.0008386878180317581 BETTER
I0328 05:24:21.383327 2190740 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0006290237652137876 old loss 0.0006337185623124242 BETTER
I0328 05:24:32.141168 2192315 finetune.py:45] layer 30_up initial loss 0.0015615234151482582
I0328 05:24:51.600751 2193014 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0006922430475242436 old loss 0.000703197147231549 BETTER
I0328 05:24:52.054222 2191560 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0008229644736275077 old loss 0.0008300390327349305 BETTER
I0328 05:24:54.808190 2190740 finetune.py:68] layer 28_up @ epoch 3 new loss 0.000624953827355057 old loss 0.0006290237652137876 BETTER
I0328 05:25:02.905274 2192315 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0015255738981068134 old loss 0.0015615234151482582 BETTER
I0328 05:25:22.990977 2193014 finetune.py:45] layer 31_up initial loss 0.004590808879584074
I0328 05:25:24.167280 2191560 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0008168533095158637 old loss 0.0008229644736275077 BETTER
I0328 05:25:28.614000 2190740 finetune.py:68] layer 28_up @ epoch 4 new loss 0.0006213285378180444 old loss 0.000624953827355057 BETTER
I0328 05:25:35.186845 2192315 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0015019905986264348 old loss 0.0015255738981068134 BETTER
I0328 05:25:53.267398 2193014 finetune.py:68] layer 31_up @ epoch 0 new loss 0.004372226539999247 old loss 0.004590808879584074 BETTER
I0328 05:25:56.046179 2191560 finetune.py:68] layer 29_up @ epoch 4 new loss 0.000811522826552391 old loss 0.0008168533095158637 BETTER
I0328 05:26:00.013339 2190740 finetune.py:45] layer 28_gate initial loss 0.0008322361973114312
I0328 05:26:07.402119 2192315 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0014824419049546123 old loss 0.0015019905986264348 BETTER
I0328 05:26:24.654929 2193014 finetune.py:68] layer 31_up @ epoch 1 new loss 0.004235001280903816 old loss 0.004372226539999247 BETTER
I0328 05:26:27.428870 2191560 finetune.py:45] layer 29_gate initial loss 0.0010922005167230964
I0328 05:26:29.824990 2190740 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.000828178133815527 old loss 0.0008322361973114312 BETTER
I0328 05:26:39.625835 2192315 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0014658200088888407 old loss 0.0014824419049546123 BETTER
I0328 05:26:56.048207 2191560 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0010866352822631598 old loss 0.0010922005167230964 BETTER
I0328 05:26:56.187355 2193014 finetune.py:68] layer 31_up @ epoch 2 new loss 0.004124330822378397 old loss 0.004235001280903816 BETTER
I0328 05:27:01.069416 2190740 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.000824876013211906 old loss 0.000828178133815527 BETTER
I0328 05:27:11.881012 2192315 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0014516526134684682 old loss 0.0014658200088888407 BETTER
I0328 05:27:25.394701 2191560 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.0010821850737556815 old loss 0.0010866352822631598 BETTER
I0328 05:27:27.865632 2193014 finetune.py:68] layer 31_up @ epoch 3 new loss 0.00402760598808527 old loss 0.004124330822378397 BETTER
I0328 05:27:32.433069 2190740 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.0008218192961066961 old loss 0.000824876013211906 BETTER
I0328 05:27:43.928569 2192315 finetune.py:45] layer 30_gate initial loss 0.0018632964929565787
I0328 05:27:55.056332 2191560 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.001078244880773127 old loss 0.0010821850737556815 BETTER
I0328 05:27:59.442137 2193014 finetune.py:68] layer 31_up @ epoch 4 new loss 0.003940539434552193 old loss 0.00402760598808527 BETTER
I0328 05:28:03.766289 2190740 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0008191734668798745 old loss 0.0008218192961066961 BETTER
I0328 05:28:12.567324 2192315 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0018484711181372404 old loss 0.0018632964929565787 BETTER
I0328 05:28:24.751164 2191560 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0010747386841103435 old loss 0.001078244880773127 BETTER
I0328 05:28:30.631148 2193014 finetune.py:45] layer 31_gate initial loss 0.004764835815876722
I0328 05:28:35.208791 2190740 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.0008166247862391174 old loss 0.0008191734668798745 BETTER
I0328 05:28:42.279973 2192315 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.0018373510101810098 old loss 0.0018484711181372404 BETTER
I0328 05:28:54.498125 2191560 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0010715987300500274 old loss 0.0010747386841103435 BETTER
I0328 05:28:58.708568 2193014 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.00468268059194088 old loss 0.004764835815876722 BETTER
I0328 05:29:11.968425 2192315 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0018273969180881977 old loss 0.0018373510101810098 BETTER
I0328 05:29:27.925331 2193014 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.004622879903763533 old loss 0.00468268059194088 BETTER
I0328 05:29:31.794073 2190740 finetune.py:45] layer 28_down initial loss 0.0013280371204018593
I0328 05:29:41.670710 2192315 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.0018183530773967505 old loss 0.0018273969180881977 BETTER
I0328 05:29:51.180943 2191560 finetune.py:45] layer 29_down initial loss 0.0017536389641463757
I0328 05:29:57.420329 2193014 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.004572962410748005 old loss 0.004622879903763533 BETTER
I0328 05:29:59.273890 2190740 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0013279488775879145 old loss 0.0013280371204018593 BETTER
I0328 05:30:11.688135 2192315 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0018104217015206814 old loss 0.0018183530773967505 BETTER
I0328 05:30:17.363719 2191560 finetune.py:68] layer 29_down @ epoch 0 new loss 0.001753532444126904 old loss 0.0017536389641463757 BETTER
I0328 05:30:26.920750 2193014 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.004527079872786999 old loss 0.004572962410748005 BETTER
I0328 05:30:27.926279 2190740 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0013278673868626356 old loss 0.0013279488775879145 BETTER
I0328 05:30:44.553326 2191560 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0017534325597807765 old loss 0.001753532444126904 BETTER
I0328 05:30:56.791443 2190740 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0013277981197461486 old loss 0.0013278673868626356 BETTER
I0328 05:30:56.815038 2193014 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.0044832052662968636 old loss 0.004527079872786999 BETTER
I0328 05:31:09.693542 2192315 finetune.py:45] layer 30_down initial loss 0.0028996386099606752
I0328 05:31:12.066266 2191560 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0017533479258418083 old loss 0.0017534325597807765 BETTER
I0328 05:31:25.736692 2190740 finetune.py:68] layer 28_down @ epoch 3 new loss 0.001327734673395753 old loss 0.0013277981197461486 BETTER
I0328 05:31:36.075870 2192315 finetune.py:68] layer 30_down @ epoch 0 new loss 0.0028995093889534473 old loss 0.0028996386099606752 BETTER
I0328 05:31:39.638681 2191560 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0017532692290842533 old loss 0.0017533479258418083 BETTER
I0328 05:31:53.180751 2193014 finetune.py:45] layer 31_down initial loss 0.007556351367384195
I0328 05:31:54.664011 2190740 finetune.py:68] layer 28_down @ epoch 4 new loss 0.001327670644968748 old loss 0.001327734673395753 BETTER
28_v proxy err 0.023049913346767426 tr(WHW.T) 601.4844360351562
bpp_loss 2.743108664290048
28_q proxy err 0.0024664949160069227 tr(WHW.T) 23188.271484375
bpp_loss 2.9774256837554276
28_k proxy err 0.0010619101813063025 tr(WHW.T) 14976.771484375
bpp_loss 3.6801628003595397
28_o proxy err 0.020795781165361404 tr(WHW.T) 2514.2880859375
bpp_loss 2.5642023908440024
28_up proxy err 0.0246215108782053 tr(WHW.T) 10246.5068359375
bpp_loss 2.532268637392138
28_gate proxy err 0.0075031607411801815 tr(WHW.T) 35872.91015625
bpp_loss 2.848210633067148
28_down proxy err 0.0273087527602911 tr(WHW.T) 7261.8486328125
bpp_loss 2.516284136084973
I0328 05:32:03.743193 2192315 finetune.py:68] layer 30_down @ epoch 1 new loss 0.0028993934392929077 old loss 0.0028995093889534473 BETTER
I0328 05:32:07.466422 2191560 finetune.py:68] layer 29_down @ epoch 4 new loss 0.001753188669681549 old loss 0.0017532692290842533 BETTER
29_v proxy err 0.01773957908153534 tr(WHW.T) 850.4290161132812
bpp_loss 2.807551866164431
29_q proxy err 0.0029741846956312656 tr(WHW.T) 20665.697265625
bpp_loss 2.9692170751513913
29_k proxy err 0.0010612114565446973 tr(WHW.T) 16360.306640625
bpp_loss 3.7534891485993285
29_o proxy err 0.013207264244556427 tr(WHW.T) 3101.735595703125
bpp_loss 2.6014948573429137
29_up proxy err 0.019118038937449455 tr(WHW.T) 12913.15625
bpp_loss 2.565177281486935
29_gate proxy err 0.006824900396168232 tr(WHW.T) 38309.171875
bpp_loss 2.8402604229216064
29_down proxy err 0.022352971136569977 tr(WHW.T) 7537.36572265625
bpp_loss 2.5300579847036198
I0328 05:32:19.224029 2193014 finetune.py:68] layer 31_down @ epoch 0 new loss 0.007555745076388121 old loss 0.007556351367384195 BETTER
I0328 05:32:31.541805 2192315 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0028992800507694483 old loss 0.0028993934392929077 BETTER
I0328 05:32:46.137948 2193014 finetune.py:68] layer 31_down @ epoch 1 new loss 0.007555168122053146 old loss 0.007555745076388121 BETTER
I0328 05:32:59.263730 2192315 finetune.py:68] layer 30_down @ epoch 3 new loss 0.00289917248301208 old loss 0.0028992800507694483 BETTER
I0328 05:33:13.251424 2193014 finetune.py:68] layer 31_down @ epoch 2 new loss 0.007554647047072649 old loss 0.007555168122053146 BETTER
I0328 05:33:27.001681 2192315 finetune.py:68] layer 30_down @ epoch 4 new loss 0.0028990632854402065 old loss 0.00289917248301208 BETTER
30_v proxy err 0.016232626512646675 tr(WHW.T) 863.060791015625
bpp_loss 3.060006795625668
30_q proxy err 0.002226469339802861 tr(WHW.T) 24083.1953125
bpp_loss 2.8784482478513382
30_k proxy err 0.0010551853338256478 tr(WHW.T) 14015.7705078125
bpp_loss 3.4609832727583125
30_o proxy err 0.009645181708037853 tr(WHW.T) 4878.1572265625
bpp_loss 2.6822936244425364
30_up proxy err 0.011352350004017353 tr(WHW.T) 21681.25
bpp_loss 2.5959844671056738
30_gate proxy err 0.005064329598098993 tr(WHW.T) 51663.5546875
bpp_loss 2.8853380094681467
30_down proxy err 0.0127440569922328 tr(WHW.T) 8866.9560546875
bpp_loss 2.5295697602144043
I0328 05:33:40.646892 2193014 finetune.py:68] layer 31_down @ epoch 3 new loss 0.007554152049124241 old loss 0.007554647047072649 BETTER
I0328 05:34:07.997506 2193014 finetune.py:68] layer 31_down @ epoch 4 new loss 0.0075536807999014854 old loss 0.007554152049124241 BETTER
31_v proxy err 0.008128536865115166 tr(WHW.T) 1808.723876953125
bpp_loss 2.9053505409974605
31_q proxy err 0.0012912247329950333 tr(WHW.T) 46196.46875
bpp_loss 3.0171133137191646
31_k proxy err 0.0008064362918958068 tr(WHW.T) 20455.96875
bpp_loss 3.6644359494093806
31_o proxy err 0.007954785600304604 tr(WHW.T) 2212.701416015625
bpp_loss 2.638479278772138
31_up proxy err 0.003519972786307335 tr(WHW.T) 68963.8671875
bpp_loss 2.7781667886967107
31_gate proxy err 0.0018048237543553114 tr(WHW.T) 143143.28125
bpp_loss 3.0814482257701457
31_down proxy err 0.004870342556387186 tr(WHW.T) 10028.9091796875
bpp_loss 2.5568529130292257
I0328 05:34:38.580815 2206639 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:34:38.581037 2206639 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:34:38.581093 2206639 utils.py:162] NumExpr defaulting to 16 threads.
I0328 05:34:38.897156 2206639 config.py:54] PyTorch version 2.6.0 available.
W0328 05:34:39.103600 2206639 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 05:34:39.215439 2206639 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.45it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.41it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.80it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  9.09it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.06it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.32it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.99it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.77it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.92it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.97it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.70it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.84it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.10it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.95it/s]
I0328 05:34:41.926013 2206639 hfize_llama.py:153] loaded layer 0
I0328 05:34:42.230833 2206639 hfize_llama.py:153] loaded layer 1
I0328 05:34:42.534476 2206639 hfize_llama.py:153] loaded layer 2
I0328 05:34:42.826915 2206639 hfize_llama.py:153] loaded layer 3
I0328 05:34:43.138557 2206639 hfize_llama.py:153] loaded layer 4
I0328 05:34:43.431888 2206639 hfize_llama.py:153] loaded layer 5
I0328 05:34:43.719412 2206639 hfize_llama.py:153] loaded layer 6
I0328 05:34:44.038654 2206639 hfize_llama.py:153] loaded layer 7
I0328 05:34:44.327032 2206639 hfize_llama.py:153] loaded layer 8
I0328 05:34:44.597628 2206639 hfize_llama.py:153] loaded layer 9
I0328 05:34:44.893402 2206639 hfize_llama.py:153] loaded layer 10
I0328 05:34:45.189360 2206639 hfize_llama.py:153] loaded layer 11
I0328 05:34:45.478334 2206639 hfize_llama.py:153] loaded layer 12
I0328 05:34:45.738263 2206639 hfize_llama.py:153] loaded layer 13
I0328 05:34:46.158975 2206639 hfize_llama.py:153] loaded layer 14
I0328 05:34:46.580151 2206639 hfize_llama.py:153] loaded layer 15
I0328 05:34:46.920064 2206639 hfize_llama.py:153] loaded layer 16
I0328 05:34:47.247988 2206639 hfize_llama.py:153] loaded layer 17
I0328 05:34:47.529229 2206639 hfize_llama.py:153] loaded layer 18
I0328 05:34:47.841156 2206639 hfize_llama.py:153] loaded layer 19
I0328 05:34:48.206069 2206639 hfize_llama.py:153] loaded layer 20
I0328 05:34:48.608546 2206639 hfize_llama.py:153] loaded layer 21
I0328 05:34:49.026594 2206639 hfize_llama.py:153] loaded layer 22
I0328 05:34:49.362137 2206639 hfize_llama.py:153] loaded layer 23
I0328 05:34:49.736981 2206639 hfize_llama.py:153] loaded layer 24
I0328 05:34:50.072611 2206639 hfize_llama.py:153] loaded layer 25
I0328 05:34:50.409992 2206639 hfize_llama.py:153] loaded layer 26
I0328 05:34:50.764907 2206639 hfize_llama.py:153] loaded layer 27
I0328 05:34:51.112843 2206639 hfize_llama.py:153] loaded layer 28
I0328 05:34:51.446800 2206639 hfize_llama.py:153] loaded layer 29
I0328 05:34:51.777450 2206639 hfize_llama.py:153] loaded layer 30
I0328 05:34:52.107910 2206639 hfize_llama.py:153] loaded layer 31
I0328 05:34:52.108025 2206639 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:02<00:12,  2.00s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:07,  1.48s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:05,  1.31s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:05<00:03,  1.19s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.16s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.10s/it]
I0328 05:35:34.424829 2206639 hfize_llama.py:167] successfully loaded hfized model
I0328 05:35:39.586563 2207475 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0328 05:35:39.586782 2207475 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0328 05:35:39.586824 2207475 utils.py:162] NumExpr defaulting to 16 threads.
W0328 05:35:39.929537 2207475 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0328 05:35:40.297638 2207475 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.18s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.13s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.14s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.13s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.10s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.12s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.06s/it]
I0328 05:35:47.819514 2207475 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.8174588680267334:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.8174588680267334:   1%|          | 1/141 [00:01<04:22,  1.88s/it]avg_loss = 2.094943642616272:   1%|          | 1/141 [00:03<04:22,  1.88s/it] avg_loss = 2.094943642616272:   1%|▏         | 2/141 [00:03<03:48,  1.65s/it]avg_loss = 2.2238597869873047:   1%|▏         | 2/141 [00:04<03:48,  1.65s/it]avg_loss = 2.2238597869873047:   2%|▏         | 3/141 [00:04<03:37,  1.58s/it]avg_loss = 2.17338103055954:   2%|▏         | 3/141 [00:06<03:37,  1.58s/it]  avg_loss = 2.17338103055954:   3%|▎         | 4/141 [00:06<03:31,  1.54s/it]avg_loss = 2.1234321117401125:   3%|▎         | 4/141 [00:07<03:31,  1.54s/it]avg_loss = 2.1234321117401125:   4%|▎         | 5/141 [00:07<03:27,  1.53s/it]avg_loss = 2.0266387462615967:   4%|▎         | 5/141 [00:09<03:27,  1.53s/it]avg_loss = 2.0266387462615967:   4%|▍         | 6/141 [00:09<03:25,  1.52s/it]avg_loss = 1.9680101871490479:   4%|▍         | 6/141 [00:10<03:25,  1.52s/it]avg_loss = 1.9680101871490479:   5%|▍         | 7/141 [00:10<03:23,  1.52s/it]avg_loss = 1.9628456830978394:   5%|▍         | 7/141 [00:12<03:23,  1.52s/it]avg_loss = 1.9628456830978394:   6%|▌         | 8/141 [00:12<03:21,  1.52s/it]avg_loss = 1.9957057105170355:   6%|▌         | 8/141 [00:13<03:21,  1.52s/it]avg_loss = 1.9957057105170355:   6%|▋         | 9/141 [00:13<03:20,  1.52s/it]avg_loss = 1.9917358994483947:   6%|▋         | 9/141 [00:15<03:20,  1.52s/it]avg_loss = 1.9917358994483947:   7%|▋         | 10/141 [00:15<03:18,  1.52s/it]avg_loss = 1.9827698469161987:   7%|▋         | 10/141 [00:16<03:18,  1.52s/it]avg_loss = 1.9827698469161987:   8%|▊         | 11/141 [00:16<03:17,  1.52s/it]avg_loss = 2.0026913781960807:   8%|▊         | 11/141 [00:18<03:17,  1.52s/it]avg_loss = 2.0026913781960807:   9%|▊         | 12/141 [00:18<03:15,  1.52s/it]avg_loss = 2.0117353200912476:   9%|▊         | 12/141 [00:19<03:15,  1.52s/it]avg_loss = 2.0117353200912476:   9%|▉         | 13/141 [00:19<03:14,  1.52s/it]avg_loss = 2.0268976943833485:   9%|▉         | 13/141 [00:21<03:14,  1.52s/it]avg_loss = 2.0268976943833485:  10%|▉         | 14/141 [00:21<03:13,  1.52s/it]avg_loss = 2.0356215556462605:  10%|▉         | 14/141 [00:23<03:13,  1.52s/it]avg_loss = 2.0356215556462605:  11%|█         | 15/141 [00:23<03:12,  1.53s/it]avg_loss = 2.0566095784306526:  11%|█         | 15/141 [00:24<03:12,  1.53s/it]avg_loss = 2.0566095784306526:  11%|█▏        | 16/141 [00:24<03:10,  1.53s/it]avg_loss = 2.058326279415804:  11%|█▏        | 16/141 [00:26<03:10,  1.53s/it] avg_loss = 2.058326279415804:  12%|█▏        | 17/141 [00:26<03:09,  1.53s/it]avg_loss = 2.0610019034809537:  12%|█▏        | 17/141 [00:27<03:09,  1.53s/it]avg_loss = 2.0610019034809537:  13%|█▎        | 18/141 [00:27<03:08,  1.53s/it]avg_loss = 2.051887135756643:  13%|█▎        | 18/141 [00:29<03:08,  1.53s/it] avg_loss = 2.051887135756643:  13%|█▎        | 19/141 [00:29<03:06,  1.53s/it]avg_loss = 2.050211954116821:  13%|█▎        | 19/141 [00:30<03:06,  1.53s/it]avg_loss = 2.050211954116821:  14%|█▍        | 20/141 [00:30<03:05,  1.53s/it]avg_loss = 2.0549241361163912:  14%|█▍        | 20/141 [00:32<03:05,  1.53s/it]avg_loss = 2.0549241361163912:  15%|█▍        | 21/141 [00:32<03:04,  1.53s/it]avg_loss = 2.057290564883839:  15%|█▍        | 21/141 [00:33<03:04,  1.53s/it] avg_loss = 2.057290564883839:  16%|█▌        | 22/141 [00:33<03:02,  1.54s/it]avg_loss = 2.059074640274048:  16%|█▌        | 22/141 [00:35<03:02,  1.54s/it]avg_loss = 2.059074640274048:  16%|█▋        | 23/141 [00:35<03:01,  1.54s/it]avg_loss = 2.0641030967235565:  16%|█▋        | 23/141 [00:36<03:01,  1.54s/it]avg_loss = 2.0641030967235565:  17%|█▋        | 24/141 [00:36<03:00,  1.54s/it]avg_loss = 2.070340337753296:  17%|█▋        | 24/141 [00:38<03:00,  1.54s/it] avg_loss = 2.070340337753296:  18%|█▊        | 25/141 [00:38<02:58,  1.54s/it]avg_loss = 2.080895524758559:  18%|█▊        | 25/141 [00:39<02:58,  1.54s/it]avg_loss = 2.080895524758559:  18%|█▊        | 26/141 [00:39<02:57,  1.54s/it]avg_loss = 2.0919106624744557:  18%|█▊        | 26/141 [00:41<02:57,  1.54s/it]avg_loss = 2.0919106624744557:  19%|█▉        | 27/141 [00:41<02:55,  1.54s/it]avg_loss = 2.0959652832576205:  19%|█▉        | 27/141 [00:43<02:55,  1.54s/it]avg_loss = 2.0959652832576205:  20%|█▉        | 28/141 [00:43<02:54,  1.55s/it]avg_loss = 2.0926347025509537:  20%|█▉        | 28/141 [00:44<02:54,  1.55s/it]avg_loss = 2.0926347025509537:  21%|██        | 29/141 [00:44<02:53,  1.55s/it]avg_loss = 2.0848116834958392:  21%|██        | 29/141 [00:46<02:53,  1.55s/it]avg_loss = 2.0848116834958392:  21%|██▏       | 30/141 [00:46<02:52,  1.55s/it]avg_loss = 2.0743405280574674:  21%|██▏       | 30/141 [00:47<02:52,  1.55s/it]avg_loss = 2.0743405280574674:  22%|██▏       | 31/141 [00:47<02:51,  1.56s/it]avg_loss = 2.0650401823222637:  22%|██▏       | 31/141 [00:49<02:51,  1.56s/it]avg_loss = 2.0650401823222637:  23%|██▎       | 32/141 [00:49<02:49,  1.56s/it]avg_loss = 2.06308435310017:  23%|██▎       | 32/141 [00:50<02:49,  1.56s/it]  avg_loss = 2.06308435310017:  23%|██▎       | 33/141 [00:50<02:48,  1.56s/it]avg_loss = 2.0616183035513935:  23%|██▎       | 33/141 [00:52<02:48,  1.56s/it]avg_loss = 2.0616183035513935:  24%|██▍       | 34/141 [00:52<02:46,  1.56s/it]avg_loss = 2.0631916352680753:  24%|██▍       | 34/141 [00:53<02:46,  1.56s/it]avg_loss = 2.0631916352680753:  25%|██▍       | 35/141 [00:53<02:45,  1.56s/it]avg_loss = 2.0460619926452637:  25%|██▍       | 35/141 [00:55<02:45,  1.56s/it]avg_loss = 2.0460619926452637:  26%|██▌       | 36/141 [00:55<02:43,  1.56s/it]avg_loss = 2.0304781262939042:  26%|██▌       | 36/141 [00:57<02:43,  1.56s/it]avg_loss = 2.0304781262939042:  26%|██▌       | 37/141 [00:57<02:42,  1.56s/it]avg_loss = 2.014886272581:  26%|██▌       | 37/141 [00:58<02:42,  1.56s/it]    avg_loss = 2.014886272581:  27%|██▋       | 38/141 [00:58<02:41,  1.56s/it]avg_loss = 2.0003392085050926:  27%|██▋       | 38/141 [01:00<02:41,  1.56s/it]avg_loss = 2.0003392085050926:  28%|██▊       | 39/141 [01:00<02:39,  1.56s/it]avg_loss = 1.991167390346527:  28%|██▊       | 39/141 [01:01<02:39,  1.56s/it] avg_loss = 1.991167390346527:  28%|██▊       | 40/141 [01:01<02:37,  1.56s/it]avg_loss = 1.9966490036103783:  28%|██▊       | 40/141 [01:03<02:37,  1.56s/it]avg_loss = 1.9966490036103783:  29%|██▉       | 41/141 [01:03<02:36,  1.57s/it]avg_loss = 2.0121192194166637:  29%|██▉       | 41/141 [01:04<02:36,  1.57s/it]avg_loss = 2.0121192194166637:  30%|██▉       | 42/141 [01:04<02:35,  1.57s/it]avg_loss = 2.028261417566344:  30%|██▉       | 42/141 [01:06<02:35,  1.57s/it] avg_loss = 2.028261417566344:  30%|███       | 43/141 [01:06<02:33,  1.57s/it]avg_loss = 2.0341760516166687:  30%|███       | 43/141 [01:08<02:33,  1.57s/it]avg_loss = 2.0341760516166687:  31%|███       | 44/141 [01:08<02:32,  1.57s/it]avg_loss = 2.03974715868632:  31%|███       | 44/141 [01:09<02:32,  1.57s/it]  avg_loss = 2.03974715868632:  32%|███▏      | 45/141 [01:09<02:30,  1.57s/it]avg_loss = 2.0435034554937612:  32%|███▏      | 45/141 [01:11<02:30,  1.57s/it]avg_loss = 2.0435034554937612:  33%|███▎      | 46/141 [01:11<02:29,  1.57s/it]avg_loss = 2.0486001917656433:  33%|███▎      | 46/141 [01:12<02:29,  1.57s/it]avg_loss = 2.0486001917656433:  33%|███▎      | 47/141 [01:12<02:27,  1.57s/it]avg_loss = 2.0503719250361123:  33%|███▎      | 47/141 [01:14<02:27,  1.57s/it]avg_loss = 2.0503719250361123:  34%|███▍      | 48/141 [01:14<02:26,  1.57s/it]avg_loss = 2.0500541949758726:  34%|███▍      | 48/141 [01:15<02:26,  1.57s/it]avg_loss = 2.0500541949758726:  35%|███▍      | 49/141 [01:15<02:24,  1.57s/it]avg_loss = 2.04965594291687:  35%|███▍      | 49/141 [01:17<02:24,  1.57s/it]  avg_loss = 2.04965594291687:  35%|███▌      | 50/141 [01:17<02:23,  1.57s/it]avg_loss = 2.044750321145151:  35%|███▌      | 50/141 [01:19<02:23,  1.57s/it]avg_loss = 2.044750321145151:  36%|███▌      | 51/141 [01:19<02:21,  1.57s/it]avg_loss = 2.040212083321351:  36%|███▌      | 51/141 [01:20<02:21,  1.57s/it]avg_loss = 2.040212083321351:  37%|███▋      | 52/141 [01:20<02:20,  1.57s/it]avg_loss = 2.0333680926628834:  37%|███▋      | 52/141 [01:22<02:20,  1.57s/it]avg_loss = 2.0333680926628834:  38%|███▊      | 53/141 [01:22<02:18,  1.58s/it]avg_loss = 2.0299108271245605:  38%|███▊      | 53/141 [01:23<02:18,  1.58s/it]avg_loss = 2.0299108271245605:  38%|███▊      | 54/141 [01:23<02:17,  1.58s/it]avg_loss = 2.0217205134305085:  38%|███▊      | 54/141 [01:25<02:17,  1.58s/it]avg_loss = 2.0217205134305085:  39%|███▉      | 55/141 [01:25<02:15,  1.58s/it]avg_loss = 2.013679480978421:  39%|███▉      | 55/141 [01:27<02:15,  1.58s/it] avg_loss = 2.013679480978421:  40%|███▉      | 56/141 [01:27<02:14,  1.58s/it]avg_loss = 2.0111360759065864:  40%|███▉      | 56/141 [01:28<02:14,  1.58s/it]avg_loss = 2.0111360759065864:  40%|████      | 57/141 [01:28<02:12,  1.58s/it]avg_loss = 2.0085380735068488:  40%|████      | 57/141 [01:30<02:12,  1.58s/it]avg_loss = 2.0085380735068488:  41%|████      | 58/141 [01:30<02:11,  1.58s/it]avg_loss = 2.0103930699623236:  41%|████      | 58/141 [01:31<02:11,  1.58s/it]avg_loss = 2.0103930699623236:  42%|████▏     | 59/141 [01:31<02:09,  1.58s/it]avg_loss = 2.0153173009554544:  42%|████▏     | 59/141 [01:33<02:09,  1.58s/it]avg_loss = 2.0153173009554544:  43%|████▎     | 60/141 [01:33<02:07,  1.58s/it]avg_loss = 2.0200860422165667:  43%|████▎     | 60/141 [01:34<02:07,  1.58s/it]avg_loss = 2.0200860422165667:  43%|████▎     | 61/141 [01:34<02:06,  1.58s/it]avg_loss = 2.0272523818477506:  43%|████▎     | 61/141 [01:36<02:06,  1.58s/it]avg_loss = 2.0272523818477506:  44%|████▍     | 62/141 [01:36<02:04,  1.58s/it]avg_loss = 2.0192333213866704:  44%|████▍     | 62/141 [01:38<02:04,  1.58s/it]avg_loss = 2.0192333213866704:  45%|████▍     | 63/141 [01:38<02:03,  1.58s/it]avg_loss = 2.016572652384639:  45%|████▍     | 63/141 [01:39<02:03,  1.58s/it] avg_loss = 2.016572652384639:  45%|████▌     | 64/141 [01:39<02:01,  1.58s/it]avg_loss = 2.014485847032987:  45%|████▌     | 64/141 [01:41<02:01,  1.58s/it]avg_loss = 2.014485847032987:  46%|████▌     | 65/141 [01:41<02:00,  1.58s/it]avg_loss = 2.008728417483243:  46%|████▌     | 65/141 [01:42<02:00,  1.58s/it]avg_loss = 2.008728417483243:  47%|████▋     | 66/141 [01:42<01:58,  1.59s/it]avg_loss = 2.0056199226806415:  47%|████▋     | 66/141 [01:44<01:58,  1.59s/it]avg_loss = 2.0056199226806415:  48%|████▊     | 67/141 [01:44<01:57,  1.59s/it]avg_loss = 2.0030472804518307:  48%|████▊     | 67/141 [01:46<01:57,  1.59s/it]avg_loss = 2.0030472804518307:  48%|████▊     | 68/141 [01:46<01:55,  1.59s/it]avg_loss = 2.000800093015035:  48%|████▊     | 68/141 [01:47<01:55,  1.59s/it] avg_loss = 2.000800093015035:  49%|████▉     | 69/141 [01:47<01:53,  1.58s/it]avg_loss = 2.0013801625796726:  49%|████▉     | 69/141 [01:49<01:53,  1.58s/it]avg_loss = 2.0013801625796726:  50%|████▉     | 70/141 [01:49<01:52,  1.58s/it]avg_loss = 2.0046491471814436:  50%|████▉     | 70/141 [01:50<01:52,  1.58s/it]avg_loss = 2.0046491471814436:  50%|█████     | 71/141 [01:50<01:50,  1.59s/it]avg_loss = 2.006659376952383:  50%|█████     | 71/141 [01:52<01:50,  1.59s/it] avg_loss = 2.006659376952383:  51%|█████     | 72/141 [01:52<01:49,  1.59s/it]avg_loss = 2.0047563428748143:  51%|█████     | 72/141 [01:53<01:49,  1.59s/it]avg_loss = 2.0047563428748143:  52%|█████▏    | 73/141 [01:53<01:47,  1.59s/it]avg_loss = 2.0064327910139754:  52%|█████▏    | 73/141 [01:55<01:47,  1.59s/it]avg_loss = 2.0064327910139754:  52%|█████▏    | 74/141 [01:55<01:46,  1.59s/it]avg_loss = 2.0060326274236044:  52%|█████▏    | 74/141 [01:57<01:46,  1.59s/it]avg_loss = 2.0060326274236044:  53%|█████▎    | 75/141 [01:57<01:44,  1.59s/it]avg_loss = 2.0044195636322626:  53%|█████▎    | 75/141 [01:58<01:44,  1.59s/it]avg_loss = 2.0044195636322626:  54%|█████▍    | 76/141 [01:58<01:43,  1.59s/it]avg_loss = 2.0057009520468774:  54%|█████▍    | 76/141 [02:00<01:43,  1.59s/it]avg_loss = 2.0057009520468774:  55%|█████▍    | 77/141 [02:00<01:41,  1.59s/it]avg_loss = 2.007923500660138:  55%|█████▍    | 77/141 [02:01<01:41,  1.59s/it] avg_loss = 2.007923500660138:  55%|█████▌    | 78/141 [02:01<01:39,  1.59s/it]avg_loss = 2.0114392555212675:  55%|█████▌    | 78/141 [02:03<01:39,  1.59s/it]avg_loss = 2.0114392555212675:  56%|█████▌    | 79/141 [02:03<01:38,  1.59s/it]avg_loss = 2.007445713877678:  56%|█████▌    | 79/141 [02:05<01:38,  1.59s/it] avg_loss = 2.007445713877678:  57%|█████▋    | 80/141 [02:05<01:36,  1.59s/it]avg_loss = 2.0061456671467535:  57%|█████▋    | 80/141 [02:06<01:36,  1.59s/it]avg_loss = 2.0061456671467535:  57%|█████▋    | 81/141 [02:06<01:35,  1.59s/it]avg_loss = 2.005217905451612:  57%|█████▋    | 81/141 [02:08<01:35,  1.59s/it] avg_loss = 2.005217905451612:  58%|█████▊    | 82/141 [02:08<01:33,  1.59s/it]avg_loss = 2.003264692892511:  58%|█████▊    | 82/141 [02:09<01:33,  1.59s/it]avg_loss = 2.003264692892511:  59%|█████▉    | 83/141 [02:09<01:32,  1.59s/it]avg_loss = 2.0008003214995065:  59%|█████▉    | 83/141 [02:11<01:32,  1.59s/it]avg_loss = 2.0008003214995065:  60%|█████▉    | 84/141 [02:11<01:30,  1.59s/it]avg_loss = 1.998432853642632:  60%|█████▉    | 84/141 [02:12<01:30,  1.59s/it] avg_loss = 1.998432853642632:  60%|██████    | 85/141 [02:12<01:28,  1.59s/it]avg_loss = 1.9996380459430605:  60%|██████    | 85/141 [02:14<01:28,  1.59s/it]avg_loss = 1.9996380459430605:  61%|██████    | 86/141 [02:14<01:27,  1.59s/it]avg_loss = 2.0012701996441544:  61%|██████    | 86/141 [02:16<01:27,  1.59s/it]avg_loss = 2.0012701996441544:  62%|██████▏   | 87/141 [02:16<01:25,  1.59s/it]avg_loss = 2.002603914250027:  62%|██████▏   | 87/141 [02:17<01:25,  1.59s/it] avg_loss = 2.002603914250027:  62%|██████▏   | 88/141 [02:17<01:24,  1.59s/it]avg_loss = 2.0111487380574258:  62%|██████▏   | 88/141 [02:19<01:24,  1.59s/it]avg_loss = 2.0111487380574258:  63%|██████▎   | 89/141 [02:19<01:22,  1.59s/it]avg_loss = 2.0184127026134067:  63%|██████▎   | 89/141 [02:20<01:22,  1.59s/it]avg_loss = 2.0184127026134067:  64%|██████▍   | 90/141 [02:20<01:21,  1.59s/it]avg_loss = 2.0216107486368537:  64%|██████▍   | 90/141 [02:22<01:21,  1.59s/it]avg_loss = 2.0216107486368537:  65%|██████▍   | 91/141 [02:22<01:19,  1.59s/it]avg_loss = 2.0266022954298104:  65%|██████▍   | 91/141 [02:24<01:19,  1.59s/it]avg_loss = 2.0266022954298104:  65%|██████▌   | 92/141 [02:24<01:18,  1.59s/it]avg_loss = 2.0315003023352673:  65%|██████▌   | 92/141 [02:25<01:18,  1.59s/it]avg_loss = 2.0315003023352673:  66%|██████▌   | 93/141 [02:25<01:16,  1.59s/it]avg_loss = 2.0319226536344974:  66%|██████▌   | 93/141 [02:27<01:16,  1.59s/it]avg_loss = 2.0319226536344974:  67%|██████▋   | 94/141 [02:27<01:14,  1.59s/it]avg_loss = 2.0355895782771865:  67%|██████▋   | 94/141 [02:28<01:14,  1.59s/it]avg_loss = 2.0355895782771865:  67%|██████▋   | 95/141 [02:28<01:13,  1.59s/it]avg_loss = 2.036000898728768:  67%|██████▋   | 95/141 [02:30<01:13,  1.59s/it] avg_loss = 2.036000898728768:  68%|██████▊   | 96/141 [02:30<01:11,  1.59s/it]avg_loss = 2.0375523333696974:  68%|██████▊   | 96/141 [02:32<01:11,  1.59s/it]avg_loss = 2.0375523333696974:  69%|██████▉   | 97/141 [02:32<01:10,  1.59s/it]avg_loss = 2.0352655442393557:  69%|██████▉   | 97/141 [02:33<01:10,  1.59s/it]avg_loss = 2.0352655442393557:  70%|██████▉   | 98/141 [02:33<01:08,  1.59s/it]avg_loss = 2.0364031201661237:  70%|██████▉   | 98/141 [02:35<01:08,  1.59s/it]avg_loss = 2.0364031201661237:  70%|███████   | 99/141 [02:35<01:07,  1.60s/it]avg_loss = 2.0388531672954557:  70%|███████   | 99/141 [02:36<01:07,  1.60s/it]avg_loss = 2.0388531672954557:  71%|███████   | 100/141 [02:36<01:05,  1.60s/it]avg_loss = 2.0387292510212056:  71%|███████   | 100/141 [02:38<01:05,  1.60s/it]avg_loss = 2.0387292510212056:  72%|███████▏  | 101/141 [02:38<01:03,  1.59s/it]avg_loss = 2.039684848458159:  72%|███████▏  | 101/141 [02:40<01:03,  1.59s/it] avg_loss = 2.039684848458159:  72%|███████▏  | 102/141 [02:40<01:02,  1.59s/it]avg_loss = 2.039478185107407:  72%|███████▏  | 102/141 [02:41<01:02,  1.59s/it]avg_loss = 2.039478185107407:  73%|███████▎  | 103/141 [02:41<01:00,  1.60s/it]avg_loss = 2.0431300504849506:  73%|███████▎  | 103/141 [02:43<01:00,  1.60s/it]avg_loss = 2.0431300504849506:  74%|███████▍  | 104/141 [02:43<00:59,  1.59s/it]avg_loss = 2.042824977920169:  74%|███████▍  | 104/141 [02:44<00:59,  1.59s/it] avg_loss = 2.042824977920169:  74%|███████▍  | 105/141 [02:44<00:57,  1.60s/it]avg_loss = 2.0426921630805395:  74%|███████▍  | 105/141 [02:46<00:57,  1.60s/it]avg_loss = 2.0426921630805395:  75%|███████▌  | 106/141 [02:46<00:55,  1.60s/it]avg_loss = 2.041132577111788:  75%|███████▌  | 106/141 [02:48<00:55,  1.60s/it] avg_loss = 2.041132577111788:  76%|███████▌  | 107/141 [02:48<00:54,  1.60s/it]avg_loss = 2.0392142214156963:  76%|███████▌  | 107/141 [02:49<00:54,  1.60s/it]avg_loss = 2.0392142214156963:  77%|███████▋  | 108/141 [02:49<00:52,  1.60s/it]avg_loss = 2.037441142108462:  77%|███████▋  | 108/141 [02:51<00:52,  1.60s/it] avg_loss = 2.037441142108462:  77%|███████▋  | 109/141 [02:51<00:51,  1.60s/it]avg_loss = 2.035221867127852:  77%|███████▋  | 109/141 [02:52<00:51,  1.60s/it]avg_loss = 2.035221867127852:  78%|███████▊  | 110/141 [02:52<00:49,  1.60s/it]avg_loss = 2.037752048389332:  78%|███████▊  | 110/141 [02:54<00:49,  1.60s/it]avg_loss = 2.037752048389332:  79%|███████▊  | 111/141 [02:54<00:47,  1.59s/it]avg_loss = 2.0373098381928036:  79%|███████▊  | 111/141 [02:56<00:47,  1.59s/it]avg_loss = 2.0373098381928036:  79%|███████▉  | 112/141 [02:56<00:46,  1.59s/it]avg_loss = 2.03831221150086:  79%|███████▉  | 112/141 [02:57<00:46,  1.59s/it]  avg_loss = 2.03831221150086:  80%|████████  | 113/141 [02:57<00:44,  1.59s/it]avg_loss = 2.039424444499769:  80%|████████  | 113/141 [02:59<00:44,  1.59s/it]avg_loss = 2.039424444499769:  81%|████████  | 114/141 [02:59<00:43,  1.59s/it]avg_loss = 2.0385524563167405:  81%|████████  | 114/141 [03:00<00:43,  1.59s/it]avg_loss = 2.0385524563167405:  82%|████████▏ | 115/141 [03:00<00:41,  1.59s/it]avg_loss = 2.037583442597554:  82%|████████▏ | 115/141 [03:02<00:41,  1.59s/it] avg_loss = 2.037583442597554:  82%|████████▏ | 116/141 [03:02<00:39,  1.59s/it]avg_loss = 2.039760158612178:  82%|████████▏ | 116/141 [03:04<00:39,  1.59s/it]avg_loss = 2.039760158612178:  83%|████████▎ | 117/141 [03:04<00:38,  1.59s/it]avg_loss = 2.0390898029683:  83%|████████▎ | 117/141 [03:05<00:38,  1.59s/it]  avg_loss = 2.0390898029683:  84%|████████▎ | 118/141 [03:05<00:36,  1.59s/it]avg_loss = 2.0375175185564185:  84%|████████▎ | 118/141 [03:07<00:36,  1.59s/it]avg_loss = 2.0375175185564185:  84%|████████▍ | 119/141 [03:07<00:35,  1.60s/it]avg_loss = 2.035639555255572:  84%|████████▍ | 119/141 [03:08<00:35,  1.60s/it] avg_loss = 2.035639555255572:  85%|████████▌ | 120/141 [03:08<00:33,  1.59s/it]avg_loss = 2.0357410858485325:  85%|████████▌ | 120/141 [03:10<00:33,  1.59s/it]avg_loss = 2.0357410858485325:  86%|████████▌ | 121/141 [03:10<00:31,  1.59s/it]avg_loss = 2.0363530164859336:  86%|████████▌ | 121/141 [03:11<00:31,  1.59s/it]avg_loss = 2.0363530164859336:  87%|████████▋ | 122/141 [03:11<00:30,  1.59s/it]avg_loss = 2.0357148123950495:  87%|████████▋ | 122/141 [03:13<00:30,  1.59s/it]avg_loss = 2.0357148123950495:  87%|████████▋ | 123/141 [03:13<00:28,  1.59s/it]avg_loss = 2.0357277508704894:  87%|████████▋ | 123/141 [03:15<00:28,  1.59s/it]avg_loss = 2.0357277508704894:  88%|████████▊ | 124/141 [03:15<00:27,  1.59s/it]avg_loss = 2.0341719970703127:  88%|████████▊ | 124/141 [03:16<00:27,  1.59s/it]avg_loss = 2.0341719970703127:  89%|████████▊ | 125/141 [03:16<00:25,  1.59s/it]avg_loss = 2.0340911982551453:  89%|████████▊ | 125/141 [03:18<00:25,  1.59s/it]avg_loss = 2.0340911982551453:  89%|████████▉ | 126/141 [03:18<00:23,  1.59s/it]avg_loss = 2.0337387895959567:  89%|████████▉ | 126/141 [03:19<00:23,  1.59s/it]avg_loss = 2.0337387895959567:  90%|█████████ | 127/141 [03:19<00:22,  1.60s/it]avg_loss = 2.0325666926801205:  90%|█████████ | 127/141 [03:21<00:22,  1.60s/it]avg_loss = 2.0325666926801205:  91%|█████████ | 128/141 [03:21<00:20,  1.60s/it]avg_loss = 2.0324074852374174:  91%|█████████ | 128/141 [03:23<00:20,  1.60s/it]avg_loss = 2.0324074852374174:  91%|█████████▏| 129/141 [03:23<00:19,  1.60s/it]avg_loss = 2.0334234145971446:  91%|█████████▏| 129/141 [03:24<00:19,  1.60s/it]avg_loss = 2.0334234145971446:  92%|█████████▏| 130/141 [03:24<00:17,  1.60s/it]avg_loss = 2.0343170621012914:  92%|█████████▏| 130/141 [03:26<00:17,  1.60s/it]avg_loss = 2.0343170621012914:  93%|█████████▎| 131/141 [03:26<00:15,  1.59s/it]avg_loss = 2.034810626145565:  93%|█████████▎| 131/141 [03:27<00:15,  1.59s/it] avg_loss = 2.034810626145565:  94%|█████████▎| 132/141 [03:27<00:14,  1.59s/it]avg_loss = 2.031685009038538:  94%|█████████▎| 132/141 [03:29<00:14,  1.59s/it]avg_loss = 2.031685009038538:  94%|█████████▍| 133/141 [03:29<00:12,  1.59s/it]avg_loss = 2.026761014070084:  94%|█████████▍| 133/141 [03:31<00:12,  1.59s/it]avg_loss = 2.026761014070084:  95%|█████████▌| 134/141 [03:31<00:11,  1.59s/it]avg_loss = 2.028982111259743:  95%|█████████▌| 134/141 [03:32<00:11,  1.59s/it]avg_loss = 2.028982111259743:  96%|█████████▌| 135/141 [03:32<00:09,  1.59s/it]avg_loss = 2.0324198936714843:  96%|█████████▌| 135/141 [03:34<00:09,  1.59s/it]avg_loss = 2.0324198936714843:  96%|█████████▋| 136/141 [03:34<00:07,  1.59s/it]avg_loss = 2.0338871948910455:  96%|█████████▋| 136/141 [03:35<00:07,  1.59s/it]avg_loss = 2.0338871948910455:  97%|█████████▋| 137/141 [03:35<00:06,  1.59s/it]avg_loss = 2.0329847508582515:  97%|█████████▋| 137/141 [03:37<00:06,  1.59s/it]avg_loss = 2.0329847508582515:  98%|█████████▊| 138/141 [03:37<00:04,  1.59s/it]avg_loss = 2.033633815298835:  98%|█████████▊| 138/141 [03:39<00:04,  1.59s/it] avg_loss = 2.033633815298835:  99%|█████████▊| 139/141 [03:39<00:03,  1.59s/it]avg_loss = 2.0346065964017597:  99%|█████████▊| 139/141 [03:40<00:03,  1.59s/it]avg_loss = 2.0346065964017597:  99%|█████████▉| 140/141 [03:40<00:01,  1.59s/it]avg_loss = 2.03591554384705:  99%|█████████▉| 140/141 [03:42<00:01,  1.59s/it]  avg_loss = 2.03591554384705: 100%|██████████| 141/141 [03:42<00:00,  1.59s/it]avg_loss = 2.03591554384705: 100%|██████████| 141/141 [03:42<00:00,  1.58s/it]
I0328 05:39:54.084365 2207475 eval_ppl.py:107] wikitext2 perplexity: 7.659261703491211
wikitext2 perplexity: 7.659
