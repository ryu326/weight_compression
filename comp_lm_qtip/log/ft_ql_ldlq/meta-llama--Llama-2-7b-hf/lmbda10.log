I0325 09:26:25.465235 583434 config.py:54] PyTorch version 2.6.0 available.
W0325 09:26:25.749890 583434 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 09:26:26.652188 583434 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.23it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.76it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.79it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.89it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  7.80it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.70it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.72it/s]
I0325 09:26:27.640147 583434 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:14,  2.09it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:14,  2.09it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:13,  2.10it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:13,  2.10it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:12,  2.11it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:12,  2.10it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:11,  2.10it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:11,  2.10it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:11,  2.09it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:04<00:10,  2.08it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:10,  2.09it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:05<00:09,  2.10it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:08,  2.12it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:06<00:08,  2.11it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:08,  2.11it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:07<00:07,  2.16it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:08<00:06,  2.21it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:08<00:06,  2.25it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:08<00:05,  2.28it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:09<00:05,  2.30it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:09<00:04,  2.32it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:10<00:04,  2.32it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:10<00:03,  2.33it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:10<00:03,  2.34it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:11<00:02,  2.35it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:11<00:02,  2.37it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:12<00:02,  2.38it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:12<00:01,  2.36it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:13<00:01,  2.37it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:13<00:00,  2.36it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:13<00:00,  2.36it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:14<00:00,  2.37it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:14<00:00,  2.23it/s]
I0325 09:26:48.016694 583434 quantize_finetune_llama.py:185] loaded compression model
I0325 09:27:03.096550 583434 quantize_finetune_llama.py:189] loaded dataset and devset
I0325 09:27:07.587165 583434 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 09:28:16.119914 583434 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 68.3882372379303s
tensor(-3.6338e-06) tensor(0.0192)
tensor(0.0192) tensor(-3.6338e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0325 09:28:38.419615 585175 config.py:54] PyTorch version 2.6.0 available.
W0325 09:28:38.705638 585175 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 09:28:39.629780 585175 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 09:28:39.633864 583434 quantize_finetune_llama.py:209] layer 1 gpu 1
I0325 09:29:49.376175 583434 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 69.57389831542969s
I0325 09:30:00.137162 586165 config.py:54] PyTorch version 2.6.0 available.
W0325 09:30:00.423599 586165 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 09:30:01.332723 586165 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 09:30:01.336866 583434 quantize_finetune_llama.py:209] layer 2 gpu 2
I0325 09:31:12.143800 583434 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 70.61633062362671s
I0325 09:31:21.017168 587173 config.py:54] PyTorch version 2.6.0 available.
W0325 09:31:21.311694 587173 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 09:31:22.320919 587173 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 09:31:22.325484 583434 quantize_finetune_llama.py:209] layer 3 gpu 0
I0325 09:31:22.338745 587173 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 09:31:41.814186 587173 finetune.py:45] layer 2_v initial loss 8.967424946604297e-05
W0325 09:31:41.814466 587173 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 09:32:13.113739 587173 finetune.py:68] layer 2_v @ epoch 0 new loss 4.78803958685603e-05 old loss 8.967424946604297e-05 BETTER
I0325 09:32:25.488339 583434 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 61.32596707344055s
I0325 09:32:28.989279 588049 config.py:54] PyTorch version 2.6.0 available.
W0325 09:32:29.302384 588049 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 09:32:30.289746 588049 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 09:32:30.293594 583434 quantize_finetune_llama.py:209] layer 4 gpu 1
I0325 09:32:30.306396 588049 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 09:32:45.421828 587173 finetune.py:68] layer 2_v @ epoch 1 new loss 3.642666706582531e-05 old loss 4.78803958685603e-05 BETTER
I0325 09:32:48.068818 588049 finetune.py:45] layer 3_v initial loss 0.00019332708325237036
W0325 09:32:48.069032 588049 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 09:33:18.246701 587173 finetune.py:68] layer 2_v @ epoch 2 new loss 3.230860602343455e-05 old loss 3.642666706582531e-05 BETTER
I0325 09:33:20.374097 588049 finetune.py:68] layer 3_v @ epoch 0 new loss 9.312611655332148e-05 old loss 0.00019332708325237036 BETTER
I0325 09:33:51.238369 587173 finetune.py:68] layer 2_v @ epoch 3 new loss 3.061514507862739e-05 old loss 3.230860602343455e-05 BETTER
I0325 09:33:54.179442 588049 finetune.py:68] layer 3_v @ epoch 1 new loss 7.154668855946511e-05 old loss 9.312611655332148e-05 BETTER
I0325 09:34:24.286872 587173 finetune.py:68] layer 2_v @ epoch 4 new loss 2.9813918445142917e-05 old loss 3.061514507862739e-05 BETTER
I0325 09:34:28.599217 588049 finetune.py:68] layer 3_v @ epoch 2 new loss 6.46994449198246e-05 old loss 7.154668855946511e-05 BETTER
I0325 09:34:43.058806 587173 finetune.py:45] layer 2_q initial loss 3.3507036278024316e-05
I0325 09:35:03.315485 588049 finetune.py:68] layer 3_v @ epoch 3 new loss 6.20235368842259e-05 old loss 6.46994449198246e-05 BETTER
I0325 09:35:14.590607 587173 finetune.py:68] layer 2_q @ epoch 0 new loss 3.216180994058959e-05 old loss 3.3507036278024316e-05 BETTER
I0325 09:35:38.388114 588049 finetune.py:68] layer 3_v @ epoch 4 new loss 6.07104338996578e-05 old loss 6.20235368842259e-05 BETTER
I0325 09:35:46.644109 587173 finetune.py:68] layer 2_q @ epoch 1 new loss 3.172153083141893e-05 old loss 3.216180994058959e-05 BETTER
I0325 09:35:57.008017 588049 finetune.py:45] layer 3_q initial loss 7.59409085731022e-05
I0325 09:36:19.074382 587173 finetune.py:68] layer 2_q @ epoch 2 new loss 3.139331238344312e-05 old loss 3.172153083141893e-05 BETTER
I0325 09:36:30.235520 588049 finetune.py:68] layer 3_q @ epoch 0 new loss 7.197293598437682e-05 old loss 7.59409085731022e-05 BETTER
I0325 09:36:51.405261 587173 finetune.py:68] layer 2_q @ epoch 3 new loss 3.111568730673753e-05 old loss 3.139331238344312e-05 BETTER
I0325 09:37:04.739498 588049 finetune.py:68] layer 3_q @ epoch 1 new loss 7.065937825245783e-05 old loss 7.197293598437682e-05 BETTER
I0325 09:37:24.394292 587173 finetune.py:68] layer 2_q @ epoch 4 new loss 3.0874914955347776e-05 old loss 3.111568730673753e-05 BETTER
I0325 09:37:39.463094 588049 finetune.py:68] layer 3_q @ epoch 2 new loss 6.977247539907694e-05 old loss 7.065937825245783e-05 BETTER
I0325 09:37:43.226015 587173 finetune.py:45] layer 2_k initial loss 3.4138643968617544e-05
I0325 09:38:14.214515 588049 finetune.py:68] layer 3_q @ epoch 3 new loss 6.906889757374302e-05 old loss 6.977247539907694e-05 BETTER
I0325 09:38:14.660559 587173 finetune.py:68] layer 2_k @ epoch 0 new loss 3.3791417081374675e-05 old loss 3.4138643968617544e-05 BETTER
I0325 09:38:46.651067 587173 finetune.py:68] layer 2_k @ epoch 1 new loss 3.3561340387677774e-05 old loss 3.3791417081374675e-05 BETTER
I0325 09:38:48.971310 588049 finetune.py:68] layer 3_q @ epoch 4 new loss 6.845311872893944e-05 old loss 6.906889757374302e-05 BETTER
I0325 09:39:07.471654 588049 finetune.py:45] layer 3_k initial loss 8.100087143247947e-05
I0325 09:39:18.760458 587173 finetune.py:68] layer 2_k @ epoch 2 new loss 3.3352789614582434e-05 old loss 3.3561340387677774e-05 BETTER
I0325 09:39:40.369709 588049 finetune.py:68] layer 3_k @ epoch 0 new loss 7.964939868543297e-05 old loss 8.100087143247947e-05 BETTER
I0325 09:39:50.871248 587173 finetune.py:68] layer 2_k @ epoch 3 new loss 3.316091897431761e-05 old loss 3.3352789614582434e-05 BETTER
I0325 09:40:14.104635 588049 finetune.py:68] layer 3_k @ epoch 1 new loss 7.894252485129982e-05 old loss 7.964939868543297e-05 BETTER
I0325 09:40:22.941274 587173 finetune.py:68] layer 2_k @ epoch 4 new loss 3.298133015050553e-05 old loss 3.316091897431761e-05 BETTER
I0325 09:40:41.746690 587173 finetune.py:45] layer 2_o initial loss 0.00011616593837970868
I0325 09:40:48.187932 588049 finetune.py:68] layer 3_k @ epoch 2 new loss 7.834952702978626e-05 old loss 7.894252485129982e-05 BETTER
I0325 09:41:12.387058 587173 finetune.py:68] layer 2_o @ epoch 0 new loss 0.00011224627087358385 old loss 0.00011616593837970868 BETTER
I0325 09:41:22.091431 588049 finetune.py:68] layer 3_k @ epoch 3 new loss 7.782982720527798e-05 old loss 7.834952702978626e-05 BETTER
I0325 09:41:43.937067 587173 finetune.py:68] layer 2_o @ epoch 1 new loss 0.00010911593562923372 old loss 0.00011224627087358385 BETTER
I0325 09:41:56.056566 588049 finetune.py:68] layer 3_k @ epoch 4 new loss 7.736792758805677e-05 old loss 7.782982720527798e-05 BETTER
I0325 09:42:14.729551 588049 finetune.py:45] layer 3_o initial loss 0.00021698173077311367
I0325 09:42:15.448447 587173 finetune.py:68] layer 2_o @ epoch 2 new loss 0.00010657855455065146 old loss 0.00010911593562923372 BETTER
I0325 09:42:47.009681 588049 finetune.py:68] layer 3_o @ epoch 0 new loss 0.0002070802729576826 old loss 0.00021698173077311367 BETTER
I0325 09:42:47.258007 587173 finetune.py:68] layer 2_o @ epoch 3 new loss 0.00010448850662214682 old loss 0.00010657855455065146 BETTER
I0325 09:43:19.048594 587173 finetune.py:68] layer 2_o @ epoch 4 new loss 0.00010276019020238891 old loss 0.00010448850662214682 BETTER
I0325 09:43:20.078310 588049 finetune.py:68] layer 3_o @ epoch 1 new loss 0.00020006534759886563 old loss 0.0002070802729576826 BETTER
I0325 09:43:44.253017 587173 finetune.py:45] layer 2_up initial loss 0.0001565662823850289
I0325 09:43:53.471859 588049 finetune.py:68] layer 3_o @ epoch 2 new loss 0.00019492463616188616 old loss 0.00020006534759886563 BETTER
I0325 09:44:13.229904 587173 finetune.py:68] layer 2_up @ epoch 0 new loss 0.0001545323757454753 old loss 0.0001565662823850289 BETTER
I0325 09:44:26.761667 588049 finetune.py:68] layer 3_o @ epoch 3 new loss 0.00019104924285784364 old loss 0.00019492463616188616 BETTER
I0325 09:44:43.022377 587173 finetune.py:68] layer 2_up @ epoch 1 new loss 0.0001527887798147276 old loss 0.0001545323757454753 BETTER
I0325 09:45:00.153251 588049 finetune.py:68] layer 3_o @ epoch 4 new loss 0.000188085570698604 old loss 0.00019104924285784364 BETTER
I0325 09:45:12.853561 587173 finetune.py:68] layer 2_up @ epoch 2 new loss 0.00015130278188735247 old loss 0.0001527887798147276 BETTER
I0325 09:45:25.375576 588049 finetune.py:45] layer 3_up initial loss 0.00032020887010730803
I0325 09:45:42.864663 587173 finetune.py:68] layer 2_up @ epoch 3 new loss 0.00015001992869656533 old loss 0.00015130278188735247 BETTER
I0325 09:45:55.753331 588049 finetune.py:68] layer 3_up @ epoch 0 new loss 0.000315975776175037 old loss 0.00032020887010730803 BETTER
I0325 09:46:12.915466 587173 finetune.py:68] layer 2_up @ epoch 4 new loss 0.00014890602324157953 old loss 0.00015001992869656533 BETTER
I0325 09:46:27.246918 588049 finetune.py:68] layer 3_up @ epoch 1 new loss 0.00031254440546035767 old loss 0.000315975776175037 BETTER
I0325 09:46:38.184070 587173 finetune.py:45] layer 2_gate initial loss 0.0001928070414578542
I0325 09:46:58.871999 588049 finetune.py:68] layer 3_up @ epoch 2 new loss 0.0003097513399552554 old loss 0.00031254440546035767 BETTER
I0325 09:47:05.538676 587173 finetune.py:68] layer 2_gate @ epoch 0 new loss 0.0001915219909278676 old loss 0.0001928070414578542 BETTER
I0325 09:47:30.603811 588049 finetune.py:68] layer 3_up @ epoch 3 new loss 0.00030740423244424164 old loss 0.0003097513399552554 BETTER
I0325 09:47:33.750439 587173 finetune.py:68] layer 2_gate @ epoch 1 new loss 0.0001903694646898657 old loss 0.0001915219909278676 BETTER
I0325 09:48:01.801856 587173 finetune.py:68] layer 2_gate @ epoch 2 new loss 0.0001893263979582116 old loss 0.0001903694646898657 BETTER
I0325 09:48:02.504234 588049 finetune.py:68] layer 3_up @ epoch 4 new loss 0.0003054218541365117 old loss 0.00030740423244424164 BETTER
I0325 09:48:27.602755 588049 finetune.py:45] layer 3_gate initial loss 0.00041122385300695896
I0325 09:48:29.994379 587173 finetune.py:68] layer 2_gate @ epoch 3 new loss 0.00018837543029803783 old loss 0.0001893263979582116 BETTER
I0325 09:48:56.506644 588049 finetune.py:68] layer 3_gate @ epoch 0 new loss 0.0004090603324584663 old loss 0.00041122385300695896 BETTER
I0325 09:48:58.325986 587173 finetune.py:68] layer 2_gate @ epoch 4 new loss 0.00018750749586615711 old loss 0.00018837543029803783 BETTER
I0325 09:49:26.376954 588049 finetune.py:68] layer 3_gate @ epoch 1 new loss 0.00040709268068894744 old loss 0.0004090603324584663 BETTER
I0325 09:49:40.725898 587173 finetune.py:45] layer 2_down initial loss 0.00027236383175477386
I0325 09:49:56.392031 588049 finetune.py:68] layer 3_gate @ epoch 2 new loss 0.0004052822187077254 old loss 0.00040709268068894744 BETTER
I0325 09:50:06.281958 587173 finetune.py:68] layer 2_down @ epoch 0 new loss 0.00027184298960492015 old loss 0.00027236383175477386 BETTER
I0325 09:50:26.502043 588049 finetune.py:68] layer 3_gate @ epoch 3 new loss 0.0004036203899886459 old loss 0.0004052822187077254 BETTER
I0325 09:50:33.014486 587173 finetune.py:68] layer 2_down @ epoch 1 new loss 0.0002713636204134673 old loss 0.00027184298960492015 BETTER
I0325 09:50:56.748531 588049 finetune.py:68] layer 3_gate @ epoch 4 new loss 0.0004020640335511416 old loss 0.0004036203899886459 BETTER
I0325 09:51:00.068490 587173 finetune.py:68] layer 2_down @ epoch 2 new loss 0.00027092231903225183 old loss 0.0002713636204134673 BETTER
I0325 09:51:26.820335 587173 finetune.py:68] layer 2_down @ epoch 3 new loss 0.00027051035431213677 old loss 0.00027092231903225183 BETTER
I0325 09:51:39.641086 588049 finetune.py:45] layer 3_down initial loss 0.000572345161344856
I0325 09:51:54.051392 587173 finetune.py:68] layer 2_down @ epoch 4 new loss 0.0002701259800232947 old loss 0.00027051035431213677 BETTER
2_v proxy err 0.08700457215309143 tr(WHW.T) 2814.01025390625
bpp_loss 1.2614079113554908
2_q proxy err 0.0018445947207510471 tr(WHW.T) 159682.265625
bpp_loss 1.9237550840771291
2_k proxy err 0.0014067870797589421 tr(WHW.T) 210372.40625
bpp_loss 1.9756635719822953
2_o proxy err 0.14077670872211456 tr(WHW.T) 5344.1103515625
bpp_loss 1.2318212729442166
2_up proxy err 0.10933131724596024 tr(WHW.T) 20078.5625
bpp_loss 1.4445548646022068
2_gate proxy err 0.07159243524074554 tr(WHW.T) 31683.0234375
bpp_loss 1.5332598495622014
2_down proxy err 0.1389751434326172 tr(WHW.T) 17684.46875
bpp_loss 1.4125068976522186
I0325 09:52:06.257311 588049 finetune.py:68] layer 3_down @ epoch 0 new loss 0.0005714985309168696 old loss 0.000572345161344856 BETTER
I0325 09:52:34.305594 588049 finetune.py:68] layer 3_down @ epoch 1 new loss 0.0005706995143555105 old loss 0.0005714985309168696 BETTER
I0325 09:53:02.426373 583434 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 63.44582486152649s
I0325 09:53:02.855125 588049 finetune.py:68] layer 3_down @ epoch 2 new loss 0.0005699376342818141 old loss 0.0005706995143555105 BETTER
I0325 09:53:05.934661 603166 config.py:54] PyTorch version 2.6.0 available.
W0325 09:53:06.237511 603166 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 09:53:07.163131 603166 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 09:53:07.167288 583434 quantize_finetune_llama.py:209] layer 5 gpu 2
I0325 09:53:07.180201 603166 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 09:53:25.956457 603166 finetune.py:45] layer 4_v initial loss 0.0003574053989723325
W0325 09:53:25.956668 603166 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 09:53:31.279725 588049 finetune.py:68] layer 3_down @ epoch 3 new loss 0.0005692202248610556 old loss 0.0005699376342818141 BETTER
I0325 09:53:57.265582 603166 finetune.py:68] layer 4_v @ epoch 0 new loss 0.00015664412057958543 old loss 0.0003574053989723325 BETTER
I0325 09:53:59.793430 588049 finetune.py:68] layer 3_down @ epoch 4 new loss 0.000568539195228368 old loss 0.0005692202248610556 BETTER
3_v proxy err 0.14119340479373932 tr(WHW.T) 3009.644287109375
bpp_loss 1.1976747765438631
3_q proxy err 0.006903971545398235 tr(WHW.T) 76303.421875
bpp_loss 1.8387409933638992
3_k proxy err 0.004979593679308891 tr(WHW.T) 106544.859375
bpp_loss 1.8903885711479234
3_o proxy err 0.11846821010112762 tr(WHW.T) 5328.0498046875
bpp_loss 1.1678338447818533
3_up proxy err 0.13506083190441132 tr(WHW.T) 17603.828125
bpp_loss 1.421676927072884
3_gate proxy err 0.08377540111541748 tr(WHW.T) 29350.32421875
bpp_loss 1.5168462030936118
3_down proxy err 0.14336733520030975 tr(WHW.T) 17439.265625
bpp_loss 1.4063055554383197
I0325 09:54:10.937186 583434 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 63.293396949768066s
I0325 09:54:14.538650 604079 config.py:54] PyTorch version 2.6.0 available.
W0325 09:54:14.843867 604079 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 09:54:15.908742 604079 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 09:54:15.913006 583434 quantize_finetune_llama.py:209] layer 6 gpu 0
I0325 09:54:15.928638 604079 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 09:54:30.404533 603166 finetune.py:68] layer 4_v @ epoch 1 new loss 0.00012115173740312457 old loss 0.00015664412057958543 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 09:54:36.063892 604079 finetune.py:45] layer 5_v initial loss 0.0005332588334567845
W0325 09:54:36.064095 604079 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 09:55:03.053525 603166 finetune.py:68] layer 4_v @ epoch 2 new loss 0.00011069727770518512 old loss 0.00012115173740312457 BETTER
I0325 09:55:07.442784 604079 finetune.py:68] layer 5_v @ epoch 0 new loss 0.000257098552538082 old loss 0.0005332588334567845 BETTER
I0325 09:55:35.944681 603166 finetune.py:68] layer 4_v @ epoch 3 new loss 0.00010645155271049589 old loss 0.00011069727770518512 BETTER
I0325 09:55:39.900957 604079 finetune.py:68] layer 5_v @ epoch 1 new loss 0.00021204799122642726 old loss 0.000257098552538082 BETTER
I0325 09:56:08.874691 603166 finetune.py:68] layer 4_v @ epoch 4 new loss 0.0001041072464431636 old loss 0.00010645155271049589 BETTER
I0325 09:56:12.871862 604079 finetune.py:68] layer 5_v @ epoch 2 new loss 0.00019734495435841382 old loss 0.00021204799122642726 BETTER
I0325 09:56:27.819374 603166 finetune.py:45] layer 4_q initial loss 0.00012461676669772714
I0325 09:56:46.101546 604079 finetune.py:68] layer 5_v @ epoch 3 new loss 0.00019036739831790328 old loss 0.00019734495435841382 BETTER
I0325 09:56:59.238954 603166 finetune.py:68] layer 4_q @ epoch 0 new loss 0.00011941191769437864 old loss 0.00012461676669772714 BETTER
I0325 09:57:19.371165 604079 finetune.py:68] layer 5_v @ epoch 4 new loss 0.00018610125698614866 old loss 0.00019036739831790328 BETTER
I0325 09:57:31.670331 603166 finetune.py:68] layer 4_q @ epoch 1 new loss 0.00011719060421455652 old loss 0.00011941191769437864 BETTER
I0325 09:57:38.294834 604079 finetune.py:45] layer 5_q initial loss 0.0002176761772716418
I0325 09:58:04.099671 603166 finetune.py:68] layer 4_q @ epoch 2 new loss 0.00011558701226022094 old loss 0.00011719060421455652 BETTER
I0325 09:58:09.734939 604079 finetune.py:68] layer 5_q @ epoch 0 new loss 0.0002091143251163885 old loss 0.0002176761772716418 BETTER
I0325 09:58:36.540801 603166 finetune.py:68] layer 4_q @ epoch 3 new loss 0.00011425017873989418 old loss 0.00011558701226022094 BETTER
I0325 09:58:42.065939 604079 finetune.py:68] layer 5_q @ epoch 1 new loss 0.00020491999748628587 old loss 0.0002091143251163885 BETTER
I0325 09:59:08.739897 603166 finetune.py:68] layer 4_q @ epoch 4 new loss 0.0001131155586335808 old loss 0.00011425017873989418 BETTER
I0325 09:59:14.355600 604079 finetune.py:68] layer 5_q @ epoch 2 new loss 0.0002019607345573604 old loss 0.00020491999748628587 BETTER
I0325 09:59:27.460756 603166 finetune.py:45] layer 4_k initial loss 0.00012991648691240698
I0325 09:59:46.886215 604079 finetune.py:68] layer 5_q @ epoch 3 new loss 0.00019944075029343367 old loss 0.0002019607345573604 BETTER
I0325 09:59:58.582633 603166 finetune.py:68] layer 4_k @ epoch 0 new loss 0.00012810139742214233 old loss 0.00012991648691240698 BETTER
I0325 10:00:19.436393 604079 finetune.py:68] layer 5_q @ epoch 4 new loss 0.00019726340542547405 old loss 0.00019944075029343367 BETTER
I0325 10:00:30.613848 603166 finetune.py:68] layer 4_k @ epoch 1 new loss 0.0001269420754397288 old loss 0.00012810139742214233 BETTER
I0325 10:00:38.285326 604079 finetune.py:45] layer 5_k initial loss 0.00021940477017778903
I0325 10:01:02.665041 603166 finetune.py:68] layer 4_k @ epoch 2 new loss 0.0001259732962353155 old loss 0.0001269420754397288 BETTER
I0325 10:01:09.679619 604079 finetune.py:68] layer 5_k @ epoch 0 new loss 0.00021644160733558238 old loss 0.00021940477017778903 BETTER
I0325 10:01:34.717314 603166 finetune.py:68] layer 4_k @ epoch 3 new loss 0.0001250884961336851 old loss 0.0001259732962353155 BETTER
I0325 10:01:41.791123 604079 finetune.py:68] layer 5_k @ epoch 1 new loss 0.00021444800950121135 old loss 0.00021644160733558238 BETTER
I0325 10:02:06.826611 603166 finetune.py:68] layer 4_k @ epoch 4 new loss 0.00012431036157067865 old loss 0.0001250884961336851 BETTER
I0325 10:02:13.841681 604079 finetune.py:68] layer 5_k @ epoch 2 new loss 0.00021271021978463978 old loss 0.00021444800950121135 BETTER
I0325 10:02:25.596884 603166 finetune.py:45] layer 4_o initial loss 0.0003700972883962095
I0325 10:02:46.013468 604079 finetune.py:68] layer 5_k @ epoch 3 new loss 0.00021110053057782352 old loss 0.00021271021978463978 BETTER
I0325 10:02:55.969214 603166 finetune.py:68] layer 4_o @ epoch 0 new loss 0.0003403170558158308 old loss 0.0003700972883962095 BETTER
I0325 10:03:18.281983 604079 finetune.py:68] layer 5_k @ epoch 4 new loss 0.00020967616001144052 old loss 0.00021110053057782352 BETTER
I0325 10:03:27.441131 603166 finetune.py:68] layer 4_o @ epoch 1 new loss 0.0003247094282414764 old loss 0.0003403170558158308 BETTER
I0325 10:03:37.091480 604079 finetune.py:45] layer 5_o initial loss 0.0005608864594250917
I0325 10:03:59.011729 603166 finetune.py:68] layer 4_o @ epoch 2 new loss 0.0003152274002786726 old loss 0.0003247094282414764 BETTER
I0325 10:04:07.905036 604079 finetune.py:68] layer 5_o @ epoch 0 new loss 0.0005207299836911261 old loss 0.0005608864594250917 BETTER
I0325 10:04:30.501682 603166 finetune.py:68] layer 4_o @ epoch 3 new loss 0.00030901579884812236 old loss 0.0003152274002786726 BETTER
I0325 10:04:39.537559 604079 finetune.py:68] layer 5_o @ epoch 1 new loss 0.0005019059171900153 old loss 0.0005207299836911261 BETTER
I0325 10:05:01.992011 603166 finetune.py:68] layer 4_o @ epoch 4 new loss 0.00030458782566711307 old loss 0.00030901579884812236 BETTER
I0325 10:05:11.086941 604079 finetune.py:68] layer 5_o @ epoch 2 new loss 0.0004909397102892399 old loss 0.0005019059171900153 BETTER
I0325 10:05:27.400334 603166 finetune.py:45] layer 4_up initial loss 0.0005513336509466171
I0325 10:05:42.718464 604079 finetune.py:68] layer 5_o @ epoch 3 new loss 0.0004836001608055085 old loss 0.0004909397102892399 BETTER
I0325 10:05:56.173304 603166 finetune.py:68] layer 4_up @ epoch 0 new loss 0.0005426954012364149 old loss 0.0005513336509466171 BETTER
I0325 10:06:14.409052 604079 finetune.py:68] layer 5_o @ epoch 4 new loss 0.0004781942698173225 old loss 0.0004836001608055085 BETTER
I0325 10:06:25.865476 603166 finetune.py:68] layer 4_up @ epoch 1 new loss 0.0005365722463466227 old loss 0.0005426954012364149 BETTER
I0325 10:06:39.660497 604079 finetune.py:45] layer 5_up initial loss 0.0008577455882914364
I0325 10:06:55.618268 603166 finetune.py:68] layer 4_up @ epoch 2 new loss 0.0005319400224834681 old loss 0.0005365722463466227 BETTER
I0325 10:07:08.589293 604079 finetune.py:68] layer 5_up @ epoch 0 new loss 0.0008464218117296696 old loss 0.0008577455882914364 BETTER
I0325 10:07:25.469256 603166 finetune.py:68] layer 4_up @ epoch 3 new loss 0.0005281534395180643 old loss 0.0005319400224834681 BETTER
I0325 10:07:38.321631 604079 finetune.py:68] layer 5_up @ epoch 1 new loss 0.0008387654088437557 old loss 0.0008464218117296696 BETTER
I0325 10:07:55.338798 603166 finetune.py:68] layer 4_up @ epoch 4 new loss 0.0005249374662525952 old loss 0.0005281534395180643 BETTER
I0325 10:08:08.292761 604079 finetune.py:68] layer 5_up @ epoch 2 new loss 0.0008329158881679177 old loss 0.0008387654088437557 BETTER
I0325 10:08:20.570005 603166 finetune.py:45] layer 4_gate initial loss 0.000699806900229305
I0325 10:08:38.301244 604079 finetune.py:68] layer 5_up @ epoch 3 new loss 0.0008280049078166485 old loss 0.0008329158881679177 BETTER
I0325 10:08:47.791062 603166 finetune.py:68] layer 4_gate @ epoch 0 new loss 0.0006958247977308929 old loss 0.000699806900229305 BETTER
I0325 10:09:08.195118 604079 finetune.py:68] layer 5_up @ epoch 4 new loss 0.0008237723377533257 old loss 0.0008280049078166485 BETTER
I0325 10:09:16.172839 603166 finetune.py:68] layer 4_gate @ epoch 1 new loss 0.0006924825720489025 old loss 0.0006958247977308929 BETTER
I0325 10:09:33.961358 604079 finetune.py:45] layer 5_gate initial loss 0.001074516330845654
I0325 10:09:44.385439 603166 finetune.py:68] layer 4_gate @ epoch 2 new loss 0.0006895119440741837 old loss 0.0006924825720489025 BETTER
I0325 10:10:01.376465 604079 finetune.py:68] layer 5_gate @ epoch 0 new loss 0.0010687196627259254 old loss 0.001074516330845654 BETTER
I0325 10:10:12.633996 603166 finetune.py:68] layer 4_gate @ epoch 3 new loss 0.0006868031923659146 old loss 0.0006895119440741837 BETTER
I0325 10:10:29.508654 604079 finetune.py:68] layer 5_gate @ epoch 1 new loss 0.00106392044108361 old loss 0.0010687196627259254 BETTER
I0325 10:10:40.968180 603166 finetune.py:68] layer 4_gate @ epoch 4 new loss 0.0006842724396847188 old loss 0.0006868031923659146 BETTER
I0325 10:10:57.779991 604079 finetune.py:68] layer 5_gate @ epoch 2 new loss 0.0010597631335258484 old loss 0.00106392044108361 BETTER
I0325 10:11:22.619265 603166 finetune.py:45] layer 4_down initial loss 0.0009960439056158066
I0325 10:11:25.831391 604079 finetune.py:68] layer 5_gate @ epoch 3 new loss 0.001055948087014258 old loss 0.0010597631335258484 BETTER
I0325 10:11:47.861169 603166 finetune.py:68] layer 4_down @ epoch 0 new loss 0.0009950378444045782 old loss 0.0009960439056158066 BETTER
I0325 10:11:53.984001 604079 finetune.py:68] layer 5_gate @ epoch 4 new loss 0.0010524047538638115 old loss 0.001055948087014258 BETTER
I0325 10:12:14.297867 603166 finetune.py:68] layer 4_down @ epoch 1 new loss 0.0009940764866769314 old loss 0.0009950378444045782 BETTER
I0325 10:12:36.237063 604079 finetune.py:45] layer 5_down initial loss 0.0015136221190914512
I0325 10:12:40.971656 603166 finetune.py:68] layer 4_down @ epoch 2 new loss 0.0009931556414812803 old loss 0.0009940764866769314 BETTER
I0325 10:13:01.720582 604079 finetune.py:68] layer 5_down @ epoch 0 new loss 0.0015125705394893885 old loss 0.0015136221190914512 BETTER
I0325 10:13:07.723558 603166 finetune.py:68] layer 4_down @ epoch 3 new loss 0.000992265879176557 old loss 0.0009931556414812803 BETTER
I0325 10:13:28.653890 604079 finetune.py:68] layer 5_down @ epoch 1 new loss 0.0015115686692297459 old loss 0.0015125705394893885 BETTER
I0325 10:13:34.557003 603166 finetune.py:68] layer 4_down @ epoch 4 new loss 0.0009914111578837037 old loss 0.000992265879176557 BETTER
4_v proxy err 0.13764572143554688 tr(WHW.T) 3131.788818359375
bpp_loss 1.23494644476159
4_q proxy err 0.006743166595697403 tr(WHW.T) 78950.2109375
bpp_loss 1.9151828141475562
4_k proxy err 0.004480193834751844 tr(WHW.T) 118876.7578125
bpp_loss 1.9437793565302854
4_o proxy err 0.13817621767520905 tr(WHW.T) 5399.755859375
bpp_loss 1.2179132479359396
4_up proxy err 0.13268138468265533 tr(WHW.T) 17722.072265625
bpp_loss 1.4089431904992737
4_gate proxy err 0.06804033368825912 tr(WHW.T) 36124.9765625
bpp_loss 1.5352880234808424
4_down proxy err 0.1390756219625473 tr(WHW.T) 17663.51953125
bpp_loss 1.3911185877202727
I0325 10:13:55.807736 604079 finetune.py:68] layer 5_down @ epoch 2 new loss 0.0015106130158528686 old loss 0.0015115686692297459 BETTER
I0325 10:14:22.678795 604079 finetune.py:68] layer 5_down @ epoch 3 new loss 0.0015096860006451607 old loss 0.0015106130158528686 BETTER
I0325 10:14:43.511090 583434 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 62.170727014541626s
I0325 10:14:47.261328 619229 config.py:54] PyTorch version 2.6.0 available.
W0325 10:14:47.614208 619229 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 10:14:48.647635 619229 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 10:14:48.651836 583434 quantize_finetune_llama.py:209] layer 7 gpu 1
I0325 10:14:48.671253 619229 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 10:14:49.470835 604079 finetune.py:68] layer 5_down @ epoch 4 new loss 0.0015087921638041735 old loss 0.0015096860006451607 BETTER
5_v proxy err 0.14779511094093323 tr(WHW.T) 3202.603515625
bpp_loss 1.2454162096692016
5_q proxy err 0.00805316586047411 tr(WHW.T) 72696.09375
bpp_loss 1.9253817427525064
5_k proxy err 0.0050641074776649475 tr(WHW.T) 116392.9296875
bpp_loss 1.9807280272507342
5_o proxy err 0.14830131828784943 tr(WHW.T) 3840.68505859375
bpp_loss 1.2213766627683071
5_up proxy err 0.13066133856773376 tr(WHW.T) 18078.939453125
bpp_loss 1.4083148154408434
5_gate proxy err 0.06391727924346924 tr(WHW.T) 38697.859375
bpp_loss 1.5423786767303598
5_down proxy err 0.15252463519573212 tr(WHW.T) 16725.73046875
bpp_loss 1.3899181077343434
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 10:15:07.155159 619229 finetune.py:45] layer 6_v initial loss 0.0006324026035144925
W0325 10:15:07.155417 619229 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 10:15:39.753963 619229 finetune.py:68] layer 6_v @ epoch 0 new loss 0.00032746425131335855 old loss 0.0006324026035144925 BETTER
I0325 10:15:56.481340 583434 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 61.208813428878784s
I0325 10:16:00.044429 620178 config.py:54] PyTorch version 2.6.0 available.
W0325 10:16:00.357294 620178 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 10:16:01.380710 620178 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 10:16:01.384857 583434 quantize_finetune_llama.py:209] layer 8 gpu 2
I0325 10:16:01.398315 620178 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 10:16:13.946494 619229 finetune.py:68] layer 6_v @ epoch 1 new loss 0.00027991391834802926 old loss 0.00032746425131335855 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 10:16:20.489613 620178 finetune.py:45] layer 7_v initial loss 0.0008860028465278447
W0325 10:16:20.489814 620178 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 10:16:48.857078 619229 finetune.py:68] layer 6_v @ epoch 2 new loss 0.0002650830429047346 old loss 0.00027991391834802926 BETTER
I0325 10:16:51.960335 620178 finetune.py:68] layer 7_v @ epoch 0 new loss 0.0004596914805006236 old loss 0.0008860028465278447 BETTER
I0325 10:17:04.679092 583434 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 62.828967571258545s
I0325 10:17:08.237390 621100 config.py:54] PyTorch version 2.6.0 available.
W0325 10:17:08.560510 621100 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 10:17:09.615209 621100 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 10:17:09.619376 583434 quantize_finetune_llama.py:209] layer 9 gpu 0
I0325 10:17:09.632738 621100 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 10:17:23.953048 619229 finetune.py:68] layer 6_v @ epoch 3 new loss 0.0002576085098553449 old loss 0.0002650830429047346 BETTER
I0325 10:17:24.401548 620178 finetune.py:68] layer 7_v @ epoch 1 new loss 0.000396867748349905 old loss 0.0004596914805006236 BETTER
I0325 10:17:28.705361 621100 finetune.py:45] layer 8_v initial loss 0.0010881011839956045
W0325 10:17:28.705769 621100 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 10:17:58.136667 620178 finetune.py:68] layer 7_v @ epoch 2 new loss 0.0003764315915759653 old loss 0.000396867748349905 BETTER
I0325 10:17:59.811492 619229 finetune.py:68] layer 6_v @ epoch 4 new loss 0.0002526648167986423 old loss 0.0002576085098553449 BETTER
I0325 10:18:01.050237 621100 finetune.py:68] layer 8_v @ epoch 0 new loss 0.0006411125650629401 old loss 0.0010881011839956045 BETTER
I0325 10:18:19.005445 619229 finetune.py:45] layer 6_q initial loss 0.00031663154368288815
I0325 10:18:31.180078 620178 finetune.py:68] layer 7_v @ epoch 3 new loss 0.00036582170287147164 old loss 0.0003764315915759653 BETTER
I0325 10:18:34.044768 621100 finetune.py:68] layer 8_v @ epoch 1 new loss 0.0005667514633387327 old loss 0.0006411125650629401 BETTER
I0325 10:18:52.749084 619229 finetune.py:68] layer 6_q @ epoch 0 new loss 0.00030195919680409133 old loss 0.00031663154368288815 BETTER
I0325 10:19:04.859252 620178 finetune.py:68] layer 7_v @ epoch 4 new loss 0.00035886335535906255 old loss 0.00036582170287147164 BETTER
I0325 10:19:07.490353 621100 finetune.py:68] layer 8_v @ epoch 2 new loss 0.0005405264673754573 old loss 0.0005667514633387327 BETTER
I0325 10:19:24.182603 620178 finetune.py:45] layer 7_q initial loss 0.00045309565030038357
I0325 10:19:27.413563 619229 finetune.py:68] layer 6_q @ epoch 1 new loss 0.00029631241341121495 old loss 0.00030195919680409133 BETTER
I0325 10:19:40.800166 621100 finetune.py:68] layer 8_v @ epoch 3 new loss 0.0005261182086542249 old loss 0.0005405264673754573 BETTER
I0325 10:19:55.795805 620178 finetune.py:68] layer 7_q @ epoch 0 new loss 0.0004296294355299324 old loss 0.00045309565030038357 BETTER
I0325 10:20:02.339505 619229 finetune.py:68] layer 6_q @ epoch 2 new loss 0.0002923003048636019 old loss 0.00029631241341121495 BETTER
I0325 10:20:14.184674 621100 finetune.py:68] layer 8_v @ epoch 4 new loss 0.0005161100416444242 old loss 0.0005261182086542249 BETTER
I0325 10:20:28.188352 620178 finetune.py:68] layer 7_q @ epoch 1 new loss 0.00042070940253324807 old loss 0.0004296294355299324 BETTER
I0325 10:20:33.547613 621100 finetune.py:45] layer 8_q initial loss 0.0006322739063762128
I0325 10:20:37.289411 619229 finetune.py:68] layer 6_q @ epoch 3 new loss 0.0002889297902584076 old loss 0.0002923003048636019 BETTER
I0325 10:21:00.837121 620178 finetune.py:68] layer 7_q @ epoch 2 new loss 0.0004145548155065626 old loss 0.00042070940253324807 BETTER
I0325 10:21:05.114263 621100 finetune.py:68] layer 8_q @ epoch 0 new loss 0.0006056640413589776 old loss 0.0006322739063762128 BETTER
I0325 10:21:12.193086 619229 finetune.py:68] layer 6_q @ epoch 4 new loss 0.00028606734122149646 old loss 0.0002889297902584076 BETTER
I0325 10:21:31.329260 619229 finetune.py:45] layer 6_k initial loss 0.00033756287302821875
I0325 10:21:33.667675 620178 finetune.py:68] layer 7_q @ epoch 3 new loss 0.00040956283919513226 old loss 0.0004145548155065626 BETTER
I0325 10:21:37.389033 621100 finetune.py:68] layer 8_q @ epoch 1 new loss 0.0005945200682617724 old loss 0.0006056640413589776 BETTER
I0325 10:22:04.121127 619229 finetune.py:68] layer 6_k @ epoch 0 new loss 0.0003282506368122995 old loss 0.00033756287302821875 BETTER
I0325 10:22:06.255916 620178 finetune.py:68] layer 7_q @ epoch 4 new loss 0.00040535215521231294 old loss 0.00040956283919513226 BETTER
I0325 10:22:09.915580 621100 finetune.py:68] layer 8_q @ epoch 2 new loss 0.0005862857215106487 old loss 0.0005945200682617724 BETTER
I0325 10:22:25.681384 620178 finetune.py:45] layer 7_k initial loss 0.0004750421503558755
I0325 10:22:37.910089 619229 finetune.py:68] layer 6_k @ epoch 1 new loss 0.00032505093258805573 old loss 0.0003282506368122995 BETTER
I0325 10:22:42.764898 621100 finetune.py:68] layer 8_q @ epoch 3 new loss 0.0005793869495391846 old loss 0.0005862857215106487 BETTER
I0325 10:22:56.859746 620178 finetune.py:68] layer 7_k @ epoch 0 new loss 0.0004652395145967603 old loss 0.0004750421503558755 BETTER
I0325 10:23:12.138265 619229 finetune.py:68] layer 6_k @ epoch 2 new loss 0.0003222934028599411 old loss 0.00032505093258805573 BETTER
I0325 10:23:15.634050 621100 finetune.py:68] layer 8_q @ epoch 4 new loss 0.000573481956962496 old loss 0.0005793869495391846 BETTER
I0325 10:23:28.969587 620178 finetune.py:68] layer 7_k @ epoch 1 new loss 0.00046060295426286757 old loss 0.0004652395145967603 BETTER
I0325 10:23:34.867123 621100 finetune.py:45] layer 8_k initial loss 0.0006610779673792422
I0325 10:23:46.251141 619229 finetune.py:68] layer 6_k @ epoch 3 new loss 0.000319927727105096 old loss 0.0003222934028599411 BETTER
I0325 10:24:01.121588 620178 finetune.py:68] layer 7_k @ epoch 2 new loss 0.00045664102071896195 old loss 0.00046060295426286757 BETTER
I0325 10:24:06.579537 621100 finetune.py:68] layer 8_k @ epoch 0 new loss 0.0006498295697383583 old loss 0.0006610779673792422 BETTER
I0325 10:24:20.325956 619229 finetune.py:68] layer 6_k @ epoch 4 new loss 0.0003178438055329025 old loss 0.000319927727105096 BETTER
I0325 10:24:33.432792 620178 finetune.py:68] layer 7_k @ epoch 3 new loss 0.0004532762395683676 old loss 0.00045664102071896195 BETTER
I0325 10:24:38.963440 621100 finetune.py:68] layer 8_k @ epoch 1 new loss 0.0006438469863496721 old loss 0.0006498295697383583 BETTER
I0325 10:24:39.669209 619229 finetune.py:45] layer 6_o initial loss 0.0008097931859083474
I0325 10:25:05.482422 620178 finetune.py:68] layer 7_k @ epoch 4 new loss 0.0004501693183556199 old loss 0.0004532762395683676 BETTER
I0325 10:25:11.159889 621100 finetune.py:68] layer 8_k @ epoch 2 new loss 0.00063878926448524 old loss 0.0006438469863496721 BETTER
I0325 10:25:11.709595 619229 finetune.py:68] layer 6_o @ epoch 0 new loss 0.0007403481286019087 old loss 0.0008097931859083474 BETTER
I0325 10:25:24.731438 620178 finetune.py:45] layer 7_o initial loss 0.0010537173366174102
I0325 10:25:43.249700 621100 finetune.py:68] layer 8_k @ epoch 3 new loss 0.0006341873086057603 old loss 0.00063878926448524 BETTER
I0325 10:25:44.824497 619229 finetune.py:68] layer 6_o @ epoch 1 new loss 0.0007105752592906356 old loss 0.0007403481286019087 BETTER
I0325 10:25:55.203095 620178 finetune.py:68] layer 7_o @ epoch 0 new loss 0.0009779477259144187 old loss 0.0010537173366174102 BETTER
I0325 10:26:15.580877 621100 finetune.py:68] layer 8_k @ epoch 4 new loss 0.000630174414254725 old loss 0.0006341873086057603 BETTER
I0325 10:26:18.282958 619229 finetune.py:68] layer 6_o @ epoch 2 new loss 0.0006950904498808086 old loss 0.0007105752592906356 BETTER
I0325 10:26:26.595132 620178 finetune.py:68] layer 7_o @ epoch 1 new loss 0.0009475108818151057 old loss 0.0009779477259144187 BETTER
I0325 10:26:34.917384 621100 finetune.py:45] layer 8_o initial loss 0.001495487755164504
I0325 10:26:51.706788 619229 finetune.py:68] layer 6_o @ epoch 3 new loss 0.0006851776270195842 old loss 0.0006950904498808086 BETTER
I0325 10:26:58.088791 620178 finetune.py:68] layer 7_o @ epoch 2 new loss 0.00093129463493824 old loss 0.0009475108818151057 BETTER
I0325 10:27:05.583339 621100 finetune.py:68] layer 8_o @ epoch 0 new loss 0.0013872404815629125 old loss 0.001495487755164504 BETTER
I0325 10:27:25.184742 619229 finetune.py:68] layer 6_o @ epoch 4 new loss 0.0006779170362278819 old loss 0.0006851776270195842 BETTER
I0325 10:27:29.779051 620178 finetune.py:68] layer 7_o @ epoch 3 new loss 0.0009204498492181301 old loss 0.00093129463493824 BETTER
I0325 10:27:37.115751 621100 finetune.py:68] layer 8_o @ epoch 1 new loss 0.0013436332810670137 old loss 0.0013872404815629125 BETTER
I0325 10:27:51.067959 619229 finetune.py:45] layer 6_up initial loss 0.0012671378208324313
I0325 10:28:01.406742 620178 finetune.py:68] layer 7_o @ epoch 4 new loss 0.0009120716713368893 old loss 0.0009204498492181301 BETTER
I0325 10:28:08.845563 621100 finetune.py:68] layer 8_o @ epoch 2 new loss 0.001320058130659163 old loss 0.0013436332810670137 BETTER
I0325 10:28:21.415005 619229 finetune.py:68] layer 6_up @ epoch 0 new loss 0.0012480210280045867 old loss 0.0012671378208324313 BETTER
I0325 10:28:27.735835 620178 finetune.py:45] layer 7_up initial loss 0.0017090743640437722
I0325 10:28:40.654660 621100 finetune.py:68] layer 8_o @ epoch 3 new loss 0.001304358127526939 old loss 0.001320058130659163 BETTER
I0325 10:28:53.104450 619229 finetune.py:68] layer 6_up @ epoch 1 new loss 0.0012365197762846947 old loss 0.0012480210280045867 BETTER
I0325 10:28:56.724570 620178 finetune.py:68] layer 7_up @ epoch 0 new loss 0.0016810829984024167 old loss 0.0017090743640437722 BETTER
I0325 10:29:13.034422 621100 finetune.py:68] layer 8_o @ epoch 4 new loss 0.0012923470931127667 old loss 0.001304358127526939 BETTER
I0325 10:29:25.107385 619229 finetune.py:68] layer 6_up @ epoch 2 new loss 0.0012280329829081893 old loss 0.0012365197762846947 BETTER
I0325 10:29:26.669690 620178 finetune.py:68] layer 7_up @ epoch 1 new loss 0.0016648949822410941 old loss 0.0016810829984024167 BETTER
I0325 10:29:39.831424 621100 finetune.py:45] layer 8_up initial loss 0.002236756728962064
I0325 10:29:56.952688 619229 finetune.py:68] layer 6_up @ epoch 3 new loss 0.0012209335109218955 old loss 0.0012280329829081893 BETTER
I0325 10:29:56.964694 620178 finetune.py:68] layer 7_up @ epoch 2 new loss 0.0016528131673112512 old loss 0.0016648949822410941 BETTER
I0325 10:30:08.686496 621100 finetune.py:68] layer 8_up @ epoch 0 new loss 0.002200923627242446 old loss 0.002236756728962064 BETTER
I0325 10:30:26.802315 620178 finetune.py:68] layer 7_up @ epoch 3 new loss 0.0016429581446573138 old loss 0.0016528131673112512 BETTER
I0325 10:30:28.804969 619229 finetune.py:68] layer 6_up @ epoch 4 new loss 0.0012148008681833744 old loss 0.0012209335109218955 BETTER
I0325 10:30:38.378829 621100 finetune.py:68] layer 8_up @ epoch 1 new loss 0.002180718583986163 old loss 0.002200923627242446 BETTER
I0325 10:30:54.910816 619229 finetune.py:45] layer 6_gate initial loss 0.0015739750815555453
I0325 10:30:56.753165 620178 finetune.py:68] layer 7_up @ epoch 4 new loss 0.0016343907918781042 old loss 0.0016429581446573138 BETTER
I0325 10:31:08.251484 621100 finetune.py:68] layer 8_up @ epoch 2 new loss 0.002165559446439147 old loss 0.002180718583986163 BETTER
I0325 10:31:22.799108 620178 finetune.py:45] layer 7_gate initial loss 0.002112233079969883
I0325 10:31:23.894470 619229 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.0015644985251128674 old loss 0.0015739750815555453 BETTER
I0325 10:31:38.290133 621100 finetune.py:68] layer 8_up @ epoch 3 new loss 0.0021530119702219963 old loss 0.002165559446439147 BETTER
I0325 10:31:50.219423 620178 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.0020988171454519033 old loss 0.002112233079969883 BETTER
I0325 10:31:53.840951 619229 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.0015574084827676415 old loss 0.0015644985251128674 BETTER
I0325 10:32:08.371911 621100 finetune.py:68] layer 8_up @ epoch 4 new loss 0.0021420891862362623 old loss 0.0021530119702219963 BETTER
I0325 10:32:18.529214 620178 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.0020886550191789865 old loss 0.0020988171454519033 BETTER
I0325 10:32:23.871545 619229 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.001551407272927463 old loss 0.0015574084827676415 BETTER
I0325 10:32:34.754578 621100 finetune.py:45] layer 8_gate initial loss 0.0027423191349953413
I0325 10:32:46.561421 620178 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.0020798398181796074 old loss 0.0020886550191789865 BETTER
I0325 10:32:54.024608 619229 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.0015459858113899827 old loss 0.001551407272927463 BETTER
I0325 10:33:02.388843 621100 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.002724747871980071 old loss 0.0027423191349953413 BETTER
I0325 10:33:14.789218 620178 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.0020718693267554045 old loss 0.0020798398181796074 BETTER
I0325 10:33:24.313282 619229 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.0015409113839268684 old loss 0.0015459858113899827 BETTER
I0325 10:33:30.446779 621100 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.002711731940507889 old loss 0.002724747871980071 BETTER
I0325 10:33:42.804727 620178 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.002064545638859272 old loss 0.0020718693267554045 BETTER
I0325 10:33:58.778863 621100 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.0027007516473531723 old loss 0.002711731940507889 BETTER
I0325 10:34:06.972275 619229 finetune.py:45] layer 6_down initial loss 0.002213118365034461
I0325 10:34:26.393471 620178 finetune.py:45] layer 7_down initial loss 0.002957424847409129
I0325 10:34:27.579503 621100 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.0026908712461590767 old loss 0.0027007516473531723 BETTER
I0325 10:34:33.605438 619229 finetune.py:68] layer 6_down @ epoch 0 new loss 0.0022117572370916605 old loss 0.002213118365034461 BETTER
I0325 10:34:51.938833 620178 finetune.py:68] layer 7_down @ epoch 0 new loss 0.002955598058179021 old loss 0.002957424847409129 BETTER
I0325 10:34:56.221132 621100 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.0026818071492016315 old loss 0.0026908712461590767 BETTER
I0325 10:35:01.800942 619229 finetune.py:68] layer 6_down @ epoch 1 new loss 0.0022104636300355196 old loss 0.0022117572370916605 BETTER
I0325 10:35:18.672863 620178 finetune.py:68] layer 7_down @ epoch 1 new loss 0.002953967312350869 old loss 0.002955598058179021 BETTER
I0325 10:35:30.256835 619229 finetune.py:68] layer 6_down @ epoch 2 new loss 0.0022092259023338556 old loss 0.0022104636300355196 BETTER
I0325 10:35:39.596243 621100 finetune.py:45] layer 8_down initial loss 0.003753483993932605
I0325 10:35:45.374704 620178 finetune.py:68] layer 7_down @ epoch 2 new loss 0.0029524299316108227 old loss 0.002953967312350869 BETTER
I0325 10:35:58.751565 619229 finetune.py:68] layer 6_down @ epoch 3 new loss 0.002208027057349682 old loss 0.0022092259023338556 BETTER
I0325 10:36:05.395204 621100 finetune.py:68] layer 8_down @ epoch 0 new loss 0.003751352895051241 old loss 0.003753483993932605 BETTER
I0325 10:36:12.158377 620178 finetune.py:68] layer 7_down @ epoch 3 new loss 0.0029509838204830885 old loss 0.0029524299316108227 BETTER
I0325 10:36:27.283850 619229 finetune.py:68] layer 6_down @ epoch 4 new loss 0.0022068768739700317 old loss 0.002208027057349682 BETTER
6_v proxy err 0.15827395021915436 tr(WHW.T) 3211.84423828125
bpp_loss 1.187514703546185
6_q proxy err 0.01151954848319292 tr(WHW.T) 54868.76171875
bpp_loss 1.8153793981327908
6_k proxy err 0.00839407742023468 tr(WHW.T) 75398.4453125
bpp_loss 1.8406430250324775
6_o proxy err 0.16157256066799164 tr(WHW.T) 4135.4228515625
bpp_loss 1.1824228983896319
6_up proxy err 0.13222651183605194 tr(WHW.T) 18058.06640625
bpp_loss 1.400303252864369
6_gate proxy err 0.05621209368109703 tr(WHW.T) 44835.875
bpp_loss 1.5592219141246968
6_down proxy err 0.15349887311458588 tr(WHW.T) 16227.978515625
bpp_loss 1.384693408016722
I0325 10:36:32.191041 621100 finetune.py:68] layer 8_down @ epoch 1 new loss 0.003749391995370388 old loss 0.003751352895051241 BETTER
I0325 10:36:39.946155 620178 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0029496019706130028 old loss 0.0029509838204830885 BETTER
7_v proxy err 0.15862245857715607 tr(WHW.T) 3282.554443359375
bpp_loss 1.1938917383231455
7_q proxy err 0.012489971704781055 tr(WHW.T) 51491.15234375
bpp_loss 1.8074108030705247
7_k proxy err 0.009383903816342354 tr(WHW.T) 68523.09375
bpp_loss 1.8193373480025912
7_o proxy err 0.17382745444774628 tr(WHW.T) 3633.42431640625
bpp_loss 1.1791589524655137
7_up proxy err 0.1286398470401764 tr(WHW.T) 18277.998046875
bpp_loss 1.400578898421034
7_gate proxy err 0.05393184721469879 tr(WHW.T) 45892.8828125
bpp_loss 1.5548615776392263
7_down proxy err 0.1541002094745636 tr(WHW.T) 15925.6162109375
bpp_loss 1.38743298846232
I0325 10:36:59.901866 621100 finetune.py:68] layer 8_down @ epoch 2 new loss 0.003747574519366026 old loss 0.003749391995370388 BETTER
I0325 10:37:26.653639 621100 finetune.py:68] layer 8_down @ epoch 3 new loss 0.003745859023183584 old loss 0.003747574519366026 BETTER
I0325 10:37:48.151889 583434 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 63.44493913650513s
I0325 10:37:51.674572 636866 config.py:54] PyTorch version 2.6.0 available.
W0325 10:37:51.967282 636866 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 10:37:52.950985 636866 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 10:37:52.955192 583434 quantize_finetune_llama.py:209] layer 10 gpu 1
I0325 10:37:52.969338 636866 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 10:37:53.625468 621100 finetune.py:68] layer 8_down @ epoch 4 new loss 0.0037442464381456375 old loss 0.003745859023183584 BETTER
8_v proxy err 0.14528943598270416 tr(WHW.T) 3503.97314453125
bpp_loss 1.2185639941162663
8_q proxy err 0.012998438440263271 tr(WHW.T) 47775.95703125
bpp_loss 1.8204194621939678
8_k proxy err 0.008826024830341339 tr(WHW.T) 70400.5234375
bpp_loss 1.8395440004242118
8_o proxy err 0.191036656498909 tr(WHW.T) 3223.697021484375
bpp_loss 1.20165662006184
8_up proxy err 0.11819271743297577 tr(WHW.T) 19860.623046875
bpp_loss 1.4146195231871896
8_gate proxy err 0.05464570224285126 tr(WHW.T) 44646.2265625
bpp_loss 1.5360687166279139
8_down proxy err 0.15313920378684998 tr(WHW.T) 15995.015625
bpp_loss 1.3973089919187303
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 10:38:10.778581 636866 finetune.py:45] layer 9_v initial loss 0.0012615106534212828
W0325 10:38:10.778799 636866 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 10:38:43.808110 636866 finetune.py:68] layer 9_v @ epoch 0 new loss 0.0008162728045135736 old loss 0.0012615106534212828 BETTER
I0325 10:38:59.278188 583434 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 60.608983278274536s
I0325 10:39:02.711410 637797 config.py:54] PyTorch version 2.6.0 available.
W0325 10:39:03.001588 637797 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 10:39:03.913223 637797 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 10:39:03.917184 583434 quantize_finetune_llama.py:209] layer 11 gpu 2
I0325 10:39:03.930343 637797 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 10:39:18.164317 636866 finetune.py:68] layer 9_v @ epoch 1 new loss 0.0007371725514531136 old loss 0.0008162728045135736 BETTER
I0325 10:39:22.299144 637797 finetune.py:45] layer 10_v initial loss 0.0016518065240234137
W0325 10:39:22.299522 637797 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 10:39:52.962454 636866 finetune.py:68] layer 9_v @ epoch 2 new loss 0.000708126462996006 old loss 0.0007371725514531136 BETTER
I0325 10:39:53.642885 637797 finetune.py:68] layer 10_v @ epoch 0 new loss 0.0011577949626371264 old loss 0.0016518065240234137 BETTER
I0325 10:40:07.291869 583434 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 62.88906931877136s
I0325 10:40:10.912024 638720 config.py:54] PyTorch version 2.6.0 available.
W0325 10:40:11.222823 638720 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 10:40:12.229693 638720 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 10:40:12.233759 583434 quantize_finetune_llama.py:209] layer 12 gpu 0
I0325 10:40:12.248121 638720 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 10:40:25.974765 637797 finetune.py:68] layer 10_v @ epoch 1 new loss 0.0010629483731463552 old loss 0.0011577949626371264 BETTER
I0325 10:40:27.822633 636866 finetune.py:68] layer 9_v @ epoch 3 new loss 0.0006910539232194424 old loss 0.000708126462996006 BETTER
I0325 10:40:31.058175 638720 finetune.py:45] layer 11_v initial loss 0.0017420968506485224
W0325 10:40:31.058401 638720 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 10:40:58.860439 637797 finetune.py:68] layer 10_v @ epoch 2 new loss 0.0010213535279035568 old loss 0.0010629483731463552 BETTER
I0325 10:41:02.731991 636866 finetune.py:68] layer 9_v @ epoch 4 new loss 0.0006787567981518805 old loss 0.0006910539232194424 BETTER
I0325 10:41:02.799419 638720 finetune.py:68] layer 11_v @ epoch 0 new loss 0.0011939717223867774 old loss 0.0017420968506485224 BETTER
I0325 10:41:21.817633 636866 finetune.py:45] layer 9_q initial loss 0.0008255091379396617
I0325 10:41:31.782086 637797 finetune.py:68] layer 10_v @ epoch 3 new loss 0.0009947981452569366 old loss 0.0010213535279035568 BETTER
I0325 10:41:35.050920 638720 finetune.py:68] layer 11_v @ epoch 1 new loss 0.0010865333024412394 old loss 0.0011939717223867774 BETTER
I0325 10:41:54.937304 636866 finetune.py:68] layer 9_q @ epoch 0 new loss 0.0007921868236735463 old loss 0.0008255091379396617 BETTER
I0325 10:42:04.506996 637797 finetune.py:68] layer 10_v @ epoch 4 new loss 0.0009752051555551589 old loss 0.0009947981452569366 BETTER
I0325 10:42:07.944430 638720 finetune.py:68] layer 11_v @ epoch 2 new loss 0.0010424526408314705 old loss 0.0010865333024412394 BETTER
I0325 10:42:23.676889 637797 finetune.py:45] layer 10_q initial loss 0.0011565466411411762
I0325 10:42:29.084279 636866 finetune.py:68] layer 9_q @ epoch 1 new loss 0.000777822220697999 old loss 0.0007921868236735463 BETTER
I0325 10:42:41.334129 638720 finetune.py:68] layer 11_v @ epoch 3 new loss 0.0010151683818548918 old loss 0.0010424526408314705 BETTER
I0325 10:42:55.108732 637797 finetune.py:68] layer 10_q @ epoch 0 new loss 0.001106060342863202 old loss 0.0011565466411411762 BETTER
I0325 10:43:03.579566 636866 finetune.py:68] layer 9_q @ epoch 2 new loss 0.0007671763305552304 old loss 0.000777822220697999 BETTER
I0325 10:43:14.484221 638720 finetune.py:68] layer 11_v @ epoch 4 new loss 0.0009953734697774053 old loss 0.0010151683818548918 BETTER
I0325 10:43:27.361857 637797 finetune.py:68] layer 10_q @ epoch 1 new loss 0.0010853324783965945 old loss 0.001106060342863202 BETTER
I0325 10:43:33.572701 638720 finetune.py:45] layer 11_q initial loss 0.001192598370835185
I0325 10:43:38.177249 636866 finetune.py:68] layer 9_q @ epoch 3 new loss 0.0007584495469927788 old loss 0.0007671763305552304 BETTER
I0325 10:43:59.849666 637797 finetune.py:68] layer 10_q @ epoch 2 new loss 0.001069193473085761 old loss 0.0010853324783965945 BETTER
I0325 10:44:05.087372 638720 finetune.py:68] layer 11_q @ epoch 0 new loss 0.0011482847621664405 old loss 0.001192598370835185 BETTER
I0325 10:44:12.891483 636866 finetune.py:68] layer 9_q @ epoch 4 new loss 0.0007506756810471416 old loss 0.0007584495469927788 BETTER
I0325 10:44:32.228859 636866 finetune.py:45] layer 9_k initial loss 0.000865732494276017
I0325 10:44:32.439886 637797 finetune.py:68] layer 10_q @ epoch 3 new loss 0.0010557464556768537 old loss 0.001069193473085761 BETTER
I0325 10:44:37.707800 638720 finetune.py:68] layer 11_q @ epoch 1 new loss 0.0011263766791671515 old loss 0.0011482847621664405 BETTER
I0325 10:45:04.951939 636866 finetune.py:68] layer 9_k @ epoch 0 new loss 0.0008476422517560422 old loss 0.000865732494276017 BETTER
I0325 10:45:05.327742 637797 finetune.py:68] layer 10_q @ epoch 4 new loss 0.0010440743062645197 old loss 0.0010557464556768537 BETTER
I0325 10:45:10.388105 638720 finetune.py:68] layer 11_q @ epoch 2 new loss 0.001110602868720889 old loss 0.0011263766791671515 BETTER
I0325 10:45:24.695981 637797 finetune.py:45] layer 10_k initial loss 0.0011821052758023143
I0325 10:45:38.887553 636866 finetune.py:68] layer 9_k @ epoch 1 new loss 0.0008396765333600342 old loss 0.0008476422517560422 BETTER
I0325 10:45:43.012806 638720 finetune.py:68] layer 11_q @ epoch 3 new loss 0.0010974907781928778 old loss 0.001110602868720889 BETTER
I0325 10:45:56.001878 637797 finetune.py:68] layer 10_k @ epoch 0 new loss 0.0011558684054762125 old loss 0.0011821052758023143 BETTER
I0325 10:46:13.070066 636866 finetune.py:68] layer 9_k @ epoch 2 new loss 0.0008328616386279464 old loss 0.0008396765333600342 BETTER
I0325 10:46:15.906903 638720 finetune.py:68] layer 11_q @ epoch 4 new loss 0.0010863910429179668 old loss 0.0010974907781928778 BETTER
I0325 10:46:28.053963 637797 finetune.py:68] layer 10_k @ epoch 1 new loss 0.0011441722745075822 old loss 0.0011558684054762125 BETTER
I0325 10:46:35.217528 638720 finetune.py:45] layer 11_k initial loss 0.0012526422506198287
I0325 10:46:47.249749 636866 finetune.py:68] layer 9_k @ epoch 3 new loss 0.0008267005905508995 old loss 0.0008328616386279464 BETTER
I0325 10:47:00.330159 637797 finetune.py:68] layer 10_k @ epoch 2 new loss 0.0011340818600729108 old loss 0.0011441722745075822 BETTER
I0325 10:47:06.607959 638720 finetune.py:68] layer 11_k @ epoch 0 new loss 0.001226281514391303 old loss 0.0012526422506198287 BETTER
I0325 10:47:21.528506 636866 finetune.py:68] layer 9_k @ epoch 4 new loss 0.0008212167886085808 old loss 0.0008267005905508995 BETTER
I0325 10:47:32.421665 637797 finetune.py:68] layer 10_k @ epoch 3 new loss 0.0011250199750065804 old loss 0.0011340818600729108 BETTER
I0325 10:47:38.660684 638720 finetune.py:68] layer 11_k @ epoch 1 new loss 0.0012145264772698283 old loss 0.001226281514391303 BETTER
I0325 10:47:40.737668 636866 finetune.py:45] layer 9_o initial loss 0.001892145723104477
I0325 10:48:04.470839 637797 finetune.py:68] layer 10_k @ epoch 4 new loss 0.0011167669435963035 old loss 0.0011250199750065804 BETTER
I0325 10:48:10.769983 638720 finetune.py:68] layer 11_k @ epoch 2 new loss 0.0012046494521200657 old loss 0.0012145264772698283 BETTER
I0325 10:48:12.934684 636866 finetune.py:68] layer 9_o @ epoch 0 new loss 0.0017727671656757593 old loss 0.001892145723104477 BETTER
I0325 10:48:23.896131 637797 finetune.py:45] layer 10_o initial loss 0.002480039605870843
I0325 10:48:42.838736 638720 finetune.py:68] layer 11_k @ epoch 3 new loss 0.0011959339026361704 old loss 0.0012046494521200657 BETTER
I0325 10:48:46.064929 636866 finetune.py:68] layer 9_o @ epoch 1 new loss 0.0017252746038138866 old loss 0.0017727671656757593 BETTER
I0325 10:48:54.449743 637797 finetune.py:68] layer 10_o @ epoch 0 new loss 0.0023533517960458994 old loss 0.002480039605870843 BETTER
I0325 10:49:15.151698 638720 finetune.py:68] layer 11_k @ epoch 4 new loss 0.0011880019446834922 old loss 0.0011959339026361704 BETTER
I0325 10:49:19.730949 636866 finetune.py:68] layer 9_o @ epoch 2 new loss 0.0016994490288197994 old loss 0.0017252746038138866 BETTER
I0325 10:49:25.785373 637797 finetune.py:68] layer 10_o @ epoch 1 new loss 0.0023022412788122892 old loss 0.0023533517960458994 BETTER
I0325 10:49:35.219036 638720 finetune.py:45] layer 11_o initial loss 0.0026031627785414457
I0325 10:49:53.199494 636866 finetune.py:68] layer 9_o @ epoch 3 new loss 0.0016817869618535042 old loss 0.0016994490288197994 BETTER
I0325 10:49:57.495212 637797 finetune.py:68] layer 10_o @ epoch 2 new loss 0.002272960264235735 old loss 0.0023022412788122892 BETTER
I0325 10:50:05.833250 638720 finetune.py:68] layer 11_o @ epoch 0 new loss 0.0024633470457047224 old loss 0.0026031627785414457 BETTER
I0325 10:50:27.230682 636866 finetune.py:68] layer 9_o @ epoch 4 new loss 0.0016680489061400294 old loss 0.0016817869618535042 BETTER
I0325 10:50:29.127333 637797 finetune.py:68] layer 10_o @ epoch 3 new loss 0.0022517370525747538 old loss 0.002272960264235735 BETTER
I0325 10:50:37.423899 638720 finetune.py:68] layer 11_o @ epoch 1 new loss 0.0024095450062304735 old loss 0.0024633470457047224 BETTER
I0325 10:50:53.355643 636866 finetune.py:45] layer 9_up initial loss 0.002753918757662177
I0325 10:51:00.816093 637797 finetune.py:68] layer 10_o @ epoch 4 new loss 0.0022345902398228645 old loss 0.0022517370525747538 BETTER
I0325 10:51:09.073584 638720 finetune.py:68] layer 11_o @ epoch 2 new loss 0.002379516139626503 old loss 0.0024095450062304735 BETTER
I0325 10:51:23.534646 636866 finetune.py:68] layer 9_up @ epoch 0 new loss 0.002714496338739991 old loss 0.002753918757662177 BETTER
I0325 10:51:26.885570 637797 finetune.py:45] layer 10_up initial loss 0.0034774011000990868
I0325 10:51:40.925928 638720 finetune.py:68] layer 11_o @ epoch 3 new loss 0.00235800351947546 old loss 0.002379516139626503 BETTER
I0325 10:51:55.104089 636866 finetune.py:68] layer 9_up @ epoch 1 new loss 0.0026929194573312998 old loss 0.002714496338739991 BETTER
I0325 10:51:55.657254 637797 finetune.py:68] layer 10_up @ epoch 0 new loss 0.0034278081730008125 old loss 0.0034774011000990868 BETTER
I0325 10:52:12.913314 638720 finetune.py:68] layer 11_o @ epoch 4 new loss 0.0023407319094985723 old loss 0.00235800351947546 BETTER
I0325 10:52:25.550675 637797 finetune.py:68] layer 10_up @ epoch 1 new loss 0.0034015027340501547 old loss 0.0034278081730008125 BETTER
I0325 10:52:26.887083 636866 finetune.py:68] layer 9_up @ epoch 2 new loss 0.0026762299239635468 old loss 0.0026929194573312998 BETTER
I0325 10:52:38.906704 638720 finetune.py:45] layer 11_up initial loss 0.003774723270907998
I0325 10:52:55.619930 637797 finetune.py:68] layer 10_up @ epoch 2 new loss 0.003380833426490426 old loss 0.0034015027340501547 BETTER
I0325 10:52:58.947775 636866 finetune.py:68] layer 9_up @ epoch 3 new loss 0.002662074286490679 old loss 0.0026762299239635468 BETTER
I0325 10:53:07.916133 638720 finetune.py:68] layer 11_up @ epoch 0 new loss 0.0037218634970486164 old loss 0.003774723270907998 BETTER
I0325 10:53:25.709306 637797 finetune.py:68] layer 10_up @ epoch 3 new loss 0.003363340860232711 old loss 0.003380833426490426 BETTER
I0325 10:53:31.164944 636866 finetune.py:68] layer 9_up @ epoch 4 new loss 0.002649595495313406 old loss 0.002662074286490679 BETTER
I0325 10:53:37.847704 638720 finetune.py:68] layer 11_up @ epoch 1 new loss 0.0036936672404408455 old loss 0.0037218634970486164 BETTER
I0325 10:53:55.841081 637797 finetune.py:68] layer 10_up @ epoch 4 new loss 0.0033475756645202637 old loss 0.003363340860232711 BETTER
I0325 10:53:56.958863 636866 finetune.py:45] layer 9_gate initial loss 0.0033598109148442745
I0325 10:54:07.999876 638720 finetune.py:68] layer 11_up @ epoch 2 new loss 0.0036721054930239916 old loss 0.0036936672404408455 BETTER
I0325 10:54:21.787089 637797 finetune.py:45] layer 10_gate initial loss 0.004191418178379536
I0325 10:54:25.939019 636866 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0033404368441551924 old loss 0.0033598109148442745 BETTER
I0325 10:54:38.289908 638720 finetune.py:68] layer 11_up @ epoch 3 new loss 0.00365363247692585 old loss 0.0036721054930239916 BETTER
I0325 10:54:49.343577 637797 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.004165526945143938 old loss 0.004191418178379536 BETTER
I0325 10:54:56.009002 636866 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.0033259945921599865 old loss 0.0033404368441551924 BETTER
I0325 10:55:08.498410 638720 finetune.py:68] layer 11_up @ epoch 4 new loss 0.003637004643678665 old loss 0.00365363247692585 BETTER
I0325 10:55:17.255432 637797 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.004147288855165243 old loss 0.004165526945143938 BETTER
I0325 10:55:26.593619 636866 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.003313726745545864 old loss 0.0033259945921599865 BETTER
I0325 10:55:35.180324 638720 finetune.py:45] layer 11_gate initial loss 0.004619857296347618
I0325 10:55:45.565695 637797 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.004132443107664585 old loss 0.004147288855165243 BETTER
I0325 10:55:56.848463 636866 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.0033027525059878826 old loss 0.003313726745545864 BETTER
I0325 10:56:02.640877 638720 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.004593015648424625 old loss 0.004619857296347618 BETTER
I0325 10:56:13.817883 637797 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.004119113087654114 old loss 0.004132443107664585 BETTER
I0325 10:56:27.338373 636866 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.003292594337835908 old loss 0.0033027525059878826 BETTER
I0325 10:56:31.055345 638720 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.004573988728225231 old loss 0.004593015648424625 BETTER
I0325 10:56:42.111571 637797 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.004106852225959301 old loss 0.004119113087654114 BETTER
I0325 10:56:59.527796 638720 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.004558057524263859 old loss 0.004573988728225231 BETTER
I0325 10:57:10.037466 636866 finetune.py:45] layer 9_down initial loss 0.004542538896203041
I0325 10:57:25.123441 637797 finetune.py:45] layer 10_down initial loss 0.005540639162063599
I0325 10:57:27.937523 638720 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.004543791990727186 old loss 0.004558057524263859 BETTER
I0325 10:57:36.856836 636866 finetune.py:68] layer 9_down @ epoch 0 new loss 0.004540452267974615 old loss 0.004542538896203041 BETTER
I0325 10:57:50.684046 637797 finetune.py:68] layer 10_down @ epoch 0 new loss 0.005538349971175194 old loss 0.005540639162063599 BETTER
I0325 10:57:56.897344 638720 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.004530603997409344 old loss 0.004543791990727186 BETTER
I0325 10:58:05.259770 636866 finetune.py:68] layer 9_down @ epoch 1 new loss 0.004538502544164658 old loss 0.004540452267974615 BETTER
I0325 10:58:17.717390 637797 finetune.py:68] layer 10_down @ epoch 1 new loss 0.005536193028092384 old loss 0.005538349971175194 BETTER
I0325 10:58:33.701516 636866 finetune.py:68] layer 9_down @ epoch 2 new loss 0.004536618012934923 old loss 0.004538502544164658 BETTER
I0325 10:58:39.923018 638720 finetune.py:45] layer 11_down initial loss 0.006134998053312302
I0325 10:58:44.550959 637797 finetune.py:68] layer 10_down @ epoch 2 new loss 0.005534143187105656 old loss 0.005536193028092384 BETTER
I0325 10:59:02.230286 636866 finetune.py:68] layer 9_down @ epoch 3 new loss 0.004534837324172258 old loss 0.004536618012934923 BETTER
I0325 10:59:05.502435 638720 finetune.py:68] layer 11_down @ epoch 0 new loss 0.006132322363555431 old loss 0.006134998053312302 BETTER
I0325 10:59:11.481542 637797 finetune.py:68] layer 10_down @ epoch 3 new loss 0.005532192066311836 old loss 0.005534143187105656 BETTER
I0325 10:59:30.821565 636866 finetune.py:68] layer 9_down @ epoch 4 new loss 0.004533120431005955 old loss 0.004534837324172258 BETTER
I0325 10:59:32.456507 638720 finetune.py:68] layer 11_down @ epoch 1 new loss 0.006129813380539417 old loss 0.006132322363555431 BETTER
9_v proxy err 0.14680282771587372 tr(WHW.T) 3711.38134765625
bpp_loss 1.2315202414174564
9_q proxy err 0.014548597857356071 tr(WHW.T) 45857.55078125
bpp_loss 1.843532560276799
9_k proxy err 0.009246733039617538 tr(WHW.T) 72413.671875
bpp_loss 1.8792530726786936
9_o proxy err 0.19648343324661255 tr(WHW.T) 3275.570068359375
bpp_loss 1.2162443150300533
9_up proxy err 0.11448908597230911 tr(WHW.T) 20610.111328125
bpp_loss 1.4218539388763696
9_gate proxy err 0.05440953001379967 tr(WHW.T) 44727.390625
bpp_loss 1.5235707248279522
9_down proxy err 0.1556771844625473 tr(WHW.T) 15934.6943359375
bpp_loss 1.4031320085402492
I0325 10:59:39.532864 637797 finetune.py:68] layer 10_down @ epoch 4 new loss 0.0055303131230175495 old loss 0.005532192066311836 BETTER
10_v proxy err 0.14864720404148102 tr(WHW.T) 3685.848388671875
bpp_loss 1.226229958148906
10_q proxy err 0.015179365873336792 tr(WHW.T) 44122.91796875
bpp_loss 1.8360819921072107
10_k proxy err 0.009564967826008797 tr(WHW.T) 70278.3515625
bpp_loss 1.8771506082848646
10_o proxy err 0.19876110553741455 tr(WHW.T) 3167.3740234375
bpp_loss 1.2154774410300888
10_up proxy err 0.10835961997509003 tr(WHW.T) 21843.2734375
bpp_loss 1.433440473233891
10_gate proxy err 0.053441762924194336 tr(WHW.T) 45321.74609375
bpp_loss 1.5168483157014085
10_down proxy err 0.14613650739192963 tr(WHW.T) 16771.958984375
bpp_loss 1.4102630077124856
I0325 11:00:00.288829 638720 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0061274305917322636 old loss 0.006129813380539417 BETTER
I0325 11:00:27.400910 638720 finetune.py:68] layer 11_down @ epoch 3 new loss 0.00612515676766634 old loss 0.0061274305917322636 BETTER
I0325 11:00:48.129979 583434 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 63.712950229644775s
I0325 11:00:51.578359 654446 config.py:54] PyTorch version 2.6.0 available.
W0325 11:00:51.881786 654446 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 11:00:52.997848 654446 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 11:00:53.002285 583434 quantize_finetune_llama.py:209] layer 13 gpu 1
I0325 11:00:53.015365 654446 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 11:00:54.231934 638720 finetune.py:68] layer 11_down @ epoch 4 new loss 0.006122974678874016 old loss 0.00612515676766634 BETTER
11_v proxy err 0.15003450214862823 tr(WHW.T) 3928.038330078125
bpp_loss 1.2543276083888486
11_q proxy err 0.01792861707508564 tr(WHW.T) 38245.2890625
bpp_loss 1.733315433259122
11_k proxy err 0.011924738995730877 tr(WHW.T) 57380.2109375
bpp_loss 1.7439558723417576
11_o proxy err 0.2009248584508896 tr(WHW.T) 3175.853515625
bpp_loss 1.2339796909946017
11_up proxy err 0.11288326233625412 tr(WHW.T) 21568.25
bpp_loss 1.4400885345327645
11_gate proxy err 0.05538204312324524 tr(WHW.T) 44819.0859375
bpp_loss 1.5123613439500332
11_down proxy err 0.15337608754634857 tr(WHW.T) 16359.142578125
bpp_loss 1.4171889376025213
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 11:01:11.142864 654446 finetune.py:45] layer 12_v initial loss 0.001761181396432221
W0325 11:01:11.143134 654446 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 11:01:44.153681 654446 finetune.py:68] layer 12_v @ epoch 0 new loss 0.0012617422034963965 old loss 0.001761181396432221 BETTER
I0325 11:01:59.930305 583434 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 61.24606442451477s
I0325 11:02:03.478852 655384 config.py:54] PyTorch version 2.6.0 available.
W0325 11:02:03.788880 655384 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 11:02:04.743534 655384 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 11:02:04.747572 583434 quantize_finetune_llama.py:209] layer 14 gpu 2
I0325 11:02:04.760898 655384 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 11:02:18.628039 654446 finetune.py:68] layer 12_v @ epoch 1 new loss 0.0011654647532850504 old loss 0.0012617422034963965 BETTER
I0325 11:02:23.645982 655384 finetune.py:45] layer 13_v initial loss 0.002133140107616782
W0325 11:02:23.646182 655384 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 11:02:53.621570 654446 finetune.py:68] layer 12_v @ epoch 2 new loss 0.001124188769608736 old loss 0.0011654647532850504 BETTER
I0325 11:02:55.067520 655384 finetune.py:68] layer 13_v @ epoch 0 new loss 0.0014439959777519107 old loss 0.002133140107616782 BETTER
I0325 11:03:07.559860 583434 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 62.346811056137085s
I0325 11:03:11.187460 656290 config.py:54] PyTorch version 2.6.0 available.
W0325 11:03:11.494939 656290 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 11:03:12.448795 656290 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 11:03:12.452821 583434 quantize_finetune_llama.py:209] layer 15 gpu 0
I0325 11:03:12.469602 656290 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 11:03:27.618770 655384 finetune.py:68] layer 13_v @ epoch 1 new loss 0.0013279502745717764 old loss 0.0014439959777519107 BETTER
I0325 11:03:28.818872 654446 finetune.py:68] layer 12_v @ epoch 3 new loss 0.0010979236103594303 old loss 0.001124188769608736 BETTER
I0325 11:03:31.390111 656290 finetune.py:45] layer 14_v initial loss 0.0023234060499817133
W0325 11:03:31.390293 656290 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 11:04:00.245161 655384 finetune.py:68] layer 13_v @ epoch 2 new loss 0.0012798592215403914 old loss 0.0013279502745717764 BETTER
I0325 11:04:03.220395 656290 finetune.py:68] layer 14_v @ epoch 0 new loss 0.0017782854847609997 old loss 0.0023234060499817133 BETTER
I0325 11:04:04.015566 654446 finetune.py:68] layer 12_v @ epoch 4 new loss 0.0010783083271235228 old loss 0.0010979236103594303 BETTER
I0325 11:04:22.922922 654446 finetune.py:45] layer 12_q initial loss 0.001319657894782722
I0325 11:04:33.341709 655384 finetune.py:68] layer 13_v @ epoch 3 new loss 0.0012491645757108927 old loss 0.0012798592215403914 BETTER
I0325 11:04:35.859911 656290 finetune.py:68] layer 14_v @ epoch 1 new loss 0.0016658235108479857 old loss 0.0017782854847609997 BETTER
I0325 11:04:56.020278 654446 finetune.py:68] layer 12_q @ epoch 0 new loss 0.0012668062699958682 old loss 0.001319657894782722 BETTER
I0325 11:05:06.570180 655384 finetune.py:68] layer 13_v @ epoch 4 new loss 0.0012262417003512383 old loss 0.0012491645757108927 BETTER
I0325 11:05:08.891545 656290 finetune.py:68] layer 14_v @ epoch 2 new loss 0.0016109319403767586 old loss 0.0016658235108479857 BETTER
I0325 11:05:25.581782 655384 finetune.py:45] layer 13_q initial loss 0.0014619415160268545
I0325 11:05:30.520538 654446 finetune.py:68] layer 12_q @ epoch 1 new loss 0.0012436655815690756 old loss 0.0012668062699958682 BETTER
I0325 11:05:42.347517 656290 finetune.py:68] layer 14_v @ epoch 3 new loss 0.0015738835791125894 old loss 0.0016109319403767586 BETTER
I0325 11:05:57.094115 655384 finetune.py:68] layer 13_q @ epoch 0 new loss 0.00141326361335814 old loss 0.0014619415160268545 BETTER
I0325 11:06:05.096471 654446 finetune.py:68] layer 12_q @ epoch 2 new loss 0.001226387801580131 old loss 0.0012436655815690756 BETTER
I0325 11:06:15.631205 656290 finetune.py:68] layer 14_v @ epoch 4 new loss 0.001545581384561956 old loss 0.0015738835791125894 BETTER
I0325 11:06:29.283929 655384 finetune.py:68] layer 13_q @ epoch 1 new loss 0.0013875787844881415 old loss 0.00141326361335814 BETTER
I0325 11:06:34.800763 656290 finetune.py:45] layer 14_q initial loss 0.0018324546981602907
I0325 11:06:39.903188 654446 finetune.py:68] layer 12_q @ epoch 3 new loss 0.0012124086497351527 old loss 0.001226387801580131 BETTER
I0325 11:07:01.614639 655384 finetune.py:68] layer 13_q @ epoch 2 new loss 0.0013683020370081067 old loss 0.0013875787844881415 BETTER
I0325 11:07:06.333652 656290 finetune.py:68] layer 14_q @ epoch 0 new loss 0.001768156886100769 old loss 0.0018324546981602907 BETTER
I0325 11:07:14.694388 654446 finetune.py:68] layer 12_q @ epoch 4 new loss 0.0012000077404081821 old loss 0.0012124086497351527 BETTER
I0325 11:07:34.081543 654446 finetune.py:45] layer 12_k initial loss 0.0013874114956706762
I0325 11:07:34.242224 655384 finetune.py:68] layer 13_q @ epoch 3 new loss 0.001352330669760704 old loss 0.0013683020370081067 BETTER
I0325 11:07:38.924903 656290 finetune.py:68] layer 14_q @ epoch 1 new loss 0.0017359725898131728 old loss 0.001768156886100769 BETTER
I0325 11:08:06.801134 654446 finetune.py:68] layer 12_k @ epoch 0 new loss 0.0013535389443859458 old loss 0.0013874114956706762 BETTER
I0325 11:08:07.013819 655384 finetune.py:68] layer 13_q @ epoch 4 new loss 0.0013384808553382754 old loss 0.001352330669760704 BETTER
I0325 11:08:11.813917 656290 finetune.py:68] layer 14_q @ epoch 2 new loss 0.0017114924266934395 old loss 0.0017359725898131728 BETTER
I0325 11:08:27.116496 655384 finetune.py:45] layer 13_k initial loss 0.001538373064249754
I0325 11:08:40.614326 654446 finetune.py:68] layer 12_k @ epoch 1 new loss 0.0013406530488282442 old loss 0.0013535389443859458 BETTER
I0325 11:08:44.699687 656290 finetune.py:68] layer 14_q @ epoch 3 new loss 0.0016911919228732586 old loss 0.0017114924266934395 BETTER
I0325 11:08:58.456767 655384 finetune.py:68] layer 13_k @ epoch 0 new loss 0.001508403685875237 old loss 0.001538373064249754 BETTER
I0325 11:09:14.877686 654446 finetune.py:68] layer 12_k @ epoch 2 new loss 0.001329664490185678 old loss 0.0013406530488282442 BETTER
I0325 11:09:17.276386 656290 finetune.py:68] layer 14_q @ epoch 4 new loss 0.001673346501775086 old loss 0.0016911919228732586 BETTER
I0325 11:09:30.608243 655384 finetune.py:68] layer 13_k @ epoch 1 new loss 0.0014942383859306574 old loss 0.001508403685875237 BETTER
I0325 11:09:37.124682 656290 finetune.py:45] layer 14_k initial loss 0.0018813449423760176
I0325 11:09:48.969677 654446 finetune.py:68] layer 12_k @ epoch 3 new loss 0.0013200279790908098 old loss 0.001329664490185678 BETTER
I0325 11:10:02.871256 655384 finetune.py:68] layer 13_k @ epoch 2 new loss 0.0014821432996541262 old loss 0.0014942383859306574 BETTER
I0325 11:10:08.537752 656290 finetune.py:68] layer 14_k @ epoch 0 new loss 0.0018547828076407313 old loss 0.0018813449423760176 BETTER
I0325 11:10:22.861782 654446 finetune.py:68] layer 12_k @ epoch 4 new loss 0.0013111967127770185 old loss 0.0013200279790908098 BETTER
I0325 11:10:35.068456 655384 finetune.py:68] layer 13_k @ epoch 3 new loss 0.00147128920070827 old loss 0.0014821432996541262 BETTER
I0325 11:10:40.708528 656290 finetune.py:68] layer 14_k @ epoch 1 new loss 0.001838200492784381 old loss 0.0018547828076407313 BETTER
I0325 11:10:42.142405 654446 finetune.py:45] layer 12_o initial loss 0.0028571945149451494
I0325 11:11:07.239133 655384 finetune.py:68] layer 13_k @ epoch 4 new loss 0.0014613880775868893 old loss 0.00147128920070827 BETTER
I0325 11:11:13.399356 656290 finetune.py:68] layer 14_k @ epoch 2 new loss 0.0018238626653328538 old loss 0.001838200492784381 BETTER
I0325 11:11:14.459422 654446 finetune.py:68] layer 12_o @ epoch 0 new loss 0.0027150153182446957 old loss 0.0028571945149451494 BETTER
I0325 11:11:26.767104 655384 finetune.py:45] layer 13_o initial loss 0.003160727210342884
I0325 11:11:45.527523 656290 finetune.py:68] layer 14_k @ epoch 3 new loss 0.0018107425421476364 old loss 0.0018238626653328538 BETTER
I0325 11:11:47.638094 654446 finetune.py:68] layer 12_o @ epoch 1 new loss 0.0026611131615936756 old loss 0.0027150153182446957 BETTER
I0325 11:11:57.263940 655384 finetune.py:68] layer 13_o @ epoch 0 new loss 0.0029913370963186026 old loss 0.003160727210342884 BETTER
I0325 11:12:17.859950 656290 finetune.py:68] layer 14_k @ epoch 4 new loss 0.001798491575755179 old loss 0.0018107425421476364 BETTER
I0325 11:12:21.179494 654446 finetune.py:68] layer 12_o @ epoch 2 new loss 0.0026306291110813618 old loss 0.0026611131615936756 BETTER
I0325 11:12:28.813073 655384 finetune.py:68] layer 13_o @ epoch 1 new loss 0.0029319580644369125 old loss 0.0029913370963186026 BETTER
I0325 11:12:37.128371 656290 finetune.py:45] layer 14_o initial loss 0.0037791228387504816
I0325 11:12:54.711469 654446 finetune.py:68] layer 12_o @ epoch 3 new loss 0.0026084717828780413 old loss 0.0026306291110813618 BETTER
I0325 11:13:00.262577 655384 finetune.py:68] layer 13_o @ epoch 2 new loss 0.0028972947038710117 old loss 0.0029319580644369125 BETTER
I0325 11:13:07.802088 656290 finetune.py:68] layer 14_o @ epoch 0 new loss 0.003628188744187355 old loss 0.0037791228387504816 BETTER
I0325 11:13:28.239128 654446 finetune.py:68] layer 12_o @ epoch 4 new loss 0.0025904104113578796 old loss 0.0026084717828780413 BETTER
I0325 11:13:31.835296 655384 finetune.py:68] layer 13_o @ epoch 3 new loss 0.0028717550449073315 old loss 0.0028972947038710117 BETTER
I0325 11:13:39.413029 656290 finetune.py:68] layer 14_o @ epoch 1 new loss 0.0035745881032198668 old loss 0.003628188744187355 BETTER
I0325 11:13:54.024997 654446 finetune.py:45] layer 12_up initial loss 0.004213348496705294
I0325 11:14:03.635506 655384 finetune.py:68] layer 13_o @ epoch 4 new loss 0.002850552322342992 old loss 0.0028717550449073315 BETTER
I0325 11:14:11.083356 656290 finetune.py:68] layer 14_o @ epoch 2 new loss 0.003541340585798025 old loss 0.0035745881032198668 BETTER
I0325 11:14:24.599385 654446 finetune.py:68] layer 12_up @ epoch 0 new loss 0.004155747592449188 old loss 0.004213348496705294 BETTER
I0325 11:14:29.734082 655384 finetune.py:45] layer 13_up initial loss 0.004794355947524309
I0325 11:14:42.833374 656290 finetune.py:68] layer 14_o @ epoch 3 new loss 0.003515401855111122 old loss 0.003541340585798025 BETTER
I0325 11:14:56.056967 654446 finetune.py:68] layer 12_up @ epoch 1 new loss 0.004124321974813938 old loss 0.004155747592449188 BETTER
I0325 11:14:58.444729 655384 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00472172349691391 old loss 0.004794355947524309 BETTER
I0325 11:15:14.492564 656290 finetune.py:68] layer 14_o @ epoch 4 new loss 0.003493327647447586 old loss 0.003515401855111122 BETTER
I0325 11:15:27.712284 654446 finetune.py:68] layer 12_up @ epoch 2 new loss 0.004099660087376833 old loss 0.004124321974813938 BETTER
I0325 11:15:28.075722 655384 finetune.py:68] layer 13_up @ epoch 1 new loss 0.004680264741182327 old loss 0.00472172349691391 BETTER
I0325 11:15:40.382898 656290 finetune.py:45] layer 14_up initial loss 0.0057440693490207195
I0325 11:15:58.146277 655384 finetune.py:68] layer 13_up @ epoch 2 new loss 0.00464782165363431 old loss 0.004680264741182327 BETTER
I0325 11:15:59.697740 654446 finetune.py:68] layer 12_up @ epoch 3 new loss 0.0040785144083201885 old loss 0.004099660087376833 BETTER
I0325 11:16:09.403166 656290 finetune.py:68] layer 14_up @ epoch 0 new loss 0.00566007848829031 old loss 0.0057440693490207195 BETTER
I0325 11:16:28.155299 655384 finetune.py:68] layer 13_up @ epoch 3 new loss 0.004620164632797241 old loss 0.00464782165363431 BETTER
I0325 11:16:31.460464 654446 finetune.py:68] layer 12_up @ epoch 4 new loss 0.004059677943587303 old loss 0.0040785144083201885 BETTER
I0325 11:16:39.214325 656290 finetune.py:68] layer 14_up @ epoch 1 new loss 0.005612131208181381 old loss 0.00566007848829031 BETTER
I0325 11:16:57.686478 654446 finetune.py:45] layer 12_gate initial loss 0.005223801825195551
I0325 11:16:58.059458 655384 finetune.py:68] layer 13_up @ epoch 4 new loss 0.004595789592713118 old loss 0.004620164632797241 BETTER
I0325 11:17:08.985548 656290 finetune.py:68] layer 14_up @ epoch 2 new loss 0.005573999602347612 old loss 0.005612131208181381 BETTER
I0325 11:17:24.253181 655384 finetune.py:45] layer 13_gate initial loss 0.006045087706297636
I0325 11:17:26.461243 654446 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.005193697288632393 old loss 0.005223801825195551 BETTER
I0325 11:17:38.837457 656290 finetune.py:68] layer 14_up @ epoch 3 new loss 0.005541645921766758 old loss 0.005573999602347612 BETTER
I0325 11:17:51.628513 655384 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.006008339114487171 old loss 0.006045087706297636 BETTER
I0325 11:17:56.271795 654446 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.005172505509108305 old loss 0.005193697288632393 BETTER
I0325 11:18:08.741266 656290 finetune.py:68] layer 14_up @ epoch 4 new loss 0.005512869916856289 old loss 0.005541645921766758 BETTER
I0325 11:18:20.060255 655384 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.00598176522180438 old loss 0.006008339114487171 BETTER
I0325 11:18:26.153692 654446 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.005154856946319342 old loss 0.005172505509108305 BETTER
I0325 11:18:34.765606 656290 finetune.py:45] layer 14_gate initial loss 0.007285685278475285
I0325 11:18:48.092335 655384 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.005959179252386093 old loss 0.00598176522180438 BETTER
I0325 11:18:56.251133 654446 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.005138846579939127 old loss 0.005154856946319342 BETTER
I0325 11:19:02.307223 656290 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.007243437226861715 old loss 0.007285685278475285 BETTER
I0325 11:19:16.043747 655384 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.005938426591455936 old loss 0.005959179252386093 BETTER
I0325 11:19:26.392939 654446 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.005123975221067667 old loss 0.005138846579939127 BETTER
I0325 11:19:30.540269 656290 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.007213166449218988 old loss 0.007243437226861715 BETTER
I0325 11:19:44.132932 655384 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0059191943146288395 old loss 0.005938426591455936 BETTER
I0325 11:19:58.372319 656290 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.00718695530667901 old loss 0.007213166449218988 BETTER
I0325 11:20:08.730213 654446 finetune.py:45] layer 12_down initial loss 0.006967558991163969
I0325 11:20:26.442300 656290 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.007162843830883503 old loss 0.00718695530667901 BETTER
I0325 11:20:26.802167 655384 finetune.py:45] layer 13_down initial loss 0.00819727499037981
I0325 11:20:35.137768 654446 finetune.py:68] layer 12_down @ epoch 0 new loss 0.006964588537812233 old loss 0.006967558991163969 BETTER
I0325 11:20:52.161744 655384 finetune.py:68] layer 13_down @ epoch 0 new loss 0.008193377405405045 old loss 0.00819727499037981 BETTER
I0325 11:20:54.964470 656290 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.007139948662370443 old loss 0.007162843830883503 BETTER
I0325 11:21:03.130477 654446 finetune.py:68] layer 12_down @ epoch 1 new loss 0.006961699575185776 old loss 0.006964588537812233 BETTER
I0325 11:21:19.261492 655384 finetune.py:68] layer 13_down @ epoch 1 new loss 0.008189648389816284 old loss 0.008193377405405045 BETTER
I0325 11:21:31.965756 654446 finetune.py:68] layer 12_down @ epoch 2 new loss 0.006958923302590847 old loss 0.006961699575185776 BETTER
I0325 11:21:38.005667 656290 finetune.py:45] layer 14_down initial loss 0.009825789369642735
I0325 11:21:46.133944 655384 finetune.py:68] layer 13_down @ epoch 2 new loss 0.00818606372922659 old loss 0.008189648389816284 BETTER
I0325 11:22:00.378356 654446 finetune.py:68] layer 12_down @ epoch 3 new loss 0.006956223398447037 old loss 0.006958923302590847 BETTER
I0325 11:22:03.629488 656290 finetune.py:68] layer 14_down @ epoch 0 new loss 0.009820632636547089 old loss 0.009825789369642735 BETTER
I0325 11:22:12.859318 655384 finetune.py:68] layer 13_down @ epoch 3 new loss 0.008182567544281483 old loss 0.00818606372922659 BETTER
I0325 11:22:28.877029 654446 finetune.py:68] layer 12_down @ epoch 4 new loss 0.006953612435609102 old loss 0.006956223398447037 BETTER
12_v proxy err 0.156559020280838 tr(WHW.T) 3843.647705078125
bpp_loss 1.2434510123712244
12_q proxy err 0.018657701089978218 tr(WHW.T) 38607.02734375
bpp_loss 1.7830446639709407
12_k proxy err 0.012122687883675098 tr(WHW.T) 59792.76171875
bpp_loss 1.831562125516939
12_o proxy err 0.20901791751384735 tr(WHW.T) 3125.44482421875
bpp_loss 1.2321811872243416
12_up proxy err 0.11307154595851898 tr(WHW.T) 21806.046875
bpp_loss 1.4493747385608595
12_gate proxy err 0.059737417846918106 tr(WHW.T) 41820.12109375
bpp_loss 1.5050344280032224
12_down proxy err 0.15336695313453674 tr(WHW.T) 16453.326171875
bpp_loss 1.4263192498298405
I0325 11:22:30.582354 656290 finetune.py:68] layer 14_down @ epoch 1 new loss 0.009815690107643604 old loss 0.009820632636547089 BETTER
I0325 11:22:40.416605 655384 finetune.py:68] layer 13_down @ epoch 4 new loss 0.008179163560271263 old loss 0.008182567544281483 BETTER
13_v proxy err 0.16415081918239594 tr(WHW.T) 3922.961669921875
bpp_loss 1.2661128002946498
13_q proxy err 0.019808346405625343 tr(WHW.T) 38293.10546875
bpp_loss 1.7719030962180113
13_k proxy err 0.013237114064395428 tr(WHW.T) 57506.44921875
bpp_loss 1.799755863568862
13_o proxy err 0.19371293485164642 tr(WHW.T) 3528.00146484375
bpp_loss 1.2569805267849006
13_up proxy err 0.10799727588891983 tr(WHW.T) 22638.17578125
bpp_loss 1.4605252772439705
13_gate proxy err 0.05799732729792595 tr(WHW.T) 42487.05078125
bpp_loss 1.5014148378354866
13_down proxy err 0.15000908076763153 tr(WHW.T) 16443.37109375
bpp_loss 1.4345229579525631
I0325 11:22:58.433253 656290 finetune.py:68] layer 14_down @ epoch 2 new loss 0.009810801595449448 old loss 0.009815690107643604 BETTER
I0325 11:23:25.276648 656290 finetune.py:68] layer 14_down @ epoch 3 new loss 0.009806067682802677 old loss 0.009810801595449448 BETTER
I0325 11:23:49.393336 583434 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 64.15239119529724s
I0325 11:23:52.057616 656290 finetune.py:68] layer 14_down @ epoch 4 new loss 0.00980140920728445 old loss 0.009806067682802677 BETTER
I0325 11:23:53.039701 672050 config.py:54] PyTorch version 2.6.0 available.
W0325 11:23:53.352722 672050 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

14_v proxy err 0.17012883722782135 tr(WHW.T) 3685.144287109375
bpp_loss 1.252417157200398
14_q proxy err 0.020097563043236732 tr(WHW.T) 37011.17578125
bpp_loss 1.7680284343659878
14_k proxy err 0.012590374797582626 tr(WHW.T) 59253.7890625
bpp_loss 1.80352898935962
14_o proxy err 0.21006950736045837 tr(WHW.T) 3190.661865234375
bpp_loss 1.2411643371160608
14_up proxy err 0.11133671551942825 tr(WHW.T) 22474.7578125
bpp_loss 1.4617263707007433
14_gate proxy err 0.061935581266880035 tr(WHW.T) 40693.640625
bpp_loss 1.4980979387020303
14_down proxy err 0.15471763908863068 tr(WHW.T) 16105.501953125
bpp_loss 1.4356441862426352
W0325 11:23:54.280171 672050 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 11:23:54.284194 583434 quantize_finetune_llama.py:209] layer 16 gpu 1
I0325 11:23:54.297331 672050 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 11:24:12.876129 672050 finetune.py:45] layer 15_v initial loss 0.00238768826238811
W0325 11:24:12.876347 672050 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 11:24:45.815257 672050 finetune.py:68] layer 15_v @ epoch 0 new loss 0.0017764277290552855 old loss 0.00238768826238811 BETTER
I0325 11:24:57.922394 583434 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 61.550556659698486s
I0325 11:25:01.564370 672942 config.py:54] PyTorch version 2.6.0 available.
W0325 11:25:01.868470 672942 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 11:25:02.866761 672942 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 11:25:02.870786 583434 quantize_finetune_llama.py:209] layer 17 gpu 2
I0325 11:25:02.883480 672942 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 11:25:20.362812 672050 finetune.py:68] layer 15_v @ epoch 1 new loss 0.0016718091210350394 old loss 0.0017764277290552855 BETTER
I0325 11:25:21.591952 672942 finetune.py:45] layer 16_v initial loss 0.0034715773072093725
W0325 11:25:21.592149 672942 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 11:25:53.049203 672942 finetune.py:68] layer 16_v @ epoch 0 new loss 0.0025116875767707825 old loss 0.0034715773072093725 BETTER
I0325 11:25:55.313756 672050 finetune.py:68] layer 15_v @ epoch 2 new loss 0.001621586037799716 old loss 0.0016718091210350394 BETTER
I0325 11:26:06.225103 583434 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 62.903860330581665s
I0325 11:26:09.874762 673843 config.py:54] PyTorch version 2.6.0 available.
W0325 11:26:10.201653 673843 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 11:26:11.179381 673843 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 11:26:11.183395 583434 quantize_finetune_llama.py:209] layer 18 gpu 0
I0325 11:26:11.196489 673843 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 11:26:25.681969 672942 finetune.py:68] layer 16_v @ epoch 1 new loss 0.002320861676707864 old loss 0.0025116875767707825 BETTER
I0325 11:26:30.387852 672050 finetune.py:68] layer 15_v @ epoch 3 new loss 0.0015866636531427503 old loss 0.001621586037799716 BETTER
I0325 11:26:30.507557 673843 finetune.py:45] layer 17_v initial loss 0.0031014138367027044
W0325 11:26:30.507740 673843 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 11:26:58.254087 672942 finetune.py:68] layer 16_v @ epoch 2 new loss 0.0022349015343934298 old loss 0.002320861676707864 BETTER
I0325 11:27:02.009584 673843 finetune.py:68] layer 17_v @ epoch 0 new loss 0.0020262659527361393 old loss 0.0031014138367027044 BETTER
I0325 11:27:05.547051 672050 finetune.py:68] layer 15_v @ epoch 4 new loss 0.0015596810262650251 old loss 0.0015866636531427503 BETTER
I0325 11:27:24.628076 672050 finetune.py:45] layer 15_q initial loss 0.0019006740767508745
I0325 11:27:31.583703 672942 finetune.py:68] layer 16_v @ epoch 3 new loss 0.0021786948200315237 old loss 0.0022349015343934298 BETTER
I0325 11:27:34.743920 673843 finetune.py:68] layer 17_v @ epoch 1 new loss 0.0018560623284429312 old loss 0.0020262659527361393 BETTER
I0325 11:27:57.987442 672050 finetune.py:68] layer 15_q @ epoch 0 new loss 0.0018204324878752232 old loss 0.0019006740767508745 BETTER
I0325 11:28:04.933627 672942 finetune.py:68] layer 16_v @ epoch 4 new loss 0.0021371087059378624 old loss 0.0021786948200315237 BETTER
I0325 11:28:07.755784 673843 finetune.py:68] layer 17_v @ epoch 2 new loss 0.0017877673963084817 old loss 0.0018560623284429312 BETTER
I0325 11:28:23.903605 672942 finetune.py:45] layer 16_q initial loss 0.0025578427594155073
I0325 11:28:32.624009 672050 finetune.py:68] layer 15_q @ epoch 1 new loss 0.0017842408269643784 old loss 0.0018204324878752232 BETTER
I0325 11:28:41.025726 673843 finetune.py:68] layer 17_v @ epoch 3 new loss 0.001744240871630609 old loss 0.0017877673963084817 BETTER
I0325 11:28:55.379063 672942 finetune.py:68] layer 16_q @ epoch 0 new loss 0.0024488489143550396 old loss 0.0025578427594155073 BETTER
I0325 11:29:07.586100 672050 finetune.py:68] layer 15_q @ epoch 2 new loss 0.0017576542450115085 old loss 0.0017842408269643784 BETTER
I0325 11:29:14.475431 673843 finetune.py:68] layer 17_v @ epoch 4 new loss 0.0017120297998189926 old loss 0.001744240871630609 BETTER
I0325 11:29:27.729198 672942 finetune.py:68] layer 16_q @ epoch 1 new loss 0.002396213822066784 old loss 0.0024488489143550396 BETTER
I0325 11:29:33.384391 673843 finetune.py:45] layer 17_q initial loss 0.00216047465801239
I0325 11:29:42.591119 672050 finetune.py:68] layer 15_q @ epoch 3 new loss 0.0017357090255245566 old loss 0.0017576542450115085 BETTER
I0325 11:30:00.297967 672942 finetune.py:68] layer 16_q @ epoch 2 new loss 0.0023582831490784883 old loss 0.002396213822066784 BETTER
I0325 11:30:05.038448 673843 finetune.py:68] layer 17_q @ epoch 0 new loss 0.0020359375048428774 old loss 0.00216047465801239 BETTER
I0325 11:30:17.489416 672050 finetune.py:68] layer 15_q @ epoch 4 new loss 0.0017163808224722743 old loss 0.0017357090255245566 BETTER
I0325 11:30:33.208742 672942 finetune.py:68] layer 16_q @ epoch 3 new loss 0.0023274298291653395 old loss 0.0023582831490784883 BETTER
I0325 11:30:37.268857 672050 finetune.py:45] layer 15_k initial loss 0.0019818826112896204
I0325 11:30:37.448889 673843 finetune.py:68] layer 17_q @ epoch 1 new loss 0.0019826232455670834 old loss 0.0020359375048428774 BETTER
I0325 11:31:06.117262 672942 finetune.py:68] layer 16_q @ epoch 4 new loss 0.0023009225260466337 old loss 0.0023274298291653395 BETTER
I0325 11:31:10.038969 673843 finetune.py:68] layer 17_q @ epoch 2 new loss 0.001946631004102528 old loss 0.0019826232455670834 BETTER
I0325 11:31:10.200473 672050 finetune.py:68] layer 15_k @ epoch 0 new loss 0.00193961919285357 old loss 0.0019818826112896204 BETTER
I0325 11:31:25.027792 672942 finetune.py:45] layer 16_k initial loss 0.0026188476476818323
I0325 11:31:42.827928 673843 finetune.py:68] layer 17_q @ epoch 3 new loss 0.0019187885336577892 old loss 0.001946631004102528 BETTER
I0325 11:31:44.353558 672050 finetune.py:68] layer 15_k @ epoch 1 new loss 0.001919749192893505 old loss 0.00193961919285357 BETTER
I0325 11:31:56.420011 672942 finetune.py:68] layer 16_k @ epoch 0 new loss 0.002574322745203972 old loss 0.0026188476476818323 BETTER
I0325 11:32:15.758350 673843 finetune.py:68] layer 17_q @ epoch 4 new loss 0.0018956511048600078 old loss 0.0019187885336577892 BETTER
I0325 11:32:18.649867 672050 finetune.py:68] layer 15_k @ epoch 2 new loss 0.0019027853850275278 old loss 0.001919749192893505 BETTER
I0325 11:32:28.411229 672942 finetune.py:68] layer 16_k @ epoch 1 new loss 0.0025484515354037285 old loss 0.002574322745203972 BETTER
I0325 11:32:34.909842 673843 finetune.py:45] layer 17_k initial loss 0.002259942004457116
I0325 11:32:52.991014 672050 finetune.py:68] layer 15_k @ epoch 3 new loss 0.0018874548841267824 old loss 0.0019027853850275278 BETTER
I0325 11:33:00.520862 672942 finetune.py:68] layer 16_k @ epoch 2 new loss 0.0025259058456867933 old loss 0.0025484515354037285 BETTER
I0325 11:33:06.342829 673843 finetune.py:68] layer 17_k @ epoch 0 new loss 0.0022145677357912064 old loss 0.002259942004457116 BETTER
I0325 11:33:27.274549 672050 finetune.py:68] layer 15_k @ epoch 4 new loss 0.0018736555939540267 old loss 0.0018874548841267824 BETTER
I0325 11:33:32.510850 672942 finetune.py:68] layer 16_k @ epoch 3 new loss 0.002505664248019457 old loss 0.0025259058456867933 BETTER
I0325 11:33:38.392219 673843 finetune.py:68] layer 17_k @ epoch 1 new loss 0.002190484432503581 old loss 0.0022145677357912064 BETTER
I0325 11:33:46.687258 672050 finetune.py:45] layer 15_o initial loss 0.003996716812252998
I0325 11:34:04.561403 672942 finetune.py:68] layer 16_k @ epoch 4 new loss 0.002487077610567212 old loss 0.002505664248019457 BETTER
I0325 11:34:10.482497 673843 finetune.py:68] layer 17_k @ epoch 2 new loss 0.0021697634365409613 old loss 0.002190484432503581 BETTER
I0325 11:34:18.812376 672050 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0038257180713117123 old loss 0.003996716812252998 BETTER
I0325 11:34:23.528130 672942 finetune.py:45] layer 16_o initial loss 0.005616535898298025
I0325 11:34:42.573296 673843 finetune.py:68] layer 17_k @ epoch 3 new loss 0.0021516212727874517 old loss 0.0021697634365409613 BETTER
I0325 11:34:52.199505 672050 finetune.py:68] layer 15_o @ epoch 1 new loss 0.0037685551214963198 old loss 0.0038257180713117123 BETTER
I0325 11:34:54.068832 672942 finetune.py:68] layer 16_o @ epoch 0 new loss 0.005353990476578474 old loss 0.005616535898298025 BETTER
I0325 11:35:14.623401 673843 finetune.py:68] layer 17_k @ epoch 4 new loss 0.002134962473064661 old loss 0.0021516212727874517 BETTER
I0325 11:35:25.575493 672942 finetune.py:68] layer 16_o @ epoch 1 new loss 0.0052602230571210384 old loss 0.005353990476578474 BETTER
I0325 11:35:25.741324 672050 finetune.py:68] layer 15_o @ epoch 2 new loss 0.003731490345671773 old loss 0.0037685551214963198 BETTER
I0325 11:35:33.745249 673843 finetune.py:45] layer 17_o initial loss 0.004397426266223192
I0325 11:35:57.125977 672942 finetune.py:68] layer 16_o @ epoch 2 new loss 0.005199795123189688 old loss 0.0052602230571210384 BETTER
I0325 11:35:59.233290 672050 finetune.py:68] layer 15_o @ epoch 3 new loss 0.00370161235332489 old loss 0.003731490345671773 BETTER
I0325 11:36:04.566177 673843 finetune.py:68] layer 17_o @ epoch 0 new loss 0.0041947877034544945 old loss 0.004397426266223192 BETTER
I0325 11:36:28.875380 672942 finetune.py:68] layer 16_o @ epoch 3 new loss 0.005152544472366571 old loss 0.005199795123189688 BETTER
I0325 11:36:32.869003 672050 finetune.py:68] layer 15_o @ epoch 4 new loss 0.003676025429740548 old loss 0.00370161235332489 BETTER
I0325 11:36:36.376367 673843 finetune.py:68] layer 17_o @ epoch 1 new loss 0.004134634044021368 old loss 0.0041947877034544945 BETTER
I0325 11:37:00.046292 672050 finetune.py:45] layer 15_up initial loss 0.006375455763190985
I0325 11:37:00.545912 672942 finetune.py:68] layer 16_o @ epoch 4 new loss 0.0051127830520272255 old loss 0.005152544472366571 BETTER
I0325 11:37:08.466166 673843 finetune.py:68] layer 17_o @ epoch 2 new loss 0.004096613265573978 old loss 0.004134634044021368 BETTER
I0325 11:37:27.532575 672942 finetune.py:45] layer 16_up initial loss 0.008677100762724876
I0325 11:37:30.459523 672050 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0062752156518399715 old loss 0.006375455763190985 BETTER
I0325 11:37:40.435045 673843 finetune.py:68] layer 17_o @ epoch 3 new loss 0.0040656691417098045 old loss 0.004096613265573978 BETTER
I0325 11:37:56.366808 672942 finetune.py:68] layer 16_up @ epoch 0 new loss 0.008541461080312729 old loss 0.008677100762724876 BETTER
I0325 11:38:02.112469 672050 finetune.py:68] layer 15_up @ epoch 1 new loss 0.006215368863195181 old loss 0.0062752156518399715 BETTER
I0325 11:38:12.569148 673843 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0040390389040112495 old loss 0.0040656691417098045 BETTER
I0325 11:38:26.259763 672942 finetune.py:68] layer 16_up @ epoch 1 new loss 0.008459515869617462 old loss 0.008541461080312729 BETTER
I0325 11:38:34.225810 672050 finetune.py:68] layer 15_up @ epoch 2 new loss 0.006167497951537371 old loss 0.006215368863195181 BETTER
I0325 11:38:39.150365 673843 finetune.py:45] layer 17_up initial loss 0.008075674064457417
I0325 11:38:56.189165 672942 finetune.py:68] layer 16_up @ epoch 2 new loss 0.008394117467105389 old loss 0.008459515869617462 BETTER
I0325 11:39:06.162775 672050 finetune.py:68] layer 15_up @ epoch 3 new loss 0.006126429885625839 old loss 0.006167497951537371 BETTER
I0325 11:39:07.986616 673843 finetune.py:68] layer 17_up @ epoch 0 new loss 0.007954094558954239 old loss 0.008075674064457417 BETTER
I0325 11:39:26.100549 672942 finetune.py:68] layer 16_up @ epoch 3 new loss 0.00833834521472454 old loss 0.008394117467105389 BETTER
I0325 11:39:37.699994 673843 finetune.py:68] layer 17_up @ epoch 1 new loss 0.007878720760345459 old loss 0.007954094558954239 BETTER
I0325 11:39:38.091079 672050 finetune.py:68] layer 15_up @ epoch 4 new loss 0.006090810988098383 old loss 0.006126429885625839 BETTER
I0325 11:39:56.150693 672942 finetune.py:68] layer 16_up @ epoch 4 new loss 0.008289530873298645 old loss 0.00833834521472454 BETTER
I0325 11:40:04.068959 672050 finetune.py:45] layer 15_gate initial loss 0.008272738195955753
I0325 11:40:07.650280 673843 finetune.py:68] layer 17_up @ epoch 2 new loss 0.007819490507245064 old loss 0.007878720760345459 BETTER
I0325 11:40:21.826761 672942 finetune.py:45] layer 16_gate initial loss 0.011249969713389874
I0325 11:40:32.863698 672050 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.008223036304116249 old loss 0.008272738195955753 BETTER
I0325 11:40:37.600974 673843 finetune.py:68] layer 17_up @ epoch 3 new loss 0.007769284304231405 old loss 0.007819490507245064 BETTER
I0325 11:40:49.372520 672942 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.011190187186002731 old loss 0.011249969713389874 BETTER
I0325 11:41:02.577277 672050 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.008186761289834976 old loss 0.008223036304116249 BETTER
I0325 11:41:07.712548 673843 finetune.py:68] layer 17_up @ epoch 4 new loss 0.007725660223513842 old loss 0.007769284304231405 BETTER
I0325 11:41:17.564603 672942 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.011144275777041912 old loss 0.011190187186002731 BETTER
I0325 11:41:32.488983 672050 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.00815476942807436 old loss 0.008186761289834976 BETTER
I0325 11:41:33.616448 673843 finetune.py:45] layer 17_gate initial loss 0.011079790070652962
I0325 11:41:45.475539 672942 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.011102983728051186 old loss 0.011144275777041912 BETTER
I0325 11:42:01.153064 673843 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.011025596410036087 old loss 0.011079790070652962 BETTER
I0325 11:42:02.510069 672050 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.00812456849962473 old loss 0.00815476942807436 BETTER
I0325 11:42:13.547710 672942 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.011064272373914719 old loss 0.011102983728051186 BETTER
I0325 11:42:29.385943 673843 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.010981994681060314 old loss 0.011025596410036087 BETTER
I0325 11:42:32.656802 672050 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.008095984347164631 old loss 0.00812456849962473 BETTER
I0325 11:42:41.658986 672942 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.011027134954929352 old loss 0.011064272373914719 BETTER
I0325 11:42:57.538110 673843 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.010942058637738228 old loss 0.010981994681060314 BETTER
I0325 11:43:15.125760 672050 finetune.py:45] layer 15_down initial loss 0.011510518379509449
I0325 11:43:24.339883 672942 finetune.py:45] layer 16_down initial loss 0.015730924904346466
I0325 11:43:25.768978 673843 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.010904110036790371 old loss 0.010942058637738228 BETTER
I0325 11:43:41.721605 672050 finetune.py:68] layer 15_down @ epoch 0 new loss 0.011504516005516052 old loss 0.011510518379509449 BETTER
I0325 11:43:49.773012 672942 finetune.py:68] layer 16_down @ epoch 0 new loss 0.01572173647582531 old loss 0.015730924904346466 BETTER
I0325 11:43:54.179179 673843 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.010867689736187458 old loss 0.010904110036790371 BETTER
I0325 11:44:09.744098 672050 finetune.py:68] layer 15_down @ epoch 1 new loss 0.01149869617074728 old loss 0.011504516005516052 BETTER
I0325 11:44:16.231421 672942 finetune.py:68] layer 16_down @ epoch 1 new loss 0.015712734311819077 old loss 0.01572173647582531 BETTER
I0325 11:44:36.826862 673843 finetune.py:45] layer 17_down initial loss 0.01617751643061638
I0325 11:44:38.001833 672050 finetune.py:68] layer 15_down @ epoch 2 new loss 0.011493023484945297 old loss 0.01149869617074728 BETTER
I0325 11:44:43.041269 672942 finetune.py:68] layer 16_down @ epoch 2 new loss 0.015703899785876274 old loss 0.015712734311819077 BETTER
I0325 11:45:02.446530 673843 finetune.py:68] layer 17_down @ epoch 0 new loss 0.01616792008280754 old loss 0.01617751643061638 BETTER
I0325 11:45:06.398251 672050 finetune.py:68] layer 15_down @ epoch 3 new loss 0.011487537063658237 old loss 0.011493023484945297 BETTER
I0325 11:45:09.931998 672942 finetune.py:68] layer 16_down @ epoch 3 new loss 0.015695195645093918 old loss 0.015703899785876274 BETTER
I0325 11:45:29.089790 673843 finetune.py:68] layer 17_down @ epoch 1 new loss 0.01615854911506176 old loss 0.01616792008280754 BETTER
I0325 11:45:34.822832 672050 finetune.py:68] layer 15_down @ epoch 4 new loss 0.011482165195047855 old loss 0.011487537063658237 BETTER
15_v proxy err 0.15623804926872253 tr(WHW.T) 4043.675048828125
bpp_loss 1.2867433318315307
15_q proxy err 0.019027702510356903 tr(WHW.T) 38553.0625
bpp_loss 1.7570394312788267
15_k proxy err 0.01254308968782425 tr(WHW.T) 59019.3046875
bpp_loss 1.8108849120035302
15_o proxy err 0.18180117011070251 tr(WHW.T) 3780.4453125
bpp_loss 1.2706747624033596
15_up proxy err 0.10687306523323059 tr(WHW.T) 23081.455078125
bpp_loss 1.4675830882692407
15_gate proxy err 0.06157558783888817 tr(WHW.T) 40322.50390625
bpp_loss 1.5035294501847305
15_down proxy err 0.1518116146326065 tr(WHW.T) 16100.970703125
bpp_loss 1.4387191447299408
I0325 11:45:36.843133 672942 finetune.py:68] layer 16_down @ epoch 4 new loss 0.01568661816418171 old loss 0.015695195645093918 BETTER
16_v proxy err 0.1635093241930008 tr(WHW.T) 4032.874755859375
bpp_loss 1.3170112118241377
16_q proxy err 0.020169230177998543 tr(WHW.T) 37180.6953125
bpp_loss 1.739096200588392
16_k proxy err 0.012496980838477612 tr(WHW.T) 60311.41015625
bpp_loss 1.784726087236777
16_o proxy err 0.16561760008335114 tr(WHW.T) 4834.41357421875
bpp_loss 1.3243516263901256
16_up proxy err 0.10535647720098495 tr(WHW.T) 23523.478515625
bpp_loss 1.4705350123735708
16_gate proxy err 0.060844786465168 tr(WHW.T) 41182.76953125
bpp_loss 1.5154688477646125
16_down proxy err 0.15368542075157166 tr(WHW.T) 15937.7880859375
bpp_loss 1.4387984482116651
I0325 11:45:56.921929 673843 finetune.py:68] layer 17_down @ epoch 2 new loss 0.016149241477251053 old loss 0.01615854911506176 BETTER
I0325 11:46:23.592739 673843 finetune.py:68] layer 17_down @ epoch 3 new loss 0.016140131279826164 old loss 0.016149241477251053 BETTER
I0325 11:46:46.338867 583434 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 63.79599380493164s
I0325 11:46:49.910058 689628 config.py:54] PyTorch version 2.6.0 available.
W0325 11:46:50.233866 689628 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 11:46:50.485034 673843 finetune.py:68] layer 17_down @ epoch 4 new loss 0.016131220385432243 old loss 0.016140131279826164 BETTER
W0325 11:46:51.143838 689628 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 11:46:51.147619 583434 quantize_finetune_llama.py:209] layer 19 gpu 1
I0325 11:46:51.162051 689628 data_utils.py:336] using 256 training seqs, 128 validation seqs
17_v proxy err 0.16185486316680908 tr(WHW.T) 4307.6240234375
bpp_loss 1.316164872419904
17_q proxy err 0.021719660609960556 tr(WHW.T) 36583.99609375
bpp_loss 1.7365960844181245
17_k proxy err 0.01457284763455391 tr(WHW.T) 54742.953125
bpp_loss 1.7727747974277008
17_o proxy err 0.18660974502563477 tr(WHW.T) 4437.19580078125
bpp_loss 1.307484752847813
17_up proxy err 0.11862726509571075 tr(WHW.T) 21716.822265625
bpp_loss 1.4652241438540607
17_gate proxy err 0.06578531861305237 tr(WHW.T) 39805.609375
bpp_loss 1.5257424603887768
17_down proxy err 0.16047722101211548 tr(WHW.T) 16006.2197265625
bpp_loss 1.438532309400914
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 11:47:09.146688 689628 finetune.py:45] layer 18_v initial loss 0.00338357943110168
W0325 11:47:09.146880 689628 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 11:47:42.125948 689628 finetune.py:68] layer 18_v @ epoch 0 new loss 0.002123014535754919 old loss 0.00338357943110168 BETTER
I0325 11:47:56.391331 583434 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 61.0152382850647s
I0325 11:47:59.962843 690557 config.py:54] PyTorch version 2.6.0 available.
W0325 11:48:00.262196 690557 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 11:48:01.202957 690557 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 11:48:01.206683 583434 quantize_finetune_llama.py:209] layer 20 gpu 2
I0325 11:48:01.219548 690557 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 11:48:16.363473 689628 finetune.py:68] layer 18_v @ epoch 1 new loss 0.0019306435715407133 old loss 0.002123014535754919 BETTER
I0325 11:48:19.751097 690557 finetune.py:45] layer 19_v initial loss 0.0031948836985975504
W0325 11:48:19.751319 690557 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 11:48:51.079119 690557 finetune.py:68] layer 19_v @ epoch 0 new loss 0.002054985146969557 old loss 0.0031948836985975504 BETTER
I0325 11:48:51.271490 689628 finetune.py:68] layer 18_v @ epoch 2 new loss 0.0018626211676746607 old loss 0.0019306435715407133 BETTER
I0325 11:49:04.114947 583434 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 62.46627688407898s
I0325 11:49:07.690654 691460 config.py:54] PyTorch version 2.6.0 available.
W0325 11:49:07.992535 691460 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 11:49:08.939886 691460 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 11:49:08.943932 583434 quantize_finetune_llama.py:209] layer 21 gpu 0
I0325 11:49:08.960161 691460 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 11:49:23.549051 690557 finetune.py:68] layer 19_v @ epoch 1 new loss 0.0018743632826954126 old loss 0.002054985146969557 BETTER
I0325 11:49:26.289314 689628 finetune.py:68] layer 18_v @ epoch 3 new loss 0.0018212891882285476 old loss 0.0018626211676746607 BETTER
I0325 11:49:27.638556 691460 finetune.py:45] layer 20_v initial loss 0.00408690283074975
W0325 11:49:27.638770 691460 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 11:49:56.322623 690557 finetune.py:68] layer 19_v @ epoch 2 new loss 0.0018114930717274547 old loss 0.0018743632826954126 BETTER
I0325 11:49:59.258893 691460 finetune.py:68] layer 20_v @ epoch 0 new loss 0.0026527943555265665 old loss 0.00408690283074975 BETTER
I0325 11:50:01.342930 689628 finetune.py:68] layer 18_v @ epoch 4 new loss 0.0017909185262396932 old loss 0.0018212891882285476 BETTER
I0325 11:50:20.464262 689628 finetune.py:45] layer 18_q initial loss 0.0023482702672481537
I0325 11:50:29.473157 690557 finetune.py:68] layer 19_v @ epoch 3 new loss 0.001773839583620429 old loss 0.0018114930717274547 BETTER
I0325 11:50:31.857139 691460 finetune.py:68] layer 20_v @ epoch 1 new loss 0.002374290721490979 old loss 0.0026527943555265665 BETTER
I0325 11:50:53.765251 689628 finetune.py:68] layer 18_q @ epoch 0 new loss 0.0022199051454663277 old loss 0.0023482702672481537 BETTER
I0325 11:51:02.917063 690557 finetune.py:68] layer 19_v @ epoch 4 new loss 0.0017461427487432957 old loss 0.001773839583620429 BETTER
I0325 11:51:05.027238 691460 finetune.py:68] layer 20_v @ epoch 2 new loss 0.0022694740910083055 old loss 0.002374290721490979 BETTER
I0325 11:51:22.053286 690557 finetune.py:45] layer 19_q initial loss 0.0023299730382859707
I0325 11:51:28.227553 689628 finetune.py:68] layer 18_q @ epoch 1 new loss 0.0021653834264725447 old loss 0.0022199051454663277 BETTER
I0325 11:51:38.198855 691460 finetune.py:68] layer 20_v @ epoch 3 new loss 0.002207981888204813 old loss 0.0022694740910083055 BETTER
I0325 11:51:53.627855 690557 finetune.py:68] layer 19_q @ epoch 0 new loss 0.002182476921007037 old loss 0.0023299730382859707 BETTER
I0325 11:52:02.945977 689628 finetune.py:68] layer 18_q @ epoch 2 new loss 0.0021282394882291555 old loss 0.0021653834264725447 BETTER
I0325 11:52:11.588569 691460 finetune.py:68] layer 20_v @ epoch 4 new loss 0.0021654709707945585 old loss 0.002207981888204813 BETTER
I0325 11:52:25.998012 690557 finetune.py:68] layer 19_q @ epoch 1 new loss 0.0021191954147070646 old loss 0.002182476921007037 BETTER
I0325 11:52:30.837193 691460 finetune.py:45] layer 20_q initial loss 0.002801962662488222
I0325 11:52:37.863478 689628 finetune.py:68] layer 18_q @ epoch 3 new loss 0.0020996439270675182 old loss 0.0021282394882291555 BETTER
I0325 11:52:58.489010 690557 finetune.py:68] layer 19_q @ epoch 2 new loss 0.0020787538960576057 old loss 0.0021191954147070646 BETTER
I0325 11:53:02.433068 691460 finetune.py:68] layer 20_q @ epoch 0 new loss 0.002655379008501768 old loss 0.002801962662488222 BETTER
I0325 11:53:12.851502 689628 finetune.py:68] layer 18_q @ epoch 4 new loss 0.002075583441182971 old loss 0.0020996439270675182 BETTER
I0325 11:53:31.148410 690557 finetune.py:68] layer 19_q @ epoch 3 new loss 0.002049272181466222 old loss 0.0020787538960576057 BETTER
I0325 11:53:32.453917 689628 finetune.py:45] layer 18_k initial loss 0.0025226366706192493
I0325 11:53:35.103426 691460 finetune.py:68] layer 20_q @ epoch 1 new loss 0.0025838774163275957 old loss 0.002655379008501768 BETTER
I0325 11:54:04.204709 690557 finetune.py:68] layer 19_q @ epoch 4 new loss 0.0020257015712559223 old loss 0.002049272181466222 BETTER
I0325 11:54:05.518546 689628 finetune.py:68] layer 18_k @ epoch 0 new loss 0.0024861835408955812 old loss 0.0025226366706192493 BETTER
I0325 11:54:07.713934 691460 finetune.py:68] layer 20_q @ epoch 2 new loss 0.0025360817089676857 old loss 0.0025838774163275957 BETTER
I0325 11:54:23.401625 690557 finetune.py:45] layer 19_k initial loss 0.002520269714295864
I0325 11:54:39.674081 689628 finetune.py:68] layer 18_k @ epoch 1 new loss 0.0024635863956063986 old loss 0.0024861835408955812 BETTER
I0325 11:54:40.453921 691460 finetune.py:68] layer 20_q @ epoch 3 new loss 0.002500168513506651 old loss 0.0025360817089676857 BETTER
I0325 11:54:54.834152 690557 finetune.py:68] layer 19_k @ epoch 0 new loss 0.0024765480775386095 old loss 0.002520269714295864 BETTER
I0325 11:55:13.513275 691460 finetune.py:68] layer 20_q @ epoch 4 new loss 0.0024710779543966055 old loss 0.002500168513506651 BETTER
I0325 11:55:13.902675 689628 finetune.py:68] layer 18_k @ epoch 2 new loss 0.002444236073642969 old loss 0.0024635863956063986 BETTER
I0325 11:55:26.942885 690557 finetune.py:68] layer 19_k @ epoch 1 new loss 0.002452071523293853 old loss 0.0024765480775386095 BETTER
I0325 11:55:32.715039 691460 finetune.py:45] layer 20_k initial loss 0.0030259047634899616
I0325 11:55:48.172829 689628 finetune.py:68] layer 18_k @ epoch 3 new loss 0.0024264645762741566 old loss 0.002444236073642969 BETTER
I0325 11:55:59.016876 690557 finetune.py:68] layer 19_k @ epoch 2 new loss 0.002431126544252038 old loss 0.002452071523293853 BETTER
I0325 11:56:04.157141 691460 finetune.py:68] layer 20_k @ epoch 0 new loss 0.002971139969304204 old loss 0.0030259047634899616 BETTER
I0325 11:56:22.476896 689628 finetune.py:68] layer 18_k @ epoch 4 new loss 0.002410557819530368 old loss 0.0024264645762741566 BETTER
I0325 11:56:31.145052 690557 finetune.py:68] layer 19_k @ epoch 3 new loss 0.002412219997495413 old loss 0.002431126544252038 BETTER
I0325 11:56:36.306500 691460 finetune.py:68] layer 20_k @ epoch 1 new loss 0.0029435534961521626 old loss 0.002971139969304204 BETTER
I0325 11:56:41.745152 689628 finetune.py:45] layer 18_o initial loss 0.005044104065746069
I0325 11:57:03.336655 690557 finetune.py:68] layer 19_k @ epoch 4 new loss 0.0023951197508722544 old loss 0.002412219997495413 BETTER
I0325 11:57:08.519839 691460 finetune.py:68] layer 20_k @ epoch 2 new loss 0.0029196252580732107 old loss 0.0029435534961521626 BETTER
I0325 11:57:14.059086 689628 finetune.py:68] layer 18_o @ epoch 0 new loss 0.004784523509442806 old loss 0.005044104065746069 BETTER
I0325 11:57:22.326787 690557 finetune.py:45] layer 19_o initial loss 0.004910702351480722
I0325 11:57:40.870616 691460 finetune.py:68] layer 20_k @ epoch 3 new loss 0.0028978458140045404 old loss 0.0029196252580732107 BETTER
I0325 11:57:47.479089 689628 finetune.py:68] layer 18_o @ epoch 1 new loss 0.0047127255238592625 old loss 0.004784523509442806 BETTER
I0325 11:57:53.079741 690557 finetune.py:68] layer 19_o @ epoch 0 new loss 0.0046683684922754765 old loss 0.004910702351480722 BETTER
I0325 11:58:13.198303 691460 finetune.py:68] layer 20_k @ epoch 4 new loss 0.0028781683649867773 old loss 0.0028978458140045404 BETTER
I0325 11:58:20.958299 689628 finetune.py:68] layer 18_o @ epoch 2 new loss 0.004669329151511192 old loss 0.0047127255238592625 BETTER
I0325 11:58:24.571329 690557 finetune.py:68] layer 19_o @ epoch 1 new loss 0.004602191038429737 old loss 0.0046683684922754765 BETTER
I0325 11:58:33.686694 691460 finetune.py:45] layer 20_o initial loss 0.006011018063873053
I0325 11:58:54.670886 689628 finetune.py:68] layer 18_o @ epoch 3 new loss 0.004635026678442955 old loss 0.004669329151511192 BETTER
I0325 11:58:56.360831 690557 finetune.py:68] layer 19_o @ epoch 2 new loss 0.004562977701425552 old loss 0.004602191038429737 BETTER
I0325 11:59:04.470568 691460 finetune.py:68] layer 20_o @ epoch 0 new loss 0.005736278370022774 old loss 0.006011018063873053 BETTER
I0325 11:59:28.331555 690557 finetune.py:68] layer 19_o @ epoch 3 new loss 0.004532329272478819 old loss 0.004562977701425552 BETTER
I0325 11:59:28.407967 689628 finetune.py:68] layer 18_o @ epoch 4 new loss 0.004605889320373535 old loss 0.004635026678442955 BETTER
I0325 11:59:36.313432 691460 finetune.py:68] layer 20_o @ epoch 1 new loss 0.005645102821290493 old loss 0.005736278370022774 BETTER
I0325 11:59:55.605823 689628 finetune.py:45] layer 18_up initial loss 0.00947677530348301
I0325 12:00:00.062479 690557 finetune.py:68] layer 19_o @ epoch 4 new loss 0.004505953285843134 old loss 0.004532329272478819 BETTER
I0325 12:00:08.064544 691460 finetune.py:68] layer 20_o @ epoch 2 new loss 0.0055900076404213905 old loss 0.005645102821290493 BETTER
I0325 12:00:25.884146 690557 finetune.py:45] layer 19_up initial loss 0.009999185800552368
I0325 12:00:26.042884 689628 finetune.py:68] layer 18_up @ epoch 0 new loss 0.009334661066532135 old loss 0.00947677530348301 BETTER
I0325 12:00:39.935398 691460 finetune.py:68] layer 20_o @ epoch 3 new loss 0.005547680892050266 old loss 0.0055900076404213905 BETTER
I0325 12:00:54.690871 690557 finetune.py:68] layer 19_up @ epoch 0 new loss 0.009846718050539494 old loss 0.009999185800552368 BETTER
I0325 12:00:57.631668 689628 finetune.py:68] layer 18_up @ epoch 1 new loss 0.009243665263056755 old loss 0.009334661066532135 BETTER
I0325 12:01:11.745481 691460 finetune.py:68] layer 20_o @ epoch 4 new loss 0.005511377938091755 old loss 0.005547680892050266 BETTER
I0325 12:01:24.536660 690557 finetune.py:68] layer 19_up @ epoch 1 new loss 0.009750835597515106 old loss 0.009846718050539494 BETTER
I0325 12:01:29.397389 689628 finetune.py:68] layer 18_up @ epoch 2 new loss 0.009171838872134686 old loss 0.009243665263056755 BETTER
I0325 12:01:37.846701 691460 finetune.py:45] layer 20_up initial loss 0.012142024002969265
I0325 12:01:54.547086 690557 finetune.py:68] layer 19_up @ epoch 2 new loss 0.009675085544586182 old loss 0.009750835597515106 BETTER
I0325 12:02:01.151497 689628 finetune.py:68] layer 18_up @ epoch 3 new loss 0.009111681021749973 old loss 0.009171838872134686 BETTER
I0325 12:02:06.666697 691460 finetune.py:68] layer 20_up @ epoch 0 new loss 0.011975852772593498 old loss 0.012142024002969265 BETTER
I0325 12:02:24.338218 690557 finetune.py:68] layer 19_up @ epoch 3 new loss 0.009612774476408958 old loss 0.009675085544586182 BETTER
I0325 12:02:33.201074 689628 finetune.py:68] layer 18_up @ epoch 4 new loss 0.009059995412826538 old loss 0.009111681021749973 BETTER
I0325 12:02:36.679289 691460 finetune.py:68] layer 20_up @ epoch 1 new loss 0.011864600703120232 old loss 0.011975852772593498 BETTER
I0325 12:02:54.350174 690557 finetune.py:68] layer 19_up @ epoch 4 new loss 0.009559337049722672 old loss 0.009612774476408958 BETTER
I0325 12:02:59.327847 689628 finetune.py:45] layer 18_gate initial loss 0.013074683956801891
I0325 12:03:06.590449 691460 finetune.py:68] layer 20_up @ epoch 2 new loss 0.01177479512989521 old loss 0.011864600703120232 BETTER
I0325 12:03:20.155723 690557 finetune.py:45] layer 19_gate initial loss 0.014143513515591621
I0325 12:03:28.147447 689628 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.013014331459999084 old loss 0.013074683956801891 BETTER
I0325 12:03:36.462222 691460 finetune.py:68] layer 20_up @ epoch 3 new loss 0.011697638779878616 old loss 0.01177479512989521 BETTER
I0325 12:03:47.531381 690557 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.014077773317694664 old loss 0.014143513515591621 BETTER
I0325 12:03:57.986118 689628 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.01296504121273756 old loss 0.013014331459999084 BETTER
I0325 12:04:06.431787 691460 finetune.py:68] layer 20_up @ epoch 4 new loss 0.011631663888692856 old loss 0.011697638779878616 BETTER
I0325 12:04:15.724025 690557 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.01402523647993803 old loss 0.014077773317694664 BETTER
I0325 12:04:27.785418 689628 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.012919643893837929 old loss 0.01296504121273756 BETTER
I0325 12:04:32.317695 691460 finetune.py:45] layer 20_gate initial loss 0.017147812992334366
I0325 12:04:43.855249 690557 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.01397729106247425 old loss 0.01402523647993803 BETTER
I0325 12:04:57.803774 689628 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.012876264750957489 old loss 0.012919643893837929 BETTER
I0325 12:04:59.894024 691460 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.017078502103686333 old loss 0.017147812992334366 BETTER
I0325 12:05:11.986558 690557 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.013931144028902054 old loss 0.01397729106247425 BETTER
I0325 12:05:27.852664 689628 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.012834767811000347 old loss 0.012876264750957489 BETTER
I0325 12:05:28.027706 691460 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.017019283026456833 old loss 0.017078502103686333 BETTER
I0325 12:05:40.198083 690557 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.013887378387153149 old loss 0.013931144028902054 BETTER
I0325 12:05:56.018851 691460 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.016963360831141472 old loss 0.017019283026456833 BETTER
I0325 12:06:10.327615 689628 finetune.py:45] layer 18_down initial loss 0.01914822682738304
I0325 12:06:22.216911 690557 finetune.py:45] layer 19_down initial loss 0.020844869315624237
I0325 12:06:24.329735 691460 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.016910940408706665 old loss 0.016963360831141472 BETTER
I0325 12:06:36.804740 689628 finetune.py:68] layer 18_down @ epoch 0 new loss 0.01913721114397049 old loss 0.01914822682738304 BETTER
I0325 12:06:47.652215 690557 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0208326056599617 old loss 0.020844869315624237 BETTER
I0325 12:06:52.281531 691460 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.016859306022524834 old loss 0.016910940408706665 BETTER
I0325 12:07:04.775487 689628 finetune.py:68] layer 18_down @ epoch 1 new loss 0.019126256927847862 old loss 0.01913721114397049 BETTER
I0325 12:07:14.047047 690557 finetune.py:68] layer 19_down @ epoch 1 new loss 0.020820584148168564 old loss 0.0208326056599617 BETTER
I0325 12:07:33.070252 689628 finetune.py:68] layer 18_down @ epoch 2 new loss 0.019115522503852844 old loss 0.019126256927847862 BETTER
I0325 12:07:34.897747 691460 finetune.py:45] layer 20_down initial loss 0.025482963770627975
I0325 12:07:40.899857 690557 finetune.py:68] layer 19_down @ epoch 2 new loss 0.020808730274438858 old loss 0.020820584148168564 BETTER
I0325 12:08:00.679407 691460 finetune.py:68] layer 20_down @ epoch 0 new loss 0.025465117767453194 old loss 0.025482963770627975 BETTER
I0325 12:08:01.525780 689628 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0191049762070179 old loss 0.019115522503852844 BETTER
I0325 12:08:07.531467 690557 finetune.py:68] layer 19_down @ epoch 3 new loss 0.020796988159418106 old loss 0.020808730274438858 BETTER
I0325 12:08:27.290792 691460 finetune.py:68] layer 20_down @ epoch 1 new loss 0.025447670370340347 old loss 0.025465117767453194 BETTER
I0325 12:08:29.908494 689628 finetune.py:68] layer 18_down @ epoch 4 new loss 0.01909446343779564 old loss 0.0191049762070179 BETTER
18_v proxy err 0.1591174155473709 tr(WHW.T) 4721.12890625
bpp_loss 1.355432158583426
18_q proxy err 0.02358453907072544 tr(WHW.T) 35392.109375
bpp_loss 1.7084209849563194
18_k proxy err 0.016967378556728363 tr(WHW.T) 49409.54296875
bpp_loss 1.739382985237171
18_o proxy err 0.17036321759223938 tr(WHW.T) 5072.3994140625
bpp_loss 1.3455151368689258
18_up proxy err 0.12720416486263275 tr(WHW.T) 20346.875
bpp_loss 1.4604319100116574
18_gate proxy err 0.07113897055387497 tr(WHW.T) 37193.890625
bpp_loss 1.5339691820662729
18_down proxy err 0.15803095698356628 tr(WHW.T) 15953.6767578125
bpp_loss 1.4369468641965542
I0325 12:08:34.164772 690557 finetune.py:68] layer 19_down @ epoch 4 new loss 0.020785480737686157 old loss 0.020796988159418106 BETTER
19_v proxy err 0.15660937130451202 tr(WHW.T) 4837.78857421875
bpp_loss 1.3669575222593267
19_q proxy err 0.025132566690444946 tr(WHW.T) 33083.46484375
bpp_loss 1.6896944661712041
19_k proxy err 0.01658395491540432 tr(WHW.T) 50308.28515625
bpp_loss 1.7216171165928245
19_o proxy err 0.17756758630275726 tr(WHW.T) 5158.31689453125
bpp_loss 1.3506865000526886
19_up proxy err 0.1286834180355072 tr(WHW.T) 20305.724609375
bpp_loss 1.458913980961539
19_gate proxy err 0.07817722856998444 tr(WHW.T) 34235.31640625
bpp_loss 1.53602181710736
19_down proxy err 0.15660427510738373 tr(WHW.T) 16328.75390625
bpp_loss 1.4408081500401158
I0325 12:08:54.942888 691460 finetune.py:68] layer 20_down @ epoch 2 new loss 0.02543056756258011 old loss 0.025447670370340347 BETTER
I0325 12:09:21.795177 691460 finetune.py:68] layer 20_down @ epoch 3 new loss 0.02541353553533554 old loss 0.02543056756258011 BETTER
I0325 12:09:44.184037 583434 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 64.06914281845093s
I0325 12:09:47.744686 707244 config.py:54] PyTorch version 2.6.0 available.
W0325 12:09:48.046968 707244 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 12:09:48.440518 691460 finetune.py:68] layer 20_down @ epoch 4 new loss 0.025396980345249176 old loss 0.02541353553533554 BETTER
W0325 12:09:49.014549 707244 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 12:09:49.018593 583434 quantize_finetune_llama.py:209] layer 22 gpu 1
I0325 12:09:49.031421 707244 data_utils.py:336] using 256 training seqs, 128 validation seqs
20_v proxy err 0.16566185653209686 tr(WHW.T) 4696.2275390625
bpp_loss 1.3778756385436282
20_q proxy err 0.02493155561387539 tr(WHW.T) 33955.8671875
bpp_loss 1.69633091360447
20_k proxy err 0.017183873802423477 tr(WHW.T) 49447.44921875
bpp_loss 1.725756052415818
20_o proxy err 0.12733161449432373 tr(WHW.T) 6959.00830078125
bpp_loss 1.382295347750187
20_up proxy err 0.12580722570419312 tr(WHW.T) 20709.669921875
bpp_loss 1.4620396461944247
20_gate proxy err 0.07648175954818726 tr(WHW.T) 34998.36328125
bpp_loss 1.546820769866192
20_down proxy err 0.15324971079826355 tr(WHW.T) 16396.2421875
bpp_loss 1.4421590889132647
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 12:10:07.933902 707244 finetune.py:45] layer 21_v initial loss 0.0038143396377563477
W0325 12:10:07.934108 707244 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 12:10:40.906782 707244 finetune.py:68] layer 21_v @ epoch 0 new loss 0.0023577187675982714 old loss 0.0038143396377563477 BETTER
I0325 12:10:54.630757 583434 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 61.70450782775879s
I0325 12:10:58.199269 708164 config.py:54] PyTorch version 2.6.0 available.
W0325 12:10:58.529336 708164 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 12:10:59.469839 708164 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 12:10:59.474003 583434 quantize_finetune_llama.py:209] layer 23 gpu 2
I0325 12:10:59.490287 708164 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 12:11:15.358894 707244 finetune.py:68] layer 21_v @ epoch 1 new loss 0.00208768667653203 old loss 0.0023577187675982714 BETTER
I0325 12:11:18.557913 708164 finetune.py:45] layer 22_v initial loss 0.005059983115643263
W0325 12:11:18.558129 708164 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 12:11:49.936950 708164 finetune.py:68] layer 22_v @ epoch 0 new loss 0.0030827110167592764 old loss 0.005059983115643263 BETTER
I0325 12:11:50.252908 707244 finetune.py:68] layer 21_v @ epoch 2 new loss 0.002002723515033722 old loss 0.00208768667653203 BETTER
I0325 12:12:02.938894 583434 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 63.00500965118408s
I0325 12:12:06.625695 709069 config.py:54] PyTorch version 2.6.0 available.
W0325 12:12:06.953440 709069 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 12:12:07.945232 709069 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 12:12:07.949232 583434 quantize_finetune_llama.py:209] layer 24 gpu 0
I0325 12:12:07.963361 709069 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 12:12:22.372127 708164 finetune.py:68] layer 22_v @ epoch 1 new loss 0.0026850467547774315 old loss 0.0030827110167592764 BETTER
I0325 12:12:25.245454 707244 finetune.py:68] layer 21_v @ epoch 3 new loss 0.00195859232917428 old loss 0.002002723515033722 BETTER
I0325 12:12:26.584630 709069 finetune.py:45] layer 23_v initial loss 0.004635524936020374
W0325 12:12:26.585032 709069 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 12:12:55.103615 708164 finetune.py:68] layer 22_v @ epoch 2 new loss 0.002556019462645054 old loss 0.0026850467547774315 BETTER
I0325 12:12:58.404366 709069 finetune.py:68] layer 23_v @ epoch 0 new loss 0.0030073875095695257 old loss 0.004635524936020374 BETTER
I0325 12:13:00.695061 707244 finetune.py:68] layer 21_v @ epoch 4 new loss 0.0019287114264443517 old loss 0.00195859232917428 BETTER
I0325 12:13:19.771739 707244 finetune.py:45] layer 21_q initial loss 0.0025548008270561695
I0325 12:13:28.393128 708164 finetune.py:68] layer 22_v @ epoch 3 new loss 0.002487075049430132 old loss 0.002556019462645054 BETTER
I0325 12:13:30.956425 709069 finetune.py:68] layer 23_v @ epoch 1 new loss 0.002633302705362439 old loss 0.0030073875095695257 BETTER
I0325 12:13:53.146810 707244 finetune.py:68] layer 21_q @ epoch 0 new loss 0.0024064420722424984 old loss 0.0025548008270561695 BETTER
I0325 12:14:01.314237 708164 finetune.py:68] layer 22_v @ epoch 4 new loss 0.002440401818603277 old loss 0.002487075049430132 BETTER
I0325 12:14:03.986944 709069 finetune.py:68] layer 23_v @ epoch 2 new loss 0.0025157779455184937 old loss 0.002633302705362439 BETTER
I0325 12:14:20.333940 708164 finetune.py:45] layer 22_q initial loss 0.003376368898898363
I0325 12:14:27.711954 707244 finetune.py:68] layer 21_q @ epoch 1 new loss 0.002337625017389655 old loss 0.0024064420722424984 BETTER
I0325 12:14:37.277812 709069 finetune.py:68] layer 23_v @ epoch 3 new loss 0.0024597030133008957 old loss 0.0025157779455184937 BETTER
I0325 12:14:51.887469 708164 finetune.py:68] layer 22_q @ epoch 0 new loss 0.0031847611535340548 old loss 0.003376368898898363 BETTER
I0325 12:15:02.553334 707244 finetune.py:68] layer 21_q @ epoch 2 new loss 0.002295059384778142 old loss 0.002337625017389655 BETTER
I0325 12:15:10.608093 709069 finetune.py:68] layer 23_v @ epoch 4 new loss 0.002424026606604457 old loss 0.0024597030133008957 BETTER
I0325 12:15:24.017791 708164 finetune.py:68] layer 22_q @ epoch 1 new loss 0.0030910265631973743 old loss 0.0031847611535340548 BETTER
I0325 12:15:29.555143 709069 finetune.py:45] layer 23_q initial loss 0.003299129894003272
I0325 12:15:37.301725 707244 finetune.py:68] layer 21_q @ epoch 3 new loss 0.0022651583421975374 old loss 0.002295059384778142 BETTER
I0325 12:15:56.488258 708164 finetune.py:68] layer 22_q @ epoch 2 new loss 0.003028116887435317 old loss 0.0030910265631973743 BETTER
I0325 12:16:01.092246 709069 finetune.py:68] layer 23_q @ epoch 0 new loss 0.0030967618804425 old loss 0.003299129894003272 BETTER
I0325 12:16:12.051262 707244 finetune.py:68] layer 21_q @ epoch 4 new loss 0.002241814974695444 old loss 0.0022651583421975374 BETTER
I0325 12:16:29.158126 708164 finetune.py:68] layer 22_q @ epoch 3 new loss 0.0029810124542564154 old loss 0.003028116887435317 BETTER
I0325 12:16:31.361370 707244 finetune.py:45] layer 21_k initial loss 0.002943343250080943
I0325 12:16:33.542472 709069 finetune.py:68] layer 23_q @ epoch 1 new loss 0.002998895011842251 old loss 0.0030967618804425 BETTER
I0325 12:17:01.987038 708164 finetune.py:68] layer 22_q @ epoch 4 new loss 0.0029435022734105587 old loss 0.0029810124542564154 BETTER
I0325 12:17:04.062628 707244 finetune.py:68] layer 21_k @ epoch 0 new loss 0.002906109904870391 old loss 0.002943343250080943 BETTER
I0325 12:17:06.013242 709069 finetune.py:68] layer 23_q @ epoch 2 new loss 0.0029389711562544107 old loss 0.002998895011842251 BETTER
I0325 12:17:21.133822 708164 finetune.py:45] layer 22_k initial loss 0.003842146834358573
I0325 12:17:37.956178 707244 finetune.py:68] layer 21_k @ epoch 1 new loss 0.0028840822633355856 old loss 0.002906109904870391 BETTER
I0325 12:17:38.662317 709069 finetune.py:68] layer 23_q @ epoch 3 new loss 0.0028971650172024965 old loss 0.0029389711562544107 BETTER
I0325 12:17:52.431779 708164 finetune.py:68] layer 22_k @ epoch 0 new loss 0.0037881110329180956 old loss 0.003842146834358573 BETTER
I0325 12:18:11.740018 709069 finetune.py:68] layer 23_q @ epoch 4 new loss 0.002865360351279378 old loss 0.0028971650172024965 BETTER
I0325 12:18:12.394470 707244 finetune.py:68] layer 21_k @ epoch 2 new loss 0.002864750800654292 old loss 0.0028840822633355856 BETTER
I0325 12:18:24.502654 708164 finetune.py:68] layer 22_k @ epoch 1 new loss 0.0037565201055258512 old loss 0.0037881110329180956 BETTER
I0325 12:18:31.115590 709069 finetune.py:45] layer 23_k initial loss 0.003806750988587737
I0325 12:18:46.543846 707244 finetune.py:68] layer 21_k @ epoch 3 new loss 0.002846565330401063 old loss 0.002864750800654292 BETTER
I0325 12:18:56.551306 708164 finetune.py:68] layer 22_k @ epoch 2 new loss 0.0037290463224053383 old loss 0.0037565201055258512 BETTER
I0325 12:19:02.515794 709069 finetune.py:68] layer 23_k @ epoch 0 new loss 0.003762026783078909 old loss 0.003806750988587737 BETTER
I0325 12:19:20.572745 707244 finetune.py:68] layer 21_k @ epoch 4 new loss 0.0028299244586378336 old loss 0.002846565330401063 BETTER
I0325 12:19:28.602381 708164 finetune.py:68] layer 22_k @ epoch 3 new loss 0.0037043082993477583 old loss 0.0037290463224053383 BETTER
I0325 12:19:34.608388 709069 finetune.py:68] layer 23_k @ epoch 1 new loss 0.0037353888619691133 old loss 0.003762026783078909 BETTER
I0325 12:19:39.981472 707244 finetune.py:45] layer 21_o initial loss 0.005501090548932552
I0325 12:20:00.753795 708164 finetune.py:68] layer 22_k @ epoch 4 new loss 0.0036814603954553604 old loss 0.0037043082993477583 BETTER
I0325 12:20:06.796950 709069 finetune.py:68] layer 23_k @ epoch 2 new loss 0.003711124649271369 old loss 0.0037353888619691133 BETTER
I0325 12:20:12.231548 707244 finetune.py:68] layer 21_o @ epoch 0 new loss 0.005258281249552965 old loss 0.005501090548932552 BETTER
I0325 12:20:20.739081 708164 finetune.py:45] layer 22_o initial loss 0.006977218668907881
I0325 12:20:39.107357 709069 finetune.py:68] layer 23_k @ epoch 3 new loss 0.003689478151500225 old loss 0.003711124649271369 BETTER
I0325 12:20:45.961194 707244 finetune.py:68] layer 21_o @ epoch 1 new loss 0.005191581789404154 old loss 0.005258281249552965 BETTER
I0325 12:20:51.376412 708164 finetune.py:68] layer 22_o @ epoch 0 new loss 0.006698249839246273 old loss 0.006977218668907881 BETTER
I0325 12:21:11.477461 709069 finetune.py:68] layer 23_k @ epoch 4 new loss 0.003669006284326315 old loss 0.003689478151500225 BETTER
I0325 12:21:19.598569 707244 finetune.py:68] layer 21_o @ epoch 2 new loss 0.005155426450073719 old loss 0.005191581789404154 BETTER
I0325 12:21:22.947500 708164 finetune.py:68] layer 22_o @ epoch 1 new loss 0.006604116875678301 old loss 0.006698249839246273 BETTER
I0325 12:21:30.568277 709069 finetune.py:45] layer 23_o initial loss 0.0067491973750293255
I0325 12:21:53.286698 707244 finetune.py:68] layer 21_o @ epoch 3 new loss 0.005128529388457537 old loss 0.005155426450073719 BETTER
I0325 12:21:54.657718 708164 finetune.py:68] layer 22_o @ epoch 2 new loss 0.0065523916855454445 old loss 0.006604116875678301 BETTER
I0325 12:22:01.280761 709069 finetune.py:68] layer 23_o @ epoch 0 new loss 0.006495520938187838 old loss 0.0067491973750293255 BETTER
I0325 12:22:26.428360 708164 finetune.py:68] layer 22_o @ epoch 3 new loss 0.006512962747365236 old loss 0.0065523916855454445 BETTER
I0325 12:22:27.018154 707244 finetune.py:68] layer 21_o @ epoch 4 new loss 0.005105788353830576 old loss 0.005128529388457537 BETTER
I0325 12:22:32.831116 709069 finetune.py:68] layer 23_o @ epoch 1 new loss 0.006404599640518427 old loss 0.006495520938187838 BETTER
I0325 12:22:52.881388 707244 finetune.py:45] layer 21_up initial loss 0.01222274824976921
I0325 12:22:58.357285 708164 finetune.py:68] layer 22_o @ epoch 4 new loss 0.006480060052126646 old loss 0.006512962747365236 BETTER
I0325 12:23:04.462013 709069 finetune.py:68] layer 23_o @ epoch 2 new loss 0.006358190905302763 old loss 0.006404599640518427 BETTER
I0325 12:23:23.665479 707244 finetune.py:68] layer 21_up @ epoch 0 new loss 0.012079546228051186 old loss 0.01222274824976921 BETTER
I0325 12:23:24.642076 708164 finetune.py:45] layer 22_up initial loss 0.014583590440452099
I0325 12:23:36.251540 709069 finetune.py:68] layer 23_o @ epoch 3 new loss 0.006323588080704212 old loss 0.006358190905302763 BETTER
I0325 12:23:53.517118 708164 finetune.py:68] layer 22_up @ epoch 0 new loss 0.014439727179706097 old loss 0.014583590440452099 BETTER
I0325 12:23:55.287712 707244 finetune.py:68] layer 21_up @ epoch 1 new loss 0.01198561117053032 old loss 0.012079546228051186 BETTER
I0325 12:24:07.862977 709069 finetune.py:68] layer 23_o @ epoch 4 new loss 0.0062959641218185425 old loss 0.006323588080704212 BETTER
I0325 12:24:23.355796 708164 finetune.py:68] layer 22_up @ epoch 1 new loss 0.014338082633912563 old loss 0.014439727179706097 BETTER
I0325 12:24:27.004349 707244 finetune.py:68] layer 21_up @ epoch 2 new loss 0.011909167282283306 old loss 0.01198561117053032 BETTER
I0325 12:24:33.826632 709069 finetune.py:45] layer 23_up initial loss 0.015231196768581867
I0325 12:24:53.302350 708164 finetune.py:68] layer 22_up @ epoch 2 new loss 0.01425558514893055 old loss 0.014338082633912563 BETTER
I0325 12:24:58.808085 707244 finetune.py:68] layer 21_up @ epoch 3 new loss 0.011845185421407223 old loss 0.011909167282283306 BETTER
I0325 12:25:02.603230 709069 finetune.py:68] layer 23_up @ epoch 0 new loss 0.015097012743353844 old loss 0.015231196768581867 BETTER
I0325 12:25:23.292913 708164 finetune.py:68] layer 22_up @ epoch 3 new loss 0.014185589738190174 old loss 0.01425558514893055 BETTER
I0325 12:25:30.734685 707244 finetune.py:68] layer 21_up @ epoch 4 new loss 0.011789217591285706 old loss 0.011845185421407223 BETTER
I0325 12:25:32.488614 709069 finetune.py:68] layer 23_up @ epoch 1 new loss 0.014998114667832851 old loss 0.015097012743353844 BETTER
I0325 12:25:53.355932 708164 finetune.py:68] layer 22_up @ epoch 4 new loss 0.014125101268291473 old loss 0.014185589738190174 BETTER
I0325 12:25:59.083110 707244 finetune.py:45] layer 21_gate initial loss 0.017560895532369614
I0325 12:26:02.464463 709069 finetune.py:68] layer 23_up @ epoch 2 new loss 0.01492062583565712 old loss 0.014998114667832851 BETTER
I0325 12:26:21.156540 708164 finetune.py:45] layer 22_gate initial loss 0.02059704251587391
I0325 12:26:28.408263 707244 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.017496896907687187 old loss 0.017560895532369614 BETTER
I0325 12:26:32.683269 709069 finetune.py:68] layer 23_up @ epoch 3 new loss 0.014853066764771938 old loss 0.01492062583565712 BETTER
I0325 12:26:48.643982 708164 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.020528310909867287 old loss 0.02059704251587391 BETTER
I0325 12:26:58.369366 707244 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.017445236444473267 old loss 0.017496896907687187 BETTER
I0325 12:27:02.982228 709069 finetune.py:68] layer 23_up @ epoch 4 new loss 0.01479483861476183 old loss 0.014853066764771938 BETTER
I0325 12:27:17.102575 708164 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.020471179857850075 old loss 0.020528310909867287 BETTER
I0325 12:27:28.523379 707244 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.017396392300724983 old loss 0.017445236444473267 BETTER
I0325 12:27:30.063172 709069 finetune.py:45] layer 23_gate initial loss 0.021934164687991142
I0325 12:27:45.546463 708164 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.020417504012584686 old loss 0.020471179857850075 BETTER
I0325 12:27:58.456363 709069 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.021871250122785568 old loss 0.021934164687991142 BETTER
I0325 12:27:59.520359 707244 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.017350617796182632 old loss 0.017396392300724983 BETTER
I0325 12:28:13.719850 708164 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.02036573551595211 old loss 0.020417504012584686 BETTER
I0325 12:28:26.781806 709069 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.021815938875079155 old loss 0.021871250122785568 BETTER
I0325 12:28:29.859943 707244 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.017306234687566757 old loss 0.017350617796182632 BETTER
I0325 12:28:41.690568 708164 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.020317627117037773 old loss 0.02036573551595211 BETTER
I0325 12:28:55.431554 709069 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.021764330565929413 old loss 0.021815938875079155 BETTER
I0325 12:29:12.950098 707244 finetune.py:45] layer 21_down initial loss 0.026210352778434753
I0325 12:29:24.531926 709069 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.02171442285180092 old loss 0.021764330565929413 BETTER
I0325 12:29:25.940382 708164 finetune.py:45] layer 22_down initial loss 0.03052515536546707
I0325 12:29:39.596581 707244 finetune.py:68] layer 21_down @ epoch 0 new loss 0.026194775477051735 old loss 0.026210352778434753 BETTER
I0325 12:29:51.405362 708164 finetune.py:68] layer 22_down @ epoch 0 new loss 0.030507925897836685 old loss 0.03052515536546707 BETTER
I0325 12:29:52.788265 709069 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.021667301654815674 old loss 0.02171442285180092 BETTER
I0325 12:30:07.730240 707244 finetune.py:68] layer 21_down @ epoch 1 new loss 0.026179568842053413 old loss 0.026194775477051735 BETTER
I0325 12:30:18.399537 708164 finetune.py:68] layer 22_down @ epoch 1 new loss 0.030491160228848457 old loss 0.030507925897836685 BETTER
I0325 12:30:37.447033 707244 finetune.py:68] layer 21_down @ epoch 2 new loss 0.026164444163441658 old loss 0.026179568842053413 BETTER
I0325 12:30:37.612457 709069 finetune.py:45] layer 23_down initial loss 0.032096680253744125
I0325 12:30:45.941046 708164 finetune.py:68] layer 22_down @ epoch 2 new loss 0.03047478385269642 old loss 0.030491160228848457 BETTER
I0325 12:31:03.737060 709069 finetune.py:68] layer 23_down @ epoch 0 new loss 0.03208022937178612 old loss 0.032096680253744125 BETTER
I0325 12:31:06.257895 707244 finetune.py:68] layer 21_down @ epoch 3 new loss 0.026149695739150047 old loss 0.026164444163441658 BETTER
I0325 12:31:13.579368 708164 finetune.py:68] layer 22_down @ epoch 3 new loss 0.03045840375125408 old loss 0.03047478385269642 BETTER
I0325 12:31:31.371913 709069 finetune.py:68] layer 23_down @ epoch 1 new loss 0.03206409513950348 old loss 0.03208022937178612 BETTER
I0325 12:31:35.295617 707244 finetune.py:68] layer 21_down @ epoch 4 new loss 0.02613494172692299 old loss 0.026149695739150047 BETTER
21_v proxy err 0.1632172018289566 tr(WHW.T) 4916.306640625
bpp_loss 1.4077349203143967
21_q proxy err 0.0280703566968441 tr(WHW.T) 30436.06640625
bpp_loss 1.6634709563077195
21_k proxy err 0.019858235493302345 tr(WHW.T) 43034.828125
bpp_loss 1.681508320834837
21_o proxy err 0.1533486545085907 tr(WHW.T) 6553.35888671875
bpp_loss 1.4018306448997464
21_up proxy err 0.13445688784122467 tr(WHW.T) 19702.25
bpp_loss 1.458363667801889
21_gate proxy err 0.08299178630113602 tr(WHW.T) 32927.9453125
bpp_loss 1.552104562196101
21_down proxy err 0.15998782217502594 tr(WHW.T) 16346.6337890625
bpp_loss 1.440294301884552
I0325 12:31:41.841627 708164 finetune.py:68] layer 22_down @ epoch 4 new loss 0.030442405492067337 old loss 0.03045840375125408 BETTER
22_v proxy err 0.15648001432418823 tr(WHW.T) 5165.8642578125
bpp_loss 1.4123567805800121
22_q proxy err 0.026925407350063324 tr(WHW.T) 32250.62890625
bpp_loss 1.694100808352232
22_k proxy err 0.019666383042931557 tr(WHW.T) 44237.08984375
bpp_loss 1.715163615430356
22_o proxy err 0.1234668418765068 tr(WHW.T) 7805.771484375
bpp_loss 1.3946490933303721
22_up proxy err 0.13639283180236816 tr(WHW.T) 19592.583984375
bpp_loss 1.4581160610325115
22_gate proxy err 0.08510798215866089 tr(WHW.T) 32478.390625
bpp_loss 1.559240332891255
22_down proxy err 0.16135664284229279 tr(WHW.T) 16375.1826171875
bpp_loss 1.441332798570308
I0325 12:31:59.453989 709069 finetune.py:68] layer 23_down @ epoch 2 new loss 0.03204817324876785 old loss 0.03206409513950348 BETTER
I0325 12:32:26.366955 709069 finetune.py:68] layer 23_down @ epoch 3 new loss 0.032032422721385956 old loss 0.03204817324876785 BETTER
I0325 12:32:51.537747 583434 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 64.86197257041931s
I0325 12:32:53.239774 709069 finetune.py:68] layer 23_down @ epoch 4 new loss 0.03201704099774361 old loss 0.032032422721385956 BETTER
23_v proxy err 0.15119007229804993 tr(WHW.T) 5725.64111328125
bpp_loss 1.4616328552656341
23_q proxy err 0.032191697508096695 tr(WHW.T) 28372.279296875
bpp_loss 1.68496761455026
23_k proxy err 0.023666689172387123 tr(WHW.T) 38626.015625
bpp_loss 1.6995508780819364
23_o proxy err 0.1565435528755188 tr(WHW.T) 6515.408203125
bpp_loss 1.4384866386826616
23_up proxy err 0.14343060553073883 tr(WHW.T) 18925.7734375
bpp_loss 1.4608471126750457
23_gate proxy err 0.09239992499351501 tr(WHW.T) 30332.68359375
bpp_loss 1.5562681936507308
23_down proxy err 0.16455420851707458 tr(WHW.T) 16324.8232421875
bpp_loss 1.44549348756541
I0325 12:32:55.085452 725045 config.py:54] PyTorch version 2.6.0 available.
W0325 12:32:55.376089 725045 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 12:32:56.279565 725045 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 12:32:56.283726 583434 quantize_finetune_llama.py:209] layer 25 gpu 1
I0325 12:32:56.296058 725045 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 12:33:14.664834 725045 finetune.py:45] layer 24_v initial loss 0.004865035880357027
W0325 12:33:14.665150 725045 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 12:33:48.119935 725045 finetune.py:68] layer 24_v @ epoch 0 new loss 0.0033628661185503006 old loss 0.004865035880357027 BETTER
I0325 12:33:59.295552 583434 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 61.67280578613281s
I0325 12:34:02.888352 725942 config.py:54] PyTorch version 2.6.0 available.
W0325 12:34:03.185459 725942 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 12:34:04.120882 725942 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 12:34:04.125161 583434 quantize_finetune_llama.py:209] layer 26 gpu 2
I0325 12:34:04.138572 725942 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 12:34:22.863999 725045 finetune.py:68] layer 24_v @ epoch 1 new loss 0.0030057416297495365 old loss 0.0033628661185503006 BETTER
I0325 12:34:23.010208 725942 finetune.py:45] layer 25_v initial loss 0.005453056190162897
W0325 12:34:23.010533 725942 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 12:34:54.468652 725942 finetune.py:68] layer 25_v @ epoch 0 new loss 0.003298442345112562 old loss 0.005453056190162897 BETTER
I0325 12:34:57.894955 725045 finetune.py:68] layer 24_v @ epoch 2 new loss 0.0028850107919424772 old loss 0.0030057416297495365 BETTER
I0325 12:35:06.937880 583434 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 62.347641706466675s
I0325 12:35:10.588972 726833 config.py:54] PyTorch version 2.6.0 available.
W0325 12:35:10.925788 726833 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 12:35:11.951255 726833 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 12:35:11.955231 583434 quantize_finetune_llama.py:209] layer 27 gpu 0
I0325 12:35:11.971282 726833 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 12:35:27.241462 725942 finetune.py:68] layer 25_v @ epoch 1 new loss 0.0028032618574798107 old loss 0.003298442345112562 BETTER
I0325 12:35:30.605939 726833 finetune.py:45] layer 26_v initial loss 0.006886993534862995
W0325 12:35:30.606119 726833 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 12:35:33.057532 725045 finetune.py:68] layer 24_v @ epoch 3 new loss 0.0028223833069205284 old loss 0.0028850107919424772 BETTER
I0325 12:35:59.851794 725942 finetune.py:68] layer 25_v @ epoch 2 new loss 0.0026592884678393602 old loss 0.0028032618574798107 BETTER
I0325 12:36:02.156557 726833 finetune.py:68] layer 26_v @ epoch 0 new loss 0.004622883629053831 old loss 0.006886993534862995 BETTER
I0325 12:36:08.360891 725045 finetune.py:68] layer 24_v @ epoch 4 new loss 0.0027815205976366997 old loss 0.0028223833069205284 BETTER
I0325 12:36:27.467813 725045 finetune.py:45] layer 24_q initial loss 0.0037127938121557236
I0325 12:36:32.883705 725942 finetune.py:68] layer 25_v @ epoch 3 new loss 0.0025969543494284153 old loss 0.0026592884678393602 BETTER
I0325 12:36:34.784867 726833 finetune.py:68] layer 26_v @ epoch 1 new loss 0.0041787512600421906 old loss 0.004622883629053831 BETTER
I0325 12:37:00.670101 725045 finetune.py:68] layer 24_q @ epoch 0 new loss 0.0035412192810326815 old loss 0.0037127938121557236 BETTER
I0325 12:37:06.032355 725942 finetune.py:68] layer 25_v @ epoch 4 new loss 0.002559764077886939 old loss 0.0025969543494284153 BETTER
I0325 12:37:07.957471 726833 finetune.py:68] layer 26_v @ epoch 2 new loss 0.0040377238765358925 old loss 0.0041787512600421906 BETTER
I0325 12:37:25.063173 725942 finetune.py:45] layer 25_q initial loss 0.0035798039752990007
I0325 12:37:35.238286 725045 finetune.py:68] layer 24_q @ epoch 1 new loss 0.00345282512716949 old loss 0.0035412192810326815 BETTER
I0325 12:37:40.969002 726833 finetune.py:68] layer 26_v @ epoch 3 new loss 0.003964561969041824 old loss 0.0040377238765358925 BETTER
I0325 12:37:56.555672 725942 finetune.py:68] layer 25_q @ epoch 0 new loss 0.0033625345677137375 old loss 0.0035798039752990007 BETTER
I0325 12:38:10.086574 725045 finetune.py:68] layer 24_q @ epoch 2 new loss 0.0033952672965824604 old loss 0.00345282512716949 BETTER
I0325 12:38:14.318209 726833 finetune.py:68] layer 26_v @ epoch 4 new loss 0.003914889879524708 old loss 0.003964561969041824 BETTER
I0325 12:38:28.826303 725942 finetune.py:68] layer 25_q @ epoch 1 new loss 0.0032548289746046066 old loss 0.0033625345677137375 BETTER
I0325 12:38:33.507997 726833 finetune.py:45] layer 26_q initial loss 0.005216917023062706
I0325 12:38:45.021361 725045 finetune.py:68] layer 24_q @ epoch 3 new loss 0.003353268140926957 old loss 0.0033952672965824604 BETTER
I0325 12:39:01.337218 725942 finetune.py:68] layer 25_q @ epoch 2 new loss 0.003189019626006484 old loss 0.0032548289746046066 BETTER
I0325 12:39:05.080593 726833 finetune.py:68] layer 26_q @ epoch 0 new loss 0.004980658181011677 old loss 0.005216917023062706 BETTER
I0325 12:39:20.010707 725045 finetune.py:68] layer 24_q @ epoch 4 new loss 0.0033209112007170916 old loss 0.003353268140926957 BETTER
I0325 12:39:33.767005 725942 finetune.py:68] layer 25_q @ epoch 3 new loss 0.003143001114949584 old loss 0.003189019626006484 BETTER
I0325 12:39:37.642536 726833 finetune.py:68] layer 26_q @ epoch 1 new loss 0.00486788060516119 old loss 0.004980658181011677 BETTER
I0325 12:39:39.288490 725045 finetune.py:45] layer 24_k initial loss 0.004402097314596176
I0325 12:40:06.018611 725942 finetune.py:68] layer 25_q @ epoch 4 new loss 0.00310828210785985 old loss 0.003143001114949584 BETTER
I0325 12:40:10.702853 726833 finetune.py:68] layer 26_q @ epoch 2 new loss 0.004797045141458511 old loss 0.00486788060516119 BETTER
I0325 12:40:12.229242 725045 finetune.py:68] layer 24_k @ epoch 0 new loss 0.004355460871011019 old loss 0.004402097314596176 BETTER
I0325 12:40:25.108454 725942 finetune.py:45] layer 25_k initial loss 0.00439028674736619
I0325 12:40:43.614331 726833 finetune.py:68] layer 26_q @ epoch 3 new loss 0.004743378609418869 old loss 0.004797045141458511 BETTER
I0325 12:40:46.079401 725045 finetune.py:68] layer 24_k @ epoch 1 new loss 0.004327368922531605 old loss 0.004355460871011019 BETTER
I0325 12:40:56.518373 725942 finetune.py:68] layer 25_k @ epoch 0 new loss 0.004337727557867765 old loss 0.00439028674736619 BETTER
I0325 12:41:16.663207 726833 finetune.py:68] layer 26_q @ epoch 4 new loss 0.004701494239270687 old loss 0.004743378609418869 BETTER
I0325 12:41:20.271542 725045 finetune.py:68] layer 24_k @ epoch 2 new loss 0.004302761983126402 old loss 0.004327368922531605 BETTER
I0325 12:41:28.652509 725942 finetune.py:68] layer 25_k @ epoch 1 new loss 0.004306431859731674 old loss 0.004337727557867765 BETTER
I0325 12:41:36.012440 726833 finetune.py:45] layer 26_k initial loss 0.006078402046114206
I0325 12:41:54.271579 725045 finetune.py:68] layer 24_k @ epoch 3 new loss 0.004280488938093185 old loss 0.004302761983126402 BETTER
I0325 12:42:00.721407 725942 finetune.py:68] layer 25_k @ epoch 2 new loss 0.004279153421521187 old loss 0.004306431859731674 BETTER
I0325 12:42:07.445636 726833 finetune.py:68] layer 26_k @ epoch 0 new loss 0.006005595903843641 old loss 0.006078402046114206 BETTER
I0325 12:42:28.327276 725045 finetune.py:68] layer 24_k @ epoch 4 new loss 0.0042593711987137794 old loss 0.004280488938093185 BETTER
I0325 12:42:32.851185 725942 finetune.py:68] layer 25_k @ epoch 3 new loss 0.004254275932908058 old loss 0.004279153421521187 BETTER
I0325 12:42:39.616161 726833 finetune.py:68] layer 26_k @ epoch 1 new loss 0.0059701865538954735 old loss 0.006005595903843641 BETTER
I0325 12:42:47.768298 725045 finetune.py:45] layer 24_o initial loss 0.007882753387093544
I0325 12:43:04.881110 725942 finetune.py:68] layer 25_k @ epoch 4 new loss 0.00423143757507205 old loss 0.004254275932908058 BETTER
I0325 12:43:11.766571 726833 finetune.py:68] layer 26_k @ epoch 2 new loss 0.005939404480159283 old loss 0.0059701865538954735 BETTER
I0325 12:43:19.928963 725045 finetune.py:68] layer 24_o @ epoch 0 new loss 0.007646530866622925 old loss 0.007882753387093544 BETTER
I0325 12:43:24.470111 725942 finetune.py:45] layer 25_o initial loss 0.007384346332401037
I0325 12:43:43.877080 726833 finetune.py:68] layer 26_k @ epoch 3 new loss 0.005911966785788536 old loss 0.005939404480159283 BETTER
I0325 12:43:53.445513 725045 finetune.py:68] layer 24_o @ epoch 1 new loss 0.007554145995527506 old loss 0.007646530866622925 BETTER
I0325 12:43:54.923518 725942 finetune.py:68] layer 25_o @ epoch 0 new loss 0.007033437956124544 old loss 0.007384346332401037 BETTER
I0325 12:44:16.067384 726833 finetune.py:68] layer 26_k @ epoch 4 new loss 0.00588606670498848 old loss 0.005911966785788536 BETTER
I0325 12:44:26.235759 725942 finetune.py:68] layer 25_o @ epoch 1 new loss 0.006919092033058405 old loss 0.007033437956124544 BETTER
I0325 12:44:27.102105 725045 finetune.py:68] layer 24_o @ epoch 2 new loss 0.00750486645847559 old loss 0.007554145995527506 BETTER
I0325 12:44:35.435341 726833 finetune.py:45] layer 26_o initial loss 0.010380107909440994
I0325 12:44:58.085437 725942 finetune.py:68] layer 25_o @ epoch 2 new loss 0.006866977084428072 old loss 0.006919092033058405 BETTER
I0325 12:45:00.428222 725045 finetune.py:68] layer 24_o @ epoch 3 new loss 0.007470116950571537 old loss 0.00750486645847559 BETTER
I0325 12:45:06.170729 726833 finetune.py:68] layer 26_o @ epoch 0 new loss 0.009980515576899052 old loss 0.010380107909440994 BETTER
I0325 12:45:29.824927 725942 finetune.py:68] layer 25_o @ epoch 3 new loss 0.0068323989398777485 old loss 0.006866977084428072 BETTER
I0325 12:45:33.863563 725045 finetune.py:68] layer 24_o @ epoch 4 new loss 0.007442066911607981 old loss 0.007470116950571537 BETTER
I0325 12:45:37.820809 726833 finetune.py:68] layer 26_o @ epoch 1 new loss 0.009871179237961769 old loss 0.009980515576899052 BETTER
I0325 12:46:01.481177 725045 finetune.py:45] layer 24_up initial loss 0.017213115468621254
I0325 12:46:02.155384 725942 finetune.py:68] layer 25_o @ epoch 4 new loss 0.006804779637604952 old loss 0.0068323989398777485 BETTER
I0325 12:46:09.671169 726833 finetune.py:68] layer 26_o @ epoch 2 new loss 0.009815134108066559 old loss 0.009871179237961769 BETTER
I0325 12:46:29.877768 725942 finetune.py:45] layer 25_up initial loss 0.017757615074515343
I0325 12:46:32.515699 725045 finetune.py:68] layer 24_up @ epoch 0 new loss 0.017080148681998253 old loss 0.017213115468621254 BETTER
I0325 12:46:41.778256 726833 finetune.py:68] layer 26_o @ epoch 3 new loss 0.009772882796823978 old loss 0.009815134108066559 BETTER
I0325 12:46:58.686619 725942 finetune.py:68] layer 25_up @ epoch 0 new loss 0.017607497051358223 old loss 0.017757615074515343 BETTER
I0325 12:47:04.050341 725045 finetune.py:68] layer 24_up @ epoch 1 new loss 0.016985619440674782 old loss 0.017080148681998253 BETTER
I0325 12:47:14.176148 726833 finetune.py:68] layer 26_o @ epoch 4 new loss 0.009736703708767891 old loss 0.009772882796823978 BETTER
I0325 12:47:28.739241 725942 finetune.py:68] layer 25_up @ epoch 1 new loss 0.017508430406451225 old loss 0.017607497051358223 BETTER
I0325 12:47:36.124394 725045 finetune.py:68] layer 24_up @ epoch 2 new loss 0.016909081488847733 old loss 0.016985619440674782 BETTER
I0325 12:47:42.226178 726833 finetune.py:45] layer 26_up initial loss 0.021774237975478172
I0325 12:47:59.949353 725942 finetune.py:68] layer 25_up @ epoch 2 new loss 0.017427558079361916 old loss 0.017508430406451225 BETTER
I0325 12:48:09.205330 725045 finetune.py:68] layer 24_up @ epoch 3 new loss 0.01684417389333248 old loss 0.016909081488847733 BETTER
I0325 12:48:12.031049 726833 finetune.py:68] layer 26_up @ epoch 0 new loss 0.021600250154733658 old loss 0.021774237975478172 BETTER
I0325 12:48:30.234301 725942 finetune.py:68] layer 25_up @ epoch 3 new loss 0.017358819022774696 old loss 0.017427558079361916 BETTER
I0325 12:48:42.187563 725045 finetune.py:68] layer 24_up @ epoch 4 new loss 0.016787145286798477 old loss 0.01684417389333248 BETTER
I0325 12:48:43.033695 726833 finetune.py:68] layer 26_up @ epoch 1 new loss 0.02149064466357231 old loss 0.021600250154733658 BETTER
I0325 12:49:00.575945 725942 finetune.py:68] layer 25_up @ epoch 4 new loss 0.017297035083174706 old loss 0.017358819022774696 BETTER
I0325 12:49:10.211866 725045 finetune.py:45] layer 24_gate initial loss 0.024643750861287117
I0325 12:49:14.040811 726833 finetune.py:68] layer 26_up @ epoch 2 new loss 0.02140156552195549 old loss 0.02149064466357231 BETTER
I0325 12:49:29.799092 725942 finetune.py:45] layer 25_gate initial loss 0.02591068111360073
I0325 12:49:40.433427 725045 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.024581730365753174 old loss 0.024643750861287117 BETTER
I0325 12:49:44.638992 726833 finetune.py:68] layer 26_up @ epoch 3 new loss 0.021325647830963135 old loss 0.02140156552195549 BETTER
I0325 12:49:58.121542 725942 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.025842467322945595 old loss 0.02591068111360073 BETTER
I0325 12:50:10.639439 725045 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.024524981155991554 old loss 0.024581730365753174 BETTER
I0325 12:50:15.575334 726833 finetune.py:68] layer 26_up @ epoch 4 new loss 0.021257834509015083 old loss 0.021325647830963135 BETTER
I0325 12:50:26.762321 725942 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.025784745812416077 old loss 0.025842467322945595 BETTER
I0325 12:50:41.708251 725045 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.024473028257489204 old loss 0.024524981155991554 BETTER
I0325 12:50:44.718350 726833 finetune.py:45] layer 26_gate initial loss 0.030858667567372322
I0325 12:50:55.523778 725942 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.025731366127729416 old loss 0.025784745812416077 BETTER
I0325 12:51:12.594379 725045 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.024423934519290924 old loss 0.024473028257489204 BETTER
I0325 12:51:13.224125 726833 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.03077654354274273 old loss 0.030858667567372322 BETTER
I0325 12:51:24.326096 725942 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.025679953396320343 old loss 0.025731366127729416 BETTER
I0325 12:51:42.286842 726833 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.03071313351392746 old loss 0.03077654354274273 BETTER
I0325 12:51:43.271143 725045 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.02437581680715084 old loss 0.024423934519290924 BETTER
I0325 12:51:53.512290 725942 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.025630861520767212 old loss 0.025679953396320343 BETTER
I0325 12:52:11.346138 726833 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.03065437264740467 old loss 0.03071313351392746 BETTER
I0325 12:52:28.850529 725045 finetune.py:45] layer 24_down initial loss 0.035373106598854065
I0325 12:52:40.528408 725942 finetune.py:45] layer 25_down initial loss 0.03700294345617294
I0325 12:52:41.442297 726833 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.03059801645576954 old loss 0.03065437264740467 BETTER
I0325 12:52:56.485629 725045 finetune.py:68] layer 24_down @ epoch 0 new loss 0.035354502499103546 old loss 0.035373106598854065 BETTER
I0325 12:53:06.484281 725942 finetune.py:68] layer 25_down @ epoch 0 new loss 0.03698386251926422 old loss 0.03700294345617294 BETTER
I0325 12:53:11.060062 726833 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.030544158071279526 old loss 0.03059801645576954 BETTER
I0325 12:53:24.785328 725045 finetune.py:68] layer 24_down @ epoch 1 new loss 0.03533628210425377 old loss 0.035354502499103546 BETTER
I0325 12:53:33.969375 725942 finetune.py:68] layer 25_down @ epoch 1 new loss 0.03696497529745102 old loss 0.03698386251926422 BETTER
I0325 12:53:54.385528 725045 finetune.py:68] layer 24_down @ epoch 2 new loss 0.03531848266720772 old loss 0.03533628210425377 BETTER
I0325 12:53:58.590847 726833 finetune.py:45] layer 26_down initial loss 0.04293696954846382
I0325 12:54:02.417531 725942 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0369468592107296 old loss 0.03696497529745102 BETTER
I0325 12:54:24.127544 725045 finetune.py:68] layer 24_down @ epoch 3 new loss 0.03530103340744972 old loss 0.03531848266720772 BETTER
I0325 12:54:25.779740 726833 finetune.py:68] layer 26_down @ epoch 0 new loss 0.04291748255491257 old loss 0.04293696954846382 BETTER
I0325 12:54:30.917242 725942 finetune.py:68] layer 25_down @ epoch 3 new loss 0.03692847117781639 old loss 0.0369468592107296 BETTER
I0325 12:54:54.359035 726833 finetune.py:68] layer 26_down @ epoch 1 new loss 0.042898762971162796 old loss 0.04291748255491257 BETTER
I0325 12:54:54.677211 725045 finetune.py:68] layer 24_down @ epoch 4 new loss 0.035283878445625305 old loss 0.03530103340744972 BETTER
24_v proxy err 0.15641580522060394 tr(WHW.T) 5382.7841796875
bpp_loss 1.4494756795611465
24_q proxy err 0.03208731859922409 tr(WHW.T) 27144.3671875
bpp_loss 1.636514914658619
24_k proxy err 0.021772446110844612 tr(WHW.T) 39956.36328125
bpp_loss 1.6482569729996612
24_o proxy err 0.11767522990703583 tr(WHW.T) 8197.91796875
bpp_loss 1.4354576567129698
24_up proxy err 0.14681655168533325 tr(WHW.T) 18698.53515625
bpp_loss 1.4628872075876178
24_gate proxy err 0.09398441016674042 tr(WHW.T) 30145.19140625
bpp_loss 1.5570646748813086
24_down proxy err 0.16531974077224731 tr(WHW.T) 16243.9560546875
bpp_loss 1.4478997805935525
I0325 12:54:58.581122 725942 finetune.py:68] layer 25_down @ epoch 4 new loss 0.03691098466515541 old loss 0.03692847117781639 BETTER
25_v proxy err 0.15229545533657074 tr(WHW.T) 5989.998046875
bpp_loss 1.4890120338532142
25_q proxy err 0.03740890696644783 tr(WHW.T) 25163.216796875
bpp_loss 1.6525096057011979
25_k proxy err 0.02785199135541916 tr(WHW.T) 33814.8828125
bpp_loss 1.6601278358866693
25_o proxy err 0.15301865339279175 tr(WHW.T) 6955.18408203125
bpp_loss 1.4829842422914226
25_up proxy err 0.1468193382024765 tr(WHW.T) 18814.90234375
bpp_loss 1.46463108697343
25_gate proxy err 0.09189218282699585 tr(WHW.T) 30963.490234375
bpp_loss 1.555376758279149
25_down proxy err 0.15876404941082 tr(WHW.T) 16411.689453125
bpp_loss 1.44969763067486
I0325 12:55:22.380681 726833 finetune.py:68] layer 26_down @ epoch 2 new loss 0.042880553752183914 old loss 0.042898762971162796 BETTER
I0325 12:55:49.112017 726833 finetune.py:68] layer 26_down @ epoch 3 new loss 0.04286213964223862 old loss 0.042880553752183914 BETTER
I0325 12:56:08.503491 583434 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 63.653682470321655s
I0325 12:56:12.093100 742830 config.py:54] PyTorch version 2.6.0 available.
W0325 12:56:12.433620 742830 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 12:56:13.471043 742830 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 12:56:13.475300 583434 quantize_finetune_llama.py:209] layer 28 gpu 1
I0325 12:56:13.489009 742830 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 12:56:16.084702 726833 finetune.py:68] layer 26_down @ epoch 4 new loss 0.04284464567899704 old loss 0.04286213964223862 BETTER
26_v proxy err 0.14960139989852905 tr(WHW.T) 5985.25439453125
bpp_loss 1.5030336803174578
26_q proxy err 0.03391474857926369 tr(WHW.T) 26866.7578125
bpp_loss 1.6325205210741842
26_k proxy err 0.02415064536035061 tr(WHW.T) 37788.953125
bpp_loss 1.6471575982577633
26_o proxy err 0.09392254799604416 tr(WHW.T) 10061.4755859375
bpp_loss 1.4965553664951585
26_up proxy err 0.13751280307769775 tr(WHW.T) 20093.490234375
bpp_loss 1.4684456784581377
26_gate proxy err 0.08528842777013779 tr(WHW.T) 33337.76171875
bpp_loss 1.5556745389043245
26_down proxy err 0.16195882856845856 tr(WHW.T) 15946.57421875
bpp_loss 1.450001683421866
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 12:56:31.801379 742830 finetune.py:45] layer 27_v initial loss 0.006591344717890024
W0325 12:56:31.801656 742830 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 12:57:04.905939 742830 finetune.py:68] layer 27_v @ epoch 0 new loss 0.004142804071307182 old loss 0.006591344717890024 BETTER
I0325 12:57:21.938698 583434 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 61.496490240097046s
I0325 12:57:25.587159 743806 config.py:54] PyTorch version 2.6.0 available.
W0325 12:57:25.906638 743806 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 12:57:26.888287 743806 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 12:57:26.892267 583434 quantize_finetune_llama.py:209] layer 29 gpu 2
I0325 12:57:26.905310 743806 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 12:57:39.471422 742830 finetune.py:68] layer 27_v @ epoch 1 new loss 0.003773969830945134 old loss 0.004142804071307182 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 12:57:45.424591 743806 finetune.py:45] layer 28_v initial loss 0.0077666244469583035
W0325 12:57:45.424877 743806 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 12:58:14.286950 742830 finetune.py:68] layer 27_v @ epoch 2 new loss 0.003668725024908781 old loss 0.003773969830945134 BETTER
I0325 12:58:16.835156 743806 finetune.py:68] layer 28_v @ epoch 0 new loss 0.00513482466340065 old loss 0.0077666244469583035 BETTER
I0325 12:58:30.037336 583434 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 62.690656900405884s
I0325 12:58:33.567293 744713 config.py:54] PyTorch version 2.6.0 available.
W0325 12:58:33.866516 744713 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 12:58:34.827961 744713 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 12:58:34.831961 583434 quantize_finetune_llama.py:209] layer 30 gpu 0
I0325 12:58:34.844255 744713 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 12:58:49.236284 742830 finetune.py:68] layer 27_v @ epoch 3 new loss 0.0036118794232606888 old loss 0.003668725024908781 BETTER
I0325 12:58:49.286283 743806 finetune.py:68] layer 28_v @ epoch 1 new loss 0.004809328354895115 old loss 0.00513482466340065 BETTER
I0325 12:58:54.216168 744713 finetune.py:45] layer 29_v initial loss 0.007290366105735302
W0325 12:58:54.216575 744713 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 12:59:22.277169 743806 finetune.py:68] layer 28_v @ epoch 2 new loss 0.004702947102487087 old loss 0.004809328354895115 BETTER
I0325 12:59:24.485407 742830 finetune.py:68] layer 27_v @ epoch 4 new loss 0.0035718735307455063 old loss 0.0036118794232606888 BETTER
I0325 12:59:26.329647 744713 finetune.py:68] layer 29_v @ epoch 0 new loss 0.005277598742395639 old loss 0.007290366105735302 BETTER
I0325 12:59:45.069133 742830 finetune.py:45] layer 27_q initial loss 0.005066857673227787
I0325 12:59:55.502769 743806 finetune.py:68] layer 28_v @ epoch 3 new loss 0.004637706093490124 old loss 0.004702947102487087 BETTER
I0325 12:59:59.395665 744713 finetune.py:68] layer 29_v @ epoch 1 new loss 0.005088508129119873 old loss 0.005277598742395639 BETTER
I0325 13:00:18.485919 742830 finetune.py:68] layer 27_q @ epoch 0 new loss 0.004740613978356123 old loss 0.005066857673227787 BETTER
I0325 13:00:28.899417 743806 finetune.py:68] layer 28_v @ epoch 4 new loss 0.0045907748863101006 old loss 0.004637706093490124 BETTER
I0325 13:00:33.118808 744713 finetune.py:68] layer 29_v @ epoch 2 new loss 0.005000127479434013 old loss 0.005088508129119873 BETTER
I0325 13:00:50.598806 743806 finetune.py:45] layer 28_q initial loss 0.006359446793794632
I0325 13:00:53.750438 742830 finetune.py:68] layer 27_q @ epoch 1 new loss 0.00459208432585001 old loss 0.004740613978356123 BETTER
I0325 13:01:06.852378 744713 finetune.py:68] layer 29_v @ epoch 3 new loss 0.004941913299262524 old loss 0.005000127479434013 BETTER
I0325 13:01:22.223357 743806 finetune.py:68] layer 28_q @ epoch 0 new loss 0.005957715213298798 old loss 0.006359446793794632 BETTER
I0325 13:01:28.735122 742830 finetune.py:68] layer 27_q @ epoch 2 new loss 0.004498228430747986 old loss 0.00459208432585001 BETTER
I0325 13:01:40.575844 744713 finetune.py:68] layer 29_v @ epoch 4 new loss 0.004895300604403019 old loss 0.004941913299262524 BETTER
I0325 13:01:54.827250 743806 finetune.py:68] layer 28_q @ epoch 1 new loss 0.0057933214120566845 old loss 0.005957715213298798 BETTER
I0325 13:02:02.485126 744713 finetune.py:45] layer 29_q initial loss 0.006493380758911371
I0325 13:02:04.598024 742830 finetune.py:68] layer 27_q @ epoch 3 new loss 0.004433659836649895 old loss 0.004498228430747986 BETTER
I0325 13:02:27.561941 743806 finetune.py:68] layer 28_q @ epoch 2 new loss 0.0056955995969474316 old loss 0.0057933214120566845 BETTER
I0325 13:02:34.460116 744713 finetune.py:68] layer 29_q @ epoch 0 new loss 0.006160410586744547 old loss 0.006493380758911371 BETTER
I0325 13:02:39.736947 742830 finetune.py:68] layer 27_q @ epoch 4 new loss 0.004384361673146486 old loss 0.004433659836649895 BETTER
I0325 13:03:01.485121 743806 finetune.py:68] layer 28_q @ epoch 3 new loss 0.005626380443572998 old loss 0.0056955995969474316 BETTER
I0325 13:03:01.663042 742830 finetune.py:45] layer 27_k initial loss 0.005759633611887693
I0325 13:03:07.431288 744713 finetune.py:68] layer 29_q @ epoch 1 new loss 0.006017870269715786 old loss 0.006160410586744547 BETTER
I0325 13:03:34.460383 742830 finetune.py:68] layer 27_k @ epoch 0 new loss 0.005687038414180279 old loss 0.005759633611887693 BETTER
I0325 13:03:34.491566 743806 finetune.py:68] layer 28_q @ epoch 4 new loss 0.0055726817809045315 old loss 0.005626380443572998 BETTER
I0325 13:03:40.991060 744713 finetune.py:68] layer 29_q @ epoch 2 new loss 0.0059382435865700245 old loss 0.006017870269715786 BETTER
I0325 13:03:54.478040 743806 finetune.py:45] layer 28_k initial loss 0.007442707195878029
I0325 13:04:08.664600 742830 finetune.py:68] layer 27_k @ epoch 1 new loss 0.005647602025419474 old loss 0.005687038414180279 BETTER
I0325 13:04:14.378288 744713 finetune.py:68] layer 29_q @ epoch 3 new loss 0.005879689939320087 old loss 0.0059382435865700245 BETTER
I0325 13:04:26.118315 743806 finetune.py:68] layer 28_k @ epoch 0 new loss 0.007345964200794697 old loss 0.007442707195878029 BETTER
I0325 13:04:44.048388 742830 finetune.py:68] layer 27_k @ epoch 2 new loss 0.005613973829895258 old loss 0.005647602025419474 BETTER
I0325 13:04:49.380573 744713 finetune.py:68] layer 29_q @ epoch 4 new loss 0.005835259333252907 old loss 0.005879689939320087 BETTER
I0325 13:04:59.012528 743806 finetune.py:68] layer 28_k @ epoch 1 new loss 0.007293157745152712 old loss 0.007345964200794697 BETTER
I0325 13:05:12.172908 744713 finetune.py:45] layer 29_k initial loss 0.0076253111474215984
I0325 13:05:19.116843 742830 finetune.py:68] layer 27_k @ epoch 3 new loss 0.00558348698541522 old loss 0.005613973829895258 BETTER
I0325 13:05:32.918130 743806 finetune.py:68] layer 28_k @ epoch 2 new loss 0.007249068468809128 old loss 0.007293157745152712 BETTER
I0325 13:05:45.181520 744713 finetune.py:68] layer 29_k @ epoch 0 new loss 0.007541070692241192 old loss 0.0076253111474215984 BETTER
I0325 13:05:53.633972 742830 finetune.py:68] layer 27_k @ epoch 4 new loss 0.0055550988763570786 old loss 0.00558348698541522 BETTER
I0325 13:06:05.275394 743806 finetune.py:68] layer 28_k @ epoch 3 new loss 0.007208627182990313 old loss 0.007249068468809128 BETTER
I0325 13:06:15.846058 742830 finetune.py:45] layer 27_o initial loss 0.00944012962281704
I0325 13:06:19.491570 744713 finetune.py:68] layer 29_k @ epoch 1 new loss 0.0074933841824531555 old loss 0.007541070692241192 BETTER
I0325 13:06:38.317847 743806 finetune.py:68] layer 28_k @ epoch 4 new loss 0.007172133773565292 old loss 0.007208627182990313 BETTER
I0325 13:06:47.821337 742830 finetune.py:68] layer 27_o @ epoch 0 new loss 0.009035727009177208 old loss 0.00944012962281704 BETTER
I0325 13:06:52.203389 744713 finetune.py:68] layer 29_k @ epoch 2 new loss 0.007454568054527044 old loss 0.0074933841824531555 BETTER
I0325 13:06:59.981896 743806 finetune.py:45] layer 28_o initial loss 0.01201567891985178
I0325 13:07:21.939223 742830 finetune.py:68] layer 27_o @ epoch 1 new loss 0.008958951570093632 old loss 0.009035727009177208 BETTER
I0325 13:07:25.301655 744713 finetune.py:68] layer 29_k @ epoch 3 new loss 0.007418896071612835 old loss 0.007454568054527044 BETTER
I0325 13:07:30.509492 743806 finetune.py:68] layer 28_o @ epoch 0 new loss 0.011576839722692966 old loss 0.01201567891985178 BETTER
I0325 13:07:55.449263 742830 finetune.py:68] layer 27_o @ epoch 2 new loss 0.00890928041189909 old loss 0.008958951570093632 BETTER
I0325 13:07:58.300811 744713 finetune.py:68] layer 29_k @ epoch 4 new loss 0.007387347053736448 old loss 0.007418896071612835 BETTER
I0325 13:08:02.119101 743806 finetune.py:68] layer 28_o @ epoch 1 new loss 0.011499043554067612 old loss 0.011576839722692966 BETTER
I0325 13:08:19.094048 744713 finetune.py:45] layer 29_o initial loss 0.012091077864170074
I0325 13:08:29.261019 742830 finetune.py:68] layer 27_o @ epoch 3 new loss 0.008867082186043262 old loss 0.00890928041189909 BETTER
I0325 13:08:33.715098 743806 finetune.py:68] layer 28_o @ epoch 2 new loss 0.01144243311136961 old loss 0.011499043554067612 BETTER
I0325 13:08:49.960861 744713 finetune.py:68] layer 29_o @ epoch 0 new loss 0.011761203408241272 old loss 0.012091077864170074 BETTER
I0325 13:09:03.144345 742830 finetune.py:68] layer 27_o @ epoch 4 new loss 0.008830443024635315 old loss 0.008867082186043262 BETTER
I0325 13:09:05.488122 743806 finetune.py:68] layer 28_o @ epoch 3 new loss 0.011393794789910316 old loss 0.01144243311136961 BETTER
I0325 13:09:22.751178 744713 finetune.py:68] layer 29_o @ epoch 1 new loss 0.011697519570589066 old loss 0.011761203408241272 BETTER
I0325 13:09:31.197154 742830 finetune.py:45] layer 27_up initial loss 0.0223605427891016
I0325 13:09:37.635932 743806 finetune.py:68] layer 28_o @ epoch 4 new loss 0.01134967990219593 old loss 0.011393794789910316 BETTER
I0325 13:09:56.174203 744713 finetune.py:68] layer 29_o @ epoch 2 new loss 0.011648868210613728 old loss 0.011697519570589066 BETTER
I0325 13:10:01.972162 742830 finetune.py:68] layer 27_up @ epoch 0 new loss 0.022151513025164604 old loss 0.0223605427891016 BETTER
I0325 13:10:06.728084 743806 finetune.py:45] layer 28_up initial loss 0.02705058827996254
I0325 13:10:28.700783 744713 finetune.py:68] layer 29_o @ epoch 3 new loss 0.011606788262724876 old loss 0.011648868210613728 BETTER
I0325 13:10:34.407166 742830 finetune.py:68] layer 27_up @ epoch 1 new loss 0.022025499492883682 old loss 0.022151513025164604 BETTER
I0325 13:10:36.307850 743806 finetune.py:68] layer 28_up @ epoch 0 new loss 0.02678258717060089 old loss 0.02705058827996254 BETTER
I0325 13:11:01.179338 744713 finetune.py:68] layer 29_o @ epoch 4 new loss 0.011570056900382042 old loss 0.011606788262724876 BETTER
I0325 13:11:06.321959 743806 finetune.py:68] layer 28_up @ epoch 1 new loss 0.026624828577041626 old loss 0.02678258717060089 BETTER
I0325 13:11:06.475723 742830 finetune.py:68] layer 27_up @ epoch 2 new loss 0.02192523144185543 old loss 0.022025499492883682 BETTER
I0325 13:11:30.745886 744713 finetune.py:45] layer 29_up initial loss 0.029800910502672195
I0325 13:11:37.572495 743806 finetune.py:68] layer 28_up @ epoch 2 new loss 0.02649914100766182 old loss 0.026624828577041626 BETTER
I0325 13:11:39.266610 742830 finetune.py:68] layer 27_up @ epoch 3 new loss 0.021840250119566917 old loss 0.02192523144185543 BETTER
I0325 13:12:01.434419 744713 finetune.py:68] layer 29_up @ epoch 0 new loss 0.02946554310619831 old loss 0.029800910502672195 BETTER
I0325 13:12:07.970649 743806 finetune.py:68] layer 28_up @ epoch 3 new loss 0.026393132284283638 old loss 0.02649914100766182 BETTER
I0325 13:12:12.088314 742830 finetune.py:68] layer 27_up @ epoch 4 new loss 0.021765559911727905 old loss 0.021840250119566917 BETTER
I0325 13:12:32.777519 744713 finetune.py:68] layer 29_up @ epoch 1 new loss 0.02928299829363823 old loss 0.02946554310619831 BETTER
I0325 13:12:38.729692 743806 finetune.py:68] layer 28_up @ epoch 4 new loss 0.026299426332116127 old loss 0.026393132284283638 BETTER
I0325 13:12:41.379806 742830 finetune.py:45] layer 27_gate initial loss 0.03264443203806877
I0325 13:13:04.018562 744713 finetune.py:68] layer 29_up @ epoch 2 new loss 0.02914375811815262 old loss 0.02928299829363823 BETTER
I0325 13:13:09.340070 743806 finetune.py:45] layer 28_gate initial loss 0.03911971300840378
I0325 13:13:11.608735 742830 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.03254101425409317 old loss 0.03264443203806877 BETTER
I0325 13:13:35.612814 744713 finetune.py:68] layer 29_up @ epoch 3 new loss 0.029024217277765274 old loss 0.02914375811815262 BETTER
I0325 13:13:37.625111 743806 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0389912910759449 old loss 0.03911971300840378 BETTER
I0325 13:13:42.148283 742830 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.032470494508743286 old loss 0.03254101425409317 BETTER
I0325 13:14:08.008393 744713 finetune.py:68] layer 29_up @ epoch 4 new loss 0.028919391334056854 old loss 0.029024217277765274 BETTER
I0325 13:14:08.177736 743806 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.03890395164489746 old loss 0.0389912910759449 BETTER
I0325 13:14:12.613425 742830 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.03240470960736275 old loss 0.032470494508743286 BETTER
I0325 13:14:38.171256 744713 finetune.py:45] layer 29_gate initial loss 0.043921660631895065
I0325 13:14:38.602306 743806 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.038821667432785034 old loss 0.03890395164489746 BETTER
I0325 13:14:43.145028 742830 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.03234083950519562 old loss 0.03240470960736275 BETTER
I0325 13:15:07.004353 744713 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.04376590624451637 old loss 0.043921660631895065 BETTER
I0325 13:15:08.338779 743806 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.03874564543366432 old loss 0.038821667432785034 BETTER
I0325 13:15:14.360363 742830 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.03228094428777695 old loss 0.03234083950519562 BETTER
I0325 13:15:37.093646 744713 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.043659549206495285 old loss 0.04376590624451637 BETTER
I0325 13:15:38.307163 743806 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.038671694695949554 old loss 0.03874564543366432 BETTER
I0325 13:15:59.627012 742830 finetune.py:45] layer 27_down initial loss 0.04610223323106766
I0325 13:16:06.275867 744713 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0435621477663517 old loss 0.043659549206495285 BETTER
I0325 13:16:25.395032 743806 finetune.py:45] layer 28_down initial loss 0.05487176403403282
I0325 13:16:26.969453 742830 finetune.py:68] layer 27_down @ epoch 0 new loss 0.046084050089120865 old loss 0.04610223323106766 BETTER
I0325 13:16:36.505648 744713 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.043471936136484146 old loss 0.0435621477663517 BETTER
I0325 13:16:51.681745 743806 finetune.py:68] layer 28_down @ epoch 0 new loss 0.054850321263074875 old loss 0.05487176403403282 BETTER
I0325 13:16:55.819978 742830 finetune.py:68] layer 27_down @ epoch 1 new loss 0.04606674611568451 old loss 0.046084050089120865 BETTER
I0325 13:17:06.847074 744713 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.043386656790971756 old loss 0.043471936136484146 BETTER
I0325 13:17:18.510182 743806 finetune.py:68] layer 28_down @ epoch 1 new loss 0.054829616099596024 old loss 0.054850321263074875 BETTER
I0325 13:17:24.499155 742830 finetune.py:68] layer 27_down @ epoch 2 new loss 0.04604924097657204 old loss 0.04606674611568451 BETTER
I0325 13:17:45.805593 743806 finetune.py:68] layer 28_down @ epoch 2 new loss 0.05480901151895523 old loss 0.054829616099596024 BETTER
I0325 13:17:54.759459 742830 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0460326261818409 old loss 0.04604924097657204 BETTER
I0325 13:17:55.066508 744713 finetune.py:45] layer 29_down initial loss 0.06259667873382568
I0325 13:18:13.809064 743806 finetune.py:68] layer 28_down @ epoch 3 new loss 0.05478931963443756 old loss 0.05480901151895523 BETTER
I0325 13:18:22.447795 744713 finetune.py:68] layer 29_down @ epoch 0 new loss 0.06257452815771103 old loss 0.06259667873382568 BETTER
I0325 13:18:24.662049 742830 finetune.py:68] layer 27_down @ epoch 4 new loss 0.046015895903110504 old loss 0.0460326261818409 BETTER
27_v proxy err 0.14470206201076508 tr(WHW.T) 6602.53369140625
bpp_loss 1.5103990750794765
27_q proxy err 0.03520360589027405 tr(WHW.T) 28315.337890625
bpp_loss 1.6891370174998883
27_k proxy err 0.025550495833158493 tr(WHW.T) 39173.05859375
bpp_loss 1.7066998976952164
27_o proxy err 0.12827883660793304 tr(WHW.T) 7495.22998046875
bpp_loss 1.5088905088196043
27_up proxy err 0.12503965198993683 tr(WHW.T) 21945.0546875
bpp_loss 1.4724314263480347
27_gate proxy err 0.07964375615119934 tr(WHW.T) 35334.01953125
bpp_loss 1.5531215629172186
27_down proxy err 0.15621820092201233 tr(WHW.T) 15609.5166015625
bpp_loss 1.4525708413552927
I0325 13:18:41.603278 743806 finetune.py:68] layer 28_down @ epoch 4 new loss 0.054769374430179596 old loss 0.05478931963443756 BETTER
28_v proxy err 0.1339058130979538 tr(WHW.T) 7146.4267578125
bpp_loss 1.545594271083246
28_q proxy err 0.03578099235892296 tr(WHW.T) 27135.23828125
bpp_loss 1.649258853372885
28_k proxy err 0.02602432854473591 tr(WHW.T) 37526.65234375
bpp_loss 1.66801463716547
28_o proxy err 0.10513211786746979 tr(WHW.T) 9121.3955078125
bpp_loss 1.5485775494016707
28_up proxy err 0.10320796817541122 tr(WHW.T) 26366.642578125
bpp_loss 1.4843522822267787
28_gate proxy err 0.0758652463555336 tr(WHW.T) 36524.01171875
bpp_loss 1.5463649087780438
28_down proxy err 0.1420438438653946 tr(WHW.T) 15470.1533203125
bpp_loss 1.4573994092426674
I0325 13:18:50.306913 744713 finetune.py:68] layer 29_down @ epoch 1 new loss 0.06255244463682175 old loss 0.06257452815771103 BETTER
I0325 13:19:17.260059 744713 finetune.py:68] layer 29_down @ epoch 2 new loss 0.06253106147050858 old loss 0.06255244463682175 BETTER
I0325 13:19:44.392336 744713 finetune.py:68] layer 29_down @ epoch 3 new loss 0.06251022219657898 old loss 0.06253106147050858 BETTER
I0325 13:19:49.574977 583434 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 63.18986368179321s
I0325 13:19:53.100680 760870 config.py:54] PyTorch version 2.6.0 available.
W0325 13:19:53.404041 760870 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 13:19:54.360561 760870 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 13:19:54.364929 583434 quantize_finetune_llama.py:209] layer 31 gpu 1
I0325 13:19:54.379058 760870 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 13:20:11.425347 744713 finetune.py:68] layer 29_down @ epoch 4 new loss 0.06248955801129341 old loss 0.06251022219657898 BETTER
I0325 13:20:13.070830 760870 finetune.py:45] layer 30_v initial loss 0.009696721099317074
W0325 13:20:13.071039 760870 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

29_v proxy err 0.13931788504123688 tr(WHW.T) 6751.8232421875
bpp_loss 1.5484828553453553
29_q proxy err 0.034424252808094025 tr(WHW.T) 27195.8359375
bpp_loss 1.610455628251657
29_k proxy err 0.02359537035226822 tr(WHW.T) 39833.546875
bpp_loss 1.6295301222126
29_o proxy err 0.09479881823062897 tr(WHW.T) 10801.4453125
bpp_loss 1.563719160592882
29_up proxy err 0.08126862347126007 tr(WHW.T) 33155.3828125
bpp_loss 1.4948636393474286
29_gate proxy err 0.06857528537511826 tr(WHW.T) 39843.47265625
bpp_loss 1.5468386241213181
29_down proxy err 0.12877404689788818 tr(WHW.T) 15197.9169921875
bpp_loss 1.4581327817706002
I0325 13:20:45.929474 760870 finetune.py:68] layer 30_v @ epoch 0 new loss 0.00548455910757184 old loss 0.009696721099317074 BETTER
I0325 13:21:17.390607 583434 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 60.9457061290741s
I0325 13:21:20.532179 760870 finetune.py:68] layer 30_v @ epoch 1 new loss 0.005178159102797508 old loss 0.00548455910757184 BETTER
I0325 13:21:20.978832 762011 config.py:54] PyTorch version 2.6.0 available.
W0325 13:21:21.279003 762011 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 13:21:22.345115 762011 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 13:21:22.362902 762011 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 13:21:40.275502 762011 finetune.py:45] layer 31_v initial loss 0.011882654391229153
W0325 13:21:40.275772 762011 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 13:21:55.325268 760870 finetune.py:68] layer 30_v @ epoch 2 new loss 0.005045277997851372 old loss 0.005178159102797508 BETTER
I0325 13:22:11.731892 762011 finetune.py:68] layer 31_v @ epoch 0 new loss 0.007832750678062439 old loss 0.011882654391229153 BETTER
I0325 13:22:30.375030 760870 finetune.py:68] layer 30_v @ epoch 3 new loss 0.004965657368302345 old loss 0.005045277997851372 BETTER
I0325 13:22:44.286538 762011 finetune.py:68] layer 31_v @ epoch 1 new loss 0.007456293795257807 old loss 0.007832750678062439 BETTER
I0325 13:23:05.574193 760870 finetune.py:68] layer 30_v @ epoch 4 new loss 0.004904467146843672 old loss 0.004965657368302345 BETTER
I0325 13:23:16.878558 762011 finetune.py:68] layer 31_v @ epoch 2 new loss 0.0072408257983624935 old loss 0.007456293795257807 BETTER
I0325 13:23:24.410009 760870 finetune.py:45] layer 30_q initial loss 0.007183081470429897
I0325 13:23:49.649319 762011 finetune.py:68] layer 31_v @ epoch 3 new loss 0.007079727482050657 old loss 0.0072408257983624935 BETTER
I0325 13:23:57.370628 760870 finetune.py:68] layer 30_q @ epoch 0 new loss 0.006570067722350359 old loss 0.007183081470429897 BETTER
I0325 13:24:22.558420 762011 finetune.py:68] layer 31_v @ epoch 4 new loss 0.0069608925841748714 old loss 0.007079727482050657 BETTER
I0325 13:24:31.482667 760870 finetune.py:68] layer 30_q @ epoch 1 new loss 0.006356444209814072 old loss 0.006570067722350359 BETTER
I0325 13:24:41.427085 762011 finetune.py:45] layer 31_q initial loss 0.015280435793101788
I0325 13:25:05.933448 760870 finetune.py:68] layer 30_q @ epoch 2 new loss 0.006236059125512838 old loss 0.006356444209814072 BETTER
I0325 13:25:12.921332 762011 finetune.py:68] layer 31_q @ epoch 0 new loss 0.013755885884165764 old loss 0.015280435793101788 BETTER
I0325 13:25:40.681726 760870 finetune.py:68] layer 30_q @ epoch 3 new loss 0.006150611210614443 old loss 0.006236059125512838 BETTER
I0325 13:25:45.273233 762011 finetune.py:68] layer 31_q @ epoch 1 new loss 0.013294070027768612 old loss 0.013755885884165764 BETTER
I0325 13:26:15.230242 760870 finetune.py:68] layer 30_q @ epoch 4 new loss 0.006084793247282505 old loss 0.006150611210614443 BETTER
I0325 13:26:17.482041 762011 finetune.py:68] layer 31_q @ epoch 2 new loss 0.012979472056031227 old loss 0.013294070027768612 BETTER
I0325 13:26:34.220625 760870 finetune.py:45] layer 30_k initial loss 0.008208786137402058
I0325 13:26:50.053268 762011 finetune.py:68] layer 31_q @ epoch 3 new loss 0.01273003313690424 old loss 0.012979472056031227 BETTER
I0325 13:27:06.856738 760870 finetune.py:68] layer 30_k @ epoch 0 new loss 0.008055956102907658 old loss 0.008208786137402058 BETTER
I0325 13:27:22.283861 762011 finetune.py:68] layer 31_q @ epoch 4 new loss 0.01252936851233244 old loss 0.01273003313690424 BETTER
I0325 13:27:40.288853 760870 finetune.py:68] layer 30_k @ epoch 1 new loss 0.007991560734808445 old loss 0.008055956102907658 BETTER
I0325 13:27:41.112878 762011 finetune.py:45] layer 31_k initial loss 0.015450313687324524
I0325 13:28:12.246412 762011 finetune.py:68] layer 31_k @ epoch 0 new loss 0.014611385762691498 old loss 0.015450313687324524 BETTER
I0325 13:28:13.906438 760870 finetune.py:68] layer 30_k @ epoch 2 new loss 0.007937904447317123 old loss 0.007991560734808445 BETTER
I0325 13:28:44.396662 762011 finetune.py:68] layer 31_k @ epoch 1 new loss 0.014432799071073532 old loss 0.014611385762691498 BETTER
I0325 13:28:47.846055 760870 finetune.py:68] layer 30_k @ epoch 3 new loss 0.007891691289842129 old loss 0.007937904447317123 BETTER
I0325 13:29:16.528702 762011 finetune.py:68] layer 31_k @ epoch 2 new loss 0.014278287068009377 old loss 0.014432799071073532 BETTER
I0325 13:29:21.712203 760870 finetune.py:68] layer 30_k @ epoch 4 new loss 0.007849684916436672 old loss 0.007891691289842129 BETTER
I0325 13:29:40.551929 760870 finetune.py:45] layer 30_o initial loss 0.013565274886786938
I0325 13:29:48.558161 762011 finetune.py:68] layer 31_k @ epoch 3 new loss 0.01414901576936245 old loss 0.014278287068009377 BETTER
I0325 13:30:12.473154 760870 finetune.py:68] layer 30_o @ epoch 0 new loss 0.013003728352487087 old loss 0.013565274886786938 BETTER
I0325 13:30:20.628615 762011 finetune.py:68] layer 31_k @ epoch 4 new loss 0.014051099307835102 old loss 0.01414901576936245 BETTER
I0325 13:30:39.493576 762011 finetune.py:45] layer 31_o initial loss 0.023699240759015083
I0325 13:30:45.490519 760870 finetune.py:68] layer 30_o @ epoch 1 new loss 0.012873847037553787 old loss 0.013003728352487087 BETTER
I0325 13:31:10.091594 762011 finetune.py:68] layer 31_o @ epoch 0 new loss 0.021643051877617836 old loss 0.023699240759015083 BETTER
I0325 13:31:18.613277 760870 finetune.py:68] layer 30_o @ epoch 2 new loss 0.012771282345056534 old loss 0.012873847037553787 BETTER
I0325 13:31:41.563977 762011 finetune.py:68] layer 31_o @ epoch 1 new loss 0.02105746977031231 old loss 0.021643051877617836 BETTER
I0325 13:31:52.175409 760870 finetune.py:68] layer 30_o @ epoch 3 new loss 0.012690258212387562 old loss 0.012771282345056534 BETTER
I0325 13:32:13.273370 762011 finetune.py:68] layer 31_o @ epoch 2 new loss 0.02068370021879673 old loss 0.02105746977031231 BETTER
I0325 13:32:25.510258 760870 finetune.py:68] layer 30_o @ epoch 4 new loss 0.012618308886885643 old loss 0.012690258212387562 BETTER
I0325 13:32:44.963793 762011 finetune.py:68] layer 31_o @ epoch 3 new loss 0.020392537117004395 old loss 0.02068370021879673 BETTER
I0325 13:32:51.043658 760870 finetune.py:45] layer 30_up initial loss 0.040019311010837555
I0325 13:33:16.553487 762011 finetune.py:68] layer 31_o @ epoch 4 new loss 0.02017020620405674 old loss 0.020392537117004395 BETTER
I0325 13:33:21.362073 760870 finetune.py:68] layer 30_up @ epoch 0 new loss 0.03937089443206787 old loss 0.040019311010837555 BETTER
I0325 13:33:42.562302 762011 finetune.py:45] layer 31_up initial loss 0.08607450127601624
I0325 13:33:52.904791 760870 finetune.py:68] layer 30_up @ epoch 1 new loss 0.038992248475551605 old loss 0.03937089443206787 BETTER
I0325 13:34:11.455997 762011 finetune.py:68] layer 31_up @ epoch 0 new loss 0.08240871876478195 old loss 0.08607450127601624 BETTER
I0325 13:34:24.521507 760870 finetune.py:68] layer 30_up @ epoch 2 new loss 0.03870519623160362 old loss 0.038992248475551605 BETTER
I0325 13:34:41.347020 762011 finetune.py:68] layer 31_up @ epoch 1 new loss 0.0807267501950264 old loss 0.08240871876478195 BETTER
I0325 13:34:56.220378 760870 finetune.py:68] layer 30_up @ epoch 3 new loss 0.03846443444490433 old loss 0.03870519623160362 BETTER
I0325 13:35:11.479724 762011 finetune.py:68] layer 31_up @ epoch 2 new loss 0.07945507764816284 old loss 0.0807267501950264 BETTER
I0325 13:35:28.168726 760870 finetune.py:68] layer 30_up @ epoch 4 new loss 0.038248248398303986 old loss 0.03846443444490433 BETTER
I0325 13:35:41.489659 762011 finetune.py:68] layer 31_up @ epoch 3 new loss 0.07840204983949661 old loss 0.07945507764816284 BETTER
I0325 13:35:54.409075 760870 finetune.py:45] layer 30_gate initial loss 0.05643400922417641
I0325 13:36:11.781105 762011 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0774788185954094 old loss 0.07840204983949661 BETTER
I0325 13:36:23.169147 760870 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.05610152333974838 old loss 0.05643400922417641 BETTER
I0325 13:36:38.999994 762011 finetune.py:45] layer 31_gate initial loss 0.11264728754758835
I0325 13:36:53.290079 760870 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.055902477353811264 old loss 0.05610152333974838 BETTER
I0325 13:37:06.501618 762011 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.1107855811715126 old loss 0.11264728754758835 BETTER
I0325 13:37:23.308188 760870 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.055722177028656006 old loss 0.055902477353811264 BETTER
I0325 13:37:35.086452 762011 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.10974352806806564 old loss 0.1107855811715126 BETTER
I0325 13:37:53.216583 760870 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.05556047707796097 old loss 0.055722177028656006 BETTER
I0325 13:38:03.364755 762011 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.10890131443738937 old loss 0.10974352806806564 BETTER
I0325 13:38:23.269100 760870 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.055405519902706146 old loss 0.05556047707796097 BETTER
I0325 13:38:32.073971 762011 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.10818519443273544 old loss 0.10890131443738937 BETTER
I0325 13:39:00.647850 762011 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.10755098611116409 old loss 0.10818519443273544 BETTER
I0325 13:39:06.526243 760870 finetune.py:45] layer 30_down initial loss 0.0829046443104744
I0325 13:39:33.121484 760870 finetune.py:68] layer 30_down @ epoch 0 new loss 0.08286585658788681 old loss 0.0829046443104744 BETTER
I0325 13:39:45.464941 762011 finetune.py:45] layer 31_down initial loss 0.16015282273292542
I0325 13:40:01.470157 760870 finetune.py:68] layer 30_down @ epoch 1 new loss 0.08282732963562012 old loss 0.08286585658788681 BETTER
I0325 13:40:11.170998 762011 finetune.py:68] layer 31_down @ epoch 0 new loss 0.16000889241695404 old loss 0.16015282273292542 BETTER
I0325 13:40:29.863313 760870 finetune.py:68] layer 30_down @ epoch 2 new loss 0.08279210329055786 old loss 0.08282732963562012 BETTER
I0325 13:40:38.068077 762011 finetune.py:68] layer 31_down @ epoch 1 new loss 0.15988324582576752 old loss 0.16000889241695404 BETTER
I0325 13:40:58.688569 760870 finetune.py:68] layer 30_down @ epoch 3 new loss 0.08275741338729858 old loss 0.08279210329055786 BETTER
I0325 13:41:05.107275 762011 finetune.py:68] layer 31_down @ epoch 2 new loss 0.15976835787296295 old loss 0.15988324582576752 BETTER
I0325 13:41:27.943307 760870 finetune.py:68] layer 30_down @ epoch 4 new loss 0.08272445201873779 old loss 0.08275741338729858 BETTER
30_v proxy err 0.11858747899532318 tr(WHW.T) 8280.2744140625
bpp_loss 1.5833607133827172
30_q proxy err 0.034043677151203156 tr(WHW.T) 28727.330078125
bpp_loss 1.627642520237714
30_k proxy err 0.025386691093444824 tr(WHW.T) 38780.8203125
bpp_loss 1.6520764324377524
30_o proxy err 0.08909057080745697 tr(WHW.T) 10398.1875
bpp_loss 1.5901151679863688
30_up proxy err 0.04835715517401695 tr(WHW.T) 54086.52734375
bpp_loss 1.5073849978142007
30_gate proxy err 0.04493512213230133 tr(WHW.T) 59136.28125
bpp_loss 1.5645730018529087
30_down proxy err 0.03924821317195892 tr(WHW.T) 26397.5234375
bpp_loss 1.447129409289161
I0325 13:41:32.420113 762011 finetune.py:68] layer 31_down @ epoch 3 new loss 0.15966325998306274 old loss 0.15976835787296295 BETTER
I0325 13:41:59.132271 762011 finetune.py:68] layer 31_down @ epoch 4 new loss 0.15956634283065796 old loss 0.15966325998306274 BETTER
31_v proxy err 0.1321108639240265 tr(WHW.T) 6800.76904296875
bpp_loss 1.4760900359106017
31_q proxy err 0.02536904625594616 tr(WHW.T) 36849.12109375
bpp_loss 1.6552166047913488
31_k proxy err 0.017085332423448563 tr(WHW.T) 55380.7109375
bpp_loss 1.701780943403719
31_o proxy err 0.054953962564468384 tr(WHW.T) 13335.5576171875
bpp_loss 1.4778759010659996
31_up proxy err 0.02577129192650318 tr(WHW.T) 96420.2109375
bpp_loss 1.5715416925201235
31_gate proxy err 0.025839926674962044 tr(WHW.T) 97994.7734375
bpp_loss 1.638689138839931
31_down proxy err 0.013852040283381939 tr(WHW.T) 38036.40625
bpp_loss 1.467774927735242
I0325 13:42:22.879905 777534 config.py:54] PyTorch version 2.6.0 available.
W0325 13:42:23.196937 777534 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 13:42:23.453907 777534 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  4.20it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  5.00it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  5.33it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  5.63it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  5.87it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.92it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.61it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  5.68it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  5.34it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  5.69it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  5.31it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  5.17it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.57it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.47it/s]
I0325 13:42:26.516158 777534 hfize_llama.py:153] loaded layer 0
I0325 13:42:27.161385 777534 hfize_llama.py:153] loaded layer 1
I0325 13:42:27.753766 777534 hfize_llama.py:153] loaded layer 2
I0325 13:42:28.332638 777534 hfize_llama.py:153] loaded layer 3
I0325 13:42:28.946587 777534 hfize_llama.py:153] loaded layer 4
I0325 13:42:29.548690 777534 hfize_llama.py:153] loaded layer 5
I0325 13:42:30.177578 777534 hfize_llama.py:153] loaded layer 6
I0325 13:42:30.803157 777534 hfize_llama.py:153] loaded layer 7
I0325 13:42:31.405782 777534 hfize_llama.py:153] loaded layer 8
I0325 13:42:32.013307 777534 hfize_llama.py:153] loaded layer 9
I0325 13:42:32.597679 777534 hfize_llama.py:153] loaded layer 10
I0325 13:42:33.205647 777534 hfize_llama.py:153] loaded layer 11
I0325 13:42:33.801706 777534 hfize_llama.py:153] loaded layer 12
I0325 13:42:34.397265 777534 hfize_llama.py:153] loaded layer 13
I0325 13:42:34.996868 777534 hfize_llama.py:153] loaded layer 14
I0325 13:42:35.590025 777534 hfize_llama.py:153] loaded layer 15
I0325 13:42:36.176337 777534 hfize_llama.py:153] loaded layer 16
I0325 13:42:36.750503 777534 hfize_llama.py:153] loaded layer 17
I0325 13:42:37.347795 777534 hfize_llama.py:153] loaded layer 18
I0325 13:42:37.938998 777534 hfize_llama.py:153] loaded layer 19
I0325 13:42:38.513945 777534 hfize_llama.py:153] loaded layer 20
I0325 13:42:39.104847 777534 hfize_llama.py:153] loaded layer 21
I0325 13:42:39.706653 777534 hfize_llama.py:153] loaded layer 22
I0325 13:42:40.316881 777534 hfize_llama.py:153] loaded layer 23
I0325 13:42:40.922888 777534 hfize_llama.py:153] loaded layer 24
I0325 13:42:41.519685 777534 hfize_llama.py:153] loaded layer 25
I0325 13:42:42.127895 777534 hfize_llama.py:153] loaded layer 26
I0325 13:42:42.738939 777534 hfize_llama.py:153] loaded layer 27
I0325 13:42:43.380130 777534 hfize_llama.py:153] loaded layer 28
I0325 13:42:44.015479 777534 hfize_llama.py:153] loaded layer 29
I0325 13:42:44.609378 777534 hfize_llama.py:153] loaded layer 30
I0325 13:42:45.211607 777534 hfize_llama.py:153] loaded layer 31
I0325 13:42:45.211733 777534 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:08,  1.79s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:05,  1.30s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:05,  1.42s/it]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 186, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 165, in main
    model, _ = model_from_hf_path(args.hf_output_path, device_map='cuda')
  File "/workspace/Weight_compression/comp_lm_qtip/lib/utils/unsafe_import.py", line 44, in model_from_hf_path
    model = model_cls.from_pretrained(path,
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4264, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4777, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 942, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 339, in set_module_tensor_to_device
    new_value = value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 8.75 MiB is free. Process 829626 has 37.37 GiB memory in use. Process 961666 has 10.12 GiB memory in use. Of the allocated memory 9.70 GiB is allocated by PyTorch, and 1.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0325 13:43:30.391626 778600 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

Traceback (most recent call last):
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 124, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 71, in main
    model, model_str = model_from_hf_path(
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 49, in model_from_hf_path
    model = maybe_wrap(use_cuda_graph)(model_cls).from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4057, in from_pretrained
    one_state_dict = load_state_dict(resolved_archive_file[0], weights_only=weights_only)
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 504, in load_state_dict
    with safe_open(checkpoint_file, framework="pt") as f:
FileNotFoundError: No such file or directory: "./hf/ft_ql_ldlq/meta-llama--Llama-2-7b-hf/lmbda10/model-00001-of-00006.safetensors"
I0325 14:04:02.434885 794161 config.py:54] PyTorch version 2.6.0 available.
W0325 14:04:02.749826 794161 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:04:02.996685 794161 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]I0325 14:08:46.887521 798190 config.py:54] PyTorch version 2.6.0 available.
W0325 14:08:47.196350 798190 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:08:47.440505 798190 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.74it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.55it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.82it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.47it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.91it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.09it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.61it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.08it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.11it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.27it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.43it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.32it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.42it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.34it/s]
I0325 14:08:49.694775 798190 hfize_llama.py:153] loaded layer 0
I0325 14:12:48.373665 802319 config.py:54] PyTorch version 2.6.0 available.
W0325 14:12:48.698199 802319 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:12:48.938795 802319 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.58it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.18it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.46it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.19it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.31it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.50it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.35it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.80it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.13it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.46it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.48it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.24it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.41it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.34it/s]
I0325 14:12:51.268974 802319 hfize_llama.py:153] loaded layer 0
I0325 14:12:51.937984 802319 hfize_llama.py:153] loaded layer 1
I0325 14:12:52.611081 802319 hfize_llama.py:153] loaded layer 2
I0325 14:12:53.303045 802319 hfize_llama.py:153] loaded layer 3
I0325 14:12:54.006584 802319 hfize_llama.py:153] loaded layer 4
I0325 14:12:54.755509 802319 hfize_llama.py:153] loaded layer 5
I0325 14:12:55.499263 802319 hfize_llama.py:153] loaded layer 6
I0325 14:12:56.192498 802319 hfize_llama.py:153] loaded layer 7
I0325 14:12:56.913044 802319 hfize_llama.py:153] loaded layer 8
I0325 14:12:57.597924 802319 hfize_llama.py:153] loaded layer 9
I0325 14:12:58.245810 802319 hfize_llama.py:153] loaded layer 10
I0325 14:12:58.932868 802319 hfize_llama.py:153] loaded layer 11
I0325 14:12:59.570225 802319 hfize_llama.py:153] loaded layer 12
I0325 14:13:00.186510 802319 hfize_llama.py:153] loaded layer 13
I0325 14:13:00.844698 802319 hfize_llama.py:153] loaded layer 14
I0325 14:13:01.489656 802319 hfize_llama.py:153] loaded layer 15
I0325 14:13:02.098880 802319 hfize_llama.py:153] loaded layer 16
I0325 14:13:02.724606 802319 hfize_llama.py:153] loaded layer 17
I0325 14:13:03.429701 802319 hfize_llama.py:153] loaded layer 18
I0325 14:13:04.085230 802319 hfize_llama.py:153] loaded layer 19
I0325 14:13:04.699572 802319 hfize_llama.py:153] loaded layer 20
I0325 14:13:05.356671 802319 hfize_llama.py:153] loaded layer 21
I0325 14:13:06.001677 802319 hfize_llama.py:153] loaded layer 22
I0325 14:13:06.646813 802319 hfize_llama.py:153] loaded layer 23
I0325 14:13:07.291218 802319 hfize_llama.py:153] loaded layer 24
I0325 14:13:07.927898 802319 hfize_llama.py:153] loaded layer 25
I0325 14:13:08.558850 802319 hfize_llama.py:153] loaded layer 26
I0325 14:13:09.224196 802319 hfize_llama.py:153] loaded layer 27
I0325 14:13:09.870796 802319 hfize_llama.py:153] loaded layer 28
I0325 14:13:10.514722 802319 hfize_llama.py:153] loaded layer 29
I0325 14:13:11.159529 802319 hfize_llama.py:153] loaded layer 30
I0325 14:13:11.825742 802319 hfize_llama.py:153] loaded layer 31
I0325 14:13:11.825880 802319 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.19s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.09s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.01s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:01,  1.03it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.05s/it]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 186, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 165, in main
    model, _ = model_from_hf_path(args.hf_output_path, device_map='cuda')
  File "/workspace/Weight_compression/comp_lm_qtip/lib/utils/unsafe_import.py", line 44, in model_from_hf_path
    model = model_cls.from_pretrained(path,
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4264, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4777, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 942, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py", line 339, in set_module_tensor_to_device
    new_value = value.to(device)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 19.00 MiB is free. Process 4182873 has 1.19 GiB memory in use. Process 1019124 has 26.65 GiB memory in use. Process 1019127 has 19.63 GiB memory in use. Of the allocated memory 19.21 GiB is allocated by PyTorch, and 1.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0325 14:13:56.916590 803508 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:13:57.575042 803508 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.12it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.04it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.02it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.27it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  2.41it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  1.68it/s]
W0325 14:14:01.313866 803508 big_modeling.py:439] Some parameters are on the meta device because they were offloaded to the cpu.
I0325 14:14:01.722112 803508 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/166 [00:00<?, ?it/s]avg_loss = 2.946873903274536:   0%|          | 0/166 [00:05<?, ?it/s]avg_loss = 2.946873903274536:   1%|          | 1/166 [00:05<14:11,  5.16s/it]avg_loss = 3.1537842750549316:   1%|          | 1/166 [00:08<14:11,  5.16s/it]avg_loss = 3.1537842750549316:   1%|          | 2/166 [00:08<11:45,  4.30s/it]avg_loss = 3.356078306833903:   1%|          | 2/166 [00:12<11:45,  4.30s/it] avg_loss = 3.356078306833903:   2%|▏         | 3/166 [00:12<11:10,  4.11s/it]avg_loss = 3.4206089973449707:   2%|▏         | 3/166 [00:16<11:10,  4.11s/it]avg_loss = 3.4206089973449707:   2%|▏         | 4/166 [00:16<10:53,  4.03s/it]avg_loss = 3.3218428611755373:   2%|▏         | 4/166 [00:20<10:53,  4.03s/it]avg_loss = 3.3218428611755373:   3%|▎         | 5/166 [00:20<10:53,  4.06s/it]avg_loss = 3.3478046655654907:   3%|▎         | 5/166 [00:24<10:53,  4.06s/it]avg_loss = 3.3478046655654907:   4%|▎         | 6/166 [00:24<10:51,  4.07s/it]avg_loss = 3.2652314731052945:   4%|▎         | 6/166 [00:28<10:51,  4.07s/it]avg_loss = 3.2652314731052945:   4%|▍         | 7/166 [00:28<10:45,  4.06s/it]avg_loss = 3.1837472915649414:   4%|▍         | 7/166 [00:32<10:45,  4.06s/it]avg_loss = 3.1837472915649414:   5%|▍         | 8/166 [00:32<10:42,  4.07s/it]avg_loss = 3.193109565311008:   5%|▍         | 8/166 [00:37<10:42,  4.07s/it] avg_loss = 3.193109565311008:   5%|▌         | 9/166 [00:37<10:38,  4.07s/it]avg_loss = 3.2315613508224486:   5%|▌         | 9/166 [00:41<10:38,  4.07s/it]avg_loss = 3.2315613508224486:   6%|▌         | 10/166 [00:41<10:36,  4.08s/it]avg_loss = 3.2624287605285645:   6%|▌         | 10/166 [00:45<10:36,  4.08s/it]avg_loss = 3.2624287605285645:   7%|▋         | 11/166 [00:45<10:39,  4.12s/it]avg_loss = 3.261483053366343:   7%|▋         | 11/166 [00:49<10:39,  4.12s/it] avg_loss = 3.261483053366343:   7%|▋         | 12/166 [00:49<10:37,  4.14s/it]avg_loss = 3.2553584575653076:   7%|▋         | 12/166 [00:53<10:37,  4.14s/it]avg_loss = 3.2553584575653076:   8%|▊         | 13/166 [00:53<10:32,  4.13s/it]avg_loss = 3.2659372772489275:   8%|▊         | 13/166 [00:57<10:32,  4.13s/it]avg_loss = 3.2659372772489275:   8%|▊         | 14/166 [00:57<10:25,  4.11s/it]avg_loss = 3.269915310541789:   8%|▊         | 14/166 [01:01<10:25,  4.11s/it] avg_loss = 3.269915310541789:   9%|▉         | 15/166 [01:01<10:17,  4.09s/it]avg_loss = 3.2779174000024796:   9%|▉         | 15/166 [01:05<10:17,  4.09s/it]avg_loss = 3.2779174000024796:  10%|▉         | 16/166 [01:05<10:17,  4.11s/it]avg_loss = 3.281475642148186:  10%|▉         | 16/166 [01:10<10:17,  4.11s/it] avg_loss = 3.281475642148186:  10%|█         | 17/166 [01:10<10:16,  4.14s/it]avg_loss = 3.2991020811928644:  10%|█         | 17/166 [01:14<10:16,  4.14s/it]avg_loss = 3.2991020811928644:  11%|█         | 18/166 [01:14<10:16,  4.17s/it]avg_loss = 3.324841675005461:  11%|█         | 18/166 [01:18<10:16,  4.17s/it] avg_loss = 3.324841675005461:  11%|█▏        | 19/166 [01:18<10:09,  4.15s/it]avg_loss = 3.329060995578766:  11%|█▏        | 19/166 [01:22<10:09,  4.15s/it]avg_loss = 3.329060995578766:  12%|█▏        | 20/166 [01:22<10:03,  4.14s/it]avg_loss = 3.3271697021666027:  12%|█▏        | 20/166 [01:26<10:03,  4.14s/it]avg_loss = 3.3271697021666027:  13%|█▎        | 21/166 [01:26<10:03,  4.16s/it]avg_loss = 3.304562958804044:  13%|█▎        | 21/166 [01:31<10:03,  4.16s/it] avg_loss = 3.304562958804044:  13%|█▎        | 22/166 [01:31<10:02,  4.18s/it]avg_loss = 3.2895971795786982:  13%|█▎        | 22/166 [01:35<10:02,  4.18s/it]avg_loss = 3.2895971795786982:  14%|█▍        | 23/166 [01:35<10:02,  4.21s/it]avg_loss = 3.3068313002586365:  14%|█▍        | 23/166 [01:39<10:02,  4.21s/it]avg_loss = 3.3068313002586365:  14%|█▍        | 24/166 [01:39<09:59,  4.22s/it]avg_loss = 3.3290771293640136:  14%|█▍        | 24/166 [01:43<09:59,  4.22s/it]avg_loss = 3.3290771293640136:  15%|█▌        | 25/166 [01:43<09:55,  4.22s/it]avg_loss = 3.337240695953369:  15%|█▌        | 25/166 [01:48<09:55,  4.22s/it] avg_loss = 3.337240695953369:  16%|█▌        | 26/166 [01:48<09:51,  4.23s/it]avg_loss = 3.3497706254323325:  16%|█▌        | 26/166 [01:52<09:51,  4.23s/it]avg_loss = 3.3497706254323325:  16%|█▋        | 27/166 [01:52<09:46,  4.22s/it]avg_loss = 3.3557471888405934:  16%|█▋        | 27/166 [01:56<09:46,  4.22s/it]avg_loss = 3.3557471888405934:  17%|█▋        | 28/166 [01:56<09:42,  4.22s/it]avg_loss = 3.3706085681915283:  17%|█▋        | 28/166 [02:00<09:42,  4.22s/it]avg_loss = 3.3706085681915283:  17%|█▋        | 29/166 [02:00<09:37,  4.21s/it]avg_loss = 3.3761467933654785:  17%|█▋        | 29/166 [02:04<09:37,  4.21s/it]avg_loss = 3.3761467933654785:  18%|█▊        | 30/166 [02:04<09:36,  4.24s/it]avg_loss = 3.3882232942888812:  18%|█▊        | 30/166 [02:09<09:36,  4.24s/it]avg_loss = 3.3882232942888812:  19%|█▊        | 31/166 [02:09<09:27,  4.21s/it]avg_loss = 3.394848607480526:  19%|█▊        | 31/166 [02:13<09:27,  4.21s/it] avg_loss = 3.394848607480526:  19%|█▉        | 32/166 [02:13<09:26,  4.23s/it]avg_loss = 3.4043250661907773:  19%|█▉        | 32/166 [02:17<09:26,  4.23s/it]avg_loss = 3.4043250661907773:  20%|█▉        | 33/166 [02:17<09:25,  4.25s/it]avg_loss = 3.408801513559678:  20%|█▉        | 33/166 [02:21<09:25,  4.25s/it] avg_loss = 3.408801513559678:  20%|██        | 34/166 [02:21<09:18,  4.23s/it]avg_loss = 3.399699068069458:  20%|██        | 34/166 [02:26<09:18,  4.23s/it]avg_loss = 3.399699068069458:  21%|██        | 35/166 [02:26<09:14,  4.23s/it]avg_loss = 3.393835266431173:  21%|██        | 35/166 [02:30<09:14,  4.23s/it]avg_loss = 3.393835266431173:  22%|██▏       | 36/166 [02:30<09:08,  4.22s/it]avg_loss = 3.3832670611304207:  22%|██▏       | 36/166 [02:34<09:08,  4.22s/it]avg_loss = 3.3832670611304207:  22%|██▏       | 37/166 [02:34<09:08,  4.25s/it]avg_loss = 3.381737621206986:  22%|██▏       | 37/166 [02:38<09:08,  4.25s/it] avg_loss = 3.381737621206986:  23%|██▎       | 38/166 [02:38<09:03,  4.25s/it]avg_loss = 3.378821073434292:  23%|██▎       | 38/166 [02:42<09:03,  4.25s/it]avg_loss = 3.378821073434292:  23%|██▎       | 39/166 [02:42<08:56,  4.22s/it]avg_loss = 3.379743218421936:  23%|██▎       | 39/166 [02:47<08:56,  4.22s/it]avg_loss = 3.379743218421936:  24%|██▍       | 40/166 [02:47<08:52,  4.22s/it]avg_loss = 3.377929867767706:  24%|██▍       | 40/166 [02:51<08:52,  4.22s/it]avg_loss = 3.377929867767706:  25%|██▍       | 41/166 [02:51<08:48,  4.23s/it]avg_loss = 3.3608951568603516:  25%|██▍       | 41/166 [02:55<08:48,  4.23s/it]avg_loss = 3.3608951568603516:  25%|██▌       | 42/166 [02:55<08:41,  4.20s/it]avg_loss = 3.345689751381098:  25%|██▌       | 42/166 [02:59<08:41,  4.20s/it] avg_loss = 3.345689751381098:  26%|██▌       | 43/166 [02:59<08:36,  4.20s/it]avg_loss = 3.3363129279830237:  26%|██▌       | 43/166 [03:04<08:36,  4.20s/it]avg_loss = 3.3363129279830237:  27%|██▋       | 44/166 [03:04<08:35,  4.22s/it]avg_loss = 3.3167567782931857:  27%|██▋       | 44/166 [03:08<08:35,  4.22s/it]avg_loss = 3.3167567782931857:  27%|██▋       | 45/166 [03:08<08:34,  4.25s/it]avg_loss = 3.305378286734871:  27%|██▋       | 45/166 [03:12<08:34,  4.25s/it] avg_loss = 3.305378286734871:  28%|██▊       | 46/166 [03:12<08:28,  4.24s/it]avg_loss = 3.2991794626763524:  28%|██▊       | 46/166 [03:16<08:28,  4.24s/it]avg_loss = 3.2991794626763524:  28%|██▊       | 47/166 [03:16<08:24,  4.24s/it]avg_loss = 3.297402565677961:  28%|██▊       | 47/166 [03:21<08:24,  4.24s/it] avg_loss = 3.297402565677961:  29%|██▉       | 48/166 [03:21<08:19,  4.24s/it]avg_loss = 3.310457404778928:  29%|██▉       | 48/166 [03:25<08:19,  4.24s/it]avg_loss = 3.310457404778928:  30%|██▉       | 49/166 [03:25<08:16,  4.24s/it]avg_loss = 3.3218128967285154:  30%|██▉       | 49/166 [03:29<08:16,  4.24s/it]avg_loss = 3.3218128967285154:  30%|███       | 50/166 [03:29<08:10,  4.23s/it]avg_loss = 3.3318337178697774:  30%|███       | 50/166 [03:33<08:10,  4.23s/it]avg_loss = 3.3318337178697774:  31%|███       | 51/166 [03:33<08:02,  4.20s/it]avg_loss = 3.339107330028827:  31%|███       | 51/166 [03:37<08:02,  4.20s/it] avg_loss = 3.339107330028827:  31%|███▏      | 52/166 [03:37<08:03,  4.24s/it]avg_loss = 3.3416596043784663:  31%|███▏      | 52/166 [03:42<08:03,  4.24s/it]avg_loss = 3.3416596043784663:  32%|███▏      | 53/166 [03:42<07:55,  4.21s/it]avg_loss = 3.33942241580398:  32%|███▏      | 53/166 [03:46<07:55,  4.21s/it]  avg_loss = 3.33942241580398:  33%|███▎      | 54/166 [03:46<07:50,  4.20s/it]avg_loss = 3.3411400838331744:  33%|███▎      | 54/166 [03:50<07:50,  4.20s/it]avg_loss = 3.3411400838331744:  33%|███▎      | 55/166 [03:50<07:49,  4.23s/it]avg_loss = 3.3430277577468326:  33%|███▎      | 55/166 [03:54<07:49,  4.23s/it]avg_loss = 3.3430277577468326:  34%|███▎      | 56/166 [03:54<07:46,  4.24s/it]avg_loss = 3.332248143982469:  34%|███▎      | 56/166 [03:59<07:46,  4.24s/it] avg_loss = 3.332248143982469:  34%|███▍      | 57/166 [03:59<07:40,  4.23s/it]avg_loss = 3.333375096321106:  34%|███▍      | 57/166 [04:03<07:40,  4.23s/it]avg_loss = 3.333375096321106:  35%|███▍      | 58/166 [04:03<07:34,  4.21s/it]avg_loss = 3.3282825744758218:  35%|███▍      | 58/166 [04:07<07:34,  4.21s/it]avg_loss = 3.3282825744758218:  36%|███▌      | 59/166 [04:07<07:33,  4.24s/it]avg_loss = 3.322874895731608:  36%|███▌      | 59/166 [04:11<07:33,  4.24s/it] avg_loss = 3.322874895731608:  36%|███▌      | 60/166 [04:11<07:29,  4.24s/it]avg_loss = 3.3186320750439755:  36%|███▌      | 60/166 [04:16<07:29,  4.24s/it]avg_loss = 3.3186320750439755:  37%|███▋      | 61/166 [04:16<07:28,  4.27s/it]avg_loss = 3.3166691833926785:  37%|███▋      | 61/166 [04:20<07:28,  4.27s/it]avg_loss = 3.3166691833926785:  37%|███▋      | 62/166 [04:20<07:20,  4.24s/it]avg_loss = 3.3124512869214255:  37%|███▋      | 62/166 [04:24<07:20,  4.24s/it]avg_loss = 3.3124512869214255:  38%|███▊      | 63/166 [04:24<07:18,  4.26s/it]avg_loss = 3.309878211468458:  38%|███▊      | 63/166 [04:28<07:18,  4.26s/it] avg_loss = 3.309878211468458:  39%|███▊      | 64/166 [04:28<07:12,  4.24s/it]avg_loss = 3.3033162337083084:  39%|███▊      | 64/166 [04:32<07:12,  4.24s/it]avg_loss = 3.3033162337083084:  39%|███▉      | 65/166 [04:32<07:07,  4.23s/it]avg_loss = 3.295990759676153:  39%|███▉      | 65/166 [04:37<07:07,  4.23s/it] avg_loss = 3.295990759676153:  40%|███▉      | 66/166 [04:37<07:06,  4.27s/it]avg_loss = 3.2936006553137482:  40%|███▉      | 66/166 [04:41<07:06,  4.27s/it]avg_loss = 3.2936006553137482:  40%|████      | 67/166 [04:41<07:00,  4.24s/it]avg_loss = 3.2924540393492756:  40%|████      | 67/166 [04:45<07:00,  4.24s/it]avg_loss = 3.2924540393492756:  41%|████      | 68/166 [04:45<06:41,  4.09s/it]avg_loss = 3.296860214592754:  41%|████      | 68/166 [04:47<06:41,  4.09s/it] avg_loss = 3.296860214592754:  42%|████▏     | 69/166 [04:47<05:49,  3.61s/it]avg_loss = 3.302369182450431:  42%|████▏     | 69/166 [04:49<05:49,  3.61s/it]avg_loss = 3.302369182450431:  42%|████▏     | 70/166 [04:49<05:05,  3.19s/it]avg_loss = 3.308339233129797:  42%|████▏     | 70/166 [04:52<05:05,  3.19s/it]avg_loss = 3.308339233129797:  43%|████▎     | 71/166 [04:52<04:33,  2.88s/it]avg_loss = 3.3138267861472235:  43%|████▎     | 71/166 [04:54<04:33,  2.88s/it]avg_loss = 3.3138267861472235:  43%|████▎     | 72/166 [04:54<04:14,  2.70s/it]avg_loss = 3.3221365295044363:  43%|████▎     | 72/166 [04:56<04:14,  2.70s/it]avg_loss = 3.3221365295044363:  44%|████▍     | 73/166 [04:56<03:59,  2.57s/it]avg_loss = 3.3168068028785087:  44%|████▍     | 73/166 [04:59<03:59,  2.57s/it]avg_loss = 3.3168068028785087:  45%|████▍     | 74/166 [04:59<03:51,  2.52s/it]avg_loss = 3.315371437072754:  45%|████▍     | 74/166 [05:01<03:51,  2.52s/it] avg_loss = 3.315371437072754:  45%|████▌     | 75/166 [05:01<03:41,  2.44s/it]avg_loss = 3.3177914525333203:  45%|████▌     | 75/166 [05:03<03:41,  2.44s/it]avg_loss = 3.3177914525333203:  46%|████▌     | 76/166 [05:03<03:31,  2.35s/it]avg_loss = 3.3156285564620775:  46%|████▌     | 76/166 [05:05<03:31,  2.35s/it]avg_loss = 3.3156285564620775:  46%|████▋     | 77/166 [05:05<03:20,  2.26s/it]avg_loss = 3.3141671969340396:  46%|████▋     | 77/166 [05:07<03:20,  2.26s/it]avg_loss = 3.3141671969340396:  47%|████▋     | 78/166 [05:07<03:16,  2.23s/it]avg_loss = 3.3111096333853807:  47%|████▋     | 78/166 [05:09<03:16,  2.23s/it]avg_loss = 3.3111096333853807:  48%|████▊     | 79/166 [05:09<03:11,  2.20s/it]avg_loss = 3.308841359615326:  48%|████▊     | 79/166 [05:11<03:11,  2.20s/it] avg_loss = 3.308841359615326:  48%|████▊     | 80/166 [05:11<03:07,  2.18s/it]avg_loss = 3.30075364937017:  48%|████▊     | 80/166 [05:14<03:07,  2.18s/it] avg_loss = 3.30075364937017:  49%|████▉     | 81/166 [05:14<03:06,  2.19s/it]avg_loss = 3.3014228140435566:  49%|████▉     | 81/166 [05:16<03:06,  2.19s/it]avg_loss = 3.3014228140435566:  49%|████▉     | 82/166 [05:16<02:58,  2.13s/it]avg_loss = 3.304170355739364:  49%|████▉     | 82/166 [05:18<02:58,  2.13s/it] avg_loss = 3.304170355739364:  50%|█████     | 83/166 [05:18<02:54,  2.10s/it]avg_loss = 3.309186577796936:  50%|█████     | 83/166 [05:20<02:54,  2.10s/it]avg_loss = 3.309186577796936:  51%|█████     | 84/166 [05:20<02:51,  2.09s/it]avg_loss = 3.3125711188596836:  51%|█████     | 84/166 [05:22<02:51,  2.09s/it]avg_loss = 3.3125711188596836:  51%|█████     | 85/166 [05:22<02:49,  2.09s/it]avg_loss = 3.3099371527516563:  51%|█████     | 85/166 [05:24<02:49,  2.09s/it]avg_loss = 3.3099371527516563:  52%|█████▏    | 86/166 [05:24<02:51,  2.15s/it]avg_loss = 3.3083365484215745:  52%|█████▏    | 86/166 [05:26<02:51,  2.15s/it]avg_loss = 3.3083365484215745:  52%|█████▏    | 87/166 [05:26<02:52,  2.19s/it]avg_loss = 3.306944803758101:  52%|█████▏    | 87/166 [05:29<02:52,  2.19s/it] avg_loss = 3.306944803758101:  53%|█████▎    | 88/166 [05:29<02:50,  2.19s/it]avg_loss = 3.307227105237125:  53%|█████▎    | 88/166 [05:31<02:50,  2.19s/it]avg_loss = 3.307227105237125:  54%|█████▎    | 89/166 [05:31<02:46,  2.16s/it]avg_loss = 3.3092026631037395:  54%|█████▎    | 89/166 [05:33<02:46,  2.16s/it]avg_loss = 3.3092026631037395:  54%|█████▍    | 90/166 [05:33<02:42,  2.14s/it]avg_loss = 3.3077502224471544:  54%|█████▍    | 90/166 [05:35<02:42,  2.14s/it]avg_loss = 3.3077502224471544:  55%|█████▍    | 91/166 [05:35<02:40,  2.14s/it]avg_loss = 3.3086879460707954:  55%|█████▍    | 91/166 [05:37<02:40,  2.14s/it]avg_loss = 3.3086879460707954:  55%|█████▌    | 92/166 [05:37<02:42,  2.19s/it]avg_loss = 3.31425731156462:  55%|█████▌    | 92/166 [05:39<02:42,  2.19s/it]  avg_loss = 3.31425731156462:  56%|█████▌    | 93/166 [05:39<02:38,  2.17s/it]avg_loss = 3.308597166487511:  56%|█████▌    | 93/166 [05:41<02:38,  2.17s/it]avg_loss = 3.308597166487511:  57%|█████▋    | 94/166 [05:41<02:35,  2.16s/it]avg_loss = 3.3043172961787173:  57%|█████▋    | 94/166 [05:44<02:35,  2.16s/it]avg_loss = 3.3043172961787173:  57%|█████▋    | 95/166 [05:44<02:33,  2.16s/it]avg_loss = 3.3017509107788405:  57%|█████▋    | 95/166 [05:46<02:33,  2.16s/it]avg_loss = 3.3017509107788405:  58%|█████▊    | 96/166 [05:46<02:31,  2.16s/it]avg_loss = 3.300215730962065:  58%|█████▊    | 96/166 [05:48<02:31,  2.16s/it] avg_loss = 3.300215730962065:  58%|█████▊    | 97/166 [05:48<02:28,  2.15s/it]avg_loss = 3.2979954359482746:  58%|█████▊    | 97/166 [05:50<02:28,  2.15s/it]avg_loss = 3.2979954359482746:  59%|█████▉    | 98/166 [05:50<02:24,  2.13s/it]avg_loss = 3.2952202088905103:  59%|█████▉    | 98/166 [05:52<02:24,  2.13s/it]avg_loss = 3.2952202088905103:  60%|█████▉    | 99/166 [05:52<02:21,  2.11s/it]avg_loss = 3.2950221705436706:  60%|█████▉    | 99/166 [05:54<02:21,  2.11s/it]avg_loss = 3.2950221705436706:  60%|██████    | 100/166 [05:54<02:21,  2.14s/it]avg_loss = 3.295883587091276:  60%|██████    | 100/166 [05:56<02:21,  2.14s/it] avg_loss = 3.295883587091276:  61%|██████    | 101/166 [05:56<02:17,  2.11s/it]avg_loss = 3.2972366973465563:  61%|██████    | 101/166 [05:58<02:17,  2.11s/it]avg_loss = 3.2972366973465563:  61%|██████▏   | 102/166 [05:58<02:15,  2.11s/it]avg_loss = 3.301051919900098:  61%|██████▏   | 102/166 [06:01<02:15,  2.11s/it] avg_loss = 3.301051919900098:  62%|██████▏   | 103/166 [06:01<02:13,  2.12s/it]avg_loss = 3.306406484200404:  62%|██████▏   | 103/166 [06:03<02:13,  2.12s/it]avg_loss = 3.306406484200404:  63%|██████▎   | 104/166 [06:03<02:10,  2.11s/it]avg_loss = 3.313835216703869:  63%|██████▎   | 104/166 [06:05<02:10,  2.11s/it]avg_loss = 3.313835216703869:  63%|██████▎   | 105/166 [06:05<02:07,  2.09s/it]avg_loss = 3.3193218820499926:  63%|██████▎   | 105/166 [06:07<02:07,  2.09s/it]avg_loss = 3.3193218820499926:  64%|██████▍   | 106/166 [06:07<02:05,  2.09s/it]avg_loss = 3.3239242896855434:  64%|██████▍   | 106/166 [06:09<02:05,  2.09s/it]avg_loss = 3.3239242896855434:  64%|██████▍   | 107/166 [06:09<02:03,  2.09s/it]avg_loss = 3.328053319895709:  64%|██████▍   | 107/166 [06:11<02:03,  2.09s/it] avg_loss = 3.328053319895709:  65%|██████▌   | 108/166 [06:11<02:00,  2.08s/it]avg_loss = 3.3346198326950773:  65%|██████▌   | 108/166 [06:13<02:00,  2.08s/it]avg_loss = 3.3346198326950773:  66%|██████▌   | 109/166 [06:13<01:59,  2.10s/it]avg_loss = 3.33873131275177:  66%|██████▌   | 109/166 [06:15<01:59,  2.10s/it]  avg_loss = 3.33873131275177:  66%|██████▋   | 110/166 [06:15<01:58,  2.11s/it]avg_loss = 3.3379788033597104:  66%|██████▋   | 110/166 [06:17<01:58,  2.11s/it]avg_loss = 3.3379788033597104:  67%|██████▋   | 111/166 [06:17<01:55,  2.11s/it]avg_loss = 3.3392281596149718:  67%|██████▋   | 111/166 [06:19<01:55,  2.11s/it]avg_loss = 3.3392281596149718:  67%|██████▋   | 112/166 [06:19<01:53,  2.09s/it]avg_loss = 3.3386338326783305:  67%|██████▋   | 112/166 [06:22<01:53,  2.09s/it]avg_loss = 3.3386338326783305:  68%|██████▊   | 113/166 [06:22<01:51,  2.11s/it]avg_loss = 3.3406663003720736:  68%|██████▊   | 113/166 [06:24<01:51,  2.11s/it]avg_loss = 3.3406663003720736:  69%|██████▊   | 114/166 [06:24<01:50,  2.13s/it]avg_loss = 3.3406136533488398:  69%|██████▊   | 114/166 [06:26<01:50,  2.13s/it]avg_loss = 3.3406136533488398:  69%|██████▉   | 115/166 [06:26<01:49,  2.14s/it]avg_loss = 3.3408051174262474:  69%|██████▉   | 115/166 [06:28<01:49,  2.14s/it]avg_loss = 3.3408051174262474:  70%|██████▉   | 116/166 [06:28<01:46,  2.12s/it]avg_loss = 3.342638227674696:  70%|██████▉   | 116/166 [06:30<01:46,  2.12s/it] avg_loss = 3.342638227674696:  70%|███████   | 117/166 [06:30<01:44,  2.13s/it]avg_loss = 3.3433204222533663:  70%|███████   | 117/166 [06:32<01:44,  2.13s/it]avg_loss = 3.3433204222533663:  71%|███████   | 118/166 [06:32<01:41,  2.12s/it]avg_loss = 3.345413316197756:  71%|███████   | 118/166 [06:34<01:41,  2.12s/it] avg_loss = 3.345413316197756:  72%|███████▏  | 119/166 [06:34<01:41,  2.17s/it]avg_loss = 3.3469768087069194:  72%|███████▏  | 119/166 [06:37<01:41,  2.17s/it]avg_loss = 3.3469768087069194:  72%|███████▏  | 120/166 [06:37<01:39,  2.17s/it]avg_loss = 3.347709980877963:  72%|███████▏  | 120/166 [06:39<01:39,  2.17s/it] avg_loss = 3.347709980877963:  73%|███████▎  | 121/166 [06:39<01:37,  2.16s/it]avg_loss = 3.3492527613874343:  73%|███████▎  | 121/166 [06:41<01:37,  2.16s/it]avg_loss = 3.3492527613874343:  73%|███████▎  | 122/166 [06:41<01:33,  2.13s/it]avg_loss = 3.3496050059310787:  73%|███████▎  | 122/166 [06:43<01:33,  2.13s/it]avg_loss = 3.3496050059310787:  74%|███████▍  | 123/166 [06:43<01:30,  2.10s/it]avg_loss = 3.3488988549478593:  74%|███████▍  | 123/166 [06:45<01:30,  2.10s/it]avg_loss = 3.3488988549478593:  75%|███████▍  | 124/166 [06:45<01:28,  2.11s/it]avg_loss = 3.34637068939209:  75%|███████▍  | 124/166 [06:47<01:28,  2.11s/it]  avg_loss = 3.34637068939209:  75%|███████▌  | 125/166 [06:47<01:26,  2.11s/it]avg_loss = 3.3452819926398143:  75%|███████▌  | 125/166 [06:49<01:26,  2.11s/it]avg_loss = 3.3452819926398143:  76%|███████▌  | 126/166 [06:49<01:24,  2.10s/it]avg_loss = 3.34189180877265:  76%|███████▌  | 126/166 [06:51<01:24,  2.10s/it]  avg_loss = 3.34189180877265:  77%|███████▋  | 127/166 [06:51<01:21,  2.10s/it]avg_loss = 3.3415819946676493:  77%|███████▋  | 127/166 [06:53<01:21,  2.10s/it]avg_loss = 3.3415819946676493:  77%|███████▋  | 128/166 [06:53<01:18,  2.07s/it]avg_loss = 3.343324435773746:  77%|███████▋  | 128/166 [06:55<01:18,  2.07s/it] avg_loss = 3.343324435773746:  78%|███████▊  | 129/166 [06:55<01:17,  2.09s/it]avg_loss = 3.3436175713172327:  78%|███████▊  | 129/166 [06:58<01:17,  2.09s/it]avg_loss = 3.3436175713172327:  78%|███████▊  | 130/166 [06:58<01:15,  2.10s/it]avg_loss = 3.343963752266105:  78%|███████▊  | 130/166 [07:00<01:15,  2.10s/it] avg_loss = 3.343963752266105:  79%|███████▉  | 131/166 [07:00<01:13,  2.11s/it]avg_loss = 3.34663020661383:  79%|███████▉  | 131/166 [07:02<01:13,  2.11s/it] avg_loss = 3.34663020661383:  80%|███████▉  | 132/166 [07:02<01:11,  2.11s/it]avg_loss = 3.3488879849139908:  80%|███████▉  | 132/166 [07:04<01:11,  2.11s/it]avg_loss = 3.3488879849139908:  80%|████████  | 133/166 [07:04<01:11,  2.16s/it]avg_loss = 3.351083139875042:  80%|████████  | 133/166 [07:06<01:11,  2.16s/it] avg_loss = 3.351083139875042:  81%|████████  | 134/166 [07:06<01:09,  2.17s/it]avg_loss = 3.3468692267382587:  81%|████████  | 134/166 [07:08<01:09,  2.17s/it]avg_loss = 3.3468692267382587:  81%|████████▏ | 135/166 [07:08<01:06,  2.14s/it]avg_loss = 3.3443864198292004:  81%|████████▏ | 135/166 [07:10<01:06,  2.14s/it]avg_loss = 3.3443864198292004:  82%|████████▏ | 136/166 [07:10<01:04,  2.14s/it]avg_loss = 3.343094996292226:  82%|████████▏ | 136/166 [07:13<01:04,  2.14s/it] avg_loss = 3.343094996292226:  83%|████████▎ | 137/166 [07:13<01:01,  2.12s/it]avg_loss = 3.342815685963285:  83%|████████▎ | 137/166 [07:15<01:01,  2.12s/it]avg_loss = 3.342815685963285:  83%|████████▎ | 138/166 [07:15<00:59,  2.12s/it]avg_loss = 3.3406274661743383:  83%|████████▎ | 138/166 [07:17<00:59,  2.12s/it]avg_loss = 3.3406274661743383:  84%|████████▎ | 139/166 [07:17<00:57,  2.13s/it]avg_loss = 3.3367884295327324:  84%|████████▎ | 139/166 [07:19<00:57,  2.13s/it]avg_loss = 3.3367884295327324:  84%|████████▍ | 140/166 [07:19<00:55,  2.12s/it]avg_loss = 3.33447003364563:  84%|████████▍ | 140/166 [07:21<00:55,  2.12s/it]  avg_loss = 3.33447003364563:  85%|████████▍ | 141/166 [07:21<00:53,  2.12s/it]avg_loss = 3.333912817525192:  85%|████████▍ | 141/166 [07:23<00:53,  2.12s/it]avg_loss = 3.333912817525192:  86%|████████▌ | 142/166 [07:23<00:50,  2.11s/it]avg_loss = 3.3321918640937005:  86%|████████▌ | 142/166 [07:25<00:50,  2.11s/it]avg_loss = 3.3321918640937005:  86%|████████▌ | 143/166 [07:25<00:48,  2.11s/it]avg_loss = 3.333315653933419:  86%|████████▌ | 143/166 [07:27<00:48,  2.11s/it] avg_loss = 3.333315653933419:  87%|████████▋ | 144/166 [07:27<00:46,  2.12s/it]avg_loss = 3.331414273689533:  87%|████████▋ | 144/166 [07:29<00:46,  2.12s/it]avg_loss = 3.331414273689533:  87%|████████▋ | 145/166 [07:29<00:44,  2.10s/it]avg_loss = 3.3309934890433532:  87%|████████▋ | 145/166 [07:32<00:44,  2.10s/it]avg_loss = 3.3309934890433532:  88%|████████▊ | 146/166 [07:32<00:41,  2.09s/it]avg_loss = 3.3275454141655745:  88%|████████▊ | 146/166 [07:34<00:41,  2.09s/it]avg_loss = 3.3275454141655745:  89%|████████▊ | 147/166 [07:34<00:40,  2.11s/it]avg_loss = 3.324804681378442:  89%|████████▊ | 147/166 [07:36<00:40,  2.11s/it] avg_loss = 3.324804681378442:  89%|████████▉ | 148/166 [07:36<00:38,  2.11s/it]avg_loss = 3.323168941792226:  89%|████████▉ | 148/166 [07:38<00:38,  2.11s/it]avg_loss = 3.323168941792226:  90%|████████▉ | 149/166 [07:38<00:35,  2.10s/it]avg_loss = 3.3257578245798745:  90%|████████▉ | 149/166 [07:40<00:35,  2.10s/it]avg_loss = 3.3257578245798745:  90%|█████████ | 150/166 [07:40<00:33,  2.07s/it]avg_loss = 3.325702455659576:  90%|█████████ | 150/166 [07:42<00:33,  2.07s/it] avg_loss = 3.325702455659576:  91%|█████████ | 151/166 [07:42<00:30,  2.06s/it]avg_loss = 3.3250745832920074:  91%|█████████ | 151/166 [07:44<00:30,  2.06s/it]avg_loss = 3.3250745832920074:  92%|█████████▏| 152/166 [07:44<00:29,  2.13s/it]avg_loss = 3.323942323136174:  92%|█████████▏| 152/166 [07:46<00:29,  2.13s/it] avg_loss = 3.323942323136174:  92%|█████████▏| 153/166 [07:46<00:27,  2.15s/it]avg_loss = 3.3261757763949307:  92%|█████████▏| 153/166 [07:48<00:27,  2.15s/it]avg_loss = 3.3261757763949307:  93%|█████████▎| 154/166 [07:48<00:25,  2.13s/it]avg_loss = 3.3248213537277715:  93%|█████████▎| 154/166 [07:51<00:25,  2.13s/it]avg_loss = 3.3248213537277715:  93%|█████████▎| 155/166 [07:51<00:23,  2.12s/it]avg_loss = 3.3247432173826756:  93%|█████████▎| 155/166 [07:53<00:23,  2.12s/it]avg_loss = 3.3247432173826756:  94%|█████████▍| 156/166 [07:53<00:21,  2.11s/it]avg_loss = 3.3212788454286613:  94%|█████████▍| 156/166 [07:55<00:21,  2.11s/it]avg_loss = 3.3212788454286613:  95%|█████████▍| 157/166 [07:55<00:18,  2.10s/it]avg_loss = 3.312188541587395:  95%|█████████▍| 157/166 [07:57<00:18,  2.10s/it] avg_loss = 3.312188541587395:  95%|█████████▌| 158/166 [07:57<00:17,  2.15s/it]avg_loss = 3.313564340273539:  95%|█████████▌| 158/166 [07:59<00:17,  2.15s/it]avg_loss = 3.313564340273539:  96%|█████████▌| 159/166 [07:59<00:14,  2.14s/it]avg_loss = 3.3154035098850727:  96%|█████████▌| 159/166 [08:01<00:14,  2.14s/it]avg_loss = 3.3154035098850727:  96%|█████████▋| 160/166 [08:01<00:12,  2.13s/it]avg_loss = 3.31828815078143:  96%|█████████▋| 160/166 [08:03<00:12,  2.13s/it]  avg_loss = 3.31828815078143:  97%|█████████▋| 161/166 [08:03<00:10,  2.12s/it]avg_loss = 3.3200566363923345:  97%|█████████▋| 161/166 [08:05<00:10,  2.12s/it]avg_loss = 3.3200566363923345:  98%|█████████▊| 162/166 [08:05<00:08,  2.10s/it]avg_loss = 3.3217115585058012:  98%|█████████▊| 162/166 [08:07<00:08,  2.10s/it]avg_loss = 3.3217115585058012:  98%|█████████▊| 163/166 [08:07<00:06,  2.11s/it]avg_loss = 3.3243563938431624:  98%|█████████▊| 163/166 [08:10<00:06,  2.11s/it]avg_loss = 3.3243563938431624:  99%|█████████▉| 164/166 [08:10<00:04,  2.11s/it]avg_loss = 3.3262197183840203:  99%|█████████▉| 164/166 [08:12<00:04,  2.11s/it]avg_loss = 3.3262197183840203:  99%|█████████▉| 165/166 [08:12<00:02,  2.10s/it]avg_loss = 3.3290182718311447:  99%|█████████▉| 165/166 [08:14<00:02,  2.10s/it]avg_loss = 3.3290182718311447: 100%|██████████| 166/166 [08:14<00:00,  2.11s/it]avg_loss = 3.3290182718311447: 100%|██████████| 166/166 [08:14<00:00,  2.98s/it]
I0325 14:23:00.128094 803508 eval_ppl.py:107] wikitext2 perplexity: 27.91092872619629
wikitext2 perplexity: 27.911
