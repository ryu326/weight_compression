I0325 04:04:15.479795 342088 config.py:54] PyTorch version 2.6.0 available.
W0325 04:04:15.769886 342088 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 04:04:16.752324 342088 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.32it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.66it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.85it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.95it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  7.55it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.74it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.72it/s]
I0325 04:04:17.746090 342088 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:14,  2.12it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:14,  2.10it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:13,  2.13it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:12,  2.22it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:11,  2.26it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:11,  2.27it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:10,  2.29it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:10,  2.30it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:03<00:09,  2.31it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:04<00:09,  2.32it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:04<00:09,  2.32it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:05<00:08,  2.33it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:05<00:08,  2.34it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:06<00:07,  2.33it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:06<00:07,  2.32it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:06<00:06,  2.32it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:07<00:06,  2.32it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:07<00:05,  2.36it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:08<00:05,  2.38it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:08<00:04,  2.41it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:09<00:04,  2.43it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:09<00:04,  2.44it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:09<00:03,  2.47it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:10<00:03,  2.44it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:10<00:03,  2.04it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:11<00:03,  1.84it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:12<00:02,  1.82it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:12<00:02,  1.80it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:13<00:01,  1.78it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:13<00:01,  1.82it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:14<00:00,  1.81it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:14<00:00,  1.78it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:14<00:00,  2.13it/s]
I0325 04:04:42.725444 342088 quantize_finetune_llama.py:185] loaded compression model
I0325 04:04:57.640480 342088 quantize_finetune_llama.py:189] loaded dataset and devset
I0325 04:04:59.926190 342088 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 04:06:03.801025 342088 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 63.724414587020874s
tensor(-3.6338e-06) tensor(0.0192)
tensor(0.0192) tensor(-3.6338e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0325 04:06:23.146804 343748 config.py:54] PyTorch version 2.6.0 available.
W0325 04:06:23.435934 343748 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 04:06:24.545178 343748 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 04:06:24.549187 342088 quantize_finetune_llama.py:209] layer 1 gpu 0
I0325 04:06:24.561556 343748 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 04:06:42.324220 343748 finetune.py:45] layer 0_v initial loss 1.7370660998494714e-06
W0325 04:06:42.324404 343748 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 04:07:12.898141 343748 finetune.py:68] layer 0_v @ epoch 0 new loss 8.814375860310975e-07 old loss 1.7370660998494714e-06 BETTER
I0325 04:07:44.796510 343748 finetune.py:68] layer 0_v @ epoch 1 new loss 5.689092290594999e-07 old loss 8.814375860310975e-07 BETTER
I0325 04:08:16.826764 343748 finetune.py:68] layer 0_v @ epoch 2 new loss 4.424371979894204e-07 old loss 5.689092290594999e-07 BETTER
I0325 04:08:48.911735 343748 finetune.py:68] layer 0_v @ epoch 3 new loss 3.8352925457729725e-07 old loss 4.424371979894204e-07 BETTER
I0325 04:09:21.324505 343748 finetune.py:68] layer 0_v @ epoch 4 new loss 3.526099021655682e-07 old loss 3.8352925457729725e-07 BETTER
I0325 04:09:39.343402 343748 finetune.py:45] layer 0_q initial loss 3.599447495616914e-07
I0325 04:10:10.728277 343748 finetune.py:68] layer 0_q @ epoch 0 new loss 3.336136842335691e-07 old loss 3.599447495616914e-07 BETTER
I0325 04:10:42.895764 343748 finetune.py:68] layer 0_q @ epoch 1 new loss 3.1759657304064604e-07 old loss 3.336136842335691e-07 BETTER
I0325 04:11:15.052153 343748 finetune.py:68] layer 0_q @ epoch 2 new loss 3.0555682428712316e-07 old loss 3.1759657304064604e-07 BETTER
I0325 04:11:46.963973 343748 finetune.py:68] layer 0_q @ epoch 3 new loss 2.959640426070109e-07 old loss 3.0555682428712316e-07 BETTER
I0325 04:12:18.954018 343748 finetune.py:68] layer 0_q @ epoch 4 new loss 2.8817962061111757e-07 old loss 2.959640426070109e-07 BETTER
I0325 04:12:37.094480 343748 finetune.py:45] layer 0_k initial loss 3.470867113719578e-07
I0325 04:13:07.530296 343748 finetune.py:68] layer 0_k @ epoch 0 new loss 2.8807659191443236e-07 old loss 3.470867113719578e-07 BETTER
I0325 04:13:38.977445 343748 finetune.py:68] layer 0_k @ epoch 1 new loss 2.814452386701305e-07 old loss 2.8807659191443236e-07 BETTER
I0325 04:14:10.440339 343748 finetune.py:68] layer 0_k @ epoch 2 new loss 2.7608194841377554e-07 old loss 2.814452386701305e-07 BETTER
I0325 04:14:42.010982 343748 finetune.py:68] layer 0_k @ epoch 3 new loss 2.714088509492285e-07 old loss 2.7608194841377554e-07 BETTER
I0325 04:15:13.559229 343748 finetune.py:68] layer 0_k @ epoch 4 new loss 2.675284633824049e-07 old loss 2.714088509492285e-07 BETTER
I0325 04:15:31.847794 343748 finetune.py:45] layer 0_o initial loss 2.9570153401436983e-06
I0325 04:16:01.720622 343748 finetune.py:68] layer 0_o @ epoch 0 new loss 2.744945504673524e-06 old loss 2.9570153401436983e-06 BETTER
I0325 04:16:32.475738 343748 finetune.py:68] layer 0_o @ epoch 1 new loss 2.582114348115283e-06 old loss 2.744945504673524e-06 BETTER
I0325 04:17:03.366525 343748 finetune.py:68] layer 0_o @ epoch 2 new loss 2.4457294784951955e-06 old loss 2.582114348115283e-06 BETTER
I0325 04:17:34.299808 343748 finetune.py:68] layer 0_o @ epoch 3 new loss 2.327735273865983e-06 old loss 2.4457294784951955e-06 BETTER
I0325 04:18:05.431035 343748 finetune.py:68] layer 0_o @ epoch 4 new loss 2.225168145741918e-06 old loss 2.327735273865983e-06 BETTER
I0325 04:18:30.204426 343748 finetune.py:45] layer 0_up initial loss 4.655963493860327e-06
I0325 04:18:58.729102 343748 finetune.py:68] layer 0_up @ epoch 0 new loss 4.374731361167505e-06 old loss 4.655963493860327e-06 BETTER
I0325 04:19:28.044695 343748 finetune.py:68] layer 0_up @ epoch 1 new loss 4.1314638110634405e-06 old loss 4.374731361167505e-06 BETTER
I0325 04:19:57.416417 343748 finetune.py:68] layer 0_up @ epoch 2 new loss 3.927198577002855e-06 old loss 4.1314638110634405e-06 BETTER
I0325 04:20:26.847346 343748 finetune.py:68] layer 0_up @ epoch 3 new loss 3.752399152290309e-06 old loss 3.927198577002855e-06 BETTER
I0325 04:20:56.558464 343748 finetune.py:68] layer 0_up @ epoch 4 new loss 3.6030746741744224e-06 old loss 3.752399152290309e-06 BETTER
I0325 04:21:21.680486 343748 finetune.py:45] layer 0_gate initial loss 7.775416634103749e-06
I0325 04:21:48.540035 343748 finetune.py:68] layer 0_gate @ epoch 0 new loss 7.475214260921348e-06 old loss 7.775416634103749e-06 BETTER
I0325 04:22:16.602562 343748 finetune.py:68] layer 0_gate @ epoch 1 new loss 7.209116120066028e-06 old loss 7.475214260921348e-06 BETTER
I0325 04:22:44.640836 343748 finetune.py:68] layer 0_gate @ epoch 2 new loss 6.967269655433483e-06 old loss 7.209116120066028e-06 BETTER
I0325 04:23:12.794711 343748 finetune.py:68] layer 0_gate @ epoch 3 new loss 6.748388386768056e-06 old loss 6.967269655433483e-06 BETTER
I0325 04:23:40.927986 343748 finetune.py:68] layer 0_gate @ epoch 4 new loss 6.548742931045126e-06 old loss 6.748388386768056e-06 BETTER
I0325 04:24:21.677047 343748 finetune.py:45] layer 0_down initial loss 1.1232435099373106e-05
I0325 04:24:46.797658 343748 finetune.py:68] layer 0_down @ epoch 0 new loss 1.1066938895964995e-05 old loss 1.1232435099373106e-05 BETTER
I0325 04:25:12.887548 343748 finetune.py:68] layer 0_down @ epoch 1 new loss 1.0914781341853086e-05 old loss 1.1066938895964995e-05 BETTER
I0325 04:25:39.049681 343748 finetune.py:68] layer 0_down @ epoch 2 new loss 1.0777534043882042e-05 old loss 1.0914781341853086e-05 BETTER
I0325 04:26:05.861155 343748 finetune.py:68] layer 0_down @ epoch 3 new loss 1.0652037417457905e-05 old loss 1.0777534043882042e-05 BETTER
I0325 04:26:32.220608 343748 finetune.py:68] layer 0_down @ epoch 4 new loss 1.0536910849623382e-05 old loss 1.0652037417457905e-05 BETTER
0_v proxy err 0.04702535644173622 tr(WHW.T) 992.1710205078125
bpp_loss 1.080940238229232
0_q proxy err 8.217752474592999e-05 tr(WHW.T) 636625.75
bpp_loss 1.1965588140010368
0_k proxy err 0.000134253001306206 tr(WHW.T) 398987.875
bpp_loss 1.3115012837515678
0_o proxy err 0.004916534759104252 tr(WHW.T) 15985.8388671875
bpp_loss 1.0131408092565835
0_up proxy err 0.05375251919031143 tr(WHW.T) 24239.228515625
bpp_loss 1.3852670808907512
0_gate proxy err 0.036934684962034225 tr(WHW.T) 35474.75390625
bpp_loss 1.4042314222125813
0_down proxy err 0.03073233552277088 tr(WHW.T) 36298.95703125
bpp_loss 1.4418856540456588
I0325 04:27:38.828344 342088 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 60.70959711074829s
I0325 04:27:42.343287 358765 config.py:54] PyTorch version 2.6.0 available.
W0325 04:27:42.638495 358765 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 04:27:43.547240 358765 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 04:27:43.551334 342088 quantize_finetune_llama.py:209] layer 2 gpu 0
I0325 04:27:43.564763 358765 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 04:28:01.766967 358765 finetune.py:45] layer 1_v initial loss 0.011291095986962318
W0325 04:28:01.767171 358765 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 04:28:33.506033 358765 finetune.py:68] layer 1_v @ epoch 0 new loss 9.32636103243567e-05 old loss 0.011291095986962318 BETTER
I0325 04:29:05.775953 358765 finetune.py:68] layer 1_v @ epoch 1 new loss 7.026742241578177e-05 old loss 9.32636103243567e-05 BETTER
I0325 04:29:38.448883 358765 finetune.py:68] layer 1_v @ epoch 2 new loss 6.0915575886610895e-05 old loss 7.026742241578177e-05 BETTER
I0325 04:30:11.796790 358765 finetune.py:68] layer 1_v @ epoch 3 new loss 5.8336645452072844e-05 old loss 6.0915575886610895e-05 BETTER
I0325 04:30:44.847200 358765 finetune.py:68] layer 1_v @ epoch 4 new loss 5.0809449021471664e-05 old loss 5.8336645452072844e-05 BETTER
I0325 04:31:04.662418 358765 finetune.py:45] layer 1_q initial loss 0.00014277518494054675
I0325 04:31:36.331042 358765 finetune.py:68] layer 1_q @ epoch 0 new loss 9.548753587296233e-05 old loss 0.00014277518494054675 BETTER
I0325 04:32:08.753665 358765 finetune.py:68] layer 1_q @ epoch 1 new loss 8.163288293872029e-05 old loss 9.548753587296233e-05 BETTER
I0325 04:32:41.506598 358765 finetune.py:68] layer 1_q @ epoch 2 new loss 7.001083577051759e-05 old loss 8.163288293872029e-05 BETTER
I0325 04:33:13.861371 358765 finetune.py:68] layer 1_q @ epoch 3 new loss 6.463695171987638e-05 old loss 7.001083577051759e-05 BETTER
I0325 04:33:46.947255 358765 finetune.py:76] layer 1_q @ epoch 4 new loss 6.698593642795458e-05 old loss 6.463695171987638e-05 WORSE
I0325 04:34:06.260531 358765 finetune.py:45] layer 1_k initial loss 0.0001073165112757124
I0325 04:34:38.336019 358765 finetune.py:68] layer 1_k @ epoch 0 new loss 9.167884854832664e-05 old loss 0.0001073165112757124 BETTER
I0325 04:35:10.222817 358765 finetune.py:68] layer 1_k @ epoch 1 new loss 8.655222336528823e-05 old loss 9.167884854832664e-05 BETTER
I0325 04:35:42.745047 358765 finetune.py:68] layer 1_k @ epoch 2 new loss 7.86627279012464e-05 old loss 8.655222336528823e-05 BETTER
I0325 04:36:14.981053 358765 finetune.py:68] layer 1_k @ epoch 3 new loss 7.851613918319345e-05 old loss 7.86627279012464e-05 BETTER
I0325 04:36:47.844639 358765 finetune.py:68] layer 1_k @ epoch 4 new loss 7.237923273351043e-05 old loss 7.851613918319345e-05 BETTER
I0325 04:37:07.118279 358765 finetune.py:45] layer 1_o initial loss 0.005288093350827694
I0325 04:37:37.910318 358765 finetune.py:68] layer 1_o @ epoch 0 new loss 0.004271632991731167 old loss 0.005288093350827694 BETTER
I0325 04:38:09.327282 358765 finetune.py:68] layer 1_o @ epoch 1 new loss 0.0034514295402914286 old loss 0.004271632991731167 BETTER
I0325 04:38:41.254918 358765 finetune.py:68] layer 1_o @ epoch 2 new loss 0.0027183082420378923 old loss 0.0034514295402914286 BETTER
I0325 04:39:12.686026 358765 finetune.py:68] layer 1_o @ epoch 3 new loss 0.0021400402765721083 old loss 0.0027183082420378923 BETTER
I0325 04:39:44.340490 358765 finetune.py:68] layer 1_o @ epoch 4 new loss 0.0016928730765357614 old loss 0.0021400402765721083 BETTER
I0325 04:40:10.694102 358765 finetune.py:45] layer 1_up initial loss 0.0020338327158242464
I0325 04:40:39.580824 358765 finetune.py:68] layer 1_up @ epoch 0 new loss 0.0016204261919483542 old loss 0.0020338327158242464 BETTER
I0325 04:41:09.409895 358765 finetune.py:68] layer 1_up @ epoch 1 new loss 0.0012762983096763492 old loss 0.0016204261919483542 BETTER
I0325 04:41:40.216901 358765 finetune.py:68] layer 1_up @ epoch 2 new loss 0.0010221777483820915 old loss 0.0012762983096763492 BETTER
I0325 04:42:10.409173 358765 finetune.py:68] layer 1_up @ epoch 3 new loss 0.0008187628700397909 old loss 0.0010221777483820915 BETTER
I0325 04:42:40.526693 358765 finetune.py:68] layer 1_up @ epoch 4 new loss 0.0006700478261336684 old loss 0.0008187628700397909 BETTER
I0325 04:43:07.442463 358765 finetune.py:45] layer 1_gate initial loss 0.0006894170073792338
I0325 04:43:35.040693 358765 finetune.py:68] layer 1_gate @ epoch 0 new loss 0.000572345859836787 old loss 0.0006894170073792338 BETTER
I0325 04:44:03.384734 358765 finetune.py:68] layer 1_gate @ epoch 1 new loss 0.0004960836376994848 old loss 0.000572345859836787 BETTER
I0325 04:44:31.385381 358765 finetune.py:68] layer 1_gate @ epoch 2 new loss 0.00044880033237859607 old loss 0.0004960836376994848 BETTER
I0325 04:45:00.135547 358765 finetune.py:68] layer 1_gate @ epoch 3 new loss 0.00042009330354630947 old loss 0.00044880033237859607 BETTER
I0325 04:45:29.582168 358765 finetune.py:68] layer 1_gate @ epoch 4 new loss 0.0004018955514766276 old loss 0.00042009330354630947 BETTER
I0325 04:46:13.126872 358765 finetune.py:45] layer 1_down initial loss 0.0005153848323971033
I0325 04:46:38.975568 358765 finetune.py:68] layer 1_down @ epoch 0 new loss 0.0004936523619107902 old loss 0.0005153848323971033 BETTER
I0325 04:47:05.734602 358765 finetune.py:68] layer 1_down @ epoch 1 new loss 0.00048162942402996123 old loss 0.0004936523619107902 BETTER
I0325 04:47:32.743673 358765 finetune.py:68] layer 1_down @ epoch 2 new loss 0.00047540327068418264 old loss 0.00048162942402996123 BETTER
I0325 04:47:59.973255 358765 finetune.py:68] layer 1_down @ epoch 3 new loss 0.0004708303895313293 old loss 0.00047540327068418264 BETTER
I0325 04:48:27.265499 358765 finetune.py:68] layer 1_down @ epoch 4 new loss 0.0004656222299672663 old loss 0.0004708303895313293 BETTER
1_v proxy err 0.11918226629495621 tr(WHW.T) 673.7838745117188
bpp_loss 1.0640344549465226
1_q proxy err 0.0005497366073541343 tr(WHW.T) 195564.953125
bpp_loss 1.8009818216232816
1_k proxy err 0.0005417400971055031 tr(WHW.T) 204635.421875
bpp_loss 1.8165733599598752
1_o proxy err 0.0873604267835617 tr(WHW.T) 4051.003662109375
bpp_loss 1.0533251135639148
1_up proxy err 0.07685291022062302 tr(WHW.T) 23420.671875
bpp_loss 1.4557383511388717
1_gate proxy err 0.03912389650940895 tr(WHW.T) 47308.640625
bpp_loss 1.531347337828646
1_down proxy err 0.000925425672903657 tr(WHW.T) 41078.09765625
bpp_loss 1.3752354278478252
I0325 04:49:35.566840 342088 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 60.35919642448425s
I0325 04:49:39.053666 374194 config.py:54] PyTorch version 2.6.0 available.
W0325 04:49:39.347268 374194 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 04:49:40.294415 374194 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 04:49:40.298420 342088 quantize_finetune_llama.py:209] layer 3 gpu 0
I0325 04:49:40.311425 374194 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 04:49:59.233289 374194 finetune.py:45] layer 2_v initial loss 8.967424946604297e-05
W0325 04:49:59.233505 374194 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 04:50:31.075788 374194 finetune.py:68] layer 2_v @ epoch 0 new loss 4.7880399506539106e-05 old loss 8.967424946604297e-05 BETTER
I0325 04:51:03.502134 374194 finetune.py:68] layer 2_v @ epoch 1 new loss 3.64266597898677e-05 old loss 4.7880399506539106e-05 BETTER
I0325 04:51:36.061447 374194 finetune.py:68] layer 2_v @ epoch 2 new loss 3.230860966141336e-05 old loss 3.64266597898677e-05 BETTER
I0325 04:52:08.953363 374194 finetune.py:68] layer 2_v @ epoch 3 new loss 3.06151487166062e-05 old loss 3.230860966141336e-05 BETTER
I0325 04:52:41.702944 374194 finetune.py:68] layer 2_v @ epoch 4 new loss 2.9813923902111128e-05 old loss 3.06151487166062e-05 BETTER
I0325 04:53:00.384005 374194 finetune.py:45] layer 2_q initial loss 3.350705810589716e-05
I0325 04:53:31.877087 374194 finetune.py:68] layer 2_q @ epoch 0 new loss 3.216182085452601e-05 old loss 3.350705810589716e-05 BETTER
I0325 04:54:04.128626 374194 finetune.py:68] layer 2_q @ epoch 1 new loss 3.172152355546132e-05 old loss 3.216182085452601e-05 BETTER
I0325 04:54:36.479721 374194 finetune.py:68] layer 2_q @ epoch 2 new loss 3.139331965940073e-05 old loss 3.172152355546132e-05 BETTER
I0325 04:55:08.509246 374194 finetune.py:68] layer 2_q @ epoch 3 new loss 3.11156727548223e-05 old loss 3.139331965940073e-05 BETTER
I0325 04:55:40.979154 374194 finetune.py:68] layer 2_q @ epoch 4 new loss 3.087491131736897e-05 old loss 3.11156727548223e-05 BETTER
I0325 04:55:59.648695 374194 finetune.py:45] layer 2_k initial loss 3.4138603950850666e-05
I0325 04:56:30.882280 374194 finetune.py:68] layer 2_k @ epoch 0 new loss 3.3791380701586604e-05 old loss 3.4138603950850666e-05 BETTER
I0325 04:57:02.693188 374194 finetune.py:68] layer 2_k @ epoch 1 new loss 3.356131855980493e-05 old loss 3.3791380701586604e-05 BETTER
I0325 04:57:34.704791 374194 finetune.py:68] layer 2_k @ epoch 2 new loss 3.33527714246884e-05 old loss 3.356131855980493e-05 BETTER
I0325 04:58:06.823557 374194 finetune.py:68] layer 2_k @ epoch 3 new loss 3.31609153363388e-05 old loss 3.33527714246884e-05 BETTER
I0325 04:58:38.872800 374194 finetune.py:68] layer 2_k @ epoch 4 new loss 3.298131196061149e-05 old loss 3.31609153363388e-05 BETTER
I0325 04:58:57.193140 374194 finetune.py:45] layer 2_o initial loss 0.00011616607662290335
I0325 04:59:27.546952 374194 finetune.py:68] layer 2_o @ epoch 0 new loss 0.00011224628542549908 old loss 0.00011616607662290335 BETTER
I0325 04:59:58.759800 374194 finetune.py:68] layer 2_o @ epoch 1 new loss 0.0001091159283532761 old loss 0.00011224628542549908 BETTER
I0325 05:00:30.070403 374194 finetune.py:68] layer 2_o @ epoch 2 new loss 0.000106578525446821 old loss 0.0001091159283532761 BETTER
I0325 05:01:01.360022 374194 finetune.py:68] layer 2_o @ epoch 3 new loss 0.00010448848479427397 old loss 0.000106578525446821 BETTER
