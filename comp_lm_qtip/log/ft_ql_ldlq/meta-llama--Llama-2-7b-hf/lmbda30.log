I0325 05:01:56.534296 382969 config.py:54] PyTorch version 2.6.0 available.
W0325 05:01:56.825226 382969 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:01:57.730436 382969 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.04it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.48it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.77it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.91it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.02it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.57it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.65it/s]
I0325 05:01:58.723249 382969 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:14,  2.13it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:14,  2.12it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:13,  2.13it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:13,  2.14it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:12,  2.15it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:12,  2.13it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:11,  2.12it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:11,  2.11it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:10,  2.10it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:04<00:10,  2.11it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:09,  2.18it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:05<00:08,  2.23it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:05<00:08,  2.27it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:06<00:07,  2.30it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:06<00:07,  2.32it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:07<00:06,  2.33it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:07<00:06,  2.35it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:08<00:05,  2.36it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:08<00:05,  2.36it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:08<00:05,  2.36it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:09<00:04,  2.36it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:09<00:04,  2.37it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:10<00:03,  2.37it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:10<00:03,  2.38it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:11<00:02,  2.38it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:11<00:02,  2.39it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:11<00:02,  2.44it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:12<00:01,  2.45it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:12<00:01,  2.49it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:13<00:00,  2.51it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:13<00:00,  2.52it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:13<00:00,  2.54it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:13<00:00,  2.32it/s]
I0325 05:02:19.894116 382969 quantize_finetune_llama.py:185] loaded compression model
I0325 05:02:34.823139 382969 quantize_finetune_llama.py:189] loaded dataset and devset
I0325 05:02:39.380274 382969 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:03:44.329319 382969 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 64.80348134040833s
tensor(-3.6338e-06) tensor(0.0192)
tensor(0.0192) tensor(-3.6338e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0325 05:04:05.622535 384683 config.py:54] PyTorch version 2.6.0 available.
W0325 05:04:05.910196 384683 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:04:06.872031 384683 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 05:04:06.876106 382969 quantize_finetune_llama.py:209] layer 1 gpu 1
I0325 05:04:06.889175 384683 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:04:26.846997 384683 finetune.py:45] layer 0_v initial loss 4.7346995302177675e-07
W0325 05:04:26.847176 384683 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 05:05:00.536604 384683 finetune.py:68] layer 0_v @ epoch 0 new loss 2.3199343957003293e-07 old loss 4.7346995302177675e-07 BETTER
I0325 05:05:19.857164 382969 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 72.80867075920105s
I0325 05:05:31.496254 385771 config.py:54] PyTorch version 2.6.0 available.
W0325 05:05:31.798634 385771 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:05:32.760340 385771 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 05:05:32.764432 382969 quantize_finetune_llama.py:209] layer 2 gpu 2
I0325 05:05:32.777901 385771 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 05:05:36.757707 384683 finetune.py:68] layer 0_v @ epoch 1 new loss 1.491376480089457e-07 old loss 2.3199343957003293e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:05:54.787981 385771 finetune.py:45] layer 1_v initial loss 0.0006490367813967168
W0325 05:05:54.788766 385771 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 05:06:12.633471 384683 finetune.py:68] layer 0_v @ epoch 2 new loss 1.1807086508497378e-07 old loss 1.491376480089457e-07 BETTER
I0325 05:06:27.793893 385771 finetune.py:68] layer 1_v @ epoch 0 new loss 1.5698111383244395e-05 old loss 0.0006490367813967168 BETTER
I0325 05:06:48.802775 384683 finetune.py:68] layer 0_v @ epoch 3 new loss 1.0398032657121803e-07 old loss 1.1807086508497378e-07 BETTER
I0325 05:06:51.400836 382969 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 78.43814325332642s
I0325 05:07:03.013767 385771 finetune.py:68] layer 1_v @ epoch 1 new loss 1.3655406291945837e-05 old loss 1.5698111383244395e-05 BETTER
I0325 05:07:03.982691 386989 config.py:54] PyTorch version 2.6.0 available.
W0325 05:07:04.280391 386989 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:07:05.250158 386989 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 05:07:05.254539 382969 quantize_finetune_llama.py:209] layer 3 gpu 0
I0325 05:07:05.268032 386989 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:07:23.667097 386989 finetune.py:45] layer 2_v initial loss 3.7097459426149726e-05
W0325 05:07:23.667399 386989 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 05:07:25.372600 384683 finetune.py:68] layer 0_v @ epoch 4 new loss 9.654856825136449e-08 old loss 1.0398032657121803e-07 BETTER
I0325 05:07:36.473697 385771 finetune.py:68] layer 1_v @ epoch 2 new loss 1.3561183550336864e-05 old loss 1.3655406291945837e-05 BETTER
I0325 05:07:45.056853 384683 finetune.py:45] layer 0_q initial loss 9.850041493564277e-08
I0325 05:07:55.147807 386989 finetune.py:68] layer 2_v @ epoch 0 new loss 1.8654960513231345e-05 old loss 3.7097459426149726e-05 BETTER
I0325 05:08:09.618139 385771 finetune.py:68] layer 1_v @ epoch 3 new loss 1.178273669211194e-05 old loss 1.3561183550336864e-05 BETTER
I0325 05:08:18.396528 384683 finetune.py:68] layer 0_q @ epoch 0 new loss 9.186451421783204e-08 old loss 9.850041493564277e-08 BETTER
I0325 05:08:27.544071 386989 finetune.py:68] layer 2_v @ epoch 1 new loss 1.2786397746822331e-05 old loss 1.8654960513231345e-05 BETTER
I0325 05:08:43.128871 385771 finetune.py:68] layer 1_v @ epoch 4 new loss 1.1142959920107387e-05 old loss 1.178273669211194e-05 BETTER
I0325 05:08:52.833880 384683 finetune.py:68] layer 0_q @ epoch 1 new loss 8.798975414947563e-08 old loss 9.186451421783204e-08 BETTER
I0325 05:09:00.434272 386989 finetune.py:68] layer 2_v @ epoch 2 new loss 1.0663549801392946e-05 old loss 1.2786397746822331e-05 BETTER
I0325 05:09:03.719474 385771 finetune.py:45] layer 1_q initial loss 1.814206552808173e-05
I0325 05:09:27.510565 384683 finetune.py:68] layer 0_q @ epoch 2 new loss 8.506818005571404e-08 old loss 8.798975414947563e-08 BETTER
I0325 05:09:33.816959 386989 finetune.py:68] layer 2_v @ epoch 3 new loss 9.825530469242949e-06 old loss 1.0663549801392946e-05 BETTER
I0325 05:09:35.500444 385771 finetune.py:68] layer 1_q @ epoch 0 new loss 1.3158422007109039e-05 old loss 1.814206552808173e-05 BETTER
I0325 05:10:02.304695 384683 finetune.py:68] layer 0_q @ epoch 3 new loss 8.282657404379279e-08 old loss 8.506818005571404e-08 BETTER
I0325 05:10:07.700562 386989 finetune.py:68] layer 2_v @ epoch 4 new loss 9.461428817303386e-06 old loss 9.825530469242949e-06 BETTER
I0325 05:10:08.641934 385771 finetune.py:76] layer 1_q @ epoch 1 new loss 1.5206316675175913e-05 old loss 1.3158422007109039e-05 WORSE
I0325 05:10:27.771251 386989 finetune.py:45] layer 2_q initial loss 1.0324411050532945e-05
I0325 05:10:37.038450 384683 finetune.py:68] layer 0_q @ epoch 4 new loss 8.093233816452994e-08 old loss 8.282657404379279e-08 BETTER
I0325 05:10:41.353154 385771 finetune.py:68] layer 1_q @ epoch 2 new loss 1.213505402120063e-05 old loss 1.3158422007109039e-05 BETTER
I0325 05:10:57.586980 384683 finetune.py:45] layer 0_k initial loss 8.670966877843966e-08
I0325 05:10:59.849524 386989 finetune.py:68] layer 2_q @ epoch 0 new loss 9.975413377105724e-06 old loss 1.0324411050532945e-05 BETTER
I0325 05:11:14.298902 385771 finetune.py:76] layer 1_q @ epoch 3 new loss 1.3467700227920432e-05 old loss 1.213505402120063e-05 WORSE
I0325 05:11:31.327277 384683 finetune.py:68] layer 0_k @ epoch 0 new loss 8.084104052841212e-08 old loss 8.670966877843966e-08 BETTER
I0325 05:11:32.619515 386989 finetune.py:68] layer 2_q @ epoch 1 new loss 9.845678505371325e-06 old loss 9.975413377105724e-06 BETTER
I0325 05:11:47.216664 385771 finetune.py:68] layer 1_q @ epoch 4 new loss 1.1304777217446826e-05 old loss 1.213505402120063e-05 BETTER
I0325 05:12:06.609864 386989 finetune.py:68] layer 2_q @ epoch 2 new loss 9.75159855443053e-06 old loss 9.845678505371325e-06 BETTER
I0325 05:12:06.664227 384683 finetune.py:68] layer 0_k @ epoch 1 new loss 7.932544576760847e-08 old loss 8.084104052841212e-08 BETTER
I0325 05:12:09.440186 385771 finetune.py:45] layer 1_k initial loss 1.7985015801968984e-05
I0325 05:12:39.745072 386989 finetune.py:68] layer 2_q @ epoch 3 new loss 9.672594387666322e-06 old loss 9.75159855443053e-06 BETTER
I0325 05:12:41.845179 384683 finetune.py:68] layer 0_k @ epoch 2 new loss 7.797904544304401e-08 old loss 7.932544576760847e-08 BETTER
I0325 05:12:42.073257 385771 finetune.py:68] layer 1_k @ epoch 0 new loss 1.5569128663628362e-05 old loss 1.7985015801968984e-05 BETTER
I0325 05:13:12.710185 386989 finetune.py:68] layer 2_q @ epoch 4 new loss 9.604334081814159e-06 old loss 9.672594387666322e-06 BETTER
I0325 05:13:15.240614 385771 finetune.py:68] layer 1_k @ epoch 1 new loss 1.5306082786992192e-05 old loss 1.5569128663628362e-05 BETTER
I0325 05:13:16.660011 384683 finetune.py:68] layer 0_k @ epoch 3 new loss 7.686995928679607e-08 old loss 7.797904544304401e-08 BETTER
I0325 05:13:34.003484 386989 finetune.py:45] layer 2_k initial loss 1.0408084563096054e-05
I0325 05:13:49.772955 385771 finetune.py:68] layer 1_k @ epoch 2 new loss 1.4524421203532256e-05 old loss 1.5306082786992192e-05 BETTER
I0325 05:13:52.227064 384683 finetune.py:68] layer 0_k @ epoch 4 new loss 7.604375440450895e-08 old loss 7.686995928679607e-08 BETTER
I0325 05:14:05.472289 386989 finetune.py:68] layer 2_k @ epoch 0 new loss 1.0325730727345217e-05 old loss 1.0408084563096054e-05 BETTER
I0325 05:14:13.466634 384683 finetune.py:45] layer 0_o initial loss 6.259172664613288e-07
I0325 05:14:24.127014 385771 finetune.py:76] layer 1_k @ epoch 3 new loss 1.493915988248773e-05 old loss 1.4524421203532256e-05 WORSE
I0325 05:14:38.535841 386989 finetune.py:68] layer 2_k @ epoch 1 new loss 1.0262969226459973e-05 old loss 1.0325730727345217e-05 BETTER
I0325 05:14:45.930500 384683 finetune.py:68] layer 0_o @ epoch 0 new loss 5.876160571460787e-07 old loss 6.259172664613288e-07 BETTER
I0325 05:14:56.235265 385771 finetune.py:68] layer 1_k @ epoch 4 new loss 1.3875919648853596e-05 old loss 1.4524421203532256e-05 BETTER
I0325 05:15:11.053560 386989 finetune.py:68] layer 2_k @ epoch 2 new loss 1.0206469596596435e-05 old loss 1.0262969226459973e-05 BETTER
I0325 05:15:19.596062 385771 finetune.py:45] layer 1_o initial loss 0.00021479940915014595
I0325 05:15:21.674065 384683 finetune.py:68] layer 0_o @ epoch 1 new loss 5.589681677520275e-07 old loss 5.876160571460787e-07 BETTER
I0325 05:15:43.450502 386989 finetune.py:68] layer 2_k @ epoch 3 new loss 1.0155520612897817e-05 old loss 1.0206469596596435e-05 BETTER
I0325 05:15:50.981782 385771 finetune.py:68] layer 1_o @ epoch 0 new loss 8.15589664853178e-05 old loss 0.00021479940915014595 BETTER
I0325 05:15:55.663115 384683 finetune.py:68] layer 0_o @ epoch 2 new loss 5.356657766242279e-07 old loss 5.589681677520275e-07 BETTER
I0325 05:16:16.759576 386989 finetune.py:68] layer 2_k @ epoch 4 new loss 1.0108139576914255e-05 old loss 1.0155520612897817e-05 BETTER
I0325 05:16:24.820674 385771 finetune.py:68] layer 1_o @ epoch 1 new loss 4.9694193876348436e-05 old loss 8.15589664853178e-05 BETTER
I0325 05:16:30.109913 384683 finetune.py:68] layer 0_o @ epoch 3 new loss 5.165231300452433e-07 old loss 5.356657766242279e-07 BETTER
I0325 05:16:39.892550 386989 finetune.py:45] layer 2_o initial loss 3.5670345823746175e-05
I0325 05:16:59.271440 385771 finetune.py:68] layer 1_o @ epoch 2 new loss 4.4029329728800803e-05 old loss 4.9694193876348436e-05 BETTER
I0325 05:17:05.141489 384683 finetune.py:68] layer 0_o @ epoch 4 new loss 5.007452728023054e-07 old loss 5.165231300452433e-07 BETTER
I0325 05:17:11.197750 386989 finetune.py:68] layer 2_o @ epoch 0 new loss 3.4724551369436085e-05 old loss 3.5670345823746175e-05 BETTER
I0325 05:17:32.908884 385771 finetune.py:68] layer 1_o @ epoch 3 new loss 4.2827698052860796e-05 old loss 4.4029329728800803e-05 BETTER
I0325 05:17:33.660045 384683 finetune.py:45] layer 0_up initial loss 9.474405828768795e-07
I0325 05:17:44.080175 386989 finetune.py:68] layer 2_o @ epoch 1 new loss 3.402385482331738e-05 old loss 3.4724551369436085e-05 BETTER
I0325 05:18:06.754368 384683 finetune.py:68] layer 0_up @ epoch 0 new loss 8.842870897751709e-07 old loss 9.474405828768795e-07 BETTER
I0325 05:18:07.599034 385771 finetune.py:68] layer 1_o @ epoch 4 new loss 4.19218631577678e-05 old loss 4.2827698052860796e-05 BETTER
I0325 05:18:16.476414 386989 finetune.py:68] layer 2_o @ epoch 2 new loss 3.348516838741489e-05 old loss 3.402385482331738e-05 BETTER
I0325 05:18:38.475744 385771 finetune.py:45] layer 1_up initial loss 6.32090523140505e-05
I0325 05:18:40.379112 384683 finetune.py:68] layer 0_up @ epoch 1 new loss 8.438110512543062e-07 old loss 8.842870897751709e-07 BETTER
I0325 05:18:49.520156 386989 finetune.py:68] layer 2_o @ epoch 3 new loss 3.305863356217742e-05 old loss 3.348516838741489e-05 BETTER
I0325 05:19:07.815283 385771 finetune.py:68] layer 1_up @ epoch 0 new loss 4.902611908619292e-05 old loss 6.32090523140505e-05 BETTER
I0325 05:19:13.752514 384683 finetune.py:68] layer 0_up @ epoch 2 new loss 8.17172008282796e-07 old loss 8.438110512543062e-07 BETTER
I0325 05:19:22.528964 386989 finetune.py:68] layer 2_o @ epoch 4 new loss 3.271088644396514e-05 old loss 3.305863356217742e-05 BETTER
I0325 05:19:39.519336 385771 finetune.py:68] layer 1_up @ epoch 1 new loss 4.731274748337455e-05 old loss 4.902611908619292e-05 BETTER
I0325 05:19:46.898966 384683 finetune.py:68] layer 0_up @ epoch 3 new loss 7.985708521118795e-07 old loss 8.17172008282796e-07 BETTER
I0325 05:19:54.790506 386989 finetune.py:45] layer 2_up initial loss 4.82211253256537e-05
I0325 05:20:11.403061 385771 finetune.py:68] layer 1_up @ epoch 2 new loss 4.601766704581678e-05 old loss 4.731274748337455e-05 BETTER
I0325 05:20:20.656843 384683 finetune.py:68] layer 0_up @ epoch 4 new loss 7.850338192838535e-07 old loss 7.985708521118795e-07 BETTER
I0325 05:20:24.408595 386989 finetune.py:68] layer 2_up @ epoch 0 new loss 4.7794103011256084e-05 old loss 4.82211253256537e-05 BETTER
I0325 05:20:42.861519 385771 finetune.py:68] layer 1_up @ epoch 3 new loss 4.47702404926531e-05 old loss 4.601766704581678e-05 BETTER
I0325 05:20:51.683829 384683 finetune.py:45] layer 0_gate initial loss 1.3499193300958723e-06
I0325 05:20:55.738375 386989 finetune.py:68] layer 2_up @ epoch 1 new loss 4.7470770368818194e-05 old loss 4.7794103011256084e-05 BETTER
I0325 05:21:14.824893 385771 finetune.py:68] layer 1_up @ epoch 4 new loss 4.442554563865997e-05 old loss 4.47702404926531e-05 BETTER
I0325 05:21:21.160931 384683 finetune.py:68] layer 0_gate @ epoch 0 new loss 1.2786421166310902e-06 old loss 1.3499193300958723e-06 BETTER
I0325 05:21:26.291550 386989 finetune.py:68] layer 2_up @ epoch 2 new loss 4.7217199607985094e-05 old loss 4.7470770368818194e-05 BETTER
I0325 05:21:47.028824 385771 finetune.py:45] layer 1_gate initial loss 5.977877299301326e-05
I0325 05:21:52.523028 384683 finetune.py:68] layer 0_gate @ epoch 1 new loss 1.2297231251068297e-06 old loss 1.2786421166310902e-06 BETTER
I0325 05:21:57.426643 386989 finetune.py:68] layer 2_up @ epoch 3 new loss 4.700685531133786e-05 old loss 4.7217199607985094e-05 BETTER
I0325 05:22:16.093161 385771 finetune.py:68] layer 1_gate @ epoch 0 new loss 4.984005499863997e-05 old loss 5.977877299301326e-05 BETTER
I0325 05:22:24.666443 384683 finetune.py:68] layer 0_gate @ epoch 2 new loss 1.1943365052502486e-06 old loss 1.2297231251068297e-06 BETTER
I0325 05:22:29.398390 386989 finetune.py:68] layer 2_up @ epoch 4 new loss 4.68282014480792e-05 old loss 4.700685531133786e-05 BETTER
I0325 05:22:46.568706 385771 finetune.py:68] layer 1_gate @ epoch 1 new loss 4.891623757430352e-05 old loss 4.984005499863997e-05 BETTER
I0325 05:22:56.712504 384683 finetune.py:68] layer 0_gate @ epoch 3 new loss 1.1675475661832024e-06 old loss 1.1943365052502486e-06 BETTER
I0325 05:23:01.166970 386989 finetune.py:45] layer 2_gate initial loss 5.843174221809022e-05
I0325 05:23:17.381898 385771 finetune.py:68] layer 1_gate @ epoch 2 new loss 4.791637184098363e-05 old loss 4.891623757430352e-05 BETTER
I0325 05:23:29.080282 384683 finetune.py:68] layer 0_gate @ epoch 4 new loss 1.1467211606941419e-06 old loss 1.1675475661832024e-06 BETTER
I0325 05:23:30.475847 386989 finetune.py:68] layer 2_gate @ epoch 0 new loss 5.812535164295696e-05 old loss 5.843174221809022e-05 BETTER
I0325 05:23:47.122645 385771 finetune.py:68] layer 1_gate @ epoch 3 new loss 4.7044697566889226e-05 old loss 4.791637184098363e-05 BETTER
I0325 05:23:59.562532 386989 finetune.py:68] layer 2_gate @ epoch 1 new loss 5.788622365798801e-05 old loss 5.812535164295696e-05 BETTER
I0325 05:24:18.231436 384683 finetune.py:45] layer 0_down initial loss 2.6336269911553245e-06
I0325 05:24:18.434508 385771 finetune.py:68] layer 1_gate @ epoch 4 new loss 4.5952310756547377e-05 old loss 4.7044697566889226e-05 BETTER
I0325 05:24:28.730738 386989 finetune.py:68] layer 2_gate @ epoch 2 new loss 5.768838309450075e-05 old loss 5.788622365798801e-05 BETTER
I0325 05:24:46.128679 384683 finetune.py:68] layer 0_down @ epoch 0 new loss 2.611098352645058e-06 old loss 2.6336269911553245e-06 BETTER
I0325 05:24:58.660987 386989 finetune.py:68] layer 2_gate @ epoch 3 new loss 5.751956268795766e-05 old loss 5.768838309450075e-05 BETTER
I0325 05:25:11.124703 385771 finetune.py:45] layer 1_down initial loss 6.501548341475427e-05
I0325 05:25:16.905094 384683 finetune.py:68] layer 0_down @ epoch 1 new loss 2.5955359888030216e-06 old loss 2.611098352645058e-06 BETTER
I0325 05:25:29.291450 386989 finetune.py:68] layer 2_gate @ epoch 4 new loss 5.737355968449265e-05 old loss 5.751956268795766e-05 BETTER
I0325 05:25:38.558718 385771 finetune.py:68] layer 1_down @ epoch 0 new loss 6.445815961342305e-05 old loss 6.501548341475427e-05 BETTER
I0325 05:25:47.558670 384683 finetune.py:68] layer 0_down @ epoch 2 new loss 2.584107278380543e-06 old loss 2.5955359888030216e-06 BETTER
I0325 05:26:07.975834 385771 finetune.py:68] layer 1_down @ epoch 1 new loss 6.396367098204792e-05 old loss 6.445815961342305e-05 BETTER
I0325 05:26:18.288301 384683 finetune.py:68] layer 0_down @ epoch 3 new loss 2.5755209662747802e-06 old loss 2.584107278380543e-06 BETTER
I0325 05:26:19.771651 386989 finetune.py:45] layer 2_down initial loss 8.903181878849864e-05
I0325 05:26:36.645236 385771 finetune.py:68] layer 1_down @ epoch 2 new loss 6.3220904849004e-05 old loss 6.396367098204792e-05 BETTER
I0325 05:26:47.036229 386989 finetune.py:68] layer 2_down @ epoch 0 new loss 8.89375587576069e-05 old loss 8.903181878849864e-05 BETTER
I0325 05:26:48.810003 384683 finetune.py:68] layer 0_down @ epoch 4 new loss 2.568509898992488e-06 old loss 2.5755209662747802e-06 BETTER
0_v proxy err 0.01611548475921154 tr(WHW.T) 992.1710205078125
bpp_loss 1.7892597619211301
0_q proxy err 3.1749095796840265e-05 tr(WHW.T) 636515.0
bpp_loss 1.881683309446089
0_k proxy err 4.7846751840552315e-05 tr(WHW.T) 398923.21875
bpp_loss 2.0050085035909433
0_o proxy err 0.00166529999114573 tr(WHW.T) 15969.26171875
bpp_loss 1.6726946974522434
0_up proxy err 0.017079276964068413 tr(WHW.T) 24234.798828125
bpp_loss 2.127108956145686
0_gate proxy err 0.011696750298142433 tr(WHW.T) 35512.65234375
bpp_loss 2.144845964387059
0_down proxy err 0.009832746349275112 tr(WHW.T) 36017.4921875
bpp_loss 2.1772762824196454
I0325 05:27:05.127280 385771 finetune.py:68] layer 1_down @ epoch 3 new loss 6.263570685405284e-05 old loss 6.3220904849004e-05 BETTER
I0325 05:27:14.846684 386989 finetune.py:68] layer 2_down @ epoch 1 new loss 8.88643553480506e-05 old loss 8.89375587576069e-05 BETTER
I0325 05:27:33.191956 385771 finetune.py:68] layer 1_down @ epoch 4 new loss 6.209382263477892e-05 old loss 6.263570685405284e-05 BETTER
1_v proxy err 0.043214816600084305 tr(WHW.T) 673.7838745117188
bpp_loss 1.7559688439941965
1_q proxy err 0.00017852161545306444 tr(WHW.T) 195552.71875
bpp_loss 2.5404650320415385
1_k proxy err 0.00017547616153024137 tr(WHW.T) 204427.25
bpp_loss 2.55749938703957
1_o proxy err 0.030306965112686157 tr(WHW.T) 4052.82373046875
bpp_loss 1.721547113294946
1_up proxy err 0.024202583357691765 tr(WHW.T) 23365.70703125
bpp_loss 2.19504592134509
1_gate proxy err 0.012208704836666584 tr(WHW.T) 47090.54296875
bpp_loss 2.265952400126776
1_down proxy err 0.00030210777185857296 tr(WHW.T) 40954.3359375
bpp_loss 2.1792678012144426
I0325 05:27:41.867750 386989 finetune.py:68] layer 2_down @ epoch 2 new loss 8.880514360498637e-05 old loss 8.88643553480506e-05 BETTER
I0325 05:28:08.572429 386989 finetune.py:68] layer 2_down @ epoch 3 new loss 8.875680214259773e-05 old loss 8.880514360498637e-05 BETTER
I0325 05:28:35.403663 386989 finetune.py:68] layer 2_down @ epoch 4 new loss 8.871585305314511e-05 old loss 8.875680214259773e-05 BETTER
2_v proxy err 0.028346069157123566 tr(WHW.T) 2814.01025390625
bpp_loss 2.000079934805399
2_q proxy err 0.0005775411846116185 tr(WHW.T) 159596.890625
bpp_loss 2.688583256327547
2_k proxy err 0.00044273893581703305 tr(WHW.T) 210201.96875
bpp_loss 2.7405255061166827
2_o proxy err 0.04695681482553482 tr(WHW.T) 5339.73828125
bpp_loss 1.978835308458656
2_up proxy err 0.03469492122530937 tr(WHW.T) 20071.798828125
bpp_loss 2.2069185945790175
2_gate proxy err 0.02235422283411026 tr(WHW.T) 31769.92578125
bpp_loss 2.2944077895737665
2_down proxy err 0.04518038406968117 tr(WHW.T) 17500.666015625
bpp_loss 2.2017504464141853
I0325 05:28:42.666948 382969 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 63.482141971588135s
I0325 05:28:45.959394 403279 config.py:54] PyTorch version 2.6.0 available.
W0325 05:28:46.242737 403279 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:28:47.147146 403279 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 05:28:47.151140 382969 quantize_finetune_llama.py:209] layer 4 gpu 1
I0325 05:28:47.164545 403279 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:29:05.389503 403279 finetune.py:45] layer 3_v initial loss 8.429137233179063e-05
W0325 05:29:05.389705 403279 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 05:29:38.133168 403279 finetune.py:68] layer 3_v @ epoch 0 new loss 3.562409256119281e-05 old loss 8.429137233179063e-05 BETTER
I0325 05:29:47.832084 382969 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 60.19295310974121s
I0325 05:29:51.313286 404132 config.py:54] PyTorch version 2.6.0 available.
W0325 05:29:51.626789 404132 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:29:52.591374 404132 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 05:29:52.596051 382969 quantize_finetune_llama.py:209] layer 5 gpu 2
I0325 05:29:52.615139 404132 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:30:13.398581 403279 finetune.py:68] layer 3_v @ epoch 1 new loss 2.4701810616534203e-05 old loss 3.562409256119281e-05 BETTER
I0325 05:30:13.576212 404132 finetune.py:45] layer 4_v initial loss 0.00014953649952076375
W0325 05:30:13.576408 404132 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 05:30:45.429561 404132 finetune.py:68] layer 4_v @ epoch 0 new loss 5.640029485221021e-05 old loss 0.00014953649952076375 BETTER
I0325 05:30:48.575831 403279 finetune.py:68] layer 3_v @ epoch 2 new loss 2.1543972252402455e-05 old loss 2.4701810616534203e-05 BETTER
I0325 05:31:00.682292 382969 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 67.59265184402466s
I0325 05:31:04.731671 405093 config.py:54] PyTorch version 2.6.0 available.
W0325 05:31:05.096104 405093 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:31:06.160032 405093 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 05:31:06.164653 382969 quantize_finetune_llama.py:209] layer 6 gpu 0
I0325 05:31:06.184234 405093 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 05:31:19.060906 404132 finetune.py:68] layer 4_v @ epoch 1 new loss 3.976627340307459e-05 old loss 5.640029485221021e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:31:24.137868 403279 finetune.py:68] layer 3_v @ epoch 3 new loss 2.0422708985279314e-05 old loss 2.1543972252402455e-05 BETTER
I0325 05:31:27.901790 405093 finetune.py:45] layer 5_v initial loss 0.00020962243434041739
W0325 05:31:27.902245 405093 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 05:31:54.065561 404132 finetune.py:68] layer 4_v @ epoch 2 new loss 3.545097933965735e-05 old loss 3.976627340307459e-05 BETTER
I0325 05:32:00.614092 403279 finetune.py:68] layer 3_v @ epoch 4 new loss 1.9912118659703992e-05 old loss 2.0422708985279314e-05 BETTER
I0325 05:32:02.081929 405093 finetune.py:68] layer 5_v @ epoch 0 new loss 9.064871846931055e-05 old loss 0.00020962243434041739 BETTER
I0325 05:32:21.876962 403279 finetune.py:45] layer 3_q initial loss 2.388521534157917e-05
I0325 05:32:29.100045 404132 finetune.py:68] layer 4_v @ epoch 3 new loss 3.381881469977088e-05 old loss 3.545097933965735e-05 BETTER
I0325 05:32:36.238905 405093 finetune.py:68] layer 5_v @ epoch 1 new loss 7.174314669100568e-05 old loss 9.064871846931055e-05 BETTER
I0325 05:32:55.535548 403279 finetune.py:68] layer 3_q @ epoch 0 new loss 2.2997384803602472e-05 old loss 2.388521534157917e-05 BETTER
I0325 05:33:04.165907 404132 finetune.py:68] layer 4_v @ epoch 4 new loss 3.2939664379227906e-05 old loss 3.381881469977088e-05 BETTER
I0325 05:33:10.650802 405093 finetune.py:68] layer 5_v @ epoch 2 new loss 6.640378705924377e-05 old loss 7.174314669100568e-05 BETTER
I0325 05:33:27.196204 404132 finetune.py:45] layer 4_q initial loss 3.869161082548089e-05
I0325 05:33:31.010007 403279 finetune.py:68] layer 3_q @ epoch 1 new loss 2.2669421014143154e-05 old loss 2.2997384803602472e-05 BETTER
I0325 05:33:46.058593 405093 finetune.py:68] layer 5_v @ epoch 3 new loss 6.398089317372069e-05 old loss 6.640378705924377e-05 BETTER
I0325 05:34:00.927504 404132 finetune.py:68] layer 4_q @ epoch 0 new loss 3.7342426367104053e-05 old loss 3.869161082548089e-05 BETTER
I0325 05:34:06.876547 403279 finetune.py:68] layer 3_q @ epoch 2 new loss 2.241684342152439e-05 old loss 2.2669421014143154e-05 BETTER
I0325 05:34:21.151557 405093 finetune.py:68] layer 5_v @ epoch 4 new loss 6.251704326132312e-05 old loss 6.398089317372069e-05 BETTER
I0325 05:34:34.476815 404132 finetune.py:68] layer 4_q @ epoch 1 new loss 3.6673369322670624e-05 old loss 3.7342426367104053e-05 BETTER
I0325 05:34:42.511027 403279 finetune.py:68] layer 3_q @ epoch 3 new loss 2.2212851035874337e-05 old loss 2.241684342152439e-05 BETTER
I0325 05:34:45.277156 405093 finetune.py:45] layer 5_q initial loss 7.129855657694861e-05
I0325 05:35:08.290402 404132 finetune.py:68] layer 4_q @ epoch 2 new loss 3.6168239603284746e-05 old loss 3.6673369322670624e-05 BETTER
I0325 05:35:18.280731 403279 finetune.py:68] layer 3_q @ epoch 4 new loss 2.2036172595107928e-05 old loss 2.2212851035874337e-05 BETTER
I0325 05:35:18.509890 405093 finetune.py:68] layer 5_q @ epoch 0 new loss 6.90235392539762e-05 old loss 7.129855657694861e-05 BETTER
I0325 05:35:39.418004 403279 finetune.py:45] layer 3_k initial loss 2.5191407985403202e-05
I0325 05:35:42.692165 404132 finetune.py:68] layer 4_q @ epoch 3 new loss 3.575473965611309e-05 old loss 3.6168239603284746e-05 BETTER
I0325 05:35:54.010091 405093 finetune.py:68] layer 5_q @ epoch 1 new loss 6.782893615309149e-05 old loss 6.90235392539762e-05 BETTER
I0325 05:36:12.939886 403279 finetune.py:68] layer 3_k @ epoch 0 new loss 2.488330937922001e-05 old loss 2.5191407985403202e-05 BETTER
I0325 05:36:16.649239 404132 finetune.py:68] layer 4_q @ epoch 4 new loss 3.5403427318669856e-05 old loss 3.575473965611309e-05 BETTER
I0325 05:36:27.132404 405093 finetune.py:68] layer 5_q @ epoch 2 new loss 6.693461909890175e-05 old loss 6.782893615309149e-05 BETTER
I0325 05:36:38.595076 404132 finetune.py:45] layer 4_k initial loss 4.011109922430478e-05
I0325 05:36:46.971264 403279 finetune.py:68] layer 3_k @ epoch 1 new loss 2.4702540031285025e-05 old loss 2.488330937922001e-05 BETTER
I0325 05:37:01.100299 405093 finetune.py:68] layer 5_q @ epoch 3 new loss 6.616272730752826e-05 old loss 6.693461909890175e-05 BETTER
I0325 05:37:10.751216 404132 finetune.py:68] layer 4_k @ epoch 0 new loss 3.9608818042324856e-05 old loss 4.011109922430478e-05 BETTER
I0325 05:37:20.697829 403279 finetune.py:68] layer 3_k @ epoch 2 new loss 2.4556480639148504e-05 old loss 2.4702540031285025e-05 BETTER
I0325 05:37:34.143971 405093 finetune.py:68] layer 5_q @ epoch 4 new loss 6.549428508151323e-05 old loss 6.616272730752826e-05 BETTER
I0325 05:37:43.233243 404132 finetune.py:68] layer 4_k @ epoch 1 new loss 3.928071600967087e-05 old loss 3.9608818042324856e-05 BETTER
I0325 05:37:53.872455 405093 finetune.py:45] layer 5_k initial loss 7.125306728994474e-05
I0325 05:37:54.530688 403279 finetune.py:68] layer 3_k @ epoch 3 new loss 2.442948243697174e-05 old loss 2.4556480639148504e-05 BETTER
I0325 05:38:15.561495 404132 finetune.py:68] layer 4_k @ epoch 2 new loss 3.900940282619558e-05 old loss 3.928071600967087e-05 BETTER
I0325 05:38:25.643299 405093 finetune.py:68] layer 5_k @ epoch 0 new loss 7.041099161142483e-05 old loss 7.125306728994474e-05 BETTER
I0325 05:38:28.519383 403279 finetune.py:68] layer 3_k @ epoch 4 new loss 2.4318258510902524e-05 old loss 2.442948243697174e-05 BETTER
I0325 05:38:50.923195 403279 finetune.py:45] layer 3_o initial loss 7.036770693957806e-05
I0325 05:38:52.445343 404132 finetune.py:68] layer 4_k @ epoch 3 new loss 3.87647669413127e-05 old loss 3.900940282619558e-05 BETTER
I0325 05:39:00.794355 405093 finetune.py:68] layer 5_k @ epoch 1 new loss 6.982462218729779e-05 old loss 7.041099161142483e-05 BETTER
I0325 05:39:23.020597 403279 finetune.py:68] layer 3_o @ epoch 0 new loss 6.77930293022655e-05 old loss 7.036770693957806e-05 BETTER
I0325 05:39:25.340046 404132 finetune.py:68] layer 4_k @ epoch 4 new loss 3.8550570025108755e-05 old loss 3.87647669413127e-05 BETTER
I0325 05:39:33.522180 405093 finetune.py:68] layer 5_k @ epoch 2 new loss 6.931555253686383e-05 old loss 6.982462218729779e-05 BETTER
I0325 05:39:49.821832 404132 finetune.py:45] layer 4_o initial loss 0.00011546639143489301
I0325 05:39:58.003073 403279 finetune.py:68] layer 3_o @ epoch 1 new loss 6.609784759348258e-05 old loss 6.77930293022655e-05 BETTER
I0325 05:40:06.900776 405093 finetune.py:68] layer 5_k @ epoch 3 new loss 6.884909817017615e-05 old loss 6.931555253686383e-05 BETTER
I0325 05:40:20.944339 404132 finetune.py:68] layer 4_o @ epoch 0 new loss 0.00010778320574900135 old loss 0.00011546639143489301 BETTER
I0325 05:40:32.630870 403279 finetune.py:68] layer 3_o @ epoch 2 new loss 6.494950503110886e-05 old loss 6.609784759348258e-05 BETTER
I0325 05:40:41.325377 405093 finetune.py:68] layer 5_k @ epoch 4 new loss 6.844224117230624e-05 old loss 6.884909817017615e-05 BETTER
I0325 05:40:54.081254 404132 finetune.py:68] layer 4_o @ epoch 1 new loss 0.00010439476318424568 old loss 0.00010778320574900135 BETTER
I0325 05:41:03.733990 405093 finetune.py:45] layer 5_o initial loss 0.00018032950174529105
I0325 05:41:07.287151 403279 finetune.py:68] layer 3_o @ epoch 3 new loss 6.412548100342974e-05 old loss 6.494950503110886e-05 BETTER
I0325 05:41:28.259471 404132 finetune.py:68] layer 4_o @ epoch 2 new loss 0.00010259322880301625 old loss 0.00010439476318424568 BETTER
I0325 05:41:36.731265 405093 finetune.py:68] layer 5_o @ epoch 0 new loss 0.00017006509006023407 old loss 0.00018032950174529105 BETTER
I0325 05:41:41.032281 403279 finetune.py:68] layer 3_o @ epoch 4 new loss 6.3511666667182e-05 old loss 6.412548100342974e-05 BETTER
I0325 05:42:01.672177 404132 finetune.py:68] layer 4_o @ epoch 3 new loss 0.00010145056148758158 old loss 0.00010259322880301625 BETTER
I0325 05:42:11.136965 403279 finetune.py:45] layer 3_up initial loss 0.00010238630056846887
I0325 05:42:11.694658 405093 finetune.py:68] layer 5_o @ epoch 1 new loss 0.0001661395508563146 old loss 0.00017006509006023407 BETTER
I0325 05:42:36.072697 404132 finetune.py:68] layer 4_o @ epoch 4 new loss 0.00010061061038868502 old loss 0.00010145056148758158 BETTER
I0325 05:42:42.552886 403279 finetune.py:68] layer 3_up @ epoch 0 new loss 0.00010151112655876204 old loss 0.00010238630056846887 BETTER
I0325 05:42:45.058559 405093 finetune.py:68] layer 5_o @ epoch 2 new loss 0.00016399804735556245 old loss 0.0001661395508563146 BETTER
I0325 05:43:09.094465 404132 finetune.py:45] layer 4_up initial loss 0.00017314542492385954
I0325 05:43:16.175131 403279 finetune.py:68] layer 3_up @ epoch 1 new loss 0.00010087686678161845 old loss 0.00010151112655876204 BETTER
I0325 05:43:19.895583 405093 finetune.py:68] layer 5_o @ epoch 3 new loss 0.00016248221800196916 old loss 0.00016399804735556245 BETTER
I0325 05:43:38.311541 404132 finetune.py:68] layer 4_up @ epoch 0 new loss 0.00017145856691058725 old loss 0.00017314542492385954 BETTER
I0325 05:43:50.065778 403279 finetune.py:68] layer 3_up @ epoch 2 new loss 0.00010039926564786583 old loss 0.00010087686678161845 BETTER
I0325 05:43:55.109899 405093 finetune.py:68] layer 5_o @ epoch 4 new loss 0.0001612884079804644 old loss 0.00016248221800196916 BETTER
I0325 05:44:10.309369 404132 finetune.py:68] layer 4_up @ epoch 1 new loss 0.00017035470227710903 old loss 0.00017145856691058725 BETTER
I0325 05:44:23.245862 403279 finetune.py:68] layer 3_up @ epoch 3 new loss 0.00010001388727687299 old loss 0.00010039926564786583 BETTER
I0325 05:44:27.557600 405093 finetune.py:45] layer 5_up initial loss 0.0002758332993835211
I0325 05:44:43.041136 404132 finetune.py:68] layer 4_up @ epoch 2 new loss 0.00016952180885709822 old loss 0.00017035470227710903 BETTER
I0325 05:44:56.865479 403279 finetune.py:68] layer 3_up @ epoch 4 new loss 9.969058010028675e-05 old loss 0.00010001388727687299 BETTER
I0325 05:44:59.257432 405093 finetune.py:68] layer 5_up @ epoch 0 new loss 0.0002734545269049704 old loss 0.0002758332993835211 BETTER
I0325 05:45:15.315589 404132 finetune.py:68] layer 4_up @ epoch 3 new loss 0.00016882768250070512 old loss 0.00016952180885709822 BETTER
I0325 05:45:28.444482 403279 finetune.py:45] layer 3_gate initial loss 0.00012885051546618342
I0325 05:45:32.323017 405093 finetune.py:68] layer 5_up @ epoch 1 new loss 0.0002718851901590824 old loss 0.0002734545269049704 BETTER
I0325 05:45:46.131219 404132 finetune.py:68] layer 4_up @ epoch 4 new loss 0.0001682174624875188 old loss 0.00016882768250070512 BETTER
I0325 05:45:56.954604 403279 finetune.py:68] layer 3_gate @ epoch 0 new loss 0.00012828763283323497 old loss 0.00012885051546618342 BETTER
I0325 05:46:04.716230 405093 finetune.py:68] layer 5_up @ epoch 2 new loss 0.00027061632135882974 old loss 0.0002718851901590824 BETTER
I0325 05:46:17.053281 404132 finetune.py:45] layer 4_gate initial loss 0.00021699522039853036
I0325 05:46:28.076565 403279 finetune.py:68] layer 3_gate @ epoch 1 new loss 0.00012781286204699427 old loss 0.00012828763283323497 BETTER
I0325 05:46:35.934461 405093 finetune.py:68] layer 5_up @ epoch 3 new loss 0.0002694901777431369 old loss 0.00027061632135882974 BETTER
I0325 05:46:46.374862 404132 finetune.py:68] layer 4_gate @ epoch 0 new loss 0.0002159339637728408 old loss 0.00021699522039853036 BETTER
I0325 05:46:58.295639 403279 finetune.py:68] layer 3_gate @ epoch 2 new loss 0.00012740847887471318 old loss 0.00012781286204699427 BETTER
I0325 05:47:08.993458 405093 finetune.py:68] layer 5_up @ epoch 4 new loss 0.00026848455308936536 old loss 0.0002694901777431369 BETTER
I0325 05:47:16.156150 404132 finetune.py:68] layer 4_gate @ epoch 1 new loss 0.0002150984073523432 old loss 0.0002159339637728408 BETTER
I0325 05:47:29.512976 403279 finetune.py:68] layer 3_gate @ epoch 3 new loss 0.00012706182315014303 old loss 0.00012740847887471318 BETTER
I0325 05:47:39.867182 405093 finetune.py:45] layer 5_gate initial loss 0.0003410328645259142
I0325 05:47:46.581304 404132 finetune.py:68] layer 4_gate @ epoch 2 new loss 0.00021437705436255783 old loss 0.0002150984073523432 BETTER
I0325 05:48:00.200119 403279 finetune.py:68] layer 3_gate @ epoch 4 new loss 0.00012675744073931128 old loss 0.00012706182315014303 BETTER
I0325 05:48:09.767804 405093 finetune.py:68] layer 5_gate @ epoch 0 new loss 0.0003393809893168509 old loss 0.0003410328645259142 BETTER
I0325 05:48:15.471497 404132 finetune.py:68] layer 4_gate @ epoch 3 new loss 0.00021372712217271328 old loss 0.00021437705436255783 BETTER
I0325 05:48:40.493306 405093 finetune.py:68] layer 5_gate @ epoch 1 new loss 0.00033809279557317495 old loss 0.0003393809893168509 BETTER
I0325 05:48:46.417306 404132 finetune.py:68] layer 4_gate @ epoch 4 new loss 0.00021313599427230656 old loss 0.00021372712217271328 BETTER
I0325 05:48:48.943913 403279 finetune.py:45] layer 3_down initial loss 0.00019227406301070005
I0325 05:49:11.558667 405093 finetune.py:68] layer 5_gate @ epoch 2 new loss 0.0003369858313817531 old loss 0.00033809279557317495 BETTER
I0325 05:49:16.588387 403279 finetune.py:68] layer 3_down @ epoch 0 new loss 0.0001920947543112561 old loss 0.00019227406301070005 BETTER
I0325 05:49:35.371753 404132 finetune.py:45] layer 4_down initial loss 0.00033436520607210696
I0325 05:49:43.584493 405093 finetune.py:68] layer 5_gate @ epoch 3 new loss 0.0003359731344971806 old loss 0.0003369858313817531 BETTER
I0325 05:49:46.960009 403279 finetune.py:68] layer 3_down @ epoch 1 new loss 0.00019195704953745008 old loss 0.0001920947543112561 BETTER
I0325 05:50:03.424854 404132 finetune.py:68] layer 4_down @ epoch 0 new loss 0.00033409640309400856 old loss 0.00033436520607210696 BETTER
I0325 05:50:14.654886 405093 finetune.py:68] layer 5_gate @ epoch 4 new loss 0.00033504507155157626 old loss 0.0003359731344971806 BETTER
I0325 05:50:16.342681 403279 finetune.py:68] layer 3_down @ epoch 2 new loss 0.00019184852135367692 old loss 0.00019195704953745008 BETTER
I0325 05:50:31.822525 404132 finetune.py:68] layer 4_down @ epoch 1 new loss 0.0003338675305712968 old loss 0.00033409640309400856 BETTER
I0325 05:50:45.686619 403279 finetune.py:68] layer 3_down @ epoch 3 new loss 0.00019175861962139606 old loss 0.00019184852135367692 BETTER
I0325 05:51:01.295042 404132 finetune.py:68] layer 4_down @ epoch 2 new loss 0.00033367358264513314 old loss 0.0003338675305712968 BETTER
I0325 05:51:05.857364 405093 finetune.py:45] layer 5_down initial loss 0.0005160936852917075
I0325 05:51:16.535154 403279 finetune.py:68] layer 3_down @ epoch 4 new loss 0.0001916822075145319 old loss 0.00019175861962139606 BETTER
3_v proxy err 0.04738690331578255 tr(WHW.T) 3009.644287109375
bpp_loss 1.9471146425057668
3_q proxy err 0.002166422549635172 tr(WHW.T) 76290.1484375
bpp_loss 2.6204775596852414
3_k proxy err 0.0015664887614548206 tr(WHW.T) 106529.7421875
bpp_loss 2.6722356044629123
3_o proxy err 0.04041292890906334 tr(WHW.T) 5305.953125
bpp_loss 1.9265070331748575
3_up proxy err 0.04322254657745361 tr(WHW.T) 17604.142578125
bpp_loss 2.2023654198923777
3_gate proxy err 0.026343604549765587 tr(WHW.T) 29485.162109375
bpp_loss 2.2991901402643253
3_down proxy err 0.0469365157186985 tr(WHW.T) 17166.1015625
bpp_loss 2.199348584640511
I0325 05:51:29.822967 404132 finetune.py:68] layer 4_down @ epoch 3 new loss 0.0003335076617076993 old loss 0.00033367358264513314 BETTER
I0325 05:51:33.096301 405093 finetune.py:68] layer 5_down @ epoch 0 new loss 0.0005158042185939848 old loss 0.0005160936852917075 BETTER
I0325 05:51:57.319549 404132 finetune.py:68] layer 4_down @ epoch 4 new loss 0.000333366944687441 old loss 0.0003335076617076993 BETTER
4_v proxy err 0.045544762164354324 tr(WHW.T) 3131.788818359375
bpp_loss 1.992028006905457
4_q proxy err 0.002103287959471345 tr(WHW.T) 78913.6015625
bpp_loss 2.7029407294176053
4_k proxy err 0.0014020184753462672 tr(WHW.T) 118816.5234375
bpp_loss 2.7300526759645436
4_o proxy err 0.04647473245859146 tr(WHW.T) 5383.37890625
bpp_loss 1.982723046647152
4_up proxy err 0.04245113581418991 tr(WHW.T) 17757.8046875
bpp_loss 2.191451218542318
4_gate proxy err 0.021249808371067047 tr(WHW.T) 36482.6015625
bpp_loss 2.32046572777421
4_down proxy err 0.0461667962372303 tr(WHW.T) 17201.01171875
bpp_loss 2.1844624270100232
I0325 05:52:00.366032 405093 finetune.py:68] layer 5_down @ epoch 1 new loss 0.0005155558465048671 old loss 0.0005158042185939848 BETTER
I0325 05:52:27.699724 405093 finetune.py:68] layer 5_down @ epoch 2 new loss 0.0005153396632522345 old loss 0.0005155558465048671 BETTER
I0325 05:52:55.136478 405093 finetune.py:68] layer 5_down @ epoch 3 new loss 0.0005151524674147367 old loss 0.0005153396632522345 BETTER
I0325 05:53:06.085851 382969 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 63.71351361274719s
I0325 05:53:09.587371 421721 config.py:54] PyTorch version 2.6.0 available.
W0325 05:53:09.883022 421721 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:53:10.837394 421721 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 05:53:10.841237 382969 quantize_finetune_llama.py:209] layer 7 gpu 1
I0325 05:53:10.854281 421721 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 05:53:22.286496 405093 finetune.py:68] layer 5_down @ epoch 4 new loss 0.0005149905919097364 old loss 0.0005151524674147367 BETTER
5_v proxy err 0.04894155636429787 tr(WHW.T) 3202.603515625
bpp_loss 2.0093079973303247
5_q proxy err 0.0025092926807701588 tr(WHW.T) 72679.0
bpp_loss 2.7202317970222794
5_k proxy err 0.0015811024932190776 tr(WHW.T) 116357.8046875
bpp_loss 2.7749957254563924
5_o proxy err 0.04990435391664505 tr(WHW.T) 3812.085205078125
bpp_loss 1.9900422350328881
5_up proxy err 0.041814014315605164 tr(WHW.T) 18118.490234375
bpp_loss 2.19160695811517
5_gate proxy err 0.01984940841794014 tr(WHW.T) 39312.78515625
bpp_loss 2.327617012094273
5_down proxy err 0.05059865117073059 tr(WHW.T) 16309.396484375
bpp_loss 2.185219626007385
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:53:28.898380 421721 finetune.py:45] layer 6_v initial loss 0.0002685104263946414
W0325 05:53:28.898782 421721 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 05:54:01.666890 421721 finetune.py:68] layer 6_v @ epoch 0 new loss 0.00011428316065575927 old loss 0.0002685104263946414 BETTER
I0325 05:54:29.842404 382969 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 61.80234980583191s
I0325 05:54:33.436929 422788 config.py:54] PyTorch version 2.6.0 available.
W0325 05:54:33.743088 422788 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:54:34.650719 422788 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 05:54:34.655085 382969 quantize_finetune_llama.py:209] layer 8 gpu 2
I0325 05:54:34.675312 422788 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 05:54:35.746555 421721 finetune.py:68] layer 6_v @ epoch 1 new loss 9.299589873990044e-05 old loss 0.00011428316065575927 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:54:53.772296 422788 finetune.py:45] layer 7_v initial loss 0.00035162249696440995
W0325 05:54:53.772697 422788 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 05:55:10.536968 421721 finetune.py:68] layer 6_v @ epoch 2 new loss 8.701194019522518e-05 old loss 9.299589873990044e-05 BETTER
I0325 05:55:25.360524 422788 finetune.py:68] layer 7_v @ epoch 0 new loss 0.00015841898857615888 old loss 0.00035162249696440995 BETTER
I0325 05:55:39.576638 382969 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 64.42706823348999s
I0325 05:55:43.470053 423717 config.py:54] PyTorch version 2.6.0 available.
W0325 05:55:43.857392 423717 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 05:55:44.927618 423717 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 05:55:44.931855 382969 quantize_finetune_llama.py:209] layer 9 gpu 0
I0325 05:55:44.947941 423717 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 05:55:46.107608 421721 finetune.py:68] layer 6_v @ epoch 3 new loss 8.415642514592037e-05 old loss 8.701194019522518e-05 BETTER
I0325 05:55:58.015121 422788 finetune.py:68] layer 7_v @ epoch 1 new loss 0.00013308653433341533 old loss 0.00015841898857615888 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 05:56:07.375218 423717 finetune.py:45] layer 8_v initial loss 0.000428553408710286
W0325 05:56:07.375653 423717 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 05:56:22.156099 421721 finetune.py:68] layer 6_v @ epoch 4 new loss 8.229833474615589e-05 old loss 8.415642514592037e-05 BETTER
I0325 05:56:32.351958 422788 finetune.py:68] layer 7_v @ epoch 2 new loss 0.00012550436076708138 old loss 0.00013308653433341533 BETTER
I0325 05:56:41.089632 423717 finetune.py:68] layer 8_v @ epoch 0 new loss 0.00022087004617787898 old loss 0.000428553408710286 BETTER
I0325 05:56:44.367915 421721 finetune.py:45] layer 6_q initial loss 0.00010109486902365461
I0325 05:57:06.082661 422788 finetune.py:68] layer 7_v @ epoch 3 new loss 0.00012161524500697851 old loss 0.00012550436076708138 BETTER
I0325 05:57:16.482618 423717 finetune.py:68] layer 8_v @ epoch 1 new loss 0.0001910318387672305 old loss 0.00022087004617787898 BETTER
I0325 05:57:18.875832 421721 finetune.py:68] layer 6_q @ epoch 0 new loss 9.755396604305133e-05 old loss 0.00010109486902365461 BETTER
I0325 05:57:40.430701 422788 finetune.py:68] layer 7_v @ epoch 4 new loss 0.00011911116598639637 old loss 0.00012161524500697851 BETTER
I0325 05:57:52.058135 423717 finetune.py:68] layer 8_v @ epoch 2 new loss 0.00018128450028598309 old loss 0.0001910318387672305 BETTER
I0325 05:57:53.856582 421721 finetune.py:68] layer 6_q @ epoch 1 new loss 9.592085552867502e-05 old loss 9.755396604305133e-05 BETTER
I0325 05:58:03.156688 422788 finetune.py:45] layer 7_q initial loss 0.00014663528418168426
I0325 05:58:28.062300 423717 finetune.py:68] layer 8_v @ epoch 3 new loss 0.00017602786829229444 old loss 0.00018128450028598309 BETTER
I0325 05:58:29.754674 421721 finetune.py:68] layer 6_q @ epoch 2 new loss 9.465983021073043e-05 old loss 9.592085552867502e-05 BETTER
I0325 05:58:35.940082 422788 finetune.py:68] layer 7_q @ epoch 0 new loss 0.00014104312867857516 old loss 0.00014663528418168426 BETTER
I0325 05:59:02.792622 423717 finetune.py:68] layer 8_v @ epoch 4 new loss 0.00017243837646674365 old loss 0.00017602786829229444 BETTER
I0325 05:59:05.244187 421721 finetune.py:68] layer 6_q @ epoch 3 new loss 9.35950520215556e-05 old loss 9.465983021073043e-05 BETTER
I0325 05:59:09.825958 422788 finetune.py:68] layer 7_q @ epoch 1 new loss 0.0001386212243232876 old loss 0.00014104312867857516 BETTER
I0325 05:59:27.221349 423717 finetune.py:45] layer 8_q initial loss 0.00020690765813924372
I0325 05:59:42.428745 421721 finetune.py:68] layer 6_q @ epoch 4 new loss 9.270551527151838e-05 old loss 9.35950520215556e-05 BETTER
I0325 05:59:44.745576 422788 finetune.py:68] layer 7_q @ epoch 2 new loss 0.00013671729539055377 old loss 0.0001386212243232876 BETTER
I0325 06:00:01.350558 423717 finetune.py:68] layer 8_q @ epoch 0 new loss 0.0002004496200243011 old loss 0.00020690765813924372 BETTER
I0325 06:00:05.940681 421721 finetune.py:45] layer 6_k initial loss 0.00010695987293729559
I0325 06:00:19.644118 422788 finetune.py:68] layer 7_q @ epoch 3 new loss 0.00013514497550204396 old loss 0.00013671729539055377 BETTER
I0325 06:00:37.054903 423717 finetune.py:68] layer 8_q @ epoch 1 new loss 0.00019706379680428654 old loss 0.0002004496200243011 BETTER
I0325 06:00:40.237050 421721 finetune.py:68] layer 6_k @ epoch 0 new loss 0.00010461015335749835 old loss 0.00010695987293729559 BETTER
I0325 06:00:54.355870 422788 finetune.py:68] layer 7_q @ epoch 4 new loss 0.00013381632743403316 old loss 0.00013514497550204396 BETTER
I0325 06:01:11.775138 423717 finetune.py:68] layer 8_q @ epoch 2 new loss 0.00019445116049610078 old loss 0.00019706379680428654 BETTER
I0325 06:01:15.602486 421721 finetune.py:68] layer 6_k @ epoch 1 new loss 0.00010369724623160437 old loss 0.00010461015335749835 BETTER
I0325 06:01:18.000561 422788 finetune.py:45] layer 7_k initial loss 0.00015386435552500188
I0325 06:01:47.221732 423717 finetune.py:68] layer 8_q @ epoch 3 new loss 0.00019220731337554753 old loss 0.00019445116049610078 BETTER
I0325 06:01:51.550089 421721 finetune.py:68] layer 6_k @ epoch 2 new loss 0.0001029342893161811 old loss 0.00010369724623160437 BETTER
I0325 06:01:52.084942 422788 finetune.py:68] layer 7_k @ epoch 0 new loss 0.00015118735609576106 old loss 0.00015386435552500188 BETTER
I0325 06:02:22.917438 423717 finetune.py:68] layer 8_q @ epoch 4 new loss 0.000190274091437459 old loss 0.00019220731337554753 BETTER
I0325 06:02:26.794704 422788 finetune.py:68] layer 7_k @ epoch 1 new loss 0.0001498701167292893 old loss 0.00015118735609576106 BETTER
I0325 06:02:26.860312 421721 finetune.py:68] layer 6_k @ epoch 3 new loss 0.00010229313193121925 old loss 0.0001029342893161811 BETTER
I0325 06:02:47.239274 423717 finetune.py:45] layer 8_k initial loss 0.00021660761558450758
I0325 06:03:01.353083 422788 finetune.py:68] layer 7_k @ epoch 2 new loss 0.00014876550994813442 old loss 0.0001498701167292893 BETTER
I0325 06:03:02.121344 421721 finetune.py:68] layer 6_k @ epoch 4 new loss 0.00010173265764024109 old loss 0.00010229313193121925 BETTER
I0325 06:03:20.329222 423717 finetune.py:68] layer 8_k @ epoch 0 new loss 0.00021336920326575637 old loss 0.00021660761558450758 BETTER
I0325 06:03:24.476328 421721 finetune.py:45] layer 6_o initial loss 0.00026891581364907324
I0325 06:03:36.076670 422788 finetune.py:68] layer 7_k @ epoch 3 new loss 0.00014784019731450826 old loss 0.00014876550994813442 BETTER
I0325 06:03:55.328491 423717 finetune.py:68] layer 8_k @ epoch 1 new loss 0.00021155124704819173 old loss 0.00021336920326575637 BETTER
I0325 06:03:57.683100 421721 finetune.py:68] layer 6_o @ epoch 0 new loss 0.00024947774363681674 old loss 0.00026891581364907324 BETTER
I0325 06:04:10.766983 422788 finetune.py:68] layer 7_k @ epoch 4 new loss 0.0001470156421419233 old loss 0.00014784019731450826 BETTER
I0325 06:04:30.958956 423717 finetune.py:68] layer 8_k @ epoch 2 new loss 0.00021005018788855523 old loss 0.00021155124704819173 BETTER
I0325 06:04:33.056814 421721 finetune.py:68] layer 6_o @ epoch 1 new loss 0.00024348522129002959 old loss 0.00024947774363681674 BETTER
I0325 06:04:35.668534 422788 finetune.py:45] layer 7_o initial loss 0.000362224702257663
I0325 06:05:06.875692 423717 finetune.py:68] layer 8_k @ epoch 3 new loss 0.00020869819854851812 old loss 0.00021005018788855523 BETTER
I0325 06:05:09.058035 422788 finetune.py:68] layer 7_o @ epoch 0 new loss 0.000340372760547325 old loss 0.000362224702257663 BETTER
I0325 06:05:09.208888 421721 finetune.py:68] layer 6_o @ epoch 2 new loss 0.00024045442114584148 old loss 0.00024348522129002959 BETTER
I0325 06:05:43.217191 423717 finetune.py:68] layer 8_k @ epoch 4 new loss 0.00020753902208525687 old loss 0.00020869819854851812 BETTER
I0325 06:05:43.999540 422788 finetune.py:68] layer 7_o @ epoch 1 new loss 0.0003337700618430972 old loss 0.000340372760547325 BETTER
I0325 06:05:45.100668 421721 finetune.py:68] layer 6_o @ epoch 3 new loss 0.000238316337345168 old loss 0.00024045442114584148 BETTER
I0325 06:06:09.048986 423717 finetune.py:45] layer 8_o initial loss 0.0005117403925396502
I0325 06:06:18.581424 422788 finetune.py:68] layer 7_o @ epoch 2 new loss 0.00032998586539179087 old loss 0.0003337700618430972 BETTER
I0325 06:06:20.517307 421721 finetune.py:68] layer 6_o @ epoch 4 new loss 0.0002366242988500744 old loss 0.000238316337345168 BETTER
I0325 06:06:42.283273 423717 finetune.py:68] layer 8_o @ epoch 0 new loss 0.00048247218364849687 old loss 0.0005117403925396502 BETTER
I0325 06:06:54.238468 422788 finetune.py:68] layer 7_o @ epoch 3 new loss 0.0003271583409514278 old loss 0.00032998586539179087 BETTER
I0325 06:06:55.938980 421721 finetune.py:45] layer 6_up initial loss 0.00041481369407847524
I0325 06:07:14.501939 423717 finetune.py:68] layer 8_o @ epoch 1 new loss 0.00047334356349892914 old loss 0.00048247218364849687 BETTER
I0325 06:07:25.830929 421721 finetune.py:68] layer 6_up @ epoch 0 new loss 0.0004108857538085431 old loss 0.00041481369407847524 BETTER
I0325 06:07:26.279752 422788 finetune.py:68] layer 7_o @ epoch 4 new loss 0.00032481280504725873 old loss 0.0003271583409514278 BETTER
I0325 06:07:47.998020 423717 finetune.py:68] layer 8_o @ epoch 2 new loss 0.000468046753667295 old loss 0.00047334356349892914 BETTER
I0325 06:07:56.129336 422788 finetune.py:45] layer 7_up initial loss 0.0005673429113812745
I0325 06:07:59.088329 421721 finetune.py:68] layer 6_up @ epoch 1 new loss 0.0004084568645339459 old loss 0.0004108857538085431 BETTER
I0325 06:08:20.460129 423717 finetune.py:68] layer 8_o @ epoch 3 new loss 0.00046403249143622816 old loss 0.000468046753667295 BETTER
I0325 06:08:25.060408 422788 finetune.py:68] layer 7_up @ epoch 0 new loss 0.0005611781380139291 old loss 0.0005673429113812745 BETTER
I0325 06:08:30.808522 421721 finetune.py:68] layer 6_up @ epoch 2 new loss 0.0004064828681293875 old loss 0.0004084568645339459 BETTER
I0325 06:08:53.682747 423717 finetune.py:68] layer 8_o @ epoch 4 new loss 0.000460689450846985 old loss 0.00046403249143622816 BETTER
I0325 06:08:56.035417 422788 finetune.py:68] layer 7_up @ epoch 1 new loss 0.0005575254908762872 old loss 0.0005611781380139291 BETTER
I0325 06:09:02.510422 421721 finetune.py:68] layer 6_up @ epoch 3 new loss 0.0004047266556881368 old loss 0.0004064828681293875 BETTER
I0325 06:09:22.222623 423717 finetune.py:45] layer 8_up initial loss 0.0007486137328669429
I0325 06:09:27.398975 422788 finetune.py:68] layer 7_up @ epoch 2 new loss 0.0005545277963392437 old loss 0.0005575254908762872 BETTER
I0325 06:09:34.898364 421721 finetune.py:68] layer 6_up @ epoch 4 new loss 0.00040314573561772704 old loss 0.0004047266556881368 BETTER
I0325 06:09:53.513284 423717 finetune.py:68] layer 8_up @ epoch 0 new loss 0.0007403861964121461 old loss 0.0007486137328669429 BETTER
I0325 06:09:58.084623 422788 finetune.py:68] layer 7_up @ epoch 3 new loss 0.000551898090634495 old loss 0.0005545277963392437 BETTER
I0325 06:10:03.876703 421721 finetune.py:45] layer 6_gate initial loss 0.0005068018799647689
I0325 06:10:25.845238 423717 finetune.py:68] layer 8_up @ epoch 1 new loss 0.0007355633424594998 old loss 0.0007403861964121461 BETTER
I0325 06:10:30.842950 422788 finetune.py:68] layer 7_up @ epoch 4 new loss 0.0005495062796398997 old loss 0.000551898090634495 BETTER
I0325 06:10:33.597146 421721 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.0005042096599936485 old loss 0.0005068018799647689 BETTER
I0325 06:10:57.444054 423717 finetune.py:68] layer 8_up @ epoch 2 new loss 0.0007315432303585112 old loss 0.0007355633424594998 BETTER
I0325 06:10:59.541739 422788 finetune.py:45] layer 7_gate initial loss 0.0006882481393404305
I0325 06:11:03.880041 421721 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.0005022867117077112 old loss 0.0005042096599936485 BETTER
I0325 06:11:28.263784 422788 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.0006843730807304382 old loss 0.0006882481393404305 BETTER
I0325 06:11:29.006787 423717 finetune.py:68] layer 8_up @ epoch 3 new loss 0.0007280068821273744 old loss 0.0007315432303585112 BETTER
I0325 06:11:34.263805 421721 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.0005005975253880024 old loss 0.0005022867117077112 BETTER
I0325 06:11:57.955006 422788 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.0006815373199060559 old loss 0.0006843730807304382 BETTER
I0325 06:12:00.999211 423717 finetune.py:68] layer 8_up @ epoch 4 new loss 0.0007247993489727378 old loss 0.0007280068821273744 BETTER
I0325 06:12:04.872152 421721 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.0004990645102225244 old loss 0.0005005975253880024 BETTER
I0325 06:12:27.789367 422788 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.0006790498737245798 old loss 0.0006815373199060559 BETTER
I0325 06:12:31.840853 423717 finetune.py:45] layer 8_gate initial loss 0.0009034828981384635
I0325 06:12:35.462584 421721 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.0004976385971531272 old loss 0.0004990645102225244 BETTER
I0325 06:12:57.023967 422788 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.0006768095190636814 old loss 0.0006790498737245798 BETTER
I0325 06:13:00.589901 423717 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.0008983148145489395 old loss 0.0009034828981384635 BETTER
I0325 06:13:22.353366 421721 finetune.py:45] layer 6_down initial loss 0.0007645418518222868
I0325 06:13:28.846660 422788 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.0006747665465809405 old loss 0.0006768095190636814 BETTER
I0325 06:13:31.575371 423717 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.0008946052985265851 old loss 0.0008983148145489395 BETTER
I0325 06:13:49.781067 421721 finetune.py:68] layer 6_down @ epoch 0 new loss 0.000764150929171592 old loss 0.0007645418518222868 BETTER
I0325 06:14:00.634923 423717 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.0008914546924643219 old loss 0.0008946052985265851 BETTER
I0325 06:14:15.750178 422788 finetune.py:45] layer 7_down initial loss 0.0010286339092999697
I0325 06:14:19.571874 421721 finetune.py:68] layer 6_down @ epoch 1 new loss 0.0007638147217221558 old loss 0.000764150929171592 BETTER
I0325 06:14:32.616993 423717 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.0008886134601198137 old loss 0.0008914546924643219 BETTER
I0325 06:14:43.260863 422788 finetune.py:68] layer 7_down @ epoch 0 new loss 0.001028155442327261 old loss 0.0010286339092999697 BETTER
I0325 06:14:48.503316 421721 finetune.py:68] layer 6_down @ epoch 2 new loss 0.0007635238580405712 old loss 0.0007638147217221558 BETTER
I0325 06:15:03.884606 423717 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.0008860139641910791 old loss 0.0008886134601198137 BETTER
I0325 06:15:11.568140 422788 finetune.py:68] layer 7_down @ epoch 1 new loss 0.0010277444962412119 old loss 0.001028155442327261 BETTER
I0325 06:15:17.900303 421721 finetune.py:68] layer 6_down @ epoch 3 new loss 0.0007632719352841377 old loss 0.0007635238580405712 BETTER
I0325 06:15:39.666690 422788 finetune.py:68] layer 7_down @ epoch 2 new loss 0.0010273815132677555 old loss 0.0010277444962412119 BETTER
I0325 06:15:46.908139 421721 finetune.py:68] layer 6_down @ epoch 4 new loss 0.0007630508625879884 old loss 0.0007632719352841377 BETTER
6_v proxy err 0.05348064750432968 tr(WHW.T) 3211.84423828125
bpp_loss 1.9445325932756532
6_q proxy err 0.0036180117167532444 tr(WHW.T) 54859.24609375
bpp_loss 2.6026145307405386
6_k proxy err 0.0026422881055623293 tr(WHW.T) 75366.3359375
bpp_loss 2.6281833990942687
6_o proxy err 0.05558721721172333 tr(WHW.T) 4099.2001953125
bpp_loss 1.9443126614787616
6_up proxy err 0.04240104928612709 tr(WHW.T) 18098.283203125
bpp_loss 2.186181602176539
6_gate proxy err 0.01748090423643589 tr(WHW.T) 45460.5703125
bpp_loss 2.347283297759849
6_down proxy err 0.051016148179769516 tr(WHW.T) 15813.5068359375
bpp_loss 2.180078201559047
I0325 06:15:52.394840 423717 finetune.py:45] layer 8_down initial loss 0.0013103513047099113
I0325 06:16:07.824823 422788 finetune.py:68] layer 7_down @ epoch 3 new loss 0.0010270680068060756 old loss 0.0010273815132677555 BETTER
I0325 06:16:19.633665 423717 finetune.py:68] layer 8_down @ epoch 0 new loss 0.0013098021736368537 old loss 0.0013103513047099113 BETTER
I0325 06:16:34.851969 422788 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0010267914040014148 old loss 0.0010270680068060756 BETTER
7_v proxy err 0.053510550409555435 tr(WHW.T) 3282.554443359375
bpp_loss 1.952596817893209
7_q proxy err 0.003929505590349436 tr(WHW.T) 51450.12890625
bpp_loss 2.5953003637841903
7_k proxy err 0.0029595252126455307 tr(WHW.T) 68389.203125
bpp_loss 2.605991511751199
7_o proxy err 0.0601634755730629 tr(WHW.T) 3591.654296875
bpp_loss 1.947143337369198
7_up proxy err 0.04120596870779991 tr(WHW.T) 18340.302734375
bpp_loss 2.188449558087213
7_gate proxy err 0.016759468242526054 tr(WHW.T) 46624.78125
bpp_loss 2.3444420602172613
7_down proxy err 0.05098632350564003 tr(WHW.T) 15579.287109375
bpp_loss 2.1823602804892457
I0325 06:16:46.883579 423717 finetune.py:68] layer 8_down @ epoch 1 new loss 0.0013093275483697653 old loss 0.0013098021736368537 BETTER
I0325 06:17:14.255383 423717 finetune.py:68] layer 8_down @ epoch 2 new loss 0.0013089139247313142 old loss 0.0013093275483697653 BETTER
I0325 06:17:41.751450 423717 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0013085505925118923 old loss 0.0013089139247313142 BETTER
I0325 06:17:42.801924 382969 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 63.3048300743103s
I0325 06:17:46.315569 440193 config.py:54] PyTorch version 2.6.0 available.
W0325 06:17:46.615152 440193 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 06:17:47.594527 440193 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 06:17:47.598686 382969 quantize_finetune_llama.py:209] layer 10 gpu 1
I0325 06:17:47.618024 440193 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 06:18:05.574447 440193 finetune.py:45] layer 9_v initial loss 0.00044017290929332376
W0325 06:18:05.574719 440193 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 06:18:09.118476 423717 finetune.py:68] layer 8_down @ epoch 4 new loss 0.0013082303339615464 old loss 0.0013085505925118923 BETTER
8_v proxy err 0.04844759777188301 tr(WHW.T) 3503.97314453125
bpp_loss 1.9785881774732843
8_q proxy err 0.0040798974223434925 tr(WHW.T) 47731.87109375
bpp_loss 2.6113809176022187
8_k proxy err 0.002777381334453821 tr(WHW.T) 70261.6484375
bpp_loss 2.6288436037721112
8_o proxy err 0.06564352661371231 tr(WHW.T) 3182.5556640625
bpp_loss 1.9730880686547607
8_up proxy err 0.03771776705980301 tr(WHW.T) 19950.654296875
bpp_loss 2.202708907858577
8_gate proxy err 0.017053768038749695 tr(WHW.T) 45259.375
bpp_loss 2.324154952021186
8_down proxy err 0.050535909831523895 tr(WHW.T) 15658.869140625
bpp_loss 2.1931527881384936
I0325 06:18:38.406699 440193 finetune.py:68] layer 9_v @ epoch 0 new loss 0.0002678004384506494 old loss 0.00044017290929332376 BETTER
I0325 06:19:12.799721 440193 finetune.py:68] layer 9_v @ epoch 1 new loss 0.00024233065778389573 old loss 0.0002678004384506494 BETTER
I0325 06:19:14.620042 382969 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 60.55653429031372s
I0325 06:19:18.010860 441385 config.py:54] PyTorch version 2.6.0 available.
W0325 06:19:18.307525 441385 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 06:19:19.223975 441385 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 06:19:19.227745 382969 quantize_finetune_llama.py:209] layer 11 gpu 2
I0325 06:19:19.240657 441385 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 06:19:36.920700 441385 finetune.py:45] layer 10_v initial loss 0.0005591209046542645
W0325 06:19:36.921004 441385 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 06:19:47.582648 440193 finetune.py:68] layer 9_v @ epoch 2 new loss 0.00023301001056097448 old loss 0.00024233065778389573 BETTER
I0325 06:20:08.195974 441385 finetune.py:68] layer 10_v @ epoch 0 new loss 0.00038051215233281255 old loss 0.0005591209046542645 BETTER
I0325 06:20:22.711741 382969 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 63.01014256477356s
I0325 06:20:22.826569 440193 finetune.py:68] layer 9_v @ epoch 3 new loss 0.00022741731663700193 old loss 0.00023301001056097448 BETTER
I0325 06:20:26.294120 442257 config.py:54] PyTorch version 2.6.0 available.
W0325 06:20:26.617271 442257 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 06:20:27.570725 442257 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 06:20:27.574800 382969 quantize_finetune_llama.py:209] layer 12 gpu 0
I0325 06:20:27.587899 442257 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 06:20:41.604453 441385 finetune.py:68] layer 10_v @ epoch 1 new loss 0.0003511651884764433 old loss 0.00038051215233281255 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 06:20:49.275089 442257 finetune.py:45] layer 11_v initial loss 0.0005953592481091619
W0325 06:20:49.275376 442257 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 06:20:58.589699 440193 finetune.py:68] layer 9_v @ epoch 4 new loss 0.0002233736595371738 old loss 0.00022741731663700193 BETTER
I0325 06:21:14.796496 441385 finetune.py:68] layer 10_v @ epoch 2 new loss 0.00033784666447900236 old loss 0.0003511651884764433 BETTER
I0325 06:21:20.947607 440193 finetune.py:45] layer 9_q initial loss 0.0002670058165676892
I0325 06:21:23.199533 442257 finetune.py:68] layer 11_v @ epoch 0 new loss 0.00039067131001502275 old loss 0.0005953592481091619 BETTER
I0325 06:21:48.049583 441385 finetune.py:68] layer 10_v @ epoch 3 new loss 0.00032912136521190405 old loss 0.00033784666447900236 BETTER
I0325 06:21:56.038363 440193 finetune.py:68] layer 9_q @ epoch 0 new loss 0.00025903337518684566 old loss 0.0002670058165676892 BETTER
I0325 06:21:57.928493 442257 finetune.py:68] layer 11_v @ epoch 1 new loss 0.00035808217944577336 old loss 0.00039067131001502275 BETTER
I0325 06:22:22.159549 441385 finetune.py:68] layer 10_v @ epoch 4 new loss 0.00032265749177895486 old loss 0.00032912136521190405 BETTER
I0325 06:22:31.488039 440193 finetune.py:68] layer 9_q @ epoch 1 new loss 0.00025498142349533737 old loss 0.00025903337518684566 BETTER
I0325 06:22:32.146432 442257 finetune.py:68] layer 11_v @ epoch 2 new loss 0.00034397109993733466 old loss 0.00035808217944577336 BETTER
I0325 06:22:42.729384 441385 finetune.py:45] layer 10_q initial loss 0.0003742164117284119
I0325 06:23:07.001258 442257 finetune.py:68] layer 11_v @ epoch 3 new loss 0.00033505784813314676 old loss 0.00034397109993733466 BETTER
I0325 06:23:07.140085 440193 finetune.py:68] layer 9_q @ epoch 2 new loss 0.0002517280518077314 old loss 0.00025498142349533737 BETTER
I0325 06:23:14.725966 441385 finetune.py:68] layer 10_q @ epoch 0 new loss 0.0003628260164987296 old loss 0.0003742164117284119 BETTER
I0325 06:23:40.857923 442257 finetune.py:68] layer 11_v @ epoch 4 new loss 0.0003286823339294642 old loss 0.00033505784813314676 BETTER
I0325 06:23:41.939413 440193 finetune.py:68] layer 9_q @ epoch 3 new loss 0.00024900215794332325 old loss 0.0002517280518077314 BETTER
I0325 06:23:47.257592 441385 finetune.py:68] layer 10_q @ epoch 1 new loss 0.0003567805397324264 old loss 0.0003628260164987296 BETTER
I0325 06:24:03.121315 442257 finetune.py:45] layer 11_q initial loss 0.0003868404892273247
I0325 06:24:17.637477 440193 finetune.py:68] layer 9_q @ epoch 4 new loss 0.0002465516154188663 old loss 0.00024900215794332325 BETTER
I0325 06:24:20.563222 441385 finetune.py:68] layer 10_q @ epoch 2 new loss 0.0003518015437293798 old loss 0.0003567805397324264 BETTER
I0325 06:24:36.653416 442257 finetune.py:68] layer 11_q @ epoch 0 new loss 0.00037583374069072306 old loss 0.0003868404892273247 BETTER
I0325 06:24:39.756167 440193 finetune.py:45] layer 9_k initial loss 0.00028082088101655245
I0325 06:24:53.333836 441385 finetune.py:68] layer 10_q @ epoch 3 new loss 0.00034755567321553826 old loss 0.0003518015437293798 BETTER
I0325 06:25:11.464566 442257 finetune.py:68] layer 11_q @ epoch 1 new loss 0.0003699557564686984 old loss 0.00037583374069072306 BETTER
I0325 06:25:13.858247 440193 finetune.py:68] layer 9_k @ epoch 0 new loss 0.00027561397291719913 old loss 0.00028082088101655245 BETTER
I0325 06:25:25.943725 441385 finetune.py:68] layer 10_q @ epoch 4 new loss 0.00034381268778815866 old loss 0.00034755567321553826 BETTER
I0325 06:25:46.359942 442257 finetune.py:68] layer 11_q @ epoch 2 new loss 0.00036531026125885546 old loss 0.0003699557564686984 BETTER
I0325 06:25:48.014573 441385 finetune.py:45] layer 10_k initial loss 0.0003836420364677906
I0325 06:25:49.643637 440193 finetune.py:68] layer 9_k @ epoch 1 new loss 0.000273295387160033 old loss 0.00027561397291719913 BETTER
I0325 06:26:20.731925 441385 finetune.py:68] layer 10_k @ epoch 0 new loss 0.00037711317418143153 old loss 0.0003836420364677906 BETTER
I0325 06:26:21.855574 442257 finetune.py:68] layer 11_q @ epoch 3 new loss 0.0003612928558140993 old loss 0.00036531026125885546 BETTER
I0325 06:26:24.563397 440193 finetune.py:68] layer 9_k @ epoch 2 new loss 0.0002713046851567924 old loss 0.000273295387160033 BETTER
I0325 06:26:54.668774 441385 finetune.py:68] layer 10_k @ epoch 1 new loss 0.00037350505590438843 old loss 0.00037711317418143153 BETTER
I0325 06:26:57.062193 442257 finetune.py:68] layer 11_q @ epoch 4 new loss 0.00035785295767709613 old loss 0.0003612928558140993 BETTER
I0325 06:26:59.405498 440193 finetune.py:68] layer 9_k @ epoch 3 new loss 0.0002695320581551641 old loss 0.0002713046851567924 BETTER
I0325 06:27:19.464011 442257 finetune.py:45] layer 11_k initial loss 0.0004059701750520617
I0325 06:27:28.138701 441385 finetune.py:68] layer 10_k @ epoch 2 new loss 0.0003704159171320498 old loss 0.00037350505590438843 BETTER
I0325 06:27:33.970675 440193 finetune.py:68] layer 9_k @ epoch 4 new loss 0.00026796484598889947 old loss 0.0002695320581551641 BETTER
I0325 06:27:52.915682 442257 finetune.py:68] layer 11_k @ epoch 0 new loss 0.00039875725633464754 old loss 0.0004059701750520617 BETTER
I0325 06:27:56.087235 440193 finetune.py:45] layer 9_o initial loss 0.000647274311631918
I0325 06:28:00.540913 441385 finetune.py:68] layer 10_k @ epoch 3 new loss 0.00036766170524060726 old loss 0.0003704159171320498 BETTER
I0325 06:28:27.705891 442257 finetune.py:68] layer 11_k @ epoch 1 new loss 0.0003952023689635098 old loss 0.00039875725633464754 BETTER
I0325 06:28:29.095009 440193 finetune.py:68] layer 9_o @ epoch 0 new loss 0.0006201526266522706 old loss 0.000647274311631918 BETTER
I0325 06:28:33.681398 441385 finetune.py:68] layer 10_k @ epoch 4 new loss 0.0003651592996902764 old loss 0.00036766170524060726 BETTER
I0325 06:28:55.162457 441385 finetune.py:45] layer 10_o initial loss 0.0008638714789412916
I0325 06:29:02.821028 442257 finetune.py:68] layer 11_k @ epoch 2 new loss 0.0003922698087990284 old loss 0.0003952023689635098 BETTER
I0325 06:29:04.095237 440193 finetune.py:68] layer 9_o @ epoch 1 new loss 0.0006115264841355383 old loss 0.0006201526266522706 BETTER
I0325 06:29:26.598970 441385 finetune.py:68] layer 10_o @ epoch 0 new loss 0.0008340621134266257 old loss 0.0008638714789412916 BETTER
I0325 06:29:36.429435 442257 finetune.py:68] layer 11_k @ epoch 3 new loss 0.0003897004935424775 old loss 0.0003922698087990284 BETTER
I0325 06:29:38.317464 440193 finetune.py:68] layer 9_o @ epoch 2 new loss 0.0006059394800104201 old loss 0.0006115264841355383 BETTER
I0325 06:29:59.020866 441385 finetune.py:68] layer 10_o @ epoch 1 new loss 0.0008234887500293553 old loss 0.0008340621134266257 BETTER
I0325 06:30:11.349554 442257 finetune.py:68] layer 11_k @ epoch 4 new loss 0.00038736488204449415 old loss 0.0003897004935424775 BETTER
I0325 06:30:13.284500 440193 finetune.py:68] layer 9_o @ epoch 3 new loss 0.0006013893871568143 old loss 0.0006059394800104201 BETTER
I0325 06:30:31.536786 441385 finetune.py:68] layer 10_o @ epoch 2 new loss 0.0008158658747561276 old loss 0.0008234887500293553 BETTER
I0325 06:30:34.212195 442257 finetune.py:45] layer 11_o initial loss 0.0009024239843711257
I0325 06:30:47.529310 440193 finetune.py:68] layer 9_o @ epoch 4 new loss 0.0005974819650873542 old loss 0.0006013893871568143 BETTER
I0325 06:31:04.539859 441385 finetune.py:68] layer 10_o @ epoch 3 new loss 0.0008094626246020198 old loss 0.0008158658747561276 BETTER
I0325 06:31:06.838975 442257 finetune.py:68] layer 11_o @ epoch 0 new loss 0.0008704958599992096 old loss 0.0009024239843711257 BETTER
I0325 06:31:18.329747 440193 finetune.py:45] layer 9_up initial loss 0.0009305174462497234
I0325 06:31:37.106247 441385 finetune.py:68] layer 10_o @ epoch 4 new loss 0.0008039133390411735 old loss 0.0008094626246020198 BETTER
I0325 06:31:40.392378 442257 finetune.py:68] layer 11_o @ epoch 1 new loss 0.0008600587025284767 old loss 0.0008704958599992096 BETTER
I0325 06:31:48.874739 440193 finetune.py:68] layer 9_up @ epoch 0 new loss 0.0009215503232553601 old loss 0.0009305174462497234 BETTER
I0325 06:32:06.537606 441385 finetune.py:45] layer 10_up initial loss 0.0011823917739093304
I0325 06:32:13.257010 442257 finetune.py:68] layer 11_o @ epoch 2 new loss 0.0008526677847839892 old loss 0.0008600587025284767 BETTER
I0325 06:32:21.164322 440193 finetune.py:68] layer 9_up @ epoch 1 new loss 0.0009160966728813946 old loss 0.0009215503232553601 BETTER
I0325 06:32:35.957202 441385 finetune.py:68] layer 10_up @ epoch 0 new loss 0.0011707652593031526 old loss 0.0011823917739093304 BETTER
I0325 06:32:47.204235 442257 finetune.py:68] layer 11_o @ epoch 3 new loss 0.0008465146529488266 old loss 0.0008526677847839892 BETTER
I0325 06:32:53.039191 440193 finetune.py:68] layer 9_up @ epoch 2 new loss 0.0009113632841035724 old loss 0.0009160966728813946 BETTER
I0325 06:33:06.363521 441385 finetune.py:68] layer 10_up @ epoch 1 new loss 0.0011636510025709867 old loss 0.0011707652593031526 BETTER
I0325 06:33:21.278185 442257 finetune.py:68] layer 11_o @ epoch 4 new loss 0.0008412629249505699 old loss 0.0008465146529488266 BETTER
I0325 06:33:25.277197 440193 finetune.py:68] layer 9_up @ epoch 3 new loss 0.0009071287349797785 old loss 0.0009113632841035724 BETTER
I0325 06:33:36.329626 441385 finetune.py:68] layer 10_up @ epoch 2 new loss 0.0011574660893529654 old loss 0.0011636510025709867 BETTER
I0325 06:33:50.818713 442257 finetune.py:45] layer 11_up initial loss 0.0012787128798663616
I0325 06:33:57.444878 440193 finetune.py:68] layer 9_up @ epoch 4 new loss 0.0009032907546497881 old loss 0.0009071287349797785 BETTER
I0325 06:34:06.726767 441385 finetune.py:68] layer 10_up @ epoch 3 new loss 0.0011519783874973655 old loss 0.0011574660893529654 BETTER
I0325 06:34:21.164720 442257 finetune.py:68] layer 11_up @ epoch 0 new loss 0.0012664644746109843 old loss 0.0012787128798663616 BETTER
I0325 06:34:27.568595 440193 finetune.py:45] layer 9_gate initial loss 0.001117622246965766
I0325 06:34:37.717804 441385 finetune.py:68] layer 10_up @ epoch 4 new loss 0.0011469044256955385 old loss 0.0011519783874973655 BETTER
I0325 06:34:53.022690 442257 finetune.py:68] layer 11_up @ epoch 1 new loss 0.0012589712860062718 old loss 0.0012664644746109843 BETTER
I0325 06:34:57.068628 440193 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0011120012495666742 old loss 0.001117622246965766 BETTER
I0325 06:35:07.226631 441385 finetune.py:45] layer 10_gate initial loss 0.0014021170791238546
I0325 06:35:25.133981 442257 finetune.py:68] layer 11_up @ epoch 2 new loss 0.0012526552891358733 old loss 0.0012589712860062718 BETTER
I0325 06:35:27.859036 440193 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.0011078320676460862 old loss 0.0011120012495666742 BETTER
I0325 06:35:34.990192 441385 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.0013948269188404083 old loss 0.0014021170791238546 BETTER
I0325 06:35:58.803495 442257 finetune.py:68] layer 11_up @ epoch 3 new loss 0.001246999017894268 old loss 0.0012526552891358733 BETTER
I0325 06:35:59.877843 440193 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.0011041677789762616 old loss 0.0011078320676460862 BETTER
I0325 06:36:04.048613 441385 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.0013896371237933636 old loss 0.0013948269188404083 BETTER
I0325 06:36:31.193419 440193 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.0011008426081389189 old loss 0.0011041677789762616 BETTER
I0325 06:36:31.341787 442257 finetune.py:68] layer 11_up @ epoch 4 new loss 0.0012417706893756986 old loss 0.001246999017894268 BETTER
I0325 06:36:33.155019 441385 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.00138510437682271 old loss 0.0013896371237933636 BETTER
I0325 06:37:01.102483 442257 finetune.py:45] layer 11_gate initial loss 0.0015384694561362267
I0325 06:37:02.367579 440193 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0010977828642353415 old loss 0.0011008426081389189 BETTER
I0325 06:37:02.511728 441385 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.001380899571813643 old loss 0.00138510437682271 BETTER
I0325 06:37:29.677953 442257 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.0015306303976103663 old loss 0.0015384694561362267 BETTER
I0325 06:37:31.210015 441385 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.001377017586492002 old loss 0.001380899571813643 BETTER
I0325 06:37:48.005530 440193 finetune.py:45] layer 9_down initial loss 0.001592836924828589
I0325 06:37:59.307612 442257 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.0015250702854245901 old loss 0.0015306303976103663 BETTER
I0325 06:38:15.109925 440193 finetune.py:68] layer 9_down @ epoch 0 new loss 0.0015922579914331436 old loss 0.001592836924828589 BETTER
I0325 06:38:17.673055 441385 finetune.py:45] layer 10_down initial loss 0.0019450142281129956
I0325 06:38:29.556078 442257 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.0015201742062345147 old loss 0.0015250702854245901 BETTER
I0325 06:38:44.047752 441385 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0019443414639681578 old loss 0.0019450142281129956 BETTER
I0325 06:38:44.182756 440193 finetune.py:68] layer 9_down @ epoch 1 new loss 0.0015917528653517365 old loss 0.0015922579914331436 BETTER
I0325 06:38:59.582582 442257 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.0015156874433159828 old loss 0.0015201742062345147 BETTER
I0325 06:39:11.897858 441385 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0019437598530203104 old loss 0.0019443414639681578 BETTER
I0325 06:39:13.627563 440193 finetune.py:68] layer 9_down @ epoch 2 new loss 0.0015913101378828287 old loss 0.0015917528653517365 BETTER
I0325 06:39:29.643148 442257 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.001511553069576621 old loss 0.0015156874433159828 BETTER
I0325 06:39:39.096175 441385 finetune.py:68] layer 10_down @ epoch 2 new loss 0.0019432506524026394 old loss 0.0019437598530203104 BETTER
I0325 06:39:42.441776 440193 finetune.py:68] layer 9_down @ epoch 3 new loss 0.0015909288777038455 old loss 0.0015913101378828287 BETTER
I0325 06:40:06.314023 441385 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0019428067607805133 old loss 0.0019432506524026394 BETTER
I0325 06:40:11.393020 440193 finetune.py:68] layer 9_down @ epoch 4 new loss 0.0015905952313914895 old loss 0.0015909288777038455 BETTER
9_v proxy err 0.048804350197315216 tr(WHW.T) 3711.38134765625
bpp_loss 1.9947041092382278
9_q proxy err 0.004544140305370092 tr(WHW.T) 45828.23046875
bpp_loss 2.637163429928478
9_k proxy err 0.0028956562746316195 tr(WHW.T) 72253.46875
bpp_loss 2.6723781392502133
9_o proxy err 0.06755511462688446 tr(WHW.T) 3224.6259765625
bpp_loss 1.9903826804074924
9_up proxy err 0.03649140149354935 tr(WHW.T) 20702.41015625
bpp_loss 2.2100331866030776
9_gate proxy err 0.017025727778673172 tr(WHW.T) 45298.28515625
bpp_loss 2.310742251011868
9_down proxy err 0.051222071051597595 tr(WHW.T) 15630.583984375
bpp_loss 2.200191050887021
I0325 06:40:16.745706 442257 finetune.py:45] layer 11_down initial loss 0.0021497851703315973
I0325 06:40:34.074088 441385 finetune.py:68] layer 10_down @ epoch 4 new loss 0.0019424182828515768 old loss 0.0019428067607805133 BETTER
10_v proxy err 0.04948342218995094 tr(WHW.T) 3685.848388671875
bpp_loss 1.9883701242797542
10_q proxy err 0.004752065055072308 tr(WHW.T) 44087.68359375
bpp_loss 2.626957299449714
10_k proxy err 0.0030052289366722107 tr(WHW.T) 70116.796875
bpp_loss 2.666591255227104
10_o proxy err 0.06782151758670807 tr(WHW.T) 3125.811279296875
bpp_loss 1.9890267650480382
10_up proxy err 0.034400079399347305 tr(WHW.T) 21985.375
bpp_loss 2.222176517251619
10_gate proxy err 0.01671845279633999 tr(WHW.T) 45987.78125
bpp_loss 2.3032465127924846
10_down proxy err 0.048110898584127426 tr(WHW.T) 16425.181640625
bpp_loss 2.208313406640014
I0325 06:40:43.673178 442257 finetune.py:68] layer 11_down @ epoch 0 new loss 0.002148966770619154 old loss 0.0021497851703315973 BETTER
I0325 06:41:10.970449 442257 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0021482561714947224 old loss 0.002148966770619154 BETTER
I0325 06:41:38.331097 442257 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0021476324182003736 old loss 0.0021482561714947224 BETTER
I0325 06:41:42.985808 382969 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 64.29645299911499s
I0325 06:41:46.478063 458230 config.py:54] PyTorch version 2.6.0 available.
W0325 06:41:46.809207 458230 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 06:41:47.805294 458230 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 06:41:47.809671 382969 quantize_finetune_llama.py:209] layer 13 gpu 1
I0325 06:41:47.823412 458230 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 06:42:05.839308 458230 finetune.py:45] layer 12_v initial loss 0.0006035320693627
W0325 06:42:05.839507 458230 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 06:42:05.850542 442257 finetune.py:68] layer 11_down @ epoch 3 new loss 0.002147070365026593 old loss 0.0021476324182003736 BETTER
I0325 06:42:33.303164 442257 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0021465779282152653 old loss 0.002147070365026593 BETTER
11_v proxy err 0.04959005489945412 tr(WHW.T) 3928.038330078125
bpp_loss 2.020913658023346
11_q proxy err 0.005677664186805487 tr(WHW.T) 38212.30859375
bpp_loss 2.5209992375748698
11_k proxy err 0.0037869515363126993 tr(WHW.T) 57210.703125
bpp_loss 2.5282934028364252
11_o proxy err 0.06846218556165695 tr(WHW.T) 3129.426025390625
bpp_loss 2.0169956413155887
11_up proxy err 0.03590802103281021 tr(WHW.T) 21660.373046875
bpp_loss 2.2314521823251665
11_gate proxy err 0.017378201708197594 tr(WHW.T) 45376.75390625
bpp_loss 2.3002753290672633
11_down proxy err 0.05034574866294861 tr(WHW.T) 16052.232421875
bpp_loss 2.2164884942670375
I0325 06:42:38.721814 458230 finetune.py:68] layer 12_v @ epoch 0 new loss 0.00041431645513512194 old loss 0.0006035320693627 BETTER
I0325 06:43:13.128721 458230 finetune.py:68] layer 12_v @ epoch 1 new loss 0.0003844108432531357 old loss 0.00041431645513512194 BETTER
I0325 06:43:38.506015 382969 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 60.39025616645813s
I0325 06:43:42.080467 459694 config.py:54] PyTorch version 2.6.0 available.
W0325 06:43:42.375150 459694 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 06:43:43.311332 459694 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 06:43:43.315516 382969 quantize_finetune_llama.py:209] layer 14 gpu 2
I0325 06:43:43.329936 459694 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 06:43:47.863539 458230 finetune.py:68] layer 12_v @ epoch 2 new loss 0.0003710182791110128 old loss 0.0003844108432531357 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 06:44:01.629135 459694 finetune.py:45] layer 13_v initial loss 0.0007635107613168657
W0325 06:44:01.629341 459694 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 06:44:22.857853 458230 finetune.py:68] layer 12_v @ epoch 3 new loss 0.00036246172385290265 old loss 0.0003710182791110128 BETTER
I0325 06:44:32.840798 459694 finetune.py:68] layer 13_v @ epoch 0 new loss 0.0004745260230265558 old loss 0.0007635107613168657 BETTER
I0325 06:44:46.891866 382969 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 63.09395360946655s
I0325 06:44:50.474725 460603 config.py:54] PyTorch version 2.6.0 available.
W0325 06:44:50.798589 460603 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 06:44:51.775755 460603 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 06:44:51.780130 382969 quantize_finetune_llama.py:209] layer 15 gpu 0
I0325 06:44:51.794517 460603 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 06:44:58.194119 458230 finetune.py:68] layer 12_v @ epoch 4 new loss 0.00035604913136921823 old loss 0.00036246172385290265 BETTER
I0325 06:45:05.316487 459694 finetune.py:68] layer 13_v @ epoch 1 new loss 0.00043466666829772294 old loss 0.0004745260230265558 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 06:45:11.862967 460603 finetune.py:45] layer 14_v initial loss 0.0007727473275735974
W0325 06:45:11.863218 460603 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 06:45:18.744904 458230 finetune.py:45] layer 12_q initial loss 0.00042588840005919337
I0325 06:45:38.198312 459694 finetune.py:68] layer 13_v @ epoch 2 new loss 0.00041789832175709307 old loss 0.00043466666829772294 BETTER
I0325 06:45:44.946975 460603 finetune.py:68] layer 14_v @ epoch 0 new loss 0.0005723379435949028 old loss 0.0007727473275735974 BETTER
I0325 06:45:52.153680 458230 finetune.py:68] layer 12_q @ epoch 0 new loss 0.0004136050119996071 old loss 0.00042588840005919337 BETTER
I0325 06:46:11.775801 459694 finetune.py:68] layer 13_v @ epoch 3 new loss 0.0004073032469023019 old loss 0.00041789832175709307 BETTER
I0325 06:46:18.619345 460603 finetune.py:68] layer 14_v @ epoch 1 new loss 0.0005391090526245534 old loss 0.0005723379435949028 BETTER
I0325 06:46:27.398009 458230 finetune.py:68] layer 12_q @ epoch 1 new loss 0.00040718307718634605 old loss 0.0004136050119996071 BETTER
I0325 06:46:45.540970 459694 finetune.py:68] layer 13_v @ epoch 4 new loss 0.0003995857259724289 old loss 0.0004073032469023019 BETTER
I0325 06:46:52.681227 460603 finetune.py:68] layer 14_v @ epoch 2 new loss 0.0005219078157097101 old loss 0.0005391090526245534 BETTER
I0325 06:47:02.417665 458230 finetune.py:68] layer 12_q @ epoch 2 new loss 0.0004020326887257397 old loss 0.00040718307718634605 BETTER
I0325 06:47:06.397623 459694 finetune.py:45] layer 13_q initial loss 0.0004705264000222087
I0325 06:47:27.245361 460603 finetune.py:68] layer 14_v @ epoch 3 new loss 0.000510120764374733 old loss 0.0005219078157097101 BETTER
I0325 06:47:38.089271 458230 finetune.py:68] layer 12_q @ epoch 3 new loss 0.00039777776692062616 old loss 0.0004020326887257397 BETTER
I0325 06:47:38.756868 459694 finetune.py:68] layer 13_q @ epoch 0 new loss 0.0004577532527036965 old loss 0.0004705264000222087 BETTER
I0325 06:48:01.153974 460603 finetune.py:68] layer 14_v @ epoch 4 new loss 0.0005009526503272355 old loss 0.000510120764374733 BETTER
I0325 06:48:11.751516 459694 finetune.py:68] layer 13_q @ epoch 1 new loss 0.00045022001722827554 old loss 0.0004577532527036965 BETTER
I0325 06:48:13.346069 458230 finetune.py:68] layer 12_q @ epoch 4 new loss 0.0003938731097150594 old loss 0.00039777776692062616 BETTER
I0325 06:48:22.461500 460603 finetune.py:45] layer 14_q initial loss 0.0005850124871358275
I0325 06:48:33.595988 458230 finetune.py:45] layer 12_k initial loss 0.0004478911287151277
I0325 06:48:44.657472 459694 finetune.py:68] layer 13_q @ epoch 2 new loss 0.00044417812023311853 old loss 0.00045022001722827554 BETTER
I0325 06:48:55.479342 460603 finetune.py:68] layer 14_q @ epoch 0 new loss 0.0005688103847205639 old loss 0.0005850124871358275 BETTER
I0325 06:49:07.098210 458230 finetune.py:68] layer 12_k @ epoch 0 new loss 0.0004391281690914184 old loss 0.0004478911287151277 BETTER
I0325 06:49:17.909611 459694 finetune.py:68] layer 13_q @ epoch 3 new loss 0.0004390124522615224 old loss 0.00044417812023311853 BETTER
I0325 06:49:28.796669 460603 finetune.py:68] layer 14_q @ epoch 1 new loss 0.0005596955888904631 old loss 0.0005688103847205639 BETTER
I0325 06:49:41.250504 458230 finetune.py:68] layer 12_k @ epoch 1 new loss 0.0004352951655164361 old loss 0.0004391281690914184 BETTER
I0325 06:49:51.335052 459694 finetune.py:68] layer 13_q @ epoch 4 new loss 0.0004345718480180949 old loss 0.0004390124522615224 BETTER
I0325 06:50:02.622777 460603 finetune.py:68] layer 14_q @ epoch 2 new loss 0.0005520742270164192 old loss 0.0005596955888904631 BETTER
I0325 06:50:12.798329 459694 finetune.py:45] layer 13_k initial loss 0.0004929685383103788
I0325 06:50:15.498868 458230 finetune.py:68] layer 12_k @ epoch 2 new loss 0.00043203833047300577 old loss 0.0004352951655164361 BETTER
I0325 06:50:36.661955 460603 finetune.py:68] layer 14_q @ epoch 3 new loss 0.0005455493228510022 old loss 0.0005520742270164192 BETTER
I0325 06:50:44.700652 459694 finetune.py:68] layer 13_k @ epoch 0 new loss 0.0004840870969928801 old loss 0.0004929685383103788 BETTER
I0325 06:50:49.827910 458230 finetune.py:68] layer 12_k @ epoch 3 new loss 0.0004291734076105058 old loss 0.00043203833047300577 BETTER
I0325 06:51:09.819489 460603 finetune.py:68] layer 14_q @ epoch 4 new loss 0.0005397324566729367 old loss 0.0005455493228510022 BETTER
I0325 06:51:17.130580 459694 finetune.py:68] layer 13_k @ epoch 1 new loss 0.00047967402497306466 old loss 0.0004840870969928801 BETTER
I0325 06:51:24.987556 458230 finetune.py:68] layer 12_k @ epoch 4 new loss 0.00042660118197090924 old loss 0.0004291734076105058 BETTER
I0325 06:51:31.195785 460603 finetune.py:45] layer 14_k initial loss 0.000602523039560765
I0325 06:51:47.447842 458230 finetune.py:45] layer 12_o initial loss 0.0009990836260840297
I0325 06:51:51.146206 459694 finetune.py:68] layer 13_k @ epoch 2 new loss 0.00047586020082235336 old loss 0.00047967402497306466 BETTER
I0325 06:52:03.259541 460603 finetune.py:68] layer 14_k @ epoch 0 new loss 0.0005950583145022392 old loss 0.000602523039560765 BETTER
I0325 06:52:20.449860 458230 finetune.py:68] layer 12_o @ epoch 0 new loss 0.0009662539232522249 old loss 0.0009990836260840297 BETTER
I0325 06:52:24.561019 459694 finetune.py:68] layer 13_k @ epoch 3 new loss 0.00047250534407794476 old loss 0.00047586020082235336 BETTER
I0325 06:52:38.046045 460603 finetune.py:68] layer 14_k @ epoch 1 new loss 0.0005897610099054873 old loss 0.0005950583145022392 BETTER
I0325 06:52:54.184493 458230 finetune.py:68] layer 12_o @ epoch 1 new loss 0.0009553625714033842 old loss 0.0009662539232522249 BETTER
I0325 06:52:57.983638 459694 finetune.py:68] layer 13_k @ epoch 4 new loss 0.00046953503624536097 old loss 0.00047250534407794476 BETTER
I0325 06:53:11.819520 460603 finetune.py:68] layer 14_k @ epoch 2 new loss 0.0005852480535395443 old loss 0.0005897610099054873 BETTER
I0325 06:53:20.264513 459694 finetune.py:45] layer 13_o initial loss 0.0010955070611089468
I0325 06:53:28.600641 458230 finetune.py:68] layer 12_o @ epoch 2 new loss 0.0009474621620029211 old loss 0.0009553625714033842 BETTER
I0325 06:53:45.181982 460603 finetune.py:68] layer 14_k @ epoch 3 new loss 0.0005811681039631367 old loss 0.0005852480535395443 BETTER
I0325 06:53:51.773530 459694 finetune.py:68] layer 13_o @ epoch 0 new loss 0.001050432212650776 old loss 0.0010955070611089468 BETTER
I0325 06:54:02.541318 458230 finetune.py:68] layer 12_o @ epoch 3 new loss 0.0009408374899066985 old loss 0.0009474621620029211 BETTER
I0325 06:54:19.730772 460603 finetune.py:68] layer 14_k @ epoch 4 new loss 0.0005773465964011848 old loss 0.0005811681039631367 BETTER
I0325 06:54:24.181410 459694 finetune.py:68] layer 13_o @ epoch 1 new loss 0.0010369413066655397 old loss 0.001050432212650776 BETTER
I0325 06:54:36.829806 458230 finetune.py:68] layer 12_o @ epoch 4 new loss 0.0009351082844659686 old loss 0.0009408374899066985 BETTER
I0325 06:54:42.646180 460603 finetune.py:45] layer 14_o initial loss 0.001329335616901517
I0325 06:54:57.006974 459694 finetune.py:68] layer 13_o @ epoch 2 new loss 0.0010271526407450438 old loss 0.0010369413066655397 BETTER
I0325 06:55:07.835198 458230 finetune.py:45] layer 12_up initial loss 0.001428435673005879
I0325 06:55:15.411675 460603 finetune.py:68] layer 14_o @ epoch 0 new loss 0.0012922817841172218 old loss 0.001329335616901517 BETTER
I0325 06:55:29.486825 459694 finetune.py:68] layer 13_o @ epoch 3 new loss 0.0010192269692197442 old loss 0.0010271526407450438 BETTER
I0325 06:55:39.125064 458230 finetune.py:68] layer 12_up @ epoch 0 new loss 0.0014148543123155832 old loss 0.001428435673005879 BETTER
I0325 06:55:49.129533 460603 finetune.py:68] layer 14_o @ epoch 1 new loss 0.0012786306906491518 old loss 0.0012922817841172218 BETTER
I0325 06:56:02.765729 459694 finetune.py:68] layer 13_o @ epoch 4 new loss 0.001012296648696065 old loss 0.0010192269692197442 BETTER
I0325 06:56:11.231103 458230 finetune.py:68] layer 12_up @ epoch 1 new loss 0.0014063650742173195 old loss 0.0014148543123155832 BETTER
I0325 06:56:22.731845 460603 finetune.py:68] layer 14_o @ epoch 2 new loss 0.001268252031877637 old loss 0.0012786306906491518 BETTER
I0325 06:56:33.463250 459694 finetune.py:45] layer 13_up initial loss 0.001596258720383048
I0325 06:56:44.481438 458230 finetune.py:68] layer 12_up @ epoch 2 new loss 0.0013991015730425715 old loss 0.0014063650742173195 BETTER
I0325 06:56:57.370925 460603 finetune.py:68] layer 14_o @ epoch 3 new loss 0.0012595388107001781 old loss 0.001268252031877637 BETTER
I0325 06:57:03.602997 459694 finetune.py:68] layer 13_up @ epoch 0 new loss 0.0015784472925588489 old loss 0.001596258720383048 BETTER
I0325 06:57:17.787547 458230 finetune.py:68] layer 12_up @ epoch 3 new loss 0.0013926276005804539 old loss 0.0013991015730425715 BETTER
I0325 06:57:32.059967 460603 finetune.py:68] layer 14_o @ epoch 4 new loss 0.0012518505100160837 old loss 0.0012595388107001781 BETTER
I0325 06:57:34.490290 459694 finetune.py:68] layer 13_up @ epoch 1 new loss 0.0015672901645302773 old loss 0.0015784472925588489 BETTER
I0325 06:57:50.537640 458230 finetune.py:68] layer 12_up @ epoch 4 new loss 0.001386736985296011 old loss 0.0013926276005804539 BETTER
I0325 06:58:03.727724 460603 finetune.py:45] layer 14_up initial loss 0.0019190111197531223
I0325 06:58:05.804997 459694 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0015579968458041549 old loss 0.0015672901645302773 BETTER
I0325 06:58:21.224301 458230 finetune.py:45] layer 12_gate initial loss 0.0017364142695441842
I0325 06:58:34.879083 460603 finetune.py:68] layer 14_up @ epoch 0 new loss 0.0018982357578352094 old loss 0.0019190111197531223 BETTER
I0325 06:58:37.234442 459694 finetune.py:68] layer 13_up @ epoch 3 new loss 0.0015498162247240543 old loss 0.0015579968458041549 BETTER
I0325 06:58:51.042905 458230 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.0017275895224884152 old loss 0.0017364142695441842 BETTER
I0325 06:59:07.554473 460603 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0018854268128052354 old loss 0.0018982357578352094 BETTER
I0325 06:59:09.316669 459694 finetune.py:68] layer 13_up @ epoch 4 new loss 0.0015424448065459728 old loss 0.0015498162247240543 BETTER
I0325 06:59:21.667786 458230 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0017212564125657082 old loss 0.0017275895224884152 BETTER
I0325 06:59:39.736304 459694 finetune.py:45] layer 13_gate initial loss 0.0019658238161355257
I0325 06:59:40.339699 460603 finetune.py:68] layer 14_up @ epoch 2 new loss 0.001874844660051167 old loss 0.0018854268128052354 BETTER
I0325 06:59:52.766331 458230 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.001715613529086113 old loss 0.0017212564125657082 BETTER
I0325 07:00:08.461294 459694 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.001954935723915696 old loss 0.0019658238161355257 BETTER
I0325 07:00:12.639788 460603 finetune.py:68] layer 14_up @ epoch 3 new loss 0.0018655778840184212 old loss 0.001874844660051167 BETTER
I0325 07:00:24.271743 458230 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.0017104267608374357 old loss 0.001715613529086113 BETTER
I0325 07:00:38.777901 459694 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0019468667451292276 old loss 0.001954935723915696 BETTER
I0325 07:00:45.294859 460603 finetune.py:68] layer 14_up @ epoch 4 new loss 0.0018571934197098017 old loss 0.0018655778840184212 BETTER
I0325 07:00:55.072048 458230 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.0017056157812476158 old loss 0.0017104267608374357 BETTER
I0325 07:01:07.676415 459694 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0019396863644942641 old loss 0.0019468667451292276 BETTER
I0325 07:01:15.872066 460603 finetune.py:45] layer 14_gate initial loss 0.002367176581174135
I0325 07:01:37.049924 459694 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.0019330403301864862 old loss 0.0019396863644942641 BETTER
I0325 07:01:41.958786 458230 finetune.py:45] layer 12_down initial loss 0.002435403410345316
I0325 07:01:45.678330 460603 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0023545455187559128 old loss 0.002367176581174135 BETTER
I0325 07:02:06.913576 459694 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0019269516924396157 old loss 0.0019330403301864862 BETTER
I0325 07:02:09.702896 458230 finetune.py:68] layer 12_down @ epoch 0 new loss 0.002434400375932455 old loss 0.002435403410345316 BETTER
I0325 07:02:16.283941 460603 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.002345205284655094 old loss 0.0023545455187559128 BETTER
I0325 07:02:38.830423 458230 finetune.py:68] layer 12_down @ epoch 1 new loss 0.0024335067719221115 old loss 0.002434400375932455 BETTER
I0325 07:02:45.929949 460603 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.002336770761758089 old loss 0.002345205284655094 BETTER
I0325 07:02:54.693918 459694 finetune.py:45] layer 13_down initial loss 0.002813895931467414
I0325 07:03:08.070993 458230 finetune.py:68] layer 12_down @ epoch 2 new loss 0.0024327016435563564 old loss 0.0024335067719221115 BETTER
I0325 07:03:16.251823 460603 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.0023290361277759075 old loss 0.002336770761758089 BETTER
I0325 07:03:20.919192 459694 finetune.py:68] layer 13_down @ epoch 0 new loss 0.002812580903992057 old loss 0.002813895931467414 BETTER
I0325 07:03:37.646239 458230 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0024319894146174192 old loss 0.0024327016435563564 BETTER
I0325 07:03:46.955114 460603 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0023218030110001564 old loss 0.0023290361277759075 BETTER
I0325 07:03:49.101843 459694 finetune.py:68] layer 13_down @ epoch 1 new loss 0.002811416983604431 old loss 0.002812580903992057 BETTER
I0325 07:04:06.552417 458230 finetune.py:68] layer 12_down @ epoch 4 new loss 0.002431346569210291 old loss 0.0024319894146174192 BETTER
12_v proxy err 0.05194605886936188 tr(WHW.T) 3843.647705078125
bpp_loss 2.0102192989725154
12_q proxy err 0.005863016005605459 tr(WHW.T) 38559.52734375
bpp_loss 2.5784134098212235
12_k proxy err 0.0038175098598003387 tr(WHW.T) 59621.98046875
bpp_loss 2.623481602378888
12_o proxy err 0.07130378484725952 tr(WHW.T) 3075.142578125
bpp_loss 2.011073688539909
12_up proxy err 0.03592591732740402 tr(WHW.T) 21906.5390625
bpp_loss 2.2421310504162033
12_gate proxy err 0.018720947206020355 tr(WHW.T) 42429.5234375
bpp_loss 2.2938356486603966
12_down proxy err 0.05037541314959526 tr(WHW.T) 16118.7021484375
bpp_loss 2.226307100248198
I0325 07:04:17.173547 459694 finetune.py:68] layer 13_down @ epoch 2 new loss 0.0028103687800467014 old loss 0.002811416983604431 BETTER
I0325 07:04:33.726431 460603 finetune.py:45] layer 14_down initial loss 0.003367462893947959
I0325 07:04:43.981530 459694 finetune.py:68] layer 13_down @ epoch 3 new loss 0.002809408586472273 old loss 0.0028103687800467014 BETTER
I0325 07:04:59.636789 460603 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0033656288869678974 old loss 0.003367462893947959 BETTER
I0325 07:05:10.836924 459694 finetune.py:68] layer 13_down @ epoch 4 new loss 0.002808554098010063 old loss 0.002809408586472273 BETTER
13_v proxy err 0.05422952026128769 tr(WHW.T) 3922.961669921875
bpp_loss 2.040907668386353
13_q proxy err 0.006231407634913921 tr(WHW.T) 38237.1875
bpp_loss 2.5682592940574978
13_k proxy err 0.00417355727404356 tr(WHW.T) 57318.4609375
bpp_loss 2.594930739403935
13_o proxy err 0.06530341506004333 tr(WHW.T) 3476.9375
bpp_loss 2.0394893975171726
13_up proxy err 0.034193288534879684 tr(WHW.T) 22785.55078125
bpp_loss 2.253987031317381
13_gate proxy err 0.018134023994207382 tr(WHW.T) 43219.90625
bpp_loss 2.289546824731799
13_down proxy err 0.04920381307601929 tr(WHW.T) 16110.0751953125
bpp_loss 2.2349570418253193
I0325 07:05:26.655215 460603 finetune.py:68] layer 14_down @ epoch 1 new loss 0.003363966243341565 old loss 0.0033656288869678974 BETTER
I0325 07:05:53.677679 460603 finetune.py:68] layer 14_down @ epoch 2 new loss 0.003362452145665884 old loss 0.003363966243341565 BETTER
I0325 07:06:17.990587 382969 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 62.57938051223755s
I0325 07:06:20.902710 460603 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0033610796090215445 old loss 0.003362452145665884 BETTER
I0325 07:06:21.584757 476818 config.py:54] PyTorch version 2.6.0 available.
W0325 07:06:21.890136 476818 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 07:06:22.942579 476818 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 07:06:22.946367 382969 quantize_finetune_llama.py:209] layer 16 gpu 1
I0325 07:06:22.959291 476818 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 07:06:40.889965 476818 finetune.py:45] layer 15_v initial loss 0.0007836918230168521
W0325 07:06:40.890184 476818 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 07:06:48.052138 460603 finetune.py:68] layer 14_down @ epoch 4 new loss 0.003359808586537838 old loss 0.0033610796090215445 BETTER
14_v proxy err 0.05636068433523178 tr(WHW.T) 3685.144287109375
bpp_loss 2.0250950004847255
14_q proxy err 0.006315120495855808 tr(WHW.T) 36978.34765625
bpp_loss 2.5615314107271843
14_k proxy err 0.003966158721596003 tr(WHW.T) 59054.53515625
bpp_loss 2.593785703094909
14_o proxy err 0.07137374579906464 tr(WHW.T) 3141.052978515625
bpp_loss 2.0246930801076815
14_up proxy err 0.03533584251999855 tr(WHW.T) 22574.0703125
bpp_loss 2.2558416489425093
14_gate proxy err 0.019430819898843765 tr(WHW.T) 41262.16015625
bpp_loss 2.286890156827001
14_down proxy err 0.05077078193426132 tr(WHW.T) 15773.7607421875
bpp_loss 2.2369439540279292
I0325 07:07:13.390078 476818 finetune.py:68] layer 15_v @ epoch 0 new loss 0.0005662164185196161 old loss 0.0007836918230168521 BETTER
I0325 07:07:47.306727 476818 finetune.py:68] layer 15_v @ epoch 1 new loss 0.0005360891227610409 old loss 0.0005662164185196161 BETTER
I0325 07:07:53.067278 382969 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 60.28377842903137s
I0325 07:07:56.491906 478045 config.py:54] PyTorch version 2.6.0 available.
W0325 07:07:56.805106 478045 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 07:07:57.726761 478045 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 07:07:57.730890 382969 quantize_finetune_llama.py:209] layer 17 gpu 2
I0325 07:07:57.744860 478045 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 07:08:15.785730 478045 finetune.py:45] layer 16_v initial loss 0.0010538160568103194
W0325 07:08:15.785944 478045 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 07:08:21.897994 476818 finetune.py:68] layer 15_v @ epoch 2 new loss 0.0005201518069952726 old loss 0.0005360891227610409 BETTER
I0325 07:08:47.201932 478045 finetune.py:68] layer 16_v @ epoch 0 new loss 0.0007353451219387352 old loss 0.0010538160568103194 BETTER
I0325 07:08:56.641885 476818 finetune.py:68] layer 15_v @ epoch 3 new loss 0.0005087804747745395 old loss 0.0005201518069952726 BETTER
I0325 07:09:01.336196 382969 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 63.13836479187012s
I0325 07:09:05.046895 478952 config.py:54] PyTorch version 2.6.0 available.
W0325 07:09:05.396156 478952 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 07:09:06.480505 478952 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 07:09:06.484546 382969 quantize_finetune_llama.py:209] layer 18 gpu 0
I0325 07:09:06.498315 478952 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 07:09:20.127221 478045 finetune.py:68] layer 16_v @ epoch 1 new loss 0.0006924150511622429 old loss 0.0007353451219387352 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 07:09:29.578601 478952 finetune.py:45] layer 17_v initial loss 0.001124212983995676
W0325 07:09:29.578835 478952 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 07:09:32.714048 476818 finetune.py:68] layer 15_v @ epoch 4 new loss 0.000499958114232868 old loss 0.0005087804747745395 BETTER
I0325 07:09:55.209932 476818 finetune.py:45] layer 15_q initial loss 0.000597929407376796
I0325 07:09:55.311333 478045 finetune.py:68] layer 16_v @ epoch 2 new loss 0.000670659530442208 old loss 0.0006924150511622429 BETTER
I0325 07:10:03.759169 478952 finetune.py:68] layer 17_v @ epoch 0 new loss 0.0006447797641158104 old loss 0.001124212983995676 BETTER
I0325 07:10:29.383913 476818 finetune.py:68] layer 15_q @ epoch 0 new loss 0.0005790089489892125 old loss 0.000597929407376796 BETTER
I0325 07:10:29.972131 478045 finetune.py:68] layer 16_v @ epoch 3 new loss 0.000655614712741226 old loss 0.000670659530442208 BETTER
I0325 07:10:39.044738 478952 finetune.py:68] layer 17_v @ epoch 1 new loss 0.000587565591558814 old loss 0.0006447797641158104 BETTER
I0325 07:11:05.013914 478045 finetune.py:68] layer 16_v @ epoch 4 new loss 0.0006439980934374034 old loss 0.000655614712741226 BETTER
I0325 07:11:05.396869 476818 finetune.py:68] layer 15_q @ epoch 1 new loss 0.0005690910038538277 old loss 0.0005790089489892125 BETTER
I0325 07:11:14.520876 478952 finetune.py:68] layer 17_v @ epoch 2 new loss 0.0005637728027068079 old loss 0.000587565591558814 BETTER
I0325 07:11:28.923212 478045 finetune.py:45] layer 16_q initial loss 0.0007599461823701859
I0325 07:11:41.909919 476818 finetune.py:68] layer 15_q @ epoch 2 new loss 0.0005609652143903077 old loss 0.0005690910038538277 BETTER
I0325 07:11:48.998772 478952 finetune.py:68] layer 17_v @ epoch 3 new loss 0.0005485473666340113 old loss 0.0005637728027068079 BETTER
I0325 07:12:02.795670 478045 finetune.py:68] layer 16_q @ epoch 0 new loss 0.0007349934312514961 old loss 0.0007599461823701859 BETTER
I0325 07:12:18.072085 476818 finetune.py:68] layer 15_q @ epoch 3 new loss 0.0005540355341508985 old loss 0.0005609652143903077 BETTER
I0325 07:12:26.144356 478952 finetune.py:68] layer 17_v @ epoch 4 new loss 0.0005373735330067575 old loss 0.0005485473666340113 BETTER
I0325 07:12:36.549449 478045 finetune.py:68] layer 16_q @ epoch 1 new loss 0.0007223444408737123 old loss 0.0007349934312514961 BETTER
I0325 07:12:50.524353 478952 finetune.py:45] layer 17_q initial loss 0.0006562988855876029
I0325 07:12:54.701452 476818 finetune.py:68] layer 15_q @ epoch 4 new loss 0.0005478448583744466 old loss 0.0005540355341508985 BETTER
I0325 07:13:10.438055 478045 finetune.py:68] layer 16_q @ epoch 2 new loss 0.0007119567017070949 old loss 0.0007223444408737123 BETTER
I0325 07:13:18.242098 476818 finetune.py:45] layer 15_k initial loss 0.0006236204062588513
I0325 07:13:24.406049 478952 finetune.py:68] layer 17_q @ epoch 0 new loss 0.0006260741502046585 old loss 0.0006562988855876029 BETTER
I0325 07:13:43.037107 478045 finetune.py:68] layer 16_q @ epoch 3 new loss 0.0007031038985587656 old loss 0.0007119567017070949 BETTER
I0325 07:13:50.852945 476818 finetune.py:68] layer 15_k @ epoch 0 new loss 0.0006122421473264694 old loss 0.0006236204062588513 BETTER
I0325 07:13:57.224455 478952 finetune.py:68] layer 17_q @ epoch 1 new loss 0.000613226555287838 old loss 0.0006260741502046585 BETTER
I0325 07:14:15.227845 478045 finetune.py:68] layer 16_q @ epoch 4 new loss 0.0006953830597922206 old loss 0.0007031038985587656 BETTER
I0325 07:14:24.603812 476818 finetune.py:68] layer 15_k @ epoch 1 new loss 0.0006063176551833749 old loss 0.0006122421473264694 BETTER
I0325 07:14:30.113106 478952 finetune.py:68] layer 17_q @ epoch 2 new loss 0.000603070599026978 old loss 0.000613226555287838 BETTER
I0325 07:14:34.685462 478045 finetune.py:45] layer 16_k initial loss 0.0007824775530025363
I0325 07:14:58.440944 476818 finetune.py:68] layer 15_k @ epoch 2 new loss 0.000601290026679635 old loss 0.0006063176551833749 BETTER
I0325 07:15:02.888261 478952 finetune.py:68] layer 17_q @ epoch 3 new loss 0.0005946486489847302 old loss 0.000603070599026978 BETTER
I0325 07:15:06.089764 478045 finetune.py:68] layer 16_k @ epoch 0 new loss 0.0007707157055847347 old loss 0.0007824775530025363 BETTER
I0325 07:15:32.261996 476818 finetune.py:68] layer 15_k @ epoch 3 new loss 0.0005968046607449651 old loss 0.000601290026679635 BETTER
I0325 07:15:36.073157 478952 finetune.py:68] layer 17_q @ epoch 4 new loss 0.000587340968195349 old loss 0.0005946486489847302 BETTER
I0325 07:15:38.261852 478045 finetune.py:68] layer 16_k @ epoch 1 new loss 0.0007633695495314896 old loss 0.0007707157055847347 BETTER
I0325 07:15:55.834129 478952 finetune.py:45] layer 17_k initial loss 0.00068009237293154
I0325 07:16:06.251112 476818 finetune.py:68] layer 15_k @ epoch 4 new loss 0.0005928040482103825 old loss 0.0005968046607449651 BETTER
I0325 07:16:10.492091 478045 finetune.py:68] layer 16_k @ epoch 2 new loss 0.0007570257293991745 old loss 0.0007633695495314896 BETTER
I0325 07:16:25.221594 476818 finetune.py:45] layer 15_o initial loss 0.0013668303145095706
I0325 07:16:27.489810 478952 finetune.py:68] layer 17_k @ epoch 0 new loss 0.0006679981597699225 old loss 0.00068009237293154 BETTER
I0325 07:16:42.940928 478045 finetune.py:68] layer 16_k @ epoch 3 new loss 0.0007514396565966308 old loss 0.0007570257293991745 BETTER
I0325 07:16:57.275255 476818 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0013268172042444348 old loss 0.0013668303145095706 BETTER
I0325 07:17:00.223665 478952 finetune.py:68] layer 17_k @ epoch 1 new loss 0.0006608692347072065 old loss 0.0006679981597699225 BETTER
I0325 07:17:15.474929 478045 finetune.py:68] layer 16_k @ epoch 4 new loss 0.0007463595829904079 old loss 0.0007514396565966308 BETTER
I0325 07:17:30.519827 476818 finetune.py:68] layer 15_o @ epoch 1 new loss 0.0013117461930960417 old loss 0.0013268172042444348 BETTER
I0325 07:17:32.857007 478952 finetune.py:68] layer 17_k @ epoch 2 new loss 0.0006547573721036315 old loss 0.0006608692347072065 BETTER
I0325 07:17:34.820988 478045 finetune.py:45] layer 16_o initial loss 0.0018205285305157304
I0325 07:18:03.826595 476818 finetune.py:68] layer 15_o @ epoch 2 new loss 0.0012999766040593386 old loss 0.0013117461930960417 BETTER
I0325 07:18:05.474210 478045 finetune.py:68] layer 16_o @ epoch 0 new loss 0.0017604791792109609 old loss 0.0018205285305157304 BETTER
I0325 07:18:05.576665 478952 finetune.py:68] layer 17_k @ epoch 3 new loss 0.0006494264234788716 old loss 0.0006547573721036315 BETTER
I0325 07:18:37.045825 478045 finetune.py:68] layer 16_o @ epoch 1 new loss 0.0017386580584570765 old loss 0.0017604791792109609 BETTER
I0325 07:18:37.333996 476818 finetune.py:68] layer 15_o @ epoch 3 new loss 0.0012899867724627256 old loss 0.0012999766040593386 BETTER
I0325 07:18:38.631911 478952 finetune.py:68] layer 17_k @ epoch 4 new loss 0.0006446648621931672 old loss 0.0006494264234788716 BETTER
I0325 07:18:58.404084 478952 finetune.py:45] layer 17_o initial loss 0.0014743548817932606
I0325 07:19:08.754146 478045 finetune.py:68] layer 16_o @ epoch 2 new loss 0.0017221792368218303 old loss 0.0017386580584570765 BETTER
I0325 07:19:10.876505 476818 finetune.py:68] layer 15_o @ epoch 4 new loss 0.0012812595814466476 old loss 0.0012899867724627256 BETTER
I0325 07:19:29.614769 478952 finetune.py:68] layer 17_o @ epoch 0 new loss 0.00141774897929281 old loss 0.0014743548817932606 BETTER
I0325 07:19:37.040922 476818 finetune.py:45] layer 15_up initial loss 0.002081072423607111
I0325 07:19:40.624057 478045 finetune.py:68] layer 16_o @ epoch 3 new loss 0.0017083939164876938 old loss 0.0017221792368218303 BETTER
I0325 07:20:01.805352 478952 finetune.py:68] layer 17_o @ epoch 1 new loss 0.0014013920444995165 old loss 0.00141774897929281 BETTER
I0325 07:20:07.432954 476818 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0020552300848066807 old loss 0.002081072423607111 BETTER
I0325 07:20:12.377511 478045 finetune.py:68] layer 16_o @ epoch 4 new loss 0.001696316059678793 old loss 0.0017083939164876938 BETTER
I0325 07:20:34.228981 478952 finetune.py:68] layer 17_o @ epoch 2 new loss 0.0013889713445678353 old loss 0.0014013920444995165 BETTER
I0325 07:20:38.615688 478045 finetune.py:45] layer 16_up initial loss 0.0027256321627646685
I0325 07:20:39.160364 476818 finetune.py:68] layer 15_up @ epoch 1 new loss 0.0020393901504576206 old loss 0.0020552300848066807 BETTER
I0325 07:21:06.458731 478952 finetune.py:68] layer 17_o @ epoch 3 new loss 0.001378447050228715 old loss 0.0013889713445678353 BETTER
I0325 07:21:07.731001 478045 finetune.py:68] layer 16_up @ epoch 0 new loss 0.002691563218832016 old loss 0.0027256321627646685 BETTER
I0325 07:21:10.969050 476818 finetune.py:68] layer 15_up @ epoch 2 new loss 0.002026427537202835 old loss 0.0020393901504576206 BETTER
I0325 07:21:37.809091 478045 finetune.py:68] layer 16_up @ epoch 1 new loss 0.002670744899660349 old loss 0.002691563218832016 BETTER
I0325 07:21:38.737253 478952 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0013693596702069044 old loss 0.001378447050228715 BETTER
I0325 07:21:42.928246 476818 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0020150027703493834 old loss 0.002026427537202835 BETTER
I0325 07:22:06.546216 478952 finetune.py:45] layer 17_up initial loss 0.002554501872509718
I0325 07:22:08.579828 478045 finetune.py:68] layer 16_up @ epoch 2 new loss 0.0026541086845099926 old loss 0.002670744899660349 BETTER
I0325 07:22:15.065347 476818 finetune.py:68] layer 15_up @ epoch 4 new loss 0.002004900248721242 old loss 0.0020150027703493834 BETTER
I0325 07:22:36.617047 478952 finetune.py:68] layer 17_up @ epoch 0 new loss 0.002522870199754834 old loss 0.002554501872509718 BETTER
I0325 07:22:38.870754 478045 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0026396173052489758 old loss 0.0026541086845099926 BETTER
I0325 07:22:42.794440 476818 finetune.py:45] layer 15_gate initial loss 0.00262629147619009
I0325 07:23:08.356164 478952 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0025036465376615524 old loss 0.002522870199754834 BETTER
I0325 07:23:10.254281 478045 finetune.py:68] layer 16_up @ epoch 4 new loss 0.0026268181391060352 old loss 0.0026396173052489758 BETTER
I0325 07:23:11.906332 476818 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.0026110662147402763 old loss 0.00262629147619009 BETTER
I0325 07:23:37.927321 478045 finetune.py:45] layer 16_gate initial loss 0.00344239454716444
I0325 07:23:40.495918 478952 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0024885283783078194 old loss 0.0025036465376615524 BETTER
I0325 07:23:42.469862 476818 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.0025995715986937284 old loss 0.0026110662147402763 BETTER
I0325 07:24:05.496975 478045 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.003423296380788088 old loss 0.00344239454716444 BETTER
I0325 07:24:11.269644 478952 finetune.py:68] layer 17_up @ epoch 3 new loss 0.0024752847384661436 old loss 0.0024885283783078194 BETTER
I0325 07:24:12.813090 476818 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.0025893044658005238 old loss 0.0025995715986937284 BETTER
I0325 07:24:34.043087 478045 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0034083377104252577 old loss 0.003423296380788088 BETTER
I0325 07:24:42.297517 478952 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0024635742884129286 old loss 0.0024752847384661436 BETTER
I0325 07:24:43.182558 476818 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0025797486305236816 old loss 0.0025893044658005238 BETTER
I0325 07:25:02.588468 478045 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.003394757630303502 old loss 0.0034083377104252577 BETTER
I0325 07:25:10.355280 478952 finetune.py:45] layer 17_gate initial loss 0.003399475710466504
I0325 07:25:14.180846 476818 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.002570891287177801 old loss 0.0025797486305236816 BETTER
I0325 07:25:31.105499 478045 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.003382293740287423 old loss 0.003394757630303502 BETTER
I0325 07:25:38.255859 478952 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.003381576156243682 old loss 0.003399475710466504 BETTER
I0325 07:25:59.259914 476818 finetune.py:45] layer 15_down initial loss 0.0038712662644684315
I0325 07:26:00.879211 478045 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.003370694350451231 old loss 0.003382293740287423 BETTER
I0325 07:26:07.303449 478952 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.003366974415257573 old loss 0.003381576156243682 BETTER
I0325 07:26:25.799415 476818 finetune.py:68] layer 15_down @ epoch 0 new loss 0.003869180101901293 old loss 0.0038712662644684315 BETTER
I0325 07:26:36.794863 478952 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.003353691194206476 old loss 0.003366974415257573 BETTER
I0325 07:26:47.410201 478045 finetune.py:45] layer 16_down initial loss 0.005147862248122692
I0325 07:26:54.149132 476818 finetune.py:68] layer 15_down @ epoch 1 new loss 0.003867301158607006 old loss 0.003869180101901293 BETTER
I0325 07:27:05.932502 478952 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.0033414687495678663 old loss 0.003353691194206476 BETTER
I0325 07:27:13.512075 478045 finetune.py:68] layer 16_down @ epoch 0 new loss 0.005144758149981499 old loss 0.005147862248122692 BETTER
I0325 07:27:22.955854 476818 finetune.py:68] layer 15_down @ epoch 2 new loss 0.003865587990731001 old loss 0.003867301158607006 BETTER
I0325 07:27:36.707962 478952 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.003330104285851121 old loss 0.0033414687495678663 BETTER
I0325 07:27:40.862391 478045 finetune.py:68] layer 16_down @ epoch 1 new loss 0.005141879431903362 old loss 0.005144758149981499 BETTER
I0325 07:27:51.993786 476818 finetune.py:68] layer 15_down @ epoch 3 new loss 0.003864025231450796 old loss 0.003865587990731001 BETTER
I0325 07:28:08.080159 478045 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0051392000168561935 old loss 0.005141879431903362 BETTER
I0325 07:28:21.587989 476818 finetune.py:68] layer 15_down @ epoch 4 new loss 0.003862591926008463 old loss 0.003864025231450796 BETTER
15_v proxy err 0.05123722553253174 tr(WHW.T) 4043.675048828125
bpp_loss 2.0623099767253734
15_q proxy err 0.005979056004434824 tr(WHW.T) 38501.41015625
bpp_loss 2.553307370748371
15_k proxy err 0.003947176970541477 tr(WHW.T) 58801.98828125
bpp_loss 2.603709223039914
15_o proxy err 0.06094498932361603 tr(WHW.T) 3717.42724609375
bpp_loss 2.053609218040947
15_up proxy err 0.033887576311826706 tr(WHW.T) 23183.0859375
bpp_loss 2.261870062541823
15_gate proxy err 0.0192714836448431 tr(WHW.T) 40966.453125
bpp_loss 2.292314168152421
15_down proxy err 0.04971965402364731 tr(WHW.T) 15792.3876953125
bpp_loss 2.239630738240775
I0325 07:28:25.326322 478952 finetune.py:45] layer 17_down initial loss 0.005337778478860855
I0325 07:28:35.859239 478045 finetune.py:68] layer 16_down @ epoch 3 new loss 0.005136673804372549 old loss 0.0051392000168561935 BETTER
I0325 07:28:52.276080 478952 finetune.py:68] layer 17_down @ epoch 0 new loss 0.005334345158189535 old loss 0.005337778478860855 BETTER
I0325 07:29:02.803929 478045 finetune.py:68] layer 16_down @ epoch 4 new loss 0.005134318955242634 old loss 0.005136673804372549 BETTER
16_v proxy err 0.05334056541323662 tr(WHW.T) 4032.874755859375
bpp_loss 2.100724526826525
16_q proxy err 0.006344116758555174 tr(WHW.T) 37181.51171875
bpp_loss 2.5345854660263285
16_k proxy err 0.003940615803003311 tr(WHW.T) 60174.5078125
bpp_loss 2.576831204118207
16_o proxy err 0.0544218085706234 tr(WHW.T) 4785.98486328125
bpp_loss 2.1061536883062217
16_up proxy err 0.033327069133520126 tr(WHW.T) 23652.755859375
bpp_loss 2.262855050522228
16_gate proxy err 0.018929630517959595 tr(WHW.T) 41956.07421875
bpp_loss 2.3027247695988695
16_down proxy err 0.05039488151669502 tr(WHW.T) 15608.0380859375
bpp_loss 2.240187534141939
I0325 07:29:18.968619 478952 finetune.py:68] layer 17_down @ epoch 1 new loss 0.005331194028258324 old loss 0.005334345158189535 BETTER
I0325 07:29:46.280100 478952 finetune.py:68] layer 17_down @ epoch 2 new loss 0.005328272003680468 old loss 0.005331194028258324 BETTER
I0325 07:30:10.509598 382969 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 63.07532238960266s
I0325 07:30:13.649910 478952 finetune.py:68] layer 17_down @ epoch 3 new loss 0.005325552076101303 old loss 0.005328272003680468 BETTER
I0325 07:30:14.140828 494723 config.py:54] PyTorch version 2.6.0 available.
W0325 07:30:14.429894 494723 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 07:30:15.353648 494723 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 07:30:15.357407 382969 quantize_finetune_llama.py:209] layer 19 gpu 1
I0325 07:30:15.370613 494723 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 07:30:33.606453 494723 finetune.py:45] layer 18_v initial loss 0.0012860923307016492
W0325 07:30:33.606763 494723 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 07:30:41.360331 478952 finetune.py:68] layer 17_down @ epoch 4 new loss 0.005323028191924095 old loss 0.005325552076101303 BETTER
17_v proxy err 0.052815280854701996 tr(WHW.T) 4307.6240234375
bpp_loss 2.0998417142836843
17_q proxy err 0.006832621525973082 tr(WHW.T) 36536.0390625
bpp_loss 2.534600108192535
17_k proxy err 0.004592870827764273 tr(WHW.T) 54568.26953125
bpp_loss 2.56835945500643
17_o proxy err 0.06167558580636978 tr(WHW.T) 4387.7607421875
bpp_loss 2.0985931041941512
17_up proxy err 0.0377117395401001 tr(WHW.T) 21761.720703125
bpp_loss 2.2592559559487326
17_gate proxy err 0.02058250829577446 tr(WHW.T) 40326.5859375
bpp_loss 2.3158096420297096
17_down proxy err 0.05247187614440918 tr(WHW.T) 15727.1201171875
bpp_loss 2.24089685108426
I0325 07:31:06.225673 494723 finetune.py:68] layer 18_v @ epoch 0 new loss 0.0006716556963510811 old loss 0.0012860923307016492 BETTER
I0325 07:31:40.366296 494723 finetune.py:68] layer 18_v @ epoch 1 new loss 0.0006066538626328111 old loss 0.0006716556963510811 BETTER
I0325 07:31:46.844616 382969 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 60.494227170944214s
I0325 07:31:50.302557 495960 config.py:54] PyTorch version 2.6.0 available.
W0325 07:31:50.595654 495960 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 07:31:51.566409 495960 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 07:31:51.570233 382969 quantize_finetune_llama.py:209] layer 20 gpu 2
I0325 07:31:51.583967 495960 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 07:32:09.419428 495960 finetune.py:45] layer 19_v initial loss 0.0012562139891088009
W0325 07:32:09.419763 495960 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 07:32:14.957402 494723 finetune.py:68] layer 18_v @ epoch 2 new loss 0.0005826990818604827 old loss 0.0006066538626328111 BETTER
I0325 07:32:40.718509 495960 finetune.py:68] layer 19_v @ epoch 0 new loss 0.0006660364451818168 old loss 0.0012562139891088009 BETTER
I0325 07:32:49.979133 494723 finetune.py:68] layer 18_v @ epoch 3 new loss 0.0005680363974533975 old loss 0.0005826990818604827 BETTER
I0325 07:32:55.025207 382969 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 62.98643636703491s
I0325 07:32:58.582460 496855 config.py:54] PyTorch version 2.6.0 available.
W0325 07:32:58.883383 496855 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 07:33:00.201387 496855 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 07:33:00.205093 382969 quantize_finetune_llama.py:209] layer 21 gpu 0
I0325 07:33:00.218031 496855 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 07:33:13.909629 495960 finetune.py:68] layer 19_v @ epoch 1 new loss 0.0005969267222099006 old loss 0.0006660364451818168 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 07:33:20.401726 496855 finetune.py:45] layer 20_v initial loss 0.0014763319632038474
W0325 07:33:20.402017 496855 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 07:33:25.065501 494723 finetune.py:68] layer 18_v @ epoch 4 new loss 0.0005573563394136727 old loss 0.0005680363974533975 BETTER
I0325 07:33:45.751981 494723 finetune.py:45] layer 18_q initial loss 0.0007114046602509916
I0325 07:33:47.227654 495960 finetune.py:68] layer 19_v @ epoch 2 new loss 0.0005732715944759548 old loss 0.0005969267222099006 BETTER
I0325 07:33:53.085210 496855 finetune.py:68] layer 20_v @ epoch 0 new loss 0.0007987605058588088 old loss 0.0014763319632038474 BETTER
I0325 07:34:19.172174 494723 finetune.py:68] layer 18_q @ epoch 0 new loss 0.0006801275885663927 old loss 0.0007114046602509916 BETTER
I0325 07:34:20.730245 495960 finetune.py:68] layer 19_v @ epoch 3 new loss 0.0005591344670392573 old loss 0.0005732715944759548 BETTER
I0325 07:34:27.303719 496855 finetune.py:68] layer 20_v @ epoch 1 new loss 0.0007120571099221706 old loss 0.0007987605058588088 BETTER
I0325 07:34:54.727452 494723 finetune.py:68] layer 18_q @ epoch 1 new loss 0.0006664927350357175 old loss 0.0006801275885663927 BETTER
I0325 07:34:55.072912 495960 finetune.py:68] layer 19_v @ epoch 4 new loss 0.0005490215844474733 old loss 0.0005591344670392573 BETTER
I0325 07:35:01.032201 496855 finetune.py:68] layer 20_v @ epoch 2 new loss 0.0006800559349358082 old loss 0.0007120571099221706 BETTER
I0325 07:35:15.406333 495960 finetune.py:45] layer 19_q initial loss 0.000696114671882242
I0325 07:35:29.792469 494723 finetune.py:68] layer 18_q @ epoch 2 new loss 0.0006560661713592708 old loss 0.0006664927350357175 BETTER
I0325 07:35:35.280837 496855 finetune.py:68] layer 20_v @ epoch 3 new loss 0.000661409692838788 old loss 0.0006800559349358082 BETTER
I0325 07:35:46.947115 495960 finetune.py:68] layer 19_q @ epoch 0 new loss 0.000661425176076591 old loss 0.000696114671882242 BETTER
I0325 07:36:04.566539 494723 finetune.py:68] layer 18_q @ epoch 3 new loss 0.0006473774556070566 old loss 0.0006560661713592708 BETTER
I0325 07:36:10.260228 496855 finetune.py:68] layer 20_v @ epoch 4 new loss 0.000648657965939492 old loss 0.000661409692838788 BETTER
I0325 07:36:19.793590 495960 finetune.py:68] layer 19_q @ epoch 1 new loss 0.0006486268830485642 old loss 0.000661425176076591 BETTER
I0325 07:36:32.630336 496855 finetune.py:45] layer 20_q initial loss 0.0008150060311891139
I0325 07:36:40.235726 494723 finetune.py:68] layer 18_q @ epoch 4 new loss 0.0006399621488526464 old loss 0.0006473774556070566 BETTER
I0325 07:36:52.425862 495960 finetune.py:68] layer 19_q @ epoch 2 new loss 0.0006391793140210211 old loss 0.0006486268830485642 BETTER
I0325 07:37:01.250293 494723 finetune.py:45] layer 18_k initial loss 0.0007586062420159578
I0325 07:37:05.893821 496855 finetune.py:68] layer 20_q @ epoch 0 new loss 0.0007801933097653091 old loss 0.0008150060311891139 BETTER
I0325 07:37:25.294950 495960 finetune.py:68] layer 19_q @ epoch 3 new loss 0.0006314641213975847 old loss 0.0006391793140210211 BETTER
I0325 07:37:34.063019 494723 finetune.py:68] layer 18_k @ epoch 0 new loss 0.0007480503991246223 old loss 0.0007586062420159578 BETTER
I0325 07:37:39.178113 496855 finetune.py:68] layer 20_q @ epoch 1 new loss 0.0007647149031981826 old loss 0.0007801933097653091 BETTER
I0325 07:37:58.044693 495960 finetune.py:68] layer 19_q @ epoch 4 new loss 0.0006248517311178148 old loss 0.0006314641213975847 BETTER
I0325 07:38:07.984992 494723 finetune.py:68] layer 18_k @ epoch 1 new loss 0.0007412990089505911 old loss 0.0007480503991246223 BETTER
I0325 07:38:12.939350 496855 finetune.py:68] layer 20_q @ epoch 2 new loss 0.0007533964817412198 old loss 0.0007647149031981826 BETTER
I0325 07:38:17.911276 495960 finetune.py:45] layer 19_k initial loss 0.0007422002381645143
I0325 07:38:42.014854 494723 finetune.py:68] layer 18_k @ epoch 2 new loss 0.0007355364505201578 old loss 0.0007412990089505911 BETTER
I0325 07:38:46.379164 496855 finetune.py:68] layer 20_q @ epoch 3 new loss 0.0007439766777679324 old loss 0.0007533964817412198 BETTER
I0325 07:38:49.481086 495960 finetune.py:68] layer 19_k @ epoch 0 new loss 0.0007300085271708667 old loss 0.0007422002381645143 BETTER
I0325 07:39:16.260077 494723 finetune.py:68] layer 18_k @ epoch 3 new loss 0.0007304606260731816 old loss 0.0007355364505201578 BETTER
I0325 07:39:19.262069 496855 finetune.py:68] layer 20_q @ epoch 4 new loss 0.0007361206226050854 old loss 0.0007439766777679324 BETTER
I0325 07:39:21.780422 495960 finetune.py:68] layer 19_k @ epoch 1 new loss 0.0007227607420645654 old loss 0.0007300085271708667 BETTER
I0325 07:39:39.837892 496855 finetune.py:45] layer 20_k initial loss 0.0008693843265064061
I0325 07:39:50.416858 494723 finetune.py:68] layer 18_k @ epoch 4 new loss 0.0007260701968334615 old loss 0.0007304606260731816 BETTER
I0325 07:39:53.949600 495960 finetune.py:68] layer 19_k @ epoch 2 new loss 0.0007167151779867709 old loss 0.0007227607420645654 BETTER
I0325 07:40:09.559979 494723 finetune.py:45] layer 18_o initial loss 0.001665556919761002
I0325 07:40:11.459916 496855 finetune.py:68] layer 20_k @ epoch 0 new loss 0.0008542497525922954 old loss 0.0008693843265064061 BETTER
I0325 07:40:25.940345 495960 finetune.py:68] layer 19_k @ epoch 3 new loss 0.0007113672327250242 old loss 0.0007167151779867709 BETTER
I0325 07:40:41.756909 494723 finetune.py:68] layer 18_o @ epoch 0 new loss 0.001594957779161632 old loss 0.001665556919761002 BETTER
I0325 07:40:43.621351 496855 finetune.py:68] layer 20_k @ epoch 1 new loss 0.0008462491678074002 old loss 0.0008542497525922954 BETTER
I0325 07:40:57.990787 495960 finetune.py:68] layer 19_k @ epoch 4 new loss 0.0007067987462505698 old loss 0.0007113672327250242 BETTER
I0325 07:41:15.219810 494723 finetune.py:68] layer 18_o @ epoch 1 new loss 0.001577078946866095 old loss 0.001594957779161632 BETTER
I0325 07:41:16.302502 496855 finetune.py:68] layer 20_k @ epoch 2 new loss 0.000839484331663698 old loss 0.0008462491678074002 BETTER
I0325 07:41:16.660000 495960 finetune.py:45] layer 19_o initial loss 0.0016105868853628635
I0325 07:41:47.201110 495960 finetune.py:68] layer 19_o @ epoch 0 new loss 0.0015437195543199778 old loss 0.0016105868853628635 BETTER
I0325 07:41:48.901494 494723 finetune.py:68] layer 18_o @ epoch 2 new loss 0.0015636050375178456 old loss 0.001577078946866095 BETTER
I0325 07:41:49.153925 496855 finetune.py:68] layer 20_k @ epoch 3 new loss 0.00083359912969172 old loss 0.000839484331663698 BETTER
I0325 07:42:18.824159 495960 finetune.py:68] layer 19_o @ epoch 1 new loss 0.0015271925367414951 old loss 0.0015437195543199778 BETTER
I0325 07:42:22.041558 496855 finetune.py:68] layer 20_k @ epoch 4 new loss 0.0008284438517875969 old loss 0.00083359912969172 BETTER
I0325 07:42:22.623117 494723 finetune.py:68] layer 18_o @ epoch 3 new loss 0.0015523702604696155 old loss 0.0015636050375178456 BETTER
I0325 07:42:41.359101 496855 finetune.py:45] layer 20_o initial loss 0.0019022938795387745
I0325 07:42:50.574798 495960 finetune.py:68] layer 19_o @ epoch 2 new loss 0.0015148691600188613 old loss 0.0015271925367414951 BETTER
I0325 07:42:56.161820 494723 finetune.py:68] layer 18_o @ epoch 4 new loss 0.0015426517929881811 old loss 0.0015523702604696155 BETTER
I0325 07:43:12.556467 496855 finetune.py:68] layer 20_o @ epoch 0 new loss 0.0018266299739480019 old loss 0.0019022938795387745 BETTER
I0325 07:43:22.345577 495960 finetune.py:68] layer 19_o @ epoch 3 new loss 0.001504946849308908 old loss 0.0015148691600188613 BETTER
I0325 07:43:22.537764 494723 finetune.py:45] layer 18_up initial loss 0.0029745891224592924
I0325 07:43:44.885395 496855 finetune.py:68] layer 20_o @ epoch 1 new loss 0.0018062066519632936 old loss 0.0018266299739480019 BETTER
I0325 07:43:52.873086 494723 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0029370307456701994 old loss 0.0029745891224592924 BETTER
I0325 07:43:54.195530 495960 finetune.py:68] layer 19_o @ epoch 4 new loss 0.0014962652930989861 old loss 0.001504946849308908 BETTER
I0325 07:44:17.033853 496855 finetune.py:68] layer 20_o @ epoch 2 new loss 0.001791397575289011 old loss 0.0018062066519632936 BETTER
I0325 07:44:20.473086 495960 finetune.py:45] layer 19_up initial loss 0.0031192523892968893
I0325 07:44:24.611531 494723 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0029146282467991114 old loss 0.0029370307456701994 BETTER
I0325 07:44:50.734395 495960 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0030789750162512064 old loss 0.0031192523892968893 BETTER
I0325 07:44:50.802086 496855 finetune.py:68] layer 20_o @ epoch 3 new loss 0.0017795484745875 old loss 0.001791397575289011 BETTER
I0325 07:44:56.352947 494723 finetune.py:68] layer 18_up @ epoch 2 new loss 0.002897049766033888 old loss 0.0029146282467991114 BETTER
I0325 07:45:20.472964 495960 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0030557301361113787 old loss 0.0030789750162512064 BETTER
I0325 07:45:23.016604 496855 finetune.py:68] layer 20_o @ epoch 4 new loss 0.0017691818065941334 old loss 0.0017795484745875 BETTER
I0325 07:45:28.258138 494723 finetune.py:68] layer 18_up @ epoch 3 new loss 0.0028819057624787092 old loss 0.002897049766033888 BETTER
I0325 07:45:50.571635 496855 finetune.py:45] layer 20_up initial loss 0.0036844233982264996
I0325 07:45:51.423331 495960 finetune.py:68] layer 19_up @ epoch 2 new loss 0.003037653863430023 old loss 0.0030557301361113787 BETTER
I0325 07:46:00.219054 494723 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0028685922734439373 old loss 0.0028819057624787092 BETTER
I0325 07:46:20.244804 496855 finetune.py:68] layer 20_up @ epoch 0 new loss 0.003640195354819298 old loss 0.0036844233982264996 BETTER
I0325 07:46:21.430860 495960 finetune.py:68] layer 19_up @ epoch 3 new loss 0.00302246306091547 old loss 0.003037653863430023 BETTER
I0325 07:46:27.155741 494723 finetune.py:45] layer 18_gate initial loss 0.003980572335422039
I0325 07:46:51.673579 496855 finetune.py:68] layer 20_up @ epoch 1 new loss 0.0036133325193077326 old loss 0.003640195354819298 BETTER
I0325 07:46:52.155644 495960 finetune.py:68] layer 19_up @ epoch 4 new loss 0.00300924526527524 old loss 0.00302246306091547 BETTER
I0325 07:46:56.143838 494723 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0039610350504517555 old loss 0.003980572335422039 BETTER
I0325 07:47:19.659520 495960 finetune.py:45] layer 19_gate initial loss 0.004315146245062351
I0325 07:47:23.248415 496855 finetune.py:68] layer 20_up @ epoch 2 new loss 0.003592471592128277 old loss 0.0036133325193077326 BETTER
I0325 07:47:26.541078 494723 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.003944719210267067 old loss 0.0039610350504517555 BETTER
I0325 07:47:47.121435 495960 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.004294275771826506 old loss 0.004315146245062351 BETTER
I0325 07:47:53.911720 496855 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0035746516659855843 old loss 0.003592471592128277 BETTER
I0325 07:47:56.867326 494723 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.003929830156266689 old loss 0.003944719210267067 BETTER
I0325 07:48:16.790495 495960 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.004277350846678019 old loss 0.004294275771826506 BETTER
I0325 07:48:25.301905 496855 finetune.py:68] layer 20_up @ epoch 4 new loss 0.00355936074629426 old loss 0.0035746516659855843 BETTER
I0325 07:48:27.449538 494723 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.003916117828339338 old loss 0.003929830156266689 BETTER
I0325 07:48:45.799708 495960 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.004262043163180351 old loss 0.004277350846678019 BETTER
I0325 07:48:54.665659 496855 finetune.py:45] layer 20_gate initial loss 0.0050920359790325165
I0325 07:48:57.785364 494723 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.003903394564986229 old loss 0.003916117828339338 BETTER
I0325 07:49:14.355400 495960 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.004247868433594704 old loss 0.004262043163180351 BETTER
I0325 07:49:22.508418 496855 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.005069959443062544 old loss 0.0050920359790325165 BETTER
I0325 07:49:42.896517 494723 finetune.py:45] layer 18_down initial loss 0.00629119947552681
I0325 07:49:43.147857 495960 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.004234856925904751 old loss 0.004247868433594704 BETTER
I0325 07:49:51.672257 496855 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0050511714071035385 old loss 0.005069959443062544 BETTER
I0325 07:50:09.574639 494723 finetune.py:68] layer 18_down @ epoch 0 new loss 0.006287334952503443 old loss 0.00629119947552681 BETTER
I0325 07:50:20.541510 496855 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.005033689551055431 old loss 0.0050511714071035385 BETTER
I0325 07:50:27.861799 495960 finetune.py:45] layer 19_down initial loss 0.006868207361549139
I0325 07:50:37.350660 494723 finetune.py:68] layer 18_down @ epoch 1 new loss 0.006283690687268972 old loss 0.006287334952503443 BETTER
I0325 07:50:49.264888 496855 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.005017866380512714 old loss 0.005033689551055431 BETTER
I0325 07:50:53.296780 495960 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0068639121018350124 old loss 0.006868207361549139 BETTER
I0325 07:51:05.558347 494723 finetune.py:68] layer 18_down @ epoch 2 new loss 0.00628027506172657 old loss 0.006283690687268972 BETTER
I0325 07:51:17.667422 496855 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.005002753809094429 old loss 0.005017866380512714 BETTER
I0325 07:51:19.874913 495960 finetune.py:68] layer 19_down @ epoch 1 new loss 0.006859895307570696 old loss 0.0068639121018350124 BETTER
I0325 07:51:34.012056 494723 finetune.py:68] layer 18_down @ epoch 3 new loss 0.006277082022279501 old loss 0.00628027506172657 BETTER
I0325 07:51:46.565284 495960 finetune.py:68] layer 19_down @ epoch 2 new loss 0.006856122985482216 old loss 0.006859895307570696 BETTER
I0325 07:52:00.440354 496855 finetune.py:45] layer 20_down initial loss 0.008226349949836731
I0325 07:52:02.350978 494723 finetune.py:68] layer 18_down @ epoch 4 new loss 0.00627402076497674 old loss 0.006277082022279501 BETTER
18_v proxy err 0.051592838019132614 tr(WHW.T) 4721.12890625
bpp_loss 2.1458428891492076
18_q proxy err 0.007429209072142839 tr(WHW.T) 35359.90625
bpp_loss 2.5087901695806067
18_k proxy err 0.005356141831725836 tr(WHW.T) 49267.49609375
bpp_loss 2.5381308536743745
18_o proxy err 0.05581788718700409 tr(WHW.T) 5016.45703125
bpp_loss 2.1388022214814555
18_up proxy err 0.040435079485177994 tr(WHW.T) 20415.201171875
bpp_loss 2.255250080423646
18_gate proxy err 0.022093476727604866 tr(WHW.T) 37923.58984375
bpp_loss 2.3261950981998165
18_down proxy err 0.051761891692876816 tr(WHW.T) 15654.478515625
bpp_loss 2.2396567770453215
I0325 07:52:14.398172 495960 finetune.py:68] layer 19_down @ epoch 3 new loss 0.006852538324892521 old loss 0.006856122985482216 BETTER
I0325 07:52:27.204271 496855 finetune.py:68] layer 20_down @ epoch 0 new loss 0.008220711722970009 old loss 0.008226349949836731 BETTER
I0325 07:52:41.250500 495960 finetune.py:68] layer 19_down @ epoch 4 new loss 0.006849190220236778 old loss 0.006852538324892521 BETTER
19_v proxy err 0.050654422491788864 tr(WHW.T) 4837.78857421875
bpp_loss 2.1565229818806984
19_q proxy err 0.007948591373860836 tr(WHW.T) 33023.33984375
bpp_loss 2.487579980137525
19_k proxy err 0.005250304471701384 tr(WHW.T) 50160.8515625
bpp_loss 2.516503342310898
19_o proxy err 0.05792839080095291 tr(WHW.T) 5104.8857421875
bpp_loss 2.147379254194675
19_up proxy err 0.04101994261145592 tr(WHW.T) 20323.279296875
bpp_loss 2.254806268873603
19_gate proxy err 0.02442988194525242 tr(WHW.T) 34682.7578125
bpp_loss 2.3304412316011134
19_down proxy err 0.05112455040216446 tr(WHW.T) 16065.416015625
bpp_loss 2.243514403094386
I0325 07:52:54.087882 496855 finetune.py:68] layer 20_down @ epoch 1 new loss 0.008215385489165783 old loss 0.008220711722970009 BETTER
I0325 07:53:21.091972 496855 finetune.py:68] layer 20_down @ epoch 2 new loss 0.00821032002568245 old loss 0.008215385489165783 BETTER
I0325 07:53:48.276285 496855 finetune.py:68] layer 20_down @ epoch 3 new loss 0.008205502294003963 old loss 0.00821032002568245 BETTER
I0325 07:53:48.483081 382969 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 62.7793390750885s
I0325 07:53:51.961191 512641 config.py:54] PyTorch version 2.6.0 available.
W0325 07:53:52.273595 512641 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 07:53:53.217427 512641 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 07:53:53.221603 382969 quantize_finetune_llama.py:209] layer 22 gpu 1
I0325 07:53:53.235306 512641 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 07:54:11.399868 512641 finetune.py:45] layer 21_v initial loss 0.0014894632622599602
W0325 07:54:11.400053 512641 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 07:54:15.555565 496855 finetune.py:68] layer 20_down @ epoch 4 new loss 0.00820097979158163 old loss 0.008205502294003963 BETTER
20_v proxy err 0.053452666848897934 tr(WHW.T) 4696.2275390625
bpp_loss 2.172399390110513
20_q proxy err 0.007887468673288822 tr(WHW.T) 33942.3125
bpp_loss 2.49405479253619
20_k proxy err 0.005446328781545162 tr(WHW.T) 49336.63671875
bpp_loss 2.5210184385068715
20_o proxy err 0.041083741933107376 tr(WHW.T) 6915.724609375
bpp_loss 2.1748450246814173
20_up proxy err 0.04006290063261986 tr(WHW.T) 20731.22265625
bpp_loss 2.2563231194088624
20_gate proxy err 0.023845860734581947 tr(WHW.T) 35454.1953125
bpp_loss 2.340310180126581
20_down proxy err 0.04994666576385498 tr(WHW.T) 16160.0869140625
bpp_loss 2.244512881412236
I0325 07:54:44.125780 512641 finetune.py:68] layer 21_v @ epoch 0 new loss 0.0007498257327824831 old loss 0.0014894632622599602 BETTER
I0325 07:55:18.211506 512641 finetune.py:68] layer 21_v @ epoch 1 new loss 0.0006496742134913802 old loss 0.0007498257327824831 BETTER
I0325 07:55:20.626295 382969 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 60.479119062423706s
I0325 07:55:24.049633 513838 config.py:54] PyTorch version 2.6.0 available.
W0325 07:55:24.341337 513838 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 07:55:25.240098 513838 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 07:55:25.244125 382969 quantize_finetune_llama.py:209] layer 23 gpu 2
I0325 07:55:25.257189 513838 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 07:55:43.397946 513838 finetune.py:45] layer 22_v initial loss 0.0017616678960621357
W0325 07:55:43.398126 513838 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 07:55:52.711491 512641 finetune.py:68] layer 21_v @ epoch 2 new loss 0.0006195128080435097 old loss 0.0006496742134913802 BETTER
I0325 07:56:14.649216 513838 finetune.py:68] layer 22_v @ epoch 0 new loss 0.0009329706663265824 old loss 0.0017616678960621357 BETTER
I0325 07:56:27.412253 512641 finetune.py:68] layer 21_v @ epoch 3 new loss 0.0006033263634890318 old loss 0.0006195128080435097 BETTER
I0325 07:56:28.281026 382969 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 62.59916019439697s
I0325 07:56:31.815642 514749 config.py:54] PyTorch version 2.6.0 available.
W0325 07:56:32.117744 514749 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 07:56:33.204413 514749 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 07:56:33.208585 382969 quantize_finetune_llama.py:209] layer 24 gpu 0
I0325 07:56:33.222601 514749 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 07:56:46.830979 513838 finetune.py:68] layer 22_v @ epoch 1 new loss 0.000813113059848547 old loss 0.0009329706663265824 BETTER
I0325 07:56:51.643514 514749 finetune.py:45] layer 23_v initial loss 0.0018677691696211696
W0325 07:56:51.643912 514749 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 07:57:02.351531 512641 finetune.py:68] layer 21_v @ epoch 4 new loss 0.0005926252924837172 old loss 0.0006033263634890318 BETTER
I0325 07:57:19.322612 513838 finetune.py:68] layer 22_v @ epoch 2 new loss 0.0007757828570902348 old loss 0.000813113059848547 BETTER
I0325 07:57:21.399357 512641 finetune.py:45] layer 21_q initial loss 0.0007459907210431993
I0325 07:57:23.183396 514749 finetune.py:68] layer 23_v @ epoch 0 new loss 0.000991751323454082 old loss 0.0018677691696211696 BETTER
I0325 07:57:52.273658 513838 finetune.py:68] layer 22_v @ epoch 3 new loss 0.0007554036565124989 old loss 0.0007757828570902348 BETTER
I0325 07:57:54.243349 512641 finetune.py:68] layer 21_q @ epoch 0 new loss 0.000709095096681267 old loss 0.0007459907210431993 BETTER
I0325 07:57:55.679184 514749 finetune.py:68] layer 23_v @ epoch 1 new loss 0.0008386718691326678 old loss 0.000991751323454082 BETTER
I0325 07:58:25.024389 513838 finetune.py:68] layer 22_v @ epoch 4 new loss 0.0007414369028992951 old loss 0.0007554036565124989 BETTER
I0325 07:58:28.292002 514749 finetune.py:68] layer 23_v @ epoch 2 new loss 0.0007955202599987388 old loss 0.0008386718691326678 BETTER
I0325 07:58:28.419383 512641 finetune.py:68] layer 21_q @ epoch 1 new loss 0.0006965143838897347 old loss 0.000709095096681267 BETTER
I0325 07:58:44.192285 513838 finetune.py:45] layer 22_q initial loss 0.0009935549460351467
I0325 07:59:01.181037 514749 finetune.py:68] layer 23_v @ epoch 3 new loss 0.0007746884366497397 old loss 0.0007955202599987388 BETTER
I0325 07:59:02.867728 512641 finetune.py:68] layer 21_q @ epoch 2 new loss 0.0006877924897707999 old loss 0.0006965143838897347 BETTER
I0325 07:59:15.666958 513838 finetune.py:68] layer 22_q @ epoch 0 new loss 0.0009433202212676406 old loss 0.0009935549460351467 BETTER
I0325 07:59:34.492643 514749 finetune.py:68] layer 23_v @ epoch 4 new loss 0.0007613792549818754 old loss 0.0007746884366497397 BETTER
I0325 07:59:37.401521 512641 finetune.py:68] layer 21_q @ epoch 3 new loss 0.0006807400495745242 old loss 0.0006877924897707999 BETTER
I0325 07:59:47.877552 513838 finetune.py:68] layer 22_q @ epoch 1 new loss 0.0009240162326022983 old loss 0.0009433202212676406 BETTER
I0325 07:59:53.559668 514749 finetune.py:45] layer 23_q initial loss 0.0009796677622944117
I0325 08:00:12.011244 512641 finetune.py:68] layer 21_q @ epoch 4 new loss 0.0006747787701897323 old loss 0.0006807400495745242 BETTER
I0325 08:00:20.447744 513838 finetune.py:68] layer 22_q @ epoch 2 new loss 0.0009102044277824461 old loss 0.0009240162326022983 BETTER
I0325 08:00:25.014466 514749 finetune.py:68] layer 23_q @ epoch 0 new loss 0.0009281421662308276 old loss 0.0009796677622944117 BETTER
I0325 08:00:30.881086 512641 finetune.py:45] layer 21_k initial loss 0.0008274646243080497
I0325 08:00:53.119062 513838 finetune.py:68] layer 22_q @ epoch 3 new loss 0.0008988153422251344 old loss 0.0009102044277824461 BETTER
I0325 08:00:57.646280 514749 finetune.py:68] layer 23_q @ epoch 1 new loss 0.0009102523908950388 old loss 0.0009281421662308276 BETTER
I0325 08:01:03.571105 512641 finetune.py:68] layer 21_k @ epoch 0 new loss 0.0008161256555467844 old loss 0.0008274646243080497 BETTER
I0325 08:01:25.986797 513838 finetune.py:68] layer 22_q @ epoch 4 new loss 0.0008893457124941051 old loss 0.0008988153422251344 BETTER
I0325 08:01:30.275838 514749 finetune.py:68] layer 23_q @ epoch 2 new loss 0.0008986295433714986 old loss 0.0009102523908950388 BETTER
I0325 08:01:37.338927 512641 finetune.py:68] layer 21_k @ epoch 1 new loss 0.0008090827031992376 old loss 0.0008161256555467844 BETTER
I0325 08:01:45.100366 513838 finetune.py:45] layer 22_k initial loss 0.0011055682552978396
I0325 08:02:02.705209 514749 finetune.py:68] layer 23_q @ epoch 3 new loss 0.000889255665242672 old loss 0.0008986295433714986 BETTER
I0325 08:02:11.055079 512641 finetune.py:68] layer 21_k @ epoch 2 new loss 0.0008031988982111216 old loss 0.0008090827031992376 BETTER
I0325 08:02:16.431768 513838 finetune.py:68] layer 22_k @ epoch 0 new loss 0.0010912480065599084 old loss 0.0011055682552978396 BETTER
I0325 08:02:35.056183 514749 finetune.py:68] layer 23_q @ epoch 4 new loss 0.0008815462933853269 old loss 0.000889255665242672 BETTER
I0325 08:02:44.974681 512641 finetune.py:68] layer 21_k @ epoch 3 new loss 0.0007978680077940226 old loss 0.0008031988982111216 BETTER
I0325 08:02:48.481002 513838 finetune.py:68] layer 22_k @ epoch 1 new loss 0.0010818139417096972 old loss 0.0010912480065599084 BETTER
I0325 08:02:54.043704 514749 finetune.py:45] layer 23_k initial loss 0.0010963126551359892
I0325 08:03:19.128629 512641 finetune.py:68] layer 21_k @ epoch 4 new loss 0.0007931312429718673 old loss 0.0007978680077940226 BETTER
I0325 08:03:20.532207 513838 finetune.py:68] layer 22_k @ epoch 2 new loss 0.0010738822165876627 old loss 0.0010818139417096972 BETTER
I0325 08:03:25.399785 514749 finetune.py:68] layer 23_k @ epoch 0 new loss 0.0010828457307070494 old loss 0.0010963126551359892 BETTER
I0325 08:03:37.880989 512641 finetune.py:45] layer 21_o initial loss 0.001758042024448514
I0325 08:03:52.582855 513838 finetune.py:68] layer 22_k @ epoch 3 new loss 0.001066847937181592 old loss 0.0010738822165876627 BETTER
I0325 08:03:57.689683 514749 finetune.py:68] layer 23_k @ epoch 1 new loss 0.0010744209866970778 old loss 0.0010828457307070494 BETTER
I0325 08:04:10.062679 512641 finetune.py:68] layer 21_o @ epoch 0 new loss 0.0016909587429836392 old loss 0.001758042024448514 BETTER
I0325 08:04:24.633013 513838 finetune.py:68] layer 22_k @ epoch 4 new loss 0.001060520182363689 old loss 0.001066847937181592 BETTER
I0325 08:04:29.740497 514749 finetune.py:68] layer 23_k @ epoch 2 new loss 0.0010670297779142857 old loss 0.0010744209866970778 BETTER
I0325 08:04:43.457079 513838 finetune.py:45] layer 22_o initial loss 0.002241002395749092
I0325 08:04:43.473364 512641 finetune.py:68] layer 21_o @ epoch 1 new loss 0.0016767258057370782 old loss 0.0016909587429836392 BETTER
I0325 08:05:01.791799 514749 finetune.py:68] layer 23_k @ epoch 3 new loss 0.0010608172742649913 old loss 0.0010670297779142857 BETTER
I0325 08:05:13.895313 513838 finetune.py:68] layer 22_o @ epoch 0 new loss 0.002164068166166544 old loss 0.002241002395749092 BETTER
I0325 08:05:16.813903 512641 finetune.py:68] layer 21_o @ epoch 2 new loss 0.0016662228154018521 old loss 0.0016767258057370782 BETTER
I0325 08:05:33.842993 514749 finetune.py:68] layer 23_k @ epoch 4 new loss 0.0010550676379352808 old loss 0.0010608172742649913 BETTER
I0325 08:05:45.327302 513838 finetune.py:68] layer 22_o @ epoch 1 new loss 0.0021443699952214956 old loss 0.002164068166166544 BETTER
I0325 08:05:50.140750 512641 finetune.py:68] layer 21_o @ epoch 3 new loss 0.001657857559621334 old loss 0.0016662228154018521 BETTER
I0325 08:05:52.850293 514749 finetune.py:45] layer 23_o initial loss 0.002173267537727952
I0325 08:06:16.936518 513838 finetune.py:68] layer 22_o @ epoch 2 new loss 0.00212976336479187 old loss 0.0021443699952214956 BETTER
I0325 08:06:23.364353 514749 finetune.py:68] layer 23_o @ epoch 0 new loss 0.0020974576473236084 old loss 0.002173267537727952 BETTER
I0325 08:06:23.379262 512641 finetune.py:68] layer 21_o @ epoch 4 new loss 0.0016508681001141667 old loss 0.001657857559621334 BETTER
I0325 08:06:48.586972 513838 finetune.py:68] layer 22_o @ epoch 3 new loss 0.0021175823640078306 old loss 0.00212976336479187 BETTER
I0325 08:06:49.142630 512641 finetune.py:45] layer 21_up initial loss 0.003762859618291259
I0325 08:06:55.134127 514749 finetune.py:68] layer 23_o @ epoch 1 new loss 0.0020787527319043875 old loss 0.0020974576473236084 BETTER
I0325 08:07:19.484586 512641 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0037244718987494707 old loss 0.003762859618291259 BETTER
I0325 08:07:20.337772 513838 finetune.py:68] layer 22_o @ epoch 4 new loss 0.0021074325777590275 old loss 0.0021175823640078306 BETTER
I0325 08:07:26.744065 514749 finetune.py:68] layer 23_o @ epoch 2 new loss 0.002066223882138729 old loss 0.0020787527319043875 BETTER
I0325 08:07:45.884945 513838 finetune.py:45] layer 22_up initial loss 0.004525693133473396
I0325 08:07:50.970245 512641 finetune.py:68] layer 21_up @ epoch 1 new loss 0.0037015466950833797 old loss 0.0037244718987494707 BETTER
I0325 08:07:58.476677 514749 finetune.py:68] layer 23_o @ epoch 3 new loss 0.0020555504597723484 old loss 0.002066223882138729 BETTER
I0325 08:08:14.778772 513838 finetune.py:68] layer 22_up @ epoch 0 new loss 0.004485412500798702 old loss 0.004525693133473396 BETTER
I0325 08:08:22.540780 512641 finetune.py:68] layer 21_up @ epoch 2 new loss 0.003683406161144376 old loss 0.0037015466950833797 BETTER
I0325 08:08:30.254734 514749 finetune.py:68] layer 23_o @ epoch 4 new loss 0.0020470002200454473 old loss 0.0020555504597723484 BETTER
I0325 08:08:44.510421 513838 finetune.py:68] layer 22_up @ epoch 1 new loss 0.004460313357412815 old loss 0.004485412500798702 BETTER
I0325 08:08:54.247238 512641 finetune.py:68] layer 21_up @ epoch 3 new loss 0.003668329445645213 old loss 0.003683406161144376 BETTER
I0325 08:08:56.033149 514749 finetune.py:45] layer 23_up initial loss 0.004749475046992302
I0325 08:09:14.762915 513838 finetune.py:68] layer 22_up @ epoch 2 new loss 0.004440696910023689 old loss 0.004460313357412815 BETTER
I0325 08:09:25.005765 514749 finetune.py:68] layer 23_up @ epoch 0 new loss 0.004711477551609278 old loss 0.004749475046992302 BETTER
I0325 08:09:26.364024 512641 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0036550331860780716 old loss 0.003668329445645213 BETTER
I0325 08:09:44.634733 513838 finetune.py:68] layer 22_up @ epoch 3 new loss 0.004424021579325199 old loss 0.004440696910023689 BETTER
I0325 08:09:52.305647 512641 finetune.py:45] layer 21_gate initial loss 0.005326471757143736
I0325 08:09:54.800790 514749 finetune.py:68] layer 23_up @ epoch 1 new loss 0.004686403553932905 old loss 0.004711477551609278 BETTER
I0325 08:10:14.564281 513838 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0044097756035625935 old loss 0.004424021579325199 BETTER
I0325 08:10:21.166153 512641 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.005306154489517212 old loss 0.005326471757143736 BETTER
I0325 08:10:24.710722 514749 finetune.py:68] layer 23_up @ epoch 2 new loss 0.004667391534894705 old loss 0.004686403553932905 BETTER
I0325 08:10:40.250686 513838 finetune.py:45] layer 22_gate initial loss 0.006313046906143427
I0325 08:10:50.913560 512641 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.005289679858833551 old loss 0.005306154489517212 BETTER
I0325 08:10:54.662706 514749 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0046507371589541435 old loss 0.004667391534894705 BETTER
I0325 08:11:07.765045 513838 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.006290946155786514 old loss 0.006313046906143427 BETTER
I0325 08:11:20.952495 512641 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.005274240393191576 old loss 0.005289679858833551 BETTER
I0325 08:11:24.573107 514749 finetune.py:68] layer 23_up @ epoch 4 new loss 0.004636466968804598 old loss 0.0046507371589541435 BETTER
I0325 08:11:35.879346 513838 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.006273119244724512 old loss 0.006290946155786514 BETTER
I0325 08:11:50.632055 514749 finetune.py:45] layer 23_gate initial loss 0.006804188713431358
I0325 08:11:51.083905 512641 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.005260366480797529 old loss 0.005274240393191576 BETTER
I0325 08:12:03.969525 513838 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0062567018903791904 old loss 0.006273119244724512 BETTER
I0325 08:12:18.194580 514749 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.006783655844628811 old loss 0.006804188713431358 BETTER
I0325 08:12:21.237818 512641 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.005247326567769051 old loss 0.005260366480797529 BETTER
I0325 08:12:32.061812 513838 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.00624141376465559 old loss 0.0062567018903791904 BETTER
I0325 08:12:46.162379 514749 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.006766109261661768 old loss 0.006783655844628811 BETTER
I0325 08:13:00.392441 513838 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.006227964535355568 old loss 0.00624141376465559 BETTER
I0325 08:13:03.190672 512641 finetune.py:45] layer 21_down initial loss 0.008620097301900387
I0325 08:13:14.438621 514749 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.006750060711055994 old loss 0.006766109261661768 BETTER
I0325 08:13:29.699748 512641 finetune.py:68] layer 21_down @ epoch 0 new loss 0.008614907041192055 old loss 0.008620097301900387 BETTER
I0325 08:13:42.586671 514749 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.0067349751479923725 old loss 0.006750060711055994 BETTER
I0325 08:13:44.495425 513838 finetune.py:45] layer 22_down initial loss 0.010080519132316113
I0325 08:13:57.640001 512641 finetune.py:68] layer 21_down @ epoch 1 new loss 0.008610027842223644 old loss 0.008614907041192055 BETTER
I0325 08:14:09.844359 513838 finetune.py:68] layer 22_down @ epoch 0 new loss 0.010075106285512447 old loss 0.010080519132316113 BETTER
I0325 08:14:10.940908 514749 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.006721351761370897 old loss 0.0067349751479923725 BETTER
I0325 08:14:26.207067 512641 finetune.py:68] layer 21_down @ epoch 2 new loss 0.008605393581092358 old loss 0.008610027842223644 BETTER
I0325 08:14:36.700824 513838 finetune.py:68] layer 22_down @ epoch 1 new loss 0.010070009157061577 old loss 0.010075106285512447 BETTER
I0325 08:14:54.540620 514749 finetune.py:45] layer 23_down initial loss 0.010730001144111156
I0325 08:14:54.781767 512641 finetune.py:68] layer 21_down @ epoch 3 new loss 0.008601019158959389 old loss 0.008605393581092358 BETTER
I0325 08:15:03.546568 513838 finetune.py:68] layer 22_down @ epoch 2 new loss 0.010065204463899136 old loss 0.010070009157061577 BETTER
I0325 08:15:20.033982 514749 finetune.py:68] layer 23_down @ epoch 0 new loss 0.010724689811468124 old loss 0.010730001144111156 BETTER
I0325 08:15:23.347834 512641 finetune.py:68] layer 21_down @ epoch 4 new loss 0.00859687477350235 old loss 0.008601019158959389 BETTER
21_v proxy err 0.05247435346245766 tr(WHW.T) 4916.306640625
bpp_loss 2.2063381467887666
21_q proxy err 0.008921091444790363 tr(WHW.T) 30404.787109375
bpp_loss 2.4610378507641144
21_k proxy err 0.006326145492494106 tr(WHW.T) 42939.90234375
bpp_loss 2.4769051822368056
21_o proxy err 0.049614645540714264 tr(WHW.T) 6508.85986328125
bpp_loss 2.2020248280023225
21_up proxy err 0.04288953170180321 tr(WHW.T) 19709.369140625
bpp_loss 2.2540386735873166
21_gate proxy err 0.02593381330370903 tr(WHW.T) 33274.4140625
bpp_loss 2.3482131756582234
21_down proxy err 0.05211459845304489 tr(WHW.T) 16132.921875
bpp_loss 2.2445557287482676
I0325 08:15:31.623296 513838 finetune.py:68] layer 22_down @ epoch 3 new loss 0.010060600005090237 old loss 0.010065204463899136 BETTER
I0325 08:15:47.628861 514749 finetune.py:68] layer 23_down @ epoch 1 new loss 0.010719714686274529 old loss 0.010724689811468124 BETTER
I0325 08:15:58.416111 513838 finetune.py:68] layer 22_down @ epoch 4 new loss 0.01005625631660223 old loss 0.010060600005090237 BETTER
22_v proxy err 0.05027097463607788 tr(WHW.T) 5165.8642578125
bpp_loss 2.2103455139440484
22_q proxy err 0.008524964563548565 tr(WHW.T) 32226.75390625
bpp_loss 2.4960118674498517
22_k proxy err 0.006242810282856226 tr(WHW.T) 44126.12890625
bpp_loss 2.514980559266405
22_o proxy err 0.03994324058294296 tr(WHW.T) 7746.44775390625
bpp_loss 2.195934977818979
22_up proxy err 0.043545179069042206 tr(WHW.T) 19586.369140625
bpp_loss 2.2538764038837926
22_gate proxy err 0.026621444150805473 tr(WHW.T) 32759.994140625
bpp_loss 2.3561600814032
22_down proxy err 0.05246470123529434 tr(WHW.T) 16188.8212890625
bpp_loss 2.245330669029161
I0325 08:16:14.322565 514749 finetune.py:68] layer 23_down @ epoch 2 new loss 0.010714941658079624 old loss 0.010719714686274529 BETTER
I0325 08:16:41.253210 514749 finetune.py:68] layer 23_down @ epoch 3 new loss 0.010710501112043858 old loss 0.010714941658079624 BETTER
I0325 08:17:05.568876 382969 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 62.67630743980408s
I0325 08:17:07.945949 514749 finetune.py:68] layer 23_down @ epoch 4 new loss 0.010706239379942417 old loss 0.010710501112043858 BETTER
I0325 08:17:09.035855 530340 config.py:54] PyTorch version 2.6.0 available.
W0325 08:17:09.323822 530340 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

23_v proxy err 0.04827823117375374 tr(WHW.T) 5725.64111328125
bpp_loss 2.264330751670059
23_q proxy err 0.010191312059760094 tr(WHW.T) 28334.78125
bpp_loss 2.4905692554602865
23_k proxy err 0.007507436443120241 tr(WHW.T) 38526.27734375
bpp_loss 2.502836145198671
23_o proxy err 0.050421103835105896 tr(WHW.T) 6462.119140625
bpp_loss 2.2479426350910217
23_up proxy err 0.04581665247678757 tr(WHW.T) 18913.548828125
bpp_loss 2.2589262366987937
23_gate proxy err 0.02900761365890503 tr(WHW.T) 30503.11328125
bpp_loss 2.3558191112567517
23_down proxy err 0.053513117134571075 tr(WHW.T) 16137.314453125
bpp_loss 2.2515798710113346
W0325 08:17:10.204556 530340 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 08:17:10.208364 382969 quantize_finetune_llama.py:209] layer 25 gpu 1
I0325 08:17:10.222213 530340 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 08:17:28.197215 530340 finetune.py:45] layer 24_v initial loss 0.0016549656866118312
W0325 08:17:28.197410 530340 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 08:18:00.912161 530340 finetune.py:68] layer 24_v @ epoch 0 new loss 0.001026200712658465 old loss 0.0016549656866118312 BETTER
I0325 08:18:12.441195 382969 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 60.302854776382446s
I0325 08:18:15.891166 531236 config.py:54] PyTorch version 2.6.0 available.
W0325 08:18:16.191634 531236 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 08:18:17.148884 531236 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 08:18:17.152823 382969 quantize_finetune_llama.py:209] layer 26 gpu 2
I0325 08:18:17.166082 531236 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 08:18:34.947024 530340 finetune.py:68] layer 24_v @ epoch 1 new loss 0.0009188354015350342 old loss 0.001026200712658465 BETTER
I0325 08:18:35.612003 531236 finetune.py:45] layer 25_v initial loss 0.001908918609842658
W0325 08:18:35.612359 531236 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 08:19:06.944758 531236 finetune.py:68] layer 25_v @ epoch 0 new loss 0.0010028030956164002 old loss 0.001908918609842658 BETTER
I0325 08:19:09.637712 530340 finetune.py:68] layer 24_v @ epoch 2 new loss 0.0008831652230583131 old loss 0.0009188354015350342 BETTER
I0325 08:19:19.544445 382969 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 61.94966268539429s
I0325 08:19:23.153946 532117 config.py:54] PyTorch version 2.6.0 available.
W0325 08:19:23.474211 532117 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 08:19:24.459278 532117 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 08:19:24.463467 382969 quantize_finetune_llama.py:209] layer 27 gpu 0
I0325 08:19:24.476468 532117 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 08:19:39.705342 531236 finetune.py:68] layer 25_v @ epoch 1 new loss 0.000855106336530298 old loss 0.0010028030956164002 BETTER
I0325 08:19:43.823737 532117 finetune.py:45] layer 26_v initial loss 0.0023579918779432774
W0325 08:19:43.824004 532117 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 08:19:44.618739 530340 finetune.py:68] layer 24_v @ epoch 3 new loss 0.0008634621044620872 old loss 0.0008831652230583131 BETTER
I0325 08:20:12.358151 531236 finetune.py:68] layer 25_v @ epoch 2 new loss 0.000815355044323951 old loss 0.000855106336530298 BETTER
I0325 08:20:15.264362 532117 finetune.py:68] layer 26_v @ epoch 0 new loss 0.0014090059557929635 old loss 0.0023579918779432774 BETTER
I0325 08:20:19.474297 530340 finetune.py:68] layer 24_v @ epoch 4 new loss 0.0008506019948981702 old loss 0.0008634621044620872 BETTER
I0325 08:20:38.367472 530340 finetune.py:45] layer 24_q initial loss 0.0010991946328431368
I0325 08:20:45.307729 531236 finetune.py:68] layer 25_v @ epoch 3 new loss 0.0007963984971866012 old loss 0.000815355044323951 BETTER
I0325 08:20:47.814079 532117 finetune.py:68] layer 26_v @ epoch 1 new loss 0.0012892305385321379 old loss 0.0014090059557929635 BETTER
I0325 08:21:11.467000 530340 finetune.py:68] layer 24_q @ epoch 0 new loss 0.0010549802100285888 old loss 0.0010991946328431368 BETTER
I0325 08:21:18.615596 531236 finetune.py:68] layer 25_v @ epoch 4 new loss 0.0007845244836062193 old loss 0.0007963984971866012 BETTER
I0325 08:21:20.705853 532117 finetune.py:68] layer 26_v @ epoch 2 new loss 0.0012491389643400908 old loss 0.0012892305385321379 BETTER
I0325 08:21:37.699497 531236 finetune.py:45] layer 25_q initial loss 0.0010375361889600754
I0325 08:21:45.644644 530340 finetune.py:68] layer 24_q @ epoch 1 new loss 0.001037820940837264 old loss 0.0010549802100285888 BETTER
I0325 08:21:53.577443 532117 finetune.py:68] layer 26_v @ epoch 3 new loss 0.0012261888477951288 old loss 0.0012491389643400908 BETTER
I0325 08:22:09.181060 531236 finetune.py:68] layer 25_q @ epoch 0 new loss 0.0009821780258789659 old loss 0.0010375361889600754 BETTER
I0325 08:22:20.124972 530340 finetune.py:68] layer 24_q @ epoch 2 new loss 0.0010262506548315287 old loss 0.001037820940837264 BETTER
I0325 08:22:26.866902 532117 finetune.py:68] layer 26_v @ epoch 4 new loss 0.0012099649757146835 old loss 0.0012261888477951288 BETTER
I0325 08:22:41.530658 531236 finetune.py:68] layer 25_q @ epoch 1 new loss 0.0009642725344747305 old loss 0.0009821780258789659 BETTER
I0325 08:22:46.090002 532117 finetune.py:45] layer 26_q initial loss 0.001559294993057847
I0325 08:22:54.768439 530340 finetune.py:68] layer 24_q @ epoch 3 new loss 0.0010171597823500633 old loss 0.0010262506548315287 BETTER
I0325 08:23:14.001655 531236 finetune.py:68] layer 25_q @ epoch 2 new loss 0.000953059468884021 old loss 0.0009642725344747305 BETTER
I0325 08:23:17.516866 532117 finetune.py:68] layer 26_q @ epoch 0 new loss 0.001504141720943153 old loss 0.001559294993057847 BETTER
I0325 08:23:29.397669 530340 finetune.py:68] layer 24_q @ epoch 4 new loss 0.0010097872000187635 old loss 0.0010171597823500633 BETTER
I0325 08:23:46.388511 531236 finetune.py:68] layer 25_q @ epoch 3 new loss 0.0009445992182008922 old loss 0.000953059468884021 BETTER
I0325 08:23:48.368349 530340 finetune.py:45] layer 24_k initial loss 0.0012553887208923697
I0325 08:23:50.021250 532117 finetune.py:68] layer 26_q @ epoch 1 new loss 0.0014819364296272397 old loss 0.001504141720943153 BETTER
I0325 08:24:19.098492 531236 finetune.py:68] layer 25_q @ epoch 4 new loss 0.0009373420034535229 old loss 0.0009445992182008922 BETTER
I0325 08:24:20.909293 530340 finetune.py:68] layer 24_k @ epoch 0 new loss 0.0012419604463502765 old loss 0.0012553887208923697 BETTER
I0325 08:24:22.451058 532117 finetune.py:68] layer 26_q @ epoch 2 new loss 0.0014667515642940998 old loss 0.0014819364296272397 BETTER
I0325 08:24:38.384768 531236 finetune.py:45] layer 25_k initial loss 0.0012115210993215442
I0325 08:24:54.600705 530340 finetune.py:68] layer 24_k @ epoch 1 new loss 0.00123309763148427 old loss 0.0012419604463502765 BETTER
I0325 08:24:55.041809 532117 finetune.py:68] layer 26_q @ epoch 3 new loss 0.0014537115348502994 old loss 0.0014667515642940998 BETTER
I0325 08:25:09.644574 531236 finetune.py:68] layer 25_k @ epoch 0 new loss 0.0011968950275331736 old loss 0.0012115210993215442 BETTER
I0325 08:25:27.312261 532117 finetune.py:68] layer 26_q @ epoch 4 new loss 0.0014431023737415671 old loss 0.0014537115348502994 BETTER
I0325 08:25:28.632292 530340 finetune.py:68] layer 24_k @ epoch 2 new loss 0.001225771615281701 old loss 0.00123309763148427 BETTER
I0325 08:25:41.734247 531236 finetune.py:68] layer 25_k @ epoch 1 new loss 0.001186966197565198 old loss 0.0011968950275331736 BETTER
I0325 08:25:46.621788 532117 finetune.py:45] layer 26_k initial loss 0.00177323492243886
I0325 08:26:02.739122 530340 finetune.py:68] layer 24_k @ epoch 3 new loss 0.0012193041620776057 old loss 0.001225771615281701 BETTER
I0325 08:26:13.845410 531236 finetune.py:68] layer 25_k @ epoch 2 new loss 0.0011788413394242525 old loss 0.001186966197565198 BETTER
I0325 08:26:17.995532 532117 finetune.py:68] layer 26_k @ epoch 0 new loss 0.001753560034558177 old loss 0.00177323492243886 BETTER
I0325 08:26:36.922708 530340 finetune.py:68] layer 24_k @ epoch 4 new loss 0.0012133549898862839 old loss 0.0012193041620776057 BETTER
I0325 08:26:45.957856 531236 finetune.py:68] layer 25_k @ epoch 3 new loss 0.0011717125307768583 old loss 0.0011788413394242525 BETTER
I0325 08:26:50.026547 532117 finetune.py:68] layer 26_k @ epoch 1 new loss 0.0017429941799491644 old loss 0.001753560034558177 BETTER
I0325 08:26:55.668503 530340 finetune.py:45] layer 24_o initial loss 0.002509149955585599
I0325 08:27:18.107192 531236 finetune.py:68] layer 25_k @ epoch 4 new loss 0.0011653329711407423 old loss 0.0011717125307768583 BETTER
I0325 08:27:22.076933 532117 finetune.py:68] layer 26_k @ epoch 2 new loss 0.0017335197189822793 old loss 0.0017429941799491644 BETTER
I0325 08:27:27.965930 530340 finetune.py:68] layer 24_o @ epoch 0 new loss 0.0024511574301868677 old loss 0.002509149955585599 BETTER
I0325 08:27:37.457212 531236 finetune.py:45] layer 25_o initial loss 0.002264323178678751
I0325 08:27:54.127287 532117 finetune.py:68] layer 26_k @ epoch 3 new loss 0.0017255092971026897 old loss 0.0017335197189822793 BETTER
I0325 08:28:01.262224 530340 finetune.py:68] layer 24_o @ epoch 1 new loss 0.0024344895500689745 old loss 0.0024511574301868677 BETTER
I0325 08:28:07.889631 531236 finetune.py:68] layer 25_o @ epoch 0 new loss 0.002187394769862294 old loss 0.002264323178678751 BETTER
I0325 08:28:26.297644 532117 finetune.py:68] layer 26_k @ epoch 4 new loss 0.0017181597650051117 old loss 0.0017255092971026897 BETTER
I0325 08:28:34.524571 530340 finetune.py:68] layer 24_o @ epoch 2 new loss 0.002422679914161563 old loss 0.0024344895500689745 BETTER
I0325 08:28:39.260044 531236 finetune.py:68] layer 25_o @ epoch 1 new loss 0.0021712847519665956 old loss 0.002187394769862294 BETTER
I0325 08:28:45.540685 532117 finetune.py:45] layer 26_o initial loss 0.003303427482023835
I0325 08:29:07.833943 530340 finetune.py:68] layer 24_o @ epoch 3 new loss 0.002413251670077443 old loss 0.002422679914161563 BETTER
I0325 08:29:10.831169 531236 finetune.py:68] layer 25_o @ epoch 2 new loss 0.0021603074856102467 old loss 0.0021712847519665956 BETTER
I0325 08:29:16.278963 532117 finetune.py:68] layer 26_o @ epoch 0 new loss 0.003217657096683979 old loss 0.003303427482023835 BETTER
I0325 08:29:41.288619 530340 finetune.py:68] layer 24_o @ epoch 4 new loss 0.002405337058007717 old loss 0.002413251670077443 BETTER
I0325 08:29:42.441795 531236 finetune.py:68] layer 25_o @ epoch 3 new loss 0.002151236869394779 old loss 0.0021603074856102467 BETTER
I0325 08:29:47.810342 532117 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0031974639277905226 old loss 0.003217657096683979 BETTER
I0325 08:30:06.948310 530340 finetune.py:45] layer 24_up initial loss 0.005364891141653061
I0325 08:30:14.091998 531236 finetune.py:68] layer 25_o @ epoch 4 new loss 0.0021435334347188473 old loss 0.002151236869394779 BETTER
I0325 08:30:19.341337 532117 finetune.py:68] layer 26_o @ epoch 2 new loss 0.003181740175932646 old loss 0.0031974639277905226 BETTER
I0325 08:30:37.384271 530340 finetune.py:68] layer 24_up @ epoch 0 new loss 0.005327069200575352 old loss 0.005364891141653061 BETTER
I0325 08:30:40.442777 531236 finetune.py:45] layer 25_up initial loss 0.005484960973262787
I0325 08:30:50.969268 532117 finetune.py:68] layer 26_o @ epoch 3 new loss 0.0031686730217188597 old loss 0.003181740175932646 BETTER
I0325 08:31:08.874383 530340 finetune.py:68] layer 24_up @ epoch 1 new loss 0.005303199868649244 old loss 0.005327069200575352 BETTER
I0325 08:31:09.192941 531236 finetune.py:68] layer 25_up @ epoch 0 new loss 0.005443203262984753 old loss 0.005484960973262787 BETTER
I0325 08:31:22.642438 532117 finetune.py:68] layer 26_o @ epoch 4 new loss 0.003157599363476038 old loss 0.0031686730217188597 BETTER
I0325 08:31:39.315018 531236 finetune.py:68] layer 25_up @ epoch 1 new loss 0.005417839158326387 old loss 0.005443203262984753 BETTER
I0325 08:31:40.654468 530340 finetune.py:68] layer 24_up @ epoch 2 new loss 0.005284523591399193 old loss 0.005303199868649244 BETTER
I0325 08:31:49.405201 532117 finetune.py:45] layer 26_up initial loss 0.006825186312198639
I0325 08:32:09.315626 531236 finetune.py:68] layer 25_up @ epoch 2 new loss 0.005397297441959381 old loss 0.005417839158326387 BETTER
I0325 08:32:12.419712 530340 finetune.py:68] layer 24_up @ epoch 3 new loss 0.005268879700452089 old loss 0.005284523591399193 BETTER
I0325 08:32:18.212985 532117 finetune.py:68] layer 26_up @ epoch 0 new loss 0.006778188049793243 old loss 0.006825186312198639 BETTER
I0325 08:32:39.268723 531236 finetune.py:68] layer 25_up @ epoch 3 new loss 0.005379942711442709 old loss 0.005397297441959381 BETTER
I0325 08:32:44.744389 530340 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0052551887929439545 old loss 0.005268879700452089 BETTER
I0325 08:32:48.887702 532117 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0067499494180083275 old loss 0.006778188049793243 BETTER
I0325 08:33:09.517194 531236 finetune.py:68] layer 25_up @ epoch 4 new loss 0.0053645833395421505 old loss 0.005379942711442709 BETTER
I0325 08:33:13.929250 530340 finetune.py:45] layer 24_gate initial loss 0.007677407469600439
I0325 08:33:18.906503 532117 finetune.py:68] layer 26_up @ epoch 2 new loss 0.0067274668253958225 old loss 0.0067499494180083275 BETTER
I0325 08:33:36.818942 531236 finetune.py:45] layer 25_gate initial loss 0.00807932484894991
I0325 08:33:42.646235 530340 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0076573374681174755 old loss 0.007677407469600439 BETTER
I0325 08:33:48.816359 532117 finetune.py:68] layer 26_up @ epoch 3 new loss 0.006708276458084583 old loss 0.0067274668253958225 BETTER
I0325 08:34:04.217346 531236 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.008057400584220886 old loss 0.00807932484894991 BETTER
I0325 08:34:12.578428 530340 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.007639688905328512 old loss 0.0076573374681174755 BETTER
I0325 08:34:18.896419 532117 finetune.py:68] layer 26_up @ epoch 4 new loss 0.00669111730530858 old loss 0.006708276458084583 BETTER
I0325 08:34:32.390799 531236 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.008039104752242565 old loss 0.008057400584220886 BETTER
I0325 08:34:42.567600 530340 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.007623715791851282 old loss 0.007639688905328512 BETTER
I0325 08:34:45.388180 532117 finetune.py:45] layer 26_gate initial loss 0.009745147079229355
I0325 08:35:00.480527 531236 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.008022378198802471 old loss 0.008039104752242565 BETTER
I0325 08:35:12.941206 530340 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.007609338965266943 old loss 0.007623715791851282 BETTER
I0325 08:35:13.136977 532117 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.009720413014292717 old loss 0.009745147079229355 BETTER
I0325 08:35:28.614811 531236 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.008006971329450607 old loss 0.008022378198802471 BETTER
I0325 08:35:41.502275 532117 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.009700530208647251 old loss 0.009720413014292717 BETTER
I0325 08:35:43.039843 530340 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.007595629896968603 old loss 0.007609338965266943 BETTER
I0325 08:35:56.950504 531236 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.007992668077349663 old loss 0.008006971329450607 BETTER
I0325 08:36:09.693269 532117 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.00968221016228199 old loss 0.009700530208647251 BETTER
I0325 08:36:25.109732 530340 finetune.py:45] layer 24_down initial loss 0.011858156882226467
I0325 08:36:37.825521 532117 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.009665260091423988 old loss 0.00968221016228199 BETTER
I0325 08:36:40.206808 531236 finetune.py:45] layer 25_down initial loss 0.012462676502764225
I0325 08:36:51.759697 530340 finetune.py:68] layer 24_down @ epoch 0 new loss 0.011852435767650604 old loss 0.011858156882226467 BETTER
I0325 08:37:05.758936 531236 finetune.py:68] layer 25_down @ epoch 0 new loss 0.012456616386771202 old loss 0.012462676502764225 BETTER
I0325 08:37:06.185445 532117 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.00964976567775011 old loss 0.009665260091423988 BETTER
I0325 08:37:19.745713 530340 finetune.py:68] layer 24_down @ epoch 1 new loss 0.011847113259136677 old loss 0.011852435767650604 BETTER
I0325 08:37:32.503530 531236 finetune.py:68] layer 25_down @ epoch 1 new loss 0.012450872920453548 old loss 0.012456616386771202 BETTER
I0325 08:37:48.163186 530340 finetune.py:68] layer 24_down @ epoch 2 new loss 0.011842114850878716 old loss 0.011847113259136677 BETTER
I0325 08:37:50.067908 532117 finetune.py:45] layer 26_down initial loss 0.014534060843288898
I0325 08:37:59.226653 531236 finetune.py:68] layer 25_down @ epoch 2 new loss 0.012445539236068726 old loss 0.012450872920453548 BETTER
I0325 08:38:15.685435 532117 finetune.py:68] layer 26_down @ epoch 0 new loss 0.014527888037264347 old loss 0.014534060843288898 BETTER
I0325 08:38:16.725476 530340 finetune.py:68] layer 24_down @ epoch 3 new loss 0.011837410740554333 old loss 0.011842114850878716 BETTER
I0325 08:38:25.957142 531236 finetune.py:68] layer 25_down @ epoch 3 new loss 0.01244060043245554 old loss 0.012445539236068726 BETTER
I0325 08:38:42.765704 532117 finetune.py:68] layer 26_down @ epoch 1 new loss 0.014522337354719639 old loss 0.014527888037264347 BETTER
I0325 08:38:45.249422 530340 finetune.py:68] layer 24_down @ epoch 4 new loss 0.011832944117486477 old loss 0.011837410740554333 BETTER
24_v proxy err 0.05003911629319191 tr(WHW.T) 5382.7841796875
bpp_loss 2.253368982957909
24_q proxy err 0.010259578935801983 tr(WHW.T) 27128.916015625
bpp_loss 2.435885091428645
24_k proxy err 0.006976231466978788 tr(WHW.T) 39898.8359375
bpp_loss 2.4443699108087458
24_o proxy err 0.03782375901937485 tr(WHW.T) 8149.005859375
bpp_loss 2.2408289493760094
24_up proxy err 0.04692753776907921 tr(WHW.T) 18678.08984375
bpp_loss 2.2622318982558194
24_gate proxy err 0.029540086165070534 tr(WHW.T) 30296.224609375
bpp_loss 2.3580175174703433
24_down proxy err 0.053723420947790146 tr(WHW.T) 16069.7763671875
bpp_loss 2.2556368571727776
I0325 08:38:54.028636 531236 finetune.py:68] layer 25_down @ epoch 4 new loss 0.01243588887155056 old loss 0.01244060043245554 BETTER
25_v proxy err 0.04850687459111214 tr(WHW.T) 5989.998046875
bpp_loss 2.2983737527101766
25_q proxy err 0.011890736408531666 tr(WHW.T) 25148.203125
bpp_loss 2.4584786658233497
25_k proxy err 0.008865362033247948 tr(WHW.T) 33757.59375
bpp_loss 2.4648573267331813
25_o proxy err 0.04886908829212189 tr(WHW.T) 6913.162109375
bpp_loss 2.293760706699686
25_up proxy err 0.04693496972322464 tr(WHW.T) 18791.322265625
bpp_loss 2.266032145943406
25_gate proxy err 0.02890058048069477 tr(WHW.T) 31133.580078125
bpp_loss 2.358135687750439
25_down proxy err 0.05171767622232437 tr(WHW.T) 16202.4384765625
bpp_loss 2.259007111527459
I0325 08:39:10.696644 532117 finetune.py:68] layer 26_down @ epoch 2 new loss 0.014517080970108509 old loss 0.014522337354719639 BETTER
I0325 08:39:37.328891 532117 finetune.py:68] layer 26_down @ epoch 3 new loss 0.014512124471366405 old loss 0.014517080970108509 BETTER
I0325 08:40:01.919515 382969 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 63.279786348342896s
I0325 08:40:04.301383 532117 finetune.py:68] layer 26_down @ epoch 4 new loss 0.014507489278912544 old loss 0.014512124471366405 BETTER
I0325 08:40:05.476312 548361 config.py:54] PyTorch version 2.6.0 available.
26_v proxy err 0.04760481417179108 tr(WHW.T) 5985.25439453125
bpp_loss 2.3127066415327135
26_q proxy err 0.01082311850041151 tr(WHW.T) 26835.12890625
bpp_loss 2.4347733694012277
26_k proxy err 0.00771690346300602 tr(WHW.T) 37686.9921875
bpp_loss 2.4470641385996714
26_o proxy err 0.03000090830028057 tr(WHW.T) 10001.6708984375
bpp_loss 2.306141587876482
26_up proxy err 0.04399323835968971 tr(WHW.T) 20045.8515625
bpp_loss 2.270380743367727
26_gate proxy err 0.02681794948875904 tr(WHW.T) 33546.5
bpp_loss 2.359018940998371
26_down proxy err 0.05271175876259804 tr(WHW.T) 15757.93359375
bpp_loss 2.2600276807409734
W0325 08:40:05.766541 548361 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 08:40:06.733655 548361 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 08:40:06.737654 382969 quantize_finetune_llama.py:209] layer 28 gpu 1
I0325 08:40:06.750498 548361 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 08:40:25.184620 548361 finetune.py:45] layer 27_v initial loss 0.002452530898153782
W0325 08:40:25.184815 548361 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 08:40:58.507452 548361 finetune.py:68] layer 27_v @ epoch 0 new loss 0.0012773487251251936 old loss 0.002452530898153782 BETTER
I0325 08:41:09.636605 382969 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 61.01413416862488s
I0325 08:41:13.145451 549253 config.py:54] PyTorch version 2.6.0 available.
W0325 08:41:13.444306 549253 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 08:41:14.470836 549253 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 08:41:14.474806 382969 quantize_finetune_llama.py:209] layer 29 gpu 2
I0325 08:41:14.487210 549253 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 08:41:32.985661 549253 finetune.py:45] layer 28_v initial loss 0.0028476458974182606
W0325 08:41:32.985903 549253 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 08:41:33.235782 548361 finetune.py:68] layer 27_v @ epoch 1 new loss 0.0011702398769557476 old loss 0.0012773487251251936 BETTER
I0325 08:42:04.487561 549253 finetune.py:68] layer 28_v @ epoch 0 new loss 0.001570283086039126 old loss 0.0028476458974182606 BETTER
I0325 08:42:08.165824 548361 finetune.py:68] layer 27_v @ epoch 2 new loss 0.0011389723513275385 old loss 0.0011702398769557476 BETTER
I0325 08:42:18.804435 382969 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 63.89388871192932s
I0325 08:42:22.343040 550186 config.py:54] PyTorch version 2.6.0 available.
W0325 08:42:22.664056 550186 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 08:42:23.594510 550186 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 08:42:23.598601 382969 quantize_finetune_llama.py:209] layer 30 gpu 0
I0325 08:42:23.611698 550186 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 08:42:36.779273 549253 finetune.py:68] layer 28_v @ epoch 1 new loss 0.0014819090720266104 old loss 0.001570283086039126 BETTER
I0325 08:42:41.634216 550186 finetune.py:45] layer 29_v initial loss 0.002525545423850417
W0325 08:42:41.634397 550186 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 08:42:43.321679 548361 finetune.py:68] layer 27_v @ epoch 3 new loss 0.0011211119126528502 old loss 0.0011389723513275385 BETTER
I0325 08:43:09.663839 549253 finetune.py:68] layer 28_v @ epoch 2 new loss 0.0014495437499135733 old loss 0.0014819090720266104 BETTER
I0325 08:43:13.167520 550186 finetune.py:68] layer 29_v @ epoch 0 new loss 0.0016293145017698407 old loss 0.002525545423850417 BETTER
I0325 08:43:18.633291 548361 finetune.py:68] layer 27_v @ epoch 4 new loss 0.0011083820136263967 old loss 0.0011211119126528502 BETTER
I0325 08:43:37.400411 548361 finetune.py:45] layer 27_q initial loss 0.0014929472235962749
I0325 08:43:42.959594 549253 finetune.py:68] layer 28_v @ epoch 3 new loss 0.001428476651199162 old loss 0.0014495437499135733 BETTER
I0325 08:43:46.049764 550186 finetune.py:68] layer 29_v @ epoch 1 new loss 0.001577076269313693 old loss 0.0016293145017698407 BETTER
I0325 08:44:10.633401 548361 finetune.py:68] layer 27_q @ epoch 0 new loss 0.0014163213782012463 old loss 0.0014929472235962749 BETTER
I0325 08:44:16.168898 549253 finetune.py:68] layer 28_v @ epoch 4 new loss 0.0014134589582681656 old loss 0.001428476651199162 BETTER
I0325 08:44:19.121360 550186 finetune.py:68] layer 29_v @ epoch 2 new loss 0.0015499048167839646 old loss 0.001577076269313693 BETTER
I0325 08:44:35.298509 549253 finetune.py:45] layer 28_q initial loss 0.0018619403708726168
I0325 08:44:45.266033 548361 finetune.py:68] layer 27_q @ epoch 1 new loss 0.0013913236325606704 old loss 0.0014163213782012463 BETTER
I0325 08:44:52.370913 550186 finetune.py:68] layer 29_v @ epoch 3 new loss 0.001532372203655541 old loss 0.0015499048167839646 BETTER
I0325 08:45:06.933682 549253 finetune.py:68] layer 28_q @ epoch 0 new loss 0.001777046942152083 old loss 0.0018619403708726168 BETTER
I0325 08:45:20.153067 548361 finetune.py:68] layer 27_q @ epoch 2 new loss 0.0013742849696427584 old loss 0.0013913236325606704 BETTER
I0325 08:45:25.921522 550186 finetune.py:68] layer 29_v @ epoch 4 new loss 0.0015173173742368817 old loss 0.001532372203655541 BETTER
I0325 08:45:39.464336 549253 finetune.py:68] layer 28_q @ epoch 1 new loss 0.0017509296303614974 old loss 0.001777046942152083 BETTER
I0325 08:45:44.769750 550186 finetune.py:45] layer 29_q initial loss 0.0019257784588262439
I0325 08:45:55.002894 548361 finetune.py:68] layer 27_q @ epoch 3 new loss 0.0013611261965706944 old loss 0.0013742849696427584 BETTER
I0325 08:46:11.774718 549253 finetune.py:68] layer 28_q @ epoch 2 new loss 0.0017322869971394539 old loss 0.0017509296303614974 BETTER
I0325 08:46:16.304783 550186 finetune.py:68] layer 29_q @ epoch 0 new loss 0.0018518809229135513 old loss 0.0019257784588262439 BETTER
I0325 08:46:29.892511 548361 finetune.py:68] layer 27_q @ epoch 4 new loss 0.0013499449705705047 old loss 0.0013611261965706944 BETTER
I0325 08:46:44.762912 549253 finetune.py:68] layer 28_q @ epoch 3 new loss 0.0017170669743791223 old loss 0.0017322869971394539 BETTER
I0325 08:46:48.630229 550186 finetune.py:68] layer 29_q @ epoch 1 new loss 0.0018299167277291417 old loss 0.0018518809229135513 BETTER
I0325 08:46:49.140392 548361 finetune.py:45] layer 27_k initial loss 0.0016793785616755486
I0325 08:47:17.996409 549253 finetune.py:68] layer 28_q @ epoch 4 new loss 0.0017045303247869015 old loss 0.0017170669743791223 BETTER
I0325 08:47:21.724930 550186 finetune.py:68] layer 29_q @ epoch 2 new loss 0.0018151013646274805 old loss 0.0018299167277291417 BETTER
I0325 08:47:22.267320 548361 finetune.py:68] layer 27_k @ epoch 0 new loss 0.0016613563057035208 old loss 0.0016793785616755486 BETTER
I0325 08:47:38.282038 549253 finetune.py:45] layer 28_k initial loss 0.002137733157724142
I0325 08:47:55.403650 550186 finetune.py:68] layer 29_q @ epoch 3 new loss 0.0018022245494648814 old loss 0.0018151013646274805 BETTER
I0325 08:47:56.894332 548361 finetune.py:68] layer 27_k @ epoch 1 new loss 0.001650424674153328 old loss 0.0016613563057035208 BETTER
I0325 08:48:09.733700 549253 finetune.py:68] layer 28_k @ epoch 0 new loss 0.0021144086495041847 old loss 0.002137733157724142 BETTER
I0325 08:48:28.424997 550186 finetune.py:68] layer 29_q @ epoch 4 new loss 0.001791779650375247 old loss 0.0018022245494648814 BETTER
I0325 08:48:30.994488 548361 finetune.py:68] layer 27_k @ epoch 2 new loss 0.001641334849409759 old loss 0.001650424674153328 BETTER
I0325 08:48:41.856082 549253 finetune.py:68] layer 28_k @ epoch 1 new loss 0.0021004413720220327 old loss 0.0021144086495041847 BETTER
I0325 08:48:49.646810 550186 finetune.py:45] layer 29_k initial loss 0.0022128443233668804
I0325 08:49:05.218525 548361 finetune.py:68] layer 27_k @ epoch 3 new loss 0.0016333016101270914 old loss 0.001641334849409759 BETTER
I0325 08:49:14.219991 549253 finetune.py:68] layer 28_k @ epoch 2 new loss 0.002089443150907755 old loss 0.0021004413720220327 BETTER
I0325 08:49:21.180824 550186 finetune.py:68] layer 29_k @ epoch 0 new loss 0.0021908024791628122 old loss 0.0022128443233668804 BETTER
I0325 08:49:39.326478 548361 finetune.py:68] layer 27_k @ epoch 4 new loss 0.001626060577109456 old loss 0.0016333016101270914 BETTER
I0325 08:49:46.507918 549253 finetune.py:68] layer 28_k @ epoch 3 new loss 0.0020792754366993904 old loss 0.002089443150907755 BETTER
I0325 08:49:53.218099 550186 finetune.py:68] layer 29_k @ epoch 1 new loss 0.002177299465984106 old loss 0.0021908024791628122 BETTER
I0325 08:49:58.741422 548361 finetune.py:45] layer 27_o initial loss 0.003024354577064514
I0325 08:50:18.638123 549253 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0020707265939563513 old loss 0.0020792754366993904 BETTER
I0325 08:50:25.269714 550186 finetune.py:68] layer 29_k @ epoch 2 new loss 0.0021666481625288725 old loss 0.002177299465984106 BETTER
I0325 08:50:30.937769 548361 finetune.py:68] layer 27_o @ epoch 0 new loss 0.002925393870100379 old loss 0.003024354577064514 BETTER
I0325 08:50:38.627615 549253 finetune.py:45] layer 28_o initial loss 0.0037824322935193777
I0325 08:50:57.458805 550186 finetune.py:68] layer 29_k @ epoch 3 new loss 0.0021569086238741875 old loss 0.0021666481625288725 BETTER
I0325 08:51:04.340497 548361 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0029032735619693995 old loss 0.002925393870100379 BETTER
I0325 08:51:09.260670 549253 finetune.py:68] layer 28_o @ epoch 0 new loss 0.0036772785242646933 old loss 0.0037824322935193777 BETTER
I0325 08:51:29.668573 550186 finetune.py:68] layer 29_k @ epoch 4 new loss 0.002148676896467805 old loss 0.0021569086238741875 BETTER
I0325 08:51:37.573098 548361 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0028860303573310375 old loss 0.0029032735619693995 BETTER
I0325 08:51:40.692384 549253 finetune.py:68] layer 28_o @ epoch 1 new loss 0.0036523519083857536 old loss 0.0036772785242646933 BETTER
I0325 08:51:49.097147 550186 finetune.py:45] layer 29_o initial loss 0.003830747678875923
I0325 08:52:11.127734 548361 finetune.py:68] layer 27_o @ epoch 3 new loss 0.0028716546948999166 old loss 0.0028860303573310375 BETTER
I0325 08:52:12.242330 549253 finetune.py:68] layer 28_o @ epoch 2 new loss 0.003632267937064171 old loss 0.0036523519083857536 BETTER
I0325 08:52:19.631860 550186 finetune.py:68] layer 29_o @ epoch 0 new loss 0.0037533538416028023 old loss 0.003830747678875923 BETTER
I0325 08:52:43.915064 549253 finetune.py:68] layer 28_o @ epoch 3 new loss 0.003616000758484006 old loss 0.003632267937064171 BETTER
I0325 08:52:44.540117 548361 finetune.py:68] layer 27_o @ epoch 4 new loss 0.00285934517160058 old loss 0.0028716546948999166 BETTER
I0325 08:52:51.323100 550186 finetune.py:68] layer 29_o @ epoch 1 new loss 0.003731920849531889 old loss 0.0037533538416028023 BETTER
I0325 08:53:10.399707 548361 finetune.py:45] layer 27_up initial loss 0.006997155956923962
I0325 08:53:15.668412 549253 finetune.py:68] layer 28_o @ epoch 4 new loss 0.003601517528295517 old loss 0.003616000758484006 BETTER
I0325 08:53:22.933763 550186 finetune.py:68] layer 29_o @ epoch 2 new loss 0.0037150129210203886 old loss 0.003731920849531889 BETTER
I0325 08:53:40.792466 548361 finetune.py:68] layer 27_up @ epoch 0 new loss 0.006941587198525667 old loss 0.006997155956923962 BETTER
I0325 08:53:41.628923 549253 finetune.py:45] layer 28_up initial loss 0.008398139849305153
I0325 08:53:54.644448 550186 finetune.py:68] layer 29_o @ epoch 3 new loss 0.0037009618245065212 old loss 0.0037150129210203886 BETTER
I0325 08:54:10.464527 549253 finetune.py:68] layer 28_up @ epoch 0 new loss 0.008328444324433804 old loss 0.008398139849305153 BETTER
I0325 08:54:12.534834 548361 finetune.py:68] layer 27_up @ epoch 1 new loss 0.006908300332725048 old loss 0.006941587198525667 BETTER
I0325 08:54:26.694257 550186 finetune.py:68] layer 29_o @ epoch 4 new loss 0.003689378499984741 old loss 0.0037009618245065212 BETTER
I0325 08:54:40.356852 549253 finetune.py:68] layer 28_up @ epoch 1 new loss 0.008287417702376842 old loss 0.008328444324433804 BETTER
I0325 08:54:44.486161 548361 finetune.py:68] layer 27_up @ epoch 2 new loss 0.006882224697619677 old loss 0.006908300332725048 BETTER
I0325 08:54:52.580540 550186 finetune.py:45] layer 29_up initial loss 0.009278169833123684
I0325 08:55:10.347108 549253 finetune.py:68] layer 28_up @ epoch 2 new loss 0.008254936896264553 old loss 0.008287417702376842 BETTER
I0325 08:55:16.630156 548361 finetune.py:68] layer 27_up @ epoch 3 new loss 0.006860004272311926 old loss 0.006882224697619677 BETTER
I0325 08:55:21.617464 550186 finetune.py:68] layer 29_up @ epoch 0 new loss 0.009190591052174568 old loss 0.009278169833123684 BETTER
I0325 08:55:40.218872 549253 finetune.py:68] layer 28_up @ epoch 3 new loss 0.008226943202316761 old loss 0.008254936896264553 BETTER
I0325 08:55:48.957934 548361 finetune.py:68] layer 27_up @ epoch 4 new loss 0.006840576883405447 old loss 0.006860004272311926 BETTER
I0325 08:55:51.707318 550186 finetune.py:68] layer 29_up @ epoch 1 new loss 0.009142889641225338 old loss 0.009190591052174568 BETTER
I0325 08:56:10.210914 549253 finetune.py:68] layer 28_up @ epoch 4 new loss 0.008202137425541878 old loss 0.008226943202316761 BETTER
I0325 08:56:14.782751 548361 finetune.py:45] layer 27_gate initial loss 0.010336913168430328
I0325 08:56:21.538791 550186 finetune.py:68] layer 29_up @ epoch 2 new loss 0.00910533219575882 old loss 0.009142889641225338 BETTER
I0325 08:56:36.294841 549253 finetune.py:45] layer 28_gate initial loss 0.012329562567174435
I0325 08:56:43.602424 548361 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.010307081043720245 old loss 0.010336913168430328 BETTER
I0325 08:56:51.669641 550186 finetune.py:68] layer 29_up @ epoch 3 new loss 0.00907272007316351 old loss 0.00910533219575882 BETTER
I0325 08:57:03.830020 549253 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.012293345294892788 old loss 0.012329562567174435 BETTER
I0325 08:57:13.498743 548361 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.010284507647156715 old loss 0.010307081043720245 BETTER
I0325 08:57:22.013046 550186 finetune.py:68] layer 29_up @ epoch 4 new loss 0.00904364325106144 old loss 0.00907272007316351 BETTER
I0325 08:57:32.359714 549253 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.012265823781490326 old loss 0.012293345294892788 BETTER
I0325 08:57:43.548866 548361 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.010263990610837936 old loss 0.010284507647156715 BETTER
I0325 08:57:48.625018 550186 finetune.py:45] layer 29_gate initial loss 0.013891860842704773
I0325 08:58:00.833692 549253 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.012240665033459663 old loss 0.012265823781490326 BETTER
I0325 08:58:14.165663 548361 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.010244909673929214 old loss 0.010263990610837936 BETTER
I0325 08:58:16.754011 550186 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.013847600668668747 old loss 0.013891860842704773 BETTER
I0325 08:58:28.946261 549253 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.012218266725540161 old loss 0.012240665033459663 BETTER
I0325 08:58:44.513372 548361 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.010227473452687263 old loss 0.010244909673929214 BETTER
I0325 08:58:45.045838 550186 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.013813842087984085 old loss 0.013847600668668747 BETTER
I0325 08:58:57.016986 549253 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.012196858413517475 old loss 0.012218266725540161 BETTER
I0325 08:59:13.504488 550186 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.013784622773528099 old loss 0.013813842087984085 BETTER
I0325 08:59:28.167110 548361 finetune.py:45] layer 27_down initial loss 0.015674183145165443
I0325 08:59:42.883296 549253 finetune.py:45] layer 28_down initial loss 0.018539663404226303
I0325 08:59:43.414836 550186 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.013757830485701561 old loss 0.013784622773528099 BETTER
I0325 08:59:55.413399 548361 finetune.py:68] layer 27_down @ epoch 0 new loss 0.015668319538235664 old loss 0.015674183145165443 BETTER
I0325 09:00:08.350023 549253 finetune.py:68] layer 28_down @ epoch 0 new loss 0.018532801419496536 old loss 0.018539663404226303 BETTER
I0325 09:00:11.976447 550186 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.013733300380408764 old loss 0.013757830485701561 BETTER
I0325 09:00:23.602260 548361 finetune.py:68] layer 27_down @ epoch 1 new loss 0.015662893652915955 old loss 0.015668319538235664 BETTER
I0325 09:00:35.071985 549253 finetune.py:68] layer 28_down @ epoch 1 new loss 0.018526503816246986 old loss 0.018532801419496536 BETTER
I0325 09:00:52.623095 548361 finetune.py:68] layer 27_down @ epoch 2 new loss 0.015657734125852585 old loss 0.015662893652915955 BETTER
I0325 09:00:56.787360 550186 finetune.py:45] layer 29_down initial loss 0.021198326721787453
I0325 09:01:02.637159 549253 finetune.py:68] layer 28_down @ epoch 2 new loss 0.018520737066864967 old loss 0.018526503816246986 BETTER
I0325 09:01:22.142246 548361 finetune.py:68] layer 27_down @ epoch 3 new loss 0.01565294712781906 old loss 0.015657734125852585 BETTER
I0325 09:01:23.469402 550186 finetune.py:68] layer 29_down @ epoch 0 new loss 0.02119089663028717 old loss 0.021198326721787453 BETTER
I0325 09:01:30.220399 549253 finetune.py:68] layer 28_down @ epoch 3 new loss 0.018515227362513542 old loss 0.018520737066864967 BETTER
I0325 09:01:51.469938 550186 finetune.py:68] layer 29_down @ epoch 1 new loss 0.021184183657169342 old loss 0.02119089663028717 BETTER
I0325 09:01:51.492414 548361 finetune.py:68] layer 27_down @ epoch 4 new loss 0.015648379921913147 old loss 0.01565294712781906 BETTER
27_v proxy err 0.0459880605340004 tr(WHW.T) 6602.53369140625
bpp_loss 2.3205439944576938
27_q proxy err 0.011114556342363358 tr(WHW.T) 28254.478515625
bpp_loss 2.500387331470847
27_k proxy err 0.008069850504398346 tr(WHW.T) 39027.1796875
bpp_loss 2.5169991615985055
27_o proxy err 0.04099098592996597 tr(WHW.T) 7416.25634765625
bpp_loss 2.3218498055648524
27_up proxy err 0.03985258936882019 tr(WHW.T) 21966.357421875
bpp_loss 2.2754880073805186
27_gate proxy err 0.025086112320423126 tr(WHW.T) 35536.0
bpp_loss 2.357179617292659
27_down proxy err 0.05081256106495857 tr(WHW.T) 15431.2119140625
bpp_loss 2.2632223491037133
I0325 09:01:58.496688 549253 finetune.py:68] layer 28_down @ epoch 4 new loss 0.01850994862616062 old loss 0.018515227362513542 BETTER
28_v proxy err 0.04243331775069237 tr(WHW.T) 7146.4267578125
bpp_loss 2.3572321181709412
28_q proxy err 0.011347992345690727 tr(WHW.T) 27109.1171875
bpp_loss 2.4549294641183224
28_k proxy err 0.008259358815848827 tr(WHW.T) 37402.05859375
bpp_loss 2.4732323834032286
28_o proxy err 0.03336281701922417 tr(WHW.T) 9042.2919921875
bpp_loss 2.3619754026294686
28_up proxy err 0.03289751335978508 tr(WHW.T) 26337.390625
bpp_loss 2.2872760711714277
28_gate proxy err 0.02388118952512741 tr(WHW.T) 36793.73046875
bpp_loss 2.349491265580751
28_down proxy err 0.046170491725206375 tr(WHW.T) 15302.263671875
bpp_loss 2.2677052270534426
I0325 09:02:19.367563 550186 finetune.py:68] layer 29_down @ epoch 2 new loss 0.021178003400564194 old loss 0.021184183657169342 BETTER
I0325 09:02:46.260892 550186 finetune.py:68] layer 29_down @ epoch 3 new loss 0.021172218024730682 old loss 0.021178003400564194 BETTER
I0325 09:03:06.135348 382969 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 63.1915500164032s
I0325 09:03:09.736211 565944 config.py:54] PyTorch version 2.6.0 available.
W0325 09:03:10.064679 565944 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 09:03:11.039122 565944 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 09:03:11.044491 382969 quantize_finetune_llama.py:209] layer 31 gpu 1
I0325 09:03:11.058334 565944 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 09:03:13.070978 550186 finetune.py:68] layer 29_down @ epoch 4 new loss 0.021166834980249405 old loss 0.021172218024730682 BETTER
29_v proxy err 0.04409026727080345 tr(WHW.T) 6751.8232421875
bpp_loss 2.361846055602655
29_q proxy err 0.010994759388267994 tr(WHW.T) 27142.4296875
bpp_loss 2.411265602160711
29_k proxy err 0.007540714927017689 tr(WHW.T) 39670.93359375
bpp_loss 2.427734491415322
29_o proxy err 0.029887568205595016 tr(WHW.T) 10747.6728515625
bpp_loss 2.3794396288867574
29_up proxy err 0.025888243690133095 tr(WHW.T) 33089.27734375
bpp_loss 2.297428417214474
29_gate proxy err 0.02164839580655098 tr(WHW.T) 40045.7578125
bpp_loss 2.3494181412943576
29_down proxy err 0.04185212031006813 tr(WHW.T) 15041.8427734375
bpp_loss 2.267596501521333
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 09:03:29.249331 565944 finetune.py:45] layer 30_v initial loss 0.003186429152265191
W0325 09:03:29.249642 565944 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 09:04:02.321018 565944 finetune.py:68] layer 30_v @ epoch 0 new loss 0.0016215131618082523 old loss 0.003186429152265191 BETTER
I0325 09:04:17.287307 382969 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 59.91464018821716s
I0325 09:04:20.898020 566899 config.py:54] PyTorch version 2.6.0 available.
W0325 09:04:21.193582 566899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 09:04:22.123927 566899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 09:04:22.141061 566899 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 09:04:36.800709 565944 finetune.py:68] layer 30_v @ epoch 1 new loss 0.001557373907417059 old loss 0.0016215131618082523 BETTER
I0325 09:04:40.278275 566899 finetune.py:45] layer 31_v initial loss 0.003974133636802435
W0325 09:04:40.278662 566899 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 09:05:11.675723 566899 finetune.py:68] layer 31_v @ epoch 0 new loss 0.002403214108198881 old loss 0.003974133636802435 BETTER
I0325 09:05:11.688987 565944 finetune.py:68] layer 30_v @ epoch 2 new loss 0.0015253928722813725 old loss 0.001557373907417059 BETTER
I0325 09:05:43.989147 566899 finetune.py:68] layer 31_v @ epoch 1 new loss 0.0023014438338577747 old loss 0.002403214108198881 BETTER
I0325 09:05:46.726530 565944 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0015044613974168897 old loss 0.0015253928722813725 BETTER
I0325 09:06:16.679615 566899 finetune.py:68] layer 31_v @ epoch 2 new loss 0.002235767198726535 old loss 0.0023014438338577747 BETTER
I0325 09:06:21.804059 565944 finetune.py:68] layer 30_v @ epoch 4 new loss 0.0014883980620652437 old loss 0.0015044613974168897 BETTER
I0325 09:06:40.490150 565944 finetune.py:45] layer 30_q initial loss 0.002050509210675955
I0325 09:06:49.450667 566899 finetune.py:68] layer 31_v @ epoch 3 new loss 0.0021916700061410666 old loss 0.002235767198726535 BETTER
I0325 09:07:13.604152 565944 finetune.py:68] layer 30_q @ epoch 0 new loss 0.0019343628082424402 old loss 0.002050509210675955 BETTER
I0325 09:07:22.561758 566899 finetune.py:68] layer 31_v @ epoch 4 new loss 0.0021587980445474386 old loss 0.0021916700061410666 BETTER
I0325 09:07:41.290402 566899 finetune.py:45] layer 31_q initial loss 0.00415244372561574
I0325 09:07:47.974347 565944 finetune.py:68] layer 30_q @ epoch 1 new loss 0.001902810879983008 old loss 0.0019343628082424402 BETTER
I0325 09:08:12.804817 566899 finetune.py:68] layer 31_q @ epoch 0 new loss 0.0038430141285061836 old loss 0.00415244372561574 BETTER
I0325 09:08:22.635943 565944 finetune.py:68] layer 30_q @ epoch 2 new loss 0.0018811392365023494 old loss 0.001902810879983008 BETTER
I0325 09:08:44.975410 566899 finetune.py:68] layer 31_q @ epoch 1 new loss 0.003739147214218974 old loss 0.0038430141285061836 BETTER
I0325 09:08:57.441738 565944 finetune.py:68] layer 30_q @ epoch 3 new loss 0.0018636231543496251 old loss 0.0018811392365023494 BETTER
I0325 09:09:17.307096 566899 finetune.py:68] layer 31_q @ epoch 2 new loss 0.0036664658691734076 old loss 0.003739147214218974 BETTER
I0325 09:09:32.356954 565944 finetune.py:68] layer 30_q @ epoch 4 new loss 0.001848577638156712 old loss 0.0018636231543496251 BETTER
I0325 09:09:50.074386 566899 finetune.py:68] layer 31_q @ epoch 3 new loss 0.003599110059440136 old loss 0.0036664658691734076 BETTER
I0325 09:09:51.915962 565944 finetune.py:45] layer 30_k initial loss 0.0023552426137030125
I0325 09:10:23.106516 566899 finetune.py:68] layer 31_q @ epoch 4 new loss 0.003549583489075303 old loss 0.003599110059440136 BETTER
I0325 09:10:24.646725 565944 finetune.py:68] layer 30_k @ epoch 0 new loss 0.002324470551684499 old loss 0.0023552426137030125 BETTER
I0325 09:10:42.435345 566899 finetune.py:45] layer 31_k initial loss 0.004189228639006615
I0325 09:10:58.622195 565944 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0023093437775969505 old loss 0.002324470551684499 BETTER
I0325 09:11:13.809146 566899 finetune.py:68] layer 31_k @ epoch 0 new loss 0.004054826684296131 old loss 0.004189228639006615 BETTER
I0325 09:11:32.613958 565944 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0022964212112128735 old loss 0.0023093437775969505 BETTER
I0325 09:11:46.020343 566899 finetune.py:68] layer 31_k @ epoch 1 new loss 0.004004001617431641 old loss 0.004054826684296131 BETTER
I0325 09:12:06.423650 565944 finetune.py:68] layer 30_k @ epoch 3 new loss 0.0022854877170175314 old loss 0.0022964212112128735 BETTER
I0325 09:12:18.070044 566899 finetune.py:68] layer 31_k @ epoch 2 new loss 0.0039681680500507355 old loss 0.004004001617431641 BETTER
I0325 09:12:40.549873 565944 finetune.py:68] layer 30_k @ epoch 4 new loss 0.002275786129757762 old loss 0.0022854877170175314 BETTER
I0325 09:12:50.179579 566899 finetune.py:68] layer 31_k @ epoch 3 new loss 0.003933381754904985 old loss 0.0039681680500507355 BETTER
I0325 09:12:59.215528 565944 finetune.py:45] layer 30_o initial loss 0.004263960290700197
I0325 09:13:22.230796 566899 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0039094896055758 old loss 0.003933381754904985 BETTER
I0325 09:13:31.283427 565944 finetune.py:68] layer 30_o @ epoch 0 new loss 0.004124890547245741 old loss 0.004263960290700197 BETTER
I0325 09:13:41.205591 566899 finetune.py:45] layer 31_o initial loss 0.006611944176256657
I0325 09:14:04.671619 565944 finetune.py:68] layer 30_o @ epoch 1 new loss 0.004080300219357014 old loss 0.004124890547245741 BETTER
I0325 09:14:11.714256 566899 finetune.py:68] layer 31_o @ epoch 0 new loss 0.006173793692141771 old loss 0.006611944176256657 BETTER
I0325 09:14:38.161500 565944 finetune.py:68] layer 30_o @ epoch 2 new loss 0.004048123024404049 old loss 0.004080300219357014 BETTER
I0325 09:14:43.184623 566899 finetune.py:68] layer 31_o @ epoch 1 new loss 0.006047699134796858 old loss 0.006173793692141771 BETTER
I0325 09:15:11.584197 565944 finetune.py:68] layer 30_o @ epoch 3 new loss 0.004021999891847372 old loss 0.004048123024404049 BETTER
I0325 09:15:14.677253 566899 finetune.py:68] layer 31_o @ epoch 2 new loss 0.005968198180198669 old loss 0.006047699134796858 BETTER
I0325 09:15:44.928565 565944 finetune.py:68] layer 30_o @ epoch 4 new loss 0.004000057000666857 old loss 0.004021999891847372 BETTER
I0325 09:15:46.165572 566899 finetune.py:68] layer 31_o @ epoch 3 new loss 0.005908925086259842 old loss 0.005968198180198669 BETTER
I0325 09:16:10.173788 565944 finetune.py:45] layer 30_up initial loss 0.012484521605074406
I0325 09:16:17.676778 566899 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0058656781911849976 old loss 0.005908925086259842 BETTER
I0325 09:16:40.587673 565944 finetune.py:68] layer 30_up @ epoch 0 new loss 0.012308035045862198 old loss 0.012484521605074406 BETTER
I0325 09:16:43.380542 566899 finetune.py:45] layer 31_up initial loss 0.025733819231390953
I0325 09:17:11.879398 565944 finetune.py:68] layer 30_up @ epoch 1 new loss 0.012201541103422642 old loss 0.012308035045862198 BETTER
I0325 09:17:12.197220 566899 finetune.py:68] layer 31_up @ epoch 0 new loss 0.02484285831451416 old loss 0.025733819231390953 BETTER
I0325 09:17:41.729096 566899 finetune.py:68] layer 31_up @ epoch 1 new loss 0.024388190358877182 old loss 0.02484285831451416 BETTER
I0325 09:17:43.231156 565944 finetune.py:68] layer 30_up @ epoch 2 new loss 0.012115657329559326 old loss 0.012201541103422642 BETTER
I0325 09:18:11.320271 566899 finetune.py:68] layer 31_up @ epoch 2 new loss 0.02401822805404663 old loss 0.024388190358877182 BETTER
I0325 09:18:14.667773 565944 finetune.py:68] layer 30_up @ epoch 3 new loss 0.012042791582643986 old loss 0.012115657329559326 BETTER
I0325 09:18:41.091606 566899 finetune.py:68] layer 31_up @ epoch 3 new loss 0.023685822263360023 old loss 0.02401822805404663 BETTER
I0325 09:18:46.142621 565944 finetune.py:68] layer 30_up @ epoch 4 new loss 0.011976304464042187 old loss 0.012042791582643986 BETTER
I0325 09:19:11.042206 566899 finetune.py:68] layer 31_up @ epoch 4 new loss 0.023389393463730812 old loss 0.023685822263360023 BETTER
I0325 09:19:12.037543 565944 finetune.py:45] layer 30_gate initial loss 0.017749082297086716
I0325 09:19:36.406212 566899 finetune.py:45] layer 31_gate initial loss 0.03269480913877487
I0325 09:19:40.553193 565944 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.017657332122325897 old loss 0.017749082297086716 BETTER
I0325 09:20:03.863553 566899 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.03227489814162254 old loss 0.03269480913877487 BETTER
I0325 09:20:10.264935 565944 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.017597679048776627 old loss 0.017657332122325897 BETTER
I0325 09:20:32.016283 566899 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.032040711492300034 old loss 0.03227489814162254 BETTER
I0325 09:20:39.894988 565944 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.017543941736221313 old loss 0.017597679048776627 BETTER
I0325 09:21:00.168368 566899 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.03185179457068443 old loss 0.032040711492300034 BETTER
I0325 09:21:09.569330 565944 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.01749710738658905 old loss 0.017543941736221313 BETTER
I0325 09:21:28.220897 566899 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.03169232979416847 old loss 0.03185179457068443 BETTER
I0325 09:21:39.373633 565944 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.017452750355005264 old loss 0.01749710738658905 BETTER
I0325 09:21:56.491038 566899 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.03154540807008743 old loss 0.03169232979416847 BETTER
I0325 09:22:20.752152 565944 finetune.py:45] layer 30_down initial loss 0.027698645368218422
I0325 09:22:38.353892 566899 finetune.py:45] layer 31_down initial loss 0.049574971199035645
I0325 09:22:47.172855 565944 finetune.py:68] layer 30_down @ epoch 0 new loss 0.027689386159181595 old loss 0.027698645368218422 BETTER
I0325 09:23:03.746905 566899 finetune.py:68] layer 31_down @ epoch 0 new loss 0.04955269396305084 old loss 0.049574971199035645 BETTER
I0325 09:23:15.004812 565944 finetune.py:68] layer 30_down @ epoch 1 new loss 0.02768171951174736 old loss 0.027689386159181595 BETTER
I0325 09:23:30.059686 566899 finetune.py:68] layer 31_down @ epoch 1 new loss 0.04953594505786896 old loss 0.04955269396305084 BETTER
I0325 09:23:43.235612 565944 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0276752021163702 old loss 0.02768171951174736 BETTER
I0325 09:23:56.713684 566899 finetune.py:68] layer 31_down @ epoch 2 new loss 0.04952200874686241 old loss 0.04953594505786896 BETTER
I0325 09:24:11.654621 565944 finetune.py:68] layer 30_down @ epoch 3 new loss 0.02766915038228035 old loss 0.0276752021163702 BETTER
I0325 09:24:23.525070 566899 finetune.py:68] layer 31_down @ epoch 3 new loss 0.049510419368743896 old loss 0.04952200874686241 BETTER
I0325 09:24:40.047264 565944 finetune.py:68] layer 30_down @ epoch 4 new loss 0.027663936838507652 old loss 0.02766915038228035 BETTER
30_v proxy err 0.03739235922694206 tr(WHW.T) 8280.2744140625
bpp_loss 2.397472902695881
30_q proxy err 0.010800495743751526 tr(WHW.T) 28667.59375
bpp_loss 2.4315197929972783
30_k proxy err 0.008053860627114773 tr(WHW.T) 38624.9375
bpp_loss 2.4551297979196534
30_o proxy err 0.02813015878200531 tr(WHW.T) 10321.751953125
bpp_loss 2.407910777197685
30_up proxy err 0.015351297333836555 tr(WHW.T) 54053.8984375
bpp_loss 2.307976001137218
30_gate proxy err 0.01420618873089552 tr(WHW.T) 59230.64453125
bpp_loss 2.3664866543353296
30_down proxy err 0.01276053860783577 tr(WHW.T) 26155.958984375
bpp_loss 2.2518538880855017
I0325 09:24:50.377145 566899 finetune.py:68] layer 31_down @ epoch 4 new loss 0.0494999960064888 old loss 0.049510419368743896 BETTER
31_v proxy err 0.04210447520017624 tr(WHW.T) 6800.76904296875
bpp_loss 2.2764414951961953
31_q proxy err 0.008013848215341568 tr(WHW.T) 36799.50390625
bpp_loss 2.453278733009938
31_k proxy err 0.005404103081673384 tr(WHW.T) 55090.30859375
bpp_loss 2.4979301988496445
31_o proxy err 0.01714438758790493 tr(WHW.T) 13247.7724609375
bpp_loss 2.2749268349725753
31_up proxy err 0.008128597401082516 tr(WHW.T) 96200.953125
bpp_loss 2.3686035617233014
31_gate proxy err 0.008137792348861694 tr(WHW.T) 97655.390625
bpp_loss 2.4384685631754786
31_down proxy err 0.00457144808024168 tr(WHW.T) 37450.58203125
bpp_loss 2.2514277519225034
I0325 09:25:13.954770 582237 config.py:54] PyTorch version 2.6.0 available.
W0325 09:25:14.353808 582237 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 09:25:14.601897 582237 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  3.76it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  4.82it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  5.45it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  5.47it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  5.63it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.99it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.53it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  5.92it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  6.02it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  5.45it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  6.53it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  7.35it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.00it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.03it/s]
I0325 09:25:17.466168 582237 hfize_llama.py:153] loaded layer 0
I0325 09:25:18.119473 582237 hfize_llama.py:153] loaded layer 1
I0325 09:25:18.773022 582237 hfize_llama.py:153] loaded layer 2
I0325 09:25:19.428021 582237 hfize_llama.py:153] loaded layer 3
I0325 09:25:20.101595 582237 hfize_llama.py:153] loaded layer 4
I0325 09:25:20.749103 582237 hfize_llama.py:153] loaded layer 5
I0325 09:25:21.427712 582237 hfize_llama.py:153] loaded layer 6
I0325 09:25:22.126165 582237 hfize_llama.py:153] loaded layer 7
I0325 09:25:22.778354 582237 hfize_llama.py:153] loaded layer 8
I0325 09:25:23.438989 582237 hfize_llama.py:153] loaded layer 9
I0325 09:25:24.079897 582237 hfize_llama.py:153] loaded layer 10
I0325 09:25:24.706934 582237 hfize_llama.py:153] loaded layer 11
I0325 09:25:25.412409 582237 hfize_llama.py:153] loaded layer 12
I0325 09:25:26.124145 582237 hfize_llama.py:153] loaded layer 13
I0325 09:25:26.781903 582237 hfize_llama.py:153] loaded layer 14
I0325 09:25:27.372958 582237 hfize_llama.py:153] loaded layer 15
I0325 09:25:27.966307 582237 hfize_llama.py:153] loaded layer 16
I0325 09:25:28.601398 582237 hfize_llama.py:153] loaded layer 17
I0325 09:25:29.205250 582237 hfize_llama.py:153] loaded layer 18
I0325 09:25:29.788891 582237 hfize_llama.py:153] loaded layer 19
I0325 09:25:30.393236 582237 hfize_llama.py:153] loaded layer 20
I0325 09:25:30.936872 582237 hfize_llama.py:153] loaded layer 21
I0325 09:25:31.491593 582237 hfize_llama.py:153] loaded layer 22
I0325 09:25:32.030526 582237 hfize_llama.py:153] loaded layer 23
I0325 09:25:32.547881 582237 hfize_llama.py:153] loaded layer 24
I0325 09:25:33.061915 582237 hfize_llama.py:153] loaded layer 25
I0325 09:25:33.597512 582237 hfize_llama.py:153] loaded layer 26
I0325 09:25:34.148176 582237 hfize_llama.py:153] loaded layer 27
I0325 09:25:34.717681 582237 hfize_llama.py:153] loaded layer 28
I0325 09:25:35.269814 582237 hfize_llama.py:153] loaded layer 29
I0325 09:25:35.834691 582237 hfize_llama.py:153] loaded layer 30
I0325 09:25:36.360877 582237 hfize_llama.py:153] loaded layer 31
I0325 09:25:36.360992 582237 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:06,  1.29s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.05s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:02,  1.04it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:01,  1.02it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.02it/s]
I0325 09:26:17.669452 582237 hfize_llama.py:167] successfully loaded hfized model
W0325 09:26:21.771928 583312 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

Traceback (most recent call last):
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 124, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 71, in main
    model, model_str = model_from_hf_path(
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 49, in model_from_hf_path
    model = maybe_wrap(use_cuda_graph)(model_cls).from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4057, in from_pretrained
    one_state_dict = load_state_dict(resolved_archive_file[0], weights_only=weights_only)
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 504, in load_state_dict
    with safe_open(checkpoint_file, framework="pt") as f:
FileNotFoundError: No such file or directory: "./hf/ft_ql_ldlq/meta-llama--Llama-2-7b-hf/lmbda30/model-00001-of-00006.safetensors"
I0325 14:02:50.972584 792811 config.py:54] PyTorch version 2.6.0 available.
W0325 14:02:51.286189 792811 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:02:51.528367 792811 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.88it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.30it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.81it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.99it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.11it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.07it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.03it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.77it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.07it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.04it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.26it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.43it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.17it/s]
I0325 14:02:53.796196 792811 hfize_llama.py:153] loaded layer 0
I0325 14:02:54.404831 792811 hfize_llama.py:153] loaded layer 1
I0325 14:02:55.009175 792811 hfize_llama.py:153] loaded layer 2
I0325 14:02:55.642726 792811 hfize_llama.py:153] loaded layer 3
I0325 14:02:56.213987 792811 hfize_llama.py:153] loaded layer 4
I0325 14:02:56.857555 792811 hfize_llama.py:153] loaded layer 5
I0325 14:02:57.460062 792811 hfize_llama.py:153] loaded layer 6
I0325 14:02:58.062505 792811 hfize_llama.py:153] loaded layer 7
I0325 14:02:58.667885 792811 hfize_llama.py:153] loaded layer 8
I0325 14:02:59.208846 792811 hfize_llama.py:153] loaded layer 9
I0325 14:02:59.843918 792811 hfize_llama.py:153] loaded layer 10
I0325 14:03:00.404728 792811 hfize_llama.py:153] loaded layer 11
I0325 14:03:00.968766 792811 hfize_llama.py:153] loaded layer 12
I0325 14:03:01.564676 792811 hfize_llama.py:153] loaded layer 13
I0325 14:03:02.156307 792811 hfize_llama.py:153] loaded layer 14
I0325 14:03:02.778371 792811 hfize_llama.py:153] loaded layer 15
I0325 14:03:03.387520 792811 hfize_llama.py:153] loaded layer 16
I0325 14:03:04.023742 792811 hfize_llama.py:153] loaded layer 17
I0325 14:03:04.636105 792811 hfize_llama.py:153] loaded layer 18
I0325 14:03:05.312933 792811 hfize_llama.py:153] loaded layer 19
I0325 14:03:05.999004 792811 hfize_llama.py:153] loaded layer 20
I0325 14:03:06.636910 792811 hfize_llama.py:153] loaded layer 21
I0325 14:03:07.254462 792811 hfize_llama.py:153] loaded layer 22
I0325 14:03:07.900387 792811 hfize_llama.py:153] loaded layer 23
I0325 14:03:08.520913 792811 hfize_llama.py:153] loaded layer 24
I0325 14:03:09.191083 792811 hfize_llama.py:153] loaded layer 25
I0325 14:03:09.816992 792811 hfize_llama.py:153] loaded layer 26
I0325 14:03:10.515089 792811 hfize_llama.py:153] loaded layer 27
I0325 14:03:11.154523 792811 hfize_llama.py:153] loaded layer 28
I0325 14:03:11.779627 792811 hfize_llama.py:153] loaded layer 29
I0325 14:03:12.435136 792811 hfize_llama.py:153] loaded layer 30
I0325 14:03:13.077656 792811 hfize_llama.py:153] loaded layer 31
I0325 14:03:13.077832 792811 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:06,  1.37s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.03s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:01,  1.04it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.10it/s]
I0325 14:03:55.127017 792811 hfize_llama.py:167] successfully loaded hfized model
W0325 14:03:59.049854 794051 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
  File "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/ft_ql_ldlq/meta-llama--Llama-2-7b-hf/lmbda30'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 124, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 71, in main
    model, model_str = model_from_hf_path(
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 49, in model_from_hf_path
    model = maybe_wrap(use_cuda_graph)(model_cls).from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3825, in from_pretrained
    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/ft_ql_ldlq/meta-llama--Llama-2-7b-hf/lmbda30'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
I0325 14:07:37.091901 797040 config.py:54] PyTorch version 2.6.0 available.
W0325 14:07:37.406152 797040 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:07:37.648046 797040 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.56it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.69it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.23it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.66it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.54it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.86it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.28it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.51it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.50it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.63it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.58it/s]
I0325 14:07:39.788541 797040 hfize_llama.py:153] loaded layer 0
I0325 14:07:40.409444 797040 hfize_llama.py:153] loaded layer 1
I0325 14:07:41.002100 797040 hfize_llama.py:153] loaded layer 2
I0325 14:07:41.574178 797040 hfize_llama.py:153] loaded layer 3
I0325 14:07:42.138802 797040 hfize_llama.py:153] loaded layer 4
I0325 14:07:42.765812 797040 hfize_llama.py:153] loaded layer 5
I0325 14:07:43.403606 797040 hfize_llama.py:153] loaded layer 6
I0325 14:07:44.025322 797040 hfize_llama.py:153] loaded layer 7
I0325 14:07:44.594084 797040 hfize_llama.py:153] loaded layer 8
I0325 14:07:45.131918 797040 hfize_llama.py:153] loaded layer 9
I0325 14:07:45.732717 797040 hfize_llama.py:153] loaded layer 10
I0325 14:07:46.281093 797040 hfize_llama.py:153] loaded layer 11
I0325 14:07:46.919183 797040 hfize_llama.py:153] loaded layer 12
I0325 14:07:47.624830 797040 hfize_llama.py:153] loaded layer 13
I0325 14:07:48.331667 797040 hfize_llama.py:153] loaded layer 14
I0325 14:07:48.945292 797040 hfize_llama.py:153] loaded layer 15
I0325 14:07:49.570213 797040 hfize_llama.py:153] loaded layer 16
I0325 14:07:50.166923 797040 hfize_llama.py:153] loaded layer 17
I0325 14:07:50.751541 797040 hfize_llama.py:153] loaded layer 18
I0325 14:07:51.380798 797040 hfize_llama.py:153] loaded layer 19
I0325 14:07:51.958974 797040 hfize_llama.py:153] loaded layer 20
I0325 14:07:52.560869 797040 hfize_llama.py:153] loaded layer 21
I0325 14:07:53.126821 797040 hfize_llama.py:153] loaded layer 22
I0325 14:07:53.790054 797040 hfize_llama.py:153] loaded layer 23
I0325 14:07:54.396521 797040 hfize_llama.py:153] loaded layer 24
I0325 14:07:54.982623 797040 hfize_llama.py:153] loaded layer 25
I0325 14:07:55.576247 797040 hfize_llama.py:153] loaded layer 26
I0325 14:07:56.197288 797040 hfize_llama.py:153] loaded layer 27
I0325 14:07:56.785202 797040 hfize_llama.py:153] loaded layer 28
I0325 14:07:57.361228 797040 hfize_llama.py:153] loaded layer 29
I0325 14:07:58.066093 797040 hfize_llama.py:153] loaded layer 30
I0325 14:07:58.661775 797040 hfize_llama.py:153] loaded layer 31
I0325 14:07:58.661939 797040 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:06,  1.33s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.08s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.01s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.06it/s]
I0325 14:08:42.800966 797040 hfize_llama.py:167] successfully loaded hfized model
W0325 14:08:46.529489 798187 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:08:47.333146 798187 modeling.py:812] Device 0 seems unavailable, Proceeding to check subsequent devices.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.92it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.36it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.33it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.21it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.24it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.71it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.80it/s]
I0325 14:08:48.594165 798187 config.py:54] PyTorch version 2.6.0 available.
I0325 14:08:59.490301 798659 config.py:54] PyTorch version 2.6.0 available.
W0325 14:08:59.811580 798659 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:09:00.058196 798659 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.96it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.51it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.95it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.29it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.26it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.26it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.08it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.97it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.19it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.40it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.29it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.22it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.40it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.32it/s]
I0325 14:09:02.255660 798659 hfize_llama.py:153] loaded layer 0
I0325 14:09:02.829009 798659 hfize_llama.py:153] loaded layer 1
I0325 14:09:03.490399 798659 hfize_llama.py:153] loaded layer 2
I0325 14:09:04.096651 798659 hfize_llama.py:153] loaded layer 3
I0325 14:09:04.707621 798659 hfize_llama.py:153] loaded layer 4
I0325 14:09:05.284823 798659 hfize_llama.py:153] loaded layer 5
I0325 14:09:05.871876 798659 hfize_llama.py:153] loaded layer 6
I0325 14:09:06.455940 798659 hfize_llama.py:153] loaded layer 7
I0325 14:09:07.070689 798659 hfize_llama.py:153] loaded layer 8
I0325 14:09:07.573382 798659 hfize_llama.py:153] loaded layer 9
I0325 14:09:08.161210 798659 hfize_llama.py:153] loaded layer 10
I0325 14:09:08.672645 798659 hfize_llama.py:153] loaded layer 11
I0325 14:09:09.184922 798659 hfize_llama.py:153] loaded layer 12
I0325 14:09:09.709690 798659 hfize_llama.py:153] loaded layer 13
I0325 14:09:10.252023 798659 hfize_llama.py:153] loaded layer 14
I0325 14:09:10.889853 798659 hfize_llama.py:153] loaded layer 15
I0325 14:09:11.461688 798659 hfize_llama.py:153] loaded layer 16
I0325 14:09:12.046835 798659 hfize_llama.py:153] loaded layer 17
I0325 14:09:12.620390 798659 hfize_llama.py:153] loaded layer 18
I0325 14:09:13.211251 798659 hfize_llama.py:153] loaded layer 19
I0325 14:09:13.766060 798659 hfize_llama.py:153] loaded layer 20
I0325 14:09:14.342152 798659 hfize_llama.py:153] loaded layer 21
I0325 14:09:14.902595 798659 hfize_llama.py:153] loaded layer 22
I0325 14:09:15.493593 798659 hfize_llama.py:153] loaded layer 23
I0325 14:09:16.071894 798659 hfize_llama.py:153] loaded layer 24
I0325 14:09:16.698303 798659 hfize_llama.py:153] loaded layer 25
I0325 14:09:17.288904 798659 hfize_llama.py:153] loaded layer 26
I0325 14:09:17.898022 798659 hfize_llama.py:153] loaded layer 27
I0325 14:09:18.489960 798659 hfize_llama.py:153] loaded layer 28
I0325 14:09:19.067164 798659 hfize_llama.py:153] loaded layer 29
I0325 14:09:19.710695 798659 hfize_llama.py:153] loaded layer 30
I0325 14:09:20.291301 798659 hfize_llama.py:153] loaded layer 31
I0325 14:09:20.291468 798659 hfize_llama.py:157] saving model...
  0%|          | 0/166 [00:00<?, ?it/s]  0%|          | 0/166 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 124, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 92, in main
    input = input_tok[ii, :].cuda().view(1, -1)
RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

I0325 14:11:23.845227 800874 config.py:54] PyTorch version 2.6.0 available.
W0325 14:11:24.163025 800874 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:11:24.406936 800874 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.56it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.15it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.51it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.42it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.48it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.49it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.22it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.29it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.41it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.59it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.88it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.70it/s]
I0325 14:11:26.586903 800874 hfize_llama.py:153] loaded layer 0
I0325 14:11:27.200737 800874 hfize_llama.py:153] loaded layer 1
I0325 14:11:27.826246 800874 hfize_llama.py:153] loaded layer 2
I0325 14:11:28.483384 800874 hfize_llama.py:153] loaded layer 3
I0325 14:11:29.100126 800874 hfize_llama.py:153] loaded layer 4
I0325 14:11:29.733335 800874 hfize_llama.py:153] loaded layer 5
I0325 14:11:30.345655 800874 hfize_llama.py:153] loaded layer 6
I0325 14:11:30.961900 800874 hfize_llama.py:153] loaded layer 7
I0325 14:11:31.544867 800874 hfize_llama.py:153] loaded layer 8
I0325 14:11:32.083981 800874 hfize_llama.py:153] loaded layer 9
I0325 14:11:32.650850 800874 hfize_llama.py:153] loaded layer 10
I0325 14:11:33.166097 800874 hfize_llama.py:153] loaded layer 11
I0325 14:11:33.742842 800874 hfize_llama.py:153] loaded layer 12
I0325 14:11:34.319894 800874 hfize_llama.py:153] loaded layer 13
I0325 14:11:34.907834 800874 hfize_llama.py:153] loaded layer 14
I0325 14:11:35.528709 800874 hfize_llama.py:153] loaded layer 15
I0325 14:11:36.148875 800874 hfize_llama.py:153] loaded layer 16
I0325 14:11:36.750055 800874 hfize_llama.py:153] loaded layer 17
I0325 14:11:37.361011 800874 hfize_llama.py:153] loaded layer 18
I0325 14:11:38.007077 800874 hfize_llama.py:153] loaded layer 19
I0325 14:11:38.603483 800874 hfize_llama.py:153] loaded layer 20
I0325 14:11:39.198575 800874 hfize_llama.py:153] loaded layer 21
I0325 14:11:39.772585 800874 hfize_llama.py:153] loaded layer 22
I0325 14:11:40.355222 800874 hfize_llama.py:153] loaded layer 23
I0325 14:11:40.952234 800874 hfize_llama.py:153] loaded layer 24
I0325 14:11:41.544971 800874 hfize_llama.py:153] loaded layer 25
I0325 14:11:42.144886 800874 hfize_llama.py:153] loaded layer 26
I0325 14:11:42.778041 800874 hfize_llama.py:153] loaded layer 27
I0325 14:11:43.439890 800874 hfize_llama.py:153] loaded layer 28
I0325 14:11:44.056982 800874 hfize_llama.py:153] loaded layer 29
I0325 14:11:44.704633 800874 hfize_llama.py:153] loaded layer 30
I0325 14:11:45.314332 800874 hfize_llama.py:153] loaded layer 31
I0325 14:11:45.314466 800874 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:06,  1.22s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.14s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.03s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:01,  1.02it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.08it/s]
I0325 14:12:43.892102 800874 hfize_llama.py:167] successfully loaded hfized model
W0325 14:12:48.096708 802316 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 14:12:48.754483 802316 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.18s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.15s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.11s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.03s/it]
I0325 14:12:55.421157 802316 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/166 [00:00<?, ?it/s]avg_loss = 1.5522973537445068:   0%|          | 0/166 [00:01<?, ?it/s]avg_loss = 1.5522973537445068:   1%|          | 1/166 [00:01<04:35,  1.67s/it]avg_loss = 1.808940052986145:   1%|          | 1/166 [00:02<04:35,  1.67s/it] avg_loss = 1.808940052986145:   1%|          | 2/166 [00:02<03:53,  1.42s/it]avg_loss = 1.9756442705790203:   1%|          | 2/166 [00:04<03:53,  1.42s/it]avg_loss = 1.9756442705790203:   2%|▏         | 3/166 [00:04<03:39,  1.34s/it]avg_loss = 2.0242941975593567:   2%|▏         | 3/166 [00:05<03:39,  1.34s/it]avg_loss = 2.0242941975593567:   2%|▏         | 4/166 [00:05<03:32,  1.31s/it]avg_loss = 1.9516436576843261:   2%|▏         | 4/166 [00:06<03:32,  1.31s/it]avg_loss = 1.9516436576843261:   3%|▎         | 5/166 [00:06<03:28,  1.30s/it]avg_loss = 1.9263410568237305:   3%|▎         | 5/166 [00:07<03:28,  1.30s/it]avg_loss = 1.9263410568237305:   4%|▎         | 6/166 [00:07<03:24,  1.28s/it]avg_loss = 1.8676239252090454:   4%|▎         | 6/166 [00:09<03:24,  1.28s/it]avg_loss = 1.8676239252090454:   4%|▍         | 7/166 [00:09<03:22,  1.27s/it]avg_loss = 1.810585379600525:   4%|▍         | 7/166 [00:10<03:22,  1.27s/it] avg_loss = 1.810585379600525:   5%|▍         | 8/166 [00:10<03:21,  1.27s/it]avg_loss = 1.8075181245803833:   5%|▍         | 8/166 [00:11<03:21,  1.27s/it]avg_loss = 1.8075181245803833:   5%|▌         | 9/166 [00:11<03:19,  1.27s/it]avg_loss = 1.8161552190780639:   5%|▌         | 9/166 [00:13<03:19,  1.27s/it]avg_loss = 1.8161552190780639:   6%|▌         | 10/166 [00:13<03:19,  1.28s/it]avg_loss = 1.8321055607362227:   6%|▌         | 10/166 [00:14<03:19,  1.28s/it]avg_loss = 1.8321055607362227:   7%|▋         | 11/166 [00:14<03:17,  1.28s/it]avg_loss = 1.8368248045444489:   7%|▋         | 11/166 [00:15<03:17,  1.28s/it]avg_loss = 1.8368248045444489:   7%|▋         | 12/166 [00:15<03:16,  1.28s/it]avg_loss = 1.8306435621701753:   7%|▋         | 12/166 [00:16<03:16,  1.28s/it]avg_loss = 1.8306435621701753:   8%|▊         | 13/166 [00:16<03:15,  1.28s/it]avg_loss = 1.8389571394239153:   8%|▊         | 13/166 [00:18<03:15,  1.28s/it]avg_loss = 1.8389571394239153:   8%|▊         | 14/166 [00:18<03:14,  1.28s/it]avg_loss = 1.8552144368489583:   8%|▊         | 14/166 [00:19<03:14,  1.28s/it]avg_loss = 1.8552144368489583:   9%|▉         | 15/166 [00:19<03:13,  1.28s/it]avg_loss = 1.8728392571210861:   9%|▉         | 15/166 [00:20<03:13,  1.28s/it]avg_loss = 1.8728392571210861:  10%|▉         | 16/166 [00:20<03:12,  1.28s/it]avg_loss = 1.8851120331708122:  10%|▉         | 16/166 [00:22<03:12,  1.28s/it]avg_loss = 1.8851120331708122:  10%|█         | 17/166 [00:22<03:11,  1.28s/it]avg_loss = 1.9003108077579074:  10%|█         | 17/166 [00:23<03:11,  1.28s/it]avg_loss = 1.9003108077579074:  11%|█         | 18/166 [00:23<03:10,  1.29s/it]avg_loss = 1.919099017193443:  11%|█         | 18/166 [00:24<03:10,  1.29s/it] avg_loss = 1.919099017193443:  11%|█▏        | 19/166 [00:24<03:09,  1.29s/it]avg_loss = 1.9253330945968627:  11%|█▏        | 19/166 [00:25<03:09,  1.29s/it]avg_loss = 1.9253330945968627:  12%|█▏        | 20/166 [00:25<03:08,  1.29s/it]avg_loss = 1.927366023971921:  12%|█▏        | 20/166 [00:27<03:08,  1.29s/it] avg_loss = 1.927366023971921:  13%|█▎        | 21/166 [00:27<03:07,  1.29s/it]avg_loss = 1.9171884276650168:  13%|█▎        | 21/166 [00:28<03:07,  1.29s/it]avg_loss = 1.9171884276650168:  13%|█▎        | 22/166 [00:28<03:06,  1.30s/it]avg_loss = 1.90705260504847:  13%|█▎        | 22/166 [00:29<03:06,  1.30s/it]  avg_loss = 1.90705260504847:  14%|█▍        | 23/166 [00:29<03:05,  1.30s/it]avg_loss = 1.9138880620400112:  14%|█▍        | 23/166 [00:31<03:05,  1.30s/it]avg_loss = 1.9138880620400112:  14%|█▍        | 24/166 [00:31<03:05,  1.30s/it]avg_loss = 1.9211267614364624:  14%|█▍        | 24/166 [00:32<03:05,  1.30s/it]avg_loss = 1.9211267614364624:  15%|█▌        | 25/166 [00:32<03:04,  1.31s/it]avg_loss = 1.9245435962310204:  15%|█▌        | 25/166 [00:33<03:04,  1.31s/it]avg_loss = 1.9245435962310204:  16%|█▌        | 26/166 [00:33<03:03,  1.31s/it]avg_loss = 1.9303141214229442:  16%|█▌        | 26/166 [00:35<03:03,  1.31s/it]avg_loss = 1.9303141214229442:  16%|█▋        | 27/166 [00:35<03:02,  1.31s/it]avg_loss = 1.9328116646834783:  16%|█▋        | 27/166 [00:36<03:02,  1.31s/it]avg_loss = 1.9328116646834783:  17%|█▋        | 28/166 [00:36<03:01,  1.31s/it]avg_loss = 1.941521303407077:  17%|█▋        | 28/166 [00:37<03:01,  1.31s/it] avg_loss = 1.941521303407077:  17%|█▋        | 29/166 [00:37<03:00,  1.32s/it]avg_loss = 1.9420724868774415:  17%|█▋        | 29/166 [00:39<03:00,  1.32s/it]avg_loss = 1.9420724868774415:  18%|█▊        | 30/166 [00:39<02:59,  1.32s/it]avg_loss = 1.9558484015926239:  18%|█▊        | 30/166 [00:40<02:59,  1.32s/it]avg_loss = 1.9558484015926239:  19%|█▊        | 31/166 [00:40<02:57,  1.32s/it]avg_loss = 1.96187973767519:  19%|█▊        | 31/166 [00:41<02:57,  1.32s/it]  avg_loss = 1.96187973767519:  19%|█▉        | 32/166 [00:41<02:56,  1.32s/it]avg_loss = 1.9667435776103626:  19%|█▉        | 32/166 [00:42<02:56,  1.32s/it]avg_loss = 1.9667435776103626:  20%|█▉        | 33/166 [00:42<02:56,  1.32s/it]avg_loss = 1.9667992872350357:  20%|█▉        | 33/166 [00:44<02:56,  1.32s/it]avg_loss = 1.9667992872350357:  20%|██        | 34/166 [00:44<02:54,  1.33s/it]avg_loss = 1.9609741142817907:  20%|██        | 34/166 [00:45<02:54,  1.33s/it]avg_loss = 1.9609741142817907:  21%|██        | 35/166 [00:45<02:53,  1.33s/it]avg_loss = 1.9540537198384602:  21%|██        | 35/166 [00:46<02:53,  1.33s/it]avg_loss = 1.9540537198384602:  22%|██▏       | 36/166 [00:46<02:52,  1.33s/it]avg_loss = 1.9459096869906864:  22%|██▏       | 36/166 [00:48<02:52,  1.33s/it]avg_loss = 1.9459096869906864:  22%|██▏       | 37/166 [00:48<02:51,  1.33s/it]avg_loss = 1.9438690292207819:  22%|██▏       | 37/166 [00:49<02:51,  1.33s/it]avg_loss = 1.9438690292207819:  23%|██▎       | 38/166 [00:49<02:50,  1.34s/it]avg_loss = 1.9412500674907978:  23%|██▎       | 38/166 [00:51<02:50,  1.34s/it]avg_loss = 1.9412500674907978:  23%|██▎       | 39/166 [00:51<02:49,  1.34s/it]avg_loss = 1.9441403508186341:  23%|██▎       | 39/166 [00:52<02:49,  1.34s/it]avg_loss = 1.9441403508186341:  24%|██▍       | 40/166 [00:52<02:48,  1.34s/it]avg_loss = 1.943721172286243:  24%|██▍       | 40/166 [00:53<02:48,  1.34s/it] avg_loss = 1.943721172286243:  25%|██▍       | 41/166 [00:53<02:47,  1.34s/it]avg_loss = 1.9309285992667788:  25%|██▍       | 41/166 [00:55<02:47,  1.34s/it]avg_loss = 1.9309285992667788:  25%|██▌       | 42/166 [00:55<02:46,  1.34s/it]avg_loss = 1.9151879299518675:  25%|██▌       | 42/166 [00:56<02:46,  1.34s/it]avg_loss = 1.9151879299518675:  26%|██▌       | 43/166 [00:56<02:45,  1.35s/it]avg_loss = 1.9043947620825334:  26%|██▌       | 43/166 [00:57<02:45,  1.35s/it]avg_loss = 1.9043947620825334:  27%|██▋       | 44/166 [00:57<02:44,  1.35s/it]avg_loss = 1.8902706887986924:  27%|██▋       | 44/166 [00:59<02:44,  1.35s/it]avg_loss = 1.8902706887986924:  27%|██▋       | 45/166 [00:59<03:08,  1.56s/it]avg_loss = 1.880250783070274:  27%|██▋       | 45/166 [01:01<03:08,  1.56s/it] avg_loss = 1.880250783070274:  28%|██▊       | 46/166 [01:01<03:22,  1.69s/it]avg_loss = 1.8726161261822314:  28%|██▊       | 46/166 [01:03<03:22,  1.69s/it]avg_loss = 1.8726161261822314:  28%|██▊       | 47/166 [01:03<03:28,  1.75s/it]avg_loss = 1.8737241923809052:  28%|██▊       | 47/166 [01:06<03:28,  1.75s/it]avg_loss = 1.8737241923809052:  29%|██▉       | 48/166 [01:06<03:50,  1.95s/it]avg_loss = 1.8841435714643828:  29%|██▉       | 48/166 [01:08<03:50,  1.95s/it]avg_loss = 1.8841435714643828:  30%|██▉       | 49/166 [01:08<04:10,  2.14s/it]avg_loss = 1.894236340522766:  30%|██▉       | 49/166 [01:10<04:10,  2.14s/it] avg_loss = 1.894236340522766:  30%|███       | 50/166 [01:10<04:02,  2.09s/it]avg_loss = 1.901501688302732:  30%|███       | 50/166 [01:13<04:02,  2.09s/it]avg_loss = 1.901501688302732:  31%|███       | 51/166 [01:13<04:17,  2.24s/it]avg_loss = 1.907976366006411:  31%|███       | 51/166 [01:15<04:17,  2.24s/it]avg_loss = 1.907976366006411:  31%|███▏      | 52/166 [01:15<04:13,  2.22s/it]avg_loss = 1.9111591060206574:  31%|███▏      | 52/166 [01:17<04:13,  2.22s/it]avg_loss = 1.9111591060206574:  32%|███▏      | 53/166 [01:17<04:14,  2.25s/it]avg_loss = 1.9109272625711229:  32%|███▏      | 53/166 [01:20<04:14,  2.25s/it]avg_loss = 1.9109272625711229:  33%|███▎      | 54/166 [01:20<04:14,  2.28s/it]avg_loss = 1.9127470081502742:  33%|███▎      | 54/166 [01:22<04:14,  2.28s/it]avg_loss = 1.9127470081502742:  33%|███▎      | 55/166 [01:22<04:08,  2.24s/it]avg_loss = 1.9154028104884284:  33%|███▎      | 55/166 [01:24<04:08,  2.24s/it]avg_loss = 1.9154028104884284:  34%|███▎      | 56/166 [01:24<04:15,  2.32s/it]avg_loss = 1.9099906118292558:  34%|███▎      | 56/166 [01:26<04:15,  2.32s/it]avg_loss = 1.9099906118292558:  34%|███▍      | 57/166 [01:26<04:00,  2.21s/it]avg_loss = 1.9134571634489914:  34%|███▍      | 57/166 [01:29<04:00,  2.21s/it]avg_loss = 1.9134571634489914:  35%|███▍      | 58/166 [01:29<04:11,  2.33s/it]avg_loss = 1.9116052793244185:  35%|███▍      | 58/166 [01:31<04:11,  2.33s/it]avg_loss = 1.9116052793244185:  36%|███▌      | 59/166 [01:31<03:59,  2.24s/it]avg_loss = 1.9075462261835734:  36%|███▌      | 59/166 [01:33<03:59,  2.24s/it]avg_loss = 1.9075462261835734:  36%|███▌      | 60/166 [01:33<04:04,  2.31s/it]avg_loss = 1.9029669116755001:  36%|███▌      | 60/166 [01:36<04:04,  2.31s/it]avg_loss = 1.9029669116755001:  37%|███▋      | 61/166 [01:36<04:01,  2.30s/it]avg_loss = 1.8988383643088802:  37%|███▋      | 61/166 [01:38<04:01,  2.30s/it]avg_loss = 1.8988383643088802:  37%|███▋      | 62/166 [01:38<03:56,  2.28s/it]avg_loss = 1.8934776801911612:  37%|███▋      | 62/166 [01:40<03:56,  2.28s/it]avg_loss = 1.8934776801911612:  38%|███▊      | 63/166 [01:40<04:02,  2.36s/it]avg_loss = 1.8894188534468412:  38%|███▊      | 63/166 [01:42<04:02,  2.36s/it]avg_loss = 1.8894188534468412:  39%|███▊      | 64/166 [01:42<03:48,  2.24s/it]avg_loss = 1.882488410289471:  39%|███▊      | 64/166 [01:45<03:48,  2.24s/it] avg_loss = 1.882488410289471:  39%|███▉      | 65/166 [01:45<03:57,  2.35s/it]avg_loss = 1.8751417777755044:  39%|███▉      | 65/166 [01:47<03:57,  2.35s/it]avg_loss = 1.8751417777755044:  40%|███▉      | 66/166 [01:47<03:42,  2.23s/it]avg_loss = 1.8716740803932077:  40%|███▉      | 66/166 [01:49<03:42,  2.23s/it]avg_loss = 1.8716740803932077:  40%|████      | 67/166 [01:49<03:51,  2.34s/it]avg_loss = 1.8706594758173998:  40%|████      | 67/166 [01:52<03:51,  2.34s/it]avg_loss = 1.8706594758173998:  41%|████      | 68/166 [01:52<03:42,  2.27s/it]avg_loss = 1.8729161473288052:  41%|████      | 68/166 [01:54<03:42,  2.27s/it]avg_loss = 1.8729161473288052:  42%|████▏     | 69/166 [01:54<03:45,  2.33s/it]avg_loss = 1.8761532255581448:  42%|████▏     | 69/166 [01:56<03:45,  2.33s/it]avg_loss = 1.8761532255581448:  42%|████▏     | 70/166 [01:56<03:45,  2.34s/it]avg_loss = 1.8798490997771142:  42%|████▏     | 70/166 [01:59<03:45,  2.34s/it]avg_loss = 1.8798490997771142:  43%|████▎     | 71/166 [01:59<03:39,  2.31s/it]avg_loss = 1.8844319300519095:  43%|████▎     | 71/166 [02:01<03:39,  2.31s/it]avg_loss = 1.8844319300519095:  43%|████▎     | 72/166 [02:01<03:46,  2.41s/it]avg_loss = 1.8906020971193707:  43%|████▎     | 72/166 [02:03<03:46,  2.41s/it]avg_loss = 1.8906020971193707:  44%|████▍     | 73/166 [02:03<03:30,  2.27s/it]avg_loss = 1.885148650891072:  44%|████▍     | 73/166 [02:06<03:30,  2.27s/it] avg_loss = 1.885148650891072:  45%|████▍     | 74/166 [02:06<03:37,  2.37s/it]avg_loss = 1.8811509688695272:  45%|████▍     | 74/166 [02:08<03:37,  2.37s/it]avg_loss = 1.8811509688695272:  45%|████▌     | 75/166 [02:08<03:26,  2.27s/it]avg_loss = 1.8803607099934627:  45%|████▌     | 75/166 [02:10<03:26,  2.27s/it]avg_loss = 1.8803607099934627:  46%|████▌     | 76/166 [02:10<03:31,  2.35s/it]avg_loss = 1.877022908879565:  46%|████▌     | 76/166 [02:13<03:31,  2.35s/it] avg_loss = 1.877022908879565:  46%|████▋     | 77/166 [02:13<03:25,  2.30s/it]avg_loss = 1.8736340052042253:  46%|████▋     | 77/166 [02:15<03:25,  2.30s/it]avg_loss = 1.8736340052042253:  47%|████▋     | 78/166 [02:15<03:24,  2.32s/it]avg_loss = 1.8708731947065909:  47%|████▋     | 78/166 [02:17<03:24,  2.32s/it]avg_loss = 1.8708731947065909:  48%|████▊     | 79/166 [02:17<03:27,  2.38s/it]avg_loss = 1.8673906773328781:  48%|████▊     | 79/166 [02:20<03:27,  2.38s/it]avg_loss = 1.8673906773328781:  48%|████▊     | 80/166 [02:20<03:18,  2.31s/it]avg_loss = 1.8588951193256142:  48%|████▊     | 80/166 [02:22<03:18,  2.31s/it]avg_loss = 1.8588951193256142:  49%|████▉     | 81/166 [02:22<03:24,  2.41s/it]avg_loss = 1.8601145657097422:  49%|████▉     | 81/166 [02:24<03:24,  2.41s/it]avg_loss = 1.8601145657097422:  49%|████▉     | 82/166 [02:24<03:10,  2.27s/it]avg_loss = 1.8619357045874538:  49%|████▉     | 82/166 [02:27<03:10,  2.27s/it]avg_loss = 1.8619357045874538:  50%|█████     | 83/166 [02:27<03:17,  2.39s/it]avg_loss = 1.8649318587212336:  50%|█████     | 83/166 [02:29<03:17,  2.39s/it]avg_loss = 1.8649318587212336:  51%|█████     | 84/166 [02:29<03:07,  2.28s/it]avg_loss = 1.8668405729181625:  51%|█████     | 84/166 [02:31<03:07,  2.28s/it]avg_loss = 1.8668405729181625:  51%|█████     | 85/166 [02:31<03:10,  2.35s/it]avg_loss = 1.8653679462366326:  51%|█████     | 85/166 [02:34<03:10,  2.35s/it]avg_loss = 1.8653679462366326:  52%|█████▏    | 86/166 [02:34<03:04,  2.31s/it]avg_loss = 1.865359551605137:  52%|█████▏    | 86/166 [02:36<03:04,  2.31s/it] avg_loss = 1.865359551605137:  52%|█████▏    | 87/166 [02:36<03:03,  2.32s/it]avg_loss = 1.8653496192260222:  52%|█████▏    | 87/166 [02:38<03:03,  2.32s/it]avg_loss = 1.8653496192260222:  53%|█████▎    | 88/166 [02:38<03:04,  2.36s/it]avg_loss = 1.8663276233030168:  53%|█████▎    | 88/166 [02:41<03:04,  2.36s/it]avg_loss = 1.8663276233030168:  54%|█████▎    | 89/166 [02:41<02:56,  2.30s/it]avg_loss = 1.8659332858191595:  54%|█████▎    | 89/166 [02:43<02:56,  2.30s/it]avg_loss = 1.8659332858191595:  54%|█████▍    | 90/166 [02:43<03:02,  2.40s/it]avg_loss = 1.8660189188443697:  54%|█████▍    | 90/166 [02:45<03:02,  2.40s/it]avg_loss = 1.8660189188443697:  55%|█████▍    | 91/166 [02:45<02:49,  2.27s/it]avg_loss = 1.866961648930674:  55%|█████▍    | 91/166 [02:48<02:49,  2.27s/it] avg_loss = 1.866961648930674:  55%|█████▌    | 92/166 [02:48<02:56,  2.39s/it]avg_loss = 1.8710068336097143:  55%|█████▌    | 92/166 [02:50<02:56,  2.39s/it]avg_loss = 1.8710068336097143:  56%|█████▌    | 93/166 [02:50<02:45,  2.27s/it]avg_loss = 1.8695145147912047:  56%|█████▌    | 93/166 [02:52<02:45,  2.27s/it]avg_loss = 1.8695145147912047:  57%|█████▋    | 94/166 [02:52<02:51,  2.38s/it]avg_loss = 1.8684023794374969:  57%|█████▋    | 94/166 [02:55<02:51,  2.38s/it]avg_loss = 1.8684023794374969:  57%|█████▋    | 95/166 [02:55<02:43,  2.31s/it]avg_loss = 1.8677257026235263:  57%|█████▋    | 95/166 [02:57<02:43,  2.31s/it]avg_loss = 1.8677257026235263:  58%|█████▊    | 96/166 [02:57<02:43,  2.34s/it]avg_loss = 1.8676438810899085:  58%|█████▊    | 96/166 [03:00<02:43,  2.34s/it]avg_loss = 1.8676438810899085:  58%|█████▊    | 97/166 [03:00<02:44,  2.38s/it]avg_loss = 1.8658840388667828:  58%|█████▊    | 97/166 [03:02<02:44,  2.38s/it]avg_loss = 1.8658840388667828:  59%|█████▉    | 98/166 [03:02<02:36,  2.31s/it]avg_loss = 1.8633478723391137:  59%|█████▉    | 98/166 [03:04<02:36,  2.31s/it]avg_loss = 1.8633478723391137:  60%|█████▉    | 99/166 [03:04<02:40,  2.40s/it]avg_loss = 1.8610194766521453:  60%|█████▉    | 99/166 [03:06<02:40,  2.40s/it]avg_loss = 1.8610194766521453:  60%|██████    | 100/166 [03:06<02:31,  2.30s/it]avg_loss = 1.8615469247987955:  60%|██████    | 100/166 [03:09<02:31,  2.30s/it]avg_loss = 1.8615469247987955:  61%|██████    | 101/166 [03:09<02:36,  2.41s/it]avg_loss = 1.8623289138663048:  61%|██████    | 101/166 [03:11<02:36,  2.41s/it]avg_loss = 1.8623289138663048:  61%|██████▏   | 102/166 [03:11<02:26,  2.28s/it]avg_loss = 1.8636229593776963:  61%|██████▏   | 102/166 [03:14<02:26,  2.28s/it]avg_loss = 1.8636229593776963:  62%|██████▏   | 103/166 [03:14<02:30,  2.39s/it]avg_loss = 1.8659381981079395:  62%|██████▏   | 103/166 [03:16<02:30,  2.39s/it]avg_loss = 1.8659381981079395:  63%|██████▎   | 104/166 [03:16<02:23,  2.31s/it]avg_loss = 1.8726111116863433:  63%|██████▎   | 104/166 [03:18<02:23,  2.31s/it]avg_loss = 1.8726111116863433:  63%|██████▎   | 105/166 [03:18<02:23,  2.36s/it]avg_loss = 1.8778440075100593:  63%|██████▎   | 105/166 [03:21<02:23,  2.36s/it]avg_loss = 1.8778440075100593:  64%|██████▍   | 106/166 [03:21<02:22,  2.38s/it]avg_loss = 1.881410848314517:  64%|██████▍   | 106/166 [03:23<02:22,  2.38s/it] avg_loss = 1.881410848314517:  64%|██████▍   | 107/166 [03:23<02:16,  2.31s/it]avg_loss = 1.8843800138544153:  64%|██████▍   | 107/166 [03:25<02:16,  2.31s/it]avg_loss = 1.8843800138544153:  65%|██████▌   | 108/166 [03:25<02:20,  2.42s/it]avg_loss = 1.8889722539744247:  65%|██████▌   | 108/166 [03:27<02:20,  2.42s/it]avg_loss = 1.8889722539744247:  66%|██████▌   | 109/166 [03:27<02:10,  2.29s/it]avg_loss = 1.892514105276628:  66%|██████▌   | 109/166 [03:30<02:10,  2.29s/it] avg_loss = 1.892514105276628:  66%|██████▋   | 110/166 [03:30<02:14,  2.41s/it]avg_loss = 1.8936747516597714:  66%|██████▋   | 110/166 [03:32<02:14,  2.41s/it]avg_loss = 1.8936747516597714:  67%|██████▋   | 111/166 [03:32<02:04,  2.26s/it]avg_loss = 1.895020774432591:  67%|██████▋   | 111/166 [03:35<02:04,  2.26s/it] avg_loss = 1.895020774432591:  67%|██████▋   | 112/166 [03:35<02:08,  2.38s/it]avg_loss = 1.8950435345151784:  67%|██████▋   | 112/166 [03:37<02:08,  2.38s/it]avg_loss = 1.8950435345151784:  68%|██████▊   | 113/166 [03:37<02:02,  2.30s/it]avg_loss = 1.8961504457289713:  68%|██████▊   | 113/166 [03:39<02:02,  2.30s/it]avg_loss = 1.8961504457289713:  69%|██████▊   | 114/166 [03:39<02:03,  2.37s/it]avg_loss = 1.8940621376037599:  69%|██████▊   | 114/166 [03:42<02:03,  2.37s/it]avg_loss = 1.8940621376037599:  69%|██████▉   | 115/166 [03:42<02:01,  2.37s/it]avg_loss = 1.893638668389156:  69%|██████▉   | 115/166 [03:44<02:01,  2.37s/it] avg_loss = 1.893638668389156:  70%|██████▉   | 116/166 [03:44<01:56,  2.32s/it]avg_loss = 1.8948788785526895:  70%|██████▉   | 116/166 [03:47<01:56,  2.32s/it]avg_loss = 1.8948788785526895:  70%|███████   | 117/166 [03:47<01:58,  2.42s/it]avg_loss = 1.8953854310310494:  70%|███████   | 117/166 [03:49<01:58,  2.42s/it]avg_loss = 1.8953854310310494:  71%|███████   | 118/166 [03:49<01:49,  2.29s/it]avg_loss = 1.8954593193631213:  71%|███████   | 118/166 [03:51<01:49,  2.29s/it]avg_loss = 1.8954593193631213:  72%|███████▏  | 119/166 [03:51<01:53,  2.41s/it]avg_loss = 1.8962640414635341:  72%|███████▏  | 119/166 [03:53<01:53,  2.41s/it]avg_loss = 1.8962640414635341:  72%|███████▏  | 120/166 [03:53<01:45,  2.30s/it]avg_loss = 1.8962750976735896:  72%|███████▏  | 120/166 [03:56<01:45,  2.30s/it]avg_loss = 1.8962750976735896:  73%|███████▎  | 121/166 [03:56<01:47,  2.39s/it]avg_loss = 1.8974023637224415:  73%|███████▎  | 121/166 [03:58<01:47,  2.39s/it]avg_loss = 1.8974023637224415:  73%|███████▎  | 122/166 [03:58<01:43,  2.35s/it]avg_loss = 1.8978408352146303:  73%|███████▎  | 122/166 [04:01<01:43,  2.35s/it]avg_loss = 1.8978408352146303:  74%|███████▍  | 123/166 [04:01<01:41,  2.37s/it]avg_loss = 1.8966836535161542:  74%|███████▍  | 123/166 [04:03<01:41,  2.37s/it]avg_loss = 1.8966836535161542:  75%|███████▍  | 124/166 [04:03<01:39,  2.37s/it]avg_loss = 1.8952904253005982:  75%|███████▍  | 124/166 [04:05<01:39,  2.37s/it]avg_loss = 1.8952904253005982:  75%|███████▌  | 125/166 [04:05<01:34,  2.30s/it]avg_loss = 1.8936728437741597:  75%|███████▌  | 125/166 [04:08<01:34,  2.30s/it]avg_loss = 1.8936728437741597:  76%|███████▌  | 126/166 [04:08<01:35,  2.39s/it]avg_loss = 1.8918427782734548:  76%|███████▌  | 126/166 [04:10<01:35,  2.39s/it]avg_loss = 1.8918427782734548:  77%|███████▋  | 127/166 [04:10<01:28,  2.27s/it]avg_loss = 1.8906698720529675:  77%|███████▋  | 127/166 [04:12<01:28,  2.27s/it]avg_loss = 1.8906698720529675:  77%|███████▋  | 128/166 [04:12<01:31,  2.40s/it]avg_loss = 1.8896822624428327:  77%|███████▋  | 128/166 [04:14<01:31,  2.40s/it]avg_loss = 1.8896822624428327:  78%|███████▊  | 129/166 [04:14<01:23,  2.26s/it]avg_loss = 1.8895912225429827:  78%|███████▊  | 129/166 [04:17<01:23,  2.26s/it]avg_loss = 1.8895912225429827:  78%|███████▊  | 130/166 [04:17<01:26,  2.39s/it]avg_loss = 1.8905811291614563:  78%|███████▊  | 130/166 [04:19<01:26,  2.39s/it]avg_loss = 1.8905811291614563:  79%|███████▉  | 131/166 [04:19<01:21,  2.33s/it]avg_loss = 1.8912831436504016:  79%|███████▉  | 131/166 [04:22<01:21,  2.33s/it]avg_loss = 1.8912831436504016:  80%|███████▉  | 132/166 [04:22<01:20,  2.37s/it]avg_loss = 1.8922582784093411:  80%|███████▉  | 132/166 [04:24<01:20,  2.37s/it]avg_loss = 1.8922582784093411:  80%|████████  | 133/166 [04:24<01:17,  2.36s/it]avg_loss = 1.8936167350455897:  80%|████████  | 133/166 [04:26<01:17,  2.36s/it]avg_loss = 1.8936167350455897:  81%|████████  | 134/166 [04:26<01:14,  2.33s/it]avg_loss = 1.8913563286816633:  81%|████████  | 134/166 [04:29<01:14,  2.33s/it]avg_loss = 1.8913563286816633:  81%|████████▏ | 135/166 [04:29<01:15,  2.43s/it]avg_loss = 1.8914695285698946:  81%|████████▏ | 135/166 [04:31<01:15,  2.43s/it]avg_loss = 1.8914695285698946:  82%|████████▏ | 136/166 [04:31<01:09,  2.31s/it]avg_loss = 1.8918585472733436:  82%|████████▏ | 136/166 [04:34<01:09,  2.31s/it]avg_loss = 1.8918585472733436:  83%|████████▎ | 137/166 [04:34<01:09,  2.41s/it]avg_loss = 1.892575442790985:  83%|████████▎ | 137/166 [04:36<01:09,  2.41s/it] avg_loss = 1.892575442790985:  83%|████████▎ | 138/166 [04:36<01:03,  2.28s/it]avg_loss = 1.8915905094832826:  83%|████████▎ | 138/166 [04:38<01:03,  2.28s/it]avg_loss = 1.8915905094832826:  84%|████████▎ | 139/166 [04:38<01:04,  2.39s/it]avg_loss = 1.8900289135319845:  84%|████████▎ | 139/166 [04:41<01:04,  2.39s/it]avg_loss = 1.8900289135319845:  84%|████████▍ | 140/166 [04:41<01:01,  2.38s/it]avg_loss = 1.8884573244879432:  84%|████████▍ | 140/166 [04:43<01:01,  2.38s/it]avg_loss = 1.8884573244879432:  85%|████████▍ | 141/166 [04:43<00:59,  2.37s/it]avg_loss = 1.8883483477041756:  85%|████████▍ | 141/166 [04:45<00:59,  2.37s/it]avg_loss = 1.8883483477041756:  86%|████████▌ | 142/166 [04:45<00:58,  2.42s/it]avg_loss = 1.886778461349594:  86%|████████▌ | 142/166 [04:48<00:58,  2.42s/it] avg_loss = 1.886778461349594:  86%|████████▌ | 143/166 [04:48<00:53,  2.32s/it]avg_loss = 1.8881396965848074:  86%|████████▌ | 143/166 [04:50<00:53,  2.32s/it]avg_loss = 1.8881396965848074:  87%|████████▋ | 144/166 [04:50<00:53,  2.42s/it]avg_loss = 1.8871010583022545:  87%|████████▋ | 144/166 [04:52<00:53,  2.42s/it]avg_loss = 1.8871010583022545:  87%|████████▋ | 145/166 [04:52<00:48,  2.29s/it]avg_loss = 1.886833023535062:  87%|████████▋ | 145/166 [04:55<00:48,  2.29s/it] avg_loss = 1.886833023535062:  88%|████████▊ | 146/166 [04:55<00:48,  2.40s/it]avg_loss = 1.8855663002753744:  88%|████████▊ | 146/166 [04:57<00:48,  2.40s/it]avg_loss = 1.8855663002753744:  89%|████████▊ | 147/166 [04:57<00:44,  2.32s/it]avg_loss = 1.8845499045140035:  89%|████████▊ | 147/166 [05:00<00:44,  2.32s/it]avg_loss = 1.8845499045140035:  89%|████████▉ | 148/166 [05:00<00:42,  2.38s/it]avg_loss = 1.8827826768759912:  89%|████████▉ | 148/166 [05:02<00:42,  2.38s/it]avg_loss = 1.8827826768759912:  90%|████████▉ | 149/166 [05:02<00:40,  2.39s/it]avg_loss = 1.8836917956670125:  90%|████████▉ | 149/166 [05:04<00:40,  2.39s/it]avg_loss = 1.8836917956670125:  90%|█████████ | 150/166 [05:04<00:37,  2.34s/it]avg_loss = 1.882760657379959:  90%|█████████ | 150/166 [05:07<00:37,  2.34s/it] avg_loss = 1.882760657379959:  91%|█████████ | 151/166 [05:07<00:36,  2.42s/it]avg_loss = 1.8825605841059434:  91%|█████████ | 151/166 [05:09<00:36,  2.42s/it]avg_loss = 1.8825605841059434:  92%|█████████▏| 152/166 [05:09<00:32,  2.31s/it]avg_loss = 1.8823468498155183:  92%|█████████▏| 152/166 [05:11<00:32,  2.31s/it]avg_loss = 1.8823468498155183:  92%|█████████▏| 153/166 [05:11<00:31,  2.42s/it]avg_loss = 1.8842081868803346:  92%|█████████▏| 153/166 [05:13<00:31,  2.42s/it]avg_loss = 1.8842081868803346:  93%|█████████▎| 154/166 [05:13<00:27,  2.28s/it]avg_loss = 1.883553571085776:  93%|█████████▎| 154/166 [05:16<00:27,  2.28s/it] avg_loss = 1.883553571085776:  93%|█████████▎| 155/166 [05:16<00:26,  2.40s/it]avg_loss = 1.8832993874183068:  93%|█████████▎| 155/166 [05:18<00:26,  2.40s/it]avg_loss = 1.8832993874183068:  94%|█████████▍| 156/166 [05:18<00:23,  2.34s/it]avg_loss = 1.8813117081951942:  94%|█████████▍| 156/166 [05:21<00:23,  2.34s/it]avg_loss = 1.8813117081951942:  95%|█████████▍| 157/166 [05:21<00:21,  2.37s/it]avg_loss = 1.8764466545249843:  95%|█████████▍| 157/166 [05:23<00:21,  2.37s/it]avg_loss = 1.8764466545249843:  95%|█████████▌| 158/166 [05:23<00:18,  2.37s/it]avg_loss = 1.8769716331793826:  95%|█████████▌| 158/166 [05:25<00:18,  2.37s/it]avg_loss = 1.8769716331793826:  96%|█████████▌| 159/166 [05:25<00:16,  2.35s/it]avg_loss = 1.878243663907051:  96%|█████████▌| 159/166 [05:28<00:16,  2.35s/it] avg_loss = 1.878243663907051:  96%|█████████▋| 160/166 [05:28<00:14,  2.44s/it]avg_loss = 1.880500754954652:  96%|█████████▋| 160/166 [05:30<00:14,  2.44s/it]avg_loss = 1.880500754954652:  97%|█████████▋| 161/166 [05:30<00:11,  2.33s/it]avg_loss = 1.881004684501224:  97%|█████████▋| 161/166 [05:33<00:11,  2.33s/it]avg_loss = 1.881004684501224:  98%|█████████▊| 162/166 [05:33<00:09,  2.43s/it]avg_loss = 1.8809873874933443:  98%|█████████▊| 162/166 [05:35<00:09,  2.43s/it]avg_loss = 1.8809873874933443:  98%|█████████▊| 163/166 [05:35<00:06,  2.28s/it]avg_loss = 1.881809403983558:  98%|█████████▊| 163/166 [05:37<00:06,  2.28s/it] avg_loss = 1.881809403983558:  99%|█████████▉| 164/166 [05:37<00:04,  2.41s/it]avg_loss = 1.8822155302221124:  99%|█████████▉| 164/166 [05:40<00:04,  2.41s/it]avg_loss = 1.8822155302221124:  99%|█████████▉| 165/166 [05:40<00:02,  2.34s/it]avg_loss = 1.8841336793210133:  99%|█████████▉| 165/166 [05:42<00:02,  2.34s/it]avg_loss = 1.8841336793210133: 100%|██████████| 166/166 [05:42<00:00,  2.37s/it]avg_loss = 1.8841336793210133: 100%|██████████| 166/166 [05:42<00:00,  2.06s/it]
I0325 14:19:30.360897 802316 eval_ppl.py:107] wikitext2 perplexity: 6.58065128326416
wikitext2 perplexity: 6.581
I0326 07:21:47.516836 1455951 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 07:21:47.517053 1455951 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 07:21:47.517096 1455951 utils.py:162] NumExpr defaulting to 16 threads.
I0326 07:21:47.832599 1455951 config.py:54] PyTorch version 2.6.0 available.
W0326 07:21:48.032023 1455951 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0326 07:21:48.138883 1455951 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.86it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.44it/s]I0326 07:22:37.275469 1456634 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 07:22:37.275679 1456634 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 07:22:37.275720 1456634 utils.py:162] NumExpr defaulting to 16 threads.
I0326 07:22:37.592573 1456634 config.py:54] PyTorch version 2.6.0 available.
W0326 07:22:37.794935 1456634 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0326 07:22:37.904263 1456634 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.96it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.60it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.89it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.02it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.92it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.05it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.91it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.93it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.93it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.94it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.96it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.12it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.02it/s]
I0326 07:22:40.190919 1456634 hfize_llama.py:153] loaded layer 0
I0326 07:22:40.791111 1456634 hfize_llama.py:153] loaded layer 1
I0326 07:22:41.420891 1456634 hfize_llama.py:153] loaded layer 2
I0326 07:22:42.071033 1456634 hfize_llama.py:153] loaded layer 3
I0326 07:22:42.683969 1456634 hfize_llama.py:153] loaded layer 4
I0326 07:22:43.302726 1456634 hfize_llama.py:153] loaded layer 5
I0326 07:22:43.904487 1456634 hfize_llama.py:153] loaded layer 6
I0326 07:22:44.517540 1456634 hfize_llama.py:153] loaded layer 7
I0326 07:22:45.133361 1456634 hfize_llama.py:153] loaded layer 8
I0326 07:22:45.710790 1456634 hfize_llama.py:153] loaded layer 9
I0326 07:22:46.336815 1456634 hfize_llama.py:153] loaded layer 10
I0326 07:22:46.868763 1456634 hfize_llama.py:153] loaded layer 11
I0326 07:22:47.408027 1456634 hfize_llama.py:153] loaded layer 12
I0326 07:22:47.944989 1456634 hfize_llama.py:153] loaded layer 13
I0326 07:22:48.560794 1456634 hfize_llama.py:153] loaded layer 14
I0326 07:22:49.146105 1456634 hfize_llama.py:153] loaded layer 15
I0326 07:22:49.744472 1456634 hfize_llama.py:153] loaded layer 16
I0326 07:22:50.302381 1456634 hfize_llama.py:153] loaded layer 17
I0326 07:22:50.861699 1456634 hfize_llama.py:153] loaded layer 18
I0326 07:22:51.421821 1456634 hfize_llama.py:153] loaded layer 19
I0326 07:22:52.030651 1456634 hfize_llama.py:153] loaded layer 20
I0326 07:22:52.625323 1456634 hfize_llama.py:153] loaded layer 21
I0326 07:22:53.241842 1456634 hfize_llama.py:153] loaded layer 22
I0326 07:22:53.887604 1456634 hfize_llama.py:153] loaded layer 23
I0326 07:22:54.467452 1456634 hfize_llama.py:153] loaded layer 24
I0326 07:22:55.036458 1456634 hfize_llama.py:153] loaded layer 25
I0326 07:22:56.191794 1456634 hfize_llama.py:153] loaded layer 26
I0326 07:22:56.764912 1456634 hfize_llama.py:153] loaded layer 27
I0326 07:22:57.336994 1456634 hfize_llama.py:153] loaded layer 28
I0326 07:22:57.881046 1456634 hfize_llama.py:153] loaded layer 29
I0326 07:22:58.523843 1456634 hfize_llama.py:153] loaded layer 30
I0326 07:22:59.110786 1456634 hfize_llama.py:153] loaded layer 31
I0326 07:22:59.110914 1456634 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.03s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.14it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.21it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.26it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s]
I0326 07:23:37.091140 1456634 hfize_llama.py:167] successfully loaded hfized model
