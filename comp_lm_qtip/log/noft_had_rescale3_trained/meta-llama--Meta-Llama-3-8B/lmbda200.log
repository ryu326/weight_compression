I0403 01:55:14.925689 3260590 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:55:14.925808 3260590 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:55:14.925856 3260590 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:55:15.420300 3260590 config.py:54] PyTorch version 2.6.0 available.
W0403 01:55:15.650678 3260590 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:55:16.480626 3260590 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.08it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  6.14it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  6.69it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.06it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.30it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.41it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.57it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.18it/s]
I0403 01:55:18.069163 3260590 quantize_finetune_llama.py:152] loaded model
I0403 01:55:18.325628 3260590 quantize_finetune_llama.py:190] loaded compression model
I0403 01:55:37.321566 3260590 quantize_finetune_llama.py:194] loaded dataset and devset
I0403 01:55:40.977591 3260590 quantize_finetune_llama.py:214] layer 0 gpu 0
I0403 01:55:43.268735 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 0 in 2.1360013484954834s
Use train scale and shift
tensor(-1.5053e-07, device='cuda:0') tensor(0.0156, device='cuda:0')
tensor(0.0156, device='cuda:0') tensor(-1.5053e-07, device='cuda:0')
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0403 01:55:57.327286 3261529 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:55:57.327383 3261529 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:55:57.327420 3261529 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:55:57.649157 3261529 config.py:54] PyTorch version 2.6.0 available.
W0403 01:55:57.836148 3261529 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:55:58.496627 3261529 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:55:58.500364 3260590 quantize_finetune_llama.py:214] layer 1 gpu 1
I0403 01:56:02.298709 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 1 in 3.5804011821746826s
I0403 01:56:05.847204 3261767 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:56:05.847300 3261767 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:56:05.847340 3261767 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:56:06.177457 3261767 config.py:54] PyTorch version 2.6.0 available.
W0403 01:56:06.377022 3261767 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:56:07.021538 3261767 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:56:07.024992 3260590 quantize_finetune_llama.py:214] layer 2 gpu 0
I0403 01:56:09.203841 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 2 in 1.0437731742858887s
I0403 01:56:12.720514 3262029 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:56:12.720617 3262029 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:56:12.720658 3262029 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:56:13.042383 3262029 config.py:54] PyTorch version 2.6.0 available.
W0403 01:56:13.242526 3262029 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:56:13.825742 3262029 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:56:13.829538 3260590 quantize_finetune_llama.py:214] layer 3 gpu 1
I0403 01:56:15.145999 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 3 in 0.9110803604125977s
I0403 01:56:18.824594 3262308 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:56:18.824685 3262308 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:56:18.824726 3262308 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:56:19.154213 3262308 config.py:54] PyTorch version 2.6.0 available.
W0403 01:56:19.348276 3262308 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:56:19.983364 3262308 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:56:19.986999 3260590 quantize_finetune_llama.py:214] layer 4 gpu 0
I0403 01:56:22.219116 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 4 in 0.9056055545806885s
I0403 01:56:25.851534 3262511 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:56:25.851628 3262511 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:56:25.851667 3262511 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:56:26.187902 3262511 config.py:54] PyTorch version 2.6.0 available.
W0403 01:56:26.396971 3262511 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:56:26.983616 3262511 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:56:26.987387 3260590 quantize_finetune_llama.py:214] layer 5 gpu 1
I0403 01:56:28.352063 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 5 in 0.9226922988891602s
I0403 01:56:32.465007 3262684 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:56:32.465195 3262684 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:56:32.465272 3262684 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:56:32.904147 3262684 config.py:54] PyTorch version 2.6.0 available.
W0403 01:56:33.143728 3262684 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:56:33.844641 3262684 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:56:33.848739 3260590 quantize_finetune_llama.py:214] layer 6 gpu 0
I0403 01:56:37.361104 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 6 in 1.081695795059204s
I0403 01:56:41.177019 3262915 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:56:41.177177 3262915 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:56:41.177256 3262915 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:56:41.586640 3262915 config.py:54] PyTorch version 2.6.0 available.
W0403 01:56:41.808265 3262915 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:56:43.011600 3262915 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:56:43.015674 3260590 quantize_finetune_llama.py:214] layer 7 gpu 1
I0403 01:56:45.272893 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 7 in 1.4213364124298096s
I0403 01:56:48.896279 3263117 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:56:48.896385 3263117 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:56:48.896425 3263117 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:56:49.227668 3263117 config.py:54] PyTorch version 2.6.0 available.
W0403 01:56:49.427175 3263117 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:56:50.038959 3263117 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:56:50.042825 3260590 quantize_finetune_llama.py:214] layer 8 gpu 0
I0403 01:56:52.323717 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 8 in 1.0631186962127686s
I0403 01:56:55.994538 3263389 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:56:55.994622 3263389 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:56:55.994658 3263389 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:56:56.329157 3263389 config.py:54] PyTorch version 2.6.0 available.
W0403 01:56:56.521962 3263389 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:56:57.383729 3263389 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:56:57.387614 3260590 quantize_finetune_llama.py:214] layer 9 gpu 1
I0403 01:56:58.753054 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 9 in 0.9082329273223877s
I0403 01:57:02.354361 3263624 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:57:02.354459 3263624 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:57:02.354501 3263624 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:57:02.707510 3263624 config.py:54] PyTorch version 2.6.0 available.
W0403 01:57:02.907302 3263624 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:57:03.680934 3263624 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:57:03.684682 3260590 quantize_finetune_llama.py:214] layer 10 gpu 0
I0403 01:57:05.625721 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 10 in 0.8420095443725586s
I0403 01:57:09.267499 3263843 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:57:09.267589 3263843 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:57:09.267627 3263843 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:57:09.606771 3263843 config.py:54] PyTorch version 2.6.0 available.
W0403 01:57:09.801318 3263843 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:57:10.363517 3263843 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:57:10.367130 3260590 quantize_finetune_llama.py:214] layer 11 gpu 1
I0403 01:57:11.695950 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 11 in 0.8793439865112305s
I0403 01:57:15.458265 3264027 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:57:15.458358 3264027 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:57:15.458400 3264027 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:57:15.797824 3264027 config.py:54] PyTorch version 2.6.0 available.
W0403 01:57:15.999961 3264027 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:57:16.567502 3264027 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:57:16.571121 3260590 quantize_finetune_llama.py:214] layer 12 gpu 0
I0403 01:57:18.921542 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 12 in 0.7467122077941895s
I0403 01:57:22.567002 3264207 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:57:22.567096 3264207 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:57:22.567137 3264207 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:57:22.942108 3264207 config.py:54] PyTorch version 2.6.0 available.
W0403 01:57:23.159418 3264207 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:57:23.890683 3264207 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:57:23.894636 3260590 quantize_finetune_llama.py:214] layer 13 gpu 1
I0403 01:57:25.243852 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 13 in 0.9075601100921631s
I0403 01:57:29.064780 3264404 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:57:29.064879 3264404 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:57:29.064922 3264404 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:57:29.417237 3264404 config.py:54] PyTorch version 2.6.0 available.
W0403 01:57:29.627980 3264404 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:57:30.232698 3264404 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:57:30.236570 3260590 quantize_finetune_llama.py:214] layer 14 gpu 0
I0403 01:57:32.854349 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 14 in 0.9799268245697021s
I0403 01:57:36.860240 3264605 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:57:36.860353 3264605 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:57:36.860400 3264605 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:57:37.261141 3264605 config.py:54] PyTorch version 2.6.0 available.
W0403 01:57:37.473751 3264605 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:57:38.087080 3264605 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:57:38.091194 3260590 quantize_finetune_llama.py:214] layer 15 gpu 1
I0403 01:57:39.655035 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 15 in 1.0316197872161865s
I0403 01:57:43.668599 3264720 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:57:43.668712 3264720 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:57:43.668753 3264720 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:57:44.055733 3264720 config.py:54] PyTorch version 2.6.0 available.
W0403 01:57:44.270668 3264720 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:57:44.898501 3264720 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:57:44.902502 3260590 quantize_finetune_llama.py:214] layer 16 gpu 0
I0403 01:57:47.330595 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 16 in 0.9211385250091553s
I0403 01:57:51.566549 3264875 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:57:51.566665 3264875 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:57:51.566711 3264875 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:57:51.994279 3264875 config.py:54] PyTorch version 2.6.0 available.
W0403 01:57:52.215136 3264875 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:57:53.262411 3264875 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:57:53.266331 3260590 quantize_finetune_llama.py:214] layer 17 gpu 1
I0403 01:57:53.419009 3264875 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 01:57:54.732266 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 17 in 0.9657800197601318s
I0403 01:57:58.887977 3265011 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:57:58.888083 3265011 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:57:58.888125 3265011 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:57:59.384734 3265011 config.py:54] PyTorch version 2.6.0 available.
W0403 01:57:59.625828 3265011 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:58:00.264235 3265011 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:58:00.268504 3260590 quantize_finetune_llama.py:214] layer 18 gpu 0
I0403 01:58:00.600429 3265011 data_utils.py:336] using 256 training seqs, 128 validation seq1616_v proxy err 0.0012814501533284783 err 0.34767821431159973 tr(WHW.T) 271.31622314453125
bpp_loss 5.73176383972168
16_q proxy err 0.00038963291444815695 err 9.521015167236328 tr(WHW.T) 24435.859375
bpp_loss 4.7215895652771
16_k proxy err 0.0001431065466022119 err 2.7873904705047607 tr(WHW.T) 19477.728515625
bpp_loss 5.957221508026123
16_o proxy err 0.005380865652114153 err 5.133098602294922 tr(WHW.T) 953.9540405273438
bpp_loss 4.33683180809021
16_up proxy err 0.004945364315062761 err 40.87849807739258 tr(WHW.T) 8266.0234375
bpp_loss 3.955389840262277
16_gate proxy err 0.0011643520556390285 err 47.81254577636719 tr(WHW.T) 41063.65234375
bpp_loss 4.1899615696498325
16_down proxy err 0.003771561896428466 err 23.47590446472168 tr(WHW.T) 6224.45166015625
bpp_loss 4.318371432168143
17_v proxy err 0.0014961272245272994 err 0.4194934666156769 tr(WHW.T) 280.38623046875
bpp_loss 5.731614112854004
17_q proxy err 0.0003812812501564622 err 10.492745399475098 tr(WHW.T) 27519.69921875
bpp_loss 4.702155113220215
17_k proxy err 0.00017469654267188162 err 3.039870023727417 tr(WHW.T) 17400.859375
bpp_loss 5.882951498031616
17_o proxy err 0.0047388519160449505 err 5.167474269866943 tr(WHW.T) 1090.4486083984375
bpp_loss 4.354488372802734
17_up proxy err 0.004945835098624229 err 41.485225677490234 tr(WHW.T) 8387.9111328125
bpp_loss 3.9490932737077986
17_gate proxy err 0.0011666857171803713 err 48.53738021850586 tr(WHW.T) 41602.7890625
bpp_loss 4.198943819318499
17_down proxy err 0.0038319211453199387 err 23.561315536499023 tr(WHW.T) 6148.6953125
bpp_loss 4.316453456878662
I0403 01:58:53.463208 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 18 in 0.9231579303741455s
I0403 01:58:57.705151 3265878 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:58:57.705285 3265878 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:58:57.705331 3265878 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:58:58.101594 3265878 config.py:54] PyTorch version 2.6.0 available.
W0403 01:58:58.343899 3265878 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:58:59.114023 3265878 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:58:59.118750 3260590 quantize_finetune_llama.py:214] layer 19 gpu 1
I0403 01:58:59.311047 3265878 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 01:59:00.625057 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 19 in 0.9156405925750732s
I0403 01:59:04.619101 3266014 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 01:59:04.619207 3266014 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 01:59:04.619249 3266014 utils.py:162] NumExpr defaulting to 16 threads.
I0403 01:59:05.113036 3266014 config.py:54] PyTorch version 2.6.0 available.
W0403 01:59:05.333969 3266014 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 01:59:05.976576 3266014 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 01:59:05.980616 3260590 quantize_finetune_llama.py:214] layer 20 gpu 0
I0403 01:59:06.190215 3266014 data_utils.py:336] using 256 training seqs, 128 validation seq1818_v proxy err 0.0012031312799081206 err 0.34253135323524475 tr(WHW.T) 284.69989013671875
bpp_loss 5.731393814086914
18_q proxy err 0.00047072197776287794 err 10.52039909362793 tr(WHW.T) 22349.49609375
bpp_loss 4.676016330718994
18_k proxy err 0.00018412362260278314 err 3.197194814682007 tr(WHW.T) 17364.392578125
bpp_loss 5.894732713699341
18_o proxy err 0.004251801874488592 err 5.050257682800293 tr(WHW.T) 1187.7923583984375
bpp_loss 4.345025062561035
18_up proxy err 0.005394797772169113 err 42.7210807800293 tr(WHW.T) 7918.9404296875
bpp_loss 3.938399042401995
18_gate proxy err 0.0014019489753991365 err 49.2480583190918 tr(WHW.T) 35128.28125
bpp_loss 4.197988101414272
18_down proxy err 0.0038064566906541586 err 23.46060562133789 tr(WHW.T) 6163.37109375
bpp_loss 4.315703392028809
19_v proxy err 0.0011321852216497064 err 0.3824412226676941 tr(WHW.T) 337.79034423828125
bpp_loss 5.731770277023315
19_q proxy err 0.0004516660119406879 err 10.83449935913086 tr(WHW.T) 23987.85546875
bpp_loss 4.66770076751709
19_k proxy err 0.00019910704577341676 err 3.091176986694336 tr(WHW.T) 15525.201171875
bpp_loss 5.832726240158081
19_o proxy err 0.004593520425260067 err 5.296783447265625 tr(WHW.T) 1153.098876953125
bpp_loss 4.340764284133911
19_up proxy err 0.0056689889170229435 err 43.00413131713867 tr(WHW.T) 7585.85546875
bpp_loss 3.933783531188965
19_gate proxy err 0.001514461007900536 err 49.515777587890625 tr(WHW.T) 32695.314453125
bpp_loss 4.204042298453195
19_down proxy err 0.003811940085142851 err 23.32834243774414 tr(WHW.T) 6119.80810546875
bpp_loss 4.315537316458566
I0403 01:59:57.689928 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 20 in 0.945157527923584s
I0403 02:00:01.686129 3267051 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:00:01.686233 3267051 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:00:01.686279 3267051 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:00:02.067024 3267051 config.py:54] PyTorch version 2.6.0 available.
W0403 02:00:02.303913 3267051 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:00:03.030483 3267051 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:00:03.034433 3260590 quantize_finetune_llama.py:214] layer 21 gpu 1
I0403 02:00:03.252495 3267051 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 02:00:04.518203 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 21 in 0.9344847202301025s
I0403 02:00:08.684015 3267206 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:00:08.684118 3267206 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:00:08.684162 3267206 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:00:09.081551 3267206 config.py:54] PyTorch version 2.6.0 available.
W0403 02:00:09.303564 3267206 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:00:10.034501 3267206 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:00:10.038614 3260590 quantize_finetune_llama.py:214] layer 22 gpu 0
I0403 02:00:10.492703 3267206 data_utils.py:336] using 256 training seqs, 128 validation seqs220_v proxy err 0.0012929521035403013 err 0.4222659468650818 tr(WHW.T) 326.5905456542969
bpp_loss 5.731359004974365
20_q proxy err 0.0004891979042440653 err 10.120870590209961 tr(WHW.T) 20688.703125
bpp_loss 4.6737260818481445
20_k proxy err 0.00019370049994904548 err 2.976884365081787 tr(WHW.T) 15368.4912109375
bpp_loss 5.840331315994263
20_o proxy err 0.00441808020696044 err 5.256010055541992 tr(WHW.T) 1189.6593017578125
bpp_loss 4.337256908416748
20_up proxy err 0.0057364823296666145 err 43.22895050048828 tr(WHW.T) 7535.79443359375
bpp_loss 3.935051645551409
20_gate proxy err 0.0016237800009548664 err 49.603824615478516 tr(WHW.T) 30548.365234375
bpp_loss 4.20314747946603
20_down proxy err 0.003770318580791354 err 23.517263412475586 tr(WHW.T) 6237.4736328125
bpp_loss 4.3154388155256
21_v proxy err 0.0012539700837805867 err 0.450215607881546 tr(WHW.T) 359.03216552734375
bpp_loss 5.733145713806152
21_q proxy err 0.00040465284837409854 err 10.436087608337402 tr(WHW.T) 25790.224609375
bpp_loss 4.649884462356567
21_k proxy err 0.00018045998876914382 err 3.0244665145874023 tr(WHW.T) 16759.76171875
bpp_loss 5.844062566757202
21_o proxy err 0.0037818988785147667 err 4.729043483734131 tr(WHW.T) 1250.4415283203125
bpp_loss 4.373498916625977
21_up proxy err 0.005593315698206425 err 43.1151008605957 tr(WHW.T) 7708.326171875
bpp_loss 3.9381204332624162
21_gate proxy err 0.0015786306466907263 err 49.64448165893555 tr(WHW.T) 31447.8125
bpp_loss 4.213106291634696
21_down proxy err 0.003756980411708355 err 23.622806549072266 tr(WHW.T) 6287.7109375
bpp_loss 4.315967355455671
I0403 02:01:05.770637 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 22 in 0.9377717971801758s
I0403 02:01:09.745818 3268075 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:01:09.745938 3268075 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:01:09.745978 3268075 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:01:10.139690 3268075 config.py:54] PyTorch version 2.6.0 available.
W0403 02:01:10.350980 3268075 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:01:11.004847 3268075 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:01:11.009092 3260590 quantize_finetune_llama.py:214] layer 23 gpu 1
I0403 02:01:11.214271 3268075 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 02:01:12.331710 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 23 in 0.8124754428863525s
I0403 02:01:16.384049 3268204 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:01:16.384170 3268204 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:01:16.384211 3268204 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:01:16.804600 3268204 config.py:54] PyTorch version 2.6.0 available.
W0403 02:01:17.024529 3268204 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:01:17.688247 3268204 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:01:17.692467 3260590 quantize_finetune_llama.py:214] layer 24 gpu 0
I0403 02:01:17.943705 3268204 data_utils.py:336] using 256 training seqs, 128 validation seqs222_v proxy err 0.0014559405390173197 err 0.4976714849472046 tr(WHW.T) 341.8212890625
bpp_loss 5.732253551483154
22_q proxy err 0.00047579934471286833 err 9.646440505981445 tr(WHW.T) 20274.177734375
bpp_loss 4.638462066650391
22_k proxy err 0.0001939401845447719 err 2.8504421710968018 tr(WHW.T) 14697.5322265625
bpp_loss 5.848238945007324
22_o proxy err 0.004962607752531767 err 5.980047702789307 tr(WHW.T) 1205.021240234375
bpp_loss 4.326810121536255
22_up proxy err 0.0058265854604542255 err 43.59803771972656 tr(WHW.T) 7482.60498046875
bpp_loss 3.9381559916904996
22_gate proxy err 0.0016849999083206058 err 49.565189361572266 tr(WHW.T) 29415.544921875
bpp_loss 4.212806565420968
22_down proxy err 0.0036848620511591434 err 23.823759078979492 tr(WHW.T) 6465.3056640625
bpp_loss 4.315419605800083
23_v proxy err 0.0014067825395613909 err 0.5531246662139893 tr(WHW.T) 393.1842041015625
bpp_loss 5.72986912727356
23_q proxy err 0.00046937359729781747 err 10.590526580810547 tr(WHW.T) 22563.107421875
bpp_loss 4.612011194229126
23_k proxy err 0.00020386461983434856 err 3.02353572845459 tr(WHW.T) 14831.095703125
bpp_loss 5.787142515182495
23_o proxy err 0.0036132363602519035 err 6.240111827850342 tr(WHW.T) 1727.0145263671875
bpp_loss 4.326296091079712
23_up proxy err 0.0059405784122645855 err 43.68728256225586 tr(WHW.T) 7354.04541015625
bpp_loss 3.9406626565115794
23_gate proxy err 0.001817510579712689 err 49.38669967651367 tr(WHW.T) 27172.716796875
bpp_loss 4.215366499764579
23_down proxy err 0.003640703624114394 err 24.036876678466797 tr(WHW.T) 6602.26123046875
bpp_loss 4.315252372196743
I0403 02:02:10.379148 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 24 in 0.8402655124664307s
I0403 02:02:14.435656 3269240 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:02:14.435755 3269240 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:02:14.435797 3269240 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:02:14.840787 3269240 config.py:54] PyTorch version 2.6.0 available.
W0403 02:02:15.068299 3269240 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:02:15.758584 3269240 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:02:15.762694 3260590 quantize_finetune_llama.py:214] layer 25 gpu 1
I0403 02:02:16.134041 3269240 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 02:02:17.278249 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 25 in 1.0147500038146973s
I0403 02:02:21.325137 3269376 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:02:21.325245 3269376 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:02:21.325289 3269376 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:02:21.729649 3269376 config.py:54] PyTorch version 2.6.0 available.
W0403 02:02:21.958759 3269376 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:02:22.641968 3269376 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:02:22.646260 3260590 quantize_finetune_llama.py:214] layer 26 gpu 0
I0403 02:02:22.892816 3269376 data_utils.py:336] using 256 training seqs, 128 validation seqs224_v proxy err 0.0014399834908545017 err 0.6646823883056641 tr(WHW.T) 461.59027099609375
bpp_loss 5.731690883636475
24_q proxy err 0.00046200191718526185 err 10.34504222869873 tr(WHW.T) 22391.7734375
bpp_loss 4.598397731781006
24_k proxy err 0.0002002418477786705 err 2.835334539413452 tr(WHW.T) 14159.55078125
bpp_loss 5.751751899719238
24_o proxy err 0.004083065316081047 err 6.411998271942139 tr(WHW.T) 1570.388427734375
bpp_loss 4.346347093582153
24_up proxy err 0.006061714608222246 err 43.91958236694336 tr(WHW.T) 7245.40576171875
bpp_loss 3.9430174146379744
24_gate proxy err 0.0019201638642698526 err 49.45829772949219 tr(WHW.T) 25757.33203125
bpp_loss 4.220074789864676
24_down proxy err 0.0036377832293510437 err 24.287948608398438 tr(WHW.T) 6676.57958984375
bpp_loss 4.3152024405343195
25_v proxy err 0.001221654936671257 err 0.6744092702865601 tr(WHW.T) 552.045654296875
bpp_loss 5.731492757797241
25_q proxy err 0.00040900512249208987 err 10.662396430969238 tr(WHW.T) 26069.1015625
bpp_loss 4.566772937774658
25_k proxy err 0.00019641582912299782 err 2.827011823654175 tr(WHW.T) 14392.994140625
bpp_loss 5.745928049087524
25_o proxy err 0.003253549337387085 err 6.413370609283447 tr(WHW.T) 1971.1920166015625
bpp_loss 4.349998950958252
25_up proxy err 0.005995132960379124 err 43.85628128051758 tr(WHW.T) 7315.31396484375
bpp_loss 3.950185911996024
25_gate proxy err 0.0018857867689803243 err 49.380821228027344 tr(WHW.T) 26185.79296875
bpp_loss 4.229165894644601
25_down proxy err 0.003771549556404352 err 24.701637268066406 tr(WHW.T) 6549.46630859375
bpp_loss 4.315229756491525
I0403 02:03:16.348238 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 26 in 0.9201328754425049s
I0403 02:03:20.459088 3270210 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:03:20.459198 3270210 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:03:20.459248 3270210 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:03:20.866802 3270210 config.py:54] PyTorch version 2.6.0 available.
W0403 02:03:21.088838 3270210 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:03:21.732352 3270210 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:03:21.736448 3260590 quantize_finetune_llama.py:214] layer 27 gpu 1
I0403 02:03:21.990540 3270210 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 02:03:23.297125 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 27 in 0.9368381500244141s
I0403 02:03:27.686156 3270356 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:03:27.686278 3270356 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:03:27.686329 3270356 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:03:28.116564 3270356 config.py:54] PyTorch version 2.6.0 available.
W0403 02:03:28.343706 3270356 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:03:29.023637 3270356 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:03:29.028172 3260590 quantize_finetune_llama.py:214] layer 28 gpu 0
I0403 02:03:29.206467 3270356 data_utils.py:336] using 256 training seqs, 128 validation seqs226_v proxy err 0.0017389394342899323 err 0.7445827722549438 tr(WHW.T) 428.18212890625
bpp_loss 5.732013940811157
26_q proxy err 0.0004690938221756369 err 10.019720077514648 tr(WHW.T) 21359.736328125
bpp_loss 4.597430229187012
26_k proxy err 0.00018724732217378914 err 2.883676052093506 tr(WHW.T) 15400.359375
bpp_loss 5.792442321777344
26_o proxy err 0.00256984937004745 err 6.087047100067139 tr(WHW.T) 2368.6396484375
bpp_loss 4.378754138946533
26_up proxy err 0.005790999624878168 err 43.882362365722656 tr(WHW.T) 7577.68359375
bpp_loss 3.9567616326468333
26_gate proxy err 0.0017201787559315562 err 49.503475189208984 tr(WHW.T) 28778.099609375
bpp_loss 4.237537520272391
26_down proxy err 0.003833384718745947 err 25.11332893371582 tr(WHW.T) 6551.21533203125
bpp_loss 4.315405096326556
27_v proxy err 0.001286208163946867 err 0.8622006773948669 tr(WHW.T) 670.343017578125
bpp_loss 5.730691432952881
27_q proxy err 0.000494600273668766 err 10.517106056213379 tr(WHW.T) 21263.849609375
bpp_loss 4.557096481323242
27_k proxy err 0.00021038689010310918 err 2.942674398422241 tr(WHW.T) 13986.966796875
bpp_loss 5.745578050613403
27_o proxy err 0.0030815659556537867 err 6.59080171585083 tr(WHW.T) 2138.783203125
bpp_loss 4.373499631881714
27_up proxy err 0.005289480555802584 err 44.462669372558594 tr(WHW.T) 8405.8671875
bpp_loss 3.964540890284947
27_gate proxy err 0.0015307742869481444 err 50.03129577636719 tr(WHW.T) 32683.65234375
bpp_loss 4.245846748352051
27_down proxy err 0.0036407175939530134 err 23.570817947387695 tr(WHW.T) 6474.22314453125
bpp_loss 4.343554633004325
I0403 02:04:22.700915 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 28 in 0.8662371635437012s
I0403 02:04:26.615527 3271261 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:04:26.615690 3271261 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:04:26.615747 3271261 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:04:26.990123 3271261 config.py:54] PyTorch version 2.6.0 available.
W0403 02:04:27.202725 3271261 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:04:27.796594 3271261 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:04:27.800484 3260590 quantize_finetune_llama.py:214] layer 29 gpu 1
I0403 02:04:28.258481 3271261 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 02:04:29.234683 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 29 in 0.977628231048584s
I0403 02:04:33.330270 3271493 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:04:33.330378 3271493 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:04:33.330419 3271493 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:04:33.723336 3271493 config.py:54] PyTorch version 2.6.0 available.
W0403 02:04:33.940083 3271493 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:04:34.556837 3271493 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:04:34.560789 3260590 quantize_finetune_llama.py:214] layer 30 gpu 0
I0403 02:04:34.794400 3271493 data_utils.py:336] using 256 training seqs, 128 validation seqs
28_v proxy err 0.0015585923101752996 err 0.925123393535614 tr(WHW.T) 593.5634155273438
bpp_loss 5.731430768966675
28_q proxy err 0.000458178692497313 err 10.593890190124512 tr(WHW.T) 23121.744140625
bpp_loss 4.556095361709595
28_k proxy err 0.00018283849931322038 err 2.737431049346924 tr(WHW.T) 14971.8525390625
bpp_loss 5.728556394577026
28_o proxy err 0.0025896811857819557 err 6.446052074432373 tr(WHW.T) 2489.129638671875
bpp_loss 4.399541854858398
28_up proxy err 0.004336803685873747 err 44.12001037597656 tr(WHW.T) 10173.3935546875
bpp_loss 3.984010560171945
28_gate proxy err 0.0013820453314110637 err 49.47313690185547 tr(WHW.T) 35797.04296875
bpp_loss 4.241050720214844
28_down proxy err 0.0033140811137855053 err 23.657764434814453 tr(WHW.T) 7138.55908203125
bpp_loss 4.353896004813058
29_v proxy err 0.0011984002776443958 err 1.0088239908218384 tr(WHW.T) 841.8088989257812
bpp_loss 5.731308698654175
29_q proxy err 0.0005757281905971467 err 11.866656303405762 tr(WHW.T) 20611.560546875
bpp_loss 4.498727321624756
29_k proxy err 0.00018308179278392345 err 2.9917900562286377 tr(WHW.T) 16341.275390625
bpp_loss 5.739565372467041
29_o proxy err 0.0018428545445203781 err 5.674108982086182 tr(WHW.T) 3078.978271484375
bpp_loss 4.457032680511475
29_up proxy err 0.0034923022612929344 err 44.67261505126953 tr(WHW.T) 12791.73828125
bpp_loss 4.005279132298061
29_gate proxy err 0.001295352354645729 err 49.53586959838867 tr(WHW.T) 38241.23046875
bpp_loss 4.237512860979352
29_down proxy err 0.0028474926948547363 err 21.061120986938477 tr(WHW.T) 7396.3740234375
bpp_loss 4.39915098462786
I0403 02:05:27.025113 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 30 in 0.792661190032959s
I0403 02:05:31.240378 3272468 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:05:31.240479 3272468 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:05:31.240519 3272468 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:05:31.611783 3272468 config.py:54] PyTorch version 2.6.0 available.
W0403 02:05:31.828647 3272468 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:05:32.570124 3272468 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:05:32.574547 3260590 quantize_finetune_llama.py:214] layer 31 gpu 1
I0403 02:05:32.884385 3272468 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0403 02:05:34.223970 3260590 quantize_finetune_llama.py:245] computed original embedding for layer 31 in 1.0734994411468506s
I0403 02:05:38.590295 3272619 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:05:38.590402 3272619 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:05:38.590448 3272619 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:05:39.011560 3272619 config.py:54] PyTorch version 2.6.0 available.
W0403 02:05:39.248884 3272619 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0403 02:05:39.969815 3272619 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0403 02:05:40.210540 3272619 data_utils.py:336] using 256 training seqs, 128 validation seqs
30_v proxy err 0.001709930133074522 err 1.4545531272888184 tr(WHW.T) 850.650634765625
bpp_loss 5.731425523757935
30_q proxy err 0.0004414590075612068 err 10.600650787353516 tr(WHW.T) 24012.763671875
bpp_loss 4.490533351898193
30_k proxy err 0.00016622102702967823 err 2.329045057296753 tr(WHW.T) 14011.7353515625
bpp_loss 5.727973461151123
30_o proxy err 0.0016849397215992212 err 8.158222198486328 tr(WHW.T) 4841.84814453125
bpp_loss 4.409672021865845
30_up proxy err 0.002145186997950077 err 46.40144729614258 tr(WHW.T) 21630.490234375
bpp_loss 4.021341732570103
30_gate proxy err 0.00099119171500206 err 51.3658561706543 tr(WHW.T) 51822.3203125
bpp_loss 4.275983265468052
30_down proxy err 0.0016994067700579762 err 14.844243049621582 tr(WHW.T) 8734.9560546875
bpp_loss 4.478470870426723
31_v proxy err 0.0006429607747122645 err 1.1566144227981567 tr(WHW.T) 1798.8880615234375
bpp_loss 5.732854843139648
31_q proxy err 0.00028357713017612696 err 13.081401824951172 tr(WHW.T) 46129.9609375
bpp_loss 4.531322240829468
31_k proxy err 0.00013350920926313847 err 2.7296345233917236 tr(WHW.T) 20445.2890625
bpp_loss 5.7468156814575195
31_o proxy err 0.001812041038647294 err 3.962841272354126 tr(WHW.T) 2186.948974609375
bpp_loss 4.565787076950073
31_up proxy err 0.0007493701414205134 err 51.63967514038086 tr(WHW.T) 68910.7734375
bpp_loss 4.157337733677456
31_gate proxy err 0.00039055157685652375 err 56.34913635253906 tr(WHW.T) 144280.90625
bpp_loss 4.449368068150112
31_down proxy err 0.0007595234783366323 err 7.504143714904785 tr(WHW.T) 9880.068359375
bpp_loss 4.579332147325788
I0403 02:06:02.368038 3272954 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:06:02.368231 3272954 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:06:02.368278 3272954 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:06:02.710071 3272954 config.py:54] PyTorch version 2.6.0 available.
W0403 02:06:02.924752 3272954 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0403 02:06:03.037907 3272954 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.80it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.12it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.31it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.49it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.51it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.37it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.51it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.41it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.28it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.51it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.45it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.48it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.46it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.59it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.66it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.56it/s]
I0403 02:06:06.367202 3272954 hfize_llama.py:161] loaded layer 0
I0403 02:06:07.338305 3272954 hfize_llama.py:161] loaded layer 1
I0403 02:06:08.186135 3272954 hfize_llama.py:161] loaded layer 2
I0403 02:06:09.029574 3272954 hfize_llama.py:161] loaded layer 3
I0403 02:06:09.861230 3272954 hfize_llama.py:161] loaded layer 4
I0403 02:06:10.730804 3272954 hfize_llama.py:161] loaded layer 5
I0403 02:06:11.575687 3272954 hfize_llama.py:161] loaded layer 6
I0403 02:06:12.421585 3272954 hfize_llama.py:161] loaded layer 7
I0403 02:06:13.291913 3272954 hfize_llama.py:161] loaded layer 8
I0403 02:06:14.079277 3272954 hfize_llama.py:161] loaded layer 9
I0403 02:06:14.905100 3272954 hfize_llama.py:161] loaded layer 10
I0403 02:06:15.714544 3272954 hfize_llama.py:161] loaded layer 11
I0403 02:06:16.477555 3272954 hfize_llama.py:161] loaded layer 12
I0403 02:06:17.234787 3272954 hfize_llama.py:161] loaded layer 13
I0403 02:06:18.009302 3272954 hfize_llama.py:161] loaded layer 14
I0403 02:06:18.767787 3272954 hfize_llama.py:161] loaded layer 15
I0403 02:06:19.488362 3272954 hfize_llama.py:161] loaded layer 16
I0403 02:06:20.198220 3272954 hfize_llama.py:161] loaded layer 17
I0403 02:06:20.938331 3272954 hfize_llama.py:161] loaded layer 18
I0403 02:06:21.680227 3272954 hfize_llama.py:161] loaded layer 19
I0403 02:06:22.429931 3272954 hfize_llama.py:161] loaded layer 20
I0403 02:06:23.187880 3272954 hfize_llama.py:161] loaded layer 21
I0403 02:06:23.963924 3272954 hfize_llama.py:161] loaded layer 22
I0403 02:06:24.679844 3272954 hfize_llama.py:161] loaded layer 23
I0403 02:06:25.384352 3272954 hfize_llama.py:161] loaded layer 24
I0403 02:06:26.083504 3272954 hfize_llama.py:161] loaded layer 25
I0403 02:06:26.767543 3272954 hfize_llama.py:161] loaded layer 26
I0403 02:06:27.493831 3272954 hfize_llama.py:161] loaded layer 27
I0403 02:06:28.201916 3272954 hfize_llama.py:161] loaded layer 28
I0403 02:06:28.915298 3272954 hfize_llama.py:161] loaded layer 29
I0403 02:06:29.656827 3272954 hfize_llama.py:161] loaded layer 30
I0403 02:06:30.388835 3272954 hfize_llama.py:161] loaded layer 31
I0403 02:06:30.388957 3272954 hfize_llama.py:165] saving model...
I0403 02:06:40.504812 3273611 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:06:40.504915 3273611 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:06:40.504957 3273611 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:06:40.855014 3273611 config.py:54] PyTorch version 2.6.0 available.
W0403 02:06:41.069877 3273611 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0403 02:06:41.195012 3273611 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.80it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.34it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.59it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.74it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.65it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.65it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.86it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.68it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.74it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.91it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.56it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.60it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.78it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.75it/s]
I0403 02:06:44.430270 3273611 hfize_llama.py:161] loaded layer 0
I0403 02:06:45.191366 3273611 hfize_llama.py:161] loaded layer 1
I0403 02:06:46.070736 3273611 hfize_llama.py:161] loaded layer 2
I0403 02:06:46.813655 3273611 hfize_llama.py:161] loaded layer 3
I0403 02:06:47.511592 3273611 hfize_llama.py:161] loaded layer 4
I0403 02:06:48.165038 3273611 hfize_llama.py:161] loaded layer 5
I0403 02:06:48.803392 3273611 hfize_llama.py:161] loaded layer 6
I0403 02:06:49.463900 3273611 hfize_llama.py:161] loaded layer 7
I0403 02:06:50.116789 3273611 hfize_llama.py:161] loaded layer 8
I0403 02:06:50.769539 3273611 hfize_llama.py:161] loaded layer 9
I0403 02:06:51.476681 3273611 hfize_llama.py:161] loaded layer 10
I0403 02:06:52.236660 3273611 hfize_llama.py:161] loaded layer 11
I0403 02:06:52.944883 3273611 hfize_llama.py:161] loaded layer 12
I0403 02:06:53.636427 3273611 hfize_llama.py:161] loaded layer 13
I0403 02:06:54.293522 3273611 hfize_llama.py:161] loaded layer 14
I0403 02:06:54.949312 3273611 hfize_llama.py:161] loaded layer 15
I0403 02:06:55.561609 3273611 hfize_llama.py:161] loaded layer 16
I0403 02:06:56.196014 3273611 hfize_llama.py:161] loaded layer 17
I0403 02:06:56.837616 3273611 hfize_llama.py:161] loaded layer 18
I0403 02:06:57.480870 3273611 hfize_llama.py:161] loaded layer 19
I0403 02:06:58.184914 3273611 hfize_llama.py:161] loaded layer 20
I0403 02:06:58.906840 3273611 hfize_llama.py:161] loaded layer 21
I0403 02:06:59.574169 3273611 hfize_llama.py:161] loaded layer 22
I0403 02:07:00.197674 3273611 hfize_llama.py:161] loaded layer 23
I0403 02:07:00.821811 3273611 hfize_llama.py:161] loaded layer 24
I0403 02:07:01.429897 3273611 hfize_llama.py:161] loaded layer 25
I0403 02:07:02.046053 3273611 hfize_llama.py:161] loaded layer 26
I0403 02:07:02.694887 3273611 hfize_llama.py:161] loaded layer 27
I0403 02:07:03.583459 3273611 hfize_llama.py:161] loaded layer 28
I0403 02:07:04.207152 3273611 hfize_llama.py:161] loaded layer 29
I0403 02:07:04.907452 3273611 hfize_llama.py:161] loaded layer 30
I0403 02:07:05.520117 3273611 hfize_llama.py:161] loaded layer 31
I0403 02:07:05.520287 3273611 hfize_llama.py:165] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 194, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 173, in main
    model, _ = model_from_hf_path(args.hf_output_path, device_map='cuda')
  File "/workspace/Weight_compression/comp_lm_qtip/lib/utils/unsafe_import.py", line 44, in model_from_hf_path
    model = model_cls.from_pretrained(path,
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4264, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4755, in _load_pretrained_model
    state_dict = load_state_dict(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 504, in load_state_dict
    with safe_open(checkpoint_file, framework="pt") as f:
safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer
I0403 02:07:14.815534 3274471 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:07:14.815664 3274471 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:07:14.815709 3274471 utils.py:162] NumExpr defaulting to 16 threads.
W0403 02:07:15.175748 3274471 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0403 02:07:15.983869 3274471 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:05,  1.04it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:02<00:14,  2.33s/it]
Traceback (most recent call last):
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 124, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 71, in main
    model, model_str = model_from_hf_path(
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 49, in model_from_hf_path
    model = maybe_wrap(use_cuda_graph)(model_cls).from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4264, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4755, in _load_pretrained_model
    state_dict = load_state_dict(
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 504, in load_state_dict
    with safe_open(checkpoint_file, framework="pt") as f:
safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 194, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 166, in main
    model.save_pretrained(args.hf_output_path, safe_serialization=True)
  File "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3034, in save_pretrained
    safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={"format": "pt"})
  File "/opt/conda/lib/python3.10/site-packages/safetensors/torch.py", line 286, in save_file
    serialize_file(_flatten(tensors), filename, metadata=metadata)
safetensors_rust.SafetensorError: Error while serializing: IoError(Os { code: 2, kind: NotFound, message: "No such file or directory" })
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
  File "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../hf_model_comp/comp_qtip/hf/noft_had_rescale3_trained/meta-llama--Meta-Llama-3-8B/lmbda200'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 124, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 71, in main
    model, model_str = model_from_hf_path(
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 34, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1021, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py", line 590, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py", line 649, in _get_config_dict
    resolved_config_file = cached_file(
  File "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: '../hf_model_comp/comp_qtip/hf/noft_had_rescale3_trained/meta-llama--Meta-Llama-3-8B/lmbda200'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
I0403 02:39:27.771404 3302591 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:39:27.771532 3302591 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:39:27.771576 3302591 utils.py:162] NumExpr defaulting to 16 threads.
I0403 02:39:28.098628 3302591 config.py:54] PyTorch version 2.6.0 available.
W0403 02:39:28.311976 3302591 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0403 02:39:28.424620 3302591 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.35it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.85it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.95it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.55it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.64it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.81it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.03it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.85it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.38it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.73it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.81it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.71it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.82it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.04it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.88it/s]
I0403 02:39:31.994260 3302591 hfize_llama.py:161] loaded layer 0
I0403 02:39:32.787900 3302591 hfize_llama.py:161] loaded layer 1
I0403 02:39:33.593649 3302591 hfize_llama.py:161] loaded layer 2
I0403 02:39:34.386559 3302591 hfize_llama.py:161] loaded layer 3
I0403 02:39:35.185139 3302591 hfize_llama.py:161] loaded layer 4
I0403 02:39:36.163874 3302591 hfize_llama.py:161] loaded layer 5
I0403 02:39:37.032952 3302591 hfize_llama.py:161] loaded layer 6
I0403 02:39:37.890159 3302591 hfize_llama.py:161] loaded layer 7
I0403 02:39:38.762444 3302591 hfize_llama.py:161] loaded layer 8
I0403 02:39:39.556923 3302591 hfize_llama.py:161] loaded layer 9
I0403 02:39:40.367750 3302591 hfize_llama.py:161] loaded layer 10
I0403 02:39:41.184030 3302591 hfize_llama.py:161] loaded layer 11
I0403 02:39:41.976540 3302591 hfize_llama.py:161] loaded layer 12
I0403 02:39:42.775533 3302591 hfize_llama.py:161] loaded layer 13
I0403 02:39:43.611870 3302591 hfize_llama.py:161] loaded layer 14
I0403 02:39:44.425339 3302591 hfize_llama.py:161] loaded layer 15
I0403 02:39:45.156008 3302591 hfize_llama.py:161] loaded layer 16
I0403 02:39:45.905025 3302591 hfize_llama.py:161] loaded layer 17
I0403 02:39:46.674256 3302591 hfize_llama.py:161] loaded layer 18
I0403 02:39:47.454537 3302591 hfize_llama.py:161] loaded layer 19
I0403 02:39:48.224298 3302591 hfize_llama.py:161] loaded layer 20
I0403 02:39:48.994161 3302591 hfize_llama.py:161] loaded layer 21
I0403 02:39:49.800967 3302591 hfize_llama.py:161] loaded layer 22
I0403 02:39:50.538418 3302591 hfize_llama.py:161] loaded layer 23
I0403 02:39:51.280056 3302591 hfize_llama.py:161] loaded layer 24
I0403 02:39:51.999183 3302591 hfize_llama.py:161] loaded layer 25
I0403 02:39:52.693428 3302591 hfize_llama.py:161] loaded layer 26
I0403 02:39:53.438091 3302591 hfize_llama.py:161] loaded layer 27
I0403 02:39:54.199870 3302591 hfize_llama.py:161] loaded layer 28
I0403 02:39:54.988543 3302591 hfize_llama.py:161] loaded layer 29
I0403 02:39:55.774821 3302591 hfize_llama.py:161] loaded layer 30
I0403 02:39:56.516011 3302591 hfize_llama.py:161] loaded layer 31
I0403 02:39:56.516143 3302591 hfize_llama.py:165] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:02<00:12,  2.03s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:07,  1.56s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:05,  1.43s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:05<00:03,  1.31s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.23s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.20s/it]
I0403 02:40:43.777351 3302591 hfize_llama.py:175] successfully loaded hfized model
I0403 02:40:48.890810 3303856 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0403 02:40:48.890938 3303856 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0403 02:40:48.890983 3303856 utils.py:162] NumExpr defaulting to 16 threads.
W0403 02:40:49.255629 3303856 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0403 02:40:49.665530 3303856 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:08,  1.48s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:07,  1.41s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:04<00:05,  1.39s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:05<00:03,  1.32s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.29s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.27s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.22s/it]
I0403 02:40:58.342936 3303856 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.5614800453186035:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.5614800453186035:   1%|          | 1/141 [00:01<04:22,  1.88s/it]avg_loss = 1.875256896018982:   1%|          | 1/141 [00:03<04:22,  1.88s/it] avg_loss = 1.875256896018982:   1%|▏         | 2/141 [00:03<03:44,  1.62s/it]avg_loss = 2.0094424883524575:   1%|▏         | 2/141 [00:04<03:44,  1.62s/it]avg_loss = 2.0094424883524575:   2%|▏         | 3/141 [00:04<03:31,  1.53s/it]avg_loss = 1.9645672738552094:   2%|▏         | 3/141 [00:06<03:31,  1.53s/it]avg_loss = 1.9645672738552094:   3%|▎         | 4/141 [00:06<03:25,  1.50s/it]avg_loss = 1.915924596786499:   3%|▎         | 4/141 [00:07<03:25,  1.50s/it] avg_loss = 1.915924596786499:   4%|▎         | 5/141 [00:07<03:21,  1.48s/it]avg_loss = 1.819828728834788:   4%|▎         | 5/141 [00:09<03:21,  1.48s/it]avg_loss = 1.819828728834788:   4%|▍         | 6/141 [00:09<03:18,  1.47s/it]avg_loss = 1.754931160381862:   4%|▍         | 6/141 [00:10<03:18,  1.47s/it]avg_loss = 1.754931160381862:   5%|▍         | 7/141 [00:10<03:16,  1.46s/it]avg_loss = 1.7511698603630066:   5%|▍         | 7/141 [00:11<03:16,  1.46s/it]avg_loss = 1.7511698603630066:   6%|▌         | 8/141 [00:11<03:14,  1.46s/it]avg_loss = 1.7858800888061523:   6%|▌         | 8/141 [00:13<03:14,  1.46s/it]avg_loss = 1.7858800888061523:   6%|▋         | 9/141 [00:13<03:12,  1.46s/it]avg_loss = 1.7906686902046203:   6%|▋         | 9/141 [00:14<03:12,  1.46s/it]avg_loss = 1.7906686902046203:   7%|▋         | 10/141 [00:14<03:11,  1.46s/it]avg_loss = 1.7868508208881726:   7%|▋         | 10/141 [00:16<03:11,  1.46s/it]avg_loss = 1.7868508208881726:   8%|▊         | 11/141 [00:16<03:10,  1.46s/it]avg_loss = 1.810124655564626:   8%|▊         | 11/141 [00:17<03:10,  1.46s/it] avg_loss = 1.810124655564626:   9%|▊         | 12/141 [00:17<03:09,  1.47s/it]avg_loss = 1.8229358287957997:   9%|▊         | 12/141 [00:19<03:09,  1.47s/it]avg_loss = 1.8229358287957997:   9%|▉         | 13/141 [00:19<03:07,  1.47s/it]avg_loss = 1.8412117191723414:   9%|▉         | 13/141 [00:20<03:07,  1.47s/it]avg_loss = 1.8412117191723414:  10%|▉         | 14/141 [00:20<03:06,  1.47s/it]avg_loss = 1.8513938665390015:  10%|▉         | 14/141 [00:22<03:06,  1.47s/it]avg_loss = 1.8513938665390015:  11%|█         | 15/141 [00:22<03:05,  1.47s/it]avg_loss = 1.8759742006659508:  11%|█         | 15/141 [00:23<03:05,  1.47s/it]avg_loss = 1.8759742006659508:  11%|█▏        | 16/141 [00:23<03:04,  1.48s/it]avg_loss = 1.8789277287090527:  11%|█▏        | 16/141 [00:25<03:04,  1.48s/it]avg_loss = 1.8789277287090527:  12%|█▏        | 17/141 [00:25<03:03,  1.48s/it]avg_loss = 1.8816353811158075:  12%|█▏        | 17/141 [00:26<03:03,  1.48s/it]avg_loss = 1.8816353811158075:  13%|█▎        | 18/141 [00:26<03:02,  1.48s/it]avg_loss = 1.8625803621191728:  13%|█▎        | 18/141 [00:28<03:02,  1.48s/it]avg_loss = 1.8625803621191728:  13%|█▎        | 19/141 [00:28<03:01,  1.49s/it]avg_loss = 1.8617848336696625:  13%|█▎        | 19/141 [00:29<03:01,  1.49s/it]avg_loss = 1.8617848336696625:  14%|█▍        | 20/141 [00:29<03:00,  1.49s/it]avg_loss = 1.8666485434486753:  14%|█▍        | 20/141 [00:31<03:00,  1.49s/it]avg_loss = 1.8666485434486753:  15%|█▍        | 21/141 [00:31<02:58,  1.49s/it]avg_loss = 1.8691658756949685:  15%|█▍        | 21/141 [00:32<02:58,  1.49s/it]avg_loss = 1.8691658756949685:  16%|█▌        | 22/141 [00:32<02:57,  1.49s/it]avg_loss = 1.8706064379733542:  16%|█▌        | 22/141 [00:34<02:57,  1.49s/it]avg_loss = 1.8706064379733542:  16%|█▋        | 23/141 [00:34<02:56,  1.50s/it]avg_loss = 1.8756049970785778:  16%|█▋        | 23/141 [00:35<02:56,  1.50s/it]avg_loss = 1.8756049970785778:  17%|█▋        | 24/141 [00:35<02:55,  1.50s/it]avg_loss = 1.8813993644714355:  17%|█▋        | 24/141 [00:37<02:55,  1.50s/it]avg_loss = 1.8813993644714355:  18%|█▊        | 25/141 [00:37<02:54,  1.50s/it]avg_loss = 1.892858441059406:  18%|█▊        | 25/141 [00:38<02:54,  1.50s/it] avg_loss = 1.892858441059406:  18%|█▊        | 26/141 [00:38<02:52,  1.50s/it]avg_loss = 1.9057192007700603:  18%|█▊        | 26/141 [00:40<02:52,  1.50s/it]avg_loss = 1.9057192007700603:  19%|█▉        | 27/141 [00:40<02:51,  1.50s/it]avg_loss = 1.9122677786009652:  19%|█▉        | 27/141 [00:41<02:51,  1.50s/it]avg_loss = 1.9122677786009652:  20%|█▉        | 28/141 [00:41<02:50,  1.51s/it]avg_loss = 1.9090935485116367:  20%|█▉        | 28/141 [00:43<02:50,  1.51s/it]avg_loss = 1.9090935485116367:  21%|██        | 29/141 [00:43<02:49,  1.51s/it]avg_loss = 1.8993938565254211:  21%|██        | 29/141 [00:44<02:49,  1.51s/it]avg_loss = 1.8993938565254211:  21%|██▏       | 30/141 [00:44<02:47,  1.51s/it]avg_loss = 1.8849215199870448:  21%|██▏       | 30/141 [00:46<02:47,  1.51s/it]avg_loss = 1.8849215199870448:  22%|██▏       | 31/141 [00:46<02:46,  1.52s/it]avg_loss = 1.8729056678712368:  22%|██▏       | 31/141 [00:47<02:46,  1.52s/it]avg_loss = 1.8729056678712368:  23%|██▎       | 32/141 [00:47<02:45,  1.52s/it]avg_loss = 1.871544021548647:  23%|██▎       | 32/141 [00:49<02:45,  1.52s/it] avg_loss = 1.871544021548647:  23%|██▎       | 33/141 [00:49<02:44,  1.52s/it]avg_loss = 1.8699611320215113:  23%|██▎       | 33/141 [00:50<02:44,  1.52s/it]avg_loss = 1.8699611320215113:  24%|██▍       | 34/141 [00:50<02:42,  1.52s/it]avg_loss = 1.8728937591825212:  24%|██▍       | 34/141 [00:52<02:42,  1.52s/it]avg_loss = 1.8728937591825212:  25%|██▍       | 35/141 [00:52<02:41,  1.52s/it]avg_loss = 1.8566207190354664:  25%|██▍       | 35/141 [00:53<02:41,  1.52s/it]avg_loss = 1.8566207190354664:  26%|██▌       | 36/141 [00:53<02:40,  1.53s/it]avg_loss = 1.8413086904061806:  26%|██▌       | 36/141 [00:55<02:40,  1.53s/it]avg_loss = 1.8413086904061806:  26%|██▌       | 37/141 [00:55<02:38,  1.53s/it]avg_loss = 1.8265589099181325:  26%|██▌       | 37/141 [00:57<02:38,  1.53s/it]avg_loss = 1.8265589099181325:  27%|██▋       | 38/141 [00:57<02:37,  1.53s/it]avg_loss = 1.812589107415615:  27%|██▋       | 38/141 [00:58<02:37,  1.53s/it] avg_loss = 1.812589107415615:  28%|██▊       | 39/141 [00:58<02:36,  1.53s/it]avg_loss = 1.8042824685573577:  28%|██▊       | 39/141 [01:00<02:36,  1.53s/it]avg_loss = 1.8042824685573577:  28%|██▊       | 40/141 [01:00<02:34,  1.53s/it]avg_loss = 1.809402419299614:  28%|██▊       | 40/141 [01:01<02:34,  1.53s/it] avg_loss = 1.809402419299614:  29%|██▉       | 41/141 [01:01<02:33,  1.53s/it]avg_loss = 1.8266831239064534:  29%|██▉       | 41/141 [01:03<02:33,  1.53s/it]avg_loss = 1.8266831239064534:  30%|██▉       | 42/141 [01:03<02:31,  1.53s/it]avg_loss = 1.8430661212566286:  30%|██▉       | 42/141 [01:04<02:31,  1.53s/it]avg_loss = 1.8430661212566286:  30%|███       | 43/141 [01:04<02:30,  1.53s/it]avg_loss = 1.8458901833404193:  30%|███       | 43/141 [01:06<02:30,  1.53s/it]avg_loss = 1.8458901833404193:  31%|███       | 44/141 [01:06<02:29,  1.54s/it]avg_loss = 1.8502326091130574:  31%|███       | 44/141 [01:07<02:29,  1.54s/it]avg_loss = 1.8502326091130574:  32%|███▏      | 45/141 [01:07<02:27,  1.54s/it]avg_loss = 1.8558290082475413:  32%|███▏      | 45/141 [01:09<02:27,  1.54s/it]avg_loss = 1.8558290082475413:  33%|███▎      | 46/141 [01:09<02:26,  1.54s/it]avg_loss = 1.862441374900493:  33%|███▎      | 46/141 [01:10<02:26,  1.54s/it] avg_loss = 1.862441374900493:  33%|███▎      | 47/141 [01:10<02:24,  1.54s/it]avg_loss = 1.8656958118081093:  33%|███▎      | 47/141 [01:12<02:24,  1.54s/it]avg_loss = 1.8656958118081093:  34%|███▍      | 48/141 [01:12<02:23,  1.54s/it]avg_loss = 1.8643134662083216:  34%|███▍      | 48/141 [01:13<02:23,  1.54s/it]avg_loss = 1.8643134662083216:  35%|███▍      | 49/141 [01:13<02:21,  1.54s/it]avg_loss = 1.8639925813674927:  35%|███▍      | 49/141 [01:15<02:21,  1.54s/it]avg_loss = 1.8639925813674927:  35%|███▌      | 50/141 [01:15<02:20,  1.54s/it]avg_loss = 1.8574881764019238:  35%|███▌      | 50/141 [01:17<02:20,  1.54s/it]avg_loss = 1.8574881764019238:  36%|███▌      | 51/141 [01:17<02:19,  1.55s/it]avg_loss = 1.8537309582416828:  36%|███▌      | 51/141 [01:18<02:19,  1.55s/it]avg_loss = 1.8537309582416828:  37%|███▋      | 52/141 [01:18<02:17,  1.55s/it]avg_loss = 1.8471439312089164:  37%|███▋      | 52/141 [01:20<02:17,  1.55s/it]avg_loss = 1.8471439312089164:  38%|███▊      | 53/141 [01:20<02:16,  1.55s/it]avg_loss = 1.844289653831058:  38%|███▊      | 53/141 [01:21<02:16,  1.55s/it] avg_loss = 1.844289653831058:  38%|███▊      | 54/141 [01:21<02:15,  1.55s/it]avg_loss = 1.8366746815768156:  38%|███▊      | 54/141 [01:23<02:15,  1.55s/it]avg_loss = 1.8366746815768156:  39%|███▉      | 55/141 [01:23<02:13,  1.56s/it]avg_loss = 1.828998674239431:  39%|███▉      | 55/141 [01:24<02:13,  1.56s/it] avg_loss = 1.828998674239431:  40%|███▉      | 56/141 [01:24<02:12,  1.56s/it]avg_loss = 1.8225106105469822:  40%|███▉      | 56/141 [01:26<02:12,  1.56s/it]avg_loss = 1.8225106105469822:  40%|████      | 57/141 [01:26<02:10,  1.56s/it]avg_loss = 1.819793033188787:  40%|████      | 57/141 [01:27<02:10,  1.56s/it] avg_loss = 1.819793033188787:  41%|████      | 58/141 [01:27<02:09,  1.56s/it]avg_loss = 1.8221192299309423:  41%|████      | 58/141 [01:29<02:09,  1.56s/it]avg_loss = 1.8221192299309423:  42%|████▏     | 59/141 [01:29<02:07,  1.56s/it]avg_loss = 1.8278822958469392:  42%|████▏     | 59/141 [01:31<02:07,  1.56s/it]avg_loss = 1.8278822958469392:  43%|████▎     | 60/141 [01:31<02:06,  1.56s/it]avg_loss = 1.8339346881772651:  43%|████▎     | 60/141 [01:32<02:06,  1.56s/it]avg_loss = 1.8339346881772651:  43%|████▎     | 61/141 [01:32<02:04,  1.56s/it]avg_loss = 1.841273425086852:  43%|████▎     | 61/141 [01:34<02:04,  1.56s/it] avg_loss = 1.841273425086852:  44%|████▍     | 62/141 [01:34<02:03,  1.56s/it]avg_loss = 1.8320897666234819:  44%|████▍     | 62/141 [01:35<02:03,  1.56s/it]avg_loss = 1.8320897666234819:  45%|████▍     | 63/141 [01:35<02:01,  1.56s/it]avg_loss = 1.8299615327268839:  45%|████▍     | 63/141 [01:37<02:01,  1.56s/it]avg_loss = 1.8299615327268839:  45%|████▌     | 64/141 [01:37<02:00,  1.56s/it]avg_loss = 1.8273854750853318:  45%|████▌     | 64/141 [01:38<02:00,  1.56s/it]avg_loss = 1.8273854750853318:  46%|████▌     | 65/141 [01:38<01:59,  1.57s/it]avg_loss = 1.8213501012686528:  46%|████▌     | 65/141 [01:40<01:59,  1.57s/it]avg_loss = 1.8213501012686528:  47%|████▋     | 66/141 [01:40<01:57,  1.57s/it]avg_loss = 1.8186738419888624:  47%|████▋     | 66/141 [01:42<01:57,  1.57s/it]avg_loss = 1.8186738419888624:  48%|████▊     | 67/141 [01:42<01:56,  1.57s/it]avg_loss = 1.8151508727494408:  48%|████▊     | 67/141 [01:43<01:56,  1.57s/it]avg_loss = 1.8151508727494408:  48%|████▊     | 68/141 [01:43<01:54,  1.57s/it]avg_loss = 1.8123310078745303:  48%|████▊     | 68/141 [01:45<01:54,  1.57s/it]avg_loss = 1.8123310078745303:  49%|████▉     | 69/141 [01:45<01:53,  1.57s/it]avg_loss = 1.8133061579295566:  49%|████▉     | 69/141 [01:46<01:53,  1.57s/it]avg_loss = 1.8133061579295566:  50%|████▉     | 70/141 [01:46<01:51,  1.57s/it]avg_loss = 1.8171839949110864:  50%|████▉     | 70/141 [01:48<01:51,  1.57s/it]avg_loss = 1.8171839949110864:  50%|█████     | 71/141 [01:48<01:50,  1.57s/it]avg_loss = 1.8195028056701024:  50%|█████     | 71/141 [01:49<01:50,  1.57s/it]avg_loss = 1.8195028056701024:  51%|█████     | 72/141 [01:49<01:48,  1.57s/it]avg_loss = 1.8181233569367292:  51%|█████     | 72/141 [01:51<01:48,  1.57s/it]avg_loss = 1.8181233569367292:  52%|█████▏    | 73/141 [01:51<01:46,  1.57s/it]avg_loss = 1.8199171633333773:  52%|█████▏    | 73/141 [01:53<01:46,  1.57s/it]avg_loss = 1.8199171633333773:  52%|█████▏    | 74/141 [01:53<01:45,  1.57s/it]avg_loss = 1.8201135555903116:  52%|█████▏    | 74/141 [01:54<01:45,  1.57s/it]avg_loss = 1.8201135555903116:  53%|█████▎    | 75/141 [01:54<01:43,  1.57s/it]avg_loss = 1.8189608737042076:  53%|█████▎    | 75/141 [01:56<01:43,  1.57s/it]avg_loss = 1.8189608737042076:  54%|█████▍    | 76/141 [01:56<01:42,  1.57s/it]avg_loss = 1.820176593669049:  54%|█████▍    | 76/141 [01:57<01:42,  1.57s/it] avg_loss = 1.820176593669049:  55%|█████▍    | 77/141 [01:57<01:40,  1.57s/it]avg_loss = 1.8226817219685285:  55%|█████▍    | 77/141 [01:59<01:40,  1.57s/it]avg_loss = 1.8226817219685285:  55%|█████▌    | 78/141 [01:59<01:38,  1.57s/it]avg_loss = 1.8268910857695568:  55%|█████▌    | 78/141 [02:00<01:38,  1.57s/it]avg_loss = 1.8268910857695568:  56%|█████▌    | 79/141 [02:00<01:37,  1.57s/it]avg_loss = 1.8239979669451714:  56%|█████▌    | 79/141 [02:02<01:37,  1.57s/it]avg_loss = 1.8239979669451714:  57%|█████▋    | 80/141 [02:02<01:36,  1.57s/it]avg_loss = 1.8229288525051541:  57%|█████▋    | 80/141 [02:04<01:36,  1.57s/it]avg_loss = 1.8229288525051541:  57%|█████▋    | 81/141 [02:04<01:34,  1.58s/it]avg_loss = 1.8222316154619542:  57%|█████▋    | 81/141 [02:05<01:34,  1.58s/it]avg_loss = 1.8222316154619542:  58%|█████▊    | 82/141 [02:05<01:33,  1.58s/it]avg_loss = 1.8205004987946476:  58%|█████▊    | 82/141 [02:07<01:33,  1.58s/it]avg_loss = 1.8205004987946476:  59%|█████▉    | 83/141 [02:07<01:31,  1.58s/it]avg_loss = 1.8184277642340887:  59%|█████▉    | 83/141 [02:08<01:31,  1.58s/it]avg_loss = 1.8184277642340887:  60%|█████▉    | 84/141 [02:08<01:30,  1.58s/it]avg_loss = 1.8160851184059592:  60%|█████▉    | 84/141 [02:10<01:30,  1.58s/it]avg_loss = 1.8160851184059592:  60%|██████    | 85/141 [02:10<01:28,  1.58s/it]avg_loss = 1.817920056886451:  60%|██████    | 85/141 [02:11<01:28,  1.58s/it] avg_loss = 1.817920056886451:  61%|██████    | 86/141 [02:11<01:26,  1.58s/it]avg_loss = 1.819949052799707:  61%|██████    | 86/141 [02:13<01:26,  1.58s/it]avg_loss = 1.819949052799707:  62%|██████▏   | 87/141 [02:13<01:25,  1.58s/it]avg_loss = 1.8201892091469332:  62%|██████▏   | 87/141 [02:15<01:25,  1.58s/it]avg_loss = 1.8201892091469332:  62%|██████▏   | 88/141 [02:15<01:23,  1.58s/it]avg_loss = 1.8289693084995398:  62%|██████▏   | 88/141 [02:16<01:23,  1.58s/it]avg_loss = 1.8289693084995398:  63%|██████▎   | 89/141 [02:16<01:22,  1.58s/it]avg_loss = 1.83647294971678:  63%|██████▎   | 89/141 [02:18<01:22,  1.58s/it]  avg_loss = 1.83647294971678:  64%|██████▍   | 90/141 [02:18<01:20,  1.58s/it]avg_loss = 1.8396094723062202:  64%|██████▍   | 90/141 [02:19<01:20,  1.58s/it]avg_loss = 1.8396094723062202:  65%|██████▍   | 91/141 [02:19<01:19,  1.58s/it]avg_loss = 1.8446496349313986:  65%|██████▍   | 91/141 [02:21<01:19,  1.58s/it]avg_loss = 1.8446496349313986:  65%|██████▌   | 92/141 [02:21<01:17,  1.58s/it]avg_loss = 1.8496941815140426:  65%|██████▌   | 92/141 [02:23<01:17,  1.58s/it]avg_loss = 1.8496941815140426:  66%|██████▌   | 93/141 [02:23<01:16,  1.58s/it]avg_loss = 1.850749392458733:  66%|██████▌   | 93/141 [02:24<01:16,  1.58s/it] avg_loss = 1.850749392458733:  67%|██████▋   | 94/141 [02:24<01:14,  1.58s/it]avg_loss = 1.8546097567206934:  67%|██████▋   | 94/141 [02:26<01:14,  1.58s/it]avg_loss = 1.8546097567206934:  67%|██████▋   | 95/141 [02:26<01:12,  1.58s/it]avg_loss = 1.8556275280813377:  67%|██████▋   | 95/141 [02:27<01:12,  1.58s/it]avg_loss = 1.8556275280813377:  68%|██████▊   | 96/141 [02:27<01:11,  1.58s/it]avg_loss = 1.8575667413239627:  68%|██████▊   | 96/141 [02:29<01:11,  1.58s/it]avg_loss = 1.8575667413239627:  69%|██████▉   | 97/141 [02:29<01:09,  1.59s/it]avg_loss = 1.8527310563593495:  69%|██████▉   | 97/141 [02:31<01:09,  1.59s/it]avg_loss = 1.8527310563593495:  70%|██████▉   | 98/141 [02:31<01:08,  1.59s/it]avg_loss = 1.8534002075291642:  70%|██████▉   | 98/141 [02:32<01:08,  1.59s/it]avg_loss = 1.8534002075291642:  70%|███████   | 99/141 [02:32<01:06,  1.59s/it]avg_loss = 1.8553197944164277:  70%|███████   | 99/141 [02:34<01:06,  1.59s/it]avg_loss = 1.8553197944164277:  71%|███████   | 100/141 [02:34<01:05,  1.59s/it]avg_loss = 1.8538727701300441:  71%|███████   | 100/141 [02:35<01:05,  1.59s/it]avg_loss = 1.8538727701300441:  72%|███████▏  | 101/141 [02:35<01:03,  1.59s/it]avg_loss = 1.8539467594202828:  72%|███████▏  | 101/141 [02:37<01:03,  1.59s/it]avg_loss = 1.8539467594202828:  72%|███████▏  | 102/141 [02:37<01:01,  1.59s/it]avg_loss = 1.8519009131829716:  72%|███████▏  | 102/141 [02:38<01:01,  1.59s/it]avg_loss = 1.8519009131829716:  73%|███████▎  | 103/141 [02:38<01:00,  1.59s/it]avg_loss = 1.854262957206139:  73%|███████▎  | 103/141 [02:40<01:00,  1.59s/it] avg_loss = 1.854262957206139:  74%|███████▍  | 104/141 [02:40<00:58,  1.59s/it]avg_loss = 1.8524629093351819:  74%|███████▍  | 104/141 [02:42<00:58,  1.59s/it]avg_loss = 1.8524629093351819:  74%|███████▍  | 105/141 [02:42<00:57,  1.59s/it]avg_loss = 1.851133015920531:  74%|███████▍  | 105/141 [02:43<00:57,  1.59s/it] avg_loss = 1.851133015920531:  75%|███████▌  | 106/141 [02:43<00:55,  1.59s/it]avg_loss = 1.8487139193811148:  75%|███████▌  | 106/141 [02:45<00:55,  1.59s/it]avg_loss = 1.8487139193811148:  76%|███████▌  | 107/141 [02:45<00:54,  1.59s/it]avg_loss = 1.846391412946913:  76%|███████▌  | 107/141 [02:46<00:54,  1.59s/it] avg_loss = 1.846391412946913:  77%|███████▋  | 108/141 [02:46<00:52,  1.59s/it]avg_loss = 1.8437332232064063:  77%|███████▋  | 108/141 [02:48<00:52,  1.59s/it]avg_loss = 1.8437332232064063:  77%|███████▋  | 109/141 [02:48<00:50,  1.59s/it]avg_loss = 1.841251674565402:  77%|███████▋  | 109/141 [02:50<00:50,  1.59s/it] avg_loss = 1.841251674565402:  78%|███████▊  | 110/141 [02:50<00:49,  1.59s/it]avg_loss = 1.8436117773657446:  78%|███████▊  | 110/141 [02:51<00:49,  1.59s/it]avg_loss = 1.8436117773657446:  79%|███████▊  | 111/141 [02:51<00:47,  1.59s/it]avg_loss = 1.8434142885463578:  79%|███████▊  | 111/141 [02:53<00:47,  1.59s/it]avg_loss = 1.8434142885463578:  79%|███████▉  | 112/141 [02:53<00:46,  1.59s/it]avg_loss = 1.8446239718293722:  79%|███████▉  | 112/141 [02:54<00:46,  1.59s/it]avg_loss = 1.8446239718293722:  80%|████████  | 113/141 [02:54<00:44,  1.59s/it]avg_loss = 1.8456278757045144:  80%|████████  | 113/141 [02:56<00:44,  1.59s/it]avg_loss = 1.8456278757045144:  81%|████████  | 114/141 [02:56<00:42,  1.59s/it]avg_loss = 1.8450601536294688:  81%|████████  | 114/141 [02:58<00:42,  1.59s/it]avg_loss = 1.8450601536294688:  82%|████████▏ | 115/141 [02:58<00:41,  1.59s/it]avg_loss = 1.8435620686103558:  82%|████████▏ | 115/141 [02:59<00:41,  1.59s/it]avg_loss = 1.8435620686103558:  82%|████████▏ | 116/141 [02:59<00:39,  1.59s/it]avg_loss = 1.8456969709477873:  82%|████████▏ | 116/141 [03:01<00:39,  1.59s/it]avg_loss = 1.8456969709477873:  83%|████████▎ | 117/141 [03:01<00:38,  1.59s/it]avg_loss = 1.8454497345423295:  83%|████████▎ | 117/141 [03:02<00:38,  1.59s/it]avg_loss = 1.8454497345423295:  84%|████████▎ | 118/141 [03:02<00:36,  1.59s/it]avg_loss = 1.8440743654715914:  84%|████████▎ | 118/141 [03:04<00:36,  1.59s/it]avg_loss = 1.8440743654715914:  84%|████████▍ | 119/141 [03:04<00:35,  1.59s/it]avg_loss = 1.8423698077599207:  84%|████████▍ | 119/141 [03:06<00:35,  1.59s/it]avg_loss = 1.8423698077599207:  85%|████████▌ | 120/141 [03:06<00:33,  1.59s/it]avg_loss = 1.8422350597775672:  85%|████████▌ | 120/141 [03:07<00:33,  1.59s/it]avg_loss = 1.8422350597775672:  86%|████████▌ | 121/141 [03:07<00:31,  1.59s/it]avg_loss = 1.8426081938821761:  86%|████████▌ | 121/141 [03:09<00:31,  1.59s/it]avg_loss = 1.8426081938821761:  87%|████████▋ | 122/141 [03:09<00:30,  1.59s/it]avg_loss = 1.8425015395249777:  87%|████████▋ | 122/141 [03:10<00:30,  1.59s/it]avg_loss = 1.8425015395249777:  87%|████████▋ | 123/141 [03:10<00:28,  1.59s/it]avg_loss = 1.8427744267448303:  87%|████████▋ | 123/141 [03:12<00:28,  1.59s/it]avg_loss = 1.8427744267448303:  88%|████████▊ | 124/141 [03:12<00:27,  1.59s/it]avg_loss = 1.841574149131775:  88%|████████▊ | 124/141 [03:13<00:27,  1.59s/it] avg_loss = 1.841574149131775:  89%|████████▊ | 125/141 [03:13<00:25,  1.59s/it]avg_loss = 1.8420687544913519:  89%|████████▊ | 125/141 [03:15<00:25,  1.59s/it]avg_loss = 1.8420687544913519:  89%|████████▉ | 126/141 [03:15<00:23,  1.59s/it]avg_loss = 1.8418223397938285:  89%|████████▉ | 126/141 [03:17<00:23,  1.59s/it]avg_loss = 1.8418223397938285:  90%|█████████ | 127/141 [03:17<00:22,  1.59s/it]avg_loss = 1.840500358492136:  90%|█████████ | 127/141 [03:18<00:22,  1.59s/it] avg_loss = 1.840500358492136:  91%|█████████ | 128/141 [03:18<00:20,  1.59s/it]avg_loss = 1.8407575233962186:  91%|█████████ | 128/141 [03:20<00:20,  1.59s/it]avg_loss = 1.8407575233962186:  91%|█████████▏| 129/141 [03:20<00:19,  1.59s/it]avg_loss = 1.841496227337764:  91%|█████████▏| 129/141 [03:21<00:19,  1.59s/it] avg_loss = 1.841496227337764:  92%|█████████▏| 130/141 [03:21<00:17,  1.60s/it]avg_loss = 1.842448448406831:  92%|█████████▏| 130/141 [03:23<00:17,  1.60s/it]avg_loss = 1.842448448406831:  93%|█████████▎| 131/141 [03:23<00:15,  1.60s/it]avg_loss = 1.8431149052851128:  93%|█████████▎| 131/141 [03:25<00:15,  1.60s/it]avg_loss = 1.8431149052851128:  94%|█████████▎| 132/141 [03:25<00:14,  1.60s/it]avg_loss = 1.8403561375194923:  94%|█████████▎| 132/141 [03:26<00:14,  1.60s/it]avg_loss = 1.8403561375194923:  94%|█████████▍| 133/141 [03:26<00:12,  1.60s/it]avg_loss = 1.8360278757650461:  94%|█████████▍| 133/141 [03:28<00:12,  1.60s/it]avg_loss = 1.8360278757650461:  95%|█████████▌| 134/141 [03:28<00:11,  1.60s/it]avg_loss = 1.8386049385424013:  95%|█████████▌| 134/141 [03:29<00:11,  1.60s/it]avg_loss = 1.8386049385424013:  96%|█████████▌| 135/141 [03:29<00:09,  1.60s/it]avg_loss = 1.8420970115591497:  96%|█████████▌| 135/141 [03:31<00:09,  1.60s/it]avg_loss = 1.8420970115591497:  96%|█████████▋| 136/141 [03:31<00:07,  1.60s/it]avg_loss = 1.8430253554434672:  96%|█████████▋| 136/141 [03:33<00:07,  1.60s/it]avg_loss = 1.8430253554434672:  97%|█████████▋| 137/141 [03:33<00:06,  1.60s/it]avg_loss = 1.8417240044345027:  97%|█████████▋| 137/141 [03:34<00:06,  1.60s/it]avg_loss = 1.8417240044345027:  98%|█████████▊| 138/141 [03:34<00:04,  1.60s/it]avg_loss = 1.8418859492102972:  98%|█████████▊| 138/141 [03:36<00:04,  1.60s/it]avg_loss = 1.8418859492102972:  99%|█████████▊| 139/141 [03:36<00:03,  1.60s/it]avg_loss = 1.8424909727913992:  99%|█████████▊| 139/141 [03:37<00:03,  1.60s/it]avg_loss = 1.8424909727913992:  99%|█████████▉| 140/141 [03:37<00:01,  1.60s/it]avg_loss = 1.8437359062492424:  99%|█████████▉| 140/141 [03:39<00:01,  1.60s/it]avg_loss = 1.8437359062492424: 100%|██████████| 141/141 [03:39<00:00,  1.60s/it]avg_loss = 1.8437359062492424: 100%|██████████| 141/141 [03:39<00:00,  1.56s/it]
I0403 02:45:04.349992 3303856 eval_ppl.py:107] wikitext2 perplexity: 6.32010555267334
wikitext2 perplexity: 6.320
