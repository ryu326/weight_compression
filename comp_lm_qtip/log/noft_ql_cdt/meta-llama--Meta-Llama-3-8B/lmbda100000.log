I0401 09:11:10.420970 2934409 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:11:10.421065 2934409 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:11:10.421112 2934409 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:11:10.759879 2934409 config.py:54] PyTorch version 2.6.0 available.
W0401 09:11:10.963402 2934409 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:11:11.713755 2934409 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.54it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.19it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.49it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.68it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.73it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.80it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.84it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.65it/s]
I0401 09:11:13.233335 2934409 quantize_finetune_llama.py:152] loaded model
I0401 09:11:13.521750 2934409 quantize_finetune_llama.py:190] loaded compression model
I0401 09:11:31.402768 2934409 quantize_finetune_llama.py:194] loaded dataset and devset
I0401 09:11:36.456675 2934409 quantize_finetune_llama.py:214] layer 0 gpu 0
I0401 09:11:38.845507 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 0 in 2.2273342609405518s
tensor([0.]) tensor([0.])
tensor([0.]) tensor([0.])
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0401 09:11:51.079375 2934941 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:11:51.079468 2934941 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:11:51.079509 2934941 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:11:51.417617 2934941 config.py:54] PyTorch version 2.6.0 available.
W0401 09:11:51.610279 2934941 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:11:52.190741 2934941 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:11:52.195077 2934409 quantize_finetune_llama.py:214] layer 1 gpu 1
I0401 09:11:52.209735 2934941 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:11:54.665112 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 1 in 2.2989964485168457s
I0401 09:11:58.532209 2935079 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:11:58.532297 2935079 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:11:58.532336 2935079 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:11:58.874103 2935079 config.py:54] PyTorch version 2.6.0 available.
W0401 09:11:59.078009 2935079 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:11:59.693790 2935079 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:11:59.697755 2934409 quantize_finetune_llama.py:214] layer 2 gpu 2
I0401 09:11:59.714298 2935079 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:12:02.104085 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 2 in 2.237046241760254s
I0401 09:12:05.975708 2935240 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:12:05.975811 2935240 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:12:05.975868 2935240 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:12:06.357072 2935240 config.py:54] PyTorch version 2.6.0 available.
W0401 09:12:06.580068 2935240 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:12:07.329975 2935240 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:12:07.334134 2934409 quantize_finetune_llama.py:214] layer 3 gpu 3
I0401 09:12:07.352090 2935240 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:12:09.750748 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 3 in 2.232236623764038s
I0401 09:12:13.934579 2935392 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:12:13.934686 2935392 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:12:13.934731 2935392 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:12:14.328907 2935392 config.py:54] PyTorch version 2.6.0 available.
W0401 09:12:14.548848 2935392 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:12:15.263673 2935392 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:12:15.268216 2934409 quantize_finetune_llama.py:214] layer 4 gpu 0
I0401 09:12:15.300618 2935392 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
0_v proxy err 0.06948500126600266 tr(WHW.T) 60.88684844970703
bpp_loss 2.9401224851608276
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
0_q proxy err 0.0006264947587624192 tr(WHW.T) 288094.65625
bpp_loss 3.1378103494644165
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
0_k proxy err 0.0005411015590652823 tr(WHW.T) 100180.734375
bpp_loss 3.245353102684021
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
0_o proxy err 0.006097093690186739 tr(WHW.T) 3123.217529296875
bpp_loss 3.074485182762146
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
0_up proxy err 0.016328677535057068 tr(WHW.T) 8924.451171875
bpp_loss 3.015179770333426
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
0_gate proxy err 0.010077029466629028 tr(WHW.T) 15778.666015625
bpp_loss 3.088402884347098
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
0_down proxy err 0.012908021919429302 tr(WHW.T) 10818.205078125
bpp_loss 3.037165948322841
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
1_v proxy err 0.05484427511692047 tr(WHW.T) 109.07096099853516
bpp_loss 2.966712474822998
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
1_q proxy err 0.0005987798213027418 tr(WHW.T) 144826.484375
bpp_loss 3.3990975618362427
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
1_k proxy err 0.00037542416248470545 tr(WHW.T) 75539.046875
bpp_loss 3.7509044408798218
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
1_o proxy err 0.011843489482998848 tr(WHW.T) 1983.8321533203125
bpp_loss 3.0797393321990967
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
1_up proxy err 0.018000639975070953 tr(WHW.T) 8230.8203125
bpp_loss 3.0211312430245534
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
1_gate proxy err 0.011667133308947086 tr(WHW.T) 13952.365234375
bpp_loss 3.0884077889578685
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
1_down proxy err 0.01471767295151949 tr(WHW.T) 13978.03125
bpp_loss 3.0244292872292653
I0401 09:13:01.939454 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 4 in 0.8937444686889648s
I0401 09:13:05.889143 2935952 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:13:05.889251 2935952 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:13:05.889293 2935952 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:13:06.283206 2935952 config.py:54] PyTorch version 2.6.0 available.
W0401 09:13:06.501451 2935952 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:13:07.147238 2935952 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:13:07.151269 2934409 quantize_finetune_llama.py:214] layer 5 gpu 1
I0401 09:13:07.167749 2935952 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
2_v proxy err 0.027692152187228203 tr(WHW.T) 155.95950317382812
bpp_loss 2.9563562870025635
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
2_q proxy err 0.0017135442467406392 tr(WHW.T) 41475.15625
bpp_loss 3.422292709350586
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
2_k proxy err 0.0010832678526639938 tr(WHW.T) 22614.533203125
bpp_loss 3.9511417150497437
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
2_o proxy err 0.012013141997158527 tr(WHW.T) 1967.3226318359375
bpp_loss 2.9803959131240845
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
2_up proxy err 0.01947181113064289 tr(WHW.T) 7600.474609375
bpp_loss 3.015033040727888
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
2_gate proxy err 0.010892034508287907 tr(WHW.T) 15113.31640625
bpp_loss 3.1211444309779575
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
2_down proxy err 0.019122030586004257 tr(WHW.T) 7726.72509765625
bpp_loss 3.0186351367405484
I0401 09:13:09.575048 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 5 in 0.9179830551147461s
I0401 09:13:13.495533 2936100 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:13:13.495638 2936100 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:13:13.495680 2936100 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:13:13.857472 2936100 config.py:54] PyTorch version 2.6.0 available.
W0401 09:13:14.064077 2936100 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:13:14.678937 2936100 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:13:14.683096 2934409 quantize_finetune_llama.py:214] layer 6 gpu 2
I0401 09:13:14.701079 2936100 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
3_v proxy err 0.02105899713933468 tr(WHW.T) 289.3331604003906
bpp_loss 2.967828392982483
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
3_q proxy err 0.0014346475945785642 tr(WHW.T) 47576.9296875
bpp_loss 3.471384644508362
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
3_k proxy err 0.000868384086061269 tr(WHW.T) 26174.515625
bpp_loss 4.111658334732056
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
3_o proxy err 0.016445914283394814 tr(WHW.T) 1857.15771484375
bpp_loss 2.993197798728943
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
3_up proxy err 0.019028089940547943 tr(WHW.T) 7536.46484375
bpp_loss 3.0113113948277066
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
3_gate proxy err 0.008343632332980633 tr(WHW.T) 20884.34765625
bpp_loss 3.1679180690220425
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
3_down proxy err 0.020188147202134132 tr(WHW.T) 6998.6103515625
bpp_loss 3.016281638826643
I0401 09:13:17.502884 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 6 in 0.8232800960540771s
I0401 09:13:21.593453 2936266 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:13:21.593562 2936266 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:13:21.593610 2936266 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:13:21.974499 2936266 config.py:54] PyTorch version 2.6.0 available.
W0401 09:13:22.200284 2936266 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:13:22.851241 2936266 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:13:22.855389 2934409 quantize_finetune_llama.py:214] layer 7 gpu 3
I0401 09:13:22.872639 2936266 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:13:24.288954 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 7 in 0.9078664779663086s
I0401 09:13:28.273664 2936402 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:13:28.273769 2936402 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:13:28.273810 2936402 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:13:28.664821 2936402 config.py:54] PyTorch version 2.6.0 available.
W0401 09:13:28.889047 2936402 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:13:29.538130 2936402 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:13:29.542092 2934409 quantize_finetune_llama.py:214] layer 8 gpu 0
I0401 09:13:29.558201 2936402 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
4_v proxy err 0.024406377226114273 tr(WHW.T) 285.30712890625
bpp_loss 2.975010871887207
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
4_q proxy err 0.0013858370948582888 tr(WHW.T) 50114.859375
bpp_loss 3.434502124786377
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
4_k proxy err 0.0007694821688346565 tr(WHW.T) 29301.974609375
bpp_loss 4.096582412719727
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
4_o proxy err 0.02383573353290558 tr(WHW.T) 1297.2481689453125
bpp_loss 3.0044023990631104
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
4_up proxy err 0.018509089946746826 tr(WHW.T) 7383.48779296875
bpp_loss 3.009023530142648
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
4_gate proxy err 0.006296801380813122 tr(WHW.T) 29130.974609375
bpp_loss 3.21991879599435
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
4_down proxy err 0.021057533100247383 tr(WHW.T) 6395.72265625
bpp_loss 3.0135463987077986
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
5_v proxy err 0.023489266633987427 tr(WHW.T) 208.81988525390625
bpp_loss 2.959714412689209
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
5_q proxy err 0.0019059294136241078 tr(WHW.T) 35991.625
bpp_loss 3.416959524154663
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
5_k proxy err 0.0010307744378224015 tr(WHW.T) 22997.75390625
bpp_loss 3.986100912094116
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
5_o proxy err 0.026277247816324234 tr(WHW.T) 1055.5865478515625
bpp_loss 2.9814058542251587
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
5_up proxy err 0.017987895756959915 tr(WHW.T) 7656.43310546875
bpp_loss 3.0121849605015347
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
5_gate proxy err 0.0060042524710297585 tr(WHW.T) 30386.947265625
bpp_loss 3.2262656348092213
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
5_down proxy err 0.02117627114057541 tr(WHW.T) 6411.12939453125
bpp_loss 3.013885736465454
I0401 09:14:17.284605 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 8 in 1.0684735774993896s
I0401 09:14:21.679597 2936971 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:14:21.679706 2936971 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:14:21.679750 2936971 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:14:22.078933 2936971 config.py:54] PyTorch version 2.6.0 available.
W0401 09:14:22.297850 2936971 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:14:22.950560 2936971 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:14:22.954535 2934409 quantize_finetune_llama.py:214] layer 9 gpu 1
I0401 09:14:22.970046 2936971 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
6_v proxy err 0.022825812920928 tr(WHW.T) 253.63377380371094
bpp_loss 2.966663360595703
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
6_q proxy err 0.0018796578515321016 tr(WHW.T) 35639.5859375
bpp_loss 3.4670718908309937
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
6_k proxy err 0.0008242857875302434 tr(WHW.T) 26163.654296875
bpp_loss 4.196059703826904
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
6_o proxy err 0.028990771621465683 tr(WHW.T) 1009.1483764648438
bpp_loss 2.9980121850967407
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
6_up proxy err 0.01748090423643589 tr(WHW.T) 7923.53564453125
bpp_loss 3.009554590497698
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
6_gate proxy err 0.0051651387475430965 tr(WHW.T) 35748.39453125
bpp_loss 3.2278503690447127
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
6_down proxy err 0.020904462784528732 tr(WHW.T) 6483.5595703125
bpp_loss 3.0140509264809743
I0401 09:14:26.004535 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 9 in 0.8708353042602539s
I0401 09:14:29.909836 2937129 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:14:29.909927 2937129 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:14:29.909970 2937129 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
7_v proxy err 0.018527524545788765 tr(WHW.T) 309.4270935058594
bpp_loss 2.9660637378692627
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
7_q proxy err 0.0018846042221412063 tr(WHW.T) 35144.953125
bpp_loss 3.413602113723755
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
7_k proxy err 0.0007990317535586655 tr(WHW.T) 26850.4921875
bpp_loss 4.241278171539307
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
7_o proxy err 0.03195931762456894 tr(WHW.T) 959.2155151367188
bpp_loss 2.988706946372986
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
7_up proxy err 0.016461297869682312 tr(WHW.T) 8627.4501953125
bpp_loss 3.0109483173915317
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
7_gate proxy err 0.0052237967029213905 tr(WHW.T) 34903.53515625
bpp_loss 3.2085932322910855
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
7_down proxy err 0.021324411034584045 tr(WHW.T) 6534.6279296875
bpp_loss 3.012601341520037
I0401 09:14:30.248048 2937129 config.py:54] PyTorch version 2.6.0 available.
W0401 09:14:30.454495 2937129 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:14:31.059888 2937129 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:14:31.063706 2934409 quantize_finetune_llama.py:214] layer 10 gpu 2
I0401 09:14:31.079526 2937129 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:14:32.436115 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 10 in 0.8885953426361084s
I0401 09:14:36.298027 2937265 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:14:36.298132 2937265 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:14:36.298175 2937265 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:14:36.683600 2937265 config.py:54] PyTorch version 2.6.0 available.
W0401 09:14:36.906861 2937265 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:14:37.678590 2937265 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:14:37.682670 2934409 quantize_finetune_llama.py:214] layer 11 gpu 3
I0401 09:14:37.699249 2937265 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:14:39.155812 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 11 in 0.9815504550933838s
I0401 09:14:43.287916 2937416 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:14:43.288026 2937416 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:14:43.288068 2937416 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:14:43.692981 2937416 config.py:54] PyTorch version 2.6.0 available.
W0401 09:14:43.929967 2937416 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:14:44.696126 2937416 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:14:44.700449 2934409 quantize_finetune_llama.py:214] layer 12 gpu 0
I0401 09:14:44.721719 2937416 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
8_v proxy err 0.02388996072113514 tr(WHW.T) 257.7052307128906
bpp_loss 2.9660959243774414
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
8_q proxy err 0.0024833357892930508 tr(WHW.T) 26600.9765625
bpp_loss 3.389266610145569
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
8_k proxy err 0.0009866786422207952 tr(WHW.T) 22528.748046875
bpp_loss 4.06436014175415
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
8_o proxy err 0.04083035886287689 tr(WHW.T) 745.6061401367188
bpp_loss 3.0113024711608887
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
8_up proxy err 0.016577625647187233 tr(WHW.T) 8495.0546875
bpp_loss 3.010660171508789
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
8_gate proxy err 0.0049333395436406136 tr(WHW.T) 37238.1484375
bpp_loss 3.210430417742048
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
8_down proxy err 0.021138232201337814 tr(WHW.T) 6488.8173828125
bpp_loss 3.0189200469425748
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
9_v proxy err 0.02288217656314373 tr(WHW.T) 351.4288024902344
bpp_loss 2.9787116050720215
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
9_q proxy err 0.0025521274656057358 tr(WHW.T) 25653.232421875
bpp_loss 3.40480375289917
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
9_k proxy err 0.0010211816988885403 tr(WHW.T) 20942.638671875
bpp_loss 4.135081768035889
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
9_o proxy err 0.045803509652614594 tr(WHW.T) 776.6017456054688
bpp_loss 2.997573494911194
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
9_up proxy err 0.016002926975488663 tr(WHW.T) 8970.6455078125
bpp_loss 3.0101394653320312
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
9_gate proxy err 0.004718018230050802 tr(WHW.T) 39415.71875
bpp_loss 3.2192256110055104
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
9_down proxy err 0.021961061283946037 tr(WHW.T) 6272.90380859375
bpp_loss 3.020289182662964
I0401 09:15:32.940376 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 12 in 0.9370238780975342s
I0401 09:15:36.999265 2938003 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:15:36.999369 2938003 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:15:36.999410 2938003 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:15:37.394998 2938003 config.py:54] PyTorch version 2.6.0 available.
W0401 09:15:37.616135 2938003 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
10_v proxy err 0.02369951643049717 tr(WHW.T) 251.83889770507812
bpp_loss 2.9641029834747314
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
10_q proxy err 0.002869298914447427 tr(WHW.T) 23338.15625
bpp_loss 3.3975229263305664
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
10_k proxy err 0.0011696761939674616 tr(WHW.T) 19743.95703125
bpp_loss 4.076197147369385
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
10_o proxy err 0.044796478003263474 tr(WHW.T) 680.6640625
bpp_loss 2.995967388153076
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
10_up proxy err 0.016086850315332413 tr(WHW.T) 9199.1572265625
bpp_loss 3.011892454964774
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
10_gate proxy err 0.004854938946664333 tr(WHW.T) 37399.37109375
bpp_loss 3.2008282797677174
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
10_down proxy err 0.02189020998775959 tr(WHW.T) 6525.2333984375
bpp_loss 3.015690735408238
W0401 09:15:38.222213 2938003 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:15:38.226238 2934409 quantize_finetune_llama.py:214] layer 13 gpu 1
I0401 09:15:38.242240 2938003 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:15:40.220254 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 13 in 0.933887243270874s
I0401 09:15:44.215388 2938139 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:15:44.215487 2938139 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:15:44.215529 2938139 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:15:44.570796 2938139 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
11_v proxy err 0.018726401031017303 tr(WHW.T) 319.5691223144531
bpp_loss 2.968797206878662
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
11_q proxy err 0.002913401462137699 tr(WHW.T) 22190.8984375
bpp_loss 3.367430329322815
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
11_k proxy err 0.0011812594020739198 tr(WHW.T) 18027.423828125
bpp_loss 4.153406143188477
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
11_o proxy err 0.056823454797267914 tr(WHW.T) 565.9476928710938
bpp_loss 2.9900290966033936
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
11_up proxy err 0.01608584262430668 tr(WHW.T) 9192.595703125
bpp_loss 3.019961084638323
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
11_gate proxy err 0.004868213552981615 tr(WHW.T) 36884.09375
bpp_loss 3.186903272356306
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
11_down proxy err 0.021718185395002365 tr(WHW.T) 6655.890625
bpp_loss 3.0170351777757918
W0401 09:15:44.775444 2938139 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:15:45.363678 2938139 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:15:45.367599 2934409 quantize_finetune_llama.py:214] layer 14 gpu 2
I0401 09:15:45.386048 2938139 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:15:46.958523 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 14 in 1.112351417541504s
I0401 09:15:50.763441 2938282 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:15:50.763543 2938282 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:15:50.763586 2938282 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:15:51.130486 2938282 config.py:54] PyTorch version 2.6.0 available.
W0401 09:15:51.358119 2938282 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:15:51.995405 2938282 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:15:51.999422 2934409 quantize_finetune_llama.py:214] layer 15 gpu 3
I0401 09:15:52.015049 2938282 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:15:53.427459 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 15 in 0.9491839408874512s
I0401 09:15:57.503597 2938416 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:15:57.503703 2938416 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:15:57.503750 2938416 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:15:57.927440 2938416 config.py:54] PyTorch version 2.6.0 available.
W0401 09:15:58.159666 2938416 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:15:58.822375 2938416 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:15:58.826667 2934409 quantize_finetune_llama.py:214] layer 16 gpu 0
I0401 09:15:58.843775 2938416 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
12_v proxy err 0.021775266155600548 tr(WHW.T) 363.6233825683594
bpp_loss 2.988142967224121
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
12_q proxy err 0.0018566770013421774 tr(WHW.T) 34102.625
bpp_loss 3.429875373840332
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
12_k proxy err 0.0008928450988605618 tr(WHW.T) 23057.041015625
bpp_loss 4.2097413539886475
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
12_o proxy err 0.04547376185655594 tr(WHW.T) 779.7713012695312
bpp_loss 3.004072904586792
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
12_up proxy err 0.015133727341890335 tr(WHW.T) 10027.1552734375
bpp_loss 3.0290095465523854
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
12_gate proxy err 0.004740535747259855 tr(WHW.T) 37334.1015625
bpp_loss 3.178487913949149
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
12_down proxy err 0.021293185651302338 tr(WHW.T) 6822.7119140625
bpp_loss 3.033304589135306
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
13_v proxy err 0.02449694834649563 tr(WHW.T) 280.153564453125
bpp_loss 2.9779772758483887
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
13_q proxy err 0.003114757128059864 tr(WHW.T) 20897.375
bpp_loss 3.386734962463379
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
13_k proxy err 0.0012272748863324523 tr(WHW.T) 17798.916015625
bpp_loss 4.138641834259033
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
13_o proxy err 0.044013574719429016 tr(WHW.T) 675.2068481445312
bpp_loss 3.0824086666107178
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
13_up proxy err 0.015380078926682472 tr(WHW.T) 10010.2880859375
bpp_loss 3.0216827392578125
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
13_gate proxy err 0.00454017473384738 tr(WHW.T) 38921.26171875
bpp_loss 3.186279841831752
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
13_down proxy err 0.02210618183016777 tr(WHW.T) 6560.505859375
bpp_loss 3.0355570997510637
I0401 09:16:47.511831 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 16 in 0.9332153797149658s
I0401 09:16:51.522895 2939010 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:16:51.522997 2939010 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:16:51.523036 2939010 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:16:51.897966 2939010 config.py:54] PyTorch version 2.6.0 available.
W0401 09:16:52.110829 2939010 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
14_v proxy err 0.024257104843854904 tr(WHW.T) 281.3382873535156
bpp_loss 2.974085569381714
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
14_q proxy err 0.003022479824721813 tr(WHW.T) 20881.8359375
bpp_loss 3.374601364135742
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
14_k proxy err 0.0012752882903441787 tr(WHW.T) 18619.298828125
bpp_loss 3.99690043926239
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
14_o proxy err 0.04948174208402634 tr(WHW.T) 686.2976684570312
bpp_loss 2.992682933807373
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
14_up proxy err 0.016771048307418823 tr(WHW.T) 9164.287109375
bpp_loss 3.0178567341395786
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
14_gate proxy err 0.004348043818026781 tr(WHW.T) 41785.8203125
bpp_loss 3.2021923065185547
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
14_down proxy err 0.022934239357709885 tr(WHW.T) 6390.916015625
bpp_loss 3.024254390171596
W0401 09:16:52.734338 2939010 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:16:52.738352 2934409 quantize_finetune_llama.py:214] layer 17 gpu 1
I0401 09:16:52.755244 2939010 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:16:54.370866 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 17 in 0.9339287281036377s
I0401 09:16:58.350096 2939146 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:16:58.350197 2939146 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:16:58.350237 2939146 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
15_v proxy err 0.027547284960746765 tr(WHW.T) 284.0271301269531
bpp_loss 2.9817577600479126
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
15_q proxy err 0.0025026400107890368 tr(WHW.T) 28079.306640625
bpp_loss 3.4349557161331177
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
15_k proxy err 0.0010950659634545445 tr(WHW.T) 18871.66796875
bpp_loss 4.1941938400268555
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
15_o proxy err 0.04067779704928398 tr(WHW.T) 827.9279174804688
bpp_loss 3.03944993019104
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
15_up proxy err 0.016771312803030014 tr(WHW.T) 8994.5693359375
bpp_loss 3.020263535635812
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
15_gate proxy err 0.0040048579685389996 tr(WHW.T) 46217.96875
bpp_loss 3.227622849600656
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
15_down proxy err 0.022727014496922493 tr(WHW.T) 6399.38671875
bpp_loss 3.0235986709594727
I0401 09:16:58.689855 2939146 config.py:54] PyTorch version 2.6.0 available.
W0401 09:16:58.890373 2939146 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:16:59.458928 2939146 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:16:59.462693 2934409 quantize_finetune_llama.py:214] layer 18 gpu 2
I0401 09:16:59.477972 2939146 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:17:00.753414 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 18 in 0.8441011905670166s
I0401 09:17:04.534391 2939272 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:17:04.534486 2939272 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:17:04.534530 2939272 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:17:04.892780 2939272 config.py:54] PyTorch version 2.6.0 available.
W0401 09:17:05.129219 2939272 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:17:05.795214 2939272 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:17:05.799184 2934409 quantize_finetune_llama.py:214] layer 19 gpu 3
I0401 09:17:05.816350 2939272 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:17:07.297756 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 19 in 1.0012381076812744s
I0401 09:17:11.458114 2939438 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:17:11.458215 2939438 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:17:11.458258 2939438 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:17:11.863736 2939438 config.py:54] PyTorch version 2.6.0 available.
W0401 09:17:12.084104 2939438 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:17:12.740170 2939438 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:17:12.744855 2934409 quantize_finetune_llama.py:214] layer 20 gpu 0
I0401 09:17:12.767732 2939438 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
16_v proxy err 0.0255818422883749 tr(WHW.T) 274.28167724609375
bpp_loss 2.9774078130722046
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
16_q proxy err 0.002693337155506015 tr(WHW.T) 24486.64453125
bpp_loss 3.447232961654663
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
16_k proxy err 0.0011158129200339317 tr(WHW.T) 19506.357421875
bpp_loss 4.131958723068237
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
16_o proxy err 0.03235284984111786 tr(WHW.T) 969.0885620117188
bpp_loss 3.059592366218567
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
16_up proxy err 0.017809167504310608 tr(WHW.T) 8331.1826171875
bpp_loss 3.012409346444266
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
16_gate proxy err 0.0045874956995248795 tr(WHW.T) 41176.12890625
bpp_loss 3.2453453881399974
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
16_down proxy err 0.022356679663062096 tr(WHW.T) 6288.2802734375
bpp_loss 3.0277277401515414
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
17_v proxy err 0.029287856072187424 tr(WHW.T) 283.9730224609375
bpp_loss 2.9904755353927612
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
17_q proxy err 0.0024429119657725096 tr(WHW.T) 27571.44140625
bpp_loss 3.4470574855804443
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
17_k proxy err 0.0012477071722969413 tr(WHW.T) 17429.814453125
bpp_loss 4.157336711883545
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
17_o proxy err 0.03110590949654579 tr(WHW.T) 1106.6993408203125
bpp_loss 3.040327310562134
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
17_up proxy err 0.01742107793688774 tr(WHW.T) 8452.5419921875
bpp_loss 3.013019561767578
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
17_gate proxy err 0.004543635528534651 tr(WHW.T) 41717.5625
bpp_loss 3.258795601981027
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
17_down proxy err 0.02289869450032711 tr(WHW.T) 6212.31494140625
bpp_loss 3.0175556114741733
I0401 09:18:01.799879 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 20 in 1.1335666179656982s
I0401 09:18:05.771324 2940012 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:18:05.771415 2940012 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:18:05.771456 2940012 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
18_v proxy err 0.0240983497351408 tr(WHW.T) 287.61376953125
bpp_loss 2.975687623023987
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
18_q proxy err 0.002858393592759967 tr(WHW.T) 22399.498046875
bpp_loss 3.47098970413208
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
18_k proxy err 0.0012357402592897415 tr(WHW.T) 17394.810546875
bpp_loss 4.230484247207642
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
18_o proxy err 0.026963872835040092 tr(WHW.T) 1203.193603515625
bpp_loss 3.048089385032654
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
18_up proxy err 0.018285667523741722 tr(WHW.T) 7982.8837890625
bpp_loss 3.0116419110979353
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
18_gate proxy err 0.00531848706305027 tr(WHW.T) 35243.5546875
bpp_loss 3.2673749923706055
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
18_down proxy err 0.02258029580116272 tr(WHW.T) 6226.57861328125
bpp_loss 3.023664746965681
I0401 09:18:06.113085 2940012 config.py:54] PyTorch version 2.6.0 available.
W0401 09:18:06.317218 2940012 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:18:07.043046 2940012 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:18:07.046986 2934409 quantize_finetune_llama.py:214] layer 21 gpu 1
I0401 09:18:07.062414 2940012 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:18:08.392620 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 21 in 0.8665909767150879s
I0401 09:18:12.327566 2940146 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:18:12.327681 2940146 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:18:12.327723 2940146 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:18:12.688522 2940146 config.py:54] PyTorch version 2.6.0 available.
W0401 09:18:12.901198 2940146 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
19_v proxy err 0.022654982283711433 tr(WHW.T) 341.0596618652344
bpp_loss 2.9822744131088257
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
19_q proxy err 0.0026892530731856823 tr(WHW.T) 24038.03515625
bpp_loss 3.4650245904922485
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
19_k proxy err 0.001325304270721972 tr(WHW.T) 15553.287109375
bpp_loss 4.176132440567017
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
19_o proxy err 0.02866349369287491 tr(WHW.T) 1168.891845703125
bpp_loss 3.04877769947052
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
19_up proxy err 0.018875863403081894 tr(WHW.T) 7649.2919921875
bpp_loss 3.0124545778547014
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
19_gate proxy err 0.0057139284908771515 tr(WHW.T) 32811.99609375
bpp_loss 3.276213918413435
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
19_down proxy err 0.02245369553565979 tr(WHW.T) 6182.73583984375
bpp_loss 3.0277488231658936
W0401 09:18:13.539130 2940146 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:18:13.543070 2934409 quantize_finetune_llama.py:214] layer 22 gpu 2
I0401 09:18:13.557631 2940146 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:18:14.853272 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 22 in 0.8761656284332275s
I0401 09:18:18.591971 2940282 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:18:18.592067 2940282 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:18:18.592108 2940282 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:18:18.936205 2940282 config.py:54] PyTorch version 2.6.0 available.
W0401 09:18:19.150806 2940282 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:18:19.800050 2940282 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:18:19.803925 2934409 quantize_finetune_llama.py:214] layer 23 gpu 3
I0401 09:18:19.819547 2940282 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:18:21.284240 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 23 in 0.9757356643676758s
I0401 09:18:25.397572 2940438 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:18:25.397672 2940438 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:18:25.397713 2940438 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:18:25.794166 2940438 config.py:54] PyTorch version 2.6.0 available.
W0401 09:18:26.017691 2940438 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:18:26.688076 2940438 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:18:26.692332 2934409 quantize_finetune_llama.py:214] layer 24 gpu 0
I0401 09:18:26.710185 2940438 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
20_v proxy err 0.025137856602668762 tr(WHW.T) 330.192138671875
bpp_loss 2.9900225400924683
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
20_q proxy err 0.0030946878250688314 tr(WHW.T) 20737.7421875
bpp_loss 3.444287419319153
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
20_k proxy err 0.0013675399823114276 tr(WHW.T) 15395.810546875
bpp_loss 4.092097997665405
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
20_o proxy err 0.02697465382516384 tr(WHW.T) 1205.1229248046875
bpp_loss 3.051129102706909
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
20_up proxy err 0.019103990867733955 tr(WHW.T) 7599.658203125
bpp_loss 3.0132085255214145
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
20_gate proxy err 0.006096587050706148 tr(WHW.T) 30665.015625
bpp_loss 3.2773549216134206
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
20_down proxy err 0.022635100409388542 tr(WHW.T) 6300.8623046875
bpp_loss 3.0169172968183244
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
21_v proxy err 0.024324504658579826 tr(WHW.T) 362.87310791015625
bpp_loss 2.9941219091415405
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
21_q proxy err 0.0024387496523559093 tr(WHW.T) 25838.111328125
bpp_loss 3.450326919555664
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
21_k proxy err 0.0012032808735966682 tr(WHW.T) 16787.453125
bpp_loss 4.191705703735352
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
21_o proxy err 0.02741754800081253 tr(WHW.T) 1266.6689453125
bpp_loss 3.0365779399871826
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
21_up proxy err 0.01869403012096882 tr(WHW.T) 7772.57080078125
bpp_loss 3.0174737657819475
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
21_gate proxy err 0.005990752950310707 tr(WHW.T) 31566.3828125
bpp_loss 3.2810228892735074
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
21_down proxy err 0.022168083116412163 tr(WHW.T) 6351.46484375
bpp_loss 3.028263807296753
I0401 09:19:15.429659 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 24 in 1.0038437843322754s
I0401 09:19:19.367128 2941002 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:19:19.367226 2941002 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:19:19.367269 2941002 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
22_v proxy err 0.02770608477294445 tr(WHW.T) 346.0789489746094
bpp_loss 3.004784107208252
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
22_q proxy err 0.0030134127009660006 tr(WHW.T) 20319.60546875
bpp_loss 3.430124044418335
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
22_k proxy err 0.0013744019670411944 tr(WHW.T) 14724.09765625
bpp_loss 4.112794399261475
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
22_o proxy err 0.030216772109270096 tr(WHW.T) 1221.8309326171875
bpp_loss 3.027147650718689
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
22_up proxy err 0.019420921802520752 tr(WHW.T) 7547.25537109375
bpp_loss 3.0153357642037526
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
22_gate proxy err 0.006333559285849333 tr(WHW.T) 29534.099609375
bpp_loss 3.291741371154785
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
22_down proxy err 0.02204362489283085 tr(WHW.T) 6529.4912109375
bpp_loss 3.0189658233097623
I0401 09:19:19.720273 2941002 config.py:54] PyTorch version 2.6.0 available.
W0401 09:19:19.931106 2941002 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:19:20.565597 2941002 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:19:20.569561 2934409 quantize_finetune_llama.py:214] layer 25 gpu 1
I0401 09:19:20.586054 2941002 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:19:22.173846 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 25 in 1.1267731189727783s
I0401 09:19:26.100423 2941160 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:19:26.100521 2941160 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:19:26.100565 2941160 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
23_v proxy err 0.02567874826490879 tr(WHW.T) 397.90704345703125
bpp_loss 3.035476565361023
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
23_q proxy err 0.00271195569075644 tr(WHW.T) 22608.931640625
bpp_loss 3.4354125261306763
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
23_k proxy err 0.0013376655988395214 tr(WHW.T) 14857.474609375
bpp_loss 4.145060300827026
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
23_o proxy err 0.02266809530556202 tr(WHW.T) 1744.470947265625
bpp_loss 3.0129387378692627
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
23_up proxy err 0.01985456794500351 tr(WHW.T) 7419.2080078125
bpp_loss 3.017698287963867
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
23_gate proxy err 0.0068298098631203175 tr(WHW.T) 27291.65234375
bpp_loss 3.2942801884242465
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
23_down proxy err 0.0217654500156641 tr(WHW.T) 6666.9853515625
bpp_loss 3.0189263139452254
I0401 09:19:26.467683 2941160 config.py:54] PyTorch version 2.6.0 available.
W0401 09:19:26.668974 2941160 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:19:27.268920 2941160 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:19:27.272675 2934409 quantize_finetune_llama.py:214] layer 26 gpu 2
I0401 09:19:27.287393 2941160 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:19:28.655416 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 26 in 0.9213900566101074s
I0401 09:19:32.429004 2941297 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:19:32.429118 2941297 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:19:32.429165 2941297 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:19:32.812138 2941297 config.py:54] PyTorch version 2.6.0 available.
W0401 09:19:33.040795 2941297 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:19:33.680582 2941297 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:19:33.684555 2934409 quantize_finetune_llama.py:214] layer 27 gpu 3
I0401 09:19:33.700552 2941297 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:19:35.090542 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 27 in 0.9244551658630371s
I0401 09:19:39.092840 2941426 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:19:39.092943 2941426 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:19:39.092993 2941426 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:19:39.484467 2941426 config.py:54] PyTorch version 2.6.0 available.
W0401 09:19:39.704504 2941426 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:19:40.340929 2941426 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:19:40.345250 2934409 quantize_finetune_llama.py:214] layer 28 gpu 0
I0401 09:19:40.362061 2941426 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
24_v proxy err 0.023595662787556648 tr(WHW.T) 467.2783508300781
bpp_loss 3.102868914604187
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
24_q proxy err 0.0027367733418941498 tr(WHW.T) 22436.099609375
bpp_loss 3.404210090637207
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
24_k proxy err 0.0014698560116812587 tr(WHW.T) 14183.4345703125
bpp_loss 3.9002819061279297
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
24_o proxy err 0.026512840762734413 tr(WHW.T) 1589.4920654296875
bpp_loss 3.0246381759643555
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
24_up proxy err 0.020311376079916954 tr(WHW.T) 7311.1435546875
bpp_loss 3.016343797956194
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
24_gate proxy err 0.007206347305327654 tr(WHW.T) 25877.26171875
bpp_loss 3.3006899697440013
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
24_down proxy err 0.021796900779008865 tr(WHW.T) 6741.93701171875
bpp_loss 3.018921341214861
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
25_v proxy err 0.019791491329669952 tr(WHW.T) 557.8086547851562
bpp_loss 3.11453378200531
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
25_q proxy err 0.0023525527212768793 tr(WHW.T) 26112.029296875
bpp_loss 3.387209177017212
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
25_k proxy err 0.001402293797582388 tr(WHW.T) 14416.6201171875
bpp_loss 3.921836495399475
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
25_o proxy err 0.021430697292089462 tr(WHW.T) 1990.481201171875
bpp_loss 3.0264484882354736
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
25_up proxy err 0.02027874067425728 tr(WHW.T) 7382.19091796875
bpp_loss 3.0226944514683316
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
25_gate proxy err 0.007131936959922314 tr(WHW.T) 26307.435546875
bpp_loss 3.3084562846592496
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
25_down proxy err 0.02259455993771553 tr(WHW.T) 6615.951171875
bpp_loss 3.0200673512050082
I0401 09:20:28.915595 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 28 in 0.9624488353729248s
I0401 09:20:33.025347 2942017 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:20:33.025447 2942017 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:20:33.025488 2942017 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:20:33.404732 2942017 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
26_v proxy err 0.02636212483048439 tr(WHW.T) 434.54583740234375
bpp_loss 3.149216651916504
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
26_q proxy err 0.0028576930053532124 tr(WHW.T) 21403.28125
bpp_loss 3.3961349725723267
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
26_k proxy err 0.0013806655770167708 tr(WHW.T) 15425.5859375
bpp_loss 3.946657657623291
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
26_o proxy err 0.01758197695016861 tr(WHW.T) 2388.72412109375
bpp_loss 3.0582486391067505
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
26_up proxy err 0.019971493631601334 tr(WHW.T) 7645.65625
bpp_loss 3.020061765398298
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
26_gate proxy err 0.006539156194776297 tr(WHW.T) 28901.4453125
bpp_loss 3.3107673100062778
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
26_down proxy err 0.022819863632321358 tr(WHW.T) 6618.82470703125
bpp_loss 3.0210625444139754
W0401 09:20:33.620456 2942017 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:20:34.297999 2942017 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:20:34.301949 2934409 quantize_finetune_llama.py:214] layer 29 gpu 1
I0401 09:20:34.317481 2942017 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:20:35.665208 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 29 in 0.8828685283660889s
I0401 09:20:39.600351 2942148 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:20:39.600450 2942148 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:20:39.600495 2942148 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
27_v proxy err 0.018116086721420288 tr(WHW.T) 677.69384765625
bpp_loss 3.2070804834365845
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
27_q proxy err 0.002828184049576521 tr(WHW.T) 21305.96875
bpp_loss 3.37385892868042
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
27_k proxy err 0.0015665555838495493 tr(WHW.T) 14011.6201171875
bpp_loss 3.8802123069763184
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
27_o proxy err 0.01912316493690014 tr(WHW.T) 2159.9560546875
bpp_loss 3.1092106103897095
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
27_up proxy err 0.017927706241607666 tr(WHW.T) 8475.5947265625
bpp_loss 3.0414161682128906
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
27_gate proxy err 0.005813197232782841 tr(WHW.T) 32809.0390625
bpp_loss 3.318016733442034
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
27_down proxy err 0.02352258935570717 tr(WHW.T) 6543.35791015625
bpp_loss 3.0211965015956332
I0401 09:20:39.944132 2942148 config.py:54] PyTorch version 2.6.0 available.
W0401 09:20:40.150929 2942148 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:20:40.776783 2942148 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:20:40.780648 2934409 quantize_finetune_llama.py:214] layer 30 gpu 2
I0401 09:20:40.795545 2942148 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:20:42.117678 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 30 in 0.8935887813568115s
I0401 09:20:45.869365 2942282 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:20:45.869463 2942282 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:20:45.869506 2942282 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:20:46.217378 2942282 config.py:54] PyTorch version 2.6.0 available.
W0401 09:20:46.435180 2942282 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:20:47.218113 2942282 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:20:47.222112 2934409 quantize_finetune_llama.py:214] layer 31 gpu 3
I0401 09:20:47.238419 2942282 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0401 09:20:48.763131 2934409 quantize_finetune_llama.py:245] computed original embedding for layer 31 in 1.0458717346191406s
I0401 09:20:52.888083 2942433 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:20:52.888192 2942433 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:20:52.888233 2942433 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:20:53.275569 2942433 config.py:54] PyTorch version 2.6.0 available.
W0401 09:20:53.509324 2942433 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0401 09:20:54.204714 2942433 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0401 09:20:54.226542 2942433 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
28_v proxy err 0.02148820459842682 tr(WHW.T) 601.4844360351562
bpp_loss 3.2501760721206665
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
28_q proxy err 0.002616936108097434 tr(WHW.T) 23163.888671875
bpp_loss 3.383458137512207
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
28_k proxy err 0.001298598013818264 tr(WHW.T) 14994.2119140625
bpp_loss 3.929380774497986
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
28_o proxy err 0.016767755150794983 tr(WHW.T) 2511.445556640625
bpp_loss 3.127277135848999
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
28_up proxy err 0.01554590743035078 tr(WHW.T) 10246.0703125
bpp_loss 3.0392772129603793
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
28_gate proxy err 0.005347583908587694 tr(WHW.T) 35920.9609375
bpp_loss 3.3013440540858676
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
28_down proxy err 0.021765200421214104 tr(WHW.T) 7210.08740234375
bpp_loss 3.032491547720773
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
29_v proxy err 0.015380128286778927 tr(WHW.T) 850.4290161132812
bpp_loss 3.299038052558899
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
29_q proxy err 0.0029113711789250374 tr(WHW.T) 20653.2265625
bpp_loss 3.3785946369171143
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
29_k proxy err 0.0012193757575005293 tr(WHW.T) 16365.796875
bpp_loss 4.023668527603149
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
29_o proxy err 0.014901582151651382 tr(WHW.T) 3102.844970703125
bpp_loss 3.1157292127609253
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
29_up proxy err 0.012590688653290272 tr(WHW.T) 12868.505859375
bpp_loss 3.068056515284947
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
29_gate proxy err 0.005045151803642511 tr(WHW.T) 38364.47265625
bpp_loss 3.2917845589773997
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
29_down proxy err 0.021422140300273895 tr(WHW.T) 7470.5244140625
bpp_loss 3.0384847777230397
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
30_v proxy err 0.017776137217879295 tr(WHW.T) 863.060791015625
bpp_loss 3.488405466079712
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
30_q proxy err 0.002520541427657008 tr(WHW.T) 24050.6328125
bpp_loss 3.292641043663025
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
30_k proxy err 0.0015400040429085493 tr(WHW.T) 14030.5888671875
bpp_loss 3.6027532815933228
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
30_o proxy err 0.009902815334498882 tr(WHW.T) 4869.1748046875
bpp_loss 3.1873730421066284
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
30_up proxy err 0.0077523114159703255 tr(WHW.T) 21711.1953125
bpp_loss 3.085975102015904
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
30_gate proxy err 0.0038974429480731487 tr(WHW.T) 51954.453125
bpp_loss 3.317636489868164
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
30_down proxy err 0.018210982903838158 tr(WHW.T) 8810.1787109375
bpp_loss 3.0462790557316373
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
31_v proxy err 0.007826815359294415 tr(WHW.T) 1808.723876953125
bpp_loss 3.3495678901672363
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
31_q proxy err 0.001440939144231379 tr(WHW.T) 46175.66015625
bpp_loss 3.3991706371307373
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 64, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
31_k proxy err 0.0010328216012567282 tr(WHW.T) 20467.64453125
bpp_loss 3.844932794570923
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
31_o proxy err 0.020884796977043152 tr(WHW.T) 2213.58154296875
bpp_loss 3.1864384412765503
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
31_up proxy err 0.0027755708433687687 tr(WHW.T) 69019.4453125
bpp_loss 3.237742151532854
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 896, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
31_gate proxy err 0.0026264037005603313 tr(WHW.T) 144452.5625
bpp_loss 3.07270199911935
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
weight_block torch.Size([2048, 256, 16])
q_level torch.Size([2048, 1])
l_cdt torch.Size([2048, 7])
31_down proxy err 0.015994438901543617 tr(WHW.T) 9960.1884765625
bpp_loss 3.0943197863442555
I0401 09:22:04.393829 2943191 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:22:04.393986 2943191 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:22:04.394027 2943191 utils.py:162] NumExpr defaulting to 16 threads.
I0401 09:22:04.802332 2943191 config.py:54] PyTorch version 2.6.0 available.
W0401 09:22:05.022823 2943191 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0401 09:22:05.150492 2943191 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.31it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.01it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  4.52it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.62it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  5.12it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  5.71it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.42it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.30it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.56it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.44it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.05it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.13it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.99it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.30it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.55it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.34it/s]
I0401 09:22:09.219543 2943191 hfize_llama.py:153] loaded layer 0
I0401 09:22:09.948997 2943191 hfize_llama.py:153] loaded layer 1
I0401 09:22:10.701645 2943191 hfize_llama.py:153] loaded layer 2
I0401 09:22:11.445333 2943191 hfize_llama.py:153] loaded layer 3
I0401 09:22:12.237665 2943191 hfize_llama.py:153] loaded layer 4
I0401 09:22:13.015104 2943191 hfize_llama.py:153] loaded layer 5
I0401 09:22:13.786417 2943191 hfize_llama.py:153] loaded layer 6
I0401 09:22:14.516035 2943191 hfize_llama.py:153] loaded layer 7
I0401 09:22:15.280823 2943191 hfize_llama.py:153] loaded layer 8
I0401 09:22:16.046517 2943191 hfize_llama.py:153] loaded layer 9
I0401 09:22:16.790338 2943191 hfize_llama.py:153] loaded layer 10
I0401 09:22:17.539505 2943191 hfize_llama.py:153] loaded layer 11
I0401 09:22:18.296434 2943191 hfize_llama.py:153] loaded layer 12
I0401 09:22:19.071805 2943191 hfize_llama.py:153] loaded layer 13
I0401 09:22:19.791784 2943191 hfize_llama.py:153] loaded layer 14
I0401 09:22:20.527117 2943191 hfize_llama.py:153] loaded layer 15
I0401 09:22:21.198883 2943191 hfize_llama.py:153] loaded layer 16
I0401 09:22:21.883544 2943191 hfize_llama.py:153] loaded layer 17
I0401 09:22:22.608038 2943191 hfize_llama.py:153] loaded layer 18
I0401 09:22:23.338279 2943191 hfize_llama.py:153] loaded layer 19
I0401 09:22:24.052632 2943191 hfize_llama.py:153] loaded layer 20
I0401 09:22:24.785305 2943191 hfize_llama.py:153] loaded layer 21
I0401 09:22:25.530437 2943191 hfize_llama.py:153] loaded layer 22
I0401 09:22:26.248923 2943191 hfize_llama.py:153] loaded layer 23
I0401 09:22:26.957220 2943191 hfize_llama.py:153] loaded layer 24
I0401 09:22:27.654565 2943191 hfize_llama.py:153] loaded layer 25
I0401 09:22:28.323003 2943191 hfize_llama.py:153] loaded layer 26
I0401 09:22:28.999722 2943191 hfize_llama.py:153] loaded layer 27
I0401 09:22:29.681351 2943191 hfize_llama.py:153] loaded layer 28
I0401 09:22:30.388987 2943191 hfize_llama.py:153] loaded layer 29
I0401 09:22:31.070165 2943191 hfize_llama.py:153] loaded layer 30
I0401 09:22:31.724047 2943191 hfize_llama.py:153] loaded layer 31
I0401 09:22:31.724186 2943191 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.10s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:01<00:04,  1.10it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:02<00:03,  1.15it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:03<00:02,  1.18it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:04<00:01,  1.20it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:05<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:05<00:00,  1.27it/s]
I0401 09:23:04.952518 2943191 hfize_llama.py:167] successfully loaded hfized model
I0401 09:23:09.723409 2943978 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0401 09:23:09.723571 2943978 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0401 09:23:09.723612 2943978 utils.py:162] NumExpr defaulting to 16 threads.
W0401 09:23:10.169872 2943978 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0401 09:23:10.571384 2943978 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.10s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.05s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.05s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.06s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.02s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.05it/s]
I0401 09:23:17.334841 2943978 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.864010214805603:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.864010214805603:   1%|          | 1/141 [00:01<04:24,  1.89s/it]avg_loss = 2.1309673190116882:   1%|          | 1/141 [00:03<04:24,  1.89s/it]avg_loss = 2.1309673190116882:   1%|▏         | 2/141 [00:03<03:47,  1.64s/it]avg_loss = 2.2480749686559043:   1%|▏         | 2/141 [00:04<03:47,  1.64s/it]avg_loss = 2.2480749686559043:   2%|▏         | 3/141 [00:04<03:35,  1.56s/it]avg_loss = 2.1960032880306244:   2%|▏         | 3/141 [00:06<03:35,  1.56s/it]avg_loss = 2.1960032880306244:   3%|▎         | 4/141 [00:06<03:28,  1.52s/it]avg_loss = 2.1539118766784666:   3%|▎         | 4/141 [00:07<03:28,  1.52s/it]avg_loss = 2.1539118766784666:   4%|▎         | 5/141 [00:07<03:25,  1.51s/it]avg_loss = 2.054184138774872:   4%|▎         | 5/141 [00:09<03:25,  1.51s/it] avg_loss = 2.054184138774872:   4%|▍         | 6/141 [00:09<03:22,  1.50s/it]avg_loss = 1.9937167848859514:   4%|▍         | 6/141 [00:10<03:22,  1.50s/it]avg_loss = 1.9937167848859514:   5%|▍         | 7/141 [00:10<03:19,  1.49s/it]avg_loss = 1.9882832914590836:   5%|▍         | 7/141 [00:12<03:19,  1.49s/it]avg_loss = 1.9882832914590836:   6%|▌         | 8/141 [00:12<03:18,  1.49s/it]avg_loss = 2.0210766659842596:   6%|▌         | 8/141 [00:13<03:18,  1.49s/it]avg_loss = 2.0210766659842596:   6%|▋         | 9/141 [00:13<03:16,  1.49s/it]avg_loss = 2.0165674924850463:   6%|▋         | 9/141 [00:15<03:16,  1.49s/it]avg_loss = 2.0165674924850463:   7%|▋         | 10/141 [00:15<03:15,  1.49s/it]avg_loss = 2.005982854149558:   7%|▋         | 10/141 [00:16<03:15,  1.49s/it] avg_loss = 2.005982854149558:   8%|▊         | 11/141 [00:16<03:14,  1.49s/it]avg_loss = 2.027084251244863:   8%|▊         | 11/141 [00:18<03:14,  1.49s/it]avg_loss = 2.027084251244863:   9%|▊         | 12/141 [00:18<03:12,  1.50s/it]avg_loss = 2.0394872151888332:   9%|▊         | 12/141 [00:19<03:12,  1.50s/it]avg_loss = 2.0394872151888332:   9%|▉         | 13/141 [00:19<03:11,  1.50s/it]avg_loss = 2.054672360420227:   9%|▉         | 13/141 [00:21<03:11,  1.50s/it] avg_loss = 2.054672360420227:  10%|▉         | 14/141 [00:21<03:10,  1.50s/it]avg_loss = 2.0630947748819985:  10%|▉         | 14/141 [00:22<03:10,  1.50s/it]avg_loss = 2.0630947748819985:  11%|█         | 15/141 [00:22<03:09,  1.50s/it]avg_loss = 2.0852297991514206:  11%|█         | 15/141 [00:24<03:09,  1.50s/it]avg_loss = 2.0852297991514206:  11%|█▏        | 16/141 [00:24<03:08,  1.50s/it]avg_loss = 2.0877387102912452:  11%|█▏        | 16/141 [00:25<03:08,  1.50s/it]avg_loss = 2.0877387102912452:  12%|█▏        | 17/141 [00:25<03:06,  1.51s/it]avg_loss = 2.090621378686693:  12%|█▏        | 17/141 [00:27<03:06,  1.51s/it] avg_loss = 2.090621378686693:  13%|█▎        | 18/141 [00:27<03:05,  1.51s/it]avg_loss = 2.0808949721486947:  13%|█▎        | 18/141 [00:28<03:05,  1.51s/it]avg_loss = 2.0808949721486947:  13%|█▎        | 19/141 [00:28<03:04,  1.51s/it]avg_loss = 2.080240547657013:  13%|█▎        | 19/141 [00:30<03:04,  1.51s/it] avg_loss = 2.080240547657013:  14%|█▍        | 20/141 [00:30<03:03,  1.52s/it]avg_loss = 2.084272623062134:  14%|█▍        | 20/141 [00:31<03:03,  1.52s/it]avg_loss = 2.084272623062134:  15%|█▍        | 21/141 [00:31<03:02,  1.52s/it]avg_loss = 2.086638168855147:  15%|█▍        | 21/141 [00:33<03:02,  1.52s/it]avg_loss = 2.086638168855147:  16%|█▌        | 22/141 [00:33<03:00,  1.52s/it]avg_loss = 2.0889039039611816:  16%|█▌        | 22/141 [00:34<03:00,  1.52s/it]avg_loss = 2.0889039039611816:  16%|█▋        | 23/141 [00:34<02:59,  1.52s/it]avg_loss = 2.093439429998398:  16%|█▋        | 23/141 [00:36<02:59,  1.52s/it] avg_loss = 2.093439429998398:  17%|█▋        | 24/141 [00:36<02:58,  1.52s/it]avg_loss = 2.0999559688568117:  17%|█▋        | 24/141 [00:37<02:58,  1.52s/it]avg_loss = 2.0999559688568117:  18%|█▊        | 25/141 [00:37<02:57,  1.53s/it]avg_loss = 2.1109731197357178:  18%|█▊        | 25/141 [00:39<02:57,  1.53s/it]avg_loss = 2.1109731197357178:  18%|█▊        | 26/141 [00:39<02:55,  1.53s/it]avg_loss = 2.1230162249671087:  18%|█▊        | 26/141 [00:40<02:55,  1.53s/it]avg_loss = 2.1230162249671087:  19%|█▉        | 27/141 [00:40<02:54,  1.53s/it]avg_loss = 2.127412165914263:  19%|█▉        | 27/141 [00:42<02:54,  1.53s/it] avg_loss = 2.127412165914263:  20%|█▉        | 28/141 [00:42<02:53,  1.53s/it]avg_loss = 2.123527378871523:  20%|█▉        | 28/141 [00:44<02:53,  1.53s/it]avg_loss = 2.123527378871523:  21%|██        | 29/141 [00:44<02:51,  1.53s/it]avg_loss = 2.1138704379399615:  21%|██        | 29/141 [00:45<02:51,  1.53s/it]avg_loss = 2.1138704379399615:  21%|██▏       | 30/141 [00:45<02:50,  1.54s/it]avg_loss = 2.101024793040368:  21%|██▏       | 30/141 [00:47<02:50,  1.54s/it] avg_loss = 2.101024793040368:  22%|██▏       | 31/141 [00:47<02:49,  1.54s/it]avg_loss = 2.089794270694256:  22%|██▏       | 31/141 [00:48<02:49,  1.54s/it]avg_loss = 2.089794270694256:  23%|██▎       | 32/141 [00:48<02:47,  1.54s/it]avg_loss = 2.0873691674434776:  23%|██▎       | 32/141 [00:50<02:47,  1.54s/it]avg_loss = 2.0873691674434776:  23%|██▎       | 33/141 [00:50<02:46,  1.54s/it]avg_loss = 2.085240399136263:  23%|██▎       | 33/141 [00:51<02:46,  1.54s/it] avg_loss = 2.085240399136263:  24%|██▍       | 34/141 [00:51<02:45,  1.54s/it]avg_loss = 2.087604297910418:  24%|██▍       | 34/141 [00:53<02:45,  1.54s/it]avg_loss = 2.087604297910418:  25%|██▍       | 35/141 [00:53<02:43,  1.55s/it]avg_loss = 2.071357273393207:  25%|██▍       | 35/141 [00:54<02:43,  1.55s/it]avg_loss = 2.071357273393207:  26%|██▌       | 36/141 [00:54<02:42,  1.55s/it]avg_loss = 2.055481623958897:  26%|██▌       | 36/141 [00:56<02:42,  1.55s/it]avg_loss = 2.055481623958897:  26%|██▌       | 37/141 [00:56<02:41,  1.55s/it]avg_loss = 2.040470744434156:  26%|██▌       | 37/141 [00:58<02:41,  1.55s/it]avg_loss = 2.040470744434156:  27%|██▋       | 38/141 [00:58<02:39,  1.55s/it]avg_loss = 2.0259403632237363:  27%|██▋       | 38/141 [00:59<02:39,  1.55s/it]avg_loss = 2.0259403632237363:  28%|██▊       | 39/141 [00:59<02:38,  1.55s/it]avg_loss = 2.01737557053566:  28%|██▊       | 39/141 [01:01<02:38,  1.55s/it]  avg_loss = 2.01737557053566:  28%|██▊       | 40/141 [01:01<02:36,  1.55s/it]avg_loss = 2.023171256228191:  28%|██▊       | 40/141 [01:02<02:36,  1.55s/it]avg_loss = 2.023171256228191:  29%|██▉       | 41/141 [01:02<02:35,  1.55s/it]avg_loss = 2.038735168320792:  29%|██▉       | 41/141 [01:04<02:35,  1.55s/it]avg_loss = 2.038735168320792:  30%|██▉       | 42/141 [01:04<02:34,  1.56s/it]avg_loss = 2.0543483800666276:  30%|██▉       | 42/141 [01:05<02:34,  1.56s/it]avg_loss = 2.0543483800666276:  30%|███       | 43/141 [01:05<02:32,  1.56s/it]avg_loss = 2.0609467137943613:  30%|███       | 43/141 [01:07<02:32,  1.56s/it]avg_loss = 2.0609467137943613:  31%|███       | 44/141 [01:07<02:31,  1.56s/it]avg_loss = 2.0659241888258193:  31%|███       | 44/141 [01:08<02:31,  1.56s/it]avg_loss = 2.0659241888258193:  32%|███▏      | 45/141 [01:08<02:29,  1.56s/it]avg_loss = 2.069903902385546:  32%|███▏      | 45/141 [01:10<02:29,  1.56s/it] avg_loss = 2.069903902385546:  33%|███▎      | 46/141 [01:10<02:28,  1.56s/it]avg_loss = 2.075345110385976:  33%|███▎      | 46/141 [01:12<02:28,  1.56s/it]avg_loss = 2.075345110385976:  33%|███▎      | 47/141 [01:12<02:27,  1.56s/it]avg_loss = 2.07756673792998:  33%|███▎      | 47/141 [01:13<02:27,  1.56s/it] avg_loss = 2.07756673792998:  34%|███▍      | 48/141 [01:13<02:25,  1.57s/it]avg_loss = 2.076844040228396:  34%|███▍      | 48/141 [01:15<02:25,  1.57s/it]avg_loss = 2.076844040228396:  35%|███▍      | 49/141 [01:15<02:24,  1.57s/it]avg_loss = 2.076888332366943:  35%|███▍      | 49/141 [01:16<02:24,  1.57s/it]avg_loss = 2.076888332366943:  35%|███▌      | 50/141 [01:16<02:22,  1.57s/it]avg_loss = 2.071075109874501:  35%|███▌      | 50/141 [01:18<02:22,  1.57s/it]avg_loss = 2.071075109874501:  36%|███▌      | 51/141 [01:18<02:21,  1.57s/it]avg_loss = 2.0660372903713813:  36%|███▌      | 51/141 [01:19<02:21,  1.57s/it]avg_loss = 2.0660372903713813:  37%|███▋      | 52/141 [01:19<02:19,  1.57s/it]avg_loss = 2.0591387636256666:  37%|███▋      | 52/141 [01:21<02:19,  1.57s/it]avg_loss = 2.0591387636256666:  38%|███▊      | 53/141 [01:21<02:18,  1.57s/it]avg_loss = 2.055692191477175:  38%|███▊      | 53/141 [01:23<02:18,  1.57s/it] avg_loss = 2.055692191477175:  38%|███▊      | 54/141 [01:23<02:16,  1.57s/it]avg_loss = 2.0472365747798573:  38%|███▊      | 54/141 [01:24<02:16,  1.57s/it]avg_loss = 2.0472365747798573:  39%|███▉      | 55/141 [01:24<02:15,  1.57s/it]avg_loss = 2.038904375263623:  39%|███▉      | 55/141 [01:26<02:15,  1.57s/it] avg_loss = 2.038904375263623:  40%|███▉      | 56/141 [01:26<02:13,  1.57s/it]avg_loss = 2.0355825675161263:  40%|███▉      | 56/141 [01:27<02:13,  1.57s/it]avg_loss = 2.0355825675161263:  40%|████      | 57/141 [01:27<02:12,  1.57s/it]avg_loss = 2.0319533923576616:  40%|████      | 57/141 [01:29<02:12,  1.57s/it]avg_loss = 2.0319533923576616:  41%|████      | 58/141 [01:29<02:10,  1.57s/it]avg_loss = 2.034233089220726:  41%|████      | 58/141 [01:30<02:10,  1.57s/it] avg_loss = 2.034233089220726:  42%|████▏     | 59/141 [01:30<02:09,  1.57s/it]avg_loss = 2.0392834583918256:  42%|████▏     | 59/141 [01:32<02:09,  1.57s/it]avg_loss = 2.0392834583918256:  43%|████▎     | 60/141 [01:32<02:07,  1.58s/it]avg_loss = 2.0441033996519495:  43%|████▎     | 60/141 [01:34<02:07,  1.58s/it]avg_loss = 2.0441033996519495:  43%|████▎     | 61/141 [01:34<02:06,  1.58s/it]avg_loss = 2.0516046747084586:  43%|████▎     | 61/141 [01:35<02:06,  1.58s/it]avg_loss = 2.0516046747084586:  44%|████▍     | 62/141 [01:35<02:04,  1.58s/it]avg_loss = 2.043771774049789:  44%|████▍     | 62/141 [01:37<02:04,  1.58s/it] avg_loss = 2.043771774049789:  45%|████▍     | 63/141 [01:37<02:03,  1.58s/it]avg_loss = 2.041238082572818:  45%|████▍     | 63/141 [01:38<02:03,  1.58s/it]avg_loss = 2.041238082572818:  45%|████▌     | 64/141 [01:38<02:01,  1.58s/it]avg_loss = 2.0389978078695443:  45%|████▌     | 64/141 [01:40<02:01,  1.58s/it]avg_loss = 2.0389978078695443:  46%|████▌     | 65/141 [01:40<02:00,  1.58s/it]avg_loss = 2.033302012718085:  46%|████▌     | 65/141 [01:41<02:00,  1.58s/it] avg_loss = 2.033302012718085:  47%|████▋     | 66/141 [01:41<01:58,  1.58s/it]avg_loss = 2.0303622217320685:  47%|████▋     | 66/141 [01:43<01:58,  1.58s/it]avg_loss = 2.0303622217320685:  48%|████▊     | 67/141 [01:43<01:56,  1.58s/it]avg_loss = 2.02842938023455:  48%|████▊     | 67/141 [01:45<01:56,  1.58s/it]  avg_loss = 2.02842938023455:  48%|████▊     | 68/141 [01:45<01:55,  1.58s/it]avg_loss = 2.0263108291487764:  48%|████▊     | 68/141 [01:46<01:55,  1.58s/it]avg_loss = 2.0263108291487764:  49%|████▉     | 69/141 [01:46<01:53,  1.58s/it]avg_loss = 2.0274529576301576:  49%|████▉     | 69/141 [01:48<01:53,  1.58s/it]avg_loss = 2.0274529576301576:  50%|████▉     | 70/141 [01:48<01:52,  1.58s/it]avg_loss = 2.031307786283359:  50%|████▉     | 70/141 [01:49<01:52,  1.58s/it] avg_loss = 2.031307786283359:  50%|█████     | 71/141 [01:49<01:50,  1.58s/it]avg_loss = 2.034272321396404:  50%|█████     | 71/141 [01:51<01:50,  1.58s/it]avg_loss = 2.034272321396404:  51%|█████     | 72/141 [01:51<01:49,  1.58s/it]avg_loss = 2.032457949364022:  51%|█████     | 72/141 [01:53<01:49,  1.58s/it]avg_loss = 2.032457949364022:  52%|█████▏    | 73/141 [01:53<01:47,  1.58s/it]avg_loss = 2.0334939151196867:  52%|█████▏    | 73/141 [01:54<01:47,  1.58s/it]avg_loss = 2.0334939151196867:  52%|█████▏    | 74/141 [01:54<01:46,  1.58s/it]avg_loss = 2.033059040705363:  52%|█████▏    | 74/141 [01:56<01:46,  1.58s/it] avg_loss = 2.033059040705363:  53%|█████▎    | 75/141 [01:56<01:44,  1.58s/it]avg_loss = 2.031940635881926:  53%|█████▎    | 75/141 [01:57<01:44,  1.58s/it]avg_loss = 2.031940635881926:  54%|█████▍    | 76/141 [01:57<01:42,  1.58s/it]avg_loss = 2.032984780026721:  54%|█████▍    | 76/141 [01:59<01:42,  1.58s/it]avg_loss = 2.032984780026721:  55%|█████▍    | 77/141 [01:59<01:41,  1.58s/it]avg_loss = 2.0352677351389175:  55%|█████▍    | 77/141 [02:01<01:41,  1.58s/it]avg_loss = 2.0352677351389175:  55%|█████▌    | 78/141 [02:01<01:39,  1.59s/it]avg_loss = 2.039057701448851:  55%|█████▌    | 78/141 [02:02<01:39,  1.59s/it] avg_loss = 2.039057701448851:  56%|█████▌    | 79/141 [02:02<01:38,  1.59s/it]avg_loss = 2.0352787986397742:  56%|█████▌    | 79/141 [02:04<01:38,  1.59s/it]avg_loss = 2.0352787986397742:  57%|█████▋    | 80/141 [02:04<01:36,  1.59s/it]avg_loss = 2.0337072287076787:  57%|█████▋    | 80/141 [02:05<01:36,  1.59s/it]avg_loss = 2.0337072287076787:  57%|█████▋    | 81/141 [02:05<01:35,  1.58s/it]avg_loss = 2.0323911774449233:  57%|█████▋    | 81/141 [02:07<01:35,  1.58s/it]avg_loss = 2.0323911774449233:  58%|█████▊    | 82/141 [02:07<01:33,  1.58s/it]avg_loss = 2.030254533491939:  58%|█████▊    | 82/141 [02:08<01:33,  1.58s/it] avg_loss = 2.030254533491939:  59%|█████▉    | 83/141 [02:08<01:32,  1.59s/it]avg_loss = 2.0275761627015614:  59%|█████▉    | 83/141 [02:10<01:32,  1.59s/it]avg_loss = 2.0275761627015614:  60%|█████▉    | 84/141 [02:10<01:30,  1.59s/it]avg_loss = 2.0252394634134627:  60%|█████▉    | 84/141 [02:12<01:30,  1.59s/it]avg_loss = 2.0252394634134627:  60%|██████    | 85/141 [02:12<01:29,  1.59s/it]avg_loss = 2.0267877869827804:  60%|██████    | 85/141 [02:13<01:29,  1.59s/it]avg_loss = 2.0267877869827804:  61%|██████    | 86/141 [02:13<01:27,  1.59s/it]avg_loss = 2.028507450531269:  61%|██████    | 86/141 [02:15<01:27,  1.59s/it] avg_loss = 2.028507450531269:  62%|██████▏   | 87/141 [02:15<01:26,  1.59s/it]avg_loss = 2.029716270891103:  62%|██████▏   | 87/141 [02:16<01:26,  1.59s/it]avg_loss = 2.029716270891103:  62%|██████▏   | 88/141 [02:16<01:24,  1.59s/it]avg_loss = 2.038448541351918:  62%|██████▏   | 88/141 [02:18<01:24,  1.59s/it]avg_loss = 2.038448541351918:  63%|██████▎   | 89/141 [02:18<01:22,  1.59s/it]avg_loss = 2.045803956190745:  63%|██████▎   | 89/141 [02:20<01:22,  1.59s/it]avg_loss = 2.045803956190745:  64%|██████▍   | 90/141 [02:20<01:21,  1.59s/it]avg_loss = 2.0489209418768413:  64%|██████▍   | 90/141 [02:21<01:21,  1.59s/it]avg_loss = 2.0489209418768413:  65%|██████▍   | 91/141 [02:21<01:19,  1.59s/it]avg_loss = 2.0541369539240133:  65%|██████▍   | 91/141 [02:23<01:19,  1.59s/it]avg_loss = 2.0541369539240133:  65%|██████▌   | 92/141 [02:23<01:17,  1.59s/it]avg_loss = 2.0591044054236463:  65%|██████▌   | 92/141 [02:24<01:17,  1.59s/it]avg_loss = 2.0591044054236463:  66%|██████▌   | 93/141 [02:24<01:16,  1.59s/it]avg_loss = 2.0595157209863055:  66%|██████▌   | 93/141 [02:26<01:16,  1.59s/it]avg_loss = 2.0595157209863055:  67%|██████▋   | 94/141 [02:26<01:14,  1.59s/it]avg_loss = 2.0632434230101735:  67%|██████▋   | 94/141 [02:28<01:14,  1.59s/it]avg_loss = 2.0632434230101735:  67%|██████▋   | 95/141 [02:28<01:13,  1.59s/it]avg_loss = 2.0638609938323498:  67%|██████▋   | 95/141 [02:29<01:13,  1.59s/it]avg_loss = 2.0638609938323498:  68%|██████▊   | 96/141 [02:29<01:11,  1.59s/it]avg_loss = 2.065365915445937:  68%|██████▊   | 96/141 [02:31<01:11,  1.59s/it] avg_loss = 2.065365915445937:  69%|██████▉   | 97/141 [02:31<01:10,  1.59s/it]avg_loss = 2.0633866640986227:  69%|██████▉   | 97/141 [02:32<01:10,  1.59s/it]avg_loss = 2.0633866640986227:  70%|██████▉   | 98/141 [02:32<01:08,  1.59s/it]avg_loss = 2.064809380155621:  70%|██████▉   | 98/141 [02:34<01:08,  1.59s/it] avg_loss = 2.064809380155621:  70%|███████   | 99/141 [02:34<01:06,  1.59s/it]avg_loss = 2.0672711753845214:  70%|███████   | 99/141 [02:36<01:06,  1.59s/it]avg_loss = 2.0672711753845214:  71%|███████   | 100/141 [02:36<01:05,  1.60s/it]avg_loss = 2.0673924460269437:  71%|███████   | 100/141 [02:37<01:05,  1.60s/it]avg_loss = 2.0673924460269437:  72%|███████▏  | 101/141 [02:37<01:03,  1.60s/it]avg_loss = 2.067955818830752:  72%|███████▏  | 101/141 [02:39<01:03,  1.60s/it] avg_loss = 2.067955818830752:  72%|███████▏  | 102/141 [02:39<01:02,  1.59s/it]avg_loss = 2.067522347552105:  72%|███████▏  | 102/141 [02:40<01:02,  1.59s/it]avg_loss = 2.067522347552105:  73%|███████▎  | 103/141 [02:40<01:00,  1.59s/it]avg_loss = 2.071294454427866:  73%|███████▎  | 103/141 [02:42<01:00,  1.59s/it]avg_loss = 2.071294454427866:  74%|███████▍  | 104/141 [02:42<00:58,  1.59s/it]avg_loss = 2.0710560072036017:  74%|███████▍  | 104/141 [02:43<00:58,  1.59s/it]avg_loss = 2.0710560072036017:  74%|███████▍  | 105/141 [02:43<00:57,  1.59s/it]avg_loss = 2.0707374968618715:  74%|███████▍  | 105/141 [02:45<00:57,  1.59s/it]avg_loss = 2.0707374968618715:  75%|███████▌  | 106/141 [02:45<00:55,  1.59s/it]avg_loss = 2.0690000614273214:  75%|███████▌  | 106/141 [02:47<00:55,  1.59s/it]avg_loss = 2.0690000614273214:  76%|███████▌  | 107/141 [02:47<00:54,  1.59s/it]avg_loss = 2.0671932763523526:  76%|███████▌  | 107/141 [02:48<00:54,  1.59s/it]avg_loss = 2.0671932763523526:  77%|███████▋  | 108/141 [02:48<00:52,  1.59s/it]avg_loss = 2.0657038918328943:  77%|███████▋  | 108/141 [02:50<00:52,  1.59s/it]avg_loss = 2.0657038918328943:  77%|███████▋  | 109/141 [02:50<00:51,  1.60s/it]avg_loss = 2.063396435434168:  77%|███████▋  | 109/141 [02:51<00:51,  1.60s/it] avg_loss = 2.063396435434168:  78%|███████▊  | 110/141 [02:51<00:49,  1.60s/it]avg_loss = 2.065699619215888:  78%|███████▊  | 110/141 [02:53<00:49,  1.60s/it]avg_loss = 2.065699619215888:  79%|███████▊  | 111/141 [02:53<00:47,  1.60s/it]avg_loss = 2.0651674707021033:  79%|███████▊  | 111/141 [02:55<00:47,  1.60s/it]avg_loss = 2.0651674707021033:  79%|███████▉  | 112/141 [02:55<00:46,  1.59s/it]avg_loss = 2.0660735685213476:  79%|███████▉  | 112/141 [02:56<00:46,  1.59s/it]avg_loss = 2.0660735685213476:  80%|████████  | 113/141 [02:56<00:44,  1.59s/it]avg_loss = 2.0671400260507014:  80%|████████  | 113/141 [02:58<00:44,  1.59s/it]avg_loss = 2.0671400260507014:  81%|████████  | 114/141 [02:58<00:43,  1.60s/it]avg_loss = 2.066189111833987:  81%|████████  | 114/141 [02:59<00:43,  1.60s/it] avg_loss = 2.066189111833987:  82%|████████▏ | 115/141 [02:59<00:41,  1.60s/it]avg_loss = 2.065421243166101:  82%|████████▏ | 115/141 [03:01<00:41,  1.60s/it]avg_loss = 2.065421243166101:  82%|████████▏ | 116/141 [03:01<00:39,  1.60s/it]avg_loss = 2.0676665886854515:  82%|████████▏ | 116/141 [03:03<00:39,  1.60s/it]avg_loss = 2.0676665886854515:  83%|████████▎ | 117/141 [03:03<00:38,  1.60s/it]avg_loss = 2.0672739895723633:  83%|████████▎ | 117/141 [03:04<00:38,  1.60s/it]avg_loss = 2.0672739895723633:  84%|████████▎ | 118/141 [03:04<00:36,  1.60s/it]avg_loss = 2.06562918675046:  84%|████████▎ | 118/141 [03:06<00:36,  1.60s/it]  avg_loss = 2.06562918675046:  84%|████████▍ | 119/141 [03:06<00:35,  1.60s/it]avg_loss = 2.0634403149286906:  84%|████████▍ | 119/141 [03:07<00:35,  1.60s/it]avg_loss = 2.0634403149286906:  85%|████████▌ | 120/141 [03:07<00:33,  1.60s/it]avg_loss = 2.063390390932067:  85%|████████▌ | 120/141 [03:09<00:33,  1.60s/it] avg_loss = 2.063390390932067:  86%|████████▌ | 121/141 [03:09<00:31,  1.60s/it]avg_loss = 2.0641190790739214:  86%|████████▌ | 121/141 [03:11<00:31,  1.60s/it]avg_loss = 2.0641190790739214:  87%|████████▋ | 122/141 [03:11<00:30,  1.60s/it]avg_loss = 2.063679280319834:  87%|████████▋ | 122/141 [03:12<00:30,  1.60s/it] avg_loss = 2.063679280319834:  87%|████████▋ | 123/141 [03:12<00:28,  1.60s/it]avg_loss = 2.063826261028167:  87%|████████▋ | 123/141 [03:14<00:28,  1.60s/it]avg_loss = 2.063826261028167:  88%|████████▊ | 124/141 [03:14<00:27,  1.60s/it]avg_loss = 2.0622175722122194:  88%|████████▊ | 124/141 [03:15<00:27,  1.60s/it]avg_loss = 2.0622175722122194:  89%|████████▊ | 125/141 [03:15<00:25,  1.60s/it]avg_loss = 2.0623633114118425:  89%|████████▊ | 125/141 [03:17<00:25,  1.60s/it]avg_loss = 2.0623633114118425:  89%|████████▉ | 126/141 [03:17<00:23,  1.60s/it]avg_loss = 2.061842488491629:  89%|████████▉ | 126/141 [03:19<00:23,  1.60s/it] avg_loss = 2.061842488491629:  90%|█████████ | 127/141 [03:19<00:22,  1.60s/it]avg_loss = 2.0605756621807814:  90%|█████████ | 127/141 [03:20<00:22,  1.60s/it]avg_loss = 2.0605756621807814:  91%|█████████ | 128/141 [03:20<00:20,  1.60s/it]avg_loss = 2.060506207074306:  91%|█████████ | 128/141 [03:22<00:20,  1.60s/it] avg_loss = 2.060506207074306:  91%|█████████▏| 129/141 [03:22<00:19,  1.60s/it]avg_loss = 2.061484749500568:  91%|█████████▏| 129/141 [03:23<00:19,  1.60s/it]avg_loss = 2.061484749500568:  92%|█████████▏| 130/141 [03:23<00:17,  1.59s/it]avg_loss = 2.062450266976393:  92%|█████████▏| 130/141 [03:25<00:17,  1.59s/it]avg_loss = 2.062450266976393:  93%|█████████▎| 131/141 [03:25<00:15,  1.59s/it]avg_loss = 2.0630376555702905:  93%|█████████▎| 131/141 [03:27<00:15,  1.59s/it]avg_loss = 2.0630376555702905:  94%|█████████▎| 132/141 [03:27<00:14,  1.59s/it]avg_loss = 2.05988275467005:  94%|█████████▎| 132/141 [03:28<00:14,  1.59s/it]  avg_loss = 2.05988275467005:  94%|█████████▍| 133/141 [03:28<00:12,  1.59s/it]avg_loss = 2.0549456468269005:  94%|█████████▍| 133/141 [03:30<00:12,  1.59s/it]avg_loss = 2.0549456468269005:  95%|█████████▌| 134/141 [03:30<00:11,  1.59s/it]avg_loss = 2.057151771474768:  95%|█████████▌| 134/141 [03:31<00:11,  1.59s/it] avg_loss = 2.057151771474768:  96%|█████████▌| 135/141 [03:31<00:09,  1.59s/it]avg_loss = 2.0606318326557385:  96%|█████████▌| 135/141 [03:33<00:09,  1.59s/it]avg_loss = 2.0606318326557385:  96%|█████████▋| 136/141 [03:33<00:07,  1.59s/it]avg_loss = 2.0624508753310153:  96%|█████████▋| 136/141 [03:35<00:07,  1.59s/it]avg_loss = 2.0624508753310153:  97%|█████████▋| 137/141 [03:35<00:06,  1.59s/it]avg_loss = 2.061525133208952:  97%|█████████▋| 137/141 [03:36<00:06,  1.59s/it] avg_loss = 2.061525133208952:  98%|█████████▊| 138/141 [03:36<00:04,  1.59s/it]avg_loss = 2.062326969002648:  98%|█████████▊| 138/141 [03:38<00:04,  1.59s/it]avg_loss = 2.062326969002648:  99%|█████████▊| 139/141 [03:38<00:03,  1.59s/it]avg_loss = 2.0633100313799724:  99%|█████████▊| 139/141 [03:39<00:03,  1.59s/it]avg_loss = 2.0633100313799724:  99%|█████████▉| 140/141 [03:39<00:01,  1.59s/it]avg_loss = 2.064538631878846:  99%|█████████▉| 140/141 [03:41<00:01,  1.59s/it] avg_loss = 2.064538631878846: 100%|██████████| 141/141 [03:41<00:00,  1.59s/it]avg_loss = 2.064538631878846: 100%|██████████| 141/141 [03:41<00:00,  1.57s/it]
I0401 09:27:23.150109 2943978 eval_ppl.py:107] wikitext2 perplexity: 7.881661415100098
wikitext2 perplexity: 7.882
