I0320 19:03:33.484079 1523378 config.py:54] PyTorch version 2.6.0 available.
W0320 19:03:33.764002 1523378 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:03:34.735331 1523378 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.48it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.98it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.13it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.23it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.30it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.98it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.04it/s]
I0320 19:03:35.686295 1523378 quantize_finetune_llama.py:144] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:14,  2.11it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:12,  2.32it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:11,  2.45it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:11,  2.49it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:10,  2.55it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:13,  1.90it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:15,  1.58it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:16,  1.47it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:05<00:16,  1.41it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:16,  1.37it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:15,  1.34it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:07<00:15,  1.31it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:08<00:14,  1.31it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:09<00:13,  1.29it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:09<00:13,  1.28it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:10<00:12,  1.28it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:11<00:11,  1.28it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:12<00:10,  1.28it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:10,  1.27it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:13<00:09,  1.27it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:14<00:08,  1.28it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:07,  1.27it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:07,  1.27it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:06,  1.27it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:05,  1.28it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.28it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:19<00:03,  1.28it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:20<00:03,  1.28it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:02,  1.29it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.29it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:22<00:00,  1.29it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.29it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.38it/s]
I0320 19:04:07.699942 1523378 quantize_finetune_llama.py:179] loaded compression model
I0320 19:04:23.569195 1523378 quantize_finetune_llama.py:183] loaded dataset and devset
I0320 19:04:29.123905 1523378 quantize_finetune_llama.py:203] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:05:42.764540 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 0 in 73.47321724891663s
tensor(0.0192) tensor(-3.6338e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0320 19:06:04.687409 1523695 config.py:54] PyTorch version 2.6.0 available.
W0320 19:06:04.969424 1523695 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:06:05.846248 1523695 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:06:05.850301 1523378 quantize_finetune_llama.py:203] layer 1 gpu 1
I0320 19:06:05.863782 1523695 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:06:17.716372 1523695 finetune.py:45] layer 0_v initial loss 1.40496713356697e-05
W0320 19:06:17.716699 1523695 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:06:51.866560 1523695 finetune.py:68] layer 0_v @ epoch 0 new loss 2.8144497719040373e-06 old loss 1.40496713356697e-05 BETTER
I0320 19:07:19.682531 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 1 in 73.66053342819214s
I0320 19:07:29.671691 1523695 finetune.py:68] layer 0_v @ epoch 1 new loss 1.1426963055782835e-06 old loss 2.8144497719040373e-06 BETTER
I0320 19:07:30.907676 1523865 config.py:54] PyTorch version 2.6.0 available.
W0320 19:07:31.269906 1523865 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:07:32.314139 1523865 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:07:32.318230 1523378 quantize_finetune_llama.py:203] layer 2 gpu 2
I0320 19:07:32.333150 1523865 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:07:45.161900 1523865 finetune.py:45] layer 1_v initial loss 0.0004491885774768889
W0320 19:07:45.162236 1523865 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:08:05.419148 1523695 finetune.py:68] layer 0_v @ epoch 2 new loss 7.428636763506802e-07 old loss 1.1426963055782835e-06 BETTER
I0320 19:08:17.807909 1523865 finetune.py:68] layer 1_v @ epoch 0 new loss 6.928208313183859e-05 old loss 0.0004491885774768889 BETTER
I0320 19:08:41.380464 1523695 finetune.py:68] layer 0_v @ epoch 3 new loss 6.125841309767566e-07 old loss 7.428636763506802e-07 BETTER
I0320 19:08:49.010610 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 2 in 76.47971725463867s
I0320 19:08:56.167387 1523865 finetune.py:68] layer 1_v @ epoch 1 new loss 4.179954339633696e-05 old loss 6.928208313183859e-05 BETTER
I0320 19:09:02.318509 1524044 config.py:54] PyTorch version 2.6.0 available.
W0320 19:09:02.721481 1524044 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:09:03.896665 1524044 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:09:03.901079 1523378 quantize_finetune_llama.py:203] layer 3 gpu 3
I0320 19:09:03.917547 1524044 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:09:18.848717 1524044 finetune.py:45] layer 2_v initial loss 1.9173146938555874e-05
W0320 19:09:18.849886 1524044 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:09:20.548110 1523695 finetune.py:68] layer 0_v @ epoch 4 new loss 5.461557179842202e-07 old loss 6.125841309767566e-07 BETTER
I0320 19:09:30.739316 1523865 finetune.py:68] layer 1_v @ epoch 2 new loss 2.8719110559904948e-05 old loss 4.179954339633696e-05 BETTER
I0320 19:09:36.808064 1523695 finetune.py:45] layer 0_q initial loss 6.301082748905174e-07
I0320 19:09:52.645402 1524044 finetune.py:68] layer 2_v @ epoch 0 new loss 1.374028215650469e-05 old loss 1.9173146938555874e-05 BETTER
I0320 19:10:06.358784 1523865 finetune.py:68] layer 1_v @ epoch 3 new loss 2.0597417460521683e-05 old loss 2.8719110559904948e-05 BETTER
I0320 19:10:12.187494 1523695 finetune.py:68] layer 0_q @ epoch 0 new loss 5.068224027127144e-07 old loss 6.301082748905174e-07 BETTER
I0320 19:10:27.731998 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 3 in 83.55959367752075s
I0320 19:10:27.828030 1524044 finetune.py:68] layer 2_v @ epoch 1 new loss 1.1071880180679727e-05 old loss 1.374028215650469e-05 BETTER
I0320 19:10:42.720283 1524235 config.py:54] PyTorch version 2.6.0 available.
W0320 19:10:43.229656 1524235 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0320 19:10:43.572528 1523865 finetune.py:68] layer 1_v @ epoch 4 new loss 1.626800985832233e-05 old loss 2.0597417460521683e-05 BETTER
W0320 19:10:44.369289 1524235 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:10:44.374009 1523378 quantize_finetune_llama.py:203] layer 4 gpu 0
I0320 19:10:44.390488 1524235 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 19:10:50.252782 1523695 finetune.py:68] layer 0_q @ epoch 1 new loss 4.5745449028800067e-07 old loss 5.068224027127144e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:11:00.858287 1523865 finetune.py:45] layer 1_q initial loss 1.701407745713368e-05
I0320 19:11:01.637633 1524235 finetune.py:45] layer 3_v initial loss 2.7625383154372685e-05
W0320 19:11:01.638041 1524235 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:11:07.157584 1524044 finetune.py:68] layer 2_v @ epoch 2 new loss 9.573825082043186e-06 old loss 1.1071880180679727e-05 BETTER
I0320 19:11:27.006222 1523695 finetune.py:68] layer 0_q @ epoch 2 new loss 4.226201895107806e-07 old loss 4.5745449028800067e-07 BETTER
I0320 19:11:36.126817 1523865 finetune.py:68] layer 1_q @ epoch 0 new loss 1.256704217666993e-05 old loss 1.701407745713368e-05 BETTER
I0320 19:11:36.573669 1524235 finetune.py:68] layer 3_v @ epoch 0 new loss 1.7661606761976145e-05 old loss 2.7625383154372685e-05 BETTER
I0320 19:11:43.372644 1524044 finetune.py:68] layer 2_v @ epoch 3 new loss 8.632404387753922e-06 old loss 9.573825082043186e-06 BETTER
I0320 19:12:04.397644 1523695 finetune.py:68] layer 0_q @ epoch 3 new loss 3.938999668662291e-07 old loss 4.226201895107806e-07 BETTER
I0320 19:12:13.167119 1523865 finetune.py:68] layer 1_q @ epoch 1 new loss 1.1275216820649803e-05 old loss 1.256704217666993e-05 BETTER
I0320 19:12:13.359992 1524235 finetune.py:68] layer 3_v @ epoch 1 new loss 1.4273231499828398e-05 old loss 1.7661606761976145e-05 BETTER
I0320 19:12:19.962931 1524044 finetune.py:68] layer 2_v @ epoch 4 new loss 7.990862286533229e-06 old loss 8.632404387753922e-06 BETTER
I0320 19:12:38.238422 1524044 finetune.py:45] layer 2_q initial loss 9.190065611619502e-06
I0320 19:12:42.301448 1523695 finetune.py:68] layer 0_q @ epoch 4 new loss 3.687960656861833e-07 old loss 3.938999668662291e-07 BETTER
I0320 19:12:48.817359 1523865 finetune.py:68] layer 1_q @ epoch 2 new loss 8.964291737356689e-06 old loss 1.1275216820649803e-05 BETTER
I0320 19:12:49.205663 1524235 finetune.py:68] layer 3_v @ epoch 2 new loss 1.2740234524244443e-05 old loss 1.4273231499828398e-05 BETTER
I0320 19:13:00.476482 1523695 finetune.py:45] layer 0_k initial loss 4.448415324986854e-07
I0320 19:13:12.654821 1524044 finetune.py:68] layer 2_q @ epoch 0 new loss 7.948059646878392e-06 old loss 9.190065611619502e-06 BETTER
I0320 19:13:25.524330 1523865 finetune.py:76] layer 1_q @ epoch 3 new loss 9.656610927777365e-06 old loss 8.964291737356689e-06 WORSE
I0320 19:13:26.267685 1524235 finetune.py:68] layer 3_v @ epoch 3 new loss 1.1829616596514825e-05 old loss 1.2740234524244443e-05 BETTER
I0320 19:13:36.459678 1523695 finetune.py:68] layer 0_k @ epoch 0 new loss 3.7719857459705963e-07 old loss 4.448415324986854e-07 BETTER
I0320 19:13:48.868601 1524044 finetune.py:68] layer 2_q @ epoch 1 new loss 7.483073659386719e-06 old loss 7.948059646878392e-06 BETTER
I0320 19:14:00.715140 1523865 finetune.py:76] layer 1_q @ epoch 4 new loss 9.517801117908675e-06 old loss 8.964291737356689e-06 WORSE
I0320 19:14:02.464298 1524235 finetune.py:68] layer 3_v @ epoch 4 new loss 1.1174558494531084e-05 old loss 1.1829616596514825e-05 BETTER
I0320 19:14:12.948634 1523695 finetune.py:68] layer 0_k @ epoch 1 new loss 3.528666070451436e-07 old loss 3.7719857459705963e-07 BETTER
I0320 19:14:16.917372 1523865 finetune.py:45] layer 1_k initial loss 9.921421224134974e-06
I0320 19:14:22.273892 1524235 finetune.py:45] layer 3_q initial loss 1.3769684301223606e-05
I0320 19:14:25.354741 1524044 finetune.py:68] layer 2_q @ epoch 2 new loss 7.132281098165549e-06 old loss 7.483073659386719e-06 BETTER
I0320 19:14:50.556113 1523695 finetune.py:68] layer 0_k @ epoch 2 new loss 3.344150911743782e-07 old loss 3.528666070451436e-07 BETTER
I0320 19:14:52.076718 1523865 finetune.py:76] layer 1_k @ epoch 0 new loss 1.025798155751545e-05 old loss 9.921421224134974e-06 WORSE
I0320 19:14:57.101874 1524235 finetune.py:68] layer 3_q @ epoch 0 new loss 1.2155011972936336e-05 old loss 1.3769684301223606e-05 BETTER
I0320 19:15:01.891830 1524044 finetune.py:68] layer 2_q @ epoch 3 new loss 6.842082257207949e-06 old loss 7.132281098165549e-06 BETTER
I0320 19:15:27.645878 1523865 finetune.py:68] layer 1_k @ epoch 1 new loss 8.440199962933548e-06 old loss 9.921421224134974e-06 BETTER
I0320 19:15:28.628048 1523695 finetune.py:68] layer 0_k @ epoch 3 new loss 3.1965484481588646e-07 old loss 3.344150911743782e-07 BETTER
I0320 19:15:33.211808 1524235 finetune.py:68] layer 3_q @ epoch 1 new loss 1.1555564014997799e-05 old loss 1.2155011972936336e-05 BETTER
I0320 19:15:38.529662 1524044 finetune.py:68] layer 2_q @ epoch 4 new loss 6.593922989850398e-06 old loss 6.842082257207949e-06 BETTER
I0320 19:15:56.099578 1524044 finetune.py:45] layer 2_k initial loss 7.499258572352119e-06
I0320 19:16:03.819298 1523865 finetune.py:68] layer 1_k @ epoch 2 new loss 8.270142643596046e-06 old loss 8.440199962933548e-06 BETTER
I0320 19:16:06.481474 1523695 finetune.py:68] layer 0_k @ epoch 4 new loss 3.0506146231346065e-07 old loss 3.1965484481588646e-07 BETTER
I0320 19:16:09.187797 1524235 finetune.py:68] layer 3_q @ epoch 2 new loss 1.1102872122137342e-05 old loss 1.1555564014997799e-05 BETTER
I0320 19:16:22.380534 1523695 finetune.py:45] layer 0_o initial loss 2.4662963369337376e-06
I0320 19:16:30.261448 1524044 finetune.py:68] layer 2_k @ epoch 0 new loss 7.075798293953994e-06 old loss 7.499258572352119e-06 BETTER
I0320 19:16:39.845073 1523865 finetune.py:76] layer 1_k @ epoch 3 new loss 9.261211744160391e-06 old loss 8.270142643596046e-06 WORSE
I0320 19:16:44.660373 1524235 finetune.py:68] layer 3_q @ epoch 3 new loss 1.0733760063885711e-05 old loss 1.1102872122137342e-05 BETTER
I0320 19:16:57.245193 1523695 finetune.py:68] layer 0_o @ epoch 0 new loss 2.2182373413670575e-06 old loss 2.4662963369337376e-06 BETTER
I0320 19:17:05.895244 1524044 finetune.py:68] layer 2_k @ epoch 1 new loss 6.8277190621302e-06 old loss 7.075798293953994e-06 BETTER
I0320 19:17:14.417550 1523865 finetune.py:68] layer 1_k @ epoch 4 new loss 8.241729119617958e-06 old loss 8.270142643596046e-06 BETTER
I0320 19:17:19.439314 1524235 finetune.py:68] layer 3_q @ epoch 4 new loss 1.0426615517644677e-05 old loss 1.0733760063885711e-05 BETTER
I0320 19:17:31.220494 1523865 finetune.py:45] layer 1_o initial loss 8.298738248413429e-05
I0320 19:17:34.519592 1523695 finetune.py:68] layer 0_o @ epoch 1 new loss 2.0259542452549795e-06 old loss 2.2182373413670575e-06 BETTER
I0320 19:17:40.079582 1524235 finetune.py:45] layer 3_k initial loss 1.2691976735368371e-05
I0320 19:17:42.466166 1524044 finetune.py:68] layer 2_k @ epoch 2 new loss 6.63045375404181e-06 old loss 6.8277190621302e-06 BETTER
I0320 19:18:05.730089 1523865 finetune.py:68] layer 1_o @ epoch 0 new loss 2.612770549603738e-05 old loss 8.298738248413429e-05 BETTER
I0320 19:18:11.986800 1523695 finetune.py:68] layer 0_o @ epoch 2 new loss 1.8713645886236918e-06 old loss 2.0259542452549795e-06 BETTER
I0320 19:18:14.894764 1524235 finetune.py:68] layer 3_k @ epoch 0 new loss 1.1899178389285225e-05 old loss 1.2691976735368371e-05 BETTER
I0320 19:18:19.076960 1524044 finetune.py:68] layer 2_k @ epoch 3 new loss 6.459843007178279e-06 old loss 6.63045375404181e-06 BETTER
I0320 19:18:41.110574 1523865 finetune.py:68] layer 1_o @ epoch 1 new loss 2.19516168726841e-05 old loss 2.612770549603738e-05 BETTER
I0320 19:18:49.829463 1523695 finetune.py:68] layer 0_o @ epoch 3 new loss 1.7416235778000555e-06 old loss 1.8713645886236918e-06 BETTER
I0320 19:18:51.387438 1524235 finetune.py:68] layer 3_k @ epoch 1 new loss 1.1569429261726327e-05 old loss 1.1899178389285225e-05 BETTER
I0320 19:18:55.325490 1524044 finetune.py:68] layer 2_k @ epoch 4 new loss 6.309129275905434e-06 old loss 6.459843007178279e-06 BETTER
I0320 19:19:14.012147 1524044 finetune.py:45] layer 2_o initial loss 1.3849844435753766e-05
I0320 19:19:16.603070 1523865 finetune.py:68] layer 1_o @ epoch 2 new loss 2.0707928342744708e-05 old loss 2.19516168726841e-05 BETTER
I0320 19:19:27.932141 1524235 finetune.py:68] layer 3_k @ epoch 2 new loss 1.1308666216791607e-05 old loss 1.1569429261726327e-05 BETTER
I0320 19:19:27.996941 1523695 finetune.py:68] layer 0_o @ epoch 4 new loss 1.6305879171341076e-06 old loss 1.7416235778000555e-06 BETTER
I0320 19:19:48.426164 1524044 finetune.py:68] layer 2_o @ epoch 0 new loss 1.3630272405862343e-05 old loss 1.3849844435753766e-05 BETTER
I0320 19:19:51.662714 1523865 finetune.py:68] layer 1_o @ epoch 3 new loss 1.9547465853975154e-05 old loss 2.0707928342744708e-05 BETTER
I0320 19:19:57.326459 1523695 finetune.py:45] layer 0_up initial loss 1.844722078203631e-06
I0320 19:20:03.068809 1524235 finetune.py:68] layer 3_k @ epoch 3 new loss 1.1078428542532492e-05 old loss 1.1308666216791607e-05 BETTER
I0320 19:20:24.709953 1524044 finetune.py:68] layer 2_o @ epoch 1 new loss 1.3467039025272243e-05 old loss 1.3630272405862343e-05 BETTER
I0320 19:20:27.672640 1523865 finetune.py:68] layer 1_o @ epoch 4 new loss 1.8583383280201815e-05 old loss 1.9547465853975154e-05 BETTER
I0320 19:20:30.657346 1523695 finetune.py:68] layer 0_up @ epoch 0 new loss 1.721151988931524e-06 old loss 1.844722078203631e-06 BETTER
I0320 19:20:38.324378 1524235 finetune.py:68] layer 3_k @ epoch 4 new loss 1.0878627108468208e-05 old loss 1.1078428542532492e-05 BETTER
I0320 19:20:56.961080 1524235 finetune.py:45] layer 3_o initial loss 2.7210386178921908e-05
I0320 19:20:58.455037 1523865 finetune.py:45] layer 1_up initial loss 0.00016951926227193326
I0320 19:21:01.563077 1524044 finetune.py:68] layer 2_o @ epoch 2 new loss 1.3329342436918523e-05 old loss 1.3467039025272243e-05 BETTER
I0320 19:21:05.358765 1523695 finetune.py:68] layer 0_up @ epoch 1 new loss 1.6263669522231794e-06 old loss 1.721151988931524e-06 BETTER
I0320 19:21:31.309437 1524235 finetune.py:68] layer 3_o @ epoch 0 new loss 2.625018169055693e-05 old loss 2.7210386178921908e-05 BETTER
I0320 19:21:31.337531 1523865 finetune.py:68] layer 1_up @ epoch 0 new loss 6.50125730317086e-05 old loss 0.00016951926227193326 BETTER
I0320 19:21:38.241743 1524044 finetune.py:68] layer 2_o @ epoch 3 new loss 1.3209753888077103e-05 old loss 1.3329342436918523e-05 BETTER
I0320 19:21:40.709173 1523695 finetune.py:68] layer 0_up @ epoch 2 new loss 1.5490379610128002e-06 old loss 1.6263669522231794e-06 BETTER
I0320 19:22:05.554542 1523865 finetune.py:68] layer 1_up @ epoch 1 new loss 2.8217818908160552e-05 old loss 6.50125730317086e-05 BETTER
I0320 19:22:06.550316 1524235 finetune.py:68] layer 3_o @ epoch 1 new loss 2.5669984097476117e-05 old loss 2.625018169055693e-05 BETTER
I0320 19:22:14.762090 1524044 finetune.py:68] layer 2_o @ epoch 4 new loss 1.3102277080179192e-05 old loss 1.3209753888077103e-05 BETTER
I0320 19:22:16.673547 1523695 finetune.py:68] layer 0_up @ epoch 3 new loss 1.4821099512118963e-06 old loss 1.5490379610128002e-06 BETTER
I0320 19:22:39.059589 1523865 finetune.py:68] layer 1_up @ epoch 2 new loss 2.1534573534154333e-05 old loss 2.8217818908160552e-05 BETTER
I0320 19:22:42.105771 1524235 finetune.py:68] layer 3_o @ epoch 2 new loss 2.52431673288811e-05 old loss 2.5669984097476117e-05 BETTER
I0320 19:22:47.568067 1524044 finetune.py:45] layer 2_up initial loss 1.7853151803137735e-05
I0320 19:22:52.072582 1523695 finetune.py:68] layer 0_up @ epoch 4 new loss 1.4237980394682381e-06 old loss 1.4821099512118963e-06 BETTER
I0320 19:23:12.569096 1523865 finetune.py:68] layer 1_up @ epoch 3 new loss 2.0077943190699443e-05 old loss 2.1534573534154333e-05 BETTER
I0320 19:23:18.080376 1524235 finetune.py:68] layer 3_o @ epoch 3 new loss 2.4893099180189893e-05 old loss 2.52431673288811e-05 BETTER
I0320 19:23:20.501330 1524044 finetune.py:68] layer 2_up @ epoch 0 new loss 1.7716110960463993e-05 old loss 1.7853151803137735e-05 BETTER
I0320 19:23:22.500728 1523695 finetune.py:45] layer 0_gate initial loss 1.6062743952716119e-06
I0320 19:23:47.257266 1523865 finetune.py:68] layer 1_up @ epoch 4 new loss 1.9599279767135158e-05 old loss 2.0077943190699443e-05 BETTER
I0320 19:23:54.071262 1524235 finetune.py:68] layer 3_o @ epoch 4 new loss 2.460291034367401e-05 old loss 2.4893099180189893e-05 BETTER
I0320 19:23:54.820671 1523695 finetune.py:68] layer 0_gate @ epoch 0 new loss 1.5274526958819479e-06 old loss 1.6062743952716119e-06 BETTER
I0320 19:23:55.077995 1524044 finetune.py:68] layer 2_up @ epoch 1 new loss 1.7613387171877548e-05 old loss 1.7716110960463993e-05 BETTER
I0320 19:24:16.947586 1523865 finetune.py:45] layer 1_gate initial loss 0.0001587670558365062
I0320 19:24:26.423213 1524235 finetune.py:45] layer 3_up initial loss 3.598590410547331e-05
I0320 19:24:29.171168 1523695 finetune.py:68] layer 0_gate @ epoch 1 new loss 1.4709688684888533e-06 old loss 1.5274526958819479e-06 BETTER
I0320 19:24:30.099210 1524044 finetune.py:68] layer 2_up @ epoch 2 new loss 1.752566640789155e-05 old loss 1.7613387171877548e-05 BETTER
I0320 19:24:47.531028 1523865 finetune.py:68] layer 1_gate @ epoch 0 new loss 7.676555105717853e-05 old loss 0.0001587670558365062 BETTER
I0320 19:24:59.511394 1524235 finetune.py:68] layer 3_up @ epoch 0 new loss 3.561487028491683e-05 old loss 3.598590410547331e-05 BETTER
I0320 19:25:03.678524 1523695 finetune.py:68] layer 0_gate @ epoch 2 new loss 1.4265147001424339e-06 old loss 1.4709688684888533e-06 BETTER
I0320 19:25:05.072999 1524044 finetune.py:68] layer 2_up @ epoch 3 new loss 1.7446576748625375e-05 old loss 1.752566640789155e-05 BETTER
I0320 19:25:19.352308 1523865 finetune.py:68] layer 1_gate @ epoch 1 new loss 3.942181501770392e-05 old loss 7.676555105717853e-05 BETTER
I0320 19:25:34.102340 1524235 finetune.py:68] layer 3_up @ epoch 1 new loss 3.5345248761586845e-05 old loss 3.561487028491683e-05 BETTER
I0320 19:25:38.115537 1523695 finetune.py:68] layer 0_gate @ epoch 3 new loss 1.3894194808017346e-06 old loss 1.4265147001424339e-06 BETTER
I0320 19:25:39.842855 1524044 finetune.py:68] layer 2_up @ epoch 4 new loss 1.7373833543388173e-05 old loss 1.7446576748625375e-05 BETTER
I0320 19:25:51.684742 1523865 finetune.py:68] layer 1_gate @ epoch 2 new loss 2.726459024415817e-05 old loss 3.942181501770392e-05 BETTER
I0320 19:26:08.238069 1524235 finetune.py:68] layer 3_up @ epoch 2 new loss 3.5119410313200206e-05 old loss 3.5345248761586845e-05 BETTER
I0320 19:26:12.811695 1523695 finetune.py:68] layer 0_gate @ epoch 4 new loss 1.3576257060776697e-06 old loss 1.3894194808017346e-06 BETTER
I0320 19:26:13.663873 1524044 finetune.py:45] layer 2_gate initial loss 2.0868894353043288e-05
I0320 19:26:23.044114 1523865 finetune.py:68] layer 1_gate @ epoch 3 new loss 2.4048891646089032e-05 old loss 2.726459024415817e-05 BETTER
I0320 19:26:42.744559 1524235 finetune.py:68] layer 3_up @ epoch 3 new loss 3.4917738958029076e-05 old loss 3.5119410313200206e-05 BETTER
I0320 19:26:45.157980 1524044 finetune.py:68] layer 2_gate @ epoch 0 new loss 2.0758739992743358e-05 old loss 2.0868894353043288e-05 BETTER
I0320 19:26:46.289472 1523695 finetune.py:45] layer 0_down initial loss 2.4829419089655858e-06
I0320 19:26:55.525020 1523865 finetune.py:68] layer 1_gate @ epoch 4 new loss 2.311153002665378e-05 old loss 2.4048891646089032e-05 BETTER
I0320 19:27:17.343949 1523695 finetune.py:68] layer 0_down @ epoch 0 new loss 2.4638004560983973e-06 old loss 2.4829419089655858e-06 BETTER
I0320 19:27:17.426944 1524235 finetune.py:68] layer 3_up @ epoch 4 new loss 3.473235119599849e-05 old loss 3.4917738958029076e-05 BETTER
I0320 19:27:18.204407 1524044 finetune.py:68] layer 2_gate @ epoch 1 new loss 2.06781714950921e-05 old loss 2.0758739992743358e-05 BETTER
I0320 19:27:28.299249 1523865 finetune.py:45] layer 1_down initial loss 0.002972659654915333
I0320 19:27:50.325167 1523695 finetune.py:68] layer 0_down @ epoch 1 new loss 2.4562150429119356e-06 old loss 2.4638004560983973e-06 BETTER
I0320 19:27:51.612828 1524044 finetune.py:68] layer 2_gate @ epoch 2 new loss 2.060977931250818e-05 old loss 2.06781714950921e-05 BETTER
I0320 19:27:52.111423 1524235 finetune.py:45] layer 3_gate initial loss 4.2966061300830916e-05
I0320 19:27:58.253437 1523865 finetune.py:68] layer 1_down @ epoch 0 new loss 0.002926889806985855 old loss 0.002972659654915333 BETTER
I0320 19:28:24.106584 1523695 finetune.py:68] layer 0_down @ epoch 2 new loss 2.452808985253796e-06 old loss 2.4562150429119356e-06 BETTER
I0320 19:28:24.405659 1524235 finetune.py:68] layer 3_gate @ epoch 0 new loss 4.272603473509662e-05 old loss 4.2966061300830916e-05 BETTER
I0320 19:28:25.771476 1524044 finetune.py:68] layer 2_gate @ epoch 3 new loss 2.05495634872932e-05 old loss 2.060977931250818e-05 BETTER
I0320 19:28:29.403394 1523865 finetune.py:68] layer 1_down @ epoch 1 new loss 0.0029138883110135794 old loss 0.002926889806985855 BETTER
I0320 19:28:57.651029 1523695 finetune.py:68] layer 0_down @ epoch 3 new loss 2.4504324755980633e-06 old loss 2.452808985253796e-06 BETTER
I0320 19:28:57.892213 1524235 finetune.py:68] layer 3_gate @ epoch 1 new loss 4.254198574926704e-05 old loss 4.272603473509662e-05 BETTER
I0320 19:28:59.731337 1524044 finetune.py:68] layer 2_gate @ epoch 4 new loss 2.049401700787712e-05 old loss 2.05495634872932e-05 BETTER
I0320 19:29:01.367349 1523865 finetune.py:68] layer 1_down @ epoch 2 new loss 0.0029114852659404278 old loss 0.0029138883110135794 BETTER
I0320 19:29:30.262623 1523695 finetune.py:68] layer 0_down @ epoch 4 new loss 2.448735358484555e-06 old loss 2.4504324755980633e-06 BETTER
I0320 19:29:30.945822 1524235 finetune.py:68] layer 3_gate @ epoch 2 new loss 4.238583642290905e-05 old loss 4.254198574926704e-05 BETTER
I0320 19:29:32.166207 1523865 finetune.py:68] layer 1_down @ epoch 3 new loss 0.0029108370654284954 old loss 0.0029114852659404278 BETTER
0_v proxy err 0.07907167822122574 tr(WHW.T) 4.225186347961426
bpp_loss 2.8014720678329468
0_q proxy err 0.0004707342595793307 tr(WHW.T) 2711.236328125
bpp_loss 2.938601493835449
0_k proxy err 0.0005335673340596259 tr(WHW.T) 1699.7862548828125
bpp_loss 3.0411417484283447
0_o proxy err 0.005542864557355642 tr(WHW.T) 0.9731641411781311
bpp_loss 2.615100622177124
0_up proxy err 0.00981828197836876 tr(WHW.T) 43.2901496887207
bpp_loss 3.1114601312681684
0_gate proxy err 0.006816041190177202 tr(WHW.T) 63.53058624267578
bpp_loss 3.129676996275436
0_down proxy err 0.006753421854227781 tr(WHW.T) 0.6594926118850708
bpp_loss 3.1570311479790267
I0320 19:29:34.651457 1524044 finetune.py:45] layer 2_down initial loss 3.0181325200828724e-05
I0320 19:30:04.346289 1523865 finetune.py:68] layer 1_down @ epoch 4 new loss 0.002910264302045107 old loss 0.0029108370654284954 BETTER
I0320 19:30:04.603823 1524235 finetune.py:68] layer 3_gate @ epoch 3 new loss 4.224521399009973e-05 old loss 4.238583642290905e-05 BETTER
I0320 19:30:05.297681 1524044 finetune.py:68] layer 2_down @ epoch 0 new loss 3.01655127259437e-05 old loss 3.0181325200828724e-05 BETTER
1_v proxy err 0.11526663601398468 tr(WHW.T) 16.465883255004883
bpp_loss 2.7479324340820312
1_q proxy err 0.0006583277136087418 tr(WHW.T) 4780.0361328125
bpp_loss 3.8160465955734253
1_k proxy err 0.0006417096010409296 tr(WHW.T) 5001.5234375
bpp_loss 3.812467932701111
1_o proxy err 0.019386151805520058 tr(WHW.T) 1.1128511428833008
bpp_loss 2.6609832048416138
1_up proxy err 0.010241390205919743 tr(WHW.T) 110.09931945800781
bpp_loss 3.1803471764852835
1_gate proxy err 0.005294230300933123 tr(WHW.T) 221.9840087890625
bpp_loss 3.2522971574650255
1_down proxy err 0.00600643502548337 tr(WHW.T) 2045.2271728515625
bpp_loss 3.1973157483477923
I0320 19:30:35.262194 1524044 finetune.py:68] layer 2_down @ epoch 1 new loss 3.0156739740050398e-05 old loss 3.01655127259437e-05 BETTER
I0320 19:30:35.419617 1524235 finetune.py:68] layer 3_gate @ epoch 4 new loss 4.2115610995097086e-05 old loss 4.224521399009973e-05 BETTER
I0320 19:31:01.801356 1524235 finetune.py:45] layer 3_down initial loss 6.25671018497087e-05
I0320 19:31:04.455508 1524044 finetune.py:68] layer 2_down @ epoch 2 new loss 3.01508407574147e-05 old loss 3.0156739740050398e-05 BETTER
I0320 19:31:26.411725 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 4 in 74.49302744865417s
I0320 19:31:29.555329 1524235 finetune.py:68] layer 3_down @ epoch 0 new loss 6.253467290662229e-05 old loss 6.25671018497087e-05 BETTER
I0320 19:31:30.934792 1525975 config.py:54] PyTorch version 2.6.0 available.
W0320 19:31:31.456054 1525975 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:31:32.650030 1525975 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:31:32.654750 1523378 quantize_finetune_llama.py:203] layer 5 gpu 1
I0320 19:31:32.670584 1525975 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 19:31:33.740061 1524044 finetune.py:68] layer 2_down @ epoch 3 new loss 3.014650610566605e-05 old loss 3.01508407574147e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:31:48.033561 1525975 finetune.py:45] layer 4_v initial loss 5.2317660447442904e-05
W0320 19:31:48.033955 1525975 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:31:59.208580 1524235 finetune.py:68] layer 3_down @ epoch 1 new loss 6.25226239208132e-05 old loss 6.253467290662229e-05 BETTER
I0320 19:32:03.838447 1524044 finetune.py:68] layer 2_down @ epoch 4 new loss 3.0143201001919806e-05 old loss 3.014650610566605e-05 BETTER
2_v proxy err 0.029816165566444397 tr(WHW.T) 136.67332458496094
bpp_loss 2.9960973262786865
2_q proxy err 0.0007879018667154014 tr(WHW.T) 7772.25537109375
bpp_loss 3.828418493270874
2_k proxy err 0.0006184070953167975 tr(WHW.T) 10225.783203125
bpp_loss 3.921951413154602
2_o proxy err 0.01582578383386135 tr(WHW.T) 1.4651926755905151
bpp_loss 2.9688650369644165
2_up proxy err 0.011955986730754375 tr(WHW.T) 193.52098083496094
bpp_loss 3.2008610215297963
2_gate proxy err 0.007778527215123177 tr(WHW.T) 306.7027893066406
bpp_loss 3.287043194438136
2_down proxy err 0.013739796355366707 tr(WHW.T) 3.018057346343994
bpp_loss 3.2083337029745413
I0320 19:32:23.206314 1525975 finetune.py:68] layer 4_v @ epoch 0 new loss 2.879838939406909e-05 old loss 5.2317660447442904e-05 BETTER
I0320 19:32:27.887751 1524235 finetune.py:68] layer 3_down @ epoch 2 new loss 6.251500599319115e-05 old loss 6.25226239208132e-05 BETTER
I0320 19:32:57.104032 1524235 finetune.py:68] layer 3_down @ epoch 3 new loss 6.25094908173196e-05 old loss 6.251500599319115e-05 BETTER
I0320 19:33:00.436954 1525975 finetune.py:68] layer 4_v @ epoch 1 new loss 2.2359281501849182e-05 old loss 2.879838939406909e-05 BETTER
I0320 19:33:25.679746 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 5 in 74.8233916759491s
I0320 19:33:26.171412 1524235 finetune.py:68] layer 3_down @ epoch 4 new loss 6.250493606785312e-05 old loss 6.25094908173196e-05 BETTER
3_v proxy err 0.02809854969382286 tr(WHW.T) 284.77557373046875
bpp_loss 2.9501672983169556
3_q proxy err 0.0014828077983111143 tr(WHW.T) 7229.84765625
bpp_loss 3.7312809228897095
3_k proxy err 0.0011018479708582163 tr(WHW.T) 10100.5947265625
bpp_loss 3.8007951974868774
3_o proxy err 0.015802906826138496 tr(WHW.T) 3.3643436431884766
bpp_loss 2.9319794178009033
3_up proxy err 0.013618801720440388 tr(WHW.T) 284.831787109375
bpp_loss 3.2093520053597384
3_gate proxy err 0.00839210208505392 tr(WHW.T) 477.976806640625
bpp_loss 3.305936059286428
3_down proxy err 0.014023695141077042 tr(WHW.T) 6.15281343460083
bpp_loss 3.211900289668593
I0320 19:33:29.639487 1526191 config.py:54] PyTorch version 2.6.0 available.
W0320 19:33:29.972624 1526191 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:33:30.962875 1526191 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:33:30.967493 1523378 quantize_finetune_llama.py:203] layer 6 gpu 2
I0320 19:33:30.986297 1526191 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 19:33:36.972207 1525975 finetune.py:68] layer 4_v @ epoch 2 new loss 1.9596178390202112e-05 old loss 2.2359281501849182e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:33:44.967987 1526191 finetune.py:45] layer 5_v initial loss 7.79431065893732e-05
W0320 19:33:44.968244 1526191 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:34:13.848649 1525975 finetune.py:68] layer 4_v @ epoch 3 new loss 1.7962151105166413e-05 old loss 1.9596178390202112e-05 BETTER
I0320 19:34:18.330848 1526191 finetune.py:68] layer 5_v @ epoch 0 new loss 4.288809577701613e-05 old loss 7.79431065893732e-05 BETTER
I0320 19:34:47.158845 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 6 in 74.67555928230286s
I0320 19:34:50.949479 1525975 finetune.py:68] layer 4_v @ epoch 4 new loss 1.680636705714278e-05 old loss 1.7962151105166413e-05 BETTER
I0320 19:34:51.485590 1526356 config.py:54] PyTorch version 2.6.0 available.
W0320 19:34:51.866089 1526356 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:34:52.894120 1526356 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:34:52.898282 1523378 quantize_finetune_llama.py:203] layer 7 gpu 3
I0320 19:34:52.916170 1526356 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 19:34:52.947289 1526191 finetune.py:68] layer 5_v @ epoch 1 new loss 3.446664413786493e-05 old loss 4.288809577701613e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:35:07.144323 1525975 finetune.py:45] layer 4_q initial loss 2.0803810912184417e-05
I0320 19:35:07.774367 1526356 finetune.py:45] layer 6_v initial loss 0.00010111287701874971
W0320 19:35:07.774635 1526356 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:35:27.983082 1526191 finetune.py:68] layer 5_v @ epoch 2 new loss 3.077014480368234e-05 old loss 3.446664413786493e-05 BETTER
I0320 19:35:42.821643 1526356 finetune.py:68] layer 6_v @ epoch 0 new loss 5.2606330427806824e-05 old loss 0.00010111287701874971 BETTER
I0320 19:35:43.461748 1525975 finetune.py:68] layer 4_q @ epoch 0 new loss 1.8225122403237037e-05 old loss 2.0803810912184417e-05 BETTER
I0320 19:36:03.783573 1526191 finetune.py:68] layer 5_v @ epoch 3 new loss 2.8498199753812514e-05 old loss 3.077014480368234e-05 BETTER
I0320 19:36:11.137297 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 7 in 77.78007912635803s
I0320 19:36:16.201583 1526524 config.py:54] PyTorch version 2.6.0 available.
W0320 19:36:16.845685 1526524 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:36:18.239260 1526524 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:36:18.244180 1523378 quantize_finetune_llama.py:203] layer 8 gpu 0
I0320 19:36:18.260990 1526524 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 19:36:18.331008 1526356 finetune.py:68] layer 6_v @ epoch 1 new loss 4.324628389440477e-05 old loss 5.2606330427806824e-05 BETTER
I0320 19:36:20.544966 1525975 finetune.py:68] layer 4_q @ epoch 1 new loss 1.7211677914019674e-05 old loss 1.8225122403237037e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:36:34.619714 1526524 finetune.py:45] layer 7_v initial loss 0.00012356651131995022
W0320 19:36:34.620132 1526524 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:36:39.815492 1526191 finetune.py:68] layer 5_v @ epoch 4 new loss 2.690638939384371e-05 old loss 2.8498199753812514e-05 BETTER
I0320 19:36:55.109626 1526356 finetune.py:68] layer 6_v @ epoch 2 new loss 3.890939842676744e-05 old loss 4.324628389440477e-05 BETTER
I0320 19:36:57.762011 1526191 finetune.py:45] layer 5_q initial loss 3.328611273900606e-05
I0320 19:36:58.652418 1525975 finetune.py:68] layer 4_q @ epoch 2 new loss 1.6460446204291657e-05 old loss 1.7211677914019674e-05 BETTER
I0320 19:37:08.942080 1526524 finetune.py:68] layer 7_v @ epoch 0 new loss 6.508048682007939e-05 old loss 0.00012356651131995022 BETTER
I0320 19:37:32.543834 1526356 finetune.py:68] layer 6_v @ epoch 3 new loss 3.62082282663323e-05 old loss 3.890939842676744e-05 BETTER
I0320 19:37:33.029219 1526191 finetune.py:68] layer 5_q @ epoch 0 new loss 2.8837972422479652e-05 old loss 3.328611273900606e-05 BETTER
I0320 19:37:36.590597 1525975 finetune.py:68] layer 4_q @ epoch 3 new loss 1.5864539818721823e-05 old loss 1.6460446204291657e-05 BETTER
I0320 19:37:44.688575 1526524 finetune.py:68] layer 7_v @ epoch 1 new loss 5.438782318378799e-05 old loss 6.508048682007939e-05 BETTER
I0320 19:38:10.362494 1526191 finetune.py:68] layer 5_q @ epoch 1 new loss 2.7411462724558078e-05 old loss 2.8837972422479652e-05 BETTER
I0320 19:38:10.586769 1526356 finetune.py:68] layer 6_v @ epoch 4 new loss 3.4328677429584786e-05 old loss 3.62082282663323e-05 BETTER
I0320 19:38:14.634528 1525975 finetune.py:68] layer 4_q @ epoch 4 new loss 1.536683521408122e-05 old loss 1.5864539818721823e-05 BETTER
I0320 19:38:20.136163 1526524 finetune.py:68] layer 7_v @ epoch 2 new loss 4.9474900151835755e-05 old loss 5.438782318378799e-05 BETTER
I0320 19:38:30.037403 1526356 finetune.py:45] layer 6_q initial loss 4.549496588879265e-05
I0320 19:38:32.355151 1525975 finetune.py:45] layer 4_k initial loss 1.8933456885861233e-05
I0320 19:38:46.575122 1526191 finetune.py:68] layer 5_q @ epoch 2 new loss 2.637123725435231e-05 old loss 2.7411462724558078e-05 BETTER
I0320 19:38:56.060843 1526524 finetune.py:68] layer 7_v @ epoch 3 new loss 4.640860788640566e-05 old loss 4.9474900151835755e-05 BETTER
I0320 19:39:05.176830 1526356 finetune.py:68] layer 6_q @ epoch 0 new loss 3.922628457075916e-05 old loss 4.549496588879265e-05 BETTER
I0320 19:39:07.915203 1525975 finetune.py:68] layer 4_k @ epoch 0 new loss 1.7530348486616276e-05 old loss 1.8933456885861233e-05 BETTER
I0320 19:39:22.120613 1526191 finetune.py:68] layer 5_q @ epoch 3 new loss 2.554602724558208e-05 old loss 2.637123725435231e-05 BETTER
I0320 19:39:31.612082 1526524 finetune.py:68] layer 7_v @ epoch 4 new loss 4.42647396994289e-05 old loss 4.640860788640566e-05 BETTER
I0320 19:39:41.010469 1526356 finetune.py:68] layer 6_q @ epoch 1 new loss 3.739280509762466e-05 old loss 3.922628457075916e-05 BETTER
I0320 19:39:44.592444 1525975 finetune.py:68] layer 4_k @ epoch 1 new loss 1.7021804524119943e-05 old loss 1.7530348486616276e-05 BETTER
I0320 19:39:52.537980 1526524 finetune.py:45] layer 7_q initial loss 5.834075636812486e-05
I0320 19:39:58.228892 1526191 finetune.py:68] layer 5_q @ epoch 4 new loss 2.4861135898390785e-05 old loss 2.554602724558208e-05 BETTER
I0320 19:40:15.860129 1526191 finetune.py:45] layer 5_k initial loss 2.9297279979800805e-05
I0320 19:40:17.968652 1526356 finetune.py:68] layer 6_q @ epoch 2 new loss 3.604521043598652e-05 old loss 3.739280509762466e-05 BETTER
I0320 19:40:22.392861 1525975 finetune.py:68] layer 4_k @ epoch 2 new loss 1.6612813851679675e-05 old loss 1.7021804524119943e-05 BETTER
I0320 19:40:26.475867 1526524 finetune.py:68] layer 7_q @ epoch 0 new loss 5.1678485760930926e-05 old loss 5.834075636812486e-05 BETTER
I0320 19:40:50.833796 1526191 finetune.py:68] layer 5_k @ epoch 0 new loss 2.717232382565271e-05 old loss 2.9297279979800805e-05 BETTER
I0320 19:40:54.539557 1526356 finetune.py:68] layer 6_q @ epoch 3 new loss 3.49850051861722e-05 old loss 3.604521043598652e-05 BETTER
I0320 19:41:00.275814 1525975 finetune.py:68] layer 4_k @ epoch 3 new loss 1.626677294552792e-05 old loss 1.6612813851679675e-05 BETTER
I0320 19:41:02.331046 1526524 finetune.py:68] layer 7_q @ epoch 1 new loss 4.9401383876102045e-05 old loss 5.1678485760930926e-05 BETTER
I0320 19:41:27.003255 1526191 finetune.py:68] layer 5_k @ epoch 1 new loss 2.6534657081356272e-05 old loss 2.717232382565271e-05 BETTER
I0320 19:41:31.687082 1526356 finetune.py:68] layer 6_q @ epoch 4 new loss 3.4088381653418764e-05 old loss 3.49850051861722e-05 BETTER
I0320 19:41:37.819103 1525975 finetune.py:68] layer 4_k @ epoch 4 new loss 1.5966914361342788e-05 old loss 1.626677294552792e-05 BETTER
I0320 19:41:38.579189 1526524 finetune.py:68] layer 7_q @ epoch 2 new loss 4.776742207468487e-05 old loss 4.9401383876102045e-05 BETTER
I0320 19:41:49.789324 1526356 finetune.py:45] layer 6_k initial loss 4.304399772081524e-05
I0320 19:41:54.852033 1525975 finetune.py:45] layer 4_o initial loss 3.916068089893088e-05
I0320 19:42:02.364603 1526191 finetune.py:68] layer 5_k @ epoch 2 new loss 2.600357765913941e-05 old loss 2.6534657081356272e-05 BETTER
I0320 19:42:13.972037 1526524 finetune.py:68] layer 7_q @ epoch 3 new loss 4.648672620533034e-05 old loss 4.776742207468487e-05 BETTER
I0320 19:42:24.284142 1526356 finetune.py:68] layer 6_k @ epoch 0 new loss 3.9421047404175624e-05 old loss 4.304399772081524e-05 BETTER
I0320 19:42:29.544659 1525975 finetune.py:68] layer 4_o @ epoch 0 new loss 3.745243884623051e-05 old loss 3.916068089893088e-05 BETTER
I0320 19:42:38.103474 1526191 finetune.py:68] layer 5_k @ epoch 3 new loss 2.5550420104991645e-05 old loss 2.600357765913941e-05 BETTER
I0320 19:42:49.377194 1526524 finetune.py:68] layer 7_q @ epoch 4 new loss 4.543137038126588e-05 old loss 4.648672620533034e-05 BETTER
I0320 19:42:59.957688 1526356 finetune.py:68] layer 6_k @ epoch 1 new loss 3.855379327433184e-05 old loss 3.9421047404175624e-05 BETTER
I0320 19:43:06.774858 1525975 finetune.py:68] layer 4_o @ epoch 1 new loss 3.663636016426608e-05 old loss 3.745243884623051e-05 BETTER
I0320 19:43:10.638349 1526524 finetune.py:45] layer 7_k initial loss 5.6244800362037495e-05
I0320 19:43:14.809168 1526191 finetune.py:68] layer 5_k @ epoch 4 new loss 2.5155775801977143e-05 old loss 2.5550420104991645e-05 BETTER
I0320 19:43:32.771324 1526191 finetune.py:45] layer 5_o initial loss 7.094888860592619e-05
I0320 19:43:36.263476 1526356 finetune.py:68] layer 6_k @ epoch 2 new loss 3.78455915779341e-05 old loss 3.855379327433184e-05 BETTER
I0320 19:43:44.421239 1525975 finetune.py:68] layer 4_o @ epoch 2 new loss 3.6069119232706726e-05 old loss 3.663636016426608e-05 BETTER
I0320 19:43:45.285347 1526524 finetune.py:68] layer 7_k @ epoch 0 new loss 5.2921091992175207e-05 old loss 5.6244800362037495e-05 BETTER
I0320 19:44:06.332274 1526191 finetune.py:68] layer 5_o @ epoch 0 new loss 6.696102354908362e-05 old loss 7.094888860592619e-05 BETTER
I0320 19:44:12.560263 1526356 finetune.py:68] layer 6_k @ epoch 3 new loss 3.7237499782349914e-05 old loss 3.78455915779341e-05 BETTER
I0320 19:44:21.555888 1525975 finetune.py:68] layer 4_o @ epoch 3 new loss 3.560449476935901e-05 old loss 3.6069119232706726e-05 BETTER
I0320 19:44:21.867218 1526524 finetune.py:68] layer 7_k @ epoch 1 new loss 5.188017166801728e-05 old loss 5.2921091992175207e-05 BETTER
I0320 19:44:41.324969 1526191 finetune.py:68] layer 5_o @ epoch 1 new loss 6.4807893068064e-05 old loss 6.696102354908362e-05 BETTER
I0320 19:44:47.848799 1526356 finetune.py:68] layer 6_k @ epoch 4 new loss 3.671584272524342e-05 old loss 3.7237499782349914e-05 BETTER
I0320 19:44:57.667659 1525975 finetune.py:68] layer 4_o @ epoch 4 new loss 3.5205073800170794e-05 old loss 3.560449476935901e-05 BETTER
I0320 19:44:57.709700 1526524 finetune.py:68] layer 7_k @ epoch 2 new loss 5.105702439323068e-05 old loss 5.188017166801728e-05 BETTER
I0320 19:45:06.257236 1526356 finetune.py:45] layer 6_o initial loss 9.433144441572949e-05
I0320 19:45:16.162765 1526191 finetune.py:68] layer 5_o @ epoch 2 new loss 6.3217208662536e-05 old loss 6.4807893068064e-05 BETTER
I0320 19:45:27.902851 1525975 finetune.py:45] layer 4_up initial loss 5.704563591280021e-05
I0320 19:45:33.523855 1526524 finetune.py:68] layer 7_k @ epoch 3 new loss 5.035691719967872e-05 old loss 5.105702439323068e-05 BETTER
I0320 19:45:40.496595 1526356 finetune.py:68] layer 6_o @ epoch 0 new loss 8.81005689734593e-05 old loss 9.433144441572949e-05 BETTER
I0320 19:45:51.272085 1526191 finetune.py:68] layer 5_o @ epoch 3 new loss 6.191155989654362e-05 old loss 6.3217208662536e-05 BETTER
I0320 19:46:00.979973 1525975 finetune.py:68] layer 4_up @ epoch 0 new loss 5.6216853408841416e-05 old loss 5.704563591280021e-05 BETTER
I0320 19:46:09.740962 1526524 finetune.py:68] layer 7_k @ epoch 4 new loss 4.972609531250782e-05 old loss 5.035691719967872e-05 BETTER
I0320 19:46:15.122222 1526356 finetune.py:68] layer 6_o @ epoch 1 new loss 8.54898607940413e-05 old loss 8.81005689734593e-05 BETTER
I0320 19:46:26.842258 1526191 finetune.py:68] layer 5_o @ epoch 4 new loss 6.080703315092251e-05 old loss 6.191155989654362e-05 BETTER
I0320 19:46:29.919732 1526524 finetune.py:45] layer 7_o initial loss 0.00012905476614832878
I0320 19:46:35.331242 1525975 finetune.py:68] layer 4_up @ epoch 1 new loss 5.5687021813355386e-05 old loss 5.6216853408841416e-05 BETTER
I0320 19:46:50.379221 1526356 finetune.py:68] layer 6_o @ epoch 2 new loss 8.365135727217421e-05 old loss 8.54898607940413e-05 BETTER
I0320 19:46:56.478480 1526191 finetune.py:45] layer 5_up initial loss 9.569515532348305e-05
I0320 19:47:03.679583 1526524 finetune.py:68] layer 7_o @ epoch 0 new loss 0.00011935707880184054 old loss 0.00012905476614832878 BETTER
I0320 19:47:10.684277 1525975 finetune.py:68] layer 4_up @ epoch 2 new loss 5.526383392862044e-05 old loss 5.5687021813355386e-05 BETTER
I0320 19:47:26.902922 1526356 finetune.py:68] layer 6_o @ epoch 3 new loss 8.220109884859994e-05 old loss 8.365135727217421e-05 BETTER
I0320 19:47:29.432470 1526191 finetune.py:68] layer 5_up @ epoch 0 new loss 9.396620589541271e-05 old loss 9.569515532348305e-05 BETTER
I0320 19:47:38.700551 1526524 finetune.py:68] layer 7_o @ epoch 1 new loss 0.00011514762445585802 old loss 0.00011935707880184054 BETTER
I0320 19:47:45.939971 1525975 finetune.py:68] layer 4_up @ epoch 3 new loss 5.4888434533495456e-05 old loss 5.526383392862044e-05 BETTER
I0320 19:48:03.647540 1526356 finetune.py:68] layer 6_o @ epoch 4 new loss 8.101091225398704e-05 old loss 8.220109884859994e-05 BETTER
I0320 19:48:04.235550 1526191 finetune.py:68] layer 5_up @ epoch 1 new loss 9.28207955439575e-05 old loss 9.396620589541271e-05 BETTER
I0320 19:48:14.106632 1526524 finetune.py:68] layer 7_o @ epoch 2 new loss 0.00011223032925045118 old loss 0.00011514762445585802 BETTER
I0320 19:48:20.694074 1525975 finetune.py:68] layer 4_up @ epoch 4 new loss 5.454598067444749e-05 old loss 5.4888434533495456e-05 BETTER
I0320 19:48:34.404761 1526356 finetune.py:45] layer 6_up initial loss 0.00013468108954839408
I0320 19:48:38.014232 1526191 finetune.py:68] layer 5_up @ epoch 2 new loss 9.18600126169622e-05 old loss 9.28207955439575e-05 BETTER
I0320 19:48:50.190221 1526524 finetune.py:68] layer 7_o @ epoch 3 new loss 0.00011000171070918441 old loss 0.00011223032925045118 BETTER
I0320 19:48:51.004512 1525975 finetune.py:45] layer 4_gate initial loss 6.88717991579324e-05
I0320 19:49:07.066572 1526356 finetune.py:68] layer 6_up @ epoch 0 new loss 0.00013181396934669465 old loss 0.00013468108954839408 BETTER
I0320 19:49:11.782970 1526191 finetune.py:68] layer 5_up @ epoch 3 new loss 9.100924944505095e-05 old loss 9.18600126169622e-05 BETTER
I0320 19:49:23.720717 1525975 finetune.py:68] layer 4_gate @ epoch 0 new loss 6.839918933110312e-05 old loss 6.88717991579324e-05 BETTER
I0320 19:49:26.024292 1526524 finetune.py:68] layer 7_o @ epoch 4 new loss 0.00010818172449944541 old loss 0.00011000171070918441 BETTER
I0320 19:49:41.337941 1526356 finetune.py:68] layer 6_up @ epoch 1 new loss 0.00013019616017118096 old loss 0.00013181396934669465 BETTER
I0320 19:49:45.393774 1526191 finetune.py:68] layer 5_up @ epoch 4 new loss 9.023926395457238e-05 old loss 9.100924944505095e-05 BETTER
I0320 19:49:57.389214 1525975 finetune.py:68] layer 4_gate @ epoch 1 new loss 6.803661381127313e-05 old loss 6.839918933110312e-05 BETTER
I0320 19:49:58.510255 1526524 finetune.py:45] layer 7_up initial loss 0.00018213997827842832
I0320 19:50:15.784709 1526356 finetune.py:68] layer 6_up @ epoch 2 new loss 0.00012885444448329508 old loss 0.00013019616017118096 BETTER
I0320 19:50:16.604380 1526191 finetune.py:45] layer 5_gate initial loss 0.00011191375233465806
I0320 19:50:31.174620 1526524 finetune.py:68] layer 7_up @ epoch 0 new loss 0.0001774228148860857 old loss 0.00018213997827842832 BETTER
I0320 19:50:31.474749 1525975 finetune.py:68] layer 4_gate @ epoch 2 new loss 6.772919732611626e-05 old loss 6.803661381127313e-05 BETTER
I0320 19:50:48.091409 1526191 finetune.py:68] layer 5_gate @ epoch 0 new loss 0.00011102615826530382 old loss 0.00011191375233465806 BETTER
I0320 19:50:50.436282 1526356 finetune.py:68] layer 6_up @ epoch 3 new loss 0.00012769023305736482 old loss 0.00012885444448329508 BETTER
I0320 19:51:05.229965 1526524 finetune.py:68] layer 7_up @ epoch 1 new loss 0.00017474695050623268 old loss 0.0001774228148860857 BETTER
I0320 19:51:05.551714 1525975 finetune.py:68] layer 4_gate @ epoch 3 new loss 6.74556358717382e-05 old loss 6.772919732611626e-05 BETTER
I0320 19:51:20.828665 1526191 finetune.py:68] layer 5_gate @ epoch 1 new loss 0.00011031587928300723 old loss 0.00011102615826530382 BETTER
I0320 19:51:25.446866 1526356 finetune.py:68] layer 6_up @ epoch 4 new loss 0.00012665416579693556 old loss 0.00012769023305736482 BETTER
I0320 19:51:39.450409 1526524 finetune.py:68] layer 7_up @ epoch 2 new loss 0.00017264542111661285 old loss 0.00017474695050623268 BETTER
I0320 19:51:39.862370 1525975 finetune.py:68] layer 4_gate @ epoch 4 new loss 6.720551755279303e-05 old loss 6.74556358717382e-05 BETTER
I0320 19:51:52.574994 1526191 finetune.py:68] layer 5_gate @ epoch 2 new loss 0.00010970118455588818 old loss 0.00011031587928300723 BETTER
I0320 19:51:57.314526 1526356 finetune.py:45] layer 6_gate initial loss 0.00015713223547209054
I0320 19:52:13.374329 1525975 finetune.py:45] layer 4_down initial loss 0.00010548078716965392
I0320 19:52:13.742427 1526524 finetune.py:68] layer 7_up @ epoch 3 new loss 0.00017082849808502942 old loss 0.00017264542111661285 BETTER
I0320 19:52:25.043741 1526191 finetune.py:68] layer 5_gate @ epoch 3 new loss 0.0001091392186935991 old loss 0.00010970118455588818 BETTER
I0320 19:52:28.325246 1526356 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.0001558255316922441 old loss 0.00015713223547209054 BETTER
I0320 19:52:44.730150 1525975 finetune.py:68] layer 4_down @ epoch 0 new loss 0.00010541638039285317 old loss 0.00010548078716965392 BETTER
I0320 19:52:48.585784 1526524 finetune.py:68] layer 7_up @ epoch 4 new loss 0.00016925099771469831 old loss 0.00017082849808502942 BETTER
I0320 19:52:57.003933 1526191 finetune.py:68] layer 5_gate @ epoch 4 new loss 0.00010862977796932682 old loss 0.0001091392186935991 BETTER
I0320 19:53:00.261505 1526356 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.00015482111484743655 old loss 0.0001558255316922441 BETTER
I0320 19:53:15.585221 1525975 finetune.py:68] layer 4_down @ epoch 1 new loss 0.00010538353672018275 old loss 0.00010541638039285317 BETTER
I0320 19:53:21.373782 1526524 finetune.py:45] layer 7_gate initial loss 0.0002108213520841673
I0320 19:53:30.614640 1526191 finetune.py:45] layer 5_down initial loss 0.0001631647755857557
I0320 19:53:35.038133 1526356 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.0001539510121801868 old loss 0.00015482111484743655 BETTER
I0320 19:53:49.277425 1525975 finetune.py:68] layer 4_down @ epoch 2 new loss 0.00010536450281506404 old loss 0.00010538353672018275 BETTER
I0320 19:53:53.920285 1526524 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.0002087475877488032 old loss 0.0002108213520841673 BETTER
I0320 19:54:00.966183 1526191 finetune.py:68] layer 5_down @ epoch 0 new loss 0.00016310292994603515 old loss 0.0001631647755857557 BETTER
I0320 19:54:08.490850 1526356 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.00015317529323510826 old loss 0.0001539510121801868 BETTER
I0320 19:54:22.470420 1525975 finetune.py:68] layer 4_down @ epoch 3 new loss 0.00010535183537285775 old loss 0.00010536450281506404 BETTER
I0320 19:54:27.276116 1526524 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.0002071975904982537 old loss 0.0002087475877488032 BETTER
I0320 19:54:32.097414 1526191 finetune.py:68] layer 5_down @ epoch 1 new loss 0.00016306850011460483 old loss 0.00016310292994603515 BETTER
I0320 19:54:41.867209 1526356 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.000152467648149468 old loss 0.00015317529323510826 BETTER
I0320 19:54:53.647016 1525975 finetune.py:68] layer 4_down @ epoch 4 new loss 0.00010534276225371286 old loss 0.00010535183537285775 BETTER
4_v proxy err 0.02682258002460003 tr(WHW.T) 274.6131286621094
bpp_loss 2.9972251653671265
4_q proxy err 0.0014246983919292688 tr(WHW.T) 6928.93701171875
bpp_loss 3.8048676252365112
4_k proxy err 0.0009952052496373653 tr(WHW.T) 10443.900390625
bpp_loss 3.8427101373672485
4_o proxy err 0.015757057815790176 tr(WHW.T) 5.148172855377197
bpp_loss 2.977257490158081
4_up proxy err 0.013442061841487885 tr(WHW.T) 397.9268493652344
bpp_loss 3.2009742204533067
4_gate proxy err 0.00683412654325366 tr(WHW.T) 820.33642578125
bpp_loss 3.3331169305845747
4_down proxy err 0.014015144668519497 tr(WHW.T) 11.635781288146973
bpp_loss 3.1983394179233287
I0320 19:54:59.168044 1526524 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.00020589519408531487 old loss 0.0002071975904982537 BETTER
I0320 19:55:01.791443 1526191 finetune.py:68] layer 5_down @ epoch 2 new loss 0.00016304657037835568 old loss 0.00016306850011460483 BETTER
I0320 19:55:15.665321 1526356 finetune.py:45] layer 6_down initial loss 0.00023485074052587152
I0320 19:55:32.057513 1526524 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.00020474751363508403 old loss 0.00020589519408531487 BETTER
I0320 19:55:32.803097 1526191 finetune.py:68] layer 5_down @ epoch 3 new loss 0.00016303212032653391 old loss 0.00016304657037835568 BETTER
I0320 19:55:44.695782 1526356 finetune.py:68] layer 6_down @ epoch 0 new loss 0.0002347596746403724 old loss 0.00023485074052587152 BETTER
I0320 19:56:03.728545 1526191 finetune.py:68] layer 5_down @ epoch 4 new loss 0.00016302167205139995 old loss 0.00016303212032653391 BETTER
I0320 19:56:04.501119 1526524 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.000203695148229599 old loss 0.00020474751363508403 BETTER
5_v proxy err 0.02669444866478443 tr(WHW.T) 298.47540283203125
bpp_loss 3.020872712135315
5_q proxy err 0.0015352206537500024 tr(WHW.T) 6783.6416015625
bpp_loss 3.8141629695892334
5_k proxy err 0.0010245571611449122 tr(WHW.T) 10869.6064453125
bpp_loss 3.8877761363983154
5_o proxy err 0.02217397466301918 tr(WHW.T) 7.97523307800293
bpp_loss 3.0003252029418945
5_up proxy err 0.013195921666920185 tr(WHW.T) 506.759765625
bpp_loss 3.2009176209915515
5_gate proxy err 0.006376044359058142 tr(WHW.T) 1103.8973388671875
bpp_loss 3.340868218000545
5_down proxy err 0.014746085740625858 tr(WHW.T) 15.748488426208496
bpp_loss 3.199377503505973
I0320 19:56:13.710714 1526356 finetune.py:68] layer 6_down @ epoch 1 new loss 0.00023470418818760663 old loss 0.0002347596746403724 BETTER
I0320 19:56:31.846427 1526524 finetune.py:45] layer 7_down initial loss 0.0003148968971800059
I0320 19:56:42.599670 1526356 finetune.py:68] layer 6_down @ epoch 2 new loss 0.0002346689288970083 old loss 0.00023470418818760663 BETTER
I0320 19:56:59.251341 1526524 finetune.py:68] layer 7_down @ epoch 0 new loss 0.00031480140751227736 old loss 0.0003148968971800059 BETTER
I0320 19:57:11.748831 1526356 finetune.py:68] layer 6_down @ epoch 3 new loss 0.00023464529658667743 old loss 0.0002346689288970083 BETTER
I0320 19:57:25.065634 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 8 in 75.79939007759094s
I0320 19:57:27.538373 1526524 finetune.py:68] layer 7_down @ epoch 1 new loss 0.0003147396491840482 old loss 0.00031480140751227736 BETTER
I0320 19:57:29.549984 1528321 config.py:54] PyTorch version 2.6.0 available.
W0320 19:57:29.965483 1528321 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:57:31.320163 1528321 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:57:31.327997 1523378 quantize_finetune_llama.py:203] layer 9 gpu 1
I0320 19:57:31.348051 1528321 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:57:40.856838 1526356 finetune.py:68] layer 6_down @ epoch 4 new loss 0.00023462926037609577 old loss 0.00023464529658667743 BETTER
6_v proxy err 0.026063157245516777 tr(WHW.T) 443.5464782714844
bpp_loss 2.954001545906067
6_q proxy err 0.001965591451153159 tr(WHW.T) 7582.67041015625
bpp_loss 3.71279776096344
6_k proxy err 0.0014879046939313412 tr(WHW.T) 10434.7958984375
bpp_loss 3.7467563152313232
6_o proxy err 0.020460430532693863 tr(WHW.T) 11.61609935760498
bpp_loss 2.941820502281189
6_up proxy err 0.013218672014772892 tr(WHW.T) 617.555419921875
bpp_loss 3.1968124744504
6_gate proxy err 0.005583558697253466 tr(WHW.T) 1554.63916015625
bpp_loss 3.3639015375181684
6_down proxy err 0.015200073830783367 tr(WHW.T) 23.14940071105957
bpp_loss 3.1927990137144575
I0320 19:57:47.474927 1528321 finetune.py:45] layer 8_v initial loss 0.00018910359358415008
W0320 19:57:47.475295 1528321 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:57:56.382190 1526524 finetune.py:68] layer 7_down @ epoch 2 new loss 0.0003147008828818798 old loss 0.0003147396491840482 BETTER
I0320 19:58:23.071333 1528321 finetune.py:68] layer 8_v @ epoch 0 new loss 9.659387433202937e-05 old loss 0.00018910359358415008 BETTER
I0320 19:58:25.897040 1526524 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00031467273947782815 old loss 0.0003147008828818798 BETTER
I0320 19:58:55.432243 1526524 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0003146533272229135 old loss 0.00031467273947782815 BETTER
7_v proxy err 0.0254356786608696 tr(WHW.T) 489.9357604980469
bpp_loss 2.9648576974868774
7_q proxy err 0.002092755399644375 tr(WHW.T) 7681.22412109375
bpp_loss 3.7097532749176025
7_k proxy err 0.0016011359402909875 tr(WHW.T) 10223.837890625
bpp_loss 3.718231439590454
7_o proxy err 0.023266473785042763 tr(WHW.T) 15.244744300842285
bpp_loss 2.9485015869140625
7_up proxy err 0.013041759841144085 tr(WHW.T) 736.1460571289062
bpp_loss 3.200876280318859
7_gate proxy err 0.0054371291771531105 tr(WHW.T) 1879.314697265625
bpp_loss 3.363043319347293
7_down proxy err 0.015400944277644157 tr(WHW.T) 30.795459747314453
bpp_loss 3.194758282151333
I0320 19:59:00.486044 1528321 finetune.py:68] layer 8_v @ epoch 1 new loss 7.945683319121599e-05 old loss 9.659387433202937e-05 BETTER
I0320 19:59:02.562050 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 9 in 75.64981532096863s
I0320 19:59:06.533918 1528507 config.py:54] PyTorch version 2.6.0 available.
W0320 19:59:06.897199 1528507 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 19:59:07.984171 1528507 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 19:59:07.988335 1523378 quantize_finetune_llama.py:203] layer 10 gpu 2
I0320 19:59:08.003489 1528507 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 19:59:21.716186 1528507 finetune.py:45] layer 9_v initial loss 0.00021675745665561408
W0320 19:59:21.716519 1528507 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 19:59:37.247406 1528321 finetune.py:68] layer 8_v @ epoch 2 new loss 7.181621913332492e-05 old loss 7.945683319121599e-05 BETTER
I0320 19:59:55.562211 1528507 finetune.py:68] layer 9_v @ epoch 0 new loss 0.00011197634012205526 old loss 0.00021675745665561408 BETTER
I0320 20:00:14.055405 1528321 finetune.py:68] layer 8_v @ epoch 3 new loss 6.711985042784363e-05 old loss 7.181621913332492e-05 BETTER
I0320 20:00:24.641559 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 10 in 76.1928813457489s
I0320 20:00:29.182027 1528675 config.py:54] PyTorch version 2.6.0 available.
I0320 20:00:29.383755 1528507 finetune.py:68] layer 9_v @ epoch 1 new loss 9.282343671657145e-05 old loss 0.00011197634012205526 BETTER
W0320 20:00:29.605157 1528675 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 20:00:30.739799 1528675 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:00:30.743992 1523378 quantize_finetune_llama.py:203] layer 11 gpu 3
I0320 20:00:30.762602 1528675 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:00:45.035478 1528675 finetune.py:45] layer 10_v initial loss 0.00030437158420681953
W0320 20:00:45.035839 1528675 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:00:50.807755 1528321 finetune.py:68] layer 8_v @ epoch 4 new loss 6.387180474121124e-05 old loss 6.711985042784363e-05 BETTER
I0320 20:01:04.412204 1528507 finetune.py:68] layer 9_v @ epoch 2 new loss 8.43555826577358e-05 old loss 9.282343671657145e-05 BETTER
I0320 20:01:07.175955 1528321 finetune.py:45] layer 8_q initial loss 8.434331539319828e-05
I0320 20:01:19.235157 1528675 finetune.py:68] layer 10_v @ epoch 0 new loss 0.00015799155517015606 old loss 0.00030437158420681953 BETTER
I0320 20:01:40.431540 1528507 finetune.py:68] layer 9_v @ epoch 3 new loss 7.920974167063832e-05 old loss 8.43555826577358e-05 BETTER
I0320 20:01:43.084875 1528321 finetune.py:68] layer 8_q @ epoch 0 new loss 7.344420009758323e-05 old loss 8.434331539319828e-05 BETTER
I0320 20:01:49.832043 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 11 in 78.58059072494507s
I0320 20:01:54.576171 1528675 finetune.py:68] layer 10_v @ epoch 1 new loss 0.00013113144086673856 old loss 0.00015799155517015606 BETTER
I0320 20:01:54.667479 1528846 config.py:54] PyTorch version 2.6.0 available.
W0320 20:01:55.140166 1528846 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 20:01:56.418921 1528846 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:01:56.429039 1523378 quantize_finetune_llama.py:203] layer 12 gpu 0
I0320 20:01:56.447058 1528846 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:02:12.916014 1528846 finetune.py:45] layer 11_v initial loss 0.0003228792629670352
W0320 20:02:12.916965 1528846 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:02:17.062811 1528507 finetune.py:68] layer 9_v @ epoch 4 new loss 7.558827201137319e-05 old loss 7.920974167063832e-05 BETTER
I0320 20:02:20.461467 1528321 finetune.py:68] layer 8_q @ epoch 1 new loss 7.008662942098454e-05 old loss 7.344420009758323e-05 BETTER
I0320 20:02:30.292524 1528675 finetune.py:68] layer 10_v @ epoch 2 new loss 0.00011897370859514922 old loss 0.00013113144086673856 BETTER
I0320 20:02:35.382484 1528507 finetune.py:45] layer 9_q initial loss 9.952463733498007e-05
I0320 20:02:47.526088 1528846 finetune.py:68] layer 11_v @ epoch 0 new loss 0.0001676853426033631 old loss 0.0003228792629670352 BETTER
I0320 20:02:57.499555 1528321 finetune.py:68] layer 8_q @ epoch 2 new loss 6.767577724531293e-05 old loss 7.008662942098454e-05 BETTER
I0320 20:03:07.585352 1528675 finetune.py:68] layer 10_v @ epoch 3 new loss 0.00011151525541208684 old loss 0.00011897370859514922 BETTER
I0320 20:03:10.708495 1528507 finetune.py:68] layer 9_q @ epoch 0 new loss 8.712142880540341e-05 old loss 9.952463733498007e-05 BETTER
I0320 20:03:23.466090 1528846 finetune.py:68] layer 11_v @ epoch 1 new loss 0.00013921884237788618 old loss 0.0001676853426033631 BETTER
I0320 20:03:35.242288 1528321 finetune.py:68] layer 8_q @ epoch 3 new loss 6.577496969839558e-05 old loss 6.767577724531293e-05 BETTER
I0320 20:03:44.530908 1528675 finetune.py:68] layer 10_v @ epoch 4 new loss 0.0001062848386936821 old loss 0.00011151525541208684 BETTER
I0320 20:03:47.036779 1528507 finetune.py:68] layer 9_q @ epoch 1 new loss 8.33539743325673e-05 old loss 8.712142880540341e-05 BETTER
I0320 20:03:59.233133 1528846 finetune.py:68] layer 11_v @ epoch 2 new loss 0.00012619006156455725 old loss 0.00013921884237788618 BETTER
I0320 20:04:03.444200 1528675 finetune.py:45] layer 10_q initial loss 0.00013453573046717793
I0320 20:04:13.178766 1528321 finetune.py:68] layer 8_q @ epoch 4 new loss 6.420645513571799e-05 old loss 6.577496969839558e-05 BETTER
I0320 20:04:22.261641 1528507 finetune.py:68] layer 9_q @ epoch 2 new loss 8.063120912993327e-05 old loss 8.33539743325673e-05 BETTER
I0320 20:04:31.014298 1528321 finetune.py:45] layer 8_k initial loss 8.020423410926014e-05
I0320 20:04:36.088278 1528846 finetune.py:68] layer 11_v @ epoch 3 new loss 0.00011834110046038404 old loss 0.00012619006156455725 BETTER
I0320 20:04:38.382761 1528675 finetune.py:68] layer 10_q @ epoch 0 new loss 0.00011950976477237418 old loss 0.00013453573046717793 BETTER
I0320 20:04:58.194334 1528507 finetune.py:68] layer 9_q @ epoch 3 new loss 7.849664689274505e-05 old loss 8.063120912993327e-05 BETTER
I0320 20:05:06.868592 1528321 finetune.py:68] layer 8_k @ epoch 0 new loss 7.451562851201743e-05 old loss 8.020423410926014e-05 BETTER
I0320 20:05:12.742554 1528846 finetune.py:68] layer 11_v @ epoch 4 new loss 0.00011283264757366851 old loss 0.00011834110046038404 BETTER
I0320 20:05:15.288713 1528675 finetune.py:68] layer 10_q @ epoch 1 new loss 0.00011433047620812431 old loss 0.00011950976477237418 BETTER
I0320 20:05:32.421246 1528846 finetune.py:45] layer 11_q initial loss 0.00014113463112153113
I0320 20:05:34.313865 1528507 finetune.py:68] layer 9_q @ epoch 4 new loss 7.674538937862962e-05 old loss 7.849664689274505e-05 BETTER
I0320 20:05:42.954633 1528321 finetune.py:68] layer 8_k @ epoch 1 new loss 7.29549428797327e-05 old loss 7.451562851201743e-05 BETTER
I0320 20:05:51.818141 1528675 finetune.py:68] layer 10_q @ epoch 2 new loss 0.00011053936759708449 old loss 0.00011433047620812431 BETTER
I0320 20:05:52.443218 1528507 finetune.py:45] layer 9_k initial loss 0.00010005210060626268
I0320 20:06:06.699247 1528846 finetune.py:68] layer 11_q @ epoch 0 new loss 0.000127903011161834 old loss 0.00014113463112153113 BETTER
I0320 20:06:20.750771 1528321 finetune.py:68] layer 8_k @ epoch 2 new loss 7.174379425123334e-05 old loss 7.29549428797327e-05 BETTER
I0320 20:06:27.433345 1528507 finetune.py:68] layer 9_k @ epoch 0 new loss 8.999402052722871e-05 old loss 0.00010005210060626268 BETTER
I0320 20:06:28.927509 1528675 finetune.py:68] layer 10_q @ epoch 3 new loss 0.00010745473991846666 old loss 0.00011053936759708449 BETTER
I0320 20:06:42.462123 1528846 finetune.py:68] layer 11_q @ epoch 1 new loss 0.00012261532538104802 old loss 0.000127903011161834 BETTER
I0320 20:06:57.304900 1528321 finetune.py:68] layer 8_k @ epoch 3 new loss 7.069215644150972e-05 old loss 7.174379425123334e-05 BETTER
I0320 20:07:03.204536 1528507 finetune.py:68] layer 9_k @ epoch 1 new loss 8.811105362838134e-05 old loss 8.999402052722871e-05 BETTER
I0320 20:07:05.111574 1528675 finetune.py:68] layer 10_q @ epoch 4 new loss 0.0001049171551130712 old loss 0.00010745473991846666 BETTER
I0320 20:07:17.204163 1528846 finetune.py:68] layer 11_q @ epoch 2 new loss 0.00011871621973114088 old loss 0.00012261532538104802 BETTER
I0320 20:07:23.918536 1528675 finetune.py:45] layer 10_k initial loss 0.00013185292482376099
I0320 20:07:34.241361 1528321 finetune.py:68] layer 8_k @ epoch 4 new loss 6.979903992032632e-05 old loss 7.069215644150972e-05 BETTER
I0320 20:07:38.419667 1528507 finetune.py:68] layer 9_k @ epoch 2 new loss 8.664250344736502e-05 old loss 8.811105362838134e-05 BETTER
I0320 20:07:52.346702 1528321 finetune.py:45] layer 8_o initial loss 0.00018868785991799086
I0320 20:07:54.291297 1528846 finetune.py:68] layer 11_q @ epoch 3 new loss 0.00011576325778150931 old loss 0.00011871621973114088 BETTER
I0320 20:07:58.575031 1528675 finetune.py:68] layer 10_k @ epoch 0 new loss 0.00012059434084221721 old loss 0.00013185292482376099 BETTER
I0320 20:08:14.724465 1528507 finetune.py:68] layer 9_k @ epoch 3 new loss 8.543668809579685e-05 old loss 8.664250344736502e-05 BETTER
I0320 20:08:28.945248 1528321 finetune.py:68] layer 8_o @ epoch 0 new loss 0.0001744008914101869 old loss 0.00018868785991799086 BETTER
I0320 20:08:31.206110 1528846 finetune.py:68] layer 11_q @ epoch 4 new loss 0.00011337793694110587 old loss 0.00011576325778150931 BETTER
I0320 20:08:34.513334 1528675 finetune.py:68] layer 10_k @ epoch 1 new loss 0.00011796131730079651 old loss 0.00012059434084221721 BETTER
I0320 20:08:50.807543 1528507 finetune.py:68] layer 9_k @ epoch 4 new loss 8.438042277703062e-05 old loss 8.543668809579685e-05 BETTER
I0320 20:08:51.649758 1528846 finetune.py:45] layer 11_k initial loss 0.000137043054564856
I0320 20:09:05.956413 1528321 finetune.py:68] layer 8_o @ epoch 1 new loss 0.00016780209261924028 old loss 0.0001744008914101869 BETTER
I0320 20:09:09.410166 1528507 finetune.py:45] layer 9_o initial loss 0.00023262383183464408
I0320 20:09:11.155287 1528675 finetune.py:68] layer 10_k @ epoch 2 new loss 0.00011589560017455369 old loss 0.00011796131730079651 BETTER
I0320 20:09:25.813676 1528846 finetune.py:68] layer 11_k @ epoch 0 new loss 0.00013022804341744632 old loss 0.000137043054564856 BETTER
I0320 20:09:43.208727 1528321 finetune.py:68] layer 8_o @ epoch 2 new loss 0.00016322723240591586 old loss 0.00016780209261924028 BETTER
I0320 20:09:44.436983 1528507 finetune.py:68] layer 9_o @ epoch 0 new loss 0.00021528366778511554 old loss 0.00023262383183464408 BETTER
I0320 20:09:47.647032 1528675 finetune.py:68] layer 10_k @ epoch 3 new loss 0.00011415740300435573 old loss 0.00011589560017455369 BETTER
I0320 20:10:01.211355 1528846 finetune.py:68] layer 11_k @ epoch 1 new loss 0.00012795842485502362 old loss 0.00013022804341744632 BETTER
I0320 20:10:21.275809 1528507 finetune.py:68] layer 9_o @ epoch 1 new loss 0.00020674438565038145 old loss 0.00021528366778511554 BETTER
I0320 20:10:21.325129 1528321 finetune.py:68] layer 8_o @ epoch 3 new loss 0.00015972320397850126 old loss 0.00016322723240591586 BETTER
I0320 20:10:24.665452 1528675 finetune.py:68] layer 10_k @ epoch 4 new loss 0.00011265641660429537 old loss 0.00011415740300435573 BETTER
I0320 20:10:36.180503 1528846 finetune.py:68] layer 11_k @ epoch 2 new loss 0.00012615285231731832 old loss 0.00012795842485502362 BETTER
I0320 20:10:43.546423 1528675 finetune.py:45] layer 10_o initial loss 0.0003170614072587341
I0320 20:10:57.574923 1528507 finetune.py:68] layer 9_o @ epoch 2 new loss 0.00020087328448425978 old loss 0.00020674438565038145 BETTER
I0320 20:10:58.712733 1528321 finetune.py:68] layer 8_o @ epoch 4 new loss 0.00015688009443692863 old loss 0.00015972320397850126 BETTER
I0320 20:11:11.870229 1528846 finetune.py:68] layer 11_k @ epoch 3 new loss 0.00012460215657483786 old loss 0.00012615285231731832 BETTER
I0320 20:11:17.861875 1528675 finetune.py:68] layer 10_o @ epoch 0 new loss 0.00029433987219817936 old loss 0.0003170614072587341 BETTER
I0320 20:11:27.483291 1528321 finetune.py:45] layer 8_up initial loss 0.0002455495414324105
I0320 20:11:32.423679 1528507 finetune.py:68] layer 9_o @ epoch 3 new loss 0.00019636594515759498 old loss 0.00020087328448425978 BETTER
I0320 20:11:48.044321 1528846 finetune.py:68] layer 11_k @ epoch 4 new loss 0.00012319705274421722 old loss 0.00012460215657483786 BETTER
I0320 20:11:52.972691 1528675 finetune.py:68] layer 10_o @ epoch 1 new loss 0.00028302709688432515 old loss 0.00029433987219817936 BETTER
I0320 20:12:00.329677 1528321 finetune.py:68] layer 8_up @ epoch 0 new loss 0.0002389386499999091 old loss 0.0002455495414324105 BETTER
I0320 20:12:08.469583 1528507 finetune.py:68] layer 9_o @ epoch 4 new loss 0.0001927359844557941 old loss 0.00019636594515759498 BETTER
I0320 20:12:09.713514 1528846 finetune.py:45] layer 11_o initial loss 0.0003350683255121112
I0320 20:12:28.261873 1528675 finetune.py:68] layer 10_o @ epoch 2 new loss 0.00027495590620674193 old loss 0.00028302709688432515 BETTER
I0320 20:12:35.120805 1528321 finetune.py:68] layer 8_up @ epoch 1 new loss 0.00023519780370406806 old loss 0.0002389386499999091 BETTER
I0320 20:12:40.177090 1528507 finetune.py:45] layer 9_up initial loss 0.00029629975324496627
I0320 20:12:42.940309 1528846 finetune.py:68] layer 11_o @ epoch 0 new loss 0.00031051860423758626 old loss 0.0003350683255121112 BETTER
I0320 20:13:04.428483 1528675 finetune.py:68] layer 10_o @ epoch 3 new loss 0.0002686673542484641 old loss 0.00027495590620674193 BETTER
I0320 20:13:10.912563 1528321 finetune.py:68] layer 8_up @ epoch 2 new loss 0.0002322235086467117 old loss 0.00023519780370406806 BETTER
I0320 20:13:13.339296 1528507 finetune.py:68] layer 9_up @ epoch 0 new loss 0.0002882417757064104 old loss 0.00029629975324496627 BETTER
I0320 20:13:17.907493 1528846 finetune.py:68] layer 11_o @ epoch 1 new loss 0.0002988833875861019 old loss 0.00031051860423758626 BETTER
I0320 20:13:40.790580 1528675 finetune.py:68] layer 10_o @ epoch 4 new loss 0.00026354449801146984 old loss 0.0002686673542484641 BETTER
I0320 20:13:46.734312 1528321 finetune.py:68] layer 8_up @ epoch 3 new loss 0.00022970479039940983 old loss 0.0002322235086467117 BETTER
I0320 20:13:46.998975 1528507 finetune.py:68] layer 9_up @ epoch 1 new loss 0.0002835612394846976 old loss 0.0002882417757064104 BETTER
I0320 20:13:52.089449 1528846 finetune.py:68] layer 11_o @ epoch 2 new loss 0.0002907832677010447 old loss 0.0002988833875861019 BETTER
I0320 20:14:14.005119 1528675 finetune.py:45] layer 10_up initial loss 0.00038142732228152454
I0320 20:14:21.259373 1528507 finetune.py:68] layer 9_up @ epoch 2 new loss 0.00027986866189166903 old loss 0.0002835612394846976 BETTER
I0320 20:14:22.718438 1528321 finetune.py:68] layer 8_up @ epoch 4 new loss 0.00022748693299945444 old loss 0.00022970479039940983 BETTER
I0320 20:14:26.988159 1528846 finetune.py:68] layer 11_o @ epoch 3 new loss 0.0002846205607056618 old loss 0.0002907832677010447 BETTER
I0320 20:14:46.320766 1528675 finetune.py:68] layer 10_up @ epoch 0 new loss 0.0003709954326041043 old loss 0.00038142732228152454 BETTER
I0320 20:14:54.461059 1528321 finetune.py:45] layer 8_gate initial loss 0.00028197458595968783
I0320 20:14:56.395674 1528507 finetune.py:68] layer 9_up @ epoch 3 new loss 0.00027674066950567067 old loss 0.00027986866189166903 BETTER
I0320 20:15:02.115508 1528846 finetune.py:68] layer 11_o @ epoch 4 new loss 0.0002795279142446816 old loss 0.0002846205607056618 BETTER
I0320 20:15:19.914173 1528675 finetune.py:68] layer 10_up @ epoch 1 new loss 0.000365070765838027 old loss 0.0003709954326041043 BETTER
I0320 20:15:26.244697 1528321 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.0002789644058793783 old loss 0.00028197458595968783 BETTER
I0320 20:15:30.301765 1528507 finetune.py:68] layer 9_up @ epoch 4 new loss 0.0002740144554991275 old loss 0.00027674066950567067 BETTER
I0320 20:15:35.343582 1528846 finetune.py:45] layer 11_up initial loss 0.00041210828931070864
I0320 20:15:54.175526 1528675 finetune.py:68] layer 10_up @ epoch 2 new loss 0.0003602839424274862 old loss 0.000365070765838027 BETTER
I0320 20:15:59.648131 1528321 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.0002767898258753121 old loss 0.0002789644058793783 BETTER
I0320 20:16:01.725283 1528507 finetune.py:45] layer 9_gate initial loss 0.00034008309012278914
I0320 20:16:07.383200 1528846 finetune.py:68] layer 11_up @ epoch 0 new loss 0.0004013788129668683 old loss 0.00041210828931070864 BETTER
I0320 20:16:29.533942 1528675 finetune.py:68] layer 10_up @ epoch 3 new loss 0.00035620041307993233 old loss 0.0003602839424274862 BETTER
I0320 20:16:33.595007 1528507 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0003363782598171383 old loss 0.00034008309012278914 BETTER
I0320 20:16:34.317307 1528321 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.0002749655395746231 old loss 0.0002767898258753121 BETTER
I0320 20:16:40.671732 1528846 finetune.py:68] layer 11_up @ epoch 1 new loss 0.00039527894114144146 old loss 0.0004013788129668683 BETTER
I0320 20:17:02.145813 1528675 finetune.py:68] layer 10_up @ epoch 4 new loss 0.00035263015888631344 old loss 0.00035620041307993233 BETTER
I0320 20:17:03.930764 1528507 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.0003337398811709136 old loss 0.0003363782598171383 BETTER
I0320 20:17:05.967149 1528321 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.0002733654691837728 old loss 0.0002749655395746231 BETTER
I0320 20:17:15.268598 1528846 finetune.py:68] layer 11_up @ epoch 2 new loss 0.0003904263721778989 old loss 0.00039527894114144146 BETTER
I0320 20:17:34.119002 1528675 finetune.py:45] layer 10_gate initial loss 0.00043183055822737515
I0320 20:17:38.396546 1528507 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.00033149824594147503 old loss 0.0003337398811709136 BETTER
I0320 20:17:40.584450 1528321 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.000271920784143731 old loss 0.0002733654691837728 BETTER
I0320 20:17:48.012534 1528846 finetune.py:68] layer 11_up @ epoch 3 new loss 0.00038626143941655755 old loss 0.0003904263721778989 BETTER
I0320 20:18:05.959369 1528675 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.0004269756027497351 old loss 0.00043183055822737515 BETTER
I0320 20:18:11.406799 1528507 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.0003295059723313898 old loss 0.00033149824594147503 BETTER
I0320 20:18:12.517209 1528321 finetune.py:45] layer 8_down initial loss 0.00040694946073926985
I0320 20:18:22.624451 1528846 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00038261478766798973 old loss 0.00038626143941655755 BETTER
I0320 20:18:40.238435 1528675 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.0004235920787323266 old loss 0.0004269756027497351 BETTER
I0320 20:18:44.236830 1528321 finetune.py:68] layer 8_down @ epoch 0 new loss 0.00040682419785298407 old loss 0.00040694946073926985 BETTER
I0320 20:18:46.511102 1528507 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0003277370997238904 old loss 0.0003295059723313898 BETTER
I0320 20:18:54.339484 1528846 finetune.py:45] layer 11_gate initial loss 0.0004713920643553138
I0320 20:19:13.814074 1528675 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.00042066926835104823 old loss 0.0004235920787323266 BETTER
I0320 20:19:18.217312 1528321 finetune.py:68] layer 8_down @ epoch 1 new loss 0.0004067473637405783 old loss 0.00040682419785298407 BETTER
I0320 20:19:23.356179 1528507 finetune.py:45] layer 9_down initial loss 0.00048448736197315156
I0320 20:19:27.532821 1528846 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.00046643035602755845 old loss 0.0004713920643553138 BETTER
I0320 20:19:48.155536 1528675 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.000418103183619678 old loss 0.00042066926835104823 BETTER
I0320 20:19:52.159873 1528321 finetune.py:68] layer 8_down @ epoch 2 new loss 0.00040669634472578764 old loss 0.0004067473637405783 BETTER
I0320 20:19:55.225679 1528507 finetune.py:68] layer 9_down @ epoch 0 new loss 0.0004843551432713866 old loss 0.00048448736197315156 BETTER
I0320 20:20:01.023043 1528846 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.0004629783798009157 old loss 0.00046643035602755845 BETTER
I0320 20:20:23.589803 1528675 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.00041580406832508743 old loss 0.000418103183619678 BETTER
I0320 20:20:26.248014 1528321 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0004066603141836822 old loss 0.00040669634472578764 BETTER
I0320 20:20:27.849099 1528507 finetune.py:68] layer 9_down @ epoch 1 new loss 0.00048426646389998496 old loss 0.0004843551432713866 BETTER
I0320 20:20:33.981068 1528846 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.00045997550478205085 old loss 0.0004629783798009157 BETTER
I0320 20:21:00.808477 1528675 finetune.py:45] layer 10_down initial loss 0.000597206293605268
I0320 20:21:00.864852 1528321 finetune.py:68] layer 8_down @ epoch 4 new loss 0.00040663493564352393 old loss 0.0004066603141836822 BETTER
I0320 20:21:01.512897 1528507 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00048420351231470704 old loss 0.00048426646389998496 BETTER
8_v proxy err 0.024089381098747253 tr(WHW.T) 530.9967041015625
bpp_loss 2.9884990453720093
8_q proxy err 0.002270004479214549 tr(WHW.T) 7240.17919921875
bpp_loss 3.726596474647522
8_k proxy err 0.0015738599468022585 tr(WHW.T) 10670.0390625
bpp_loss 3.7367738485336304
8_o proxy err 0.026319582015275955 tr(WHW.T) 20.284799575805664
bpp_loss 2.973792552947998
8_up proxy err 0.012032654136419296 tr(WHW.T) 866.4723510742188
bpp_loss 3.2155573645303415
8_gate proxy err 0.005581630393862724 tr(WHW.T) 1971.6414794921875
bpp_loss 3.3413976624954578
8_down proxy err 0.01534628588706255 tr(WHW.T) 37.4548225402832
bpp_loss 3.207034465878509
I0320 20:21:05.947856 1528846 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.00045734940795227885 old loss 0.00045997550478205085 BETTER
I0320 20:21:28.992558 1528675 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0005970080965198576 old loss 0.000597206293605268 BETTER
I0320 20:21:31.980880 1528507 finetune.py:68] layer 9_down @ epoch 3 new loss 0.00048415770288556814 old loss 0.00048420351231470704 BETTER
I0320 20:21:38.720225 1528846 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.0004550287267193198 old loss 0.00045734940795227885 BETTER
I0320 20:21:59.420713 1528675 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0005968838231638074 old loss 0.0005970080965198576 BETTER
I0320 20:22:01.942293 1528507 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00048412446631118655 old loss 0.00048415770288556814 BETTER
9_v proxy err 0.02268657647073269 tr(WHW.T) 565.0663452148438
bpp_loss 3.0062332153320312
9_q proxy err 0.0023362180218100548 tr(WHW.T) 6977.08544921875
bpp_loss 3.7288708686828613
9_k proxy err 0.0015289578586816788 tr(WHW.T) 11015.9306640625
bpp_loss 3.7721242904663086
9_o proxy err 0.026227833703160286 tr(WHW.T) 25.817201614379883
bpp_loss 2.993558168411255
9_up proxy err 0.01162340771406889 tr(WHW.T) 970.4138793945312
bpp_loss 3.22367663716161
9_gate proxy err 0.005555071402341127 tr(WHW.T) 2131.581787109375
bpp_loss 3.328544439271439
9_down proxy err 0.015396113507449627 tr(WHW.T) 43.294315338134766
bpp_loss 3.2153580465982126
I0320 20:22:07.296433 1528846 finetune.py:45] layer 11_down initial loss 0.000655278330668807
I0320 20:22:27.301315 1528675 finetune.py:68] layer 10_down @ epoch 2 new loss 0.0005967954057268798 old loss 0.0005968838231638074 BETTER
I0320 20:22:33.685583 1528846 finetune.py:68] layer 11_down @ epoch 0 new loss 0.0006550801335833967 old loss 0.000655278330668807 BETTER
I0320 20:22:55.066584 1528675 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0005967324250377715 old loss 0.0005967954057268798 BETTER
I0320 20:23:00.695695 1528846 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0006549452082253993 old loss 0.0006550801335833967 BETTER
I0320 20:23:15.489065 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 12 in 67.19411277770996s
I0320 20:23:19.683528 1530260 config.py:54] PyTorch version 2.6.0 available.
W0320 20:23:20.076972 1530260 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 20:23:21.231970 1530260 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:23:21.236210 1523378 quantize_finetune_llama.py:203] layer 13 gpu 1
I0320 20:23:21.252662 1530260 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 20:23:25.117753 1528675 finetune.py:68] layer 10_down @ epoch 4 new loss 0.0005966859753243625 old loss 0.0005967324250377715 BETTER
10_v proxy err 0.022925667464733124 tr(WHW.T) 578.807373046875
bpp_loss 2.9970957040786743
10_q proxy err 0.0024107289500534534 tr(WHW.T) 6922.29052734375
bpp_loss 3.72649610042572
10_k proxy err 0.0015923905884847045 tr(WHW.T) 11026.205078125
bpp_loss 3.779232978820801
10_o proxy err 0.027004418894648552 tr(WHW.T) 35.47623062133789
bpp_loss 2.9899812936782837
10_up proxy err 0.010991104878485203 tr(WHW.T) 1080.0980224609375
bpp_loss 3.23646740580714
10_gate proxy err 0.005487353075295687 tr(WHW.T) 2263.05419921875
bpp_loss 3.3243614019349566
10_down proxy err 0.014670160599052906 tr(WHW.T) 52.7833251953125
bpp_loss 3.2256350184595863
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:23:28.823265 1528846 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0006548517267219722 old loss 0.0006549452082253993 BETTER
I0320 20:23:34.063604 1530260 finetune.py:45] layer 12_v initial loss 0.00033064940362237394
W0320 20:23:34.063984 1530260 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:23:56.179979 1528846 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0006547820521518588 old loss 0.0006548517267219722 BETTER
I0320 20:24:08.598376 1530260 finetune.py:68] layer 12_v @ epoch 0 new loss 0.00017386705440003425 old loss 0.00033064940362237394 BETTER
I0320 20:24:24.051110 1528846 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0006547251832671463 old loss 0.0006547820521518588 BETTER
11_v proxy err 0.021623488515615463 tr(WHW.T) 723.1956176757812
bpp_loss 3.0362213850021362
11_q proxy err 0.0027171734254807234 tr(WHW.T) 7035.22119140625
bpp_loss 3.640990972518921
11_k proxy err 0.00187910336535424 tr(WHW.T) 10542.1953125
bpp_loss 3.636608839035034
11_o proxy err 0.02724369615316391 tr(WHW.T) 36.95368957519531
bpp_loss 3.026963233947754
11_up proxy err 0.011187780648469925 tr(WHW.T) 1139.996826171875
bpp_loss 3.2468194296193675
11_gate proxy err 0.0055405632592737675 tr(WHW.T) 2396.572998046875
bpp_loss 3.319531108057776
11_down proxy err 0.015033800154924393 tr(WHW.T) 56.599884033203125
bpp_loss 3.2334998818330987
I0320 20:24:37.338732 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 13 in 66.70393085479736s
I0320 20:24:40.757081 1530314 config.py:54] PyTorch version 2.6.0 available.
W0320 20:24:41.057108 1530314 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 20:24:41.990884 1530314 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:24:41.994915 1523378 quantize_finetune_llama.py:203] layer 14 gpu 2
I0320 20:24:42.007699 1530314 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 20:24:43.236277 1530260 finetune.py:68] layer 12_v @ epoch 1 new loss 0.00014665842172689736 old loss 0.00017386705440003425 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:24:55.479680 1530314 finetune.py:45] layer 13_v initial loss 0.00031823484459891915
W0320 20:24:55.479953 1530314 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:25:19.895862 1530260 finetune.py:68] layer 12_v @ epoch 2 new loss 0.00013432736159302294 old loss 0.00014665842172689736 BETTER
I0320 20:25:27.851376 1530314 finetune.py:68] layer 13_v @ epoch 0 new loss 0.0001722393644740805 old loss 0.00031823484459891915 BETTER
I0320 20:25:56.788743 1530260 finetune.py:68] layer 12_v @ epoch 3 new loss 0.00012662724475376308 old loss 0.00013432736159302294 BETTER
I0320 20:25:59.075414 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 14 in 76.59795117378235s
I0320 20:26:02.523245 1530314 finetune.py:68] layer 13_v @ epoch 1 new loss 0.0001473336888011545 old loss 0.0001722393644740805 BETTER
I0320 20:26:03.398218 1530368 config.py:54] PyTorch version 2.6.0 available.
W0320 20:26:03.774846 1530368 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 20:26:04.797563 1530368 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:26:04.801869 1523378 quantize_finetune_llama.py:203] layer 15 gpu 3
I0320 20:26:04.820762 1530368 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:26:19.360395 1530368 finetune.py:45] layer 14_v initial loss 0.0004133290203753859
W0320 20:26:19.360862 1530368 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:26:34.827542 1530260 finetune.py:68] layer 12_v @ epoch 4 new loss 0.00012117462756577879 old loss 0.00012662724475376308 BETTER
I0320 20:26:38.079617 1530314 finetune.py:68] layer 13_v @ epoch 2 new loss 0.00013568524445872754 old loss 0.0001473336888011545 BETTER
I0320 20:26:52.625665 1530260 finetune.py:45] layer 12_q initial loss 0.00015273709141183645
I0320 20:26:54.440533 1530368 finetune.py:68] layer 14_v @ epoch 0 new loss 0.00022592686582356691 old loss 0.0004133290203753859 BETTER
I0320 20:27:14.815592 1530314 finetune.py:68] layer 13_v @ epoch 3 new loss 0.00012834373046644032 old loss 0.00013568524445872754 BETTER
I0320 20:27:25.125686 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 15 in 79.82359099388123s
I0320 20:27:28.210367 1530260 finetune.py:68] layer 12_q @ epoch 0 new loss 0.00013909826520830393 old loss 0.00015273709141183645 BETTER
I0320 20:27:29.788925 1530422 config.py:54] PyTorch version 2.6.0 available.
W0320 20:27:30.199933 1530422 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0320 20:27:30.715748 1530368 finetune.py:68] layer 14_v @ epoch 1 new loss 0.00019353585958015174 old loss 0.00022592686582356691 BETTER
W0320 20:27:31.315715 1530422 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:27:31.320190 1523378 quantize_finetune_llama.py:203] layer 16 gpu 0
I0320 20:27:31.335012 1530422 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:27:48.094989 1530422 finetune.py:45] layer 15_v initial loss 0.0004063808300998062
W0320 20:27:48.095372 1530422 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:27:52.056648 1530314 finetune.py:68] layer 13_v @ epoch 4 new loss 0.00012306691496632993 old loss 0.00012834373046644032 BETTER
I0320 20:28:06.086205 1530260 finetune.py:68] layer 12_q @ epoch 1 new loss 0.0001336044369963929 old loss 0.00013909826520830393 BETTER
I0320 20:28:07.913700 1530368 finetune.py:68] layer 14_v @ epoch 2 new loss 0.00017792430298868567 old loss 0.00019353585958015174 BETTER
I0320 20:28:10.986891 1530314 finetune.py:45] layer 13_q initial loss 0.00015927606727927923
I0320 20:28:23.103921 1530422 finetune.py:68] layer 15_v @ epoch 0 new loss 0.00021767352882307023 old loss 0.0004063808300998062 BETTER
I0320 20:28:44.471237 1530260 finetune.py:68] layer 12_q @ epoch 2 new loss 0.00012953131226822734 old loss 0.0001336044369963929 BETTER
I0320 20:28:45.958134 1530368 finetune.py:68] layer 14_v @ epoch 3 new loss 0.00016795791452750564 old loss 0.00017792430298868567 BETTER
I0320 20:28:46.771801 1530314 finetune.py:68] layer 13_q @ epoch 0 new loss 0.00014163111336529255 old loss 0.00015927606727927923 BETTER
I0320 20:28:59.221962 1530422 finetune.py:68] layer 15_v @ epoch 1 new loss 0.00018797120719682425 old loss 0.00021767352882307023 BETTER
I0320 20:29:23.733693 1530260 finetune.py:68] layer 12_q @ epoch 3 new loss 0.0001263416779693216 old loss 0.00012953131226822734 BETTER
I0320 20:29:24.033725 1530368 finetune.py:68] layer 14_v @ epoch 4 new loss 0.00016102181689348072 old loss 0.00016795791452750564 BETTER
I0320 20:29:24.458027 1530314 finetune.py:68] layer 13_q @ epoch 1 new loss 0.00013605112326331437 old loss 0.00014163111336529255 BETTER
I0320 20:29:35.359493 1530422 finetune.py:68] layer 15_v @ epoch 2 new loss 0.00017354961892124265 old loss 0.00018797120719682425 BETTER
I0320 20:29:42.453917 1530368 finetune.py:45] layer 14_q initial loss 0.0001988291769521311
I0320 20:30:01.275294 1530314 finetune.py:68] layer 13_q @ epoch 2 new loss 0.00013197118823882192 old loss 0.00013605112326331437 BETTER
I0320 20:30:01.318097 1530260 finetune.py:68] layer 12_q @ epoch 4 new loss 0.0001236655079992488 old loss 0.0001263416779693216 BETTER
I0320 20:30:11.745681 1530422 finetune.py:68] layer 15_v @ epoch 3 new loss 0.00016430328832939267 old loss 0.00017354961892124265 BETTER
I0320 20:30:17.909308 1530368 finetune.py:68] layer 14_q @ epoch 0 new loss 0.0001813203125493601 old loss 0.0001988291769521311 BETTER
I0320 20:30:18.990211 1530260 finetune.py:45] layer 12_k initial loss 0.00015446259931195527
I0320 20:30:37.605480 1530314 finetune.py:68] layer 13_q @ epoch 3 new loss 0.0001287349296035245 old loss 0.00013197118823882192 BETTER
I0320 20:30:49.114181 1530422 finetune.py:68] layer 15_v @ epoch 4 new loss 0.00015751048340462148 old loss 0.00016430328832939267 BETTER
I0320 20:30:54.612479 1530260 finetune.py:68] layer 12_k @ epoch 0 new loss 0.00014211652160156518 old loss 0.00015446259931195527 BETTER
I0320 20:30:54.788534 1530368 finetune.py:68] layer 14_q @ epoch 1 new loss 0.00017413574096281081 old loss 0.0001813203125493601 BETTER
I0320 20:31:06.883099 1530422 finetune.py:45] layer 15_q initial loss 0.00020146957831457257
I0320 20:31:12.939487 1530314 finetune.py:68] layer 13_q @ epoch 4 new loss 0.0001260928256670013 old loss 0.0001287349296035245 BETTER
I0320 20:31:30.029112 1530314 finetune.py:45] layer 13_k initial loss 0.00015674250607844442
I0320 20:31:32.013027 1530260 finetune.py:68] layer 12_k @ epoch 1 new loss 0.0001396375591866672 old loss 0.00014211652160156518 BETTER
I0320 20:31:32.270318 1530368 finetune.py:68] layer 14_q @ epoch 2 new loss 0.0001688487536739558 old loss 0.00017413574096281081 BETTER
I0320 20:31:41.035114 1530422 finetune.py:68] layer 15_q @ epoch 0 new loss 0.00018089928198605776 old loss 0.00020146957831457257 BETTER
I0320 20:32:05.590844 1530314 finetune.py:68] layer 13_k @ epoch 0 new loss 0.00014549486513715237 old loss 0.00015674250607844442 BETTER
I0320 20:32:09.739561 1530368 finetune.py:68] layer 14_q @ epoch 3 new loss 0.0001646067830733955 old loss 0.0001688487536739558 BETTER
I0320 20:32:10.161402 1530260 finetune.py:68] layer 12_k @ epoch 2 new loss 0.00013760116416960955 old loss 0.0001396375591866672 BETTER
I0320 20:32:16.460082 1530422 finetune.py:68] layer 15_q @ epoch 1 new loss 0.00017364155792165548 old loss 0.00018089928198605776 BETTER
I0320 20:32:41.961171 1530314 finetune.py:68] layer 13_k @ epoch 1 new loss 0.0001429924159310758 old loss 0.00014549486513715237 BETTER
I0320 20:32:46.913022 1530368 finetune.py:68] layer 14_q @ epoch 4 new loss 0.00016110447177197784 old loss 0.0001646067830733955 BETTER
I0320 20:32:47.576000 1530260 finetune.py:68] layer 12_k @ epoch 3 new loss 0.0001358663139399141 old loss 0.00013760116416960955 BETTER
I0320 20:32:52.839334 1530422 finetune.py:68] layer 15_q @ epoch 2 new loss 0.00016823505575302988 old loss 0.00017364155792165548 BETTER
I0320 20:33:04.348170 1530368 finetune.py:45] layer 14_k initial loss 0.00019155387417413294
I0320 20:33:17.769259 1530314 finetune.py:68] layer 13_k @ epoch 2 new loss 0.00014095738879404962 old loss 0.0001429924159310758 BETTER
I0320 20:33:24.708194 1530260 finetune.py:68] layer 12_k @ epoch 4 new loss 0.00013431823754217476 old loss 0.0001358663139399141 BETTER
I0320 20:33:28.369693 1530422 finetune.py:68] layer 15_q @ epoch 3 new loss 0.00016394758131355047 old loss 0.00016823505575302988 BETTER
I0320 20:33:39.020951 1530368 finetune.py:68] layer 14_k @ epoch 0 new loss 0.0001834948780015111 old loss 0.00019155387417413294 BETTER
I0320 20:33:43.379311 1530260 finetune.py:45] layer 12_o initial loss 0.000368822569726035
I0320 20:33:53.396190 1530314 finetune.py:68] layer 13_k @ epoch 3 new loss 0.0001391990517731756 old loss 0.00014095738879404962 BETTER
I0320 20:34:04.072180 1530422 finetune.py:68] layer 15_q @ epoch 4 new loss 0.00016038079047575593 old loss 0.00016394758131355047 BETTER
I0320 20:34:14.454250 1530368 finetune.py:68] layer 14_k @ epoch 1 new loss 0.00018030167848337442 old loss 0.0001834948780015111 BETTER
I0320 20:34:18.361044 1530260 finetune.py:68] layer 12_o @ epoch 0 new loss 0.0003427285118959844 old loss 0.000368822569726035 BETTER
I0320 20:34:23.120060 1530422 finetune.py:45] layer 15_k initial loss 0.00019253557547926903
I0320 20:34:29.165299 1530314 finetune.py:68] layer 13_k @ epoch 4 new loss 0.00013768632197752595 old loss 0.0001391990517731756 BETTER
I0320 20:34:45.415059 1530314 finetune.py:45] layer 13_o initial loss 0.0003747151349671185
I0320 20:34:50.932438 1530368 finetune.py:68] layer 14_k @ epoch 2 new loss 0.00017766360542736948 old loss 0.00018030167848337442 BETTER
I0320 20:34:55.456880 1530260 finetune.py:68] layer 12_o @ epoch 1 new loss 0.00032999765244312584 old loss 0.0003427285118959844 BETTER
I0320 20:34:58.037908 1530422 finetune.py:68] layer 15_k @ epoch 0 new loss 0.00018431573698762804 old loss 0.00019253557547926903 BETTER
I0320 20:35:20.086897 1530314 finetune.py:68] layer 13_o @ epoch 0 new loss 0.0003467453352641314 old loss 0.0003747151349671185 BETTER
I0320 20:35:27.928890 1530368 finetune.py:68] layer 14_k @ epoch 3 new loss 0.00017538241809234023 old loss 0.00017766360542736948 BETTER
I0320 20:35:32.352530 1530260 finetune.py:68] layer 12_o @ epoch 2 new loss 0.0003209667047485709 old loss 0.00032999765244312584 BETTER
I0320 20:35:34.163725 1530422 finetune.py:68] layer 15_k @ epoch 1 new loss 0.00018101601745001972 old loss 0.00018431573698762804 BETTER
I0320 20:35:55.585769 1530314 finetune.py:68] layer 13_o @ epoch 1 new loss 0.0003332339692860842 old loss 0.0003467453352641314 BETTER
I0320 20:36:04.335693 1530368 finetune.py:68] layer 14_k @ epoch 4 new loss 0.00017342876526527107 old loss 0.00017538241809234023 BETTER
I0320 20:36:09.111504 1530260 finetune.py:68] layer 12_o @ epoch 3 new loss 0.00031405576737597585 old loss 0.0003209667047485709 BETTER
I0320 20:36:09.896408 1530422 finetune.py:68] layer 15_k @ epoch 2 new loss 0.00017833831952884793 old loss 0.00018101601745001972 BETTER
I0320 20:36:22.511696 1530368 finetune.py:45] layer 14_o initial loss 0.00047114494373090565
I0320 20:36:31.084415 1530314 finetune.py:68] layer 13_o @ epoch 2 new loss 0.000323792191920802 old loss 0.0003332339692860842 BETTER
I0320 20:36:45.675038 1530422 finetune.py:68] layer 15_k @ epoch 3 new loss 0.00017605521134100854 old loss 0.00017833831952884793 BETTER
I0320 20:36:45.731042 1530260 finetune.py:68] layer 12_o @ epoch 4 new loss 0.00030854696524329484 old loss 0.00031405576737597585 BETTER
I0320 20:36:56.769634 1530368 finetune.py:68] layer 14_o @ epoch 0 new loss 0.00043988722609356046 old loss 0.00047114494373090565 BETTER
I0320 20:37:06.339940 1530314 finetune.py:68] layer 13_o @ epoch 3 new loss 0.000316555961035192 old loss 0.000323792191920802 BETTER
I0320 20:37:15.033804 1530260 finetune.py:45] layer 12_up initial loss 0.00045669786049984396
I0320 20:37:20.769500 1530422 finetune.py:68] layer 15_k @ epoch 4 new loss 0.00017406010010745376 old loss 0.00017605521134100854 BETTER
I0320 20:37:31.658698 1530368 finetune.py:68] layer 14_o @ epoch 1 new loss 0.0004247490142006427 old loss 0.00043988722609356046 BETTER
I0320 20:37:39.324661 1530422 finetune.py:45] layer 15_o initial loss 0.0004753871471621096
I0320 20:37:42.094529 1530314 finetune.py:68] layer 13_o @ epoch 4 new loss 0.00031077282619662583 old loss 0.000316555961035192 BETTER
I0320 20:37:47.232557 1530260 finetune.py:68] layer 12_up @ epoch 0 new loss 0.0004451083659660071 old loss 0.00045669786049984396 BETTER
I0320 20:38:07.735026 1530368 finetune.py:68] layer 14_o @ epoch 2 new loss 0.0004140544042456895 old loss 0.0004247490142006427 BETTER
I0320 20:38:11.714304 1530314 finetune.py:45] layer 13_up initial loss 0.00048820750089362264
I0320 20:38:13.477766 1530422 finetune.py:68] layer 15_o @ epoch 0 new loss 0.00043672253377735615 old loss 0.0004753871471621096 BETTER
I0320 20:38:21.721145 1530260 finetune.py:68] layer 12_up @ epoch 1 new loss 0.00043837318662554026 old loss 0.0004451083659660071 BETTER
I0320 20:38:44.601689 1530314 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00047362264012917876 old loss 0.00048820750089362264 BETTER
I0320 20:38:44.953704 1530368 finetune.py:68] layer 14_o @ epoch 3 new loss 0.00040570509736426175 old loss 0.0004140544042456895 BETTER
I0320 20:38:48.332725 1530422 finetune.py:68] layer 15_o @ epoch 1 new loss 0.000419728021370247 old loss 0.00043672253377735615 BETTER
I0320 20:38:56.078991 1530260 finetune.py:68] layer 12_up @ epoch 2 new loss 0.0004329834191594273 old loss 0.00043837318662554026 BETTER
I0320 20:39:19.343262 1530314 finetune.py:68] layer 13_up @ epoch 1 new loss 0.0004655530210584402 old loss 0.00047362264012917876 BETTER
I0320 20:39:21.657707 1530368 finetune.py:68] layer 14_o @ epoch 4 new loss 0.00039895030204206705 old loss 0.00040570509736426175 BETTER
I0320 20:39:23.575060 1530422 finetune.py:68] layer 15_o @ epoch 2 new loss 0.00040796451503410935 old loss 0.000419728021370247 BETTER
I0320 20:39:30.507137 1530260 finetune.py:68] layer 12_up @ epoch 3 new loss 0.0004283167654648423 old loss 0.0004329834191594273 BETTER
I0320 20:39:52.411403 1530368 finetune.py:45] layer 14_up initial loss 0.0005952719366177917
I0320 20:39:53.921271 1530314 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0004591938923113048 old loss 0.0004655530210584402 BETTER
I0320 20:39:58.412218 1530422 finetune.py:68] layer 15_o @ epoch 3 new loss 0.0003990961704403162 old loss 0.00040796451503410935 BETTER
I0320 20:40:04.953852 1530260 finetune.py:68] layer 12_up @ epoch 4 new loss 0.00042430125176906586 old loss 0.0004283167654648423 BETTER
I0320 20:40:25.926810 1530368 finetune.py:68] layer 14_up @ epoch 0 new loss 0.0005802697269245982 old loss 0.0005952719366177917 BETTER
I0320 20:40:27.994688 1530314 finetune.py:68] layer 13_up @ epoch 3 new loss 0.00045379556831903756 old loss 0.0004591938923113048 BETTER
I0320 20:40:33.807511 1530422 finetune.py:68] layer 15_o @ epoch 4 new loss 0.00039198686135932803 old loss 0.0003990961704403162 BETTER
I0320 20:40:35.536754 1530260 finetune.py:45] layer 12_gate initial loss 0.0005281207268126309
I0320 20:41:00.746907 1530368 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0005715237348340452 old loss 0.0005802697269245982 BETTER
I0320 20:41:02.981942 1530314 finetune.py:68] layer 13_up @ epoch 4 new loss 0.0004491720756050199 old loss 0.00045379556831903756 BETTER
I0320 20:41:04.412688 1530422 finetune.py:45] layer 15_up initial loss 0.0006312003824859858
I0320 20:41:07.102565 1530260 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.0005226460052654147 old loss 0.0005281207268126309 BETTER
I0320 20:41:33.409652 1530314 finetune.py:45] layer 13_gate initial loss 0.0005753628211095929
I0320 20:41:36.305619 1530368 finetune.py:68] layer 14_up @ epoch 2 new loss 0.0005646641366183758 old loss 0.0005715237348340452 BETTER
I0320 20:41:37.278898 1530422 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0006121559417806566 old loss 0.0006312003824859858 BETTER
I0320 20:41:40.293881 1530260 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0005187572096474469 old loss 0.0005226460052654147 BETTER
I0320 20:42:04.501091 1530314 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.0005684675998054445 old loss 0.0005753628211095929 BETTER
I0320 20:42:11.860870 1530368 finetune.py:68] layer 14_up @ epoch 3 new loss 0.0005587964551523328 old loss 0.0005646641366183758 BETTER
I0320 20:42:12.117101 1530422 finetune.py:68] layer 15_up @ epoch 1 new loss 0.0006015970138832927 old loss 0.0006121559417806566 BETTER
I0320 20:42:14.252065 1530260 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.000515432155225426 old loss 0.0005187572096474469 BETTER
I0320 20:42:36.766422 1530314 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0005637678550556302 old loss 0.0005684675998054445 BETTER
I0320 20:42:47.244995 1530422 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0005932962521910667 old loss 0.0006015970138832927 BETTER
I0320 20:42:47.253171 1530368 finetune.py:68] layer 14_up @ epoch 4 new loss 0.0005536783719435334 old loss 0.0005587964551523328 BETTER
I0320 20:42:48.471835 1530260 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.000512528873514384 old loss 0.000515432155225426 BETTER
I0320 20:43:08.459325 1530314 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0005598182906396687 old loss 0.0005637678550556302 BETTER
I0320 20:43:18.426165 1530368 finetune.py:45] layer 14_gate initial loss 0.0007000437472015619
I0320 20:43:22.076218 1530422 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0005864565027877688 old loss 0.0005932962521910667 BETTER
I0320 20:43:22.517846 1530260 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.0005099321133457124 old loss 0.000512528873514384 BETTER
I0320 20:43:40.831688 1530314 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.000556342420168221 old loss 0.0005598182906396687 BETTER
I0320 20:43:49.572584 1530368 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0006925510242581367 old loss 0.0007000437472015619 BETTER
I0320 20:43:55.616491 1530422 finetune.py:68] layer 15_up @ epoch 4 new loss 0.000580645224545151 old loss 0.0005864565027877688 BETTER
I0320 20:43:56.360103 1530260 finetune.py:45] layer 12_down initial loss 0.0007391573744826019
I0320 20:44:12.664999 1530314 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0005532726063393056 old loss 0.000556342420168221 BETTER
I0320 20:44:21.904387 1530368 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0006873473175801337 old loss 0.0006925510242581367 BETTER
I0320 20:44:25.859145 1530422 finetune.py:45] layer 15_gate initial loss 0.0007623148849233985
I0320 20:44:26.095437 1530260 finetune.py:68] layer 12_down @ epoch 0 new loss 0.0007389920647256076 old loss 0.0007391573744826019 BETTER
I0320 20:44:45.432971 1530314 finetune.py:45] layer 13_down initial loss 0.000837718544062227
I0320 20:44:55.442134 1530368 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.000682992220390588 old loss 0.0006873473175801337 BETTER
I0320 20:44:57.746428 1530422 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.000752683263272047 old loss 0.0007623148849233985 BETTER
I0320 20:44:57.943214 1530260 finetune.py:68] layer 12_down @ epoch 1 new loss 0.0007388713420368731 old loss 0.0007389920647256076 BETTER
I0320 20:45:14.920156 1530314 finetune.py:68] layer 13_down @ epoch 0 new loss 0.0008374879835173488 old loss 0.000837718544062227 BETTER
I0320 20:45:29.479200 1530368 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.0006791490013711154 old loss 0.000682992220390588 BETTER
I0320 20:45:30.640955 1530260 finetune.py:68] layer 12_down @ epoch 2 new loss 0.0007387903751805425 old loss 0.0007388713420368731 BETTER
I0320 20:45:30.961885 1530422 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.0007462330395355821 old loss 0.000752683263272047 BETTER
I0320 20:45:46.131514 1530314 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0008373166201636195 old loss 0.0008374879835173488 BETTER
I0320 20:46:03.084619 1530260 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0007387303048744798 old loss 0.0007387903751805425 BETTER
I0320 20:46:03.178736 1530368 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0006757627706974745 old loss 0.0006791490013711154 BETTER
I0320 20:46:03.917653 1530422 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.0007409589597955346 old loss 0.0007462330395355821 BETTER
I0320 20:46:16.124505 1530314 finetune.py:68] layer 13_down @ epoch 2 new loss 0.0008372006705030799 old loss 0.0008373166201636195 BETTER
I0320 20:46:35.099581 1530260 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0007386848446913064 old loss 0.0007387303048744798 BETTER
I0320 20:46:36.208950 1530368 finetune.py:45] layer 14_down initial loss 0.0010069416603073478
I0320 20:46:36.303621 1530422 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0007363618933595717 old loss 0.0007409589597955346 BETTER
12_v proxy err 0.022119823843240738 tr(WHW.T) 703.318603515625
bpp_loss 3.0242568254470825
12_q proxy err 0.002752101980149746 tr(WHW.T) 7053.453125
bpp_loss 3.6746968030929565
12_k proxy err 0.001822157297283411 tr(WHW.T) 10924.6806640625
bpp_loss 3.7267955541610718
12_o proxy err 0.02767476998269558 tr(WHW.T) 39.67379379272461
bpp_loss 3.0159828662872314
12_up proxy err 0.011101721785962582 tr(WHW.T) 1228.7237548828125
bpp_loss 3.258089464764262
12_gate proxy err 0.00589985866099596 tr(WHW.T) 2382.945068359375
bpp_loss 3.311267231785974
12_down proxy err 0.01503546442836523 tr(WHW.T) 64.68367004394531
bpp_loss 3.243788342143214
I0320 20:46:46.014183 1530314 finetune.py:68] layer 13_down @ epoch 3 new loss 0.0008371140575036407 old loss 0.0008372006705030799 BETTER
I0320 20:47:05.755610 1530368 finetune.py:68] layer 14_down @ epoch 0 new loss 0.001006679143756628 old loss 0.0010069416603073478 BETTER
I0320 20:47:08.558515 1530422 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0007323719328269362 old loss 0.0007363618933595717 BETTER
I0320 20:47:15.300125 1530314 finetune.py:68] layer 13_down @ epoch 4 new loss 0.0008370516006834805 old loss 0.0008371140575036407 BETTER
13_v proxy err 0.021957654505968094 tr(WHW.T) 714.5677490234375
bpp_loss 3.058993697166443
13_q proxy err 0.002756670583039522 tr(WHW.T) 6963.2294921875
bpp_loss 3.6588058471679688
13_k proxy err 0.0018946572672575712 tr(WHW.T) 10466.728515625
bpp_loss 3.6890556812286377
13_o proxy err 0.02464834228157997 tr(WHW.T) 46.18444061279297
bpp_loss 3.049157738685608
13_up proxy err 0.01069874968379736 tr(WHW.T) 1367.9117431640625
bpp_loss 3.270751243413881
13_gate proxy err 0.005785844288766384 tr(WHW.T) 2605.149169921875
bpp_loss 3.306487504826036
13_down proxy err 0.015087414532899857 tr(WHW.T) 79.9759750366211
bpp_loss 3.2537260277326716
I0320 20:47:35.199199 1530368 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0010065063834190369 old loss 0.001006679143756628 BETTER
I0320 20:47:36.776614 1530422 finetune.py:45] layer 15_down initial loss 0.001151155331172049
I0320 20:48:04.430071 1530368 finetune.py:68] layer 14_down @ epoch 2 new loss 0.001006383216008544 old loss 0.0010065063834190369 BETTER
I0320 20:48:04.634708 1530422 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0011508313473314047 old loss 0.001151155331172049 BETTER
I0320 20:48:30.623440 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 16 in 69.90159606933594s
I0320 20:48:32.892737 1530422 finetune.py:68] layer 15_down @ epoch 1 new loss 0.0011506038717925549 old loss 0.0011508313473314047 BETTER
I0320 20:48:33.274867 1530368 finetune.py:68] layer 14_down @ epoch 3 new loss 0.001006299164146185 old loss 0.001006383216008544 BETTER
I0320 20:48:34.989404 1530476 config.py:54] PyTorch version 2.6.0 available.
W0320 20:48:35.395755 1530476 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 20:48:36.487134 1530476 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:48:36.491294 1523378 quantize_finetune_llama.py:203] layer 17 gpu 1
I0320 20:48:36.505427 1530476 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:48:50.795608 1530476 finetune.py:45] layer 16_v initial loss 0.00047559113590978086
W0320 20:48:50.795930 1530476 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:49:02.344708 1530422 finetune.py:68] layer 15_down @ epoch 2 new loss 0.0011504455469548702 old loss 0.0011506038717925549 BETTER
I0320 20:49:03.901427 1530368 finetune.py:68] layer 14_down @ epoch 4 new loss 0.0010062344372272491 old loss 0.001006299164146185 BETTER
14_v proxy err 0.023211998865008354 tr(WHW.T) 706.1612548828125
bpp_loss 3.0407663583755493
14_q proxy err 0.0028007118962705135 tr(WHW.T) 7083.91650390625
bpp_loss 3.6543177366256714
14_k proxy err 0.0018234197050333023 tr(WHW.T) 11329.296875
bpp_loss 3.6866581439971924
14_o proxy err 0.02703014202415943 tr(WHW.T) 51.47877883911133
bpp_loss 3.03025221824646
14_up proxy err 0.010798719711601734 tr(WHW.T) 1465.6612548828125
bpp_loss 3.2721912472747094
14_gate proxy err 0.006042055785655975 tr(WHW.T) 2686.5771484375
bpp_loss 3.3039155117301053
14_down proxy err 0.015409641899168491 tr(WHW.T) 91.052490234375
bpp_loss 3.255071152088254
I0320 20:49:24.534780 1530476 finetune.py:68] layer 16_v @ epoch 0 new loss 0.00027578341541811824 old loss 0.00047559113590978086 BETTER
I0320 20:49:29.852195 1530422 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0011503316927701235 old loss 0.0011504455469548702 BETTER
I0320 20:49:59.779924 1530422 finetune.py:68] layer 15_down @ epoch 4 new loss 0.0011502454290166497 old loss 0.0011503316927701235 BETTER
I0320 20:50:01.388802 1530476 finetune.py:68] layer 16_v @ epoch 1 new loss 0.0002418786461930722 old loss 0.00027578341541811824 BETTER
15_v proxy err 0.02123790793120861 tr(WHW.T) 762.7275390625
bpp_loss 3.079659342765808
15_q proxy err 0.002716618124395609 tr(WHW.T) 7260.47412109375
bpp_loss 3.634125828742981
15_k proxy err 0.0018344331765547395 tr(WHW.T) 11111.2353515625
bpp_loss 3.690922975540161
15_o proxy err 0.023123450577259064 tr(WHW.T) 60.20128631591797
bpp_loss 3.066714882850647
15_up proxy err 0.010535482317209244 tr(WHW.T) 1642.1114501953125
bpp_loss 3.2789537296738733
15_gate proxy err 0.006107467692345381 tr(WHW.T) 2909.224365234375
bpp_loss 3.3108240171920422
15_down proxy err 0.015407392755150795 tr(WHW.T) 115.10923767089844
bpp_loss 3.259624436844227
I0320 20:50:20.130895 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 17 in 70.27185845375061s
I0320 20:50:23.575114 1530530 config.py:54] PyTorch version 2.6.0 available.
W0320 20:50:23.878046 1530530 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 20:50:24.801456 1530530 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:50:24.805414 1523378 quantize_finetune_llama.py:203] layer 18 gpu 2
I0320 20:50:24.818778 1530530 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:50:37.978913 1530530 finetune.py:45] layer 17_v initial loss 0.0003906428755726665
W0320 20:50:37.979116 1530530 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:50:39.000747 1530476 finetune.py:68] layer 16_v @ epoch 2 new loss 0.0002242747723357752 old loss 0.0002418786461930722 BETTER
I0320 20:51:10.968803 1530530 finetune.py:68] layer 17_v @ epoch 0 new loss 0.00022559554781764746 old loss 0.0003906428755726665 BETTER
I0320 20:51:15.655782 1530476 finetune.py:68] layer 16_v @ epoch 3 new loss 0.00021265813848003745 old loss 0.0002242747723357752 BETTER
I0320 20:51:38.381754 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 18 in 73.06977319717407s
I0320 20:51:42.831562 1530584 config.py:54] PyTorch version 2.6.0 available.
W0320 20:51:43.234672 1530584 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 20:51:44.346814 1530584 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:51:44.351221 1523378 quantize_finetune_llama.py:203] layer 19 gpu 3
I0320 20:51:44.365075 1530584 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 20:51:46.265707 1530530 finetune.py:68] layer 17_v @ epoch 1 new loss 0.0002002486289711669 old loss 0.00022559554781764746 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:51:53.596661 1530476 finetune.py:68] layer 16_v @ epoch 4 new loss 0.00020415584731381387 old loss 0.00021265813848003745 BETTER
I0320 20:51:59.481002 1530584 finetune.py:45] layer 18_v initial loss 0.00040752149652689695
W0320 20:51:59.483942 1530584 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:52:11.044690 1530476 finetune.py:45] layer 16_q initial loss 0.00025628024013713
I0320 20:52:21.936062 1530530 finetune.py:68] layer 17_v @ epoch 2 new loss 0.00018685145187191665 old loss 0.0002002486289711669 BETTER
I0320 20:52:34.661750 1530584 finetune.py:68] layer 18_v @ epoch 0 new loss 0.0002198240254074335 old loss 0.00040752149652689695 BETTER
I0320 20:52:47.506957 1530476 finetune.py:68] layer 16_q @ epoch 0 new loss 0.00023154741211328655 old loss 0.00025628024013713 BETTER
I0320 20:52:58.061041 1530530 finetune.py:68] layer 17_v @ epoch 3 new loss 0.00017772879800759256 old loss 0.00018685145187191665 BETTER
I0320 20:53:05.822734 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 19 in 80.83452486991882s
I0320 20:53:10.491930 1530638 config.py:54] PyTorch version 2.6.0 available.
I0320 20:53:10.560350 1530584 finetune.py:68] layer 18_v @ epoch 1 new loss 0.00019607605645433068 old loss 0.0002198240254074335 BETTER
W0320 20:53:10.842587 1530638 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 20:53:11.898834 1530638 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 20:53:11.903132 1523378 quantize_finetune_llama.py:203] layer 20 gpu 0
I0320 20:53:11.920668 1530638 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 20:53:25.193812 1530476 finetune.py:68] layer 16_q @ epoch 1 new loss 0.00022222817642614245 old loss 0.00023154741211328655 BETTER
I0320 20:53:29.959875 1530638 finetune.py:45] layer 19_v initial loss 0.0003938778827432543
W0320 20:53:29.960138 1530638 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 20:53:36.228432 1530530 finetune.py:68] layer 17_v @ epoch 4 new loss 0.0001712923840386793 old loss 0.00017772879800759256 BETTER
I0320 20:53:47.185370 1530584 finetune.py:68] layer 18_v @ epoch 2 new loss 0.0001836492883739993 old loss 0.00019607605645433068 BETTER
I0320 20:53:54.408407 1530530 finetune.py:45] layer 17_q initial loss 0.00022060555056668818
I0320 20:54:03.823957 1530476 finetune.py:68] layer 16_q @ epoch 2 new loss 0.0002155699476134032 old loss 0.00022222817642614245 BETTER
I0320 20:54:05.842303 1530638 finetune.py:68] layer 19_v @ epoch 0 new loss 0.00021103903418406844 old loss 0.0003938778827432543 BETTER
I0320 20:54:24.871351 1530584 finetune.py:68] layer 18_v @ epoch 3 new loss 0.00017567156464792788 old loss 0.0001836492883739993 BETTER
I0320 20:54:29.973203 1530530 finetune.py:68] layer 17_q @ epoch 0 new loss 0.00019753207743633538 old loss 0.00022060555056668818 BETTER
I0320 20:54:43.253925 1530476 finetune.py:68] layer 16_q @ epoch 3 new loss 0.00021027130424045026 old loss 0.0002155699476134032 BETTER
I0320 20:54:43.582292 1530638 finetune.py:68] layer 19_v @ epoch 1 new loss 0.00018811826885212213 old loss 0.00021103903418406844 BETTER
I0320 20:55:02.505205 1530584 finetune.py:68] layer 18_v @ epoch 4 new loss 0.00016964925453066826 old loss 0.00017567156464792788 BETTER
I0320 20:55:05.871114 1530530 finetune.py:68] layer 17_q @ epoch 1 new loss 0.0001900225761346519 old loss 0.00019753207743633538 BETTER
I0320 20:55:20.336435 1530638 finetune.py:68] layer 19_v @ epoch 2 new loss 0.000176469940925017 old loss 0.00018811826885212213 BETTER
I0320 20:55:20.812074 1530584 finetune.py:45] layer 18_q initial loss 0.00023466967104468495
I0320 20:55:22.212572 1530476 finetune.py:68] layer 16_q @ epoch 4 new loss 0.00020589515042956918 old loss 0.00021027130424045026 BETTER
I0320 20:55:41.539104 1530476 finetune.py:45] layer 16_k initial loss 0.00024521813611499965
I0320 20:55:43.708569 1530530 finetune.py:68] layer 17_q @ epoch 2 new loss 0.00018447483307681978 old loss 0.0001900225761346519 BETTER
I0320 20:55:57.249022 1530584 finetune.py:68] layer 18_q @ epoch 0 new loss 0.00020730806863866746 old loss 0.00023466967104468495 BETTER
I0320 20:55:57.393759 1530638 finetune.py:68] layer 19_v @ epoch 3 new loss 0.00016883616626728326 old loss 0.000176469940925017 BETTER
I0320 20:56:17.992977 1530476 finetune.py:68] layer 16_k @ epoch 0 new loss 0.00023366592358797789 old loss 0.00024521813611499965 BETTER
I0320 20:56:21.039052 1530530 finetune.py:68] layer 17_q @ epoch 3 new loss 0.00018012654618360102 old loss 0.00018447483307681978 BETTER
I0320 20:56:34.796003 1530638 finetune.py:68] layer 19_v @ epoch 4 new loss 0.00016315942048095167 old loss 0.00016883616626728326 BETTER
I0320 20:56:35.050745 1530584 finetune.py:68] layer 18_q @ epoch 1 new loss 0.00019951447029598057 old loss 0.00020730806863866746 BETTER
I0320 20:56:52.551247 1530638 finetune.py:45] layer 19_q initial loss 0.0002184629556722939
I0320 20:56:55.985103 1530476 finetune.py:68] layer 16_k @ epoch 1 new loss 0.00022995920153334737 old loss 0.00023366592358797789 BETTER
I0320 20:56:57.437666 1530530 finetune.py:68] layer 17_q @ epoch 4 new loss 0.00017649034271016717 old loss 0.00018012654618360102 BETTER
I0320 20:57:11.566869 1530584 finetune.py:68] layer 18_q @ epoch 2 new loss 0.00019385330961085856 old loss 0.00019951447029598057 BETTER
I0320 20:57:15.817484 1530530 finetune.py:45] layer 17_k initial loss 0.00021644168009515852
I0320 20:57:27.278117 1530638 finetune.py:68] layer 19_q @ epoch 0 new loss 0.00019561730732675642 old loss 0.0002184629556722939 BETTER
I0320 20:57:33.058029 1530476 finetune.py:68] layer 16_k @ epoch 2 new loss 0.00022689392790198326 old loss 0.00022995920153334737 BETTER
I0320 20:57:48.505359 1530584 finetune.py:68] layer 18_q @ epoch 3 new loss 0.00018933144747279584 old loss 0.00019385330961085856 BETTER
I0320 20:57:50.919373 1530530 finetune.py:68] layer 17_k @ epoch 0 new loss 0.00020366568060126156 old loss 0.00021644168009515852 BETTER
I0320 20:58:03.700666 1530638 finetune.py:68] layer 19_q @ epoch 1 new loss 0.00018860763520933688 old loss 0.00019561730732675642 BETTER
I0320 20:58:10.470854 1530476 finetune.py:68] layer 16_k @ epoch 3 new loss 0.00022427475778385997 old loss 0.00022689392790198326 BETTER
I0320 20:58:26.007221 1530584 finetune.py:68] layer 18_q @ epoch 4 new loss 0.00018564696074463427 old loss 0.00018933144747279584 BETTER
I0320 20:58:27.390563 1530530 finetune.py:68] layer 17_k @ epoch 1 new loss 0.00020050100283697248 old loss 0.00020366568060126156 BETTER
I0320 20:58:39.747542 1530638 finetune.py:68] layer 19_q @ epoch 2 new loss 0.00018365059804636985 old loss 0.00018860763520933688 BETTER
I0320 20:58:44.957664 1530584 finetune.py:45] layer 18_k initial loss 0.000237833519349806
I0320 20:58:48.641573 1530476 finetune.py:68] layer 16_k @ epoch 4 new loss 0.0002219510788563639 old loss 0.00022427475778385997 BETTER
I0320 20:59:03.243510 1530530 finetune.py:68] layer 17_k @ epoch 2 new loss 0.00019788264762610197 old loss 0.00020050100283697248 BETTER
I0320 20:59:06.756214 1530476 finetune.py:45] layer 16_o initial loss 0.0005852787871845067
I0320 20:59:16.028151 1530638 finetune.py:68] layer 19_q @ epoch 3 new loss 0.0001796675060177222 old loss 0.00018365059804636985 BETTER
I0320 20:59:19.966874 1530584 finetune.py:68] layer 18_k @ epoch 0 new loss 0.00022276760137174278 old loss 0.000237833519349806 BETTER
I0320 20:59:39.290307 1530530 finetune.py:68] layer 17_k @ epoch 3 new loss 0.00019565783441066742 old loss 0.00019788264762610197 BETTER
I0320 20:59:42.519553 1530476 finetune.py:68] layer 16_o @ epoch 0 new loss 0.0005437256768345833 old loss 0.0005852787871845067 BETTER
I0320 20:59:53.118489 1530638 finetune.py:68] layer 19_q @ epoch 4 new loss 0.00017651630332693458 old loss 0.0001796675060177222 BETTER
I0320 20:59:56.254669 1530584 finetune.py:68] layer 18_k @ epoch 1 new loss 0.000219331297557801 old loss 0.00022276760137174278 BETTER
I0320 21:00:10.918063 1530638 finetune.py:45] layer 19_k initial loss 0.00022840267047286034
I0320 21:00:15.805383 1530530 finetune.py:68] layer 17_k @ epoch 4 new loss 0.00019363319734111428 old loss 0.00019565783441066742 BETTER
I0320 21:00:19.061560 1530476 finetune.py:68] layer 16_o @ epoch 1 new loss 0.0005251208203844726 old loss 0.0005437256768345833 BETTER
I0320 21:00:32.010357 1530584 finetune.py:68] layer 18_k @ epoch 2 new loss 0.00021663811639882624 old loss 0.000219331297557801 BETTER
I0320 21:00:33.156804 1530530 finetune.py:45] layer 17_o initial loss 0.00046466963249258697
I0320 21:00:45.351297 1530638 finetune.py:68] layer 19_k @ epoch 0 new loss 0.00020954382489435375 old loss 0.00022840267047286034 BETTER
I0320 21:00:56.274170 1530476 finetune.py:68] layer 16_o @ epoch 2 new loss 0.0005118778790347278 old loss 0.0005251208203844726 BETTER
I0320 21:01:08.689317 1530530 finetune.py:68] layer 17_o @ epoch 0 new loss 0.00043721875408664346 old loss 0.00046466963249258697 BETTER
I0320 21:01:09.199800 1530584 finetune.py:68] layer 18_k @ epoch 3 new loss 0.000214340107049793 old loss 0.00021663811639882624 BETTER
I0320 21:01:21.522716 1530638 finetune.py:68] layer 19_k @ epoch 1 new loss 0.00020655184925999492 old loss 0.00020954382489435375 BETTER
I0320 21:01:32.538222 1530476 finetune.py:68] layer 16_o @ epoch 3 new loss 0.0005016378709115088 old loss 0.0005118778790347278 BETTER
I0320 21:01:44.684283 1530530 finetune.py:68] layer 17_o @ epoch 1 new loss 0.0004244574229232967 old loss 0.00043721875408664346 BETTER
I0320 21:01:45.864464 1530584 finetune.py:68] layer 18_k @ epoch 4 new loss 0.00021232120343483984 old loss 0.000214340107049793 BETTER
I0320 21:01:56.885411 1530638 finetune.py:68] layer 19_k @ epoch 2 new loss 0.00020412269805092365 old loss 0.00020655184925999492 BETTER
I0320 21:02:04.576933 1530584 finetune.py:45] layer 18_o initial loss 0.0005080412374809384
I0320 21:02:09.571540 1530476 finetune.py:68] layer 16_o @ epoch 4 new loss 0.0004933568998239934 old loss 0.0005016378709115088 BETTER
I0320 21:02:19.743605 1530530 finetune.py:68] layer 17_o @ epoch 2 new loss 0.00041550068999640644 old loss 0.0004244574229232967 BETTER
I0320 21:02:32.346989 1530638 finetune.py:68] layer 19_k @ epoch 3 new loss 0.0002020974934566766 old loss 0.00020412269805092365 BETTER
I0320 21:02:39.560387 1530584 finetune.py:68] layer 18_o @ epoch 0 new loss 0.00047627935418859124 old loss 0.0005080412374809384 BETTER
I0320 21:02:40.134452 1530476 finetune.py:45] layer 16_up initial loss 0.0007970295264385641
I0320 21:02:55.355748 1530530 finetune.py:68] layer 17_o @ epoch 3 new loss 0.0004085148102603853 old loss 0.00041550068999640644 BETTER
I0320 21:03:08.574344 1530638 finetune.py:68] layer 19_k @ epoch 4 new loss 0.00020035187480971217 old loss 0.0002020974934566766 BETTER
I0320 21:03:13.770040 1530476 finetune.py:68] layer 16_up @ epoch 0 new loss 0.0007754657999612391 old loss 0.0007970295264385641 BETTER
I0320 21:03:15.270827 1530584 finetune.py:68] layer 18_o @ epoch 1 new loss 0.0004626611771527678 old loss 0.00047627935418859124 BETTER
I0320 21:03:26.266486 1530638 finetune.py:45] layer 19_o initial loss 0.00046549970284104347
I0320 21:03:30.571930 1530530 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0004029040574096143 old loss 0.0004085148102603853 BETTER
I0320 21:03:48.163329 1530476 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0007632304332219064 old loss 0.0007754657999612391 BETTER
I0320 21:03:50.656155 1530584 finetune.py:68] layer 18_o @ epoch 2 new loss 0.00045313319424167275 old loss 0.0004626611771527678 BETTER
I0320 21:04:00.586235 1530638 finetune.py:68] layer 19_o @ epoch 0 new loss 0.0004378952435217798 old loss 0.00046549970284104347 BETTER
I0320 21:04:00.594455 1530530 finetune.py:45] layer 17_up initial loss 0.0007388121448457241
I0320 21:04:24.116628 1530476 finetune.py:68] layer 16_up @ epoch 2 new loss 0.0007534811738878489 old loss 0.0007632304332219064 BETTER
I0320 21:04:27.046200 1530584 finetune.py:68] layer 18_o @ epoch 3 new loss 0.000445770361693576 old loss 0.00045313319424167275 BETTER
I0320 21:04:34.084303 1530530 finetune.py:68] layer 17_up @ epoch 0 new loss 0.0007203100249171257 old loss 0.0007388121448457241 BETTER
I0320 21:04:36.505959 1530638 finetune.py:68] layer 19_o @ epoch 1 new loss 0.0004267547046765685 old loss 0.0004378952435217798 BETTER
I0320 21:04:59.547564 1530476 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0007453617290593684 old loss 0.0007534811738878489 BETTER
I0320 21:05:04.008440 1530584 finetune.py:68] layer 18_o @ epoch 4 new loss 0.00043997602188028395 old loss 0.000445770361693576 BETTER
I0320 21:05:08.144427 1530530 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0007095443434081972 old loss 0.0007203100249171257 BETTER
I0320 21:05:10.655647 1530638 finetune.py:68] layer 19_o @ epoch 2 new loss 0.0004191297630313784 old loss 0.0004267547046765685 BETTER
I0320 21:05:34.524293 1530584 finetune.py:45] layer 18_up initial loss 0.0008419975056312978
I0320 21:05:35.680961 1530476 finetune.py:68] layer 16_up @ epoch 4 new loss 0.0007384188938885927 old loss 0.0007453617290593684 BETTER
I0320 21:05:41.602092 1530530 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0007012140122242272 old loss 0.0007095443434081972 BETTER
I0320 21:05:45.747240 1530638 finetune.py:68] layer 19_o @ epoch 3 new loss 0.0004133023030590266 old loss 0.0004191297630313784 BETTER
I0320 21:06:06.962726 1530476 finetune.py:45] layer 16_gate initial loss 0.0009712740429677069
I0320 21:06:08.184955 1530584 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0008207953651435673 old loss 0.0008419975056312978 BETTER
I0320 21:06:16.453931 1530530 finetune.py:68] layer 17_up @ epoch 3 new loss 0.0006943164626136422 old loss 0.0007012140122242272 BETTER
I0320 21:06:21.692211 1530638 finetune.py:68] layer 19_o @ epoch 4 new loss 0.0004087099223397672 old loss 0.0004133023030590266 BETTER
I0320 21:06:38.585978 1530476 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.0009599053882993758 old loss 0.0009712740429677069 BETTER
I0320 21:06:42.087932 1530584 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0008089613984338939 old loss 0.0008207953651435673 BETTER
I0320 21:06:50.572007 1530530 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0006883665919303894 old loss 0.0006943164626136422 BETTER
I0320 21:06:53.281623 1530638 finetune.py:45] layer 19_up initial loss 0.0008597429259680212
I0320 21:07:11.081008 1530476 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0009522135369479656 old loss 0.0009599053882993758 BETTER
I0320 21:07:15.681655 1530584 finetune.py:68] layer 18_up @ epoch 2 new loss 0.0007997631910257041 old loss 0.0008089613984338939 BETTER
I0320 21:07:21.469523 1530530 finetune.py:45] layer 17_gate initial loss 0.0009453654638491571
I0320 21:07:26.297394 1530638 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0008385645924136043 old loss 0.0008597429259680212 BETTER
I0320 21:07:45.317318 1530476 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.0009456820553168654 old loss 0.0009522135369479656 BETTER
I0320 21:07:51.242434 1530584 finetune.py:68] layer 18_up @ epoch 3 new loss 0.00079204177018255 old loss 0.0007997631910257041 BETTER
I0320 21:07:52.821485 1530530 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0009357333765365183 old loss 0.0009453654638491571 BETTER
I0320 21:07:59.654035 1530638 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0008268059464171529 old loss 0.0008385645924136043 BETTER
I0320 21:08:19.377070 1530476 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0009401642018929124 old loss 0.0009456820553168654 BETTER
I0320 21:08:25.602462 1530530 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0009289424633607268 old loss 0.0009357333765365183 BETTER
I0320 21:08:25.921655 1530584 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0007856418960727751 old loss 0.00079204177018255 BETTER
I0320 21:08:32.635605 1530638 finetune.py:68] layer 19_up @ epoch 2 new loss 0.0008176774135790765 old loss 0.0008268059464171529 BETTER
I0320 21:08:53.768567 1530476 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.000935242569539696 old loss 0.0009401642018929124 BETTER
I0320 21:08:56.945368 1530584 finetune.py:45] layer 18_gate initial loss 0.001088205724954605
I0320 21:08:57.857012 1530530 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.0009232665179297328 old loss 0.0009289424633607268 BETTER
I0320 21:09:05.578161 1530638 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0008103054133243859 old loss 0.0008176774135790765 BETTER
I0320 21:09:27.252997 1530476 finetune.py:45] layer 16_down initial loss 0.0015007720794528723
I0320 21:09:29.024501 1530584 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.001078293309547007 old loss 0.001088205724954605 BETTER
I0320 21:09:30.669084 1530530 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.0009183986694552004 old loss 0.0009232665179297328 BETTER
I0320 21:09:40.006667 1530638 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0008040882530622184 old loss 0.0008103054133243859 BETTER
I0320 21:09:57.701938 1530476 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0015003324951976538 old loss 0.0015007720794528723 BETTER
I0320 21:10:02.446964 1530584 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.001070935744792223 old loss 0.001078293309547007 BETTER
I0320 21:10:02.884680 1530530 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.0009141675545834005 old loss 0.0009183986694552004 BETTER
I0320 21:10:11.606283 1530638 finetune.py:45] layer 19_gate initial loss 0.0011578615522012115
I0320 21:10:29.412210 1530476 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0015000237617641687 old loss 0.0015003324951976538 BETTER
I0320 21:10:35.667279 1530584 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.001064833952113986 old loss 0.001070935744792223 BETTER
I0320 21:10:36.805501 1530530 finetune.py:45] layer 17_down initial loss 0.001519503421150148
I0320 21:10:43.091583 1530638 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.001147300354205072 old loss 0.0011578615522012115 BETTER
I0320 21:11:02.745474 1530476 finetune.py:68] layer 16_down @ epoch 2 new loss 0.001499790232628584 old loss 0.0015000237617641687 BETTER
I0320 21:11:07.059136 1530530 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0015189879341050982 old loss 0.001519503421150148 BETTER
I0320 21:11:09.191380 1530584 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0010596163338050246 old loss 0.001064833952113986 BETTER
I0320 21:11:15.798503 1530638 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0011398402275517583 old loss 0.001147300354205072 BETTER
I0320 21:11:36.356490 1530476 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0014996182871982455 old loss 0.001499790232628584 BETTER
I0320 21:11:39.390360 1530530 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0015186646487563848 old loss 0.0015189879341050982 BETTER
I0320 21:11:43.314181 1530584 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.001055068918503821 old loss 0.0010596163338050246 BETTER
I0320 21:11:48.142848 1530638 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0011336102616041899 old loss 0.0011398402275517583 BETTER
I0320 21:12:08.419593 1530476 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0014994866214692593 old loss 0.0014996182871982455 BETTER
I0320 21:12:10.068224 1530530 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0015184313524514437 old loss 0.0015186646487563848 BETTER
16_v proxy err 0.021492592990398407 tr(WHW.T) 780.7407836914062
bpp_loss 3.1230247020721436
16_q proxy err 0.0027870896738022566 tr(WHW.T) 7198.833984375
bpp_loss 3.6224032640457153
16_k proxy err 0.0017883166437968612 tr(WHW.T) 11669.671875
bpp_loss 3.666140913963318
16_o proxy err 0.018214261159300804 tr(WHW.T) 88.78990173339844
bpp_loss 3.11247181892395
16_up proxy err 0.010320808738470078 tr(WHW.T) 1890.5115966796875
bpp_loss 3.2779432784679323
16_gate proxy err 0.005965820048004389 tr(WHW.T) 3365.770751953125
bpp_loss 3.3198935930119005
16_down proxy err 0.015625014901161194 tr(WHW.T) 153.4078826904297
bpp_loss 3.258771541506745
I0320 21:12:18.025283 1530584 finetune.py:45] layer 18_down initial loss 0.0017854716861620545
I0320 21:12:20.286639 1530638 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.001128348638303578 old loss 0.0011336102616041899 BETTER
I0320 21:12:41.362302 1530530 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0015182553324848413 old loss 0.0015184313524514437 BETTER
I0320 21:12:48.390357 1530584 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0017849513096734881 old loss 0.0017854716861620545 BETTER
I0320 21:12:52.844413 1530638 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.0011237936560064554 old loss 0.001128348638303578 BETTER
I0320 21:13:11.061376 1530530 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0015181178459897637 old loss 0.0015182553324848413 BETTER
17_v proxy err 0.02023928053677082 tr(WHW.T) 845.7654418945312
bpp_loss 3.1193453073501587
17_q proxy err 0.002825705334544182 tr(WHW.T) 7170.92822265625
bpp_loss 3.609313726425171
17_k proxy err 0.0019541396759450436 tr(WHW.T) 10736.4560546875
bpp_loss 3.6455007791519165
17_o proxy err 0.019871603697538376 tr(WHW.T) 58.51771926879883
bpp_loss 3.1131420135498047
17_up proxy err 0.011214716359972954 tr(WHW.T) 1921.43603515625
bpp_loss 3.2731711809025255
17_gate proxy err 0.006216627545654774 tr(WHW.T) 3574.634521484375
bpp_loss 3.3321582882903344
17_down proxy err 0.015486936084926128 tr(WHW.T) 166.71156311035156
bpp_loss 3.257546824078227
I0320 21:13:17.327225 1530584 finetune.py:68] layer 18_down @ epoch 1 new loss 0.001784582040272653 old loss 0.0017849513096734881 BETTER
I0320 21:13:21.039282 1530638 finetune.py:45] layer 19_down initial loss 0.0019209375604987144
I0320 21:13:46.165470 1530584 finetune.py:68] layer 18_down @ epoch 2 new loss 0.001784302294254303 old loss 0.001784582040272653 BETTER
I0320 21:13:47.988486 1530638 finetune.py:68] layer 19_down @ epoch 0 new loss 0.00192032172344625 old loss 0.0019209375604987144 BETTER
I0320 21:14:15.830600 1530584 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0017840925138443708 old loss 0.001784302294254303 BETTER
I0320 21:14:17.245932 1530638 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0019198994850739837 old loss 0.00192032172344625 BETTER
I0320 21:14:28.240291 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 20 in 70.96718645095825s
I0320 21:14:32.376262 1530692 config.py:54] PyTorch version 2.6.0 available.
W0320 21:14:32.758217 1530692 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 21:14:33.882270 1530692 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 21:14:33.886837 1523378 quantize_finetune_llama.py:203] layer 21 gpu 1
I0320 21:14:33.905747 1530692 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 21:14:46.730758 1530584 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0017839280189946294 old loss 0.0017840925138443708 BETTER
I0320 21:14:47.399063 1530638 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0019195930799469352 old loss 0.0019198994850739837 BETTER
I0320 21:14:48.950877 1530692 finetune.py:45] layer 20_v initial loss 0.00043869251385331154
W0320 21:14:48.951207 1530692 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

18_v proxy err 0.01861220970749855 tr(WHW.T) 1003.7705078125
bpp_loss 3.1702330112457275
18_q proxy err 0.0028944313526153564 tr(WHW.T) 7516.095703125
bpp_loss 3.5730996131896973
18_k proxy err 0.0021351452451199293 tr(WHW.T) 10502.119140625
bpp_loss 3.6081143617630005
18_o proxy err 0.017573753371834755 tr(WHW.T) 70.3639907836914
bpp_loss 3.154766321182251
18_up proxy err 0.011932970955967903 tr(WHW.T) 2021.9896240234375
bpp_loss 3.269506055255269
18_gate proxy err 0.006602201610803604 tr(WHW.T) 3779.103515625
bpp_loss 3.3441130172374636
18_down proxy err 0.015546780079603195 tr(WHW.T) 200.08197021484375
bpp_loss 3.256345371867335
I0320 21:15:14.927040 1530638 finetune.py:68] layer 19_down @ epoch 3 new loss 0.001919358503073454 old loss 0.0019195930799469352 BETTER
I0320 21:15:23.009439 1530692 finetune.py:68] layer 20_v @ epoch 0 new loss 0.00023826060350984335 old loss 0.00043869251385331154 BETTER
I0320 21:15:44.124236 1530638 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0019191824831068516 old loss 0.001919358503073454 BETTER
19_v proxy err 0.01817951537668705 tr(WHW.T) 1019.1412353515625
bpp_loss 3.1802327632904053
19_q proxy err 0.00308213010430336 tr(WHW.T) 6952.38134765625
bpp_loss 3.5537655353546143
19_k proxy err 0.0020827490370720625 tr(WHW.T) 10594.826171875
bpp_loss 3.584289312362671
19_o proxy err 0.017345471307635307 tr(WHW.T) 62.538597106933594
bpp_loss 3.1689093112945557
19_up proxy err 0.011996312998235226 tr(WHW.T) 2149.1318359375
bpp_loss 3.2693862915039062
19_gate proxy err 0.007216311059892178 tr(WHW.T) 3686.689697265625
bpp_loss 3.348876420841661
19_down proxy err 0.015150022692978382 tr(WHW.T) 224.6279296875
bpp_loss 3.25912919155387
I0320 21:15:57.938379 1530692 finetune.py:68] layer 20_v @ epoch 1 new loss 0.00021227925026323646 old loss 0.00023826060350984335 BETTER
I0320 21:16:00.502767 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 21 in 68.27583646774292s
I0320 21:16:03.955096 1530746 config.py:54] PyTorch version 2.6.0 available.
W0320 21:16:04.254898 1530746 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 21:16:05.169412 1530746 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 21:16:05.173391 1523378 quantize_finetune_llama.py:203] layer 22 gpu 2
I0320 21:16:05.187778 1530746 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 21:16:18.432142 1530746 finetune.py:45] layer 21_v initial loss 0.00039362197276204824
W0320 21:16:18.432404 1530746 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 21:16:35.567964 1530692 finetune.py:68] layer 20_v @ epoch 2 new loss 0.00019933161092922091 old loss 0.00021227925026323646 BETTER
I0320 21:16:50.816475 1530746 finetune.py:68] layer 21_v @ epoch 0 new loss 0.00021541453315876424 old loss 0.00039362197276204824 BETTER
I0320 21:17:13.211996 1530692 finetune.py:68] layer 20_v @ epoch 3 new loss 0.00019108741253148764 old loss 0.00019933161092922091 BETTER
I0320 21:17:20.754430 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 22 in 75.04613161087036s
I0320 21:17:25.515969 1530800 config.py:54] PyTorch version 2.6.0 available.
I0320 21:17:25.621495 1530746 finetune.py:68] layer 21_v @ epoch 1 new loss 0.00019431467808317393 old loss 0.00021541453315876424 BETTER
W0320 21:17:25.903939 1530800 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 21:17:26.975922 1530800 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 21:17:26.980188 1523378 quantize_finetune_llama.py:203] layer 23 gpu 3
I0320 21:17:26.995230 1530800 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 21:17:41.372794 1530800 finetune.py:45] layer 22_v initial loss 0.00047740963054820895
W0320 21:17:41.373103 1530800 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 21:17:50.554954 1530692 finetune.py:68] layer 20_v @ epoch 4 new loss 0.00018539378652349114 old loss 0.00019108741253148764 BETTER
I0320 21:18:01.120997 1530746 finetune.py:68] layer 21_v @ epoch 2 new loss 0.00018399082182440907 old loss 0.00019431467808317393 BETTER
I0320 21:18:07.283059 1530692 finetune.py:45] layer 20_q initial loss 0.0002473349159117788
I0320 21:18:16.278181 1530800 finetune.py:68] layer 22_v @ epoch 0 new loss 0.0002666953078005463 old loss 0.00047740963054820895 BETTER
I0320 21:18:37.401769 1530746 finetune.py:68] layer 21_v @ epoch 3 new loss 0.00017737166490405798 old loss 0.00018399082182440907 BETTER
I0320 21:18:43.206416 1530692 finetune.py:68] layer 20_q @ epoch 0 new loss 0.00022340261784847826 old loss 0.0002473349159117788 BETTER
I0320 21:18:46.484117 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 23 in 79.03266954421997s
I0320 21:18:51.675546 1530854 config.py:54] PyTorch version 2.6.0 available.
I0320 21:18:51.985821 1530800 finetune.py:68] layer 22_v @ epoch 1 new loss 0.0002401697274763137 old loss 0.0002666953078005463 BETTER
W0320 21:18:52.113276 1530854 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 21:18:53.601575 1530854 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 21:18:53.620698 1523378 quantize_finetune_llama.py:203] layer 24 gpu 0
I0320 21:18:53.648192 1530854 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 21:19:10.256502 1530854 finetune.py:45] layer 23_v initial loss 0.00048210815293714404
W0320 21:19:10.258631 1530854 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 21:19:13.850132 1530746 finetune.py:68] layer 21_v @ epoch 4 new loss 0.0001726368209347129 old loss 0.00017737166490405798 BETTER
I0320 21:19:20.255264 1530692 finetune.py:68] layer 20_q @ epoch 1 new loss 0.00021554656268563122 old loss 0.00022340261784847826 BETTER
I0320 21:19:28.763273 1530800 finetune.py:68] layer 22_v @ epoch 2 new loss 0.00022678864479530603 old loss 0.0002401697274763137 BETTER
I0320 21:19:33.032427 1530746 finetune.py:45] layer 21_q initial loss 0.00022548693232238293
I0320 21:19:45.110965 1530854 finetune.py:68] layer 23_v @ epoch 0 new loss 0.0002655875578057021 old loss 0.00048210815293714404 BETTER
I0320 21:19:57.421680 1530692 finetune.py:68] layer 20_q @ epoch 2 new loss 0.00020998180843889713 old loss 0.00021554656268563122 BETTER
I0320 21:20:05.390337 1530800 finetune.py:68] layer 22_v @ epoch 3 new loss 0.0002183726755902171 old loss 0.00022678864479530603 BETTER
I0320 21:20:07.931899 1530746 finetune.py:68] layer 21_q @ epoch 0 new loss 0.0002073464565910399 old loss 0.00022548693232238293 BETTER
I0320 21:20:21.408514 1530854 finetune.py:68] layer 23_v @ epoch 1 new loss 0.00023851834703236818 old loss 0.0002655875578057021 BETTER
I0320 21:20:35.583282 1530692 finetune.py:68] layer 20_q @ epoch 3 new loss 0.0002055475051747635 old loss 0.00020998180843889713 BETTER
I0320 21:20:43.382889 1530800 finetune.py:68] layer 22_v @ epoch 4 new loss 0.00021214566368144006 old loss 0.0002183726755902171 BETTER
I0320 21:20:44.522522 1530746 finetune.py:68] layer 21_q @ epoch 1 new loss 0.00020093984494451433 old loss 0.0002073464565910399 BETTER
I0320 21:20:57.642652 1530854 finetune.py:68] layer 23_v @ epoch 2 new loss 0.0002260214532725513 old loss 0.00023851834703236818 BETTER
I0320 21:21:01.403999 1530800 finetune.py:45] layer 22_q initial loss 0.00030711706494912505
I0320 21:21:14.039511 1530692 finetune.py:68] layer 20_q @ epoch 4 new loss 0.00020193101954646409 old loss 0.0002055475051747635 BETTER
I0320 21:21:20.337705 1530746 finetune.py:68] layer 21_q @ epoch 2 new loss 0.0001964674302143976 old loss 0.00020093984494451433 BETTER
I0320 21:21:32.289779 1530692 finetune.py:45] layer 20_k initial loss 0.00025143910897895694
I0320 21:21:34.335680 1530854 finetune.py:68] layer 23_v @ epoch 3 new loss 0.0002181385352741927 old loss 0.0002260214532725513 BETTER
I0320 21:21:37.057111 1530800 finetune.py:68] layer 22_q @ epoch 0 new loss 0.0002693045826163143 old loss 0.00030711706494912505 BETTER
I0320 21:21:56.113954 1530746 finetune.py:68] layer 21_q @ epoch 3 new loss 0.00019290675118099898 old loss 0.0001964674302143976 BETTER
I0320 21:22:08.634188 1530692 finetune.py:68] layer 20_k @ epoch 0 new loss 0.00023993602371774614 old loss 0.00025143910897895694 BETTER
I0320 21:22:11.898486 1530854 finetune.py:68] layer 23_v @ epoch 4 new loss 0.0002124293678207323 old loss 0.0002181385352741927 BETTER
I0320 21:22:13.867399 1530800 finetune.py:68] layer 22_q @ epoch 1 new loss 0.00025979048223234713 old loss 0.0002693045826163143 BETTER
I0320 21:22:29.601034 1530854 finetune.py:45] layer 23_q initial loss 0.00028167275013402104
I0320 21:22:32.545444 1530746 finetune.py:68] layer 21_q @ epoch 4 new loss 0.0001901013747556135 old loss 0.00019290675118099898 BETTER
I0320 21:22:45.442801 1530692 finetune.py:68] layer 20_k @ epoch 1 new loss 0.00023659816361032426 old loss 0.00023993602371774614 BETTER
I0320 21:22:51.219839 1530746 finetune.py:45] layer 21_k initial loss 0.0002433718036627397
I0320 21:22:51.231425 1530800 finetune.py:68] layer 22_q @ epoch 2 new loss 0.00025298172840848565 old loss 0.00025979048223234713 BETTER
I0320 21:23:03.917657 1530854 finetune.py:68] layer 23_q @ epoch 0 new loss 0.00025970867136493325 old loss 0.00028167275013402104 BETTER
I0320 21:23:23.611273 1530692 finetune.py:68] layer 20_k @ epoch 2 new loss 0.00023388212139252573 old loss 0.00023659816361032426 BETTER
I0320 21:23:26.273427 1530746 finetune.py:68] layer 21_k @ epoch 0 new loss 0.00022831805108580738 old loss 0.0002433718036627397 BETTER
I0320 21:23:28.046017 1530800 finetune.py:68] layer 22_q @ epoch 3 new loss 0.0002476800582371652 old loss 0.00025298172840848565 BETTER
I0320 21:23:39.453199 1530854 finetune.py:68] layer 23_q @ epoch 1 new loss 0.0002519234549254179 old loss 0.00025970867136493325 BETTER
I0320 21:24:02.720247 1530692 finetune.py:68] layer 20_k @ epoch 3 new loss 0.00023161958961281925 old loss 0.00023388212139252573 BETTER
I0320 21:24:03.788147 1530746 finetune.py:68] layer 21_k @ epoch 1 new loss 0.0002256414300063625 old loss 0.00022831805108580738 BETTER
I0320 21:24:04.972268 1530800 finetune.py:68] layer 22_q @ epoch 4 new loss 0.00024330105225089937 old loss 0.0002476800582371652 BETTER
I0320 21:24:14.682045 1530854 finetune.py:68] layer 23_q @ epoch 2 new loss 0.0002462669217493385 old loss 0.0002519234549254179 BETTER
I0320 21:24:23.071535 1530800 finetune.py:45] layer 22_k initial loss 0.0003075231215916574
I0320 21:24:40.121757 1530746 finetune.py:68] layer 21_k @ epoch 2 new loss 0.00022348100901581347 old loss 0.0002256414300063625 BETTER
I0320 21:24:40.142972 1530692 finetune.py:68] layer 20_k @ epoch 4 new loss 0.00022958956833463162 old loss 0.00023161958961281925 BETTER
I0320 21:24:49.727660 1530854 finetune.py:68] layer 23_q @ epoch 3 new loss 0.0002420024247840047 old loss 0.0002462669217493385 BETTER
I0320 21:24:58.057119 1530692 finetune.py:45] layer 20_o initial loss 0.0005446270224638283
I0320 21:24:58.690853 1530800 finetune.py:68] layer 22_k @ epoch 0 new loss 0.00029775407165288925 old loss 0.0003075231215916574 BETTER
I0320 21:25:15.170776 1530746 finetune.py:68] layer 21_k @ epoch 3 new loss 0.00022169873409438878 old loss 0.00022348100901581347 BETTER
I0320 21:25:25.338012 1530854 finetune.py:68] layer 23_q @ epoch 4 new loss 0.00023837755725253373 old loss 0.0002420024247840047 BETTER
I0320 21:25:33.416706 1530692 finetune.py:68] layer 20_o @ epoch 0 new loss 0.0005109593039378524 old loss 0.0005446270224638283 BETTER
I0320 21:25:34.471066 1530800 finetune.py:68] layer 22_k @ epoch 1 new loss 0.00029407255351543427 old loss 0.00029775407165288925 BETTER
I0320 21:25:42.850205 1530854 finetune.py:45] layer 23_k initial loss 0.00029697423451580107
I0320 21:25:50.061252 1530746 finetune.py:68] layer 21_k @ epoch 4 new loss 0.00022015179274603724 old loss 0.00022169873409438878 BETTER
I0320 21:26:07.397204 1530746 finetune.py:45] layer 21_o initial loss 0.00048181760939769447
I0320 21:26:10.545831 1530692 finetune.py:68] layer 20_o @ epoch 1 new loss 0.0004982006503269076 old loss 0.0005109593039378524 BETTER
I0320 21:26:10.839877 1530800 finetune.py:68] layer 22_k @ epoch 2 new loss 0.0002911984920501709 old loss 0.00029407255351543427 BETTER
I0320 21:26:17.184471 1530854 finetune.py:68] layer 23_k @ epoch 0 new loss 0.00028965770616196096 old loss 0.00029697423451580107 BETTER
I0320 21:26:41.988852 1530746 finetune.py:68] layer 21_o @ epoch 0 new loss 0.0004623575950972736 old loss 0.00048181760939769447 BETTER
I0320 21:26:48.519031 1530800 finetune.py:68] layer 22_k @ epoch 3 new loss 0.0002887588052544743 old loss 0.0002911984920501709 BETTER
I0320 21:26:48.973865 1530692 finetune.py:68] layer 20_o @ epoch 2 new loss 0.0004891045391559601 old loss 0.0004982006503269076 BETTER
I0320 21:26:53.641898 1530854 finetune.py:68] layer 23_k @ epoch 1 new loss 0.00028679793467745185 old loss 0.00028965770616196096 BETTER
I0320 21:27:18.321985 1530746 finetune.py:68] layer 21_o @ epoch 1 new loss 0.0004550851590465754 old loss 0.0004623575950972736 BETTER
I0320 21:27:25.592417 1530800 finetune.py:68] layer 22_k @ epoch 4 new loss 0.0002867422590497881 old loss 0.0002887588052544743 BETTER
I0320 21:27:26.204951 1530692 finetune.py:68] layer 20_o @ epoch 3 new loss 0.00048224598867818713 old loss 0.0004891045391559601 BETTER
I0320 21:27:29.661924 1530854 finetune.py:68] layer 23_k @ epoch 2 new loss 0.00028449599631130695 old loss 0.00028679793467745185 BETTER
I0320 21:27:42.763557 1530800 finetune.py:45] layer 22_o initial loss 0.0006274979095906019
I0320 21:27:53.720191 1530746 finetune.py:68] layer 21_o @ epoch 2 new loss 0.00045002694241702557 old loss 0.0004550851590465754 BETTER
I0320 21:28:03.043568 1530692 finetune.py:68] layer 20_o @ epoch 4 new loss 0.0004765635821968317 old loss 0.00048224598867818713 BETTER
I0320 21:28:05.351883 1530854 finetune.py:68] layer 23_k @ epoch 3 new loss 0.0002826140553224832 old loss 0.00028449599631130695 BETTER
I0320 21:28:16.613347 1530800 finetune.py:68] layer 22_o @ epoch 0 new loss 0.0005974680534563959 old loss 0.0006274979095906019 BETTER
I0320 21:28:28.932079 1530746 finetune.py:68] layer 21_o @ epoch 3 new loss 0.00044602638809010386 old loss 0.00045002694241702557 BETTER
I0320 21:28:33.239656 1530692 finetune.py:45] layer 20_up initial loss 0.0010048311669379473
I0320 21:28:41.112741 1530854 finetune.py:68] layer 23_k @ epoch 4 new loss 0.0002808973949868232 old loss 0.0002826140553224832 BETTER
I0320 21:28:51.855247 1530800 finetune.py:68] layer 22_o @ epoch 1 new loss 0.0005855757044628263 old loss 0.0005974680534563959 BETTER
I0320 21:28:59.051054 1530854 finetune.py:45] layer 23_o initial loss 0.0005836171330884099
I0320 21:29:04.998938 1530746 finetune.py:68] layer 21_o @ epoch 4 new loss 0.0004428976390045136 old loss 0.00044602638809010386 BETTER
I0320 21:29:06.741838 1530692 finetune.py:68] layer 20_up @ epoch 0 new loss 0.0009811638155952096 old loss 0.0010048311669379473 BETTER
I0320 21:29:27.370293 1530800 finetune.py:68] layer 22_o @ epoch 2 new loss 0.0005769490962848067 old loss 0.0005855757044628263 BETTER
I0320 21:29:33.486443 1530854 finetune.py:68] layer 23_o @ epoch 0 new loss 0.0005617608549073339 old loss 0.0005836171330884099 BETTER
I0320 21:29:34.699035 1530746 finetune.py:45] layer 21_up initial loss 0.0010166659485548735
I0320 21:29:41.534864 1530692 finetune.py:68] layer 20_up @ epoch 1 new loss 0.0009678557980805635 old loss 0.0009811638155952096 BETTER
I0320 21:30:04.382697 1530800 finetune.py:68] layer 22_o @ epoch 3 new loss 0.0005704234936274588 old loss 0.0005769490962848067 BETTER
I0320 21:30:07.626967 1530746 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0009961628820747137 old loss 0.0010166659485548735 BETTER
I0320 21:30:08.919068 1530854 finetune.py:68] layer 23_o @ epoch 1 new loss 0.0005529539193958044 old loss 0.0005617608549073339 BETTER
I0320 21:30:15.863699 1530692 finetune.py:68] layer 20_up @ epoch 2 new loss 0.000957787677180022 old loss 0.0009678557980805635 BETTER
I0320 21:30:41.562412 1530800 finetune.py:68] layer 22_o @ epoch 4 new loss 0.0005652044201269746 old loss 0.0005704234936274588 BETTER
I0320 21:30:42.718228 1530746 finetune.py:68] layer 21_up @ epoch 1 new loss 0.000984594109468162 old loss 0.0009961628820747137 BETTER
I0320 21:30:44.546159 1530854 finetune.py:68] layer 23_o @ epoch 2 new loss 0.0005468300078064203 old loss 0.0005529539193958044 BETTER
I0320 21:30:51.222752 1530692 finetune.py:68] layer 20_up @ epoch 3 new loss 0.000949614797718823 old loss 0.000957787677180022 BETTER
I0320 21:31:11.379590 1530800 finetune.py:45] layer 22_up initial loss 0.0012133665150031447
I0320 21:31:17.417024 1530746 finetune.py:68] layer 21_up @ epoch 2 new loss 0.0009758985252119601 old loss 0.000984594109468162 BETTER
I0320 21:31:19.679008 1530854 finetune.py:68] layer 23_o @ epoch 3 new loss 0.0005422059330157936 old loss 0.0005468300078064203 BETTER
I0320 21:31:26.390389 1530692 finetune.py:68] layer 20_up @ epoch 4 new loss 0.0009424485615454614 old loss 0.000949614797718823 BETTER
I0320 21:31:44.001103 1530800 finetune.py:68] layer 22_up @ epoch 0 new loss 0.001192079042084515 old loss 0.0012133665150031447 BETTER
I0320 21:31:51.016847 1530746 finetune.py:68] layer 21_up @ epoch 3 new loss 0.0009687173878774047 old loss 0.0009758985252119601 BETTER
I0320 21:31:55.452493 1530854 finetune.py:68] layer 23_o @ epoch 4 new loss 0.00053861626656726 old loss 0.0005422059330157936 BETTER
I0320 21:31:57.850461 1530692 finetune.py:45] layer 20_gate initial loss 0.0013540377840399742
I0320 21:32:17.415940 1530800 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0011795047903433442 old loss 0.001192079042084515 BETTER
I0320 21:32:25.566985 1530746 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0009628221741877496 old loss 0.0009687173878774047 BETTER
I0320 21:32:26.294049 1530854 finetune.py:45] layer 23_up initial loss 0.0012491107918322086
I0320 21:32:30.242416 1530692 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.0013422704068943858 old loss 0.0013540377840399742 BETTER
I0320 21:32:51.692672 1530800 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0011698477901518345 old loss 0.0011795047903433442 BETTER
I0320 21:32:57.638055 1530746 finetune.py:45] layer 21_gate initial loss 0.0014086317969486117
I0320 21:32:58.830916 1530854 finetune.py:68] layer 23_up @ epoch 0 new loss 0.001227907370775938 old loss 0.0012491107918322086 BETTER
I0320 21:33:03.281012 1530692 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0013339390279725194 old loss 0.0013422704068943858 BETTER
I0320 21:33:27.081987 1530800 finetune.py:68] layer 22_up @ epoch 3 new loss 0.001161704771220684 old loss 0.0011698477901518345 BETTER
I0320 21:33:29.118882 1530746 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0013987318379804492 old loss 0.0014086317969486117 BETTER
I0320 21:33:32.851232 1530854 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0012159717734903097 old loss 0.001227907370775938 BETTER
I0320 21:33:36.849108 1530692 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.0013267220929265022 old loss 0.0013339390279725194 BETTER
I0320 21:34:02.240127 1530746 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0013915154850110412 old loss 0.0013987318379804492 BETTER
I0320 21:34:02.985551 1530800 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0011550678173080087 old loss 0.001161704771220684 BETTER
I0320 21:34:07.172463 1530854 finetune.py:68] layer 23_up @ epoch 2 new loss 0.001206895336508751 old loss 0.0012159717734903097 BETTER
I0320 21:34:10.239198 1530692 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0013207909651100636 old loss 0.0013267220929265022 BETTER
I0320 21:34:33.270465 1530800 finetune.py:45] layer 22_gate initial loss 0.0016606529243290424
I0320 21:34:34.704941 1530746 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0013855028664693236 old loss 0.0013915154850110412 BETTER
I0320 21:34:41.229120 1530854 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0011993884108960629 old loss 0.001206895336508751 BETTER
I0320 21:34:44.407658 1530692 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.001315548550337553 old loss 0.0013207909651100636 BETTER
I0320 21:35:04.605542 1530800 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.001650358084589243 old loss 0.0016606529243290424 BETTER
I0320 21:35:06.912960 1530746 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.0013802708126604557 old loss 0.0013855028664693236 BETTER
I0320 21:35:14.774461 1530854 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0011930835898965597 old loss 0.0011993884108960629 BETTER
I0320 21:35:17.986909 1530692 finetune.py:45] layer 20_down initial loss 0.002298334613442421
I0320 21:35:36.840304 1530800 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.0016425112262368202 old loss 0.001650358084589243 BETTER
I0320 21:35:39.175736 1530746 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.0013758731074631214 old loss 0.0013802708126604557 BETTER
I0320 21:35:45.770493 1530854 finetune.py:45] layer 23_gate initial loss 0.0017611937364563346
I0320 21:35:47.955321 1530692 finetune.py:68] layer 20_down @ epoch 0 new loss 0.0022975378669798374 old loss 0.002298334613442421 BETTER
I0320 21:36:10.149616 1530800 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0016359752044081688 old loss 0.0016425112262368202 BETTER
I0320 21:36:13.404947 1530746 finetune.py:45] layer 21_down initial loss 0.002370100002735853
I0320 21:36:17.330887 1530854 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0017510424368083477 old loss 0.0017611937364563346 BETTER
I0320 21:36:20.318571 1530692 finetune.py:68] layer 20_down @ epoch 1 new loss 0.002296936698257923 old loss 0.0022975378669798374 BETTER
I0320 21:36:43.694755 1530746 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0023693193215876818 old loss 0.002370100002735853 BETTER
I0320 21:36:43.832936 1530800 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.001630256650969386 old loss 0.0016359752044081688 BETTER
I0320 21:36:49.708409 1530854 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.001743554719723761 old loss 0.0017510424368083477 BETTER
I0320 21:36:52.584626 1530692 finetune.py:68] layer 20_down @ epoch 2 new loss 0.002296474063768983 old loss 0.002296936698257923 BETTER
I0320 21:37:15.111500 1530746 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0023687155917286873 old loss 0.0023693193215876818 BETTER
I0320 21:37:18.503458 1530800 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0016254163347184658 old loss 0.001630256650969386 BETTER
I0320 21:37:22.132719 1530854 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.001737389829941094 old loss 0.001743554719723761 BETTER
I0320 21:37:24.582836 1530692 finetune.py:68] layer 20_down @ epoch 3 new loss 0.002296116668730974 old loss 0.002296474063768983 BETTER
I0320 21:37:45.504406 1530746 finetune.py:68] layer 21_down @ epoch 2 new loss 0.002368261804804206 old loss 0.0023687155917286873 BETTER
I0320 21:37:53.252317 1530800 finetune.py:45] layer 22_down initial loss 0.002721107332035899
I0320 21:37:54.824453 1530854 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.0017319926992058754 old loss 0.001737389829941094 BETTER
I0320 21:37:57.314148 1530692 finetune.py:68] layer 20_down @ epoch 4 new loss 0.00229583028703928 old loss 0.002296116668730974 BETTER
20_v proxy err 0.018854908645153046 tr(WHW.T) 990.5983276367188
bpp_loss 3.1979482173919678
20_q proxy err 0.003003202611580491 tr(WHW.T) 7156.45654296875
bpp_loss 3.5638694763183594
20_k proxy err 0.002128689084202051 tr(WHW.T) 10429.4599609375
bpp_loss 3.5929462909698486
20_o proxy err 0.012904897332191467 tr(WHW.T) 100.6236801147461
bpp_loss 3.1912307739257812
20_up proxy err 0.011748282238841057 tr(WHW.T) 2340.24951171875
bpp_loss 3.2686068512672604
20_gate proxy err 0.007071938831359148 tr(WHW.T) 4019.875732421875
bpp_loss 3.3562460611032887
20_down proxy err 0.015050996094942093 tr(WHW.T) 277.154052734375
bpp_loss 3.2590276141499364
I0320 21:38:14.443780 1530746 finetune.py:68] layer 21_down @ epoch 3 new loss 0.0023679030127823353 old loss 0.002368261804804206 BETTER
I0320 21:38:23.978886 1530800 finetune.py:68] layer 22_down @ epoch 0 new loss 0.002720346674323082 old loss 0.002721107332035899 BETTER
I0320 21:38:26.711141 1530854 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0017274292185902596 old loss 0.0017319926992058754 BETTER
I0320 21:38:43.306423 1530746 finetune.py:68] layer 21_down @ epoch 4 new loss 0.002367620123550296 old loss 0.0023679030127823353 BETTER
21_v proxy err 0.018091760575771332 tr(WHW.T) 1144.5655517578125
bpp_loss 3.2343860864639282
21_q proxy err 0.003316758666187525 tr(WHW.T) 7072.92919921875
bpp_loss 3.5287370681762695
21_k proxy err 0.002415027469396591 tr(WHW.T) 10015.59765625
bpp_loss 3.5458433628082275
21_o proxy err 0.013802122324705124 tr(WHW.T) 75.74512481689453
bpp_loss 3.217109203338623
21_up proxy err 0.012351908721029758 tr(WHW.T) 2360.817626953125
bpp_loss 3.266539995060411
21_gate proxy err 0.00755323888733983 tr(WHW.T) 3998.25
bpp_loss 3.3653727686682413
21_down proxy err 0.015078042633831501 tr(WHW.T) 278.4961242675781
bpp_loss 3.2588458393895348
I0320 21:38:53.701097 1530800 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0027197308372706175 old loss 0.002720346674323082 BETTER
I0320 21:38:55.729966 1530854 finetune.py:45] layer 23_down initial loss 0.0028572881128638983
I0320 21:39:23.387274 1530800 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0027192437555640936 old loss 0.0027197308372706175 BETTER
I0320 21:39:23.463208 1530854 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0028565877582877874 old loss 0.0028572881128638983 BETTER
I0320 21:39:50.659002 1530854 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0028560557402670383 old loss 0.0028565877582877874 BETTER
I0320 21:39:51.226196 1530800 finetune.py:68] layer 22_down @ epoch 3 new loss 0.002718850038945675 old loss 0.0027192437555640936 BETTER
I0320 21:40:00.998694 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 24 in 71.89430141448975s
I0320 21:40:05.173574 1530908 config.py:54] PyTorch version 2.6.0 available.
W0320 21:40:05.533687 1530908 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 21:40:06.639489 1530908 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 21:40:06.644055 1523378 quantize_finetune_llama.py:203] layer 25 gpu 1
I0320 21:40:06.660022 1530908 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 21:40:20.816672 1530854 finetune.py:68] layer 23_down @ epoch 2 new loss 0.002855633618310094 old loss 0.0028560557402670383 BETTER
I0320 21:40:22.025416 1530908 finetune.py:45] layer 24_v initial loss 0.00055326969595626
W0320 21:40:22.025657 1530908 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 21:40:22.093814 1530800 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0027185268700122833 old loss 0.002718850038945675 BETTER
22_v proxy err 0.017366303130984306 tr(WHW.T) 1243.2529296875
bpp_loss 3.2385815382003784
22_q proxy err 0.0031650480814278126 tr(WHW.T) 7752.10498046875
bpp_loss 3.561266303062439
22_k proxy err 0.002355920849367976 tr(WHW.T) 10638.490234375
bpp_loss 3.5813987255096436
22_o proxy err 0.011670692823827267 tr(WHW.T) 114.63162231445312
bpp_loss 3.220090866088867
22_up proxy err 0.012422226369380951 tr(WHW.T) 2475.24560546875
bpp_loss 3.2659158041310863
22_gate proxy err 0.007683430332690477 tr(WHW.T) 4153.4970703125
bpp_loss 3.3732429326966753
22_down proxy err 0.015004300512373447 tr(WHW.T) 314.0162048339844
bpp_loss 3.259436008542083
I0320 21:40:48.225911 1530854 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0028552976436913013 old loss 0.002855633618310094 BETTER
I0320 21:40:55.956987 1530908 finetune.py:68] layer 24_v @ epoch 0 new loss 0.0003164403315167874 old loss 0.00055326969595626 BETTER
I0320 21:41:17.653231 1530854 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0028550364077091217 old loss 0.0028552976436913013 BETTER
23_v proxy err 0.015879493206739426 tr(WHW.T) 1486.037353515625
bpp_loss 3.296311378479004
23_q proxy err 0.00356980855576694 tr(WHW.T) 7353.0517578125
bpp_loss 3.5502530336380005
23_k proxy err 0.002667670138180256 tr(WHW.T) 10019.5009765625
bpp_loss 3.562597870826721
23_o proxy err 0.014098076149821281 tr(WHW.T) 85.45782470703125
bpp_loss 3.2810134887695312
23_up proxy err 0.012873342260718346 tr(WHW.T) 2534.349365234375
bpp_loss 3.2721126467682593
23_gate proxy err 0.008232996799051762 tr(WHW.T) 4097.6142578125
bpp_loss 3.372902271359466
23_down proxy err 0.015083670616149902 tr(WHW.T) 323.3678283691406
bpp_loss 3.2667611809663994
I0320 21:41:31.114984 1530908 finetune.py:68] layer 24_v @ epoch 1 new loss 0.00028256484074518085 old loss 0.0003164403315167874 BETTER
I0320 21:41:36.298925 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 25 in 68.93178200721741s
I0320 21:41:39.754621 1530962 config.py:54] PyTorch version 2.6.0 available.
W0320 21:41:40.070046 1530962 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 21:41:41.090268 1530962 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 21:41:41.094275 1523378 quantize_finetune_llama.py:203] layer 26 gpu 2
I0320 21:41:41.108302 1530962 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 21:41:54.671903 1530962 finetune.py:45] layer 25_v initial loss 0.0005839124205522239
W0320 21:41:54.672093 1530962 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 21:42:08.470071 1530908 finetune.py:68] layer 24_v @ epoch 2 new loss 0.000266237766481936 old loss 0.00028256484074518085 BETTER
I0320 21:42:26.785315 1530962 finetune.py:68] layer 25_v @ epoch 0 new loss 0.00028836759156547487 old loss 0.0005839124205522239 BETTER
I0320 21:42:46.196088 1530908 finetune.py:68] layer 24_v @ epoch 3 new loss 0.0002565484319347888 old loss 0.000266237766481936 BETTER
I0320 21:42:55.120884 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 26 in 73.54113626480103s
I0320 21:42:59.585731 1531016 config.py:54] PyTorch version 2.6.0 available.
W0320 21:43:00.027074 1531016 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 21:43:01.160255 1531016 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 21:43:01.165044 1523378 quantize_finetune_llama.py:203] layer 27 gpu 3
I0320 21:43:01.197203 1531016 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 21:43:01.787541 1530962 finetune.py:68] layer 25_v @ epoch 1 new loss 0.00025505691883154213 old loss 0.00028836759156547487 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 21:43:16.090915 1531016 finetune.py:45] layer 26_v initial loss 0.0008051237091422081
W0320 21:43:16.091703 1531016 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 21:43:23.669023 1530908 finetune.py:68] layer 24_v @ epoch 4 new loss 0.0002494020445737988 old loss 0.0002565484319347888 BETTER
I0320 21:43:37.709454 1530962 finetune.py:68] layer 25_v @ epoch 2 new loss 0.00024163111811503768 old loss 0.00025505691883154213 BETTER
I0320 21:43:40.781404 1530908 finetune.py:45] layer 24_q initial loss 0.00033225459628738463
I0320 21:43:51.919106 1531016 finetune.py:68] layer 26_v @ epoch 0 new loss 0.00042552960803732276 old loss 0.0008051237091422081 BETTER
I0320 21:44:14.781788 1530962 finetune.py:68] layer 25_v @ epoch 3 new loss 0.00023373278963845223 old loss 0.00024163111811503768 BETTER
I0320 21:44:17.450139 1530908 finetune.py:68] layer 24_q @ epoch 0 new loss 0.0003081295290030539 old loss 0.00033225459628738463 BETTER
I0320 21:44:21.472684 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 27 in 79.72830629348755s
I0320 21:44:26.122564 1531070 config.py:54] PyTorch version 2.6.0 available.
W0320 21:44:26.563662 1531070 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 21:44:27.735282 1531070 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 21:44:27.739578 1523378 quantize_finetune_llama.py:203] layer 28 gpu 0
I0320 21:44:27.753859 1531070 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 21:44:28.477551 1531016 finetune.py:68] layer 26_v @ epoch 1 new loss 0.0003855878603644669 old loss 0.00042552960803732276 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 21:44:43.551545 1531070 finetune.py:45] layer 27_v initial loss 0.0005957915564067662
W0320 21:44:43.552145 1531070 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 21:44:50.951354 1530962 finetune.py:68] layer 25_v @ epoch 4 new loss 0.00022767116024624556 old loss 0.00023373278963845223 BETTER
I0320 21:44:54.777286 1530908 finetune.py:68] layer 24_q @ epoch 1 new loss 0.0002989575732499361 old loss 0.0003081295290030539 BETTER
I0320 21:45:05.023197 1531016 finetune.py:68] layer 26_v @ epoch 2 new loss 0.00036635715514421463 old loss 0.0003855878603644669 BETTER
I0320 21:45:09.755713 1530962 finetune.py:45] layer 25_q initial loss 0.00030886687454767525
I0320 21:45:18.234125 1531070 finetune.py:68] layer 27_v @ epoch 0 new loss 0.0003219463105779141 old loss 0.0005957915564067662 BETTER
I0320 21:45:32.316901 1530908 finetune.py:68] layer 24_q @ epoch 2 new loss 0.0002925902372226119 old loss 0.0002989575732499361 BETTER
I0320 21:45:42.325595 1531016 finetune.py:68] layer 26_v @ epoch 3 new loss 0.0003537710872478783 old loss 0.00036635715514421463 BETTER
I0320 21:45:44.949321 1530962 finetune.py:68] layer 25_q @ epoch 0 new loss 0.0002818118373397738 old loss 0.00030886687454767525 BETTER
I0320 21:45:54.605858 1531070 finetune.py:68] layer 27_v @ epoch 1 new loss 0.00030171844991855323 old loss 0.0003219463105779141 BETTER
I0320 21:46:09.339667 1530908 finetune.py:68] layer 24_q @ epoch 3 new loss 0.0002874737838283181 old loss 0.0002925902372226119 BETTER
I0320 21:46:19.633753 1531016 finetune.py:68] layer 26_v @ epoch 4 new loss 0.00034485216019675136 old loss 0.0003537710872478783 BETTER
I0320 21:46:21.370799 1530962 finetune.py:68] layer 25_q @ epoch 1 new loss 0.000274226360488683 old loss 0.0002818118373397738 BETTER
I0320 21:46:29.855654 1531070 finetune.py:68] layer 27_v @ epoch 2 new loss 0.0002910762559622526 old loss 0.00030171844991855323 BETTER
I0320 21:46:37.263786 1531016 finetune.py:45] layer 26_q initial loss 0.0004543954273685813
I0320 21:46:47.543571 1530908 finetune.py:68] layer 24_q @ epoch 4 new loss 0.000283457717159763 old loss 0.0002874737838283181 BETTER
I0320 21:46:56.309431 1530962 finetune.py:68] layer 25_q @ epoch 2 new loss 0.00026874232571572065 old loss 0.000274226360488683 BETTER
I0320 21:47:05.796779 1530908 finetune.py:45] layer 24_k initial loss 0.000355766445863992
I0320 21:47:06.695105 1531070 finetune.py:68] layer 27_v @ epoch 3 new loss 0.00028381007723510265 old loss 0.0002910762559622526 BETTER
I0320 21:47:12.112879 1531016 finetune.py:68] layer 26_q @ epoch 0 new loss 0.0004237389948684722 old loss 0.0004543954273685813 BETTER
I0320 21:47:32.243208 1530962 finetune.py:68] layer 25_q @ epoch 3 new loss 0.0002646435459610075 old loss 0.00026874232571572065 BETTER
I0320 21:47:42.302165 1530908 finetune.py:68] layer 24_k @ epoch 0 new loss 0.0003461912856437266 old loss 0.000355766445863992 BETTER
I0320 21:47:43.803704 1531070 finetune.py:68] layer 27_v @ epoch 4 new loss 0.0002782554947771132 old loss 0.00028381007723510265 BETTER
I0320 21:47:48.339617 1531016 finetune.py:68] layer 26_q @ epoch 1 new loss 0.00041185939335264266 old loss 0.0004237389948684722 BETTER
I0320 21:48:02.132559 1531070 finetune.py:45] layer 27_q initial loss 0.0003930994134861976
I0320 21:48:07.790784 1530962 finetune.py:68] layer 25_q @ epoch 4 new loss 0.00026125754811801016 old loss 0.0002646435459610075 BETTER
I0320 21:48:18.732691 1530908 finetune.py:68] layer 24_k @ epoch 1 new loss 0.00034248331212438643 old loss 0.0003461912856437266 BETTER
I0320 21:48:25.159482 1531016 finetune.py:68] layer 26_q @ epoch 2 new loss 0.00040307617746293545 old loss 0.00041185939335264266 BETTER
I0320 21:48:25.923603 1530962 finetune.py:45] layer 25_k initial loss 0.00033745859400369227
I0320 21:48:37.211757 1531070 finetune.py:68] layer 27_q @ epoch 0 new loss 0.0003574782167561352 old loss 0.0003930994134861976 BETTER
I0320 21:48:56.322479 1530908 finetune.py:68] layer 24_k @ epoch 2 new loss 0.000339449878083542 old loss 0.00034248331212438643 BETTER
I0320 21:49:01.195557 1530962 finetune.py:68] layer 25_k @ epoch 0 new loss 0.0003224374959245324 old loss 0.00033745859400369227 BETTER
I0320 21:49:02.010571 1531016 finetune.py:68] layer 26_q @ epoch 3 new loss 0.0003963961498811841 old loss 0.00040307617746293545 BETTER
I0320 21:49:12.926048 1531070 finetune.py:68] layer 27_q @ epoch 1 new loss 0.00034838865394704044 old loss 0.0003574782167561352 BETTER
I0320 21:49:34.574694 1530908 finetune.py:68] layer 24_k @ epoch 3 new loss 0.00033692040597088635 old loss 0.000339449878083542 BETTER
I0320 21:49:38.067049 1530962 finetune.py:68] layer 25_k @ epoch 1 new loss 0.00031951855635270476 old loss 0.0003224374959245324 BETTER
I0320 21:49:39.759664 1531016 finetune.py:68] layer 26_q @ epoch 4 new loss 0.00039075021049939096 old loss 0.0003963961498811841 BETTER
I0320 21:49:47.793028 1531070 finetune.py:68] layer 27_q @ epoch 2 new loss 0.0003416905819904059 old loss 0.00034838865394704044 BETTER
I0320 21:49:57.919254 1531016 finetune.py:45] layer 26_k initial loss 0.0004911027499474585
I0320 21:50:12.970108 1530908 finetune.py:68] layer 24_k @ epoch 4 new loss 0.00033469582558609545 old loss 0.00033692040597088635 BETTER
I0320 21:50:14.137609 1530962 finetune.py:68] layer 25_k @ epoch 2 new loss 0.00031727884197607636 old loss 0.00031951855635270476 BETTER
I0320 21:50:23.172700 1531070 finetune.py:68] layer 27_q @ epoch 3 new loss 0.0003364565782248974 old loss 0.0003416905819904059 BETTER
I0320 21:50:32.350857 1530908 finetune.py:45] layer 24_o initial loss 0.0007242968422360718
I0320 21:50:33.431443 1531016 finetune.py:68] layer 26_k @ epoch 0 new loss 0.0004736895498353988 old loss 0.0004911027499474585 BETTER
I0320 21:50:50.223696 1530962 finetune.py:68] layer 25_k @ epoch 3 new loss 0.0003151401469949633 old loss 0.00031727884197607636 BETTER
I0320 21:50:59.123268 1531070 finetune.py:68] layer 27_q @ epoch 4 new loss 0.0003321611729916185 old loss 0.0003364565782248974 BETTER
I0320 21:51:07.037542 1530908 finetune.py:68] layer 24_o @ epoch 0 new loss 0.0006871687364764512 old loss 0.0007242968422360718 BETTER
I0320 21:51:09.147922 1531016 finetune.py:68] layer 26_k @ epoch 1 new loss 0.00046883593313395977 old loss 0.0004736895498353988 BETTER
I0320 21:51:18.166029 1531070 finetune.py:45] layer 27_k initial loss 0.00042576849227771163
I0320 21:51:25.704232 1530962 finetune.py:68] layer 25_k @ epoch 4 new loss 0.0003134592843707651 old loss 0.0003151401469949633 BETTER
I0320 21:51:44.277764 1530962 finetune.py:45] layer 25_o initial loss 0.0006119306781329215
I0320 21:51:45.225368 1530908 finetune.py:68] layer 24_o @ epoch 1 new loss 0.0006762385019101202 old loss 0.0006871687364764512 BETTER
I0320 21:51:46.351042 1531016 finetune.py:68] layer 26_k @ epoch 2 new loss 0.0004649760667234659 old loss 0.00046883593313395977 BETTER
I0320 21:51:52.849011 1531070 finetune.py:68] layer 27_k @ epoch 0 new loss 0.00041162106208503246 old loss 0.00042576849227771163 BETTER
I0320 21:52:19.250260 1530962 finetune.py:68] layer 25_o @ epoch 0 new loss 0.0005932265194132924 old loss 0.0006119306781329215 BETTER
I0320 21:52:23.741241 1530908 finetune.py:68] layer 24_o @ epoch 2 new loss 0.0006687602726742625 old loss 0.0006762385019101202 BETTER
I0320 21:52:24.626615 1531016 finetune.py:68] layer 26_k @ epoch 3 new loss 0.00046168797416612506 old loss 0.0004649760667234659 BETTER
I0320 21:52:29.255183 1531070 finetune.py:68] layer 27_k @ epoch 1 new loss 0.0004078788042534143 old loss 0.00041162106208503246 BETTER
I0320 21:52:55.467196 1530962 finetune.py:68] layer 25_o @ epoch 1 new loss 0.000586559996008873 old loss 0.0005932265194132924 BETTER
I0320 21:53:02.335837 1531016 finetune.py:68] layer 26_k @ epoch 4 new loss 0.00045885206782259047 old loss 0.00046168797416612506 BETTER
I0320 21:53:02.364916 1530908 finetune.py:68] layer 24_o @ epoch 3 new loss 0.0006629704148508608 old loss 0.0006687602726742625 BETTER
I0320 21:53:05.832280 1531070 finetune.py:68] layer 27_k @ epoch 2 new loss 0.00040507709491066635 old loss 0.0004078788042534143 BETTER
I0320 21:53:20.350207 1531016 finetune.py:45] layer 26_o initial loss 0.0009510032250545919
I0320 21:53:30.054665 1530962 finetune.py:68] layer 25_o @ epoch 2 new loss 0.000582005362957716 old loss 0.000586559996008873 BETTER
I0320 21:53:40.095304 1530908 finetune.py:68] layer 24_o @ epoch 4 new loss 0.0006584803923033178 old loss 0.0006629704148508608 BETTER
I0320 21:53:41.682772 1531070 finetune.py:68] layer 27_k @ epoch 3 new loss 0.00040278208325617015 old loss 0.00040507709491066635 BETTER
I0320 21:53:54.283445 1531016 finetune.py:68] layer 26_o @ epoch 0 new loss 0.0008916515507735312 old loss 0.0009510032250545919 BETTER
I0320 21:54:05.250956 1530962 finetune.py:68] layer 25_o @ epoch 3 new loss 0.0005784312379546463 old loss 0.000582005362957716 BETTER
I0320 21:54:10.344815 1530908 finetune.py:45] layer 24_up initial loss 0.001425736933015287
I0320 21:54:16.577353 1531070 finetune.py:68] layer 27_k @ epoch 4 new loss 0.0004007092211395502 old loss 0.00040278208325617015 BETTER
I0320 21:54:29.745980 1531016 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0008782024960964918 old loss 0.0008916515507735312 BETTER
I0320 21:54:34.409782 1531070 finetune.py:45] layer 27_o initial loss 0.0008120210841298103
I0320 21:54:41.857557 1530962 finetune.py:68] layer 25_o @ epoch 4 new loss 0.0005758273182436824 old loss 0.0005784312379546463 BETTER
I0320 21:54:44.143657 1530908 finetune.py:68] layer 24_up @ epoch 0 new loss 0.0014053399208933115 old loss 0.001425736933015287 BETTER
I0320 21:55:05.138876 1531016 finetune.py:68] layer 26_o @ epoch 2 new loss 0.0008685745997354388 old loss 0.0008782024960964918 BETTER
I0320 21:55:08.384663 1531070 finetune.py:68] layer 27_o @ epoch 0 new loss 0.0007709214696660638 old loss 0.0008120210841298103 BETTER
I0320 21:55:11.421833 1530962 finetune.py:45] layer 25_up initial loss 0.00143870385363698
I0320 21:55:18.785822 1530908 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0013935640454292297 old loss 0.0014053399208933115 BETTER
I0320 21:55:42.738693 1531016 finetune.py:68] layer 26_o @ epoch 3 new loss 0.0008613505051471293 old loss 0.0008685745997354388 BETTER
I0320 21:55:44.715574 1531070 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0007566629210487008 old loss 0.0007709214696660638 BETTER
I0320 21:55:44.963891 1530962 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0014142910949885845 old loss 0.00143870385363698 BETTER
I0320 21:55:54.462312 1530908 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0013841937761753798 old loss 0.0013935640454292297 BETTER
I0320 21:56:19.881354 1530962 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0014012090396136045 old loss 0.0014142910949885845 BETTER
I0320 21:56:19.987409 1531016 finetune.py:68] layer 26_o @ epoch 4 new loss 0.0008555774111300707 old loss 0.0008613505051471293 BETTER
I0320 21:56:20.884119 1531070 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0007467766990885139 old loss 0.0007566629210487008 BETTER
I0320 21:56:29.228857 1530908 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0013765412149950862 old loss 0.0013841937761753798 BETTER
I0320 21:56:50.043655 1531016 finetune.py:45] layer 26_up initial loss 0.0017955515068024397
I0320 21:56:54.972167 1530962 finetune.py:68] layer 25_up @ epoch 2 new loss 0.0013911881251260638 old loss 0.0014012090396136045 BETTER
I0320 21:56:55.994002 1531070 finetune.py:68] layer 27_o @ epoch 3 new loss 0.0007398542948067188 old loss 0.0007467766990885139 BETTER
I0320 21:57:04.628062 1530908 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0013701386051252484 old loss 0.0013765412149950862 BETTER
I0320 21:57:22.375225 1531016 finetune.py:68] layer 26_up @ epoch 0 new loss 0.0017676681745797396 old loss 0.0017955515068024397 BETTER
I0320 21:57:29.033579 1530962 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0013833849225193262 old loss 0.0013911881251260638 BETTER
I0320 21:57:31.739092 1531070 finetune.py:68] layer 27_o @ epoch 4 new loss 0.0007340434240177274 old loss 0.0007398542948067188 BETTER
I0320 21:57:35.233504 1530908 finetune.py:45] layer 24_gate initial loss 0.0019987057894468307
I0320 21:57:57.078912 1531016 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0017524348804727197 old loss 0.0017676681745797396 BETTER
I0320 21:58:02.972595 1531070 finetune.py:45] layer 27_up initial loss 0.0018094673287123442
I0320 21:58:03.831394 1530962 finetune.py:68] layer 25_up @ epoch 4 new loss 0.0013766037300229073 old loss 0.0013833849225193262 BETTER
I0320 21:58:07.635224 1530908 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.001988884760066867 old loss 0.0019987057894468307 BETTER
I0320 21:58:31.970772 1531016 finetune.py:68] layer 26_up @ epoch 2 new loss 0.0017407421255484223 old loss 0.0017524348804727197 BETTER
I0320 21:58:34.638419 1530962 finetune.py:45] layer 25_gate initial loss 0.002077839570119977
I0320 21:58:35.790506 1531070 finetune.py:68] layer 27_up @ epoch 0 new loss 0.001770642353221774 old loss 0.0018094673287123442 BETTER
I0320 21:58:40.783397 1530908 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.001981579465791583 old loss 0.001988884760066867 BETTER
I0320 21:59:06.369568 1530962 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.0020674553234130144 old loss 0.002077839570119977 BETTER
I0320 21:59:07.700538 1531016 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0017313966527581215 old loss 0.0017407421255484223 BETTER
I0320 21:59:10.721497 1531070 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0017515033250674605 old loss 0.001770642353221774 BETTER
I0320 21:59:14.461911 1530908 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.0019752983935177326 old loss 0.001981579465791583 BETTER
I0320 21:59:39.684850 1530962 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.0020598338451236486 old loss 0.0020674553234130144 BETTER
I0320 21:59:43.389441 1531016 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0017234609695151448 old loss 0.0017313966527581215 BETTER
I0320 21:59:45.640950 1531070 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0017374256858602166 old loss 0.0017515033250674605 BETTER
I0320 21:59:48.046167 1530908 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.001969807781279087 old loss 0.0019752983935177326 BETTER
I0320 22:00:11.465415 1530962 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.002053317613899708 old loss 0.0020598338451236486 BETTER
I0320 22:00:14.673273 1531016 finetune.py:45] layer 26_gate initial loss 0.002510300138965249
I0320 22:00:19.911694 1531070 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0017262283945456147 old loss 0.0017374256858602166 BETTER
I0320 22:00:21.652828 1530908 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.0019652401097118855 old loss 0.001969807781279087 BETTER
I0320 22:00:43.342576 1530962 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.002047866815701127 old loss 0.002053317613899708 BETTER
I0320 22:00:45.236009 1531016 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.002498509595170617 old loss 0.002510300138965249 BETTER
I0320 22:00:54.164895 1531070 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0017169463681057096 old loss 0.0017262283945456147 BETTER
I0320 22:00:54.518120 1530908 finetune.py:45] layer 24_down initial loss 0.0031753976363688707
I0320 22:01:14.924764 1530962 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0020430178847163916 old loss 0.002047866815701127 BETTER
I0320 22:01:17.121675 1531016 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.0024898976553231478 old loss 0.002498509595170617 BETTER
I0320 22:01:24.335624 1530908 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0031745461747050285 old loss 0.0031753976363688707 BETTER
I0320 22:01:24.988629 1531070 finetune.py:45] layer 27_gate initial loss 0.0026297515723854303
I0320 22:01:48.334269 1530962 finetune.py:45] layer 25_down initial loss 0.0033666901290416718
I0320 22:01:51.172828 1531016 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0024825006257742643 old loss 0.0024898976553231478 BETTER
I0320 22:01:56.885833 1530908 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0031738632824271917 old loss 0.0031745461747050285 BETTER
I0320 22:01:56.934017 1531070 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.002613682299852371 old loss 0.0026297515723854303 BETTER
I0320 22:02:18.620462 1530962 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0033659369219094515 old loss 0.0033666901290416718 BETTER
I0320 22:02:25.681636 1531016 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.002476298715919256 old loss 0.0024825006257742643 BETTER
I0320 22:02:30.061736 1531070 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.002602588152512908 old loss 0.002613682299852371 BETTER
I0320 22:02:30.332540 1530908 finetune.py:68] layer 24_down @ epoch 2 new loss 0.003173330333083868 old loss 0.0031738632824271917 BETTER
I0320 22:02:49.555273 1530962 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0033652910497039557 old loss 0.0033659369219094515 BETTER
I0320 22:02:59.430868 1531016 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0024707738775759935 old loss 0.002476298715919256 BETTER
I0320 22:03:02.770543 1530908 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0031729009933769703 old loss 0.003173330333083868 BETTER
I0320 22:03:02.828274 1531070 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.0025936122983694077 old loss 0.002602588152512908 BETTER
I0320 22:03:19.679097 1530962 finetune.py:68] layer 25_down @ epoch 2 new loss 0.003364748088642955 old loss 0.0033652910497039557 BETTER
I0320 22:03:33.935388 1531016 finetune.py:45] layer 26_down initial loss 0.003923308104276657
I0320 22:03:36.200511 1531070 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.00258613470941782 old loss 0.0025936122983694077 BETTER
I0320 22:03:36.369431 1530908 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0031725377775728703 old loss 0.0031729009933769703 BETTER
24_v proxy err 0.016751199960708618 tr(WHW.T) 1394.900634765625
bpp_loss 3.2846686840057373
24_q proxy err 0.003671102225780487 tr(WHW.T) 7026.24853515625
bpp_loss 3.5069626569747925
24_k proxy err 0.0025600523222237825 tr(WHW.T) 10361.0556640625
bpp_loss 3.515405058860779
24_o proxy err 0.011194111779332161 tr(WHW.T) 134.25941467285156
bpp_loss 3.263099193572998
24_up proxy err 0.01304713636636734 tr(WHW.T) 2623.146240234375
bpp_loss 3.2760258164516713
24_gate proxy err 0.008301079273223877 tr(WHW.T) 4263.708984375
bpp_loss 3.3762618663699127
24_down proxy err 0.015166576951742172 tr(WHW.T) 342.27838134765625
bpp_loss 3.2711762494819108
I0320 22:03:50.047123 1530962 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0033642868511378765 old loss 0.003364748088642955 BETTER
I0320 22:04:05.030427 1531016 finetune.py:68] layer 26_down @ epoch 0 new loss 0.00392252579331398 old loss 0.003923308104276657 BETTER
I0320 22:04:09.364047 1531070 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.002579488791525364 old loss 0.00258613470941782 BETTER
I0320 22:04:19.358990 1530962 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0033639029134064913 old loss 0.0033642868511378765 BETTER
25_v proxy err 0.015283032320439816 tr(WHW.T) 1707.664794921875
bpp_loss 3.332846522331238
25_q proxy err 0.003948458004742861 tr(WHW.T) 7165.806640625
bpp_loss 3.519814610481262
25_k proxy err 0.002991745015606284 tr(WHW.T) 9640.2109375
bpp_loss 3.525153160095215
25_o proxy err 0.013329986482858658 tr(WHW.T) 83.86080932617188
bpp_loss 3.3145419359207153
25_up proxy err 0.01298400480300188 tr(WHW.T) 2805.68310546875
bpp_loss 3.281519335369731
25_gate proxy err 0.00808921828866005 tr(WHW.T) 4662.96826171875
bpp_loss 3.3788193103879
25_down proxy err 0.015069537796080112 tr(WHW.T) 376.12408447265625
bpp_loss 3.2773609161376953
I0320 22:04:33.751111 1531016 finetune.py:68] layer 26_down @ epoch 1 new loss 0.00392193766310811 old loss 0.00392252579331398 BETTER
I0320 22:04:38.149701 1531070 finetune.py:45] layer 27_down initial loss 0.004312380217015743
I0320 22:05:04.477761 1531016 finetune.py:68] layer 26_down @ epoch 2 new loss 0.003921450115740299 old loss 0.00392193766310811 BETTER
I0320 22:05:06.167870 1531070 finetune.py:68] layer 27_down @ epoch 0 new loss 0.004311683587729931 old loss 0.004312380217015743 BETTER
I0320 22:05:33.612597 1531016 finetune.py:68] layer 26_down @ epoch 3 new loss 0.003921041265130043 old loss 0.003921450115740299 BETTER
I0320 22:05:34.547797 1531070 finetune.py:68] layer 27_down @ epoch 1 new loss 0.004311090800911188 old loss 0.004311683587729931 BETTER
I0320 22:05:36.996402 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 28 in 72.05983901023865s
I0320 22:05:41.453586 1531124 config.py:54] PyTorch version 2.6.0 available.
W0320 22:05:41.878125 1531124 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 22:05:43.022768 1531124 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 22:05:43.027220 1523378 quantize_finetune_llama.py:203] layer 29 gpu 1
I0320 22:05:43.044419 1531124 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 22:05:58.355060 1531124 finetune.py:45] layer 28_v initial loss 0.0007790315430611372
W0320 22:05:58.355649 1531124 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 22:06:04.301452 1531070 finetune.py:68] layer 27_down @ epoch 2 new loss 0.004310600459575653 old loss 0.004311090800911188 BETTER
I0320 22:06:04.309609 1531016 finetune.py:68] layer 26_down @ epoch 4 new loss 0.003920710179954767 old loss 0.003921041265130043 BETTER
26_v proxy err 0.015377088449895382 tr(WHW.T) 1668.8843994140625
bpp_loss 3.354248285293579
26_q proxy err 0.0036852110642939806 tr(WHW.T) 7476.76806640625
bpp_loss 3.4957809448242188
26_k proxy err 0.0027130083180963993 tr(WHW.T) 10525.8251953125
bpp_loss 3.507898449897766
26_o proxy err 0.009294359013438225 tr(WHW.T) 204.00880432128906
bpp_loss 3.3456244468688965
26_up proxy err 0.012208441272377968 tr(WHW.T) 3157.94482421875
bpp_loss 3.2860485343045966
26_gate proxy err 0.007520223967730999 tr(WHW.T) 5305.29443359375
bpp_loss 3.38304812409157
26_down proxy err 0.01548005361109972 tr(WHW.T) 404.01715087890625
bpp_loss 3.2811035111893054
I0320 22:06:31.496734 1531070 finetune.py:68] layer 27_down @ epoch 3 new loss 0.004310163669288158 old loss 0.004310600459575653 BETTER
I0320 22:06:32.016758 1531124 finetune.py:68] layer 28_v @ epoch 0 new loss 0.00042691046837717295 old loss 0.0007790315430611372 BETTER
I0320 22:07:00.222627 1531070 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0043097976595163345 old loss 0.004310163669288158 BETTER
27_v proxy err 0.014004361815750599 tr(WHW.T) 1799.3350830078125
bpp_loss 3.356634020805359
27_q proxy err 0.003570046043023467 tr(WHW.T) 7693.4931640625
bpp_loss 3.5518025159835815
27_k proxy err 0.0026274488773196936 tr(WHW.T) 10654.88671875
bpp_loss 3.567033529281616
27_o proxy err 0.012566044926643372 tr(WHW.T) 126.64131164550781
bpp_loss 3.354691505432129
27_up proxy err 0.011175335384905338 tr(WHW.T) 3690.048095703125
bpp_loss 3.29243682151617
27_gate proxy err 0.007125186733901501 tr(WHW.T) 5983.2314453125
bpp_loss 3.3845724061478015
27_down proxy err 0.01584332250058651 tr(WHW.T) 470.25982666015625
bpp_loss 3.2879731377889945
I0320 22:07:06.882120 1531124 finetune.py:68] layer 28_v @ epoch 1 new loss 0.0004005380324088037 old loss 0.00042691046837717295 BETTER
I0320 22:07:18.209251 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 29 in 68.36405229568481s
I0320 22:07:21.683177 1531178 config.py:54] PyTorch version 2.6.0 available.
W0320 22:07:21.988566 1531178 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 22:07:22.912258 1531178 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 22:07:22.916231 1523378 quantize_finetune_llama.py:203] layer 30 gpu 2
I0320 22:07:22.930897 1531178 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 22:07:36.491528 1531178 finetune.py:45] layer 29_v initial loss 0.0007325169863179326
W0320 22:07:36.491735 1531178 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 22:07:43.959864 1531124 finetune.py:68] layer 28_v @ epoch 2 new loss 0.0003862048324663192 old loss 0.0004005380324088037 BETTER
I0320 22:08:08.729076 1531178 finetune.py:68] layer 29_v @ epoch 0 new loss 0.00046445129555650055 old loss 0.0007325169863179326 BETTER
I0320 22:08:20.578703 1531124 finetune.py:68] layer 28_v @ epoch 3 new loss 0.00037630650331266224 old loss 0.0003862048324663192 BETTER
I0320 22:08:36.268620 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 30 in 72.84718203544617s
I0320 22:08:40.613281 1531232 config.py:54] PyTorch version 2.6.0 available.
W0320 22:08:41.050544 1531232 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 22:08:42.180540 1531232 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 22:08:42.185433 1523378 quantize_finetune_llama.py:203] layer 31 gpu 3
I0320 22:08:42.212297 1531232 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 22:08:43.605364 1531178 finetune.py:68] layer 29_v @ epoch 1 new loss 0.0004388128290884197 old loss 0.00046445129555650055 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 22:08:57.507592 1531232 finetune.py:45] layer 30_v initial loss 0.0007488448754884303
W0320 22:08:57.508088 1531232 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 22:08:58.506149 1531124 finetune.py:68] layer 28_v @ epoch 4 new loss 0.0003690672747325152 old loss 0.00037630650331266224 BETTER
I0320 22:09:14.572992 1531124 finetune.py:45] layer 28_q initial loss 0.0004956264747306705
I0320 22:09:19.204461 1531178 finetune.py:68] layer 29_v @ epoch 2 new loss 0.0004238457477185875 old loss 0.0004388128290884197 BETTER
I0320 22:09:32.283481 1531232 finetune.py:68] layer 30_v @ epoch 0 new loss 0.0004292573721613735 old loss 0.0007488448754884303 BETTER
I0320 22:09:51.470986 1531124 finetune.py:68] layer 28_q @ epoch 0 new loss 0.0004641386913135648 old loss 0.0004956264747306705 BETTER
I0320 22:09:55.430158 1531178 finetune.py:68] layer 29_v @ epoch 3 new loss 0.0004140638338867575 old loss 0.0004238457477185875 BETTER
I0320 22:10:00.818125 1523378 quantize_finetune_llama.py:234] computed original embedding for layer 31 in 78.02615690231323s
I0320 22:10:05.324124 1531286 config.py:54] PyTorch version 2.6.0 available.
W0320 22:10:05.764019 1531286 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 22:10:07.204093 1531286 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 22:10:07.222645 1531286 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 22:10:08.111189 1531232 finetune.py:68] layer 30_v @ epoch 1 new loss 0.0004087518318556249 old loss 0.0004292573721613735 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 22:10:23.639316 1531286 finetune.py:45] layer 31_v initial loss 0.00155582744628191
W0320 22:10:23.640105 1531286 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 22:10:29.751615 1531124 finetune.py:68] layer 28_q @ epoch 1 new loss 0.0004528956487774849 old loss 0.0004641386913135648 BETTER
I0320 22:10:31.879137 1531178 finetune.py:68] layer 29_v @ epoch 4 new loss 0.000406099105020985 old loss 0.0004140638338867575 BETTER
I0320 22:10:44.927887 1531232 finetune.py:68] layer 30_v @ epoch 2 new loss 0.000397551862988621 old loss 0.0004087518318556249 BETTER
I0320 22:10:50.393891 1531178 finetune.py:45] layer 29_q initial loss 0.0005216876743361354
I0320 22:10:58.046222 1531286 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0006831527571193874 old loss 0.00155582744628191 BETTER
I0320 22:11:08.227684 1531124 finetune.py:68] layer 28_q @ epoch 2 new loss 0.0004447998071555048 old loss 0.0004528956487774849 BETTER
I0320 22:11:22.072819 1531232 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0003890454245265573 old loss 0.000397551862988621 BETTER
I0320 22:11:25.402091 1531178 finetune.py:68] layer 29_q @ epoch 0 new loss 0.0004929723218083382 old loss 0.0005216876743361354 BETTER
I0320 22:11:34.880459 1531286 finetune.py:68] layer 31_v @ epoch 1 new loss 0.0006345574511215091 old loss 0.0006831527571193874 BETTER
I0320 22:11:46.797896 1531124 finetune.py:68] layer 28_q @ epoch 3 new loss 0.00043859201832674444 old loss 0.0004447998071555048 BETTER
I0320 22:11:59.677624 1531232 finetune.py:68] layer 30_v @ epoch 4 new loss 0.00038265722105279565 old loss 0.0003890454245265573 BETTER
I0320 22:12:01.903982 1531178 finetune.py:68] layer 29_q @ epoch 1 new loss 0.00048215792048722506 old loss 0.0004929723218083382 BETTER
I0320 22:12:10.057523 1531286 finetune.py:68] layer 31_v @ epoch 2 new loss 0.0006186182145029306 old loss 0.0006345574511215091 BETTER
I0320 22:12:17.947562 1531232 finetune.py:45] layer 30_q initial loss 0.0005553704686462879
I0320 22:12:25.005152 1531124 finetune.py:68] layer 28_q @ epoch 4 new loss 0.0004334870900493115 old loss 0.00043859201832674444 BETTER
I0320 22:12:37.984401 1531178 finetune.py:68] layer 29_q @ epoch 2 new loss 0.00047475218889303505 old loss 0.00048215792048722506 BETTER
I0320 22:12:43.101306 1531124 finetune.py:45] layer 28_k initial loss 0.0005512017523869872
I0320 22:12:46.998054 1531286 finetune.py:68] layer 31_v @ epoch 3 new loss 0.0005970093188807368 old loss 0.0006186182145029306 BETTER
I0320 22:12:53.014691 1531232 finetune.py:68] layer 30_q @ epoch 0 new loss 0.0004971211310476065 old loss 0.0005553704686462879 BETTER
I0320 22:13:14.973232 1531178 finetune.py:68] layer 29_q @ epoch 3 new loss 0.00046861282316967845 old loss 0.00047475218889303505 BETTER
I0320 22:13:19.571068 1531124 finetune.py:68] layer 28_k @ epoch 0 new loss 0.0005353166488930583 old loss 0.0005512017523869872 BETTER
I0320 22:13:23.821757 1531286 finetune.py:68] layer 31_v @ epoch 4 new loss 0.0005779586499556899 old loss 0.0005970093188807368 BETTER
I0320 22:13:29.364359 1531232 finetune.py:68] layer 30_q @ epoch 1 new loss 0.0004855140869040042 old loss 0.0004971211310476065 BETTER
I0320 22:13:42.910907 1531286 finetune.py:45] layer 31_q initial loss 0.0012265898985788226
I0320 22:13:50.859807 1531178 finetune.py:68] layer 29_q @ epoch 4 new loss 0.00046370731433853507 old loss 0.00046861282316967845 BETTER
I0320 22:13:56.257236 1531124 finetune.py:68] layer 28_k @ epoch 1 new loss 0.0005307632964104414 old loss 0.0005353166488930583 BETTER
I0320 22:14:05.838136 1531232 finetune.py:68] layer 30_q @ epoch 2 new loss 0.0004768233047798276 old loss 0.0004855140869040042 BETTER
I0320 22:14:09.573859 1531178 finetune.py:45] layer 29_k initial loss 0.0005770411225967109
I0320 22:14:17.366150 1531286 finetune.py:68] layer 31_q @ epoch 0 new loss 0.0010026455856859684 old loss 0.0012265898985788226 BETTER
I0320 22:14:33.656111 1531124 finetune.py:68] layer 28_k @ epoch 2 new loss 0.0005273955757729709 old loss 0.0005307632964104414 BETTER
I0320 22:14:43.270730 1531232 finetune.py:68] layer 30_q @ epoch 3 new loss 0.0004701498837675899 old loss 0.0004768233047798276 BETTER
I0320 22:14:44.790426 1531178 finetune.py:68] layer 29_k @ epoch 0 new loss 0.0005618181312456727 old loss 0.0005770411225967109 BETTER
I0320 22:14:53.255595 1531286 finetune.py:68] layer 31_q @ epoch 1 new loss 0.0009649824351072311 old loss 0.0010026455856859684 BETTER
I0320 22:15:10.405038 1531124 finetune.py:68] layer 28_k @ epoch 3 new loss 0.0005243000341579318 old loss 0.0005273955757729709 BETTER
I0320 22:15:20.421472 1531232 finetune.py:68] layer 30_q @ epoch 4 new loss 0.0004660863196477294 old loss 0.0004701498837675899 BETTER
I0320 22:15:21.234104 1531178 finetune.py:68] layer 29_k @ epoch 1 new loss 0.0005569230415858328 old loss 0.0005618181312456727 BETTER
I0320 22:15:28.467753 1531286 finetune.py:68] layer 31_q @ epoch 2 new loss 0.0009350127074867487 old loss 0.0009649824351072311 BETTER
I0320 22:15:37.201819 1531232 finetune.py:45] layer 30_k initial loss 0.000612640636973083
I0320 22:15:46.968827 1531124 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0005217022844590247 old loss 0.0005243000341579318 BETTER
I0320 22:15:56.185759 1531178 finetune.py:68] layer 29_k @ epoch 2 new loss 0.0005531168426387012 old loss 0.0005569230415858328 BETTER
I0320 22:16:04.413302 1531286 finetune.py:68] layer 31_q @ epoch 3 new loss 0.000916124030482024 old loss 0.0009350127074867487 BETTER
I0320 22:16:06.025971 1531124 finetune.py:45] layer 28_o initial loss 0.0010583513649180532
I0320 22:16:12.482168 1531232 finetune.py:68] layer 30_k @ epoch 0 new loss 0.0005801672814413905 old loss 0.000612640636973083 BETTER
I0320 22:16:31.990025 1531178 finetune.py:68] layer 29_k @ epoch 3 new loss 0.0005498995305970311 old loss 0.0005531168426387012 BETTER
I0320 22:16:41.126227 1531286 finetune.py:68] layer 31_q @ epoch 4 new loss 0.000900499988347292 old loss 0.000916124030482024 BETTER
I0320 22:16:41.854848 1531124 finetune.py:68] layer 28_o @ epoch 0 new loss 0.0010089547140523791 old loss 0.0010583513649180532 BETTER
I0320 22:16:47.981688 1531232 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0005749420961365104 old loss 0.0005801672814413905 BETTER
I0320 22:16:58.693792 1531286 finetune.py:45] layer 31_k initial loss 0.0011518665123730898
I0320 22:17:07.047189 1531178 finetune.py:68] layer 29_k @ epoch 4 new loss 0.0005470716278068721 old loss 0.0005498995305970311 BETTER
I0320 22:17:17.616555 1531124 finetune.py:68] layer 28_o @ epoch 1 new loss 0.0009919247822836041 old loss 0.0010089547140523791 BETTER
I0320 22:17:24.203882 1531232 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0005727274110540748 old loss 0.0005749420961365104 BETTER
I0320 22:17:25.247935 1531178 finetune.py:45] layer 29_o initial loss 0.0010270847706124187
I0320 22:17:32.849697 1531286 finetune.py:68] layer 31_k @ epoch 0 new loss 0.001061689225025475 old loss 0.0011518665123730898 BETTER
I0320 22:17:55.658521 1531124 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0009800087427720428 old loss 0.0009919247822836041 BETTER
I0320 22:18:00.150708 1531178 finetune.py:68] layer 29_o @ epoch 0 new loss 0.0009808457689359784 old loss 0.0010270847706124187 BETTER
I0320 22:18:01.423594 1531232 finetune.py:68] layer 30_k @ epoch 3 new loss 0.0005689251702278852 old loss 0.0005727274110540748 BETTER
I0320 22:18:07.922488 1531286 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0010410579852759838 old loss 0.001061689225025475 BETTER
I0320 22:18:33.708645 1531124 finetune.py:68] layer 28_o @ epoch 3 new loss 0.0009711861493997276 old loss 0.0009800087427720428 BETTER
I0320 22:18:36.857132 1531178 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0009677054476924241 old loss 0.0009808457689359784 BETTER
I0320 22:18:38.993133 1531232 finetune.py:68] layer 30_k @ epoch 4 new loss 0.0005661437171511352 old loss 0.0005689251702278852 BETTER
I0320 22:18:43.985492 1531286 finetune.py:68] layer 31_k @ epoch 2 new loss 0.0010282022412866354 old loss 0.0010410579852759838 BETTER
I0320 22:18:57.471490 1531232 finetune.py:45] layer 30_o initial loss 0.001201106351800263
I0320 22:19:10.879540 1531124 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0009638779447413981 old loss 0.0009711861493997276 BETTER
I0320 22:19:12.596087 1531178 finetune.py:68] layer 29_o @ epoch 2 new loss 0.0009590613190084696 old loss 0.0009677054476924241 BETTER
I0320 22:19:19.173534 1531286 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0010201665572822094 old loss 0.0010282022412866354 BETTER
I0320 22:19:31.520444 1531232 finetune.py:68] layer 30_o @ epoch 0 new loss 0.0011051996843889356 old loss 0.001201106351800263 BETTER
I0320 22:19:40.044377 1531124 finetune.py:45] layer 28_up initial loss 0.0022436787839978933
I0320 22:19:48.127463 1531178 finetune.py:68] layer 29_o @ epoch 3 new loss 0.0009523897897452116 old loss 0.0009590613190084696 BETTER
I0320 22:19:54.568422 1531286 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0010133491596207023 old loss 0.0010201665572822094 BETTER
I0320 22:20:07.231414 1531232 finetune.py:68] layer 30_o @ epoch 1 new loss 0.001078113098628819 old loss 0.0011051996843889356 BETTER
I0320 22:20:12.949563 1531286 finetune.py:45] layer 31_o initial loss 0.002071392722427845
I0320 22:20:14.068092 1531124 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0021845505107194185 old loss 0.0022436787839978933 BETTER
I0320 22:20:24.471096 1531178 finetune.py:68] layer 29_o @ epoch 4 new loss 0.0009474835242144763 old loss 0.0009523897897452116 BETTER
I0320 22:20:42.993736 1531232 finetune.py:68] layer 30_o @ epoch 2 new loss 0.001061518327333033 old loss 0.001078113098628819 BETTER
I0320 22:20:46.457816 1531286 finetune.py:68] layer 31_o @ epoch 0 new loss 0.001734702498652041 old loss 0.002071392722427845 BETTER
I0320 22:20:48.750126 1531124 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0021586227230727673 old loss 0.0021845505107194185 BETTER
I0320 22:20:55.766777 1531178 finetune.py:45] layer 29_up initial loss 0.0024663424119353294
I0320 22:21:20.017579 1531232 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0010491872671991587 old loss 0.001061518327333033 BETTER
I0320 22:21:22.477541 1531286 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0016573388129472733 old loss 0.001734702498652041 BETTER
I0320 22:21:24.988015 1531124 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0021391590125858784 old loss 0.0021586227230727673 BETTER
I0320 22:21:27.918221 1531178 finetune.py:68] layer 29_up @ epoch 0 new loss 0.00237547280266881 old loss 0.0024663424119353294 BETTER
I0320 22:21:57.628768 1531232 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0010397587902843952 old loss 0.0010491872671991587 BETTER
I0320 22:21:59.412087 1531286 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0016132005257532 old loss 0.0016573388129472733 BETTER
I0320 22:22:01.458558 1531124 finetune.py:68] layer 28_up @ epoch 3 new loss 0.0021244732197374105 old loss 0.0021391590125858784 BETTER
I0320 22:22:02.207388 1531178 finetune.py:68] layer 29_up @ epoch 1 new loss 0.0023416487965732813 old loss 0.00237547280266881 BETTER
I0320 22:22:26.786692 1531232 finetune.py:45] layer 30_up initial loss 0.0036557200364768505
I0320 22:22:35.166789 1531286 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0015845362795516849 old loss 0.0016132005257532 BETTER
I0320 22:22:36.563258 1531178 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0023180621210485697 old loss 0.0023416487965732813 BETTER
I0320 22:22:37.612636 1531124 finetune.py:68] layer 28_up @ epoch 4 new loss 0.0021120004821568727 old loss 0.0021244732197374105 BETTER
I0320 22:22:59.032554 1531232 finetune.py:68] layer 30_up @ epoch 0 new loss 0.003339136950671673 old loss 0.0036557200364768505 BETTER
I0320 22:23:07.967959 1531124 finetune.py:45] layer 28_gate initial loss 0.0032163222786039114
I0320 22:23:11.551587 1531178 finetune.py:68] layer 29_up @ epoch 3 new loss 0.002299671992659569 old loss 0.0023180621210485697 BETTER
I0320 22:23:11.840049 1531286 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0015658466145396233 old loss 0.0015845362795516849 BETTER
I0320 22:23:32.874973 1531232 finetune.py:68] layer 30_up @ epoch 1 new loss 0.003236542921513319 old loss 0.003339136950671673 BETTER
I0320 22:23:39.775841 1531124 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0031907414086163044 old loss 0.0032163222786039114 BETTER
I0320 22:23:42.529470 1531286 finetune.py:45] layer 31_up initial loss 0.00867877621203661
I0320 22:23:46.350190 1531178 finetune.py:68] layer 29_up @ epoch 4 new loss 0.002284982707351446 old loss 0.002299671992659569 BETTER
I0320 22:24:07.265694 1531232 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0031654774211347103 old loss 0.003236542921513319 BETTER
I0320 22:24:13.716157 1531124 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0031754656229168177 old loss 0.0031907414086163044 BETTER
I0320 22:24:15.492141 1531286 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0068045794032514095 old loss 0.00867877621203661 BETTER
I0320 22:24:18.585338 1531178 finetune.py:45] layer 29_gate initial loss 0.0036042004358023405
I0320 22:24:42.149944 1531232 finetune.py:68] layer 30_up @ epoch 3 new loss 0.003109757322818041 old loss 0.0031654774211347103 BETTER
I0320 22:24:48.846657 1531124 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.0031631283927708864 old loss 0.0031754656229168177 BETTER
I0320 22:24:49.854673 1531286 finetune.py:68] layer 31_up @ epoch 1 new loss 0.006407774519175291 old loss 0.0068045794032514095 BETTER
I0320 22:24:49.953387 1531178 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0035665854811668396 old loss 0.0036042004358023405 BETTER
I0320 22:25:17.894961 1531232 finetune.py:68] layer 30_up @ epoch 4 new loss 0.003066344652324915 old loss 0.003109757322818041 BETTER
I0320 22:25:22.769760 1531178 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.003546355990692973 old loss 0.0035665854811668396 BETTER
I0320 22:25:23.921968 1531124 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0031528754625469446 old loss 0.0031631283927708864 BETTER
I0320 22:25:23.926197 1531286 finetune.py:68] layer 31_up @ epoch 2 new loss 0.006157648749649525 old loss 0.006407774519175291 BETTER
I0320 22:25:48.172178 1531232 finetune.py:45] layer 30_gate initial loss 0.004917898681014776
I0320 22:25:55.830873 1531178 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0035302150063216686 old loss 0.003546355990692973 BETTER
I0320 22:25:58.181723 1531124 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.0031439736485481262 old loss 0.0031528754625469446 BETTER
I0320 22:25:58.560007 1531286 finetune.py:68] layer 31_up @ epoch 3 new loss 0.005967048462480307 old loss 0.006157648749649525 BETTER
I0320 22:26:18.669761 1531232 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.004796862602233887 old loss 0.004917898681014776 BETTER
I0320 22:26:28.670571 1531178 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.003516911529004574 old loss 0.0035302150063216686 BETTER
I0320 22:26:31.132349 1531124 finetune.py:45] layer 28_down initial loss 0.005415861960500479
I0320 22:26:32.121879 1531286 finetune.py:68] layer 31_up @ epoch 4 new loss 0.005811822135001421 old loss 0.005967048462480307 BETTER
I0320 22:26:50.838649 1531232 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.004724372178316116 old loss 0.004796862602233887 BETTER
I0320 22:27:01.878051 1531124 finetune.py:68] layer 28_down @ epoch 0 new loss 0.00541478069499135 old loss 0.005415861960500479 BETTER
I0320 22:27:02.080234 1531178 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0035058059729635715 old loss 0.003516911529004574 BETTER
I0320 22:27:03.827198 1531286 finetune.py:45] layer 31_gate initial loss 0.009129781275987625
I0320 22:27:22.924805 1531232 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.004672946408390999 old loss 0.004724372178316116 BETTER
I0320 22:27:34.150182 1531124 finetune.py:68] layer 28_down @ epoch 1 new loss 0.005413902457803488 old loss 0.00541478069499135 BETTER
I0320 22:27:34.866682 1531286 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.008425849489867687 old loss 0.009129781275987625 BETTER
I0320 22:27:35.790168 1531178 finetune.py:45] layer 29_down initial loss 0.0064779301173985004
I0320 22:27:55.754164 1531232 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.004631934687495232 old loss 0.004672946408390999 BETTER
I0320 22:28:06.387393 1531178 finetune.py:68] layer 29_down @ epoch 0 new loss 0.006476734299212694 old loss 0.0064779301173985004 BETTER
I0320 22:28:07.690601 1531124 finetune.py:68] layer 28_down @ epoch 2 new loss 0.005413183942437172 old loss 0.005413902457803488 BETTER
I0320 22:28:08.025548 1531286 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.008199331350624561 old loss 0.008425849489867687 BETTER
I0320 22:28:29.068916 1531232 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.004596256185323 old loss 0.004631934687495232 BETTER
I0320 22:28:36.988907 1531178 finetune.py:68] layer 29_down @ epoch 1 new loss 0.006475712172687054 old loss 0.006476734299212694 BETTER
I0320 22:28:40.169379 1531286 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.008054211735725403 old loss 0.008199331350624561 BETTER
I0320 22:28:40.229922 1531124 finetune.py:68] layer 28_down @ epoch 3 new loss 0.0054125408641994 old loss 0.005413183942437172 BETTER
I0320 22:29:04.120998 1531232 finetune.py:45] layer 30_down initial loss 0.012806873768568039
I0320 22:29:09.151163 1531178 finetune.py:68] layer 29_down @ epoch 2 new loss 0.006474788300693035 old loss 0.006475712172687054 BETTER
I0320 22:29:13.331734 1531124 finetune.py:68] layer 28_down @ epoch 4 new loss 0.005411997437477112 old loss 0.0054125408641994 BETTER
I0320 22:29:13.575142 1531286 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.007945331744849682 old loss 0.008054211735725403 BETTER
28_v proxy err 0.01306244544684887 tr(WHW.T) 2018.944091796875
bpp_loss 3.399941921234131
28_q proxy err 0.0036863337736576796 tr(WHW.T) 7653.681640625
bpp_loss 3.5078402757644653
28_k proxy err 0.0027308242861181498 tr(WHW.T) 10589.5322265625
bpp_loss 3.52616024017334
28_o proxy err 0.01039511151611805 tr(WHW.T) 195.83656311035156
bpp_loss 3.3944698572158813
28_up proxy err 0.00939408503472805 tr(WHW.T) 4659.9912109375
bpp_loss 3.305092745049055
28_gate proxy err 0.006874024402350187 tr(WHW.T) 6534.33154296875
bpp_loss 3.37861846214117
28_down proxy err 0.015931036323308945 tr(WHW.T) 609.1082763671875
bpp_loss 3.295773461807606
I0320 22:29:32.118200 1531232 finetune.py:68] layer 30_down @ epoch 0 new loss 0.012781823053956032 old loss 0.012806873768568039 BETTER
I0320 22:29:39.861568 1531178 finetune.py:68] layer 29_down @ epoch 3 new loss 0.00647398829460144 old loss 0.006474788300693035 BETTER
I0320 22:29:44.664218 1531286 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.007857452146708965 old loss 0.007945331744849682 BETTER
I0320 22:30:01.142011 1531232 finetune.py:68] layer 30_down @ epoch 1 new loss 0.012757287360727787 old loss 0.012781823053956032 BETTER
I0320 22:30:09.322754 1531178 finetune.py:68] layer 29_down @ epoch 4 new loss 0.006473270710557699 old loss 0.00647398829460144 BETTER
29_v proxy err 0.013823729939758778 tr(WHW.T) 1801.7730712890625
bpp_loss 3.405286192893982
29_q proxy err 0.003654665779322386 tr(WHW.T) 7232.69091796875
bpp_loss 3.46847927570343
29_k proxy err 0.0025577933993190527 tr(WHW.T) 10615.0771484375
bpp_loss 3.481760025024414
29_o proxy err 0.008814745582640171 tr(WHW.T) 208.50222778320312
bpp_loss 3.410703659057617
29_up proxy err 0.007553760893642902 tr(WHW.T) 6067.302734375
bpp_loss 3.3172272083371186
29_gate proxy err 0.006350033916532993 tr(WHW.T) 7363.125
bpp_loss 3.38234142924464
29_down proxy err 0.016168639063835144 tr(WHW.T) 790.4268798828125
bpp_loss 3.300346019656159
I0320 22:30:14.479144 1531286 finetune.py:45] layer 31_down initial loss 0.036083295941352844
I0320 22:30:29.224855 1531232 finetune.py:68] layer 30_down @ epoch 2 new loss 0.012736296281218529 old loss 0.012757287360727787 BETTER
I0320 22:30:42.340268 1531286 finetune.py:68] layer 31_down @ epoch 0 new loss 0.03596781939268112 old loss 0.036083295941352844 BETTER
I0320 22:30:59.053806 1531232 finetune.py:68] layer 30_down @ epoch 3 new loss 0.012715429998934269 old loss 0.012736296281218529 BETTER
I0320 22:31:09.547162 1531286 finetune.py:68] layer 31_down @ epoch 1 new loss 0.03587211295962334 old loss 0.03596781939268112 BETTER
I0320 22:31:28.842742 1531232 finetune.py:68] layer 30_down @ epoch 4 new loss 0.012695877812802792 old loss 0.012715429998934269 BETTER
30_v proxy err 0.011421695351600647 tr(WHW.T) 2261.489501953125
bpp_loss 3.4384795427322388
30_q proxy err 0.003480846993625164 tr(WHW.T) 7818.6298828125
bpp_loss 3.478130578994751
30_k proxy err 0.002636868739500642 tr(WHW.T) 10572.0029296875
bpp_loss 3.498512387275696
30_o proxy err 0.009246548637747765 tr(WHW.T) 252.78536987304688
bpp_loss 3.446340322494507
30_up proxy err 0.004732345696538687 tr(WHW.T) 10006.55078125
bpp_loss 3.3391003276026527
30_gate proxy err 0.004395114723592997 tr(WHW.T) 10982.9453125
bpp_loss 3.414017965627271
30_down proxy err 0.009455841965973377 tr(WHW.T) 3622.287353515625
bpp_loss 3.2924656757088595
I0320 22:31:36.822348 1531286 finetune.py:68] layer 31_down @ epoch 2 new loss 0.035793378949165344 old loss 0.03587211295962334 BETTER
I0320 22:32:04.311600 1531286 finetune.py:68] layer 31_down @ epoch 3 new loss 0.03572941571474075 old loss 0.035793378949165344 BETTER
I0320 22:32:31.817957 1531286 finetune.py:68] layer 31_down @ epoch 4 new loss 0.035677194595336914 old loss 0.03572941571474075 BETTER
31_v proxy err 0.013428391888737679 tr(WHW.T) 1268.2034912109375
bpp_loss 3.3106155395507812
31_q proxy err 0.0027624957729130983 tr(WHW.T) 6859.54248046875
bpp_loss 3.495511531829834
31_k proxy err 0.0019264009315520525 tr(WHW.T) 10286.7998046875
bpp_loss 3.550523281097412
31_o proxy err 0.007039461750537157 tr(WHW.T) 460.1990051269531
bpp_loss 3.324147343635559
31_up proxy err 0.0027752607129514217 tr(WHW.T) 14576.1064453125
bpp_loss 3.391079924827398
31_gate proxy err 0.0027742371894419193 tr(WHW.T) 14851.8212890625
bpp_loss 3.4757170566292697
31_down proxy err 0.006597483064979315 tr(WHW.T) 18228.59375
bpp_loss 3.291456466497377
I0320 22:33:02.250532 1531340 config.py:54] PyTorch version 2.6.0 available.
W0320 22:33:02.555805 1531340 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0320 22:33:02.809880 1531340 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  2.72it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:01,  3.37it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  3.80it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:01<00:00,  4.06it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  4.12it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  3.91it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  3.81it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.10it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.42it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.98it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.27it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.43it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.35it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.08it/s]
I0320 22:33:06.601148 1531340 hfize_llama.py:153] loaded layer 0
I0320 22:33:07.817733 1531340 hfize_llama.py:153] loaded layer 1
I0320 22:33:09.025169 1531340 hfize_llama.py:153] loaded layer 2
I0320 22:33:10.254058 1531340 hfize_llama.py:153] loaded layer 3
I0320 22:33:11.445795 1531340 hfize_llama.py:153] loaded layer 4
I0320 22:33:12.593013 1531340 hfize_llama.py:153] loaded layer 5
I0320 22:33:13.721480 1531340 hfize_llama.py:153] loaded layer 6
I0320 22:33:14.817828 1531340 hfize_llama.py:153] loaded layer 7
I0320 22:33:15.941893 1531340 hfize_llama.py:153] loaded layer 8
I0320 22:33:17.308697 1531340 hfize_llama.py:153] loaded layer 9
I0320 22:33:18.615678 1531340 hfize_llama.py:153] loaded layer 10
I0320 22:33:19.892251 1531340 hfize_llama.py:153] loaded layer 11
I0320 22:33:21.214591 1531340 hfize_llama.py:153] loaded layer 12
I0320 22:33:22.536514 1531340 hfize_llama.py:153] loaded layer 13
I0320 22:33:24.004860 1531340 hfize_llama.py:153] loaded layer 14
I0320 22:33:25.248878 1531340 hfize_llama.py:153] loaded layer 15
I0320 22:33:26.426540 1531340 hfize_llama.py:153] loaded layer 16
I0320 22:33:27.619636 1531340 hfize_llama.py:153] loaded layer 17
I0320 22:33:28.797690 1531340 hfize_llama.py:153] loaded layer 18
I0320 22:33:30.032785 1531340 hfize_llama.py:153] loaded layer 19
I0320 22:33:31.207528 1531340 hfize_llama.py:153] loaded layer 20
I0320 22:33:32.376883 1531340 hfize_llama.py:153] loaded layer 21
I0320 22:33:33.534152 1531340 hfize_llama.py:153] loaded layer 22
I0320 22:33:34.721027 1531340 hfize_llama.py:153] loaded layer 23
I0320 22:33:35.870806 1531340 hfize_llama.py:153] loaded layer 24
I0320 22:33:37.024773 1531340 hfize_llama.py:153] loaded layer 25
I0320 22:33:38.194252 1531340 hfize_llama.py:153] loaded layer 26
I0320 22:33:39.343995 1531340 hfize_llama.py:153] loaded layer 27
I0320 22:33:40.492981 1531340 hfize_llama.py:153] loaded layer 28
I0320 22:33:41.636354 1531340 hfize_llama.py:153] loaded layer 29
I0320 22:33:42.859542 1531340 hfize_llama.py:153] loaded layer 30
I0320 22:33:44.039595 1531340 hfize_llama.py:153] loaded layer 31
I0320 22:33:44.039706 1531340 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.19s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:03,  1.00it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.12it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.13it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.17it/s]
I0320 22:34:37.460976 1531340 hfize_llama.py:167] successfully loaded hfized model
