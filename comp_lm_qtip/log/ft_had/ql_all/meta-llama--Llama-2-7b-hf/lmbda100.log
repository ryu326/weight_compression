I0320 22:39:15.175441 1531629 config.py:54] PyTorch version 2.6.0 available.
W0320 22:39:15.455647 1531629 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 22:39:16.417644 1531629 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.59it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.24it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.65it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.91it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.03it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.20it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.95it/s]
I0320 22:39:17.376279 1531629 quantize_finetune_llama.py:144] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:14,  2.07it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:14,  2.09it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:13,  2.11it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:13,  2.11it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:12,  2.22it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:11,  2.31it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:10,  2.40it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:09,  2.46it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:11,  1.97it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:13,  1.59it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:14,  1.41it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:15,  1.30it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:15,  1.24it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:08<00:15,  1.20it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:09<00:14,  1.17it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:10<00:13,  1.17it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:11<00:12,  1.20it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:12<00:11,  1.22it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:12<00:10,  1.24it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:13<00:09,  1.25it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:14<00:08,  1.26it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:07,  1.26it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:15<00:07,  1.26it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:16<00:06,  1.27it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:05,  1.27it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.27it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:19<00:03,  1.27it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:19<00:03,  1.27it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:02,  1.28it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.28it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:22<00:00,  1.28it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.17it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.38it/s]
I0320 22:39:49.168003 1531629 quantize_finetune_llama.py:179] loaded compression model
I0320 22:40:03.566335 1531629 quantize_finetune_llama.py:183] loaded dataset and devset
I0320 22:40:08.983966 1531629 quantize_finetune_llama.py:203] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 22:41:24.799351 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 0 in 75.67437815666199s
tensor(0.0192) tensor(-3.6338e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0320 22:41:48.633434 1531744 config.py:54] PyTorch version 2.6.0 available.
W0320 22:41:48.916000 1531744 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 22:41:49.800464 1531744 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 22:41:49.804437 1531629 quantize_finetune_llama.py:203] layer 1 gpu 1
I0320 22:41:49.817480 1531744 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 22:42:01.364791 1531744 finetune.py:45] layer 0_v initial loss 5.642715223075356e-06
W0320 22:42:01.364994 1531744 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 22:42:35.164848 1531744 finetune.py:68] layer 0_v @ epoch 0 new loss 1.56589851485478e-06 old loss 5.642715223075356e-06 BETTER
I0320 22:43:02.739803 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 1 in 72.7710485458374s
I0320 22:43:12.689095 1531744 finetune.py:68] layer 0_v @ epoch 1 new loss 6.413553137463168e-07 old loss 1.56589851485478e-06 BETTER
I0320 22:43:13.918944 1531800 config.py:54] PyTorch version 2.6.0 available.
W0320 22:43:14.254446 1531800 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 22:43:15.230355 1531800 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 22:43:15.234870 1531629 quantize_finetune_llama.py:203] layer 2 gpu 2
I0320 22:43:15.248410 1531800 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 22:43:27.901433 1531800 finetune.py:45] layer 1_v initial loss 0.00016559541109018028
W0320 22:43:27.901735 1531800 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 22:43:48.239267 1531744 finetune.py:68] layer 0_v @ epoch 2 new loss 3.8777997701799904e-07 old loss 6.413553137463168e-07 BETTER
I0320 22:44:00.701512 1531800 finetune.py:68] layer 1_v @ epoch 0 new loss 3.421835208428092e-05 old loss 0.00016559541109018028 BETTER
I0320 22:44:24.012362 1531744 finetune.py:68] layer 0_v @ epoch 3 new loss 3.0627819569417625e-07 old loss 3.8777997701799904e-07 BETTER
I0320 22:44:33.841973 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 2 in 78.39050054550171s
I0320 22:44:36.109221 1531800 finetune.py:68] layer 1_v @ epoch 1 new loss 1.97485296666855e-05 old loss 3.421835208428092e-05 BETTER
I0320 22:44:46.745654 1531856 config.py:54] PyTorch version 2.6.0 available.
W0320 22:44:47.101756 1531856 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 22:44:48.143309 1531856 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 22:44:48.147602 1531629 quantize_finetune_llama.py:203] layer 3 gpu 3
I0320 22:44:48.184388 1531856 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 22:45:02.907981 1531744 finetune.py:68] layer 0_v @ epoch 4 new loss 2.698260459510493e-07 old loss 3.0627819569417625e-07 BETTER
I0320 22:45:03.376367 1531856 finetune.py:45] layer 2_v initial loss 9.555295036989264e-06
W0320 22:45:03.376701 1531856 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 22:45:13.331138 1531800 finetune.py:68] layer 1_v @ epoch 2 new loss 1.2604818039108068e-05 old loss 1.97485296666855e-05 BETTER
I0320 22:45:18.704774 1531744 finetune.py:45] layer 0_q initial loss 2.9135026125004515e-07
I0320 22:45:37.655959 1531856 finetune.py:68] layer 2_v @ epoch 0 new loss 7.005811312410515e-06 old loss 9.555295036989264e-06 BETTER
I0320 22:45:49.483504 1531800 finetune.py:68] layer 1_v @ epoch 3 new loss 8.944889486883767e-06 old loss 1.2604818039108068e-05 BETTER
I0320 22:45:53.778089 1531744 finetune.py:68] layer 0_q @ epoch 0 new loss 2.4520369379388285e-07 old loss 2.9135026125004515e-07 BETTER
I0320 22:46:10.561644 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 3 in 82.1770122051239s
I0320 22:46:17.367712 1531856 finetune.py:68] layer 2_v @ epoch 1 new loss 5.6713365665927995e-06 old loss 7.005811312410515e-06 BETTER
I0320 22:46:24.574009 1531912 config.py:54] PyTorch version 2.6.0 available.
W0320 22:46:25.110322 1531912 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 22:46:26.514460 1531912 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 22:46:26.519285 1531629 quantize_finetune_llama.py:203] layer 4 gpu 0
I0320 22:46:26.552314 1531912 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 22:46:27.223767 1531800 finetune.py:68] layer 1_v @ epoch 4 new loss 7.452909358107718e-06 old loss 8.944889486883767e-06 BETTER
I0320 22:46:31.627738 1531744 finetune.py:68] layer 0_q @ epoch 1 new loss 2.2411140321310086e-07 old loss 2.4520369379388285e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 22:46:42.678105 1531912 finetune.py:45] layer 3_v initial loss 1.3875282093067653e-05
W0320 22:46:42.678328 1531912 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 22:46:45.448098 1531800 finetune.py:45] layer 1_q initial loss 8.677184268890414e-06
I0320 22:46:52.960376 1531856 finetune.py:68] layer 2_v @ epoch 2 new loss 4.896733571513323e-06 old loss 5.6713365665927995e-06 BETTER
I0320 22:47:08.188402 1531744 finetune.py:68] layer 0_q @ epoch 2 new loss 2.082401096004105e-07 old loss 2.2411140321310086e-07 BETTER
I0320 22:47:17.450863 1531912 finetune.py:68] layer 3_v @ epoch 0 new loss 8.818832611723337e-06 old loss 1.3875282093067653e-05 BETTER
I0320 22:47:20.599505 1531800 finetune.py:68] layer 1_q @ epoch 0 new loss 6.795140052417992e-06 old loss 8.677184268890414e-06 BETTER
I0320 22:47:29.005651 1531856 finetune.py:68] layer 2_v @ epoch 3 new loss 4.4018670450896025e-06 old loss 4.896733571513323e-06 BETTER
I0320 22:47:44.951776 1531744 finetune.py:68] layer 0_q @ epoch 3 new loss 1.9511271887040493e-07 old loss 2.082401096004105e-07 BETTER
I0320 22:47:53.122402 1531912 finetune.py:68] layer 3_v @ epoch 1 new loss 7.104705218807794e-06 old loss 8.818832611723337e-06 BETTER
I0320 22:47:56.199986 1531800 finetune.py:68] layer 1_q @ epoch 1 new loss 5.1924967010563705e-06 old loss 6.795140052417992e-06 BETTER
I0320 22:48:05.583140 1531856 finetune.py:68] layer 2_v @ epoch 4 new loss 4.063653250341304e-06 old loss 4.4018670450896025e-06 BETTER
I0320 22:48:22.271903 1531744 finetune.py:68] layer 0_q @ epoch 4 new loss 1.8312202598735894e-07 old loss 1.9511271887040493e-07 BETTER
I0320 22:48:23.010043 1531856 finetune.py:45] layer 2_q initial loss 4.53200618721894e-06
I0320 22:48:28.777504 1531912 finetune.py:68] layer 3_v @ epoch 2 new loss 6.331258646241622e-06 old loss 7.104705218807794e-06 BETTER
I0320 22:48:31.749599 1531800 finetune.py:68] layer 1_q @ epoch 2 new loss 5.138015694683418e-06 old loss 5.1924967010563705e-06 BETTER
I0320 22:48:40.543109 1531744 finetune.py:45] layer 0_k initial loss 2.0151038881977001e-07
I0320 22:48:57.673993 1531856 finetune.py:68] layer 2_q @ epoch 0 new loss 4.009983058494981e-06 old loss 4.53200618721894e-06 BETTER
I0320 22:49:04.937462 1531912 finetune.py:68] layer 3_v @ epoch 3 new loss 5.871877419849625e-06 old loss 6.331258646241622e-06 BETTER
I0320 22:49:08.058806 1531800 finetune.py:68] layer 1_q @ epoch 3 new loss 3.987705895269755e-06 old loss 5.138015694683418e-06 BETTER
I0320 22:49:16.644320 1531744 finetune.py:68] layer 0_k @ epoch 0 new loss 1.8000409340857004e-07 old loss 2.0151038881977001e-07 BETTER
I0320 22:49:33.590184 1531856 finetune.py:68] layer 2_q @ epoch 1 new loss 3.7766296827612678e-06 old loss 4.009983058494981e-06 BETTER
I0320 22:49:41.502047 1531912 finetune.py:68] layer 3_v @ epoch 4 new loss 5.542554390558507e-06 old loss 5.871877419849625e-06 BETTER
I0320 22:49:44.207466 1531800 finetune.py:76] layer 1_q @ epoch 4 new loss 4.477049969864311e-06 old loss 3.987705895269755e-06 WORSE
I0320 22:49:52.828376 1531744 finetune.py:68] layer 0_k @ epoch 1 new loss 1.6984560602395504e-07 old loss 1.8000409340857004e-07 BETTER
I0320 22:49:59.461650 1531912 finetune.py:45] layer 3_q initial loss 6.714051778544672e-06
I0320 22:50:01.819344 1531800 finetune.py:45] layer 1_k initial loss 4.312523742555641e-06
I0320 22:50:09.797369 1531856 finetune.py:68] layer 2_q @ epoch 2 new loss 3.599596311687492e-06 old loss 3.7766296827612678e-06 BETTER
I0320 22:50:29.539980 1531744 finetune.py:68] layer 0_k @ epoch 2 new loss 1.6184327478185878e-07 old loss 1.6984560602395504e-07 BETTER
I0320 22:50:34.252272 1531912 finetune.py:68] layer 3_q @ epoch 0 new loss 5.969426638330333e-06 old loss 6.714051778544672e-06 BETTER
I0320 22:50:36.401925 1531800 finetune.py:76] layer 1_k @ epoch 0 new loss 5.851408786838874e-06 old loss 4.312523742555641e-06 WORSE
I0320 22:50:45.422933 1531856 finetune.py:68] layer 2_q @ epoch 3 new loss 3.454327952567837e-06 old loss 3.599596311687492e-06 BETTER
I0320 22:51:07.248610 1531744 finetune.py:68] layer 0_k @ epoch 3 new loss 1.5536124919890426e-07 old loss 1.6184327478185878e-07 BETTER
I0320 22:51:10.717399 1531912 finetune.py:68] layer 3_q @ epoch 1 new loss 5.683371000486659e-06 old loss 5.969426638330333e-06 BETTER
I0320 22:51:11.717783 1531800 finetune.py:68] layer 1_k @ epoch 1 new loss 4.270393219485413e-06 old loss 4.312523742555641e-06 BETTER
I0320 22:51:21.657837 1531856 finetune.py:68] layer 2_q @ epoch 4 new loss 3.3309190712316195e-06 old loss 3.454327952567837e-06 BETTER
I0320 22:51:38.708697 1531856 finetune.py:45] layer 2_k initial loss 3.759662149604992e-06
I0320 22:51:46.114033 1531744 finetune.py:68] layer 0_k @ epoch 4 new loss 1.4880330923006113e-07 old loss 1.5536124919890426e-07 BETTER
I0320 22:51:47.029293 1531912 finetune.py:68] layer 3_q @ epoch 2 new loss 5.466120910568861e-06 old loss 5.683371000486659e-06 BETTER
I0320 22:51:47.760674 1531800 finetune.py:68] layer 1_k @ epoch 2 new loss 3.6319986520538805e-06 old loss 4.270393219485413e-06 BETTER
I0320 22:52:02.203101 1531744 finetune.py:45] layer 0_o initial loss 1.0425452501294785e-06
I0320 22:52:12.857074 1531856 finetune.py:68] layer 2_k @ epoch 0 new loss 3.5668124382937094e-06 old loss 3.759662149604992e-06 BETTER
I0320 22:52:22.557561 1531912 finetune.py:68] layer 3_q @ epoch 3 new loss 5.2896175475325435e-06 old loss 5.466120910568861e-06 BETTER
I0320 22:52:24.057700 1531800 finetune.py:76] layer 1_k @ epoch 3 new loss 3.6889773582515772e-06 old loss 3.6319986520538805e-06 WORSE
I0320 22:52:37.459934 1531744 finetune.py:68] layer 0_o @ epoch 0 new loss 9.13669680357998e-07 old loss 1.0425452501294785e-06 BETTER
I0320 22:52:48.365632 1531856 finetune.py:68] layer 2_k @ epoch 1 new loss 3.446977643761784e-06 old loss 3.5668124382937094e-06 BETTER
I0320 22:52:58.029240 1531912 finetune.py:68] layer 3_q @ epoch 4 new loss 5.143688667885726e-06 old loss 5.2896175475325435e-06 BETTER
I0320 22:52:58.548201 1531800 finetune.py:76] layer 1_k @ epoch 4 new loss 3.797437784669455e-06 old loss 3.6319986520538805e-06 WORSE
I0320 22:53:14.744494 1531744 finetune.py:68] layer 0_o @ epoch 1 new loss 8.248395602095115e-07 old loss 9.13669680357998e-07 BETTER
I0320 22:53:15.248968 1531800 finetune.py:45] layer 1_o initial loss 3.185722016496584e-05
I0320 22:53:16.006808 1531912 finetune.py:45] layer 3_k initial loss 6.058367034711409e-06
I0320 22:53:24.084302 1531856 finetune.py:68] layer 2_k @ epoch 2 new loss 3.3501501093269326e-06 old loss 3.446977643761784e-06 BETTER
I0320 22:53:50.224006 1531800 finetune.py:68] layer 1_o @ epoch 0 new loss 9.236312507709954e-06 old loss 3.185722016496584e-05 BETTER
I0320 22:53:51.241714 1531912 finetune.py:68] layer 3_k @ epoch 0 new loss 5.754966878157575e-06 old loss 6.058367034711409e-06 BETTER
I0320 22:53:52.238749 1531744 finetune.py:68] layer 0_o @ epoch 2 new loss 7.585153980471659e-07 old loss 8.248395602095115e-07 BETTER
I0320 22:53:59.391767 1531856 finetune.py:68] layer 2_k @ epoch 3 new loss 3.266146677560755e-06 old loss 3.3501501093269326e-06 BETTER
I0320 22:54:26.304276 1531800 finetune.py:68] layer 1_o @ epoch 1 new loss 8.31682700663805e-06 old loss 9.236312507709954e-06 BETTER
I0320 22:54:27.407335 1531912 finetune.py:68] layer 3_k @ epoch 1 new loss 5.61090155315469e-06 old loss 5.754966878157575e-06 BETTER
I0320 22:54:29.200721 1531744 finetune.py:68] layer 0_o @ epoch 3 new loss 7.05275908785552e-07 old loss 7.585153980471659e-07 BETTER
I0320 22:54:35.017382 1531856 finetune.py:68] layer 2_k @ epoch 4 new loss 3.191880523445434e-06 old loss 3.266146677560755e-06 BETTER
I0320 22:54:52.008740 1531856 finetune.py:45] layer 2_o initial loss 6.888926691317465e-06
I0320 22:55:02.003954 1531800 finetune.py:68] layer 1_o @ epoch 2 new loss 7.88574106991291e-06 old loss 8.31682700663805e-06 BETTER
I0320 22:55:04.145990 1531912 finetune.py:68] layer 3_k @ epoch 2 new loss 5.4962843023531605e-06 old loss 5.61090155315469e-06 BETTER
I0320 22:55:06.204420 1531744 finetune.py:68] layer 0_o @ epoch 4 new loss 6.612424954255403e-07 old loss 7.05275908785552e-07 BETTER
I0320 22:55:25.512284 1531856 finetune.py:68] layer 2_o @ epoch 0 new loss 6.791053237975575e-06 old loss 6.888926691317465e-06 BETTER
I0320 22:55:35.285590 1531744 finetune.py:45] layer 0_up initial loss 7.570073989882076e-07
I0320 22:55:38.092554 1531800 finetune.py:68] layer 1_o @ epoch 3 new loss 7.554308467661031e-06 old loss 7.88574106991291e-06 BETTER
I0320 22:55:40.072541 1531912 finetune.py:68] layer 3_k @ epoch 3 new loss 5.394338586484082e-06 old loss 5.4962843023531605e-06 BETTER
I0320 22:56:00.778177 1531856 finetune.py:68] layer 2_o @ epoch 1 new loss 6.716373263770947e-06 old loss 6.791053237975575e-06 BETTER
I0320 22:56:09.175395 1531744 finetune.py:68] layer 0_up @ epoch 0 new loss 7.083642117322597e-07 old loss 7.570073989882076e-07 BETTER
I0320 22:56:14.224500 1531800 finetune.py:68] layer 1_o @ epoch 4 new loss 7.201149401225848e-06 old loss 7.554308467661031e-06 BETTER
I0320 22:56:15.804018 1531912 finetune.py:68] layer 3_k @ epoch 4 new loss 5.305906597641297e-06 old loss 5.394338586484082e-06 BETTER
I0320 22:56:32.193247 1531912 finetune.py:45] layer 3_o initial loss 1.3377799405134283e-05
I0320 22:56:35.672806 1531856 finetune.py:68] layer 2_o @ epoch 2 new loss 6.652734555245843e-06 old loss 6.716373263770947e-06 BETTER
I0320 22:56:44.341659 1531744 finetune.py:68] layer 0_up @ epoch 1 new loss 6.736201498824812e-07 old loss 7.083642117322597e-07 BETTER
I0320 22:56:45.093364 1531800 finetune.py:45] layer 1_up initial loss 5.0651207857299596e-05
I0320 22:57:05.501950 1531912 finetune.py:68] layer 3_o @ epoch 0 new loss 1.2936983694089577e-05 old loss 1.3377799405134283e-05 BETTER
I0320 22:57:11.644007 1531856 finetune.py:68] layer 2_o @ epoch 3 new loss 6.597306764888344e-06 old loss 6.652734555245843e-06 BETTER
I0320 22:57:18.022158 1531800 finetune.py:68] layer 1_up @ epoch 0 new loss 1.0666008165571839e-05 old loss 5.0651207857299596e-05 BETTER
I0320 22:57:19.821673 1531744 finetune.py:68] layer 0_up @ epoch 2 new loss 6.456115784203575e-07 old loss 6.736201498824812e-07 BETTER
I0320 22:57:40.572199 1531912 finetune.py:68] layer 3_o @ epoch 1 new loss 1.2662389963224996e-05 old loss 1.2936983694089577e-05 BETTER
I0320 22:57:48.224058 1531856 finetune.py:68] layer 2_o @ epoch 4 new loss 6.547120847244514e-06 old loss 6.597306764888344e-06 BETTER
I0320 22:57:52.070531 1531800 finetune.py:68] layer 1_up @ epoch 1 new loss 8.067480848694686e-06 old loss 1.0666008165571839e-05 BETTER
I0320 22:57:54.156961 1531744 finetune.py:68] layer 0_up @ epoch 3 new loss 6.218086809894885e-07 old loss 6.456115784203575e-07 BETTER
I0320 22:58:15.472997 1531912 finetune.py:68] layer 3_o @ epoch 2 new loss 1.24631469589076e-05 old loss 1.2662389963224996e-05 BETTER
I0320 22:58:18.527998 1531856 finetune.py:45] layer 2_up initial loss 8.909641110221855e-06
I0320 22:58:26.023964 1531800 finetune.py:68] layer 1_up @ epoch 2 new loss 7.95748746895697e-06 old loss 8.067480848694686e-06 BETTER
I0320 22:58:29.335850 1531744 finetune.py:68] layer 0_up @ epoch 4 new loss 6.015604867570801e-07 old loss 6.218086809894885e-07 BETTER
I0320 22:58:50.838732 1531912 finetune.py:68] layer 3_o @ epoch 3 new loss 1.2299308764340822e-05 old loss 1.24631469589076e-05 BETTER
I0320 22:58:51.709274 1531856 finetune.py:68] layer 2_up @ epoch 0 new loss 8.850315680319909e-06 old loss 8.909641110221855e-06 BETTER
I0320 22:59:00.475660 1531800 finetune.py:68] layer 1_up @ epoch 3 new loss 7.69127109379042e-06 old loss 7.95748746895697e-06 BETTER
I0320 22:59:00.563879 1531744 finetune.py:45] layer 0_gate initial loss 6.802852112741675e-07
I0320 22:59:26.551425 1531856 finetune.py:68] layer 2_up @ epoch 1 new loss 8.805343895801343e-06 old loss 8.850315680319909e-06 BETTER
I0320 22:59:26.892394 1531912 finetune.py:68] layer 3_o @ epoch 4 new loss 1.2163090104877483e-05 old loss 1.2299308764340822e-05 BETTER
I0320 22:59:32.710674 1531744 finetune.py:68] layer 0_gate @ epoch 0 new loss 6.531192298098176e-07 old loss 6.802852112741675e-07 BETTER
I0320 22:59:33.715258 1531800 finetune.py:68] layer 1_up @ epoch 4 new loss 7.544826530647697e-06 old loss 7.69127109379042e-06 BETTER
I0320 22:59:56.436702 1531912 finetune.py:45] layer 3_up initial loss 1.784196683729533e-05
I0320 23:00:00.933657 1531856 finetune.py:68] layer 2_up @ epoch 2 new loss 8.765491656959057e-06 old loss 8.805343895801343e-06 BETTER
I0320 23:00:05.057276 1531800 finetune.py:45] layer 1_gate initial loss 4.945397813571617e-05
I0320 23:00:06.257469 1531744 finetune.py:68] layer 0_gate @ epoch 1 new loss 6.360841098285164e-07 old loss 6.531192298098176e-07 BETTER
I0320 23:00:28.999858 1531912 finetune.py:68] layer 3_up @ epoch 0 new loss 1.7674807168077677e-05 old loss 1.784196683729533e-05 BETTER
I0320 23:00:36.755642 1531856 finetune.py:68] layer 2_up @ epoch 3 new loss 8.728657121537253e-06 old loss 8.765491656959057e-06 BETTER
I0320 23:00:37.080893 1531800 finetune.py:68] layer 1_gate @ epoch 0 new loss 1.2270587831153534e-05 old loss 4.945397813571617e-05 BETTER
I0320 23:00:40.975974 1531744 finetune.py:68] layer 0_gate @ epoch 2 new loss 6.225445758900605e-07 old loss 6.360841098285164e-07 BETTER
I0320 23:01:03.125073 1531912 finetune.py:68] layer 3_up @ epoch 1 new loss 1.75521108758403e-05 old loss 1.7674807168077677e-05 BETTER
I0320 23:01:10.121340 1531800 finetune.py:68] layer 1_gate @ epoch 1 new loss 8.694316420587711e-06 old loss 1.2270587831153534e-05 BETTER
I0320 23:01:12.046062 1531856 finetune.py:68] layer 2_up @ epoch 4 new loss 8.694116331753321e-06 old loss 8.728657121537253e-06 BETTER
I0320 23:01:14.714364 1531744 finetune.py:68] layer 0_gate @ epoch 3 new loss 6.111927177698817e-07 old loss 6.225445758900605e-07 BETTER
I0320 23:01:36.821263 1531912 finetune.py:68] layer 3_up @ epoch 2 new loss 1.7445714547648095e-05 old loss 1.75521108758403e-05 BETTER
I0320 23:01:42.966482 1531856 finetune.py:45] layer 2_gate initial loss 1.0409859896753915e-05
I0320 23:01:43.962973 1531800 finetune.py:68] layer 1_gate @ epoch 2 new loss 8.481575605401304e-06 old loss 8.694316420587711e-06 BETTER
I0320 23:01:48.546068 1531744 finetune.py:68] layer 0_gate @ epoch 4 new loss 6.013779056956992e-07 old loss 6.111927177698817e-07 BETTER
I0320 23:02:11.113828 1531912 finetune.py:68] layer 3_up @ epoch 3 new loss 1.7349060726701282e-05 old loss 1.7445714547648095e-05 BETTER
I0320 23:02:14.543277 1531856 finetune.py:68] layer 2_gate @ epoch 0 new loss 1.0363085493736435e-05 old loss 1.0409859896753915e-05 BETTER
I0320 23:02:15.831861 1531800 finetune.py:68] layer 1_gate @ epoch 3 new loss 8.37232255435083e-06 old loss 8.481575605401304e-06 BETTER
I0320 23:02:21.212332 1531744 finetune.py:45] layer 0_down initial loss 1.1531794825714314e-06
I0320 23:02:45.936405 1531912 finetune.py:68] layer 3_up @ epoch 4 new loss 1.725932816043496e-05 old loss 1.7349060726701282e-05 BETTER
I0320 23:02:48.138311 1531856 finetune.py:68] layer 2_gate @ epoch 1 new loss 1.033047192322556e-05 old loss 1.0363085493736435e-05 BETTER
I0320 23:02:48.841895 1531800 finetune.py:68] layer 1_gate @ epoch 4 new loss 8.228075785154942e-06 old loss 8.37232255435083e-06 BETTER
I0320 23:02:51.386430 1531744 finetune.py:68] layer 0_down @ epoch 0 new loss 1.1470153822301654e-06 old loss 1.1531794825714314e-06 BETTER
I0320 23:03:16.725352 1531912 finetune.py:45] layer 3_gate initial loss 2.1339457816793583e-05
I0320 23:03:22.088217 1531856 finetune.py:68] layer 2_gate @ epoch 2 new loss 1.030155544867739e-05 old loss 1.033047192322556e-05 BETTER
I0320 23:03:22.165935 1531800 finetune.py:45] layer 1_down initial loss 0.0014725528890267015
I0320 23:03:23.615512 1531744 finetune.py:68] layer 0_down @ epoch 1 new loss 1.1456879747129278e-06 old loss 1.1470153822301654e-06 BETTER
I0320 23:03:48.423777 1531912 finetune.py:68] layer 3_gate @ epoch 0 new loss 2.123202102666255e-05 old loss 2.1339457816793583e-05 BETTER
I0320 23:03:52.766164 1531800 finetune.py:68] layer 1_down @ epoch 0 new loss 0.0014566041063517332 old loss 0.0014725528890267015 BETTER
I0320 23:03:55.837324 1531856 finetune.py:68] layer 2_gate @ epoch 3 new loss 1.0275273780280259e-05 old loss 1.030155544867739e-05 BETTER
I0320 23:03:56.226970 1531744 finetune.py:68] layer 0_down @ epoch 2 new loss 1.1450151760072913e-06 old loss 1.1456879747129278e-06 BETTER
I0320 23:04:21.827755 1531912 finetune.py:68] layer 3_gate @ epoch 1 new loss 2.1151874534552917e-05 old loss 2.123202102666255e-05 BETTER
I0320 23:04:24.549712 1531800 finetune.py:68] layer 1_down @ epoch 1 new loss 0.0014560821000486612 old loss 0.0014566041063517332 BETTER
I0320 23:04:28.826938 1531744 finetune.py:68] layer 0_down @ epoch 3 new loss 1.1444955134720658e-06 old loss 1.1450151760072913e-06 BETTER
I0320 23:04:29.222842 1531856 finetune.py:68] layer 2_gate @ epoch 4 new loss 1.0250119885313325e-05 old loss 1.0275273780280259e-05 BETTER
I0320 23:04:54.084121 1531912 finetune.py:68] layer 3_gate @ epoch 2 new loss 2.1081705199321732e-05 old loss 2.1151874534552917e-05 BETTER
I0320 23:04:55.362002 1531800 finetune.py:68] layer 1_down @ epoch 2 new loss 0.001455872436054051 old loss 0.0014560821000486612 BETTER
I0320 23:05:00.881323 1531744 finetune.py:68] layer 0_down @ epoch 4 new loss 1.1440114349170472e-06 old loss 1.1444955134720658e-06 BETTER
0_v proxy err 0.03965747356414795 tr(WHW.T) 4.225186347961426
bpp_loss 3.303962826728821
0_q proxy err 0.0001614121865713969 tr(WHW.T) 2710.606689453125
bpp_loss 3.4393850564956665
0_k proxy err 0.00019645005522761494 tr(WHW.T) 1699.218017578125
bpp_loss 3.5403919219970703
0_o proxy err 0.002799158450216055 tr(WHW.T) 0.9710482954978943
bpp_loss 3.117640495300293
0_up proxy err 0.004899368155747652 tr(WHW.T) 43.290138244628906
bpp_loss 3.6095847196357194
0_gate proxy err 0.003385437186807394 tr(WHW.T) 63.488494873046875
bpp_loss 3.6275402335233466
0_down proxy err 0.0033542211167514324 tr(WHW.T) 0.6580609083175659
bpp_loss 3.654481111570846
I0320 23:05:03.870179 1531856 finetune.py:45] layer 2_down initial loss 1.5144805729505606e-05
I0320 23:05:27.199585 1531912 finetune.py:68] layer 3_gate @ epoch 3 new loss 2.1017105609644204e-05 old loss 2.1081705199321732e-05 BETTER
I0320 23:05:27.970493 1531800 finetune.py:68] layer 1_down @ epoch 3 new loss 0.0014556164387613535 old loss 0.001455872436054051 BETTER
I0320 23:05:33.959733 1531856 finetune.py:68] layer 2_down @ epoch 0 new loss 1.5141125913942233e-05 old loss 1.5144805729505606e-05 BETTER
I0320 23:05:59.212133 1531800 finetune.py:68] layer 1_down @ epoch 4 new loss 0.001455313409678638 old loss 0.0014556164387613535 BETTER
I0320 23:05:59.360197 1531912 finetune.py:68] layer 3_gate @ epoch 4 new loss 2.0956584194209427e-05 old loss 2.1017105609644204e-05 BETTER
1_v proxy err 0.05781306326389313 tr(WHW.T) 16.465883255004883
bpp_loss 3.2507294416427612
1_q proxy err 0.00030145407072268426 tr(WHW.T) 4779.80126953125
bpp_loss 4.322580814361572
1_k proxy err 0.0002915026852861047 tr(WHW.T) 4999.890625
bpp_loss 4.318829298019409
1_o proxy err 0.009700675494968891 tr(WHW.T) 1.1117877960205078
bpp_loss 3.163782238960266
1_up proxy err 0.005132959224283695 tr(WHW.T) 109.92166137695312
bpp_loss 3.677332323650981
1_gate proxy err 0.0026274980045855045 tr(WHW.T) 221.72103881835938
bpp_loss 3.748407940531886
1_down proxy err 0.002983252052217722 tr(WHW.T) 2043.5308837890625
bpp_loss 3.6939858724904613
I0320 23:06:03.578343 1531856 finetune.py:68] layer 2_down @ epoch 1 new loss 1.5139336937863845e-05 old loss 1.5141125913942233e-05 BETTER
I0320 23:06:27.135320 1531912 finetune.py:45] layer 3_down initial loss 3.132334677502513e-05
I0320 23:06:32.652361 1531856 finetune.py:68] layer 2_down @ epoch 2 new loss 1.5138068192754872e-05 old loss 1.5139336937863845e-05 BETTER
I0320 23:06:54.431361 1531912 finetune.py:68] layer 3_down @ epoch 0 new loss 3.131413541268557e-05 old loss 3.132334677502513e-05 BETTER
I0320 23:07:02.381922 1531856 finetune.py:68] layer 2_down @ epoch 3 new loss 1.513691768195713e-05 old loss 1.5138068192754872e-05 BETTER
I0320 23:07:19.946005 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 4 in 73.75597047805786s
I0320 23:07:22.928022 1531912 finetune.py:68] layer 3_down @ epoch 1 new loss 3.131218545604497e-05 old loss 3.131413541268557e-05 BETTER
I0320 23:07:24.272064 1531966 config.py:54] PyTorch version 2.6.0 available.
W0320 23:07:24.660297 1531966 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 23:07:25.785001 1531966 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 23:07:25.789299 1531629 quantize_finetune_llama.py:203] layer 5 gpu 1
I0320 23:07:25.805216 1531966 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0320 23:07:31.467480 1531856 finetune.py:68] layer 2_down @ epoch 4 new loss 1.5135921785258688e-05 old loss 1.513691768195713e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2_v proxy err 0.014974652789533138 tr(WHW.T) 136.67332458496094
bpp_loss 3.496095299720764
2_q proxy err 0.0003684539988171309 tr(WHW.T) 7766.84765625
bpp_loss 4.3353307247161865
2_k proxy err 0.00028841313906013966 tr(WHW.T) 10218.611328125
bpp_loss 4.4339001178741455
2_o proxy err 0.007901201955974102 tr(WHW.T) 1.4652316570281982
bpp_loss 3.469383955001831
2_up proxy err 0.00599703611806035 tr(WHW.T) 193.498291015625
bpp_loss 3.6977012101994005
2_gate proxy err 0.003888170700520277 tr(WHW.T) 306.6650085449219
bpp_loss 3.783025164936864
2_down proxy err 0.006908738985657692 tr(WHW.T) 3.01460337638855
bpp_loss 3.705028533935547
I0320 23:07:41.027342 1531966 finetune.py:45] layer 4_v initial loss 2.5711022317409515e-05
W0320 23:07:41.027948 1531966 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 23:07:51.452714 1531912 finetune.py:68] layer 3_down @ epoch 2 new loss 3.1310581107391044e-05 old loss 3.131218545604497e-05 BETTER
I0320 23:08:15.738539 1531966 finetune.py:68] layer 4_v @ epoch 0 new loss 1.397021424054401e-05 old loss 2.5711022317409515e-05 BETTER
I0320 23:08:19.987242 1531912 finetune.py:68] layer 3_down @ epoch 3 new loss 3.1309555197367445e-05 old loss 3.1310581107391044e-05 BETTER
I0320 23:08:49.062409 1531912 finetune.py:68] layer 3_down @ epoch 4 new loss 3.130850018351339e-05 old loss 3.1309555197367445e-05 BETTER
I0320 23:08:51.680335 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 5 in 73.11000537872314s
3_v proxy err 0.014078691601753235 tr(WHW.T) 284.77557373046875
bpp_loss 3.4509406089782715
3_q proxy err 0.0007122599054127932 tr(WHW.T) 7224.14794921875
bpp_loss 4.233824968338013
3_k proxy err 0.0005226695211604238 tr(WHW.T) 10088.0283203125
bpp_loss 4.3062543869018555
3_o proxy err 0.007882476784288883 tr(WHW.T) 3.360722780227661
bpp_loss 3.433118224143982
3_up proxy err 0.006841047666966915 tr(WHW.T) 284.8253173828125
bpp_loss 3.706118384072947
3_gate proxy err 0.004197556059807539 tr(WHW.T) 478.0318908691406
bpp_loss 3.8017689904501273
3_down proxy err 0.007056008093059063 tr(WHW.T) 6.143034934997559
bpp_loss 3.7085673753605333
I0320 23:08:52.203755 1531966 finetune.py:68] layer 4_v @ epoch 1 new loss 1.0910729542956688e-05 old loss 1.397021424054401e-05 BETTER
I0320 23:08:55.591183 1532020 config.py:54] PyTorch version 2.6.0 available.
W0320 23:08:55.941963 1532020 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 23:08:56.925393 1532020 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 23:08:56.929783 1531629 quantize_finetune_llama.py:203] layer 6 gpu 2
I0320 23:08:56.944260 1532020 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 23:09:09.382744 1532020 finetune.py:45] layer 5_v initial loss 3.917486174032092e-05
W0320 23:09:09.382942 1532020 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 23:09:28.310437 1531966 finetune.py:68] layer 4_v @ epoch 2 new loss 9.582284292264376e-06 old loss 1.0910729542956688e-05 BETTER
I0320 23:09:42.576483 1532020 finetune.py:68] layer 5_v @ epoch 0 new loss 2.0865460101049393e-05 old loss 3.917486174032092e-05 BETTER
I0320 23:10:05.186738 1531966 finetune.py:68] layer 4_v @ epoch 3 new loss 8.788435479800683e-06 old loss 9.582284292264376e-06 BETTER
I0320 23:10:12.150040 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 6 in 74.75110697746277s
I0320 23:10:16.651045 1532020 finetune.py:68] layer 5_v @ epoch 1 new loss 1.6817979485495016e-05 old loss 2.0865460101049393e-05 BETTER
I0320 23:10:16.796858 1532074 config.py:54] PyTorch version 2.6.0 available.
W0320 23:10:17.156589 1532074 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 23:10:18.329171 1532074 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 23:10:18.333587 1531629 quantize_finetune_llama.py:203] layer 7 gpu 3
I0320 23:10:18.351654 1532074 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 23:10:32.590395 1532074 finetune.py:45] layer 6_v initial loss 5.056495501776226e-05
W0320 23:10:32.590991 1532074 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 23:10:42.105388 1531966 finetune.py:68] layer 4_v @ epoch 4 new loss 8.227457328757737e-06 old loss 8.788435479800683e-06 BETTER
I0320 23:10:51.179447 1532020 finetune.py:68] layer 5_v @ epoch 2 new loss 1.5021375475043897e-05 old loss 1.6817979485495016e-05 BETTER
I0320 23:10:58.506911 1531966 finetune.py:45] layer 4_q initial loss 1.0036449566541705e-05
I0320 23:11:07.084998 1532074 finetune.py:68] layer 6_v @ epoch 0 new loss 2.5382089006598108e-05 old loss 5.056495501776226e-05 BETTER
I0320 23:11:26.701056 1532020 finetune.py:68] layer 5_v @ epoch 3 new loss 1.3916244824940804e-05 old loss 1.5021375475043897e-05 BETTER
I0320 23:11:33.701067 1531966 finetune.py:68] layer 4_q @ epoch 0 new loss 8.845687261782587e-06 old loss 1.0036449566541705e-05 BETTER
I0320 23:11:37.718989 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 7 in 78.8266806602478s
I0320 23:11:42.279001 1532128 config.py:54] PyTorch version 2.6.0 available.
I0320 23:11:42.521136 1532074 finetune.py:68] layer 6_v @ epoch 1 new loss 2.0939307432854548e-05 old loss 2.5382089006598108e-05 BETTER
W0320 23:11:42.701570 1532128 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 23:11:43.963339 1532128 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 23:11:43.968735 1531629 quantize_finetune_llama.py:203] layer 8 gpu 0
I0320 23:11:43.988389 1532128 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 23:12:00.766444 1532128 finetune.py:45] layer 7_v initial loss 6.230418512132019e-05
W0320 23:12:00.766836 1532128 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 23:12:03.226247 1532020 finetune.py:68] layer 5_v @ epoch 4 new loss 1.31474753288785e-05 old loss 1.3916244824940804e-05 BETTER
I0320 23:12:10.181993 1531966 finetune.py:68] layer 4_q @ epoch 1 new loss 8.381778570765164e-06 old loss 8.845687261782587e-06 BETTER
I0320 23:12:18.837651 1532074 finetune.py:68] layer 6_v @ epoch 2 new loss 1.8867989638238214e-05 old loss 2.0939307432854548e-05 BETTER
I0320 23:12:21.323127 1532020 finetune.py:45] layer 5_q initial loss 1.590665488038212e-05
I0320 23:12:34.958640 1532128 finetune.py:68] layer 7_v @ epoch 0 new loss 3.117396408924833e-05 old loss 6.230418512132019e-05 BETTER
I0320 23:12:47.124850 1531966 finetune.py:68] layer 4_q @ epoch 2 new loss 8.032923688006122e-06 old loss 8.381778570765164e-06 BETTER
I0320 23:12:56.083040 1532074 finetune.py:68] layer 6_v @ epoch 3 new loss 1.7552523786434904e-05 old loss 1.8867989638238214e-05 BETTER
I0320 23:12:56.637807 1532020 finetune.py:68] layer 5_q @ epoch 0 new loss 1.3983783901494462e-05 old loss 1.590665488038212e-05 BETTER
I0320 23:13:10.756802 1532128 finetune.py:68] layer 7_v @ epoch 1 new loss 2.6191528377239592e-05 old loss 3.117396408924833e-05 BETTER
I0320 23:13:24.721054 1531966 finetune.py:68] layer 4_q @ epoch 3 new loss 7.75522039475618e-06 old loss 8.032923688006122e-06 BETTER
I0320 23:13:33.548904 1532020 finetune.py:68] layer 5_q @ epoch 1 new loss 1.3332534763321746e-05 old loss 1.3983783901494462e-05 BETTER
I0320 23:13:33.576411 1532074 finetune.py:68] layer 6_v @ epoch 4 new loss 1.6662965208524838e-05 old loss 1.7552523786434904e-05 BETTER
I0320 23:13:46.593557 1532128 finetune.py:68] layer 7_v @ epoch 2 new loss 2.3885173504822887e-05 old loss 2.6191528377239592e-05 BETTER
I0320 23:13:51.220399 1532074 finetune.py:45] layer 6_q initial loss 2.1772057152702473e-05
I0320 23:14:03.321644 1531966 finetune.py:68] layer 4_q @ epoch 4 new loss 7.523689873778494e-06 old loss 7.75522039475618e-06 BETTER
I0320 23:14:08.980965 1532020 finetune.py:68] layer 5_q @ epoch 2 new loss 1.2861942195740994e-05 old loss 1.3332534763321746e-05 BETTER
I0320 23:14:20.424678 1531966 finetune.py:45] layer 4_k initial loss 9.018522177939303e-06
I0320 23:14:23.672642 1532128 finetune.py:68] layer 7_v @ epoch 3 new loss 2.2456679289462045e-05 old loss 2.3885173504822887e-05 BETTER
I0320 23:14:26.420908 1532074 finetune.py:68] layer 6_q @ epoch 0 new loss 1.8943763279821724e-05 old loss 2.1772057152702473e-05 BETTER
I0320 23:14:44.798837 1532020 finetune.py:68] layer 5_q @ epoch 3 new loss 1.2485337720136158e-05 old loss 1.2861942195740994e-05 BETTER
I0320 23:14:56.923040 1531966 finetune.py:68] layer 4_k @ epoch 0 new loss 8.474185051454697e-06 old loss 9.018522177939303e-06 BETTER
I0320 23:15:00.722360 1532128 finetune.py:68] layer 7_v @ epoch 4 new loss 2.1465237296069972e-05 old loss 2.2456679289462045e-05 BETTER
I0320 23:15:02.863281 1532074 finetune.py:68] layer 6_q @ epoch 1 new loss 1.8098704458680004e-05 old loss 1.8943763279821724e-05 BETTER
I0320 23:15:20.505179 1532128 finetune.py:45] layer 7_q initial loss 2.821650377882179e-05
I0320 23:15:21.228116 1532020 finetune.py:68] layer 5_q @ epoch 4 new loss 1.217189674207475e-05 old loss 1.2485337720136158e-05 BETTER
I0320 23:15:33.854394 1531966 finetune.py:68] layer 4_k @ epoch 1 new loss 8.244845957960933e-06 old loss 8.474185051454697e-06 BETTER
I0320 23:15:39.532871 1532020 finetune.py:45] layer 5_k initial loss 1.3921672689320985e-05
I0320 23:15:39.772552 1532074 finetune.py:68] layer 6_q @ epoch 2 new loss 1.7478536392445676e-05 old loss 1.8098704458680004e-05 BETTER
I0320 23:15:54.863779 1532128 finetune.py:68] layer 7_q @ epoch 0 new loss 2.5118617486441508e-05 old loss 2.821650377882179e-05 BETTER
I0320 23:16:12.277654 1531966 finetune.py:68] layer 4_k @ epoch 2 new loss 8.061357220867649e-06 old loss 8.244845957960933e-06 BETTER
I0320 23:16:15.334409 1532020 finetune.py:68] layer 5_k @ epoch 0 new loss 1.3175185813452117e-05 old loss 1.3921672689320985e-05 BETTER
I0320 23:16:16.960439 1532074 finetune.py:68] layer 6_q @ epoch 3 new loss 1.6988604329526424e-05 old loss 1.7478536392445676e-05 BETTER
I0320 23:16:30.475163 1532128 finetune.py:68] layer 7_q @ epoch 1 new loss 2.405434861429967e-05 old loss 2.5118617486441508e-05 BETTER
I0320 23:16:50.815568 1531966 finetune.py:68] layer 4_k @ epoch 3 new loss 7.906939572421834e-06 old loss 8.061357220867649e-06 BETTER
I0320 23:16:52.452934 1532020 finetune.py:68] layer 5_k @ epoch 1 new loss 1.288567636947846e-05 old loss 1.3175185813452117e-05 BETTER
I0320 23:16:54.261857 1532074 finetune.py:68] layer 6_q @ epoch 4 new loss 1.6577290807617828e-05 old loss 1.6988604329526424e-05 BETTER
I0320 23:17:05.849527 1532128 finetune.py:68] layer 7_q @ epoch 2 new loss 2.3296304789255373e-05 old loss 2.405434861429967e-05 BETTER
I0320 23:17:12.422725 1532074 finetune.py:45] layer 6_k initial loss 2.0387791664688848e-05
I0320 23:17:28.761273 1531966 finetune.py:68] layer 4_k @ epoch 4 new loss 7.77355489844922e-06 old loss 7.906939572421834e-06 BETTER
I0320 23:17:28.764262 1532020 finetune.py:68] layer 5_k @ epoch 2 new loss 1.2647137737076264e-05 old loss 1.288567636947846e-05 BETTER
I0320 23:17:41.442374 1532128 finetune.py:68] layer 7_q @ epoch 3 new loss 2.2701447960571386e-05 old loss 2.3296304789255373e-05 BETTER
I0320 23:17:46.890801 1531966 finetune.py:45] layer 4_o initial loss 1.9144188627251424e-05
I0320 23:17:47.751885 1532074 finetune.py:68] layer 6_k @ epoch 0 new loss 1.8967341020470485e-05 old loss 2.0387791664688848e-05 BETTER
I0320 23:18:04.505930 1532020 finetune.py:68] layer 5_k @ epoch 3 new loss 1.2444707863323856e-05 old loss 1.2647137737076264e-05 BETTER
I0320 23:18:17.261680 1532128 finetune.py:68] layer 7_q @ epoch 4 new loss 2.2205944333109073e-05 old loss 2.2701447960571386e-05 BETTER
I0320 23:18:22.397885 1531966 finetune.py:68] layer 4_o @ epoch 0 new loss 1.8390257537248544e-05 old loss 1.9144188627251424e-05 BETTER
I0320 23:18:23.377954 1532074 finetune.py:68] layer 6_k @ epoch 1 new loss 1.8562119294074364e-05 old loss 1.8967341020470485e-05 BETTER
I0320 23:18:37.142529 1532128 finetune.py:45] layer 7_k initial loss 2.7327885618433356e-05
I0320 23:18:39.763674 1532020 finetune.py:68] layer 5_k @ epoch 4 new loss 1.2269863873370923e-05 old loss 1.2444707863323856e-05 BETTER
I0320 23:18:57.878119 1532020 finetune.py:45] layer 5_o initial loss 3.489888695185073e-05
I0320 23:19:00.129793 1531966 finetune.py:68] layer 4_o @ epoch 1 new loss 1.8026901670964435e-05 old loss 1.8390257537248544e-05 BETTER
I0320 23:19:00.649311 1532074 finetune.py:68] layer 6_k @ epoch 2 new loss 1.8245276805828325e-05 old loss 1.8562119294074364e-05 BETTER
I0320 23:19:11.146118 1532128 finetune.py:68] layer 7_k @ epoch 0 new loss 2.5752287911018357e-05 old loss 2.7327885618433356e-05 BETTER
I0320 23:19:32.393991 1532020 finetune.py:68] layer 5_o @ epoch 0 new loss 3.29714675899595e-05 old loss 3.489888695185073e-05 BETTER
I0320 23:19:37.729270 1532074 finetune.py:68] layer 6_k @ epoch 3 new loss 1.7977125025936402e-05 old loss 1.8245276805828325e-05 BETTER
I0320 23:19:38.016005 1531966 finetune.py:68] layer 4_o @ epoch 2 new loss 1.7764046788215637e-05 old loss 1.8026901670964435e-05 BETTER
I0320 23:19:46.794108 1532128 finetune.py:68] layer 7_k @ epoch 1 new loss 2.525266791053582e-05 old loss 2.5752287911018357e-05 BETTER
I0320 23:20:07.859203 1532020 finetune.py:68] layer 5_o @ epoch 1 new loss 3.1903033232083544e-05 old loss 3.29714675899595e-05 BETTER
I0320 23:20:14.694483 1532074 finetune.py:68] layer 6_k @ epoch 4 new loss 1.7758085959940217e-05 old loss 1.7977125025936402e-05 BETTER
I0320 23:20:15.864864 1531966 finetune.py:68] layer 4_o @ epoch 3 new loss 1.7545384253025986e-05 old loss 1.7764046788215637e-05 BETTER
I0320 23:20:21.664777 1532128 finetune.py:68] layer 7_k @ epoch 2 new loss 2.48709420702653e-05 old loss 2.525266791053582e-05 BETTER
I0320 23:20:32.065526 1532074 finetune.py:45] layer 6_o initial loss 4.6192886657081544e-05
I0320 23:20:42.462179 1532020 finetune.py:68] layer 5_o @ epoch 2 new loss 3.110606121481396e-05 old loss 3.1903033232083544e-05 BETTER
I0320 23:20:52.069055 1531966 finetune.py:68] layer 4_o @ epoch 4 new loss 1.735642035782803e-05 old loss 1.7545384253025986e-05 BETTER
I0320 23:20:57.130095 1532128 finetune.py:68] layer 7_k @ epoch 3 new loss 2.454233617754653e-05 old loss 2.48709420702653e-05 BETTER
I0320 23:21:05.935778 1532074 finetune.py:68] layer 6_o @ epoch 0 new loss 4.318405990488827e-05 old loss 4.6192886657081544e-05 BETTER
I0320 23:21:17.162218 1532020 finetune.py:68] layer 5_o @ epoch 3 new loss 3.0455878004431725e-05 old loss 3.110606121481396e-05 BETTER
I0320 23:21:22.892930 1531966 finetune.py:45] layer 4_up initial loss 2.8251401090528816e-05
I0320 23:21:32.229126 1532128 finetune.py:68] layer 7_k @ epoch 4 new loss 2.4259876227006316e-05 old loss 2.454233617754653e-05 BETTER
I0320 23:21:40.486010 1532074 finetune.py:68] layer 6_o @ epoch 1 new loss 4.1895764297805727e-05 old loss 4.318405990488827e-05 BETTER
I0320 23:21:53.434956 1532020 finetune.py:68] layer 5_o @ epoch 4 new loss 2.990982284245547e-05 old loss 3.0455878004431725e-05 BETTER
I0320 23:21:53.791482 1532128 finetune.py:45] layer 7_o initial loss 6.376537203323096e-05
I0320 23:21:56.791712 1531966 finetune.py:68] layer 4_up @ epoch 0 new loss 2.78557236015331e-05 old loss 2.8251401090528816e-05 BETTER
I0320 23:22:15.270798 1532074 finetune.py:68] layer 6_o @ epoch 2 new loss 4.0985396481119096e-05 old loss 4.1895764297805727e-05 BETTER
I0320 23:22:23.435340 1532020 finetune.py:45] layer 5_up initial loss 4.734672256745398e-05
I0320 23:22:27.891114 1532128 finetune.py:68] layer 7_o @ epoch 0 new loss 5.875888018636033e-05 old loss 6.376537203323096e-05 BETTER
I0320 23:22:31.322152 1531966 finetune.py:68] layer 4_up @ epoch 1 new loss 2.7605505238170736e-05 old loss 2.78557236015331e-05 BETTER
I0320 23:22:51.226547 1532074 finetune.py:68] layer 6_o @ epoch 3 new loss 4.027207978651859e-05 old loss 4.0985396481119096e-05 BETTER
I0320 23:22:55.761847 1532020 finetune.py:68] layer 5_up @ epoch 0 new loss 4.648016329156235e-05 old loss 4.734672256745398e-05 BETTER
I0320 23:23:03.362298 1532128 finetune.py:68] layer 7_o @ epoch 1 new loss 5.66366761631798e-05 old loss 5.875888018636033e-05 BETTER
I0320 23:23:06.997283 1531966 finetune.py:68] layer 4_up @ epoch 2 new loss 2.7401529223425314e-05 old loss 2.7605505238170736e-05 BETTER
I0320 23:23:27.577729 1532074 finetune.py:68] layer 6_o @ epoch 4 new loss 3.9692378777544945e-05 old loss 4.027207978651859e-05 BETTER
I0320 23:23:29.853255 1532020 finetune.py:68] layer 5_up @ epoch 1 new loss 4.5913609937997535e-05 old loss 4.648016329156235e-05 BETTER
I0320 23:23:38.242499 1532128 finetune.py:68] layer 7_o @ epoch 2 new loss 5.517817407962866e-05 old loss 5.66366761631798e-05 BETTER
I0320 23:23:41.742811 1531966 finetune.py:68] layer 4_up @ epoch 3 new loss 2.7218005925533362e-05 old loss 2.7401529223425314e-05 BETTER
I0320 23:23:57.002617 1532074 finetune.py:45] layer 6_up initial loss 6.651446892647073e-05
I0320 23:24:02.980580 1532020 finetune.py:68] layer 5_up @ epoch 2 new loss 4.5435001084115356e-05 old loss 4.5913609937997535e-05 BETTER
I0320 23:24:14.729701 1532128 finetune.py:68] layer 7_o @ epoch 3 new loss 5.407903518062085e-05 old loss 5.517817407962866e-05 BETTER
I0320 23:24:17.948412 1531966 finetune.py:68] layer 4_up @ epoch 4 new loss 2.7049725758843124e-05 old loss 2.7218005925533362e-05 BETTER
I0320 23:24:29.790096 1532074 finetune.py:68] layer 6_up @ epoch 0 new loss 6.503975600935519e-05 old loss 6.651446892647073e-05 BETTER
I0320 23:24:35.989667 1532020 finetune.py:68] layer 5_up @ epoch 3 new loss 4.501047806115821e-05 old loss 4.5435001084115356e-05 BETTER
I0320 23:24:49.303825 1531966 finetune.py:45] layer 4_gate initial loss 3.415333048906177e-05
I0320 23:24:50.193848 1532128 finetune.py:68] layer 7_o @ epoch 4 new loss 5.3186322475085035e-05 old loss 5.407903518062085e-05 BETTER
I0320 23:25:03.520204 1532074 finetune.py:68] layer 6_up @ epoch 1 new loss 6.421980651794001e-05 old loss 6.503975600935519e-05 BETTER
I0320 23:25:09.284440 1532020 finetune.py:68] layer 5_up @ epoch 4 new loss 4.462735523702577e-05 old loss 4.501047806115821e-05 BETTER
I0320 23:25:20.642874 1531966 finetune.py:68] layer 4_gate @ epoch 0 new loss 3.39299876941368e-05 old loss 3.415333048906177e-05 BETTER
I0320 23:25:22.050235 1532128 finetune.py:45] layer 7_up initial loss 9.018215496325865e-05
I0320 23:25:38.179122 1532074 finetune.py:68] layer 6_up @ epoch 2 new loss 6.353934440994635e-05 old loss 6.421980651794001e-05 BETTER
I0320 23:25:40.563572 1532020 finetune.py:45] layer 5_gate initial loss 5.540060737985186e-05
I0320 23:25:54.717406 1532128 finetune.py:68] layer 7_up @ epoch 0 new loss 8.769574924372137e-05 old loss 9.018215496325865e-05 BETTER
I0320 23:25:54.957798 1531966 finetune.py:68] layer 4_gate @ epoch 1 new loss 3.376493987161666e-05 old loss 3.39299876941368e-05 BETTER
I0320 23:26:12.766111 1532020 finetune.py:68] layer 5_gate @ epoch 0 new loss 5.4967666073935106e-05 old loss 5.540060737985186e-05 BETTER
I0320 23:26:13.851310 1532074 finetune.py:68] layer 6_up @ epoch 3 new loss 6.2953433371149e-05 old loss 6.353934440994635e-05 BETTER
I0320 23:26:29.249068 1531966 finetune.py:68] layer 4_gate @ epoch 2 new loss 3.3624022762523964e-05 old loss 3.376493987161666e-05 BETTER
I0320 23:26:29.387398 1532128 finetune.py:68] layer 7_up @ epoch 1 new loss 8.633056131657213e-05 old loss 8.769574924372137e-05 BETTER
I0320 23:26:45.342890 1532020 finetune.py:68] layer 5_gate @ epoch 1 new loss 5.46294468222186e-05 old loss 5.4967666073935106e-05 BETTER
I0320 23:26:48.888490 1532074 finetune.py:68] layer 6_up @ epoch 4 new loss 6.243397365324199e-05 old loss 6.2953433371149e-05 BETTER
I0320 23:27:03.477379 1531966 finetune.py:68] layer 4_gate @ epoch 3 new loss 3.349653343320824e-05 old loss 3.3624022762523964e-05 BETTER
I0320 23:27:03.562713 1532128 finetune.py:68] layer 7_up @ epoch 2 new loss 8.526405144948512e-05 old loss 8.633056131657213e-05 BETTER
I0320 23:27:18.124896 1532020 finetune.py:68] layer 5_gate @ epoch 2 new loss 5.433726983028464e-05 old loss 5.46294468222186e-05 BETTER
I0320 23:27:20.779211 1532074 finetune.py:45] layer 6_gate initial loss 7.75947337388061e-05
I0320 23:27:37.609129 1531966 finetune.py:68] layer 4_gate @ epoch 4 new loss 3.337824819027446e-05 old loss 3.349653343320824e-05 BETTER
I0320 23:27:37.840122 1532128 finetune.py:68] layer 7_up @ epoch 3 new loss 8.434987830696627e-05 old loss 8.526405144948512e-05 BETTER
I0320 23:27:50.876310 1532020 finetune.py:68] layer 5_gate @ epoch 3 new loss 5.406835407484323e-05 old loss 5.433726983028464e-05 BETTER
I0320 23:27:51.718995 1532074 finetune.py:68] layer 6_gate @ epoch 0 new loss 7.694725354667753e-05 old loss 7.75947337388061e-05 BETTER
I0320 23:28:11.529782 1531966 finetune.py:45] layer 4_down initial loss 5.277068339637481e-05
I0320 23:28:11.956472 1532128 finetune.py:68] layer 7_up @ epoch 4 new loss 8.356064063264057e-05 old loss 8.434987830696627e-05 BETTER
I0320 23:28:23.386827 1532020 finetune.py:68] layer 5_gate @ epoch 4 new loss 5.382407471188344e-05 old loss 5.406835407484323e-05 BETTER
I0320 23:28:24.347987 1532074 finetune.py:68] layer 6_gate @ epoch 1 new loss 7.646235462743789e-05 old loss 7.694725354667753e-05 BETTER
I0320 23:28:41.363497 1531966 finetune.py:68] layer 4_down @ epoch 0 new loss 5.275219518807717e-05 old loss 5.277068339637481e-05 BETTER
I0320 23:28:45.077708 1532128 finetune.py:45] layer 7_gate initial loss 0.00010427653614897281
I0320 23:28:56.692748 1532020 finetune.py:45] layer 5_down initial loss 8.147508924594149e-05
I0320 23:28:56.954183 1532074 finetune.py:68] layer 6_gate @ epoch 2 new loss 7.604523125337437e-05 old loss 7.646235462743789e-05 BETTER
I0320 23:29:14.274822 1531966 finetune.py:68] layer 4_down @ epoch 1 new loss 5.274586510495283e-05 old loss 5.275219518807717e-05 BETTER
I0320 23:29:16.498104 1532128 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.00010322166053811088 old loss 0.00010427653614897281 BETTER
I0320 23:29:27.019450 1532020 finetune.py:68] layer 5_down @ epoch 0 new loss 8.146141772158444e-05 old loss 8.147508924594149e-05 BETTER
I0320 23:29:30.242945 1532074 finetune.py:68] layer 6_gate @ epoch 3 new loss 7.56732260924764e-05 old loss 7.604523125337437e-05 BETTER
I0320 23:29:47.007475 1531966 finetune.py:68] layer 4_down @ epoch 2 new loss 5.2742514526471496e-05 old loss 5.274586510495283e-05 BETTER
I0320 23:29:48.657058 1532128 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.00010246477904729545 old loss 0.00010322166053811088 BETTER
I0320 23:29:58.176031 1532020 finetune.py:68] layer 5_down @ epoch 1 new loss 8.145524043357e-05 old loss 8.146141772158444e-05 BETTER
I0320 23:30:03.550724 1532074 finetune.py:68] layer 6_gate @ epoch 4 new loss 7.533318421337754e-05 old loss 7.56732260924764e-05 BETTER
I0320 23:30:19.299878 1531966 finetune.py:68] layer 4_down @ epoch 3 new loss 5.274029172142036e-05 old loss 5.2742514526471496e-05 BETTER
I0320 23:30:20.917107 1532128 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.00010183592530665919 old loss 0.00010246477904729545 BETTER
I0320 23:30:28.304799 1532020 finetune.py:68] layer 5_down @ epoch 2 new loss 8.145194442477077e-05 old loss 8.145524043357e-05 BETTER
I0320 23:30:37.963199 1532074 finetune.py:45] layer 6_down initial loss 0.00011714641732396558
I0320 23:30:52.070583 1531966 finetune.py:68] layer 4_down @ epoch 4 new loss 5.273838178254664e-05 old loss 5.274029172142036e-05 BETTER
I0320 23:30:53.607368 1532128 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.00010128506983164698 old loss 0.00010183592530665919 BETTER
4_v proxy err 0.013461359776556492 tr(WHW.T) 274.6131286621094
bpp_loss 3.4971959590911865
4_q proxy err 0.0006847326876595616 tr(WHW.T) 6923.92431640625
bpp_loss 4.310526609420776
4_k proxy err 0.00047089994768612087 tr(WHW.T) 10432.6640625
bpp_loss 4.350158929824829
4_o proxy err 0.007871086709201336 tr(WHW.T) 5.145592212677002
bpp_loss 3.477746605873108
4_up proxy err 0.006752131972461939 tr(WHW.T) 397.8131103515625
bpp_loss 3.6978344584620277
4_gate proxy err 0.003410657402127981 tr(WHW.T) 820.3585205078125
bpp_loss 3.828789289607558
4_down proxy err 0.007059122901409864 tr(WHW.T) 11.599902153015137
bpp_loss 3.695038174473962
I0320 23:30:58.614876 1532020 finetune.py:68] layer 5_down @ epoch 3 new loss 8.144925959641114e-05 old loss 8.145194442477077e-05 BETTER
I0320 23:31:08.954268 1532074 finetune.py:68] layer 6_down @ epoch 0 new loss 0.00011712200648617 old loss 0.00011714641732396558 BETTER
I0320 23:31:26.800620 1532128 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.00010077939805341884 old loss 0.00010128506983164698 BETTER
I0320 23:31:30.156411 1532020 finetune.py:68] layer 5_down @ epoch 4 new loss 8.144714956870303e-05 old loss 8.144925959641114e-05 BETTER
5_v proxy err 0.013410351239144802 tr(WHW.T) 298.47540283203125
bpp_loss 3.520462989807129
5_q proxy err 0.0007409557001665235 tr(WHW.T) 6778.8916015625
bpp_loss 4.320307970046997
5_k proxy err 0.00048742149374447763 tr(WHW.T) 10857.9453125
bpp_loss 4.397663354873657
5_o proxy err 0.011137139052152634 tr(WHW.T) 7.961273670196533
bpp_loss 3.5003554821014404
5_up proxy err 0.0066271559335291386 tr(WHW.T) 506.6600646972656
bpp_loss 3.6977657939112465
5_gate proxy err 0.0031778402626514435 tr(WHW.T) 1104.355712890625
bpp_loss 3.8364853969840116
5_down proxy err 0.007434831466525793 tr(WHW.T) 15.695380210876465
bpp_loss 3.696066324100938
I0320 23:31:37.707199 1532074 finetune.py:68] layer 6_down @ epoch 1 new loss 0.0001171113908640109 old loss 0.00011712200648617 BETTER
I0320 23:31:54.345596 1532128 finetune.py:45] layer 7_down initial loss 0.00015724542026873678
I0320 23:32:06.803292 1532074 finetune.py:68] layer 6_down @ epoch 2 new loss 0.0001171057447209023 old loss 0.0001171113908640109 BETTER
I0320 23:32:22.061231 1532128 finetune.py:68] layer 7_down @ epoch 0 new loss 0.0001572228648001328 old loss 0.00015724542026873678 BETTER
I0320 23:32:36.170113 1532074 finetune.py:68] layer 6_down @ epoch 3 new loss 0.00011710173566825688 old loss 0.0001171057447209023 BETTER
I0320 23:32:50.601779 1532128 finetune.py:68] layer 7_down @ epoch 1 new loss 0.00015721160161774606 old loss 0.0001572228648001328 BETTER
I0320 23:32:51.990229 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 8 in 75.40810751914978s
I0320 23:32:56.518126 1532182 config.py:54] PyTorch version 2.6.0 available.
W0320 23:32:56.937521 1532182 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 23:32:58.083450 1532182 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 23:32:58.088779 1531629 quantize_finetune_llama.py:203] layer 9 gpu 1
I0320 23:32:58.111445 1532182 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 23:33:05.531016 1532074 finetune.py:68] layer 6_down @ epoch 4 new loss 0.00011709883256116882 old loss 0.00011710173566825688 BETTER
6_v proxy err 0.013063895516097546 tr(WHW.T) 443.5464782714844
bpp_loss 3.454642653465271
6_q proxy err 0.0009521631873212755 tr(WHW.T) 7580.90625
bpp_loss 4.214670181274414
6_k proxy err 0.0007108948193490505 tr(WHW.T) 10426.7216796875
bpp_loss 4.249807834625244
6_o proxy err 0.010248545557260513 tr(WHW.T) 11.593282699584961
bpp_loss 3.4427833557128906
6_up proxy err 0.006635603029280901 tr(WHW.T) 617.46484375
bpp_loss 3.693729666776435
6_gate proxy err 0.002776945009827614 tr(WHW.T) 1554.95751953125
bpp_loss 3.8594818115234375
6_down proxy err 0.007664451375603676 tr(WHW.T) 23.072065353393555
bpp_loss 3.6895178196042084
I0320 23:33:13.070466 1532182 finetune.py:45] layer 8_v initial loss 9.481045708525926e-05
W0320 23:33:13.070699 1532182 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 23:33:19.631278 1532128 finetune.py:68] layer 7_down @ epoch 2 new loss 0.00015720573719590902 old loss 0.00015721160161774606 BETTER
I0320 23:33:48.289856 1532182 finetune.py:68] layer 8_v @ epoch 0 new loss 4.5745175157207996e-05 old loss 9.481045708525926e-05 BETTER
I0320 23:33:48.743444 1532128 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00015719934890512377 old loss 0.00015720573719590902 BETTER
I0320 23:34:17.623655 1532128 finetune.py:68] layer 7_down @ epoch 4 new loss 0.00015719501243438572 old loss 0.00015719934890512377 BETTER
7_v proxy err 0.012750092893838882 tr(WHW.T) 489.9357604980469
bpp_loss 3.4653502702713013
7_q proxy err 0.001017545466311276 tr(WHW.T) 7677.787109375
bpp_loss 4.211457252502441
7_k proxy err 0.000771447317674756 tr(WHW.T) 10213.0751953125
bpp_loss 4.220105171203613
7_o proxy err 0.011700780130922794 tr(WHW.T) 15.186613082885742
bpp_loss 3.449340581893921
7_up proxy err 0.006546773016452789 tr(WHW.T) 736.0653076171875
bpp_loss 3.697751244833303
7_gate proxy err 0.0027052557561546564 tr(WHW.T) 1877.51953125
bpp_loss 3.858637432719386
7_down proxy err 0.0077689532190561295 tr(WHW.T) 30.68963623046875
bpp_loss 3.691411306691724
I0320 23:34:23.403377 1532182 finetune.py:68] layer 8_v @ epoch 1 new loss 3.79502926080022e-05 old loss 4.5745175157207996e-05 BETTER
I0320 23:34:25.092100 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 9 in 73.28627443313599s
I0320 23:34:28.949406 1532236 config.py:54] PyTorch version 2.6.0 available.
W0320 23:34:29.283427 1532236 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 23:34:30.300077 1532236 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 23:34:30.304549 1531629 quantize_finetune_llama.py:203] layer 10 gpu 2
I0320 23:34:30.324754 1532236 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 23:34:43.752106 1532236 finetune.py:45] layer 9_v initial loss 0.0001108574797399342
W0320 23:34:43.752645 1532236 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 23:34:59.989798 1532182 finetune.py:68] layer 8_v @ epoch 2 new loss 3.4429802326485515e-05 old loss 3.79502926080022e-05 BETTER
I0320 23:35:15.859520 1532236 finetune.py:68] layer 9_v @ epoch 0 new loss 5.350653736968525e-05 old loss 0.0001108574797399342 BETTER
I0320 23:35:36.548838 1532182 finetune.py:68] layer 8_v @ epoch 3 new loss 3.2278650905936956e-05 old loss 3.4429802326485515e-05 BETTER
I0320 23:35:44.269402 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 10 in 73.46160316467285s
I0320 23:35:48.783187 1532290 config.py:54] PyTorch version 2.6.0 available.
W0320 23:35:49.267715 1532290 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0320 23:35:49.787995 1532236 finetune.py:68] layer 9_v @ epoch 1 new loss 4.4575932406587526e-05 old loss 5.350653736968525e-05 BETTER
W0320 23:35:50.378798 1532290 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 23:35:50.382577 1531629 quantize_finetune_llama.py:203] layer 11 gpu 3
I0320 23:35:50.397735 1532290 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 23:36:04.426820 1532290 finetune.py:45] layer 10_v initial loss 0.00015393424837384373
W0320 23:36:04.427126 1532290 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 23:36:13.878143 1532182 finetune.py:68] layer 8_v @ epoch 4 new loss 3.0787690775468946e-05 old loss 3.2278650905936956e-05 BETTER
I0320 23:36:24.522731 1532236 finetune.py:68] layer 9_v @ epoch 2 new loss 4.063844244228676e-05 old loss 4.4575932406587526e-05 BETTER
I0320 23:36:30.435459 1532182 finetune.py:45] layer 8_q initial loss 4.031362186651677e-05
I0320 23:36:38.827195 1532290 finetune.py:68] layer 10_v @ epoch 0 new loss 7.500359060941264e-05 old loss 0.00015393424837384373 BETTER
I0320 23:37:00.489180 1532236 finetune.py:68] layer 9_v @ epoch 3 new loss 3.82281796191819e-05 old loss 4.063844244228676e-05 BETTER
I0320 23:37:06.726079 1532182 finetune.py:68] layer 8_q @ epoch 0 new loss 3.5314213164383546e-05 old loss 4.031362186651677e-05 BETTER
I0320 23:37:11.421758 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 11 in 80.48503851890564s
I0320 23:37:14.537252 1532290 finetune.py:68] layer 10_v @ epoch 1 new loss 6.27770641585812e-05 old loss 7.500359060941264e-05 BETTER
I0320 23:37:15.840602 1532344 config.py:54] PyTorch version 2.6.0 available.
W0320 23:37:16.249631 1532344 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 23:37:17.467544 1532344 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 23:37:17.473975 1531629 quantize_finetune_llama.py:203] layer 12 gpu 0
I0320 23:37:17.490044 1532344 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 23:37:33.328303 1532344 finetune.py:45] layer 11_v initial loss 0.0001604733697604388
W0320 23:37:33.328716 1532344 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 23:37:37.136623 1532236 finetune.py:68] layer 9_v @ epoch 4 new loss 3.654035390354693e-05 old loss 3.82281796191819e-05 BETTER
I0320 23:37:43.222098 1532182 finetune.py:68] layer 8_q @ epoch 1 new loss 3.378715700819157e-05 old loss 3.5314213164383546e-05 BETTER
I0320 23:37:50.542843 1532290 finetune.py:68] layer 10_v @ epoch 2 new loss 5.713634891435504e-05 old loss 6.27770641585812e-05 BETTER
I0320 23:37:55.198664 1532236 finetune.py:45] layer 9_q initial loss 4.785502460435964e-05
I0320 23:38:08.017192 1532344 finetune.py:68] layer 11_v @ epoch 0 new loss 7.897328760009259e-05 old loss 0.0001604733697604388 BETTER
I0320 23:38:20.631356 1532182 finetune.py:68] layer 8_q @ epoch 2 new loss 3.271115565439686e-05 old loss 3.378715700819157e-05 BETTER
I0320 23:38:27.099349 1532290 finetune.py:68] layer 10_v @ epoch 3 new loss 5.366215918911621e-05 old loss 5.713634891435504e-05 BETTER
I0320 23:38:29.738999 1532236 finetune.py:68] layer 9_q @ epoch 0 new loss 4.205647564958781e-05 old loss 4.785502460435964e-05 BETTER
I0320 23:38:43.664475 1532344 finetune.py:68] layer 11_v @ epoch 1 new loss 6.618339102715254e-05 old loss 7.897328760009259e-05 BETTER
I0320 23:38:58.352579 1532182 finetune.py:68] layer 8_q @ epoch 3 new loss 3.185054083587602e-05 old loss 3.271115565439686e-05 BETTER
I0320 23:39:04.603939 1532290 finetune.py:68] layer 10_v @ epoch 4 new loss 5.127981421537697e-05 old loss 5.366215918911621e-05 BETTER
I0320 23:39:05.941328 1532236 finetune.py:68] layer 9_q @ epoch 1 new loss 4.03014782932587e-05 old loss 4.205647564958781e-05 BETTER
I0320 23:39:19.851324 1532344 finetune.py:68] layer 11_v @ epoch 2 new loss 6.033271347405389e-05 old loss 6.618339102715254e-05 BETTER
I0320 23:39:22.358569 1532290 finetune.py:45] layer 10_q initial loss 6.482350727310404e-05
I0320 23:39:36.803224 1532182 finetune.py:68] layer 8_q @ epoch 4 new loss 3.116063089692034e-05 old loss 3.185054083587602e-05 BETTER
I0320 23:39:42.067760 1532236 finetune.py:68] layer 9_q @ epoch 2 new loss 3.9047114114509895e-05 old loss 4.03014782932587e-05 BETTER
I0320 23:39:55.371106 1532182 finetune.py:45] layer 8_k initial loss 3.8513717299792916e-05
I0320 23:39:56.949524 1532344 finetune.py:68] layer 11_v @ epoch 3 new loss 5.6801538448780775e-05 old loss 6.033271347405389e-05 BETTER
I0320 23:39:57.886209 1532290 finetune.py:68] layer 10_q @ epoch 0 new loss 5.7618464779807255e-05 old loss 6.482350727310404e-05 BETTER
I0320 23:40:17.706430 1532236 finetune.py:68] layer 9_q @ epoch 3 new loss 3.8078604120528325e-05 old loss 3.9047114114509895e-05 BETTER
I0320 23:40:32.428328 1532182 finetune.py:68] layer 8_k @ epoch 0 new loss 3.5933509934693575e-05 old loss 3.8513717299792916e-05 BETTER
I0320 23:40:34.391100 1532344 finetune.py:68] layer 11_v @ epoch 4 new loss 5.426302595878951e-05 old loss 5.6801538448780775e-05 BETTER
I0320 23:40:35.255870 1532290 finetune.py:68] layer 10_q @ epoch 1 new loss 5.523893560166471e-05 old loss 5.7618464779807255e-05 BETTER
I0320 23:40:52.387290 1532344 finetune.py:45] layer 11_q initial loss 6.754809874109924e-05
I0320 23:40:54.070233 1532236 finetune.py:68] layer 9_q @ epoch 4 new loss 3.7272813642630354e-05 old loss 3.8078604120528325e-05 BETTER
I0320 23:41:10.249602 1532182 finetune.py:68] layer 8_k @ epoch 1 new loss 3.5212990042055026e-05 old loss 3.5933509934693575e-05 BETTER
I0320 23:41:11.768845 1532290 finetune.py:68] layer 10_q @ epoch 2 new loss 5.348346894606948e-05 old loss 5.523893560166471e-05 BETTER
I0320 23:41:12.267148 1532236 finetune.py:45] layer 9_k initial loss 4.731396256829612e-05
I0320 23:41:26.254284 1532344 finetune.py:68] layer 11_q @ epoch 0 new loss 6.15174722042866e-05 old loss 6.754809874109924e-05 BETTER
I0320 23:41:48.730911 1532236 finetune.py:68] layer 9_k @ epoch 0 new loss 4.3373154767323285e-05 old loss 4.731396256829612e-05 BETTER
I0320 23:41:49.111913 1532182 finetune.py:68] layer 8_k @ epoch 2 new loss 3.466523776296526e-05 old loss 3.5212990042055026e-05 BETTER
I0320 23:41:49.737454 1532290 finetune.py:68] layer 10_q @ epoch 3 new loss 5.206073183217086e-05 old loss 5.348346894606948e-05 BETTER
I0320 23:42:02.674322 1532344 finetune.py:68] layer 11_q @ epoch 1 new loss 5.91396055824589e-05 old loss 6.15174722042866e-05 BETTER
I0320 23:42:25.974284 1532236 finetune.py:68] layer 9_k @ epoch 1 new loss 4.250321944709867e-05 old loss 4.3373154767323285e-05 BETTER
I0320 23:42:27.282029 1532290 finetune.py:68] layer 10_q @ epoch 4 new loss 5.088869511382654e-05 old loss 5.206073183217086e-05 BETTER
I0320 23:42:27.334065 1532182 finetune.py:68] layer 8_k @ epoch 3 new loss 3.419487620703876e-05 old loss 3.466523776296526e-05 BETTER
I0320 23:42:37.791228 1532344 finetune.py:68] layer 11_q @ epoch 2 new loss 5.740356209571473e-05 old loss 5.91396055824589e-05 BETTER
I0320 23:42:45.087191 1532290 finetune.py:45] layer 10_k initial loss 6.408932677004486e-05
I0320 23:43:02.477228 1532236 finetune.py:68] layer 9_k @ epoch 2 new loss 4.183991768513806e-05 old loss 4.250321944709867e-05 BETTER
I0320 23:43:04.595515 1532182 finetune.py:68] layer 8_k @ epoch 4 new loss 3.38087702402845e-05 old loss 3.419487620703876e-05 BETTER
I0320 23:43:13.048617 1532344 finetune.py:68] layer 11_q @ epoch 3 new loss 5.61080414627213e-05 old loss 5.740356209571473e-05 BETTER
I0320 23:43:19.716288 1532290 finetune.py:68] layer 10_k @ epoch 0 new loss 5.8297115174354985e-05 old loss 6.408932677004486e-05 BETTER
I0320 23:43:23.419621 1532182 finetune.py:45] layer 8_o initial loss 9.316467912867665e-05
I0320 23:43:38.052079 1532236 finetune.py:68] layer 9_k @ epoch 3 new loss 4.129721855861135e-05 old loss 4.183991768513806e-05 BETTER
I0320 23:43:48.538693 1532344 finetune.py:68] layer 11_q @ epoch 4 new loss 5.517990575754084e-05 old loss 5.61080414627213e-05 BETTER
I0320 23:43:55.126859 1532290 finetune.py:68] layer 10_k @ epoch 1 new loss 5.7074823416769505e-05 old loss 5.8297115174354985e-05 BETTER
I0320 23:43:57.592211 1532182 finetune.py:68] layer 8_o @ epoch 0 new loss 8.569136844016612e-05 old loss 9.316467912867665e-05 BETTER
I0320 23:44:07.789638 1532344 finetune.py:45] layer 11_k initial loss 6.562807539012283e-05
I0320 23:44:13.368179 1532236 finetune.py:68] layer 9_k @ epoch 4 new loss 4.08407358918339e-05 old loss 4.129721855861135e-05 BETTER
I0320 23:44:31.770050 1532236 finetune.py:45] layer 9_o initial loss 0.00011486428411444649
I0320 23:44:32.624571 1532290 finetune.py:68] layer 10_k @ epoch 2 new loss 5.613629400613718e-05 old loss 5.7074823416769505e-05 BETTER
I0320 23:44:35.226382 1532182 finetune.py:68] layer 8_o @ epoch 1 new loss 8.231170795625076e-05 old loss 8.569136844016612e-05 BETTER
I0320 23:44:41.689874 1532344 finetune.py:68] layer 11_k @ epoch 0 new loss 6.287305586738512e-05 old loss 6.562807539012283e-05 BETTER
I0320 23:45:06.550695 1532236 finetune.py:68] layer 9_o @ epoch 0 new loss 0.00010570748418103904 old loss 0.00011486428411444649 BETTER
I0320 23:45:09.496136 1532290 finetune.py:68] layer 10_k @ epoch 3 new loss 5.534603769774549e-05 old loss 5.613629400613718e-05 BETTER
I0320 23:45:12.236390 1532182 finetune.py:68] layer 8_o @ epoch 2 new loss 8.000257366802543e-05 old loss 8.231170795625076e-05 BETTER
I0320 23:45:17.200477 1532344 finetune.py:68] layer 11_k @ epoch 1 new loss 6.182167999213561e-05 old loss 6.287305586738512e-05 BETTER
I0320 23:45:42.443245 1532236 finetune.py:68] layer 9_o @ epoch 1 new loss 0.00010128408757736906 old loss 0.00010570748418103904 BETTER
I0320 23:45:46.879789 1532290 finetune.py:68] layer 10_k @ epoch 4 new loss 5.468678500619717e-05 old loss 5.534603769774549e-05 BETTER
I0320 23:45:49.149152 1532182 finetune.py:68] layer 8_o @ epoch 3 new loss 7.825665670679882e-05 old loss 8.000257366802543e-05 BETTER
I0320 23:45:52.519695 1532344 finetune.py:68] layer 11_k @ epoch 2 new loss 6.102544648456387e-05 old loss 6.182167999213561e-05 BETTER
I0320 23:46:04.572135 1532290 finetune.py:45] layer 10_o initial loss 0.00015737488865852356
I0320 23:46:17.938274 1532236 finetune.py:68] layer 9_o @ epoch 2 new loss 9.832176147028804e-05 old loss 0.00010128408757736906 BETTER
I0320 23:46:26.001935 1532182 finetune.py:68] layer 8_o @ epoch 4 new loss 7.68563331803307e-05 old loss 7.825665670679882e-05 BETTER
I0320 23:46:27.946381 1532344 finetune.py:68] layer 11_k @ epoch 3 new loss 6.035737897036597e-05 old loss 6.102544648456387e-05 BETTER
I0320 23:46:38.772800 1532290 finetune.py:68] layer 10_o @ epoch 0 new loss 0.00014508435560856014 old loss 0.00015737488865852356 BETTER
I0320 23:46:53.123324 1532236 finetune.py:68] layer 9_o @ epoch 3 new loss 9.608273830963299e-05 old loss 9.832176147028804e-05 BETTER
I0320 23:46:55.930581 1532182 finetune.py:45] layer 8_up initial loss 0.00012128110392950475
I0320 23:47:03.702850 1532344 finetune.py:68] layer 11_k @ epoch 4 new loss 5.976183092570864e-05 old loss 6.035737897036597e-05 BETTER
I0320 23:47:13.299279 1532290 finetune.py:68] layer 10_o @ epoch 1 new loss 0.00013903126819059253 old loss 0.00014508435560856014 BETTER
I0320 23:47:21.441988 1532344 finetune.py:45] layer 11_o initial loss 0.00016638678789604455
I0320 23:47:29.072756 1532236 finetune.py:68] layer 9_o @ epoch 4 new loss 9.42985134315677e-05 old loss 9.608273830963299e-05 BETTER
I0320 23:47:30.077510 1532182 finetune.py:68] layer 8_up @ epoch 0 new loss 0.000117685784061905 old loss 0.00012128110392950475 BETTER
I0320 23:47:48.913881 1532290 finetune.py:68] layer 10_o @ epoch 2 new loss 0.00013484655937645584 old loss 0.00013903126819059253 BETTER
I0320 23:47:54.905350 1532344 finetune.py:68] layer 11_o @ epoch 0 new loss 0.0001531574089312926 old loss 0.00016638678789604455 BETTER
I0320 23:47:59.350872 1532236 finetune.py:45] layer 9_up initial loss 0.0001461927458876744
I0320 23:48:04.895688 1532182 finetune.py:68] layer 8_up @ epoch 1 new loss 0.00011576720862649381 old loss 0.000117685784061905 BETTER
I0320 23:48:25.695058 1532290 finetune.py:68] layer 10_o @ epoch 3 new loss 0.0001316468114964664 old loss 0.00013484655937645584 BETTER
I0320 23:48:31.216671 1532344 finetune.py:68] layer 11_o @ epoch 1 new loss 0.00014698650920763612 old loss 0.0001531574089312926 BETTER
I0320 23:48:32.245249 1532236 finetune.py:68] layer 9_up @ epoch 0 new loss 0.00014186058251652867 old loss 0.0001461927458876744 BETTER
I0320 23:48:39.913532 1532182 finetune.py:68] layer 8_up @ epoch 2 new loss 0.000114260139525868 old loss 0.00011576720862649381 BETTER
I0320 23:49:02.244125 1532290 finetune.py:68] layer 10_o @ epoch 4 new loss 0.0001290725194849074 old loss 0.0001316468114964664 BETTER
I0320 23:49:06.549490 1532344 finetune.py:68] layer 11_o @ epoch 2 new loss 0.00014283369819168001 old loss 0.00014698650920763612 BETTER
I0320 23:49:06.729035 1532236 finetune.py:68] layer 9_up @ epoch 1 new loss 0.00013945891987532377 old loss 0.00014186058251652867 BETTER
I0320 23:49:14.884888 1532182 finetune.py:68] layer 8_up @ epoch 3 new loss 0.00011299647303530946 old loss 0.000114260139525868 BETTER
I0320 23:49:32.088150 1532290 finetune.py:45] layer 10_up initial loss 0.0001878648326965049
I0320 23:49:41.285752 1532236 finetune.py:68] layer 9_up @ epoch 2 new loss 0.00013758709246758372 old loss 0.00013945891987532377 BETTER
I0320 23:49:41.401422 1532344 finetune.py:68] layer 11_o @ epoch 3 new loss 0.00013973534805700183 old loss 0.00014283369819168001 BETTER
I0320 23:49:50.696738 1532182 finetune.py:68] layer 8_up @ epoch 4 new loss 0.0001118895597755909 old loss 0.00011299647303530946 BETTER
I0320 23:50:04.304705 1532290 finetune.py:68] layer 10_up @ epoch 0 new loss 0.00018234533490613103 old loss 0.0001878648326965049 BETTER
I0320 23:50:15.317499 1532236 finetune.py:68] layer 9_up @ epoch 3 new loss 0.00013602228136733174 old loss 0.00013758709246758372 BETTER
I0320 23:50:16.353487 1532344 finetune.py:68] layer 11_o @ epoch 4 new loss 0.00013720618153456599 old loss 0.00013973534805700183 BETTER
I0320 23:50:21.298704 1532182 finetune.py:45] layer 8_gate initial loss 0.00013905878586228937
I0320 23:50:38.063285 1532290 finetune.py:68] layer 10_up @ epoch 1 new loss 0.00017928457236848772 old loss 0.00018234533490613103 BETTER
I0320 23:50:45.896279 1532344 finetune.py:45] layer 11_up initial loss 0.00020351487910375
I0320 23:50:50.327285 1532236 finetune.py:68] layer 9_up @ epoch 4 new loss 0.00013466665404848754 old loss 0.00013602228136733174 BETTER
I0320 23:50:52.975034 1532182 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.00013751073856838048 old loss 0.00013905878586228937 BETTER
I0320 23:51:12.076121 1532290 finetune.py:68] layer 10_up @ epoch 2 new loss 0.00017685354396235198 old loss 0.00017928457236848772 BETTER
I0320 23:51:18.210967 1532344 finetune.py:68] layer 11_up @ epoch 0 new loss 0.0001977347710635513 old loss 0.00020351487910375 BETTER
I0320 23:51:22.145243 1532236 finetune.py:45] layer 9_gate initial loss 0.00016763867461122572
I0320 23:51:26.848228 1532182 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.00013644016871694475 old loss 0.00013751073856838048 BETTER
I0320 23:51:47.038100 1532290 finetune.py:68] layer 10_up @ epoch 3 new loss 0.00017480456153862178 old loss 0.00017685354396235198 BETTER
I0320 23:51:52.470160 1532344 finetune.py:68] layer 11_up @ epoch 1 new loss 0.00019456385052762926 old loss 0.0001977347710635513 BETTER
I0320 23:51:53.504819 1532236 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.00016571332525927573 old loss 0.00016763867461122572 BETTER
I0320 23:52:00.034666 1532182 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.00013555304030887783 old loss 0.00013644016871694475 BETTER
I0320 23:52:22.722032 1532290 finetune.py:68] layer 10_up @ epoch 4 new loss 0.00017302960623055696 old loss 0.00017480456153862178 BETTER
I0320 23:52:26.539332 1532236 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.0001644038420636207 old loss 0.00016571332525927573 BETTER
I0320 23:52:26.760009 1532344 finetune.py:68] layer 11_up @ epoch 2 new loss 0.0001921007060445845 old loss 0.00019456385052762926 BETTER
I0320 23:52:33.600035 1532182 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.0001347819488728419 old loss 0.00013555304030887783 BETTER
I0320 23:52:53.656518 1532290 finetune.py:45] layer 10_gate initial loss 0.0002125575119862333
I0320 23:52:59.418188 1532236 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.00016331375809386373 old loss 0.0001644038420636207 BETTER
I0320 23:53:00.738264 1532344 finetune.py:68] layer 11_up @ epoch 3 new loss 0.00019000483734998852 old loss 0.0001921007060445845 BETTER
I0320 23:53:06.719508 1532182 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.0001340869057457894 old loss 0.0001347819488728419 BETTER
I0320 23:53:23.995648 1532290 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.00021003045549150556 old loss 0.0002125575119862333 BETTER
I0320 23:53:31.897150 1532236 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.0001623532152734697 old loss 0.00016331375809386373 BETTER
I0320 23:53:34.420496 1532344 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00018818819080479443 old loss 0.00019000483734998852 BETTER
I0320 23:53:39.858541 1532182 finetune.py:45] layer 8_down initial loss 0.00020264268096070737
I0320 23:53:55.530075 1532290 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.00020834003225900233 old loss 0.00021003045549150556 BETTER
I0320 23:54:04.590869 1532236 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0001615065266378224 old loss 0.0001623532152734697 BETTER
I0320 23:54:05.547798 1532344 finetune.py:45] layer 11_gate initial loss 0.00023253810650203377
I0320 23:54:10.030897 1532182 finetune.py:68] layer 8_down @ epoch 0 new loss 0.00020261119061615318 old loss 0.00020264268096070737 BETTER
I0320 23:54:27.565255 1532290 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.00020691101963166147 old loss 0.00020834003225900233 BETTER
I0320 23:54:36.827965 1532344 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.0002299523475812748 old loss 0.00023253810650203377 BETTER
I0320 23:54:39.030765 1532236 finetune.py:45] layer 9_down initial loss 0.00024113638210110366
I0320 23:54:42.451061 1532182 finetune.py:68] layer 8_down @ epoch 1 new loss 0.0002025961148319766 old loss 0.00020261119061615318 BETTER
I0320 23:55:01.449334 1532290 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.0002056745142908767 old loss 0.00020691101963166147 BETTER
I0320 23:55:09.438738 1532236 finetune.py:68] layer 9_down @ epoch 0 new loss 0.00024110483354888856 old loss 0.00024113638210110366 BETTER
I0320 23:55:10.188626 1532344 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.00022822749451734126 old loss 0.0002299523475812748 BETTER
I0320 23:55:15.027489 1532182 finetune.py:68] layer 8_down @ epoch 2 new loss 0.0002025875583058223 old loss 0.0002025961148319766 BETTER
I0320 23:55:35.052622 1532290 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.00020457150822039694 old loss 0.0002056745142908767 BETTER
I0320 23:55:40.579328 1532236 finetune.py:68] layer 9_down @ epoch 1 new loss 0.00024108709476422518 old loss 0.00024110483354888856 BETTER
I0320 23:55:41.950065 1532344 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.00022676675871480256 old loss 0.00022822749451734126 BETTER
I0320 23:55:47.009735 1532182 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0002025809808401391 old loss 0.0002025875583058223 BETTER
I0320 23:56:09.400824 1532290 finetune.py:45] layer 10_down initial loss 0.0002967572945635766
I0320 23:56:12.522566 1532236 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00024107701028697193 old loss 0.00024108709476422518 BETTER
I0320 23:56:14.981562 1532344 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.00022549905406776816 old loss 0.00022676675871480256 BETTER
I0320 23:56:19.276523 1532182 finetune.py:68] layer 8_down @ epoch 4 new loss 0.00020257511641830206 old loss 0.0002025809808401391 BETTER
8_v proxy err 0.012079006992280483 tr(WHW.T) 530.9967041015625
bpp_loss 3.488605260848999
8_q proxy err 0.0011042403057217598 tr(WHW.T) 7235.314453125
bpp_loss 4.228878021240234
8_k proxy err 0.0007598486263304949 tr(WHW.T) 10658.125
bpp_loss 4.2394185066223145
8_o proxy err 0.013239695690572262 tr(WHW.T) 20.22292709350586
bpp_loss 3.4742250442504883
8_up proxy err 0.006037752144038677 tr(WHW.T) 866.4569091796875
bpp_loss 3.71224532016488
8_gate proxy err 0.002776016714051366 tr(WHW.T) 1970.8406982421875
bpp_loss 3.837012002634448
8_down proxy err 0.007741833571344614 tr(WHW.T) 37.32512283325195
bpp_loss 3.7035359670949535
I0320 23:56:40.955509 1532290 finetune.py:68] layer 10_down @ epoch 0 new loss 0.00029670418007299304 old loss 0.0002967572945635766 BETTER
I0320 23:56:45.377512 1532236 finetune.py:68] layer 9_down @ epoch 3 new loss 0.00024106980708893389 old loss 0.00024107701028697193 BETTER
I0320 23:56:48.807468 1532344 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.00022439366148319095 old loss 0.00022549905406776816 BETTER
I0320 23:57:10.612790 1532290 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0002966752799693495 old loss 0.00029670418007299304 BETTER
I0320 23:57:15.304093 1532236 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00024106379714794457 old loss 0.00024106980708893389 BETTER
9_v proxy err 0.011371701955795288 tr(WHW.T) 565.0663452148438
bpp_loss 3.5060571432113647
9_q proxy err 0.0011397955240681767 tr(WHW.T) 6974.95849609375
bpp_loss 4.231244087219238
9_k proxy err 0.0007367400685325265 tr(WHW.T) 11004.34765625
bpp_loss 4.276098966598511
9_o proxy err 0.013196355663239956 tr(WHW.T) 25.7464542388916
bpp_loss 3.493689179420471
9_up proxy err 0.005830372218042612 tr(WHW.T) 970.7380981445312
bpp_loss 3.7202946862509085
9_gate proxy err 0.0027605793438851833 tr(WHW.T) 2132.4365234375
bpp_loss 3.8241332298101383
9_down proxy err 0.00776748126372695 tr(WHW.T) 43.14749526977539
bpp_loss 3.711713724358137
I0320 23:57:19.235746 1532344 finetune.py:45] layer 11_down initial loss 0.00032604989246465266
I0320 23:57:40.579577 1532290 finetune.py:68] layer 10_down @ epoch 2 new loss 0.00029666084446944296 old loss 0.0002966752799693495 BETTER
I0320 23:57:46.934312 1532344 finetune.py:68] layer 11_down @ epoch 0 new loss 0.00032599602127447724 old loss 0.00032604989246465266 BETTER
I0320 23:58:10.175363 1532290 finetune.py:68] layer 10_down @ epoch 3 new loss 0.000296650396194309 old loss 0.00029666084446944296 BETTER
I0320 23:58:15.463694 1532344 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0003259694785811007 old loss 0.00032599602127447724 BETTER
I0320 23:58:39.790489 1532290 finetune.py:68] layer 10_down @ epoch 4 new loss 0.00029664288740605116 old loss 0.000296650396194309 BETTER
I0320 23:58:40.272112 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 12 in 77.43934082984924s
10_v proxy err 0.011494613252580166 tr(WHW.T) 578.807373046875
bpp_loss 3.4970861673355103
10_q proxy err 0.0011779465712606907 tr(WHW.T) 6919.88671875
bpp_loss 4.228840351104736
10_k proxy err 0.0007677818648517132 tr(WHW.T) 11013.6767578125
bpp_loss 4.283559560775757
10_o proxy err 0.013592629693448544 tr(WHW.T) 35.3609733581543
bpp_loss 3.4901102781295776
10_up proxy err 0.005512581672519445 tr(WHW.T) 1079.93896484375
bpp_loss 3.7329218664834665
10_gate proxy err 0.002729804953560233 tr(WHW.T) 2260.345947265625
bpp_loss 3.820006791935411
10_down proxy err 0.007403098046779633 tr(WHW.T) 52.578731536865234
bpp_loss 3.7218886752461278
I0320 23:58:43.900401 1532344 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0003259551012888551 old loss 0.0003259694785811007 BETTER
I0320 23:58:44.298871 1532398 config.py:54] PyTorch version 2.6.0 available.
W0320 23:58:44.606566 1532398 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0320 23:58:45.670016 1532398 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0320 23:58:45.673961 1531629 quantize_finetune_llama.py:203] layer 13 gpu 1
I0320 23:58:45.688183 1532398 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0320 23:58:59.156417 1532398 finetune.py:45] layer 12_v initial loss 0.00016738144040573388
W0320 23:58:59.156714 1532398 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0320 23:59:12.446106 1532344 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0003259431105107069 old loss 0.0003259551012888551 BETTER
I0320 23:59:34.227494 1532398 finetune.py:68] layer 12_v @ epoch 0 new loss 8.266291115432978e-05 old loss 0.00016738144040573388 BETTER
I0320 23:59:41.169448 1532344 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0003259344375692308 old loss 0.0003259431105107069 BETTER
11_v proxy err 0.010858329944312572 tr(WHW.T) 723.1956176757812
bpp_loss 3.535570502281189
11_q proxy err 0.0013331706868484616 tr(WHW.T) 7032.1611328125
bpp_loss 4.140434265136719
11_k proxy err 0.0009103913907893002 tr(WHW.T) 10528.255859375
bpp_loss 4.135746240615845
11_o proxy err 0.013740943744778633 tr(WHW.T) 36.8162956237793
bpp_loss 3.526512026786804
11_up proxy err 0.005612241104245186 tr(WHW.T) 1140.126708984375
bpp_loss 3.7431958220725834
11_gate proxy err 0.0027525965124368668 tr(WHW.T) 2396.47021484375
bpp_loss 3.815196015114008
11_down proxy err 0.007587254513055086 tr(WHW.T) 56.38547134399414
bpp_loss 3.729678908059763
I0320 23:59:56.553869 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 13 in 70.41246461868286s
I0321 00:00:00.289681 1532452 config.py:54] PyTorch version 2.6.0 available.
W0321 00:00:00.645209 1532452 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 00:00:01.719962 1532452 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:00:01.724018 1531629 quantize_finetune_llama.py:203] layer 14 gpu 2
I0321 00:00:01.738613 1532452 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:00:10.440798 1532398 finetune.py:68] layer 12_v @ epoch 1 new loss 7.027304673101753e-05 old loss 8.266291115432978e-05 BETTER
I0321 00:00:16.357165 1532452 finetune.py:45] layer 13_v initial loss 0.00015774517669342458
W0321 00:00:16.357442 1532452 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:00:47.649854 1532398 finetune.py:68] layer 12_v @ epoch 2 new loss 6.461442535510287e-05 old loss 7.027304673101753e-05 BETTER
I0321 00:00:50.434340 1532452 finetune.py:68] layer 13_v @ epoch 0 new loss 8.143946615746245e-05 old loss 0.00015774517669342458 BETTER
I0321 00:01:18.480899 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 14 in 76.2850193977356s
I0321 00:01:23.047250 1532506 config.py:54] PyTorch version 2.6.0 available.
W0321 00:01:23.541566 1532506 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 00:01:24.673196 1532506 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:01:24.678299 1531629 quantize_finetune_llama.py:203] layer 15 gpu 3
I0321 00:01:24.696299 1532506 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0321 00:01:25.248434 1532398 finetune.py:68] layer 12_v @ epoch 3 new loss 6.103799387346953e-05 old loss 6.461442535510287e-05 BETTER
I0321 00:01:25.696343 1532452 finetune.py:68] layer 13_v @ epoch 1 new loss 7.027730316622183e-05 old loss 8.143946615746245e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:01:39.151971 1532506 finetune.py:45] layer 14_v initial loss 0.0002092529321089387
W0321 00:01:39.153520 1532506 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:02:00.667659 1532452 finetune.py:68] layer 13_v @ epoch 2 new loss 6.50124202365987e-05 old loss 7.027730316622183e-05 BETTER
I0321 00:02:02.646969 1532398 finetune.py:68] layer 12_v @ epoch 4 new loss 5.8499961596680805e-05 old loss 6.103799387346953e-05 BETTER
I0321 00:02:13.138319 1532506 finetune.py:68] layer 14_v @ epoch 0 new loss 0.00010758308053482324 old loss 0.0002092529321089387 BETTER
I0321 00:02:19.307358 1532398 finetune.py:45] layer 12_q initial loss 7.345824997173622e-05
I0321 00:02:37.021393 1532452 finetune.py:68] layer 13_v @ epoch 3 new loss 6.164739170344546e-05 old loss 6.50124202365987e-05 BETTER
I0321 00:02:44.663032 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 15 in 79.39679336547852s
I0321 00:02:49.221469 1532506 finetune.py:68] layer 14_v @ epoch 1 new loss 9.259532816940919e-05 old loss 0.00010758308053482324 BETTER
I0321 00:02:49.281694 1532560 config.py:54] PyTorch version 2.6.0 available.
W0321 00:02:49.734930 1532560 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 00:02:51.010231 1532560 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:02:51.015290 1531629 quantize_finetune_llama.py:203] layer 16 gpu 0
I0321 00:02:51.044335 1532560 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0321 00:02:54.850512 1532398 finetune.py:68] layer 12_q @ epoch 0 new loss 6.709533045068383e-05 old loss 7.345824997173622e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:03:07.896148 1532560 finetune.py:45] layer 15_v initial loss 0.00020385788229759783
W0321 00:03:07.897022 1532560 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:03:13.232985 1532452 finetune.py:68] layer 13_v @ epoch 4 new loss 5.923577555222437e-05 old loss 6.164739170344546e-05 BETTER
I0321 00:03:25.220359 1532506 finetune.py:68] layer 14_v @ epoch 2 new loss 8.538275142200291e-05 old loss 9.259532816940919e-05 BETTER
I0321 00:03:31.695884 1532452 finetune.py:45] layer 13_q initial loss 7.546667620772496e-05
I0321 00:03:32.900525 1532398 finetune.py:68] layer 12_q @ epoch 1 new loss 6.453747482737526e-05 old loss 6.709533045068383e-05 BETTER
I0321 00:03:42.450293 1532560 finetune.py:68] layer 15_v @ epoch 0 new loss 0.00010347668285248801 old loss 0.00020385788229759783 BETTER
I0321 00:04:02.746088 1532506 finetune.py:68] layer 14_v @ epoch 3 new loss 8.074499783106148e-05 old loss 8.538275142200291e-05 BETTER
I0321 00:04:07.056951 1532452 finetune.py:68] layer 13_q @ epoch 0 new loss 6.79661170579493e-05 old loss 7.546667620772496e-05 BETTER
I0321 00:04:10.766491 1532398 finetune.py:68] layer 12_q @ epoch 2 new loss 6.265981937758625e-05 old loss 6.453747482737526e-05 BETTER
I0321 00:04:18.379003 1532560 finetune.py:68] layer 15_v @ epoch 1 new loss 8.984816668089479e-05 old loss 0.00010347668285248801 BETTER
I0321 00:04:40.401641 1532506 finetune.py:68] layer 14_v @ epoch 4 new loss 7.748731877654791e-05 old loss 8.074499783106148e-05 BETTER
I0321 00:04:43.173486 1532452 finetune.py:68] layer 13_q @ epoch 1 new loss 6.549023237312213e-05 old loss 6.79661170579493e-05 BETTER
I0321 00:04:47.807502 1532398 finetune.py:68] layer 12_q @ epoch 3 new loss 6.117701559560373e-05 old loss 6.265981937758625e-05 BETTER
I0321 00:04:53.397271 1532560 finetune.py:68] layer 15_v @ epoch 2 new loss 8.315320883411914e-05 old loss 8.984816668089479e-05 BETTER
I0321 00:05:00.205803 1532506 finetune.py:45] layer 14_q initial loss 9.504538320470601e-05
I0321 00:05:19.079359 1532452 finetune.py:68] layer 13_q @ epoch 2 new loss 6.366216985043138e-05 old loss 6.549023237312213e-05 BETTER
I0321 00:05:26.283692 1532398 finetune.py:68] layer 12_q @ epoch 4 new loss 5.995696483296342e-05 old loss 6.117701559560373e-05 BETTER
I0321 00:05:29.769912 1532560 finetune.py:68] layer 15_v @ epoch 3 new loss 7.883268699515611e-05 old loss 8.315320883411914e-05 BETTER
I0321 00:05:34.496803 1532506 finetune.py:68] layer 14_q @ epoch 0 new loss 8.722036727704108e-05 old loss 9.504538320470601e-05 BETTER
I0321 00:05:44.552660 1532398 finetune.py:45] layer 12_k initial loss 7.301295408979058e-05
I0321 00:05:55.132304 1532452 finetune.py:68] layer 13_q @ epoch 3 new loss 6.223531090654433e-05 old loss 6.366216985043138e-05 BETTER
I0321 00:06:06.020971 1532560 finetune.py:68] layer 15_v @ epoch 4 new loss 7.56576846470125e-05 old loss 7.883268699515611e-05 BETTER
I0321 00:06:11.016863 1532506 finetune.py:68] layer 14_q @ epoch 1 new loss 8.387559500988573e-05 old loss 8.722036727704108e-05 BETTER
I0321 00:06:20.241521 1532398 finetune.py:68] layer 12_k @ epoch 0 new loss 6.853674130979925e-05 old loss 7.301295408979058e-05 BETTER
I0321 00:06:24.045933 1532560 finetune.py:45] layer 15_q initial loss 9.629393753129989e-05
I0321 00:06:31.209630 1532452 finetune.py:68] layer 13_q @ epoch 4 new loss 6.107559602241963e-05 old loss 6.223531090654433e-05 BETTER
I0321 00:06:48.110601 1532506 finetune.py:68] layer 14_q @ epoch 2 new loss 8.144890307448804e-05 old loss 8.387559500988573e-05 BETTER
I0321 00:06:49.362679 1532452 finetune.py:45] layer 13_k initial loss 7.433947757817805e-05
I0321 00:06:57.647712 1532398 finetune.py:68] layer 12_k @ epoch 1 new loss 6.745647988282144e-05 old loss 6.853674130979925e-05 BETTER
I0321 00:06:59.072354 1532560 finetune.py:68] layer 15_q @ epoch 0 new loss 8.680132305016741e-05 old loss 9.629393753129989e-05 BETTER
I0321 00:07:24.740717 1532452 finetune.py:68] layer 13_k @ epoch 0 new loss 6.99357406119816e-05 old loss 7.433947757817805e-05 BETTER
I0321 00:07:25.494966 1532506 finetune.py:68] layer 14_q @ epoch 3 new loss 7.950373401399702e-05 old loss 8.144890307448804e-05 BETTER
I0321 00:07:36.097871 1532398 finetune.py:68] layer 12_k @ epoch 2 new loss 6.654160824837163e-05 old loss 6.745647988282144e-05 BETTER
I0321 00:07:36.262278 1532560 finetune.py:68] layer 15_q @ epoch 1 new loss 8.342808723682538e-05 old loss 8.680132305016741e-05 BETTER
I0321 00:08:01.486923 1532452 finetune.py:68] layer 13_k @ epoch 1 new loss 6.884267349960282e-05 old loss 6.99357406119816e-05 BETTER
I0321 00:08:02.865908 1532506 finetune.py:68] layer 14_q @ epoch 4 new loss 7.78991452534683e-05 old loss 7.950373401399702e-05 BETTER
I0321 00:08:11.826610 1532560 finetune.py:68] layer 15_q @ epoch 2 new loss 8.093820360954851e-05 old loss 8.342808723682538e-05 BETTER
I0321 00:08:13.007605 1532398 finetune.py:68] layer 12_k @ epoch 3 new loss 6.576521991519257e-05 old loss 6.654160824837163e-05 BETTER
I0321 00:08:21.322962 1532506 finetune.py:45] layer 14_k initial loss 9.27072178456001e-05
I0321 00:08:36.751798 1532452 finetune.py:68] layer 13_k @ epoch 2 new loss 6.793326610932127e-05 old loss 6.884267349960282e-05 BETTER
I0321 00:08:47.805007 1532560 finetune.py:68] layer 15_q @ epoch 3 new loss 7.899056072346866e-05 old loss 8.093820360954851e-05 BETTER
I0321 00:08:50.919101 1532398 finetune.py:68] layer 12_k @ epoch 4 new loss 6.508332444354892e-05 old loss 6.576521991519257e-05 BETTER
I0321 00:08:55.864853 1532506 finetune.py:68] layer 14_k @ epoch 0 new loss 8.875148341758177e-05 old loss 9.27072178456001e-05 BETTER
I0321 00:09:09.992697 1532398 finetune.py:45] layer 12_o initial loss 0.00018276978516951203
I0321 00:09:12.110709 1532452 finetune.py:68] layer 13_k @ epoch 3 new loss 6.721389945596457e-05 old loss 6.793326610932127e-05 BETTER
I0321 00:09:23.294147 1532560 finetune.py:68] layer 15_q @ epoch 4 new loss 7.736271800240502e-05 old loss 7.899056072346866e-05 BETTER
I0321 00:09:30.636976 1532506 finetune.py:68] layer 14_k @ epoch 1 new loss 8.723959763301536e-05 old loss 8.875148341758177e-05 BETTER
I0321 00:09:41.015497 1532560 finetune.py:45] layer 15_k initial loss 9.332483023172244e-05
I0321 00:09:45.558663 1532398 finetune.py:68] layer 12_o @ epoch 0 new loss 0.00016862682241480798 old loss 0.00018276978516951203 BETTER
I0321 00:09:47.983426 1532452 finetune.py:68] layer 13_k @ epoch 4 new loss 6.654854951193556e-05 old loss 6.721389945596457e-05 BETTER
I0321 00:10:06.188329 1532452 finetune.py:45] layer 13_o initial loss 0.00018590503896120936
I0321 00:10:07.729147 1532506 finetune.py:68] layer 14_k @ epoch 2 new loss 8.602122397860512e-05 old loss 8.723959763301536e-05 BETTER
I0321 00:10:15.541806 1532560 finetune.py:68] layer 15_k @ epoch 0 new loss 8.882502879714593e-05 old loss 9.332483023172244e-05 BETTER
I0321 00:10:22.019676 1532398 finetune.py:68] layer 12_o @ epoch 1 new loss 0.0001618598762433976 old loss 0.00016862682241480798 BETTER
I0321 00:10:40.633465 1532452 finetune.py:68] layer 13_o @ epoch 0 new loss 0.00017035978089552373 old loss 0.00018590503896120936 BETTER
I0321 00:10:43.728828 1532506 finetune.py:68] layer 14_k @ epoch 3 new loss 8.499059185851365e-05 old loss 8.602122397860512e-05 BETTER
I0321 00:10:50.380016 1532560 finetune.py:68] layer 15_k @ epoch 1 new loss 8.731975685805082e-05 old loss 8.882502879714593e-05 BETTER
I0321 00:10:57.464189 1532398 finetune.py:68] layer 12_o @ epoch 2 new loss 0.00015722832176834345 old loss 0.0001618598762433976 BETTER
I0321 00:11:15.987633 1532452 finetune.py:68] layer 13_o @ epoch 1 new loss 0.00016313882952090353 old loss 0.00017035978089552373 BETTER
I0321 00:11:20.267724 1532506 finetune.py:68] layer 14_k @ epoch 4 new loss 8.411726594204083e-05 old loss 8.499059185851365e-05 BETTER
I0321 00:11:24.999453 1532560 finetune.py:68] layer 15_k @ epoch 2 new loss 8.612649980932474e-05 old loss 8.731975685805082e-05 BETTER
I0321 00:11:33.453276 1532398 finetune.py:68] layer 12_o @ epoch 3 new loss 0.00015375092334579676 old loss 0.00015722832176834345 BETTER
I0321 00:11:38.809172 1532506 finetune.py:45] layer 14_o initial loss 0.0002343416417716071
I0321 00:11:51.079724 1532452 finetune.py:68] layer 13_o @ epoch 2 new loss 0.00015828954929020256 old loss 0.00016313882952090353 BETTER
I0321 00:12:00.561383 1532560 finetune.py:68] layer 15_k @ epoch 3 new loss 8.512449858244509e-05 old loss 8.612649980932474e-05 BETTER
I0321 00:12:10.370983 1532398 finetune.py:68] layer 12_o @ epoch 4 new loss 0.00015101458120625466 old loss 0.00015375092334579676 BETTER
I0321 00:12:13.131645 1532506 finetune.py:68] layer 14_o @ epoch 0 new loss 0.0002170150983147323 old loss 0.0002343416417716071 BETTER
I0321 00:12:25.465519 1532452 finetune.py:68] layer 13_o @ epoch 3 new loss 0.00015464823809452355 old loss 0.00015828954929020256 BETTER
I0321 00:12:35.616157 1532560 finetune.py:68] layer 15_k @ epoch 4 new loss 8.428323781117797e-05 old loss 8.512449858244509e-05 BETTER
I0321 00:12:39.380119 1532398 finetune.py:45] layer 12_up initial loss 0.00022517962497659028
I0321 00:12:47.685960 1532506 finetune.py:68] layer 14_o @ epoch 1 new loss 0.0002087982138618827 old loss 0.0002170150983147323 BETTER
I0321 00:12:54.441226 1532560 finetune.py:45] layer 15_o initial loss 0.0002362580125918612
I0321 00:13:00.838024 1532452 finetune.py:68] layer 13_o @ epoch 4 new loss 0.00015179281763266772 old loss 0.00015464823809452355 BETTER
I0321 00:13:12.275622 1532398 finetune.py:68] layer 12_up @ epoch 0 new loss 0.00021891137294005603 old loss 0.00022517962497659028 BETTER
I0321 00:13:24.042261 1532506 finetune.py:68] layer 14_o @ epoch 2 new loss 0.0002032006304943934 old loss 0.0002087982138618827 BETTER
I0321 00:13:27.979990 1532560 finetune.py:68] layer 15_o @ epoch 0 new loss 0.00021450090571306646 old loss 0.0002362580125918612 BETTER
I0321 00:13:30.461608 1532452 finetune.py:45] layer 13_up initial loss 0.0002404615079285577
I0321 00:13:47.607362 1532398 finetune.py:68] layer 12_up @ epoch 1 new loss 0.00021540063607972115 old loss 0.00021891137294005603 BETTER
I0321 00:14:00.980197 1532506 finetune.py:68] layer 14_o @ epoch 3 new loss 0.00019890512339770794 old loss 0.0002032006304943934 BETTER
I0321 00:14:03.508128 1532452 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00023252348182722926 old loss 0.0002404615079285577 BETTER
I0321 00:14:03.752172 1532560 finetune.py:68] layer 15_o @ epoch 1 new loss 0.00020546728046610951 old loss 0.00021450090571306646 BETTER
I0321 00:14:22.521343 1532398 finetune.py:68] layer 12_up @ epoch 2 new loss 0.000212647850275971 old loss 0.00021540063607972115 BETTER
I0321 00:14:38.523931 1532452 finetune.py:68] layer 13_up @ epoch 1 new loss 0.00022830306261312217 old loss 0.00023252348182722926 BETTER
I0321 00:14:38.627570 1532506 finetune.py:68] layer 14_o @ epoch 4 new loss 0.00019550046999938786 old loss 0.00019890512339770794 BETTER
I0321 00:14:39.566389 1532560 finetune.py:68] layer 15_o @ epoch 2 new loss 0.0001994313352042809 old loss 0.00020546728046610951 BETTER
I0321 00:14:57.116845 1532398 finetune.py:68] layer 12_up @ epoch 3 new loss 0.00021029279741924256 old loss 0.000212647850275971 BETTER
I0321 00:15:09.332417 1532506 finetune.py:45] layer 14_up initial loss 0.0002935614320449531
I0321 00:15:13.132359 1532452 finetune.py:68] layer 13_up @ epoch 2 new loss 0.00022506050299853086 old loss 0.00022830306261312217 BETTER
I0321 00:15:14.591635 1532560 finetune.py:68] layer 15_o @ epoch 3 new loss 0.0001949934958247468 old loss 0.0001994313352042809 BETTER
I0321 00:15:32.118126 1532398 finetune.py:68] layer 12_up @ epoch 4 new loss 0.0002082917926600203 old loss 0.00021029279741924256 BETTER
I0321 00:15:41.908739 1532506 finetune.py:68] layer 14_up @ epoch 0 new loss 0.00028535170713439584 old loss 0.0002935614320449531 BETTER
I0321 00:15:46.979162 1532452 finetune.py:68] layer 13_up @ epoch 3 new loss 0.0002223476767539978 old loss 0.00022506050299853086 BETTER
I0321 00:15:49.057168 1532560 finetune.py:68] layer 15_o @ epoch 4 new loss 0.00019149479339830577 old loss 0.0001949934958247468 BETTER
I0321 00:16:02.535315 1532398 finetune.py:45] layer 12_gate initial loss 0.00026026368141174316
I0321 00:16:16.867761 1532506 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0002807702694553882 old loss 0.00028535170713439584 BETTER
I0321 00:16:20.509422 1532560 finetune.py:45] layer 15_up initial loss 0.0003110719844698906
I0321 00:16:22.153242 1532452 finetune.py:68] layer 13_up @ epoch 4 new loss 0.00022005350911058486 old loss 0.0002223476767539978 BETTER
I0321 00:16:34.031813 1532398 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.00025737404939718544 old loss 0.00026026368141174316 BETTER
I0321 00:16:52.160434 1532506 finetune.py:68] layer 14_up @ epoch 2 new loss 0.00027726287953555584 old loss 0.0002807702694553882 BETTER
I0321 00:16:53.671129 1532560 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0003005216713063419 old loss 0.0003110719844698906 BETTER
I0321 00:16:53.861981 1532452 finetune.py:45] layer 13_gate initial loss 0.0002831687161233276
I0321 00:17:07.613298 1532398 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0002554177481215447 old loss 0.00025737404939718544 BETTER
I0321 00:17:24.833040 1532452 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.00027949517243541777 old loss 0.0002831687161233276 BETTER
I0321 00:17:28.223471 1532560 finetune.py:68] layer 15_up @ epoch 1 new loss 0.00029502189136110246 old loss 0.0003005216713063419 BETTER
I0321 00:17:28.405605 1532506 finetune.py:68] layer 14_up @ epoch 3 new loss 0.0002743007498793304 old loss 0.00027726287953555584 BETTER
I0321 00:17:40.770675 1532398 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.0002537872060202062 old loss 0.0002554177481215447 BETTER
I0321 00:17:58.182203 1532452 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0002771267609205097 old loss 0.00027949517243541777 BETTER
I0321 00:18:03.624963 1532560 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0002907954913098365 old loss 0.00029502189136110246 BETTER
I0321 00:18:04.176995 1532506 finetune.py:68] layer 14_up @ epoch 4 new loss 0.0002717485185712576 old loss 0.0002743007498793304 BETTER
I0321 00:18:14.040717 1532398 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.0002523819275666028 old loss 0.0002537872060202062 BETTER
I0321 00:18:31.084361 1532452 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0002752001164481044 old loss 0.0002771267609205097 BETTER
I0321 00:18:37.154098 1532506 finetune.py:45] layer 14_gate initial loss 0.0003446655464358628
I0321 00:18:37.865951 1532560 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0002873786143027246 old loss 0.0002907954913098365 BETTER
I0321 00:18:46.869068 1532398 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.0002511425700504333 old loss 0.0002523819275666028 BETTER
I0321 00:19:03.122978 1532452 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.00027353790937922895 old loss 0.0002752001164481044 BETTER
I0321 00:19:08.170196 1532506 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0003407329204492271 old loss 0.0003446655464358628 BETTER
I0321 00:19:11.541503 1532560 finetune.py:68] layer 15_up @ epoch 4 new loss 0.00028452189872041345 old loss 0.0002873786143027246 BETTER
I0321 00:19:18.767054 1532398 finetune.py:45] layer 12_down initial loss 0.0003676903434097767
I0321 00:19:34.917159 1532452 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0002720826305449009 old loss 0.00027353790937922895 BETTER
I0321 00:19:40.574002 1532506 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.00033812038600444794 old loss 0.0003407329204492271 BETTER
I0321 00:19:43.099344 1532560 finetune.py:45] layer 15_gate initial loss 0.0003750867326743901
I0321 00:19:49.039162 1532398 finetune.py:68] layer 12_down @ epoch 0 new loss 0.00036765728145837784 old loss 0.0003676903434097767 BETTER
I0321 00:20:07.881378 1532452 finetune.py:45] layer 13_down initial loss 0.00041659484850242734
I0321 00:20:14.957839 1532560 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.00036990485386922956 old loss 0.0003750867326743901 BETTER
I0321 00:20:15.156054 1532506 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.00033600631286390126 old loss 0.00033812038600444794 BETTER
I0321 00:20:21.806077 1532398 finetune.py:68] layer 12_down @ epoch 1 new loss 0.0003676320193335414 old loss 0.00036765728145837784 BETTER
I0321 00:20:37.873211 1532452 finetune.py:68] layer 13_down @ epoch 0 new loss 0.00041654740925878286 old loss 0.00041659484850242734 BETTER
I0321 00:20:48.290168 1532560 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.00036666603409685194 old loss 0.00036990485386922956 BETTER
I0321 00:20:49.316581 1532506 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.00033417053055018187 old loss 0.00033600631286390126 BETTER
I0321 00:20:54.074797 1532398 finetune.py:68] layer 12_down @ epoch 2 new loss 0.0003676202322822064 old loss 0.0003676320193335414 BETTER
I0321 00:21:09.046598 1532452 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0004165192658547312 old loss 0.00041654740925878286 BETTER
I0321 00:21:21.038895 1532560 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.0003641286166384816 old loss 0.00036666603409685194 BETTER
I0321 00:21:23.125386 1532506 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.00033257403993047774 old loss 0.00033417053055018187 BETTER
I0321 00:21:26.572454 1532398 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0003676129854284227 old loss 0.0003676202322822064 BETTER
I0321 00:21:39.618480 1532452 finetune.py:68] layer 13_down @ epoch 2 new loss 0.00041649985359981656 old loss 0.0004165192658547312 BETTER
I0321 00:21:53.409806 1532560 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0003619571798481047 old loss 0.0003641286166384816 BETTER
I0321 00:21:59.061786 1532398 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0003676027408801019 old loss 0.0003676129854284227 BETTER
I0321 00:21:59.247220 1532506 finetune.py:45] layer 14_down initial loss 0.0005007735453546047
12_v proxy err 0.011099142953753471 tr(WHW.T) 703.318603515625
bpp_loss 3.5237964391708374
12_q proxy err 0.0013463806826621294 tr(WHW.T) 7050.29931640625
bpp_loss 4.175138473510742
12_k proxy err 0.0008848775760270655 tr(WHW.T) 10912.7177734375
bpp_loss 4.22894024848938
12_o proxy err 0.013957081362605095 tr(WHW.T) 39.5162353515625
bpp_loss 3.5157140493392944
12_up proxy err 0.00556804146617651 tr(WHW.T) 1228.728759765625
bpp_loss 3.7543384640715844
12_gate proxy err 0.002938950667157769 tr(WHW.T) 2381.41064453125
bpp_loss 3.806921670603198
12_down proxy err 0.007589979562908411 tr(WHW.T) 64.44345092773438
bpp_loss 3.7398071510847224
I0321 00:22:10.978653 1532452 finetune.py:68] layer 13_down @ epoch 3 new loss 0.00041648754267953336 old loss 0.00041649985359981656 BETTER
I0321 00:22:27.010288 1532560 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0003601043135859072 old loss 0.0003619571798481047 BETTER
I0321 00:22:29.764222 1532506 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0005007204599678516 old loss 0.0005007735453546047 BETTER
I0321 00:22:40.701567 1532452 finetune.py:68] layer 13_down @ epoch 4 new loss 0.00041647584293968976 old loss 0.00041648754267953336 BETTER
13_v proxy err 0.011026892811059952 tr(WHW.T) 714.5677490234375
bpp_loss 3.557979106903076
13_q proxy err 0.0013472544960677624 tr(WHW.T) 6960.8896484375
bpp_loss 4.158705949783325
13_k proxy err 0.0009167614043690264 tr(WHW.T) 10451.8271484375
bpp_loss 4.189858436584473
13_o proxy err 0.01242339052259922 tr(WHW.T) 46.03984451293945
bpp_loss 3.5483267307281494
13_up proxy err 0.005368180572986603 tr(WHW.T) 1367.76513671875
bpp_loss 3.76692412620367
13_gate proxy err 0.002878322033211589 tr(WHW.T) 2602.98388671875
bpp_loss 3.8021825746048328
13_down proxy err 0.007618349511176348 tr(WHW.T) 79.67709350585938
bpp_loss 3.7496052231899526
I0321 00:22:56.060640 1532560 finetune.py:45] layer 15_down initial loss 0.000572477001696825
I0321 00:22:59.651392 1532506 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0005006910650990903 old loss 0.0005007204599678516 BETTER
I0321 00:23:24.111587 1532560 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0005724111688323319 old loss 0.000572477001696825 BETTER
I0321 00:23:29.581622 1532506 finetune.py:68] layer 14_down @ epoch 2 new loss 0.0005006722058169544 old loss 0.0005006910650990903 BETTER
I0321 00:23:53.634217 1532560 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00057237345026806 old loss 0.0005724111688323319 BETTER
I0321 00:23:59.323704 1532506 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0005006587016396224 old loss 0.0005006722058169544 BETTER
I0321 00:24:04.165410 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 16 in 77.20057773590088s
I0321 00:24:08.463146 1532614 config.py:54] PyTorch version 2.6.0 available.
W0321 00:24:08.874789 1532614 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 00:24:10.066651 1532614 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:24:10.078285 1531629 quantize_finetune_llama.py:203] layer 17 gpu 1
I0321 00:24:10.097842 1532614 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:24:23.226118 1532560 finetune.py:68] layer 15_down @ epoch 2 new loss 0.000572348537389189 old loss 0.00057237345026806 BETTER
I0321 00:24:26.339800 1532614 finetune.py:45] layer 16_v initial loss 0.00024015437520574778
W0321 00:24:26.340385 1532614 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:24:29.915872 1532506 finetune.py:68] layer 14_down @ epoch 4 new loss 0.0005006474093534052 old loss 0.0005006587016396224 BETTER
14_v proxy err 0.01165449433028698 tr(WHW.T) 706.1612548828125
bpp_loss 3.5400612354278564
14_q proxy err 0.0013780399458482862 tr(WHW.T) 7080.75
bpp_loss 4.15415358543396
14_k proxy err 0.0008821543306112289 tr(WHW.T) 11316.4091796875
bpp_loss 4.187345266342163
14_o proxy err 0.013645685277879238 tr(WHW.T) 51.252437591552734
bpp_loss 3.5296322107315063
14_up proxy err 0.00541680958122015 tr(WHW.T) 1465.4935302734375
bpp_loss 3.7683178214139716
14_gate proxy err 0.003011850407347083 tr(WHW.T) 2684.039794921875
bpp_loss 3.799610049225563
14_down proxy err 0.007783304899930954 tr(WHW.T) 90.69793701171875
bpp_loss 3.7508745193481445
I0321 00:24:52.534489 1532560 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0005723298527300358 old loss 0.000572348537389189 BETTER
I0321 00:25:01.675186 1532614 finetune.py:68] layer 16_v @ epoch 0 new loss 0.00013161776587367058 old loss 0.00024015437520574778 BETTER
I0321 00:25:21.328385 1532560 finetune.py:68] layer 15_down @ epoch 4 new loss 0.0005723148351535201 old loss 0.0005723298527300358 BETTER
15_v proxy err 0.010668395087122917 tr(WHW.T) 762.7275390625
bpp_loss 3.578308343887329
15_q proxy err 0.001328361569903791 tr(WHW.T) 7256.95751953125
bpp_loss 4.133378505706787
15_k proxy err 0.000892824842594564 tr(WHW.T) 11095.6162109375
bpp_loss 4.19167685508728
15_o proxy err 0.011669824831187725 tr(WHW.T) 59.94387435913086
bpp_loss 3.5653825998306274
15_up proxy err 0.005287954583764076 tr(WHW.T) 1641.3719482421875
bpp_loss 3.775030624034793
15_gate proxy err 0.0030421207193285227 tr(WHW.T) 2906.453857421875
bpp_loss 3.8064589389534884
15_down proxy err 0.007783796172589064 tr(WHW.T) 114.64061737060547
bpp_loss 3.755335253338481
I0321 00:25:37.150846 1532614 finetune.py:68] layer 16_v @ epoch 1 new loss 0.00011561710562091321 old loss 0.00013161776587367058 BETTER
I0321 00:25:47.594943 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 17 in 71.97471284866333s
I0321 00:25:51.707312 1532668 config.py:54] PyTorch version 2.6.0 available.
W0321 00:25:52.090208 1532668 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 00:25:53.133197 1532668 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:25:53.138313 1531629 quantize_finetune_llama.py:203] layer 18 gpu 2
I0321 00:25:53.157141 1532668 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:26:06.917564 1532668 finetune.py:45] layer 17_v initial loss 0.00019925073138438165
W0321 00:26:06.917878 1532668 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:26:14.584525 1532614 finetune.py:68] layer 16_v @ epoch 2 new loss 0.00010736710100900382 old loss 0.00011561710562091321 BETTER
I0321 00:26:39.103520 1532668 finetune.py:68] layer 17_v @ epoch 0 new loss 0.00010885761730605736 old loss 0.00019925073138438165 BETTER
I0321 00:26:51.834789 1532614 finetune.py:68] layer 16_v @ epoch 3 new loss 0.00010200480028288439 old loss 0.00010736710100900382 BETTER
I0321 00:27:06.122839 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 18 in 72.44002175331116s
I0321 00:27:10.673505 1532722 config.py:54] PyTorch version 2.6.0 available.
W0321 00:27:11.183088 1532722 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 00:27:12.280112 1532722 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:27:12.284268 1531629 quantize_finetune_llama.py:203] layer 19 gpu 3
I0321 00:27:12.297977 1532722 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0321 00:27:13.494419 1532668 finetune.py:68] layer 17_v @ epoch 1 new loss 9.659640636527911e-05 old loss 0.00010885761730605736 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:27:27.049606 1532722 finetune.py:45] layer 18_v initial loss 0.00020357086032163352
W0321 00:27:27.050371 1532722 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:27:29.415950 1532614 finetune.py:68] layer 16_v @ epoch 4 new loss 9.803691500565037e-05 old loss 0.00010200480028288439 BETTER
I0321 00:27:45.826716 1532614 finetune.py:45] layer 16_q initial loss 0.0001214160438394174
I0321 00:27:48.964066 1532668 finetune.py:68] layer 17_v @ epoch 2 new loss 9.031326771946624e-05 old loss 9.659640636527911e-05 BETTER
I0321 00:28:01.135465 1532722 finetune.py:68] layer 18_v @ epoch 0 new loss 0.00010540820221649483 old loss 0.00020357086032163352 BETTER
I0321 00:28:22.306163 1532614 finetune.py:68] layer 16_q @ epoch 0 new loss 0.0001110700104618445 old loss 0.0001214160438394174 BETTER
I0321 00:28:25.641083 1532668 finetune.py:68] layer 17_v @ epoch 3 new loss 8.604961476521567e-05 old loss 9.031326771946624e-05 BETTER
I0321 00:28:30.555783 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 19 in 77.74170708656311s
I0321 00:28:35.120796 1532776 config.py:54] PyTorch version 2.6.0 available.
W0321 00:28:35.550480 1532776 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0321 00:28:36.457620 1532722 finetune.py:68] layer 18_v @ epoch 1 new loss 9.435813262825832e-05 old loss 0.00010540820221649483 BETTER
W0321 00:28:36.643290 1532776 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:28:36.647414 1531629 quantize_finetune_llama.py:203] layer 20 gpu 0
I0321 00:28:36.664358 1532776 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:28:52.075961 1532776 finetune.py:45] layer 19_v initial loss 0.0001972831814782694
W0321 00:28:52.076872 1532776 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:29:00.617480 1532614 finetune.py:68] layer 16_q @ epoch 1 new loss 0.00010683383152354509 old loss 0.0001110700104618445 BETTER
I0321 00:29:02.220067 1532668 finetune.py:68] layer 17_v @ epoch 4 new loss 8.309660915983841e-05 old loss 8.604961476521567e-05 BETTER
I0321 00:29:11.895959 1532722 finetune.py:68] layer 18_v @ epoch 2 new loss 8.858426008373499e-05 old loss 9.435813262825832e-05 BETTER
I0321 00:29:19.880310 1532668 finetune.py:45] layer 17_q initial loss 0.00010496938193682581
I0321 00:29:26.594486 1532776 finetune.py:68] layer 19_v @ epoch 0 new loss 0.0001014170702546835 old loss 0.0001972831814782694 BETTER
I0321 00:29:37.871464 1532614 finetune.py:68] layer 16_q @ epoch 2 new loss 0.0001038934788084589 old loss 0.00010683383152354509 BETTER
I0321 00:29:47.957973 1532722 finetune.py:68] layer 18_v @ epoch 3 new loss 8.522519055986777e-05 old loss 8.858426008373499e-05 BETTER
I0321 00:29:54.841669 1532668 finetune.py:68] layer 17_q @ epoch 0 new loss 9.530394163448364e-05 old loss 0.00010496938193682581 BETTER
I0321 00:30:02.599663 1532776 finetune.py:68] layer 19_v @ epoch 1 new loss 9.080701420316473e-05 old loss 0.0001014170702546835 BETTER
I0321 00:30:15.429363 1532614 finetune.py:68] layer 16_q @ epoch 3 new loss 0.00010155689233215526 old loss 0.0001038934788084589 BETTER
I0321 00:30:24.673832 1532722 finetune.py:68] layer 18_v @ epoch 4 new loss 8.236080611823127e-05 old loss 8.522519055986777e-05 BETTER
I0321 00:30:30.496697 1532668 finetune.py:68] layer 17_q @ epoch 1 new loss 9.189016418531537e-05 old loss 9.530394163448364e-05 BETTER
I0321 00:30:38.507689 1532776 finetune.py:68] layer 19_v @ epoch 2 new loss 8.541344868717715e-05 old loss 9.080701420316473e-05 BETTER
I0321 00:30:43.023431 1532722 finetune.py:45] layer 18_q initial loss 0.00011184717732248828
I0321 00:30:53.016414 1532614 finetune.py:68] layer 16_q @ epoch 4 new loss 9.965735807782039e-05 old loss 0.00010155689233215526 BETTER
I0321 00:31:06.825715 1532668 finetune.py:68] layer 17_q @ epoch 2 new loss 8.944905857788399e-05 old loss 9.189016418531537e-05 BETTER
I0321 00:31:12.000849 1532614 finetune.py:45] layer 16_k initial loss 0.00011733594874385744
I0321 00:31:15.429871 1532776 finetune.py:68] layer 19_v @ epoch 3 new loss 8.180244185496122e-05 old loss 8.541344868717715e-05 BETTER
I0321 00:31:18.265973 1532722 finetune.py:68] layer 18_q @ epoch 0 new loss 0.00010037353786174208 old loss 0.00011184717732248828 BETTER
I0321 00:31:43.869261 1532668 finetune.py:68] layer 17_q @ epoch 3 new loss 8.754432201385498e-05 old loss 8.944905857788399e-05 BETTER
I0321 00:31:48.940786 1532614 finetune.py:68] layer 16_k @ epoch 0 new loss 0.00011290139809716493 old loss 0.00011733594874385744 BETTER
I0321 00:31:52.303771 1532776 finetune.py:68] layer 19_v @ epoch 4 new loss 7.92499486124143e-05 old loss 8.180244185496122e-05 BETTER
I0321 00:31:54.816729 1532722 finetune.py:68] layer 18_q @ epoch 1 new loss 9.685931581771001e-05 old loss 0.00010037353786174208 BETTER
I0321 00:32:10.026415 1532776 finetune.py:45] layer 19_q initial loss 0.00010309452045476064
I0321 00:32:20.725224 1532668 finetune.py:68] layer 17_q @ epoch 4 new loss 8.6004743934609e-05 old loss 8.754432201385498e-05 BETTER
I0321 00:32:25.751783 1532614 finetune.py:68] layer 16_k @ epoch 1 new loss 0.00011125143646495417 old loss 0.00011290139809716493 BETTER
I0321 00:32:30.361265 1532722 finetune.py:68] layer 18_q @ epoch 2 new loss 9.4312104920391e-05 old loss 9.685931581771001e-05 BETTER
I0321 00:32:39.912540 1532668 finetune.py:45] layer 17_k initial loss 0.00010300742724211887
I0321 00:32:44.485193 1532776 finetune.py:68] layer 19_q @ epoch 0 new loss 9.465149196330458e-05 old loss 0.00010309452045476064 BETTER
I0321 00:33:02.738976 1532614 finetune.py:68] layer 16_k @ epoch 2 new loss 0.00010993365140166134 old loss 0.00011125143646495417 BETTER
I0321 00:33:06.822555 1532722 finetune.py:68] layer 18_q @ epoch 3 new loss 9.23104744288139e-05 old loss 9.4312104920391e-05 BETTER
I0321 00:33:15.218159 1532668 finetune.py:68] layer 17_k @ epoch 0 new loss 9.861212311079726e-05 old loss 0.00010300742724211887 BETTER
I0321 00:33:20.246915 1532776 finetune.py:68] layer 19_q @ epoch 1 new loss 9.150525875156745e-05 old loss 9.465149196330458e-05 BETTER
I0321 00:33:41.116151 1532614 finetune.py:68] layer 16_k @ epoch 3 new loss 0.00010883720096899197 old loss 0.00010993365140166134 BETTER
I0321 00:33:43.583650 1532722 finetune.py:68] layer 18_q @ epoch 4 new loss 9.066801430890337e-05 old loss 9.23104744288139e-05 BETTER
I0321 00:33:50.716983 1532668 finetune.py:68] layer 17_k @ epoch 1 new loss 9.725036943564191e-05 old loss 9.861212311079726e-05 BETTER
I0321 00:33:56.091857 1532776 finetune.py:68] layer 19_q @ epoch 2 new loss 8.931316551752388e-05 old loss 9.150525875156745e-05 BETTER
I0321 00:34:01.944640 1532722 finetune.py:45] layer 18_k initial loss 0.00011458112567197531
I0321 00:34:18.059371 1532614 finetune.py:68] layer 16_k @ epoch 4 new loss 0.00010787420615088195 old loss 0.00010883720096899197 BETTER
I0321 00:34:25.987025 1532668 finetune.py:68] layer 17_k @ epoch 2 new loss 9.616341412765905e-05 old loss 9.725036943564191e-05 BETTER
I0321 00:34:32.542064 1532776 finetune.py:68] layer 19_q @ epoch 3 new loss 8.750187407713383e-05 old loss 8.931316551752388e-05 BETTER
I0321 00:34:37.180710 1532722 finetune.py:68] layer 18_k @ epoch 0 new loss 0.00010799356823554263 old loss 0.00011458112567197531 BETTER
I0321 00:34:37.196310 1532614 finetune.py:45] layer 16_o initial loss 0.00029040861409157515
I0321 00:35:02.175059 1532668 finetune.py:68] layer 17_k @ epoch 3 new loss 9.524945926386863e-05 old loss 9.616341412765905e-05 BETTER
I0321 00:35:09.945222 1532776 finetune.py:68] layer 19_q @ epoch 4 new loss 8.612933743279427e-05 old loss 8.750187407713383e-05 BETTER
I0321 00:35:13.326278 1532614 finetune.py:68] layer 16_o @ epoch 0 new loss 0.0002672130358405411 old loss 0.00029040861409157515 BETTER
I0321 00:35:13.638810 1532722 finetune.py:68] layer 18_k @ epoch 1 new loss 0.00010644915164448321 old loss 0.00010799356823554263 BETTER
I0321 00:35:28.217699 1532776 finetune.py:45] layer 19_k initial loss 0.00010622452828101814
I0321 00:35:38.253884 1532668 finetune.py:68] layer 17_k @ epoch 4 new loss 9.438224515179172e-05 old loss 9.524945926386863e-05 BETTER
I0321 00:35:49.864016 1532722 finetune.py:68] layer 18_k @ epoch 2 new loss 0.0001052576772053726 old loss 0.00010644915164448321 BETTER
I0321 00:35:50.072233 1532614 finetune.py:68] layer 16_o @ epoch 1 new loss 0.000257105624768883 old loss 0.0002672130358405411 BETTER
I0321 00:35:56.875748 1532668 finetune.py:45] layer 17_o initial loss 0.0002308059629285708
I0321 00:36:02.632208 1532776 finetune.py:68] layer 19_k @ epoch 0 new loss 0.00010086649854201823 old loss 0.00010622452828101814 BETTER
I0321 00:36:27.466630 1532722 finetune.py:68] layer 18_k @ epoch 3 new loss 0.00010427970119053498 old loss 0.0001052576772053726 BETTER
I0321 00:36:28.018465 1532614 finetune.py:68] layer 16_o @ epoch 2 new loss 0.0002501621493138373 old loss 0.000257105624768883 BETTER
I0321 00:36:31.614116 1532668 finetune.py:68] layer 17_o @ epoch 0 new loss 0.00021558710432145745 old loss 0.0002308059629285708 BETTER
I0321 00:36:38.177869 1532776 finetune.py:68] layer 19_k @ epoch 1 new loss 9.957439033314586e-05 old loss 0.00010086649854201823 BETTER
I0321 00:37:04.460210 1532722 finetune.py:68] layer 18_k @ epoch 4 new loss 0.00010345171176595613 old loss 0.00010427970119053498 BETTER
I0321 00:37:06.204005 1532614 finetune.py:68] layer 16_o @ epoch 3 new loss 0.00024492060765624046 old loss 0.0002501621493138373 BETTER
I0321 00:37:07.804043 1532668 finetune.py:68] layer 17_o @ epoch 1 new loss 0.0002087251777993515 old loss 0.00021558710432145745 BETTER
I0321 00:37:13.154165 1532776 finetune.py:68] layer 19_k @ epoch 2 new loss 9.8542841442395e-05 old loss 9.957439033314586e-05 BETTER
I0321 00:37:23.234058 1532722 finetune.py:45] layer 18_o initial loss 0.0002525420277379453
I0321 00:37:43.928588 1532614 finetune.py:68] layer 16_o @ epoch 4 new loss 0.00024078979913610965 old loss 0.00024492060765624046 BETTER
I0321 00:37:43.946301 1532668 finetune.py:68] layer 17_o @ epoch 2 new loss 0.00020408832642715424 old loss 0.0002087251777993515 BETTER
I0321 00:37:48.484060 1532776 finetune.py:68] layer 19_k @ epoch 3 new loss 9.770462929736823e-05 old loss 9.8542841442395e-05 BETTER
I0321 00:37:57.363491 1532722 finetune.py:68] layer 18_o @ epoch 0 new loss 0.0002350196155020967 old loss 0.0002525420277379453 BETTER
I0321 00:38:14.055319 1532614 finetune.py:45] layer 16_up initial loss 0.0003924329939763993
I0321 00:38:19.373787 1532668 finetune.py:68] layer 17_o @ epoch 3 new loss 0.00020055784261785448 old loss 0.00020408832642715424 BETTER
I0321 00:38:24.001343 1532776 finetune.py:68] layer 19_k @ epoch 4 new loss 9.703460818855092e-05 old loss 9.770462929736823e-05 BETTER
I0321 00:38:31.980572 1532722 finetune.py:68] layer 18_o @ epoch 1 new loss 0.0002277308376505971 old loss 0.0002350196155020967 BETTER
I0321 00:38:43.038770 1532776 finetune.py:45] layer 19_o initial loss 0.00023030521697364748
I0321 00:38:48.074962 1532614 finetune.py:68] layer 16_up @ epoch 0 new loss 0.0003804742300417274 old loss 0.0003924329939763993 BETTER
I0321 00:38:54.706736 1532668 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0001978013606276363 old loss 0.00020055784261785448 BETTER
I0321 00:39:07.583349 1532722 finetune.py:68] layer 18_o @ epoch 2 new loss 0.00022282369900494814 old loss 0.0002277308376505971 BETTER
I0321 00:39:16.755401 1532776 finetune.py:68] layer 19_o @ epoch 0 new loss 0.00021530581580009311 old loss 0.00023030521697364748 BETTER
I0321 00:39:22.989602 1532614 finetune.py:68] layer 16_up @ epoch 1 new loss 0.00037399737630039454 old loss 0.0003804742300417274 BETTER
I0321 00:39:26.188977 1532668 finetune.py:45] layer 17_up initial loss 0.0003655205073300749
I0321 00:39:42.683908 1532722 finetune.py:68] layer 18_o @ epoch 3 new loss 0.00021914586250204593 old loss 0.00022282369900494814 BETTER
I0321 00:39:50.908385 1532776 finetune.py:68] layer 19_o @ epoch 1 new loss 0.00020945399592164904 old loss 0.00021530581580009311 BETTER
I0321 00:39:58.533567 1532614 finetune.py:68] layer 16_up @ epoch 2 new loss 0.00036895344965159893 old loss 0.00037399737630039454 BETTER
I0321 00:39:59.000487 1532668 finetune.py:68] layer 17_up @ epoch 0 new loss 0.00035523431142792106 old loss 0.0003655205073300749 BETTER
I0321 00:40:18.735283 1532722 finetune.py:68] layer 18_o @ epoch 4 new loss 0.00021631803247146308 old loss 0.00021914586250204593 BETTER
I0321 00:40:25.690702 1532776 finetune.py:68] layer 19_o @ epoch 2 new loss 0.00020564212172757834 old loss 0.00020945399592164904 BETTER
I0321 00:40:32.814426 1532668 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0003494500124361366 old loss 0.00035523431142792106 BETTER
I0321 00:40:33.775193 1532614 finetune.py:68] layer 16_up @ epoch 3 new loss 0.00036486610770225525 old loss 0.00036895344965159893 BETTER
I0321 00:40:49.556777 1532722 finetune.py:45] layer 18_up initial loss 0.000417294999351725
I0321 00:41:01.002801 1532776 finetune.py:68] layer 19_o @ epoch 3 new loss 0.00020281679462641478 old loss 0.00020564212172757834 BETTER
I0321 00:41:07.061442 1532668 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0003451201773714274 old loss 0.0003494500124361366 BETTER
I0321 00:41:09.546180 1532614 finetune.py:68] layer 16_up @ epoch 4 new loss 0.00036141875898465514 old loss 0.00036486610770225525 BETTER
I0321 00:41:22.145021 1532722 finetune.py:68] layer 18_up @ epoch 0 new loss 0.000405511527787894 old loss 0.000417294999351725 BETTER
I0321 00:41:36.774219 1532776 finetune.py:68] layer 19_o @ epoch 4 new loss 0.00020068084995727986 old loss 0.00020281679462641478 BETTER
I0321 00:41:40.910561 1532614 finetune.py:45] layer 16_gate initial loss 0.0004773146065417677
I0321 00:41:41.001691 1532668 finetune.py:68] layer 17_up @ epoch 3 new loss 0.0003416238469071686 old loss 0.0003451201773714274 BETTER
I0321 00:41:55.894843 1532722 finetune.py:68] layer 18_up @ epoch 1 new loss 0.00039914652006700635 old loss 0.000405511527787894 BETTER
I0321 00:42:08.526821 1532776 finetune.py:45] layer 19_up initial loss 0.0004262360744178295
I0321 00:42:13.286709 1532614 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.0004711817018687725 old loss 0.0004773146065417677 BETTER
I0321 00:42:14.874581 1532668 finetune.py:68] layer 17_up @ epoch 4 new loss 0.00033866666490212083 old loss 0.0003416238469071686 BETTER
I0321 00:42:29.385595 1532722 finetune.py:68] layer 18_up @ epoch 2 new loss 0.00039434374775737524 old loss 0.00039914652006700635 BETTER
I0321 00:42:40.281899 1532776 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0004143724509049207 old loss 0.0004262360744178295 BETTER
I0321 00:42:46.380753 1532668 finetune.py:45] layer 17_gate initial loss 0.00046672631287947297
I0321 00:42:47.435483 1532614 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.00046726592699997127 old loss 0.0004711817018687725 BETTER
I0321 00:43:03.635869 1532722 finetune.py:68] layer 18_up @ epoch 3 new loss 0.0003904087352566421 old loss 0.00039434374775737524 BETTER
I0321 00:43:14.969230 1532776 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0004080515936948359 old loss 0.0004143724509049207 BETTER
I0321 00:43:17.834778 1532668 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.000461538351373747 old loss 0.00046672631287947297 BETTER
I0321 00:43:21.195938 1532614 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.0004640821716748178 old loss 0.00046726592699997127 BETTER
I0321 00:43:37.960101 1532722 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0003872342349495739 old loss 0.0003904087352566421 BETTER
I0321 00:43:49.143946 1532776 finetune.py:68] layer 19_up @ epoch 2 new loss 0.000403311918489635 old loss 0.0004080515936948359 BETTER
I0321 00:43:50.095761 1532668 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0004580861423164606 old loss 0.000461538351373747 BETTER
I0321 00:43:54.828381 1532614 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0004614761855918914 old loss 0.0004640821716748178 BETTER
I0321 00:44:10.279148 1532722 finetune.py:45] layer 18_gate initial loss 0.0005377467023208737
I0321 00:44:24.077989 1532668 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.00045532433432526886 old loss 0.0004580861423164606 BETTER
I0321 00:44:24.082545 1532776 finetune.py:68] layer 19_up @ epoch 3 new loss 0.00039960487629286945 old loss 0.000403311918489635 BETTER
I0321 00:44:28.978305 1532614 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.00045918786781840026 old loss 0.0004614761855918914 BETTER
I0321 00:44:41.197173 1532722 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0005324488738551736 old loss 0.0005377467023208737 BETTER
I0321 00:44:56.971438 1532668 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.0004530353471636772 old loss 0.00045532433432526886 BETTER
I0321 00:44:58.163628 1532776 finetune.py:68] layer 19_up @ epoch 4 new loss 0.00039655089494772255 old loss 0.00039960487629286945 BETTER
I0321 00:45:02.475121 1532614 finetune.py:45] layer 16_down initial loss 0.0007461211644113064
I0321 00:45:13.653898 1532722 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.0005287283565849066 old loss 0.0005324488738551736 BETTER
I0321 00:45:29.898331 1532668 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.000451092841103673 old loss 0.0004530353471636772 BETTER
I0321 00:45:32.020385 1532776 finetune.py:45] layer 19_gate initial loss 0.0005732339923270047
I0321 00:45:32.768165 1532614 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0007460351334884763 old loss 0.0007461211644113064 BETTER
I0321 00:45:45.759070 1532722 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0005257939337752759 old loss 0.0005287283565849066 BETTER
I0321 00:46:03.789343 1532776 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.0005674805142916739 old loss 0.0005732339923270047 BETTER
I0321 00:46:05.185079 1532668 finetune.py:45] layer 17_down initial loss 0.0007582266116514802
I0321 00:46:05.612804 1532614 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0007459830376319587 old loss 0.0007460351334884763 BETTER
I0321 00:46:19.930360 1532722 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0005233825650066137 old loss 0.0005257939337752759 BETTER
I0321 00:46:36.362332 1532668 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0007581246318295598 old loss 0.0007582266116514802 BETTER
I0321 00:46:37.249921 1532776 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0005636988207697868 old loss 0.0005674805142916739 BETTER
I0321 00:46:38.350356 1532614 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0007459457265213132 old loss 0.0007459830376319587 BETTER
I0321 00:46:53.393518 1532722 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.0005213148542679846 old loss 0.0005233825650066137 BETTER
I0321 00:47:07.719157 1532668 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0007580783567391336 old loss 0.0007581246318295598 BETTER
I0321 00:47:09.323757 1532776 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0005606949562206864 old loss 0.0005636988207697868 BETTER
I0321 00:47:10.564059 1532614 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0007459181942977011 old loss 0.0007459457265213132 BETTER
I0321 00:47:28.152292 1532722 finetune.py:45] layer 18_down initial loss 0.0008921416592784226
I0321 00:47:39.699851 1532668 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0007580496603623033 old loss 0.0007580783567391336 BETTER
I0321 00:47:42.585112 1532776 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0005582520388998091 old loss 0.0005606949562206864 BETTER
I0321 00:47:43.353092 1532614 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0007458949694409966 old loss 0.0007459181942977011 BETTER
16_v proxy err 0.01081458106637001 tr(WHW.T) 780.7407836914062
bpp_loss 3.6209797859191895
16_q proxy err 0.0013666641898453236 tr(WHW.T) 7197.17431640625
bpp_loss 4.121272325515747
16_k proxy err 0.0008677087025716901 tr(WHW.T) 11652.08984375
bpp_loss 4.166062116622925
16_o proxy err 0.009175283834338188 tr(WHW.T) 88.55754852294922
bpp_loss 3.6104336977005005
16_up proxy err 0.005173232406377792 tr(WHW.T) 1891.1649169921875
bpp_loss 3.774035786473474
16_gate proxy err 0.002965718973428011 tr(WHW.T) 3369.06640625
bpp_loss 3.815455059672511
16_down proxy err 0.007893162779510021 tr(WHW.T) 152.7859344482422
bpp_loss 3.7545085064200467
I0321 00:47:59.059061 1532722 finetune.py:68] layer 18_down @ epoch 0 new loss 0.000892029027454555 old loss 0.0008921416592784226 BETTER
I0321 00:48:11.710820 1532668 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0007580256788060069 old loss 0.0007580496603623033 BETTER
I0321 00:48:15.420312 1532776 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.0005562027217820287 old loss 0.0005582520388998091 BETTER
I0321 00:48:28.922033 1532722 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0008919708780013025 old loss 0.000892029027454555 BETTER
I0321 00:48:41.233919 1532668 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0007580072269774973 old loss 0.0007580256788060069 BETTER
17_v proxy err 0.010176604613661766 tr(WHW.T) 845.7654418945312
bpp_loss 3.617333769798279
17_q proxy err 0.0013859790051355958 tr(WHW.T) 7166.83251953125
bpp_loss 4.10785698890686
17_k proxy err 0.0009472332312725484 tr(WHW.T) 10719.216796875
bpp_loss 4.144883394241333
17_o proxy err 0.010021000169217587 tr(WHW.T) 58.36525344848633
bpp_loss 3.6111456155776978
17_up proxy err 0.005625956226140261 tr(WHW.T) 1921.9869384765625
bpp_loss 3.7693121266919514
17_gate proxy err 0.0030973702669143677 tr(WHW.T) 3576.6904296875
bpp_loss 3.8276858662450035
17_down proxy err 0.007818946614861488 tr(WHW.T) 166.12156677246094
bpp_loss 3.753233022468035
I0321 00:48:45.930437 1532776 finetune.py:45] layer 19_down initial loss 0.0009606289095245302
I0321 00:48:58.706415 1532722 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0008919296087697148 old loss 0.0008919708780013025 BETTER
I0321 00:49:13.914524 1532776 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0009604977094568312 old loss 0.0009606289095245302 BETTER
I0321 00:49:28.732799 1532722 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0008918975363485515 old loss 0.0008919296087697148 BETTER
I0321 00:49:42.997517 1532776 finetune.py:68] layer 19_down @ epoch 1 new loss 0.000960435951128602 old loss 0.0009604977094568312 BETTER
I0321 00:49:58.375900 1532722 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0008918726234696805 old loss 0.0008918975363485515 BETTER
18_v proxy err 0.009368526749312878 tr(WHW.T) 1003.7705078125
bpp_loss 3.6675031185150146
18_q proxy err 0.0014241280732676387 tr(WHW.T) 7513.525390625
bpp_loss 4.070798397064209
18_k proxy err 0.0010395684512332082 tr(WHW.T) 10484.7080078125
bpp_loss 4.106471538543701
18_o proxy err 0.008859071880578995 tr(WHW.T) 70.18033599853516
bpp_loss 3.652099609375
18_up proxy err 0.005991219077259302 tr(WHW.T) 2022.25390625
bpp_loss 3.7656863900118096
18_gate proxy err 0.003291082102805376 tr(WHW.T) 3780.718017578125
bpp_loss 3.8396338529365006
18_down proxy err 0.007850638590753078 tr(WHW.T) 199.35882568359375
bpp_loss 3.752066922742267
I0321 00:50:05.645784 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 20 in 77.12695026397705s
I0321 00:50:09.632552 1532830 config.py:54] PyTorch version 2.6.0 available.
W0321 00:50:10.018067 1532830 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 00:50:11.006558 1532830 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:50:11.010512 1531629 quantize_finetune_llama.py:203] layer 21 gpu 1
I0321 00:50:11.024263 1532830 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0321 00:50:11.168178 1532776 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0009603978833183646 old loss 0.000960435951128602 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:50:24.995157 1532830 finetune.py:45] layer 20_v initial loss 0.00022324529709294438
W0321 00:50:24.995376 1532830 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:50:39.716913 1532776 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0009603687794879079 old loss 0.0009603978833183646 BETTER
I0321 00:50:59.859718 1532830 finetune.py:68] layer 20_v @ epoch 0 new loss 0.00011494554200908169 old loss 0.00022324529709294438 BETTER
I0321 00:51:09.191449 1532776 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0009603470098227262 old loss 0.0009603687794879079 BETTER
19_v proxy err 0.00914890505373478 tr(WHW.T) 1019.1412353515625
bpp_loss 3.677343010902405
19_q proxy err 0.0015152741689234972 tr(WHW.T) 6948.58154296875
bpp_loss 4.051127672195435
19_k proxy err 0.0010163096012547612 tr(WHW.T) 10574.2041015625
bpp_loss 4.082043170928955
19_o proxy err 0.008736618794500828 tr(WHW.T) 62.43318557739258
bpp_loss 3.666049003601074
19_up proxy err 0.006021797191351652 tr(WHW.T) 2149.580810546875
bpp_loss 3.7656159511832303
19_gate proxy err 0.0036021864507347345 tr(WHW.T) 3689.68212890625
bpp_loss 3.8444096764852835
19_down proxy err 0.007647961378097534 tr(WHW.T) 223.81698608398438
bpp_loss 3.7547702123952464
I0321 00:51:23.572002 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 21 in 72.0979950428009s
I0321 00:51:27.319248 1532884 config.py:54] PyTorch version 2.6.0 available.
W0321 00:51:27.692178 1532884 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 00:51:28.714612 1532884 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:51:28.718991 1531629 quantize_finetune_llama.py:203] layer 22 gpu 2
I0321 00:51:28.738132 1532884 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:51:36.047497 1532830 finetune.py:68] layer 20_v @ epoch 1 new loss 0.00010283332812832668 old loss 0.00011494554200908169 BETTER
I0321 00:51:43.344892 1532884 finetune.py:45] layer 21_v initial loss 0.00019882975902874023
W0321 00:51:43.345307 1532884 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:52:13.014602 1532830 finetune.py:68] layer 20_v @ epoch 2 new loss 9.685057011665776e-05 old loss 0.00010283332812832668 BETTER
I0321 00:52:16.756526 1532884 finetune.py:68] layer 21_v @ epoch 0 new loss 0.0001043665615725331 old loss 0.00019882975902874023 BETTER
I0321 00:52:45.087934 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 22 in 75.80019664764404s
I0321 00:52:49.700777 1532938 config.py:54] PyTorch version 2.6.0 available.
W0321 00:52:50.145670 1532938 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0321 00:52:50.215302 1532830 finetune.py:68] layer 20_v @ epoch 3 new loss 9.312762995250523e-05 old loss 9.685057011665776e-05 BETTER
W0321 00:52:51.199377 1532938 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:52:51.203779 1531629 quantize_finetune_llama.py:203] layer 23 gpu 3
I0321 00:52:51.219177 1532938 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0321 00:52:51.646440 1532884 finetune.py:68] layer 21_v @ epoch 1 new loss 9.464297909289598e-05 old loss 0.0001043665615725331 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:53:06.093057 1532938 finetune.py:45] layer 22_v initial loss 0.00023876158229541034
W0321 00:53:06.093762 1532938 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:53:27.451235 1532884 finetune.py:68] layer 21_v @ epoch 2 new loss 8.992649236461148e-05 old loss 9.464297909289598e-05 BETTER
I0321 00:53:27.937190 1532830 finetune.py:68] layer 20_v @ epoch 4 new loss 9.093108383240178e-05 old loss 9.312762995250523e-05 BETTER
I0321 00:53:40.499625 1532938 finetune.py:68] layer 22_v @ epoch 0 new loss 0.00012852666259277612 old loss 0.00023876158229541034 BETTER
I0321 00:53:45.294676 1532830 finetune.py:45] layer 20_q initial loss 0.00011967457248829305
I0321 00:54:03.289872 1532884 finetune.py:68] layer 21_v @ epoch 3 new loss 8.685123611940071e-05 old loss 8.992649236461148e-05 BETTER
I0321 00:54:07.456765 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 23 in 75.71832036972046s
I0321 00:54:12.057183 1532992 config.py:54] PyTorch version 2.6.0 available.
W0321 00:54:12.506311 1532992 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 00:54:13.685799 1532992 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 00:54:13.691115 1531629 quantize_finetune_llama.py:203] layer 24 gpu 0
I0321 00:54:13.707286 1532992 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0321 00:54:15.734185 1532938 finetune.py:68] layer 22_v @ epoch 1 new loss 0.00011659126175800338 old loss 0.00012852666259277612 BETTER
I0321 00:54:20.489842 1532830 finetune.py:68] layer 20_q @ epoch 0 new loss 0.00010874500731006265 old loss 0.00011967457248829305 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 00:54:30.611700 1532992 finetune.py:45] layer 23_v initial loss 0.00024097487039398402
W0321 00:54:30.612326 1532992 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 00:54:39.226529 1532884 finetune.py:68] layer 21_v @ epoch 4 new loss 8.47565388539806e-05 old loss 8.685123611940071e-05 BETTER
I0321 00:54:51.792848 1532938 finetune.py:68] layer 22_v @ epoch 2 new loss 0.000110572153062094 old loss 0.00011659126175800338 BETTER
I0321 00:54:57.373798 1532884 finetune.py:45] layer 21_q initial loss 0.00010916247265413404
I0321 00:54:58.453231 1532830 finetune.py:68] layer 20_q @ epoch 1 new loss 0.00010525832476560026 old loss 0.00010874500731006265 BETTER
I0321 00:55:04.585947 1532992 finetune.py:68] layer 23_v @ epoch 0 new loss 0.00012777872325386852 old loss 0.00024097487039398402 BETTER
I0321 00:55:29.418373 1532938 finetune.py:68] layer 22_v @ epoch 3 new loss 0.00010672934877220541 old loss 0.000110572153062094 BETTER
I0321 00:55:32.982301 1532884 finetune.py:68] layer 21_q @ epoch 0 new loss 0.00010124673281097785 old loss 0.00010916247265413404 BETTER
I0321 00:55:36.382919 1532830 finetune.py:68] layer 20_q @ epoch 2 new loss 0.00010280034621246159 old loss 0.00010525832476560026 BETTER
I0321 00:55:40.593290 1532992 finetune.py:68] layer 23_v @ epoch 1 new loss 0.0001161121399491094 old loss 0.00012777872325386852 BETTER
I0321 00:56:06.788905 1532938 finetune.py:68] layer 22_v @ epoch 4 new loss 0.0001039273411151953 old loss 0.00010672934877220541 BETTER
I0321 00:56:09.565529 1532884 finetune.py:68] layer 21_q @ epoch 1 new loss 9.834209777181968e-05 old loss 0.00010124673281097785 BETTER
I0321 00:56:13.873119 1532830 finetune.py:68] layer 20_q @ epoch 3 new loss 0.00010084574751090258 old loss 0.00010280034621246159 BETTER
I0321 00:56:15.986781 1532992 finetune.py:68] layer 23_v @ epoch 2 new loss 0.00011038836237275973 old loss 0.0001161121399491094 BETTER
I0321 00:56:26.116036 1532938 finetune.py:45] layer 22_q initial loss 0.0001440654305042699
I0321 00:56:45.525495 1532884 finetune.py:68] layer 21_q @ epoch 2 new loss 9.633394074626267e-05 old loss 9.834209777181968e-05 BETTER
I0321 00:56:51.964128 1532830 finetune.py:68] layer 20_q @ epoch 4 new loss 9.9258977570571e-05 old loss 0.00010084574751090258 BETTER
I0321 00:56:52.715077 1532992 finetune.py:68] layer 23_v @ epoch 3 new loss 0.0001068219862645492 old loss 0.00011038836237275973 BETTER
I0321 00:57:00.535236 1532938 finetune.py:68] layer 22_q @ epoch 0 new loss 0.00013075840251985937 old loss 0.0001440654305042699 BETTER
I0321 00:57:09.426475 1532830 finetune.py:45] layer 20_k initial loss 0.00011990116036031395
I0321 00:57:21.365090 1532884 finetune.py:68] layer 21_q @ epoch 3 new loss 9.47499938774854e-05 old loss 9.633394074626267e-05 BETTER
I0321 00:57:28.413147 1532992 finetune.py:68] layer 23_v @ epoch 4 new loss 0.0001042173316818662 old loss 0.0001068219862645492 BETTER
I0321 00:57:35.828232 1532938 finetune.py:68] layer 22_q @ epoch 1 new loss 0.00012647431867662817 old loss 0.00013075840251985937 BETTER
I0321 00:57:44.822674 1532830 finetune.py:68] layer 20_k @ epoch 0 new loss 0.00011604265455389395 old loss 0.00011990116036031395 BETTER
I0321 00:57:49.586857 1532992 finetune.py:45] layer 23_q initial loss 0.00013628894521389157
I0321 00:57:58.056320 1532884 finetune.py:68] layer 21_q @ epoch 4 new loss 9.351998596685007e-05 old loss 9.47499938774854e-05 BETTER
I0321 00:58:11.862158 1532938 finetune.py:68] layer 22_q @ epoch 2 new loss 0.00012336269719526172 old loss 0.00012647431867662817 BETTER
I0321 00:58:15.977503 1532884 finetune.py:45] layer 21_k initial loss 0.0001144344059866853
I0321 00:58:21.986587 1532830 finetune.py:68] layer 20_k @ epoch 1 new loss 0.00011462678230600432 old loss 0.00011604265455389395 BETTER
I0321 00:58:24.545675 1532992 finetune.py:68] layer 23_q @ epoch 0 new loss 0.00012654511374421418 old loss 0.00013628894521389157 BETTER
I0321 00:58:49.417092 1532938 finetune.py:68] layer 22_q @ epoch 3 new loss 0.00012111028627259657 old loss 0.00012336269719526172 BETTER
I0321 00:58:51.112815 1532884 finetune.py:68] layer 21_k @ epoch 0 new loss 0.000110150984255597 old loss 0.0001144344059866853 BETTER
I0321 00:58:59.030726 1532830 finetune.py:68] layer 20_k @ epoch 2 new loss 0.0001135254351538606 old loss 0.00011462678230600432 BETTER
I0321 00:59:01.128358 1532992 finetune.py:68] layer 23_q @ epoch 1 new loss 0.00012303923722356558 old loss 0.00012654511374421418 BETTER
I0321 00:59:26.411772 1532938 finetune.py:68] layer 22_q @ epoch 4 new loss 0.0001190810144180432 old loss 0.00012111028627259657 BETTER
I0321 00:59:27.443968 1532884 finetune.py:68] layer 21_k @ epoch 1 new loss 0.00010907616524491459 old loss 0.000110150984255597 BETTER
I0321 00:59:36.723945 1532830 finetune.py:68] layer 20_k @ epoch 3 new loss 0.00011262309271842241 old loss 0.0001135254351538606 BETTER
I0321 00:59:37.260474 1532992 finetune.py:68] layer 23_q @ epoch 2 new loss 0.00012052022793795913 old loss 0.00012303923722356558 BETTER
I0321 00:59:45.644065 1532938 finetune.py:45] layer 22_k initial loss 0.00014791134162805974
I0321 01:00:03.046581 1532884 finetune.py:68] layer 21_k @ epoch 2 new loss 0.00010826375364558771 old loss 0.00010907616524491459 BETTER
I0321 01:00:14.452408 1532992 finetune.py:68] layer 23_q @ epoch 3 new loss 0.00011864118278026581 old loss 0.00012052022793795913 BETTER
I0321 01:00:15.060734 1532830 finetune.py:68] layer 20_k @ epoch 4 new loss 0.00011186557821929455 old loss 0.00011262309271842241 BETTER
I0321 01:00:19.744809 1532938 finetune.py:68] layer 22_k @ epoch 0 new loss 0.00014415747136808932 old loss 0.00014791134162805974 BETTER
I0321 01:00:32.298608 1532830 finetune.py:45] layer 20_o initial loss 0.0002699119795579463
I0321 01:00:39.198378 1532884 finetune.py:68] layer 21_k @ epoch 3 new loss 0.00010758532152976841 old loss 0.00010826375364558771 BETTER
I0321 01:00:49.823535 1532992 finetune.py:68] layer 23_q @ epoch 4 new loss 0.00011708527017617598 old loss 0.00011864118278026581 BETTER
I0321 01:00:54.858009 1532938 finetune.py:68] layer 22_k @ epoch 1 new loss 0.00014263698540162295 old loss 0.00014415747136808932 BETTER
I0321 01:01:07.199654 1532830 finetune.py:68] layer 20_o @ epoch 0 new loss 0.0002519238041713834 old loss 0.0002699119795579463 BETTER
I0321 01:01:11.333029 1532992 finetune.py:45] layer 23_k initial loss 0.00014412388554774225
I0321 01:01:15.788015 1532884 finetune.py:68] layer 21_k @ epoch 4 new loss 0.00010708445915952325 old loss 0.00010758532152976841 BETTER
I0321 01:01:31.252656 1532938 finetune.py:68] layer 22_k @ epoch 2 new loss 0.00014149055641610175 old loss 0.00014263698540162295 BETTER
I0321 01:01:33.816426 1532884 finetune.py:45] layer 21_o initial loss 0.00023858998611103743
I0321 01:01:43.850425 1532830 finetune.py:68] layer 20_o @ epoch 1 new loss 0.00024508850765414536 old loss 0.0002519238041713834 BETTER
I0321 01:01:45.486946 1532992 finetune.py:68] layer 23_k @ epoch 0 new loss 0.00014094900689087808 old loss 0.00014412388554774225 BETTER
I0321 01:02:08.356510 1532938 finetune.py:68] layer 22_k @ epoch 3 new loss 0.00014055086649022996 old loss 0.00014149055641610175 BETTER
I0321 01:02:08.756583 1532884 finetune.py:68] layer 21_o @ epoch 0 new loss 0.00022860002354718745 old loss 0.00023858998611103743 BETTER
I0321 01:02:21.760783 1532830 finetune.py:68] layer 20_o @ epoch 2 new loss 0.0002404572005616501 old loss 0.00024508850765414536 BETTER
I0321 01:02:22.532544 1532992 finetune.py:68] layer 23_k @ epoch 1 new loss 0.0001397050218656659 old loss 0.00014094900689087808 BETTER
I0321 01:02:44.850664 1532884 finetune.py:68] layer 21_o @ epoch 1 new loss 0.00022485037334263325 old loss 0.00022860002354718745 BETTER
I0321 01:02:44.966145 1532938 finetune.py:68] layer 22_k @ epoch 4 new loss 0.000139773910632357 old loss 0.00014055086649022996 BETTER
I0321 01:02:58.471916 1532992 finetune.py:68] layer 23_k @ epoch 2 new loss 0.0001387470547342673 old loss 0.0001397050218656659 BETTER
I0321 01:02:58.552535 1532830 finetune.py:68] layer 20_o @ epoch 3 new loss 0.0002370982401771471 old loss 0.0002404572005616501 BETTER
I0321 01:03:05.309319 1532938 finetune.py:45] layer 22_o initial loss 0.00031066889641806483
I0321 01:03:20.391888 1532884 finetune.py:68] layer 21_o @ epoch 2 new loss 0.00022237372468225658 old loss 0.00022485037334263325 BETTER
I0321 01:03:34.652097 1532992 finetune.py:68] layer 23_k @ epoch 3 new loss 0.00013796749408356845 old loss 0.0001387470547342673 BETTER
I0321 01:03:35.524181 1532830 finetune.py:68] layer 20_o @ epoch 4 new loss 0.00023439707001671195 old loss 0.0002370982401771471 BETTER
I0321 01:03:39.421152 1532938 finetune.py:68] layer 22_o @ epoch 0 new loss 0.000294929021038115 old loss 0.00031066889641806483 BETTER
I0321 01:03:55.141704 1532884 finetune.py:68] layer 21_o @ epoch 3 new loss 0.00022048571554478258 old loss 0.00022237372468225658 BETTER
I0321 01:04:04.829631 1532830 finetune.py:45] layer 20_up initial loss 0.000497755769174546
I0321 01:04:10.601933 1532992 finetune.py:68] layer 23_k @ epoch 4 new loss 0.00013730148202739656 old loss 0.00013796749408356845 BETTER
I0321 01:04:14.405280 1532938 finetune.py:68] layer 22_o @ epoch 1 new loss 0.0002885654103010893 old loss 0.000294929021038115 BETTER
I0321 01:04:30.153383 1532992 finetune.py:45] layer 23_o initial loss 0.00029000607901252806
I0321 01:04:30.591975 1532884 finetune.py:68] layer 21_o @ epoch 4 new loss 0.00021906429901719093 old loss 0.00022048571554478258 BETTER
I0321 01:04:37.223690 1532830 finetune.py:68] layer 20_up @ epoch 0 new loss 0.0004849411197938025 old loss 0.000497755769174546 BETTER
I0321 01:04:49.831569 1532938 finetune.py:68] layer 22_o @ epoch 2 new loss 0.00028421918977983296 old loss 0.0002885654103010893 BETTER
I0321 01:05:01.036681 1532884 finetune.py:45] layer 21_up initial loss 0.0005059015238657594
I0321 01:05:04.222888 1532992 finetune.py:68] layer 23_o @ epoch 0 new loss 0.00027839993708766997 old loss 0.00029000607901252806 BETTER
I0321 01:05:12.468164 1532830 finetune.py:68] layer 20_up @ epoch 1 new loss 0.0004777848953381181 old loss 0.0004849411197938025 BETTER
I0321 01:05:25.136290 1532938 finetune.py:68] layer 22_o @ epoch 3 new loss 0.0002810559526551515 old loss 0.00028421918977983296 BETTER
I0321 01:05:33.727409 1532884 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0004944676766172051 old loss 0.0005059015238657594 BETTER
I0321 01:05:39.464164 1532992 finetune.py:68] layer 23_o @ epoch 1 new loss 0.0002737513859756291 old loss 0.00027839993708766997 BETTER
I0321 01:05:47.598657 1532830 finetune.py:68] layer 20_up @ epoch 2 new loss 0.00047255511162802577 old loss 0.0004777848953381181 BETTER
I0321 01:06:01.398939 1532938 finetune.py:68] layer 22_o @ epoch 4 new loss 0.00027862301794812083 old loss 0.0002810559526551515 BETTER
I0321 01:06:06.558932 1532884 finetune.py:68] layer 21_up @ epoch 1 new loss 0.0004881775239482522 old loss 0.0004944676766172051 BETTER
I0321 01:06:14.228133 1532992 finetune.py:68] layer 23_o @ epoch 2 new loss 0.0002707076782826334 old loss 0.0002737513859756291 BETTER
I0321 01:06:22.588125 1532830 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0004684393643401563 old loss 0.00047255511162802577 BETTER
I0321 01:06:34.432730 1532938 finetune.py:45] layer 22_up initial loss 0.0006026945193298161
I0321 01:06:39.947707 1532884 finetune.py:68] layer 21_up @ epoch 2 new loss 0.00048363348469138145 old loss 0.0004881775239482522 BETTER
I0321 01:06:49.305163 1532992 finetune.py:68] layer 23_o @ epoch 3 new loss 0.00026852742303162813 old loss 0.0002707076782826334 BETTER
I0321 01:06:57.724244 1532830 finetune.py:68] layer 20_up @ epoch 4 new loss 0.0004649031034205109 old loss 0.0004684393643401563 BETTER
I0321 01:07:06.417852 1532938 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0005908194580115378 old loss 0.0006026945193298161 BETTER
I0321 01:07:13.139774 1532884 finetune.py:68] layer 21_up @ epoch 3 new loss 0.00048000915558077395 old loss 0.00048363348469138145 BETTER
I0321 01:07:23.767790 1532992 finetune.py:68] layer 23_o @ epoch 4 new loss 0.00026691818493418396 old loss 0.00026852742303162813 BETTER
I0321 01:07:27.568235 1532830 finetune.py:45] layer 20_gate initial loss 0.000669930363073945
I0321 01:07:40.129145 1532938 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0005839965306222439 old loss 0.0005908194580115378 BETTER
I0321 01:07:46.310713 1532884 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0004771284875459969 old loss 0.00048000915558077395 BETTER
I0321 01:07:56.840191 1532992 finetune.py:45] layer 23_up initial loss 0.0006226092227734625
I0321 01:07:59.515222 1532830 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.000663582410197705 old loss 0.000669930363073945 BETTER
I0321 01:08:14.357167 1532938 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0005789755377918482 old loss 0.0005839965306222439 BETTER
I0321 01:08:17.475458 1532884 finetune.py:45] layer 21_gate initial loss 0.0006994706345722079
I0321 01:08:29.243906 1532992 finetune.py:68] layer 23_up @ epoch 0 new loss 0.0006105939974077046 old loss 0.0006226092227734625 BETTER
I0321 01:08:33.447932 1532830 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0006593379657715559 old loss 0.000663582410197705 BETTER
I0321 01:08:49.104758 1532884 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0006941893370822072 old loss 0.0006994706345722079 BETTER
I0321 01:08:49.475317 1532938 finetune.py:68] layer 22_up @ epoch 3 new loss 0.0005748886615037918 old loss 0.0005789755377918482 BETTER
I0321 01:09:03.011027 1532992 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0006041303277015686 old loss 0.0006105939974077046 BETTER
I0321 01:09:07.048493 1532830 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.0006558477762155235 old loss 0.0006593379657715559 BETTER
I0321 01:09:22.171881 1532884 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0006905565387569368 old loss 0.0006941893370822072 BETTER
I0321 01:09:24.963551 1532938 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0005716704181395471 old loss 0.0005748886615037918 BETTER
I0321 01:09:36.391489 1532992 finetune.py:68] layer 23_up @ epoch 2 new loss 0.0005994006060063839 old loss 0.0006041303277015686 BETTER
I0321 01:09:40.103605 1532830 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.000653131166473031 old loss 0.0006558477762155235 BETTER
I0321 01:09:54.367081 1532884 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0006877022096887231 old loss 0.0006905565387569368 BETTER
I0321 01:09:58.263223 1532938 finetune.py:45] layer 22_gate initial loss 0.0008241258328780532
I0321 01:10:10.847904 1532992 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0005956318927928805 old loss 0.0005994006060063839 BETTER
I0321 01:10:14.091488 1532830 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.0006507900543510914 old loss 0.000653131166473031 BETTER
I0321 01:10:26.487568 1532884 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.0006853248924016953 old loss 0.0006877022096887231 BETTER
I0321 01:10:29.200902 1532938 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0008186083869077265 old loss 0.0008241258328780532 BETTER
I0321 01:10:45.642827 1532992 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0005925700534135103 old loss 0.0005956318927928805 BETTER
I0321 01:10:48.473658 1532830 finetune.py:45] layer 20_down initial loss 0.0011490528704598546
I0321 01:10:58.822881 1532884 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.0006834144587628543 old loss 0.0006853248924016953 BETTER
I0321 01:11:01.626806 1532938 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.0008146502659656107 old loss 0.0008186083869077265 BETTER
I0321 01:11:18.303092 1532992 finetune.py:45] layer 23_gate initial loss 0.0008770478889346123
I0321 01:11:19.107611 1532830 finetune.py:68] layer 20_down @ epoch 0 new loss 0.0011488735908642411 old loss 0.0011490528704598546 BETTER
I0321 01:11:32.250196 1532884 finetune.py:45] layer 21_down initial loss 0.001187665737234056
I0321 01:11:35.080812 1532938 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0008115581586025655 old loss 0.0008146502659656107 BETTER
I0321 01:11:50.328566 1532992 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0008715701405890286 old loss 0.0008770478889346123 BETTER
I0321 01:11:51.795632 1532830 finetune.py:68] layer 20_down @ epoch 1 new loss 0.001148761948570609 old loss 0.0011488735908642411 BETTER
I0321 01:12:02.428085 1532884 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0011875126510858536 old loss 0.001187665737234056 BETTER
I0321 01:12:08.573087 1532938 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.0008089736220426857 old loss 0.0008115581586025655 BETTER
I0321 01:12:23.192901 1532992 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.0008677689475007355 old loss 0.0008715701405890286 BETTER
I0321 01:12:24.114192 1532830 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0011486862786114216 old loss 0.001148761948570609 BETTER
I0321 01:12:32.977210 1532884 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0011874138144776225 old loss 0.0011875126510858536 BETTER
I0321 01:12:41.742061 1532938 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0008069004979915917 old loss 0.0008089736220426857 BETTER
I0321 01:12:54.978264 1532992 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0008648325456306338 old loss 0.0008677689475007355 BETTER
I0321 01:12:55.905971 1532830 finetune.py:68] layer 20_down @ epoch 3 new loss 0.001148632145486772 old loss 0.0011486862786114216 BETTER
I0321 01:13:02.651333 1532884 finetune.py:68] layer 21_down @ epoch 2 new loss 0.0011873524636030197 old loss 0.0011874138144776225 BETTER
I0321 01:13:17.428454 1532938 finetune.py:45] layer 22_down initial loss 0.0013625533320009708
I0321 01:13:28.043036 1532992 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.0008623891626484692 old loss 0.0008648325456306338 BETTER
I0321 01:13:28.798695 1532830 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0011485895374789834 old loss 0.001148632145486772 BETTER
20_v proxy err 0.009495759382843971 tr(WHW.T) 990.5983276367188
bpp_loss 3.6948658227920532
20_q proxy err 0.0014799815835431218 tr(WHW.T) 7154.125
bpp_loss 4.061401844024658
20_k proxy err 0.001037854584865272 tr(WHW.T) 10410.966796875
bpp_loss 4.090919494628906
20_o proxy err 0.006483473815023899 tr(WHW.T) 100.50587463378906
bpp_loss 3.6879305839538574
20_up proxy err 0.00589741999283433 tr(WHW.T) 2340.571044921875
bpp_loss 3.7647955251294514
20_gate proxy err 0.003528857370838523 tr(WHW.T) 4023.122802734375
bpp_loss 3.851736024368641
20_down proxy err 0.007598807103931904 tr(WHW.T) 276.1263732910156
bpp_loss 3.7547256780225178
I0321 01:13:32.831180 1532884 finetune.py:68] layer 21_down @ epoch 3 new loss 0.0011873055482283235 old loss 0.0011873524636030197 BETTER
I0321 01:13:47.472210 1532938 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0013623913982883096 old loss 0.0013625533320009708 BETTER
I0321 01:14:01.609218 1532992 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0008604180766269565 old loss 0.0008623891626484692 BETTER
I0321 01:14:04.276937 1532884 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0011872684117406607 old loss 0.0011873055482283235 BETTER
21_v proxy err 0.009113771840929985 tr(WHW.T) 1144.5655517578125
bpp_loss 3.730854034423828
21_q proxy err 0.0016363138565793633 tr(WHW.T) 7069.08203125
bpp_loss 4.025635242462158
21_k proxy err 0.0011768372496590018 tr(WHW.T) 9999.126953125
bpp_loss 4.042912721633911
21_o proxy err 0.006936784368008375 tr(WHW.T) 75.6786880493164
bpp_loss 3.7137084007263184
21_up proxy err 0.006202767137438059 tr(WHW.T) 2361.2958984375
bpp_loss 3.762755815372911
21_gate proxy err 0.0037724680732935667 tr(WHW.T) 4001.726806640625
bpp_loss 3.8609257187954213
21_down proxy err 0.007608325220644474 tr(WHW.T) 277.64752197265625
bpp_loss 3.7545848003653592
I0321 01:14:16.768948 1532938 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0013622831320390105 old loss 0.0013623913982883096 BETTER
I0321 01:14:29.691073 1532992 finetune.py:45] layer 23_down initial loss 0.0014340532943606377
I0321 01:14:46.022279 1532938 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0013622078113257885 old loss 0.0013622831320390105 BETTER
I0321 01:14:57.174055 1532992 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0014339144108816981 old loss 0.0014340532943606377 BETTER
I0321 01:15:15.741437 1532938 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0013621520483866334 old loss 0.0013622078113257885 BETTER
I0321 01:15:26.229003 1532992 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0014338272158056498 old loss 0.0014339144108816981 BETTER
I0321 01:15:29.201611 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 24 in 78.52735161781311s
I0321 01:15:33.802129 1533046 config.py:54] PyTorch version 2.6.0 available.
W0321 01:15:34.225864 1533046 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 01:15:35.464396 1533046 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 01:15:35.468758 1531629 quantize_finetune_llama.py:203] layer 25 gpu 1
I0321 01:15:35.485696 1533046 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 01:15:45.326352 1532938 finetune.py:68] layer 22_down @ epoch 4 new loss 0.001362113980576396 old loss 0.0013621520483866334 BETTER
22_v proxy err 0.008742555044591427 tr(WHW.T) 1243.2529296875
bpp_loss 3.735073447227478
22_q proxy err 0.0015559722669422626 tr(WHW.T) 7749.4443359375
bpp_loss 4.0586864948272705
22_k proxy err 0.0011519357794895768 tr(WHW.T) 10622.513671875
bpp_loss 4.079096555709839
22_o proxy err 0.005860623437911272 tr(WHW.T) 114.47338104248047
bpp_loss 3.7164711952209473
22_up proxy err 0.006238732952624559 tr(WHW.T) 2475.303955078125
bpp_loss 3.762170303699582
22_gate proxy err 0.0038399233017116785 tr(WHW.T) 4155.72119140625
bpp_loss 3.868806528490643
22_down proxy err 0.007569769863039255 tr(WHW.T) 313.14202880859375
bpp_loss 3.7551599546920422
I0321 01:15:50.608584 1533046 finetune.py:45] layer 24_v initial loss 0.0002795569889713079
W0321 01:15:50.608860 1533046 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 01:15:55.065541 1532992 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0014337679604068398 old loss 0.0014338272158056498 BETTER
I0321 01:16:24.070566 1532992 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0014337229076772928 old loss 0.0014337679604068398 BETTER
I0321 01:16:25.658396 1533046 finetune.py:68] layer 24_v @ epoch 0 new loss 0.00015225137758534402 old loss 0.0002795569889713079 BETTER
I0321 01:16:53.223186 1532992 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0014336920576170087 old loss 0.0014337229076772928 BETTER
23_v proxy err 0.00799660012125969 tr(WHW.T) 1486.037353515625
bpp_loss 3.792277693748474
23_q proxy err 0.0017618552083149552 tr(WHW.T) 7350.35498046875
bpp_loss 4.0475172996521
23_k proxy err 0.0013098182389512658 tr(WHW.T) 10003.33203125
bpp_loss 4.060006141662598
23_o proxy err 0.007099397014826536 tr(WHW.T) 85.35491943359375
bpp_loss 3.7769992351531982
23_up proxy err 0.006469112355262041 tr(WHW.T) 2534.113525390625
bpp_loss 3.768257318541061
23_gate proxy err 0.004119206219911575 tr(WHW.T) 4098.439453125
bpp_loss 3.868452560069949
23_down proxy err 0.0076078674755990505 tr(WHW.T) 322.51385498046875
bpp_loss 3.7624289490455807
I0321 01:17:01.026193 1533046 finetune.py:68] layer 24_v @ epoch 1 new loss 0.0001373601844534278 old loss 0.00015225137758534402 BETTER
I0321 01:17:03.823156 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 25 in 72.09773206710815s
I0321 01:17:07.802906 1533100 config.py:54] PyTorch version 2.6.0 available.
W0321 01:17:08.205674 1533100 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 01:17:09.287314 1533100 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 01:17:09.291730 1531629 quantize_finetune_llama.py:203] layer 26 gpu 2
I0321 01:17:09.310634 1533100 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 01:17:23.164891 1533100 finetune.py:45] layer 25_v initial loss 0.00029220592114143074
W0321 01:17:23.165178 1533100 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 01:17:37.847683 1533046 finetune.py:68] layer 24_v @ epoch 2 new loss 0.0001298434508498758 old loss 0.0001373601844534278 BETTER
I0321 01:17:55.380780 1533100 finetune.py:68] layer 25_v @ epoch 0 new loss 0.0001377240987494588 old loss 0.00029220592114143074 BETTER
I0321 01:18:15.121842 1533046 finetune.py:68] layer 24_v @ epoch 3 new loss 0.0001255666429642588 old loss 0.0001298434508498758 BETTER
I0321 01:18:23.214286 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 26 in 73.39935731887817s
I0321 01:18:27.619904 1533154 config.py:54] PyTorch version 2.6.0 available.
W0321 01:18:28.046684 1533154 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 01:18:29.252549 1533154 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 01:18:29.259160 1531629 quantize_finetune_llama.py:203] layer 27 gpu 3
I0321 01:18:29.277085 1533154 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0321 01:18:29.788973 1533100 finetune.py:68] layer 25_v @ epoch 1 new loss 0.00012453617819119245 old loss 0.0001377240987494588 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 01:18:43.520350 1533154 finetune.py:45] layer 26_v initial loss 0.00040639343205839396
W0321 01:18:43.520741 1533154 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 01:18:52.324645 1533046 finetune.py:68] layer 24_v @ epoch 4 new loss 0.00012235318718012422 old loss 0.0001255666429642588 BETTER
I0321 01:19:04.983516 1533100 finetune.py:68] layer 25_v @ epoch 2 new loss 0.00011861325037898496 old loss 0.00012453617819119245 BETTER
I0321 01:19:09.387746 1533046 finetune.py:45] layer 24_q initial loss 0.00015985956997610629
I0321 01:19:17.639566 1533154 finetune.py:68] layer 26_v @ epoch 0 new loss 0.00020546119776554406 old loss 0.00040639343205839396 BETTER
I0321 01:19:41.584671 1533100 finetune.py:68] layer 25_v @ epoch 3 new loss 0.00011504221038194373 old loss 0.00011861325037898496 BETTER
I0321 01:19:45.288474 1533046 finetune.py:68] layer 24_q @ epoch 0 new loss 0.0001501584192737937 old loss 0.00015985956997610629 BETTER
I0321 01:19:47.187202 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 27 in 77.31166744232178s
I0321 01:19:52.200505 1533208 config.py:54] PyTorch version 2.6.0 available.
W0321 01:19:52.646953 1533208 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0321 01:19:52.866152 1533154 finetune.py:68] layer 26_v @ epoch 1 new loss 0.0001876178284874186 old loss 0.00020546119776554406 BETTER
W0321 01:19:53.797085 1533208 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 01:19:53.801446 1531629 quantize_finetune_llama.py:203] layer 28 gpu 0
I0321 01:19:53.815699 1533208 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 01:20:09.176464 1533208 finetune.py:45] layer 27_v initial loss 0.0003007073828484863
W0321 01:20:09.176703 1533208 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 01:20:18.175615 1533100 finetune.py:68] layer 25_v @ epoch 4 new loss 0.00011220735177630559 old loss 0.00011504221038194373 BETTER
I0321 01:20:22.214499 1533046 finetune.py:68] layer 24_q @ epoch 1 new loss 0.00014613696839660406 old loss 0.0001501584192737937 BETTER
I0321 01:20:28.629967 1533154 finetune.py:68] layer 26_v @ epoch 2 new loss 0.00017883031978271902 old loss 0.0001876178284874186 BETTER
I0321 01:20:36.188276 1533100 finetune.py:45] layer 25_q initial loss 0.00014773383736610413
I0321 01:20:42.915957 1533208 finetune.py:68] layer 27_v @ epoch 0 new loss 0.00015776777581777424 old loss 0.0003007073828484863 BETTER
I0321 01:20:59.710839 1533046 finetune.py:68] layer 24_q @ epoch 2 new loss 0.00014328367251437157 old loss 0.00014613696839660406 BETTER
I0321 01:21:05.960812 1533154 finetune.py:68] layer 26_v @ epoch 3 new loss 0.0001728776696836576 old loss 0.00017883031978271902 BETTER
I0321 01:21:10.765991 1533100 finetune.py:68] layer 25_q @ epoch 0 new loss 0.0001377674052491784 old loss 0.00014773383736610413 BETTER
I0321 01:21:18.330367 1533208 finetune.py:68] layer 27_v @ epoch 1 new loss 0.0001483630039729178 old loss 0.00015776777581777424 BETTER
I0321 01:21:37.499672 1533046 finetune.py:68] layer 24_q @ epoch 3 new loss 0.00014114902296569198 old loss 0.00014328367251437157 BETTER
I0321 01:21:43.227653 1533154 finetune.py:68] layer 26_v @ epoch 4 new loss 0.00016883820353541523 old loss 0.0001728776696836576 BETTER
I0321 01:21:46.449751 1533100 finetune.py:68] layer 25_q @ epoch 1 new loss 0.00013430211402010173 old loss 0.0001377674052491784 BETTER
I0321 01:21:53.153489 1533208 finetune.py:68] layer 27_v @ epoch 2 new loss 0.0001433986471965909 old loss 0.0001483630039729178 BETTER
I0321 01:22:01.008831 1533154 finetune.py:45] layer 26_q initial loss 0.00022217932564672083
I0321 01:22:15.485085 1533046 finetune.py:68] layer 24_q @ epoch 4 new loss 0.00013928127009421587 old loss 0.00014114902296569198 BETTER
I0321 01:22:21.599843 1533100 finetune.py:68] layer 25_q @ epoch 2 new loss 0.00013194484927225858 old loss 0.00013430211402010173 BETTER
I0321 01:22:28.750071 1533208 finetune.py:68] layer 27_v @ epoch 3 new loss 0.00014013072359375656 old loss 0.0001433986471965909 BETTER
I0321 01:22:33.720013 1533046 finetune.py:45] layer 24_k initial loss 0.00017194339307025075
I0321 01:22:35.999382 1533154 finetune.py:68] layer 26_q @ epoch 0 new loss 0.00020703952759504318 old loss 0.00022217932564672083 BETTER
I0321 01:22:57.229507 1533100 finetune.py:68] layer 25_q @ epoch 3 new loss 0.00013031085836701095 old loss 0.00013194484927225858 BETTER
I0321 01:23:05.455386 1533208 finetune.py:68] layer 27_v @ epoch 4 new loss 0.00013730743376072496 old loss 0.00014013072359375656 BETTER
I0321 01:23:09.916160 1533046 finetune.py:68] layer 24_k @ epoch 0 new loss 0.00016778950521256775 old loss 0.00017194339307025075 BETTER
I0321 01:23:12.201581 1533154 finetune.py:68] layer 26_q @ epoch 1 new loss 0.00020146227325312793 old loss 0.00020703952759504318 BETTER
I0321 01:23:23.819519 1533208 finetune.py:45] layer 27_q initial loss 0.00019030288967769593
I0321 01:23:32.546473 1533100 finetune.py:68] layer 25_q @ epoch 4 new loss 0.000128726358525455 old loss 0.00013031085836701095 BETTER
I0321 01:23:47.662195 1533046 finetune.py:68] layer 24_k @ epoch 1 new loss 0.0001661494461586699 old loss 0.00016778950521256775 BETTER
I0321 01:23:49.396842 1533154 finetune.py:68] layer 26_q @ epoch 2 new loss 0.00019743137818295509 old loss 0.00020146227325312793 BETTER
I0321 01:23:50.132296 1533100 finetune.py:45] layer 25_k initial loss 0.00015988883387763053
I0321 01:23:58.759802 1533208 finetune.py:68] layer 27_q @ epoch 0 new loss 0.00017517420928925276 old loss 0.00019030288967769593 BETTER
I0321 01:24:25.653455 1533046 finetune.py:68] layer 24_k @ epoch 2 new loss 0.00016489013796672225 old loss 0.0001661494461586699 BETTER
I0321 01:24:25.800625 1533100 finetune.py:68] layer 25_k @ epoch 0 new loss 0.0001561195240356028 old loss 0.00015988883387763053 BETTER
I0321 01:24:26.920759 1533154 finetune.py:68] layer 26_q @ epoch 3 new loss 0.0001944196264958009 old loss 0.00019743137818295509 BETTER
I0321 01:24:34.482246 1533208 finetune.py:68] layer 27_q @ epoch 1 new loss 0.0001710340438876301 old loss 0.00017517420928925276 BETTER
I0321 01:25:03.287653 1533100 finetune.py:68] layer 25_k @ epoch 1 new loss 0.0001549530861666426 old loss 0.0001561195240356028 BETTER
I0321 01:25:04.229989 1533046 finetune.py:68] layer 24_k @ epoch 3 new loss 0.00016383131151087582 old loss 0.00016489013796672225 BETTER
I0321 01:25:04.726349 1533154 finetune.py:68] layer 26_q @ epoch 4 new loss 0.00019190926104784012 old loss 0.0001944196264958009 BETTER
I0321 01:25:09.812129 1533208 finetune.py:68] layer 27_q @ epoch 2 new loss 0.0001680202258285135 old loss 0.0001710340438876301 BETTER
I0321 01:25:22.480970 1533154 finetune.py:45] layer 26_k initial loss 0.00023906798742245883
I0321 01:25:39.452125 1533100 finetune.py:68] layer 25_k @ epoch 2 new loss 0.00015409458137582988 old loss 0.0001549530861666426 BETTER
I0321 01:25:41.952172 1533046 finetune.py:68] layer 24_k @ epoch 4 new loss 0.00016295759996864945 old loss 0.00016383131151087582 BETTER
I0321 01:25:45.574266 1533208 finetune.py:68] layer 27_q @ epoch 3 new loss 0.00016570626758038998 old loss 0.0001680202258285135 BETTER
I0321 01:25:57.407924 1533154 finetune.py:68] layer 26_k @ epoch 0 new loss 0.00023050450545269996 old loss 0.00023906798742245883 BETTER
I0321 01:26:00.488230 1533046 finetune.py:45] layer 24_o initial loss 0.0003581741766538471
I0321 01:26:16.435807 1533100 finetune.py:68] layer 25_k @ epoch 3 new loss 0.00015329866437241435 old loss 0.00015409458137582988 BETTER
I0321 01:26:21.721215 1533208 finetune.py:68] layer 27_q @ epoch 4 new loss 0.00016376469284296036 old loss 0.00016570626758038998 BETTER
I0321 01:26:33.511106 1533154 finetune.py:68] layer 26_k @ epoch 1 new loss 0.00022839600569568574 old loss 0.00023050450545269996 BETTER
I0321 01:26:35.183350 1533046 finetune.py:68] layer 24_o @ epoch 0 new loss 0.00033947720658034086 old loss 0.0003581741766538471 BETTER
I0321 01:26:40.877246 1533208 finetune.py:45] layer 27_k initial loss 0.00020541784761007875
I0321 01:26:51.885002 1533100 finetune.py:68] layer 25_k @ epoch 4 new loss 0.00015271858137566596 old loss 0.00015329866437241435 BETTER
I0321 01:27:10.435689 1533100 finetune.py:45] layer 25_o initial loss 0.0003026821359526366
I0321 01:27:11.023589 1533154 finetune.py:68] layer 26_k @ epoch 2 new loss 0.0002267552918056026 old loss 0.00022839600569568574 BETTER
I0321 01:27:13.124059 1533046 finetune.py:68] layer 24_o @ epoch 1 new loss 0.0003337758535053581 old loss 0.00033947720658034086 BETTER
I0321 01:27:15.711638 1533208 finetune.py:68] layer 27_k @ epoch 0 new loss 0.00020046562713105232 old loss 0.00020541784761007875 BETTER
I0321 01:27:46.056799 1533100 finetune.py:68] layer 25_o @ epoch 0 new loss 0.0002933726937044412 old loss 0.0003026821359526366 BETTER
I0321 01:27:48.851488 1533154 finetune.py:68] layer 26_k @ epoch 3 new loss 0.0002254113496746868 old loss 0.0002267552918056026 BETTER
I0321 01:27:51.079911 1533046 finetune.py:68] layer 24_o @ epoch 2 new loss 0.00033010070910677314 old loss 0.0003337758535053581 BETTER
I0321 01:27:52.378223 1533208 finetune.py:68] layer 27_k @ epoch 1 new loss 0.00019885855726897717 old loss 0.00020046562713105232 BETTER
I0321 01:28:22.220802 1533100 finetune.py:68] layer 25_o @ epoch 1 new loss 0.00029000252834521234 old loss 0.0002933726937044412 BETTER
I0321 01:28:25.807453 1533154 finetune.py:68] layer 26_k @ epoch 4 new loss 0.00022430640819948167 old loss 0.0002254113496746868 BETTER
I0321 01:28:28.550342 1533046 finetune.py:68] layer 24_o @ epoch 3 new loss 0.0003274007758591324 old loss 0.00033010070910677314 BETTER
I0321 01:28:28.925757 1533208 finetune.py:68] layer 27_k @ epoch 2 new loss 0.0001977608335437253 old loss 0.00019885855726897717 BETTER
I0321 01:28:43.897282 1533154 finetune.py:45] layer 26_o initial loss 0.00047104907571338117
I0321 01:28:57.303089 1533100 finetune.py:68] layer 25_o @ epoch 2 new loss 0.00028783315792679787 old loss 0.00029000252834521234 BETTER
I0321 01:29:05.222540 1533208 finetune.py:68] layer 27_k @ epoch 3 new loss 0.00019698039977811277 old loss 0.0001977608335437253 BETTER
I0321 01:29:05.772926 1533046 finetune.py:68] layer 24_o @ epoch 4 new loss 0.000325430475641042 old loss 0.0003274007758591324 BETTER
I0321 01:29:17.802685 1533154 finetune.py:68] layer 26_o @ epoch 0 new loss 0.00044068359420634806 old loss 0.00047104907571338117 BETTER
I0321 01:29:32.140447 1533100 finetune.py:68] layer 25_o @ epoch 3 new loss 0.00028625220875255764 old loss 0.00028783315792679787 BETTER
I0321 01:29:36.383002 1533046 finetune.py:45] layer 24_up initial loss 0.0007097583147697151
I0321 01:29:40.228916 1533208 finetune.py:68] layer 27_k @ epoch 4 new loss 0.00019612538744695485 old loss 0.00019698039977811277 BETTER
I0321 01:29:52.731080 1533154 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0004335798730608076 old loss 0.00044068359420634806 BETTER
I0321 01:29:57.354886 1533208 finetune.py:45] layer 27_o initial loss 0.00040286354487761855
I0321 01:30:07.607494 1533100 finetune.py:68] layer 25_o @ epoch 4 new loss 0.00028514687437564135 old loss 0.00028625220875255764 BETTER
I0321 01:30:10.234189 1533046 finetune.py:68] layer 24_up @ epoch 0 new loss 0.0006983065977692604 old loss 0.0007097583147697151 BETTER
I0321 01:30:29.088960 1533154 finetune.py:68] layer 26_o @ epoch 2 new loss 0.00042876569204963744 old loss 0.0004335798730608076 BETTER
I0321 01:30:30.823272 1533208 finetune.py:68] layer 27_o @ epoch 0 new loss 0.0003808709152508527 old loss 0.00040286354487761855 BETTER
I0321 01:30:38.172611 1533100 finetune.py:45] layer 25_up initial loss 0.0007171910256147385
I0321 01:30:45.037259 1533046 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0006919120787642896 old loss 0.0006983065977692604 BETTER
I0321 01:31:06.065283 1533154 finetune.py:68] layer 26_o @ epoch 3 new loss 0.0004253232036717236 old loss 0.00042876569204963744 BETTER
I0321 01:31:06.494677 1533208 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0003734331112354994 old loss 0.0003808709152508527 BETTER
I0321 01:31:11.424662 1533100 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0007037596660666168 old loss 0.0007171910256147385 BETTER
I0321 01:31:20.202986 1533046 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0006870579672977328 old loss 0.0006919120787642896 BETTER
I0321 01:31:42.638067 1533208 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0003685205592773855 old loss 0.0003734331112354994 BETTER
I0321 01:31:42.801709 1533154 finetune.py:68] layer 26_o @ epoch 4 new loss 0.00042269908590242267 old loss 0.0004253232036717236 BETTER
I0321 01:31:45.513895 1533100 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0006966387736611068 old loss 0.0007037596660666168 BETTER
I0321 01:31:54.918625 1533046 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0006832670187577605 old loss 0.0006870579672977328 BETTER
I0321 01:32:12.508337 1533154 finetune.py:45] layer 26_up initial loss 0.0008936855010688305
I0321 01:32:17.734172 1533208 finetune.py:68] layer 27_o @ epoch 3 new loss 0.0003652486193459481 old loss 0.0003685205592773855 BETTER
I0321 01:32:19.495105 1533100 finetune.py:68] layer 25_up @ epoch 2 new loss 0.0006913919933140278 old loss 0.0006966387736611068 BETTER
I0321 01:32:31.173440 1533046 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0006802083225920796 old loss 0.0006832670187577605 BETTER
I0321 01:32:44.857897 1533154 finetune.py:68] layer 26_up @ epoch 0 new loss 0.0008781251963227987 old loss 0.0008936855010688305 BETTER
I0321 01:32:52.866405 1533208 finetune.py:68] layer 27_o @ epoch 4 new loss 0.0003625907702371478 old loss 0.0003652486193459481 BETTER
I0321 01:32:53.549805 1533100 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0006874749669805169 old loss 0.0006913919933140278 BETTER
I0321 01:33:02.257656 1533046 finetune.py:45] layer 24_gate initial loss 0.0009950886014848948
I0321 01:33:19.162699 1533154 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0008698469609953463 old loss 0.0008781251963227987 BETTER
I0321 01:33:23.789046 1533208 finetune.py:45] layer 27_up initial loss 0.0009015796240419149
I0321 01:33:27.805212 1533100 finetune.py:68] layer 25_up @ epoch 4 new loss 0.0006841762224212289 old loss 0.0006874749669805169 BETTER
I0321 01:33:34.099822 1533046 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0009899234864860773 old loss 0.0009950886014848948 BETTER
I0321 01:33:53.874260 1533154 finetune.py:68] layer 26_up @ epoch 2 new loss 0.0008637664723210037 old loss 0.0008698469609953463 BETTER
I0321 01:33:56.484182 1533208 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0008795182220637798 old loss 0.0009015796240419149 BETTER
I0321 01:33:57.830711 1533100 finetune.py:45] layer 25_gate initial loss 0.001035807654261589
I0321 01:34:07.911319 1533046 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0009862642036750913 old loss 0.0009899234864860773 BETTER
I0321 01:34:29.836702 1533154 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0008591496152803302 old loss 0.0008637664723210037 BETTER
I0321 01:34:30.019493 1533100 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.0010303136659786105 old loss 0.001035807654261589 BETTER
I0321 01:34:31.063536 1533208 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0008691328694112599 old loss 0.0008795182220637798 BETTER
I0321 01:34:41.909196 1533046 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.0009833277435973287 old loss 0.0009862642036750913 BETTER
I0321 01:35:03.360797 1533100 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.0010264539159834385 old loss 0.0010303136659786105 BETTER
I0321 01:35:05.516990 1533154 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0008553497609682381 old loss 0.0008591496152803302 BETTER
I0321 01:35:05.941035 1533208 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0008618014398962259 old loss 0.0008691328694112599 BETTER
I0321 01:35:15.130544 1533046 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0009808671893551946 old loss 0.0009833277435973287 BETTER
I0321 01:35:36.651076 1533100 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.001023375429213047 old loss 0.0010264539159834385 BETTER
I0321 01:35:37.506721 1533154 finetune.py:45] layer 26_gate initial loss 0.0012502992758527398
I0321 01:35:39.967703 1533208 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0008562159491702914 old loss 0.0008618014398962259 BETTER
I0321 01:35:49.122443 1533046 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.00097893294878304 old loss 0.0009808671893551946 BETTER
I0321 01:36:09.487353 1533154 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0012439399724826217 old loss 0.0012502992758527398 BETTER
I0321 01:36:10.109051 1533100 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.001020948402583599 old loss 0.001023375429213047 BETTER
I0321 01:36:13.714043 1533208 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0008517460664734244 old loss 0.0008562159491702914 BETTER
I0321 01:36:22.010742 1533046 finetune.py:45] layer 24_down initial loss 0.0015935797709971666
I0321 01:36:42.406134 1533154 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.001239586272276938 old loss 0.0012439399724826217 BETTER
I0321 01:36:42.636625 1533100 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0010188680607825518 old loss 0.001020948402583599 BETTER
I0321 01:36:46.285818 1533208 finetune.py:45] layer 27_gate initial loss 0.0013101542135700583
I0321 01:36:51.855042 1533046 finetune.py:68] layer 24_down @ epoch 0 new loss 0.001593399909324944 old loss 0.0015935797709971666 BETTER
I0321 01:37:15.588928 1533154 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0012360549299046397 old loss 0.001239586272276938 BETTER
I0321 01:37:17.542100 1533100 finetune.py:45] layer 25_down initial loss 0.0016917851753532887
I0321 01:37:18.250757 1533208 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0013013877905905247 old loss 0.0013101542135700583 BETTER
I0321 01:37:24.355967 1533046 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0015932853566482663 old loss 0.001593399909324944 BETTER
I0321 01:37:47.697677 1533100 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0016915927408263087 old loss 0.0016917851753532887 BETTER
I0321 01:37:50.403253 1533154 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0012332390761002898 old loss 0.0012360549299046397 BETTER
I0321 01:37:51.209281 1533208 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.0012957199942320585 old loss 0.0013013877905905247 BETTER
I0321 01:37:56.354971 1533046 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0015932029346004128 old loss 0.0015932853566482663 BETTER
I0321 01:38:19.661887 1533100 finetune.py:68] layer 25_down @ epoch 1 new loss 0.001691449317149818 old loss 0.0016915927408263087 BETTER
I0321 01:38:24.695917 1533208 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.001291377004235983 old loss 0.0012957199942320585 BETTER
I0321 01:38:25.006121 1533154 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.001230859663337469 old loss 0.0012332390761002898 BETTER
I0321 01:38:28.456458 1533046 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0015931398374959826 old loss 0.0015932029346004128 BETTER
I0321 01:38:50.791581 1533100 finetune.py:68] layer 25_down @ epoch 2 new loss 0.001691343728452921 old loss 0.001691449317149818 BETTER
I0321 01:38:56.727776 1533208 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0012879858259111643 old loss 0.001291377004235983 BETTER
I0321 01:39:01.184689 1533154 finetune.py:45] layer 26_down initial loss 0.001968629425391555
I0321 01:39:01.376612 1533046 finetune.py:68] layer 24_down @ epoch 4 new loss 0.001593089196830988 old loss 0.0015931398374959826 BETTER
24_v proxy err 0.008438339456915855 tr(WHW.T) 1394.900634765625
bpp_loss 3.7807196378707886
24_q proxy err 0.001813399256207049 tr(WHW.T) 7023.85205078125
bpp_loss 4.003644943237305
24_k proxy err 0.0012528665829449892 tr(WHW.T) 10343.091796875
bpp_loss 4.011998176574707
24_o proxy err 0.005614766385406256 tr(WHW.T) 134.20298767089844
bpp_loss 3.7592103481292725
24_up proxy err 0.00655966904014349 tr(WHW.T) 2622.575439453125
bpp_loss 3.772105194801508
24_gate proxy err 0.0041555846109986305 tr(WHW.T) 4264.2001953125
bpp_loss 3.871823865313863
24_down proxy err 0.0076508293859660625 tr(WHW.T) 341.39434814453125
bpp_loss 3.7668465237284816
I0321 01:39:23.008875 1533100 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0016912572318688035 old loss 0.001691343728452921 BETTER
I0321 01:39:29.997839 1533208 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.0012850528582930565 old loss 0.0012879858259111643 BETTER
I0321 01:39:31.594569 1533154 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0019684515427798033 old loss 0.001968629425391555 BETTER
I0321 01:39:52.565755 1533100 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0016911919228732586 old loss 0.0016912572318688035 BETTER
25_v proxy err 0.007694459054619074 tr(WHW.T) 1707.664794921875
bpp_loss 3.828631639480591
25_q proxy err 0.001953070517629385 tr(WHW.T) 7164.638671875
bpp_loss 4.016658306121826
25_k proxy err 0.00147143448702991 tr(WHW.T) 9627.3720703125
bpp_loss 4.021947622299194
25_o proxy err 0.006710554473102093 tr(WHW.T) 83.77545166015625
bpp_loss 3.8104346990585327
25_up proxy err 0.006524302996695042 tr(WHW.T) 2805.936279296875
bpp_loss 3.7775820355082668
25_gate proxy err 0.004044110421091318 tr(WHW.T) 4666.63427734375
bpp_loss 3.8744283276934954
25_down proxy err 0.007605480961501598 tr(WHW.T) 374.92572021484375
bpp_loss 3.7730633713478268
I0321 01:39:58.391671 1533208 finetune.py:45] layer 27_down initial loss 0.002166906138882041
I0321 01:40:01.167063 1533154 finetune.py:68] layer 26_down @ epoch 1 new loss 0.001968328608199954 old loss 0.0019684515427798033 BETTER
I0321 01:40:26.496075 1533208 finetune.py:68] layer 27_down @ epoch 0 new loss 0.002166684251278639 old loss 0.002166906138882041 BETTER
I0321 01:40:30.994167 1533154 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0019682340789586306 old loss 0.001968328608199954 BETTER
I0321 01:40:55.398416 1533208 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0021665245294570923 old loss 0.002166684251278639 BETTER
I0321 01:41:00.545519 1533154 finetune.py:68] layer 26_down @ epoch 3 new loss 0.001968156546354294 old loss 0.0019682340789586306 BETTER
I0321 01:41:18.047553 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 28 in 77.97269010543823s
I0321 01:41:22.391499 1553746 config.py:54] PyTorch version 2.6.0 available.
W0321 01:41:22.824631 1553746 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 01:41:23.990349 1553746 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 01:41:23.994660 1531629 quantize_finetune_llama.py:203] layer 29 gpu 1
I0321 01:41:24.009543 1553746 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0321 01:41:24.745538 1533208 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0021663992665708065 old loss 0.0021665245294570923 BETTER
I0321 01:41:30.476446 1533154 finetune.py:68] layer 26_down @ epoch 4 new loss 0.0019680948462337255 old loss 0.001968156546354294 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
26_v proxy err 0.0077408673241734505 tr(WHW.T) 1668.8843994140625
bpp_loss 3.849985122680664
26_q proxy err 0.0018256963230669498 tr(WHW.T) 7473.96142578125
bpp_loss 3.992268919944763
26_k proxy err 0.001330613624304533 tr(WHW.T) 10508.3720703125
bpp_loss 4.004412889480591
26_o proxy err 0.004663027822971344 tr(WHW.T) 203.6204071044922
bpp_loss 3.841212272644043
26_up proxy err 0.006135771982371807 tr(WHW.T) 3156.6064453125
bpp_loss 3.782093136809593
26_gate proxy err 0.0037584532983601093 tr(WHW.T) 5305.44140625
bpp_loss 3.878652528274891
26_down proxy err 0.007810984738171101 tr(WHW.T) 402.8623352050781
bpp_loss 3.7768519867298216
I0321 01:41:39.033674 1553746 finetune.py:45] layer 28_v initial loss 0.00039407634176313877
W0321 01:41:39.034199 1553746 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 01:41:53.806807 1533208 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0021662896033376455 old loss 0.0021663992665708065 BETTER
I0321 01:42:14.627182 1553746 finetune.py:68] layer 28_v @ epoch 0 new loss 0.00020878245413769037 old loss 0.00039407634176313877 BETTER
I0321 01:42:22.937716 1533208 finetune.py:68] layer 27_down @ epoch 4 new loss 0.002166204387322068 old loss 0.0021662896033376455 BETTER
27_v proxy err 0.007044043857604265 tr(WHW.T) 1799.3350830078125
bpp_loss 3.8523597717285156
27_q proxy err 0.0017627582419663668 tr(WHW.T) 7692.38818359375
bpp_loss 4.049105167388916
27_k proxy err 0.0012873649829998612 tr(WHW.T) 10638.7177734375
bpp_loss 4.064507007598877
27_o proxy err 0.006322076078504324 tr(WHW.T) 126.49198913574219
bpp_loss 3.8503451347351074
27_up proxy err 0.005607373081147671 tr(WHW.T) 3691.2939453125
bpp_loss 3.7884668749432233
27_gate proxy err 0.003555997973307967 tr(WHW.T) 5987.9697265625
bpp_loss 3.8802117636037425
27_down proxy err 0.008001384325325489 tr(WHW.T) 468.82391357421875
bpp_loss 3.783640417941781
I0321 01:42:48.035091 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 29 in 70.83491063117981s
I0321 01:42:50.385097 1553746 finetune.py:68] layer 28_v @ epoch 1 new loss 0.00019633033662103117 old loss 0.00020878245413769037 BETTER
I0321 01:42:51.871515 1555363 config.py:54] PyTorch version 2.6.0 available.
W0321 01:42:52.238708 1555363 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 01:42:53.339892 1555363 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 01:42:53.343997 1531629 quantize_finetune_llama.py:203] layer 30 gpu 2
I0321 01:42:53.358388 1555363 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 01:43:06.904573 1555363 finetune.py:45] layer 29_v initial loss 0.0003683252143673599
W0321 01:43:06.904875 1555363 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 01:43:27.565106 1553746 finetune.py:68] layer 28_v @ epoch 2 new loss 0.00018958337022922933 old loss 0.00019633033662103117 BETTER
I0321 01:43:40.555418 1555363 finetune.py:68] layer 29_v @ epoch 0 new loss 0.00022802164312452078 old loss 0.0003683252143673599 BETTER
I0321 01:44:04.813457 1553746 finetune.py:68] layer 28_v @ epoch 3 new loss 0.00018504656327422708 old loss 0.00018958337022922933 BETTER
I0321 01:44:08.589764 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 30 in 74.74037480354309s
I0321 01:44:12.799503 1556370 config.py:54] PyTorch version 2.6.0 available.
W0321 01:44:13.232424 1556370 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 01:44:14.323734 1556370 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 01:44:14.328118 1531629 quantize_finetune_llama.py:203] layer 31 gpu 3
I0321 01:44:14.343750 1556370 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0321 01:44:15.253153 1555363 finetune.py:68] layer 29_v @ epoch 1 new loss 0.00021562547772191465 old loss 0.00022802164312452078 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 01:44:29.964196 1556370 finetune.py:45] layer 30_v initial loss 0.0003758613020181656
W0321 01:44:29.964587 1556370 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 01:44:42.186222 1553746 finetune.py:68] layer 28_v @ epoch 4 new loss 0.00018169953546021134 old loss 0.00018504656327422708 BETTER
I0321 01:44:50.209062 1555363 finetune.py:68] layer 29_v @ epoch 2 new loss 0.00020874709298368543 old loss 0.00021562547772191465 BETTER
I0321 01:44:59.804988 1553746 finetune.py:45] layer 28_q initial loss 0.00024119295994751155
I0321 01:45:04.454860 1556370 finetune.py:68] layer 30_v @ epoch 0 new loss 0.00021134060807526112 old loss 0.0003758613020181656 BETTER
I0321 01:45:26.423518 1555363 finetune.py:68] layer 29_v @ epoch 3 new loss 0.00020417655468918383 old loss 0.00020874709298368543 BETTER
I0321 01:45:34.731931 1531629 quantize_finetune_llama.py:234] computed original embedding for layer 31 in 79.79207468032837s
I0321 01:45:36.270077 1553746 finetune.py:68] layer 28_q @ epoch 0 new loss 0.00022751593496650457 old loss 0.00024119295994751155 BETTER
I0321 01:45:39.440958 1557181 config.py:54] PyTorch version 2.6.0 available.
I0321 01:45:39.832631 1556370 finetune.py:68] layer 30_v @ epoch 1 new loss 0.00020224247418809682 old loss 0.00021134060807526112 BETTER
W0321 01:45:39.846761 1557181 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 01:45:40.983607 1557181 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 01:45:41.001302 1557181 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 01:45:56.925449 1557181 finetune.py:45] layer 31_v initial loss 0.0007662412244826555
W0321 01:45:56.926002 1557181 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 01:46:02.790739 1555363 finetune.py:68] layer 29_v @ epoch 4 new loss 0.0002005295391427353 old loss 0.00020417655468918383 BETTER
I0321 01:46:14.042754 1553746 finetune.py:68] layer 28_q @ epoch 1 new loss 0.00022241483384277672 old loss 0.00022751593496650457 BETTER
I0321 01:46:16.103446 1556370 finetune.py:68] layer 30_v @ epoch 2 new loss 0.00019767227058764547 old loss 0.00020224247418809682 BETTER
I0321 01:46:21.974537 1555363 finetune.py:45] layer 29_q initial loss 0.00025471250410191715
I0321 01:46:31.965798 1557181 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0003352479252498597 old loss 0.0007662412244826555 BETTER
I0321 01:46:53.033823 1553746 finetune.py:68] layer 28_q @ epoch 2 new loss 0.00021878683764953166 old loss 0.00022241483384277672 BETTER
I0321 01:46:53.634732 1556370 finetune.py:68] layer 30_v @ epoch 3 new loss 0.00019302155124023557 old loss 0.00019767227058764547 BETTER
I0321 01:46:57.592365 1555363 finetune.py:68] layer 29_q @ epoch 0 new loss 0.00024188045063056052 old loss 0.00025471250410191715 BETTER
I0321 01:47:07.931375 1557181 finetune.py:68] layer 31_v @ epoch 1 new loss 0.0003184524830430746 old loss 0.0003352479252498597 BETTER
I0321 01:47:31.546751 1556370 finetune.py:68] layer 30_v @ epoch 4 new loss 0.00018986998475156724 old loss 0.00019302155124023557 BETTER
I0321 01:47:32.524329 1553746 finetune.py:68] layer 28_q @ epoch 3 new loss 0.00021602591732516885 old loss 0.00021878683764953166 BETTER
I0321 01:47:34.019062 1555363 finetune.py:68] layer 29_q @ epoch 1 new loss 0.00023683592735324055 old loss 0.00024188045063056052 BETTER
I0321 01:47:43.544204 1557181 finetune.py:68] layer 31_v @ epoch 2 new loss 0.00031777206459082663 old loss 0.0003184524830430746 BETTER
I0321 01:47:49.834758 1556370 finetune.py:45] layer 30_q initial loss 0.0002683945931494236
I0321 01:48:10.892049 1553746 finetune.py:68] layer 28_q @ epoch 4 new loss 0.00021376003860495985 old loss 0.00021602591732516885 BETTER
I0321 01:48:11.068987 1555363 finetune.py:68] layer 29_q @ epoch 2 new loss 0.00023364850494544953 old loss 0.00023683592735324055 BETTER
I0321 01:48:20.392155 1557181 finetune.py:68] layer 31_v @ epoch 3 new loss 0.00030273222364485264 old loss 0.00031777206459082663 BETTER
I0321 01:48:24.980050 1556370 finetune.py:68] layer 30_q @ epoch 0 new loss 0.000244801864027977 old loss 0.0002683945931494236 BETTER
I0321 01:48:29.740153 1553746 finetune.py:45] layer 28_k initial loss 0.0002669918758329004
I0321 01:48:46.969118 1555363 finetune.py:68] layer 29_q @ epoch 3 new loss 0.00023094004427548498 old loss 0.00023364850494544953 BETTER
I0321 01:48:57.299802 1557181 finetune.py:68] layer 31_v @ epoch 4 new loss 0.0002900941763073206 old loss 0.00030273222364485264 BETTER
I0321 01:49:01.819154 1556370 finetune.py:68] layer 30_q @ epoch 1 new loss 0.00023985872394405305 old loss 0.000244801864027977 BETTER
I0321 01:49:05.308935 1553746 finetune.py:68] layer 28_k @ epoch 0 new loss 0.00026106074801646173 old loss 0.0002669918758329004 BETTER
I0321 01:49:16.644150 1557181 finetune.py:45] layer 31_q initial loss 0.0005723866634070873
I0321 01:49:22.676282 1555363 finetune.py:68] layer 29_q @ epoch 4 new loss 0.00022893330606166273 old loss 0.00023094004427548498 BETTER
I0321 01:49:38.513082 1556370 finetune.py:68] layer 30_q @ epoch 2 new loss 0.00023542804410681129 old loss 0.00023985872394405305 BETTER
I0321 01:49:41.010153 1555363 finetune.py:45] layer 29_k initial loss 0.0002791690931189805
I0321 01:49:43.192928 1553746 finetune.py:68] layer 28_k @ epoch 1 new loss 0.0002590285730548203 old loss 0.00026106074801646173 BETTER
I0321 01:49:50.567238 1557181 finetune.py:68] layer 31_q @ epoch 0 new loss 0.00048679206520318985 old loss 0.0005723866634070873 BETTER
I0321 01:50:16.616399 1556370 finetune.py:68] layer 30_q @ epoch 3 new loss 0.00023254001280292869 old loss 0.00023542804410681129 BETTER
I0321 01:50:16.840017 1555363 finetune.py:68] layer 29_k @ epoch 0 new loss 0.00027359227533452213 old loss 0.0002791690931189805 BETTER
I0321 01:50:20.363255 1553746 finetune.py:68] layer 28_k @ epoch 2 new loss 0.00025770097272470593 old loss 0.0002590285730548203 BETTER
I0321 01:50:26.548102 1557181 finetune.py:68] layer 31_q @ epoch 1 new loss 0.00046986344386823475 old loss 0.00048679206520318985 BETTER
I0321 01:50:53.673724 1555363 finetune.py:68] layer 29_k @ epoch 1 new loss 0.00027174243587069213 old loss 0.00027359227533452213 BETTER
I0321 01:50:53.782184 1556370 finetune.py:68] layer 30_q @ epoch 4 new loss 0.00023187680926639587 old loss 0.00023254001280292869 BETTER
I0321 01:50:58.222079 1553746 finetune.py:68] layer 28_k @ epoch 3 new loss 0.00025652049225755036 old loss 0.00025770097272470593 BETTER
I0321 01:51:01.660218 1557181 finetune.py:68] layer 31_q @ epoch 2 new loss 0.00045565119944512844 old loss 0.00046986344386823475 BETTER
I0321 01:51:12.154917 1556370 finetune.py:45] layer 30_k initial loss 0.00029395465389825404
I0321 01:51:29.895512 1555363 finetune.py:68] layer 29_k @ epoch 2 new loss 0.0002699958859011531 old loss 0.00027174243587069213 BETTER
I0321 01:51:36.067008 1553746 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0002554982784204185 old loss 0.00025652049225755036 BETTER
I0321 01:51:37.726478 1557181 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0004507542180363089 old loss 0.00045565119944512844 BETTER
I0321 01:51:46.873150 1556370 finetune.py:68] layer 30_k @ epoch 0 new loss 0.0002848550211638212 old loss 0.00029395465389825404 BETTER
I0321 01:51:54.220628 1553746 finetune.py:45] layer 28_o initial loss 0.0005253828130662441
I0321 01:52:06.125712 1555363 finetune.py:68] layer 29_k @ epoch 3 new loss 0.00026874919421970844 old loss 0.0002699958859011531 BETTER
I0321 01:52:13.620290 1557181 finetune.py:68] layer 31_q @ epoch 4 new loss 0.0004420219920575619 old loss 0.0004507542180363089 BETTER
I0321 01:52:23.011398 1556370 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0002827004645951092 old loss 0.0002848550211638212 BETTER
I0321 01:52:29.055606 1553746 finetune.py:68] layer 28_o @ epoch 0 new loss 0.0004989016451872885 old loss 0.0005253828130662441 BETTER
I0321 01:52:33.198284 1557181 finetune.py:45] layer 31_k initial loss 0.0005495393997989595
I0321 01:52:41.990845 1555363 finetune.py:68] layer 29_k @ epoch 4 new loss 0.0002676151634659618 old loss 0.00026874919421970844 BETTER
I0321 01:52:59.951936 1555363 finetune.py:45] layer 29_o initial loss 0.0005088144680485129
I0321 01:53:00.252090 1556370 finetune.py:76] layer 30_k @ epoch 2 new loss 0.0002832668833434582 old loss 0.0002827004645951092 WORSE
I0321 01:53:06.615438 1553746 finetune.py:68] layer 28_o @ epoch 1 new loss 0.0004898022743873298 old loss 0.0004989016451872885 BETTER
I0321 01:53:08.105590 1557181 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0005131355137564242 old loss 0.0005495393997989595 BETTER
I0321 01:53:35.043028 1555363 finetune.py:68] layer 29_o @ epoch 0 new loss 0.0004852444981224835 old loss 0.0005088144680485129 BETTER
I0321 01:53:35.930986 1556370 finetune.py:68] layer 30_k @ epoch 3 new loss 0.0002804539690259844 old loss 0.0002827004645951092 BETTER
I0321 01:53:43.968935 1553746 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0004837728920392692 old loss 0.0004898022743873298 BETTER
I0321 01:53:44.571663 1557181 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0005027707666158676 old loss 0.0005131355137564242 BETTER
I0321 01:54:10.721333 1555363 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0004785002092830837 old loss 0.0004852444981224835 BETTER
I0321 01:54:12.559720 1556370 finetune.py:68] layer 30_k @ epoch 4 new loss 0.0002792578306980431 old loss 0.0002804539690259844 BETTER
I0321 01:54:20.515453 1553746 finetune.py:68] layer 28_o @ epoch 3 new loss 0.00047949733561836183 old loss 0.0004837728920392692 BETTER
I0321 01:54:20.670120 1557181 finetune.py:68] layer 31_k @ epoch 2 new loss 0.0004971289308741689 old loss 0.0005027707666158676 BETTER
I0321 01:54:31.485536 1556370 finetune.py:45] layer 30_o initial loss 0.0005989645724184811
I0321 01:54:45.905610 1555363 finetune.py:68] layer 29_o @ epoch 2 new loss 0.000474487547762692 old loss 0.0004785002092830837 BETTER
I0321 01:54:56.831439 1557181 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0004957139608450234 old loss 0.0004971289308741689 BETTER
I0321 01:54:57.431257 1553746 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0004760568554047495 old loss 0.00047949733561836183 BETTER
I0321 01:55:05.424442 1556370 finetune.py:68] layer 30_o @ epoch 0 new loss 0.0005468404269777238 old loss 0.0005989645724184811 BETTER
I0321 01:55:20.384923 1555363 finetune.py:68] layer 29_o @ epoch 3 new loss 0.00047146622091531754 old loss 0.000474487547762692 BETTER
I0321 01:55:27.730807 1553746 finetune.py:45] layer 28_up initial loss 0.0011154478415846825
I0321 01:55:32.461001 1557181 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0004937908379361033 old loss 0.0004957139608450234 BETTER
I0321 01:55:40.119781 1556370 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0005330875283107162 old loss 0.0005468404269777238 BETTER
I0321 01:55:51.742966 1557181 finetune.py:45] layer 31_o initial loss 0.0009958751033991575
I0321 01:55:56.022436 1555363 finetune.py:68] layer 29_o @ epoch 4 new loss 0.00046941000618971884 old loss 0.00047146622091531754 BETTER
I0321 01:56:00.723588 1553746 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0010834806598722935 old loss 0.0011154478415846825 BETTER
I0321 01:56:15.404435 1556370 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0005248913075774908 old loss 0.0005330875283107162 BETTER
I0321 01:56:25.681488 1557181 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0008340222993865609 old loss 0.0009958751033991575 BETTER
I0321 01:56:26.797805 1555363 finetune.py:45] layer 29_up initial loss 0.0012271396117284894
I0321 01:56:35.667109 1553746 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0010697594843804836 old loss 0.0010834806598722935 BETTER
I0321 01:56:51.096027 1556370 finetune.py:68] layer 30_o @ epoch 3 new loss 0.000519196386449039 old loss 0.0005248913075774908 BETTER
I0321 01:56:59.832950 1555363 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0011778228217735887 old loss 0.0012271396117284894 BETTER
I0321 01:57:01.313919 1557181 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0008006860734894872 old loss 0.0008340222993865609 BETTER
I0321 01:57:10.801676 1553746 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0010598438093438745 old loss 0.0010697594843804836 BETTER
I0321 01:57:27.262830 1556370 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0005152503727003932 old loss 0.000519196386449039 BETTER
I0321 01:57:33.832489 1555363 finetune.py:68] layer 29_up @ epoch 1 new loss 0.0011600967263802886 old loss 0.0011778228217735887 BETTER
I0321 01:57:36.085920 1557181 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0007817532168701291 old loss 0.0008006860734894872 BETTER
I0321 01:57:45.658981 1553746 finetune.py:68] layer 28_up @ epoch 3 new loss 0.001052589388564229 old loss 0.0010598438093438745 BETTER
I0321 01:57:58.052129 1556370 finetune.py:45] layer 30_up initial loss 0.0017810534918680787
I0321 01:58:08.125566 1555363 finetune.py:68] layer 29_up @ epoch 2 new loss 0.001148118870332837 old loss 0.0011600967263802886 BETTER
I0321 01:58:11.202466 1557181 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0007702943403273821 old loss 0.0007817532168701291 BETTER
I0321 01:58:20.524612 1553746 finetune.py:68] layer 28_up @ epoch 4 new loss 0.001046599936671555 old loss 0.001052589388564229 BETTER
I0321 01:58:30.616613 1556370 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0016212423797696829 old loss 0.0017810534918680787 BETTER
I0321 01:58:42.128257 1555363 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0011391034349799156 old loss 0.001148118870332837 BETTER
I0321 01:58:46.360551 1557181 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0007635175134055316 old loss 0.0007702943403273821 BETTER
I0321 01:58:50.783119 1553746 finetune.py:45] layer 28_gate initial loss 0.0015999998431652784
I0321 01:59:04.001247 1556370 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0015719286166131496 old loss 0.0016212423797696829 BETTER
I0321 01:59:16.458048 1555363 finetune.py:68] layer 29_up @ epoch 4 new loss 0.001131989760324359 old loss 0.0011391034349799156 BETTER
I0321 01:59:18.262351 1557181 finetune.py:45] layer 31_up initial loss 0.004121832549571991
I0321 01:59:22.691401 1553746 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0015865025343373418 old loss 0.0015999998431652784 BETTER
I0321 01:59:38.076887 1556370 finetune.py:68] layer 30_up @ epoch 2 new loss 0.001539064571261406 old loss 0.0015719286166131496 BETTER
I0321 01:59:47.456083 1555363 finetune.py:45] layer 29_gate initial loss 0.0017930003814399242
I0321 01:59:50.741914 1557181 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0032711478415876627 old loss 0.004121832549571991 BETTER
I0321 01:59:56.262964 1553746 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0015788576565682888 old loss 0.0015865025343373418 BETTER
I0321 02:00:12.928664 1556370 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0015153530985116959 old loss 0.001539064571261406 BETTER
I0321 02:00:19.020134 1555363 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0017728013917803764 old loss 0.0017930003814399242 BETTER
I0321 02:00:25.568410 1557181 finetune.py:68] layer 31_up @ epoch 1 new loss 0.0030773470643907785 old loss 0.0032711478415876627 BETTER
I0321 02:00:30.085861 1553746 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.0015729953302070498 old loss 0.0015788576565682888 BETTER
I0321 02:00:48.292546 1556370 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0014967476017773151 old loss 0.0015153530985116959 BETTER
I0321 02:00:51.267468 1555363 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.0017625277396291494 old loss 0.0017728013917803764 BETTER
I0321 02:00:58.876364 1557181 finetune.py:68] layer 31_up @ epoch 2 new loss 0.002949758665636182 old loss 0.0030773470643907785 BETTER
I0321 02:01:03.654489 1553746 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0015682884259149432 old loss 0.0015729953302070498 BETTER
I0321 02:01:20.538274 1556370 finetune.py:45] layer 30_gate initial loss 0.0023510223254561424
I0321 02:01:23.740458 1555363 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0017549160402268171 old loss 0.0017625277396291494 BETTER
I0321 02:01:33.251423 1557181 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0028518252074718475 old loss 0.002949758665636182 BETTER
I0321 02:01:38.067544 1553746 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.0015643831575289369 old loss 0.0015682884259149432 BETTER
I0321 02:01:52.132408 1556370 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.002293875440955162 old loss 0.0023510223254561424 BETTER
I0321 02:01:56.129695 1555363 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0017487467266619205 old loss 0.0017549160402268171 BETTER
I0321 02:02:06.942310 1557181 finetune.py:68] layer 31_up @ epoch 4 new loss 0.002772043226286769 old loss 0.0028518252074718475 BETTER
I0321 02:02:11.690108 1553746 finetune.py:45] layer 28_down initial loss 0.0027195990551263094
I0321 02:02:24.552658 1556370 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.002266226802021265 old loss 0.002293875440955162 BETTER
I0321 02:02:27.743286 1555363 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0017438852228224277 old loss 0.0017487467266619205 BETTER
I0321 02:02:39.048671 1557181 finetune.py:45] layer 31_gate initial loss 0.004284719470888376
I0321 02:02:41.940869 1553746 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0027192882262170315 old loss 0.0027195990551263094 BETTER
I0321 02:02:57.660783 1556370 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.002247030846774578 old loss 0.002266226802021265 BETTER
I0321 02:03:02.025541 1555363 finetune.py:45] layer 29_down initial loss 0.003256241325289011
I0321 02:03:10.514994 1557181 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.003977902699261904 old loss 0.004284719470888376 BETTER
I0321 02:03:14.556897 1553746 finetune.py:68] layer 28_down @ epoch 1 new loss 0.002719037001952529 old loss 0.0027192882262170315 BETTER
I0321 02:03:31.311146 1556370 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.002230189274996519 old loss 0.002247030846774578 BETTER
I0321 02:03:32.477480 1555363 finetune.py:68] layer 29_down @ epoch 0 new loss 0.0032557640224695206 old loss 0.003256241325289011 BETTER
I0321 02:03:43.831949 1557181 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.0038917111232876778 old loss 0.003977902699261904 BETTER
I0321 02:03:47.091346 1553746 finetune.py:68] layer 28_down @ epoch 2 new loss 0.002718832576647401 old loss 0.002719037001952529 BETTER
I0321 02:04:03.908085 1555363 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0032553886994719505 old loss 0.0032557640224695206 BETTER
I0321 02:04:04.125822 1556370 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0022181617096066475 old loss 0.002230189274996519 BETTER
I0321 02:04:16.369607 1557181 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.0038357286248356104 old loss 0.0038917111232876778 BETTER
I0321 02:04:19.142395 1553746 finetune.py:68] layer 28_down @ epoch 3 new loss 0.0027186532970517874 old loss 0.002718832576647401 BETTER
I0321 02:04:34.480200 1555363 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0032550671603530645 old loss 0.0032553886994719505 BETTER
I0321 02:04:38.552399 1556370 finetune.py:45] layer 30_down initial loss 0.0062948428094387054
I0321 02:04:49.604734 1557181 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.0037914542481303215 old loss 0.0038357286248356104 BETTER
I0321 02:04:52.599755 1553746 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0027184970676898956 old loss 0.0027186532970517874 BETTER
28_v proxy err 0.006570468191057444 tr(WHW.T) 2018.944091796875
bpp_loss 3.89560866355896
28_q proxy err 0.0018216405296698213 tr(WHW.T) 7652.810546875
bpp_loss 4.0044286251068115
28_k proxy err 0.001337851514108479 tr(WHW.T) 10569.3076171875
bpp_loss 4.022932767868042
28_o proxy err 0.0052252644672989845 tr(WHW.T) 195.45721435546875
bpp_loss 3.8899452686309814
28_up proxy err 0.004705933853983879 tr(WHW.T) 4660.990234375
bpp_loss 3.8009631134742916
28_gate proxy err 0.003428705735132098 tr(WHW.T) 6541.9443359375
bpp_loss 3.8741888001907703
28_down proxy err 0.008054424077272415 tr(WHW.T) 606.7355346679688
bpp_loss 3.7913770453874456
I0321 02:05:06.713325 1555363 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0032547905575484037 old loss 0.0032550671603530645 BETTER
I0321 02:05:08.657699 1556370 finetune.py:68] layer 30_down @ epoch 0 new loss 0.006286986637860537 old loss 0.0062948428094387054 BETTER
I0321 02:05:22.665302 1557181 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.0037538812030106783 old loss 0.0037914542481303215 BETTER
I0321 02:05:36.990281 1555363 finetune.py:68] layer 29_down @ epoch 4 new loss 0.0032545309513807297 old loss 0.0032547905575484037 BETTER
I0321 02:05:38.356710 1556370 finetune.py:68] layer 30_down @ epoch 1 new loss 0.006281035952270031 old loss 0.006286986637860537 BETTER
29_v proxy err 0.006953801028430462 tr(WHW.T) 1801.7730712890625
bpp_loss 3.9010671377182007
29_q proxy err 0.0018023498123511672 tr(WHW.T) 7229.8076171875
bpp_loss 3.9646869897842407
29_k proxy err 0.0012519399169832468 tr(WHW.T) 10590.6572265625
bpp_loss 3.977916955947876
29_o proxy err 0.004420009441673756 tr(WHW.T) 208.31231689453125
bpp_loss 3.906263828277588
29_up proxy err 0.0037730783224105835 tr(WHW.T) 6070.54833984375
bpp_loss 3.8130264282226562
29_gate proxy err 0.003163976827636361 tr(WHW.T) 7368.54248046875
bpp_loss 3.877924542094386
29_down proxy err 0.00818187277764082 tr(WHW.T) 786.6886596679688
bpp_loss 3.7959108574445857
I0321 02:05:51.531499 1557181 finetune.py:45] layer 31_down initial loss 0.01789746806025505
I0321 02:06:07.829932 1556370 finetune.py:68] layer 30_down @ epoch 2 new loss 0.006276260130107403 old loss 0.006281035952270031 BETTER
I0321 02:06:19.037740 1557181 finetune.py:68] layer 31_down @ epoch 0 new loss 0.017846712842583656 old loss 0.01789746806025505 BETTER
I0321 02:06:37.610050 1556370 finetune.py:68] layer 30_down @ epoch 3 new loss 0.006272275932133198 old loss 0.006276260130107403 BETTER
I0321 02:06:47.910953 1557181 finetune.py:68] layer 31_down @ epoch 1 new loss 0.01781294122338295 old loss 0.017846712842583656 BETTER
I0321 02:07:07.088715 1556370 finetune.py:68] layer 30_down @ epoch 4 new loss 0.006269357167184353 old loss 0.006272275932133198 BETTER
30_v proxy err 0.005731129087507725 tr(WHW.T) 2261.489501953125
bpp_loss 3.9343677759170532
30_q proxy err 0.0017145464662462473 tr(WHW.T) 7817.3173828125
bpp_loss 3.974349617958069
30_k proxy err 0.001291528344154358 tr(WHW.T) 10548.5029296875
bpp_loss 3.994840145111084
30_o proxy err 0.004641400184482336 tr(WHW.T) 252.38368225097656
bpp_loss 3.942049741744995
30_up proxy err 0.0023423954844474792 tr(WHW.T) 10012.912109375
bpp_loss 3.834981696550236
30_gate proxy err 0.0021737900096923113 tr(WHW.T) 10993.8994140625
bpp_loss 3.9097825870957483
30_down proxy err 0.004750764928758144 tr(WHW.T) 3604.76904296875
bpp_loss 3.788120092347611
I0321 02:07:16.598538 1557181 finetune.py:68] layer 31_down @ epoch 2 new loss 0.017791112884879112 old loss 0.01781294122338295 BETTER
I0321 02:07:45.683369 1557181 finetune.py:68] layer 31_down @ epoch 3 new loss 0.017776453867554665 old loss 0.017791112884879112 BETTER
I0321 02:08:15.095820 1557181 finetune.py:68] layer 31_down @ epoch 4 new loss 0.01776657998561859 old loss 0.017776453867554665 BETTER
31_v proxy err 0.006751200649887323 tr(WHW.T) 1268.2034912109375
bpp_loss 3.806483745574951
31_q proxy err 0.001352202845737338 tr(WHW.T) 6858.7421875
bpp_loss 3.9920750856399536
31_k proxy err 0.000932298309635371 tr(WHW.T) 10261.8916015625
bpp_loss 4.047713994979858
31_o proxy err 0.0035219192504882812 tr(WHW.T) 459.12451171875
bpp_loss 3.819838047027588
31_up proxy err 0.001357648870907724 tr(WHW.T) 14572.5185546875
bpp_loss 3.8868780801462575
31_gate proxy err 0.0013602346880361438 tr(WHW.T) 14838.9658203125
bpp_loss 3.9718629703965296
31_down proxy err 0.003313876921311021 tr(WHW.T) 18042.32421875
bpp_loss 3.7869402197904365
I0321 02:08:45.505177 1569203 config.py:54] PyTorch version 2.6.0 available.
W0321 02:08:45.811083 1569203 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0321 02:08:46.064834 1569203 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  4.34it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:01,  3.42it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  3.78it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:01<00:00,  4.02it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  4.13it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.21it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.05it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  4.12it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  4.33it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  4.41it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  4.42it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  4.45it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.48it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.43it/s]
I0321 02:08:50.428634 1569203 hfize_llama.py:153] loaded layer 0
I0321 02:08:51.675064 1569203 hfize_llama.py:153] loaded layer 1
I0321 02:08:52.928828 1569203 hfize_llama.py:153] loaded layer 2
I0321 02:08:54.196387 1569203 hfize_llama.py:153] loaded layer 3
I0321 02:08:55.460003 1569203 hfize_llama.py:153] loaded layer 4
I0321 02:08:56.706109 1569203 hfize_llama.py:153] loaded layer 5
I0321 02:08:57.851398 1569203 hfize_llama.py:153] loaded layer 6
I0321 02:08:59.014499 1569203 hfize_llama.py:153] loaded layer 7
I0321 02:09:00.156434 1569203 hfize_llama.py:153] loaded layer 8
I0321 02:09:01.617247 1569203 hfize_llama.py:153] loaded layer 9
I0321 02:09:04.028112 1569203 hfize_llama.py:153] loaded layer 10
I0321 02:09:06.400655 1569203 hfize_llama.py:153] loaded layer 11
I0321 02:09:08.401057 1569203 hfize_llama.py:153] loaded layer 12
I0321 02:09:10.657602 1569203 hfize_llama.py:153] loaded layer 13
I0321 02:09:13.070815 1569203 hfize_llama.py:153] loaded layer 14
I0321 02:09:15.466403 1569203 hfize_llama.py:153] loaded layer 15
I0321 02:09:17.916662 1569203 hfize_llama.py:153] loaded layer 16
I0321 02:09:20.304811 1569203 hfize_llama.py:153] loaded layer 17
I0321 02:09:22.176852 1569203 hfize_llama.py:153] loaded layer 18
I0321 02:09:24.524621 1569203 hfize_llama.py:153] loaded layer 19
I0321 02:09:25.741387 1569203 hfize_llama.py:153] loaded layer 20
I0321 02:09:27.887377 1569203 hfize_llama.py:153] loaded layer 21
I0321 02:09:30.076395 1569203 hfize_llama.py:153] loaded layer 22
I0321 02:09:32.241462 1569203 hfize_llama.py:153] loaded layer 23
I0321 02:09:33.409488 1569203 hfize_llama.py:153] loaded layer 24
I0321 02:09:35.540385 1569203 hfize_llama.py:153] loaded layer 25
I0321 02:09:37.698853 1569203 hfize_llama.py:153] loaded layer 26
I0321 02:09:39.859276 1569203 hfize_llama.py:153] loaded layer 27
I0321 02:09:42.047581 1569203 hfize_llama.py:153] loaded layer 28
I0321 02:09:44.039177 1569203 hfize_llama.py:153] loaded layer 29
I0321 02:09:45.156432 1569203 hfize_llama.py:153] loaded layer 30
I0321 02:09:46.995291 1569203 hfize_llama.py:153] loaded layer 31
I0321 02:09:46.995407 1569203 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.02it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.09it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.16it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.22it/s]
I0321 02:10:49.357784 1569203 hfize_llama.py:167] successfully loaded hfized model
