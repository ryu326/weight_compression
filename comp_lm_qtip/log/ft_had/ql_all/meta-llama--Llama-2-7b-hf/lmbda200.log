I0321 02:15:56.091294 1573206 config.py:54] PyTorch version 2.6.0 available.
W0321 02:15:56.372471 1573206 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 02:15:57.331033 1573206 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.59it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.28it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.65it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.91it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.04it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.22it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.97it/s]
I0321 02:15:58.289436 1573206 quantize_finetune_llama.py:144] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:15,  2.01it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:00<00:14,  2.03it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:14,  2.05it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:01<00:13,  2.05it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:12,  2.14it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:02<00:11,  2.25it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:10,  2.35it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:03<00:09,  2.43it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:12,  1.89it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:14,  1.54it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:15,  1.38it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:07<00:15,  1.28it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:15,  1.22it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:08<00:15,  1.18it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:09<00:14,  1.16it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:10<00:14,  1.14it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:11<00:13,  1.13it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:12<00:12,  1.12it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:11,  1.11it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:14<00:10,  1.14it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:15<00:09,  1.17it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:08,  1.20it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:07,  1.21it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:06,  1.23it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:18<00:05,  1.24it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:19<00:04,  1.25it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:19<00:03,  1.25it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:20<00:03,  1.25it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:21<00:02,  1.26it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:22<00:01,  1.26it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:22<00:00,  1.27it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.27it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.35it/s]
I0321 02:16:31.022932 1573206 quantize_finetune_llama.py:179] loaded compression model
I0321 02:16:45.687725 1573206 quantize_finetune_llama.py:183] loaded dataset and devset
I0321 02:16:51.050789 1573206 quantize_finetune_llama.py:203] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 02:18:11.418202 1573206 quantize_finetune_llama.py:234] computed original embedding for layer 0 in 80.2235803604126s
tensor(0.0192) tensor(-3.6338e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0321 02:18:37.786275 1574694 config.py:54] PyTorch version 2.6.0 available.
W0321 02:18:38.069695 1574694 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 02:18:38.972051 1574694 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 02:18:38.976080 1573206 quantize_finetune_llama.py:203] layer 1 gpu 1
I0321 02:18:38.989498 1574694 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 02:18:50.504050 1574694 finetune.py:45] layer 0_v initial loss 2.738742978181108e-06
W0321 02:18:50.504366 1574694 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 02:19:23.557273 1574694 finetune.py:68] layer 0_v @ epoch 0 new loss 8.951240033638896e-07 old loss 2.738742978181108e-06 BETTER
I0321 02:19:51.465581 1573206 quantize_finetune_llama.py:234] computed original embedding for layer 1 in 72.3160388469696s
I0321 02:20:00.769737 1574694 finetune.py:68] layer 0_v @ epoch 1 new loss 3.8895700527064037e-07 old loss 8.951240033638896e-07 BETTER
I0321 02:20:04.392951 1575524 config.py:54] PyTorch version 2.6.0 available.
W0321 02:20:04.686644 1575524 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 02:20:05.622449 1575524 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 02:20:05.626182 1573206 quantize_finetune_llama.py:203] layer 2 gpu 2
I0321 02:20:05.639310 1575524 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 02:20:16.221770 1575524 finetune.py:45] layer 1_v initial loss 6.441713048843667e-05
W0321 02:20:16.221972 1575524 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 02:20:36.595500 1574694 finetune.py:68] layer 0_v @ epoch 2 new loss 2.2550554490408103e-07 old loss 3.8895700527064037e-07 BETTER
I0321 02:20:47.531830 1575524 finetune.py:68] layer 1_v @ epoch 0 new loss 1.7783910152502358e-05 old loss 6.441713048843667e-05 BETTER
I0321 02:21:11.437882 1574694 finetune.py:68] layer 0_v @ epoch 3 new loss 1.6786879086794215e-07 old loss 2.2550554490408103e-07 BETTER
I0321 02:21:15.357258 1573206 quantize_finetune_llama.py:234] computed original embedding for layer 2 in 69.51983451843262s
I0321 02:21:21.905689 1575524 finetune.py:68] layer 1_v @ epoch 1 new loss 9.756106010172516e-06 old loss 1.7783910152502358e-05 BETTER
I0321 02:21:25.030050 1578530 config.py:54] PyTorch version 2.6.0 available.
W0321 02:21:25.344296 1578530 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 02:21:26.362806 1578530 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 02:21:26.366494 1573206 quantize_finetune_llama.py:203] layer 3 gpu 3
I0321 02:21:26.379804 1578530 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 02:21:36.718940 1578530 finetune.py:45] layer 2_v initial loss 4.844376690016361e-06
W0321 02:21:36.719253 1578530 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 02:21:46.568197 1574694 finetune.py:68] layer 0_v @ epoch 4 new loss 1.430908014299348e-07 old loss 1.6786879086794215e-07 BETTER
I0321 02:21:54.440677 1575524 finetune.py:68] layer 1_v @ epoch 2 new loss 6.0681354625558015e-06 old loss 9.756106010172516e-06 BETTER
I0321 02:21:58.994718 1574694 finetune.py:45] layer 0_q initial loss 1.4851464413823123e-07
I0321 02:22:07.997580 1578530 finetune.py:68] layer 2_v @ epoch 0 new loss 3.587461151255411e-06 old loss 4.844376690016361e-06 BETTER
I0321 02:22:28.474921 1575524 finetune.py:68] layer 1_v @ epoch 3 new loss 4.469096893444657e-06 old loss 6.0681354625558015e-06 BETTER
I0321 02:22:33.376979 1574694 finetune.py:68] layer 0_q @ epoch 0 new loss 1.269592218022808e-07 old loss 1.4851464413823123e-07 BETTER
I0321 02:22:38.902063 1573206 quantize_finetune_llama.py:234] computed original embedding for layer 3 in 72.318687915802s
I0321 02:22:44.387980 1578530 finetune.py:68] layer 2_v @ epoch 1 new loss 2.906864210672211e-06 old loss 3.587461151255411e-06 BETTER
I0321 02:22:50.956125 1579331 config.py:54] PyTorch version 2.6.0 available.
W0321 02:22:51.290409 1579331 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0321 02:22:52.265947 1579331 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0321 02:22:52.270439 1573206 quantize_finetune_llama.py:203] layer 4 gpu 0
I0321 02:22:52.287536 1579331 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0321 02:23:01.488693 1575524 finetune.py:68] layer 1_v @ epoch 4 new loss 3.954568455810659e-06 old loss 4.469096893444657e-06 BETTER
I0321 02:23:02.935993 1579331 finetune.py:45] layer 3_v initial loss 7.054267825878924e-06
W0321 02:23:02.936241 1579331 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0321 02:23:08.499791 1574694 finetune.py:68] layer 0_q @ epoch 1 new loss 1.1594372040235612e-07 old loss 1.269592218022808e-07 BETTER
I0321 02:23:13.307013 1575524 finetune.py:45] layer 1_q initial loss 4.9163022595166694e-06
I0321 02:23:17.082223 1578530 finetune.py:68] layer 2_v @ epoch 2 new loss 2.5062274744414026e-06 old loss 2.906864210672211e-06 BETTER
I0321 02:23:33.819828 1579331 finetune.py:68] layer 3_v @ epoch 0 new loss 4.388518846099032e-06 old loss 7.054267825878924e-06 BETTER
I0321 02:23:43.107168 1574694 finetune.py:68] layer 0_q @ epoch 2 new loss 1.0784314241618631e-07 old loss 1.1594372040235612e-07 BETTER
I0321 02:23:45.159536 1575524 finetune.py:68] layer 1_q @ epoch 0 new loss 4.5801439227943774e-06 old loss 4.9163022595166694e-06 BETTER
I0321 02:23:49.938093 1578530 finetune.py:68] layer 2_v @ epoch 3 new loss 2.2487288333650213e-06 old loss 2.5062274744414026e-06 BETTER
I0321 02:24:05.866315 1579331 finetune.py:68] layer 3_v @ epoch 1 new loss 3.5250477594672702e-06 old loss 4.388518846099032e-06 BETTER
I0321 02:24:17.928020 1574694 finetune.py:68] layer 0_q @ epoch 3 new loss 1.0120804461166699e-07 old loss 1.0784314241618631e-07 BETTER
I0321 02:24:18.169897 1575524 finetune.py:68] layer 1_q @ epoch 1 new loss 3.1802937883185223e-06 old loss 4.5801439227943774e-06 BETTER
I0321 02:24:23.002515 1578530 finetune.py:68] layer 2_v @ epoch 4 new loss 2.0722043245768873e-06 old loss 2.2487288333650213e-06 BETTER
I0321 02:24:34.317872 1578530 finetune.py:45] layer 2_q initial loss 2.3223183234222233e-06
I0321 02:24:38.199783 1579331 finetune.py:68] layer 3_v @ epoch 2 new loss 3.141196657452383e-06 old loss 3.5250477594672702e-06 BETTER
I0321 02:24:50.960510 1575524 finetune.py:68] layer 1_q @ epoch 2 new loss 2.391642055954435e-06 old loss 3.1802937883185223e-06 BETTER
I0321 02:24:52.909472 1574694 finetune.py:68] layer 0_q @ epoch 4 new loss 9.50428500345879e-08 old loss 1.0120804461166699e-07 BETTER
I0321 02:25:05.324271 1574694 finetune.py:45] layer 0_k initial loss 1.0235667957658734e-07
I0321 02:25:06.158058 1578530 finetune.py:68] layer 2_q @ epoch 0 new loss 2.033810005741543e-06 old loss 2.3223183234222233e-06 BETTER
I0321 02:25:10.644161 1579331 finetune.py:68] layer 3_v @ epoch 3 new loss 2.912562649726169e-06 old loss 3.141196657452383e-06 BETTER
I0321 02:25:23.602169 1575524 finetune.py:68] layer 1_q @ epoch 3 new loss 2.0337249679869274e-06 old loss 2.391642055954435e-06 BETTER
I0321 02:25:38.959943 1574694 finetune.py:68] layer 0_k @ epoch 0 new loss 9.251759536255122e-08 old loss 1.0235667957658734e-07 BETTER
I0321 02:25:39.158068 1578530 finetune.py:68] layer 2_q @ epoch 1 new loss 1.9150124899169896e-06 old loss 2.033810005741543e-06 BETTER
I0321 02:25:43.356593 1579331 finetune.py:68] layer 3_v @ epoch 4 new loss 2.748131237240159e-06 old loss 2.912562649726169e-06 BETTER
I0321 02:25:54.627103 1579331 finetune.py:45] layer 3_q initial loss 3.2903378723858623e-06
I0321 02:25:56.478851 1575524 finetune.py:68] layer 1_q @ epoch 4 new loss 1.9423675894358894e-06 old loss 2.0337249679869274e-06 BETTER
I0321 02:26:07.934576 1575524 finetune.py:45] layer 1_k initial loss 2.3549753223051084e-06
I0321 02:26:12.046800 1578530 finetune.py:68] layer 2_q @ epoch 2 new loss 1.8251416804559994e-06 old loss 1.9150124899169896e-06 BETTER
I0321 02:26:13.107635 1574694 finetune.py:68] layer 0_k @ epoch 1 new loss 8.751250391014764e-08 old loss 9.251759536255122e-08 BETTER
I0321 02:26:26.051353 1579331 finetune.py:68] layer 3_q @ epoch 0 new loss 2.951435362774646e-06 old loss 3.2903378723858623e-06 BETTER
I0321 02:26:39.462322 1575524 finetune.py:68] layer 1_k @ epoch 0 new loss 2.048059741355246e-06 old loss 2.3549753223051084e-06 BETTER
I0321 02:26:44.938457 1578530 finetune.py:68] layer 2_q @ epoch 3 new loss 1.7517496644359198e-06 old loss 1.8251416804559994e-06 BETTER
I0321 02:26:47.325486 1574694 finetune.py:68] layer 0_k @ epoch 2 new loss 8.358288994259055e-08 old loss 8.751250391014764e-08 BETTER
I0321 02:26:58.201545 1579331 finetune.py:68] layer 3_q @ epoch 1 new loss 2.811228114296682e-06 old loss 2.951435362774646e-06 BETTER
I0321 02:27:11.873382 1575524 finetune.py:76] layer 1_k @ epoch 1 new loss 2.1944931631878717e-06 old loss 2.048059741355246e-06 WORSE
I0321 02:27:17.464617 1578530 finetune.py:68] layer 2_q @ epoch 4 new loss 1.6895895669222227e-06 old loss 1.7517496644359198e-06 BETTER
I0321 02:27:21.361204 1574694 finetune.py:68] layer 0_k @ epoch 3 new loss 8.029905984585639e-08 old loss 8.358288994259055e-08 BETTER
I0321 02:27:29.282953 1578530 finetune.py:45] layer 2_k initial loss 1.8711065195020637e-06
I0321 02:27:30.550688 1579331 finetune.py:68] layer 3_q @ epoch 2 new loss 2.705697170313215e-06 old loss 2.811228114296682e-06 BETTER
I0321 02:27:43.486114 1575524 finetune.py:76] layer 1_k @ epoch 2 new loss 2.050871444225777e-06 old loss 2.048059741355246e-06 WORSE
I0321 02:27:56.047793 1574694 finetune.py:68] layer 0_k @ epoch 4 new loss 7.707890148367369e-08 old loss 8.029905984585639e-08 BETTER
I0321 02:28:01.098709 1578530 finetune.py:68] layer 2_k @ epoch 0 new loss 1.7865687595985946e-06 old loss 1.8711065195020637e-06 BETTER
I0321 02:28:02.923194 1579331 finetune.py:68] layer 3_q @ epoch 3 new loss 2.620299255795544e-06 old loss 2.705697170313215e-06 BETTER
I0321 02:28:08.644864 1574694 finetune.py:45] layer 0_o initial loss 4.6016688770578185e-07
I0321 02:28:15.043066 1575524 finetune.py:76] layer 1_k @ epoch 3 new loss 2.319887698831735e-06 old loss 2.048059741355246e-06 WORSE
I0321 02:28:33.495559 1578530 finetune.py:68] layer 2_k @ epoch 1 new loss 1.730399844745989e-06 old loss 1.7865687595985946e-06 BETTER
I0321 02:28:35.163414 1579331 finetune.py:68] layer 3_q @ epoch 4 new loss 2.54988935921574e-06 old loss 2.620299255795544e-06 BETTER
I0321 02:28:40.904729 1574694 finetune.py:68] layer 0_o @ epoch 0 new loss 3.994575479282503e-07 old loss 4.6016688770578185e-07 BETTER
I0321 02:28:46.969527 1575524 finetune.py:68] layer 1_k @ epoch 4 new loss 1.8781137214318733e-06 old loss 2.048059741355246e-06 BETTER
I0321 02:28:47.278358 1579331 finetune.py:45] layer 3_k initial loss 2.963054157589795e-06
I0321 02:28:58.487560 1575524 finetune.py:45] layer 1_o initial loss 7.095113232935546e-06
I0321 02:29:05.669432 1578530 finetune.py:68] layer 2_k @ epoch 2 new loss 1.6837856264828588e-06 old loss 1.730399844745989e-06 BETTER
I0321 02:29:14.229435 1574694 finetune.py:68] layer 0_o @ epoch 1 new loss 3.6161927141620254e-07 old loss 3.994575479282503e-07 BETTER
I0321 02:29:18.264091 1579331 finetune.py:68] layer 3_k @ epoch 0 new loss 2.8333888622000813e-06 old loss 2.963054157589795e-06 BETTER
I0321 02:29:29.375245 1575524 finetune.py:68] layer 1_o @ epoch 0 new loss 3.6447345337364823e-06 old loss 7.095113232935546e-06 BETTER
I0321 02:29:37.860771 1578530 finetune.py:68] layer 2_k @ epoch 3 new loss 1.6429597735623247e-06 old loss 1.6837856264828588e-06 BETTER
I0321 02:29:47.794603 1574694 finetune.py:68] layer 0_o @ epoch 2 new loss 3.3402133681192936e-07 old loss 3.6161927141620254e-07 BETTER
I0321 02:29:50.131003 1579331 finetune.py:68] layer 3_k @ epoch 1 new loss 2.7674707325786585e-06 old loss 2.8333888622000813e-06 BETTER
I0321 02:30:01.073230 1575524 finetune.py:68] layer 1_o @ epoch 1 new loss 3.420198709136457e-06 old loss 3.6447345337364823e-06 BETTER
I0321 02:30:09.976594 1578530 finetune.py:68] layer 2_k @ epoch 4 new loss 1.6067829164967407e-06 old loss 1.6429597735623247e-06 BETTER
I0321 02:30:21.592600 1578530 finetune.py:45] layer 2_o initial loss 3.4526542549428996e-06
I0321 02:30:21.879057 1574694 finetune.py:68] layer 0_o @ epoch 3 new loss 3.119804432571982e-07 old loss 3.3402133681192936e-07 BETTER
I0321 02:30:22.459561 1579331 finetune.py:68] layer 3_k @ epoch 2 new loss 2.7149267225468066e-06 old loss 2.7674707325786585e-06 BETTER
I0321 02:30:32.894982 1575524 finetune.py:68] layer 1_o @ epoch 2 new loss 3.294888529126183e-06 old loss 3.420198709136457e-06 BETTER
I0321 02:30:52.804090 1578530 finetune.py:68] layer 2_o @ epoch 0 new loss 3.4076122119586216e-06 old loss 3.4526542549428996e-06 BETTER
I0321 02:30:54.890996 1579331 finetune.py:68] layer 3_k @ epoch 3 new loss 2.6679315396904713e-06 old loss 2.7149267225468066e-06 BETTER
I0321 02:30:55.605734 1574694 finetune.py:68] layer 0_o @ epoch 4 new loss 2.939601699836203e-07 old loss 3.119804432571982e-07 BETTER
I0321 02:31:04.696201 1575524 finetune.py:68] layer 1_o @ epoch 3 new loss 3.2152879612112883e-06 old loss 3.294888529126183e-06 BETTER
I0321 02:31:16.209988 1574694 finetune.py:45] layer 0_up initial loss 3.375307073838485e-07
