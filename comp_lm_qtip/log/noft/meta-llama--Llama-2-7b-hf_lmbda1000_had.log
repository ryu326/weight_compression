I0314 09:36:04.290737 2580651 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.26it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.46it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.39it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.36it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.15it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.87it/s]
I0314 09:36:06.311896 2580651 quantize_finetune_llama.py:142] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:23,  1.32it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:22,  1.31it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:22,  1.32it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:03<00:21,  1.31it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:20,  1.34it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:19,  1.34it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:05<00:18,  1.33it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:17,  1.35it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:17,  1.34it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:07<00:16,  1.35it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:08<00:15,  1.36it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:14,  1.37it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:14,  1.35it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:10<00:13,  1.34it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:11<00:12,  1.33it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:11,  1.34it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:12<00:11,  1.34it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:13<00:10,  1.36it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:14<00:09,  1.36it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:14<00:08,  1.37it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:15<00:08,  1.37it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:16<00:07,  1.41it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:06,  1.42it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:05,  1.43it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:18<00:04,  1.43it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:19<00:04,  1.43it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:19<00:03,  1.44it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:20<00:02,  1.44it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:21<00:02,  1.43it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.44it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:22<00:00,  1.44it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.45it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.38it/s]
I0314 09:36:39.679663 2580651 quantize_finetune_llama.py:167] loaded compression model
I0314 09:36:53.854153 2580651 quantize_finetune_llama.py:171] loaded dataset and devset
I0314 09:36:59.268821 2580651 quantize_finetune_llama.py:191] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:38:08.277876 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 0 in 68.89623832702637s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 09:38:28.582428 2581215 config.py:54] PyTorch version 2.1.1 available.
I0314 09:38:29.514128 2580651 quantize_finetune_llama.py:191] layer 1 gpu 1
I0314 09:38:29.569321 2581215 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:38:38.268184 2581215 finetune.py:45] layer 0_v initial loss 6.072997393857804e-07
I0314 09:38:46.743226 2581215 finetune.py:45] layer 0_q initial loss 6.050901788512419e-07
I0314 09:38:55.442369 2581215 finetune.py:45] layer 0_k initial loss 6.058796202523808e-07
I0314 09:39:04.173264 2581215 finetune.py:45] layer 0_o initial loss 6.796382763241127e-07
I0314 09:39:19.357934 2581215 finetune.py:45] layer 0_up initial loss 6.864284500807116e-07
I0314 09:39:34.631330 2581215 finetune.py:45] layer 0_gate initial loss 6.920240025465318e-07
I0314 09:39:38.687210 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 1 in 69.03119540214539s
I0314 09:39:46.928172 2581443 config.py:54] PyTorch version 2.1.1 available.
I0314 09:39:48.036772 2580651 quantize_finetune_llama.py:191] layer 2 gpu 2
I0314 09:39:48.101211 2581443 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:39:54.527619 2581215 finetune.py:45] layer 0_down initial loss 7.518317488575121e-07
0_v proxy err 0.00473904050886631 tr(WHW.T) 4.225186347961426
0_q proxy err 2.5384922992088832e-05 tr(WHW.T) 2710.363037109375
0_k proxy err 2.8059572287020274e-05 tr(WHW.T) 1698.7349853515625
0_o proxy err 0.00031706158188171685 tr(WHW.T) 0.9668058156967163
0_up proxy err 0.0005331535940058529 tr(WHW.T) 43.27138900756836
0_gate proxy err 0.0003692425962071866 tr(WHW.T) 63.47430419921875
0_down proxy err 0.0003675262851174921 tr(WHW.T) 0.656814694404602
I0314 09:39:57.119016 2581443 finetune.py:45] layer 1_v initial loss 1.093157334253192e-05
I0314 09:40:05.760163 2581443 finetune.py:45] layer 1_q initial loss 1.2089213669241872e-05
I0314 09:40:14.417061 2581443 finetune.py:45] layer 1_k initial loss 1.2301376045797952e-05
I0314 09:40:23.166514 2581443 finetune.py:45] layer 1_o initial loss 1.28534811665304e-05
I0314 09:40:38.152838 2581443 finetune.py:45] layer 1_up initial loss 1.0202032171946485e-05
I0314 09:40:53.267033 2581443 finetune.py:45] layer 1_gate initial loss 8.44124770082999e-06
I0314 09:40:59.776036 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 2 in 71.50394296646118s
I0314 09:41:08.154109 2581674 config.py:54] PyTorch version 2.1.1 available.
I0314 09:41:09.237923 2580651 quantize_finetune_llama.py:191] layer 3 gpu 3
I0314 09:41:09.295585 2581674 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 09:41:10.878975 2581443 finetune.py:45] layer 1_down initial loss 0.00016633739869575948
1_v proxy err 0.006887158844619989 tr(WHW.T) 16.465883255004883
1_q proxy err 3.674783874885179e-05 tr(WHW.T) 4778.43994140625
1_k proxy err 3.6752513551618904e-05 tr(WHW.T) 4995.39208984375
1_o proxy err 0.001160909072495997 tr(WHW.T) 1.1115814447402954
1_up proxy err 0.0005534743540920317 tr(WHW.T) 109.66383361816406
1_gate proxy err 0.0002816625055857003 tr(WHW.T) 221.3038787841797
1_down proxy err 0.000322215462801978 tr(WHW.T) 2041.4736328125
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:41:18.379448 2581674 finetune.py:45] layer 2_v initial loss 1.0614023722155252e-06
I0314 09:41:27.045864 2581674 finetune.py:45] layer 2_q initial loss 1.133224941440858e-06
I0314 09:41:35.847143 2581674 finetune.py:45] layer 2_k initial loss 1.1959094763369649e-06
I0314 09:41:44.709283 2581674 finetune.py:45] layer 2_o initial loss 1.603678697392752e-06
I0314 09:41:59.624247 2581674 finetune.py:45] layer 2_up initial loss 1.8571897726360476e-06
I0314 09:42:14.609249 2581674 finetune.py:45] layer 2_gate initial loss 2.0427398794709006e-06
I0314 09:42:22.149607 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 3 in 72.74335360527039s
I0314 09:42:30.631637 2581908 config.py:54] PyTorch version 2.1.1 available.
I0314 09:42:31.709055 2580651 quantize_finetune_llama.py:191] layer 4 gpu 0
I0314 09:42:31.776790 2581908 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 09:42:32.884988 2581674 finetune.py:45] layer 2_down initial loss 2.5685660602903226e-06
2_v proxy err 0.0016713050426915288 tr(WHW.T) 136.67332458496094
2_q proxy err 4.3694875785149634e-05 tr(WHW.T) 7752.85205078125
2_k proxy err 3.694445695145987e-05 tr(WHW.T) 10205.837890625
2_o proxy err 0.0008929840405471623 tr(WHW.T) 1.4603197574615479
2_up proxy err 0.0006422987789846957 tr(WHW.T) 193.43603515625
2_gate proxy err 0.0004107593558728695 tr(WHW.T) 306.6622619628906
2_down proxy err 0.0007376908324658871 tr(WHW.T) 3.010739803314209
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:42:41.153430 2581908 finetune.py:45] layer 3_v initial loss 1.603887767487322e-06
I0314 09:42:49.699434 2581908 finetune.py:45] layer 3_q initial loss 1.7353388557239668e-06
I0314 09:42:58.254524 2581908 finetune.py:45] layer 3_k initial loss 1.8613022803037893e-06
I0314 09:43:06.887177 2581908 finetune.py:45] layer 3_o initial loss 2.7721573587768944e-06
I0314 09:43:21.844229 2581908 finetune.py:45] layer 3_up initial loss 3.37713458975486e-06
I0314 09:43:37.007865 2581908 finetune.py:45] layer 3_gate initial loss 3.8098021377663827e-06
I0314 09:43:42.488852 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 4 in 69.59291648864746s
I0314 09:43:45.707277 2582128 config.py:54] PyTorch version 2.1.1 available.
I0314 09:43:46.746594 2580651 quantize_finetune_llama.py:191] layer 5 gpu 1
I0314 09:43:46.817155 2582128 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:43:53.387826 2581908 finetune.py:45] layer 3_down initial loss 4.923844699078472e-06
3_v proxy err 0.0015688840067014098 tr(WHW.T) 284.77557373046875
3_q proxy err 7.648926111869514e-05 tr(WHW.T) 7217.63720703125
3_k proxy err 5.814760515931994e-05 tr(WHW.T) 10074.73828125
3_o proxy err 0.0009013185044750571 tr(WHW.T) 3.3527450561523438
3_up proxy err 0.0007304512546397746 tr(WHW.T) 284.7950744628906
3_gate proxy err 0.0004408111562952399 tr(WHW.T) 478.13714599609375
3_down proxy err 0.00075393239967525 tr(WHW.T) 6.133229732513428
I0314 09:43:55.884895 2582128 finetune.py:45] layer 4_v initial loss 2.825861656674533e-06
I0314 09:44:04.903464 2582128 finetune.py:45] layer 4_q initial loss 3.062731593672652e-06
I0314 09:44:13.841547 2582128 finetune.py:45] layer 4_k initial loss 3.324764065837371e-06
I0314 09:44:22.840335 2582128 finetune.py:45] layer 4_o initial loss 4.564974005916156e-06
I0314 09:44:38.295301 2582128 finetune.py:45] layer 4_up initial loss 5.713358859793516e-06
I0314 09:44:53.914625 2582128 finetune.py:45] layer 4_gate initial loss 6.445739018090535e-06
I0314 09:44:55.683730 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 5 in 68.50448942184448s
I0314 09:44:58.684736 2582345 config.py:54] PyTorch version 2.1.1 available.
I0314 09:44:59.751092 2580651 quantize_finetune_llama.py:191] layer 6 gpu 2
I0314 09:44:59.821335 2582345 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:45:08.805806 2582345 finetune.py:45] layer 5_v initial loss 4.250799065630417e-06
I0314 09:45:10.747417 2582128 finetune.py:45] layer 4_down initial loss 8.539025657228194e-06
4_v proxy err 0.0015012830263003707 tr(WHW.T) 274.6131286621094
4_q proxy err 7.406897930195555e-05 tr(WHW.T) 6914.9892578125
4_k proxy err 5.4781798098701984e-05 tr(WHW.T) 10415.33203125
4_o proxy err 0.0008878119406290352 tr(WHW.T) 5.139806270599365
4_up proxy err 0.000721248856279999 tr(WHW.T) 397.6960144042969
4_gate proxy err 0.00035762859624810517 tr(WHW.T) 821.1856689453125
4_down proxy err 0.00075786147499457 tr(WHW.T) 11.562739372253418
I0314 09:45:17.673882 2582345 finetune.py:45] layer 5_q initial loss 4.552892278297804e-06
I0314 09:45:26.322392 2582345 finetune.py:45] layer 5_k initial loss 4.829497811442707e-06
I0314 09:45:35.013399 2582345 finetune.py:45] layer 5_o initial loss 7.3091714511974715e-06
I0314 09:45:49.947394 2582345 finetune.py:45] layer 5_up initial loss 9.159297405858524e-06
I0314 09:46:05.061000 2582345 finetune.py:45] layer 5_gate initial loss 1.0291563739883713e-05
I0314 09:46:08.710310 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 6 in 68.52354288101196s
I0314 09:46:11.835577 2582562 config.py:54] PyTorch version 2.1.1 available.
I0314 09:46:12.852437 2580651 quantize_finetune_llama.py:191] layer 7 gpu 3
I0314 09:46:12.922415 2582562 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:46:21.438147 2582345 finetune.py:45] layer 5_down initial loss 1.3284979104355443e-05
I0314 09:46:21.782000 2582562 finetune.py:45] layer 6_v initial loss 5.541892278415617e-06
5_v proxy err 0.0014813680900260806 tr(WHW.T) 298.47540283203125
5_q proxy err 7.935405301395804e-05 tr(WHW.T) 6770.97509765625
5_k proxy err 5.5183812946779653e-05 tr(WHW.T) 10841.955078125
5_o proxy err 0.001249707187525928 tr(WHW.T) 7.947142601013184
5_up proxy err 0.0007079311762936413 tr(WHW.T) 506.6408386230469
5_gate proxy err 0.00033334302133880556 tr(WHW.T) 1104.867919921875
5_down proxy err 0.0007980371592566371 tr(WHW.T) 15.6494779586792
I0314 09:46:30.335619 2582562 finetune.py:45] layer 6_q initial loss 6.118372766650282e-06
I0314 09:46:39.071792 2582562 finetune.py:45] layer 6_k initial loss 6.691494036203949e-06
I0314 09:46:48.235121 2582562 finetune.py:45] layer 6_o initial loss 9.840144230111036e-06
I0314 09:47:03.578875 2582562 finetune.py:45] layer 6_up initial loss 1.2709311704384163e-05
I0314 09:47:18.983958 2582562 finetune.py:45] layer 6_gate initial loss 1.4294667380454484e-05
I0314 09:47:21.774425 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 7 in 68.50331139564514s
I0314 09:47:24.946520 2582782 config.py:54] PyTorch version 2.1.1 available.
I0314 09:47:26.140581 2580651 quantize_finetune_llama.py:191] layer 8 gpu 0
I0314 09:47:26.213157 2582782 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:47:35.930699 2582782 finetune.py:45] layer 7_v initial loss 7.0617429628327955e-06
I0314 09:47:36.356503 2582562 finetune.py:45] layer 6_down initial loss 1.8838194591808133e-05
6_v proxy err 0.0014698157319799066 tr(WHW.T) 443.5464782714844
6_q proxy err 0.00010052353900391608 tr(WHW.T) 7576.53857421875
6_k proxy err 7.651870691915974e-05 tr(WHW.T) 10409.4033203125
6_o proxy err 0.0011648079380393028 tr(WHW.T) 11.564380645751953
6_up proxy err 0.0007101100054569542 tr(WHW.T) 617.2608642578125
6_gate proxy err 0.0002912336785811931 tr(WHW.T) 1554.7271728515625
6_down proxy err 0.0008243114571087062 tr(WHW.T) 22.988168716430664
I0314 09:47:44.617211 2582782 finetune.py:45] layer 7_q initial loss 7.809681846993044e-06
I0314 09:47:53.195709 2582782 finetune.py:45] layer 7_k initial loss 8.584649549447931e-06
I0314 09:48:01.834422 2582782 finetune.py:45] layer 7_o initial loss 1.3010000657232013e-05
I0314 09:48:16.852665 2582782 finetune.py:45] layer 7_up initial loss 1.6939533452386968e-05
I0314 09:48:32.562609 2582782 finetune.py:45] layer 7_gate initial loss 1.913780943141319e-05
I0314 09:48:37.396955 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 8 in 70.10589551925659s
I0314 09:48:40.610811 2583002 config.py:54] PyTorch version 2.1.1 available.
I0314 09:48:41.644481 2580651 quantize_finetune_llama.py:191] layer 9 gpu 1
I0314 09:48:41.716808 2583002 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:48:49.635728 2582782 finetune.py:45] layer 7_down initial loss 2.5231811378034763e-05
7_v proxy err 0.0014377407496795058 tr(WHW.T) 489.9357604980469
7_q proxy err 0.00010550937440712005 tr(WHW.T) 7672.17919921875
7_k proxy err 8.203285688068718e-05 tr(WHW.T) 10198.3701171875
7_o proxy err 0.0013291029026731849 tr(WHW.T) 15.11335563659668
7_up proxy err 0.0006997339660301805 tr(WHW.T) 735.8538818359375
7_gate proxy err 0.0002840121160261333 tr(WHW.T) 1876.0390625
7_down proxy err 0.0008346559479832649 tr(WHW.T) 30.58672523498535
I0314 09:48:50.930417 2583002 finetune.py:45] layer 8_v initial loss 1.1058509699068964e-05
I0314 09:48:59.940276 2583002 finetune.py:45] layer 8_q initial loss 1.2079567568434868e-05
I0314 09:49:09.063179 2583002 finetune.py:45] layer 8_k initial loss 1.2942796274728607e-05
I0314 09:49:18.182987 2583002 finetune.py:45] layer 8_o initial loss 1.9575951228034683e-05
I0314 09:49:34.118902 2583002 finetune.py:45] layer 8_up initial loss 2.4277465854538605e-05
I0314 09:49:49.851439 2583002 finetune.py:45] layer 8_gate initial loss 2.7131805836688727e-05
I0314 09:49:51.082567 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 9 in 68.96863174438477s
I0314 09:49:54.117717 2583222 config.py:54] PyTorch version 2.1.1 available.
I0314 09:49:55.181431 2580651 quantize_finetune_llama.py:191] layer 10 gpu 2
I0314 09:49:55.245429 2583222 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:50:04.385676 2583222 finetune.py:45] layer 9_v initial loss 1.1840739716717508e-05
I0314 09:50:06.875413 2583002 finetune.py:45] layer 8_down initial loss 3.451029988355003e-05
8_v proxy err 0.001362087787128985 tr(WHW.T) 530.9967041015625
8_q proxy err 0.00011337058094795793 tr(WHW.T) 7228.1201171875
8_k proxy err 8.28237461973913e-05 tr(WHW.T) 10639.1015625
8_o proxy err 0.0015000172425061464 tr(WHW.T) 20.092191696166992
8_up proxy err 0.0006444308673962951 tr(WHW.T) 866.312744140625
8_gate proxy err 0.0002914289361797273 tr(WHW.T) 1970.857177734375
8_down proxy err 0.0008288790704682469 tr(WHW.T) 37.177734375
I0314 09:50:13.305327 2583222 finetune.py:45] layer 9_q initial loss 1.2870033970102668e-05
I0314 09:50:22.222649 2583222 finetune.py:45] layer 9_k initial loss 1.3980292351334356e-05
I0314 09:50:31.142463 2583222 finetune.py:45] layer 9_o initial loss 2.2211192117538303e-05
I0314 09:50:46.091166 2583222 finetune.py:45] layer 9_up initial loss 2.768175909295678e-05
I0314 09:51:01.094739 2583222 finetune.py:45] layer 9_gate initial loss 3.116115476586856e-05
I0314 09:51:03.284283 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 10 in 67.65621137619019s
I0314 09:51:06.299772 2583436 config.py:54] PyTorch version 2.1.1 available.
I0314 09:51:07.360192 2580651 quantize_finetune_llama.py:191] layer 11 gpu 3
I0314 09:51:07.433084 2583436 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:51:16.150941 2583436 finetune.py:45] layer 10_v initial loss 1.5900814105407335e-05
I0314 09:51:17.164677 2583222 finetune.py:45] layer 9_down initial loss 3.968101373175159e-05
9_v proxy err 0.001265360857360065 tr(WHW.T) 565.0663452148438
9_q proxy err 0.00011737279419321567 tr(WHW.T) 6970.3359375
9_k proxy err 7.891663699410856e-05 tr(WHW.T) 10987.3515625
9_o proxy err 0.001488504814915359 tr(WHW.T) 25.610172271728516
9_up proxy err 0.0006212120642885566 tr(WHW.T) 970.8984375
9_gate proxy err 0.0002906606823671609 tr(WHW.T) 2132.69384765625
9_down proxy err 0.0008297551539726555 tr(WHW.T) 42.99482727050781
I0314 09:51:24.825871 2583436 finetune.py:45] layer 10_q initial loss 1.7126249076682143e-05
I0314 09:51:33.567181 2583436 finetune.py:45] layer 10_k initial loss 1.8346032447880134e-05
I0314 09:51:42.378772 2583436 finetune.py:45] layer 10_o initial loss 2.9808648832840845e-05
I0314 09:51:57.342190 2583436 finetune.py:45] layer 10_up initial loss 3.604327139328234e-05
I0314 09:52:12.465412 2583436 finetune.py:45] layer 10_gate initial loss 4.02582336391788e-05
I0314 09:52:14.099908 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 11 in 66.34372878074646s
I0314 09:52:17.058101 2583650 config.py:54] PyTorch version 2.1.1 available.
I0314 09:52:18.050270 2580651 quantize_finetune_llama.py:191] layer 12 gpu 0
I0314 09:52:18.116725 2583650 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:52:27.647399 2583650 finetune.py:45] layer 11_v initial loss 1.7910082533489913e-05
I0314 09:52:29.326178 2583436 finetune.py:45] layer 10_down initial loss 5.014905036659911e-05
10_v proxy err 0.0012705727713182569 tr(WHW.T) 578.807373046875
10_q proxy err 0.00011986475874437019 tr(WHW.T) 6915.87109375
10_k proxy err 8.1409583799541e-05 tr(WHW.T) 10996.2431640625
10_o proxy err 0.0015244348905980587 tr(WHW.T) 35.184165954589844
10_up proxy err 0.0005859498050995171 tr(WHW.T) 1080.198486328125
10_gate proxy err 0.00028788551571778953 tr(WHW.T) 2260.88330078125
10_down proxy err 0.0007908878615126014 tr(WHW.T) 52.33584976196289
I0314 09:52:36.424563 2583650 finetune.py:45] layer 11_q initial loss 1.923821218952071e-05
I0314 09:52:45.044697 2583650 finetune.py:45] layer 11_k initial loss 2.0375289750518277e-05
I0314 09:52:53.660508 2583650 finetune.py:45] layer 11_o initial loss 3.220570943085477e-05
I0314 09:53:08.578041 2583650 finetune.py:45] layer 11_up initial loss 3.9212533010868356e-05
I0314 09:53:23.610269 2583650 finetune.py:45] layer 11_gate initial loss 4.392816117615439e-05
I0314 09:53:27.306932 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 12 in 68.09868597984314s
I0314 09:53:30.409014 2583870 config.py:54] PyTorch version 2.1.1 available.
I0314 09:53:31.500048 2580651 quantize_finetune_llama.py:191] layer 13 gpu 1
I0314 09:53:31.568489 2583870 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:53:39.841143 2583650 finetune.py:45] layer 11_down initial loss 5.4793810704723e-05
I0314 09:53:40.558517 2583870 finetune.py:45] layer 12_v initial loss 1.8148330127587542e-05
11_v proxy err 0.001201352453790605 tr(WHW.T) 723.1956176757812
11_q proxy err 0.00013666281301993877 tr(WHW.T) 7027.10986328125
11_k proxy err 9.656134352553636e-05 tr(WHW.T) 10511.23046875
11_o proxy err 0.0015279207145795226 tr(WHW.T) 36.654052734375
11_up proxy err 0.000595415011048317 tr(WHW.T) 1139.6044921875
11_gate proxy err 0.00029102127882651985 tr(WHW.T) 2392.716552734375
11_down proxy err 0.0008090579067356884 tr(WHW.T) 56.13530731201172
I0314 09:53:49.288931 2583870 finetune.py:45] layer 12_q initial loss 1.9721297576325014e-05
I0314 09:53:58.120843 2583870 finetune.py:45] layer 12_k initial loss 2.1218762412900105e-05
I0314 09:54:06.935698 2583870 finetune.py:45] layer 12_o initial loss 3.425771865295246e-05
I0314 09:54:22.050433 2583870 finetune.py:45] layer 12_up initial loss 4.205572258797474e-05
I0314 09:54:37.318911 2583870 finetune.py:45] layer 12_gate initial loss 4.757933129440062e-05
I0314 09:54:38.363000 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 13 in 66.43685221672058s
I0314 09:54:41.398138 2584084 config.py:54] PyTorch version 2.1.1 available.
I0314 09:54:42.412016 2580651 quantize_finetune_llama.py:191] layer 14 gpu 2
I0314 09:54:42.468220 2584084 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:54:51.035332 2584084 finetune.py:45] layer 13_v initial loss 1.743608299875632e-05
I0314 09:54:53.822693 2583870 finetune.py:45] layer 12_down initial loss 6.002635200275108e-05
12_v proxy err 0.0012320007663220167 tr(WHW.T) 703.318603515625
12_q proxy err 0.0001362877374049276 tr(WHW.T) 7045.6435546875
12_k proxy err 9.27122964640148e-05 tr(WHW.T) 10893.65625
12_o proxy err 0.0015567910159006715 tr(WHW.T) 39.29071044921875
12_up proxy err 0.0005885126884095371 tr(WHW.T) 1228.298583984375
12_gate proxy err 0.0003105197101831436 tr(WHW.T) 2381.994873046875
12_down proxy err 0.0008058499661274254 tr(WHW.T) 64.17745208740234
I0314 09:54:59.693750 2584084 finetune.py:45] layer 13_q initial loss 1.8865144738811068e-05
I0314 09:55:08.262092 2584084 finetune.py:45] layer 13_k initial loss 2.027920345426537e-05
I0314 09:55:16.886469 2584084 finetune.py:45] layer 13_o initial loss 3.346627272549085e-05
I0314 09:55:31.726658 2584084 finetune.py:45] layer 13_up initial loss 4.277484549675137e-05
I0314 09:55:46.605649 2584084 finetune.py:45] layer 13_gate initial loss 4.953342431690544e-05
I0314 09:55:49.333049 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 14 in 66.54232120513916s
I0314 09:55:52.428854 2584298 config.py:54] PyTorch version 2.1.1 available.
I0314 09:55:53.484652 2580651 quantize_finetune_llama.py:191] layer 15 gpu 3
I0314 09:55:53.556994 2584298 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:56:02.130438 2584298 finetune.py:45] layer 14_v initial loss 2.2409223674912937e-05
I0314 09:56:02.659425 2584084 finetune.py:45] layer 13_down initial loss 6.49071516818367e-05
13_v proxy err 0.0012065424816682935 tr(WHW.T) 714.5677490234375
13_q proxy err 0.00013617095828521997 tr(WHW.T) 6956.03564453125
13_k proxy err 9.653947199694812e-05 tr(WHW.T) 10426.6318359375
13_o proxy err 0.0013798432191833854 tr(WHW.T) 45.8377571105957
13_up proxy err 0.0005663508200086653 tr(WHW.T) 1367.6221923828125
13_gate proxy err 0.0003042781609110534 tr(WHW.T) 2601.504638671875
13_down proxy err 0.0008075981168076396 tr(WHW.T) 79.3589096069336
I0314 09:56:10.603011 2584298 finetune.py:45] layer 14_q initial loss 2.4303268219227903e-05
I0314 09:56:19.129297 2584298 finetune.py:45] layer 14_k initial loss 2.5976782126235776e-05
I0314 09:56:27.743077 2584298 finetune.py:45] layer 14_o initial loss 4.2679592297645286e-05
I0314 09:56:42.624502 2584298 finetune.py:45] layer 14_up initial loss 5.302579666022211e-05
I0314 09:56:57.582814 2584298 finetune.py:45] layer 14_gate initial loss 6.0810350987594575e-05
I0314 09:56:59.450083 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 15 in 65.57694506645203s
I0314 09:57:02.376950 2584512 config.py:54] PyTorch version 2.1.1 available.
I0314 09:57:03.375009 2580651 quantize_finetune_llama.py:191] layer 16 gpu 0
I0314 09:57:03.448580 2584512 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:57:12.569321 2584512 finetune.py:45] layer 15_v initial loss 2.1729139916715212e-05
I0314 09:57:14.513235 2584298 finetune.py:45] layer 14_down initial loss 7.870141416788101e-05
14_v proxy err 0.0012838987167924643 tr(WHW.T) 706.1612548828125
14_q proxy err 0.00014148290210869163 tr(WHW.T) 7077.06103515625
14_k proxy err 9.283579856855795e-05 tr(WHW.T) 11295.16796875
14_o proxy err 0.001527652028016746 tr(WHW.T) 50.921180725097656
14_up proxy err 0.0005722133792005479 tr(WHW.T) 1464.7159423828125
14_gate proxy err 0.00031908616074360907 tr(WHW.T) 2682.584716796875
14_down proxy err 0.0008243468473665416 tr(WHW.T) 90.28684997558594
I0314 09:57:21.135957 2584512 finetune.py:45] layer 15_q initial loss 2.3821232389309444e-05
I0314 09:57:29.555643 2584512 finetune.py:45] layer 15_k initial loss 2.5862917027552612e-05
I0314 09:57:38.031413 2584512 finetune.py:45] layer 15_o initial loss 4.25183170591481e-05
I0314 09:57:52.717777 2584512 finetune.py:45] layer 15_up initial loss 5.5057382269296795e-05
I0314 09:58:07.530729 2584512 finetune.py:45] layer 15_gate initial loss 6.476956332335249e-05
I0314 09:58:10.912757 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 16 in 66.43831706047058s
I0314 09:58:14.022758 2584726 config.py:54] PyTorch version 2.1.1 available.
I0314 09:58:15.113497 2580651 quantize_finetune_llama.py:191] layer 17 gpu 1
I0314 09:58:15.181148 2584726 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:58:23.481041 2584512 finetune.py:45] layer 15_down initial loss 8.732920105103403e-05
I0314 09:58:24.096455 2584726 finetune.py:45] layer 16_v initial loss 2.608282375149429e-05
15_v proxy err 0.0011688319500535727 tr(WHW.T) 762.7275390625
15_q proxy err 0.00013617992226500064 tr(WHW.T) 7252.0009765625
15_k proxy err 9.409748599864542e-05 tr(WHW.T) 11072.3974609375
15_o proxy err 0.0012899494031444192 tr(WHW.T) 59.61664962768555
15_up proxy err 0.0005572842201218009 tr(WHW.T) 1641.0228271484375
15_gate proxy err 0.00032184983137995005 tr(WHW.T) 2905.140380859375
15_down proxy err 0.000823145906906575 tr(WHW.T) 114.09001922607422
I0314 09:58:32.667996 2584726 finetune.py:45] layer 16_q initial loss 2.868921001208946e-05
I0314 09:58:41.265416 2584726 finetune.py:45] layer 16_k initial loss 3.063100666622631e-05
I0314 09:58:50.126165 2584726 finetune.py:45] layer 16_o initial loss 5.061844422016293e-05
I0314 09:59:05.437324 2584726 finetune.py:45] layer 16_up initial loss 6.654248136328533e-05
I0314 09:59:20.916334 2584726 finetune.py:45] layer 16_gate initial loss 7.897695468273014e-05
I0314 09:59:22.555330 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 17 in 67.07059073448181s
I0314 09:59:25.581314 2584943 config.py:54] PyTorch version 2.1.1 available.
I0314 09:59:26.679266 2580651 quantize_finetune_llama.py:191] layer 18 gpu 2
I0314 09:59:26.747689 2584943 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:59:35.971779 2584943 finetune.py:45] layer 17_v initial loss 2.186110577895306e-05
I0314 09:59:37.832748 2584726 finetune.py:45] layer 16_down initial loss 0.00010929627023870125
16_v proxy err 0.0011801199289038777 tr(WHW.T) 780.7407836914062
16_q proxy err 0.00014039600500836968 tr(WHW.T) 7193.3974609375
16_k proxy err 9.132856939686462e-05 tr(WHW.T) 11630.361328125
16_o proxy err 0.001005431986413896 tr(WHW.T) 88.22785186767578
16_up proxy err 0.00054499990073964 tr(WHW.T) 1890.9385986328125
16_gate proxy err 0.000311729236273095 tr(WHW.T) 3369.859130859375
16_down proxy err 0.0008333101868629456 tr(WHW.T) 152.0294952392578
I0314 09:59:44.704509 2584943 finetune.py:45] layer 17_q initial loss 2.4208336981246248e-05
I0314 09:59:53.640892 2584943 finetune.py:45] layer 17_k initial loss 2.6216872356599197e-05
I0314 10:00:02.419392 2584943 finetune.py:45] layer 17_o initial loss 4.1034782043425366e-05
I0314 10:00:17.396981 2584943 finetune.py:45] layer 17_up initial loss 5.865425191586837e-05
I0314 10:00:32.585029 2584943 finetune.py:45] layer 17_gate initial loss 7.217115489766002e-05
I0314 10:00:34.468942 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 18 in 67.34696817398071s
I0314 10:00:37.518567 2585157 config.py:54] PyTorch version 2.1.1 available.
I0314 10:00:38.628893 2580651 quantize_finetune_llama.py:191] layer 19 gpu 3
I0314 10:00:38.700941 2585157 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:00:47.662490 2585157 finetune.py:45] layer 18_v initial loss 2.149426654796116e-05
I0314 10:00:49.102557 2584943 finetune.py:45] layer 17_down initial loss 0.00010475421731825918
17_v proxy err 0.0011061595287173986 tr(WHW.T) 845.7654418945312
17_q proxy err 0.00014195518451742828 tr(WHW.T) 7163.2734375
17_k proxy err 9.987605881178752e-05 tr(WHW.T) 10697.431640625
17_o proxy err 0.001096839434467256 tr(WHW.T) 58.14826965332031
17_up proxy err 0.0005936237284913659 tr(WHW.T) 1921.07861328125
17_gate proxy err 0.0003264255356043577 tr(WHW.T) 3571.31640625
17_down proxy err 0.0008263870840892196 tr(WHW.T) 165.43495178222656
I0314 10:00:56.448224 2585157 finetune.py:45] layer 18_q initial loss 2.426385435683187e-05
I0314 10:01:05.238898 2585157 finetune.py:45] layer 18_k initial loss 2.703222526179161e-05
I0314 10:01:14.000296 2585157 finetune.py:45] layer 18_o initial loss 4.3247553549008444e-05
I0314 10:01:29.073901 2585157 finetune.py:45] layer 18_up initial loss 6.446521001635119e-05
I0314 10:01:44.063962 2585157 finetune.py:45] layer 18_gate initial loss 8.041188266361132e-05
I0314 10:01:45.728464 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 19 in 66.62848615646362s
I0314 10:01:48.804362 2585374 config.py:54] PyTorch version 2.1.1 available.
I0314 10:01:49.824061 2580651 quantize_finetune_llama.py:191] layer 20 gpu 0
I0314 10:01:49.896939 2585374 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:01:59.026608 2585374 finetune.py:45] layer 19_v initial loss 2.0444018446141854e-05
I0314 10:02:00.614772 2585157 finetune.py:45] layer 18_down initial loss 0.00011999681737506762
18_v proxy err 0.0010072956793010235 tr(WHW.T) 1003.7705078125
18_q proxy err 0.00014780201308894902 tr(WHW.T) 7510.48046875
18_k proxy err 0.00011002646351698786 tr(WHW.T) 10462.6650390625
18_o proxy err 0.0009579053148627281 tr(WHW.T) 69.96558380126953
18_up proxy err 0.0006308218580670655 tr(WHW.T) 2023.183837890625
18_gate proxy err 0.00034435593988746405 tr(WHW.T) 3783.076416015625
18_down proxy err 0.0008330415585078299 tr(WHW.T) 198.52699279785156
I0314 10:02:07.516544 2585374 finetune.py:45] layer 19_q initial loss 2.2957809051149525e-05
I0314 10:02:16.187757 2585374 finetune.py:45] layer 19_k initial loss 2.513066829124e-05
I0314 10:02:24.641809 2585374 finetune.py:45] layer 19_o initial loss 3.9450955227948725e-05
I0314 10:02:40.082348 2585374 finetune.py:45] layer 19_up initial loss 6.319805106613785e-05
I0314 10:02:56.464976 2585374 finetune.py:45] layer 19_gate initial loss 8.181932207662612e-05
I0314 10:03:00.294535 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 20 in 69.38780927658081s
I0314 10:03:03.695683 2585594 config.py:54] PyTorch version 2.1.1 available.
I0314 10:03:04.732577 2580651 quantize_finetune_llama.py:191] layer 21 gpu 1
I0314 10:03:04.792943 2585594 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:03:16.224853 2585594 finetune.py:45] layer 20_v initial loss 2.2915164663572796e-05
I0314 10:03:16.569411 2585374 finetune.py:45] layer 19_down initial loss 0.0001247944892384112
19_v proxy err 0.0009860498830676079 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.00015598553000018 tr(WHW.T) 6944.4130859375
19_k proxy err 0.00010774117981782183 tr(WHW.T) 10548.4892578125
19_o proxy err 0.0009427237091585994 tr(WHW.T) 62.291683197021484
19_up proxy err 0.0006345395231619477 tr(WHW.T) 2149.330322265625
19_gate proxy err 0.0003757971280720085 tr(WHW.T) 3687.5126953125
19_down proxy err 0.0008095570956356823 tr(WHW.T) 222.93177795410156
I0314 10:03:26.079707 2585594 finetune.py:45] layer 20_q initial loss 2.5795388864935376e-05
I0314 10:03:35.465433 2585594 finetune.py:45] layer 20_k initial loss 2.809527541103307e-05
I0314 10:03:44.879768 2585594 finetune.py:45] layer 20_o initial loss 4.507866469793953e-05
I0314 10:04:00.720608 2585594 finetune.py:45] layer 20_up initial loss 7.273813389474526e-05
I0314 10:04:14.699084 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 21 in 69.5411696434021s
I0314 10:04:17.535893 2585594 finetune.py:45] layer 20_gate initial loss 9.433992818230763e-05
I0314 10:04:18.034003 2585811 config.py:54] PyTorch version 2.1.1 available.
I0314 10:04:19.085944 2580651 quantize_finetune_llama.py:191] layer 22 gpu 2
I0314 10:04:19.153378 2585811 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:04:28.146431 2585811 finetune.py:45] layer 21_v initial loss 2.065069929813035e-05
I0314 10:04:35.058580 2585594 finetune.py:45] layer 20_down initial loss 0.00014733916032128036
20_v proxy err 0.0010084828827530146 tr(WHW.T) 990.5983276367188
20_q proxy err 0.0001528947614133358 tr(WHW.T) 7150.947265625
20_k proxy err 0.00011038118100259453 tr(WHW.T) 10386.2470703125
20_o proxy err 0.0006974379648454487 tr(WHW.T) 100.31707000732422
20_up proxy err 0.0006217605550773442 tr(WHW.T) 2340.89453125
20_gate proxy err 0.0003676307969726622 tr(WHW.T) 4024.62744140625
20_down proxy err 0.000805385410785675 tr(WHW.T) 274.8815002441406
I0314 10:04:37.151527 2585811 finetune.py:45] layer 21_q initial loss 2.3068825612426735e-05
I0314 10:04:46.256720 2585811 finetune.py:45] layer 21_k initial loss 2.5348650524392724e-05
I0314 10:04:55.130257 2585811 finetune.py:45] layer 21_o initial loss 3.9372149331029505e-05
I0314 10:05:09.994171 2585811 finetune.py:45] layer 21_up initial loss 6.951402610866353e-05
I0314 10:05:24.741706 2585811 finetune.py:45] layer 21_gate initial loss 9.269802831113338e-05
I0314 10:05:25.519346 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 22 in 66.04806160926819s
I0314 10:05:28.581768 2586028 config.py:54] PyTorch version 2.1.1 available.
I0314 10:05:29.591961 2580651 quantize_finetune_llama.py:191] layer 23 gpu 3
I0314 10:05:29.660958 2586028 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:05:38.151357 2586028 finetune.py:45] layer 22_v initial loss 2.516229505999945e-05
I0314 10:05:40.810806 2585811 finetune.py:45] layer 21_down initial loss 0.0001463629596401006
21_v proxy err 0.0009679594077169895 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.00016835478891152889 tr(WHW.T) 7064.314453125
21_k proxy err 0.0001233921793755144 tr(WHW.T) 9976.4658203125
21_o proxy err 0.0007424862124025822 tr(WHW.T) 75.50972747802734
21_up proxy err 0.0006539128953590989 tr(WHW.T) 2361.650390625
21_gate proxy err 0.00039195537101477385 tr(WHW.T) 4004.37646484375
21_down proxy err 0.0008053081692196429 tr(WHW.T) 276.5857849121094
I0314 10:05:46.688811 2586028 finetune.py:45] layer 22_q initial loss 2.9280679882504046e-05
I0314 10:05:55.037451 2586028 finetune.py:45] layer 22_k initial loss 3.3608925150474533e-05
I0314 10:06:03.579347 2586028 finetune.py:45] layer 22_o initial loss 5.1929218898294494e-05
I0314 10:06:18.462779 2586028 finetune.py:45] layer 22_up initial loss 8.611917292000726e-05
I0314 10:06:33.315010 2586028 finetune.py:45] layer 22_gate initial loss 0.00011251417163293809
I0314 10:06:35.788718 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 23 in 65.82297968864441s
I0314 10:06:38.872043 2586239 config.py:54] PyTorch version 2.1.1 available.
I0314 10:06:39.985205 2580651 quantize_finetune_llama.py:191] layer 24 gpu 0
I0314 10:06:40.059672 2586239 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:06:49.267764 2586239 finetune.py:45] layer 23_v initial loss 2.450968895573169e-05
I0314 10:06:49.858603 2586028 finetune.py:45] layer 22_down initial loss 0.00017170388309750706
22_v proxy err 0.0009223347879014909 tr(WHW.T) 1243.2529296875
22_q proxy err 0.00016041664639487863 tr(WHW.T) 7746.84765625
22_k proxy err 0.00012170941045042127 tr(WHW.T) 10603.2041015625
22_o proxy err 0.0006292332545854151 tr(WHW.T) 114.30065155029297
22_up proxy err 0.0006579364417120814 tr(WHW.T) 2474.510498046875
22_gate proxy err 0.0003975214494857937 tr(WHW.T) 4156.64013671875
22_down proxy err 0.0008009572047740221 tr(WHW.T) 311.8800048828125
I0314 10:06:57.774472 2586239 finetune.py:45] layer 23_q initial loss 2.7726062398869544e-05
I0314 10:07:06.154770 2586239 finetune.py:45] layer 23_k initial loss 3.073778498219326e-05
I0314 10:07:14.546966 2586239 finetune.py:45] layer 23_o initial loss 4.677259857999161e-05
I0314 10:07:29.027125 2586239 finetune.py:45] layer 23_up initial loss 8.415763295488432e-05
I0314 10:07:43.727330 2586239 finetune.py:45] layer 23_gate initial loss 0.00011379527859389782
I0314 10:07:47.858991 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 24 in 66.6920235157013s
I0314 10:07:51.023935 2586456 config.py:54] PyTorch version 2.1.1 available.
I0314 10:07:52.060729 2580651 quantize_finetune_llama.py:191] layer 25 gpu 1
I0314 10:07:52.135397 2586456 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:07:59.590355 2586239 finetune.py:45] layer 23_down initial loss 0.00017485377611592412
23_v proxy err 0.0008325489470735192 tr(WHW.T) 1486.037353515625
23_q proxy err 0.00017782473878469318 tr(WHW.T) 7346.60986328125
23_k proxy err 0.00013624317944049835 tr(WHW.T) 9982.2392578125
23_o proxy err 0.0007481661159545183 tr(WHW.T) 85.13458251953125
23_up proxy err 0.0006803949363529682 tr(WHW.T) 2533.51025390625
23_gate proxy err 0.00042684352956712246 tr(WHW.T) 4097.51953125
23_down proxy err 0.0008032608311623335 tr(WHW.T) 321.33892822265625
I0314 10:08:01.228489 2586456 finetune.py:45] layer 24_v initial loss 2.9217349947430193e-05
I0314 10:08:09.816575 2586456 finetune.py:45] layer 24_q initial loss 3.368854959262535e-05
I0314 10:08:18.399930 2586456 finetune.py:45] layer 24_k initial loss 3.704953633132391e-05
I0314 10:08:27.147478 2586456 finetune.py:45] layer 24_o initial loss 5.749872434535064e-05
I0314 10:08:42.155014 2586456 finetune.py:45] layer 24_up initial loss 9.788398892851546e-05
I0314 10:08:57.075085 2586456 finetune.py:45] layer 24_gate initial loss 0.00013070885324850678
I0314 10:08:57.972358 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 25 in 65.46261739730835s
I0314 10:09:01.005601 2586670 config.py:54] PyTorch version 2.1.1 available.
I0314 10:09:02.026845 2580651 quantize_finetune_llama.py:191] layer 26 gpu 2
I0314 10:09:02.082516 2586670 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:09:10.717965 2586670 finetune.py:45] layer 25_v initial loss 3.12070878862869e-05
I0314 10:09:13.751118 2586456 finetune.py:45] layer 24_down initial loss 0.00019573817553464323
24_v proxy err 0.0008858669316396117 tr(WHW.T) 1394.900634765625
24_q proxy err 0.00018853555957321078 tr(WHW.T) 7020.447265625
24_k proxy err 0.00013238881365396082 tr(WHW.T) 10323.43359375
24_o proxy err 0.0005951421335339546 tr(WHW.T) 133.98797607421875
24_up proxy err 0.0006895827827975154 tr(WHW.T) 2621.76513671875
24_gate proxy err 0.0004302040033508092 tr(WHW.T) 4262.74853515625
24_down proxy err 0.0008071771590039134 tr(WHW.T) 340.22412109375
I0314 10:09:19.322755 2586670 finetune.py:45] layer 25_q initial loss 3.467323404038325e-05
I0314 10:09:27.801244 2586670 finetune.py:45] layer 25_k initial loss 3.8041529478505254e-05
I0314 10:09:36.132151 2586670 finetune.py:45] layer 25_o initial loss 5.357664849725552e-05
I0314 10:09:50.653717 2586670 finetune.py:45] layer 25_up initial loss 9.891023364616558e-05
I0314 10:10:05.745044 2586670 finetune.py:45] layer 25_gate initial loss 0.0001356056600343436
I0314 10:10:09.595827 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 26 in 67.19517612457275s
I0314 10:10:12.797180 2586884 config.py:54] PyTorch version 2.1.1 available.
I0314 10:10:13.865393 2580651 quantize_finetune_llama.py:191] layer 27 gpu 3
I0314 10:10:13.936705 2586884 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:10:22.510504 2586670 finetune.py:45] layer 25_down initial loss 0.00020687587675638497
25_v proxy err 0.0007979214424267411 tr(WHW.T) 1707.664794921875
25_q proxy err 0.00019988513668067753 tr(WHW.T) 7162.16357421875
25_k proxy err 0.00015376426745206118 tr(WHW.T) 9611.58984375
25_o proxy err 0.0007016210583969951 tr(WHW.T) 83.535888671875
25_up proxy err 0.000685754872392863 tr(WHW.T) 2805.728515625
25_gate proxy err 0.0004182476259302348 tr(WHW.T) 4666.4404296875
25_down proxy err 0.0008016960346139967 tr(WHW.T) 373.460693359375
I0314 10:10:23.662697 2586884 finetune.py:45] layer 26_v initial loss 4.087432535015978e-05
I0314 10:10:32.354011 2586884 finetune.py:45] layer 26_q initial loss 4.6093751734588295e-05
I0314 10:10:41.070751 2586884 finetune.py:45] layer 26_k initial loss 5.105162927065976e-05
I0314 10:10:49.813865 2586884 finetune.py:45] layer 26_o initial loss 7.687947800150141e-05
I0314 10:11:04.965679 2586884 finetune.py:45] layer 26_up initial loss 0.00012610643170773983
I0314 10:11:20.249563 2586884 finetune.py:45] layer 26_gate initial loss 0.00016748618509154767
I0314 10:11:22.918800 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 27 in 68.56120538711548s
I0314 10:11:26.165620 2587104 config.py:54] PyTorch version 2.1.1 available.
I0314 10:11:27.197009 2580651 quantize_finetune_llama.py:191] layer 28 gpu 0
I0314 10:11:27.265595 2587104 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:11:39.295163 2587104 finetune.py:45] layer 27_v initial loss 3.0447445169556886e-05
I0314 10:11:39.376412 2586884 finetune.py:45] layer 26_down initial loss 0.0002455568464938551
26_v proxy err 0.0007963839452713728 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.00018944185285363346 tr(WHW.T) 7469.98291015625
26_k proxy err 0.00013809332449454814 tr(WHW.T) 10487.8740234375
26_o proxy err 0.0004911629948765039 tr(WHW.T) 202.88172912597656
26_up proxy err 0.0006441662553697824 tr(WHW.T) 3154.75146484375
26_gate proxy err 0.0003895707777701318 tr(WHW.T) 5302.16455078125
26_down proxy err 0.0008232016116380692 tr(WHW.T) 401.19390869140625
I0314 10:11:49.197070 2587104 finetune.py:45] layer 27_q initial loss 3.592453504097648e-05
I0314 10:11:58.182803 2587104 finetune.py:45] layer 27_k initial loss 4.069296483066864e-05
I0314 10:12:07.230084 2587104 finetune.py:45] layer 27_o initial loss 6.215058965608478e-05
I0314 10:12:23.687518 2587104 finetune.py:45] layer 27_up initial loss 0.00011855610500788316
I0314 10:12:40.996356 2587104 finetune.py:45] layer 27_gate initial loss 0.00016639730893075466
I0314 10:12:41.632200 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 28 in 73.27738785743713s
I0314 10:12:44.890828 2587327 config.py:54] PyTorch version 2.1.1 available.
I0314 10:12:45.966666 2580651 quantize_finetune_llama.py:191] layer 29 gpu 1
I0314 10:12:46.025807 2587327 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:12:57.619791 2587327 finetune.py:45] layer 28_v initial loss 3.9237278542714193e-05
I0314 10:13:01.504514 2587104 finetune.py:45] layer 27_down initial loss 0.0002592520322650671
27_v proxy err 0.0007224035798572004 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.0001813832059269771 tr(WHW.T) 7691.708984375
27_k proxy err 0.0001350377278868109 tr(WHW.T) 10618.70703125
27_o proxy err 0.0006541933980770409 tr(WHW.T) 126.13690185546875
27_up proxy err 0.0005881650140509009 tr(WHW.T) 3691.557861328125
27_gate proxy err 0.00036787695717066526 tr(WHW.T) 5990.82568359375
27_down proxy err 0.0008380886865779757 tr(WHW.T) 466.9318542480469
I0314 10:13:08.263548 2587327 finetune.py:45] layer 28_q initial loss 4.534651816356927e-05
I0314 10:13:17.605001 2587327 finetune.py:45] layer 28_k initial loss 5.100458656670526e-05
I0314 10:13:27.064876 2587327 finetune.py:45] layer 28_o initial loss 7.891703717177734e-05
I0314 10:13:44.762183 2587327 finetune.py:45] layer 28_up initial loss 0.00014597710105590522
I0314 10:14:00.472251 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 29 in 74.06394267082214s
I0314 10:14:02.293649 2587327 finetune.py:45] layer 28_gate initial loss 0.0002037359809037298
I0314 10:14:03.561325 2587553 config.py:54] PyTorch version 2.1.1 available.
I0314 10:14:04.624130 2580651 quantize_finetune_llama.py:191] layer 30 gpu 2
I0314 10:14:04.676925 2587553 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:14:15.371451 2587553 finetune.py:45] layer 29_v initial loss 3.734632264240645e-05
I0314 10:14:23.067105 2587327 finetune.py:45] layer 28_down initial loss 0.00032521726097911596
28_v proxy err 0.0006706534186378121 tr(WHW.T) 2018.944091796875
28_q proxy err 0.0001880738855106756 tr(WHW.T) 7651.126953125
28_k proxy err 0.0001399749016854912 tr(WHW.T) 10544.8251953125
28_o proxy err 0.0005393022438511252 tr(WHW.T) 194.8240966796875
28_up proxy err 0.0004933668533340096 tr(WHW.T) 4661.2666015625
28_gate proxy err 0.00035541653051041067 tr(WHW.T) 6547.48193359375
28_down proxy err 0.0008439478697255254 tr(WHW.T) 603.8403930664062
I0314 10:14:27.235191 2587553 finetune.py:45] layer 29_q initial loss 4.277326297597028e-05
I0314 10:14:37.798374 2587553 finetune.py:45] layer 29_k initial loss 4.8150945076486096e-05
I0314 10:14:48.274177 2587553 finetune.py:45] layer 29_o initial loss 7.332043605856597e-05
I0314 10:15:06.243038 2587553 finetune.py:45] layer 29_up initial loss 0.00015317088400479406
I0314 10:15:21.437445 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 30 in 76.4327962398529s
I0314 10:15:23.457691 2587553 finetune.py:45] layer 29_gate initial loss 0.00022229137539397925
I0314 10:15:24.481662 2587782 config.py:54] PyTorch version 2.1.1 available.
I0314 10:15:25.506135 2580651 quantize_finetune_llama.py:191] layer 31 gpu 3
I0314 10:15:25.571511 2587782 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:15:34.405076 2587782 finetune.py:45] layer 30_v initial loss 3.728293813765049e-05
I0314 10:15:40.678701 2587553 finetune.py:45] layer 29_down initial loss 0.00038238620618358254
29_v proxy err 0.0007109494763426483 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.00018735448247753084 tr(WHW.T) 7227.0009765625
29_k proxy err 0.00013316472177393734 tr(WHW.T) 10558.609375
29_o proxy err 0.0004577393119689077 tr(WHW.T) 207.9054412841797
29_up proxy err 0.0003962507762480527 tr(WHW.T) 6070.0498046875
29_gate proxy err 0.0003288394946139306 tr(WHW.T) 7369.6142578125
29_down proxy err 0.0008638605359010398 tr(WHW.T) 782.2448120117188
I0314 10:15:43.281953 2587782 finetune.py:45] layer 30_q initial loss 4.4623080611927435e-05
I0314 10:15:52.000971 2587782 finetune.py:45] layer 30_k initial loss 5.1284576329635456e-05
I0314 10:16:00.698083 2587782 finetune.py:45] layer 30_o initial loss 8.389233698835596e-05
I0314 10:16:16.156029 2587782 finetune.py:45] layer 30_up initial loss 0.00021585257491096854
I0314 10:16:31.450542 2587782 finetune.py:45] layer 30_gate initial loss 0.00030480424175038934
I0314 10:16:34.256733 2580651 quantize_finetune_llama.py:218] computed original embedding for layer 31 in 68.30815696716309s
I0314 10:16:37.290925 2587999 config.py:54] PyTorch version 2.1.1 available.
I0314 10:16:38.441312 2587999 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 10:16:47.721231 2587999 finetune.py:45] layer 31_v initial loss 6.737996591255069e-05
I0314 10:16:48.199389 2587782 finetune.py:45] layer 30_down initial loss 0.000736180751118809
30_v proxy err 0.0005816435441374779 tr(WHW.T) 2261.489501953125
30_q proxy err 0.00017786592070478946 tr(WHW.T) 7815.9453125
30_k proxy err 0.0001352110120933503 tr(WHW.T) 10521.625
30_o proxy err 0.000475546985398978 tr(WHW.T) 251.96908569335938
30_up proxy err 0.0002481148694641888 tr(WHW.T) 10016.376953125
30_gate proxy err 0.00022674941283185035 tr(WHW.T) 11001.119140625
30_down proxy err 0.0005018241354264319 tr(WHW.T) 3582.617919921875
I0314 10:16:56.465603 2587999 finetune.py:45] layer 31_q initial loss 9.384835720993578e-05
I0314 10:17:04.964763 2587999 finetune.py:45] layer 31_k initial loss 0.00010572963947197422
I0314 10:17:13.421542 2587999 finetune.py:45] layer 31_o initial loss 0.0001572821056470275
I0314 10:17:28.262199 2587999 finetune.py:45] layer 31_up initial loss 0.0005054802750237286
I0314 10:17:43.276921 2587999 finetune.py:45] layer 31_gate initial loss 0.0006683125393465161
I0314 10:17:59.351631 2587999 finetune.py:45] layer 31_down initial loss 0.0022049155086278915
31_v proxy err 0.0007040580385364592 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.00014207293861545622 tr(WHW.T) 6858.09130859375
31_k proxy err 9.958637383533642e-05 tr(WHW.T) 10233.677734375
31_o proxy err 0.00037177803460508585 tr(WHW.T) 457.7950744628906
31_up proxy err 0.00014573791122529656 tr(WHW.T) 14563.890625
31_gate proxy err 0.00014363409718498588 tr(WHW.T) 14836.2939453125
31_down proxy err 0.00035886987461708486 tr(WHW.T) 17873.55859375
