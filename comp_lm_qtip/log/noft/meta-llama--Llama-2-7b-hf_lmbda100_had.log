I0314 08:06:59.078058 2528979 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.69it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.83it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.65it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.12it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.52it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.13it/s]
I0314 08:07:01.139267 2528979 quantize_finetune_llama.py:142] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:23,  1.33it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:22,  1.36it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:21,  1.37it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:20,  1.38it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:19,  1.36it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:19,  1.36it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:05<00:18,  1.35it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:17,  1.34it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:16,  1.35it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:07<00:16,  1.34it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:08<00:15,  1.34it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:14,  1.35it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:14,  1.36it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:10<00:13,  1.36it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:11<00:12,  1.38it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:11,  1.38it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:12<00:10,  1.40it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:13<00:09,  1.41it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:09,  1.41it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:14<00:08,  1.41it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:15<00:07,  1.41it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:06,  1.43it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:06,  1.44it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:05,  1.45it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:18<00:04,  1.43it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.42it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:19<00:03,  1.41it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:20<00:02,  1.41it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:02,  1.43it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.42it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:22<00:00,  1.42it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.40it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.39it/s]
I0314 08:07:34.047925 2528979 quantize_finetune_llama.py:167] loaded compression model
I0314 08:07:55.839967 2528979 quantize_finetune_llama.py:171] loaded dataset and devset
I0314 08:08:01.111137 2528979 quantize_finetune_llama.py:191] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:09:20.203597 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 0 in 78.97864103317261s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 08:09:42.883178 2530748 config.py:54] PyTorch version 2.1.1 available.
I0314 08:09:43.822269 2528979 quantize_finetune_llama.py:191] layer 1 gpu 1
I0314 08:10:51.787734 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 1 in 67.813725233078s
I0314 08:11:02.244306 2531519 config.py:54] PyTorch version 2.1.1 available.
I0314 08:11:03.210562 2528979 quantize_finetune_llama.py:191] layer 2 gpu 2
I0314 08:12:11.264094 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 2 in 67.92664074897766s
I0314 08:12:19.036309 2532285 config.py:54] PyTorch version 2.1.1 available.
I0314 08:12:19.970716 2528979 quantize_finetune_llama.py:191] layer 3 gpu 3
I0314 08:12:20.025143 2532285 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:12:28.581059 2532285 finetune.py:45] layer 2_v initial loss 9.464671165915206e-06
I0314 08:12:37.195432 2532285 finetune.py:45] layer 2_q initial loss 9.99448911898071e-06
I0314 08:12:45.858143 2532285 finetune.py:45] layer 2_k initial loss 1.0578367437119596e-05
I0314 08:12:54.699047 2532285 finetune.py:45] layer 2_o initial loss 1.4224928236217238e-05
I0314 08:13:09.612591 2532285 finetune.py:45] layer 2_up initial loss 1.660810266912449e-05
I0314 08:13:24.568911 2532285 finetune.py:45] layer 2_gate initial loss 1.8404374714009464e-05
I0314 08:13:32.738748 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 3 in 72.59770369529724s
I0314 08:13:42.968240 2532285 finetune.py:45] layer 2_down initial loss 2.33085283980472e-05
I0314 08:13:43.451087 2533300 config.py:54] PyTorch version 2.1.1 available.
2_v proxy err 0.01495397835969925 tr(WHW.T) 136.67332458496094
2_q proxy err 0.0003689636941999197 tr(WHW.T) 7752.85205078125
2_k proxy err 0.0002892393968068063 tr(WHW.T) 10205.837890625
2_o proxy err 0.007906598970293999 tr(WHW.T) 1.4603197574615479
2_up proxy err 0.005999243818223476 tr(WHW.T) 193.43603515625
2_gate proxy err 0.0038906331174075603 tr(WHW.T) 306.6622619628906
2_down proxy err 0.006909828633069992 tr(WHW.T) 3.010739803314209
I0314 08:13:44.395508 2528979 quantize_finetune_llama.py:191] layer 4 gpu 0
I0314 08:13:44.452387 2533300 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:13:53.609581 2533300 finetune.py:45] layer 3_v initial loss 1.4534792171616573e-05
I0314 08:14:02.050238 2533300 finetune.py:45] layer 3_q initial loss 1.5771085600135848e-05
I0314 08:14:10.387331 2533300 finetune.py:45] layer 3_k initial loss 1.7125537851825356e-05
I0314 08:14:18.930615 2533300 finetune.py:45] layer 3_o initial loss 2.5037383238668554e-05
I0314 08:14:33.721914 2533300 finetune.py:45] layer 3_up initial loss 3.074206688324921e-05
I0314 08:14:48.599371 2533300 finetune.py:45] layer 3_gate initial loss 3.4878987207775936e-05
I0314 08:14:54.099488 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 4 in 68.40991592407227s
I0314 08:14:57.251459 2534204 config.py:54] PyTorch version 2.1.1 available.
I0314 08:14:58.359579 2528979 quantize_finetune_llama.py:191] layer 5 gpu 1
I0314 08:14:58.430042 2534204 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:15:04.604139 2533300 finetune.py:45] layer 3_down initial loss 4.524040923570283e-05
3_v proxy err 0.013855881057679653 tr(WHW.T) 284.77557373046875
3_q proxy err 0.0007092630257830024 tr(WHW.T) 7217.63720703125
3_k proxy err 0.0005223305197432637 tr(WHW.T) 10074.73828125
3_o proxy err 0.007895415648818016 tr(WHW.T) 3.3527450561523438
3_up proxy err 0.006845729425549507 tr(WHW.T) 284.7950744628906
3_gate proxy err 0.004196794703602791 tr(WHW.T) 478.13714599609375
3_down proxy err 0.007066783495247364 tr(WHW.T) 6.133229732513428
I0314 08:15:07.303617 2534204 finetune.py:45] layer 4_v initial loss 2.5683382773422636e-05
I0314 08:15:15.941382 2534204 finetune.py:45] layer 4_q initial loss 2.7607142328633927e-05
I0314 08:15:24.661015 2534204 finetune.py:45] layer 4_k initial loss 2.9626844479935244e-05
I0314 08:15:33.530010 2534204 finetune.py:45] layer 4_o initial loss 4.067090048920363e-05
I0314 08:15:48.749084 2534204 finetune.py:45] layer 4_up initial loss 5.142293230164796e-05
I0314 08:16:03.937169 2534204 finetune.py:45] layer 4_gate initial loss 5.841251913807355e-05
I0314 08:16:05.901904 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 5 in 67.17812085151672s
I0314 08:16:08.961642 2535102 config.py:54] PyTorch version 2.1.1 available.
I0314 08:16:09.977865 2528979 quantize_finetune_llama.py:191] layer 6 gpu 2
I0314 08:16:10.036756 2535102 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:16:18.656969 2535102 finetune.py:45] layer 5_v initial loss 3.879802898154594e-05
I0314 08:16:20.464651 2534204 finetune.py:45] layer 4_down initial loss 7.774877303745598e-05
4_v proxy err 0.01343601755797863 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0006854275707155466 tr(WHW.T) 6914.9892578125
4_k proxy err 0.0004732768575195223 tr(WHW.T) 10415.33203125
4_o proxy err 0.00786665640771389 tr(WHW.T) 5.139806270599365
4_up proxy err 0.006744324695318937 tr(WHW.T) 397.6960144042969
4_gate proxy err 0.003410682315006852 tr(WHW.T) 821.1856689453125
4_down proxy err 0.007081431336700916 tr(WHW.T) 11.562739372253418
I0314 08:16:27.177413 2535102 finetune.py:45] layer 5_q initial loss 4.1590621549403295e-05
I0314 08:16:35.834067 2535102 finetune.py:45] layer 5_k initial loss 4.446811362868175e-05
I0314 08:16:44.416081 2535102 finetune.py:45] layer 5_o initial loss 6.657058838754892e-05
I0314 08:16:59.308376 2535102 finetune.py:45] layer 5_up initial loss 8.356809848919511e-05
I0314 08:17:14.284051 2535102 finetune.py:45] layer 5_gate initial loss 9.412459621671587e-05
I0314 08:17:17.534048 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 6 in 67.13737654685974s
I0314 08:17:20.649416 2536000 config.py:54] PyTorch version 2.1.1 available.
I0314 08:17:21.689877 2528979 quantize_finetune_llama.py:191] layer 7 gpu 3
I0314 08:17:21.756718 2536000 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:17:30.262829 2535102 finetune.py:45] layer 5_down initial loss 0.00012145018990850076
I0314 08:17:30.448771 2536000 finetune.py:45] layer 6_v initial loss 4.9252139433519915e-05
5_v proxy err 0.013320306316018105 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0007558940560556948 tr(WHW.T) 6770.97509765625
5_k proxy err 0.00048388971481472254 tr(WHW.T) 10841.955078125
5_o proxy err 0.011164824478328228 tr(WHW.T) 7.947142601013184
5_up proxy err 0.0066215405240654945 tr(WHW.T) 506.6408386230469
5_gate proxy err 0.003177930600941181 tr(WHW.T) 1104.867919921875
5_down proxy err 0.007457731757313013 tr(WHW.T) 15.6494779586792
I0314 08:17:39.163458 2536000 finetune.py:45] layer 6_q initial loss 5.3948897402733564e-05
I0314 08:17:47.929476 2536000 finetune.py:45] layer 6_k initial loss 5.871877510799095e-05
I0314 08:17:56.662069 2536000 finetune.py:45] layer 6_o initial loss 8.697543671587482e-05
I0314 08:18:11.959741 2536000 finetune.py:45] layer 6_up initial loss 0.00011363164958311245
I0314 08:18:27.190337 2536000 finetune.py:45] layer 6_gate initial loss 0.00012886585318483412
I0314 08:18:30.164924 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 7 in 68.05345821380615s
I0314 08:18:33.290087 2536883 config.py:54] PyTorch version 2.1.1 available.
I0314 08:18:34.342951 2528979 quantize_finetune_llama.py:191] layer 8 gpu 0
I0314 08:18:34.421398 2536883 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:18:44.160521 2536883 finetune.py:45] layer 7_v initial loss 6.202182703418657e-05
I0314 08:18:44.667805 2536000 finetune.py:45] layer 6_down initial loss 0.00017078356177080423
6_v proxy err 0.012994624674320221 tr(WHW.T) 443.5464782714844
6_q proxy err 0.0009624926024116576 tr(WHW.T) 7576.53857421875
6_k proxy err 0.0007100931252352893 tr(WHW.T) 10409.4033203125
6_o proxy err 0.010266484692692757 tr(WHW.T) 11.564380645751953
6_up proxy err 0.006633632816374302 tr(WHW.T) 617.2608642578125
6_gate proxy err 0.00278117205016315 tr(WHW.T) 1554.7271728515625
6_down proxy err 0.007697157096117735 tr(WHW.T) 22.988168716430664
I0314 08:18:52.878101 2536883 finetune.py:45] layer 7_q initial loss 6.940764433238655e-05
I0314 08:19:01.558486 2536883 finetune.py:45] layer 7_k initial loss 7.594276394229382e-05
I0314 08:19:10.310468 2536883 finetune.py:45] layer 7_o initial loss 0.00011520290718181059
I0314 08:19:25.114580 2536883 finetune.py:45] layer 7_up initial loss 0.00015213388542179018
I0314 08:19:40.099437 2536883 finetune.py:45] layer 7_gate initial loss 0.00017309663235209882
I0314 08:19:44.962235 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 8 in 69.31114530563354s
I0314 08:19:48.137025 2537799 config.py:54] PyTorch version 2.1.1 available.
I0314 08:19:49.142312 2528979 quantize_finetune_llama.py:191] layer 9 gpu 1
I0314 08:19:49.201514 2537799 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:19:56.685271 2536883 finetune.py:45] layer 7_down initial loss 0.00022931868443265557
7_v proxy err 0.012773698195815086 tr(WHW.T) 489.9357604980469
7_q proxy err 0.0010185425635427237 tr(WHW.T) 7672.17919921875
7_k proxy err 0.0007739675347693264 tr(WHW.T) 10198.3701171875
7_o proxy err 0.011728908866643906 tr(WHW.T) 15.11335563659668
7_up proxy err 0.006542703602463007 tr(WHW.T) 735.8538818359375
7_gate proxy err 0.002710497472435236 tr(WHW.T) 1876.0390625
7_down proxy err 0.0077965655364096165 tr(WHW.T) 30.58672523498535
I0314 08:19:58.159271 2537799 finetune.py:45] layer 8_v initial loss 9.624395170249045e-05
I0314 08:20:06.931419 2537799 finetune.py:45] layer 8_q initial loss 0.00010693948570406064
I0314 08:20:15.943343 2537799 finetune.py:45] layer 8_k initial loss 0.00011571982759051025
I0314 08:20:24.953084 2537799 finetune.py:45] layer 8_o initial loss 0.00017584754095878452
I0314 08:20:40.264280 2537799 finetune.py:45] layer 8_up initial loss 0.00022038636961951852
I0314 08:20:55.317229 2537799 finetune.py:45] layer 8_gate initial loss 0.0002474926586728543
I0314 08:20:57.646632 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 9 in 68.08379697799683s
I0314 08:21:00.772468 2538739 config.py:54] PyTorch version 2.1.1 available.
I0314 08:21:01.808765 2528979 quantize_finetune_llama.py:191] layer 10 gpu 2
I0314 08:21:01.877237 2538739 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:21:10.509757 2538739 finetune.py:45] layer 9_v initial loss 0.0001068325072992593
I0314 08:21:11.517778 2537799 finetune.py:45] layer 8_down initial loss 0.0003158292092848569
8_v proxy err 0.012163190171122551 tr(WHW.T) 530.9967041015625
8_q proxy err 0.0010931880678981543 tr(WHW.T) 7228.1201171875
8_k proxy err 0.0007577102514915168 tr(WHW.T) 10639.1015625
8_o proxy err 0.013333423063158989 tr(WHW.T) 20.092191696166992
8_up proxy err 0.006041212007403374 tr(WHW.T) 866.312744140625
8_gate proxy err 0.0027768590953201056 tr(WHW.T) 1970.857177734375
8_down proxy err 0.007766516413539648 tr(WHW.T) 37.177734375
I0314 08:21:18.994263 2538739 finetune.py:45] layer 9_q initial loss 0.00011805020039901137
I0314 08:21:27.473200 2538739 finetune.py:45] layer 9_k initial loss 0.00012892279482912272
I0314 08:21:36.257433 2538739 finetune.py:45] layer 9_o initial loss 0.00020271324319764972
I0314 08:21:51.058522 2538739 finetune.py:45] layer 9_up initial loss 0.0002536402898840606
I0314 08:22:05.913182 2538739 finetune.py:45] layer 9_gate initial loss 0.00028651091270148754
I0314 08:22:09.120711 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 10 in 66.87930226325989s
I0314 08:22:12.176250 2539759 config.py:54] PyTorch version 2.1.1 available.
I0314 08:22:13.188865 2528979 quantize_finetune_llama.py:191] layer 11 gpu 3
I0314 08:22:13.261626 2539759 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:22:21.975193 2539759 finetune.py:45] layer 10_v initial loss 0.0001444895751774311
I0314 08:22:22.191175 2538739 finetune.py:45] layer 9_down initial loss 0.0003651923325378448
9_v proxy err 0.011334186419844627 tr(WHW.T) 565.0663452148438
9_q proxy err 0.0011426141718402505 tr(WHW.T) 6970.3359375
9_k proxy err 0.0007438203319907188 tr(WHW.T) 10987.3515625
9_o proxy err 0.01329864002764225 tr(WHW.T) 25.610172271728516
9_up proxy err 0.005832672119140625 tr(WHW.T) 970.8984375
9_gate proxy err 0.0027602892369031906 tr(WHW.T) 2132.69384765625
9_down proxy err 0.007788482587784529 tr(WHW.T) 42.99482727050781
I0314 08:22:30.549575 2539759 finetune.py:45] layer 10_q initial loss 0.0001577369257574901
I0314 08:22:39.152194 2539759 finetune.py:45] layer 10_k initial loss 0.00017246004426851869
I0314 08:22:47.873135 2539759 finetune.py:45] layer 10_o initial loss 0.00027424967265687883
I0314 08:23:02.802603 2539759 finetune.py:45] layer 10_up initial loss 0.00033210020046681166
I0314 08:23:17.904634 2539759 finetune.py:45] layer 10_gate initial loss 0.0003711247118189931
I0314 08:23:20.999625 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 11 in 67.37239575386047s
I0314 08:23:24.210092 2540956 config.py:54] PyTorch version 2.1.1 available.
I0314 08:23:25.291878 2528979 quantize_finetune_llama.py:191] layer 12 gpu 0
I0314 08:23:25.361055 2540956 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:23:34.891331 2540956 finetune.py:45] layer 11_v initial loss 0.00016044019139371812
I0314 08:23:34.984912 2539759 finetune.py:45] layer 10_down initial loss 0.00046231623855419457
10_v proxy err 0.011367769911885262 tr(WHW.T) 578.807373046875
10_q proxy err 0.0011795292375609279 tr(WHW.T) 6915.87109375
10_k proxy err 0.0007664550794288516 tr(WHW.T) 10996.2431640625
10_o proxy err 0.013606153428554535 tr(WHW.T) 35.184165954589844
10_up proxy err 0.005512564908713102 tr(WHW.T) 1080.198486328125
10_gate proxy err 0.0027272948063910007 tr(WHW.T) 2260.88330078125
10_down proxy err 0.007433515507727861 tr(WHW.T) 52.33584976196289
I0314 08:23:43.230635 2540956 finetune.py:45] layer 11_q initial loss 0.00017249377560801804
I0314 08:23:51.649849 2540956 finetune.py:45] layer 11_k initial loss 0.0001848760002758354
I0314 08:24:00.027513 2540956 finetune.py:45] layer 11_o initial loss 0.0002884229179471731
I0314 08:24:14.768105 2540956 finetune.py:45] layer 11_up initial loss 0.0003545271174516529
I0314 08:24:29.543804 2540956 finetune.py:45] layer 11_gate initial loss 0.00039877460221759975
I0314 08:24:35.013563 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 12 in 68.41327166557312s
I0314 08:24:38.231019 2542137 config.py:54] PyTorch version 2.1.1 available.
I0314 08:24:39.343851 2528979 quantize_finetune_llama.py:191] layer 13 gpu 1
I0314 08:24:39.413850 2542137 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:24:45.557294 2540956 finetune.py:45] layer 11_down initial loss 0.0004997985670343041
11_v proxy err 0.01084186788648367 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0013368603540584445 tr(WHW.T) 7027.10986328125
11_k proxy err 0.0009098804439418018 tr(WHW.T) 10511.23046875
11_o proxy err 0.013768082484602928 tr(WHW.T) 36.654052734375
11_up proxy err 0.005616991780698299 tr(WHW.T) 1139.6044921875
11_gate proxy err 0.002754107816144824 tr(WHW.T) 2392.716552734375
11_down proxy err 0.007626493461430073 tr(WHW.T) 56.13530731201172
I0314 08:24:48.450269 2542137 finetune.py:45] layer 12_v initial loss 0.00016447367670480162
I0314 08:24:57.022141 2542137 finetune.py:45] layer 12_q initial loss 0.00018091949459630996
I0314 08:25:05.735200 2542137 finetune.py:45] layer 12_k initial loss 0.00020008735009469092
I0314 08:25:14.561031 2542137 finetune.py:45] layer 12_o initial loss 0.0003165849484503269
I0314 08:25:29.593244 2542137 finetune.py:45] layer 12_up initial loss 0.0003890030784532428
I0314 08:25:44.645548 2542137 finetune.py:45] layer 12_gate initial loss 0.00044071191223338246
I0314 08:25:46.233816 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 13 in 66.49039363861084s
I0314 08:25:49.142709 2543130 config.py:54] PyTorch version 2.1.1 available.
I0314 08:25:50.155428 2528979 quantize_finetune_llama.py:191] layer 14 gpu 2
I0314 08:25:50.226370 2543130 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:25:58.977942 2543130 finetune.py:45] layer 13_v initial loss 0.00016060991038102657
I0314 08:26:00.906209 2542137 finetune.py:45] layer 12_down initial loss 0.000556230777874589
12_v proxy err 0.011078215204179287 tr(WHW.T) 703.318603515625
12_q proxy err 0.0013393890112638474 tr(WHW.T) 7045.6435546875
12_k proxy err 0.0008821230148896575 tr(WHW.T) 10893.65625
12_o proxy err 0.013993863016366959 tr(WHW.T) 39.29071044921875
12_up proxy err 0.00556483119726181 tr(WHW.T) 1228.298583984375
12_gate proxy err 0.0029398654587566853 tr(WHW.T) 2381.994873046875
12_down proxy err 0.007617837283760309 tr(WHW.T) 64.17745208740234
I0314 08:26:07.696567 2543130 finetune.py:45] layer 13_q initial loss 0.00017624163592699915
I0314 08:26:16.000062 2543130 finetune.py:45] layer 13_k initial loss 0.00019230176985729486
I0314 08:26:24.553701 2543130 finetune.py:45] layer 13_o initial loss 0.0003112518461421132
I0314 08:26:39.209131 2543130 finetune.py:45] layer 13_up initial loss 0.00039888022001832724
I0314 08:26:53.907090 2543130 finetune.py:45] layer 13_gate initial loss 0.00046174402814358473
I0314 08:26:57.289923 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 14 in 66.7363932132721s
I0314 08:27:00.319027 2544037 config.py:54] PyTorch version 2.1.1 available.
I0314 08:27:01.418321 2528979 quantize_finetune_llama.py:191] layer 15 gpu 3
I0314 08:27:01.488496 2544037 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:27:09.776420 2543130 finetune.py:45] layer 13_down initial loss 0.000605703447945416
I0314 08:27:10.070020 2544037 finetune.py:45] layer 14_v initial loss 0.00020221328304614872
13_v proxy err 0.010946877300739288 tr(WHW.T) 714.5677490234375
13_q proxy err 0.00134132185485214 tr(WHW.T) 6956.03564453125
13_k proxy err 0.0009217669139616191 tr(WHW.T) 10426.6318359375
13_o proxy err 0.012496464885771275 tr(WHW.T) 45.8377571105957
13_up proxy err 0.0053681801073253155 tr(WHW.T) 1367.6221923828125
13_gate proxy err 0.0028757185209542513 tr(WHW.T) 2601.504638671875
13_down proxy err 0.007649940438568592 tr(WHW.T) 79.3589096069336
I0314 08:27:18.410924 2544037 finetune.py:45] layer 14_q initial loss 0.00022078138135839254
I0314 08:27:26.787886 2544037 finetune.py:45] layer 14_k initial loss 0.00023871494340710342
I0314 08:27:35.390397 2544037 finetune.py:45] layer 14_o initial loss 0.00038829317782074213
I0314 08:27:50.101434 2544037 finetune.py:45] layer 14_up initial loss 0.0004871138371527195
I0314 08:28:05.206407 2544037 finetune.py:45] layer 14_gate initial loss 0.0005608579376712441
I0314 08:28:07.822161 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 15 in 65.98937582969666s
I0314 08:28:10.903434 2544958 config.py:54] PyTorch version 2.1.1 available.
I0314 08:28:11.941818 2528979 quantize_finetune_llama.py:191] layer 16 gpu 0
I0314 08:28:12.009717 2544958 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:28:21.224911 2544958 finetune.py:45] layer 15_v initial loss 0.00020102613780181855
I0314 08:28:21.914729 2544037 finetune.py:45] layer 14_down initial loss 0.0007290366920642555
14_v proxy err 0.011596912518143654 tr(WHW.T) 706.1612548828125
14_q proxy err 0.0013803212204948068 tr(WHW.T) 7077.06103515625
14_k proxy err 0.0008862394606694579 tr(WHW.T) 11295.16796875
14_o proxy err 0.013765125535428524 tr(WHW.T) 50.921180725097656
14_up proxy err 0.00542820431292057 tr(WHW.T) 1464.7159423828125
14_gate proxy err 0.0030197736341506243 tr(WHW.T) 2682.584716796875
14_down proxy err 0.00781492330133915 tr(WHW.T) 90.28684997558594
I0314 08:28:29.723683 2544958 finetune.py:45] layer 15_q initial loss 0.00022048718528822064
I0314 08:28:37.962077 2544958 finetune.py:45] layer 15_k initial loss 0.00023676114506088197
I0314 08:28:46.472346 2544958 finetune.py:45] layer 15_o initial loss 0.0003877712006215006
I0314 08:29:01.330619 2544958 finetune.py:45] layer 15_up initial loss 0.0005078533431515098
I0314 08:29:16.633655 2544958 finetune.py:45] layer 15_gate initial loss 0.0005999862332828343
I0314 08:29:21.538524 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 16 in 68.29611349105835s
I0314 08:29:24.974621 2545972 config.py:54] PyTorch version 2.1.1 available.
I0314 08:29:25.999552 2528979 quantize_finetune_llama.py:191] layer 17 gpu 1
I0314 08:29:26.065493 2545972 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:29:33.149635 2544958 finetune.py:45] layer 15_down initial loss 0.0008117870893329382
15_v proxy err 0.010664517059922218 tr(WHW.T) 762.7275390625
15_q proxy err 0.0013271596981212497 tr(WHW.T) 7252.0009765625
15_k proxy err 0.0008919494575820863 tr(WHW.T) 11072.3974609375
15_o proxy err 0.011726339347660542 tr(WHW.T) 59.61664962768555
15_up proxy err 0.0052902791649103165 tr(WHW.T) 1641.0228271484375
15_gate proxy err 0.0030452420469373465 tr(WHW.T) 2905.140380859375
15_down proxy err 0.0078103188425302505 tr(WHW.T) 114.09001922607422
I0314 08:29:35.219729 2545972 finetune.py:45] layer 16_v initial loss 0.00023965525906533003
I0314 08:29:44.014421 2545972 finetune.py:45] layer 16_q initial loss 0.0002648557419888675
I0314 08:29:53.083497 2545972 finetune.py:45] layer 16_k initial loss 0.0002864128618966788
I0314 08:30:02.205206 2545972 finetune.py:45] layer 16_o initial loss 0.000469105871161446
I0314 08:30:17.360495 2545972 finetune.py:45] layer 16_up initial loss 0.0006214909953996539
I0314 08:30:32.681193 2545972 finetune.py:45] layer 16_gate initial loss 0.0007401803159154952
I0314 08:30:34.090314 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 17 in 67.67265558242798s
I0314 08:30:37.212244 2546926 config.py:54] PyTorch version 2.1.1 available.
I0314 08:30:38.637054 2528979 quantize_finetune_llama.py:191] layer 18 gpu 2
I0314 08:30:38.704658 2546926 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:30:47.509166 2546926 finetune.py:45] layer 17_v initial loss 0.00019913735741283745
I0314 08:30:49.335523 2545972 finetune.py:45] layer 16_down initial loss 0.001024890225380659
16_v proxy err 0.010882602073252201 tr(WHW.T) 780.7407836914062
16_q proxy err 0.001368974568322301 tr(WHW.T) 7193.3974609375
16_k proxy err 0.0008636934217065573 tr(WHW.T) 11630.361328125
16_o proxy err 0.009226889349520206 tr(WHW.T) 88.22785186767578
16_up proxy err 0.005173148587346077 tr(WHW.T) 1890.9385986328125
16_gate proxy err 0.0029592299833893776 tr(WHW.T) 3369.859130859375
16_down proxy err 0.007909773848950863 tr(WHW.T) 152.0294952392578
I0314 08:30:56.229286 2546926 finetune.py:45] layer 17_q initial loss 0.00022134650498628616
I0314 08:31:05.187019 2546926 finetune.py:45] layer 17_k initial loss 0.00024487587506882846
I0314 08:31:13.904122 2546926 finetune.py:45] layer 17_o initial loss 0.00038084786501713097
I0314 08:31:28.959987 2546926 finetune.py:45] layer 17_up initial loss 0.0005489821778610349
I0314 08:31:44.000945 2546926 finetune.py:45] layer 17_gate initial loss 0.0006785505102016032
I0314 08:31:47.337116 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 18 in 68.2565541267395s
I0314 08:31:50.356696 2547934 config.py:54] PyTorch version 2.1.1 available.
I0314 08:31:51.375696 2528979 quantize_finetune_llama.py:191] layer 19 gpu 3
I0314 08:31:51.445174 2547934 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:32:00.072802 2546926 finetune.py:45] layer 17_down initial loss 0.000985518330708146
I0314 08:32:00.141971 2547934 finetune.py:45] layer 18_v initial loss 0.0002037433732766658
17_v proxy err 0.010175209492444992 tr(WHW.T) 845.7654418945312
17_q proxy err 0.0013873324496671557 tr(WHW.T) 7163.2734375
17_k proxy err 0.0009509673691354692 tr(WHW.T) 10697.431640625
17_o proxy err 0.01007506251335144 tr(WHW.T) 58.14826965332031
17_up proxy err 0.005633306689560413 tr(WHW.T) 1921.07861328125
17_gate proxy err 0.003103044116869569 tr(WHW.T) 3571.31640625
17_down proxy err 0.00783925037831068 tr(WHW.T) 165.43495178222656
I0314 08:32:08.474674 2547934 finetune.py:45] layer 18_q initial loss 0.00023398826306220144
I0314 08:32:16.975882 2547934 finetune.py:45] layer 18_k initial loss 0.0002767925907392055
I0314 08:32:25.462815 2547934 finetune.py:45] layer 18_o initial loss 0.0004276045074220747
I0314 08:32:40.325904 2547934 finetune.py:45] layer 18_up initial loss 0.000631419534329325
I0314 08:32:56.987016 2547934 finetune.py:45] layer 18_gate initial loss 0.0007849046960473061
I0314 08:33:00.842732 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 19 in 69.05404210090637s
I0314 08:33:04.233355 2549148 config.py:54] PyTorch version 2.1.1 available.
I0314 08:33:05.256400 2528979 quantize_finetune_llama.py:191] layer 20 gpu 0
I0314 08:33:05.325104 2549148 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:33:18.039566 2549148 finetune.py:45] layer 19_v initial loss 0.00019248497847002
I0314 08:33:18.327484 2547934 finetune.py:45] layer 18_down initial loss 0.0011576212709769607
18_v proxy err 0.009383522905409336 tr(WHW.T) 1003.7705078125
18_q proxy err 0.0014289438258856535 tr(WHW.T) 7510.48046875
18_k proxy err 0.0010459802579134703 tr(WHW.T) 10462.6650390625
18_o proxy err 0.008875546045601368 tr(WHW.T) 69.96558380126953
18_up proxy err 0.005988034885376692 tr(WHW.T) 2023.183837890625
18_gate proxy err 0.003286142600700259 tr(WHW.T) 3783.076416015625
18_down proxy err 0.007898966781795025 tr(WHW.T) 198.52699279785156
I0314 08:33:27.777957 2549148 finetune.py:45] layer 19_q initial loss 0.0002226017531938851
I0314 08:33:36.980177 2549148 finetune.py:45] layer 19_k initial loss 0.0002488127793185413
I0314 08:33:45.819074 2549148 finetune.py:45] layer 19_o initial loss 0.0003822777362074703
I0314 08:34:02.250518 2549148 finetune.py:45] layer 19_up initial loss 0.0006090695387683809
I0314 08:34:18.831707 2549148 finetune.py:45] layer 19_gate initial loss 0.0007880802149884403
I0314 08:34:19.546421 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 20 in 72.94617629051208s
I0314 08:34:22.671404 2550272 config.py:54] PyTorch version 2.1.1 available.
I0314 08:34:23.726908 2528979 quantize_finetune_llama.py:191] layer 21 gpu 1
I0314 08:34:23.792983 2550272 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:34:33.036292 2550272 finetune.py:45] layer 20_v initial loss 0.00021787417063023895
I0314 08:34:37.295752 2549148 finetune.py:45] layer 19_down initial loss 0.0011956386733800173
19_v proxy err 0.009197059087455273 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.0015187053941190243 tr(WHW.T) 6944.4130859375
19_k proxy err 0.0010138159850612283 tr(WHW.T) 10548.4892578125
19_o proxy err 0.008766187354922295 tr(WHW.T) 62.291683197021484
19_up proxy err 0.006020944565534592 tr(WHW.T) 2149.330322265625
19_gate proxy err 0.003602025331929326 tr(WHW.T) 3687.5126953125
19_down proxy err 0.007679217029362917 tr(WHW.T) 222.93177795410156
I0314 08:34:42.585767 2550272 finetune.py:45] layer 20_q initial loss 0.00024542826577089727
I0314 08:34:51.810750 2550272 finetune.py:45] layer 20_k initial loss 0.0002697682357393205
I0314 08:35:00.797866 2550272 finetune.py:45] layer 20_o initial loss 0.00042812703759409487
I0314 08:35:15.699115 2550272 finetune.py:45] layer 20_up initial loss 0.0006940312450751662
I0314 08:35:30.709994 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 21 in 66.6346504688263s
I0314 08:35:30.749006 2550272 finetune.py:45] layer 20_gate initial loss 0.0009044946054928005
I0314 08:35:33.663790 2551483 config.py:54] PyTorch version 2.1.1 available.
I0314 08:35:34.645102 2528979 quantize_finetune_llama.py:191] layer 22 gpu 2
I0314 08:35:34.707023 2551483 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:35:43.356932 2551483 finetune.py:45] layer 21_v initial loss 0.00019858514133375138
I0314 08:35:47.141029 2550272 finetune.py:45] layer 20_down initial loss 0.001407114788889885
20_v proxy err 0.009445221163332462 tr(WHW.T) 990.5983276367188
20_q proxy err 0.001477844431065023 tr(WHW.T) 7150.947265625
20_k proxy err 0.001039715250954032 tr(WHW.T) 10386.2470703125
20_o proxy err 0.0064969430677592754 tr(WHW.T) 100.31707000732422
20_up proxy err 0.005899449810385704 tr(WHW.T) 2340.89453125
20_gate proxy err 0.003528906498104334 tr(WHW.T) 4024.62744140625
20_down proxy err 0.007638480514287949 tr(WHW.T) 274.8815002441406
I0314 08:35:52.044544 2551483 finetune.py:45] layer 21_q initial loss 0.00022232883202377707
I0314 08:36:00.494262 2551483 finetune.py:45] layer 21_k initial loss 0.000249321514274925
I0314 08:36:08.978330 2551483 finetune.py:45] layer 21_o initial loss 0.00037998834159225225
I0314 08:36:23.554310 2551483 finetune.py:45] layer 21_up initial loss 0.0006661259103566408
I0314 08:36:38.348929 2551483 finetune.py:45] layer 21_gate initial loss 0.0008908499730750918
I0314 08:36:41.762341 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 22 in 66.71818256378174s
I0314 08:36:44.903664 2552646 config.py:54] PyTorch version 2.1.1 available.
I0314 08:36:45.913125 2528979 quantize_finetune_llama.py:191] layer 23 gpu 3
I0314 08:36:45.985510 2552646 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:36:54.263627 2551483 finetune.py:45] layer 21_down initial loss 0.001395802479237318
I0314 08:36:54.538067 2552646 finetune.py:45] layer 22_v initial loss 0.0002368585701333359
21_v proxy err 0.009151172824203968 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.0016344168689101934 tr(WHW.T) 7064.314453125
21_k proxy err 0.0011765684466809034 tr(WHW.T) 9976.4658203125
21_o proxy err 0.006958073005080223 tr(WHW.T) 75.50972747802734
21_up proxy err 0.006202619057148695 tr(WHW.T) 2361.650390625
21_gate proxy err 0.0037696112412959337 tr(WHW.T) 4004.37646484375
21_down proxy err 0.007638134062290192 tr(WHW.T) 276.5857849121094
I0314 08:37:02.948519 2552646 finetune.py:45] layer 22_q initial loss 0.00027719547506421804
I0314 08:37:11.646944 2552646 finetune.py:45] layer 22_k initial loss 0.00031420279992744327
I0314 08:37:20.248447 2552646 finetune.py:45] layer 22_o initial loss 0.00048613877152092755
I0314 08:37:34.960735 2552646 finetune.py:45] layer 22_up initial loss 0.0008117161341942847
I0314 08:37:49.745062 2552646 finetune.py:45] layer 22_gate initial loss 0.0010676586534827948
I0314 08:37:52.960987 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 23 in 66.54134821891785s
I0314 08:37:56.078425 2553518 config.py:54] PyTorch version 2.1.1 available.
I0314 08:37:57.115376 2528979 quantize_finetune_llama.py:191] layer 24 gpu 0
I0314 08:37:57.189071 2553518 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:38:06.410607 2552646 finetune.py:45] layer 22_down initial loss 0.0016265392769128084
I0314 08:38:06.524452 2553518 finetune.py:45] layer 23_v initial loss 0.0002359168865950778
22_v proxy err 0.0087269376963377 tr(WHW.T) 1243.2529296875
22_q proxy err 0.0015507423086091876 tr(WHW.T) 7746.84765625
22_k proxy err 0.0011519944528117776 tr(WHW.T) 10603.2041015625
22_o proxy err 0.00587047915905714 tr(WHW.T) 114.30065155029297
22_up proxy err 0.006240156013518572 tr(WHW.T) 2474.510498046875
22_gate proxy err 0.0038354245480149984 tr(WHW.T) 4156.64013671875
22_down proxy err 0.007598021067678928 tr(WHW.T) 311.8800048828125
I0314 08:38:15.243528 2553518 finetune.py:45] layer 23_q initial loss 0.00026646131300367415
I0314 08:38:23.561677 2553518 finetune.py:45] layer 23_k initial loss 0.0002953631628770381
I0314 08:38:31.946094 2553518 finetune.py:45] layer 23_o initial loss 0.00044678273843601346
I0314 08:38:46.427652 2553518 finetune.py:45] layer 23_up initial loss 0.0008024421404115856
I0314 08:39:01.053346 2553518 finetune.py:45] layer 23_gate initial loss 0.001088645774871111
I0314 08:39:06.064200 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 24 in 67.59464120864868s
I0314 08:39:09.180451 2555261 config.py:54] PyTorch version 2.1.1 available.
I0314 08:39:10.251479 2528979 quantize_finetune_llama.py:191] layer 25 gpu 1
I0314 08:39:10.322418 2555261 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:39:17.282060 2553518 finetune.py:45] layer 23_down initial loss 0.0016640794929116964
23_v proxy err 0.007973327301442623 tr(WHW.T) 1486.037353515625
23_q proxy err 0.0017448145663365722 tr(WHW.T) 7346.60986328125
23_k proxy err 0.0013060357887297869 tr(WHW.T) 9982.2392578125
23_o proxy err 0.007122991606593132 tr(WHW.T) 85.13458251953125
23_up proxy err 0.006466536317020655 tr(WHW.T) 2533.51025390625
23_gate proxy err 0.004120949190109968 tr(WHW.T) 4097.51953125
23_down proxy err 0.0076349880546331406 tr(WHW.T) 321.33892822265625
I0314 08:39:19.845884 2555261 finetune.py:45] layer 24_v initial loss 0.0002767481200862676
I0314 08:39:28.342461 2555261 finetune.py:45] layer 24_q initial loss 0.0003201194340363145
I0314 08:39:37.030256 2555261 finetune.py:45] layer 24_k initial loss 0.00035474449396133423
I0314 08:39:45.760545 2555261 finetune.py:45] layer 24_o initial loss 0.0005489427130669355
I0314 08:40:00.759892 2555261 finetune.py:45] layer 24_up initial loss 0.000934201292693615
I0314 08:40:16.121967 2555261 finetune.py:45] layer 24_gate initial loss 0.0012517377035692334
I0314 08:40:17.713729 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 25 in 67.05223751068115s
I0314 08:40:20.709017 2556638 config.py:54] PyTorch version 2.1.1 available.
I0314 08:40:21.898454 2528979 quantize_finetune_llama.py:191] layer 26 gpu 2
I0314 08:40:21.970419 2556638 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:40:31.119082 2556638 finetune.py:45] layer 25_v initial loss 0.00029895472107455134
I0314 08:40:33.089405 2555261 finetune.py:45] layer 24_down initial loss 0.0018680802313610911
24_v proxy err 0.008465752936899662 tr(WHW.T) 1394.900634765625
24_q proxy err 0.0018164427019655704 tr(WHW.T) 7020.447265625
24_k proxy err 0.0012486648047342896 tr(WHW.T) 10323.43359375
24_o proxy err 0.005607868079096079 tr(WHW.T) 133.98797607421875
24_up proxy err 0.006560529116541147 tr(WHW.T) 2621.76513671875
24_gate proxy err 0.004157314542680979 tr(WHW.T) 4262.74853515625
24_down proxy err 0.007678835652768612 tr(WHW.T) 340.22412109375
I0314 08:40:40.226085 2556638 finetune.py:45] layer 25_q initial loss 0.00033633591374382377
I0314 08:40:48.842987 2556638 finetune.py:45] layer 25_k initial loss 0.0003729347372427583
I0314 08:40:57.414671 2556638 finetune.py:45] layer 25_o initial loss 0.0005216571153141558
I0314 08:41:12.458899 2556638 finetune.py:45] layer 25_up initial loss 0.0009525842033326626
I0314 08:41:27.816247 2556638 finetune.py:45] layer 25_gate initial loss 0.0013052003923803568
I0314 08:41:31.049054 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 26 in 68.6973009109497s
I0314 08:41:34.280295 2557386 config.py:54] PyTorch version 2.1.1 available.
I0314 08:41:35.327986 2528979 quantize_finetune_llama.py:191] layer 27 gpu 3
I0314 08:41:35.400000 2557386 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:41:45.284550 2556638 finetune.py:45] layer 25_down initial loss 0.0019810786470770836
I0314 08:41:45.680628 2557386 finetune.py:45] layer 26_v initial loss 0.00040370391798205674
25_v proxy err 0.007700431626290083 tr(WHW.T) 1707.664794921875
25_q proxy err 0.001956408144906163 tr(WHW.T) 7162.16357421875
25_k proxy err 0.0014760041376575828 tr(WHW.T) 9611.58984375
25_o proxy err 0.006729903165251017 tr(WHW.T) 83.535888671875
25_up proxy err 0.006533612962812185 tr(WHW.T) 2805.728515625
25_gate proxy err 0.004042202606797218 tr(WHW.T) 4666.4404296875
25_down proxy err 0.007637970149517059 tr(WHW.T) 373.460693359375
I0314 08:41:54.816375 2557386 finetune.py:45] layer 26_q initial loss 0.00045347961713559926
I0314 08:42:03.552052 2557386 finetune.py:45] layer 26_k initial loss 0.0005170686054043472
I0314 08:42:12.288197 2557386 finetune.py:45] layer 26_o initial loss 0.0007671079365536571
I0314 08:42:27.584759 2557386 finetune.py:45] layer 26_up initial loss 0.0012367329327389598
I0314 08:42:42.943875 2557386 finetune.py:45] layer 26_gate initial loss 0.001635042019188404
I0314 08:42:45.025327 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 27 in 69.30192923545837s
I0314 08:42:48.123090 2558141 config.py:54] PyTorch version 2.1.1 available.
I0314 08:42:49.188753 2528979 quantize_finetune_llama.py:191] layer 28 gpu 0
I0314 08:42:49.257199 2558141 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:43:00.469953 2558141 finetune.py:45] layer 27_v initial loss 0.0002969568595290184
I0314 08:43:02.020594 2557386 finetune.py:45] layer 26_down initial loss 0.0023697521537542343
26_v proxy err 0.007727355696260929 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.001839383621700108 tr(WHW.T) 7469.98291015625
26_k proxy err 0.0013212068006396294 tr(WHW.T) 10487.8740234375
26_o proxy err 0.004679668229073286 tr(WHW.T) 202.88172912597656
26_up proxy err 0.006138211116194725 tr(WHW.T) 3154.75146484375
26_gate proxy err 0.003762747859582305 tr(WHW.T) 5302.16455078125
26_down proxy err 0.007850130088627338 tr(WHW.T) 401.19390869140625
I0314 08:43:09.932474 2558141 finetune.py:45] layer 27_q initial loss 0.0003518376615829766
I0314 08:43:18.692947 2558141 finetune.py:45] layer 27_k initial loss 0.000397758383769542
I0314 08:43:27.423009 2558141 finetune.py:45] layer 27_o initial loss 0.0006047380156815052
I0314 08:43:42.005188 2558141 finetune.py:45] layer 27_up initial loss 0.0011394935427233577
I0314 08:43:56.783021 2558141 finetune.py:45] layer 27_gate initial loss 0.0015988409286364913
I0314 08:43:59.941474 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 28 in 69.19736528396606s
I0314 08:44:03.231177 2558898 config.py:54] PyTorch version 2.1.1 available.
I0314 08:44:04.316546 2528979 quantize_finetune_llama.py:191] layer 29 gpu 1
I0314 08:44:04.388808 2558898 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:44:16.531021 2558141 finetune.py:45] layer 27_down initial loss 0.002478473586961627
I0314 08:44:16.541359 2558898 finetune.py:45] layer 28_v initial loss 0.0003861910954583436
27_v proxy err 0.007006275933235884 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.0017634864198043942 tr(WHW.T) 7691.708984375
27_k proxy err 0.0012920096050947905 tr(WHW.T) 10618.70703125
27_o proxy err 0.006328393705189228 tr(WHW.T) 126.13690185546875
27_up proxy err 0.005605840589851141 tr(WHW.T) 3691.557861328125
27_gate proxy err 0.003553404239937663 tr(WHW.T) 5990.82568359375
27_down proxy err 0.008003761060535908 tr(WHW.T) 466.9318542480469
I0314 08:44:27.638070 2558898 finetune.py:45] layer 28_q initial loss 0.0004446009697858244
I0314 08:44:39.088311 2558898 finetune.py:45] layer 28_k initial loss 0.000503722345456481
I0314 08:44:50.130063 2558898 finetune.py:45] layer 28_o initial loss 0.0007745252805761993
I0314 08:45:08.615141 2558898 finetune.py:45] layer 28_up initial loss 0.001414809376001358
I0314 08:45:21.279772 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 29 in 76.54679107666016s
I0314 08:45:24.818569 2559738 config.py:54] PyTorch version 2.1.1 available.
I0314 08:45:25.882462 2528979 quantize_finetune_llama.py:191] layer 30 gpu 2
I0314 08:45:25.956720 2559738 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 08:45:26.259027 2558898 finetune.py:45] layer 28_gate initial loss 0.001969726989045739
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:45:36.447366 2559738 finetune.py:45] layer 29_v initial loss 0.00037044871714897454
I0314 08:45:46.059524 2558898 finetune.py:45] layer 28_down initial loss 0.0031213548500090837
28_v proxy err 0.006565732415765524 tr(WHW.T) 2018.944091796875
28_q proxy err 0.0018251327564939857 tr(WHW.T) 7651.126953125
28_k proxy err 0.0013388138031587005 tr(WHW.T) 10544.8251953125
28_o proxy err 0.005240078084170818 tr(WHW.T) 194.8240966796875
28_up proxy err 0.00470749381929636 tr(WHW.T) 4661.2666015625
28_gate proxy err 0.0034277650993317366 tr(WHW.T) 6547.48193359375
28_down proxy err 0.008077594451606274 tr(WHW.T) 603.8403930664062
I0314 08:45:47.722619 2559738 finetune.py:45] layer 29_q initial loss 0.0004247759061399847
I0314 08:45:57.153442 2559738 finetune.py:45] layer 29_k initial loss 0.0004835424479097128
I0314 08:46:06.206057 2559738 finetune.py:45] layer 29_o initial loss 0.0007266452303156257
I0314 08:46:21.981625 2559738 finetune.py:45] layer 29_up initial loss 0.0014830823056399822
I0314 08:46:37.393196 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 30 in 71.08224153518677s
I0314 08:46:37.414294 2559738 finetune.py:45] layer 29_gate initial loss 0.0021465939935296774
I0314 08:46:40.449932 2560616 config.py:54] PyTorch version 2.1.1 available.
I0314 08:46:41.400801 2528979 quantize_finetune_llama.py:191] layer 31 gpu 3
I0314 08:46:41.456189 2560616 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:46:50.385049 2560616 finetune.py:45] layer 30_v initial loss 0.0003715093480423093
I0314 08:46:54.789463 2559738 finetune.py:45] layer 29_down initial loss 0.0036702770739793777
29_v proxy err 0.006969184149056673 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.0018066606717184186 tr(WHW.T) 7227.0009765625
29_k proxy err 0.0012548923259600997 tr(WHW.T) 10558.609375
29_o proxy err 0.004431052133440971 tr(WHW.T) 207.9054412841797
29_up proxy err 0.003773268312215805 tr(WHW.T) 6070.0498046875
29_gate proxy err 0.0031662434339523315 tr(WHW.T) 7369.6142578125
29_down proxy err 0.008278679102659225 tr(WHW.T) 782.2448120117188
I0314 08:46:59.527643 2560616 finetune.py:45] layer 30_q initial loss 0.00045703555224463344
I0314 08:47:08.345519 2560616 finetune.py:45] layer 30_k initial loss 0.0005531007773242891
I0314 08:47:17.223353 2560616 finetune.py:45] layer 30_o initial loss 0.0008813762106001377
I0314 08:47:32.620951 2560616 finetune.py:45] layer 30_up initial loss 0.0021394211798906326
I0314 08:47:47.908935 2560616 finetune.py:45] layer 30_gate initial loss 0.003041123505681753
I0314 08:47:50.831146 2528979 quantize_finetune_llama.py:218] computed original embedding for layer 31 in 68.99571442604065s
I0314 08:47:53.905234 2561477 config.py:54] PyTorch version 2.1.1 available.
I0314 08:47:55.017270 2561477 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:48:03.419929 2561477 finetune.py:45] layer 31_v initial loss 0.0006532638799399137
I0314 08:48:04.404232 2560616 finetune.py:45] layer 30_down initial loss 0.0071959360502660275
30_v proxy err 0.005729150027036667 tr(WHW.T) 2261.489501953125
30_q proxy err 0.0017133348155766726 tr(WHW.T) 7815.9453125
30_k proxy err 0.0012913909740746021 tr(WHW.T) 10521.625
30_o proxy err 0.004654602147638798 tr(WHW.T) 251.96908569335938
30_up proxy err 0.002348121954128146 tr(WHW.T) 10016.376953125
30_gate proxy err 0.0021720086224377155 tr(WHW.T) 11001.119140625
30_down proxy err 0.004773431923240423 tr(WHW.T) 3582.617919921875
I0314 08:48:11.899999 2561477 finetune.py:45] layer 31_q initial loss 0.0009262250387109816
I0314 08:48:20.507250 2561477 finetune.py:45] layer 31_k initial loss 0.0011185196926817298
I0314 08:48:29.114838 2561477 finetune.py:45] layer 31_o initial loss 0.0016594537300989032
I0314 08:48:44.107373 2561477 finetune.py:45] layer 31_up initial loss 0.004786014556884766
I0314 08:48:59.129240 2561477 finetune.py:45] layer 31_gate initial loss 0.006508237682282925
I0314 08:49:15.296340 2561477 finetune.py:45] layer 31_down initial loss 0.02120111510157585
31_v proxy err 0.0067534674890339375 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.0013502977089956403 tr(WHW.T) 6858.09130859375
31_k proxy err 0.0009344267891719937 tr(WHW.T) 10233.677734375
31_o proxy err 0.003522933926433325 tr(WHW.T) 457.7950744628906
31_up proxy err 0.0013615461066365242 tr(WHW.T) 14563.890625
31_gate proxy err 0.0013572153402492404 tr(WHW.T) 14836.2939453125
31_down proxy err 0.0033575957641005516 tr(WHW.T) 17873.55859375
