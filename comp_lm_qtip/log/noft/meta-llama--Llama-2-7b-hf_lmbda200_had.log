I0314 08:51:15.916481 2563616 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.48it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.75it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 10.05it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.64it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.66it/s]
I0314 08:51:17.945539 2563616 quantize_finetune_llama.py:142] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.43it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:21,  1.41it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:24,  1.20it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:03<00:22,  1.26it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:20,  1.31it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:20,  1.29it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:05<00:18,  1.32it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:06<00:18,  1.32it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:17,  1.32it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:07<00:16,  1.33it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:08<00:15,  1.32it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:09<00:14,  1.34it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:14,  1.35it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:10<00:13,  1.36it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:11<00:12,  1.35it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:12<00:11,  1.34it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:12<00:11,  1.33it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:13<00:10,  1.35it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:14<00:09,  1.35it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:15<00:08,  1.34it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:15<00:08,  1.34it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:16<00:07,  1.34it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:17<00:06,  1.35it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:05,  1.37it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:18<00:05,  1.37it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:19<00:04,  1.39it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:20<00:03,  1.38it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:20<00:02,  1.39it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:21<00:02,  1.42it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:22<00:01,  1.43it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:22<00:00,  1.43it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.42it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:23<00:00,  1.36it/s]
I0314 08:51:51.608969 2563616 quantize_finetune_llama.py:167] loaded compression model
I0314 08:52:05.863217 2563616 quantize_finetune_llama.py:171] loaded dataset and devset
I0314 08:52:11.550389 2563616 quantize_finetune_llama.py:191] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:53:23.866100 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 0 in 72.20255398750305s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 08:53:51.045650 2565233 config.py:54] PyTorch version 2.1.1 available.
I0314 08:53:52.033497 2563616 quantize_finetune_llama.py:191] layer 1 gpu 1
I0314 08:53:52.089092 2565233 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:54:01.257610 2565233 finetune.py:45] layer 0_v initial loss 2.7306180072628194e-06
I0314 08:54:09.955847 2565233 finetune.py:45] layer 0_q initial loss 2.7531966679816833e-06
I0314 08:54:18.651312 2565233 finetune.py:45] layer 0_k initial loss 2.7773612600867637e-06
I0314 08:54:27.458564 2565233 finetune.py:45] layer 0_o initial loss 3.249755536671728e-06
I0314 08:54:42.483027 2565233 finetune.py:45] layer 0_up initial loss 3.2708478556742193e-06
I0314 08:54:57.704893 2565233 finetune.py:45] layer 0_gate initial loss 3.2888872283365345e-06
I0314 08:55:01.764120 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 1 in 69.60691428184509s
I0314 08:55:12.486082 2566194 config.py:54] PyTorch version 2.1.1 available.
I0314 08:55:13.500143 2563616 quantize_finetune_llama.py:191] layer 2 gpu 2
I0314 08:55:13.565277 2566194 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:55:20.235602 2565233 finetune.py:45] layer 0_down initial loss 3.559896640581428e-06
0_v proxy err 0.020335961133241653 tr(WHW.T) 4.225186347961426
0_q proxy err 6.531924736918882e-05 tr(WHW.T) 2710.363037109375
0_k proxy err 8.225283090723678e-05 tr(WHW.T) 1698.7349853515625
0_o proxy err 0.0013704034499824047 tr(WHW.T) 0.9668058156967163
0_up proxy err 0.0024454272352159023 tr(WHW.T) 43.27138900756836
0_gate proxy err 0.0016858788440003991 tr(WHW.T) 63.47430419921875
0_down proxy err 0.0016830284148454666 tr(WHW.T) 0.656814694404602
I0314 08:55:22.561183 2566194 finetune.py:45] layer 1_v initial loss 4.017358151031658e-05
I0314 08:55:31.347993 2566194 finetune.py:45] layer 1_q initial loss 4.086669650860131e-05
I0314 08:55:40.046041 2566194 finetune.py:45] layer 1_k initial loss 4.173100023763254e-05
I0314 08:55:48.786837 2566194 finetune.py:45] layer 1_o initial loss 4.303167952457443e-05
I0314 08:56:03.699479 2566194 finetune.py:45] layer 1_up initial loss 3.5218319681007415e-05
I0314 08:56:18.810179 2566194 finetune.py:45] layer 1_gate initial loss 4.956207703799009e-05
I0314 08:56:26.192919 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 2 in 72.46734356880188s
I0314 08:56:36.956894 2567060 config.py:54] PyTorch version 2.1.1 available.
I0314 08:56:37.557271 2566194 finetune.py:45] layer 1_down initial loss 0.0008173500536940992
I0314 08:56:37.961354 2563616 quantize_finetune_llama.py:191] layer 3 gpu 3
I0314 08:56:38.016392 2567060 data_utils.py:336] using 256 training seqs, 128 validation seqs
1_v proxy err 0.029082903638482094 tr(WHW.T) 16.465883255004883
1_q proxy err 0.00014562354772351682 tr(WHW.T) 4778.43994140625
1_k proxy err 0.00014088087482377887 tr(WHW.T) 4995.39208984375
1_o proxy err 0.004764126613736153 tr(WHW.T) 1.1115814447402954
1_up proxy err 0.0025798394344747066 tr(WHW.T) 109.66383361816406
1_gate proxy err 0.001315043424256146 tr(WHW.T) 221.3038787841797
1_down proxy err 0.001526519889011979 tr(WHW.T) 2041.4736328125
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:56:46.872270 2567060 finetune.py:45] layer 2_v initial loss 4.7385419748025015e-06
I0314 08:56:55.597231 2567060 finetune.py:45] layer 2_q initial loss 5.015080660086824e-06
I0314 08:57:04.406273 2567060 finetune.py:45] layer 2_k initial loss 5.244116891844897e-06
I0314 08:57:13.287368 2567060 finetune.py:45] layer 2_o initial loss 7.071262189128902e-06
I0314 08:57:28.261820 2567060 finetune.py:45] layer 2_up initial loss 8.255501597886905e-06
I0314 08:57:43.591267 2567060 finetune.py:45] layer 2_gate initial loss 9.137460438068956e-06
I0314 08:57:51.661920 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 3 in 73.53719139099121s
I0314 08:58:00.019543 2568349 config.py:54] PyTorch version 2.1.1 available.
I0314 08:58:01.074014 2563616 quantize_finetune_llama.py:191] layer 4 gpu 0
I0314 08:58:01.140661 2568349 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 08:58:01.458420 2567060 finetune.py:45] layer 2_down initial loss 1.1604499377426691e-05
2_v proxy err 0.0075363414362072945 tr(WHW.T) 136.67332458496094
2_q proxy err 0.00017961391131393611 tr(WHW.T) 7752.85205078125
2_k proxy err 0.00014026384451426566 tr(WHW.T) 10205.837890625
2_o proxy err 0.003976122010499239 tr(WHW.T) 1.4603197574615479
2_up proxy err 0.0030148399528115988 tr(WHW.T) 193.43603515625
2_gate proxy err 0.0019495966844260693 tr(WHW.T) 306.6622619628906
2_down proxy err 0.0034752620849758387 tr(WHW.T) 3.010739803314209
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:58:10.696014 2568349 finetune.py:45] layer 3_v initial loss 7.21520109436824e-06
I0314 08:58:19.250451 2568349 finetune.py:45] layer 3_q initial loss 7.746019946353044e-06
I0314 08:58:28.014731 2568349 finetune.py:45] layer 3_k initial loss 8.3098757386324e-06
I0314 08:58:36.629626 2568349 finetune.py:45] layer 3_o initial loss 1.228839664690895e-05
I0314 08:58:51.803583 2568349 finetune.py:45] layer 3_up initial loss 1.5146337318583392e-05
I0314 08:59:06.954813 2568349 finetune.py:45] layer 3_gate initial loss 1.720851651043631e-05
I0314 08:59:12.737473 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 4 in 70.29360485076904s
I0314 08:59:15.824257 2569240 config.py:54] PyTorch version 2.1.1 available.
I0314 08:59:16.897364 2563616 quantize_finetune_llama.py:191] layer 5 gpu 1
I0314 08:59:16.962003 2569240 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 08:59:23.504432 2568349 finetune.py:45] layer 3_down initial loss 2.244302595499903e-05
3_v proxy err 0.006981702521443367 tr(WHW.T) 284.77557373046875
3_q proxy err 0.0003465084300842136 tr(WHW.T) 7217.63720703125
3_k proxy err 0.00025552307488396764 tr(WHW.T) 10074.73828125
3_o proxy err 0.0039675128646194935 tr(WHW.T) 3.3527450561523438
3_up proxy err 0.003441855311393738 tr(WHW.T) 284.7950744628906
3_gate proxy err 0.00210410519503057 tr(WHW.T) 478.13714599609375
3_down proxy err 0.0035533590707927942 tr(WHW.T) 6.133229732513428
I0314 08:59:26.159453 2569240 finetune.py:45] layer 4_v initial loss 1.289579267904628e-05
I0314 08:59:34.949753 2569240 finetune.py:45] layer 4_q initial loss 1.3713992302655242e-05
I0314 08:59:43.731436 2569240 finetune.py:45] layer 4_k initial loss 1.448548209737055e-05
I0314 08:59:52.734326 2569240 finetune.py:45] layer 4_o initial loss 2.002880137297325e-05
I0314 09:00:08.085350 2569240 finetune.py:45] layer 4_up initial loss 2.5433062546653673e-05
I0314 09:00:23.524808 2569240 finetune.py:45] layer 4_gate initial loss 2.8929209292982705e-05
I0314 09:00:26.284590 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 5 in 69.00194931030273s
I0314 09:00:29.363596 2569906 config.py:54] PyTorch version 2.1.1 available.
I0314 09:00:30.362768 2563616 quantize_finetune_llama.py:191] layer 6 gpu 2
I0314 09:00:30.433685 2569906 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:00:39.425271 2569906 finetune.py:45] layer 5_v initial loss 1.9429338863119483e-05
I0314 09:00:40.365980 2569240 finetune.py:45] layer 4_down initial loss 3.872142769978382e-05
4_v proxy err 0.0067634801380336285 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0003367489262018353 tr(WHW.T) 6914.9892578125
4_k proxy err 0.00023050281743053347 tr(WHW.T) 10415.33203125
4_o proxy err 0.003956028260290623 tr(WHW.T) 5.139806270599365
4_up proxy err 0.003391385544091463 tr(WHW.T) 397.6960144042969
4_gate proxy err 0.0017071082256734371 tr(WHW.T) 821.1856689453125
4_down proxy err 0.0035619218833744526 tr(WHW.T) 11.562739372253418
I0314 09:00:48.131909 2569906 finetune.py:45] layer 5_q initial loss 2.0601137293851934e-05
I0314 09:00:56.850107 2569906 finetune.py:45] layer 5_k initial loss 2.1672287402907386e-05
I0314 09:01:05.564257 2569906 finetune.py:45] layer 5_o initial loss 3.292204564786516e-05
I0314 09:01:20.576020 2569906 finetune.py:45] layer 5_up initial loss 4.1576124203857034e-05
I0314 09:01:35.740386 2569906 finetune.py:45] layer 5_gate initial loss 4.69185397378169e-05
I0314 09:01:40.264912 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 6 in 69.45106172561646s
I0314 09:01:43.472226 2570216 config.py:54] PyTorch version 2.1.1 available.
I0314 09:01:44.526989 2563616 quantize_finetune_llama.py:191] layer 7 gpu 3
I0314 09:01:44.597490 2570216 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:01:52.185238 2569906 finetune.py:45] layer 5_down initial loss 6.0782269429182634e-05
5_v proxy err 0.006711235735565424 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0003721953253261745 tr(WHW.T) 6770.97509765625
5_k proxy err 0.00023632105148863047 tr(WHW.T) 10841.955078125
5_o proxy err 0.00562320277094841 tr(WHW.T) 7.947142601013184
5_up proxy err 0.003329518251121044 tr(WHW.T) 506.6408386230469
5_gate proxy err 0.0015893668169155717 tr(WHW.T) 1104.867919921875
5_down proxy err 0.003751655574887991 tr(WHW.T) 15.6494779586792
I0314 09:01:53.443105 2570216 finetune.py:45] layer 6_v initial loss 2.4611741537228227e-05
I0314 09:02:02.122751 2570216 finetune.py:45] layer 6_q initial loss 2.674692041182425e-05
I0314 09:02:10.906315 2570216 finetune.py:45] layer 6_k initial loss 2.859842606994789e-05
I0314 09:02:19.398204 2570216 finetune.py:45] layer 6_o initial loss 4.26994010922499e-05
I0314 09:02:34.715088 2570216 finetune.py:45] layer 6_up initial loss 5.615755799226463e-05
I0314 09:02:50.426293 2570216 finetune.py:45] layer 6_gate initial loss 6.374344957293943e-05
I0314 09:02:54.072154 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 7 in 69.1274721622467s
I0314 09:02:57.184514 2570583 config.py:54] PyTorch version 2.1.1 available.
I0314 09:02:58.592246 2563616 quantize_finetune_llama.py:191] layer 8 gpu 0
I0314 09:02:58.657496 2570583 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:03:07.986149 2570216 finetune.py:45] layer 6_down initial loss 8.49809730425477e-05
I0314 09:03:08.340555 2570583 finetune.py:45] layer 7_v initial loss 3.12183219648432e-05
6_v proxy err 0.006545237265527248 tr(WHW.T) 443.5464782714844
6_q proxy err 0.00047486164839938283 tr(WHW.T) 7576.53857421875
6_k proxy err 0.00034840096486732364 tr(WHW.T) 10409.4033203125
6_o proxy err 0.005164233967661858 tr(WHW.T) 11.564380645751953
6_up proxy err 0.0033353432081639767 tr(WHW.T) 617.2608642578125
6_gate proxy err 0.0013893662253394723 tr(WHW.T) 1554.7271728515625
6_down proxy err 0.003872684668749571 tr(WHW.T) 22.988168716430664
I0314 09:03:16.919294 2570583 finetune.py:45] layer 7_q initial loss 3.458611172391102e-05
I0314 09:03:25.557633 2570583 finetune.py:45] layer 7_k initial loss 3.710511009558104e-05
I0314 09:03:34.296999 2570583 finetune.py:45] layer 7_o initial loss 5.6749551731627434e-05
I0314 09:03:49.595343 2570583 finetune.py:45] layer 7_up initial loss 7.531978917540982e-05
I0314 09:04:04.847514 2570583 finetune.py:45] layer 7_gate initial loss 8.576957770856097e-05
I0314 09:04:10.324565 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 8 in 70.6308331489563s
I0314 09:04:13.596216 2570941 config.py:54] PyTorch version 2.1.1 available.
I0314 09:04:14.787251 2563616 quantize_finetune_llama.py:191] layer 9 gpu 1
I0314 09:04:14.857658 2570941 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:04:21.748250 2570583 finetune.py:45] layer 7_down initial loss 0.00011421203089412302
7_v proxy err 0.0064276643097400665 tr(WHW.T) 489.9357604980469
7_q proxy err 0.0005032074404880404 tr(WHW.T) 7672.17919921875
7_k proxy err 0.00038043572567403316 tr(WHW.T) 10198.3701171875
7_o proxy err 0.005902143660932779 tr(WHW.T) 15.11335563659668
7_up proxy err 0.003288933774456382 tr(WHW.T) 735.8538818359375
7_gate proxy err 0.001353029627352953 tr(WHW.T) 1876.0390625
7_down proxy err 0.003922620322555304 tr(WHW.T) 30.58672523498535
I0314 09:04:24.072214 2570941 finetune.py:45] layer 8_v initial loss 4.9887585191754624e-05
I0314 09:04:33.240767 2570941 finetune.py:45] layer 8_q initial loss 5.468666859087534e-05
I0314 09:04:42.301617 2570941 finetune.py:45] layer 8_k initial loss 5.8121113397646695e-05
I0314 09:04:51.401012 2570941 finetune.py:45] layer 8_o initial loss 8.825758413877338e-05
I0314 09:05:07.029493 2570941 finetune.py:45] layer 8_up initial loss 0.00011059483222197741
I0314 09:05:22.527032 2570941 finetune.py:45] layer 8_gate initial loss 0.00012411097122821957
I0314 09:05:24.251413 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 9 in 68.99044609069824s
I0314 09:05:27.269580 2571307 config.py:54] PyTorch version 2.1.1 available.
I0314 09:05:28.314320 2563616 quantize_finetune_llama.py:191] layer 10 gpu 2
I0314 09:05:28.385462 2571307 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:05:37.238261 2571307 finetune.py:45] layer 9_v initial loss 5.326627069734968e-05
I0314 09:05:39.226994 2570941 finetune.py:45] layer 8_down initial loss 0.00015868863556534052
8_v proxy err 0.006122807506471872 tr(WHW.T) 530.9967041015625
8_q proxy err 0.0005404662224464118 tr(WHW.T) 7228.1201171875
8_k proxy err 0.0003735731588676572 tr(WHW.T) 10639.1015625
8_o proxy err 0.006717538461089134 tr(WHW.T) 20.092191696166992
8_up proxy err 0.003035973059013486 tr(WHW.T) 866.312744140625
8_gate proxy err 0.0013873245334252715 tr(WHW.T) 1970.857177734375
8_down proxy err 0.003907405771315098 tr(WHW.T) 37.177734375
I0314 09:05:45.875962 2571307 finetune.py:45] layer 9_q initial loss 5.865951970918104e-05
I0314 09:05:54.577794 2571307 finetune.py:45] layer 9_k initial loss 6.384342850651592e-05
I0314 09:06:03.197776 2571307 finetune.py:45] layer 9_o initial loss 0.0001010866544675082
I0314 09:06:18.509415 2571307 finetune.py:45] layer 9_up initial loss 0.0001268736377824098
I0314 09:06:33.537194 2571307 finetune.py:45] layer 9_gate initial loss 0.00014338074834086
I0314 09:06:37.194051 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 10 in 68.45655846595764s
I0314 09:06:40.312741 2571678 config.py:54] PyTorch version 2.1.1 available.
I0314 09:06:41.309479 2563616 quantize_finetune_llama.py:191] layer 11 gpu 3
I0314 09:06:41.376641 2571678 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:06:50.101969 2571307 finetune.py:45] layer 9_down initial loss 0.00018330925377085805
I0314 09:06:50.467775 2571678 finetune.py:45] layer 10_v initial loss 7.21235919627361e-05
9_v proxy err 0.005705859512090683 tr(WHW.T) 565.0663452148438
9_q proxy err 0.0005662118783220649 tr(WHW.T) 6970.3359375
9_k proxy err 0.00036418234230950475 tr(WHW.T) 10987.3515625
9_o proxy err 0.006698610261082649 tr(WHW.T) 25.610172271728516
9_up proxy err 0.0029301070608198643 tr(WHW.T) 970.8984375
9_gate proxy err 0.0013791123637929559 tr(WHW.T) 2132.69384765625
9_down proxy err 0.003918312955647707 tr(WHW.T) 42.99482727050781
I0314 09:06:59.049056 2571678 finetune.py:45] layer 10_q initial loss 7.802752952557057e-05
I0314 09:07:07.747294 2571678 finetune.py:45] layer 10_k initial loss 8.498688839608803e-05
I0314 09:07:16.512383 2571678 finetune.py:45] layer 10_o initial loss 0.0001367055665468797
I0314 09:07:31.658193 2571678 finetune.py:45] layer 10_up initial loss 0.00016602386313024908
I0314 09:07:46.858222 2571678 finetune.py:45] layer 10_gate initial loss 0.00018574234854895622
I0314 09:07:49.928858 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 11 in 68.19289302825928s
I0314 09:07:53.070965 2572045 config.py:54] PyTorch version 2.1.1 available.
I0314 09:07:54.111007 2563616 quantize_finetune_llama.py:191] layer 12 gpu 0
I0314 09:07:54.176375 2572045 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:08:03.729812 2572045 finetune.py:45] layer 11_v initial loss 8.149025234160945e-05
I0314 09:08:03.773920 2571678 finetune.py:45] layer 10_down initial loss 0.00023214744578581303
10_v proxy err 0.005725111346691847 tr(WHW.T) 578.807373046875
10_q proxy err 0.000582885171752423 tr(WHW.T) 6915.87109375
10_k proxy err 0.00037588051054626703 tr(WHW.T) 10996.2431640625
10_o proxy err 0.006855970714241266 tr(WHW.T) 35.184165954589844
10_up proxy err 0.002769564976915717 tr(WHW.T) 1080.198486328125
10_gate proxy err 0.0013629748718813062 tr(WHW.T) 2260.88330078125
10_down proxy err 0.003738505532965064 tr(WHW.T) 52.33584976196289
I0314 09:08:12.207270 2572045 finetune.py:45] layer 11_q initial loss 8.730393165023997e-05
I0314 09:08:20.747369 2572045 finetune.py:45] layer 11_k initial loss 9.278478682972491e-05
I0314 09:08:29.346621 2572045 finetune.py:45] layer 11_o initial loss 0.00014564814046025276
I0314 09:08:44.174491 2572045 finetune.py:45] layer 11_up initial loss 0.00017878897779155523
I0314 09:08:58.938604 2572045 finetune.py:45] layer 11_gate initial loss 0.00020096026128157973
I0314 09:09:04.397528 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 12 in 69.17160677909851s
I0314 09:09:07.562619 2572397 config.py:54] PyTorch version 2.1.1 available.
I0314 09:09:08.584631 2563616 quantize_finetune_llama.py:191] layer 13 gpu 1
I0314 09:09:08.656585 2572397 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:09:15.132568 2572045 finetune.py:45] layer 11_down initial loss 0.00025210247258655727
11_v proxy err 0.00545865623280406 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0006626335089094937 tr(WHW.T) 7027.10986328125
11_k proxy err 0.0004489970742724836 tr(WHW.T) 10511.23046875
11_o proxy err 0.006940208375453949 tr(WHW.T) 36.654052734375
11_up proxy err 0.0028217679355293512 tr(WHW.T) 1139.6044921875
11_gate proxy err 0.001377331675030291 tr(WHW.T) 2392.716552734375
11_down proxy err 0.003835865296423435 tr(WHW.T) 56.13530731201172
I0314 09:09:17.701595 2572397 finetune.py:45] layer 12_v initial loss 8.19190900074318e-05
I0314 09:09:26.449368 2572397 finetune.py:45] layer 12_q initial loss 8.962726133177057e-05
I0314 09:09:35.240113 2572397 finetune.py:45] layer 12_k initial loss 9.794929792406037e-05
I0314 09:09:44.075277 2572397 finetune.py:45] layer 12_o initial loss 0.00015719477960374206
I0314 09:09:59.194431 2572397 finetune.py:45] layer 12_up initial loss 0.00019378856814000756
I0314 09:10:14.358421 2572397 finetune.py:45] layer 12_gate initial loss 0.00021968697546981275
I0314 09:10:16.826949 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 13 in 67.81777882575989s
I0314 09:10:19.867866 2572764 config.py:54] PyTorch version 2.1.1 available.
I0314 09:10:20.972803 2563616 quantize_finetune_llama.py:191] layer 14 gpu 2
I0314 09:10:21.049510 2572764 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:10:29.846235 2572764 finetune.py:45] layer 13_v initial loss 8.07776814326644e-05
I0314 09:10:31.412850 2572397 finetune.py:45] layer 12_down initial loss 0.000278243503998965
12_v proxy err 0.005577338393777609 tr(WHW.T) 703.318603515625
12_q proxy err 0.0006620174972340465 tr(WHW.T) 7045.6435546875
12_k proxy err 0.00043506204383447766 tr(WHW.T) 10893.65625
12_o proxy err 0.007050048094242811 tr(WHW.T) 39.29071044921875
12_up proxy err 0.0027942389715462923 tr(WHW.T) 1228.298583984375
12_gate proxy err 0.001470275572501123 tr(WHW.T) 2381.994873046875
12_down proxy err 0.0038305092602968216 tr(WHW.T) 64.17745208740234
I0314 09:10:38.375291 2572764 finetune.py:45] layer 13_q initial loss 8.762147626839578e-05
I0314 09:10:46.785704 2572764 finetune.py:45] layer 13_k initial loss 9.382433199789375e-05
I0314 09:10:55.280614 2572764 finetune.py:45] layer 13_o initial loss 0.0001540262601338327
I0314 09:11:10.023213 2572764 finetune.py:45] layer 13_up initial loss 0.0001982686808332801
I0314 09:11:24.904222 2572764 finetune.py:45] layer 13_gate initial loss 0.00022976158652454615
I0314 09:11:29.321052 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 14 in 67.96505975723267s
I0314 09:11:32.555699 2573131 config.py:54] PyTorch version 2.1.1 available.
I0314 09:11:33.598837 2563616 quantize_finetune_llama.py:191] layer 15 gpu 3
I0314 09:11:33.669691 2573131 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:11:41.289792 2572764 finetune.py:45] layer 13_down initial loss 0.0003026561171282083
13_v proxy err 0.005513984709978104 tr(WHW.T) 714.5677490234375
13_q proxy err 0.0006640554638579488 tr(WHW.T) 6956.03564453125
13_k proxy err 0.00045249564573168755 tr(WHW.T) 10426.6318359375
13_o proxy err 0.00629716319963336 tr(WHW.T) 45.8377571105957
13_up proxy err 0.00269565568305552 tr(WHW.T) 1367.6221923828125
13_gate proxy err 0.0014366357354447246 tr(WHW.T) 2601.504638671875
13_down proxy err 0.0038469366263598204 tr(WHW.T) 79.3589096069336
I0314 09:11:42.540884 2573131 finetune.py:45] layer 14_v initial loss 0.0001015184898278676
I0314 09:11:51.090612 2573131 finetune.py:45] layer 14_q initial loss 0.00011040941899409518
I0314 09:11:59.711750 2573131 finetune.py:45] layer 14_k initial loss 0.00011853259638883173
I0314 09:12:08.359226 2573131 finetune.py:45] layer 14_o initial loss 0.0001941401424119249
I0314 09:12:23.332406 2573131 finetune.py:45] layer 14_up initial loss 0.0002436586219118908
I0314 09:12:38.348065 2573131 finetune.py:45] layer 14_gate initial loss 0.0002804153482429683
I0314 09:12:41.642918 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 15 in 67.64644503593445s
I0314 09:12:44.789620 2573498 config.py:54] PyTorch version 2.1.1 available.
I0314 09:12:45.793639 2563616 quantize_finetune_llama.py:191] layer 16 gpu 0
I0314 09:12:45.860807 2573498 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:12:54.938183 2573498 finetune.py:45] layer 15_v initial loss 9.978840535040945e-05
I0314 09:12:55.020686 2573131 finetune.py:45] layer 14_down initial loss 0.0003652363084256649
14_v proxy err 0.005840526893734932 tr(WHW.T) 706.1612548828125
14_q proxy err 0.000683206832036376 tr(WHW.T) 7077.06103515625
14_k proxy err 0.0004370531241875142 tr(WHW.T) 11295.16796875
14_o proxy err 0.0069384025409817696 tr(WHW.T) 50.921180725097656
14_up proxy err 0.002725807949900627 tr(WHW.T) 1464.7159423828125
14_gate proxy err 0.0015093242982402444 tr(WHW.T) 2682.584716796875
14_down proxy err 0.003930569160729647 tr(WHW.T) 90.28684997558594
I0314 09:13:03.257939 2573498 finetune.py:45] layer 15_q initial loss 0.00010963016393361613
I0314 09:13:11.637325 2573498 finetune.py:45] layer 15_k initial loss 0.00011681063188007101
I0314 09:13:20.038275 2573498 finetune.py:45] layer 15_o initial loss 0.0001926589902723208
I0314 09:13:35.030596 2573498 finetune.py:45] layer 15_up initial loss 0.0002527891774661839
I0314 09:13:50.394226 2573498 finetune.py:45] layer 15_gate initial loss 0.0002986211620736867
I0314 09:13:56.538169 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 16 in 69.61924600601196s
I0314 09:13:59.700480 2573868 config.py:54] PyTorch version 2.1.1 available.
I0314 09:14:00.748149 2563616 quantize_finetune_llama.py:191] layer 17 gpu 1
I0314 09:14:00.808790 2573868 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:14:06.983036 2573498 finetune.py:45] layer 15_down initial loss 0.0004057296318933368
15_v proxy err 0.005367799196392298 tr(WHW.T) 762.7275390625
15_q proxy err 0.0006576422019861639 tr(WHW.T) 7252.0009765625
15_k proxy err 0.00044049372081644833 tr(WHW.T) 11072.3974609375
15_o proxy err 0.005908118560910225 tr(WHW.T) 59.61664962768555
15_up proxy err 0.002655855379998684 tr(WHW.T) 1641.0228271484375
15_gate proxy err 0.0015231641009449959 tr(WHW.T) 2905.140380859375
15_down proxy err 0.003927848767489195 tr(WHW.T) 114.09001922607422
I0314 09:14:10.068103 2573868 finetune.py:45] layer 16_v initial loss 0.00011964770965278149
I0314 09:14:18.880877 2573868 finetune.py:45] layer 16_q initial loss 0.00013144944387022406
I0314 09:14:27.814488 2573868 finetune.py:45] layer 16_k initial loss 0.00014091285993345082
I0314 09:14:36.856369 2573868 finetune.py:45] layer 16_o initial loss 0.00023300331667996943
I0314 09:14:52.389771 2573868 finetune.py:45] layer 16_up initial loss 0.00030908212647773325
I0314 09:15:07.941577 2573868 finetune.py:45] layer 16_gate initial loss 0.00036802596878260374
I0314 09:15:09.885076 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 17 in 68.70987391471863s
I0314 09:15:13.068685 2574220 config.py:54] PyTorch version 2.1.1 available.
I0314 09:15:14.138887 2563616 quantize_finetune_llama.py:191] layer 18 gpu 2
I0314 09:15:14.209732 2574220 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:15:23.114253 2574220 finetune.py:45] layer 17_v initial loss 0.0001014687804854475
I0314 09:15:24.722434 2573868 finetune.py:45] layer 16_down initial loss 0.0005119690904393792
16_v proxy err 0.005481251981109381 tr(WHW.T) 780.7407836914062
16_q proxy err 0.0006780166877433658 tr(WHW.T) 7193.3974609375
16_k proxy err 0.000426302693085745 tr(WHW.T) 11630.361328125
16_o proxy err 0.004645573440939188 tr(WHW.T) 88.22785186767578
16_up proxy err 0.0025963231455534697 tr(WHW.T) 1890.9385986328125
16_gate proxy err 0.0014793002046644688 tr(WHW.T) 3369.859130859375
16_down proxy err 0.003976619336754084 tr(WHW.T) 152.0294952392578
I0314 09:15:32.467323 2574220 finetune.py:45] layer 17_q initial loss 0.00011203670874238014
I0314 09:15:41.260627 2574220 finetune.py:45] layer 17_k initial loss 0.0001225395972141996
I0314 09:15:50.010601 2574220 finetune.py:45] layer 17_o initial loss 0.00019106366380583495
I0314 09:16:05.072000 2574220 finetune.py:45] layer 17_up initial loss 0.0002753919397946447
I0314 09:16:20.181608 2574220 finetune.py:45] layer 17_gate initial loss 0.0003397509572096169
I0314 09:16:24.037776 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 18 in 69.48002362251282s
I0314 09:16:27.184298 2574605 config.py:54] PyTorch version 2.1.1 available.
I0314 09:16:28.351974 2563616 quantize_finetune_llama.py:191] layer 19 gpu 3
I0314 09:16:28.421438 2574605 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:16:36.151670 2574220 finetune.py:45] layer 17_down initial loss 0.0004944032407365739
17_v proxy err 0.005125176161527634 tr(WHW.T) 845.7654418945312
17_q proxy err 0.0006867399206385016 tr(WHW.T) 7163.2734375
17_k proxy err 0.0004692731308750808 tr(WHW.T) 10697.431640625
17_o proxy err 0.005071875639259815 tr(WHW.T) 58.14826965332031
17_up proxy err 0.002828957512974739 tr(WHW.T) 1921.07861328125
17_gate proxy err 0.0015522358007729053 tr(WHW.T) 3571.31640625
17_down proxy err 0.003942628391087055 tr(WHW.T) 165.43495178222656
I0314 09:16:37.089565 2574605 finetune.py:45] layer 18_v initial loss 0.00010268233017995954
I0314 09:16:45.847853 2574605 finetune.py:45] layer 18_q initial loss 0.00011744229414034635
I0314 09:16:54.513158 2574605 finetune.py:45] layer 18_k initial loss 0.0001356240827590227
I0314 09:17:03.102153 2574605 finetune.py:45] layer 18_o initial loss 0.0002115544193657115
I0314 09:17:18.635233 2574605 finetune.py:45] layer 18_up initial loss 0.00031367051997222006
I0314 09:17:35.654289 2574605 finetune.py:45] layer 18_gate initial loss 0.00038986062281765044
I0314 09:17:39.742135 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 19 in 70.9629065990448s
I0314 09:17:43.412864 2574975 config.py:54] PyTorch version 2.1.1 available.
I0314 09:17:44.462276 2563616 quantize_finetune_llama.py:191] layer 20 gpu 0
I0314 09:17:44.533712 2574975 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:17:56.701799 2574605 finetune.py:45] layer 18_down initial loss 0.000577971339225769
I0314 09:17:56.915184 2574975 finetune.py:45] layer 19_v initial loss 9.802971180761233e-05
18_v proxy err 0.004721987526863813 tr(WHW.T) 1003.7705078125
18_q proxy err 0.000708957901224494 tr(WHW.T) 7510.48046875
18_k proxy err 0.0005158300627954304 tr(WHW.T) 10462.6650390625
18_o proxy err 0.004468671511858702 tr(WHW.T) 69.96558380126953
18_up proxy err 0.0030077146366238594 tr(WHW.T) 2023.183837890625
18_gate proxy err 0.0016449978575110435 tr(WHW.T) 3783.076416015625
18_down proxy err 0.003972290083765984 tr(WHW.T) 198.52699279785156
I0314 09:18:06.836078 2574975 finetune.py:45] layer 19_q initial loss 0.00011178254499100149
I0314 09:18:17.367718 2574975 finetune.py:45] layer 19_k initial loss 0.00012445036554709077
I0314 09:18:28.139893 2574975 finetune.py:45] layer 19_o initial loss 0.0001918929337989539
I0314 09:18:45.941555 2574975 finetune.py:45] layer 19_up initial loss 0.00030546518974006176
I0314 09:18:59.747634 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 20 in 74.1494448184967s
I0314 09:19:02.867529 2575354 config.py:54] PyTorch version 2.1.1 available.
I0314 09:19:03.347930 2574975 finetune.py:45] layer 19_gate initial loss 0.00039471586933359504
I0314 09:19:03.893995 2563616 quantize_finetune_llama.py:191] layer 21 gpu 1
I0314 09:19:03.948842 2575354 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:19:12.659957 2575354 finetune.py:45] layer 20_v initial loss 0.0001094263861887157
I0314 09:19:20.382272 2574975 finetune.py:45] layer 19_down initial loss 0.0005999747081659734
19_v proxy err 0.004630641080439091 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.0007531606242991984 tr(WHW.T) 6944.4130859375
19_k proxy err 0.0005034011555835605 tr(WHW.T) 10548.4892578125
19_o proxy err 0.004411702509969473 tr(WHW.T) 62.291683197021484
19_up proxy err 0.0030243624933063984 tr(WHW.T) 2149.330322265625
19_gate proxy err 0.0018034300301223993 tr(WHW.T) 3687.5126953125
19_down proxy err 0.003861582837998867 tr(WHW.T) 222.93177795410156
I0314 09:19:21.805050 2575354 finetune.py:45] layer 20_q initial loss 0.00012250279542058706
I0314 09:19:30.676383 2575354 finetune.py:45] layer 20_k initial loss 0.00013251764175947756
I0314 09:19:39.513999 2575354 finetune.py:45] layer 20_o initial loss 0.00021202690550126135
I0314 09:19:54.647631 2575354 finetune.py:45] layer 20_up initial loss 0.00034457986475899816
I0314 09:20:09.733890 2575354 finetune.py:45] layer 20_gate initial loss 0.00044848344987258315
I0314 09:20:11.151131 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 21 in 66.92405200004578s
I0314 09:20:14.193145 2575718 config.py:54] PyTorch version 2.1.1 available.
I0314 09:20:15.232134 2563616 quantize_finetune_llama.py:191] layer 22 gpu 2
I0314 09:20:15.300720 2575718 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:20:23.784586 2575718 finetune.py:45] layer 21_v initial loss 9.878154378384352e-05
I0314 09:20:26.390382 2575354 finetune.py:45] layer 20_down initial loss 0.000700903357937932
20_v proxy err 0.0047548506408929825 tr(WHW.T) 990.5983276367188
20_q proxy err 0.0007345271878875792 tr(WHW.T) 7150.947265625
20_k proxy err 0.0005147067713551223 tr(WHW.T) 10386.2470703125
20_o proxy err 0.003266283543780446 tr(WHW.T) 100.31707000732422
20_up proxy err 0.002963195787742734 tr(WHW.T) 2340.89453125
20_gate proxy err 0.0017660083249211311 tr(WHW.T) 4024.62744140625
20_down proxy err 0.0038405978120863438 tr(WHW.T) 274.8815002441406
I0314 09:20:32.553826 2575718 finetune.py:45] layer 21_q initial loss 0.00011017896031262353
I0314 09:20:41.018794 2575718 finetune.py:45] layer 21_k initial loss 0.00012205845268908888
I0314 09:20:49.660973 2575718 finetune.py:45] layer 21_o initial loss 0.00018808396998792887
I0314 09:21:04.492596 2575718 finetune.py:45] layer 21_up initial loss 0.0003317540977150202
I0314 09:21:19.320011 2575718 finetune.py:45] layer 21_gate initial loss 0.00044323914335109293
I0314 09:21:22.390554 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 22 in 66.73184442520142s
I0314 09:21:25.521892 2576100 config.py:54] PyTorch version 2.1.1 available.
I0314 09:21:26.531707 2563616 quantize_finetune_llama.py:191] layer 23 gpu 3
I0314 09:21:26.602025 2576100 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:21:35.235348 2576100 finetune.py:45] layer 22_v initial loss 0.00011848843132611364
I0314 09:21:35.309226 2575718 finetune.py:45] layer 21_down initial loss 0.0006982079357840121
21_v proxy err 0.0046060108579695225 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.0008127521723508835 tr(WHW.T) 7064.314453125
21_k proxy err 0.0005819611251354218 tr(WHW.T) 9976.4658203125
21_o proxy err 0.003500570310279727 tr(WHW.T) 75.50972747802734
21_up proxy err 0.0031164325773715973 tr(WHW.T) 2361.650390625
21_gate proxy err 0.001887314603663981 tr(WHW.T) 4004.37646484375
21_down proxy err 0.0038412355352193117 tr(WHW.T) 276.5857849121094
I0314 09:21:43.993550 2576100 finetune.py:45] layer 22_q initial loss 0.00013758214481640607
I0314 09:21:52.815302 2576100 finetune.py:45] layer 22_k initial loss 0.00015310969320125878
I0314 09:22:01.411308 2576100 finetune.py:45] layer 22_o initial loss 0.000239399669226259
I0314 09:22:16.233182 2576100 finetune.py:45] layer 22_up initial loss 0.0004020471533294767
I0314 09:22:31.323504 2576100 finetune.py:45] layer 22_gate initial loss 0.000529515789821744
I0314 09:22:33.985287 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 23 in 67.00006794929504s
I0314 09:22:37.065555 2576464 config.py:54] PyTorch version 2.1.1 available.
I0314 09:22:38.099636 2563616 quantize_finetune_llama.py:191] layer 24 gpu 0
I0314 09:22:38.172908 2576464 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:22:47.533462 2576464 finetune.py:45] layer 23_v initial loss 0.00011752043064916506
I0314 09:22:47.935411 2576100 finetune.py:45] layer 22_down initial loss 0.0008115184609778225
22_v proxy err 0.004390126559883356 tr(WHW.T) 1243.2529296875
22_q proxy err 0.000770473328884691 tr(WHW.T) 7746.84765625
22_k proxy err 0.0005704417126253247 tr(WHW.T) 10603.2041015625
22_o proxy err 0.0029504168778657913 tr(WHW.T) 114.30065155029297
22_up proxy err 0.0031353430822491646 tr(WHW.T) 2474.510498046875
22_gate proxy err 0.001920676906593144 tr(WHW.T) 4156.64013671875
22_down proxy err 0.0038203056901693344 tr(WHW.T) 311.8800048828125
I0314 09:22:56.093545 2576464 finetune.py:45] layer 23_q initial loss 0.0001325188495684415
I0314 09:23:04.492298 2576464 finetune.py:45] layer 23_k initial loss 0.00014592631487175822
I0314 09:23:12.910765 2576464 finetune.py:45] layer 23_o initial loss 0.00022238207748159766
I0314 09:23:27.428080 2576464 finetune.py:45] layer 23_up initial loss 0.00040070133400149643
I0314 09:23:42.094967 2576464 finetune.py:45] layer 23_gate initial loss 0.00054360885405913
I0314 09:23:46.796343 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 24 in 67.60280466079712s
I0314 09:23:49.941493 2576831 config.py:54] PyTorch version 2.1.1 available.
I0314 09:23:50.994730 2563616 quantize_finetune_llama.py:191] layer 25 gpu 1
I0314 09:23:51.065168 2576831 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:23:58.312391 2576464 finetune.py:45] layer 23_down initial loss 0.0008341047214344144
23_v proxy err 0.004008859861642122 tr(WHW.T) 1486.037353515625
23_q proxy err 0.0008679254679009318 tr(WHW.T) 7346.60986328125
23_k proxy err 0.0006475543486885726 tr(WHW.T) 9982.2392578125
23_o proxy err 0.0035800712648779154 tr(WHW.T) 85.13458251953125
23_up proxy err 0.003249427070841193 tr(WHW.T) 2533.51025390625
23_gate proxy err 0.0020641447044909 tr(WHW.T) 4097.51953125
23_down proxy err 0.0038392883725464344 tr(WHW.T) 321.33892822265625
I0314 09:24:00.573688 2576831 finetune.py:45] layer 24_v initial loss 0.00014004875265527517
I0314 09:24:09.332470 2576831 finetune.py:45] layer 24_q initial loss 0.0001615672226762399
I0314 09:24:18.061803 2576831 finetune.py:45] layer 24_k initial loss 0.00017778993060346693
I0314 09:24:26.807626 2576831 finetune.py:45] layer 24_o initial loss 0.0002753553562797606
I0314 09:24:42.087759 2576831 finetune.py:45] layer 24_up initial loss 0.0004684940504375845
I0314 09:24:57.583967 2576831 finetune.py:45] layer 24_gate initial loss 0.0006273050094023347
I0314 09:24:58.921392 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 25 in 67.49455952644348s
I0314 09:25:02.087798 2577193 config.py:54] PyTorch version 2.1.1 available.
I0314 09:25:03.148891 2563616 quantize_finetune_llama.py:191] layer 26 gpu 2
I0314 09:25:03.220908 2577193 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:25:12.823522 2577193 finetune.py:45] layer 25_v initial loss 0.00015102801262401044
I0314 09:25:14.874973 2576831 finetune.py:45] layer 24_down initial loss 0.0009383526630699635
24_v proxy err 0.0042569623328745365 tr(WHW.T) 1394.900634765625
24_q proxy err 0.0009033627575263381 tr(WHW.T) 7020.447265625
24_k proxy err 0.0006180688505992293 tr(WHW.T) 10323.43359375
24_o proxy err 0.0028164193499833345 tr(WHW.T) 133.98797607421875
24_up proxy err 0.003296930342912674 tr(WHW.T) 2621.76513671875
24_gate proxy err 0.0020828167907893658 tr(WHW.T) 4262.74853515625
24_down proxy err 0.003861800068989396 tr(WHW.T) 340.22412109375
I0314 09:25:22.123007 2577193 finetune.py:45] layer 25_q initial loss 0.00016805794439278543
I0314 09:25:30.949458 2577193 finetune.py:45] layer 25_k initial loss 0.000184391945367679
I0314 09:25:39.865939 2577193 finetune.py:45] layer 25_o initial loss 0.0002590736257843673
I0314 09:25:55.161334 2577193 finetune.py:45] layer 25_up initial loss 0.0004760518786497414
I0314 09:26:10.286296 2577193 finetune.py:45] layer 25_gate initial loss 0.0006526281358674169
I0314 09:26:12.360189 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 26 in 68.77972912788391s
I0314 09:26:15.562377 2577565 config.py:54] PyTorch version 2.1.1 available.
I0314 09:26:16.664316 2563616 quantize_finetune_llama.py:191] layer 27 gpu 3
I0314 09:26:16.734446 2577565 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:26:27.755230 2577565 finetune.py:45] layer 26_v initial loss 0.00020432368910405785
I0314 09:26:28.493941 2577193 finetune.py:45] layer 25_down initial loss 0.0009939204901456833
25_v proxy err 0.0038696029223501682 tr(WHW.T) 1707.664794921875
25_q proxy err 0.0009720852831378579 tr(WHW.T) 7162.16357421875
25_k proxy err 0.0007338012801483274 tr(WHW.T) 9611.58984375
25_o proxy err 0.003380006877705455 tr(WHW.T) 83.535888671875
25_up proxy err 0.0032825625967234373 tr(WHW.T) 2805.728515625
25_gate proxy err 0.002023928565904498 tr(WHW.T) 4666.4404296875
25_down proxy err 0.0038403694052249193 tr(WHW.T) 373.460693359375
I0314 09:26:37.214343 2577565 finetune.py:45] layer 26_q initial loss 0.00022849424567539245
I0314 09:26:46.278947 2577565 finetune.py:45] layer 26_k initial loss 0.0002559655113145709
I0314 09:26:55.071537 2577565 finetune.py:45] layer 26_o initial loss 0.00038043729728087783
I0314 09:27:10.076724 2577565 finetune.py:45] layer 26_up initial loss 0.0006160962511785328
I0314 09:27:25.590017 2577565 finetune.py:45] layer 26_gate initial loss 0.0008154790848493576
I0314 09:27:25.776441 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 27 in 68.71117210388184s
I0314 09:27:28.876765 2577917 config.py:54] PyTorch version 2.1.1 available.
I0314 09:27:29.851183 2563616 quantize_finetune_llama.py:191] layer 28 gpu 0
I0314 09:27:29.921515 2577917 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:27:40.733437 2577917 finetune.py:45] layer 27_v initial loss 0.00014999671839177608
I0314 09:27:44.501337 2577565 finetune.py:45] layer 26_down initial loss 0.0011864617699757218
26_v proxy err 0.0038806141819804907 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.0009147183736786246 tr(WHW.T) 7469.98291015625
26_k proxy err 0.0006533611449413002 tr(WHW.T) 10487.8740234375
26_o proxy err 0.002346779452636838 tr(WHW.T) 202.88172912597656
26_up proxy err 0.00308339879848063 tr(WHW.T) 3154.75146484375
26_gate proxy err 0.0018840502016246319 tr(WHW.T) 5302.16455078125
26_down proxy err 0.003948454279452562 tr(WHW.T) 401.19390869140625
I0314 09:27:50.491395 2577917 finetune.py:45] layer 27_q initial loss 0.00017640172154642642
I0314 09:27:59.154554 2577917 finetune.py:45] layer 27_k initial loss 0.00019682601850945503
I0314 09:28:07.994991 2577917 finetune.py:45] layer 27_o initial loss 0.00030081084696576
I0314 09:28:24.247740 2577917 finetune.py:45] layer 27_up initial loss 0.0005693186540156603
I0314 09:28:42.139300 2577917 finetune.py:45] layer 27_gate initial loss 0.000799296423792839
I0314 09:28:42.469189 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 28 in 71.55618119239807s
I0314 09:28:45.728577 2578290 config.py:54] PyTorch version 2.1.1 available.
I0314 09:28:46.777336 2563616 quantize_finetune_llama.py:191] layer 29 gpu 1
I0314 09:28:46.853635 2578290 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:28:58.129183 2578290 finetune.py:45] layer 28_v initial loss 0.0001928909041453153
I0314 09:29:02.636702 2577917 finetune.py:45] layer 27_down initial loss 0.0012422142317518592
27_v proxy err 0.0035179047845304012 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.0008768431725911796 tr(WHW.T) 7691.708984375
27_k proxy err 0.0006403612205758691 tr(WHW.T) 10618.70703125
27_o proxy err 0.0031761808786541224 tr(WHW.T) 126.13690185546875
27_up proxy err 0.0028141983784735203 tr(WHW.T) 3691.557861328125
27_gate proxy err 0.001777739031240344 tr(WHW.T) 5990.82568359375
27_down proxy err 0.004024703986942768 tr(WHW.T) 466.9318542480469
I0314 09:29:09.384793 2578290 finetune.py:45] layer 28_q initial loss 0.0002213335392298177
I0314 09:29:19.754751 2578290 finetune.py:45] layer 28_k initial loss 0.00024911490618251264
I0314 09:29:30.854786 2578290 finetune.py:45] layer 28_o initial loss 0.0003848392516374588
I0314 09:29:49.160353 2578290 finetune.py:45] layer 28_up initial loss 0.0007061911164782941
I0314 09:30:02.444113 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 29 in 75.23291850090027s
I0314 09:30:05.826476 2578669 config.py:54] PyTorch version 2.1.1 available.
I0314 09:30:06.740572 2578290 finetune.py:45] layer 28_gate initial loss 0.0009837013203650713
I0314 09:30:06.847282 2563616 quantize_finetune_llama.py:191] layer 30 gpu 2
I0314 09:30:06.904777 2578669 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:30:15.790767 2578669 finetune.py:45] layer 29_v initial loss 0.0001842551864683628
I0314 09:30:24.509359 2578669 finetune.py:45] layer 29_q initial loss 0.00021058694983366877
I0314 09:30:25.256383 2578290 finetune.py:45] layer 28_down initial loss 0.0015642318176105618
28_v proxy err 0.003292986424639821 tr(WHW.T) 2018.944091796875
28_q proxy err 0.0009078114526346326 tr(WHW.T) 7651.126953125
28_k proxy err 0.0006629928830079734 tr(WHW.T) 10544.8251953125
28_o proxy err 0.0026271739043295383 tr(WHW.T) 194.8240966796875
28_up proxy err 0.002360869664698839 tr(WHW.T) 4661.2666015625
28_gate proxy err 0.001714854035526514 tr(WHW.T) 6547.48193359375
28_down proxy err 0.004061662591993809 tr(WHW.T) 603.8403930664062
I0314 09:30:33.308911 2578669 finetune.py:45] layer 29_k initial loss 0.00023759178293403238
I0314 09:30:42.027426 2578669 finetune.py:45] layer 29_o initial loss 0.00035989779280498624
I0314 09:30:57.035500 2578669 finetune.py:45] layer 29_up initial loss 0.0007389724487438798
I0314 09:31:12.158393 2578669 finetune.py:45] layer 29_gate initial loss 0.0010704400483518839
I0314 09:31:15.970690 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 30 in 68.7083671092987s
I0314 09:31:19.066917 2579051 config.py:54] PyTorch version 2.1.1 available.
I0314 09:31:20.115273 2563616 quantize_finetune_llama.py:191] layer 31 gpu 3
I0314 09:31:20.186074 2579051 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:31:28.404225 2578669 finetune.py:45] layer 29_down initial loss 0.0018404650036245584
I0314 09:31:28.753047 2579051 finetune.py:45] layer 30_v initial loss 0.00018433206423651427
29_v proxy err 0.0034948871470987797 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.0008992102229967713 tr(WHW.T) 7227.0009765625
29_k proxy err 0.0006217030459083617 tr(WHW.T) 10558.609375
29_o proxy err 0.0022201917599886656 tr(WHW.T) 207.9054412841797
29_up proxy err 0.001889946754090488 tr(WHW.T) 6070.0498046875
29_gate proxy err 0.001582906348630786 tr(WHW.T) 7369.6142578125
29_down proxy err 0.004161732736974955 tr(WHW.T) 782.2448120117188
I0314 09:31:37.469823 2579051 finetune.py:45] layer 30_q initial loss 0.00022134829487185925
I0314 09:31:46.202060 2579051 finetune.py:45] layer 30_k initial loss 0.0002579307183623314
I0314 09:31:54.990627 2579051 finetune.py:45] layer 30_o initial loss 0.0004203779681120068
I0314 09:32:10.215450 2579051 finetune.py:45] layer 30_up initial loss 0.0010522282682359219
I0314 09:32:25.315145 2579051 finetune.py:45] layer 30_gate initial loss 0.0014842628734186292
I0314 09:32:28.360480 2563616 quantize_finetune_llama.py:218] computed original embedding for layer 31 in 67.83414316177368s
I0314 09:32:31.401620 2579418 config.py:54] PyTorch version 2.1.1 available.
I0314 09:32:32.503037 2579418 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 09:32:42.390707 2579418 finetune.py:45] layer 31_v initial loss 0.00032169281621463597
I0314 09:32:42.416996 2579051 finetune.py:45] layer 30_down initial loss 0.003572664223611355
30_v proxy err 0.002871435834094882 tr(WHW.T) 2261.489501953125
30_q proxy err 0.000853095029015094 tr(WHW.T) 7815.9453125
30_k proxy err 0.0006411495851352811 tr(WHW.T) 10521.625
30_o proxy err 0.002331020776182413 tr(WHW.T) 251.96908569335938
30_up proxy err 0.0011710029793903232 tr(WHW.T) 10016.376953125
30_gate proxy err 0.0010832457337528467 tr(WHW.T) 11001.119140625
30_down proxy err 0.002399865770712495 tr(WHW.T) 3582.617919921875
I0314 09:32:51.209131 2579418 finetune.py:45] layer 31_q initial loss 0.0004541112284641713
I0314 09:32:59.581313 2579418 finetune.py:45] layer 31_k initial loss 0.0005271494737826288
I0314 09:33:08.113690 2579418 finetune.py:45] layer 31_o initial loss 0.0007861767080612481
I0314 09:33:22.810614 2579418 finetune.py:45] layer 31_up initial loss 0.002355573931708932
I0314 09:33:37.682621 2579418 finetune.py:45] layer 31_gate initial loss 0.0031242757104337215
I0314 09:33:53.606621 2579418 finetune.py:45] layer 31_down initial loss 0.010377861559391022
31_v proxy err 0.0033932828810065985 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.0006704424158670008 tr(WHW.T) 6858.09130859375
31_k proxy err 0.00045992861851118505 tr(WHW.T) 10233.677734375
31_o proxy err 0.0017665710765868425 tr(WHW.T) 457.7950744628906
31_up proxy err 0.0006754512432962656 tr(WHW.T) 14563.890625
31_gate proxy err 0.0006731933099217713 tr(WHW.T) 14836.2939453125
31_down proxy err 0.0016822150209918618 tr(WHW.T) 17873.55859375
