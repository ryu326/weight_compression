I0314 03:13:14.030771 2357215 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.31it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.75it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 10.11it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.09it/s]
I0314 03:13:15.868814 2357215 quantize_finetune_llama.py:142] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:22,  1.39it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:21,  1.38it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:20,  1.40it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:19,  1.40it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:18,  1.42it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:18,  1.44it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:17,  1.45it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:16,  1.46it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:15,  1.45it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:06<00:15,  1.46it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:07<00:14,  1.46it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:13,  1.46it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:13,  1.46it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:09<00:12,  1.46it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:10<00:11,  1.46it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:10,  1.46it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:11<00:10,  1.46it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:12<00:09,  1.46it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:08,  1.46it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:13<00:08,  1.46it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:14<00:07,  1.45it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:06,  1.45it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:15<00:06,  1.46it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:16<00:05,  1.47it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:04,  1.48it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:17<00:04,  1.48it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:18<00:03,  1.49it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:19<00:02,  1.50it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:19<00:02,  1.50it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:20<00:01,  1.51it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:21<00:00,  1.51it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:21<00:00,  1.52it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:21<00:00,  1.47it/s]
I0314 03:13:46.809574 2357215 quantize_finetune_llama.py:167] loaded compression model
I0314 03:14:01.050558 2357215 quantize_finetune_llama.py:171] loaded dataset and devset
I0314 03:14:05.949462 2357215 quantize_finetune_llama.py:191] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 03:15:24.922662 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 0 in 78.84438061714172s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 03:15:43.080420 2358843 config.py:54] PyTorch version 2.1.1 available.
I0314 03:15:44.007141 2357215 quantize_finetune_llama.py:191] layer 1 gpu 1
I0314 03:15:44.064991 2358843 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 03:15:52.700119 2358843 finetune.py:45] layer 0_v initial loss 1.1026614629372489e-05
I0314 03:16:25.042020 2358843 finetune.py:68] layer 0_v @ epoch 0 new loss 2.6895781957136933e-06 old loss 1.1026614629372489e-05 BETTER
I0314 03:16:50.614280 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 1 in 66.46855568885803s
I0314 03:16:58.691647 2358843 finetune.py:68] layer 0_v @ epoch 1 new loss 1.1080957165177097e-06 old loss 2.6895781957136933e-06 BETTER
I0314 03:16:58.710484 2359699 config.py:54] PyTorch version 2.1.1 available.
I0314 03:16:59.674044 2357215 quantize_finetune_llama.py:191] layer 2 gpu 2
I0314 03:16:59.729427 2359699 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 03:17:08.094652 2359699 finetune.py:45] layer 1_v initial loss 0.0001730442891130224
I0314 03:17:32.503003 2358843 finetune.py:68] layer 0_v @ epoch 2 new loss 7.369714012384065e-07 old loss 1.1080957165177097e-06 BETTER
I0314 03:17:39.191695 2359699 finetune.py:68] layer 1_v @ epoch 0 new loss 5.547481123358011e-05 old loss 0.0001730442891130224 BETTER
I0314 03:18:06.593327 2358843 finetune.py:68] layer 0_v @ epoch 3 new loss 6.167391006783873e-07 old loss 7.369714012384065e-07 BETTER
I0314 03:18:07.956702 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 2 in 68.142911195755s
I0314 03:18:13.886127 2359699 finetune.py:68] layer 1_v @ epoch 1 new loss 2.988586129504256e-05 old loss 5.547481123358011e-05 BETTER
I0314 03:18:18.561657 2360622 config.py:54] PyTorch version 2.1.1 available.
I0314 03:18:19.562432 2357215 quantize_finetune_llama.py:191] layer 3 gpu 3
I0314 03:18:19.629813 2360622 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 03:18:28.514392 2360622 finetune.py:45] layer 2_v initial loss 1.862994031398557e-05
I0314 03:18:41.022435 2358843 finetune.py:68] layer 0_v @ epoch 4 new loss 5.545889507629909e-07 old loss 6.167391006783873e-07 BETTER
I0314 03:18:46.197788 2359699 finetune.py:68] layer 1_v @ epoch 2 new loss 1.890376915980596e-05 old loss 2.988586129504256e-05 BETTER
I0314 03:18:51.418154 2358843 finetune.py:45] layer 0_q initial loss 6.364623459376162e-07
I0314 03:18:59.735210 2360622 finetune.py:68] layer 2_v @ epoch 0 new loss 1.3381435564951971e-05 old loss 1.862994031398557e-05 BETTER
I0314 03:19:18.608249 2359699 finetune.py:68] layer 1_v @ epoch 3 new loss 1.3719084563490469e-05 old loss 1.890376915980596e-05 BETTER
I0314 03:19:24.510098 2358843 finetune.py:68] layer 0_q @ epoch 0 new loss 5.141421297594206e-07 old loss 6.364623459376162e-07 BETTER
I0314 03:19:32.088831 2360622 finetune.py:68] layer 2_v @ epoch 1 new loss 1.0798285984492395e-05 old loss 1.3381435564951971e-05 BETTER
I0314 03:19:35.346708 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 3 in 75.6103720664978s
I0314 03:19:44.503462 2361668 config.py:54] PyTorch version 2.1.1 available.
I0314 03:19:45.688357 2357215 quantize_finetune_llama.py:191] layer 4 gpu 0
I0314 03:19:45.744512 2361668 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 03:19:52.238226 2359699 finetune.py:68] layer 1_v @ epoch 4 new loss 1.1570952665351797e-05 old loss 1.3719084563490469e-05 BETTER
I0314 03:19:55.296218 2361668 finetune.py:45] layer 3_v initial loss 2.8852718969574198e-05
I0314 03:19:58.942406 2358843 finetune.py:68] layer 0_q @ epoch 1 new loss 4.685246324243053e-07 old loss 5.141421297594206e-07 BETTER
I0314 03:20:03.230170 2359699 finetune.py:45] layer 1_q initial loss 1.3648213098349515e-05
I0314 03:20:05.262653 2360622 finetune.py:68] layer 2_v @ epoch 2 new loss 9.345216312794946e-06 old loss 1.0798285984492395e-05 BETTER
I0314 03:20:25.973130 2361668 finetune.py:68] layer 3_v @ epoch 0 new loss 1.7990463675232604e-05 old loss 2.8852718969574198e-05 BETTER
I0314 03:20:33.073534 2358843 finetune.py:68] layer 0_q @ epoch 2 new loss 4.3456859089019417e-07 old loss 4.685246324243053e-07 BETTER
I0314 03:20:34.663540 2359699 finetune.py:68] layer 1_q @ epoch 0 new loss 1.1583148989302572e-05 old loss 1.3648213098349515e-05 BETTER
I0314 03:20:37.753305 2360622 finetune.py:68] layer 2_v @ epoch 3 new loss 8.4359689935809e-06 old loss 9.345216312794946e-06 BETTER
I0314 03:20:57.866966 2361668 finetune.py:68] layer 3_v @ epoch 1 new loss 1.4419078070204705e-05 old loss 1.7990463675232604e-05 BETTER
I0314 03:21:06.735208 2359699 finetune.py:68] layer 1_q @ epoch 1 new loss 8.893321137293242e-06 old loss 1.1583148989302572e-05 BETTER
I0314 03:21:07.251437 2358843 finetune.py:68] layer 0_q @ epoch 3 new loss 4.0630763464832853e-07 old loss 4.3456859089019417e-07 BETTER
I0314 03:21:10.679206 2360622 finetune.py:68] layer 2_v @ epoch 4 new loss 7.815567187208217e-06 old loss 8.4359689935809e-06 BETTER
I0314 03:21:20.920327 2360622 finetune.py:45] layer 2_q initial loss 8.917519153328612e-06
I0314 03:21:29.863473 2361668 finetune.py:68] layer 3_v @ epoch 2 new loss 1.280187916563591e-05 old loss 1.4419078070204705e-05 BETTER
I0314 03:21:38.785790 2359699 finetune.py:76] layer 1_q @ epoch 2 new loss 9.929025509336498e-06 old loss 8.893321137293242e-06 WORSE
I0314 03:21:41.747785 2358843 finetune.py:68] layer 0_q @ epoch 4 new loss 3.810919793068024e-07 old loss 4.0630763464832853e-07 BETTER
I0314 03:21:52.108132 2358843 finetune.py:45] layer 0_k initial loss 4.738823236039025e-07
I0314 03:21:52.532573 2360622 finetune.py:68] layer 2_q @ epoch 0 new loss 7.789923984091729e-06 old loss 8.917519153328612e-06 BETTER
I0314 03:22:02.011992 2361668 finetune.py:68] layer 3_v @ epoch 3 new loss 1.1854815966216847e-05 old loss 1.280187916563591e-05 BETTER
I0314 03:22:10.517057 2359699 finetune.py:68] layer 1_q @ epoch 3 new loss 7.694895430176985e-06 old loss 8.893321137293242e-06 BETTER
I0314 03:22:24.822314 2360622 finetune.py:68] layer 2_q @ epoch 1 new loss 7.34639343136223e-06 old loss 7.789923984091729e-06 BETTER
I0314 03:22:25.184039 2358843 finetune.py:68] layer 0_k @ epoch 0 new loss 3.8488417430926347e-07 old loss 4.738823236039025e-07 BETTER
I0314 03:22:34.657537 2361668 finetune.py:68] layer 3_v @ epoch 4 new loss 1.1179970897501335e-05 old loss 1.1854815966216847e-05 BETTER
I0314 03:22:42.867327 2359699 finetune.py:76] layer 1_q @ epoch 4 new loss 8.200965567084495e-06 old loss 7.694895430176985e-06 WORSE
I0314 03:22:44.840053 2361668 finetune.py:45] layer 3_q initial loss 1.3775284969597124e-05
I0314 03:22:52.582499 2359699 finetune.py:45] layer 1_k initial loss 9.02651845535729e-06
I0314 03:22:57.270908 2360622 finetune.py:68] layer 2_q @ epoch 2 new loss 7.008897682680981e-06 old loss 7.34639343136223e-06 BETTER
I0314 03:22:59.006414 2358843 finetune.py:68] layer 0_k @ epoch 1 new loss 3.6113104329160706e-07 old loss 3.8488417430926347e-07 BETTER
I0314 03:23:16.191638 2361668 finetune.py:68] layer 3_q @ epoch 0 new loss 1.2133344171161298e-05 old loss 1.3775284969597124e-05 BETTER
I0314 03:23:23.953052 2359699 finetune.py:76] layer 1_k @ epoch 0 new loss 1.2199696357129142e-05 old loss 9.02651845535729e-06 WORSE
I0314 03:23:30.338405 2360622 finetune.py:68] layer 2_q @ epoch 3 new loss 6.7296496126800776e-06 old loss 7.008897682680981e-06 BETTER
I0314 03:23:33.173404 2358843 finetune.py:68] layer 0_k @ epoch 2 new loss 3.433291908550018e-07 old loss 3.6113104329160706e-07 BETTER
I0314 03:23:48.183159 2361668 finetune.py:68] layer 3_q @ epoch 1 new loss 1.1518307474034373e-05 old loss 1.2133344171161298e-05 BETTER
I0314 03:23:55.363957 2359699 finetune.py:68] layer 1_k @ epoch 1 new loss 7.425623607559828e-06 old loss 9.02651845535729e-06 BETTER
I0314 03:24:03.044254 2360622 finetune.py:68] layer 2_q @ epoch 4 new loss 6.489865882031154e-06 old loss 6.7296496126800776e-06 BETTER
I0314 03:24:07.097106 2358843 finetune.py:68] layer 0_k @ epoch 3 new loss 3.2852955200723954e-07 old loss 3.433291908550018e-07 BETTER
I0314 03:24:13.748617 2360622 finetune.py:45] layer 2_k initial loss 7.391884082608158e-06
I0314 03:24:20.232668 2361668 finetune.py:68] layer 3_q @ epoch 2 new loss 1.106141644413583e-05 old loss 1.1518307474034373e-05 BETTER
I0314 03:24:27.393516 2359699 finetune.py:68] layer 1_k @ epoch 2 new loss 7.025031663943082e-06 old loss 7.425623607559828e-06 BETTER
I0314 03:24:41.058689 2358843 finetune.py:68] layer 0_k @ epoch 4 new loss 3.138014506021136e-07 old loss 3.2852955200723954e-07 BETTER
I0314 03:24:45.199041 2360622 finetune.py:68] layer 2_k @ epoch 0 new loss 6.987224878685083e-06 old loss 7.391884082608158e-06 BETTER
I0314 03:24:51.772980 2358843 finetune.py:45] layer 0_o initial loss 2.441062633806723e-06
I0314 03:24:52.483344 2361668 finetune.py:68] layer 3_q @ epoch 3 new loss 1.0690680937841535e-05 old loss 1.106141644413583e-05 BETTER
I0314 03:24:59.464818 2359699 finetune.py:68] layer 1_k @ epoch 3 new loss 6.969414698687615e-06 old loss 7.025031663943082e-06 BETTER
I0314 03:25:17.229446 2360622 finetune.py:68] layer 2_k @ epoch 1 new loss 6.755919912393438e-06 old loss 6.987224878685083e-06 BETTER
I0314 03:25:24.420776 2358843 finetune.py:68] layer 0_o @ epoch 0 new loss 2.1808136807521805e-06 old loss 2.441062633806723e-06 BETTER
I0314 03:25:24.454060 2361668 finetune.py:68] layer 3_q @ epoch 4 new loss 1.038418758980697e-05 old loss 1.0690680937841535e-05 BETTER
I0314 03:25:31.514523 2359699 finetune.py:68] layer 1_k @ epoch 4 new loss 6.927903996256646e-06 old loss 6.969414698687615e-06 BETTER
I0314 03:25:34.640049 2361668 finetune.py:45] layer 3_k initial loss 1.2530438652902376e-05
I0314 03:25:41.748415 2359699 finetune.py:45] layer 1_o initial loss 4.368368900031783e-05
I0314 03:25:49.219028 2360622 finetune.py:68] layer 2_k @ epoch 2 new loss 6.568654498551041e-06 old loss 6.755919912393438e-06 BETTER
I0314 03:25:57.709503 2358843 finetune.py:68] layer 0_o @ epoch 1 new loss 1.9906231045752065e-06 old loss 2.1808136807521805e-06 BETTER
I0314 03:26:05.478195 2361668 finetune.py:68] layer 3_k @ epoch 0 new loss 1.1842877938761376e-05 old loss 1.2530438652902376e-05 BETTER
I0314 03:26:12.562107 2359699 finetune.py:68] layer 1_o @ epoch 0 new loss 2.1339967133826576e-05 old loss 4.368368900031783e-05 BETTER
I0314 03:26:21.189785 2360622 finetune.py:68] layer 2_k @ epoch 3 new loss 6.404440227925079e-06 old loss 6.568654498551041e-06 BETTER
I0314 03:26:30.900865 2358843 finetune.py:68] layer 0_o @ epoch 2 new loss 1.8390959439784638e-06 old loss 1.9906231045752065e-06 BETTER
I0314 03:26:36.970731 2361668 finetune.py:68] layer 3_k @ epoch 1 new loss 1.1522489330673125e-05 old loss 1.1842877938761376e-05 BETTER
I0314 03:26:44.031379 2359699 finetune.py:68] layer 1_o @ epoch 1 new loss 1.8353977793594822e-05 old loss 2.1339967133826576e-05 BETTER
I0314 03:26:53.220143 2360622 finetune.py:68] layer 2_k @ epoch 4 new loss 6.258383564272663e-06 old loss 6.404440227925079e-06 BETTER
I0314 03:27:03.474788 2360622 finetune.py:45] layer 2_o initial loss 1.3781243069388438e-05
I0314 03:27:04.293559 2358843 finetune.py:68] layer 0_o @ epoch 3 new loss 1.7125917111115996e-06 old loss 1.8390959439784638e-06 BETTER
I0314 03:27:08.482613 2361668 finetune.py:68] layer 3_k @ epoch 2 new loss 1.1270104550931137e-05 old loss 1.1522489330673125e-05 BETTER
I0314 03:27:15.400911 2359699 finetune.py:68] layer 1_o @ epoch 2 new loss 1.7192594896187074e-05 old loss 1.8353977793594822e-05 BETTER
I0314 03:27:34.165303 2360622 finetune.py:68] layer 2_o @ epoch 0 new loss 1.3561389096139465e-05 old loss 1.3781243069388438e-05 BETTER
I0314 03:27:37.735914 2358843 finetune.py:68] layer 0_o @ epoch 4 new loss 1.6046435575844953e-06 old loss 1.7125917111115996e-06 BETTER
I0314 03:27:40.042980 2361668 finetune.py:68] layer 3_k @ epoch 3 new loss 1.1046548934245948e-05 old loss 1.1270104550931137e-05 BETTER
I0314 03:27:46.772174 2359699 finetune.py:68] layer 1_o @ epoch 3 new loss 1.6433268683613278e-05 old loss 1.7192594896187074e-05 BETTER
I0314 03:27:54.629280 2358843 finetune.py:45] layer 0_up initial loss 1.8164514585805591e-06
I0314 03:28:05.756782 2360622 finetune.py:68] layer 2_o @ epoch 1 new loss 1.3399742783803958e-05 old loss 1.3561389096139465e-05 BETTER
I0314 03:28:11.571583 2361668 finetune.py:68] layer 3_k @ epoch 4 new loss 1.0853266758203972e-05 old loss 1.1046548934245948e-05 BETTER
I0314 03:28:18.062876 2359699 finetune.py:68] layer 1_o @ epoch 4 new loss 1.5591187548125163e-05 old loss 1.6433268683613278e-05 BETTER
I0314 03:28:21.533371 2361668 finetune.py:45] layer 3_o initial loss 2.7228907129028812e-05
I0314 03:28:25.009264 2358843 finetune.py:68] layer 0_up @ epoch 0 new loss 1.6961322444331017e-06 old loss 1.8164514585805591e-06 BETTER
I0314 03:28:34.630069 2359699 finetune.py:45] layer 1_up initial loss 0.00014166886103339493
I0314 03:28:37.386829 2360622 finetune.py:68] layer 2_o @ epoch 2 new loss 1.3263696018839255e-05 old loss 1.3399742783803958e-05 BETTER
I0314 03:28:51.668053 2361668 finetune.py:68] layer 3_o @ epoch 0 new loss 2.6349729523644783e-05 old loss 2.7228907129028812e-05 BETTER
I0314 03:28:56.350227 2358843 finetune.py:68] layer 0_up @ epoch 1 new loss 1.6041961998780607e-06 old loss 1.6961322444331017e-06 BETTER
I0314 03:29:03.347527 2359699 finetune.py:68] layer 1_up @ epoch 0 new loss 4.967011045664549e-05 old loss 0.00014166886103339493 BETTER
I0314 03:29:08.857569 2360622 finetune.py:68] layer 2_o @ epoch 3 new loss 1.3145385310053825e-05 old loss 1.3263696018839255e-05 BETTER
I0314 03:29:22.617501 2361668 finetune.py:68] layer 3_o @ epoch 1 new loss 2.5769764761207625e-05 old loss 2.6349729523644783e-05 BETTER
I0314 03:29:27.965391 2358843 finetune.py:68] layer 0_up @ epoch 2 new loss 1.5292229136321112e-06 old loss 1.6041961998780607e-06 BETTER
I0314 03:29:32.949328 2359699 finetune.py:68] layer 1_up @ epoch 1 new loss 2.2182604880072176e-05 old loss 4.967011045664549e-05 BETTER
I0314 03:29:40.748217 2360622 finetune.py:68] layer 2_o @ epoch 4 new loss 1.3039046280027833e-05 old loss 1.3145385310053825e-05 BETTER
I0314 03:29:53.527877 2361668 finetune.py:68] layer 3_o @ epoch 2 new loss 2.534213854232803e-05 old loss 2.5769764761207625e-05 BETTER
I0314 03:29:57.436518 2360622 finetune.py:45] layer 2_up initial loss 1.7791828213375993e-05
I0314 03:29:59.604392 2358843 finetune.py:68] layer 0_up @ epoch 3 new loss 1.4646115005234606e-06 old loss 1.5292229136321112e-06 BETTER
I0314 03:30:02.669695 2359699 finetune.py:68] layer 1_up @ epoch 2 new loss 1.7371736248605885e-05 old loss 2.2182604880072176e-05 BETTER
I0314 03:30:24.601630 2361668 finetune.py:68] layer 3_o @ epoch 3 new loss 2.4991410100483336e-05 old loss 2.534213854232803e-05 BETTER
I0314 03:30:26.663755 2360622 finetune.py:68] layer 2_up @ epoch 0 new loss 1.7655644114711322e-05 old loss 1.7791828213375993e-05 BETTER
I0314 03:30:31.283399 2358843 finetune.py:68] layer 0_up @ epoch 4 new loss 1.4082136203796836e-06 old loss 1.4646115005234606e-06 BETTER
I0314 03:30:32.520503 2359699 finetune.py:68] layer 1_up @ epoch 3 new loss 1.6612930267001502e-05 old loss 1.7371736248605885e-05 BETTER
I0314 03:30:48.459108 2358843 finetune.py:45] layer 0_gate initial loss 1.5938720707708853e-06
I0314 03:30:55.629881 2361668 finetune.py:68] layer 3_o @ epoch 4 new loss 2.4700282665435225e-05 old loss 2.4991410100483336e-05 BETTER
I0314 03:30:56.643813 2360622 finetune.py:68] layer 2_up @ epoch 1 new loss 1.7553304132889025e-05 old loss 1.7655644114711322e-05 BETTER
I0314 03:31:02.450920 2359699 finetune.py:68] layer 1_up @ epoch 4 new loss 1.6146557754836977e-05 old loss 1.6612930267001502e-05 BETTER
I0314 03:31:12.662587 2361668 finetune.py:45] layer 3_up initial loss 3.609917257563211e-05
I0314 03:31:17.411015 2358843 finetune.py:68] layer 0_gate @ epoch 0 new loss 1.5159356507865596e-06 old loss 1.5938720707708853e-06 BETTER
I0314 03:31:19.797240 2359699 finetune.py:45] layer 1_gate initial loss 0.0001518483622930944
I0314 03:31:26.374510 2360622 finetune.py:68] layer 2_up @ epoch 2 new loss 1.7465772543800995e-05 old loss 1.7553304132889025e-05 BETTER
I0314 03:31:41.332333 2361668 finetune.py:68] layer 3_up @ epoch 0 new loss 3.572924470063299e-05 old loss 3.609917257563211e-05 BETTER
I0314 03:31:47.313010 2358843 finetune.py:68] layer 0_gate @ epoch 1 new loss 1.460964881516702e-06 old loss 1.5159356507865596e-06 BETTER
I0314 03:31:47.314810 2359699 finetune.py:68] layer 1_gate @ epoch 0 new loss 7.01901808497496e-05 old loss 0.0001518483622930944 BETTER
I0314 03:31:56.245894 2360622 finetune.py:68] layer 2_up @ epoch 3 new loss 1.738685205054935e-05 old loss 1.7465772543800995e-05 BETTER
I0314 03:32:10.747476 2361668 finetune.py:68] layer 3_up @ epoch 1 new loss 3.54588519257959e-05 old loss 3.572924470063299e-05 BETTER
I0314 03:32:15.627597 2359699 finetune.py:68] layer 1_gate @ epoch 1 new loss 3.385429590707645e-05 old loss 7.01901808497496e-05 BETTER
I0314 03:32:17.381243 2358843 finetune.py:68] layer 0_gate @ epoch 2 new loss 1.4177774119161768e-06 old loss 1.460964881516702e-06 BETTER
I0314 03:32:26.252822 2360622 finetune.py:68] layer 2_up @ epoch 4 new loss 1.7314210708718747e-05 old loss 1.738685205054935e-05 BETTER
I0314 03:32:40.269530 2361668 finetune.py:68] layer 3_up @ epoch 2 new loss 3.523207487887703e-05 old loss 3.54588519257959e-05 BETTER
I0314 03:32:43.679292 2360622 finetune.py:45] layer 2_gate initial loss 2.081510319840163e-05
I0314 03:32:43.971404 2359699 finetune.py:68] layer 1_gate @ epoch 2 new loss 2.239807872683741e-05 old loss 3.385429590707645e-05 BETTER
I0314 03:32:47.412968 2358843 finetune.py:68] layer 0_gate @ epoch 3 new loss 1.3820359754390665e-06 old loss 1.4177774119161768e-06 BETTER
I0314 03:33:09.798217 2361668 finetune.py:68] layer 3_up @ epoch 3 new loss 3.502980689518154e-05 old loss 3.523207487887703e-05 BETTER
I0314 03:33:11.352504 2360622 finetune.py:68] layer 2_gate @ epoch 0 new loss 2.0703068003058434e-05 old loss 2.081510319840163e-05 BETTER
I0314 03:33:12.290974 2359699 finetune.py:68] layer 1_gate @ epoch 3 new loss 1.9512874132487923e-05 old loss 2.239807872683741e-05 BETTER
I0314 03:33:17.365414 2358843 finetune.py:68] layer 0_gate @ epoch 4 new loss 1.351296077700681e-06 old loss 1.3820359754390665e-06 BETTER
I0314 03:33:35.422853 2358843 finetune.py:45] layer 0_down initial loss 2.4746923372731544e-06
I0314 03:33:39.377367 2361668 finetune.py:68] layer 3_up @ epoch 4 new loss 3.4842825698433444e-05 old loss 3.502980689518154e-05 BETTER
I0314 03:33:39.652383 2360622 finetune.py:68] layer 2_gate @ epoch 1 new loss 2.0621384464902803e-05 old loss 2.0703068003058434e-05 BETTER
I0314 03:33:40.542352 2359699 finetune.py:68] layer 1_gate @ epoch 4 new loss 1.8906363038695417e-05 old loss 1.9512874132487923e-05 BETTER
I0314 03:33:55.812202 2361668 finetune.py:45] layer 3_gate initial loss 4.3094092688988894e-05
I0314 03:33:58.602566 2359699 finetune.py:45] layer 1_down initial loss 0.002984001999720931
I0314 03:34:02.470174 2358843 finetune.py:68] layer 0_down @ epoch 0 new loss 2.456129550409969e-06 old loss 2.4746923372731544e-06 BETTER
I0314 03:34:08.043853 2360622 finetune.py:68] layer 2_gate @ epoch 2 new loss 2.055250297416933e-05 old loss 2.0621384464902803e-05 BETTER
I0314 03:34:22.845726 2361668 finetune.py:68] layer 3_gate @ epoch 0 new loss 4.285525210434571e-05 old loss 4.3094092688988894e-05 BETTER
I0314 03:34:24.366489 2359699 finetune.py:68] layer 1_down @ epoch 0 new loss 0.002944604493677616 old loss 0.002984001999720931 BETTER
I0314 03:34:30.923285 2358843 finetune.py:68] layer 0_down @ epoch 1 new loss 2.4487385417160112e-06 old loss 2.456129550409969e-06 BETTER
I0314 03:34:36.155910 2360622 finetune.py:68] layer 2_gate @ epoch 3 new loss 2.049190879915841e-05 old loss 2.055250297416933e-05 BETTER
I0314 03:34:51.261811 2361668 finetune.py:68] layer 3_gate @ epoch 1 new loss 4.2670249968068674e-05 old loss 4.285525210434571e-05 BETTER
I0314 03:34:51.478422 2359699 finetune.py:68] layer 1_down @ epoch 1 new loss 0.002936611883342266 old loss 0.002944604493677616 BETTER
I0314 03:34:59.397585 2358843 finetune.py:68] layer 0_down @ epoch 2 new loss 2.4447710984532023e-06 old loss 2.4487385417160112e-06 BETTER
I0314 03:35:04.484217 2360622 finetune.py:68] layer 2_gate @ epoch 4 new loss 2.0436264094314538e-05 old loss 2.049190879915841e-05 BETTER
I0314 03:35:18.483089 2359699 finetune.py:68] layer 1_down @ epoch 2 new loss 0.0029355594888329506 old loss 0.002936611883342266 BETTER
I0314 03:35:19.309718 2361668 finetune.py:68] layer 3_gate @ epoch 2 new loss 4.2512587242526934e-05 old loss 4.2670249968068674e-05 BETTER
I0314 03:35:23.536714 2360622 finetune.py:45] layer 2_down initial loss 3.0107985367067158e-05
I0314 03:35:27.856789 2358843 finetune.py:68] layer 0_down @ epoch 3 new loss 2.4423284230579156e-06 old loss 2.4447710984532023e-06 BETTER
I0314 03:35:45.608800 2359699 finetune.py:68] layer 1_down @ epoch 3 new loss 0.002935244468972087 old loss 0.0029355594888329506 BETTER
I0314 03:35:47.760184 2361668 finetune.py:68] layer 3_gate @ epoch 3 new loss 4.2370396840851754e-05 old loss 4.2512587242526934e-05 BETTER
I0314 03:35:49.471401 2360622 finetune.py:68] layer 2_down @ epoch 0 new loss 3.009196007042192e-05 old loss 3.0107985367067158e-05 BETTER
I0314 03:35:56.417981 2358843 finetune.py:68] layer 0_down @ epoch 4 new loss 2.4405519525316777e-06 old loss 2.4423284230579156e-06 BETTER
0_v proxy err 0.08050522953271866 tr(WHW.T) 4.225186347961426
0_q proxy err 0.0004832620616070926 tr(WHW.T) 2710.47119140625
0_k proxy err 0.0005284149083308876 tr(WHW.T) 1699.797607421875
0_o proxy err 0.005578760989010334 tr(WHW.T) 0.9704830050468445
0_up proxy err 0.009763509035110474 tr(WHW.T) 43.3036003112793
0_gate proxy err 0.006789292674511671 tr(WHW.T) 63.55564498901367
0_down proxy err 0.006742761004716158 tr(WHW.T) 0.6594889163970947
I0314 03:36:13.376085 2359699 finetune.py:68] layer 1_down @ epoch 4 new loss 0.0029346286319196224 old loss 0.002935244468972087 BETTER
1_v proxy err 0.11517074704170227 tr(WHW.T) 16.465883255004883
1_q proxy err 0.0006658961647190154 tr(WHW.T) 4780.7177734375
1_k proxy err 0.0006362255662679672 tr(WHW.T) 5002.150390625
1_o proxy err 0.019050728529691696 tr(WHW.T) 1.11350417137146
1_up proxy err 0.010229953564703465 tr(WHW.T) 110.06100463867188
1_gate proxy err 0.005287602078169584 tr(WHW.T) 222.1815185546875
1_down proxy err 0.006059425417333841 tr(WHW.T) 2044.833251953125
I0314 03:36:16.692591 2361668 finetune.py:68] layer 3_gate @ epoch 4 new loss 4.223885480314493e-05 old loss 4.2370396840851754e-05 BETTER
I0314 03:36:17.144378 2360622 finetune.py:68] layer 2_down @ epoch 1 new loss 3.0082976081757806e-05 old loss 3.009196007042192e-05 BETTER
I0314 03:36:34.045961 2361668 finetune.py:45] layer 3_down initial loss 6.266542914090678e-05
I0314 03:36:43.897359 2360622 finetune.py:68] layer 2_down @ epoch 2 new loss 3.007730992976576e-05 old loss 3.0082976081757806e-05 BETTER
I0314 03:36:59.448780 2361668 finetune.py:68] layer 3_down @ epoch 0 new loss 6.263561954256147e-05 old loss 6.266542914090678e-05 BETTER
I0314 03:37:10.659917 2360622 finetune.py:68] layer 2_down @ epoch 3 new loss 3.007293344126083e-05 old loss 3.007730992976576e-05 BETTER
I0314 03:37:25.631714 2361668 finetune.py:68] layer 3_down @ epoch 1 new loss 6.262214446906e-05 old loss 6.263561954256147e-05 BETTER
I0314 03:37:30.000434 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 4 in 71.44314885139465s
I0314 03:37:33.157947 2373291 config.py:54] PyTorch version 2.1.1 available.
I0314 03:37:34.244895 2357215 quantize_finetune_llama.py:191] layer 5 gpu 1
I0314 03:37:34.316777 2373291 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 03:37:37.400768 2360622 finetune.py:68] layer 2_down @ epoch 4 new loss 3.006973201991059e-05 old loss 3.007293344126083e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2_v proxy err 0.02979182079434395 tr(WHW.T) 136.67332458496094
2_q proxy err 0.0007878380711190403 tr(WHW.T) 7770.7021484375
2_k proxy err 0.0006266316049732268 tr(WHW.T) 10224.837890625
2_o proxy err 0.015778923407197 tr(WHW.T) 1.4656692743301392
2_up proxy err 0.011958514340221882 tr(WHW.T) 193.4906768798828
2_gate proxy err 0.007787596434354782 tr(WHW.T) 306.5665283203125
2_down proxy err 0.01372738741338253 tr(WHW.T) 3.01717209815979
I0314 03:37:43.412969 2373291 finetune.py:45] layer 4_v initial loss 5.182664972380735e-05
I0314 03:37:52.064540 2361668 finetune.py:68] layer 3_down @ epoch 2 new loss 6.261323142098263e-05 old loss 6.262214446906e-05 BETTER
I0314 03:38:16.458445 2373291 finetune.py:68] layer 4_v @ epoch 0 new loss 2.8409320293576457e-05 old loss 5.182664972380735e-05 BETTER
I0314 03:38:18.395016 2361668 finetune.py:68] layer 3_down @ epoch 3 new loss 6.260653753997758e-05 old loss 6.261323142098263e-05 BETTER
I0314 03:38:44.927110 2361668 finetune.py:68] layer 3_down @ epoch 4 new loss 6.260123336687684e-05 old loss 6.260653753997758e-05 BETTER
3_v proxy err 0.027675889432430267 tr(WHW.T) 284.77557373046875
3_q proxy err 0.0014669335214421153 tr(WHW.T) 7231.1455078125
3_k proxy err 0.001096801133826375 tr(WHW.T) 10100.3876953125
3_o proxy err 0.01578538306057453 tr(WHW.T) 3.363485813140869
3_up proxy err 0.013624663464725018 tr(WHW.T) 284.8811340332031
3_gate proxy err 0.008382500149309635 tr(WHW.T) 478.4920654296875
3_down proxy err 0.014021174050867558 tr(WHW.T) 6.153166770935059
I0314 03:38:50.390613 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 5 in 68.52503633499146s
I0314 03:38:51.029151 2373291 finetune.py:68] layer 4_v @ epoch 1 new loss 2.2128151613287628e-05 old loss 2.8409320293576457e-05 BETTER
I0314 03:38:53.422803 2374234 config.py:54] PyTorch version 2.1.1 available.
I0314 03:38:54.403733 2357215 quantize_finetune_llama.py:191] layer 6 gpu 2
I0314 03:38:54.464745 2374234 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 03:39:02.926576 2374234 finetune.py:45] layer 5_v initial loss 7.82306888140738e-05
I0314 03:39:25.772879 2373291 finetune.py:68] layer 4_v @ epoch 2 new loss 1.9443112250883132e-05 old loss 2.2128151613287628e-05 BETTER
I0314 03:39:34.206070 2374234 finetune.py:68] layer 5_v @ epoch 0 new loss 4.2630545067368075e-05 old loss 7.82306888140738e-05 BETTER
I0314 03:40:00.651745 2373291 finetune.py:68] layer 4_v @ epoch 3 new loss 1.7837894120020792e-05 old loss 1.9443112250883132e-05 BETTER
I0314 03:40:02.808432 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 6 in 68.03828716278076s
I0314 03:40:06.023807 2375091 config.py:54] PyTorch version 2.1.1 available.
I0314 03:40:06.499620 2374234 finetune.py:68] layer 5_v @ epoch 1 new loss 3.4160908398916945e-05 old loss 4.2630545067368075e-05 BETTER
I0314 03:40:07.023849 2357215 quantize_finetune_llama.py:191] layer 7 gpu 3
I0314 03:40:07.089730 2375091 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 03:40:15.958089 2375091 finetune.py:45] layer 6_v initial loss 9.743315604282543e-05
I0314 03:40:35.560888 2373291 finetune.py:68] layer 4_v @ epoch 4 new loss 1.670198980718851e-05 old loss 1.7837894120020792e-05 BETTER
I0314 03:40:38.730439 2374234 finetune.py:68] layer 5_v @ epoch 2 new loss 3.0514405807480216e-05 old loss 3.4160908398916945e-05 BETTER
I0314 03:40:45.936728 2373291 finetune.py:45] layer 4_q initial loss 2.121932993759401e-05
I0314 03:40:47.430739 2375091 finetune.py:68] layer 6_v @ epoch 0 new loss 5.1455725042615086e-05 old loss 9.743315604282543e-05 BETTER
I0314 03:41:11.600865 2374234 finetune.py:68] layer 5_v @ epoch 3 new loss 2.827462594723329e-05 old loss 3.0514405807480216e-05 BETTER
I0314 03:41:15.911456 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 7 in 68.41450500488281s
I0314 03:41:19.305685 2375996 config.py:54] PyTorch version 2.1.1 available.
I0314 03:41:19.562370 2373291 finetune.py:68] layer 4_q @ epoch 0 new loss 1.8179658582084812e-05 old loss 2.121932993759401e-05 BETTER
I0314 03:41:20.268677 2375091 finetune.py:68] layer 6_v @ epoch 1 new loss 4.244762021698989e-05 old loss 5.1455725042615086e-05 BETTER
I0314 03:41:20.357885 2357215 quantize_finetune_llama.py:191] layer 8 gpu 0
I0314 03:41:20.424337 2375996 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 03:41:29.323287 2375996 finetune.py:45] layer 7_v initial loss 0.0001247208274435252
I0314 03:41:44.550202 2374234 finetune.py:68] layer 5_v @ epoch 4 new loss 2.670546564331744e-05 old loss 2.827462594723329e-05 BETTER
I0314 03:41:53.239784 2375091 finetune.py:68] layer 6_v @ epoch 2 new loss 3.825261956080794e-05 old loss 4.244762021698989e-05 BETTER
I0314 03:41:53.986554 2373291 finetune.py:68] layer 4_q @ epoch 1 new loss 1.7165797544294037e-05 old loss 1.8179658582084812e-05 BETTER
I0314 03:41:54.986008 2374234 finetune.py:45] layer 5_q initial loss 3.279236261732876e-05
I0314 03:42:00.418116 2375996 finetune.py:68] layer 7_v @ epoch 0 new loss 6.535070133395493e-05 old loss 0.0001247208274435252 BETTER
I0314 03:42:26.510423 2375091 finetune.py:68] layer 6_v @ epoch 3 new loss 3.564284634194337e-05 old loss 3.825261956080794e-05 BETTER
I0314 03:42:26.837588 2374234 finetune.py:68] layer 5_q @ epoch 0 new loss 2.865577334887348e-05 old loss 3.279236261732876e-05 BETTER
I0314 03:42:28.614625 2373291 finetune.py:68] layer 4_q @ epoch 2 new loss 1.6413034245488234e-05 old loss 1.7165797544294037e-05 BETTER
I0314 03:42:32.390603 2375996 finetune.py:68] layer 7_v @ epoch 1 new loss 5.455702921608463e-05 old loss 6.535070133395493e-05 BETTER
I0314 03:42:59.147537 2374234 finetune.py:68] layer 5_q @ epoch 1 new loss 2.7223855795455165e-05 old loss 2.865577334887348e-05 BETTER
I0314 03:42:59.700081 2375091 finetune.py:68] layer 6_v @ epoch 4 new loss 3.382521026651375e-05 old loss 3.564284634194337e-05 BETTER
I0314 03:43:03.461117 2373291 finetune.py:68] layer 4_q @ epoch 3 new loss 1.582153345225379e-05 old loss 1.6413034245488234e-05 BETTER
I0314 03:43:04.374135 2375996 finetune.py:68] layer 7_v @ epoch 2 new loss 4.961121521773748e-05 old loss 5.455702921608463e-05 BETTER
I0314 03:43:10.395050 2375091 finetune.py:45] layer 6_q initial loss 4.5539465645560995e-05
I0314 03:43:31.611179 2374234 finetune.py:68] layer 5_q @ epoch 2 new loss 2.6194760721409693e-05 old loss 2.7223855795455165e-05 BETTER
I0314 03:43:36.695344 2375996 finetune.py:68] layer 7_v @ epoch 3 new loss 4.652444476960227e-05 old loss 4.961121521773748e-05 BETTER
I0314 03:43:38.159580 2373291 finetune.py:68] layer 4_q @ epoch 4 new loss 1.5322515537263826e-05 old loss 1.582153345225379e-05 BETTER
I0314 03:43:42.027010 2375091 finetune.py:68] layer 6_q @ epoch 0 new loss 3.894075780408457e-05 old loss 4.5539465645560995e-05 BETTER
I0314 03:43:48.757607 2373291 finetune.py:45] layer 4_k initial loss 1.8757260477286763e-05
I0314 03:44:04.229530 2374234 finetune.py:68] layer 5_q @ epoch 3 new loss 2.5381637897226028e-05 old loss 2.6194760721409693e-05 BETTER
I0314 03:44:09.047806 2375996 finetune.py:68] layer 7_v @ epoch 4 new loss 4.437004463397898e-05 old loss 4.652444476960227e-05 BETTER
I0314 03:44:14.356833 2375091 finetune.py:68] layer 6_q @ epoch 1 new loss 3.7116886232979596e-05 old loss 3.894075780408457e-05 BETTER
I0314 03:44:19.424864 2375996 finetune.py:45] layer 7_q initial loss 6.0259466408751905e-05
I0314 03:44:21.693951 2373291 finetune.py:68] layer 4_k @ epoch 0 new loss 1.7566155293025076e-05 old loss 1.8757260477286763e-05 BETTER
I0314 03:44:36.779622 2374234 finetune.py:68] layer 5_q @ epoch 4 new loss 2.4704719180590473e-05 old loss 2.5381637897226028e-05 BETTER
I0314 03:44:46.747953 2375091 finetune.py:68] layer 6_q @ epoch 2 new loss 3.578600808396004e-05 old loss 3.7116886232979596e-05 BETTER
I0314 03:44:46.854276 2374234 finetune.py:45] layer 5_k initial loss 2.8806605769204907e-05
I0314 03:44:50.734274 2375996 finetune.py:68] layer 7_q @ epoch 0 new loss 5.203440741752274e-05 old loss 6.0259466408751905e-05 BETTER
I0314 03:44:55.294234 2373291 finetune.py:68] layer 4_k @ epoch 1 new loss 1.706284638203215e-05 old loss 1.7566155293025076e-05 BETTER
I0314 03:45:18.770659 2374234 finetune.py:68] layer 5_k @ epoch 0 new loss 2.6959874958265573e-05 old loss 2.8806605769204907e-05 BETTER
I0314 03:45:19.635686 2375091 finetune.py:68] layer 6_q @ epoch 3 new loss 3.4743588912533596e-05 old loss 3.578600808396004e-05 BETTER
I0314 03:45:22.858951 2375996 finetune.py:68] layer 7_q @ epoch 1 new loss 4.968539360561408e-05 old loss 5.203440741752274e-05 BETTER
I0314 03:45:29.289412 2373291 finetune.py:68] layer 4_k @ epoch 2 new loss 1.6655079889460467e-05 old loss 1.706284638203215e-05 BETTER
I0314 03:45:51.245317 2374234 finetune.py:68] layer 5_k @ epoch 1 new loss 2.632711039041169e-05 old loss 2.6959874958265573e-05 BETTER
I0314 03:45:52.584914 2375091 finetune.py:68] layer 6_q @ epoch 4 new loss 3.3860218536574394e-05 old loss 3.4743588912533596e-05 BETTER
I0314 03:45:55.014776 2375996 finetune.py:68] layer 7_q @ epoch 2 new loss 4.802348848897964e-05 old loss 4.968539360561408e-05 BETTER
I0314 03:46:03.211749 2373291 finetune.py:68] layer 4_k @ epoch 3 new loss 1.6309379134327173e-05 old loss 1.6655079889460467e-05 BETTER
I0314 03:46:03.415715 2375091 finetune.py:45] layer 6_k initial loss 4.243662260705605e-05
I0314 03:46:23.507526 2374234 finetune.py:68] layer 5_k @ epoch 2 new loss 2.5808783902903087e-05 old loss 2.632711039041169e-05 BETTER
I0314 03:46:27.044140 2375996 finetune.py:68] layer 7_q @ epoch 3 new loss 4.671953502111137e-05 old loss 4.802348848897964e-05 BETTER
I0314 03:46:34.912128 2375091 finetune.py:68] layer 6_k @ epoch 0 new loss 3.918963193427771e-05 old loss 4.243662260705605e-05 BETTER
I0314 03:46:37.308236 2373291 finetune.py:68] layer 4_k @ epoch 4 new loss 1.6005478755687363e-05 old loss 1.6309379134327173e-05 BETTER
I0314 03:46:48.153366 2373291 finetune.py:45] layer 4_o initial loss 3.916401692549698e-05
I0314 03:46:55.675113 2374234 finetune.py:68] layer 5_k @ epoch 3 new loss 2.5366251065861434e-05 old loss 2.5808783902903087e-05 BETTER
I0314 03:46:59.095437 2375996 finetune.py:68] layer 7_q @ epoch 4 new loss 4.5647444494534284e-05 old loss 4.671953502111137e-05 BETTER
I0314 03:47:07.023420 2375091 finetune.py:68] layer 6_k @ epoch 1 new loss 3.8306432543322444e-05 old loss 3.918963193427771e-05 BETTER
I0314 03:47:09.227789 2375996 finetune.py:45] layer 7_k initial loss 5.748807961936109e-05
I0314 03:47:20.411530 2373291 finetune.py:68] layer 4_o @ epoch 0 new loss 3.745242065633647e-05 old loss 3.916401692549698e-05 BETTER
I0314 03:47:27.785211 2374234 finetune.py:68] layer 5_k @ epoch 4 new loss 2.498365574865602e-05 old loss 2.5366251065861434e-05 BETTER
I0314 03:47:38.036350 2374234 finetune.py:45] layer 5_o initial loss 7.095068576745689e-05
I0314 03:47:39.151940 2375091 finetune.py:68] layer 6_k @ epoch 2 new loss 3.760406980291009e-05 old loss 3.8306432543322444e-05 BETTER
I0314 03:47:39.989442 2375996 finetune.py:68] layer 7_k @ epoch 0 new loss 5.382377639762126e-05 old loss 5.748807961936109e-05 BETTER
I0314 03:47:53.450124 2373291 finetune.py:68] layer 4_o @ epoch 1 new loss 3.6634603020502254e-05 old loss 3.745242065633647e-05 BETTER
I0314 03:48:08.626603 2374234 finetune.py:68] layer 5_o @ epoch 0 new loss 6.696634227409959e-05 old loss 7.095068576745689e-05 BETTER
I0314 03:48:11.203451 2375091 finetune.py:68] layer 6_k @ epoch 3 new loss 3.7000416341470554e-05 old loss 3.760406980291009e-05 BETTER
I0314 03:48:11.589998 2375996 finetune.py:68] layer 7_k @ epoch 1 new loss 5.270912515698001e-05 old loss 5.382377639762126e-05 BETTER
I0314 03:48:26.666205 2373291 finetune.py:68] layer 4_o @ epoch 2 new loss 3.606832615332678e-05 old loss 3.6634603020502254e-05 BETTER
I0314 03:48:40.115025 2374234 finetune.py:68] layer 5_o @ epoch 1 new loss 6.481229502242059e-05 old loss 6.696634227409959e-05 BETTER
I0314 03:48:43.252478 2375091 finetune.py:68] layer 6_k @ epoch 4 new loss 3.647897392511368e-05 old loss 3.7000416341470554e-05 BETTER
I0314 03:48:43.361233 2375996 finetune.py:68] layer 7_k @ epoch 2 new loss 5.1835639169439673e-05 old loss 5.270912515698001e-05 BETTER
I0314 03:48:53.307919 2375091 finetune.py:45] layer 6_o initial loss 9.394037624588236e-05
I0314 03:48:59.743269 2373291 finetune.py:68] layer 4_o @ epoch 3 new loss 3.560450568329543e-05 old loss 3.606832615332678e-05 BETTER
I0314 03:49:11.650230 2374234 finetune.py:68] layer 5_o @ epoch 2 new loss 6.322253466350958e-05 old loss 6.481229502242059e-05 BETTER
I0314 03:49:14.986401 2375996 finetune.py:68] layer 7_k @ epoch 3 new loss 5.1100319979013875e-05 old loss 5.1835639169439673e-05 BETTER
I0314 03:49:24.097407 2375091 finetune.py:68] layer 6_o @ epoch 0 new loss 8.785319369053468e-05 old loss 9.394037624588236e-05 BETTER
I0314 03:49:33.289392 2373291 finetune.py:68] layer 4_o @ epoch 4 new loss 3.5206412576371804e-05 old loss 3.560450568329543e-05 BETTER
I0314 03:49:43.060751 2374234 finetune.py:68] layer 5_o @ epoch 3 new loss 6.191938882693648e-05 old loss 6.322253466350958e-05 BETTER
I0314 03:49:46.578293 2375996 finetune.py:68] layer 7_k @ epoch 4 new loss 5.044675344834104e-05 old loss 5.1100319979013875e-05 BETTER
I0314 03:49:50.098292 2373291 finetune.py:45] layer 4_up initial loss 5.7095199736068025e-05
I0314 03:49:55.509238 2375091 finetune.py:68] layer 6_o @ epoch 1 new loss 8.527001045877114e-05 old loss 8.785319369053468e-05 BETTER
I0314 03:49:56.952309 2375996 finetune.py:45] layer 7_o initial loss 0.00012957990111317486
I0314 03:50:14.490199 2374234 finetune.py:68] layer 5_o @ epoch 4 new loss 6.0818532801931724e-05 old loss 6.191938882693648e-05 BETTER
I0314 03:50:20.431299 2373291 finetune.py:68] layer 4_up @ epoch 0 new loss 5.6275584938703105e-05 old loss 5.7095199736068025e-05 BETTER
I0314 03:50:27.158404 2375091 finetune.py:68] layer 6_o @ epoch 2 new loss 8.344188972841948e-05 old loss 8.527001045877114e-05 BETTER
I0314 03:50:27.285393 2375996 finetune.py:68] layer 7_o @ epoch 0 new loss 0.00011984677257714793 old loss 0.00012957990111317486 BETTER
I0314 03:50:31.741765 2374234 finetune.py:45] layer 5_up initial loss 9.563604544382542e-05
I0314 03:50:51.871684 2373291 finetune.py:68] layer 4_up @ epoch 1 new loss 5.57435596419964e-05 old loss 5.6275584938703105e-05 BETTER
I0314 03:50:58.195033 2375996 finetune.py:68] layer 7_o @ epoch 1 new loss 0.00011566605826374143 old loss 0.00011984677257714793 BETTER
I0314 03:50:58.760506 2375091 finetune.py:68] layer 6_o @ epoch 3 new loss 8.19947017589584e-05 old loss 8.344188972841948e-05 BETTER
I0314 03:51:00.573159 2374234 finetune.py:68] layer 5_up @ epoch 0 new loss 9.394492371939123e-05 old loss 9.563604544382542e-05 BETTER
I0314 03:51:23.365525 2373291 finetune.py:68] layer 4_up @ epoch 2 new loss 5.531950591830537e-05 old loss 5.57435596419964e-05 BETTER
I0314 03:51:29.367723 2375996 finetune.py:68] layer 7_o @ epoch 2 new loss 0.00011275407450739294 old loss 0.00011566605826374143 BETTER
I0314 03:51:30.344658 2374234 finetune.py:68] layer 5_up @ epoch 1 new loss 9.281275561079383e-05 old loss 9.394492371939123e-05 BETTER
I0314 03:51:30.639790 2375091 finetune.py:68] layer 6_o @ epoch 4 new loss 8.080695261014625e-05 old loss 8.19947017589584e-05 BETTER
I0314 03:51:46.908196 2375091 finetune.py:45] layer 6_up initial loss 0.0001344567717751488
I0314 03:51:54.843759 2373291 finetune.py:68] layer 4_up @ epoch 3 new loss 5.494393553817645e-05 old loss 5.531950591830537e-05 BETTER
I0314 03:51:59.876972 2374234 finetune.py:68] layer 5_up @ epoch 2 new loss 9.185951785184443e-05 old loss 9.281275561079383e-05 BETTER
I0314 03:52:00.254376 2375996 finetune.py:68] layer 7_o @ epoch 3 new loss 0.00011053194612031803 old loss 0.00011275407450739294 BETTER
I0314 03:52:15.663389 2375091 finetune.py:68] layer 6_up @ epoch 0 new loss 0.00013162025425117463 old loss 0.0001344567717751488 BETTER
I0314 03:52:26.374152 2373291 finetune.py:68] layer 4_up @ epoch 4 new loss 5.460171450977214e-05 old loss 5.494393553817645e-05 BETTER
I0314 03:52:29.566965 2374234 finetune.py:68] layer 5_up @ epoch 3 new loss 9.101301111513749e-05 old loss 9.185951785184443e-05 BETTER
I0314 03:52:31.042657 2375996 finetune.py:68] layer 7_o @ epoch 4 new loss 0.00010870755795622244 old loss 0.00011053194612031803 BETTER
I0314 03:52:43.734898 2373291 finetune.py:45] layer 4_gate initial loss 6.895436672493815e-05
I0314 03:52:45.355895 2375091 finetune.py:68] layer 6_up @ epoch 1 new loss 0.0001300094445468858 old loss 0.00013162025425117463 BETTER
I0314 03:52:48.227956 2375996 finetune.py:45] layer 7_up initial loss 0.00018257321789860725
I0314 03:52:59.198788 2374234 finetune.py:68] layer 5_up @ epoch 4 new loss 9.024784958455712e-05 old loss 9.101301111513749e-05 BETTER
I0314 03:53:12.698128 2373291 finetune.py:68] layer 4_gate @ epoch 0 new loss 6.84873157297261e-05 old loss 6.895436672493815e-05 BETTER
I0314 03:53:15.221860 2375091 finetune.py:68] layer 6_up @ epoch 2 new loss 0.00012867322948295623 old loss 0.0001300094445468858 BETTER
I0314 03:53:16.123066 2374234 finetune.py:45] layer 5_gate initial loss 0.00011182629532413557
I0314 03:53:16.849921 2375996 finetune.py:68] layer 7_up @ epoch 0 new loss 0.00017784473311621696 old loss 0.00018257321789860725 BETTER
I0314 03:53:42.742887 2373291 finetune.py:68] layer 4_gate @ epoch 1 new loss 6.812674837419763e-05 old loss 6.84873157297261e-05 BETTER
I0314 03:53:43.639591 2374234 finetune.py:68] layer 5_gate @ epoch 0 new loss 0.00011095147783635184 old loss 0.00011182629532413557 BETTER
I0314 03:53:45.344605 2375091 finetune.py:68] layer 6_up @ epoch 3 new loss 0.00012751376198139042 old loss 0.00012867322948295623 BETTER
I0314 03:53:46.432800 2375996 finetune.py:68] layer 7_up @ epoch 1 new loss 0.0001751738163875416 old loss 0.00017784473311621696 BETTER
I0314 03:54:12.086022 2374234 finetune.py:68] layer 5_gate @ epoch 1 new loss 0.00011025383719243109 old loss 0.00011095147783635184 BETTER
I0314 03:54:12.843972 2373291 finetune.py:68] layer 4_gate @ epoch 2 new loss 6.781988486181945e-05 old loss 6.812674837419763e-05 BETTER
I0314 03:54:15.417953 2375091 finetune.py:68] layer 6_up @ epoch 4 new loss 0.00012647974654100835 old loss 0.00012751376198139042 BETTER
I0314 03:54:15.935382 2375996 finetune.py:68] layer 7_up @ epoch 2 new loss 0.00017307089001405984 old loss 0.0001751738163875416 BETTER
I0314 03:54:32.374521 2375091 finetune.py:45] layer 6_gate initial loss 0.00015682712546549737
I0314 03:54:40.445631 2374234 finetune.py:68] layer 5_gate @ epoch 2 new loss 0.0001096488194889389 old loss 0.00011025383719243109 BETTER
I0314 03:54:42.792684 2373291 finetune.py:68] layer 4_gate @ epoch 3 new loss 6.754686182830483e-05 old loss 6.781988486181945e-05 BETTER
I0314 03:54:45.358405 2375996 finetune.py:68] layer 7_up @ epoch 3 new loss 0.00017125607701018453 old loss 0.00017307089001405984 BETTER
I0314 03:54:59.912778 2375091 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.0001555340422783047 old loss 0.00015682712546549737 BETTER
I0314 03:55:08.684785 2374234 finetune.py:68] layer 5_gate @ epoch 3 new loss 0.00010909377306234092 old loss 0.0001096488194889389 BETTER
I0314 03:55:12.748474 2373291 finetune.py:68] layer 4_gate @ epoch 4 new loss 6.72976384521462e-05 old loss 6.754686182830483e-05 BETTER
I0314 03:55:14.735726 2375996 finetune.py:68] layer 7_up @ epoch 4 new loss 0.00016968045383691788 old loss 0.00017125607701018453 BETTER
I0314 03:55:28.061841 2375091 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.0001545453123981133 old loss 0.0001555340422783047 BETTER
I0314 03:55:31.358341 2373291 finetune.py:45] layer 4_down initial loss 0.00010556375491432846
I0314 03:55:31.606031 2375996 finetune.py:45] layer 7_gate initial loss 0.00021127892250660807
I0314 03:55:36.806630 2374234 finetune.py:68] layer 5_gate @ epoch 4 new loss 0.00010858926543733105 old loss 0.00010909377306234092 BETTER
I0314 03:55:54.761506 2374234 finetune.py:45] layer 5_down initial loss 0.0001631019258638844
I0314 03:55:56.335321 2375091 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.00015368740423582494 old loss 0.0001545453123981133 BETTER
I0314 03:55:58.619943 2375996 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.00020919600501656532 old loss 0.00021127892250660807 BETTER
I0314 03:55:58.685955 2373291 finetune.py:68] layer 4_down @ epoch 0 new loss 0.00010550153820076957 old loss 0.00010556375491432846 BETTER
I0314 03:56:20.687537 2374234 finetune.py:68] layer 5_down @ epoch 0 new loss 0.00016303711163345724 old loss 0.0001631019258638844 BETTER
I0314 03:56:25.083023 2375091 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.0001529217988718301 old loss 0.00015368740423582494 BETTER
I0314 03:56:27.262864 2375996 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.00020764315559063107 old loss 0.00020919600501656532 BETTER
I0314 03:56:27.476425 2373291 finetune.py:68] layer 4_down @ epoch 1 new loss 0.0001054700042004697 old loss 0.00010550153820076957 BETTER
I0314 03:56:48.570291 2374234 finetune.py:68] layer 5_down @ epoch 1 new loss 0.00016300517017953098 old loss 0.00016303711163345724 BETTER
I0314 03:56:55.059524 2375091 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.00015222221554722637 old loss 0.0001529217988718301 BETTER
I0314 03:56:57.121848 2375996 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.00020634040993172675 old loss 0.00020764315559063107 BETTER
I0314 03:56:57.466958 2373291 finetune.py:68] layer 4_down @ epoch 2 new loss 0.0001054515887517482 old loss 0.0001054700042004697 BETTER
I0314 03:57:18.126218 2375091 finetune.py:45] layer 6_down initial loss 0.00023467869323212653
I0314 03:57:18.278026 2374234 finetune.py:68] layer 5_down @ epoch 2 new loss 0.0001629865146242082 old loss 0.00016300517017953098 BETTER
I0314 03:57:26.909807 2375996 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.00020519352983683348 old loss 0.00020634040993172675 BETTER
I0314 03:57:27.036778 2373291 finetune.py:68] layer 4_down @ epoch 3 new loss 0.00010543990356381983 old loss 0.0001054515887517482 BETTER
I0314 03:57:44.991153 2375091 finetune.py:68] layer 6_down @ epoch 0 new loss 0.0002346005931030959 old loss 0.00023467869323212653 BETTER
I0314 03:57:46.581680 2374234 finetune.py:68] layer 5_down @ epoch 3 new loss 0.00016297405818477273 old loss 0.0001629865146242082 BETTER
I0314 03:57:57.303846 2375996 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.000204144322196953 old loss 0.00020519352983683348 BETTER
I0314 03:57:57.733123 2373291 finetune.py:68] layer 4_down @ epoch 4 new loss 0.00010543131065787748 old loss 0.00010543990356381983 BETTER
4_v proxy err 0.02675006166100502 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0014212093083187938 tr(WHW.T) 6931.59765625
4_k proxy err 0.000995297566987574 tr(WHW.T) 10445.142578125
4_o proxy err 0.01570253260433674 tr(WHW.T) 5.154862403869629
4_up proxy err 0.013433423824608326 tr(WHW.T) 397.6706237792969
4_gate proxy err 0.006841691210865974 tr(WHW.T) 820.867919921875
4_down proxy err 0.014026965014636517 tr(WHW.T) 11.6253023147583
I0314 03:58:12.948226 2375091 finetune.py:68] layer 6_down @ epoch 1 new loss 0.0002345555549254641 old loss 0.0002346005931030959 BETTER
I0314 03:58:14.398805 2374234 finetune.py:68] layer 5_down @ epoch 4 new loss 0.00016296497778967023 old loss 0.00016297405818477273 BETTER
5_v proxy err 0.0265257116407156 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0015729242004454136 tr(WHW.T) 6780.1220703125
5_k proxy err 0.0010209708707407117 tr(WHW.T) 10866.8662109375
5_o proxy err 0.022190174087882042 tr(WHW.T) 7.972754001617432
5_up proxy err 0.013189015910029411 tr(WHW.T) 506.67822265625
5_gate proxy err 0.006373519077897072 tr(WHW.T) 1105.2281494140625
5_down proxy err 0.014751180075109005 tr(WHW.T) 15.743633270263672
I0314 03:58:16.895940 2375996 finetune.py:45] layer 7_down initial loss 0.00031542626675218344
I0314 03:58:39.723269 2375091 finetune.py:68] layer 6_down @ epoch 2 new loss 0.0002345271350350231 old loss 0.0002345555549254641 BETTER
I0314 03:58:41.967159 2375996 finetune.py:68] layer 7_down @ epoch 0 new loss 0.0003153306897729635 old loss 0.00031542626675218344 BETTER
I0314 03:59:06.435138 2375091 finetune.py:68] layer 6_down @ epoch 3 new loss 0.000234508202993311 old loss 0.0002345271350350231 BETTER
I0314 03:59:07.997857 2375996 finetune.py:68] layer 7_down @ epoch 1 new loss 0.00031527344253845513 old loss 0.0003153306897729635 BETTER
I0314 03:59:29.921513 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 8 in 71.58056616783142s
I0314 03:59:33.267668 2387577 config.py:54] PyTorch version 2.1.1 available.
I0314 03:59:33.345328 2375091 finetune.py:68] layer 6_down @ epoch 4 new loss 0.0002344943641219288 old loss 0.000234508202993311 BETTER
I0314 03:59:34.008748 2375996 finetune.py:68] layer 7_down @ epoch 2 new loss 0.0003152367426082492 old loss 0.00031527344253845513 BETTER
I0314 03:59:34.254142 2357215 quantize_finetune_llama.py:191] layer 9 gpu 1
I0314 03:59:34.312566 2387577 data_utils.py:336] using 256 training seqs, 128 validation seqs
6_v proxy err 0.025919219478964806 tr(WHW.T) 443.5464782714844
6_q proxy err 0.0019813990220427513 tr(WHW.T) 7583.67041015625
6_k proxy err 0.001470469986088574 tr(WHW.T) 10436.04296875
6_o proxy err 0.0204171109944582 tr(WHW.T) 11.630277633666992
6_up proxy err 0.013212458230555058 tr(WHW.T) 617.3934326171875
6_gate proxy err 0.005590371787548065 tr(WHW.T) 1555.07421875
6_down proxy err 0.015222038142383099 tr(WHW.T) 23.128158569335938
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 03:59:43.298438 2387577 finetune.py:45] layer 8_v initial loss 0.00019245523435529321
I0314 03:59:59.986575 2375996 finetune.py:68] layer 7_down @ epoch 3 new loss 0.000315213983412832 old loss 0.0003152367426082492 BETTER
I0314 04:00:16.164936 2387577 finetune.py:68] layer 8_v @ epoch 0 new loss 9.782581037143245e-05 old loss 0.00019245523435529321 BETTER
I0314 04:00:26.055339 2375996 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0003151963755954057 old loss 0.000315213983412832 BETTER
7_v proxy err 0.02547689713537693 tr(WHW.T) 489.9357604980469
7_q proxy err 0.002091931411996484 tr(WHW.T) 7679.64306640625
7_k proxy err 0.0016039416659623384 tr(WHW.T) 10224.46875
7_o proxy err 0.023240597918629646 tr(WHW.T) 15.229231834411621
7_up proxy err 0.013028738088905811 tr(WHW.T) 736.1842041015625
7_gate proxy err 0.005452319514006376 tr(WHW.T) 1877.53515625
7_down proxy err 0.015410510823130608 tr(WHW.T) 30.78579330444336
I0314 04:00:45.513205 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 9 in 68.6995017528534s
I0314 04:00:48.568057 2388486 config.py:54] PyTorch version 2.1.1 available.
I0314 04:00:49.539683 2357215 quantize_finetune_llama.py:191] layer 10 gpu 2
I0314 04:00:49.595825 2388486 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 04:00:50.613312 2387577 finetune.py:68] layer 8_v @ epoch 1 new loss 8.03322036517784e-05 old loss 9.782581037143245e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:00:58.234347 2388486 finetune.py:45] layer 9_v initial loss 0.0002155496331397444
I0314 04:01:25.144363 2387577 finetune.py:68] layer 8_v @ epoch 2 new loss 7.253425428643823e-05 old loss 8.03322036517784e-05 BETTER
I0314 04:01:29.751656 2388486 finetune.py:68] layer 9_v @ epoch 0 new loss 0.0001111058154492639 old loss 0.0002155496331397444 BETTER
I0314 04:01:57.810447 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 10 in 67.87807631492615s
I0314 04:01:59.832522 2387577 finetune.py:68] layer 8_v @ epoch 3 new loss 6.77465068292804e-05 old loss 7.253425428643823e-05 BETTER
I0314 04:02:01.084837 2389328 config.py:54] PyTorch version 2.1.1 available.
I0314 04:02:01.602520 2388486 finetune.py:68] layer 9_v @ epoch 1 new loss 9.225845860783011e-05 old loss 0.0001111058154492639 BETTER
I0314 04:02:02.092035 2357215 quantize_finetune_llama.py:191] layer 11 gpu 3
I0314 04:02:02.157668 2389328 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:02:10.981822 2389328 finetune.py:45] layer 10_v initial loss 0.00028579289210028946
I0314 04:02:33.955675 2388486 finetune.py:68] layer 9_v @ epoch 2 new loss 8.393496682401747e-05 old loss 9.225845860783011e-05 BETTER
I0314 04:02:34.529331 2387577 finetune.py:68] layer 8_v @ epoch 4 new loss 6.44346873741597e-05 old loss 6.77465068292804e-05 BETTER
I0314 04:02:42.448049 2389328 finetune.py:68] layer 10_v @ epoch 0 new loss 0.00015396343951579183 old loss 0.00028579289210028946 BETTER
I0314 04:02:44.699286 2387577 finetune.py:45] layer 8_q initial loss 8.770635031396523e-05
I0314 04:03:06.706550 2388486 finetune.py:68] layer 9_v @ epoch 3 new loss 7.885524246376008e-05 old loss 8.393496682401747e-05 BETTER
I0314 04:03:11.448138 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 11 in 68.93445038795471s
I0314 04:03:14.710901 2390213 config.py:54] PyTorch version 2.1.1 available.
I0314 04:03:14.894347 2389328 finetune.py:68] layer 10_v @ epoch 1 new loss 0.00012915433035232127 old loss 0.00015396343951579183 BETTER
I0314 04:03:15.736767 2357215 quantize_finetune_llama.py:191] layer 12 gpu 0
I0314 04:03:15.793331 2390213 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 04:03:18.155472 2387577 finetune.py:68] layer 8_q @ epoch 0 new loss 7.43305790820159e-05 old loss 8.770635031396523e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:03:24.588840 2390213 finetune.py:45] layer 11_v initial loss 0.0003209235437680036
I0314 04:03:39.529397 2388486 finetune.py:68] layer 9_v @ epoch 4 new loss 7.528396236011758e-05 old loss 7.885524246376008e-05 BETTER
I0314 04:03:47.627238 2389328 finetune.py:68] layer 10_v @ epoch 2 new loss 0.0001175603101728484 old loss 0.00012915433035232127 BETTER
I0314 04:03:50.252732 2388486 finetune.py:45] layer 9_q initial loss 9.818733087740839e-05
I0314 04:03:52.592607 2387577 finetune.py:68] layer 8_q @ epoch 1 new loss 7.080283830873668e-05 old loss 7.43305790820159e-05 BETTER
I0314 04:03:55.655569 2390213 finetune.py:68] layer 11_v @ epoch 0 new loss 0.00016672337369527668 old loss 0.0003209235437680036 BETTER
I0314 04:04:20.811677 2389328 finetune.py:68] layer 10_v @ epoch 3 new loss 0.00011038492084480822 old loss 0.0001175603101728484 BETTER
I0314 04:04:21.803019 2388486 finetune.py:68] layer 9_q @ epoch 0 new loss 8.681017789058387e-05 old loss 9.818733087740839e-05 BETTER
I0314 04:04:27.313832 2387577 finetune.py:68] layer 8_q @ epoch 2 new loss 6.83210018905811e-05 old loss 7.080283830873668e-05 BETTER
I0314 04:04:27.682655 2390213 finetune.py:68] layer 11_v @ epoch 1 new loss 0.00013876215962227434 old loss 0.00016672337369527668 BETTER
I0314 04:04:53.966911 2389328 finetune.py:68] layer 10_v @ epoch 4 new loss 0.00010535644832998514 old loss 0.00011038492084480822 BETTER
I0314 04:04:54.233934 2388486 finetune.py:68] layer 9_q @ epoch 1 new loss 8.307833195431158e-05 old loss 8.681017789058387e-05 BETTER
I0314 04:04:59.918469 2390213 finetune.py:68] layer 11_v @ epoch 2 new loss 0.000125912920339033 old loss 0.00013876215962227434 BETTER
I0314 04:05:02.056653 2387577 finetune.py:68] layer 8_q @ epoch 3 new loss 6.635439058300108e-05 old loss 6.83210018905811e-05 BETTER
I0314 04:05:04.661638 2389328 finetune.py:45] layer 10_q initial loss 0.00013310182839632034
I0314 04:05:26.763584 2388486 finetune.py:68] layer 9_q @ epoch 2 new loss 8.035856444621459e-05 old loss 8.307833195431158e-05 BETTER
I0314 04:05:32.038666 2390213 finetune.py:68] layer 11_v @ epoch 3 new loss 0.00011814526078524068 old loss 0.000125912920339033 BETTER
I0314 04:05:36.092913 2389328 finetune.py:68] layer 10_q @ epoch 0 new loss 0.00011849787551909685 old loss 0.00013310182839632034 BETTER
I0314 04:05:36.670312 2387577 finetune.py:68] layer 8_q @ epoch 4 new loss 6.476035923697054e-05 old loss 6.635439058300108e-05 BETTER
I0314 04:05:47.224840 2387577 finetune.py:45] layer 8_k initial loss 8.26697942102328e-05
I0314 04:05:59.413973 2388486 finetune.py:68] layer 9_q @ epoch 3 new loss 7.824390195310116e-05 old loss 8.035856444621459e-05 BETTER
I0314 04:06:04.742643 2390213 finetune.py:68] layer 11_v @ epoch 4 new loss 0.00011267299123574048 old loss 0.00011814526078524068 BETTER
I0314 04:06:08.562819 2389328 finetune.py:68] layer 10_q @ epoch 1 new loss 0.00011348583939252421 old loss 0.00011849787551909685 BETTER
I0314 04:06:15.129692 2390213 finetune.py:45] layer 11_q initial loss 0.0001409945252817124
I0314 04:06:20.137906 2387577 finetune.py:68] layer 8_k @ epoch 0 new loss 7.46644291211851e-05 old loss 8.26697942102328e-05 BETTER
I0314 04:06:31.604484 2388486 finetune.py:68] layer 9_q @ epoch 4 new loss 7.650537736481056e-05 old loss 7.824390195310116e-05 BETTER
I0314 04:06:41.291904 2389328 finetune.py:68] layer 10_q @ epoch 2 new loss 0.00010980637307511643 old loss 0.00011348583939252421 BETTER
I0314 04:06:42.148673 2388486 finetune.py:45] layer 9_k initial loss 9.496734128333628e-05
I0314 04:06:46.501417 2390213 finetune.py:68] layer 11_q @ epoch 0 new loss 0.00012781060650013387 old loss 0.0001409945252817124 BETTER
I0314 04:06:53.960582 2387577 finetune.py:68] layer 8_k @ epoch 1 new loss 7.313948299270123e-05 old loss 7.46644291211851e-05 BETTER
I0314 04:07:13.629063 2388486 finetune.py:68] layer 9_k @ epoch 0 new loss 8.861924288794398e-05 old loss 9.496734128333628e-05 BETTER
I0314 04:07:13.803884 2389328 finetune.py:68] layer 10_q @ epoch 3 new loss 0.00010678496619220823 old loss 0.00010980637307511643 BETTER
I0314 04:07:18.452077 2390213 finetune.py:68] layer 11_q @ epoch 1 new loss 0.000122573459520936 old loss 0.00012781060650013387 BETTER
I0314 04:07:27.837104 2387577 finetune.py:68] layer 8_k @ epoch 2 new loss 7.193702913355082e-05 old loss 7.313948299270123e-05 BETTER
I0314 04:07:46.106889 2388486 finetune.py:68] layer 9_k @ epoch 1 new loss 8.693707786733285e-05 old loss 8.861924288794398e-05 BETTER
I0314 04:07:46.611441 2389328 finetune.py:68] layer 10_q @ epoch 4 new loss 0.00010433579882374033 old loss 0.00010678496619220823 BETTER
I0314 04:07:50.476109 2390213 finetune.py:68] layer 11_q @ epoch 2 new loss 0.00011867453576996922 old loss 0.000122573459520936 BETTER
I0314 04:07:57.528512 2389328 finetune.py:45] layer 10_k initial loss 0.00012750409950967878
I0314 04:08:01.644794 2387577 finetune.py:68] layer 8_k @ epoch 3 new loss 7.090117287589237e-05 old loss 7.193702913355082e-05 BETTER
I0314 04:08:19.190908 2388486 finetune.py:68] layer 9_k @ epoch 2 new loss 8.558970148442313e-05 old loss 8.693707786733285e-05 BETTER
I0314 04:08:23.060290 2390213 finetune.py:68] layer 11_q @ epoch 3 new loss 0.00011573516530916095 old loss 0.00011867453576996922 BETTER
I0314 04:08:28.999575 2389328 finetune.py:68] layer 10_k @ epoch 0 new loss 0.00011949306644964963 old loss 0.00012750409950967878 BETTER
I0314 04:08:35.758911 2387577 finetune.py:68] layer 8_k @ epoch 4 new loss 6.999921606620774e-05 old loss 7.090117287589237e-05 BETTER
I0314 04:08:46.889757 2387577 finetune.py:45] layer 8_o initial loss 0.0001894582819659263
I0314 04:08:51.837847 2388486 finetune.py:68] layer 9_k @ epoch 3 new loss 8.446140418527648e-05 old loss 8.558970148442313e-05 BETTER
I0314 04:08:55.619951 2390213 finetune.py:68] layer 11_q @ epoch 4 new loss 0.00011337411706335843 old loss 0.00011573516530916095 BETTER
I0314 04:09:01.624625 2389328 finetune.py:68] layer 10_k @ epoch 1 new loss 0.00011704058852046728 old loss 0.00011949306644964963 BETTER
I0314 04:09:08.290685 2390213 finetune.py:45] layer 11_k initial loss 0.00013869610847905278
I0314 04:09:20.284574 2387577 finetune.py:68] layer 8_o @ epoch 0 new loss 0.00017498404486104846 old loss 0.0001894582819659263 BETTER
I0314 04:09:24.518300 2388486 finetune.py:68] layer 9_k @ epoch 4 new loss 8.347426046384498e-05 old loss 8.446140418527648e-05 BETTER
I0314 04:09:33.739130 2389328 finetune.py:68] layer 10_k @ epoch 2 new loss 0.0001150730240624398 old loss 0.00011704058852046728 BETTER
I0314 04:09:35.283439 2388486 finetune.py:45] layer 9_o initial loss 0.00023143550788518041
I0314 04:09:39.031261 2390213 finetune.py:68] layer 11_k @ epoch 0 new loss 0.00013061598292551935 old loss 0.00013869610847905278 BETTER
I0314 04:09:53.719575 2387577 finetune.py:68] layer 8_o @ epoch 1 new loss 0.0001683246373431757 old loss 0.00017498404486104846 BETTER
I0314 04:10:05.915031 2388486 finetune.py:68] layer 9_o @ epoch 0 new loss 0.0002145918842870742 old loss 0.00023143550788518041 BETTER
I0314 04:10:05.929778 2389328 finetune.py:68] layer 10_k @ epoch 3 new loss 0.00011342234211042523 old loss 0.0001150730240624398 BETTER
I0314 04:10:10.539228 2390213 finetune.py:68] layer 11_k @ epoch 1 new loss 0.00012833652726840228 old loss 0.00013061598292551935 BETTER
I0314 04:10:27.080158 2387577 finetune.py:68] layer 8_o @ epoch 2 new loss 0.00016370389494113624 old loss 0.0001683246373431757 BETTER
I0314 04:10:37.306074 2388486 finetune.py:68] layer 9_o @ epoch 1 new loss 0.00020609739294741303 old loss 0.0002145918842870742 BETTER
I0314 04:10:37.979285 2389328 finetune.py:68] layer 10_k @ epoch 4 new loss 0.00011195429397048429 old loss 0.00011342234211042523 BETTER
I0314 04:10:42.024408 2390213 finetune.py:68] layer 11_k @ epoch 2 new loss 0.00012649538984987885 old loss 0.00012833652726840228 BETTER
I0314 04:10:48.453932 2389328 finetune.py:45] layer 10_o initial loss 0.0003160751366522163
I0314 04:11:00.166365 2387577 finetune.py:68] layer 8_o @ epoch 3 new loss 0.00016016355948522687 old loss 0.00016370389494113624 BETTER
I0314 04:11:08.834704 2388486 finetune.py:68] layer 9_o @ epoch 2 new loss 0.00020024864352308214 old loss 0.00020609739294741303 BETTER
I0314 04:11:13.769458 2390213 finetune.py:68] layer 11_k @ epoch 3 new loss 0.00012491547386161983 old loss 0.00012649538984987885 BETTER
I0314 04:11:19.276188 2389328 finetune.py:68] layer 10_o @ epoch 0 new loss 0.0002937210665550083 old loss 0.0003160751366522163 BETTER
I0314 04:11:33.547466 2387577 finetune.py:68] layer 8_o @ epoch 4 new loss 0.00015728732978459448 old loss 0.00016016355948522687 BETTER
I0314 04:11:40.386528 2388486 finetune.py:68] layer 9_o @ epoch 3 new loss 0.00019574706675484776 old loss 0.00020024864352308214 BETTER
I0314 04:11:45.227730 2390213 finetune.py:68] layer 11_k @ epoch 4 new loss 0.00012348855671007186 old loss 0.00012491547386161983 BETTER
I0314 04:11:50.759428 2387577 finetune.py:45] layer 8_up initial loss 0.0002462079864926636
I0314 04:11:50.916518 2389328 finetune.py:68] layer 10_o @ epoch 1 new loss 0.0002824841649271548 old loss 0.0002937210665550083 BETTER
I0314 04:11:55.996234 2390213 finetune.py:45] layer 11_o initial loss 0.00033429311588406563
I0314 04:12:11.756222 2388486 finetune.py:68] layer 9_o @ epoch 4 new loss 0.00019212848565075547 old loss 0.00019574706675484776 BETTER
I0314 04:12:21.160980 2387577 finetune.py:68] layer 8_up @ epoch 0 new loss 0.00023943257110659033 old loss 0.0002462079864926636 BETTER
I0314 04:12:22.525677 2389328 finetune.py:68] layer 10_o @ epoch 2 new loss 0.0002744628582149744 old loss 0.0002824841649271548 BETTER
I0314 04:12:25.959685 2390213 finetune.py:68] layer 11_o @ epoch 0 new loss 0.0003101920592598617 old loss 0.00033429311588406563 BETTER
I0314 04:12:28.478570 2388486 finetune.py:45] layer 9_up initial loss 0.00029559878748841584
I0314 04:12:52.554763 2387577 finetune.py:68] layer 8_up @ epoch 1 new loss 0.00023564671573694795 old loss 0.00023943257110659033 BETTER
I0314 04:12:53.942861 2389328 finetune.py:68] layer 10_o @ epoch 3 new loss 0.0002681924670469016 old loss 0.0002744628582149744 BETTER
I0314 04:12:56.756993 2390213 finetune.py:68] layer 11_o @ epoch 1 new loss 0.00029866231488995254 old loss 0.0003101920592598617 BETTER
I0314 04:12:57.340937 2388486 finetune.py:68] layer 9_up @ epoch 0 new loss 0.00028760326677002013 old loss 0.00029559878748841584 BETTER
I0314 04:13:24.254911 2387577 finetune.py:68] layer 8_up @ epoch 2 new loss 0.00023264512128662318 old loss 0.00023564671573694795 BETTER
I0314 04:13:25.708281 2389328 finetune.py:68] layer 10_o @ epoch 4 new loss 0.00026309204986318946 old loss 0.0002681924670469016 BETTER
I0314 04:13:27.112253 2388486 finetune.py:68] layer 9_up @ epoch 1 new loss 0.00028295867377892137 old loss 0.00028760326677002013 BETTER
I0314 04:13:27.764286 2390213 finetune.py:68] layer 11_o @ epoch 2 new loss 0.00029059514054097235 old loss 0.00029866231488995254 BETTER
I0314 04:13:42.377343 2389328 finetune.py:45] layer 10_up initial loss 0.00038122868863865733
I0314 04:13:55.896507 2387577 finetune.py:68] layer 8_up @ epoch 3 new loss 0.00023010309087112546 old loss 0.00023264512128662318 BETTER
I0314 04:13:56.643433 2388486 finetune.py:68] layer 9_up @ epoch 2 new loss 0.00027928329654969275 old loss 0.00028295867377892137 BETTER
I0314 04:13:58.566817 2390213 finetune.py:68] layer 11_o @ epoch 3 new loss 0.00028445429052226245 old loss 0.00029059514054097235 BETTER
I0314 04:14:11.271674 2389328 finetune.py:68] layer 10_up @ epoch 0 new loss 0.0003706010465975851 old loss 0.00038122868863865733 BETTER
I0314 04:14:26.264231 2388486 finetune.py:68] layer 9_up @ epoch 3 new loss 0.000276173377642408 old loss 0.00027928329654969275 BETTER
I0314 04:14:27.529094 2387577 finetune.py:68] layer 8_up @ epoch 4 new loss 0.00022786266345065087 old loss 0.00023010309087112546 BETTER
I0314 04:14:29.358258 2390213 finetune.py:68] layer 11_o @ epoch 4 new loss 0.0002794003812596202 old loss 0.00028445429052226245 BETTER
I0314 04:14:40.923508 2389328 finetune.py:68] layer 10_up @ epoch 1 new loss 0.00036465725861489773 old loss 0.0003706010465975851 BETTER
I0314 04:14:44.311244 2387577 finetune.py:45] layer 8_gate initial loss 0.00028236277285031974
I0314 04:14:46.109371 2390213 finetune.py:45] layer 11_up initial loss 0.0004120571247767657
I0314 04:14:56.005751 2388486 finetune.py:68] layer 9_up @ epoch 4 new loss 0.00027345932903699577 old loss 0.000276173377642408 BETTER
I0314 04:15:10.787317 2389328 finetune.py:68] layer 10_up @ epoch 2 new loss 0.00035985768772661686 old loss 0.00036465725861489773 BETTER
I0314 04:15:12.866324 2388486 finetune.py:45] layer 9_gate initial loss 0.0003394657396711409
I0314 04:15:13.435417 2387577 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.0002793589956127107 old loss 0.00028236277285031974 BETTER
I0314 04:15:14.664356 2390213 finetune.py:68] layer 11_up @ epoch 0 new loss 0.000401495024561882 old loss 0.0004120571247767657 BETTER
I0314 04:15:40.726286 2388486 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0003358192043378949 old loss 0.0003394657396711409 BETTER
I0314 04:15:41.179302 2389328 finetune.py:68] layer 10_up @ epoch 3 new loss 0.00035576982190832496 old loss 0.00035985768772661686 BETTER
I0314 04:15:43.460438 2387577 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.0002771788276731968 old loss 0.0002793589956127107 BETTER
I0314 04:15:44.389862 2390213 finetune.py:68] layer 11_up @ epoch 1 new loss 0.0003953763225581497 old loss 0.000401495024561882 BETTER
I0314 04:16:09.142123 2388486 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.0003331955522298813 old loss 0.0003358192043378949 BETTER
I0314 04:16:11.032463 2389328 finetune.py:68] layer 10_up @ epoch 4 new loss 0.0003521936887409538 old loss 0.00035576982190832496 BETTER
I0314 04:16:13.386836 2387577 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.0002753496519289911 old loss 0.0002771788276731968 BETTER
I0314 04:16:13.913584 2390213 finetune.py:68] layer 11_up @ epoch 2 new loss 0.00039052305510267615 old loss 0.0003953763225581497 BETTER
I0314 04:16:28.566420 2389328 finetune.py:45] layer 10_gate initial loss 0.00043131926213391125
I0314 04:16:37.614773 2388486 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.00033096299739554524 old loss 0.0003331955522298813 BETTER
I0314 04:16:43.419715 2387577 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.0002737433242145926 old loss 0.0002753496519289911 BETTER
I0314 04:16:43.458803 2390213 finetune.py:68] layer 11_up @ epoch 3 new loss 0.0003863399033434689 old loss 0.00039052305510267615 BETTER
I0314 04:16:56.093397 2389328 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.0004264754825271666 old loss 0.00043131926213391125 BETTER
I0314 04:17:05.663439 2388486 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.0003289774467702955 old loss 0.00033096299739554524 BETTER
I0314 04:17:12.968900 2390213 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00038268908974714577 old loss 0.0003863399033434689 BETTER
I0314 04:17:13.447302 2387577 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.00027229863917455077 old loss 0.0002737433242145926 BETTER
I0314 04:17:24.306687 2389328 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.0004230812774039805 old loss 0.0004264754825271666 BETTER
I0314 04:17:29.653303 2390213 finetune.py:45] layer 11_gate initial loss 0.00047127174912020564
I0314 04:17:32.329541 2387577 finetune.py:45] layer 8_down initial loss 0.0004071494040545076
I0314 04:17:33.975939 2388486 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0003272203612141311 old loss 0.0003289774467702955 BETTER
I0314 04:17:52.203562 2388486 finetune.py:45] layer 9_down initial loss 0.0004839829634875059
I0314 04:17:52.665340 2389328 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.0004201469128020108 old loss 0.0004230812774039805 BETTER
I0314 04:17:56.868971 2390213 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.0004663034051191062 old loss 0.00047127174912020564 BETTER
I0314 04:17:59.544176 2387577 finetune.py:68] layer 8_down @ epoch 0 new loss 0.0004070292925462127 old loss 0.0004071494040545076 BETTER
I0314 04:18:17.742583 2388486 finetune.py:68] layer 9_down @ epoch 0 new loss 0.00048386288108304143 old loss 0.0004839829634875059 BETTER
I0314 04:18:20.977017 2389328 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.0004175762296654284 old loss 0.0004201469128020108 BETTER
I0314 04:18:24.796702 2390213 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.0004628645838238299 old loss 0.0004663034051191062 BETTER
I0314 04:18:27.916823 2387577 finetune.py:68] layer 8_down @ epoch 1 new loss 0.00040695606730878353 old loss 0.0004070292925462127 BETTER
I0314 04:18:44.716259 2388486 finetune.py:68] layer 9_down @ epoch 1 new loss 0.000483781099319458 old loss 0.00048386288108304143 BETTER
I0314 04:18:52.716352 2389328 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.00041527338908053935 old loss 0.0004175762296654284 BETTER
I0314 04:18:55.973400 2390213 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.0004598679661285132 old loss 0.0004628645838238299 BETTER
I0314 04:18:58.923483 2387577 finetune.py:68] layer 8_down @ epoch 2 new loss 0.00040690938476473093 old loss 0.00040695606730878353 BETTER
I0314 04:19:14.709577 2388486 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00048372673336416483 old loss 0.000483781099319458 BETTER
I0314 04:19:19.409861 2389328 finetune.py:45] layer 10_down initial loss 0.0005965279415249825
I0314 04:19:26.699271 2390213 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.0004572474572341889 old loss 0.0004598679661285132 BETTER
I0314 04:19:29.946210 2387577 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0004068753041792661 old loss 0.00040690938476473093 BETTER
I0314 04:19:44.282820 2388486 finetune.py:68] layer 9_down @ epoch 3 new loss 0.00048368918942287564 old loss 0.00048372673336416483 BETTER
I0314 04:19:47.768470 2389328 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0005963064031675458 old loss 0.0005965279415249825 BETTER
I0314 04:19:57.796044 2390213 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.0004549284349195659 old loss 0.0004572474572341889 BETTER
I0314 04:20:00.941339 2387577 finetune.py:68] layer 8_down @ epoch 4 new loss 0.00040685254498384893 old loss 0.0004068753041792661 BETTER
8_v proxy err 0.024247094988822937 tr(WHW.T) 530.9967041015625
8_q proxy err 0.0022520150523632765 tr(WHW.T) 7238.44921875
8_k proxy err 0.0015747530851513147 tr(WHW.T) 10667.326171875
8_o proxy err 0.026385270059108734 tr(WHW.T) 20.24341583251953
8_up proxy err 0.012034316547214985 tr(WHW.T) 866.7197875976562
8_gate proxy err 0.005583036690950394 tr(WHW.T) 1971.4783935546875
8_down proxy err 0.015330277383327484 tr(WHW.T) 37.469242095947266
I0314 04:20:12.796171 2388486 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00048365973634645343 old loss 0.00048368918942287564 BETTER
9_v proxy err 0.022587226703763008 tr(WHW.T) 565.0663452148438
9_q proxy err 0.002342864638194442 tr(WHW.T) 6977.77978515625
9_k proxy err 0.0015339296078309417 tr(WHW.T) 11018.1708984375
9_o proxy err 0.026249626651406288 tr(WHW.T) 25.8564395904541
9_up proxy err 0.011625871062278748 tr(WHW.T) 970.8832397460938
9_gate proxy err 0.0055480883456766605 tr(WHW.T) 2133.584228515625
9_down proxy err 0.01538433413952589 tr(WHW.T) 43.29253005981445
I0314 04:20:15.715643 2389328 finetune.py:68] layer 10_down @ epoch 1 new loss 0.00059616903308779 old loss 0.0005963064031675458 BETTER
I0314 04:20:20.085126 2390213 finetune.py:45] layer 11_down initial loss 0.0006551051628775895
I0314 04:20:42.517545 2389328 finetune.py:68] layer 10_down @ epoch 2 new loss 0.0005960742710158229 old loss 0.00059616903308779 BETTER
I0314 04:20:45.142616 2390213 finetune.py:68] layer 11_down @ epoch 0 new loss 0.000654908362776041 old loss 0.0006551051628775895 BETTER
I0314 04:21:09.305110 2389328 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0005960094858892262 old loss 0.0005960742710158229 BETTER
I0314 04:21:11.291350 2390213 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0006547703524120152 old loss 0.000654908362776041 BETTER
I0314 04:21:30.059617 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 12 in 72.91609907150269s
I0314 04:21:33.508857 2398708 config.py:54] PyTorch version 2.1.1 available.
I0314 04:21:34.682393 2357215 quantize_finetune_llama.py:191] layer 13 gpu 1
I0314 04:21:34.742741 2398708 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 04:21:36.135780 2389328 finetune.py:68] layer 10_down @ epoch 4 new loss 0.0005959615809842944 old loss 0.0005960094858892262 BETTER
I0314 04:21:37.507709 2390213 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0006546720978803933 old loss 0.0006547703524120152 BETTER
10_v proxy err 0.02267281338572502 tr(WHW.T) 578.807373046875
10_q proxy err 0.0024064313620328903 tr(WHW.T) 6922.7431640625
10_k proxy err 0.0015813192585483193 tr(WHW.T) 11030.51953125
10_o proxy err 0.02689572051167488 tr(WHW.T) 35.47556686401367
10_up proxy err 0.010995668359100819 tr(WHW.T) 1079.9061279296875
10_gate proxy err 0.005489001981914043 tr(WHW.T) 2261.471435546875
10_down proxy err 0.014675965532660484 tr(WHW.T) 52.73909378051758
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:21:44.589569 2398708 finetune.py:45] layer 12_v initial loss 0.000325297616655007
I0314 04:22:04.059736 2390213 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0006545985233969986 old loss 0.0006546720978803933 BETTER
I0314 04:22:17.571812 2398708 finetune.py:68] layer 12_v @ epoch 0 new loss 0.00017296460282523185 old loss 0.000325297616655007 BETTER
I0314 04:22:30.336195 2390213 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0006545439246110618 old loss 0.0006545985233969986 BETTER
11_v proxy err 0.021599337458610535 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0027273516170680523 tr(WHW.T) 7034.26904296875
11_k proxy err 0.0018670331919565797 tr(WHW.T) 10543.7099609375
11_o proxy err 0.027139823883771896 tr(WHW.T) 37.002113342285156
11_up proxy err 0.011192113161087036 tr(WHW.T) 1140.1219482421875
11_gate proxy err 0.005530084948986769 tr(WHW.T) 2397.84521484375
11_down proxy err 0.015046280808746815 tr(WHW.T) 56.585880279541016
I0314 04:22:49.631945 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 13 in 69.91438674926758s
I0314 04:22:51.877141 2398708 finetune.py:68] layer 12_v @ epoch 1 new loss 0.00014591868966817856 old loss 0.00017296460282523185 BETTER
I0314 04:22:52.859960 2398959 config.py:54] PyTorch version 2.1.1 available.
I0314 04:22:53.872547 2357215 quantize_finetune_llama.py:191] layer 14 gpu 2
I0314 04:22:53.936828 2398959 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:23:02.682388 2398959 finetune.py:45] layer 13_v initial loss 0.00031930190743878484
I0314 04:23:26.473948 2398708 finetune.py:68] layer 12_v @ epoch 2 new loss 0.0001336399873252958 old loss 0.00014591868966817856 BETTER
I0314 04:23:33.920031 2398959 finetune.py:68] layer 13_v @ epoch 0 new loss 0.00017050495080184191 old loss 0.00031930190743878484 BETTER
I0314 04:24:01.398166 2398708 finetune.py:68] layer 12_v @ epoch 3 new loss 0.00012596855231095105 old loss 0.0001336399873252958 BETTER
I0314 04:24:03.768610 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 14 in 69.51871943473816s
I0314 04:24:06.326557 2398959 finetune.py:68] layer 13_v @ epoch 1 new loss 0.0001462004438508302 old loss 0.00017050495080184191 BETTER
I0314 04:24:06.912755 2399195 config.py:54] PyTorch version 2.1.1 available.
I0314 04:24:07.944960 2357215 quantize_finetune_llama.py:191] layer 15 gpu 3
I0314 04:24:08.004447 2399195 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:24:16.773508 2399195 finetune.py:45] layer 14_v initial loss 0.00039941363502293825
I0314 04:24:36.093617 2398708 finetune.py:68] layer 12_v @ epoch 4 new loss 0.00012055497791152447 old loss 0.00012596855231095105 BETTER
I0314 04:24:38.643119 2398959 finetune.py:68] layer 13_v @ epoch 2 new loss 0.00013480927736964077 old loss 0.0001462004438508302 BETTER
I0314 04:24:46.414721 2398708 finetune.py:45] layer 12_q initial loss 0.00015639225603081286
I0314 04:24:48.143261 2399195 finetune.py:68] layer 14_v @ epoch 0 new loss 0.00022270237968768924 old loss 0.00039941363502293825 BETTER
I0314 04:25:11.333944 2398959 finetune.py:68] layer 13_v @ epoch 3 new loss 0.0001276266120839864 old loss 0.00013480927736964077 BETTER
I0314 04:25:17.205229 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 15 in 68.86977577209473s
I0314 04:25:19.849740 2398708 finetune.py:68] layer 12_q @ epoch 0 new loss 0.0001392413687426597 old loss 0.00015639225603081286 BETTER
I0314 04:25:20.363910 2399195 finetune.py:68] layer 14_v @ epoch 1 new loss 0.00019169731240253896 old loss 0.00022270237968768924 BETTER
I0314 04:25:20.569781 2399461 config.py:54] PyTorch version 2.1.1 available.
I0314 04:25:21.607916 2357215 quantize_finetune_llama.py:191] layer 16 gpu 0
I0314 04:25:21.664935 2399461 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:25:30.247431 2399461 finetune.py:45] layer 15_v initial loss 0.0003957828157581389
I0314 04:25:43.804921 2398959 finetune.py:68] layer 13_v @ epoch 4 new loss 0.00012246286496520042 old loss 0.0001276266120839864 BETTER
I0314 04:25:52.755391 2399195 finetune.py:68] layer 14_v @ epoch 2 new loss 0.00017653686518315226 old loss 0.00019169731240253896 BETTER
I0314 04:25:53.997875 2398959 finetune.py:45] layer 13_q initial loss 0.00015888830239418894
I0314 04:25:54.003027 2398708 finetune.py:68] layer 12_q @ epoch 1 new loss 0.00013351680536288768 old loss 0.0001392413687426597 BETTER
I0314 04:26:01.202511 2399461 finetune.py:68] layer 15_v @ epoch 0 new loss 0.0002161439770134166 old loss 0.0003957828157581389 BETTER
I0314 04:26:25.760487 2398959 finetune.py:68] layer 13_q @ epoch 0 new loss 0.00014081787958275527 old loss 0.00015888830239418894 BETTER
I0314 04:26:25.884758 2399195 finetune.py:68] layer 14_v @ epoch 3 new loss 0.00016684098227415234 old loss 0.00017653686518315226 BETTER
I0314 04:26:28.599105 2398708 finetune.py:68] layer 12_q @ epoch 2 new loss 0.00012939475709572434 old loss 0.00013351680536288768 BETTER
I0314 04:26:33.169568 2399461 finetune.py:68] layer 15_v @ epoch 1 new loss 0.0001873502042144537 old loss 0.0002161439770134166 BETTER
I0314 04:26:58.213595 2398959 finetune.py:68] layer 13_q @ epoch 1 new loss 0.00013539486099034548 old loss 0.00014081787958275527 BETTER
I0314 04:26:58.917091 2399195 finetune.py:68] layer 14_v @ epoch 4 new loss 0.00015993784472811967 old loss 0.00016684098227415234 BETTER
I0314 04:27:03.053459 2398708 finetune.py:68] layer 12_q @ epoch 3 new loss 0.00012617046013474464 old loss 0.00012939475709572434 BETTER
I0314 04:27:05.166471 2399461 finetune.py:68] layer 15_v @ epoch 2 new loss 0.00017319509061053395 old loss 0.0001873502042144537 BETTER
I0314 04:27:09.730716 2399195 finetune.py:45] layer 14_q initial loss 0.0001964164839591831
I0314 04:27:30.711391 2398959 finetune.py:68] layer 13_q @ epoch 2 new loss 0.0001314333057962358 old loss 0.00013539486099034548 BETTER
I0314 04:27:37.406968 2399461 finetune.py:68] layer 15_v @ epoch 3 new loss 0.00016402099572587758 old loss 0.00017319509061053395 BETTER
I0314 04:27:37.680237 2398708 finetune.py:68] layer 12_q @ epoch 4 new loss 0.0001234864757861942 old loss 0.00012617046013474464 BETTER
I0314 04:27:41.483228 2399195 finetune.py:68] layer 14_q @ epoch 0 new loss 0.0001801464386517182 old loss 0.0001964164839591831 BETTER
I0314 04:27:48.459547 2398708 finetune.py:45] layer 12_k initial loss 0.00015416265523526818
I0314 04:28:02.922974 2398959 finetune.py:68] layer 13_q @ epoch 3 new loss 0.00012830630294047296 old loss 0.0001314333057962358 BETTER
I0314 04:28:09.756226 2399461 finetune.py:68] layer 15_v @ epoch 4 new loss 0.0001572843611938879 old loss 0.00016402099572587758 BETTER
I0314 04:28:13.833944 2399195 finetune.py:68] layer 14_q @ epoch 1 new loss 0.00017312821000814438 old loss 0.0001801464386517182 BETTER
I0314 04:28:20.302356 2399461 finetune.py:45] layer 15_q initial loss 0.00020685697381850332
I0314 04:28:21.339728 2398708 finetune.py:68] layer 12_k @ epoch 0 new loss 0.00014218027354218066 old loss 0.00015416265523526818 BETTER
I0314 04:28:35.394229 2398959 finetune.py:68] layer 13_q @ epoch 4 new loss 0.00012572365812957287 old loss 0.00012830630294047296 BETTER
I0314 04:28:45.668897 2398959 finetune.py:45] layer 13_k initial loss 0.00015736797649879009
I0314 04:28:46.302657 2399195 finetune.py:68] layer 14_q @ epoch 2 new loss 0.00016796989075373858 old loss 0.00017312821000814438 BETTER
I0314 04:28:51.655866 2399461 finetune.py:68] layer 15_q @ epoch 0 new loss 0.00018166999507229775 old loss 0.00020685697381850332 BETTER
I0314 04:28:55.255831 2398708 finetune.py:68] layer 12_k @ epoch 1 new loss 0.00013951728760730475 old loss 0.00014218027354218066 BETTER
I0314 04:29:17.178483 2398959 finetune.py:68] layer 13_k @ epoch 0 new loss 0.00014481750258710235 old loss 0.00015736797649879009 BETTER
I0314 04:29:18.734755 2399195 finetune.py:68] layer 14_q @ epoch 3 new loss 0.00016379699809476733 old loss 0.00016796989075373858 BETTER
I0314 04:29:23.665426 2399461 finetune.py:68] layer 15_q @ epoch 1 new loss 0.00017425755504518747 old loss 0.00018166999507229775 BETTER
I0314 04:29:29.098183 2398708 finetune.py:68] layer 12_k @ epoch 2 new loss 0.00013742766168434173 old loss 0.00013951728760730475 BETTER
I0314 04:29:49.489992 2398959 finetune.py:68] layer 13_k @ epoch 1 new loss 0.00014242544420994818 old loss 0.00014481750258710235 BETTER
I0314 04:29:52.011049 2399195 finetune.py:68] layer 14_q @ epoch 4 new loss 0.00016036829038057476 old loss 0.00016379699809476733 BETTER
I0314 04:29:55.813081 2399461 finetune.py:68] layer 15_q @ epoch 2 new loss 0.00016873668937478215 old loss 0.00017425755504518747 BETTER
I0314 04:30:02.918512 2399195 finetune.py:45] layer 14_k initial loss 0.00019185442943125963
I0314 04:30:03.018874 2398708 finetune.py:68] layer 12_k @ epoch 3 new loss 0.00013563511311076581 old loss 0.00013742766168434173 BETTER
I0314 04:30:22.403350 2398959 finetune.py:68] layer 13_k @ epoch 2 new loss 0.00014044923591427505 old loss 0.00014242544420994818 BETTER
I0314 04:30:27.974894 2399461 finetune.py:68] layer 15_q @ epoch 3 new loss 0.00016440803301520646 old loss 0.00016873668937478215 BETTER
I0314 04:30:34.749888 2399195 finetune.py:68] layer 14_k @ epoch 0 new loss 0.0001824231439968571 old loss 0.00019185442943125963 BETTER
I0314 04:30:37.656363 2398708 finetune.py:68] layer 12_k @ epoch 4 new loss 0.00013407108781393617 old loss 0.00013563511311076581 BETTER
I0314 04:30:48.781450 2398708 finetune.py:45] layer 12_o initial loss 0.0003661095688585192
I0314 04:30:54.842676 2398959 finetune.py:68] layer 13_k @ epoch 3 new loss 0.00013876492448616773 old loss 0.00014044923591427505 BETTER
I0314 04:31:00.580783 2399461 finetune.py:68] layer 15_q @ epoch 4 new loss 0.00016079211491160095 old loss 0.00016440803301520646 BETTER
I0314 04:31:07.353228 2399195 finetune.py:68] layer 14_k @ epoch 1 new loss 0.000179246038896963 old loss 0.0001824231439968571 BETTER
I0314 04:31:12.423391 2399461 finetune.py:45] layer 15_k initial loss 0.0001954117906279862
I0314 04:31:21.477840 2398708 finetune.py:68] layer 12_o @ epoch 0 new loss 0.0003411595826037228 old loss 0.0003661095688585192 BETTER
I0314 04:31:27.040783 2398959 finetune.py:68] layer 13_k @ epoch 4 new loss 0.0001372906845062971 old loss 0.00013876492448616773 BETTER
I0314 04:31:38.232622 2398959 finetune.py:45] layer 13_o initial loss 0.00037485442589968443
I0314 04:31:40.300483 2399195 finetune.py:68] layer 14_k @ epoch 2 new loss 0.0001766079367371276 old loss 0.000179246038896963 BETTER
I0314 04:31:43.578339 2399461 finetune.py:68] layer 15_k @ epoch 0 new loss 0.00018536967399995774 old loss 0.0001954117906279862 BETTER
I0314 04:31:55.521415 2398708 finetune.py:68] layer 12_o @ epoch 1 new loss 0.0003287012514192611 old loss 0.0003411595826037228 BETTER
I0314 04:32:08.745704 2398959 finetune.py:68] layer 13_o @ epoch 0 new loss 0.0003464888723101467 old loss 0.00037485442589968443 BETTER
I0314 04:32:12.560916 2399195 finetune.py:68] layer 14_k @ epoch 3 new loss 0.00017433855100534856 old loss 0.0001766079367371276 BETTER
I0314 04:32:15.150791 2399461 finetune.py:68] layer 15_k @ epoch 1 new loss 0.00018206192180514336 old loss 0.00018536967399995774 BETTER
I0314 04:32:28.988493 2398708 finetune.py:68] layer 12_o @ epoch 2 new loss 0.0003197957994416356 old loss 0.0003287012514192611 BETTER
I0314 04:32:40.096294 2398959 finetune.py:68] layer 13_o @ epoch 1 new loss 0.0003330104809720069 old loss 0.0003464888723101467 BETTER
I0314 04:32:44.631212 2399195 finetune.py:68] layer 14_k @ epoch 4 new loss 0.00017238075088243932 old loss 0.00017433855100534856 BETTER
I0314 04:32:46.717332 2399461 finetune.py:68] layer 15_k @ epoch 2 new loss 0.00017934197967406362 old loss 0.00018206192180514336 BETTER
I0314 04:32:54.925373 2399195 finetune.py:45] layer 14_o initial loss 0.0004701444413512945
I0314 04:33:02.543901 2398708 finetune.py:68] layer 12_o @ epoch 3 new loss 0.00031296935048885643 old loss 0.0003197957994416356 BETTER
I0314 04:33:11.567636 2398959 finetune.py:68] layer 13_o @ epoch 2 new loss 0.0003235640178900212 old loss 0.0003330104809720069 BETTER
I0314 04:33:18.325913 2399461 finetune.py:68] layer 15_k @ epoch 3 new loss 0.00017706750077195466 old loss 0.00017934197967406362 BETTER
I0314 04:33:25.638166 2399195 finetune.py:68] layer 14_o @ epoch 0 new loss 0.0004384024068713188 old loss 0.0004701444413512945 BETTER
I0314 04:33:36.014002 2398708 finetune.py:68] layer 12_o @ epoch 4 new loss 0.00030752181191928685 old loss 0.00031296935048885643 BETTER
I0314 04:33:42.858036 2398959 finetune.py:68] layer 13_o @ epoch 3 new loss 0.00031631378806196153 old loss 0.0003235640178900212 BETTER
I0314 04:33:49.882934 2399461 finetune.py:68] layer 15_k @ epoch 4 new loss 0.00017504848074167967 old loss 0.00017706750077195466 BETTER
I0314 04:33:52.818472 2398708 finetune.py:45] layer 12_up initial loss 0.0004555600753519684
I0314 04:33:57.227218 2399195 finetune.py:68] layer 14_o @ epoch 1 new loss 0.0004231251950841397 old loss 0.0004384024068713188 BETTER
I0314 04:34:00.261801 2399461 finetune.py:45] layer 15_o initial loss 0.0004780369927175343
I0314 04:34:14.228193 2398959 finetune.py:68] layer 13_o @ epoch 4 new loss 0.000310515082674101 old loss 0.00031631378806196153 BETTER
I0314 04:34:23.212855 2398708 finetune.py:68] layer 12_up @ epoch 0 new loss 0.00044395681470632553 old loss 0.0004555600753519684 BETTER
I0314 04:34:28.838346 2399195 finetune.py:68] layer 14_o @ epoch 2 new loss 0.00041237848927266896 old loss 0.0004231251950841397 BETTER
I0314 04:34:30.330799 2399461 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0004384007188491523 old loss 0.0004780369927175343 BETTER
I0314 04:34:31.154028 2398959 finetune.py:45] layer 13_up initial loss 0.0004877340979874134
I0314 04:34:54.551756 2398708 finetune.py:68] layer 12_up @ epoch 1 new loss 0.0004372417170088738 old loss 0.00044395681470632553 BETTER
I0314 04:35:00.090165 2398959 finetune.py:68] layer 13_up @ epoch 0 new loss 0.000473360123578459 old loss 0.0004877340979874134 BETTER
I0314 04:35:00.387629 2399195 finetune.py:68] layer 14_o @ epoch 3 new loss 0.00040400601574219763 old loss 0.00041237848927266896 BETTER
I0314 04:35:01.060436 2399461 finetune.py:68] layer 15_o @ epoch 1 new loss 0.0004213692154735327 old loss 0.0004384007188491523 BETTER
I0314 04:35:26.164617 2398708 finetune.py:68] layer 12_up @ epoch 2 new loss 0.000431851833127439 old loss 0.0004372417170088738 BETTER
I0314 04:35:29.900706 2398959 finetune.py:68] layer 13_up @ epoch 1 new loss 0.00046530633699148893 old loss 0.000473360123578459 BETTER
I0314 04:35:32.151501 2399461 finetune.py:68] layer 15_o @ epoch 2 new loss 0.0004095664480701089 old loss 0.0004213692154735327 BETTER
I0314 04:35:32.319793 2399195 finetune.py:68] layer 14_o @ epoch 4 new loss 0.0003972520353272557 old loss 0.00040400601574219763 BETTER
I0314 04:35:48.788584 2399195 finetune.py:45] layer 14_up initial loss 0.0005931861232966185
I0314 04:35:57.872072 2398708 finetune.py:68] layer 12_up @ epoch 3 new loss 0.0004271881189197302 old loss 0.000431851833127439 BETTER
I0314 04:35:59.814389 2398959 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0004589504096657038 old loss 0.00046530633699148893 BETTER
I0314 04:36:03.055287 2399461 finetune.py:68] layer 15_o @ epoch 3 new loss 0.0004006786912214011 old loss 0.0004095664480701089 BETTER
I0314 04:36:17.798173 2399195 finetune.py:68] layer 14_up @ epoch 0 new loss 0.0005783084197901189 old loss 0.0005931861232966185 BETTER
I0314 04:36:29.607026 2398959 finetune.py:68] layer 13_up @ epoch 3 new loss 0.0004535565385594964 old loss 0.0004589504096657038 BETTER
I0314 04:36:29.614899 2398708 finetune.py:68] layer 12_up @ epoch 4 new loss 0.0004231879720464349 old loss 0.0004271881189197302 BETTER
I0314 04:36:33.994621 2399461 finetune.py:68] layer 15_o @ epoch 4 new loss 0.00039354717591777444 old loss 0.0004006786912214011 BETTER
I0314 04:36:46.484353 2398708 finetune.py:45] layer 12_gate initial loss 0.0005271972040645778
I0314 04:36:47.616374 2399195 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0005695822765119374 old loss 0.0005783084197901189 BETTER
I0314 04:36:50.729695 2399461 finetune.py:45] layer 15_up initial loss 0.000632211216725409
I0314 04:36:59.372528 2398959 finetune.py:68] layer 13_up @ epoch 4 new loss 0.000448935927124694 old loss 0.0004535565385594964 BETTER
I0314 04:37:15.944313 2398708 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.0005217123543843627 old loss 0.0005271972040645778 BETTER
I0314 04:37:16.342602 2398959 finetune.py:45] layer 13_gate initial loss 0.0005747492541559041
I0314 04:37:18.027525 2399195 finetune.py:68] layer 14_up @ epoch 2 new loss 0.0005627439823001623 old loss 0.0005695822765119374 BETTER
I0314 04:37:19.809497 2399461 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0006135136936791241 old loss 0.000632211216725409 BETTER
I0314 04:37:43.900249 2398959 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.0005679039750248194 old loss 0.0005747492541559041 BETTER
I0314 04:37:46.016690 2398708 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0005178235005587339 old loss 0.0005217123543843627 BETTER
I0314 04:37:48.075086 2399195 finetune.py:68] layer 14_up @ epoch 3 new loss 0.0005569023196585476 old loss 0.0005627439823001623 BETTER
I0314 04:37:49.469866 2399461 finetune.py:68] layer 15_up @ epoch 1 new loss 0.0006030331132933497 old loss 0.0006135136936791241 BETTER
I0314 04:38:12.254129 2398959 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0005632208776660264 old loss 0.0005679039750248194 BETTER
I0314 04:38:16.127853 2398708 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.0005144903552718461 old loss 0.0005178235005587339 BETTER
I0314 04:38:18.167353 2399195 finetune.py:68] layer 14_up @ epoch 4 new loss 0.000551785749848932 old loss 0.0005569023196585476 BETTER
I0314 04:38:18.981996 2399461 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0005947809549979866 old loss 0.0006030331132933497 BETTER
I0314 04:38:35.159998 2399195 finetune.py:45] layer 14_gate initial loss 0.0006982783088460565
I0314 04:38:40.524973 2398959 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.00055929075460881 old loss 0.0005632208776660264 BETTER
I0314 04:38:46.388018 2398708 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.0005115804960951209 old loss 0.0005144903552718461 BETTER
I0314 04:38:48.413046 2399461 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0005879536038264632 old loss 0.0005947809549979866 BETTER
I0314 04:39:02.612204 2399195 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0006908058421686292 old loss 0.0006982783088460565 BETTER
I0314 04:39:08.596149 2398959 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.0005558467237278819 old loss 0.00055929075460881 BETTER
I0314 04:39:16.563006 2398708 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.0005089843762107193 old loss 0.0005115804960951209 BETTER
I0314 04:39:17.803904 2399461 finetune.py:68] layer 15_up @ epoch 4 new loss 0.0005821514641866088 old loss 0.0005879536038264632 BETTER
I0314 04:39:30.804558 2399195 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0006855865940451622 old loss 0.0006908058421686292 BETTER
I0314 04:39:35.190268 2399461 finetune.py:45] layer 15_gate initial loss 0.0007636490627191961
I0314 04:39:35.489758 2398708 finetune.py:45] layer 12_down initial loss 0.0007383999763987958
I0314 04:39:37.200943 2398959 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0005527911707758904 old loss 0.0005558467237278819 BETTER
I0314 04:39:55.618820 2398959 finetune.py:45] layer 13_down initial loss 0.0008374078897759318
I0314 04:39:59.008915 2399195 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.0006812182837165892 old loss 0.0006855865940451622 BETTER
I0314 04:40:02.468681 2399461 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.0007541392114944756 old loss 0.0007636490627191961 BETTER
I0314 04:40:02.704992 2398708 finetune.py:68] layer 12_down @ epoch 0 new loss 0.0007382096955552697 old loss 0.0007383999763987958 BETTER
I0314 04:40:21.413676 2398959 finetune.py:68] layer 13_down @ epoch 0 new loss 0.0008371614385396242 old loss 0.0008374078897759318 BETTER
I0314 04:40:27.188214 2399195 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.0006773886852897704 old loss 0.0006812182837165892 BETTER
I0314 04:40:30.340894 2399461 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.0007477300241589546 old loss 0.0007541392114944756 BETTER
I0314 04:40:30.971733 2398708 finetune.py:68] layer 12_down @ epoch 1 new loss 0.0007380788447335362 old loss 0.0007382096955552697 BETTER
I0314 04:40:48.325014 2398959 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0008369889692403376 old loss 0.0008371614385396242 BETTER
I0314 04:40:57.856939 2399195 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0006739931995980442 old loss 0.0006773886852897704 BETTER
I0314 04:41:01.963185 2399461 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.0007424760260619223 old loss 0.0007477300241589546 BETTER
I0314 04:41:03.001476 2398708 finetune.py:68] layer 12_down @ epoch 2 new loss 0.0007379825692623854 old loss 0.0007380788447335362 BETTER
I0314 04:41:17.504853 2398959 finetune.py:68] layer 13_down @ epoch 2 new loss 0.0008368634735234082 old loss 0.0008369889692403376 BETTER
I0314 04:41:23.388428 2399195 finetune.py:45] layer 14_down initial loss 0.0010053111473098397
I0314 04:41:32.867754 2399461 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0007379186572507024 old loss 0.0007424760260619223 BETTER
I0314 04:41:33.699252 2398708 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0007379140006378293 old loss 0.0007379825692623854 BETTER
I0314 04:41:46.656425 2398959 finetune.py:68] layer 13_down @ epoch 3 new loss 0.0008367722039110959 old loss 0.0008368634735234082 BETTER
I0314 04:41:51.341484 2399195 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0010050396667793393 old loss 0.0010053111473098397 BETTER
I0314 04:42:04.072651 2399461 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0007339277653954923 old loss 0.0007379186572507024 BETTER
I0314 04:42:05.044276 2398708 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0007378631853498518 old loss 0.0007379140006378293 BETTER
12_v proxy err 0.022093219682574272 tr(WHW.T) 703.318603515625
12_q proxy err 0.0027288782875984907 tr(WHW.T) 7052.7275390625
12_k proxy err 0.0018213975708931684 tr(WHW.T) 10916.7373046875
12_o proxy err 0.027579745277762413 tr(WHW.T) 39.68062210083008
12_up proxy err 0.011099843308329582 tr(WHW.T) 1227.9261474609375
12_gate proxy err 0.005898831877857447 tr(WHW.T) 2386.78662109375
12_down proxy err 0.015032988041639328 tr(WHW.T) 64.66146087646484
I0314 04:42:15.305108 2398959 finetune.py:68] layer 13_down @ epoch 4 new loss 0.000836703460663557 old loss 0.0008367722039110959 BETTER
13_v proxy err 0.021793480962514877 tr(WHW.T) 714.5677490234375
13_q proxy err 0.002732683438807726 tr(WHW.T) 6962.576171875
13_k proxy err 0.001894485205411911 tr(WHW.T) 10462.66796875
13_o proxy err 0.024638008326292038 tr(WHW.T) 46.26448440551758
13_up proxy err 0.010703140869736671 tr(WHW.T) 1367.5008544921875
13_gate proxy err 0.00578231131657958 tr(WHW.T) 2600.906005859375
13_down proxy err 0.015076793730258942 tr(WHW.T) 80.04866027832031
I0314 04:42:19.771441 2399195 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0010048574768006802 old loss 0.0010050396667793393 BETTER
I0314 04:42:26.188511 2399461 finetune.py:45] layer 15_down initial loss 0.0011517208768054843
I0314 04:42:46.600626 2399195 finetune.py:68] layer 14_down @ epoch 2 new loss 0.001004722435027361 old loss 0.0010048574768006802 BETTER
I0314 04:42:51.116149 2399461 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0011514167999848723 old loss 0.0011517208768054843 BETTER
I0314 04:43:13.674123 2399195 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0010046320967376232 old loss 0.001004722435027361 BETTER
I0314 04:43:17.231607 2399461 finetune.py:68] layer 15_down @ epoch 1 new loss 0.0011512042256072164 old loss 0.0011514167999848723 BETTER
I0314 04:43:32.227926 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 16 in 72.53260493278503s
I0314 04:43:35.439546 2401752 config.py:54] PyTorch version 2.1.1 available.
I0314 04:43:36.514994 2357215 quantize_finetune_llama.py:191] layer 17 gpu 1
I0314 04:43:36.581446 2401752 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 04:43:40.549198 2399195 finetune.py:68] layer 14_down @ epoch 4 new loss 0.0010045637609437108 old loss 0.0010046320967376232 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
14_v proxy err 0.023100681602954865 tr(WHW.T) 706.1612548828125
14_q proxy err 0.0028106102254241705 tr(WHW.T) 7084.79833984375
14_k proxy err 0.0018237174954265356 tr(WHW.T) 11325.5908203125
14_o proxy err 0.02709921821951866 tr(WHW.T) 51.47985076904297
14_up proxy err 0.010820837691426277 tr(WHW.T) 1464.847900390625
14_gate proxy err 0.006060576997697353 tr(WHW.T) 2684.2255859375
14_down proxy err 0.015411748550832272 tr(WHW.T) 91.01891326904297
I0314 04:43:43.581389 2399461 finetune.py:68] layer 15_down @ epoch 2 new loss 0.001151057193055749 old loss 0.0011512042256072164 BETTER
I0314 04:43:46.039993 2401752 finetune.py:45] layer 16_v initial loss 0.00047739624278619885
I0314 04:44:10.174293 2399461 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0011509512551128864 old loss 0.001151057193055749 BETTER
I0314 04:44:19.165467 2401752 finetune.py:68] layer 16_v @ epoch 0 new loss 0.00027540227165445685 old loss 0.00047739624278619885 BETTER
I0314 04:44:36.623608 2399461 finetune.py:68] layer 15_down @ epoch 4 new loss 0.001150872907601297 old loss 0.0011509512551128864 BETTER
15_v proxy err 0.021225182339549065 tr(WHW.T) 762.7275390625
15_q proxy err 0.0027096469420939684 tr(WHW.T) 7256.482421875
15_k proxy err 0.0018402396235615015 tr(WHW.T) 11103.2744140625
15_o proxy err 0.023125307634472847 tr(WHW.T) 60.17587661743164
15_up proxy err 0.01053347997367382 tr(WHW.T) 1643.0064697265625
15_gate proxy err 0.006110806949436665 tr(WHW.T) 2907.95556640625
15_down proxy err 0.01538681611418724 tr(WHW.T) 115.12620544433594
I0314 04:44:53.602568 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 17 in 69.19154000282288s
I0314 04:44:53.641775 2401752 finetune.py:68] layer 16_v @ epoch 1 new loss 0.00024137229775078595 old loss 0.00027540227165445685 BETTER
I0314 04:44:56.701689 2402003 config.py:54] PyTorch version 2.1.1 available.
I0314 04:44:57.666103 2357215 quantize_finetune_llama.py:191] layer 18 gpu 2
I0314 04:44:57.720355 2402003 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:45:06.369991 2402003 finetune.py:45] layer 17_v initial loss 0.0003947251825593412
I0314 04:45:28.379262 2401752 finetune.py:68] layer 16_v @ epoch 2 new loss 0.00022386465570889413 old loss 0.00024137229775078595 BETTER
I0314 04:45:37.676862 2402003 finetune.py:68] layer 17_v @ epoch 0 new loss 0.00022526949760504067 old loss 0.0003947251825593412 BETTER
I0314 04:46:03.196294 2401752 finetune.py:68] layer 16_v @ epoch 3 new loss 0.00021236600878182799 old loss 0.00022386465570889413 BETTER
I0314 04:46:06.331800 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 18 in 68.25181269645691s
I0314 04:46:09.507396 2402224 config.py:54] PyTorch version 2.1.1 available.
I0314 04:46:09.840187 2402003 finetune.py:68] layer 17_v @ epoch 1 new loss 0.0001999420637730509 old loss 0.00022526949760504067 BETTER
I0314 04:46:10.499582 2357215 quantize_finetune_llama.py:191] layer 19 gpu 3
I0314 04:46:10.568661 2402224 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:46:19.434297 2402224 finetune.py:45] layer 18_v initial loss 0.00040681325481273234
I0314 04:46:37.878861 2401752 finetune.py:68] layer 16_v @ epoch 4 new loss 0.00020394922466948628 old loss 0.00021236600878182799 BETTER
I0314 04:46:41.994593 2402003 finetune.py:68] layer 17_v @ epoch 2 new loss 0.00018652838480193168 old loss 0.0001999420637730509 BETTER
I0314 04:46:48.246130 2401752 finetune.py:45] layer 16_q initial loss 0.0002578394487500191
I0314 04:46:50.857007 2402224 finetune.py:68] layer 18_v @ epoch 0 new loss 0.00022002728655934334 old loss 0.00040681325481273234 BETTER
I0314 04:47:14.825160 2402003 finetune.py:68] layer 17_v @ epoch 3 new loss 0.00017746131925377995 old loss 0.00018652838480193168 BETTER
I0314 04:47:20.091006 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 19 in 69.19771909713745s
I0314 04:47:21.755577 2401752 finetune.py:68] layer 16_q @ epoch 0 new loss 0.0002314088778803125 old loss 0.0002578394487500191 BETTER
I0314 04:47:23.195715 2402224 finetune.py:68] layer 18_v @ epoch 1 new loss 0.00019643532868940383 old loss 0.00022002728655934334 BETTER
I0314 04:47:23.394089 2402490 config.py:54] PyTorch version 2.1.1 available.
I0314 04:47:24.460393 2357215 quantize_finetune_llama.py:191] layer 20 gpu 0
I0314 04:47:24.520652 2402490 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 04:47:33.238409 2402490 finetune.py:45] layer 19_v initial loss 0.00038599324761889875
I0314 04:47:48.034822 2402003 finetune.py:68] layer 17_v @ epoch 4 new loss 0.00017094639770220965 old loss 0.00017746131925377995 BETTER
I0314 04:47:55.887425 2402224 finetune.py:68] layer 18_v @ epoch 2 new loss 0.00018405329319648445 old loss 0.00019643532868940383 BETTER
I0314 04:47:56.223973 2401752 finetune.py:68] layer 16_q @ epoch 1 new loss 0.0002220830210717395 old loss 0.0002314088778803125 BETTER
I0314 04:47:58.208573 2402003 finetune.py:45] layer 17_q initial loss 0.00021654815645888448
I0314 04:48:04.296283 2402490 finetune.py:68] layer 19_v @ epoch 0 new loss 0.00020912317268084735 old loss 0.00038599324761889875 BETTER
I0314 04:48:28.933632 2402224 finetune.py:68] layer 18_v @ epoch 3 new loss 0.00017598136037122458 old loss 0.00018405329319648445 BETTER
I0314 04:48:29.781190 2402003 finetune.py:68] layer 17_q @ epoch 0 new loss 0.00019674391660373658 old loss 0.00021654815645888448 BETTER
I0314 04:48:30.722527 2401752 finetune.py:68] layer 16_q @ epoch 2 new loss 0.00021544413175433874 old loss 0.0002220830210717395 BETTER
I0314 04:48:36.244392 2402490 finetune.py:68] layer 19_v @ epoch 1 new loss 0.0001869767438620329 old loss 0.00020912317268084735 BETTER
I0314 04:49:01.977741 2402224 finetune.py:68] layer 18_v @ epoch 4 new loss 0.0001700027787592262 old loss 0.00017598136037122458 BETTER
I0314 04:49:02.118990 2402003 finetune.py:68] layer 17_q @ epoch 1 new loss 0.00018931784143205732 old loss 0.00019674391660373658 BETTER
I0314 04:49:05.275505 2401752 finetune.py:68] layer 16_q @ epoch 3 new loss 0.0002101453283103183 old loss 0.00021544413175433874 BETTER
I0314 04:49:08.457405 2402490 finetune.py:68] layer 19_v @ epoch 2 new loss 0.00017570181807968765 old loss 0.0001869767438620329 BETTER
I0314 04:49:12.737039 2402224 finetune.py:45] layer 18_q initial loss 0.00023860573128331453
I0314 04:49:34.363708 2402003 finetune.py:68] layer 17_q @ epoch 2 new loss 0.00018380284018348902 old loss 0.00018931784143205732 BETTER
I0314 04:49:40.008118 2401752 finetune.py:68] layer 16_q @ epoch 4 new loss 0.00020577451505232602 old loss 0.0002101453283103183 BETTER
I0314 04:49:41.029658 2402490 finetune.py:68] layer 19_v @ epoch 3 new loss 0.00016814609989523888 old loss 0.00017570181807968765 BETTER
I0314 04:49:44.274980 2402224 finetune.py:68] layer 18_q @ epoch 0 new loss 0.00020849712018389255 old loss 0.00023860573128331453 BETTER
I0314 04:49:50.924004 2401752 finetune.py:45] layer 16_k initial loss 0.0002439635863993317
I0314 04:50:06.654256 2402003 finetune.py:68] layer 17_q @ epoch 3 new loss 0.00017947130254469812 old loss 0.00018380284018348902 BETTER
I0314 04:50:13.474523 2402490 finetune.py:68] layer 19_v @ epoch 4 new loss 0.0001626292068976909 old loss 0.00016814609989523888 BETTER
I0314 04:50:16.824681 2402224 finetune.py:68] layer 18_q @ epoch 1 new loss 0.00020054171909578145 old loss 0.00020849712018389255 BETTER
I0314 04:50:23.922721 2401752 finetune.py:68] layer 16_k @ epoch 0 new loss 0.0002339991187909618 old loss 0.0002439635863993317 BETTER
I0314 04:50:23.955219 2402490 finetune.py:45] layer 19_q initial loss 0.00022225745487958193
I0314 04:50:39.382236 2402003 finetune.py:68] layer 17_q @ epoch 4 new loss 0.00017588181071914732 old loss 0.00017947130254469812 BETTER
I0314 04:50:49.077499 2402224 finetune.py:68] layer 18_q @ epoch 2 new loss 0.00019475871522445232 old loss 0.00020054171909578145 BETTER
I0314 04:50:49.837702 2402003 finetune.py:45] layer 17_k initial loss 0.0002176757698180154
I0314 04:50:55.348216 2402490 finetune.py:68] layer 19_q @ epoch 0 new loss 0.00019571412121877074 old loss 0.00022225745487958193 BETTER
I0314 04:50:57.742677 2401752 finetune.py:68] layer 16_k @ epoch 1 new loss 0.00023010022414382547 old loss 0.0002339991187909618 BETTER
I0314 04:51:21.289193 2402003 finetune.py:68] layer 17_k @ epoch 0 new loss 0.00020408787531778216 old loss 0.0002176757698180154 BETTER
I0314 04:51:21.546037 2402224 finetune.py:68] layer 18_q @ epoch 3 new loss 0.0001901888899737969 old loss 0.00019475871522445232 BETTER
I0314 04:51:27.371376 2402490 finetune.py:68] layer 19_q @ epoch 1 new loss 0.00018872627697419375 old loss 0.00019571412121877074 BETTER
I0314 04:51:31.638370 2401752 finetune.py:68] layer 16_k @ epoch 2 new loss 0.0002269079996040091 old loss 0.00023010022414382547 BETTER
I0314 04:51:53.319755 2402003 finetune.py:68] layer 17_k @ epoch 1 new loss 0.00020087258599232882 old loss 0.00020408787531778216 BETTER
I0314 04:51:54.075929 2402224 finetune.py:68] layer 18_q @ epoch 4 new loss 0.00018648030527401716 old loss 0.0001901888899737969 BETTER
I0314 04:52:01.530571 2402490 finetune.py:68] layer 19_q @ epoch 2 new loss 0.0001837393647292629 old loss 0.00018872627697419375 BETTER
I0314 04:52:06.900824 2401752 finetune.py:68] layer 16_k @ epoch 3 new loss 0.00022415835701394826 old loss 0.0002269079996040091 BETTER
I0314 04:52:08.413831 2402224 finetune.py:45] layer 18_k initial loss 0.00024891115026548505
I0314 04:52:27.042009 2402003 finetune.py:68] layer 17_k @ epoch 2 new loss 0.00019820455054286867 old loss 0.00020087258599232882 BETTER
I0314 04:52:35.214491 2402490 finetune.py:68] layer 19_q @ epoch 3 new loss 0.00017974931688513607 old loss 0.0001837393647292629 BETTER
I0314 04:52:41.985155 2402224 finetune.py:68] layer 18_k @ epoch 0 new loss 0.00022423293557949364 old loss 0.00024891115026548505 BETTER
I0314 04:52:43.562057 2401752 finetune.py:68] layer 16_k @ epoch 4 new loss 0.00022180096129886806 old loss 0.00022415835701394826 BETTER
I0314 04:52:58.293020 2401752 finetune.py:45] layer 16_o initial loss 0.0005852407775819302
I0314 04:53:01.881087 2402003 finetune.py:68] layer 17_k @ epoch 3 new loss 0.00019598215294536203 old loss 0.00019820455054286867 BETTER
I0314 04:53:09.559106 2402490 finetune.py:68] layer 19_q @ epoch 4 new loss 0.00017657381249591708 old loss 0.00017974931688513607 BETTER
I0314 04:53:15.852300 2402224 finetune.py:68] layer 18_k @ epoch 1 new loss 0.0002206099743489176 old loss 0.00022423293557949364 BETTER
I0314 04:53:24.574083 2402490 finetune.py:45] layer 19_k initial loss 0.00021855805243831128
I0314 04:53:32.017340 2401752 finetune.py:68] layer 16_o @ epoch 0 new loss 0.0005434915074147284 old loss 0.0005852407775819302 BETTER
I0314 04:53:36.412917 2402003 finetune.py:68] layer 17_k @ epoch 4 new loss 0.00019392698595765978 old loss 0.00019598215294536203 BETTER
I0314 04:53:48.978947 2402224 finetune.py:68] layer 18_k @ epoch 2 new loss 0.00021780813403893262 old loss 0.0002206099743489176 BETTER
I0314 04:53:49.727129 2402003 finetune.py:45] layer 17_o initial loss 0.00046536180889233947
I0314 04:53:55.270841 2402490 finetune.py:68] layer 19_k @ epoch 0 new loss 0.00020801584469154477 old loss 0.00021855805243831128 BETTER
I0314 04:54:05.409823 2401752 finetune.py:68] layer 16_o @ epoch 1 new loss 0.0005248772213235497 old loss 0.0005434915074147284 BETTER
I0314 04:54:20.133045 2402003 finetune.py:68] layer 17_o @ epoch 0 new loss 0.0004375395947135985 old loss 0.00046536180889233947 BETTER
I0314 04:54:21.130303 2402224 finetune.py:68] layer 18_k @ epoch 3 new loss 0.00021543701586779207 old loss 0.00021780813403893262 BETTER
I0314 04:54:26.803769 2402490 finetune.py:68] layer 19_k @ epoch 1 new loss 0.00020515682990662754 old loss 0.00020801584469154477 BETTER
I0314 04:54:38.780762 2401752 finetune.py:68] layer 16_o @ epoch 2 new loss 0.0005116129759699106 old loss 0.0005248772213235497 BETTER
I0314 04:54:51.584192 2402003 finetune.py:68] layer 17_o @ epoch 1 new loss 0.00042470748303458095 old loss 0.0004375395947135985 BETTER
I0314 04:54:53.111397 2402224 finetune.py:68] layer 18_k @ epoch 4 new loss 0.0002133580856025219 old loss 0.00021543701586779207 BETTER
I0314 04:54:58.352675 2402490 finetune.py:68] layer 19_k @ epoch 2 new loss 0.0002027441223617643 old loss 0.00020515682990662754 BETTER
I0314 04:55:03.382721 2402224 finetune.py:45] layer 18_o initial loss 0.0005091133061796427
I0314 04:55:12.364499 2401752 finetune.py:68] layer 16_o @ epoch 3 new loss 0.000501372735016048 old loss 0.0005116129759699106 BETTER
I0314 04:55:23.174824 2402003 finetune.py:68] layer 17_o @ epoch 2 new loss 0.00041569865425117314 old loss 0.00042470748303458095 BETTER
I0314 04:55:30.106709 2402490 finetune.py:68] layer 19_k @ epoch 3 new loss 0.00020079550449736416 old loss 0.0002027441223617643 BETTER
I0314 04:55:34.307572 2402224 finetune.py:68] layer 18_o @ epoch 0 new loss 0.0004770189116243273 old loss 0.0005091133061796427 BETTER
I0314 04:55:45.796325 2401752 finetune.py:68] layer 16_o @ epoch 4 new loss 0.0004930472932755947 old loss 0.000501372735016048 BETTER
I0314 04:55:54.904594 2402003 finetune.py:68] layer 17_o @ epoch 3 new loss 0.00040866632480174303 old loss 0.00041569865425117314 BETTER
I0314 04:56:02.008911 2402490 finetune.py:68] layer 19_k @ epoch 4 new loss 0.00019906600937247276 old loss 0.00020079550449736416 BETTER
I0314 04:56:02.606956 2401752 finetune.py:45] layer 16_up initial loss 0.0007956287590786815
I0314 04:56:06.138050 2402224 finetune.py:68] layer 18_o @ epoch 1 new loss 0.000463365635368973 old loss 0.0004770189116243273 BETTER
I0314 04:56:12.209426 2402490 finetune.py:45] layer 19_o initial loss 0.00046166617539711297
I0314 04:56:26.576352 2402003 finetune.py:68] layer 17_o @ epoch 4 new loss 0.00040303272544406354 old loss 0.00040866632480174303 BETTER
I0314 04:56:33.324525 2401752 finetune.py:68] layer 16_up @ epoch 0 new loss 0.0007747585186734796 old loss 0.0007956287590786815 BETTER
I0314 04:56:37.988138 2402224 finetune.py:68] layer 18_o @ epoch 2 new loss 0.0004538030771072954 old loss 0.000463365635368973 BETTER
I0314 04:56:42.403500 2402490 finetune.py:68] layer 19_o @ epoch 0 new loss 0.0004349870723672211 old loss 0.00046166617539711297 BETTER
I0314 04:56:43.645739 2402003 finetune.py:45] layer 17_up initial loss 0.0007392619736492634
I0314 04:57:05.111919 2401752 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0007625830476172268 old loss 0.0007747585186734796 BETTER
I0314 04:57:10.047639 2402224 finetune.py:68] layer 18_o @ epoch 3 new loss 0.00044642790453508496 old loss 0.0004538030771072954 BETTER
I0314 04:57:12.579412 2402003 finetune.py:68] layer 17_up @ epoch 0 new loss 0.0007205032161436975 old loss 0.0007392619736492634 BETTER
I0314 04:57:13.602878 2402490 finetune.py:68] layer 19_o @ epoch 1 new loss 0.0004240935668349266 old loss 0.0004349870723672211 BETTER
I0314 04:57:37.032453 2401752 finetune.py:68] layer 16_up @ epoch 2 new loss 0.0007528631831519306 old loss 0.0007625830476172268 BETTER
I0314 04:57:42.310506 2402224 finetune.py:68] layer 18_o @ epoch 4 new loss 0.00044063947279937565 old loss 0.00044642790453508496 BETTER
I0314 04:57:42.590822 2402003 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0007097205962054431 old loss 0.0007205032161436975 BETTER
I0314 04:57:44.761606 2402490 finetune.py:68] layer 19_o @ epoch 2 new loss 0.00041666647302918136 old loss 0.0004240935668349266 BETTER
I0314 04:57:59.600196 2402224 finetune.py:45] layer 18_up initial loss 0.0008434865740127861
I0314 04:58:09.012808 2401752 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0007447770913131535 old loss 0.0007528631831519306 BETTER
I0314 04:58:12.661881 2402003 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0007013588328845799 old loss 0.0007097205962054431 BETTER
I0314 04:58:15.895130 2402490 finetune.py:68] layer 19_o @ epoch 3 new loss 0.0004109522851649672 old loss 0.00041666647302918136 BETTER
I0314 04:58:28.671272 2402224 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0008217288414016366 old loss 0.0008434865740127861 BETTER
I0314 04:58:41.061036 2401752 finetune.py:68] layer 16_up @ epoch 4 new loss 0.000737842230591923 old loss 0.0007447770913131535 BETTER
I0314 04:58:42.493755 2402003 finetune.py:68] layer 17_up @ epoch 3 new loss 0.0006944296765141189 old loss 0.0007013588328845799 BETTER
I0314 04:58:46.790970 2402490 finetune.py:68] layer 19_o @ epoch 4 new loss 0.00040647166315466166 old loss 0.0004109522851649672 BETTER
I0314 04:58:58.559114 2402224 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0008098022663034499 old loss 0.0008217288414016366 BETTER
I0314 04:58:58.566704 2401752 finetune.py:45] layer 16_gate initial loss 0.000970224617049098
I0314 04:59:03.925202 2402490 finetune.py:45] layer 19_up initial loss 0.0008579104905948043
I0314 04:59:12.522942 2402003 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0006884575122967362 old loss 0.0006944296765141189 BETTER
I0314 04:59:27.724647 2401752 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.0009590319241397083 old loss 0.000970224617049098 BETTER
I0314 04:59:28.707429 2402224 finetune.py:68] layer 18_up @ epoch 2 new loss 0.0008005618001334369 old loss 0.0008098022663034499 BETTER
I0314 04:59:29.869952 2402003 finetune.py:45] layer 17_gate initial loss 0.0009456929983571172
I0314 04:59:32.580927 2402490 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0008366569527424872 old loss 0.0008579104905948043 BETTER
I0314 04:59:57.740229 2402003 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0009360191179439425 old loss 0.0009456929983571172 BETTER
I0314 04:59:57.852990 2401752 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0009513784316368401 old loss 0.0009590319241397083 BETTER
I0314 04:59:59.125068 2402224 finetune.py:68] layer 18_up @ epoch 3 new loss 0.0007928325794637203 old loss 0.0008005618001334369 BETTER
I0314 05:00:01.967681 2402490 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0008248794474638999 old loss 0.0008366569527424872 BETTER
I0314 05:00:26.240637 2402003 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0009292085887864232 old loss 0.0009360191179439425 BETTER
I0314 05:00:28.151415 2401752 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.0009448789642192423 old loss 0.0009513784316368401 BETTER
I0314 05:00:29.280754 2402224 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0007864144281484187 old loss 0.0007928325794637203 BETTER
I0314 05:00:31.510933 2402490 finetune.py:68] layer 19_up @ epoch 2 new loss 0.0008157721604220569 old loss 0.0008248794474638999 BETTER
I0314 05:00:46.490638 2402224 finetune.py:45] layer 18_gate initial loss 0.0010888677788898349
I0314 05:00:54.730318 2402003 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.0009234853205271065 old loss 0.0009292085887864232 BETTER
I0314 05:00:58.411482 2401752 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0009393812506459653 old loss 0.0009448789642192423 BETTER
I0314 05:01:01.123997 2402490 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0008084484725259244 old loss 0.0008157721604220569 BETTER
I0314 05:01:13.964881 2402224 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0010789106599986553 old loss 0.0010888677788898349 BETTER
I0314 05:01:23.063058 2402003 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.0009186196839436889 old loss 0.0009234853205271065 BETTER
I0314 05:01:28.765611 2401752 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.0009344537393189967 old loss 0.0009393812506459653 BETTER
I0314 05:01:30.593882 2402490 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0008022527326829731 old loss 0.0008084484725259244 BETTER
I0314 05:01:42.293479 2402224 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.0010715145617723465 old loss 0.0010789106599986553 BETTER
I0314 05:01:48.280883 2402490 finetune.py:45] layer 19_gate initial loss 0.0011562859872356057
I0314 05:01:48.292449 2401752 finetune.py:45] layer 16_down initial loss 0.0014982243301346898
I0314 05:01:51.654130 2402003 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.0009143748320639133 old loss 0.0009186196839436889 BETTER
I0314 05:02:10.417688 2402003 finetune.py:45] layer 17_down initial loss 0.0015191268175840378
I0314 05:02:10.683630 2402224 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0010653828503564 old loss 0.0010715145617723465 BETTER
I0314 05:02:15.460890 2402490 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.0011456601787358522 old loss 0.0011562859872356057 BETTER
I0314 05:02:15.532812 2401752 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0014977745013311505 old loss 0.0014982243301346898 BETTER
I0314 05:02:36.572065 2402003 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0015186737291514874 old loss 0.0015191268175840378 BETTER
I0314 05:02:39.136208 2402224 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0010601462563499808 old loss 0.0010653828503564 BETTER
I0314 05:02:43.662390 2402490 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0011381958611309528 old loss 0.0011456601787358522 BETTER
I0314 05:02:43.975637 2401752 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0014974466757848859 old loss 0.0014977745013311505 BETTER
I0314 05:03:06.279618 2402003 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0015183371724560857 old loss 0.0015186737291514874 BETTER
I0314 05:03:10.920572 2402224 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.0010555896442383528 old loss 0.0010601462563499808 BETTER
I0314 05:03:14.429956 2402490 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.001131985685788095 old loss 0.0011381958611309528 BETTER
I0314 05:03:14.947154 2401752 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0014971989439800382 old loss 0.0014974466757848859 BETTER
I0314 05:03:35.589263 2402003 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0015180899063125253 old loss 0.0015183371724560857 BETTER
I0314 05:03:36.195508 2402224 finetune.py:45] layer 18_down initial loss 0.0017878081416711211
I0314 05:03:45.533833 2402490 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0011267608497291803 old loss 0.001131985685788095 BETTER
I0314 05:03:46.034543 2401752 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0014970168704167008 old loss 0.0014971989439800382 BETTER
I0314 05:04:04.608445 2402224 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0017872853204607964 old loss 0.0017878081416711211 BETTER
I0314 05:04:05.162988 2402003 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0015178982866927981 old loss 0.0015180899063125253 BETTER
I0314 05:04:16.650558 2402490 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.0011222244938835502 old loss 0.0011267608497291803 BETTER
I0314 05:04:17.145250 2401752 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0014968817122280598 old loss 0.0014970168704167008 BETTER
16_v proxy err 0.02163354679942131 tr(WHW.T) 780.7407836914062
16_q proxy err 0.002790320897474885 tr(WHW.T) 7198.97412109375
16_k proxy err 0.0017865344416350126 tr(WHW.T) 11669.9716796875
16_o proxy err 0.018240535631775856 tr(WHW.T) 88.87107849121094
16_up proxy err 0.010321238078176975 tr(WHW.T) 1889.9969482421875
16_gate proxy err 0.005949827842414379 tr(WHW.T) 3367.051513671875
16_down proxy err 0.015586139634251595 tr(WHW.T) 153.3455352783203
I0314 05:04:32.780993 2402224 finetune.py:68] layer 18_down @ epoch 1 new loss 0.001786899520084262 old loss 0.0017872853204607964 BETTER
I0314 05:04:32.924050 2402003 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0015177513705566525 old loss 0.0015178982866927981 BETTER
17_v proxy err 0.020239919424057007 tr(WHW.T) 845.7654418945312
17_q proxy err 0.0028211178723722696 tr(WHW.T) 7171.50732421875
17_k proxy err 0.0019534744787961245 tr(WHW.T) 10738.3828125
17_o proxy err 0.019925177097320557 tr(WHW.T) 58.45782470703125
17_up proxy err 0.011226048693060875 tr(WHW.T) 1921.03369140625
17_gate proxy err 0.006226522848010063 tr(WHW.T) 3572.251953125
17_down proxy err 0.015458942390978336 tr(WHW.T) 166.78628540039062
I0314 05:04:38.114510 2402490 finetune.py:45] layer 19_down initial loss 0.0019197750370949507
I0314 05:04:59.512749 2402224 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0017866078997030854 old loss 0.001786899520084262 BETTER
I0314 05:05:03.026896 2402490 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0019191385945305228 old loss 0.0019197750370949507 BETTER
I0314 05:05:26.185020 2402224 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0017863863613456488 old loss 0.0017866078997030854 BETTER
I0314 05:05:28.868062 2402490 finetune.py:68] layer 19_down @ epoch 1 new loss 0.001918727532029152 old loss 0.0019191385945305228 BETTER
I0314 05:05:49.416065 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 20 in 72.06501078605652s
I0314 05:05:52.720707 2404736 config.py:54] PyTorch version 2.1.1 available.
I0314 05:05:52.916057 2402224 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0017862134845927358 old loss 0.0017863863613456488 BETTER
I0314 05:05:53.769847 2357215 quantize_finetune_llama.py:191] layer 21 gpu 1
I0314 05:05:53.834584 2404736 data_utils.py:336] using 256 training seqs, 128 validation seqs
18_v proxy err 0.01865421049296856 tr(WHW.T) 1003.7705078125
18_q proxy err 0.0029101106338202953 tr(WHW.T) 7515.47021484375
18_k proxy err 0.002145544160157442 tr(WHW.T) 10498.263671875
18_o proxy err 0.017541224136948586 tr(WHW.T) 70.42353057861328
18_up proxy err 0.011932885274291039 tr(WHW.T) 2021.7628173828125
18_gate proxy err 0.00660457881167531 tr(WHW.T) 3773.612060546875
18_down proxy err 0.015573782846331596 tr(WHW.T) 200.15623474121094
I0314 05:05:54.981106 2402490 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0019184305565431714 old loss 0.001918727532029152 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:06:03.026758 2404736 finetune.py:45] layer 20_v initial loss 0.00043264098349027336
I0314 05:06:21.095443 2402490 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0019182118121534586 old loss 0.0019184305565431714 BETTER
I0314 05:06:35.830982 2404736 finetune.py:68] layer 20_v @ epoch 0 new loss 0.00023581928689964116 old loss 0.00043264098349027336 BETTER
I0314 05:06:47.171007 2402490 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0019180414965376258 old loss 0.0019182118121534586 BETTER
19_v proxy err 0.01828213967382908 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.003086024895310402 tr(WHW.T) 6952.64306640625
19_k proxy err 0.0020735550206154585 tr(WHW.T) 10592.0400390625
19_o proxy err 0.01732061244547367 tr(WHW.T) 62.701717376708984
19_up proxy err 0.01198630966246128 tr(WHW.T) 2149.914306640625
19_gate proxy err 0.007212146185338497 tr(WHW.T) 3685.338623046875
19_down proxy err 0.01515060756355524 tr(WHW.T) 224.65293884277344
I0314 05:07:05.562085 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 21 in 69.15975594520569s
I0314 05:07:08.821246 2406548 config.py:54] PyTorch version 2.1.1 available.
I0314 05:07:09.864356 2357215 quantize_finetune_llama.py:191] layer 22 gpu 2
I0314 05:07:09.921219 2406548 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 05:07:10.063431 2404736 finetune.py:68] layer 20_v @ epoch 1 new loss 0.0002104720624629408 old loss 0.00023581928689964116 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:07:18.700030 2406548 finetune.py:45] layer 21_v initial loss 0.00039292153087444603
I0314 05:07:44.559107 2404736 finetune.py:68] layer 20_v @ epoch 2 new loss 0.0001978554792003706 old loss 0.0002104720624629408 BETTER
I0314 05:07:49.907164 2406548 finetune.py:68] layer 21_v @ epoch 0 new loss 0.00021587945229839534 old loss 0.00039292153087444603 BETTER
I0314 05:08:19.183965 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 22 in 68.92934036254883s
I0314 05:08:19.244164 2404736 finetune.py:68] layer 20_v @ epoch 3 new loss 0.00018970412202179432 old loss 0.0001978554792003706 BETTER
I0314 05:08:21.877858 2406548 finetune.py:68] layer 21_v @ epoch 1 new loss 0.00019459120812825859 old loss 0.00021587945229839534 BETTER
I0314 05:08:22.468144 2407456 config.py:54] PyTorch version 2.1.1 available.
I0314 05:08:23.496099 2357215 quantize_finetune_llama.py:191] layer 23 gpu 3
I0314 05:08:23.556420 2407456 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:08:32.771625 2407456 finetune.py:45] layer 22_v initial loss 0.00047125606215558946
I0314 05:08:54.094612 2404736 finetune.py:68] layer 20_v @ epoch 4 new loss 0.00018414818623568863 old loss 0.00018970412202179432 BETTER
I0314 05:08:54.242180 2406548 finetune.py:68] layer 21_v @ epoch 2 new loss 0.00018424083827994764 old loss 0.00019459120812825859 BETTER
I0314 05:09:04.488409 2407456 finetune.py:68] layer 22_v @ epoch 0 new loss 0.00026572783826850355 old loss 0.00047125606215558946 BETTER
I0314 05:09:04.646064 2404736 finetune.py:45] layer 20_q initial loss 0.00024325132835656404
I0314 05:09:26.852081 2406548 finetune.py:68] layer 21_v @ epoch 3 new loss 0.0001776023709680885 old loss 0.00018424083827994764 BETTER
I0314 05:09:33.772807 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 23 in 69.85834765434265s
I0314 05:09:36.985277 2407456 finetune.py:68] layer 22_v @ epoch 1 new loss 0.00023961417900864035 old loss 0.00026572783826850355 BETTER
I0314 05:09:37.167618 2408354 config.py:54] PyTorch version 2.1.1 available.
I0314 05:09:38.138346 2404736 finetune.py:68] layer 20_q @ epoch 0 new loss 0.00022242465638555586 old loss 0.00024325132835656404 BETTER
I0314 05:09:38.231708 2357215 quantize_finetune_llama.py:191] layer 24 gpu 0
I0314 05:09:38.293436 2408354 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:09:46.940567 2408354 finetune.py:45] layer 23_v initial loss 0.0004654364020098001
I0314 05:09:59.522901 2406548 finetune.py:68] layer 21_v @ epoch 4 new loss 0.00017284271598327905 old loss 0.0001776023709680885 BETTER
I0314 05:10:09.492285 2406548 finetune.py:45] layer 21_q initial loss 0.00022409085067920387
I0314 05:10:09.770913 2407456 finetune.py:68] layer 22_v @ epoch 2 new loss 0.00022655756038147956 old loss 0.00023961417900864035 BETTER
I0314 05:10:12.485641 2404736 finetune.py:68] layer 20_q @ epoch 1 new loss 0.0002147127961507067 old loss 0.00022242465638555586 BETTER
I0314 05:10:17.814568 2408354 finetune.py:68] layer 23_v @ epoch 0 new loss 0.00026154154329560697 old loss 0.0004654364020098001 BETTER
I0314 05:10:41.268811 2406548 finetune.py:68] layer 21_q @ epoch 0 new loss 0.0002069087204290554 old loss 0.00022409085067920387 BETTER
I0314 05:10:42.788990 2407456 finetune.py:68] layer 22_v @ epoch 3 new loss 0.0002180105948355049 old loss 0.00022655756038147956 BETTER
I0314 05:10:46.909481 2404736 finetune.py:68] layer 20_q @ epoch 2 new loss 0.00020925105491187423 old loss 0.0002147127961507067 BETTER
I0314 05:10:49.757906 2408354 finetune.py:68] layer 23_v @ epoch 1 new loss 0.00023627319023944438 old loss 0.00026154154329560697 BETTER
I0314 05:11:13.506860 2406548 finetune.py:68] layer 21_q @ epoch 1 new loss 0.00020061968825757504 old loss 0.0002069087204290554 BETTER
I0314 05:11:15.937175 2407456 finetune.py:68] layer 22_v @ epoch 4 new loss 0.00021192480926401913 old loss 0.0002180105948355049 BETTER
I0314 05:11:21.564501 2404736 finetune.py:68] layer 20_q @ epoch 3 new loss 0.00020485403365455568 old loss 0.00020925105491187423 BETTER
I0314 05:11:21.866993 2408354 finetune.py:68] layer 23_v @ epoch 2 new loss 0.0002243334020022303 old loss 0.00023627319023944438 BETTER
I0314 05:11:26.931952 2407456 finetune.py:45] layer 22_q initial loss 0.0003094540734309703
I0314 05:11:45.690404 2406548 finetune.py:68] layer 21_q @ epoch 2 new loss 0.0001961833331733942 old loss 0.00020061968825757504 BETTER
I0314 05:11:53.929804 2408354 finetune.py:68] layer 23_v @ epoch 3 new loss 0.0002166591730201617 old loss 0.0002243334020022303 BETTER
I0314 05:11:56.010861 2404736 finetune.py:68] layer 20_q @ epoch 4 new loss 0.0002013878693105653 old loss 0.00020485403365455568 BETTER
I0314 05:11:58.583376 2407456 finetune.py:68] layer 22_q @ epoch 0 new loss 0.00026872160378843546 old loss 0.0003094540734309703 BETTER
I0314 05:12:06.731829 2404736 finetune.py:45] layer 20_k initial loss 0.0002469744358677417
I0314 05:12:17.944514 2406548 finetune.py:68] layer 21_q @ epoch 3 new loss 0.00019262603018432856 old loss 0.0001961833331733942 BETTER
I0314 05:12:26.231741 2408354 finetune.py:68] layer 23_v @ epoch 4 new loss 0.0002111928624799475 old loss 0.0002166591730201617 BETTER
I0314 05:12:31.265888 2407456 finetune.py:68] layer 22_q @ epoch 1 new loss 0.00025942226056940854 old loss 0.00026872160378843546 BETTER
I0314 05:12:36.885863 2408354 finetune.py:45] layer 23_q initial loss 0.0002759394992608577
I0314 05:12:39.602964 2404736 finetune.py:68] layer 20_k @ epoch 0 new loss 0.00023691948445048183 old loss 0.0002469744358677417 BETTER
I0314 05:12:50.349966 2406548 finetune.py:68] layer 21_q @ epoch 4 new loss 0.00018982785695698112 old loss 0.00019262603018432856 BETTER
I0314 05:13:00.923750 2406548 finetune.py:45] layer 21_k initial loss 0.00024119681620504707
I0314 05:13:03.583397 2407456 finetune.py:68] layer 22_q @ epoch 2 new loss 0.00025269552133977413 old loss 0.00025942226056940854 BETTER
I0314 05:13:08.237905 2408354 finetune.py:68] layer 23_q @ epoch 0 new loss 0.0002578452113084495 old loss 0.0002759394992608577 BETTER
I0314 05:13:13.242166 2404736 finetune.py:68] layer 20_k @ epoch 1 new loss 0.0002338402991881594 old loss 0.00023691948445048183 BETTER
I0314 05:13:32.484722 2406548 finetune.py:68] layer 21_k @ epoch 0 new loss 0.000228040837100707 old loss 0.00024119681620504707 BETTER
I0314 05:13:36.370696 2407456 finetune.py:68] layer 22_q @ epoch 3 new loss 0.0002474020002409816 old loss 0.00025269552133977413 BETTER
I0314 05:13:40.388108 2408354 finetune.py:68] layer 23_q @ epoch 1 new loss 0.00025033787824213505 old loss 0.0002578452113084495 BETTER
I0314 05:13:47.129136 2404736 finetune.py:68] layer 20_k @ epoch 2 new loss 0.0002312762662768364 old loss 0.0002338402991881594 BETTER
I0314 05:14:04.667638 2406548 finetune.py:68] layer 21_k @ epoch 1 new loss 0.00022542871010955423 old loss 0.000228040837100707 BETTER
I0314 05:14:08.863300 2407456 finetune.py:68] layer 22_q @ epoch 4 new loss 0.0002430076856398955 old loss 0.0002474020002409816 BETTER
I0314 05:14:12.396632 2408354 finetune.py:68] layer 23_q @ epoch 2 new loss 0.000244912167545408 old loss 0.00025033787824213505 BETTER
I0314 05:14:19.337936 2407456 finetune.py:45] layer 22_k initial loss 0.0003188075206708163
I0314 05:14:20.977839 2404736 finetune.py:68] layer 20_k @ epoch 3 new loss 0.00022914224246051162 old loss 0.0002312762662768364 BETTER
I0314 05:14:37.432550 2406548 finetune.py:68] layer 21_k @ epoch 2 new loss 0.00022331683430820704 old loss 0.00022542871010955423 BETTER
I0314 05:14:44.389874 2408354 finetune.py:68] layer 23_q @ epoch 3 new loss 0.00024071881489362568 old loss 0.000244912167545408 BETTER
I0314 05:14:50.715144 2407456 finetune.py:68] layer 22_k @ epoch 0 new loss 0.00029897948843427 old loss 0.0003188075206708163 BETTER
I0314 05:14:54.761470 2404736 finetune.py:68] layer 20_k @ epoch 4 new loss 0.00022724084556102753 old loss 0.00022914224246051162 BETTER
I0314 05:15:05.354730 2404736 finetune.py:45] layer 20_o initial loss 0.000544137554243207
I0314 05:15:09.546796 2406548 finetune.py:68] layer 21_k @ epoch 3 new loss 0.00022152969904709607 old loss 0.00022331683430820704 BETTER
I0314 05:15:16.460231 2408354 finetune.py:68] layer 23_q @ epoch 4 new loss 0.0002372340386500582 old loss 0.00024071881489362568 BETTER
I0314 05:15:22.962228 2407456 finetune.py:68] layer 22_k @ epoch 1 new loss 0.000294953235425055 old loss 0.00029897948843427 BETTER
I0314 05:15:27.235639 2408354 finetune.py:45] layer 23_k initial loss 0.0002945550950244069
I0314 05:15:38.085377 2404736 finetune.py:68] layer 20_o @ epoch 0 new loss 0.000509861798491329 old loss 0.000544137554243207 BETTER
I0314 05:15:41.656166 2406548 finetune.py:68] layer 21_k @ epoch 4 new loss 0.00022004134370945394 old loss 0.00022152969904709607 BETTER
I0314 05:15:52.185843 2406548 finetune.py:45] layer 21_o initial loss 0.00048234863788820803
I0314 05:15:55.288145 2407456 finetune.py:68] layer 22_k @ epoch 2 new loss 0.0002918938116636127 old loss 0.000294953235425055 BETTER
I0314 05:15:58.039445 2408354 finetune.py:68] layer 23_k @ epoch 0 new loss 0.0002870187454391271 old loss 0.0002945550950244069 BETTER
I0314 05:16:11.651866 2404736 finetune.py:68] layer 20_o @ epoch 1 new loss 0.0004970949376001954 old loss 0.000509861798491329 BETTER
I0314 05:16:22.804377 2406548 finetune.py:68] layer 21_o @ epoch 0 new loss 0.00046256196219474077 old loss 0.00048234863788820803 BETTER
I0314 05:16:27.677088 2407456 finetune.py:68] layer 22_k @ epoch 3 new loss 0.0002893717319238931 old loss 0.0002918938116636127 BETTER
I0314 05:16:29.728480 2408354 finetune.py:68] layer 23_k @ epoch 1 new loss 0.0002842402027454227 old loss 0.0002870187454391271 BETTER
I0314 05:16:45.065351 2404736 finetune.py:68] layer 20_o @ epoch 2 new loss 0.00048804504331201315 old loss 0.0004970949376001954 BETTER
I0314 05:16:54.294743 2406548 finetune.py:68] layer 21_o @ epoch 1 new loss 0.00045529386261478066 old loss 0.00046256196219474077 BETTER
I0314 05:16:59.832339 2407456 finetune.py:68] layer 22_k @ epoch 4 new loss 0.0002872451732400805 old loss 0.0002893717319238931 BETTER
I0314 05:17:01.328975 2408354 finetune.py:68] layer 23_k @ epoch 2 new loss 0.0002820122172124684 old loss 0.0002842402027454227 BETTER
I0314 05:17:10.365718 2407456 finetune.py:45] layer 22_o initial loss 0.0006268197321332991
I0314 05:17:18.573303 2404736 finetune.py:68] layer 20_o @ epoch 3 new loss 0.0004811619292013347 old loss 0.00048804504331201315 BETTER
I0314 05:17:25.725755 2406548 finetune.py:68] layer 21_o @ epoch 2 new loss 0.0004502300580497831 old loss 0.00045529386261478066 BETTER
I0314 05:17:32.910889 2408354 finetune.py:68] layer 23_k @ epoch 3 new loss 0.0002801442751660943 old loss 0.0002820122172124684 BETTER
I0314 05:17:41.258466 2407456 finetune.py:68] layer 22_o @ epoch 0 new loss 0.0005974287050776184 old loss 0.0006268197321332991 BETTER
I0314 05:17:51.814006 2404736 finetune.py:68] layer 20_o @ epoch 4 new loss 0.00047551473835483193 old loss 0.0004811619292013347 BETTER
I0314 05:17:57.187834 2406548 finetune.py:68] layer 21_o @ epoch 3 new loss 0.0004462634969968349 old loss 0.0004502300580497831 BETTER
I0314 05:18:04.569431 2408354 finetune.py:68] layer 23_k @ epoch 4 new loss 0.00027848262106999755 old loss 0.0002801442751660943 BETTER
I0314 05:18:09.258683 2404736 finetune.py:45] layer 20_up initial loss 0.0010035355808213353
I0314 05:18:12.948566 2407456 finetune.py:68] layer 22_o @ epoch 1 new loss 0.0005854590563103557 old loss 0.0005974287050776184 BETTER
I0314 05:18:15.002034 2408354 finetune.py:45] layer 23_o initial loss 0.0005819471552968025
I0314 05:18:28.765969 2406548 finetune.py:68] layer 21_o @ epoch 4 new loss 0.0004431381821632385 old loss 0.0004462634969968349 BETTER
I0314 05:18:39.862695 2404736 finetune.py:68] layer 20_up @ epoch 0 new loss 0.0009806302841752768 old loss 0.0010035355808213353 BETTER
I0314 05:18:45.216134 2407456 finetune.py:68] layer 22_o @ epoch 2 new loss 0.0005767823313362896 old loss 0.0005854590563103557 BETTER
I0314 05:18:45.635393 2408354 finetune.py:68] layer 23_o @ epoch 0 new loss 0.0005600307486020029 old loss 0.0005819471552968025 BETTER
I0314 05:18:46.315933 2406548 finetune.py:45] layer 21_up initial loss 0.0010159570956602693
I0314 05:19:11.612577 2404736 finetune.py:68] layer 20_up @ epoch 1 new loss 0.000967387284617871 old loss 0.0009806302841752768 BETTER
I0314 05:19:15.476802 2406548 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0009956569410860538 old loss 0.0010159570956602693 BETTER
I0314 05:19:16.759682 2408354 finetune.py:68] layer 23_o @ epoch 1 new loss 0.0005511650233529508 old loss 0.0005600307486020029 BETTER
I0314 05:19:17.327678 2407456 finetune.py:68] layer 22_o @ epoch 3 new loss 0.0005701810005120933 old loss 0.0005767823313362896 BETTER
I0314 05:19:43.372687 2404736 finetune.py:68] layer 20_up @ epoch 2 new loss 0.0009573218412697315 old loss 0.000967387284617871 BETTER
I0314 05:19:45.441204 2406548 finetune.py:68] layer 21_up @ epoch 1 new loss 0.0009841395076364279 old loss 0.0009956569410860538 BETTER
I0314 05:19:47.787883 2408354 finetune.py:68] layer 23_o @ epoch 2 new loss 0.0005450138705782592 old loss 0.0005511650233529508 BETTER
I0314 05:19:49.078717 2407456 finetune.py:68] layer 22_o @ epoch 4 new loss 0.0005649017402902246 old loss 0.0005701810005120933 BETTER
I0314 05:20:05.809866 2407456 finetune.py:45] layer 22_up initial loss 0.0012123392662033439
I0314 05:20:15.189065 2404736 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0009491745731793344 old loss 0.0009573218412697315 BETTER
I0314 05:20:15.332102 2406548 finetune.py:68] layer 21_up @ epoch 2 new loss 0.0009754422935657203 old loss 0.0009841395076364279 BETTER
I0314 05:20:18.889287 2408354 finetune.py:68] layer 23_o @ epoch 3 new loss 0.0005403348477557302 old loss 0.0005450138705782592 BETTER
I0314 05:20:34.663469 2407456 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0011911988258361816 old loss 0.0012123392662033439 BETTER
I0314 05:20:45.101274 2406548 finetune.py:68] layer 21_up @ epoch 3 new loss 0.0009682972449809313 old loss 0.0009754422935657203 BETTER
I0314 05:20:47.071361 2404736 finetune.py:68] layer 20_up @ epoch 4 new loss 0.0009420104324817657 old loss 0.0009491745731793344 BETTER
I0314 05:20:49.721357 2408354 finetune.py:68] layer 23_o @ epoch 4 new loss 0.0005367660196498036 old loss 0.0005403348477557302 BETTER
I0314 05:21:04.127873 2404736 finetune.py:45] layer 20_gate initial loss 0.0013537812046706676
I0314 05:21:04.540250 2407456 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0011785991955548525 old loss 0.0011911988258361816 BETTER
I0314 05:21:06.635587 2408354 finetune.py:45] layer 23_up initial loss 0.0012470963411033154
I0314 05:21:14.915415 2406548 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0009624307276681066 old loss 0.0009682972449809313 BETTER
I0314 05:21:31.775125 2406548 finetune.py:45] layer 21_gate initial loss 0.0014082638081163168
I0314 05:21:33.175294 2404736 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.0013421123148873448 old loss 0.0013537812046706676 BETTER
I0314 05:21:34.706696 2407456 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0011688844533637166 old loss 0.0011785991955548525 BETTER
I0314 05:21:35.331431 2408354 finetune.py:68] layer 23_up @ epoch 0 new loss 0.0012258666101843119 old loss 0.0012470963411033154 BETTER
I0314 05:21:59.450332 2406548 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.001398417865857482 old loss 0.0014082638081163168 BETTER
I0314 05:22:03.460871 2404736 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0013337865239009261 old loss 0.0013421123148873448 BETTER
I0314 05:22:05.283572 2407456 finetune.py:68] layer 22_up @ epoch 3 new loss 0.00116068497300148 old loss 0.0011688844533637166 BETTER
I0314 05:22:05.292503 2408354 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0012139634927734733 old loss 0.0012258666101843119 BETTER
I0314 05:22:27.810515 2406548 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0013912443537265062 old loss 0.001398417865857482 BETTER
I0314 05:22:33.789921 2404736 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.0013265570160001516 old loss 0.0013337865239009261 BETTER
I0314 05:22:35.180273 2408354 finetune.py:68] layer 23_up @ epoch 2 new loss 0.0012048922944813967 old loss 0.0012139634927734733 BETTER
I0314 05:22:35.767203 2407456 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0011540050618350506 old loss 0.00116068497300148 BETTER
I0314 05:22:52.995064 2407456 finetune.py:45] layer 22_gate initial loss 0.0016598169459030032
I0314 05:22:55.961982 2406548 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0013852400006726384 old loss 0.0013912443537265062 BETTER
I0314 05:23:04.120445 2404736 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.001320620416663587 old loss 0.0013265570160001516 BETTER
I0314 05:23:04.757273 2408354 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0011973987566307187 old loss 0.0012048922944813967 BETTER
I0314 05:23:20.370879 2407456 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0016494692536070943 old loss 0.0016598169459030032 BETTER
I0314 05:23:23.934808 2406548 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.0013800340238958597 old loss 0.0013852400006726384 BETTER
I0314 05:23:34.485811 2408354 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0011910805478692055 old loss 0.0011973987566307187 BETTER
I0314 05:23:34.500610 2404736 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.0013153830077499151 old loss 0.001320620416663587 BETTER
I0314 05:23:49.103250 2407456 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.001641607959754765 old loss 0.0016494692536070943 BETTER
I0314 05:23:52.397166 2408354 finetune.py:45] layer 23_gate initial loss 0.001758734229952097
I0314 05:23:52.542928 2406548 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.001375666237436235 old loss 0.0013800340238958597 BETTER
I0314 05:23:54.430512 2404736 finetune.py:45] layer 20_down initial loss 0.0022992179729044437
I0314 05:24:11.214271 2406548 finetune.py:45] layer 21_down initial loss 0.002368127927184105
I0314 05:24:17.532789 2407456 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.001635050750337541 old loss 0.001641607959754765 BETTER
I0314 05:24:19.492783 2408354 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0017488737357780337 old loss 0.001758734229952097 BETTER
I0314 05:24:21.721915 2404736 finetune.py:68] layer 20_down @ epoch 0 new loss 0.002298377687111497 old loss 0.0022992179729044437 BETTER
I0314 05:24:36.984723 2406548 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0023674159310758114 old loss 0.002368127927184105 BETTER
I0314 05:24:46.039011 2407456 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.0016293027438223362 old loss 0.001635050750337541 BETTER
I0314 05:24:47.685455 2408354 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.0017414333997294307 old loss 0.0017488737357780337 BETTER
I0314 05:24:50.166379 2404736 finetune.py:68] layer 20_down @ epoch 1 new loss 0.0022977383341640234 old loss 0.002298377687111497 BETTER
I0314 05:25:03.502171 2406548 finetune.py:68] layer 21_down @ epoch 1 new loss 0.002366854576393962 old loss 0.0023674159310758114 BETTER
I0314 05:25:14.302268 2407456 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0016244309954345226 old loss 0.0016293027438223362 BETTER
I0314 05:25:15.814194 2408354 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0017353264847770333 old loss 0.0017414333997294307 BETTER
I0314 05:25:18.590496 2404736 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0022972337901592255 old loss 0.0022977383341640234 BETTER
I0314 05:25:30.153711 2406548 finetune.py:68] layer 21_down @ epoch 2 new loss 0.002366406377404928 old loss 0.002366854576393962 BETTER
I0314 05:25:33.033621 2407456 finetune.py:45] layer 22_down initial loss 0.002720227697864175
I0314 05:25:46.793442 2408354 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.0017299664905294776 old loss 0.0017353264847770333 BETTER
I0314 05:25:49.321834 2404736 finetune.py:68] layer 20_down @ epoch 3 new loss 0.00229685683734715 old loss 0.0022972337901592255 BETTER
I0314 05:25:58.187696 2406548 finetune.py:68] layer 21_down @ epoch 3 new loss 0.0023660575971007347 old loss 0.002366406377404928 BETTER
I0314 05:26:00.626249 2407456 finetune.py:68] layer 22_down @ epoch 0 new loss 0.00271941558457911 old loss 0.002720227697864175 BETTER
I0314 05:26:16.085268 2408354 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0017254432896152139 old loss 0.0017299664905294776 BETTER
I0314 05:26:18.905708 2404736 finetune.py:68] layer 20_down @ epoch 4 new loss 0.002296557417139411 old loss 0.00229685683734715 BETTER
20_v proxy err 0.01875452511012554 tr(WHW.T) 990.5983276367188
20_q proxy err 0.0030057744588702917 tr(WHW.T) 7155.24853515625
20_k proxy err 0.0021314562764018774 tr(WHW.T) 10424.576171875
20_o proxy err 0.012910103425383568 tr(WHW.T) 100.5959701538086
20_up proxy err 0.011752672493457794 tr(WHW.T) 2340.462890625
20_gate proxy err 0.007074952591210604 tr(WHW.T) 4020.193115234375
20_down proxy err 0.015064547769725323 tr(WHW.T) 277.1145324707031
I0314 05:26:26.629018 2406548 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0023657840210944414 old loss 0.0023660575971007347 BETTER
21_v proxy err 0.01816338114440441 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.003310332540422678 tr(WHW.T) 7073.36572265625
21_k proxy err 0.002403323771432042 tr(WHW.T) 10016.0791015625
21_o proxy err 0.013802464120090008 tr(WHW.T) 75.78683471679688
21_up proxy err 0.012350821867585182 tr(WHW.T) 2361.310791015625
21_gate proxy err 0.007548181805759668 tr(WHW.T) 4000.4970703125
21_down proxy err 0.015078876167535782 tr(WHW.T) 278.56744384765625
I0314 05:26:28.874059 2407456 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0027187836822122335 old loss 0.00271941558457911 BETTER
I0314 05:26:37.849489 2408354 finetune.py:45] layer 23_down initial loss 0.0028553607407957315
I0314 05:26:55.726382 2407456 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0027182905469089746 old loss 0.0027187836822122335 BETTER
I0314 05:27:02.875093 2408354 finetune.py:68] layer 23_down @ epoch 0 new loss 0.002854695776477456 old loss 0.0028553607407957315 BETTER
I0314 05:27:22.438225 2407456 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0027178805321455 old loss 0.0027182905469089746 BETTER
I0314 05:27:28.970298 2408354 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0028541942592710257 old loss 0.002854695776477456 BETTER
I0314 05:27:44.402515 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 24 in 73.46486163139343s
I0314 05:27:47.776162 2420037 config.py:54] PyTorch version 2.1.1 available.
I0314 05:27:48.854415 2357215 quantize_finetune_llama.py:191] layer 25 gpu 1
I0314 05:27:48.922260 2420037 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 05:27:49.287764 2407456 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0027175575960427523 old loss 0.0027178805321455 BETTER
22_v proxy err 0.01731920801103115 tr(WHW.T) 1243.2529296875
22_q proxy err 0.0031509841792285442 tr(WHW.T) 7752.80908203125
22_k proxy err 0.0023574223741889 tr(WHW.T) 10637.0791015625
22_o proxy err 0.011661012656986713 tr(WHW.T) 114.77911376953125
22_up proxy err 0.012421654537320137 tr(WHW.T) 2474.641357421875
22_gate proxy err 0.007676623295992613 tr(WHW.T) 4153.42919921875
22_down proxy err 0.015007090754806995 tr(WHW.T) 313.933349609375
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:27:55.086586 2408354 finetune.py:68] layer 23_down @ epoch 2 new loss 0.002853789832442999 old loss 0.0028541942592710257 BETTER
I0314 05:27:58.300083 2420037 finetune.py:45] layer 24_v initial loss 0.0005542698199860752
I0314 05:28:21.308321 2408354 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0028534727171063423 old loss 0.002853789832442999 BETTER
I0314 05:28:31.304078 2420037 finetune.py:68] layer 24_v @ epoch 0 new loss 0.00031524887890554965 old loss 0.0005542698199860752 BETTER
I0314 05:28:47.349066 2408354 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0028532228898257017 old loss 0.0028534727171063423 BETTER
23_v proxy err 0.015833020210266113 tr(WHW.T) 1486.037353515625
23_q proxy err 0.0035341631155461073 tr(WHW.T) 7352.0771484375
23_k proxy err 0.00265743350610137 tr(WHW.T) 10015.2822265625
23_o proxy err 0.014112746343016624 tr(WHW.T) 85.44514465332031
23_up proxy err 0.012866518460214138 tr(WHW.T) 2533.90869140625
23_gate proxy err 0.008234835229814053 tr(WHW.T) 4096.3994140625
23_down proxy err 0.01508321426808834 tr(WHW.T) 323.3981628417969
I0314 05:29:02.161449 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 25 in 69.14038491249084s
I0314 05:29:05.424202 2420971 config.py:54] PyTorch version 2.1.1 available.
I0314 05:29:05.654242 2420037 finetune.py:68] layer 24_v @ epoch 1 new loss 0.00028146541444584727 old loss 0.00031524887890554965 BETTER
I0314 05:29:06.422723 2357215 quantize_finetune_llama.py:191] layer 26 gpu 2
I0314 05:29:06.478474 2420971 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:29:15.196935 2420971 finetune.py:45] layer 25_v initial loss 0.0005943177966400981
I0314 05:29:40.485487 2420037 finetune.py:68] layer 24_v @ epoch 2 new loss 0.00026546139270067215 old loss 0.00028146541444584727 BETTER
I0314 05:29:46.549913 2420971 finetune.py:68] layer 25_v @ epoch 0 new loss 0.00029035279294475913 old loss 0.0005943177966400981 BETTER
I0314 05:30:15.335698 2420037 finetune.py:68] layer 24_v @ epoch 3 new loss 0.0002559269778430462 old loss 0.00026546139270067215 BETTER
I0314 05:30:16.239645 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 26 in 69.37937092781067s
I0314 05:30:18.741355 2420971 finetune.py:68] layer 25_v @ epoch 1 new loss 0.00025585421826690435 old loss 0.00029035279294475913 BETTER
I0314 05:30:19.475338 2421880 config.py:54] PyTorch version 2.1.1 available.
I0314 05:30:20.458632 2357215 quantize_finetune_llama.py:191] layer 27 gpu 3
I0314 05:30:20.515138 2421880 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:30:29.440917 2421880 finetune.py:45] layer 26_v initial loss 0.0007993309991434216
I0314 05:30:50.323267 2420037 finetune.py:68] layer 24_v @ epoch 4 new loss 0.00024890349595807493 old loss 0.0002559269778430462 BETTER
I0314 05:30:51.213084 2420971 finetune.py:68] layer 25_v @ epoch 2 new loss 0.0002422608231427148 old loss 0.00025585421826690435 BETTER
I0314 05:31:00.728686 2420037 finetune.py:45] layer 24_q initial loss 0.00033638987224549055
I0314 05:31:00.878090 2421880 finetune.py:68] layer 26_v @ epoch 0 new loss 0.00042328942799940705 old loss 0.0007993309991434216 BETTER
I0314 05:31:24.002592 2420971 finetune.py:68] layer 25_v @ epoch 3 new loss 0.00023431141744367778 old loss 0.0002422608231427148 BETTER
I0314 05:31:29.995903 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 27 in 69.09752655029297s
I0314 05:31:33.313087 2422836 config.py:54] PyTorch version 2.1.1 available.
I0314 05:31:33.437635 2421880 finetune.py:68] layer 26_v @ epoch 1 new loss 0.00038403680082410574 old loss 0.00042328942799940705 BETTER
I0314 05:31:34.250138 2420037 finetune.py:68] layer 24_q @ epoch 0 new loss 0.00030807690927758813 old loss 0.00033638987224549055 BETTER
I0314 05:31:34.342665 2357215 quantize_finetune_llama.py:191] layer 28 gpu 0
I0314 05:31:34.398584 2422836 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:31:43.144180 2422836 finetune.py:45] layer 27_v initial loss 0.0005905501311644912
I0314 05:31:56.894023 2420971 finetune.py:68] layer 25_v @ epoch 4 new loss 0.00022822672326583415 old loss 0.00023431141744367778 BETTER
I0314 05:32:06.359607 2421880 finetune.py:68] layer 26_v @ epoch 2 new loss 0.0003648717247415334 old loss 0.00038403680082410574 BETTER
I0314 05:32:06.948665 2420971 finetune.py:45] layer 25_q initial loss 0.00030929927015677094
I0314 05:32:08.500073 2420037 finetune.py:68] layer 24_q @ epoch 1 new loss 0.0002990356006193906 old loss 0.00030807690927758813 BETTER
I0314 05:32:14.045463 2422836 finetune.py:68] layer 27_v @ epoch 0 new loss 0.00031924794893711805 old loss 0.0005905501311644912 BETTER
I0314 05:32:38.380105 2420971 finetune.py:68] layer 25_q @ epoch 0 new loss 0.0002819829969666898 old loss 0.00030929927015677094 BETTER
I0314 05:32:39.357024 2421880 finetune.py:68] layer 26_v @ epoch 3 new loss 0.00035240547731518745 old loss 0.0003648717247415334 BETTER
I0314 05:32:43.001508 2420037 finetune.py:68] layer 24_q @ epoch 2 new loss 0.0002926965826191008 old loss 0.0002990356006193906 BETTER
I0314 05:32:45.896082 2422836 finetune.py:68] layer 27_v @ epoch 1 new loss 0.00029953353805467486 old loss 0.00031924794893711805 BETTER
I0314 05:33:10.728127 2420971 finetune.py:68] layer 25_q @ epoch 1 new loss 0.0002743880613707006 old loss 0.0002819829969666898 BETTER
I0314 05:33:12.607799 2421880 finetune.py:68] layer 26_v @ epoch 4 new loss 0.000343592168064788 old loss 0.00035240547731518745 BETTER
I0314 05:33:17.476116 2420037 finetune.py:68] layer 24_q @ epoch 3 new loss 0.0002876160142477602 old loss 0.0002926965826191008 BETTER
I0314 05:33:17.883744 2422836 finetune.py:68] layer 27_v @ epoch 2 new loss 0.00028905243379995227 old loss 0.00029953353805467486 BETTER
I0314 05:33:23.442230 2421880 finetune.py:45] layer 26_q initial loss 0.0004516708431765437
I0314 05:33:43.241755 2420971 finetune.py:68] layer 25_q @ epoch 2 new loss 0.00026907114079222083 old loss 0.0002743880613707006 BETTER
I0314 05:33:50.115538 2422836 finetune.py:68] layer 27_v @ epoch 3 new loss 0.0002820473746396601 old loss 0.00028905243379995227 BETTER
I0314 05:33:52.126907 2420037 finetune.py:68] layer 24_q @ epoch 4 new loss 0.000283564644632861 old loss 0.0002876160142477602 BETTER
I0314 05:33:55.136259 2421880 finetune.py:68] layer 26_q @ epoch 0 new loss 0.00042222917545586824 old loss 0.0004516708431765437 BETTER
I0314 05:34:02.807088 2420037 finetune.py:45] layer 24_k initial loss 0.00035311575629748404
I0314 05:34:15.852381 2420971 finetune.py:68] layer 25_q @ epoch 3 new loss 0.00026487259310670197 old loss 0.00026907114079222083 BETTER
I0314 05:34:22.368990 2422836 finetune.py:68] layer 27_v @ epoch 4 new loss 0.00027657285681925714 old loss 0.0002820473746396601 BETTER
I0314 05:34:27.905515 2421880 finetune.py:68] layer 26_q @ epoch 1 new loss 0.00041039084317162633 old loss 0.00042222917545586824 BETTER
I0314 05:34:33.041467 2422836 finetune.py:45] layer 27_q initial loss 0.00039315238245762885
I0314 05:34:35.896918 2420037 finetune.py:68] layer 24_k @ epoch 0 new loss 0.00034471211256459355 old loss 0.00035311575629748404 BETTER
I0314 05:34:48.462013 2420971 finetune.py:68] layer 25_q @ epoch 4 new loss 0.00026163601432926953 old loss 0.00026487259310670197 BETTER
I0314 05:34:58.870684 2420971 finetune.py:45] layer 25_k initial loss 0.000331844697939232
I0314 05:35:00.444561 2421880 finetune.py:68] layer 26_q @ epoch 2 new loss 0.00040170218562707305 old loss 0.00041039084317162633 BETTER
I0314 05:35:04.384366 2422836 finetune.py:68] layer 27_q @ epoch 0 new loss 0.00035699739237315953 old loss 0.00039315238245762885 BETTER
I0314 05:35:09.755656 2420037 finetune.py:68] layer 24_k @ epoch 1 new loss 0.0003411555371712893 old loss 0.00034471211256459355 BETTER
I0314 05:35:30.326928 2420971 finetune.py:68] layer 25_k @ epoch 0 new loss 0.00031953409779816866 old loss 0.000331844697939232 BETTER
I0314 05:35:32.992036 2421880 finetune.py:68] layer 26_q @ epoch 3 new loss 0.00039501761784777045 old loss 0.00040170218562707305 BETTER
I0314 05:35:36.425097 2422836 finetune.py:68] layer 27_q @ epoch 1 new loss 0.0003475628618616611 old loss 0.00035699739237315953 BETTER
I0314 05:35:43.616595 2420037 finetune.py:68] layer 24_k @ epoch 2 new loss 0.0003382498398423195 old loss 0.0003411555371712893 BETTER
I0314 05:36:02.457222 2420971 finetune.py:68] layer 25_k @ epoch 1 new loss 0.00031688952003605664 old loss 0.00031953409779816866 BETTER
I0314 05:36:05.156837 2421880 finetune.py:68] layer 26_q @ epoch 4 new loss 0.00038954109186306596 old loss 0.00039501761784777045 BETTER
I0314 05:36:08.384998 2422836 finetune.py:68] layer 27_q @ epoch 2 new loss 0.000340821425197646 old loss 0.0003475628618616611 BETTER
I0314 05:36:15.513983 2421880 finetune.py:45] layer 26_k initial loss 0.0004964557592757046
I0314 05:36:17.355295 2420037 finetune.py:68] layer 24_k @ epoch 3 new loss 0.0003358254616614431 old loss 0.0003382498398423195 BETTER
I0314 05:36:34.487545 2420971 finetune.py:68] layer 25_k @ epoch 2 new loss 0.00031473455601371825 old loss 0.00031688952003605664 BETTER
I0314 05:36:40.527565 2422836 finetune.py:68] layer 27_q @ epoch 3 new loss 0.0003354840155225247 old loss 0.000340821425197646 BETTER
I0314 05:36:47.421854 2421880 finetune.py:68] layer 26_k @ epoch 0 new loss 0.000475357286632061 old loss 0.0004964557592757046 BETTER
I0314 05:36:51.359125 2420037 finetune.py:68] layer 24_k @ epoch 4 new loss 0.00033371205790899694 old loss 0.0003358254616614431 BETTER
I0314 05:37:02.754909 2420037 finetune.py:45] layer 24_o initial loss 0.0007219442632049322
I0314 05:37:07.899883 2420971 finetune.py:68] layer 25_k @ epoch 3 new loss 0.0003127524396404624 old loss 0.00031473455601371825 BETTER
I0314 05:37:12.770925 2422836 finetune.py:68] layer 27_q @ epoch 4 new loss 0.00033119440195150673 old loss 0.0003354840155225247 BETTER
I0314 05:37:20.200673 2421880 finetune.py:68] layer 26_k @ epoch 1 new loss 0.0004702535516116768 old loss 0.000475357286632061 BETTER
I0314 05:37:25.390182 2422836 finetune.py:45] layer 27_k initial loss 0.0004204667638987303
I0314 05:37:36.597514 2420037 finetune.py:68] layer 24_o @ epoch 0 new loss 0.0006856854306533933 old loss 0.0007219442632049322 BETTER
I0314 05:37:40.847311 2420971 finetune.py:68] layer 25_k @ epoch 4 new loss 0.0003112221311312169 old loss 0.0003127524396404624 BETTER
I0314 05:37:54.535559 2420971 finetune.py:45] layer 25_o initial loss 0.0006104937056079507
I0314 05:37:54.845247 2421880 finetune.py:68] layer 26_k @ epoch 2 new loss 0.0004662967985495925 old loss 0.0004702535516116768 BETTER
I0314 05:37:57.869217 2422836 finetune.py:68] layer 27_k @ epoch 0 new loss 0.0004073185846209526 old loss 0.0004204667638987303 BETTER
I0314 05:38:10.388842 2420037 finetune.py:68] layer 24_o @ epoch 1 new loss 0.0006750240572728217 old loss 0.0006856854306533933 BETTER
I0314 05:38:26.658489 2420971 finetune.py:68] layer 25_o @ epoch 0 new loss 0.0005916144582442939 old loss 0.0006104937056079507 BETTER
I0314 05:38:29.136501 2421880 finetune.py:68] layer 26_k @ epoch 3 new loss 0.00046299208770506084 old loss 0.0004662967985495925 BETTER
I0314 05:38:30.686236 2422836 finetune.py:68] layer 27_k @ epoch 1 new loss 0.000403918675146997 old loss 0.0004073185846209526 BETTER
I0314 05:38:44.060128 2420037 finetune.py:68] layer 24_o @ epoch 2 new loss 0.0006677955971099436 old loss 0.0006750240572728217 BETTER
I0314 05:38:58.323470 2420971 finetune.py:68] layer 25_o @ epoch 1 new loss 0.0005849298322573304 old loss 0.0005916144582442939 BETTER
I0314 05:39:01.381963 2421880 finetune.py:68] layer 26_k @ epoch 4 new loss 0.00046016142005100846 old loss 0.00046299208770506084 BETTER
I0314 05:39:02.381355 2422836 finetune.py:68] layer 27_k @ epoch 2 new loss 0.0004013024445157498 old loss 0.000403918675146997 BETTER
I0314 05:39:11.636768 2421880 finetune.py:45] layer 26_o initial loss 0.0009538651793263853
I0314 05:39:17.373748 2420037 finetune.py:68] layer 24_o @ epoch 3 new loss 0.0006622050423175097 old loss 0.0006677955971099436 BETTER
I0314 05:39:30.134075 2420971 finetune.py:68] layer 25_o @ epoch 2 new loss 0.0005804013344459236 old loss 0.0005849298322573304 BETTER
I0314 05:39:34.151517 2422836 finetune.py:68] layer 27_k @ epoch 3 new loss 0.0003991999546997249 old loss 0.0004013024445157498 BETTER
I0314 05:39:42.649368 2421880 finetune.py:68] layer 26_o @ epoch 0 new loss 0.0008935571531765163 old loss 0.0009538651793263853 BETTER
I0314 05:39:51.195178 2420037 finetune.py:68] layer 24_o @ epoch 4 new loss 0.0006578583270311356 old loss 0.0006622050423175097 BETTER
I0314 05:40:01.683124 2420971 finetune.py:68] layer 25_o @ epoch 3 new loss 0.0005768750561401248 old loss 0.0005804013344459236 BETTER
I0314 05:40:05.722468 2422836 finetune.py:68] layer 27_k @ epoch 4 new loss 0.0003972594568040222 old loss 0.0003991999546997249 BETTER
I0314 05:40:07.965467 2420037 finetune.py:45] layer 24_up initial loss 0.0014265282079577446
I0314 05:40:14.259323 2421880 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0008799917413853109 old loss 0.0008935571531765163 BETTER
I0314 05:40:15.955212 2422836 finetune.py:45] layer 27_o initial loss 0.0008070117328315973
I0314 05:40:33.213305 2420971 finetune.py:68] layer 25_o @ epoch 4 new loss 0.0005742704961448908 old loss 0.0005768750561401248 BETTER
I0314 05:40:38.418622 2420037 finetune.py:68] layer 24_up @ epoch 0 new loss 0.0014059627428650856 old loss 0.0014265282079577446 BETTER
I0314 05:40:46.069956 2421880 finetune.py:68] layer 26_o @ epoch 2 new loss 0.0008702563354745507 old loss 0.0008799917413853109 BETTER
I0314 05:40:46.170797 2422836 finetune.py:68] layer 27_o @ epoch 0 new loss 0.0007666847086511552 old loss 0.0008070117328315973 BETTER
I0314 05:40:50.127792 2420971 finetune.py:45] layer 25_up initial loss 0.0014380026841536164
I0314 05:41:09.894066 2420037 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0013941184151917696 old loss 0.0014059627428650856 BETTER
I0314 05:41:17.295385 2422836 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0007526609115302563 old loss 0.0007666847086511552 BETTER
I0314 05:41:17.840203 2421880 finetune.py:68] layer 26_o @ epoch 3 new loss 0.0008629963267594576 old loss 0.0008702563354745507 BETTER
I0314 05:41:19.097379 2420971 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0014138990081846714 old loss 0.0014380026841536164 BETTER
I0314 05:41:41.425323 2420037 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0013847118243575096 old loss 0.0013941184151917696 BETTER
I0314 05:41:48.387776 2422836 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0007429220131598413 old loss 0.0007526609115302563 BETTER
I0314 05:41:48.938196 2420971 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0014008405851200223 old loss 0.0014138990081846714 BETTER
I0314 05:41:49.650399 2421880 finetune.py:68] layer 26_o @ epoch 4 new loss 0.0008571464568376541 old loss 0.0008629963267594576 BETTER
I0314 05:42:06.157680 2421880 finetune.py:45] layer 26_up initial loss 0.0017956042429432273
I0314 05:42:13.024561 2420037 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0013770819641649723 old loss 0.0013847118243575096 BETTER
I0314 05:42:18.621486 2420971 finetune.py:68] layer 25_up @ epoch 2 new loss 0.001390736666508019 old loss 0.0014008405851200223 BETTER
I0314 05:42:19.260734 2422836 finetune.py:68] layer 27_o @ epoch 3 new loss 0.0007360903546214104 old loss 0.0007429220131598413 BETTER
I0314 05:42:35.194553 2421880 finetune.py:68] layer 26_up @ epoch 0 new loss 0.0017684195190668106 old loss 0.0017956042429432273 BETTER
I0314 05:42:44.583404 2420037 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0013706721365451813 old loss 0.0013770819641649723 BETTER
I0314 05:42:48.450816 2420971 finetune.py:68] layer 25_up @ epoch 3 new loss 0.001382965943776071 old loss 0.001390736666508019 BETTER
I0314 05:42:50.046800 2422836 finetune.py:68] layer 27_o @ epoch 4 new loss 0.0007303584134206176 old loss 0.0007360903546214104 BETTER
I0314 05:43:01.753237 2420037 finetune.py:45] layer 24_gate initial loss 0.001999309053644538
I0314 05:43:05.025788 2421880 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0017532012425363064 old loss 0.0017684195190668106 BETTER
I0314 05:43:06.738858 2422836 finetune.py:45] layer 27_up initial loss 0.0018055766122415662
I0314 05:43:18.459183 2420971 finetune.py:68] layer 25_up @ epoch 4 new loss 0.001376196974888444 old loss 0.001382965943776071 BETTER
I0314 05:43:30.668807 2420037 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.001989440293982625 old loss 0.001999309053644538 BETTER
I0314 05:43:35.065238 2421880 finetune.py:68] layer 26_up @ epoch 2 new loss 0.0017414437606930733 old loss 0.0017532012425363064 BETTER
I0314 05:43:35.318236 2422836 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0017669674707576632 old loss 0.0018055766122415662 BETTER
I0314 05:43:35.449038 2420971 finetune.py:45] layer 25_gate initial loss 0.0020768013782799244
I0314 05:44:00.466480 2420037 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.001982197631150484 old loss 0.001989440293982625 BETTER
I0314 05:44:02.886987 2420971 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.0020664185285568237 old loss 0.0020768013782799244 BETTER
I0314 05:44:04.807778 2422836 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0017480734968557954 old loss 0.0017669674707576632 BETTER
I0314 05:44:04.958539 2421880 finetune.py:68] layer 26_up @ epoch 3 new loss 0.001732135540805757 old loss 0.0017414437606930733 BETTER
I0314 05:44:30.644438 2420037 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.001975986175239086 old loss 0.001982197631150484 BETTER
I0314 05:44:31.661727 2420971 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.002058862242847681 old loss 0.0020664185285568237 BETTER
I0314 05:44:34.317965 2422836 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0017341808415949345 old loss 0.0017480734968557954 BETTER
I0314 05:44:35.037905 2421880 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0017241868190467358 old loss 0.001732135540805757 BETTER
I0314 05:44:52.094389 2421880 finetune.py:45] layer 26_gate initial loss 0.00251212902367115
I0314 05:45:00.165036 2420971 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.002052412601187825 old loss 0.002058862242847681 BETTER
I0314 05:45:00.783359 2420037 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0019705661106854677 old loss 0.001975986175239086 BETTER
I0314 05:45:03.882583 2422836 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0017230194061994553 old loss 0.0017341808415949345 BETTER
I0314 05:45:19.642780 2421880 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0025001107715070248 old loss 0.00251212902367115 BETTER
I0314 05:45:28.357863 2420971 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.002047036774456501 old loss 0.002052412601187825 BETTER
I0314 05:45:30.791777 2420037 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.001965995179489255 old loss 0.0019705661106854677 BETTER
I0314 05:45:33.302623 2422836 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0017137997783720493 old loss 0.0017230194061994553 BETTER
I0314 05:45:47.870229 2421880 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.0024914913810789585 old loss 0.0025001107715070248 BETTER
I0314 05:45:49.958338 2420037 finetune.py:45] layer 24_down initial loss 0.0031767133623361588
I0314 05:45:50.701236 2422836 finetune.py:45] layer 27_gate initial loss 0.0026256965938955545
I0314 05:45:56.494387 2420971 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0020422402303665876 old loss 0.002047036774456501 BETTER
I0314 05:46:15.502207 2420971 finetune.py:45] layer 25_down initial loss 0.003367064520716667
I0314 05:46:16.784608 2421880 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0024840254336595535 old loss 0.0024914913810789585 BETTER
I0314 05:46:17.518061 2420037 finetune.py:68] layer 24_down @ epoch 0 new loss 0.003175947582349181 old loss 0.0031767133623361588 BETTER
I0314 05:46:18.086762 2422836 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0026096911169588566 old loss 0.0026256965938955545 BETTER
I0314 05:46:41.357471 2420971 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0033663446083664894 old loss 0.003367064520716667 BETTER
I0314 05:46:45.331247 2421880 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.002477822592481971 old loss 0.0024840254336595535 BETTER
I0314 05:46:46.126815 2420037 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0031753540970385075 old loss 0.003175947582349181 BETTER
I0314 05:46:46.477927 2422836 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.002598819322884083 old loss 0.0026096911169588566 BETTER
I0314 05:47:08.503949 2420971 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0033657492604106665 old loss 0.0033663446083664894 BETTER
I0314 05:47:14.021167 2421880 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0024722639936953783 old loss 0.002477822592481971 BETTER
I0314 05:47:14.748204 2420037 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0031748537439852953 old loss 0.0031753540970385075 BETTER
I0314 05:47:14.931354 2422836 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.00258994335308671 old loss 0.002598819322884083 BETTER
I0314 05:47:32.318090 2421880 finetune.py:45] layer 26_down initial loss 0.003924424760043621
I0314 05:47:35.183182 2420971 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0033652540296316147 old loss 0.0033657492604106665 BETTER
I0314 05:47:43.199355 2422836 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.002582529094070196 old loss 0.00258994335308671 BETTER
I0314 05:47:43.211930 2420037 finetune.py:68] layer 24_down @ epoch 3 new loss 0.003174450481310487 old loss 0.0031748537439852953 BETTER
I0314 05:48:01.886527 2421880 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0039237202145159245 old loss 0.003924424760043621 BETTER
I0314 05:48:04.423653 2420971 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0033648330718278885 old loss 0.0033652540296316147 BETTER
I0314 05:48:12.694583 2420037 finetune.py:68] layer 24_down @ epoch 4 new loss 0.003174126148223877 old loss 0.003174450481310487 BETTER
I0314 05:48:12.791985 2422836 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.002576072234660387 old loss 0.002582529094070196 BETTER
24_v proxy err 0.0168044064193964 tr(WHW.T) 1394.900634765625
24_q proxy err 0.0036723718512803316 tr(WHW.T) 7027.22900390625
24_k proxy err 0.0025444028433412313 tr(WHW.T) 10362.16015625
24_o proxy err 0.011152281425893307 tr(WHW.T) 134.3756103515625
24_up proxy err 0.013045725412666798 tr(WHW.T) 2623.42333984375
24_gate proxy err 0.008311795070767403 tr(WHW.T) 4260.3369140625
24_down proxy err 0.015173062682151794 tr(WHW.T) 342.3114013671875
I0314 05:48:30.144022 2421880 finetune.py:68] layer 26_down @ epoch 1 new loss 0.003923162817955017 old loss 0.0039237202145159245 BETTER
I0314 05:48:32.448800 2420971 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0033644894137978554 old loss 0.0033648330718278885 BETTER
I0314 05:48:33.206027 2422836 finetune.py:45] layer 27_down initial loss 0.004303904715925455
25_v proxy err 0.015297290869057178 tr(WHW.T) 1707.664794921875
25_q proxy err 0.003958000335842371 tr(WHW.T) 7166.833984375
25_k proxy err 0.0029972861520946026 tr(WHW.T) 9643.5625
25_o proxy err 0.0133363613858819 tr(WHW.T) 83.82357788085938
25_up proxy err 0.0130000039935112 tr(WHW.T) 2805.828125
25_gate proxy err 0.008087409660220146 tr(WHW.T) 4660.86279296875
25_down proxy err 0.015083168633282185 tr(WHW.T) 375.97601318359375
I0314 05:48:56.959741 2421880 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0039226762019097805 old loss 0.003923162817955017 BETTER
I0314 05:48:58.475361 2422836 finetune.py:68] layer 27_down @ epoch 0 new loss 0.004303295165300369 old loss 0.004303904715925455 BETTER
I0314 05:49:23.827422 2421880 finetune.py:68] layer 26_down @ epoch 3 new loss 0.003922291565686464 old loss 0.0039226762019097805 BETTER
I0314 05:49:24.776329 2422836 finetune.py:68] layer 27_down @ epoch 1 new loss 0.00430277269333601 old loss 0.004303295165300369 BETTER
I0314 05:49:50.404245 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 28 in 73.87434482574463s
I0314 05:49:50.600600 2421880 finetune.py:68] layer 26_down @ epoch 4 new loss 0.00392194464802742 old loss 0.003922291565686464 BETTER
I0314 05:49:51.380469 2422836 finetune.py:68] layer 27_down @ epoch 2 new loss 0.004302340094000101 old loss 0.00430277269333601 BETTER
26_v proxy err 0.015344790183007717 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.003719539847224951 tr(WHW.T) 7475.1865234375
26_k proxy err 0.002685520565137267 tr(WHW.T) 10526.9345703125
26_o proxy err 0.00931167509406805 tr(WHW.T) 203.5560302734375
26_up proxy err 0.012205854058265686 tr(WHW.T) 3157.989501953125
26_gate proxy err 0.007528888527303934 tr(WHW.T) 5303.060546875
26_down proxy err 0.015494038350880146 tr(WHW.T) 403.9794006347656
I0314 05:49:53.554194 2434520 config.py:54] PyTorch version 2.1.1 available.
I0314 05:49:54.796019 2357215 quantize_finetune_llama.py:191] layer 29 gpu 1
I0314 05:49:54.849449 2434520 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:50:04.048671 2434520 finetune.py:45] layer 28_v initial loss 0.0007596868090331554
I0314 05:50:17.734277 2422836 finetune.py:68] layer 27_down @ epoch 3 new loss 0.004301974084228277 old loss 0.004302340094000101 BETTER
I0314 05:50:37.070844 2434520 finetune.py:68] layer 28_v @ epoch 0 new loss 0.0004247348406352103 old loss 0.0007596868090331554 BETTER
I0314 05:50:44.175646 2422836 finetune.py:68] layer 27_down @ epoch 4 new loss 0.004301663488149643 old loss 0.004301974084228277 BETTER
27_v proxy err 0.013926185667514801 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.00357781071215868 tr(WHW.T) 7695.13232421875
27_k proxy err 0.002631141571328044 tr(WHW.T) 10656.8681640625
27_o proxy err 0.012532011605799198 tr(WHW.T) 126.7615966796875
27_up proxy err 0.011171823367476463 tr(WHW.T) 3689.481689453125
27_gate proxy err 0.007115609012544155 tr(WHW.T) 5986.45068359375
27_down proxy err 0.01579369232058525 tr(WHW.T) 470.03680419921875
I0314 05:51:04.874883 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 29 in 69.66305756568909s
I0314 05:51:08.078200 2435428 config.py:54] PyTorch version 2.1.1 available.
I0314 05:51:09.124416 2357215 quantize_finetune_llama.py:191] layer 30 gpu 2
I0314 05:51:09.184582 2435428 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 05:51:11.483243 2434520 finetune.py:68] layer 28_v @ epoch 1 new loss 0.00039915036177262664 old loss 0.0004247348406352103 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:51:18.199699 2435428 finetune.py:45] layer 29_v initial loss 0.0007415175787173212
I0314 05:51:46.068314 2434520 finetune.py:68] layer 28_v @ epoch 2 new loss 0.000385259190807119 old loss 0.00039915036177262664 BETTER
I0314 05:51:49.539809 2435428 finetune.py:68] layer 29_v @ epoch 0 new loss 0.00046507452498190105 old loss 0.0007415175787173212 BETTER
I0314 05:52:18.558665 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 30 in 68.98380756378174s
I0314 05:52:20.909858 2434520 finetune.py:68] layer 28_v @ epoch 3 new loss 0.00037539302138611674 old loss 0.000385259190807119 BETTER
I0314 05:52:21.976066 2436283 config.py:54] PyTorch version 2.1.1 available.
I0314 05:52:22.006262 2435428 finetune.py:68] layer 29_v @ epoch 1 new loss 0.00043872944661416113 old loss 0.00046507452498190105 BETTER
I0314 05:52:23.013666 2357215 quantize_finetune_llama.py:191] layer 31 gpu 3
I0314 05:52:23.081524 2436283 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:52:32.091533 2436283 finetune.py:45] layer 30_v initial loss 0.0007376470603048801
I0314 05:52:54.401541 2435428 finetune.py:68] layer 29_v @ epoch 2 new loss 0.0004238914989400655 old loss 0.00043872944661416113 BETTER
I0314 05:52:55.918839 2434520 finetune.py:68] layer 28_v @ epoch 4 new loss 0.00036829677992500365 old loss 0.00037539302138611674 BETTER
I0314 05:53:03.535959 2436283 finetune.py:68] layer 30_v @ epoch 0 new loss 0.00042613912955857813 old loss 0.0007376470603048801 BETTER
I0314 05:53:06.351915 2434520 finetune.py:45] layer 28_q initial loss 0.0004953740281052887
I0314 05:53:26.933079 2435428 finetune.py:68] layer 29_v @ epoch 3 new loss 0.00041361802141182125 old loss 0.0004238914989400655 BETTER
I0314 05:53:33.305924 2357215 quantize_finetune_llama.py:218] computed original embedding for layer 31 in 69.9055688381195s
I0314 05:53:36.127959 2436283 finetune.py:68] layer 30_v @ epoch 1 new loss 0.00040613175951875746 old loss 0.00042613912955857813 BETTER
I0314 05:53:36.556626 2437178 config.py:54] PyTorch version 2.1.1 available.
I0314 05:53:37.670120 2437178 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 05:53:39.839294 2434520 finetune.py:68] layer 28_q @ epoch 0 new loss 0.0004638226528186351 old loss 0.0004953740281052887 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 05:53:46.496550 2437178 finetune.py:45] layer 31_v initial loss 0.0013285791501402855
I0314 05:53:59.904105 2435428 finetune.py:68] layer 29_v @ epoch 4 new loss 0.0004059001512359828 old loss 0.00041361802141182125 BETTER
I0314 05:54:08.700159 2436283 finetune.py:68] layer 30_v @ epoch 2 new loss 0.00039489957271143794 old loss 0.00040613175951875746 BETTER
I0314 05:54:10.161945 2435428 finetune.py:45] layer 29_q initial loss 0.0005214422708377242
I0314 05:54:14.124085 2434520 finetune.py:68] layer 28_q @ epoch 1 new loss 0.0004528324061539024 old loss 0.0004638226528186351 BETTER
I0314 05:54:17.481042 2437178 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0006707853171974421 old loss 0.0013285791501402855 BETTER
I0314 05:54:41.590171 2435428 finetune.py:68] layer 29_q @ epoch 0 new loss 0.0004922817461192608 old loss 0.0005214422708377242 BETTER
I0314 05:54:41.709244 2436283 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0003862603334710002 old loss 0.00039489957271143794 BETTER
I0314 05:54:48.436043 2434520 finetune.py:68] layer 28_q @ epoch 2 new loss 0.0004446081002242863 old loss 0.0004528324061539024 BETTER
I0314 05:54:49.408325 2437178 finetune.py:68] layer 31_v @ epoch 1 new loss 0.0006294578779488802 old loss 0.0006707853171974421 BETTER
I0314 05:55:13.950587 2435428 finetune.py:68] layer 29_q @ epoch 1 new loss 0.00048146533663384616 old loss 0.0004922817461192608 BETTER
I0314 05:55:14.959594 2436283 finetune.py:68] layer 30_v @ epoch 4 new loss 0.000380546844098717 old loss 0.0003862603334710002 BETTER
I0314 05:55:21.640101 2437178 finetune.py:68] layer 31_v @ epoch 2 new loss 0.0006143612554296851 old loss 0.0006294578779488802 BETTER
I0314 05:55:23.082962 2434520 finetune.py:68] layer 28_q @ epoch 3 new loss 0.0004383752529975027 old loss 0.0004446081002242863 BETTER
I0314 05:55:25.794436 2436283 finetune.py:45] layer 30_q initial loss 0.0005541062564589083
I0314 05:55:46.329904 2435428 finetune.py:68] layer 29_q @ epoch 2 new loss 0.00047422017087228596 old loss 0.00048146533663384616 BETTER
I0314 05:55:54.382203 2437178 finetune.py:68] layer 31_v @ epoch 3 new loss 0.0005939701804891229 old loss 0.0006143612554296851 BETTER
I0314 05:55:57.617953 2434520 finetune.py:68] layer 28_q @ epoch 4 new loss 0.0004332247481215745 old loss 0.0004383752529975027 BETTER
I0314 05:55:57.766502 2436283 finetune.py:68] layer 30_q @ epoch 0 new loss 0.0004943337407894433 old loss 0.0005541062564589083 BETTER
I0314 05:56:08.271496 2434520 finetune.py:45] layer 28_k initial loss 0.0005474311183206737
I0314 05:56:18.439643 2435428 finetune.py:68] layer 29_q @ epoch 3 new loss 0.00046780979027971625 old loss 0.00047422017087228596 BETTER
I0314 05:56:26.999178 2437178 finetune.py:68] layer 31_v @ epoch 4 new loss 0.0005778282065875828 old loss 0.0005939701804891229 BETTER
I0314 05:56:30.196970 2436283 finetune.py:68] layer 30_q @ epoch 1 new loss 0.0004823633353225887 old loss 0.0004943337407894433 BETTER
I0314 05:56:37.755285 2437178 finetune.py:45] layer 31_q initial loss 0.0011639120057225227
I0314 05:56:41.101875 2434520 finetune.py:68] layer 28_k @ epoch 0 new loss 0.0005343439406715333 old loss 0.0005474311183206737 BETTER
I0314 05:56:50.570170 2435428 finetune.py:68] layer 29_q @ epoch 4 new loss 0.0004631160700228065 old loss 0.00046780979027971625 BETTER
I0314 05:57:01.104397 2435428 finetune.py:45] layer 29_k initial loss 0.0005737438332289457
I0314 05:57:02.547480 2436283 finetune.py:68] layer 30_q @ epoch 2 new loss 0.00047384071513079107 old loss 0.0004823633353225887 BETTER
I0314 05:57:09.067696 2437178 finetune.py:68] layer 31_q @ epoch 0 new loss 0.0009783682180568576 old loss 0.0011639120057225227 BETTER
I0314 05:57:14.638642 2434520 finetune.py:68] layer 28_k @ epoch 1 new loss 0.0005299313343130052 old loss 0.0005343439406715333 BETTER
I0314 05:57:32.635840 2435428 finetune.py:68] layer 29_k @ epoch 0 new loss 0.0005582920275628567 old loss 0.0005737438332289457 BETTER
I0314 05:57:35.195233 2436283 finetune.py:68] layer 30_q @ epoch 3 new loss 0.00046751610352657735 old loss 0.00047384071513079107 BETTER
I0314 05:57:40.989682 2437178 finetune.py:68] layer 31_q @ epoch 1 new loss 0.0009407602483406663 old loss 0.0009783682180568576 BETTER
I0314 05:57:48.260366 2434520 finetune.py:68] layer 28_k @ epoch 2 new loss 0.0005264687351882458 old loss 0.0005299313343130052 BETTER
I0314 05:58:04.807125 2435428 finetune.py:68] layer 29_k @ epoch 1 new loss 0.0005534890806302428 old loss 0.0005582920275628567 BETTER
I0314 05:58:07.627962 2436283 finetune.py:68] layer 30_q @ epoch 4 new loss 0.0004635803052224219 old loss 0.00046751610352657735 BETTER
I0314 05:58:13.087975 2437178 finetune.py:68] layer 31_q @ epoch 2 new loss 0.0009139754110947251 old loss 0.0009407602483406663 BETTER
I0314 05:58:18.101873 2436283 finetune.py:45] layer 30_k initial loss 0.0006223447853699327
I0314 05:58:22.231456 2434520 finetune.py:68] layer 28_k @ epoch 3 new loss 0.0005234420532360673 old loss 0.0005264687351882458 BETTER
I0314 05:58:36.855874 2435428 finetune.py:68] layer 29_k @ epoch 2 new loss 0.0005497309030033648 old loss 0.0005534890806302428 BETTER
I0314 05:58:45.095464 2437178 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0008978736586868763 old loss 0.0009139754110947251 BETTER
I0314 05:58:49.533101 2436283 finetune.py:68] layer 30_k @ epoch 0 new loss 0.0005801960360258818 old loss 0.0006223447853699327 BETTER
I0314 05:58:57.621231 2434520 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0005208479124121368 old loss 0.0005234420532360673 BETTER
I0314 05:59:11.233008 2435428 finetune.py:68] layer 29_k @ epoch 3 new loss 0.0005466730799525976 old loss 0.0005497309030033648 BETTER
I0314 05:59:12.394107 2434520 finetune.py:45] layer 28_o initial loss 0.0010567954741418362
I0314 05:59:18.377669 2437178 finetune.py:68] layer 31_q @ epoch 4 new loss 0.000883424945641309 old loss 0.0008978736586868763 BETTER
I0314 05:59:22.818626 2436283 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0005744988447986543 old loss 0.0005801960360258818 BETTER
I0314 05:59:30.259215 2437178 finetune.py:45] layer 31_k initial loss 0.0011804469395428896
I0314 05:59:44.775002 2435428 finetune.py:68] layer 29_k @ epoch 4 new loss 0.0005439722444862127 old loss 0.0005466730799525976 BETTER
I0314 05:59:46.120172 2434520 finetune.py:68] layer 28_o @ epoch 0 new loss 0.0010073212906718254 old loss 0.0010567954741418362 BETTER
I0314 05:59:56.860545 2436283 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0005719993496313691 old loss 0.0005744988447986543 BETTER
I0314 05:59:59.715295 2435428 finetune.py:45] layer 29_o initial loss 0.001024922588840127
I0314 06:00:02.899498 2437178 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0010515603935346007 old loss 0.0011804469395428896 BETTER
I0314 06:00:19.706959 2434520 finetune.py:68] layer 28_o @ epoch 1 new loss 0.0009903792524710298 old loss 0.0010073212906718254 BETTER
I0314 06:00:31.426858 2436283 finetune.py:68] layer 30_k @ epoch 3 new loss 0.000568198855035007 old loss 0.0005719993496313691 BETTER
I0314 06:00:31.992736 2435428 finetune.py:68] layer 29_o @ epoch 0 new loss 0.0009787544840946794 old loss 0.001024922588840127 BETTER
I0314 06:00:36.649563 2437178 finetune.py:68] layer 31_k @ epoch 1 new loss 0.001030145795084536 old loss 0.0010515603935346007 BETTER
I0314 06:00:53.312371 2434520 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0009785568108782172 old loss 0.0009903792524710298 BETTER
I0314 06:01:03.671859 2435428 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0009656734764575958 old loss 0.0009787544840946794 BETTER
I0314 06:01:03.964138 2436283 finetune.py:68] layer 30_k @ epoch 4 new loss 0.0005655650165863335 old loss 0.000568198855035007 BETTER
I0314 06:01:08.590986 2437178 finetune.py:68] layer 31_k @ epoch 2 new loss 0.001018152921460569 old loss 0.001030145795084536 BETTER
I0314 06:01:14.306243 2436283 finetune.py:45] layer 30_o initial loss 0.0012008089106529951
I0314 06:01:26.415422 2434520 finetune.py:68] layer 28_o @ epoch 3 new loss 0.0009698442881926894 old loss 0.0009785568108782172 BETTER
I0314 06:01:35.023169 2435428 finetune.py:68] layer 29_o @ epoch 2 new loss 0.0009569526882842183 old loss 0.0009656734764575958 BETTER
I0314 06:01:40.243728 2437178 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0010070641292259097 old loss 0.001018152921460569 BETTER
I0314 06:01:45.218487 2436283 finetune.py:68] layer 30_o @ epoch 0 new loss 0.0011045564897358418 old loss 0.0012008089106529951 BETTER
I0314 06:01:59.528908 2434520 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0009625962120480835 old loss 0.0009698442881926894 BETTER
I0314 06:02:06.492387 2435428 finetune.py:68] layer 29_o @ epoch 3 new loss 0.0009503350011073053 old loss 0.0009569526882842183 BETTER
I0314 06:02:11.835194 2437178 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0010003262432292104 old loss 0.0010070641292259097 BETTER
I0314 06:02:16.676321 2434520 finetune.py:45] layer 28_up initial loss 0.00223913905210793
I0314 06:02:16.689921 2436283 finetune.py:68] layer 30_o @ epoch 1 new loss 0.001077563501894474 old loss 0.0011045564897358418 BETTER
I0314 06:02:22.489220 2437178 finetune.py:45] layer 31_o initial loss 0.00204252521507442
I0314 06:02:37.861429 2435428 finetune.py:68] layer 29_o @ epoch 4 new loss 0.0009454176761209965 old loss 0.0009503350011073053 BETTER
I0314 06:02:47.088116 2434520 finetune.py:68] layer 28_up @ epoch 0 new loss 0.002182029653340578 old loss 0.00223913905210793 BETTER
I0314 06:02:48.375838 2436283 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0010608375305309892 old loss 0.001077563501894474 BETTER
I0314 06:02:52.559533 2437178 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0017126050079241395 old loss 0.00204252521507442 BETTER
I0314 06:02:54.508809 2435428 finetune.py:45] layer 29_up initial loss 0.0024664178490638733
I0314 06:03:18.561132 2434520 finetune.py:68] layer 28_up @ epoch 1 new loss 0.002156306290999055 old loss 0.002182029653340578 BETTER
I0314 06:03:20.024433 2436283 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0010483779478818178 old loss 0.0010608375305309892 BETTER
I0314 06:03:23.346716 2435428 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0023731368128210306 old loss 0.0024664178490638733 BETTER
I0314 06:03:23.492517 2437178 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0016389077063649893 old loss 0.0017126050079241395 BETTER
I0314 06:03:50.323896 2434520 finetune.py:68] layer 28_up @ epoch 2 new loss 0.002137117786332965 old loss 0.002156306290999055 BETTER
I0314 06:03:51.806319 2436283 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0010388368973508477 old loss 0.0010483779478818178 BETTER
I0314 06:03:52.928445 2435428 finetune.py:68] layer 29_up @ epoch 1 new loss 0.002339463448151946 old loss 0.0023731368128210306 BETTER
I0314 06:03:54.463004 2437178 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0015979334712028503 old loss 0.0016389077063649893 BETTER
I0314 06:04:08.611270 2436283 finetune.py:45] layer 30_up initial loss 0.0036435709334909916
I0314 06:04:22.142213 2434520 finetune.py:68] layer 28_up @ epoch 3 new loss 0.0021224801894277334 old loss 0.002137117786332965 BETTER
I0314 06:04:22.712391 2435428 finetune.py:68] layer 29_up @ epoch 2 new loss 0.002315883757546544 old loss 0.002339463448151946 BETTER
I0314 06:04:25.376783 2437178 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0015689439605921507 old loss 0.0015979334712028503 BETTER
I0314 06:04:37.745853 2436283 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0033337525092065334 old loss 0.0036435709334909916 BETTER
I0314 06:04:52.650244 2435428 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0022976624313741922 old loss 0.002315883757546544 BETTER
I0314 06:04:54.015007 2434520 finetune.py:68] layer 28_up @ epoch 4 new loss 0.0021101385354995728 old loss 0.0021224801894277334 BETTER
I0314 06:04:56.426323 2437178 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0015514609403908253 old loss 0.0015689439605921507 BETTER
I0314 06:05:07.849399 2436283 finetune.py:68] layer 30_up @ epoch 1 new loss 0.00323297455906868 old loss 0.0033337525092065334 BETTER
I0314 06:05:11.244025 2434520 finetune.py:45] layer 28_gate initial loss 0.0032130663748830557
I0314 06:05:13.517693 2437178 finetune.py:45] layer 31_up initial loss 0.00865569431334734
I0314 06:05:22.671834 2435428 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0022826914209872484 old loss 0.0022976624313741922 BETTER
I0314 06:05:38.348080 2436283 finetune.py:68] layer 30_up @ epoch 2 new loss 0.003159688785672188 old loss 0.00323297455906868 BETTER
I0314 06:05:40.368996 2435428 finetune.py:45] layer 29_gate initial loss 0.003600512631237507
I0314 06:05:40.989828 2434520 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0031891013495624065 old loss 0.0032130663748830557 BETTER
I0314 06:05:42.610489 2437178 finetune.py:68] layer 31_up @ epoch 0 new loss 0.006777806673198938 old loss 0.00865569431334734 BETTER
I0314 06:06:08.049909 2435428 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0035641209688037634 old loss 0.003600512631237507 BETTER
I0314 06:06:08.870050 2436283 finetune.py:68] layer 30_up @ epoch 3 new loss 0.003106995951384306 old loss 0.003159688785672188 BETTER
I0314 06:06:11.010242 2434520 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.003174185287207365 old loss 0.0031891013495624065 BETTER
I0314 06:06:12.144011 2437178 finetune.py:68] layer 31_up @ epoch 1 new loss 0.006385291926562786 old loss 0.006777806673198938 BETTER
I0314 06:06:36.506640 2435428 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.003543953411281109 old loss 0.0035641209688037634 BETTER
I0314 06:06:39.372114 2436283 finetune.py:68] layer 30_up @ epoch 4 new loss 0.003062458708882332 old loss 0.003106995951384306 BETTER
I0314 06:06:41.350558 2434520 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.003161941422149539 old loss 0.003174185287207365 BETTER
I0314 06:06:41.894370 2437178 finetune.py:68] layer 31_up @ epoch 2 new loss 0.006137127988040447 old loss 0.006385291926562786 BETTER
I0314 06:06:56.761201 2436283 finetune.py:45] layer 30_gate initial loss 0.004902959335595369
I0314 06:07:04.852035 2435428 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.003527888096868992 old loss 0.003543953411281109 BETTER
I0314 06:07:11.610471 2434520 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0031516451854258776 old loss 0.003161941422149539 BETTER
I0314 06:07:11.686115 2437178 finetune.py:68] layer 31_up @ epoch 3 new loss 0.005942100193351507 old loss 0.006137127988040447 BETTER
I0314 06:07:24.375882 2436283 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.004788222722709179 old loss 0.004902959335595369 BETTER
I0314 06:07:33.154874 2435428 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.003514630952849984 old loss 0.003527888096868992 BETTER
I0314 06:07:41.377380 2437178 finetune.py:68] layer 31_up @ epoch 4 new loss 0.005786433350294828 old loss 0.005942100193351507 BETTER
I0314 06:07:41.827519 2434520 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.003142803441733122 old loss 0.0031516451854258776 BETTER
I0314 06:07:52.785909 2436283 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.004725069273263216 old loss 0.004788222722709179 BETTER
I0314 06:07:58.940397 2437178 finetune.py:45] layer 31_gate initial loss 0.009135108441114426
I0314 06:08:01.529592 2434520 finetune.py:45] layer 28_down initial loss 0.0054125175811350346
I0314 06:08:01.810262 2435428 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0035033910535275936 old loss 0.003514630952849984 BETTER
I0314 06:08:20.969008 2435428 finetune.py:45] layer 29_down initial loss 0.006492920219898224
I0314 06:08:21.268810 2436283 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0046675559133291245 old loss 0.004725069273263216 BETTER
I0314 06:08:26.284382 2437178 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.008422089740633965 old loss 0.009135108441114426 BETTER
I0314 06:08:28.912290 2434520 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0054114512167871 old loss 0.0054125175811350346 BETTER
I0314 06:08:47.044629 2435428 finetune.py:68] layer 29_down @ epoch 0 new loss 0.006491792853921652 old loss 0.006492920219898224 BETTER
I0314 06:08:49.730607 2436283 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.00462568923830986 old loss 0.0046675559133291245 BETTER
I0314 06:08:54.515117 2437178 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.008201654069125652 old loss 0.008422089740633965 BETTER
I0314 06:08:57.470747 2434520 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0054105776362121105 old loss 0.0054114512167871 BETTER
I0314 06:09:14.083352 2435428 finetune.py:68] layer 29_down @ epoch 1 new loss 0.006490826606750488 old loss 0.006491792853921652 BETTER
I0314 06:09:18.502750 2436283 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.004589403048157692 old loss 0.00462568923830986 BETTER
I0314 06:09:22.786040 2437178 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.00806016568094492 old loss 0.008201654069125652 BETTER
I0314 06:09:25.921717 2434520 finetune.py:68] layer 28_down @ epoch 2 new loss 0.005409854929894209 old loss 0.0054105776362121105 BETTER
I0314 06:09:37.550523 2436283 finetune.py:45] layer 30_down initial loss 0.01275719702243805
I0314 06:09:41.034288 2435428 finetune.py:68] layer 29_down @ epoch 2 new loss 0.006489958148449659 old loss 0.006490826606750488 BETTER
I0314 06:09:51.052800 2437178 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.007954082451760769 old loss 0.00806016568094492 BETTER
I0314 06:09:54.523642 2434520 finetune.py:68] layer 28_down @ epoch 3 new loss 0.005409227218478918 old loss 0.005409854929894209 BETTER
I0314 06:10:05.340144 2436283 finetune.py:68] layer 30_down @ epoch 0 new loss 0.012734471820294857 old loss 0.01275719702243805 BETTER
I0314 06:10:10.430870 2435428 finetune.py:68] layer 29_down @ epoch 3 new loss 0.00648918654769659 old loss 0.006489958148449659 BETTER
I0314 06:10:22.536134 2437178 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.007866858504712582 old loss 0.007954082451760769 BETTER
I0314 06:10:25.116461 2434520 finetune.py:68] layer 28_down @ epoch 4 new loss 0.005408682394772768 old loss 0.005409227218478918 BETTER
28_v proxy err 0.013056372292339802 tr(WHW.T) 2018.944091796875
28_q proxy err 0.0036959555000066757 tr(WHW.T) 7653.05712890625
28_k proxy err 0.0027254363521933556 tr(WHW.T) 10581.365234375
28_o proxy err 0.01039859838783741 tr(WHW.T) 195.78192138671875
28_up proxy err 0.009396999143064022 tr(WHW.T) 4660.63427734375
28_gate proxy err 0.006877100095152855 tr(WHW.T) 6535.8681640625
28_down proxy err 0.01590822823345661 tr(WHW.T) 608.9929809570312
I0314 06:10:32.819707 2436283 finetune.py:68] layer 30_down @ epoch 1 new loss 0.012712637893855572 old loss 0.012734471820294857 BETTER
I0314 06:10:37.957520 2435428 finetune.py:68] layer 29_down @ epoch 4 new loss 0.006488511338829994 old loss 0.00648918654769659 BETTER
29_v proxy err 0.013850681483745575 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.003653228748589754 tr(WHW.T) 7233.27587890625
29_k proxy err 0.002556420164182782 tr(WHW.T) 10610.15234375
29_o proxy err 0.008819689974188805 tr(WHW.T) 208.56369018554688
29_up proxy err 0.007550252601504326 tr(WHW.T) 6067.5615234375
29_gate proxy err 0.0063550123013556 tr(WHW.T) 7358.677734375
29_down proxy err 0.016270071268081665 tr(WHW.T) 790.5086669921875
I0314 06:10:43.532892 2437178 finetune.py:45] layer 31_down initial loss 0.036236926913261414
I0314 06:10:59.556318 2436283 finetune.py:68] layer 30_down @ epoch 2 new loss 0.01269126683473587 old loss 0.012712637893855572 BETTER
I0314 06:11:08.369641 2437178 finetune.py:68] layer 31_down @ epoch 0 new loss 0.036131516098976135 old loss 0.036236926913261414 BETTER
I0314 06:11:26.265492 2436283 finetune.py:68] layer 30_down @ epoch 3 new loss 0.012673615477979183 old loss 0.01269126683473587 BETTER
I0314 06:11:34.505324 2437178 finetune.py:68] layer 31_down @ epoch 1 new loss 0.036044664680957794 old loss 0.036131516098976135 BETTER
I0314 06:11:53.036173 2436283 finetune.py:68] layer 30_down @ epoch 4 new loss 0.012656038627028465 old loss 0.012673615477979183 BETTER
30_v proxy err 0.01140847709029913 tr(WHW.T) 2261.489501953125
30_q proxy err 0.003473029239103198 tr(WHW.T) 7819.20751953125
30_k proxy err 0.0026290228124707937 tr(WHW.T) 10569.58984375
30_o proxy err 0.009255338460206985 tr(WHW.T) 253.04849243164062
30_up proxy err 0.0047441809438169 tr(WHW.T) 10000.390625
30_gate proxy err 0.004393888637423515 tr(WHW.T) 10984.025390625
30_down proxy err 0.00944247841835022 tr(WHW.T) 3617.936767578125
I0314 06:12:00.617670 2437178 finetune.py:68] layer 31_down @ epoch 2 new loss 0.03597395494580269 old loss 0.036044664680957794 BETTER
I0314 06:12:26.696164 2437178 finetune.py:68] layer 31_down @ epoch 3 new loss 0.03591563552618027 old loss 0.03597395494580269 BETTER
I0314 06:12:52.861808 2437178 finetune.py:68] layer 31_down @ epoch 4 new loss 0.03586796298623085 old loss 0.03591563552618027 BETTER
31_v proxy err 0.01342795044183731 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.0027575998101383448 tr(WHW.T) 6858.9833984375
31_k proxy err 0.0019204583950340748 tr(WHW.T) 10290.98828125
31_o proxy err 0.007036363240331411 tr(WHW.T) 459.6426086425781
31_up proxy err 0.0027827685698866844 tr(WHW.T) 14569.1435546875
31_gate proxy err 0.0027681810315698385 tr(WHW.T) 14838.548828125
31_down proxy err 0.0066152289509773254 tr(WHW.T) 18247.57421875
