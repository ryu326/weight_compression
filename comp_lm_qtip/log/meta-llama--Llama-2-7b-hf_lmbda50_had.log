I0314 06:51:10.547751 2474246 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.41it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.68it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.08it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.78it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.55it/s]
I0314 06:51:12.547252 2474246 quantize_finetune_llama.py:142] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:22,  1.38it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:22,  1.35it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:21,  1.38it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:20,  1.38it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:19,  1.35it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:19,  1.34it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:05<00:18,  1.34it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:17,  1.35it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:16,  1.36it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:07<00:16,  1.35it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:08<00:15,  1.34it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:14,  1.36it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:14,  1.35it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:10<00:13,  1.37it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:10<00:12,  1.41it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:11,  1.40it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:12<00:10,  1.42it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:13<00:09,  1.42it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:09,  1.43it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:14<00:08,  1.42it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:15<00:07,  1.42it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:07,  1.41it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:06,  1.40it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:05,  1.42it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:18<00:04,  1.41it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.40it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:19<00:03,  1.42it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:20<00:02,  1.43it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:02,  1.44it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.42it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:22<00:00,  1.44it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.43it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.40it/s]
I0314 06:51:45.327250 2474246 quantize_finetune_llama.py:167] loaded compression model
I0314 06:52:00.067629 2474246 quantize_finetune_llama.py:171] loaded dataset and devset
I0314 06:52:05.417234 2474246 quantize_finetune_llama.py:191] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 06:53:27.878401 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 0 in 82.3422338962555s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 06:53:53.271326 2475998 config.py:54] PyTorch version 2.1.1 available.
I0314 06:53:54.250844 2474246 quantize_finetune_llama.py:191] layer 1 gpu 1
I0314 06:53:54.308361 2475998 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 06:54:03.018689 2475998 finetune.py:45] layer 0_v initial loss 1.1026614629372489e-05
I0314 06:54:11.864123 2475998 finetune.py:45] layer 0_q initial loss 1.1112493666587397e-05
I0314 06:54:20.740203 2475998 finetune.py:45] layer 0_k initial loss 1.1385009202058427e-05
I0314 06:54:29.557727 2475998 finetune.py:45] layer 0_o initial loss 1.3616388059745077e-05
I0314 06:54:44.774528 2475998 finetune.py:45] layer 0_up initial loss 1.3759028661297634e-05
I0314 06:55:00.049605 2475998 finetune.py:45] layer 0_gate initial loss 1.3949475032859482e-05
I0314 06:55:04.176647 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 1 in 69.75373935699463s
I0314 06:55:15.371982 2476976 config.py:54] PyTorch version 2.1.1 available.
I0314 06:55:16.456392 2474246 quantize_finetune_llama.py:191] layer 2 gpu 2
I0314 06:55:16.526164 2476976 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 06:55:22.391228 2475998 finetune.py:45] layer 0_down initial loss 1.5146400073717814e-05
0_v proxy err 0.08050522953271866 tr(WHW.T) 4.225186347961426
0_q proxy err 0.000483269861433655 tr(WHW.T) 2710.363037109375
0_k proxy err 0.0005285662482492626 tr(WHW.T) 1698.7349853515625
0_o proxy err 0.00559605797752738 tr(WHW.T) 0.9668058156967163
0_up proxy err 0.009770506992936134 tr(WHW.T) 43.27138900756836
0_gate proxy err 0.006797471549361944 tr(WHW.T) 63.47430419921875
0_down proxy err 0.006767538841813803 tr(WHW.T) 0.656814694404602
I0314 06:55:25.656922 2476976 finetune.py:45] layer 1_v initial loss 0.0001730442891130224
I0314 06:55:34.118330 2476976 finetune.py:45] layer 1_q initial loss 0.0001648545148782432
I0314 06:55:42.816256 2476976 finetune.py:45] layer 1_k initial loss 0.00015851663192734122
I0314 06:55:51.576045 2476976 finetune.py:45] layer 1_o initial loss 0.00017002277309074998
I0314 06:56:06.772921 2476976 finetune.py:45] layer 1_up initial loss 0.0003934494743589312
I0314 06:56:22.007431 2476976 finetune.py:45] layer 1_gate initial loss 0.0008811406441964209
I0314 06:56:28.363642 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 2 in 71.70531678199768s
I0314 06:56:40.518395 2477993 config.py:54] PyTorch version 2.1.1 available.
I0314 06:56:41.462702 2476976 finetune.py:45] layer 1_down initial loss 0.004185318015515804
I0314 06:56:41.568388 2474246 quantize_finetune_llama.py:191] layer 3 gpu 3
I0314 06:56:41.624509 2477993 data_utils.py:336] using 256 training seqs, 128 validation seqs
1_v proxy err 0.11517074704170227 tr(WHW.T) 16.465883255004883
1_q proxy err 0.0006661515799351037 tr(WHW.T) 4778.43994140625
1_k proxy err 0.0006369134061969817 tr(WHW.T) 4995.39208984375
1_o proxy err 0.019083527848124504 tr(WHW.T) 1.1115814447402954
1_up proxy err 0.010265398770570755 tr(WHW.T) 109.66383361816406
1_gate proxy err 0.005307184066623449 tr(WHW.T) 221.3038787841797
1_down proxy err 0.0060687605291605 tr(WHW.T) 2041.4736328125
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 06:56:50.441771 2477993 finetune.py:45] layer 2_v initial loss 1.862994031398557e-05
I0314 06:56:59.178872 2477993 finetune.py:45] layer 2_q initial loss 1.9955170500907116e-05
I0314 06:57:08.058738 2477993 finetune.py:45] layer 2_k initial loss 2.1524663679883815e-05
I0314 06:57:16.894421 2477993 finetune.py:45] layer 2_o initial loss 2.869275886041578e-05
I0314 06:57:31.891001 2477993 finetune.py:45] layer 2_up initial loss 3.352238854859024e-05
I0314 06:57:47.125272 2477993 finetune.py:45] layer 2_gate initial loss 3.728556839632802e-05
I0314 06:57:54.986104 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 3 in 73.29560565948486s
I0314 06:58:05.633978 2478993 config.py:54] PyTorch version 2.1.1 available.
I0314 06:58:05.789823 2477993 finetune.py:45] layer 2_down initial loss 4.696658652392216e-05
I0314 06:58:06.627483 2474246 quantize_finetune_llama.py:191] layer 4 gpu 0
I0314 06:58:06.682355 2478993 data_utils.py:336] using 256 training seqs, 128 validation seqs
2_v proxy err 0.02979182079434395 tr(WHW.T) 136.67332458496094
2_q proxy err 0.0007893718429841101 tr(WHW.T) 7752.85205078125
2_k proxy err 0.0006276157801039517 tr(WHW.T) 10205.837890625
2_o proxy err 0.015836002305150032 tr(WHW.T) 1.4603197574615479
2_up proxy err 0.011961858719587326 tr(WHW.T) 193.43603515625
2_gate proxy err 0.007785068824887276 tr(WHW.T) 306.6622619628906
2_down proxy err 0.013755856081843376 tr(WHW.T) 3.010739803314209
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 06:58:16.461421 2478993 finetune.py:45] layer 3_v initial loss 2.8852718969574198e-05
I0314 06:58:25.099450 2478993 finetune.py:45] layer 3_q initial loss 3.166556780342944e-05
I0314 06:58:33.639237 2478993 finetune.py:45] layer 3_k initial loss 3.4973432775586843e-05
I0314 06:58:42.343950 2478993 finetune.py:45] layer 3_o initial loss 5.064894139650278e-05
I0314 06:58:57.358199 2478993 finetune.py:45] layer 3_up initial loss 6.219327042344958e-05
I0314 06:59:12.522369 2478993 finetune.py:45] layer 3_gate initial loss 7.077303598634899e-05
I0314 06:59:18.408847 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 4 in 70.30491828918457s
I0314 06:59:21.590251 2479910 config.py:54] PyTorch version 2.1.1 available.
I0314 06:59:22.605787 2474246 quantize_finetune_llama.py:191] layer 5 gpu 1
I0314 06:59:22.680852 2479910 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 06:59:29.165953 2478993 finetune.py:45] layer 3_down initial loss 9.112621773965657e-05
3_v proxy err 0.027675889432430267 tr(WHW.T) 284.77557373046875
3_q proxy err 0.0014693690463900566 tr(WHW.T) 7217.63720703125
3_k proxy err 0.001099240849725902 tr(WHW.T) 10074.73828125
3_o proxy err 0.015836071223020554 tr(WHW.T) 3.3527450561523438
3_up proxy err 0.013628588058054447 tr(WHW.T) 284.7950744628906
3_gate proxy err 0.008388304151594639 tr(WHW.T) 478.13714599609375
3_down proxy err 0.014065581373870373 tr(WHW.T) 6.133229732513428
I0314 06:59:31.926140 2479910 finetune.py:45] layer 4_v initial loss 5.182664972380735e-05
I0314 06:59:40.827924 2479910 finetune.py:45] layer 4_q initial loss 5.655842323903926e-05
I0314 06:59:49.859655 2479910 finetune.py:45] layer 4_k initial loss 6.206548277987167e-05
I0314 06:59:58.753242 2479910 finetune.py:45] layer 4_o initial loss 8.441584213869646e-05
I0314 07:00:14.193853 2479910 finetune.py:45] layer 4_up initial loss 0.00010596946958685294
I0314 07:00:29.777512 2479910 finetune.py:45] layer 4_gate initial loss 0.00012022918235743418
I0314 07:00:32.512616 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 5 in 69.46000099182129s
I0314 07:00:35.728780 2480809 config.py:54] PyTorch version 2.1.1 available.
I0314 07:00:36.737080 2474246 quantize_finetune_llama.py:191] layer 6 gpu 2
I0314 07:00:36.808746 2480809 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:00:45.701892 2480809 finetune.py:45] layer 5_v initial loss 7.82306888140738e-05
I0314 07:00:46.474298 2479910 finetune.py:45] layer 4_down initial loss 0.00015821245324332267
4_v proxy err 0.02675006166100502 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0014243618352338672 tr(WHW.T) 6914.9892578125
4_k proxy err 0.000997802009806037 tr(WHW.T) 10415.33203125
4_o proxy err 0.015748750418424606 tr(WHW.T) 5.139806270599365
4_up proxy err 0.01343252882361412 tr(WHW.T) 397.6960144042969
4_gate proxy err 0.006838975474238396 tr(WHW.T) 821.1856689453125
4_down proxy err 0.014100339263677597 tr(WHW.T) 11.562739372253418
I0314 07:00:54.519298 2480809 finetune.py:45] layer 5_q initial loss 8.371375588467345e-05
I0314 07:01:03.300430 2480809 finetune.py:45] layer 5_k initial loss 9.045968181453645e-05
I0314 07:01:12.334809 2480809 finetune.py:45] layer 5_o initial loss 0.00013423673226498067
I0314 07:01:27.892925 2480809 finetune.py:45] layer 5_up initial loss 0.00016826798673719168
I0314 07:01:43.338593 2480809 finetune.py:45] layer 5_gate initial loss 0.00018961918249260634
I0314 07:01:47.367021 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 6 in 70.18118119239807s
I0314 07:01:50.528176 2481727 config.py:54] PyTorch version 2.1.1 available.
I0314 07:01:51.600760 2474246 quantize_finetune_llama.py:191] layer 7 gpu 3
I0314 07:01:51.670688 2481727 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:02:00.006749 2480809 finetune.py:45] layer 5_down initial loss 0.00024340132949873805
I0314 07:02:00.622311 2481727 finetune.py:45] layer 6_v initial loss 9.743315604282543e-05
5_v proxy err 0.0265257116407156 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0015749051235616207 tr(WHW.T) 6770.97509765625
5_k proxy err 0.0010229730978608131 tr(WHW.T) 10841.955078125
5_o proxy err 0.022263474762439728 tr(WHW.T) 7.947142601013184
5_up proxy err 0.013189880177378654 tr(WHW.T) 506.6408386230469
5_gate proxy err 0.006375222001224756 tr(WHW.T) 1104.867919921875
5_down proxy err 0.014837494120001793 tr(WHW.T) 15.6494779586792
I0314 07:02:09.538055 2481727 finetune.py:45] layer 6_q initial loss 0.00010913154983427376
I0314 07:02:18.625352 2481727 finetune.py:45] layer 6_k initial loss 0.00012353458441793919
I0314 07:02:27.896251 2481727 finetune.py:45] layer 6_o initial loss 0.00018120865570381284
I0314 07:02:43.320118 2481727 finetune.py:45] layer 6_up initial loss 0.0002342511434108019
I0314 07:02:58.785605 2481727 finetune.py:45] layer 6_gate initial loss 0.0002644786436576396
I0314 07:03:01.758398 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 7 in 69.72352051734924s
I0314 07:03:04.959063 2482627 config.py:54] PyTorch version 2.1.1 available.
I0314 07:03:06.018702 2474246 quantize_finetune_llama.py:191] layer 8 gpu 0
I0314 07:03:06.085084 2482627 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:03:16.305054 2481727 finetune.py:45] layer 6_down initial loss 0.00034639457589946687
I0314 07:03:16.307569 2482627 finetune.py:45] layer 7_v initial loss 0.0001247208274435252
6_v proxy err 0.025919219478964806 tr(WHW.T) 443.5464782714844
6_q proxy err 0.001983138034120202 tr(WHW.T) 7576.53857421875
6_k proxy err 0.001473771408200264 tr(WHW.T) 10409.4033203125
6_o proxy err 0.0205326396971941 tr(WHW.T) 11.564380645751953
6_up proxy err 0.0132152633741498 tr(WHW.T) 617.2608642578125
6_gate proxy err 0.005591365043073893 tr(WHW.T) 1554.7271728515625
6_down proxy err 0.015311747789382935 tr(WHW.T) 22.988168716430664
I0314 07:03:25.185122 2482627 finetune.py:45] layer 7_q initial loss 0.0001416127779521048
I0314 07:03:34.226164 2482627 finetune.py:45] layer 7_k initial loss 0.00015766013530083
I0314 07:03:42.942794 2482627 finetune.py:45] layer 7_o initial loss 0.000237123531405814
I0314 07:03:57.913319 2482627 finetune.py:45] layer 7_up initial loss 0.000310372794046998
I0314 07:04:12.874936 2482627 finetune.py:45] layer 7_gate initial loss 0.000352667790139094
I0314 07:04:17.582947 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 8 in 70.13551568984985s
I0314 07:04:20.841898 2483544 config.py:54] PyTorch version 2.1.1 available.
I0314 07:04:21.935281 2474246 quantize_finetune_llama.py:191] layer 9 gpu 1
I0314 07:04:22.009223 2483544 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:04:28.952829 2482627 finetune.py:45] layer 7_down initial loss 0.00046279412345029414
7_v proxy err 0.02547689713537693 tr(WHW.T) 489.9357604980469
7_q proxy err 0.0020937900990247726 tr(WHW.T) 7672.17919921875
7_k proxy err 0.0016077467007562518 tr(WHW.T) 10198.3701171875
7_o proxy err 0.023417849093675613 tr(WHW.T) 15.11335563659668
7_up proxy err 0.013034407049417496 tr(WHW.T) 735.8538818359375
7_gate proxy err 0.00545627111569047 tr(WHW.T) 1876.0390625
7_down proxy err 0.015507887117564678 tr(WHW.T) 30.58672523498535
I0314 07:04:30.782776 2483544 finetune.py:45] layer 8_v initial loss 0.00019245523435529321
I0314 07:04:39.462738 2483544 finetune.py:45] layer 8_q initial loss 0.00021345893037505448
I0314 07:04:48.128171 2483544 finetune.py:45] layer 8_k initial loss 0.00023629401403013617
I0314 07:04:57.224600 2483544 finetune.py:45] layer 8_o initial loss 0.0003541206242516637
I0314 07:05:12.266928 2483544 finetune.py:45] layer 8_up initial loss 0.00044230130151845515
I0314 07:05:27.451070 2483544 finetune.py:45] layer 8_gate initial loss 0.0004962891107425094
I0314 07:05:30.052944 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 9 in 67.69204211235046s
I0314 07:05:33.159474 2484426 config.py:54] PyTorch version 2.1.1 available.
I0314 07:05:34.184700 2474246 quantize_finetune_llama.py:191] layer 10 gpu 2
I0314 07:05:34.253725 2484426 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:05:43.238712 2484426 finetune.py:45] layer 9_v initial loss 0.0002155496331397444
I0314 07:05:43.977358 2483544 finetune.py:45] layer 8_down initial loss 0.0006299294764176011
8_v proxy err 0.024247094988822937 tr(WHW.T) 530.9967041015625
8_q proxy err 0.0022549668792635202 tr(WHW.T) 7228.1201171875
8_k proxy err 0.0015784145798534155 tr(WHW.T) 10639.1015625
8_o proxy err 0.026584111154079437 tr(WHW.T) 20.092191696166992
8_up proxy err 0.012039722874760628 tr(WHW.T) 866.312744140625
8_gate proxy err 0.005584351718425751 tr(WHW.T) 1970.857177734375
8_down proxy err 0.015446488745510578 tr(WHW.T) 37.177734375
I0314 07:05:51.865666 2484426 finetune.py:45] layer 9_q initial loss 0.00024234277952928096
I0314 07:06:00.465873 2484426 finetune.py:45] layer 9_k initial loss 0.0002722718636505306
I0314 07:06:09.264801 2484426 finetune.py:45] layer 9_o initial loss 0.00041920648072846234
I0314 07:06:24.447044 2484426 finetune.py:45] layer 9_up initial loss 0.0005201317835599184
I0314 07:06:39.562138 2484426 finetune.py:45] layer 9_gate initial loss 0.0005857986980117857
I0314 07:06:42.963217 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 10 in 68.31504559516907s
I0314 07:06:46.159830 2485331 config.py:54] PyTorch version 2.1.1 available.
I0314 07:06:47.209765 2474246 quantize_finetune_llama.py:191] layer 11 gpu 3
I0314 07:06:47.280898 2485331 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:06:55.856918 2485331 finetune.py:45] layer 10_v initial loss 0.00028579289210028946
I0314 07:06:56.022682 2484426 finetune.py:45] layer 9_down initial loss 0.0007390367682091892
9_v proxy err 0.022587226703763008 tr(WHW.T) 565.0663452148438
9_q proxy err 0.0023451768793165684 tr(WHW.T) 6970.3359375
9_k proxy err 0.001537787145934999 tr(WHW.T) 10987.3515625
9_o proxy err 0.0264996699988842 tr(WHW.T) 25.610172271728516
9_up proxy err 0.011625904589891434 tr(WHW.T) 970.8984375
9_gate proxy err 0.005549952387809753 tr(WHW.T) 2132.69384765625
9_down proxy err 0.015487485565245152 tr(WHW.T) 42.99482727050781
I0314 07:07:04.451230 2485331 finetune.py:45] layer 10_q initial loss 0.0003186672693118453
I0314 07:07:13.427435 2485331 finetune.py:45] layer 10_k initial loss 0.0003521500912029296
I0314 07:07:22.186379 2485331 finetune.py:45] layer 10_o initial loss 0.0005557372933253646
I0314 07:07:37.220974 2485331 finetune.py:45] layer 10_up initial loss 0.0006687362329103053
I0314 07:07:52.278333 2485331 finetune.py:45] layer 10_gate initial loss 0.0007460600463673472
I0314 07:07:55.810679 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 11 in 68.19156408309937s
I0314 07:07:58.949778 2486217 config.py:54] PyTorch version 2.1.1 available.
I0314 07:07:59.987528 2474246 quantize_finetune_llama.py:191] layer 12 gpu 0
I0314 07:08:00.065410 2486217 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:08:09.352877 2485331 finetune.py:45] layer 10_down initial loss 0.0009230207069776952
I0314 07:08:09.437340 2486217 finetune.py:45] layer 11_v initial loss 0.0003209235437680036
10_v proxy err 0.02267281338572502 tr(WHW.T) 578.807373046875
10_q proxy err 0.002408674219623208 tr(WHW.T) 6915.87109375
10_k proxy err 0.001585732796229422 tr(WHW.T) 10996.2431640625
10_o proxy err 0.027117306366562843 tr(WHW.T) 35.184165954589844
10_up proxy err 0.010992737486958504 tr(WHW.T) 1080.198486328125
10_gate proxy err 0.005490026902407408 tr(WHW.T) 2260.88330078125
10_down proxy err 0.014785285107791424 tr(WHW.T) 52.33584976196289
I0314 07:08:18.135355 2486217 finetune.py:45] layer 11_q initial loss 0.00034746393794193864
I0314 07:08:26.715914 2486217 finetune.py:45] layer 11_k initial loss 0.0003773349744733423
I0314 07:08:35.286153 2486217 finetune.py:45] layer 11_o initial loss 0.0005807445850223303
I0314 07:08:50.349345 2486217 finetune.py:45] layer 11_up initial loss 0.0007119852234609425
I0314 07:09:05.607378 2486217 finetune.py:45] layer 11_gate initial loss 0.0008001047535799444
I0314 07:09:09.986933 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 12 in 68.57481288909912s
I0314 07:09:13.133691 2487129 config.py:54] PyTorch version 2.1.1 available.
I0314 07:09:14.142309 2474246 quantize_finetune_llama.py:191] layer 13 gpu 1
I0314 07:09:14.212928 2487129 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:09:21.682988 2486217 finetune.py:45] layer 11_down initial loss 0.000997602823190391
11_v proxy err 0.021599337458610535 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0027299392968416214 tr(WHW.T) 7027.10986328125
11_k proxy err 0.0018721893429756165 tr(WHW.T) 10511.23046875
11_o proxy err 0.027397509664297104 tr(WHW.T) 36.654052734375
11_up proxy err 0.011196772567927837 tr(WHW.T) 1139.6044921875
11_gate proxy err 0.005541007965803146 tr(WHW.T) 2392.716552734375
11_down proxy err 0.015163278207182884 tr(WHW.T) 56.13530731201172
I0314 07:09:23.171929 2487129 finetune.py:45] layer 12_v initial loss 0.000325297616655007
I0314 07:09:31.770919 2487129 finetune.py:45] layer 12_q initial loss 0.00036384956911206245
I0314 07:09:40.503700 2487129 finetune.py:45] layer 12_k initial loss 0.0004094150208402425
I0314 07:09:49.247517 2487129 finetune.py:45] layer 12_o initial loss 0.0006388024776242673
I0314 07:10:04.413184 2487129 finetune.py:45] layer 12_up initial loss 0.00078120845137164
I0314 07:10:19.594230 2487129 finetune.py:45] layer 12_gate initial loss 0.0008834059117361903
I0314 07:10:21.586119 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 13 in 67.0330548286438s
I0314 07:10:24.668538 2488005 config.py:54] PyTorch version 2.1.1 available.
I0314 07:10:25.677456 2474246 quantize_finetune_llama.py:191] layer 14 gpu 2
I0314 07:10:25.748623 2488005 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:10:34.217291 2488005 finetune.py:45] layer 13_v initial loss 0.00031930190743878484
I0314 07:10:36.068874 2487129 finetune.py:45] layer 12_down initial loss 0.0011086625745519996
12_v proxy err 0.022093219682574272 tr(WHW.T) 703.318603515625
12_q proxy err 0.002731393091380596 tr(WHW.T) 7045.6435546875
12_k proxy err 0.0018248375272378325 tr(WHW.T) 10893.65625
12_o proxy err 0.027850260958075523 tr(WHW.T) 39.29071044921875
12_up proxy err 0.011096615344285965 tr(WHW.T) 1228.298583984375
12_gate proxy err 0.005909852217882872 tr(WHW.T) 2381.994873046875
12_down proxy err 0.015142285265028477 tr(WHW.T) 64.17745208740234
I0314 07:10:42.895601 2488005 finetune.py:45] layer 13_q initial loss 0.0003559240431059152
I0314 07:10:51.387108 2488005 finetune.py:45] layer 13_k initial loss 0.0004047058173455298
I0314 07:10:59.830865 2488005 finetune.py:45] layer 13_o initial loss 0.0006414931849576533
I0314 07:11:14.584224 2488005 finetune.py:45] layer 13_up initial loss 0.0008167263003997505
I0314 07:11:29.294341 2488005 finetune.py:45] layer 13_gate initial loss 0.0009428388439118862
I0314 07:11:33.209153 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 14 in 67.12080454826355s
I0314 07:11:36.325572 2488884 config.py:54] PyTorch version 2.1.1 available.
I0314 07:11:37.362560 2474246 quantize_finetune_llama.py:191] layer 15 gpu 3
I0314 07:11:37.428657 2488884 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:11:45.239244 2488005 finetune.py:45] layer 13_down initial loss 0.0012244299286976457
13_v proxy err 0.021793480962514877 tr(WHW.T) 714.5677490234375
13_q proxy err 0.002734989859163761 tr(WHW.T) 6956.03564453125
13_k proxy err 0.0019003825727850199 tr(WHW.T) 10426.6318359375
13_o proxy err 0.024865014478564262 tr(WHW.T) 45.8377571105957
13_up proxy err 0.010702219791710377 tr(WHW.T) 1367.6221923828125
13_gate proxy err 0.005780529696494341 tr(WHW.T) 2601.504638671875
13_down proxy err 0.015203593298792839 tr(WHW.T) 79.3589096069336
I0314 07:11:46.256566 2488884 finetune.py:45] layer 14_v initial loss 0.00039941363502293825
I0314 07:11:54.746442 2488884 finetune.py:45] layer 14_q initial loss 0.00044135571806691587
I0314 07:12:03.275527 2488884 finetune.py:45] layer 14_k initial loss 0.0004872184945270419
I0314 07:12:12.047967 2488884 finetune.py:45] layer 14_o initial loss 0.0007781801396049559
I0314 07:12:27.283475 2488884 finetune.py:45] layer 14_up initial loss 0.0009740099194459617
I0314 07:12:42.839973 2488884 finetune.py:45] layer 14_gate initial loss 0.0011228435905650258
I0314 07:12:45.728049 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 15 in 67.96293425559998s
I0314 07:12:49.103348 2489785 config.py:54] PyTorch version 2.1.1 available.
I0314 07:12:50.154794 2474246 quantize_finetune_llama.py:191] layer 16 gpu 0
I0314 07:12:50.216419 2489785 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:12:59.727022 2489785 finetune.py:45] layer 15_v initial loss 0.0003957828157581389
I0314 07:13:00.106366 2488884 finetune.py:45] layer 14_down initial loss 0.0014529080362990499
14_v proxy err 0.023100681602954865 tr(WHW.T) 706.1612548828125
14_q proxy err 0.0028134987223893404 tr(WHW.T) 7077.06103515625
14_k proxy err 0.0018280686344951391 tr(WHW.T) 11295.16796875
14_o proxy err 0.027392012998461723 tr(WHW.T) 50.921180725097656
14_up proxy err 0.010821698233485222 tr(WHW.T) 1464.7159423828125
14_gate proxy err 0.0060634794645011425 tr(WHW.T) 2682.584716796875
14_down proxy err 0.015531010925769806 tr(WHW.T) 90.28684997558594
I0314 07:13:08.245114 2489785 finetune.py:45] layer 15_q initial loss 0.0004341980384197086
I0314 07:13:16.762983 2489785 finetune.py:45] layer 15_k initial loss 0.0004751301312353462
I0314 07:13:25.555251 2489785 finetune.py:45] layer 15_o initial loss 0.0007732175290584564
I0314 07:13:40.480699 2489785 finetune.py:45] layer 15_up initial loss 0.0010125016560778022
I0314 07:13:55.490152 2489785 finetune.py:45] layer 15_gate initial loss 0.0011984666343778372
I0314 07:14:01.714550 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 16 in 70.09558892250061s
I0314 07:14:04.975925 2490706 config.py:54] PyTorch version 2.1.1 available.
I0314 07:14:06.010855 2474246 quantize_finetune_llama.py:191] layer 17 gpu 1
I0314 07:14:06.080994 2490706 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:14:11.812382 2489785 finetune.py:45] layer 15_down initial loss 0.0016155819175764918
15_v proxy err 0.021225182339549065 tr(WHW.T) 762.7275390625
15_q proxy err 0.002711224602535367 tr(WHW.T) 7252.0009765625
15_k proxy err 0.001844771089963615 tr(WHW.T) 11072.3974609375
15_o proxy err 0.023338228464126587 tr(WHW.T) 59.61664962768555
15_up proxy err 0.010546036064624786 tr(WHW.T) 1641.0228271484375
15_gate proxy err 0.006115878466516733 tr(WHW.T) 2905.140380859375
15_down proxy err 0.01552056148648262 tr(WHW.T) 114.09001922607422
I0314 07:14:15.055676 2490706 finetune.py:45] layer 16_v initial loss 0.00047739624278619885
I0314 07:14:23.764463 2490706 finetune.py:45] layer 16_q initial loss 0.0005369805148802698
I0314 07:14:32.640781 2490706 finetune.py:45] layer 16_k initial loss 0.000591510848607868
I0314 07:14:41.500374 2490706 finetune.py:45] layer 16_o initial loss 0.0009550206596031785
I0314 07:14:56.638492 2490706 finetune.py:45] layer 16_up initial loss 0.0012614759616553783
I0314 07:15:11.898994 2490706 finetune.py:45] layer 16_gate initial loss 0.0015028835041448474
I0314 07:15:14.716397 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 17 in 68.2939236164093s
I0314 07:15:17.830568 2491580 config.py:54] PyTorch version 2.1.1 available.
I0314 07:15:18.865110 2474246 quantize_finetune_llama.py:191] layer 18 gpu 2
I0314 07:15:18.935664 2491580 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:15:27.622445 2491580 finetune.py:45] layer 17_v initial loss 0.0003947251825593412
I0314 07:15:28.123399 2490706 finetune.py:45] layer 16_down initial loss 0.002066705608740449
16_v proxy err 0.02163354679942131 tr(WHW.T) 780.7407836914062
16_q proxy err 0.0027923001907765865 tr(WHW.T) 7193.3974609375
16_k proxy err 0.0017919255187734962 tr(WHW.T) 11630.361328125
16_o proxy err 0.018369415774941444 tr(WHW.T) 88.22785186767578
16_up proxy err 0.010316072031855583 tr(WHW.T) 1890.9385986328125
16_gate proxy err 0.0059442296624183655 tr(WHW.T) 3369.859130859375
16_down proxy err 0.015714678913354874 tr(WHW.T) 152.0294952392578
I0314 07:15:35.902614 2491580 finetune.py:45] layer 17_q initial loss 0.00044283055467531085
I0314 07:15:44.503250 2491580 finetune.py:45] layer 17_k initial loss 0.0005054164794273674
I0314 07:15:52.919010 2491580 finetune.py:45] layer 17_o initial loss 0.0007720106514170766
I0314 07:16:08.273791 2491580 finetune.py:45] layer 17_up initial loss 0.0011072923662140965
I0314 07:16:24.330464 2491580 finetune.py:45] layer 17_gate initial loss 0.0013716204557567835
I0314 07:16:29.285132 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 18 in 70.0165946483612s
I0314 07:16:32.793194 2492480 config.py:54] PyTorch version 2.1.1 available.
I0314 07:16:33.886899 2474246 quantize_finetune_llama.py:191] layer 19 gpu 3
I0314 07:16:33.953221 2492480 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:16:43.470895 2491580 finetune.py:45] layer 17_down initial loss 0.001977104227989912
I0314 07:16:44.917680 2492480 finetune.py:45] layer 18_v initial loss 0.00040681325481273234
17_v proxy err 0.020239919424057007 tr(WHW.T) 845.7654418945312
17_q proxy err 0.0028241341933608055 tr(WHW.T) 7163.2734375
17_k proxy err 0.001960202818736434 tr(WHW.T) 10697.431640625
17_o proxy err 0.02002955600619316 tr(WHW.T) 58.14826965332031
17_up proxy err 0.011225768364965916 tr(WHW.T) 1921.07861328125
17_gate proxy err 0.006227413192391396 tr(WHW.T) 3571.31640625
17_down proxy err 0.015578803606331348 tr(WHW.T) 165.43495178222656
I0314 07:16:54.306342 2492480 finetune.py:45] layer 18_q initial loss 0.0004840338951908052
I0314 07:17:03.439392 2492480 finetune.py:45] layer 18_k initial loss 0.0006026736227795482
I0314 07:17:12.439272 2492480 finetune.py:45] layer 18_o initial loss 0.0009003709419630468
I0314 07:17:27.696478 2492480 finetune.py:45] layer 18_up initial loss 0.0013086633989587426
I0314 07:17:43.433578 2492480 finetune.py:45] layer 18_gate initial loss 0.0016192510956898332
I0314 07:17:43.776012 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 19 in 69.48741960525513s
I0314 07:17:46.826466 2493391 config.py:54] PyTorch version 2.1.1 available.
I0314 07:17:47.832556 2474246 quantize_finetune_llama.py:191] layer 20 gpu 0
I0314 07:17:47.898835 2493391 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:17:57.454723 2493391 finetune.py:45] layer 19_v initial loss 0.00038599324761889875
I0314 07:18:00.807126 2492480 finetune.py:45] layer 18_down initial loss 0.002352713141590357
18_v proxy err 0.01865421049296856 tr(WHW.T) 1003.7705078125
18_q proxy err 0.0029118626844137907 tr(WHW.T) 7510.48046875
18_k proxy err 0.002152088563889265 tr(WHW.T) 10462.6650390625
18_o proxy err 0.017652375623583794 tr(WHW.T) 69.96558380126953
18_up proxy err 0.011925016529858112 tr(WHW.T) 2023.183837890625
18_gate proxy err 0.00658805388957262 tr(WHW.T) 3783.076416015625
18_down proxy err 0.015695005655288696 tr(WHW.T) 198.52699279785156
I0314 07:18:05.913686 2493391 finetune.py:45] layer 19_q initial loss 0.00046081875916570425
I0314 07:18:14.234823 2493391 finetune.py:45] layer 19_k initial loss 0.0005378946079872549
I0314 07:18:22.691421 2493391 finetune.py:45] layer 19_o initial loss 0.000801233749371022
I0314 07:18:37.384414 2493391 finetune.py:45] layer 19_up initial loss 0.0012546817306429148
I0314 07:18:52.164403 2493391 finetune.py:45] layer 19_gate initial loss 0.0016163967084139585
I0314 07:18:56.239561 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 20 in 66.91817021369934s
I0314 07:18:59.434082 2494296 config.py:54] PyTorch version 2.1.1 available.
I0314 07:19:00.446207 2474246 quantize_finetune_llama.py:191] layer 21 gpu 1
I0314 07:19:00.508841 2494296 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:19:08.135680 2493391 finetune.py:45] layer 19_down initial loss 0.0024200775660574436
19_v proxy err 0.01828213967382908 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.0030893844086676836 tr(WHW.T) 6944.4130859375
19_k proxy err 0.0020814051385968924 tr(WHW.T) 10548.4892578125
19_o proxy err 0.017430247738957405 tr(WHW.T) 62.291683197021484
19_up proxy err 0.011989641934633255 tr(WHW.T) 2149.330322265625
19_gate proxy err 0.0072073545306921005 tr(WHW.T) 3687.5126953125
19_down proxy err 0.01526142843067646 tr(WHW.T) 222.93177795410156
I0314 07:19:09.339558 2494296 finetune.py:45] layer 20_v initial loss 0.00043264098349027336
I0314 07:19:17.989435 2494296 finetune.py:45] layer 20_q initial loss 0.0004899298073723912
I0314 07:19:26.642714 2494296 finetune.py:45] layer 20_k initial loss 0.0005498455138877034
I0314 07:19:35.448897 2494296 finetune.py:45] layer 20_o initial loss 0.0008655132260173559
I0314 07:19:50.492094 2494296 finetune.py:45] layer 20_up initial loss 0.0014019308146089315
I0314 07:20:05.499983 2494296 finetune.py:45] layer 20_gate initial loss 0.0018304215045645833
I0314 07:20:07.266657 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 21 in 66.3926990032196s
I0314 07:20:10.284684 2495157 config.py:54] PyTorch version 2.1.1 available.
I0314 07:20:11.322903 2474246 quantize_finetune_llama.py:191] layer 22 gpu 2
I0314 07:20:11.393160 2495157 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:20:19.993762 2495157 finetune.py:45] layer 21_v initial loss 0.00039292153087444603
I0314 07:20:22.041272 2494296 finetune.py:45] layer 20_down initial loss 0.0028241064865142107
20_v proxy err 0.01875452511012554 tr(WHW.T) 990.5983276367188
20_q proxy err 0.003007428254932165 tr(WHW.T) 7150.947265625
20_k proxy err 0.002138549229130149 tr(WHW.T) 10386.2470703125
20_o proxy err 0.012943663634359837 tr(WHW.T) 100.31707000732422
20_up proxy err 0.011750449426472187 tr(WHW.T) 2340.89453125
20_gate proxy err 0.007066559046506882 tr(WHW.T) 4024.62744140625
20_down proxy err 0.015180503018200397 tr(WHW.T) 274.8815002441406
I0314 07:20:28.712445 2495157 finetune.py:45] layer 21_q initial loss 0.00044740771409124136
I0314 07:20:37.185828 2495157 finetune.py:45] layer 21_k initial loss 0.0005233781994320452
I0314 07:20:45.670920 2495157 finetune.py:45] layer 21_o initial loss 0.0007822333718650043
I0314 07:21:00.338906 2495157 finetune.py:45] layer 21_up initial loss 0.0013548429124057293
I0314 07:21:15.221583 2495157 finetune.py:45] layer 21_gate initial loss 0.0018087183125317097
I0314 07:21:18.167139 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 22 in 66.39686250686646s
I0314 07:21:21.314797 2496029 config.py:54] PyTorch version 2.1.1 available.
I0314 07:21:22.334799 2474246 quantize_finetune_llama.py:191] layer 23 gpu 3
I0314 07:21:22.408862 2496029 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:21:31.118272 2496029 finetune.py:45] layer 22_v initial loss 0.00047125606215558946
I0314 07:21:31.158554 2495157 finetune.py:45] layer 21_down initial loss 0.0028040630277246237
21_v proxy err 0.01816338114440441 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.0033142610918730497 tr(WHW.T) 7064.314453125
21_k proxy err 0.0024119578301906586 tr(WHW.T) 9976.4658203125
21_o proxy err 0.013850344344973564 tr(WHW.T) 75.50972747802734
21_up proxy err 0.012349134311079979 tr(WHW.T) 2361.650390625
21_gate proxy err 0.007540448103100061 tr(WHW.T) 4004.37646484375
21_down proxy err 0.015181154012680054 tr(WHW.T) 276.5857849121094
I0314 07:21:39.458414 2496029 finetune.py:45] layer 22_q initial loss 0.0005824142135679722
I0314 07:21:47.914791 2496029 finetune.py:45] layer 22_k initial loss 0.0007185774156823754
I0314 07:21:56.512799 2496029 finetune.py:45] layer 22_o initial loss 0.001071044011041522
I0314 07:22:11.379672 2496029 finetune.py:45] layer 22_up initial loss 0.001721382956020534
I0314 07:22:26.087449 2496029 finetune.py:45] layer 22_gate initial loss 0.0022376759443432093
I0314 07:22:29.598725 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 23 in 66.84310412406921s
I0314 07:22:32.792155 2496905 config.py:54] PyTorch version 2.1.1 available.
I0314 07:22:33.807173 2474246 quantize_finetune_llama.py:191] layer 24 gpu 0
I0314 07:22:33.878019 2496905 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:22:43.181164 2496029 finetune.py:45] layer 22_down initial loss 0.003342820331454277
I0314 07:22:43.186738 2496905 finetune.py:45] layer 23_v initial loss 0.0004654364020098001
22_v proxy err 0.01731920801103115 tr(WHW.T) 1243.2529296875
22_q proxy err 0.00315318675711751 tr(WHW.T) 7746.84765625
22_k proxy err 0.0023642927408218384 tr(WHW.T) 10603.2041015625
22_o proxy err 0.011707557365298271 tr(WHW.T) 114.30065155029297
22_up proxy err 0.012422473169863224 tr(WHW.T) 2474.510498046875
22_gate proxy err 0.007670348975807428 tr(WHW.T) 4156.64013671875
22_down proxy err 0.015099884010851383 tr(WHW.T) 311.8800048828125
I0314 07:22:51.645283 2496905 finetune.py:45] layer 23_q initial loss 0.0005304280202835798
I0314 07:22:59.986311 2496905 finetune.py:45] layer 23_k initial loss 0.0006007328629493713
I0314 07:23:08.908660 2496905 finetune.py:45] layer 23_o initial loss 0.0008991563227027655
I0314 07:23:23.858095 2496905 finetune.py:45] layer 23_up initial loss 0.0016095322789624333
I0314 07:23:38.863338 2496905 finetune.py:45] layer 23_gate initial loss 0.0021855684462934732
I0314 07:23:43.753437 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 24 in 68.4993097782135s
I0314 07:23:47.032200 2497804 config.py:54] PyTorch version 2.1.1 available.
I0314 07:23:48.316998 2474246 quantize_finetune_llama.py:191] layer 25 gpu 1
I0314 07:23:48.388799 2497804 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:23:55.051549 2496905 finetune.py:45] layer 23_down initial loss 0.0033208634704351425
23_v proxy err 0.015833020210266113 tr(WHW.T) 1486.037353515625
23_q proxy err 0.0035365938674658537 tr(WHW.T) 7346.60986328125
23_k proxy err 0.002665481064468622 tr(WHW.T) 9982.2392578125
23_o proxy err 0.014161558821797371 tr(WHW.T) 85.13458251953125
23_up proxy err 0.01286843977868557 tr(WHW.T) 2533.51025390625
23_gate proxy err 0.008231970481574535 tr(WHW.T) 4097.51953125
23_down proxy err 0.015174251981079578 tr(WHW.T) 321.33892822265625
I0314 07:23:57.689489 2497804 finetune.py:45] layer 24_v initial loss 0.0005542698199860752
I0314 07:24:06.480646 2497804 finetune.py:45] layer 24_q initial loss 0.0006484428886324167
I0314 07:24:15.315662 2497804 finetune.py:45] layer 24_k initial loss 0.0007375488639809191
I0314 07:24:24.278455 2497804 finetune.py:45] layer 24_o initial loss 0.0011268482776358724
I0314 07:24:39.400580 2497804 finetune.py:45] layer 24_up initial loss 0.001896292669698596
I0314 07:24:54.695976 2497804 finetune.py:45] layer 24_gate initial loss 0.002532660961151123
I0314 07:24:57.134078 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 25 in 68.40171980857849s
I0314 07:25:00.357446 2498773 config.py:54] PyTorch version 2.1.1 available.
I0314 07:25:01.492550 2474246 quantize_finetune_llama.py:191] layer 26 gpu 2
I0314 07:25:01.561717 2498773 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:25:11.771075 2498773 finetune.py:45] layer 25_v initial loss 0.0005943177966400981
I0314 07:25:12.219562 2497804 finetune.py:45] layer 24_down initial loss 0.0037494883872568607
24_v proxy err 0.0168044064193964 tr(WHW.T) 1394.900634765625
24_q proxy err 0.0036756573244929314 tr(WHW.T) 7020.447265625
24_k proxy err 0.002553051570430398 tr(WHW.T) 10323.43359375
24_o proxy err 0.011182587593793869 tr(WHW.T) 133.98797607421875
24_up proxy err 0.0130537748336792 tr(WHW.T) 2621.76513671875
24_gate proxy err 0.008306612260639668 tr(WHW.T) 4262.74853515625
24_down proxy err 0.015261068940162659 tr(WHW.T) 340.22412109375
I0314 07:25:20.770216 2498773 finetune.py:45] layer 25_q initial loss 0.0006768350722268224
I0314 07:25:29.378185 2498773 finetune.py:45] layer 25_k initial loss 0.0007801993051543832
I0314 07:25:38.036244 2498773 finetune.py:45] layer 25_o initial loss 0.0010727766202762723
I0314 07:25:53.051611 2498773 finetune.py:45] layer 25_up initial loss 0.0019308002665638924
I0314 07:26:08.171224 2498773 finetune.py:45] layer 25_gate initial loss 0.0026364875957369804
I0314 07:26:10.701832 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 26 in 68.77769446372986s
I0314 07:26:14.011531 2499660 config.py:54] PyTorch version 2.1.1 available.
I0314 07:26:15.125172 2474246 quantize_finetune_llama.py:191] layer 27 gpu 3
I0314 07:26:15.199226 2499660 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:26:25.521289 2498773 finetune.py:45] layer 25_down initial loss 0.003965577110648155
I0314 07:26:25.697504 2499660 finetune.py:45] layer 26_v initial loss 0.0007993309991434216
25_v proxy err 0.015297290869057178 tr(WHW.T) 1707.664794921875
25_q proxy err 0.003960439469665289 tr(WHW.T) 7162.16357421875
25_k proxy err 0.0030064466409385204 tr(WHW.T) 9611.58984375
25_o proxy err 0.013379947282373905 tr(WHW.T) 83.535888671875
25_up proxy err 0.013000458478927612 tr(WHW.T) 2805.728515625
25_gate proxy err 0.00807743426412344 tr(WHW.T) 4666.4404296875
25_down proxy err 0.015179312787950039 tr(WHW.T) 373.460693359375
I0314 07:26:35.099014 2499660 finetune.py:45] layer 26_q initial loss 0.0009016862604767084
I0314 07:26:43.835976 2499660 finetune.py:45] layer 26_k initial loss 0.0010336663108319044
I0314 07:26:52.462094 2499660 finetune.py:45] layer 26_o initial loss 0.0015334454365074635
I0314 07:27:08.620082 2499660 finetune.py:45] layer 26_up initial loss 0.0024693480227142572
I0314 07:27:25.690530 2499660 finetune.py:45] layer 26_gate initial loss 0.003265432547777891
I0314 07:27:26.143885 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 27 in 70.62263512611389s
I0314 07:27:29.362857 2500555 config.py:54] PyTorch version 2.1.1 available.
I0314 07:27:30.370780 2474246 quantize_finetune_llama.py:191] layer 28 gpu 0
I0314 07:27:30.435078 2500555 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:27:41.207589 2500555 finetune.py:45] layer 27_v initial loss 0.0005905501311644912
I0314 07:27:46.189244 2499660 finetune.py:45] layer 26_down initial loss 0.004712649621069431
26_v proxy err 0.015344790183007717 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.003721968736499548 tr(WHW.T) 7469.98291015625
26_k proxy err 0.0026946766301989555 tr(WHW.T) 10487.8740234375
26_o proxy err 0.009341317228972912 tr(WHW.T) 202.88172912597656
26_up proxy err 0.012218216434121132 tr(WHW.T) 3154.75146484375
26_gate proxy err 0.007529816124588251 tr(WHW.T) 5302.16455078125
26_down proxy err 0.015597864054143429 tr(WHW.T) 401.19390869140625
I0314 07:27:52.067430 2500555 finetune.py:45] layer 27_q initial loss 0.0007202007691375911
I0314 07:28:02.109483 2500555 finetune.py:45] layer 27_k initial loss 0.0008459927048534155
I0314 07:28:12.373618 2500555 finetune.py:45] layer 27_o initial loss 0.001257730065844953
I0314 07:28:30.469106 2500555 finetune.py:45] layer 27_up initial loss 0.002313711680471897
I0314 07:28:46.329487 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 28 in 74.50099182128906s
I0314 07:28:48.477749 2500555 finetune.py:45] layer 27_gate initial loss 0.0032310406677424908
I0314 07:28:49.490167 2501518 config.py:54] PyTorch version 2.1.1 available.
I0314 07:28:50.480643 2474246 quantize_finetune_llama.py:191] layer 29 gpu 1
I0314 07:28:50.540825 2501518 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:28:59.631761 2501518 finetune.py:45] layer 28_v initial loss 0.0007596868090331554
I0314 07:29:05.883927 2500555 finetune.py:45] layer 27_down initial loss 0.004959151614457369
27_v proxy err 0.013926185667514801 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.0035792600829154253 tr(WHW.T) 7691.708984375
27_k proxy err 0.0026398503687232733 tr(WHW.T) 10618.70703125
27_o proxy err 0.012591217644512653 tr(WHW.T) 126.13690185546875
27_up proxy err 0.011165866628289223 tr(WHW.T) 3691.557861328125
27_gate proxy err 0.00711029302328825 tr(WHW.T) 5990.82568359375
27_down proxy err 0.015894142910838127 tr(WHW.T) 466.9318542480469
I0314 07:29:08.793206 2501518 finetune.py:45] layer 28_q initial loss 0.0008855204796418548
I0314 07:29:17.583033 2501518 finetune.py:45] layer 28_k initial loss 0.0010293901432305574
I0314 07:29:26.743977 2501518 finetune.py:45] layer 28_o initial loss 0.0015693091554567218
I0314 07:29:42.114989 2501518 finetune.py:45] layer 28_up initial loss 0.0028459832537919283
I0314 07:29:57.649627 2501518 finetune.py:45] layer 28_gate initial loss 0.003956711385399103
I0314 07:30:00.254464 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 29 in 69.31942415237427s
I0314 07:30:03.423493 2502452 config.py:54] PyTorch version 2.1.1 available.
I0314 07:30:04.445431 2474246 quantize_finetune_llama.py:191] layer 30 gpu 2
I0314 07:30:04.507874 2502452 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:30:13.247092 2502452 finetune.py:45] layer 29_v initial loss 0.0007415175787173212
I0314 07:30:14.393605 2501518 finetune.py:45] layer 28_down initial loss 0.006224616430699825
28_v proxy err 0.013056372292339802 tr(WHW.T) 2018.944091796875
28_q proxy err 0.00369678414426744 tr(WHW.T) 7651.126953125
28_k proxy err 0.0027340499218553305 tr(WHW.T) 10544.8251953125
28_o proxy err 0.010446752421557903 tr(WHW.T) 194.8240966796875
28_up proxy err 0.009395701810717583 tr(WHW.T) 4661.2666015625
28_gate proxy err 0.006865033879876137 tr(WHW.T) 6547.48193359375
28_down proxy err 0.016040122136473656 tr(WHW.T) 603.8403930664062
I0314 07:30:21.809329 2502452 finetune.py:45] layer 29_q initial loss 0.000858343904837966
I0314 07:30:30.444768 2502452 finetune.py:45] layer 29_k initial loss 0.0010060517815873027
I0314 07:30:39.196741 2502452 finetune.py:45] layer 29_o initial loss 0.0014924178831279278
I0314 07:30:54.340180 2502452 finetune.py:45] layer 29_up initial loss 0.00299674179404974
I0314 07:31:09.454763 2502452 finetune.py:45] layer 29_gate initial loss 0.0043244087137281895
I0314 07:31:12.856212 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 30 in 67.97778511047363s
I0314 07:31:16.111312 2503328 config.py:54] PyTorch version 2.1.1 available.
I0314 07:31:17.160958 2474246 quantize_finetune_llama.py:191] layer 31 gpu 3
I0314 07:31:17.228790 2503328 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 07:31:25.810378 2502452 finetune.py:45] layer 29_down initial loss 0.007329708896577358
I0314 07:31:26.006693 2503328 finetune.py:45] layer 30_v initial loss 0.0007376470603048801
29_v proxy err 0.013850681483745575 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.0036561761517077684 tr(WHW.T) 7227.0009765625
29_k proxy err 0.0025677501689642668 tr(WHW.T) 10558.609375
29_o proxy err 0.00884593091905117 tr(WHW.T) 207.9054412841797
29_up proxy err 0.007547225337475538 tr(WHW.T) 6070.0498046875
29_gate proxy err 0.006345616653561592 tr(WHW.T) 7369.6142578125
29_down proxy err 0.016436070203781128 tr(WHW.T) 782.2448120117188
I0314 07:31:34.554861 2503328 finetune.py:45] layer 30_q initial loss 0.0009374815272167325
I0314 07:31:43.119319 2503328 finetune.py:45] layer 30_k initial loss 0.001212207949720323
I0314 07:31:51.865990 2503328 finetune.py:45] layer 30_o initial loss 0.001874817768111825
I0314 07:32:06.979259 2503328 finetune.py:45] layer 30_up initial loss 0.004432725720107555
I0314 07:32:22.175274 2503328 finetune.py:45] layer 30_gate initial loss 0.006416148040443659
I0314 07:32:25.065673 2474246 quantize_finetune_llama.py:218] computed original embedding for layer 31 in 67.43806052207947s
I0314 07:32:28.156130 2504293 config.py:54] PyTorch version 2.1.1 available.
I0314 07:32:29.284949 2504293 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
