I0402 14:02:26.494897 3217779 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:02:26.494990 3217779 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:02:26.495031 3217779 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:02:26.817908 3217779 config.py:54] PyTorch version 2.6.0 available.
W0402 14:02:27.007689 3217779 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:02:27.670353 3217779 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  6.66it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.31it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.77it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  7.94it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.05it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.79it/s]
I0402 14:02:28.644575 3217779 quantize_finetune_llama.py:152] loaded model
I0402 14:02:28.967329 3217779 quantize_finetune_llama.py:190] loaded compression model
I0402 14:02:43.213771 3217779 quantize_finetune_llama.py:194] loaded dataset and devset
I0402 14:02:48.173679 3217779 quantize_finetune_llama.py:214] layer 0 gpu 0
I0402 14:02:50.839189 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 0 in 2.4933547973632812s
Use train scale and shift
tensor(2.2655e-07, device='cuda:0') tensor(0.0204, device='cuda:0')
tensor(0.0204, device='cuda:0') tensor(2.2655e-07, device='cuda:0')
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0402 14:03:04.222100 3218312 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:03:04.222193 3218312 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:03:04.222231 3218312 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:03:04.547258 3218312 config.py:54] PyTorch version 2.6.0 available.
W0402 14:03:04.735546 3218312 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:03:05.288902 3218312 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:03:05.292696 3217779 quantize_finetune_llama.py:214] layer 1 gpu 1
I0402 14:03:05.514548 3218312 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:03:08.043093 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 1 in 2.579292058944702s
I0402 14:03:11.741188 3218453 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:03:11.741300 3218453 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:03:11.741348 3218453 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:03:12.118389 3218453 config.py:54] PyTorch version 2.6.0 available.
W0402 14:03:12.335097 3218453 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:03:12.966563 3218453 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:03:12.970607 3217779 quantize_finetune_llama.py:214] layer 2 gpu 2
I0402 14:03:13.129943 3218453 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:03:15.976122 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 2 in 2.840392589569092s
I0402 14:03:19.797708 3218613 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:03:19.797807 3218613 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:03:19.797851 3218613 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:03:20.190981 3218613 config.py:54] PyTorch version 2.6.0 available.
W0402 14:03:20.422568 3218613 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:03:21.084646 3218613 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:03:21.088731 3217779 quantize_finetune_llama.py:214] layer 3 gpu 3
I0402 14:03:21.267381 3218613 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:03:23.999368 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 3 in 2.7327487468719482s
I0402 14:03:28.031445 3218765 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:03:28.031561 3218765 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:03:28.031600 3218765 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:03:28.429131 3218765 config.py:54] PyTorch version 2.6.0 available.
W0402 14:03:28.645061 3218765 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:03:29.357753 3218765 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:03:29.361772 3217779 quantize_finetune_llama.py:214] layer 4 gpu 0
I0402 14:03:29.550497 3218765 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_v proxy err 0.020630396902561188 tr(WHW.T) 971.8771362304688
bpp_loss 2.057901978492737
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_q proxy err 0.004989468026906252 tr(WHW.T) 636425.375
bpp_loss 2.046262741088867
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_k proxy err 0.005162598565220833 tr(WHW.T) 398864.15625
bpp_loss 2.135666608810425
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_o proxy err 0.00877940934151411 tr(WHW.T) 15925.943359375
bpp_loss 1.910283088684082
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
0_up proxy err 0.04115772247314453 tr(WHW.T) 24117.18359375
bpp_loss 1.94877420469772
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
0_gate proxy err 0.03031103126704693 tr(WHW.T) 35427.40234375
bpp_loss 1.9619774042173874
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
0_down proxy err 0.02291092276573181 tr(WHW.T) 35796.33984375
bpp_loss 2.132326758185098
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_v proxy err 0.05974111333489418 tr(WHW.T) 657.0241088867188
bpp_loss 2.0107643604278564
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_q proxy err 0.004103958141058683 tr(WHW.T) 195417.453125
bpp_loss 2.782771944999695
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_k proxy err 0.004091174341738224 tr(WHW.T) 204295.5625
bpp_loss 2.777386784553528
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_o proxy err 0.03274955973029137 tr(WHW.T) 4039.630859375
bpp_loss 1.8922552466392517
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
1_up proxy err 0.05491173267364502 tr(WHW.T) 23209.44921875
bpp_loss 1.9698178490927054
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
1_gate proxy err 0.030572351068258286 tr(WHW.T) 46954.0546875
bpp_loss 2.027799118396848
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
1_down proxy err 0.0027096217963844538 tr(WHW.T) 40779.421875
bpp_loss 2.2956735588783443
I0402 14:04:12.658465 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 4 in 1.0333421230316162s
I0402 14:04:16.664002 3219321 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:04:16.664104 3219321 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:04:16.664152 3219321 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:04:17.045627 3219321 config.py:54] PyTorch version 2.6.0 available.
W0402 14:04:17.256702 3219321 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:04:17.880564 3219321 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:04:17.884871 3217779 quantize_finetune_llama.py:214] layer 5 gpu 1
I0402 14:04:18.091370 3219321 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_v proxy err 0.06164577975869179 tr(WHW.T) 2779.86376953125
bpp_loss 2.0752432346343994
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_q proxy err 0.004611999727785587 tr(WHW.T) 159508.203125
bpp_loss 2.763291120529175
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_k proxy err 0.004181092604994774 tr(WHW.T) 210000.984375
bpp_loss 2.8544952869415283
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_o proxy err 0.05713436380028725 tr(WHW.T) 5300.58984375
bpp_loss 1.9559941291809082
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
2_up proxy err 0.06654824316501617 tr(WHW.T) 19933.5703125
bpp_loss 1.9782902030057685
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
2_gate proxy err 0.044626690447330475 tr(WHW.T) 31660.716796875
bpp_loss 2.049751636593841
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
2_down proxy err 0.06621171534061432 tr(WHW.T) 17283.607421875
bpp_loss 2.0431605716084325
I0402 14:04:20.928167 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 5 in 1.0034825801849365s
I0402 14:04:24.902230 3219476 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:04:24.902343 3219476 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:04:24.902391 3219476 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:04:25.307330 3219476 config.py:54] PyTorch version 2.6.0 available.
W0402 14:04:25.539063 3219476 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:04:26.200659 3219476 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:04:26.204839 3217779 quantize_finetune_llama.py:214] layer 6 gpu 2
I0402 14:04:26.415347 3219476 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_v proxy err 0.08515313267707825 tr(WHW.T) 2978.910400390625
bpp_loss 1.9732338786125183
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_q proxy err 0.00749499537050724 tr(WHW.T) 76174.0546875
bpp_loss 2.6100858449935913
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_k proxy err 0.006357510108500719 tr(WHW.T) 106360.5859375
bpp_loss 2.676257371902466
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_o proxy err 0.051681309938430786 tr(WHW.T) 5255.58154296875
bpp_loss 1.956270158290863
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
3_up proxy err 0.07417622953653336 tr(WHW.T) 17467.81640625
bpp_loss 1.9875080197356467
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
3_gate proxy err 0.047225430607795715 tr(WHW.T) 29396.990234375
bpp_loss 2.0673915951751
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
3_down proxy err 0.06636843830347061 tr(WHW.T) 16907.59375
bpp_loss 2.052581776020139
I0402 14:04:28.963970 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 6 in 1.0564689636230469s
I0402 14:04:32.909305 3219639 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:04:32.909406 3219639 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:04:32.909447 3219639 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:04:33.298548 3219639 config.py:54] PyTorch version 2.6.0 available.
W0402 14:04:33.518388 3219639 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:04:34.157494 3219639 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:04:34.161539 3217779 quantize_finetune_llama.py:214] layer 7 gpu 3
I0402 14:04:34.345867 3219639 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:04:35.596642 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 7 in 0.9510009288787842s
I0402 14:04:39.684343 3219771 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:04:39.684456 3219771 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:04:39.684497 3219771 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:04:40.084518 3219771 config.py:54] PyTorch version 2.6.0 available.
W0402 14:04:40.310876 3219771 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:04:41.003053 3219771 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:04:41.007796 3217779 quantize_finetune_llama.py:214] layer 8 gpu 0
I0402 14:04:41.203451 3219771 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_v proxy err 0.07900228351354599 tr(WHW.T) 3097.555419921875
bpp_loss 2.0153170824050903
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_q proxy err 0.007049486041069031 tr(WHW.T) 78746.46875
bpp_loss 2.693802237510681
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_k proxy err 0.005901268217712641 tr(WHW.T) 118660.75
bpp_loss 2.7258955240249634
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_o proxy err 0.06222906336188316 tr(WHW.T) 5336.9765625
bpp_loss 1.9362868666648865
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
4_up proxy err 0.07097040861845016 tr(WHW.T) 17664.685546875
bpp_loss 1.9855265950047694
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
4_gate proxy err 0.03813767433166504 tr(WHW.T) 36588.140625
bpp_loss 2.095043182373047
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
4_down proxy err 0.06615236401557922 tr(WHW.T) 16830.9296875
bpp_loss 2.0435353767040163
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_v proxy err 0.08208406716585159 tr(WHW.T) 3166.572021484375
bpp_loss 2.0203635692596436
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_q proxy err 0.007573698181658983 tr(WHW.T) 72535.90625
bpp_loss 2.694288492202759
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_k proxy err 0.006053521763533354 tr(WHW.T) 116207.296875
bpp_loss 2.7637733221054077
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_o proxy err 0.07068614661693573 tr(WHW.T) 3769.5478515625
bpp_loss 2.0028257369995117
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
5_up proxy err 0.07033765316009521 tr(WHW.T) 18016.64453125
bpp_loss 1.9841629738031432
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
5_gate proxy err 0.03604067489504814 tr(WHW.T) 39417.078125
bpp_loss 2.100770196249319
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
5_down proxy err 0.07189308106899261 tr(WHW.T) 15953.3203125
bpp_loss 2.03179156503012
I0402 14:05:25.135438 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 8 in 1.0003364086151123s
I0402 14:05:29.058329 3220317 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:05:29.058435 3220317 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:05:29.058480 3220317 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:05:29.415688 3220317 config.py:54] PyTorch version 2.6.0 available.
W0402 14:05:29.635093 3220317 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:05:30.296250 3220317 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:05:30.300234 3217779 quantize_finetune_llama.py:214] layer 9 gpu 1
I0402 14:05:30.493923 3220317 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_v proxy err 0.08780684322118759 tr(WHW.T) 3180.833740234375
bpp_loss 1.9542322754859924
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_q proxy err 0.009573216550052166 tr(WHW.T) 54760.6953125
bpp_loss 2.5735479593276978
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_k proxy err 0.008019853383302689 tr(WHW.T) 75270.1171875
bpp_loss 2.6059168577194214
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_o proxy err 0.07246802002191544 tr(WHW.T) 4041.38525390625
bpp_loss 1.9383522868156433
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
6_up proxy err 0.0701577439904213 tr(WHW.T) 17971.6953125
bpp_loss 1.9814524096111918
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
6_gate proxy err 0.03198922798037529 tr(WHW.T) 45430.91015625
bpp_loss 2.121280315310456
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
6_down proxy err 0.07337163388729095 tr(WHW.T) 15434.75390625
bpp_loss 2.0299715108649674
I0402 14:05:34.200330 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 9 in 1.0095455646514893s
I0402 14:05:38.237201 3220480 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:05:38.237303 3220480 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:05:38.237345 3220480 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:05:38.605490 3220480 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_v proxy err 0.08738695085048676 tr(WHW.T) 3250.746826171875
bpp_loss 1.9583494067192078
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_q proxy err 0.010053054429590702 tr(WHW.T) 51300.6015625
bpp_loss 2.5679455995559692
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_k proxy err 0.008585894480347633 tr(WHW.T) 68225.015625
bpp_loss 2.5741571187973022
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_o proxy err 0.08010150492191315 tr(WHW.T) 3528.39208984375
bpp_loss 1.949342131614685
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
7_up proxy err 0.06787880510091782 tr(WHW.T) 18230.072265625
bpp_loss 1.987854802331259
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
7_gate proxy err 0.030804190784692764 tr(WHW.T) 46648.5078125
bpp_loss 2.1232069148573767
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
7_down proxy err 0.07462520152330399 tr(WHW.T) 15240.3759765625
bpp_loss 2.0296417058900347
W0402 14:05:38.813730 3220480 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:05:39.437175 3220480 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:05:39.440784 3217779 quantize_finetune_llama.py:214] layer 10 gpu 2
I0402 14:05:39.603546 3220480 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:05:40.913565 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 10 in 1.0244717597961426s
I0402 14:05:44.848225 3220623 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:05:44.848322 3220623 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:05:44.848362 3220623 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:05:45.211236 3220623 config.py:54] PyTorch version 2.6.0 available.
W0402 14:05:45.438453 3220623 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:05:46.089048 3220623 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:05:46.093359 3217779 quantize_finetune_llama.py:214] layer 11 gpu 3
I0402 14:05:46.282070 3220623 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:05:47.844156 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 11 in 1.2477467060089111s
I0402 14:05:51.926599 3220761 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:05:51.926710 3220761 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:05:51.926751 3220761 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:05:52.330054 3220761 config.py:54] PyTorch version 2.6.0 available.
W0402 14:05:52.548470 3220761 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:05:53.189154 3220761 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:05:53.193250 3217779 quantize_finetune_llama.py:214] layer 12 gpu 0
I0402 14:05:53.367434 3220761 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_v proxy err 0.08005043864250183 tr(WHW.T) 3470.401611328125
bpp_loss 1.9823996424674988
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_q proxy err 0.010172992944717407 tr(WHW.T) 47592.55859375
bpp_loss 2.5947283506393433
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_k proxy err 0.008148575201630592 tr(WHW.T) 70099.9296875
bpp_loss 2.601583480834961
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_o proxy err 0.08744367212057114 tr(WHW.T) 3118.695556640625
bpp_loss 1.974465548992157
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
8_up proxy err 0.06217360869050026 tr(WHW.T) 19852.556640625
bpp_loss 2.0016499896382176
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
8_gate proxy err 0.031181151047348976 tr(WHW.T) 45311.83203125
bpp_loss 2.1058318559513536
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
8_down proxy err 0.07479190826416016 tr(WHW.T) 15322.3779296875
bpp_loss 2.037472736003787
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_v proxy err 0.07989431172609329 tr(WHW.T) 3676.465087890625
bpp_loss 1.9827719926834106
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_q proxy err 0.010692877694964409 tr(WHW.T) 45676.421875
bpp_loss 2.5885679721832275
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_k proxy err 0.00822370033711195 tr(WHW.T) 72055.03125
bpp_loss 2.6264398097991943
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_o proxy err 0.09263550490140915 tr(WHW.T) 3148.60498046875
bpp_loss 1.9723919034004211
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
9_up proxy err 0.05992060899734497 tr(WHW.T) 20605.642578125
bpp_loss 2.008882212084393
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
9_gate proxy err 0.031002413481473923 tr(WHW.T) 45406.61328125
bpp_loss 2.09514520334643
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
9_down proxy err 0.07594476640224457 tr(WHW.T) 15324.0654296875
bpp_loss 2.03900998137718
I0402 14:06:38.977457 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 12 in 1.0187079906463623s
I0402 14:06:43.063099 3221303 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:06:43.063220 3221303 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:06:43.063261 3221303 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:06:43.458066 3221303 config.py:54] PyTorch version 2.6.0 available.
W0402 14:06:43.676093 3221303 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_v proxy err 0.08010649681091309 tr(WHW.T) 3651.62158203125
bpp_loss 1.97689950466156
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_q proxy err 0.010946081019937992 tr(WHW.T) 43935.53515625
bpp_loss 2.587390184402466
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_k proxy err 0.008323300629854202 tr(WHW.T) 69912.859375
bpp_loss 2.634230136871338
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_o proxy err 0.09946701675653458 tr(WHW.T) 3058.283935546875
bpp_loss 1.9581985473632812
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
10_up proxy err 0.056796181946992874 tr(WHW.T) 21904.43359375
bpp_loss 2.0195297418638716
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
10_gate proxy err 0.03068235144019127 tr(WHW.T) 45983.0625
bpp_loss 2.0916142131006996
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
10_down proxy err 0.07191430777311325 tr(WHW.T) 16111.1357421875
bpp_loss 2.0505096080691314
W0402 14:06:44.273804 3221303 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:06:44.277647 3217779 quantize_finetune_llama.py:214] layer 13 gpu 1
I0402 14:06:44.447036 3221303 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:06:46.223771 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 13 in 0.9961373805999756s
I0402 14:06:50.266541 3221430 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:06:50.266637 3221430 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:06:50.266678 3221430 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_v proxy err 0.08000337332487106 tr(WHW.T) 3890.81982421875
bpp_loss 1.9914643168449402
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_q proxy err 0.01266715582460165 tr(WHW.T) 38073.125
bpp_loss 2.4903886318206787
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_k proxy err 0.009911620989441872 tr(WHW.T) 56997.671875
bpp_loss 2.4820245504379272
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_o proxy err 0.10179650038480759 tr(WHW.T) 3054.2177734375
bpp_loss 1.9805664420127869
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
11_up proxy err 0.05826495960354805 tr(WHW.T) 21548.3046875
bpp_loss 2.0268245963163154
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
11_gate proxy err 0.03127669543027878 tr(WHW.T) 45386.7265625
bpp_loss 2.085898288460665
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
11_down proxy err 0.07500851899385452 tr(WHW.T) 15730.783203125
bpp_loss 2.049896484197572
I0402 14:06:50.646719 3221430 config.py:54] PyTorch version 2.6.0 available.
W0402 14:06:50.859160 3221430 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:06:51.489947 3221430 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:06:51.494009 3217779 quantize_finetune_llama.py:214] layer 14 gpu 2
I0402 14:06:51.750537 3221430 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:06:52.932584 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 14 in 0.9857194423675537s
I0402 14:06:56.867297 3221546 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:06:56.867400 3221546 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:06:56.867442 3221546 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:06:57.215192 3221546 config.py:54] PyTorch version 2.6.0 available.
W0402 14:06:57.411594 3221546 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:06:58.027491 3221546 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:06:58.031250 3217779 quantize_finetune_llama.py:214] layer 15 gpu 3
I0402 14:06:58.226666 3221546 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:06:59.697281 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 15 in 1.188582420349121s
I0402 14:07:03.797711 3221688 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:07:03.797833 3221688 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:07:03.797886 3221688 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:07:04.205450 3221688 config.py:54] PyTorch version 2.6.0 available.
W0402 14:07:04.426545 3221688 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:07:05.070807 3221688 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:07:05.074822 3217779 quantize_finetune_llama.py:214] layer 16 gpu 0
I0402 14:07:05.314833 3221688 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_v proxy err 0.08248644322156906 tr(WHW.T) 3807.353515625
bpp_loss 1.9794365763664246
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_q proxy err 0.012664781883358955 tr(WHW.T) 38406.0
bpp_loss 2.521557927131653
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_k proxy err 0.009676700457930565 tr(WHW.T) 59429.25
bpp_loss 2.567671298980713
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_o proxy err 0.10585620999336243 tr(WHW.T) 2997.118896484375
bpp_loss 1.9667124152183533
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
12_up proxy err 0.05804164707660675 tr(WHW.T) 21806.251953125
bpp_loss 2.0354225690974745
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
12_gate proxy err 0.033201366662979126 tr(WHW.T) 42415.66015625
bpp_loss 2.0779893564623455
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
12_down proxy err 0.07434115558862686 tr(WHW.T) 15783.7470703125
bpp_loss 2.061244576476341
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_v proxy err 0.08500003069639206 tr(WHW.T) 3883.96826171875
bpp_loss 1.992139220237732
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_q proxy err 0.013249690644443035 tr(WHW.T) 38091.8046875
bpp_loss 2.4948772192001343
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_k proxy err 0.010275078937411308 tr(WHW.T) 57141.765625
bpp_loss 2.5198259353637695
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_o proxy err 0.0961848720908165 tr(WHW.T) 3392.266845703125
bpp_loss 1.985466182231903
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
13_up proxy err 0.05595852807164192 tr(WHW.T) 22688.076171875
bpp_loss 2.046366758124773
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
13_gate proxy err 0.03262685611844063 tr(WHW.T) 43286.52734375
bpp_loss 2.074196926383085
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
13_down proxy err 0.07271608710289001 tr(WHW.T) 15750.5673828125
bpp_loss 2.078082794366881
I0402 14:07:51.062926 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 16 in 1.0000226497650146s
I0402 14:07:55.096188 3222350 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:07:55.096299 3222350 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:07:55.096340 3222350 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:07:55.483041 3222350 config.py:54] PyTorch version 2.6.0 available.
W0402 14:07:55.696214 3222350 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_v proxy err 0.08972591906785965 tr(WHW.T) 3647.569580078125
bpp_loss 1.979457974433899
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_q proxy err 0.013441234827041626 tr(WHW.T) 36835.87109375
bpp_loss 2.4920432567596436
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_k proxy err 0.010053632780909538 tr(WHW.T) 58844.5546875
bpp_loss 2.516680955886841
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_o proxy err 0.10877023637294769 tr(WHW.T) 3063.885498046875
bpp_loss 1.9628145694732666
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
14_up proxy err 0.05719929561018944 tr(WHW.T) 22452.671875
bpp_loss 2.0455848338992095
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
14_gate proxy err 0.03427163511514664 tr(WHW.T) 41240.93359375
bpp_loss 2.070156274839889
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
14_down proxy err 0.0739416629076004 tr(WHW.T) 15409.48828125
bpp_loss 2.080521594646365
W0402 14:07:56.331524 3222350 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:07:56.335531 3217779 quantize_finetune_llama.py:214] layer 17 gpu 1
I0402 14:07:56.505908 3222350 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:07:58.341304 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 17 in 1.2413442134857178s
I0402 14:08:02.394373 3222486 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:08:02.394478 3222486 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:08:02.394526 3222486 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_v proxy err 0.08095323294401169 tr(WHW.T) 4003.05126953125
bpp_loss 2.0145925283432007
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_q proxy err 0.012908004224300385 tr(WHW.T) 38353.34765625
bpp_loss 2.477785348892212
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_k proxy err 0.00990443304181099 tr(WHW.T) 58600.94921875
bpp_loss 2.5276334285736084
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_o proxy err 0.09450838714838028 tr(WHW.T) 3628.3759765625
bpp_loss 1.985413670539856
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
15_up proxy err 0.055736083537340164 tr(WHW.T) 23066.35546875
bpp_loss 2.051851228226063
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
15_gate proxy err 0.034387338906526566 tr(WHW.T) 40946.2890625
bpp_loss 2.0768130102822946
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
15_down proxy err 0.07211890816688538 tr(WHW.T) 15429.0400390625
bpp_loss 2.092604481896689
I0402 14:08:02.780803 3222486 config.py:54] PyTorch version 2.6.0 available.
W0402 14:08:02.984251 3222486 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:08:03.623472 3222486 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:08:03.627397 3217779 quantize_finetune_llama.py:214] layer 18 gpu 2
I0402 14:08:03.872162 3222486 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:08:05.056442 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 18 in 0.9689657688140869s
I0402 14:08:09.076678 3222609 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:08:09.076773 3222609 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:08:09.076812 3222609 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:08:09.412120 3222609 config.py:54] PyTorch version 2.6.0 available.
W0402 14:08:09.624170 3222609 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:08:10.213745 3222609 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:08:10.217609 3217779 quantize_finetune_llama.py:214] layer 19 gpu 3
I0402 14:08:10.419106 3222609 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:08:11.783860 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 19 in 1.0709819793701172s
I0402 14:08:15.888657 3222735 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:08:15.888764 3222735 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:08:15.888808 3222735 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:08:16.304966 3222735 config.py:54] PyTorch version 2.6.0 available.
W0402 14:08:16.524441 3222735 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:08:17.171839 3222735 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:08:17.175970 3217779 quantize_finetune_llama.py:214] layer 20 gpu 0
I0402 14:08:17.434776 3222735 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_v proxy err 0.08486171066761017 tr(WHW.T) 3988.751708984375
bpp_loss 2.035948395729065
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_q proxy err 0.013636179268360138 tr(WHW.T) 37064.6328125
bpp_loss 2.456272840499878
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_k proxy err 0.010088175535202026 tr(WHW.T) 59978.5
bpp_loss 2.492145299911499
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_o proxy err 0.08154048025608063 tr(WHW.T) 4695.22509765625
bpp_loss 1.992430865764618
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
16_up proxy err 0.05584589019417763 tr(WHW.T) 23603.60546875
bpp_loss 2.0476035184638444
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
16_gate proxy err 0.03422830253839493 tr(WHW.T) 42174.73828125
bpp_loss 2.0814655215241187
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
16_down proxy err 0.07327930629253387 tr(WHW.T) 15228.4970703125
bpp_loss 2.0907971027285552
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_v proxy err 0.08449698239564896 tr(WHW.T) 4263.80712890625
bpp_loss 2.0156748294830322
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_q proxy err 0.014504334889352322 tr(WHW.T) 36392.74609375
bpp_loss 2.428145170211792
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_k proxy err 0.011252407915890217 tr(WHW.T) 54388.6953125
bpp_loss 2.4576005935668945
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_o proxy err 0.08729495108127594 tr(WHW.T) 4297.923828125
bpp_loss 1.997882068157196
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
17_up proxy err 0.0615067332983017 tr(WHW.T) 21615.3203125
bpp_loss 2.040772105372229
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
17_gate proxy err 0.0360756479203701 tr(WHW.T) 40300.40234375
bpp_loss 2.0894946608432505
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
17_down proxy err 0.07482917606830597 tr(WHW.T) 15353.4287109375
bpp_loss 2.079338240069012
I0402 14:09:03.510429 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 20 in 1.0435576438903809s
I0402 14:09:07.493347 3223268 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:09:07.493456 3223268 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:09:07.493496 3223268 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:09:07.866477 3223268 config.py:54] PyTorch version 2.6.0 available.
W0402 14:09:08.089087 3223268 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:09:08.731738 3223268 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:09:08.735687 3217779 quantize_finetune_llama.py:214] layer 21 gpu 1
I0402 14:09:08.918890 3223268 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_v proxy err 0.08086825162172318 tr(WHW.T) 4673.08837890625
bpp_loss 2.0437417030334473
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_q proxy err 0.015257387422025204 tr(WHW.T) 35237.796875
bpp_loss 2.3841185569763184
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_k proxy err 0.012243700213730335 tr(WHW.T) 49119.20703125
bpp_loss 2.413482189178467
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_o proxy err 0.07575573027133942 tr(WHW.T) 4923.25390625
bpp_loss 2.0364197492599487
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
18_up proxy err 0.0654420554637909 tr(WHW.T) 20299.947265625
bpp_loss 2.036887767703034
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
18_gate proxy err 0.03806358948349953 tr(WHW.T) 38072.26171875
bpp_loss 2.099460158237191
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
18_down proxy err 0.0732731819152832 tr(WHW.T) 15266.072265625
bpp_loss 2.0873012320939885
I0402 14:09:11.335560 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 21 in 1.0776612758636475s
I0402 14:09:15.258813 3223396 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:09:15.258914 3223396 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:09:15.258957 3223396 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_v proxy err 0.07974031567573547 tr(WHW.T) 4788.90576171875
bpp_loss 2.049254894256592
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_q proxy err 0.016218747943639755 tr(WHW.T) 32879.76953125
bpp_loss 2.3632677793502808
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_k proxy err 0.012346816249191761 tr(WHW.T) 49984.76171875
bpp_loss 2.3869004249572754
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_o proxy err 0.07836782187223434 tr(WHW.T) 5005.189453125
bpp_loss 2.0315970182418823
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
19_up proxy err 0.06599196791648865 tr(WHW.T) 20172.62109375
bpp_loss 2.0363110387048056
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
19_gate proxy err 0.04132305085659027 tr(WHW.T) 34698.6953125
bpp_loss 2.103396925815316
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
19_down proxy err 0.0716826394200325 tr(WHW.T) 15697.3017578125
bpp_loss 2.088491206945375
I0402 14:09:15.620862 3223396 config.py:54] PyTorch version 2.6.0 available.
W0402 14:09:15.840208 3223396 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:09:16.475368 3223396 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:09:16.479400 3217779 quantize_finetune_llama.py:214] layer 22 gpu 2
I0402 14:09:16.721815 3223396 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:09:17.903072 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 22 in 0.9633176326751709s
I0402 14:09:21.774267 3223512 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:09:21.774357 3223512 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:09:21.774396 3223512 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:09:22.116949 3223512 config.py:54] PyTorch version 2.6.0 available.
W0402 14:09:22.304332 3223512 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:09:22.935542 3223512 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:09:22.939577 3217779 quantize_finetune_llama.py:214] layer 23 gpu 3
I0402 14:09:23.136360 3223512 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:09:24.755130 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 23 in 1.2786357402801514s
I0402 14:09:28.872341 3223635 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:09:28.872440 3223635 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:09:28.872483 3223635 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:09:29.257077 3223635 config.py:54] PyTorch version 2.6.0 available.
W0402 14:09:29.481457 3223635 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:09:30.144073 3223635 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:09:30.148178 3217779 quantize_finetune_llama.py:214] layer 24 gpu 0
I0402 14:09:30.323521 3223635 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_v proxy err 0.08337091654539108 tr(WHW.T) 4645.8291015625
bpp_loss 2.0582767724990845
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_q proxy err 0.01608714461326599 tr(WHW.T) 33815.2578125
bpp_loss 2.367579936981201
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_k proxy err 0.012532425113022327 tr(WHW.T) 49149.9921875
bpp_loss 2.3895301818847656
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_o proxy err 0.05836361646652222 tr(WHW.T) 6820.419921875
bpp_loss 2.053953528404236
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
20_up proxy err 0.06505697220563889 tr(WHW.T) 20603.083984375
bpp_loss 2.034893834313681
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
20_gate proxy err 0.04063213989138603 tr(WHW.T) 35509.98046875
bpp_loss 2.1093266509300053
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
20_down proxy err 0.07025198638439178 tr(WHW.T) 15803.095703125
bpp_loss 2.09345334075218
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_v proxy err 0.0820511132478714 tr(WHW.T) 4862.75634765625
bpp_loss 2.0804306268692017
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_q proxy err 0.017843643203377724 tr(WHW.T) 30261.54296875
bpp_loss 2.328454852104187
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_k proxy err 0.014020556584000587 tr(WHW.T) 42768.43359375
bpp_loss 2.340118646621704
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_o proxy err 0.06809023767709732 tr(WHW.T) 6399.25439453125
bpp_loss 2.0413609743118286
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
21_up proxy err 0.06832261383533478 tr(WHW.T) 19562.884765625
bpp_loss 2.0328614878100018
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
21_gate proxy err 0.043064020574092865 tr(WHW.T) 33251.44921875
bpp_loss 2.117368919904842
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
21_down proxy err 0.07159748673439026 tr(WHW.T) 15792.2314453125
bpp_loss 2.0866740359816442
I0402 14:10:15.195059 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 24 in 1.003413438796997s
I0402 14:10:19.292066 3224116 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:10:19.292178 3224116 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:10:19.292222 3224116 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:10:19.680059 3224116 config.py:54] PyTorch version 2.6.0 available.
W0402 14:10:19.893345 3224116 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_v proxy err 0.07776595652103424 tr(WHW.T) 5111.93212890625
bpp_loss 2.0864334106445312
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_q proxy err 0.01686984673142433 tr(WHW.T) 32103.412109375
bpp_loss 2.361201047897339
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_k proxy err 0.013644305057823658 tr(WHW.T) 43969.734375
bpp_loss 2.3763893842697144
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_o proxy err 0.05427742004394531 tr(WHW.T) 7627.48828125
bpp_loss 2.0686912536621094
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
22_up proxy err 0.06890431046485901 tr(WHW.T) 19439.994140625
bpp_loss 2.0318779169126997
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
22_gate proxy err 0.04377647489309311 tr(WHW.T) 32731.19921875
bpp_loss 2.124074004417242
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
22_down proxy err 0.07199924439191818 tr(WHW.T) 15874.603515625
bpp_loss 2.0835406170334925
W0402 14:10:20.508166 3224116 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:10:20.512276 3217779 quantize_finetune_llama.py:214] layer 25 gpu 1
I0402 14:10:20.702676 3224116 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:10:22.651431 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 25 in 1.00978422164917s
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_v proxy err 0.07398577779531479 tr(WHW.T) 5666.529296875
bpp_loss 2.120568871498108
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_q proxy err 0.019268184900283813 tr(WHW.T) 28221.720703125
bpp_loss 2.339055895805359
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_k proxy err 0.015505901537835598 tr(WHW.T) 38375.3828125
bpp_loss 2.3477890491485596
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_o proxy err 0.06631731986999512 tr(WHW.T) 6345.388671875
bpp_loss 2.1086541414260864
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
23_up proxy err 0.07119300961494446 tr(WHW.T) 18753.931640625
bpp_loss 2.0371961371843206
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
23_gate proxy err 0.04658197984099388 tr(WHW.T) 30400.564453125
bpp_loss 2.124141426973565
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
23_down proxy err 0.07234463840723038 tr(WHW.T) 15816.7724609375
bpp_loss 2.0893822270770404
I0402 14:10:26.646796 3224244 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:10:26.646895 3224244 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:10:26.646934 3224244 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:10:26.996200 3224244 config.py:54] PyTorch version 2.6.0 available.
W0402 14:10:27.202420 3224244 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:10:27.802959 3224244 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:10:27.806873 3217779 quantize_finetune_llama.py:214] layer 26 gpu 2
I0402 14:10:28.009485 3224244 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:10:29.277925 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 26 in 0.9805095195770264s
I0402 14:10:33.262357 3224360 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:10:33.262454 3224360 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:10:33.262496 3224360 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:10:33.600345 3224360 config.py:54] PyTorch version 2.6.0 available.
W0402 14:10:33.815315 3224360 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:10:34.411166 3224360 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:10:34.415148 3217779 quantize_finetune_llama.py:214] layer 27 gpu 3
I0402 14:10:34.567568 3224360 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:10:35.958032 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 27 in 1.0485129356384277s
I0402 14:10:40.000235 3224483 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:10:40.000350 3224483 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:10:40.000395 3224483 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:10:40.387435 3224483 config.py:54] PyTorch version 2.6.0 available.
W0402 14:10:40.601634 3224483 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:10:41.218696 3224483 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:10:41.222605 3217779 quantize_finetune_llama.py:214] layer 28 gpu 0
I0402 14:10:41.419126 3224483 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_v proxy err 0.07584056258201599 tr(WHW.T) 5324.72998046875
bpp_loss 2.121768832206726
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_q proxy err 0.01938018575310707 tr(WHW.T) 27011.373046875
bpp_loss 2.3087007999420166
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_k proxy err 0.014874020591378212 tr(WHW.T) 39756.4296875
bpp_loss 2.311466097831726
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_o proxy err 0.053276240825653076 tr(WHW.T) 8038.615234375
bpp_loss 2.095002770423889
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
24_up proxy err 0.07218658924102783 tr(WHW.T) 18512.99609375
bpp_loss 2.0403334151866823
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
24_gate proxy err 0.04701618105173111 tr(WHW.T) 30171.666015625
bpp_loss 2.126770640528479
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
24_down proxy err 0.07226220518350601 tr(WHW.T) 15756.2470703125
bpp_loss 2.0951531765072846
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_v proxy err 0.07271301746368408 tr(WHW.T) 5927.5078125
bpp_loss 2.143980860710144
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_q proxy err 0.021484486758708954 tr(WHW.T) 25041.619140625
bpp_loss 2.3036047220230103
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_k proxy err 0.01726789027452469 tr(WHW.T) 33632.96875
bpp_loss 2.305805802345276
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_o proxy err 0.0673794075846672 tr(WHW.T) 6779.06689453125
bpp_loss 2.1104652881622314
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
25_up proxy err 0.07153483480215073 tr(WHW.T) 18624.029296875
bpp_loss 2.045587140460347
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
25_gate proxy err 0.04570116847753525 tr(WHW.T) 31053.6875
bpp_loss 2.1297447736873183
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
25_down proxy err 0.06874727457761765 tr(WHW.T) 15877.4140625
bpp_loss 2.11509294288103
I0402 14:11:26.140071 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 28 in 0.9795975685119629s
I0402 14:11:30.162368 3224964 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:11:30.162476 3224964 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:11:30.162519 3224964 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:11:30.558669 3224964 config.py:54] PyTorch version 2.6.0 available.
W0402 14:11:30.773446 3224964 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_v proxy err 0.07112397998571396 tr(WHW.T) 5920.73828125
bpp_loss 2.1713370084762573
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_q proxy err 0.0201102402061224 tr(WHW.T) 26711.5390625
bpp_loss 2.2886260747909546
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_k proxy err 0.015789618715643883 tr(WHW.T) 37533.6015625
bpp_loss 2.296031355857849
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_o proxy err 0.04143432527780533 tr(WHW.T) 9866.05859375
bpp_loss 2.1894643306732178
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
26_up proxy err 0.06757311522960663 tr(WHW.T) 19856.3125
bpp_loss 2.0492952036303143
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
26_gate proxy err 0.04281037300825119 tr(WHW.T) 33454.33203125
bpp_loss 2.1333327182503634
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
26_down proxy err 0.06898987293243408 tr(WHW.T) 15425.537109375
bpp_loss 2.1263335028360055
W0402 14:11:31.366251 3224964 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:11:31.370558 3217779 quantize_finetune_llama.py:214] layer 29 gpu 1
I0402 14:11:31.556259 3224964 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:11:33.632359 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 29 in 1.2645316123962402s
I0402 14:11:37.643965 3225085 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:11:37.644062 3225085 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:11:37.644103 3225085 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_v proxy err 0.06895525008440018 tr(WHW.T) 6537.79541015625
bpp_loss 2.1519033908843994
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_q proxy err 0.020282190293073654 tr(WHW.T) 28139.56640625
bpp_loss 2.32110857963562
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_k proxy err 0.015887562185525894 tr(WHW.T) 38878.3359375
bpp_loss 2.3337020874023438
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_o proxy err 0.0532740019261837 tr(WHW.T) 7266.2978515625
bpp_loss 2.2047648429870605
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
27_up proxy err 0.06191487982869148 tr(WHW.T) 21823.326171875
bpp_loss 2.055173385974973
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
27_gate proxy err 0.040641915053129196 tr(WHW.T) 35491.0
bpp_loss 2.1349718847940133
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
27_down proxy err 0.06632424890995026 tr(WHW.T) 15119.01171875
bpp_loss 2.150702421055284
I0402 14:11:38.021062 3225085 config.py:54] PyTorch version 2.6.0 available.
W0402 14:11:38.228238 3225085 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:11:38.839735 3225085 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:11:38.843785 3217779 quantize_finetune_llama.py:214] layer 30 gpu 2
I0402 14:11:38.996407 3225085 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:11:40.259269 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 30 in 0.9456453323364258s
I0402 14:11:44.148081 3225208 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:11:44.148173 3225208 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:11:44.148213 3225208 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:11:44.492395 3225208 config.py:54] PyTorch version 2.6.0 available.
W0402 14:11:44.700514 3225208 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:11:45.272798 3225208 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:11:45.276638 3217779 quantize_finetune_llama.py:214] layer 31 gpu 3
I0402 14:11:45.439970 3225208 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:11:46.854617 3217779 quantize_finetune_llama.py:245] computed original embedding for layer 31 in 1.0472619533538818s
I0402 14:11:50.748526 3225324 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:11:50.748638 3225324 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:11:50.748678 3225324 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:11:51.124023 3225324 config.py:54] PyTorch version 2.6.0 available.
W0402 14:11:51.341916 3225324 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:11:51.959226 3225324 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:11:52.132871 3225324 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_v proxy err 0.06448755413293839 tr(WHW.T) 7077.5078125
bpp_loss 2.189309000968933
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_q proxy err 0.021018365398049355 tr(WHW.T) 27002.65625
bpp_loss 2.278843402862549
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_k proxy err 0.016578782349824905 tr(WHW.T) 37243.53125
bpp_loss 2.294067144393921
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_o proxy err 0.045978154987096786 tr(WHW.T) 8884.01171875
bpp_loss 2.2292022705078125
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
28_up proxy err 0.05238886550068855 tr(WHW.T) 26161.169921875
bpp_loss 2.0666864971781886
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
28_gate proxy err 0.03921884298324585 tr(WHW.T) 36794.0625
bpp_loss 2.1304208622422327
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
28_down proxy err 0.06115126237273216 tr(WHW.T) 14984.466796875
bpp_loss 2.1827456673910453
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_v proxy err 0.06726501137018204 tr(WHW.T) 6682.36328125
bpp_loss 2.1986407041549683
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_q proxy err 0.02097126841545105 tr(WHW.T) 27006.056640625
bpp_loss 2.2459455728530884
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_k proxy err 0.015920743346214294 tr(WHW.T) 39489.33984375
bpp_loss 2.254490375518799
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_o proxy err 0.043908219784498215 tr(WHW.T) 10599.048828125
bpp_loss 2.2064476013183594
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
29_up proxy err 0.04283919557929039 tr(WHW.T) 32884.11328125
bpp_loss 2.077327195988145
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
29_gate proxy err 0.03650445491075516 tr(WHW.T) 39942.984375
bpp_loss 2.133907584256904
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
29_down proxy err 0.05616238713264465 tr(WHW.T) 14732.544921875
bpp_loss 2.212349525717802
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_v proxy err 0.0585717149078846 tr(WHW.T) 8207.525390625
bpp_loss 2.2108447551727295
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_q proxy err 0.0208762027323246 tr(WHW.T) 28540.68359375
bpp_loss 2.2379733324050903
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_k proxy err 0.016794806346297264 tr(WHW.T) 38445.2421875
bpp_loss 2.2549387216567993
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_o proxy err 0.04026303067803383 tr(WHW.T) 10163.1123046875
bpp_loss 2.2800674438476562
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
30_up proxy err 0.02843775600194931 tr(WHW.T) 53873.95703125
bpp_loss 2.09610384564067
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
30_gate proxy err 0.026454677805304527 tr(WHW.T) 59167.79296875
bpp_loss 2.162259212760038
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
30_down proxy err 0.021812675520777702 tr(WHW.T) 25842.498046875
bpp_loss 2.283858665200167
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_v proxy err 0.06909399479627609 tr(WHW.T) 6740.33837890625
bpp_loss 2.0992759466171265
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_q proxy err 0.01743294857442379 tr(WHW.T) 36698.0
bpp_loss 2.251480221748352
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_k proxy err 0.013351871632039547 tr(WHW.T) 54793.80859375
bpp_loss 2.2999879121780396
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_o proxy err 0.026358548551797867 tr(WHW.T) 13121.5947265625
bpp_loss 2.2333234548568726
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
31_up proxy err 0.01825825870037079 tr(WHW.T) 95778.3046875
bpp_loss 2.1427131475404253
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
31_gate proxy err 0.01799432560801506 tr(WHW.T) 97550.4453125
bpp_loss 2.219544255456259
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
31_down proxy err 0.01302291639149189 tr(WHW.T) 37026.6875
bpp_loss 2.3322113281072574
I0402 14:12:58.640903 3225988 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:12:58.641013 3225988 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:12:58.641052 3225988 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:12:58.962288 3225988 config.py:54] PyTorch version 2.6.0 available.
W0402 14:12:59.188139 3225988 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0402 14:12:59.302661 3225988 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  2.51it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:01,  3.34it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  4.12it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  4.63it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  4.16it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.74it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.23it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.63it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.70it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.75it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.77it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.59it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.86it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.77it/s]
I0402 14:13:02.682619 3225988 hfize_llama.py:153] loaded layer 0
I0402 14:13:03.653810 3225988 hfize_llama.py:153] loaded layer 1
I0402 14:13:04.617530 3225988 hfize_llama.py:153] loaded layer 2
I0402 14:13:05.600918 3225988 hfize_llama.py:153] loaded layer 3
I0402 14:13:06.568986 3225988 hfize_llama.py:153] loaded layer 4
I0402 14:13:07.525912 3225988 hfize_llama.py:153] loaded layer 5
I0402 14:13:08.480541 3225988 hfize_llama.py:153] loaded layer 6
I0402 14:13:09.422338 3225988 hfize_llama.py:153] loaded layer 7
I0402 14:13:10.372593 3225988 hfize_llama.py:153] loaded layer 8
I0402 14:13:11.336146 3225988 hfize_llama.py:153] loaded layer 9
I0402 14:13:12.279895 3225988 hfize_llama.py:153] loaded layer 10
I0402 14:13:13.239015 3225988 hfize_llama.py:153] loaded layer 11
I0402 14:13:14.177041 3225988 hfize_llama.py:153] loaded layer 12
I0402 14:13:15.149801 3225988 hfize_llama.py:153] loaded layer 13
I0402 14:13:16.153421 3225988 hfize_llama.py:153] loaded layer 14
I0402 14:13:17.127452 3225988 hfize_llama.py:153] loaded layer 15
I0402 14:13:18.035622 3225988 hfize_llama.py:153] loaded layer 16
I0402 14:13:18.932414 3225988 hfize_llama.py:153] loaded layer 17
I0402 14:13:19.828692 3225988 hfize_llama.py:153] loaded layer 18
I0402 14:13:20.726848 3225988 hfize_llama.py:153] loaded layer 19
I0402 14:13:21.620954 3225988 hfize_llama.py:153] loaded layer 20
I0402 14:13:22.495628 3225988 hfize_llama.py:153] loaded layer 21
I0402 14:13:23.400446 3225988 hfize_llama.py:153] loaded layer 22
I0402 14:13:24.291725 3225988 hfize_llama.py:153] loaded layer 23
I0402 14:13:25.170725 3225988 hfize_llama.py:153] loaded layer 24
I0402 14:13:26.038967 3225988 hfize_llama.py:153] loaded layer 25
I0402 14:13:26.902755 3225988 hfize_llama.py:153] loaded layer 26
I0402 14:13:27.776976 3225988 hfize_llama.py:153] loaded layer 27
I0402 14:13:28.653035 3225988 hfize_llama.py:153] loaded layer 28
I0402 14:13:29.533080 3225988 hfize_llama.py:153] loaded layer 29
I0402 14:13:30.410993 3225988 hfize_llama.py:153] loaded layer 30
I0402 14:13:31.294791 3225988 hfize_llama.py:153] loaded layer 31
I0402 14:13:31.294899 3225988 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.18s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:03,  1.02it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.08it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.11it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.18it/s]
I0402 14:14:02.825079 3225988 hfize_llama.py:167] successfully loaded hfized model
I0402 14:14:07.306798 3226274 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:14:07.306966 3226274 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:14:07.307013 3226274 utils.py:162] NumExpr defaulting to 16 threads.
W0402 14:14:07.757579 3226274 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0402 14:14:08.153454 3226274 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.10s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.10s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.04s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.02s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.06it/s]
I0402 14:14:13.949007 3226274 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/166 [00:00<?, ?it/s]avg_loss = 1.7172104120254517:   0%|          | 0/166 [00:01<?, ?it/s]avg_loss = 1.7172104120254517:   1%|          | 1/166 [00:01<04:23,  1.59s/it]avg_loss = 1.9532639384269714:   1%|          | 1/166 [00:02<04:23,  1.59s/it]avg_loss = 1.9532639384269714:   1%|          | 2/166 [00:02<03:44,  1.37s/it]avg_loss = 2.123023788134257:   1%|          | 2/166 [00:04<03:44,  1.37s/it] avg_loss = 2.123023788134257:   2%|▏         | 3/166 [00:04<03:32,  1.30s/it]avg_loss = 2.178603559732437:   2%|▏         | 3/166 [00:05<03:32,  1.30s/it]avg_loss = 2.178603559732437:   2%|▏         | 4/166 [00:05<03:26,  1.27s/it]avg_loss = 2.098067808151245:   2%|▏         | 4/166 [00:06<03:26,  1.27s/it]avg_loss = 2.098067808151245:   3%|▎         | 5/166 [00:06<03:22,  1.26s/it]avg_loss = 2.078144450982412:   3%|▎         | 5/166 [00:07<03:22,  1.26s/it]avg_loss = 2.078144450982412:   4%|▎         | 6/166 [00:07<03:19,  1.25s/it]avg_loss = 2.022473999432155:   4%|▎         | 6/166 [00:08<03:19,  1.25s/it]avg_loss = 2.022473999432155:   4%|▍         | 7/166 [00:08<03:17,  1.25s/it]avg_loss = 1.9749028086662292:   4%|▍         | 7/166 [00:10<03:17,  1.25s/it]avg_loss = 1.9749028086662292:   5%|▍         | 8/166 [00:10<03:16,  1.24s/it]avg_loss = 1.9692308108011882:   5%|▍         | 8/166 [00:11<03:16,  1.24s/it]avg_loss = 1.9692308108011882:   5%|▌         | 9/166 [00:11<03:14,  1.24s/it]avg_loss = 1.9785192728042602:   5%|▌         | 9/166 [00:12<03:14,  1.24s/it]avg_loss = 1.9785192728042602:   6%|▌         | 10/166 [00:12<03:13,  1.24s/it]avg_loss = 1.9941336458379573:   6%|▌         | 10/166 [00:13<03:13,  1.24s/it]avg_loss = 1.9941336458379573:   7%|▋         | 11/166 [00:13<03:12,  1.24s/it]avg_loss = 1.996229887008667:   7%|▋         | 11/166 [00:15<03:12,  1.24s/it] avg_loss = 1.996229887008667:   7%|▋         | 12/166 [00:15<03:11,  1.24s/it]avg_loss = 1.9889254111510057:   7%|▋         | 12/166 [00:16<03:11,  1.24s/it]avg_loss = 1.9889254111510057:   8%|▊         | 13/166 [00:16<03:10,  1.25s/it]avg_loss = 1.9963664412498474:   8%|▊         | 13/166 [00:17<03:10,  1.25s/it]avg_loss = 1.9963664412498474:   8%|▊         | 14/166 [00:17<03:09,  1.25s/it]avg_loss = 2.0103240569432574:   8%|▊         | 14/166 [00:18<03:09,  1.25s/it]avg_loss = 2.0103240569432574:   9%|▉         | 15/166 [00:18<03:08,  1.25s/it]avg_loss = 2.0262980982661247:   9%|▉         | 15/166 [00:20<03:08,  1.25s/it]avg_loss = 2.0262980982661247:  10%|▉         | 16/166 [00:20<03:07,  1.25s/it]avg_loss = 2.0384208104189705:  10%|▉         | 16/166 [00:21<03:07,  1.25s/it]avg_loss = 2.0384208104189705:  10%|█         | 17/166 [00:21<03:06,  1.25s/it]avg_loss = 2.052392688062456:  10%|█         | 17/166 [00:22<03:06,  1.25s/it] avg_loss = 2.052392688062456:  11%|█         | 18/166 [00:22<03:05,  1.26s/it]avg_loss = 2.0694129153301843:  11%|█         | 18/166 [00:23<03:05,  1.26s/it]avg_loss = 2.0694129153301843:  11%|█▏        | 19/166 [00:23<03:04,  1.26s/it]avg_loss = 2.074871677160263:  11%|█▏        | 19/166 [00:25<03:04,  1.26s/it] avg_loss = 2.074871677160263:  12%|█▏        | 20/166 [00:25<03:03,  1.26s/it]avg_loss = 2.076466827165513:  12%|█▏        | 20/166 [00:26<03:03,  1.26s/it]avg_loss = 2.076466827165513:  13%|█▎        | 21/166 [00:26<03:02,  1.26s/it]avg_loss = 2.064343132755973:  13%|█▎        | 21/166 [00:27<03:02,  1.26s/it]avg_loss = 2.064343132755973:  13%|█▎        | 22/166 [00:27<03:01,  1.26s/it]avg_loss = 2.0540098366530044:  13%|█▎        | 22/166 [00:29<03:01,  1.26s/it]avg_loss = 2.0540098366530044:  14%|█▍        | 23/166 [00:29<03:00,  1.26s/it]avg_loss = 2.062159443895022:  14%|█▍        | 23/166 [00:30<03:00,  1.26s/it] avg_loss = 2.062159443895022:  14%|█▍        | 24/166 [00:30<02:59,  1.27s/it]avg_loss = 2.0703128004074096:  14%|█▍        | 24/166 [00:31<02:59,  1.27s/it]avg_loss = 2.0703128004074096:  15%|█▌        | 25/166 [00:31<02:58,  1.27s/it]avg_loss = 2.074188741353842:  15%|█▌        | 25/166 [00:32<02:58,  1.27s/it] avg_loss = 2.074188741353842:  16%|█▌        | 26/166 [00:32<02:57,  1.27s/it]avg_loss = 2.0792563800458557:  16%|█▌        | 26/166 [00:34<02:57,  1.27s/it]avg_loss = 2.0792563800458557:  16%|█▋        | 27/166 [00:34<02:56,  1.27s/it]avg_loss = 2.0816420188971927:  16%|█▋        | 27/166 [00:35<02:56,  1.27s/it]avg_loss = 2.0816420188971927:  17%|█▋        | 28/166 [00:35<02:55,  1.27s/it]avg_loss = 2.0896469930122636:  17%|█▋        | 28/166 [00:36<02:55,  1.27s/it]avg_loss = 2.0896469930122636:  17%|█▋        | 29/166 [00:36<02:54,  1.27s/it]avg_loss = 2.0896227320035297:  17%|█▋        | 29/166 [00:37<02:54,  1.27s/it]avg_loss = 2.0896227320035297:  18%|█▊        | 30/166 [00:37<02:53,  1.28s/it]avg_loss = 2.103886362045042:  18%|█▊        | 30/166 [00:39<02:53,  1.28s/it] avg_loss = 2.103886362045042:  19%|█▊        | 31/166 [00:39<02:52,  1.28s/it]avg_loss = 2.110720496624708:  19%|█▊        | 31/166 [00:40<02:52,  1.28s/it]avg_loss = 2.110720496624708:  19%|█▉        | 32/166 [00:40<02:51,  1.28s/it]avg_loss = 2.114556215026162:  19%|█▉        | 32/166 [00:41<02:51,  1.28s/it]avg_loss = 2.114556215026162:  20%|█▉        | 33/166 [00:41<02:50,  1.28s/it]avg_loss = 2.1153965452138115:  20%|█▉        | 33/166 [00:43<02:50,  1.28s/it]avg_loss = 2.1153965452138115:  20%|██        | 34/166 [00:43<02:48,  1.28s/it]avg_loss = 2.1106999601636613:  20%|██        | 34/166 [00:44<02:48,  1.28s/it]avg_loss = 2.1106999601636613:  21%|██        | 35/166 [00:44<02:47,  1.28s/it]avg_loss = 2.1061658958594003:  21%|██        | 35/166 [00:45<02:47,  1.28s/it]avg_loss = 2.1061658958594003:  22%|██▏       | 36/166 [00:45<02:46,  1.28s/it]avg_loss = 2.0989393575771436:  22%|██▏       | 36/166 [00:46<02:46,  1.28s/it]avg_loss = 2.0989393575771436:  22%|██▏       | 37/166 [00:46<02:45,  1.28s/it]avg_loss = 2.09769963904431:  22%|██▏       | 37/166 [00:48<02:45,  1.28s/it]  avg_loss = 2.09769963904431:  23%|██▎       | 38/166 [00:48<02:44,  1.28s/it]avg_loss = 2.094955945626283:  23%|██▎       | 38/166 [00:49<02:44,  1.28s/it]avg_loss = 2.094955945626283:  23%|██▎       | 39/166 [00:49<02:43,  1.29s/it]avg_loss = 2.0977635085582733:  23%|██▎       | 39/166 [00:50<02:43,  1.29s/it]avg_loss = 2.0977635085582733:  24%|██▍       | 40/166 [00:50<02:42,  1.29s/it]avg_loss = 2.098158539795294:  24%|██▍       | 40/166 [00:52<02:42,  1.29s/it] avg_loss = 2.098158539795294:  25%|██▍       | 41/166 [00:52<02:41,  1.29s/it]avg_loss = 2.0840406644911993:  25%|██▍       | 41/166 [00:53<02:41,  1.29s/it]avg_loss = 2.0840406644911993:  25%|██▌       | 42/166 [00:53<02:39,  1.29s/it]avg_loss = 2.0677934302840124:  25%|██▌       | 42/166 [00:54<02:39,  1.29s/it]avg_loss = 2.0677934302840124:  26%|██▌       | 43/166 [00:54<02:38,  1.29s/it]avg_loss = 2.056330073963512:  26%|██▌       | 43/166 [00:55<02:38,  1.29s/it] avg_loss = 2.056330073963512:  27%|██▋       | 44/166 [00:55<02:37,  1.29s/it]avg_loss = 2.041792000664605:  27%|██▋       | 44/166 [00:57<02:37,  1.29s/it]avg_loss = 2.041792000664605:  27%|██▋       | 45/166 [00:57<02:36,  1.29s/it]avg_loss = 2.030856155830881:  27%|██▋       | 45/166 [00:58<02:36,  1.29s/it]avg_loss = 2.030856155830881:  28%|██▊       | 46/166 [00:58<02:35,  1.29s/it]avg_loss = 2.0233706514886083:  28%|██▊       | 46/166 [00:59<02:35,  1.29s/it]avg_loss = 2.0233706514886083:  28%|██▊       | 47/166 [00:59<02:33,  1.29s/it]avg_loss = 2.024489243825277:  28%|██▊       | 47/166 [01:01<02:33,  1.29s/it] avg_loss = 2.024489243825277:  29%|██▉       | 48/166 [01:01<02:32,  1.30s/it]avg_loss = 2.034904426457931:  29%|██▉       | 48/166 [01:02<02:32,  1.30s/it]avg_loss = 2.034904426457931:  30%|██▉       | 49/166 [01:02<02:31,  1.30s/it]avg_loss = 2.0445985174179078:  30%|██▉       | 49/166 [01:03<02:31,  1.30s/it]avg_loss = 2.0445985174179078:  30%|███       | 50/166 [01:03<02:30,  1.30s/it]avg_loss = 2.052220999025831:  30%|███       | 50/166 [01:05<02:30,  1.30s/it] avg_loss = 2.052220999025831:  31%|███       | 51/166 [01:05<02:29,  1.30s/it]avg_loss = 2.059475004673004:  31%|███       | 51/166 [01:06<02:29,  1.30s/it]avg_loss = 2.059475004673004:  31%|███▏      | 52/166 [01:06<02:28,  1.30s/it]avg_loss = 2.062356543990801:  31%|███▏      | 52/166 [01:07<02:28,  1.30s/it]avg_loss = 2.062356543990801:  32%|███▏      | 53/166 [01:07<02:26,  1.30s/it]avg_loss = 2.061060481601291:  32%|███▏      | 53/166 [01:08<02:26,  1.30s/it]avg_loss = 2.061060481601291:  33%|███▎      | 54/166 [01:08<02:25,  1.30s/it]avg_loss = 2.06313042640686:  33%|███▎      | 54/166 [01:10<02:25,  1.30s/it] avg_loss = 2.06313042640686:  33%|███▎      | 55/166 [01:10<02:24,  1.30s/it]avg_loss = 2.0654434093407223:  33%|███▎      | 55/166 [01:11<02:24,  1.30s/it]avg_loss = 2.0654434093407223:  34%|███▎      | 56/166 [01:11<02:22,  1.30s/it]avg_loss = 2.059735979950219:  34%|███▎      | 56/166 [01:12<02:22,  1.30s/it] avg_loss = 2.059735979950219:  34%|███▍      | 57/166 [01:12<02:21,  1.30s/it]avg_loss = 2.0624212519875886:  34%|███▍      | 57/166 [01:14<02:21,  1.30s/it]avg_loss = 2.0624212519875886:  35%|███▍      | 58/166 [01:14<02:20,  1.30s/it]avg_loss = 2.0601400298587347:  35%|███▍      | 58/166 [01:15<02:20,  1.30s/it]avg_loss = 2.0601400298587347:  36%|███▌      | 59/166 [01:15<02:19,  1.30s/it]avg_loss = 2.0558722615242004:  36%|███▌      | 59/166 [01:16<02:19,  1.30s/it]avg_loss = 2.0558722615242004:  36%|███▌      | 60/166 [01:16<02:17,  1.30s/it]avg_loss = 2.051357232156347:  36%|███▌      | 60/166 [01:18<02:17,  1.30s/it] avg_loss = 2.051357232156347:  37%|███▋      | 61/166 [01:18<02:16,  1.30s/it]avg_loss = 2.0473411525449445:  37%|███▋      | 61/166 [01:19<02:16,  1.30s/it]avg_loss = 2.0473411525449445:  37%|███▋      | 62/166 [01:19<02:15,  1.30s/it]avg_loss = 2.041967571727813:  37%|███▋      | 62/166 [01:20<02:15,  1.30s/it] avg_loss = 2.041967571727813:  38%|███▊      | 63/166 [01:20<02:13,  1.30s/it]avg_loss = 2.037336127832532:  38%|███▊      | 63/166 [01:21<02:13,  1.30s/it]avg_loss = 2.037336127832532:  39%|███▊      | 64/166 [01:21<02:12,  1.30s/it]avg_loss = 2.0301661289655244:  39%|███▊      | 64/166 [01:23<02:12,  1.30s/it]avg_loss = 2.0301661289655244:  39%|███▉      | 65/166 [01:23<02:11,  1.30s/it]avg_loss = 2.0226765726551865:  39%|███▉      | 65/166 [01:24<02:11,  1.30s/it]avg_loss = 2.0226765726551865:  40%|███▉      | 66/166 [01:24<02:10,  1.30s/it]avg_loss = 2.0196536370177767:  40%|███▉      | 66/166 [01:25<02:10,  1.30s/it]avg_loss = 2.0196536370177767:  40%|████      | 67/166 [01:25<02:08,  1.30s/it]avg_loss = 2.0187301898703858:  40%|████      | 67/166 [01:27<02:08,  1.30s/it]avg_loss = 2.0187301898703858:  41%|████      | 68/166 [01:27<02:07,  1.30s/it]avg_loss = 2.021359517954398:  41%|████      | 68/166 [01:28<02:07,  1.30s/it] avg_loss = 2.021359517954398:  42%|████▏     | 69/166 [01:28<02:06,  1.30s/it]avg_loss = 2.024822522912707:  42%|████▏     | 69/166 [01:29<02:06,  1.30s/it]avg_loss = 2.024822522912707:  42%|████▏     | 70/166 [01:29<02:05,  1.30s/it]avg_loss = 2.0286230688363736:  42%|████▏     | 70/166 [01:31<02:05,  1.30s/it]avg_loss = 2.0286230688363736:  43%|████▎     | 71/166 [01:31<02:04,  1.31s/it]avg_loss = 2.0335150013367334:  43%|████▎     | 71/166 [01:32<02:04,  1.31s/it]avg_loss = 2.0335150013367334:  43%|████▎     | 72/166 [01:32<02:02,  1.31s/it]avg_loss = 2.039764614954387:  43%|████▎     | 72/166 [01:33<02:02,  1.31s/it] avg_loss = 2.039764614954387:  44%|████▍     | 73/166 [01:33<02:01,  1.31s/it]avg_loss = 2.0350390559918172:  44%|████▍     | 73/166 [01:35<02:01,  1.31s/it]avg_loss = 2.0350390559918172:  45%|████▍     | 74/166 [01:35<02:00,  1.31s/it]avg_loss = 2.0310168425242106:  45%|████▍     | 74/166 [01:36<02:00,  1.31s/it]avg_loss = 2.0310168425242106:  45%|████▌     | 75/166 [01:36<01:59,  1.31s/it]avg_loss = 2.0309448712750484:  45%|████▌     | 75/166 [01:37<01:59,  1.31s/it]avg_loss = 2.0309448712750484:  46%|████▌     | 76/166 [01:37<01:57,  1.31s/it]avg_loss = 2.027673190290278:  46%|████▌     | 76/166 [01:38<01:57,  1.31s/it] avg_loss = 2.027673190290278:  46%|████▋     | 77/166 [01:38<01:56,  1.31s/it]avg_loss = 2.0241650618039646:  46%|████▋     | 77/166 [01:40<01:56,  1.31s/it]avg_loss = 2.0241650618039646:  47%|████▋     | 78/166 [01:40<01:55,  1.31s/it]avg_loss = 2.020952058743827:  47%|████▋     | 78/166 [01:41<01:55,  1.31s/it] avg_loss = 2.020952058743827:  48%|████▊     | 79/166 [01:41<01:54,  1.31s/it]avg_loss = 2.01758633852005:  48%|████▊     | 79/166 [01:42<01:54,  1.31s/it] avg_loss = 2.01758633852005:  48%|████▊     | 80/166 [01:42<01:52,  1.31s/it]avg_loss = 2.009799705611335:  48%|████▊     | 80/166 [01:44<01:52,  1.31s/it]avg_loss = 2.009799705611335:  49%|████▉     | 81/166 [01:44<01:51,  1.31s/it]avg_loss = 2.010963290202908:  49%|████▉     | 81/166 [01:45<01:51,  1.31s/it]avg_loss = 2.010963290202908:  49%|████▉     | 82/166 [01:45<01:50,  1.31s/it]avg_loss = 2.0127304505152877:  49%|████▉     | 82/166 [01:46<01:50,  1.31s/it]avg_loss = 2.0127304505152877:  50%|█████     | 83/166 [01:46<01:48,  1.31s/it]avg_loss = 2.0162187261240825:  50%|█████     | 83/166 [01:48<01:48,  1.31s/it]avg_loss = 2.0162187261240825:  51%|█████     | 84/166 [01:48<01:47,  1.31s/it]avg_loss = 2.017994231336257:  51%|█████     | 84/166 [01:49<01:47,  1.31s/it] avg_loss = 2.017994231336257:  51%|█████     | 85/166 [01:49<01:46,  1.31s/it]avg_loss = 2.0162481235903362:  51%|█████     | 85/166 [01:50<01:46,  1.31s/it]avg_loss = 2.0162481235903362:  52%|█████▏    | 86/166 [01:50<01:44,  1.31s/it]avg_loss = 2.016043323209916:  52%|█████▏    | 86/166 [01:52<01:44,  1.31s/it] avg_loss = 2.016043323209916:  52%|█████▏    | 87/166 [01:52<01:43,  1.31s/it]avg_loss = 2.015646820718592:  52%|█████▏    | 87/166 [01:53<01:43,  1.31s/it]avg_loss = 2.015646820718592:  53%|█████▎    | 88/166 [01:53<01:42,  1.31s/it]avg_loss = 2.016601977723368:  53%|█████▎    | 88/166 [01:54<01:42,  1.31s/it]avg_loss = 2.016601977723368:  54%|█████▎    | 89/166 [01:54<01:41,  1.31s/it]avg_loss = 2.0169162326388888:  54%|█████▎    | 89/166 [01:56<01:41,  1.31s/it]avg_loss = 2.0169162326388888:  54%|█████▍    | 90/166 [01:56<01:39,  1.31s/it]avg_loss = 2.016932235969292:  54%|█████▍    | 90/166 [01:57<01:39,  1.31s/it] avg_loss = 2.016932235969292:  55%|█████▍    | 91/166 [01:57<01:38,  1.31s/it]avg_loss = 2.0174566481424416:  55%|█████▍    | 91/166 [01:58<01:38,  1.31s/it]avg_loss = 2.0174566481424416:  55%|█████▌    | 92/166 [01:58<01:37,  1.31s/it]avg_loss = 2.0216539803371636:  55%|█████▌    | 92/166 [01:59<01:37,  1.31s/it]avg_loss = 2.0216539803371636:  56%|█████▌    | 93/166 [01:59<01:36,  1.32s/it]avg_loss = 2.0196240721864904:  56%|█████▌    | 93/166 [02:01<01:36,  1.32s/it]avg_loss = 2.0196240721864904:  57%|█████▋    | 94/166 [02:01<01:34,  1.32s/it]avg_loss = 2.0183403793134187:  57%|█████▋    | 94/166 [02:02<01:34,  1.32s/it]avg_loss = 2.0183403793134187:  57%|█████▋    | 95/166 [02:02<01:33,  1.32s/it]avg_loss = 2.01747606943051:  57%|█████▋    | 95/166 [02:03<01:33,  1.32s/it]  avg_loss = 2.01747606943051:  58%|█████▊    | 96/166 [02:03<01:32,  1.32s/it]avg_loss = 2.01748332780661:  58%|█████▊    | 96/166 [02:05<01:32,  1.32s/it]avg_loss = 2.01748332780661:  58%|█████▊    | 97/166 [02:05<01:30,  1.32s/it]avg_loss = 2.0152783673636767:  58%|█████▊    | 97/166 [02:06<01:30,  1.32s/it]avg_loss = 2.0152783673636767:  59%|█████▉    | 98/166 [02:06<01:29,  1.32s/it]avg_loss = 2.0124056230891836:  59%|█████▉    | 98/166 [02:07<01:29,  1.32s/it]avg_loss = 2.0124056230891836:  60%|█████▉    | 99/166 [02:07<01:28,  1.32s/it]avg_loss = 2.01011674284935:  60%|█████▉    | 99/166 [02:09<01:28,  1.32s/it]  avg_loss = 2.01011674284935:  60%|██████    | 100/166 [02:09<01:26,  1.32s/it]avg_loss = 2.010340349508984:  60%|██████    | 100/166 [02:10<01:26,  1.32s/it]avg_loss = 2.010340349508984:  61%|██████    | 101/166 [02:10<01:25,  1.32s/it]avg_loss = 2.011030880843892:  61%|██████    | 101/166 [02:11<01:25,  1.32s/it]avg_loss = 2.011030880843892:  61%|██████▏   | 102/166 [02:11<01:24,  1.32s/it]avg_loss = 2.012801590475064:  61%|██████▏   | 102/166 [02:13<01:24,  1.32s/it]avg_loss = 2.012801590475064:  62%|██████▏   | 103/166 [02:13<01:22,  1.32s/it]avg_loss = 2.015036458006272:  62%|██████▏   | 103/166 [02:14<01:22,  1.32s/it]avg_loss = 2.015036458006272:  63%|██████▎   | 104/166 [02:14<01:21,  1.32s/it]avg_loss = 2.0214178119386945:  63%|██████▎   | 104/166 [02:15<01:21,  1.32s/it]avg_loss = 2.0214178119386945:  63%|██████▎   | 105/166 [02:15<01:20,  1.32s/it]avg_loss = 2.026183837989591:  63%|██████▎   | 105/166 [02:17<01:20,  1.32s/it] avg_loss = 2.026183837989591:  64%|██████▍   | 106/166 [02:17<01:19,  1.32s/it]avg_loss = 2.0298847628531056:  64%|██████▍   | 106/166 [02:18<01:19,  1.32s/it]avg_loss = 2.0298847628531056:  64%|██████▍   | 107/166 [02:18<01:17,  1.32s/it]avg_loss = 2.033251447810067:  64%|██████▍   | 107/166 [02:19<01:17,  1.32s/it] avg_loss = 2.033251447810067:  65%|██████▌   | 108/166 [02:19<01:16,  1.32s/it]avg_loss = 2.0384833779903726:  65%|██████▌   | 108/166 [02:21<01:16,  1.32s/it]avg_loss = 2.0384833779903726:  66%|██████▌   | 109/166 [02:21<01:15,  1.32s/it]avg_loss = 2.042220955545252:  66%|██████▌   | 109/166 [02:22<01:15,  1.32s/it] avg_loss = 2.042220955545252:  66%|██████▋   | 110/166 [02:22<01:13,  1.32s/it]avg_loss = 2.0432969610970297:  66%|██████▋   | 110/166 [02:23<01:13,  1.32s/it]avg_loss = 2.0432969610970297:  67%|██████▋   | 111/166 [02:23<01:12,  1.32s/it]avg_loss = 2.0443229622074535:  67%|██████▋   | 111/166 [02:24<01:12,  1.32s/it]avg_loss = 2.0443229622074535:  67%|██████▋   | 112/166 [02:24<01:11,  1.32s/it]avg_loss = 2.043870387879093:  67%|██████▋   | 112/166 [02:26<01:11,  1.32s/it] avg_loss = 2.043870387879093:  68%|██████▊   | 113/166 [02:26<01:09,  1.32s/it]avg_loss = 2.0449685004719518:  68%|██████▊   | 113/166 [02:27<01:09,  1.32s/it]avg_loss = 2.0449685004719518:  69%|██████▊   | 114/166 [02:27<01:08,  1.32s/it]avg_loss = 2.043480015837628:  69%|██████▊   | 114/166 [02:28<01:08,  1.32s/it] avg_loss = 2.043480015837628:  69%|██████▉   | 115/166 [02:28<01:07,  1.32s/it]avg_loss = 2.0434660264130295:  69%|██████▉   | 115/166 [02:30<01:07,  1.32s/it]avg_loss = 2.0434660264130295:  70%|██████▉   | 116/166 [02:30<01:05,  1.32s/it]avg_loss = 2.044359725764674:  70%|██████▉   | 116/166 [02:31<01:05,  1.32s/it] avg_loss = 2.044359725764674:  70%|███████   | 117/166 [02:31<01:04,  1.32s/it]avg_loss = 2.0447683849577176:  70%|███████   | 117/166 [02:32<01:04,  1.32s/it]avg_loss = 2.0447683849577176:  71%|███████   | 118/166 [02:32<01:03,  1.32s/it]avg_loss = 2.0449429069246565:  71%|███████   | 118/166 [02:34<01:03,  1.32s/it]avg_loss = 2.0449429069246565:  72%|███████▏  | 119/166 [02:34<01:02,  1.32s/it]avg_loss = 2.045908358693123:  72%|███████▏  | 119/166 [02:35<01:02,  1.32s/it] avg_loss = 2.045908358693123:  72%|███████▏  | 120/166 [02:35<01:00,  1.32s/it]avg_loss = 2.0463044850294256:  72%|███████▏  | 120/166 [02:36<01:00,  1.32s/it]avg_loss = 2.0463044850294256:  73%|███████▎  | 121/166 [02:36<00:59,  1.32s/it]avg_loss = 2.0477954940717726:  73%|███████▎  | 121/166 [02:38<00:59,  1.32s/it]avg_loss = 2.0477954940717726:  73%|███████▎  | 122/166 [02:38<00:58,  1.32s/it]avg_loss = 2.0481379216279443:  73%|███████▎  | 122/166 [02:39<00:58,  1.32s/it]avg_loss = 2.0481379216279443:  74%|███████▍  | 123/166 [02:39<00:56,  1.32s/it]avg_loss = 2.046820297356575:  74%|███████▍  | 123/166 [02:40<00:56,  1.32s/it] avg_loss = 2.046820297356575:  75%|███████▍  | 124/166 [02:40<00:55,  1.32s/it]avg_loss = 2.045176737785339:  75%|███████▍  | 124/166 [02:42<00:55,  1.32s/it]avg_loss = 2.045176737785339:  75%|███████▌  | 125/166 [02:42<00:54,  1.32s/it]avg_loss = 2.043586625939324:  75%|███████▌  | 125/166 [02:43<00:54,  1.32s/it]avg_loss = 2.043586625939324:  76%|███████▌  | 126/166 [02:43<00:52,  1.32s/it]avg_loss = 2.0415187454599093:  76%|███████▌  | 126/166 [02:44<00:52,  1.32s/it]avg_loss = 2.0415187454599093:  77%|███████▋  | 127/166 [02:44<00:51,  1.32s/it]avg_loss = 2.040524943731725:  77%|███████▋  | 127/166 [02:46<00:51,  1.32s/it] avg_loss = 2.040524943731725:  77%|███████▋  | 128/166 [02:46<00:50,  1.32s/it]avg_loss = 2.039803329364274:  77%|███████▋  | 128/166 [02:47<00:50,  1.32s/it]avg_loss = 2.039803329364274:  78%|███████▊  | 129/166 [02:47<00:48,  1.32s/it]avg_loss = 2.0400058232820952:  78%|███████▊  | 129/166 [02:48<00:48,  1.32s/it]avg_loss = 2.0400058232820952:  78%|███████▊  | 130/166 [02:48<00:47,  1.32s/it]avg_loss = 2.0409350486202094:  78%|███████▊  | 130/166 [02:50<00:47,  1.32s/it]avg_loss = 2.0409350486202094:  79%|███████▉  | 131/166 [02:50<00:46,  1.32s/it]avg_loss = 2.0417621460827915:  79%|███████▉  | 131/166 [02:51<00:46,  1.32s/it]avg_loss = 2.0417621460827915:  80%|███████▉  | 132/166 [02:51<00:44,  1.32s/it]avg_loss = 2.0427402983930776:  80%|███████▉  | 132/166 [02:52<00:44,  1.32s/it]avg_loss = 2.0427402983930776:  80%|████████  | 133/166 [02:52<00:43,  1.32s/it]avg_loss = 2.04420859600181:  80%|████████  | 133/166 [02:53<00:43,  1.32s/it]  avg_loss = 2.04420859600181:  81%|████████  | 134/166 [02:53<00:42,  1.32s/it]avg_loss = 2.0418923775355022:  81%|████████  | 134/166 [02:55<00:42,  1.32s/it]avg_loss = 2.0418923775355022:  81%|████████▏ | 135/166 [02:55<00:40,  1.32s/it]avg_loss = 2.0414188267553555:  81%|████████▏ | 135/166 [02:56<00:40,  1.32s/it]avg_loss = 2.0414188267553555:  82%|████████▏ | 136/166 [02:56<00:39,  1.32s/it]avg_loss = 2.0420966139675056:  82%|████████▏ | 136/166 [02:57<00:39,  1.32s/it]avg_loss = 2.0420966139675056:  83%|████████▎ | 137/166 [02:57<00:38,  1.32s/it]avg_loss = 2.0427459521570066:  83%|████████▎ | 137/166 [02:59<00:38,  1.32s/it]avg_loss = 2.0427459521570066:  83%|████████▎ | 138/166 [02:59<00:36,  1.32s/it]avg_loss = 2.0415803854414025:  83%|████████▎ | 138/166 [03:00<00:36,  1.32s/it]avg_loss = 2.0415803854414025:  84%|████████▎ | 139/166 [03:00<00:35,  1.32s/it]avg_loss = 2.039882504088538:  84%|████████▎ | 139/166 [03:01<00:35,  1.32s/it] avg_loss = 2.039882504088538:  84%|████████▍ | 140/166 [03:01<00:34,  1.32s/it]avg_loss = 2.038259057288474:  84%|████████▍ | 140/166 [03:03<00:34,  1.32s/it]avg_loss = 2.038259057288474:  85%|████████▍ | 141/166 [03:03<00:33,  1.32s/it]avg_loss = 2.0378038622963595:  85%|████████▍ | 141/166 [03:04<00:33,  1.32s/it]avg_loss = 2.0378038622963595:  86%|████████▌ | 142/166 [03:04<00:31,  1.32s/it]avg_loss = 2.0363197968556332:  86%|████████▌ | 142/166 [03:05<00:31,  1.32s/it]avg_loss = 2.0363197968556332:  86%|████████▌ | 143/166 [03:05<00:30,  1.32s/it]avg_loss = 2.037446711626318:  86%|████████▌ | 143/166 [03:07<00:30,  1.32s/it] avg_loss = 2.037446711626318:  87%|████████▋ | 144/166 [03:07<00:29,  1.32s/it]avg_loss = 2.0361827562595236:  87%|████████▋ | 144/166 [03:08<00:29,  1.32s/it]avg_loss = 2.0361827562595236:  87%|████████▋ | 145/166 [03:08<00:27,  1.32s/it]avg_loss = 2.0357893933988596:  87%|████████▋ | 145/166 [03:09<00:27,  1.32s/it]avg_loss = 2.0357893933988596:  88%|████████▊ | 146/166 [03:09<00:26,  1.32s/it]avg_loss = 2.034546663971985:  88%|████████▊ | 146/166 [03:11<00:26,  1.32s/it] avg_loss = 2.034546663971985:  89%|████████▊ | 147/166 [03:11<00:25,  1.32s/it]avg_loss = 2.033246729824994:  89%|████████▊ | 147/166 [03:12<00:25,  1.32s/it]avg_loss = 2.033246729824994:  89%|████████▉ | 148/166 [03:12<00:23,  1.32s/it]avg_loss = 2.0314106805212546:  89%|████████▉ | 148/166 [03:13<00:23,  1.32s/it]avg_loss = 2.0314106805212546:  90%|████████▉ | 149/166 [03:13<00:22,  1.32s/it]avg_loss = 2.0322607445716856:  90%|████████▉ | 149/166 [03:15<00:22,  1.32s/it]avg_loss = 2.0322607445716856:  90%|█████████ | 150/166 [03:15<00:21,  1.32s/it]avg_loss = 2.0311469573848293:  90%|█████████ | 150/166 [03:16<00:21,  1.32s/it]avg_loss = 2.0311469573848293:  91%|█████████ | 151/166 [03:16<00:19,  1.32s/it]avg_loss = 2.0308738029316853:  91%|█████████ | 151/166 [03:17<00:19,  1.32s/it]avg_loss = 2.0308738029316853:  92%|█████████▏| 152/166 [03:17<00:18,  1.32s/it]avg_loss = 2.030606933668548:  92%|█████████▏| 152/166 [03:19<00:18,  1.32s/it] avg_loss = 2.030606933668548:  92%|█████████▏| 153/166 [03:19<00:17,  1.32s/it]avg_loss = 2.032600614931676:  92%|█████████▏| 153/166 [03:20<00:17,  1.32s/it]avg_loss = 2.032600614931676:  93%|█████████▎| 154/166 [03:20<00:15,  1.32s/it]avg_loss = 2.031784590598076:  93%|█████████▎| 154/166 [03:21<00:15,  1.32s/it]avg_loss = 2.031784590598076:  93%|█████████▎| 155/166 [03:21<00:14,  1.32s/it]avg_loss = 2.0314667385358076:  93%|█████████▎| 155/166 [03:23<00:14,  1.32s/it]avg_loss = 2.0314667385358076:  94%|█████████▍| 156/166 [03:23<00:13,  1.32s/it]avg_loss = 2.029182252610565:  94%|█████████▍| 156/166 [03:24<00:13,  1.32s/it] avg_loss = 2.029182252610565:  95%|█████████▍| 157/166 [03:24<00:11,  1.32s/it]avg_loss = 2.0239429353158687:  95%|█████████▍| 157/166 [03:25<00:11,  1.32s/it]avg_loss = 2.0239429353158687:  95%|█████████▌| 158/166 [03:25<00:10,  1.32s/it]avg_loss = 2.0246737213254726:  95%|█████████▌| 158/166 [03:27<00:10,  1.32s/it]avg_loss = 2.0246737213254726:  96%|█████████▌| 159/166 [03:27<00:09,  1.32s/it]avg_loss = 2.026117016375065:  96%|█████████▌| 159/166 [03:28<00:09,  1.32s/it] avg_loss = 2.026117016375065:  96%|█████████▋| 160/166 [03:28<00:07,  1.32s/it]avg_loss = 2.028401309659022:  96%|█████████▋| 160/166 [03:29<00:07,  1.32s/it]avg_loss = 2.028401309659022:  97%|█████████▋| 161/166 [03:29<00:06,  1.32s/it]avg_loss = 2.0291574590000105:  97%|█████████▋| 161/166 [03:30<00:06,  1.32s/it]avg_loss = 2.0291574590000105:  98%|█████████▊| 162/166 [03:30<00:05,  1.32s/it]avg_loss = 2.0292062788653227:  98%|█████████▊| 162/166 [03:32<00:05,  1.32s/it]avg_loss = 2.0292062788653227:  98%|█████████▊| 163/166 [03:32<00:03,  1.32s/it]avg_loss = 2.0302320398935456:  98%|█████████▊| 163/166 [03:33<00:03,  1.32s/it]avg_loss = 2.0302320398935456:  99%|█████████▉| 164/166 [03:33<00:02,  1.32s/it]avg_loss = 2.0307557915196273:  99%|█████████▉| 164/166 [03:34<00:02,  1.32s/it]avg_loss = 2.0307557915196273:  99%|█████████▉| 165/166 [03:34<00:01,  1.32s/it]avg_loss = 2.032802813024406:  99%|█████████▉| 165/166 [03:36<00:01,  1.32s/it] avg_loss = 2.032802813024406: 100%|██████████| 166/166 [03:36<00:00,  1.32s/it]avg_loss = 2.032802813024406: 100%|██████████| 166/166 [03:36<00:00,  1.30s/it]
I0402 14:18:35.121102 3226274 eval_ppl.py:107] wikitext2 perplexity: 7.6354570388793945
wikitext2 perplexity: 7.635
