I0402 14:18:43.917709 3226562 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:18:43.917805 3226562 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:18:43.917845 3226562 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:18:44.239749 3226562 config.py:54] PyTorch version 2.6.0 available.
W0402 14:18:44.427901 3226562 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:18:45.115608 3226562 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.47it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.94it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.11it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.81it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  7.93it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.14it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.01it/s]
I0402 14:18:46.066700 3226562 quantize_finetune_llama.py:152] loaded model
I0402 14:18:46.368450 3226562 quantize_finetune_llama.py:190] loaded compression model
I0402 14:19:00.644306 3226562 quantize_finetune_llama.py:194] loaded dataset and devset
I0402 14:19:05.664188 3226562 quantize_finetune_llama.py:214] layer 0 gpu 0
I0402 14:19:08.135425 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 0 in 2.3235857486724854s
Use train scale and shift
tensor(2.2655e-07, device='cuda:0') tensor(0.0204, device='cuda:0')
tensor(0.0204, device='cuda:0') tensor(2.2655e-07, device='cuda:0')
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0402 14:19:20.971081 3226691 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:19:20.971175 3226691 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:19:20.971212 3226691 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:19:21.294414 3226691 config.py:54] PyTorch version 2.6.0 available.
W0402 14:19:21.482169 3226691 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:19:22.033139 3226691 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:19:22.036893 3226562 quantize_finetune_llama.py:214] layer 1 gpu 1
I0402 14:19:22.186252 3226691 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:19:24.865893 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 1 in 2.658328056335449s
I0402 14:19:28.572499 3226759 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:19:28.572595 3226759 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:19:28.572636 3226759 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:19:28.933803 3226759 config.py:54] PyTorch version 2.6.0 available.
W0402 14:19:29.136587 3226759 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:19:29.736162 3226759 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:19:29.740073 3226562 quantize_finetune_llama.py:214] layer 2 gpu 2
I0402 14:19:29.986702 3226759 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:19:32.383157 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 2 in 2.4847910404205322s
I0402 14:19:36.238545 3226829 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:19:36.238646 3226829 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:19:36.238688 3226829 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:19:36.619920 3226829 config.py:54] PyTorch version 2.6.0 available.
W0402 14:19:36.841830 3226829 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:19:37.475765 3226829 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:19:37.479947 3226562 quantize_finetune_llama.py:214] layer 3 gpu 3
I0402 14:19:37.646053 3226829 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:19:40.067014 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 3 in 2.4055120944976807s
I0402 14:19:44.240864 3226899 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:19:44.240983 3226899 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:19:44.241029 3226899 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:19:44.662323 3226899 config.py:54] PyTorch version 2.6.0 available.
W0402 14:19:44.885687 3226899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:19:45.569187 3226899 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:19:45.573658 3226562 quantize_finetune_llama.py:214] layer 4 gpu 0
I0402 14:19:45.747399 3226899 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_v proxy err 0.003449175273999572 tr(WHW.T) 971.8771362304688
bpp_loss 3.211097836494446
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_q proxy err 0.0002568631316535175 tr(WHW.T) 636425.375
bpp_loss 3.201342821121216
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_k proxy err 0.000272555771516636 tr(WHW.T) 398864.15625
bpp_loss 3.297811508178711
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_o proxy err 0.0009674336179159582 tr(WHW.T) 15925.943359375
bpp_loss 3.052086114883423
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
0_up proxy err 0.007850322872400284 tr(WHW.T) 24117.18359375
bpp_loss 3.0919672056686047
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
0_gate proxy err 0.005502333864569664 tr(WHW.T) 35427.40234375
bpp_loss 3.106160186057867
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
0_down proxy err 0.003950825892388821 tr(WHW.T) 35796.33984375
bpp_loss 3.292078506114871
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_v proxy err 0.011852291412651539 tr(WHW.T) 657.0241088867188
bpp_loss 3.15949022769928
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_q proxy err 0.0003321862895973027 tr(WHW.T) 195417.453125
bpp_loss 4.0230371952056885
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_k proxy err 0.00032386110979132354 tr(WHW.T) 204295.5625
bpp_loss 4.017259359359741
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_o proxy err 0.006058722734451294 tr(WHW.T) 4039.630859375
bpp_loss 3.030909776687622
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
1_up proxy err 0.010825087316334248 tr(WHW.T) 23209.44921875
bpp_loss 3.1146513473155886
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
1_gate proxy err 0.005551290698349476 tr(WHW.T) 46954.0546875
bpp_loss 3.1776667750158976
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
1_down proxy err 0.00011641337914625183 tr(WHW.T) 40779.421875
bpp_loss 3.461853271306947
I0402 14:20:28.692006 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 4 in 1.0241553783416748s
I0402 14:20:32.661764 3226967 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:20:32.661867 3226967 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:20:32.661908 3226967 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:20:33.037898 3226967 config.py:54] PyTorch version 2.6.0 available.
W0402 14:20:33.258848 3226967 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:20:33.922042 3226967 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:20:33.926096 3226562 quantize_finetune_llama.py:214] layer 5 gpu 1
I0402 14:20:34.099011 3226967 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_v proxy err 0.012232761830091476 tr(WHW.T) 2779.86376953125
bpp_loss 3.2293663024902344
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_q proxy err 0.00043228495633229613 tr(WHW.T) 159508.203125
bpp_loss 4.000728607177734
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_k proxy err 0.00036977496347390115 tr(WHW.T) 210000.984375
bpp_loss 4.104736089706421
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_o proxy err 0.01132125686854124 tr(WHW.T) 5300.58984375
bpp_loss 3.0996798276901245
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
2_up proxy err 0.013338969089090824 tr(WHW.T) 19933.5703125
bpp_loss 3.123904738315316
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
2_gate proxy err 0.008583360351622105 tr(WHW.T) 31660.716796875
bpp_loss 3.2015620386877726
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
2_down proxy err 0.013221661560237408 tr(WHW.T) 17283.607421875
bpp_loss 3.1944512877353404
I0402 14:20:37.772643 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 5 in 1.0220720767974854s
I0402 14:20:41.677898 3227033 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:20:41.678004 3227033 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:20:41.678045 3227033 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:20:42.052362 3227033 config.py:54] PyTorch version 2.6.0 available.
W0402 14:20:42.263470 3227033 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:20:42.884996 3227033 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:20:42.888877 3226562 quantize_finetune_llama.py:214] layer 6 gpu 2
I0402 14:20:43.079126 3227033 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_v proxy err 0.017366541549563408 tr(WHW.T) 2978.910400390625
bpp_loss 3.1183557510375977
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_q proxy err 0.0009398179245181382 tr(WHW.T) 76174.0546875
bpp_loss 3.8257137537002563
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_k proxy err 0.0007324354373849928 tr(WHW.T) 106360.5859375
bpp_loss 3.9008774757385254
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_o proxy err 0.010134989395737648 tr(WHW.T) 5255.58154296875
bpp_loss 3.1000843048095703
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
3_up proxy err 0.014982213266193867 tr(WHW.T) 17467.81640625
bpp_loss 3.133835193722747
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
3_gate proxy err 0.009136275388300419 tr(WHW.T) 29396.990234375
bpp_loss 3.220843647801599
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
3_down proxy err 0.013249322772026062 tr(WHW.T) 16907.59375
bpp_loss 3.2047361773113874
I0402 14:20:45.328630 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 6 in 1.0624911785125732s
I0402 14:20:49.114691 3227101 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:20:49.114808 3227101 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:20:49.114851 3227101 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:20:49.489565 3227101 config.py:54] PyTorch version 2.6.0 available.
W0402 14:20:49.711424 3227101 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:20:50.358302 3227101 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:20:50.362410 3226562 quantize_finetune_llama.py:214] layer 7 gpu 3
I0402 14:20:50.542676 3227101 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:20:51.830497 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 7 in 0.9513936042785645s
I0402 14:20:55.999618 3227169 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:20:55.999726 3227169 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:20:55.999774 3227169 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:20:56.386698 3227169 config.py:54] PyTorch version 2.6.0 available.
W0402 14:20:56.610430 3227169 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:20:57.274379 3227169 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:20:57.278482 3226562 quantize_finetune_llama.py:214] layer 8 gpu 0
I0402 14:20:57.528736 3227169 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_v proxy err 0.015997903421521187 tr(WHW.T) 3097.555419921875
bpp_loss 3.1642234325408936
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_q proxy err 0.0008777098264545202 tr(WHW.T) 78746.46875
bpp_loss 3.9207407236099243
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_k proxy err 0.0006593862199224532 tr(WHW.T) 118660.75
bpp_loss 3.957667589187622
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_o proxy err 0.012436631135642529 tr(WHW.T) 5336.9765625
bpp_loss 3.0784976482391357
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
4_up proxy err 0.01429271325469017 tr(WHW.T) 17664.685546875
bpp_loss 3.131678204203761
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
4_gate proxy err 0.007182993460446596 tr(WHW.T) 36588.140625
bpp_loss 3.2511094559070677
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
4_down proxy err 0.013212486170232296 tr(WHW.T) 16830.9296875
bpp_loss 3.1947506971137467
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_v proxy err 0.01665625534951687 tr(WHW.T) 3166.572021484375
bpp_loss 3.169681429862976
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_q proxy err 0.0009794790530577302 tr(WHW.T) 72535.90625
bpp_loss 3.921308398246765
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_k proxy err 0.0006953954580239952 tr(WHW.T) 116207.296875
bpp_loss 4.000486373901367
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_o proxy err 0.014219697564840317 tr(WHW.T) 3769.5478515625
bpp_loss 3.1505424976348877
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
5_up proxy err 0.014153903350234032 tr(WHW.T) 18016.64453125
bpp_loss 3.130246184593023
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
5_gate proxy err 0.006740120239555836 tr(WHW.T) 39417.078125
bpp_loss 3.2573630754337755
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
5_down proxy err 0.014459307305514812 tr(WHW.T) 15953.3203125
bpp_loss 3.181986609170603
I0402 14:21:41.015074 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 8 in 1.0152969360351562s
I0402 14:21:45.059377 3227237 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:21:45.059482 3227237 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:21:45.059522 3227237 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:21:45.490550 3227237 config.py:54] PyTorch version 2.6.0 available.
W0402 14:21:45.711763 3227237 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:21:46.373090 3227237 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:21:46.377215 3226562 quantize_finetune_llama.py:214] layer 9 gpu 1
I0402 14:21:46.530515 3227237 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_v proxy err 0.01796438917517662 tr(WHW.T) 3180.833740234375
bpp_loss 3.097932457923889
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_q proxy err 0.0013379445299506187 tr(WHW.T) 54760.6953125
bpp_loss 3.784398913383484
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_k proxy err 0.0010320674628019333 tr(WHW.T) 75270.1171875
bpp_loss 3.8209500312805176
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_o proxy err 0.014646322466433048 tr(WHW.T) 4041.38525390625
bpp_loss 3.0807838439941406
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
6_up proxy err 0.01411226112395525 tr(WHW.T) 17971.6953125
bpp_loss 3.1273624508879907
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
6_gate proxy err 0.005871480330824852 tr(WHW.T) 45430.91015625
bpp_loss 3.279813899550327
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
6_down proxy err 0.014776758849620819 tr(WHW.T) 15434.75390625
bpp_loss 3.1799936294555664
I0402 14:21:49.261285 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 9 in 1.0055665969848633s
I0402 14:21:53.257802 3227303 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:21:53.257904 3227303 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:21:53.257944 3227303 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:21:53.635228 3227303 config.py:54] PyTorch version 2.6.0 available.
W0402 14:21:53.845429 3227303 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_v proxy err 0.017863120883703232 tr(WHW.T) 3250.746826171875
bpp_loss 3.1022469997406006
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_q proxy err 0.001434191013686359 tr(WHW.T) 51300.6015625
bpp_loss 3.777897357940674
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_k proxy err 0.0011361814104020596 tr(WHW.T) 68225.015625
bpp_loss 3.785091519355774
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_o proxy err 0.016298500820994377 tr(WHW.T) 3528.39208984375
bpp_loss 3.092591643333435
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
7_up proxy err 0.013621313497424126 tr(WHW.T) 18230.072265625
bpp_loss 3.1342785857444584
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
7_gate proxy err 0.005617193877696991 tr(WHW.T) 46648.5078125
bpp_loss 3.281926709552144
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
7_down proxy err 0.01504883635789156 tr(WHW.T) 15240.3759765625
bpp_loss 3.1796501292738806
W0402 14:21:54.460768 3227303 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:21:54.464770 3226562 quantize_finetune_llama.py:214] layer 10 gpu 2
I0402 14:21:54.618052 3227303 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:21:56.065576 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 10 in 0.8539316654205322s
I0402 14:21:59.994807 3227371 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:21:59.994899 3227371 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:21:59.994937 3227371 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:22:00.333751 3227371 config.py:54] PyTorch version 2.6.0 available.
W0402 14:22:00.542434 3227371 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:22:01.159258 3227371 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:22:01.163217 3226562 quantize_finetune_llama.py:214] layer 11 gpu 3
I0402 14:22:01.344875 3227371 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:22:02.917624 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 11 in 1.2334656715393066s
I0402 14:22:07.060843 3227439 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:22:07.060950 3227439 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:22:07.060993 3227439 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:22:07.414067 3227439 config.py:54] PyTorch version 2.6.0 available.
W0402 14:22:07.637826 3227439 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:22:08.270852 3227439 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:22:08.274840 3226562 quantize_finetune_llama.py:214] layer 12 gpu 0
I0402 14:22:08.470078 3227439 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_v proxy err 0.016249461099505424 tr(WHW.T) 3470.401611328125
bpp_loss 3.1283682584762573
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_q proxy err 0.001465755281969905 tr(WHW.T) 47592.55859375
bpp_loss 3.8083332777023315
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_k proxy err 0.0010659329127520323 tr(WHW.T) 70099.9296875
bpp_loss 3.816149950027466
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_o proxy err 0.01785803586244583 tr(WHW.T) 3118.695556640625
bpp_loss 3.1198254823684692
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
8_up proxy err 0.012379336170852184 tr(WHW.T) 19852.556640625
bpp_loss 3.1492339733035064
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
8_gate proxy err 0.005702021531760693 tr(WHW.T) 45311.83203125
bpp_loss 3.2628418678461117
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
8_down proxy err 0.015073164366185665 tr(WHW.T) 15322.3779296875
bpp_loss 3.1882273873617484
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_v proxy err 0.016220035031437874 tr(WHW.T) 3676.465087890625
bpp_loss 3.128753423690796
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_q proxy err 0.001571179018355906 tr(WHW.T) 45676.421875
bpp_loss 3.801352381706238
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_k proxy err 0.0010872537968680263 tr(WHW.T) 72055.03125
bpp_loss 3.844399333000183
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_o proxy err 0.018989475443959236 tr(WHW.T) 3148.60498046875
bpp_loss 3.117524266242981
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
9_up proxy err 0.0118890181183815 tr(WHW.T) 20605.642578125
bpp_loss 3.157138380893441
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
9_gate proxy err 0.005662914365530014 tr(WHW.T) 45406.61328125
bpp_loss 3.25112453726835
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
9_down proxy err 0.01532070990651846 tr(WHW.T) 15324.0654296875
bpp_loss 3.189862140389376
I0402 14:22:52.677993 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 12 in 0.9877538681030273s
I0402 14:22:56.585364 3227507 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:22:56.585473 3227507 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:22:56.585515 3227507 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:22:56.969402 3227507 config.py:54] PyTorch version 2.6.0 available.
W0402 14:22:57.186429 3227507 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:22:57.812913 3227507 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_v proxy err 0.016271358355879784 tr(WHW.T) 3651.62158203125
bpp_loss 3.122366428375244
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_q proxy err 0.0016192721668630838 tr(WHW.T) 43935.53515625
bpp_loss 3.800056576728821
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_k proxy err 0.0011109693441540003 tr(WHW.T) 69912.859375
bpp_loss 3.853150248527527
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_o proxy err 0.020489200949668884 tr(WHW.T) 3058.283935546875
bpp_loss 3.1020052433013916
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
10_up proxy err 0.011208592914044857 tr(WHW.T) 21904.43359375
bpp_loss 3.1686577020689497
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
10_gate proxy err 0.005590620916336775 tr(WHW.T) 45983.0625
bpp_loss 3.247361648914426
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
10_down proxy err 0.014446889981627464 tr(WHW.T) 16111.1357421875
bpp_loss 3.2024169966231946
I0402 14:22:57.816790 3226562 quantize_finetune_llama.py:214] layer 13 gpu 1
I0402 14:22:57.998184 3227507 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:23:00.099210 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 13 in 1.0127718448638916s
I0402 14:23:04.090541 3227573 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:23:04.090648 3227573 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:23:04.090690 3227573 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:23:04.469369 3227573 config.py:54] PyTorch version 2.6.0 available.
W0402 14:23:04.675161 3227573 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_v proxy err 0.016239916905760765 tr(WHW.T) 3890.81982421875
bpp_loss 3.1381722688674927
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_q proxy err 0.0019459305331110954 tr(WHW.T) 38073.125
bpp_loss 3.690475821495056
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_k proxy err 0.0013825824717059731 tr(WHW.T) 56997.671875
bpp_loss 3.68112051486969
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_o proxy err 0.02096632309257984 tr(WHW.T) 3054.2177734375
bpp_loss 3.126431107521057
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
11_up proxy err 0.01152049284428358 tr(WHW.T) 21548.3046875
bpp_loss 3.1766048697538154
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
11_gate proxy err 0.005718966480344534 tr(WHW.T) 45386.7265625
bpp_loss 3.2410908188930776
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
11_down proxy err 0.015107898972928524 tr(WHW.T) 15730.783203125
bpp_loss 3.2018387594888376
W0402 14:23:05.262874 3227573 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:23:05.266827 3226562 quantize_finetune_llama.py:214] layer 14 gpu 2
I0402 14:23:05.476863 3227573 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:23:07.263795 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 14 in 0.9230339527130127s
I0402 14:23:11.159455 3227641 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:23:11.159545 3227641 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:23:11.159582 3227641 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:23:11.504176 3227641 config.py:54] PyTorch version 2.6.0 available.
W0402 14:23:11.710829 3227641 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:23:12.332794 3227641 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:23:12.336767 3226562 quantize_finetune_llama.py:214] layer 15 gpu 3
I0402 14:23:12.510046 3227641 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:23:13.872616 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 15 in 1.0291128158569336s
I0402 14:23:17.914673 3227709 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:23:17.914775 3227709 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:23:17.914820 3227709 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:23:18.292965 3227709 config.py:54] PyTorch version 2.6.0 available.
W0402 14:23:18.510745 3227709 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:23:19.147237 3227709 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:23:19.151278 3226562 quantize_finetune_llama.py:214] layer 16 gpu 0
I0402 14:23:19.381015 3227709 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_v proxy err 0.01678355410695076 tr(WHW.T) 3807.353515625
bpp_loss 3.125212073326111
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_q proxy err 0.0019479549955576658 tr(WHW.T) 38406.0
bpp_loss 3.7255553007125854
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_k proxy err 0.0013498401967808604 tr(WHW.T) 59429.25
bpp_loss 3.777609705924988
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_o proxy err 0.021857159212231636 tr(WHW.T) 2997.118896484375
bpp_loss 3.1113134622573853
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
12_up proxy err 0.011467655189335346 tr(WHW.T) 21806.251953125
bpp_loss 3.185976959938227
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
12_gate proxy err 0.006133186165243387 tr(WHW.T) 42415.66015625
bpp_loss 3.232436956361283
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
12_down proxy err 0.01495423074811697 tr(WHW.T) 15783.7470703125
bpp_loss 3.214163114858228
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_v proxy err 0.017314499244093895 tr(WHW.T) 3883.96826171875
bpp_loss 3.1389567852020264
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_q proxy err 0.002059673424810171 tr(WHW.T) 38091.8046875
bpp_loss 3.6953951120376587
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_k proxy err 0.0014590582577511668 tr(WHW.T) 57141.765625
bpp_loss 3.7235900163650513
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_o proxy err 0.019741788506507874 tr(WHW.T) 3392.266845703125
bpp_loss 3.131727933883667
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
13_up proxy err 0.011022082529962063 tr(WHW.T) 22688.076171875
bpp_loss 3.197845458984375
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
13_gate proxy err 0.00600526062771678 tr(WHW.T) 43286.52734375
bpp_loss 3.228291711141897
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
13_down proxy err 0.014594768173992634 tr(WHW.T) 15750.5673828125
bpp_loss 3.232555300690407
I0402 14:24:03.689259 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 16 in 0.9645280838012695s
I0402 14:24:07.692296 3227777 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:24:07.692406 3227777 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:24:07.692449 3227777 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:24:08.086581 3227777 config.py:54] PyTorch version 2.6.0 available.
W0402 14:24:08.302328 3227777 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_v proxy err 0.018345890566706657 tr(WHW.T) 3647.569580078125
bpp_loss 3.1251394748687744
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_q proxy err 0.002102487487718463 tr(WHW.T) 36835.87109375
bpp_loss 3.6923463344573975
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_k proxy err 0.0014166890177875757 tr(WHW.T) 58844.5546875
bpp_loss 3.719924569129944
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_o proxy err 0.0224919430911541 tr(WHW.T) 3063.885498046875
bpp_loss 3.1071503162384033
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
14_up proxy err 0.01128298882395029 tr(WHW.T) 22452.671875
bpp_loss 3.1970782612645348
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
14_gate proxy err 0.0063608549535274506 tr(WHW.T) 41240.93359375
bpp_loss 3.223872340002725
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
14_down proxy err 0.014854890294373035 tr(WHW.T) 15409.48828125
bpp_loss 3.235201724739962
W0402 14:24:08.976010 3227777 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:24:08.980171 3226562 quantize_finetune_llama.py:214] layer 17 gpu 1
I0402 14:24:09.210436 3227777 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:24:11.208758 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 17 in 1.2471368312835693s
I0402 14:24:15.155260 3227843 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:24:15.155356 3227843 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:24:15.155399 3227843 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:24:15.523527 3227843 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_v proxy err 0.016421834006905556 tr(WHW.T) 4003.05126953125
bpp_loss 3.1633018255233765
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_q proxy err 0.0019838076550513506 tr(WHW.T) 38353.34765625
bpp_loss 3.676385760307312
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_k proxy err 0.001392542035318911 tr(WHW.T) 58600.94921875
bpp_loss 3.732180953025818
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_o proxy err 0.019378740340471268 tr(WHW.T) 3628.3759765625
bpp_loss 3.1316381692886353
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
15_up proxy err 0.01096426509320736 tr(WHW.T) 23066.35546875
bpp_loss 3.2038987625476927
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
15_gate proxy err 0.00638204300776124 tr(WHW.T) 40946.2890625
bpp_loss 3.2310948926349017
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
15_down proxy err 0.014459476806223392 tr(WHW.T) 15429.0400390625
bpp_loss 3.248369993165482
W0402 14:24:15.728365 3227843 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:24:16.328079 3227843 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:24:16.331875 3226562 quantize_finetune_llama.py:214] layer 18 gpu 2
I0402 14:24:16.679910 3227843 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:24:17.818246 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 18 in 0.9757061004638672s
I0402 14:24:21.824428 3227911 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:24:21.824517 3227911 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:24:21.824555 3227911 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:24:22.164874 3227911 config.py:54] PyTorch version 2.6.0 available.
W0402 14:24:22.351080 3227911 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:24:22.977550 3227911 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:24:22.981589 3226562 quantize_finetune_llama.py:214] layer 19 gpu 3
I0402 14:24:23.202727 3227911 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:24:24.477467 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 19 in 0.9622395038604736s
I0402 14:24:28.513220 3227979 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:24:28.513324 3227979 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:24:28.513369 3227979 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:24:28.908236 3227979 config.py:54] PyTorch version 2.6.0 available.
W0402 14:24:29.116871 3227979 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:24:29.766395 3227979 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:24:29.770713 3226562 quantize_finetune_llama.py:214] layer 20 gpu 0
I0402 14:24:29.964982 3227979 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_v proxy err 0.017240555956959724 tr(WHW.T) 3988.751708984375
bpp_loss 3.1866270303726196
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_q proxy err 0.0021271223668009043 tr(WHW.T) 37064.6328125
bpp_loss 3.6517505645751953
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_k proxy err 0.0014142965665087104 tr(WHW.T) 59978.5
bpp_loss 3.692152738571167
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_o proxy err 0.01657016947865486 tr(WHW.T) 4695.22509765625
bpp_loss 3.1391561031341553
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
16_up proxy err 0.010989176109433174 tr(WHW.T) 23603.60546875
bpp_loss 3.1992799625840296
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
16_gate proxy err 0.006349651608616114 tr(WHW.T) 42174.73828125
bpp_loss 3.2361936347429143
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
16_down proxy err 0.014700767584145069 tr(WHW.T) 15228.4970703125
bpp_loss 3.246479034423828
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_v proxy err 0.017184434458613396 tr(WHW.T) 4263.80712890625
bpp_loss 3.164485454559326
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_q proxy err 0.0022923299111425877 tr(WHW.T) 36392.74609375
bpp_loss 3.6203943490982056
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_k proxy err 0.0016309759812429547 tr(WHW.T) 54388.6953125
bpp_loss 3.6534194946289062
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_o proxy err 0.017805444076657295 tr(WHW.T) 4297.923828125
bpp_loss 3.145080327987671
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
17_up proxy err 0.012212061323225498 tr(WHW.T) 21615.3203125
bpp_loss 3.1918104304823767
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
17_gate proxy err 0.006747347768396139 tr(WHW.T) 40300.40234375
bpp_loss 3.2450628945993825
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
17_down proxy err 0.015046640299260616 tr(WHW.T) 15353.4287109375
bpp_loss 3.233863719674044
I0402 14:25:14.640100 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 20 in 0.9546256065368652s
I0402 14:25:18.661349 3228047 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:25:18.661458 3228047 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:25:18.661498 3228047 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:25:19.050023 3228047 config.py:54] PyTorch version 2.6.0 available.
W0402 14:25:19.264834 3228047 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_v proxy err 0.01637410745024681 tr(WHW.T) 4673.08837890625
bpp_loss 3.195167899131775
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_q proxy err 0.0024407550226897 tr(WHW.T) 35237.796875
bpp_loss 3.57098126411438
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_k proxy err 0.0018258034251630306 tr(WHW.T) 49119.20703125
bpp_loss 3.6038697957992554
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_o proxy err 0.015282153151929379 tr(WHW.T) 4923.25390625
bpp_loss 3.187143087387085
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
18_up proxy err 0.013062957674264908 tr(WHW.T) 20299.947265625
bpp_loss 3.187581084495367
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
18_gate proxy err 0.007167808245867491 tr(WHW.T) 38072.26171875
bpp_loss 3.2559007156726927
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
18_down proxy err 0.014711318537592888 tr(WHW.T) 15266.072265625
bpp_loss 3.2425944749699083
W0402 14:25:19.879048 3228047 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:25:19.882991 3226562 quantize_finetune_llama.py:214] layer 21 gpu 1
I0402 14:25:20.104246 3228047 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:25:21.724506 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 21 in 0.90989089012146s
I0402 14:25:25.807652 3228113 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:25:25.807762 3228113 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:25:25.807806 3228113 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:25:26.187720 3228113 config.py:54] PyTorch version 2.6.0 available.
W0402 14:25:26.409089 3228113 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_v proxy err 0.016127070412039757 tr(WHW.T) 4788.90576171875
bpp_loss 3.2010860443115234
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_q proxy err 0.002627012552693486 tr(WHW.T) 32879.76953125
bpp_loss 3.5478016138076782
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_k proxy err 0.0018259508069604635 tr(WHW.T) 49984.76171875
bpp_loss 3.5739041566848755
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_o proxy err 0.015849951654672623 tr(WHW.T) 5005.189453125
bpp_loss 3.181825041770935
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
19_up proxy err 0.013178546912968159 tr(WHW.T) 20172.62109375
bpp_loss 3.1870095896166424
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
19_gate proxy err 0.007861664518713951 tr(WHW.T) 34698.6953125
bpp_loss 3.2602196626884994
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
19_down proxy err 0.014363597147166729 tr(WHW.T) 15697.3017578125
bpp_loss 3.243936294733092
W0402 14:25:27.090447 3228113 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:25:27.094424 3226562 quantize_finetune_llama.py:214] layer 22 gpu 2
I0402 14:25:27.341739 3228113 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:25:28.735377 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 22 in 0.9377739429473877s
I0402 14:25:32.673719 3228181 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:25:32.673814 3228181 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:25:32.673856 3228181 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:25:33.015732 3228181 config.py:54] PyTorch version 2.6.0 available.
W0402 14:25:33.224037 3228181 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:25:33.810296 3228181 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:25:33.814074 3226562 quantize_finetune_llama.py:214] layer 23 gpu 3
I0402 14:25:34.133159 3228181 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:25:35.502318 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 23 in 1.2079908847808838s
I0402 14:25:39.476435 3228249 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:25:39.476534 3228249 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:25:39.476576 3228249 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:25:39.865945 3228249 config.py:54] PyTorch version 2.6.0 available.
W0402 14:25:40.074403 3228249 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:25:40.768442 3228249 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:25:40.772396 3226562 quantize_finetune_llama.py:214] layer 24 gpu 0
I0402 14:25:40.958739 3228249 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_v proxy err 0.016899894922971725 tr(WHW.T) 4645.8291015625
bpp_loss 3.2109488248825073
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_q proxy err 0.002601352520287037 tr(WHW.T) 33815.2578125
bpp_loss 3.552389621734619
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_k proxy err 0.00187750777695328 tr(WHW.T) 49149.9921875
bpp_loss 3.5771923065185547
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_o proxy err 0.011533874087035656 tr(WHW.T) 6820.419921875
bpp_loss 3.206204652786255
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
20_up proxy err 0.012980829924345016 tr(WHW.T) 20603.083984375
bpp_loss 3.1853905611259994
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
20_gate proxy err 0.007710649166256189 tr(WHW.T) 35509.98046875
bpp_loss 3.266775086868641
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
20_down proxy err 0.014055056497454643 tr(WHW.T) 15803.095703125
bpp_loss 3.2493404565855513
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_v proxy err 0.01659626141190529 tr(WHW.T) 4862.75634765625
bpp_loss 3.235037088394165
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_q proxy err 0.002944192150607705 tr(WHW.T) 30261.54296875
bpp_loss 3.5090047121047974
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_k proxy err 0.002164786448702216 tr(WHW.T) 42768.43359375
bpp_loss 3.521804928779602
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_o proxy err 0.013627600856125355 tr(WHW.T) 6399.25439453125
bpp_loss 3.192610025405884
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
21_up proxy err 0.013685005716979504 tr(WHW.T) 19562.884765625
bpp_loss 3.1831588745117188
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
21_gate proxy err 0.008235597051680088 tr(WHW.T) 33251.44921875
bpp_loss 3.275526534679324
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
21_down proxy err 0.014349672943353653 tr(WHW.T) 15792.2314453125
bpp_loss 3.2419235761775527
I0402 14:26:25.758255 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 24 in 1.0037953853607178s
I0402 14:26:29.778394 3228317 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:26:29.778498 3228317 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:26:29.778540 3228317 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:26:30.162418 3228317 config.py:54] PyTorch version 2.6.0 available.
W0402 14:26:30.371742 3228317 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_v proxy err 0.01567155122756958 tr(WHW.T) 5111.93212890625
bpp_loss 3.2416672706604004
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_q proxy err 0.002762292744591832 tr(WHW.T) 32103.412109375
bpp_loss 3.545489549636841
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_k proxy err 0.002093113027513027 tr(WHW.T) 43969.734375
bpp_loss 3.562350034713745
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_o proxy err 0.010652967728674412 tr(WHW.T) 7627.48828125
bpp_loss 3.222295045852661
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
22_up proxy err 0.013810674659907818 tr(WHW.T) 19439.994140625
bpp_loss 3.182108147199764
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
22_gate proxy err 0.008383394218981266 tr(WHW.T) 32731.19921875
bpp_loss 3.282900255779887
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
22_down proxy err 0.014435218647122383 tr(WHW.T) 15874.603515625
bpp_loss 3.2384963811830034
W0402 14:26:30.967208 3228317 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:26:30.971120 3226562 quantize_finetune_llama.py:214] layer 25 gpu 1
I0402 14:26:31.187396 3228317 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:26:32.746603 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 25 in 0.9613523483276367s
I0402 14:26:36.737852 3228383 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:26:36.737948 3228383 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:26:36.737991 3228383 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:26:37.098014 3228383 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_v proxy err 0.014826459810137749 tr(WHW.T) 5666.529296875
bpp_loss 3.279098868370056
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_q proxy err 0.0032431960571557283 tr(WHW.T) 28221.720703125
bpp_loss 3.520706295967102
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_k proxy err 0.0024662187788635492 tr(WHW.T) 38375.3828125
bpp_loss 3.5304096937179565
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_o proxy err 0.013204698450863361 tr(WHW.T) 6345.388671875
bpp_loss 3.2659794092178345
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
23_up proxy err 0.014300431124866009 tr(WHW.T) 18753.931640625
bpp_loss 3.187927778377089
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
23_gate proxy err 0.008982460014522076 tr(WHW.T) 30400.564453125
bpp_loss 3.282892626385356
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
23_down proxy err 0.014506912790238857 tr(WHW.T) 15816.7724609375
bpp_loss 3.2448954027752546
W0402 14:26:37.302794 3228383 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:26:37.945520 3228383 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:26:37.949505 3226562 quantize_finetune_llama.py:214] layer 26 gpu 2
I0402 14:26:38.215567 3228383 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:26:39.399841 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 26 in 0.9664714336395264s
I0402 14:26:43.258902 3228451 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:26:43.259002 3228451 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:26:43.259044 3228451 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:26:43.612576 3228451 config.py:54] PyTorch version 2.6.0 available.
W0402 14:26:43.808141 3228451 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:26:44.400517 3228451 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:26:44.404393 3226562 quantize_finetune_llama.py:214] layer 27 gpu 3
I0402 14:26:44.562956 3228451 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:26:45.827950 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 27 in 0.959263801574707s
I0402 14:26:49.811930 3228519 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:26:49.812028 3228519 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:26:49.812069 3228519 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:26:50.186558 3228519 config.py:54] PyTorch version 2.6.0 available.
W0402 14:26:50.404570 3228519 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:26:51.080148 3228519 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:26:51.084082 3226562 quantize_finetune_llama.py:214] layer 28 gpu 0
I0402 14:26:51.256128 3228519 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_v proxy err 0.0152240926399827 tr(WHW.T) 5324.72998046875
bpp_loss 3.280334949493408
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_q proxy err 0.0032657114788889885 tr(WHW.T) 27011.373046875
bpp_loss 3.486903667449951
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_k proxy err 0.0023215217515826225 tr(WHW.T) 39756.4296875
bpp_loss 3.490050435066223
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_o proxy err 0.010419320315122604 tr(WHW.T) 8038.615234375
bpp_loss 3.2511156797409058
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
24_up proxy err 0.014515134505927563 tr(WHW.T) 18512.99609375
bpp_loss 3.191232903059139
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
24_gate proxy err 0.009073792025446892 tr(WHW.T) 30171.666015625
bpp_loss 3.28581042622411
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
24_down proxy err 0.014479536563158035 tr(WHW.T) 15756.2470703125
bpp_loss 3.2512190397395644
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_v proxy err 0.01453827228397131 tr(WHW.T) 5927.5078125
bpp_loss 3.304826498031616
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_q proxy err 0.00370137020945549 tr(WHW.T) 25041.619140625
bpp_loss 3.481110453605652
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_k proxy err 0.002822659909725189 tr(WHW.T) 33632.96875
bpp_loss 3.4836610555648804
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_o proxy err 0.013428757898509502 tr(WHW.T) 6779.06689453125
bpp_loss 3.2680011987686157
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
25_up proxy err 0.0143655464053154 tr(WHW.T) 18624.029296875
bpp_loss 3.1970103064248727
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
25_gate proxy err 0.00879143550992012 tr(WHW.T) 31053.6875
bpp_loss 3.2890795330668605
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
25_down proxy err 0.013719719834625721 tr(WHW.T) 15877.4140625
bpp_loss 3.273108903751817
I0402 14:27:36.641997 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 28 in 0.9575350284576416s
I0402 14:27:40.640924 3228587 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:27:40.641026 3228587 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:27:40.641066 3228587 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:27:41.024440 3228587 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_v proxy err 0.014180038124322891 tr(WHW.T) 5920.73828125
bpp_loss 3.334803819656372
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_q proxy err 0.00340343383140862 tr(WHW.T) 26711.5390625
bpp_loss 3.464705228805542
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_k proxy err 0.0025089315604418516 tr(WHW.T) 37533.6015625
bpp_loss 3.472787380218506
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_o proxy err 0.007881979458034039 tr(WHW.T) 9866.05859375
bpp_loss 3.3550949096679688
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
26_up proxy err 0.013512692414224148 tr(WHW.T) 19856.3125
bpp_loss 3.2010851128156794
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
26_gate proxy err 0.008177886717021465 tr(WHW.T) 33454.33203125
bpp_loss 3.293121337890625
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
26_down proxy err 0.013759497553110123 tr(WHW.T) 15425.537109375
bpp_loss 3.2853653708169626
W0402 14:27:41.246248 3228587 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:27:41.874501 3228587 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:27:41.878549 3226562 quantize_finetune_llama.py:214] layer 29 gpu 1
I0402 14:27:42.194164 3228587 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:27:43.556229 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 29 in 1.1551649570465088s
I0402 14:27:47.453262 3228653 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:27:47.453347 3228653 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:27:47.453383 3228653 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_v proxy err 0.013734258711338043 tr(WHW.T) 6537.79541015625
bpp_loss 3.3134087324142456
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_q proxy err 0.003451462835073471 tr(WHW.T) 28139.56640625
bpp_loss 3.5006591081619263
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_k proxy err 0.002554655307903886 tr(WHW.T) 38878.3359375
bpp_loss 3.5147178173065186
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_o proxy err 0.010372056625783443 tr(WHW.T) 7266.2978515625
bpp_loss 3.3717366456985474
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
27_up proxy err 0.012289093807339668 tr(WHW.T) 21823.326171875
bpp_loss 3.207536209461301
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
27_gate proxy err 0.007707604672759771 tr(WHW.T) 35491.0
bpp_loss 3.2949491988780886
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
27_down proxy err 0.01317306887358427 tr(WHW.T) 15119.01171875
bpp_loss 3.3121228328970975
I0402 14:27:47.773765 3228653 config.py:54] PyTorch version 2.6.0 available.
W0402 14:27:47.980908 3228653 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:27:48.632359 3228653 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:27:48.636322 3226562 quantize_finetune_llama.py:214] layer 30 gpu 2
I0402 14:27:48.843872 3228653 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:27:50.011103 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 30 in 0.8901212215423584s
I0402 14:27:53.857603 3228721 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:27:53.857703 3228721 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:27:53.857746 3228721 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:27:54.218389 3228721 config.py:54] PyTorch version 2.6.0 available.
W0402 14:27:54.423754 3228721 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:27:55.070110 3228721 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:27:55.073923 3226562 quantize_finetune_llama.py:214] layer 31 gpu 3
I0402 14:27:55.260535 3228721 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:27:56.576134 3226562 quantize_finetune_llama.py:245] computed original embedding for layer 31 in 1.0076384544372559s
I0402 14:28:00.629420 3228789 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:28:00.629523 3228789 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:28:00.629564 3228789 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:28:01.031660 3228789 config.py:54] PyTorch version 2.6.0 available.
W0402 14:28:01.243143 3228789 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:28:01.882452 3228789 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:28:02.083811 3228789 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_v proxy err 0.012759819626808167 tr(WHW.T) 7077.5078125
bpp_loss 3.3547351360321045
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_q proxy err 0.003592107445001602 tr(WHW.T) 27002.65625
bpp_loss 3.453751564025879
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_k proxy err 0.002672964707016945 tr(WHW.T) 37243.53125
bpp_loss 3.4705817699432373
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_o proxy err 0.008827760815620422 tr(WHW.T) 8884.01171875
bpp_loss 3.398705244064331
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
28_up proxy err 0.010248529724776745 tr(WHW.T) 26161.169921875
bpp_loss 3.220051965048147
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
28_gate proxy err 0.007410010322928429 tr(WHW.T) 36794.0625
bpp_loss 3.289794212163881
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
28_down proxy err 0.012051510624587536 tr(WHW.T) 14984.466796875
bpp_loss 3.3474185411320176
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_v proxy err 0.01333667989820242 tr(WHW.T) 6682.36328125
bpp_loss 3.3648968935012817
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_q proxy err 0.003563020611181855 tr(WHW.T) 27006.056640625
bpp_loss 3.417202353477478
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_k proxy err 0.0025199544616043568 tr(WHW.T) 39489.33984375
bpp_loss 3.4265825748443604
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_o proxy err 0.008393821306526661 tr(WHW.T) 10599.048828125
bpp_loss 3.3736857175827026
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
29_up proxy err 0.008194022811949253 tr(WHW.T) 32884.11328125
bpp_loss 3.2316395959188773
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
29_gate proxy err 0.0068276748061180115 tr(WHW.T) 39942.984375
bpp_loss 3.2936834290970203
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
29_down proxy err 0.010981843806803226 tr(WHW.T) 14732.544921875
bpp_loss 3.380009739897972
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_v proxy err 0.011488964781165123 tr(WHW.T) 8207.525390625
bpp_loss 3.3784862756729126
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_q proxy err 0.0035421415232121944 tr(WHW.T) 28540.68359375
bpp_loss 3.4083274602890015
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_k proxy err 0.0026969353202730417 tr(WHW.T) 38445.2421875
bpp_loss 3.427168369293213
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_o proxy err 0.007619166746735573 tr(WHW.T) 10163.1123046875
bpp_loss 3.4550697803497314
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
30_up proxy err 0.005104781594127417 tr(WHW.T) 53873.95703125
bpp_loss 3.2521836036859555
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
30_gate proxy err 0.00469553004950285 tr(WHW.T) 59167.79296875
bpp_loss 3.3246672874273258
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
30_down proxy err 0.0038528554141521454 tr(WHW.T) 25842.498046875
bpp_loss 3.4577718335528704
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_v proxy err 0.013794874772429466 tr(WHW.T) 6740.33837890625
bpp_loss 3.2557804584503174
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_q proxy err 0.0028274324722588062 tr(WHW.T) 36698.0
bpp_loss 3.42337703704834
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_k proxy err 0.0019811068195849657 tr(WHW.T) 54793.80859375
bpp_loss 3.4772950410842896
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_o proxy err 0.00469482596963644 tr(WHW.T) 13121.5947265625
bpp_loss 3.4032315015792847
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
31_up proxy err 0.002959523117169738 tr(WHW.T) 95778.3046875
bpp_loss 3.303309063578761
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
31_gate proxy err 0.0029247424099594355 tr(WHW.T) 97550.4453125
bpp_loss 3.388023908748183
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
31_down proxy err 0.0019254091894254088 tr(WHW.T) 37026.6875
bpp_loss 3.513652025267135
I0402 14:29:08.773691 3228857 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:29:08.773845 3228857 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:29:08.773888 3228857 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:29:09.242977 3228857 config.py:54] PyTorch version 2.6.0 available.
W0402 14:29:09.445889 3228857 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0402 14:29:09.564218 3228857 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  3.14it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:01,  3.86it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  4.25it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  4.49it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  4.64it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.30it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.24it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.17it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.95it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.35it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.58it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.63it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.70it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.47it/s]
I0402 14:29:12.941919 3228857 hfize_llama.py:153] loaded layer 0
I0402 14:29:13.918920 3228857 hfize_llama.py:153] loaded layer 1
I0402 14:29:14.892956 3228857 hfize_llama.py:153] loaded layer 2
I0402 14:29:15.868697 3228857 hfize_llama.py:153] loaded layer 3
I0402 14:29:16.766137 3228857 hfize_llama.py:153] loaded layer 4
I0402 14:29:17.628648 3228857 hfize_llama.py:153] loaded layer 5
I0402 14:29:18.522768 3228857 hfize_llama.py:153] loaded layer 6
I0402 14:29:19.412715 3228857 hfize_llama.py:153] loaded layer 7
I0402 14:29:20.304390 3228857 hfize_llama.py:153] loaded layer 8
I0402 14:29:21.178884 3228857 hfize_llama.py:153] loaded layer 9
I0402 14:29:22.072284 3228857 hfize_llama.py:153] loaded layer 10
I0402 14:29:22.956237 3228857 hfize_llama.py:153] loaded layer 11
I0402 14:29:23.836405 3228857 hfize_llama.py:153] loaded layer 12
I0402 14:29:24.714946 3228857 hfize_llama.py:153] loaded layer 13
I0402 14:29:25.588092 3228857 hfize_llama.py:153] loaded layer 14
I0402 14:29:26.465762 3228857 hfize_llama.py:153] loaded layer 15
I0402 14:29:27.352859 3228857 hfize_llama.py:153] loaded layer 16
I0402 14:29:28.245977 3228857 hfize_llama.py:153] loaded layer 17
I0402 14:29:29.127961 3228857 hfize_llama.py:153] loaded layer 18
I0402 14:29:30.000622 3228857 hfize_llama.py:153] loaded layer 19
I0402 14:29:30.869812 3228857 hfize_llama.py:153] loaded layer 20
I0402 14:29:31.767889 3228857 hfize_llama.py:153] loaded layer 21
I0402 14:29:32.644028 3228857 hfize_llama.py:153] loaded layer 22
I0402 14:29:33.510149 3228857 hfize_llama.py:153] loaded layer 23
I0402 14:29:34.386017 3228857 hfize_llama.py:153] loaded layer 24
I0402 14:29:35.258477 3228857 hfize_llama.py:153] loaded layer 25
I0402 14:29:36.132663 3228857 hfize_llama.py:153] loaded layer 26
I0402 14:29:36.988943 3228857 hfize_llama.py:153] loaded layer 27
I0402 14:29:37.846107 3228857 hfize_llama.py:153] loaded layer 28
I0402 14:29:38.688099 3228857 hfize_llama.py:153] loaded layer 29
I0402 14:29:39.552634 3228857 hfize_llama.py:153] loaded layer 30
I0402 14:29:40.426593 3228857 hfize_llama.py:153] loaded layer 31
I0402 14:29:40.426706 3228857 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.06s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.06it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.10it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.13it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.19it/s]
I0402 14:30:11.909920 3228857 hfize_llama.py:167] successfully loaded hfized model
I0402 14:30:16.358616 3229075 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:30:16.358770 3229075 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:30:16.358811 3229075 utils.py:162] NumExpr defaulting to 16 threads.
W0402 14:30:16.793729 3229075 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0402 14:30:17.243553 3229075 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.09s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.05s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.02s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.05it/s]
I0402 14:30:23.067038 3229075 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/166 [00:00<?, ?it/s]avg_loss = 1.405918836593628:   0%|          | 0/166 [00:01<?, ?it/s]avg_loss = 1.405918836593628:   1%|          | 1/166 [00:01<04:25,  1.61s/it]avg_loss = 1.6689251065254211:   1%|          | 1/166 [00:02<04:25,  1.61s/it]avg_loss = 1.6689251065254211:   1%|          | 2/166 [00:02<03:44,  1.37s/it]avg_loss = 1.835756738980611:   1%|          | 2/166 [00:04<03:44,  1.37s/it] avg_loss = 1.835756738980611:   2%|▏         | 3/166 [00:04<03:30,  1.29s/it]avg_loss = 1.868335872888565:   2%|▏         | 3/166 [00:05<03:30,  1.29s/it]avg_loss = 1.868335872888565:   2%|▏         | 4/166 [00:05<03:23,  1.26s/it]avg_loss = 1.7993559122085572:   2%|▏         | 4/166 [00:06<03:23,  1.26s/it]avg_loss = 1.7993559122085572:   3%|▎         | 5/166 [00:06<03:19,  1.24s/it]avg_loss = 1.7747478882471721:   3%|▎         | 5/166 [00:07<03:19,  1.24s/it]avg_loss = 1.7747478882471721:   4%|▎         | 6/166 [00:07<03:17,  1.23s/it]avg_loss = 1.7160362005233765:   4%|▎         | 6/166 [00:08<03:17,  1.23s/it]avg_loss = 1.7160362005233765:   4%|▍         | 7/166 [00:08<03:15,  1.23s/it]avg_loss = 1.6604879051446915:   4%|▍         | 7/166 [00:10<03:15,  1.23s/it]avg_loss = 1.6604879051446915:   5%|▍         | 8/166 [00:10<03:13,  1.22s/it]avg_loss = 1.6546632713741727:   5%|▍         | 8/166 [00:11<03:13,  1.22s/it]avg_loss = 1.6546632713741727:   5%|▌         | 9/166 [00:11<03:11,  1.22s/it]avg_loss = 1.6603623390197755:   5%|▌         | 9/166 [00:12<03:11,  1.22s/it]avg_loss = 1.6603623390197755:   6%|▌         | 10/166 [00:12<03:10,  1.22s/it]avg_loss = 1.6772244626825505:   6%|▌         | 10/166 [00:13<03:10,  1.22s/it]avg_loss = 1.6772244626825505:   7%|▋         | 11/166 [00:13<03:09,  1.22s/it]avg_loss = 1.6862015624841054:   7%|▋         | 11/166 [00:14<03:09,  1.22s/it]avg_loss = 1.6862015624841054:   7%|▋         | 12/166 [00:14<03:08,  1.23s/it]avg_loss = 1.6817102157152617:   7%|▋         | 12/166 [00:16<03:08,  1.23s/it]avg_loss = 1.6817102157152617:   8%|▊         | 13/166 [00:16<03:07,  1.23s/it]avg_loss = 1.6936203241348267:   8%|▊         | 13/166 [00:17<03:07,  1.23s/it]avg_loss = 1.6936203241348267:   8%|▊         | 14/166 [00:17<03:06,  1.23s/it]avg_loss = 1.710767110188802:   8%|▊         | 14/166 [00:18<03:06,  1.23s/it] avg_loss = 1.710767110188802:   9%|▉         | 15/166 [00:18<03:05,  1.23s/it]avg_loss = 1.7296863198280334:   9%|▉         | 15/166 [00:19<03:05,  1.23s/it]avg_loss = 1.7296863198280334:  10%|▉         | 16/166 [00:19<03:04,  1.23s/it]avg_loss = 1.743026614189148:  10%|▉         | 16/166 [00:21<03:04,  1.23s/it] avg_loss = 1.743026614189148:  10%|█         | 17/166 [00:21<03:03,  1.23s/it]avg_loss = 1.7579568028450012:  10%|█         | 17/166 [00:22<03:03,  1.23s/it]avg_loss = 1.7579568028450012:  11%|█         | 18/166 [00:22<03:02,  1.24s/it]avg_loss = 1.7771962002704018:  11%|█         | 18/166 [00:23<03:02,  1.24s/it]avg_loss = 1.7771962002704018:  11%|█▏        | 19/166 [00:23<03:01,  1.24s/it]avg_loss = 1.7834509015083313:  11%|█▏        | 19/166 [00:24<03:01,  1.24s/it]avg_loss = 1.7834509015083313:  12%|█▏        | 20/166 [00:24<03:01,  1.24s/it]avg_loss = 1.7846125931966872:  12%|█▏        | 20/166 [00:26<03:01,  1.24s/it]avg_loss = 1.7846125931966872:  13%|█▎        | 21/166 [00:26<03:00,  1.24s/it]avg_loss = 1.7743418162519282:  13%|█▎        | 21/166 [00:27<03:00,  1.24s/it]avg_loss = 1.7743418162519282:  13%|█▎        | 22/166 [00:27<02:59,  1.24s/it]avg_loss = 1.7640484986097917:  13%|█▎        | 22/166 [00:28<02:59,  1.24s/it]avg_loss = 1.7640484986097917:  14%|█▍        | 23/166 [00:28<02:58,  1.25s/it]avg_loss = 1.7713235368331273:  14%|█▍        | 23/166 [00:29<02:58,  1.25s/it]avg_loss = 1.7713235368331273:  14%|█▍        | 24/166 [00:29<02:57,  1.25s/it]avg_loss = 1.778657922744751:  14%|█▍        | 24/166 [00:31<02:57,  1.25s/it] avg_loss = 1.778657922744751:  15%|█▌        | 25/166 [00:31<02:56,  1.25s/it]avg_loss = 1.7832604600833013:  15%|█▌        | 25/166 [00:32<02:56,  1.25s/it]avg_loss = 1.7832604600833013:  16%|█▌        | 26/166 [00:32<02:55,  1.25s/it]avg_loss = 1.7898784434353863:  16%|█▌        | 26/166 [00:33<02:55,  1.25s/it]avg_loss = 1.7898784434353863:  16%|█▋        | 27/166 [00:33<02:54,  1.25s/it]avg_loss = 1.793264091014862:  16%|█▋        | 27/166 [00:34<02:54,  1.25s/it] avg_loss = 1.793264091014862:  17%|█▋        | 28/166 [00:34<02:52,  1.25s/it]avg_loss = 1.802773064580457:  17%|█▋        | 28/166 [00:36<02:52,  1.25s/it]avg_loss = 1.802773064580457:  17%|█▋        | 29/166 [00:36<02:51,  1.25s/it]avg_loss = 1.8029227296511332:  17%|█▋        | 29/166 [00:37<02:51,  1.25s/it]avg_loss = 1.8029227296511332:  18%|█▊        | 30/166 [00:37<02:50,  1.26s/it]avg_loss = 1.8169449644704019:  18%|█▊        | 30/166 [00:38<02:50,  1.26s/it]avg_loss = 1.8169449644704019:  19%|█▊        | 31/166 [00:38<02:49,  1.26s/it]avg_loss = 1.8232032470405102:  19%|█▊        | 31/166 [00:39<02:49,  1.26s/it]avg_loss = 1.8232032470405102:  19%|█▉        | 32/166 [00:39<02:48,  1.26s/it]avg_loss = 1.827941526066173:  19%|█▉        | 32/166 [00:41<02:48,  1.26s/it] avg_loss = 1.827941526066173:  20%|█▉        | 33/166 [00:41<02:47,  1.26s/it]avg_loss = 1.827266019933364:  20%|█▉        | 33/166 [00:42<02:47,  1.26s/it]avg_loss = 1.827266019933364:  20%|██        | 34/166 [00:42<02:46,  1.26s/it]avg_loss = 1.8212540558406285:  20%|██        | 34/166 [00:43<02:46,  1.26s/it]avg_loss = 1.8212540558406285:  21%|██        | 35/166 [00:43<02:45,  1.26s/it]avg_loss = 1.8132462700208027:  21%|██        | 35/166 [00:45<02:45,  1.26s/it]avg_loss = 1.8132462700208027:  22%|██▏       | 36/166 [00:45<02:44,  1.27s/it]avg_loss = 1.803553581237793:  22%|██▏       | 36/166 [00:46<02:44,  1.27s/it] avg_loss = 1.803553581237793:  22%|██▏       | 37/166 [00:46<02:43,  1.27s/it]avg_loss = 1.8012697571202327:  22%|██▏       | 37/166 [00:47<02:43,  1.27s/it]avg_loss = 1.8012697571202327:  23%|██▎       | 38/166 [00:47<02:42,  1.27s/it]avg_loss = 1.7989577299509294:  23%|██▎       | 38/166 [00:48<02:42,  1.27s/it]avg_loss = 1.7989577299509294:  23%|██▎       | 39/166 [00:48<02:41,  1.27s/it]avg_loss = 1.8024226784706117:  23%|██▎       | 39/166 [00:50<02:41,  1.27s/it]avg_loss = 1.8024226784706117:  24%|██▍       | 40/166 [00:50<02:40,  1.27s/it]avg_loss = 1.8023846440198945:  24%|██▍       | 40/166 [00:51<02:40,  1.27s/it]avg_loss = 1.8023846440198945:  25%|██▍       | 41/166 [00:51<02:39,  1.27s/it]avg_loss = 1.7896985979307265:  25%|██▍       | 41/166 [00:52<02:39,  1.27s/it]avg_loss = 1.7896985979307265:  25%|██▌       | 42/166 [00:52<02:38,  1.27s/it]avg_loss = 1.7740568477053975:  25%|██▌       | 42/166 [00:53<02:38,  1.27s/it]avg_loss = 1.7740568477053975:  26%|██▌       | 43/166 [00:53<02:36,  1.28s/it]avg_loss = 1.7635742154988376:  26%|██▌       | 43/166 [00:55<02:36,  1.28s/it]avg_loss = 1.7635742154988376:  27%|██▋       | 44/166 [00:55<02:35,  1.28s/it]avg_loss = 1.7499106036292182:  27%|██▋       | 44/166 [00:56<02:35,  1.28s/it]avg_loss = 1.7499106036292182:  27%|██▋       | 45/166 [00:56<02:34,  1.28s/it]avg_loss = 1.7394100816353508:  27%|██▋       | 45/166 [00:57<02:34,  1.28s/it]avg_loss = 1.7394100816353508:  28%|██▊       | 46/166 [00:57<02:33,  1.28s/it]avg_loss = 1.732489677185708:  28%|██▊       | 46/166 [00:59<02:33,  1.28s/it] avg_loss = 1.732489677185708:  28%|██▊       | 47/166 [00:59<02:32,  1.28s/it]avg_loss = 1.7333201269308727:  28%|██▊       | 47/166 [01:00<02:32,  1.28s/it]avg_loss = 1.7333201269308727:  29%|██▉       | 48/166 [01:00<02:31,  1.28s/it]avg_loss = 1.7439792350846894:  29%|██▉       | 48/166 [01:01<02:31,  1.28s/it]avg_loss = 1.7439792350846894:  30%|██▉       | 49/166 [01:01<02:29,  1.28s/it]avg_loss = 1.754454674720764:  30%|██▉       | 49/166 [01:02<02:29,  1.28s/it] avg_loss = 1.754454674720764:  30%|███       | 50/166 [01:02<02:28,  1.28s/it]avg_loss = 1.7614059261247224:  30%|███       | 50/166 [01:04<02:28,  1.28s/it]avg_loss = 1.7614059261247224:  31%|███       | 51/166 [01:04<02:27,  1.29s/it]avg_loss = 1.7666463760229258:  31%|███       | 51/166 [01:05<02:27,  1.29s/it]avg_loss = 1.7666463760229258:  31%|███▏      | 52/166 [01:05<02:26,  1.29s/it]avg_loss = 1.7697576014500744:  31%|███▏      | 52/166 [01:06<02:26,  1.29s/it]avg_loss = 1.7697576014500744:  32%|███▏      | 53/166 [01:06<02:25,  1.29s/it]avg_loss = 1.770397667531614:  32%|███▏      | 53/166 [01:08<02:25,  1.29s/it] avg_loss = 1.770397667531614:  33%|███▎      | 54/166 [01:08<02:24,  1.29s/it]avg_loss = 1.7729372934861616:  33%|███▎      | 54/166 [01:09<02:24,  1.29s/it]avg_loss = 1.7729372934861616:  33%|███▎      | 55/166 [01:09<02:23,  1.29s/it]avg_loss = 1.776199800627572:  33%|███▎      | 55/166 [01:10<02:23,  1.29s/it] avg_loss = 1.776199800627572:  34%|███▎      | 56/166 [01:10<02:21,  1.29s/it]avg_loss = 1.7711622631340695:  34%|███▎      | 56/166 [01:11<02:21,  1.29s/it]avg_loss = 1.7711622631340695:  34%|███▍      | 57/166 [01:11<02:20,  1.29s/it]avg_loss = 1.7746213345692075:  34%|███▍      | 57/166 [01:13<02:20,  1.29s/it]avg_loss = 1.7746213345692075:  35%|███▍      | 58/166 [01:13<02:19,  1.29s/it]avg_loss = 1.7728716155229989:  35%|███▍      | 58/166 [01:14<02:19,  1.29s/it]avg_loss = 1.7728716155229989:  36%|███▌      | 59/166 [01:14<02:18,  1.29s/it]avg_loss = 1.7682433088620504:  36%|███▌      | 59/166 [01:15<02:18,  1.29s/it]avg_loss = 1.7682433088620504:  36%|███▌      | 60/166 [01:15<02:16,  1.29s/it]avg_loss = 1.763953996486351:  36%|███▌      | 60/166 [01:17<02:16,  1.29s/it] avg_loss = 1.763953996486351:  37%|███▋      | 61/166 [01:17<02:15,  1.29s/it]avg_loss = 1.7601150331958648:  37%|███▋      | 61/166 [01:18<02:15,  1.29s/it]avg_loss = 1.7601150331958648:  37%|███▋      | 62/166 [01:18<02:14,  1.29s/it]avg_loss = 1.7542632515468295:  37%|███▋      | 62/166 [01:19<02:14,  1.29s/it]avg_loss = 1.7542632515468295:  38%|███▊      | 63/166 [01:19<02:13,  1.29s/it]avg_loss = 1.7500778995454311:  38%|███▊      | 63/166 [01:20<02:13,  1.29s/it]avg_loss = 1.7500778995454311:  39%|███▊      | 64/166 [01:20<02:12,  1.30s/it]avg_loss = 1.7431900959748488:  39%|███▊      | 64/166 [01:22<02:12,  1.30s/it]avg_loss = 1.7431900959748488:  39%|███▉      | 65/166 [01:22<02:10,  1.30s/it]avg_loss = 1.73591803781914:  39%|███▉      | 65/166 [01:23<02:10,  1.30s/it]  avg_loss = 1.73591803781914:  40%|███▉      | 66/166 [01:23<02:09,  1.30s/it]avg_loss = 1.7307154100332687:  40%|███▉      | 66/166 [01:24<02:09,  1.30s/it]avg_loss = 1.7307154100332687:  40%|████      | 67/166 [01:24<02:08,  1.30s/it]avg_loss = 1.729560944963904:  40%|████      | 67/166 [01:26<02:08,  1.30s/it] avg_loss = 1.729560944963904:  41%|████      | 68/166 [01:26<02:07,  1.30s/it]avg_loss = 1.7314852631610373:  41%|████      | 68/166 [01:27<02:07,  1.30s/it]avg_loss = 1.7314852631610373:  42%|████▏     | 69/166 [01:27<02:05,  1.30s/it]avg_loss = 1.7344438058989389:  42%|████▏     | 69/166 [01:28<02:05,  1.30s/it]avg_loss = 1.7344438058989389:  42%|████▏     | 70/166 [01:28<02:04,  1.30s/it]avg_loss = 1.7383803330676657:  42%|████▏     | 70/166 [01:30<02:04,  1.30s/it]avg_loss = 1.7383803330676657:  43%|████▎     | 71/166 [01:30<02:03,  1.30s/it]avg_loss = 1.7431254933277767:  43%|████▎     | 71/166 [01:31<02:03,  1.30s/it]avg_loss = 1.7431254933277767:  43%|████▎     | 72/166 [01:31<02:02,  1.30s/it]avg_loss = 1.7491334186841363:  43%|████▎     | 72/166 [01:32<02:02,  1.30s/it]avg_loss = 1.7491334186841363:  44%|████▍     | 73/166 [01:32<02:00,  1.30s/it]avg_loss = 1.743528883199434:  44%|████▍     | 73/166 [01:33<02:00,  1.30s/it] avg_loss = 1.743528883199434:  45%|████▍     | 74/166 [01:33<01:59,  1.30s/it]avg_loss = 1.7391481177012125:  45%|████▍     | 74/166 [01:35<01:59,  1.30s/it]avg_loss = 1.7391481177012125:  45%|████▌     | 75/166 [01:35<01:58,  1.30s/it]avg_loss = 1.7383579752947156:  45%|████▌     | 75/166 [01:36<01:58,  1.30s/it]avg_loss = 1.7383579752947156:  46%|████▌     | 76/166 [01:36<01:56,  1.30s/it]avg_loss = 1.7348362718309676:  46%|████▌     | 76/166 [01:37<01:56,  1.30s/it]avg_loss = 1.7348362718309676:  46%|████▋     | 77/166 [01:37<01:55,  1.30s/it]avg_loss = 1.7313409478236468:  46%|████▋     | 77/166 [01:39<01:55,  1.30s/it]avg_loss = 1.7313409478236468:  47%|████▋     | 78/166 [01:39<01:54,  1.30s/it]avg_loss = 1.7286659116986431:  47%|████▋     | 78/166 [01:40<01:54,  1.30s/it]avg_loss = 1.7286659116986431:  48%|████▊     | 79/166 [01:40<01:53,  1.30s/it]avg_loss = 1.7252325043082237:  48%|████▊     | 79/166 [01:41<01:53,  1.30s/it]avg_loss = 1.7252325043082237:  48%|████▊     | 80/166 [01:41<01:51,  1.30s/it]avg_loss = 1.7161313435177745:  48%|████▊     | 80/166 [01:43<01:51,  1.30s/it]avg_loss = 1.7161313435177745:  49%|████▉     | 81/166 [01:43<01:50,  1.30s/it]avg_loss = 1.7177938186540835:  49%|████▉     | 81/166 [01:44<01:50,  1.30s/it]avg_loss = 1.7177938186540835:  49%|████▉     | 82/166 [01:44<01:49,  1.30s/it]avg_loss = 1.7197839689542012:  49%|████▉     | 82/166 [01:45<01:49,  1.30s/it]avg_loss = 1.7197839689542012:  50%|█████     | 83/166 [01:45<01:48,  1.30s/it]avg_loss = 1.7226887522708803:  50%|█████     | 83/166 [01:47<01:48,  1.30s/it]avg_loss = 1.7226887522708803:  51%|█████     | 84/166 [01:47<01:46,  1.30s/it]avg_loss = 1.7243762626367456:  51%|█████     | 84/166 [01:48<01:46,  1.30s/it]avg_loss = 1.7243762626367456:  51%|█████     | 85/166 [01:48<01:45,  1.30s/it]avg_loss = 1.723315404598103:  51%|█████     | 85/166 [01:49<01:45,  1.30s/it] avg_loss = 1.723315404598103:  52%|█████▏    | 86/166 [01:49<01:44,  1.30s/it]avg_loss = 1.7235991865739055:  52%|█████▏    | 86/166 [01:50<01:44,  1.30s/it]avg_loss = 1.7235991865739055:  52%|█████▏    | 87/166 [01:50<01:42,  1.30s/it]avg_loss = 1.7237292805855924:  52%|█████▏    | 87/166 [01:52<01:42,  1.30s/it]avg_loss = 1.7237292805855924:  53%|█████▎    | 88/166 [01:52<01:41,  1.31s/it]avg_loss = 1.7249329270941487:  53%|█████▎    | 88/166 [01:53<01:41,  1.31s/it]avg_loss = 1.7249329270941487:  54%|█████▎    | 89/166 [01:53<01:40,  1.31s/it]avg_loss = 1.7247146043512556:  54%|█████▎    | 89/166 [01:54<01:40,  1.31s/it]avg_loss = 1.7247146043512556:  54%|█████▍    | 90/166 [01:54<01:39,  1.30s/it]avg_loss = 1.7251096184437091:  54%|█████▍    | 90/166 [01:56<01:39,  1.30s/it]avg_loss = 1.7251096184437091:  55%|█████▍    | 91/166 [01:56<01:37,  1.31s/it]avg_loss = 1.7261058461406957:  55%|█████▍    | 91/166 [01:57<01:37,  1.31s/it]avg_loss = 1.7261058461406957:  55%|█████▌    | 92/166 [01:57<01:36,  1.31s/it]avg_loss = 1.7300641594394561:  55%|█████▌    | 92/166 [01:58<01:36,  1.31s/it]avg_loss = 1.7300641594394561:  56%|█████▌    | 93/166 [01:58<01:35,  1.31s/it]avg_loss = 1.7290861676348017:  56%|█████▌    | 93/166 [02:00<01:35,  1.31s/it]avg_loss = 1.7290861676348017:  57%|█████▋    | 94/166 [02:00<01:34,  1.31s/it]avg_loss = 1.728369845214643:  57%|█████▋    | 94/166 [02:01<01:34,  1.31s/it] avg_loss = 1.728369845214643:  57%|█████▋    | 95/166 [02:01<01:32,  1.31s/it]avg_loss = 1.7280115615576506:  57%|█████▋    | 95/166 [02:02<01:32,  1.31s/it]avg_loss = 1.7280115615576506:  58%|█████▊    | 96/166 [02:02<01:31,  1.31s/it]avg_loss = 1.728006421290722:  58%|█████▊    | 96/166 [02:04<01:31,  1.31s/it] avg_loss = 1.728006421290722:  58%|█████▊    | 97/166 [02:04<01:30,  1.31s/it]avg_loss = 1.7262230497233721:  58%|█████▊    | 97/166 [02:05<01:30,  1.31s/it]avg_loss = 1.7262230497233721:  59%|█████▉    | 98/166 [02:05<01:29,  1.31s/it]avg_loss = 1.7237160115530996:  59%|█████▉    | 98/166 [02:06<01:29,  1.31s/it]avg_loss = 1.7237160115530996:  60%|█████▉    | 99/166 [02:06<01:27,  1.31s/it]avg_loss = 1.7210625749826431:  60%|█████▉    | 99/166 [02:07<01:27,  1.31s/it]avg_loss = 1.7210625749826431:  60%|██████    | 100/166 [02:07<01:26,  1.31s/it]avg_loss = 1.7214895610762115:  60%|██████    | 100/166 [02:09<01:26,  1.31s/it]avg_loss = 1.7214895610762115:  61%|██████    | 101/166 [02:09<01:25,  1.31s/it]avg_loss = 1.7223992014632505:  61%|██████    | 101/166 [02:10<01:25,  1.31s/it]avg_loss = 1.7223992014632505:  61%|██████▏   | 102/166 [02:10<01:23,  1.31s/it]avg_loss = 1.7235349422519646:  61%|██████▏   | 102/166 [02:11<01:23,  1.31s/it]avg_loss = 1.7235349422519646:  62%|██████▏   | 103/166 [02:11<01:22,  1.31s/it]avg_loss = 1.7256741277300394:  62%|██████▏   | 103/166 [02:13<01:22,  1.31s/it]avg_loss = 1.7256741277300394:  63%|██████▎   | 104/166 [02:13<01:21,  1.31s/it]avg_loss = 1.732349714211055:  63%|██████▎   | 104/166 [02:14<01:21,  1.31s/it] avg_loss = 1.732349714211055:  63%|██████▎   | 105/166 [02:14<01:20,  1.31s/it]avg_loss = 1.737478438975676:  63%|██████▎   | 105/166 [02:15<01:20,  1.31s/it]avg_loss = 1.737478438975676:  64%|██████▍   | 106/166 [02:15<01:18,  1.31s/it]avg_loss = 1.7409913701431774:  64%|██████▍   | 106/166 [02:17<01:18,  1.31s/it]avg_loss = 1.7409913701431774:  64%|██████▍   | 107/166 [02:17<01:17,  1.31s/it]avg_loss = 1.7441490562977615:  64%|██████▍   | 107/166 [02:18<01:17,  1.31s/it]avg_loss = 1.7441490562977615:  65%|██████▌   | 108/166 [02:18<01:16,  1.31s/it]avg_loss = 1.7488880359798396:  65%|██████▌   | 108/166 [02:19<01:16,  1.31s/it]avg_loss = 1.7488880359798396:  66%|██████▌   | 109/166 [02:19<01:14,  1.31s/it]avg_loss = 1.7523285459388387:  66%|██████▌   | 109/166 [02:21<01:14,  1.31s/it]avg_loss = 1.7523285459388387:  66%|██████▋   | 110/166 [02:21<01:13,  1.31s/it]avg_loss = 1.7538576217385027:  66%|██████▋   | 110/166 [02:22<01:13,  1.31s/it]avg_loss = 1.7538576217385027:  67%|██████▋   | 111/166 [02:22<01:12,  1.31s/it]avg_loss = 1.7550488534782613:  67%|██████▋   | 111/166 [02:23<01:12,  1.31s/it]avg_loss = 1.7550488534782613:  67%|██████▋   | 112/166 [02:23<01:10,  1.31s/it]avg_loss = 1.755291227218324:  67%|██████▋   | 112/166 [02:25<01:10,  1.31s/it] avg_loss = 1.755291227218324:  68%|██████▊   | 113/166 [02:25<01:09,  1.31s/it]avg_loss = 1.7566641302485215:  68%|██████▊   | 113/166 [02:26<01:09,  1.31s/it]avg_loss = 1.7566641302485215:  69%|██████▊   | 114/166 [02:26<01:08,  1.31s/it]avg_loss = 1.7539934142776157:  69%|██████▊   | 114/166 [02:27<01:08,  1.31s/it]avg_loss = 1.7539934142776157:  69%|██████▉   | 115/166 [02:27<01:06,  1.31s/it]avg_loss = 1.7533099276238475:  69%|██████▉   | 115/166 [02:28<01:06,  1.31s/it]avg_loss = 1.7533099276238475:  70%|██████▉   | 116/166 [02:28<01:05,  1.31s/it]avg_loss = 1.7542243874990022:  70%|██████▉   | 116/166 [02:30<01:05,  1.31s/it]avg_loss = 1.7542243874990022:  70%|███████   | 117/166 [02:30<01:04,  1.31s/it]avg_loss = 1.7544349053148496:  70%|███████   | 117/166 [02:31<01:04,  1.31s/it]avg_loss = 1.7544349053148496:  71%|███████   | 118/166 [02:31<01:03,  1.31s/it]avg_loss = 1.7538631959121769:  71%|███████   | 118/166 [02:32<01:03,  1.31s/it]avg_loss = 1.7538631959121769:  72%|███████▏  | 119/166 [02:32<01:01,  1.31s/it]avg_loss = 1.7544888203342757:  72%|███████▏  | 119/166 [02:34<01:01,  1.31s/it]avg_loss = 1.7544888203342757:  72%|███████▏  | 120/166 [02:34<01:00,  1.31s/it]avg_loss = 1.7540221386704564:  72%|███████▏  | 120/166 [02:35<01:00,  1.31s/it]avg_loss = 1.7540221386704564:  73%|███████▎  | 121/166 [02:35<00:59,  1.31s/it]avg_loss = 1.7545598535264124:  73%|███████▎  | 121/166 [02:36<00:59,  1.31s/it]avg_loss = 1.7545598535264124:  73%|███████▎  | 122/166 [02:36<00:57,  1.32s/it]avg_loss = 1.7547671373297529:  73%|███████▎  | 122/166 [02:38<00:57,  1.32s/it]avg_loss = 1.7547671373297529:  74%|███████▍  | 123/166 [02:38<00:56,  1.31s/it]avg_loss = 1.7534047610336734:  74%|███████▍  | 123/166 [02:39<00:56,  1.31s/it]avg_loss = 1.7534047610336734:  75%|███████▍  | 124/166 [02:39<00:55,  1.32s/it]avg_loss = 1.751767553806305:  75%|███████▍  | 124/166 [02:40<00:55,  1.32s/it] avg_loss = 1.751767553806305:  75%|███████▌  | 125/166 [02:40<00:53,  1.31s/it]avg_loss = 1.7496255593640464:  75%|███████▌  | 125/166 [02:42<00:53,  1.31s/it]avg_loss = 1.7496255593640464:  76%|███████▌  | 126/166 [02:42<00:52,  1.32s/it]avg_loss = 1.7474458597776459:  76%|███████▌  | 126/166 [02:43<00:52,  1.32s/it]avg_loss = 1.7474458597776459:  77%|███████▋  | 127/166 [02:43<00:51,  1.31s/it]avg_loss = 1.7460813685320318:  77%|███████▋  | 127/166 [02:44<00:51,  1.31s/it]avg_loss = 1.7460813685320318:  77%|███████▋  | 128/166 [02:44<00:49,  1.32s/it]avg_loss = 1.744843761588252:  77%|███████▋  | 128/166 [02:46<00:49,  1.32s/it] avg_loss = 1.744843761588252:  78%|███████▊  | 129/166 [02:46<00:48,  1.32s/it]avg_loss = 1.7448423637793615:  78%|███████▊  | 129/166 [02:47<00:48,  1.32s/it]avg_loss = 1.7448423637793615:  78%|███████▊  | 130/166 [02:47<00:47,  1.32s/it]avg_loss = 1.7459065800404732:  78%|███████▊  | 130/166 [02:48<00:47,  1.32s/it]avg_loss = 1.7459065800404732:  79%|███████▉  | 131/166 [02:48<00:46,  1.32s/it]avg_loss = 1.7464560491569114:  79%|███████▉  | 131/166 [02:49<00:46,  1.32s/it]avg_loss = 1.7464560491569114:  80%|███████▉  | 132/166 [02:49<00:44,  1.32s/it]avg_loss = 1.7474431081822044:  80%|███████▉  | 132/166 [02:51<00:44,  1.32s/it]avg_loss = 1.7474431081822044:  80%|████████  | 133/166 [02:51<00:43,  1.32s/it]avg_loss = 1.7487859748192687:  80%|████████  | 133/166 [02:52<00:43,  1.32s/it]avg_loss = 1.7487859748192687:  81%|████████  | 134/166 [02:52<00:42,  1.32s/it]avg_loss = 1.7466979066530863:  81%|████████  | 134/166 [02:53<00:42,  1.32s/it]avg_loss = 1.7466979066530863:  81%|████████▏ | 135/166 [02:53<00:40,  1.32s/it]avg_loss = 1.746883019366685:  81%|████████▏ | 135/166 [02:55<00:40,  1.32s/it] avg_loss = 1.746883019366685:  82%|████████▏ | 136/166 [02:55<00:39,  1.32s/it]avg_loss = 1.747186253105637:  82%|████████▏ | 136/166 [02:56<00:39,  1.32s/it]avg_loss = 1.747186253105637:  83%|████████▎ | 137/166 [02:56<00:38,  1.32s/it]avg_loss = 1.7480140635068866:  83%|████████▎ | 137/166 [02:57<00:38,  1.32s/it]avg_loss = 1.7480140635068866:  83%|████████▎ | 138/166 [02:57<00:36,  1.32s/it]avg_loss = 1.7471118912422399:  83%|████████▎ | 138/166 [02:59<00:36,  1.32s/it]avg_loss = 1.7471118912422399:  84%|████████▎ | 139/166 [02:59<00:35,  1.32s/it]avg_loss = 1.7457756540605:  84%|████████▎ | 139/166 [03:00<00:35,  1.32s/it]   avg_loss = 1.7457756540605:  84%|████████▍ | 140/166 [03:00<00:34,  1.32s/it]avg_loss = 1.7443604372071881:  84%|████████▍ | 140/166 [03:01<00:34,  1.32s/it]avg_loss = 1.7443604372071881:  85%|████████▍ | 141/166 [03:01<00:32,  1.32s/it]avg_loss = 1.7439428337023293:  85%|████████▍ | 141/166 [03:03<00:32,  1.32s/it]avg_loss = 1.7439428337023293:  86%|████████▌ | 142/166 [03:03<00:31,  1.32s/it]avg_loss = 1.7423064204362722:  86%|████████▌ | 142/166 [03:04<00:31,  1.32s/it]avg_loss = 1.7423064204362722:  86%|████████▌ | 143/166 [03:04<00:30,  1.32s/it]avg_loss = 1.7434657915598817:  86%|████████▌ | 143/166 [03:05<00:30,  1.32s/it]avg_loss = 1.7434657915598817:  87%|████████▋ | 144/166 [03:05<00:29,  1.32s/it]avg_loss = 1.7426915312635487:  87%|████████▋ | 144/166 [03:07<00:29,  1.32s/it]avg_loss = 1.7426915312635487:  87%|████████▋ | 145/166 [03:07<00:27,  1.32s/it]avg_loss = 1.7425225324010196:  87%|████████▋ | 145/166 [03:08<00:27,  1.32s/it]avg_loss = 1.7425225324010196:  88%|████████▊ | 146/166 [03:08<00:26,  1.32s/it]avg_loss = 1.741348931173078:  88%|████████▊ | 146/166 [03:09<00:26,  1.32s/it] avg_loss = 1.741348931173078:  89%|████████▊ | 147/166 [03:09<00:25,  1.32s/it]avg_loss = 1.740437304248681:  89%|████████▊ | 147/166 [03:11<00:25,  1.32s/it]avg_loss = 1.740437304248681:  89%|████████▉ | 148/166 [03:11<00:23,  1.32s/it]avg_loss = 1.738668943811583:  89%|████████▉ | 148/166 [03:12<00:23,  1.32s/it]avg_loss = 1.738668943811583:  90%|████████▉ | 149/166 [03:12<00:22,  1.32s/it]avg_loss = 1.7396040093898772:  90%|████████▉ | 149/166 [03:13<00:22,  1.32s/it]avg_loss = 1.7396040093898772:  90%|█████████ | 150/166 [03:13<00:21,  1.32s/it]avg_loss = 1.7387632751306952:  90%|█████████ | 150/166 [03:15<00:21,  1.32s/it]avg_loss = 1.7387632751306952:  91%|█████████ | 151/166 [03:15<00:19,  1.32s/it]avg_loss = 1.738560470703401:  91%|█████████ | 151/166 [03:16<00:19,  1.32s/it] avg_loss = 1.738560470703401:  92%|█████████▏| 152/166 [03:16<00:18,  1.32s/it]avg_loss = 1.7383326740046732:  92%|█████████▏| 152/166 [03:17<00:18,  1.32s/it]avg_loss = 1.7383326740046732:  92%|█████████▏| 153/166 [03:17<00:17,  1.32s/it]avg_loss = 1.7399540261014717:  92%|█████████▏| 153/166 [03:19<00:17,  1.32s/it]avg_loss = 1.7399540261014717:  93%|█████████▎| 154/166 [03:19<00:15,  1.32s/it]avg_loss = 1.7394829784670183:  93%|█████████▎| 154/166 [03:20<00:15,  1.32s/it]avg_loss = 1.7394829784670183:  93%|█████████▎| 155/166 [03:20<00:14,  1.32s/it]avg_loss = 1.7393112385120146:  93%|█████████▎| 155/166 [03:21<00:14,  1.32s/it]avg_loss = 1.7393112385120146:  94%|█████████▍| 156/166 [03:21<00:13,  1.32s/it]avg_loss = 1.7374699992738711:  94%|█████████▍| 156/166 [03:22<00:13,  1.32s/it]avg_loss = 1.7374699992738711:  95%|█████████▍| 157/166 [03:22<00:11,  1.32s/it]avg_loss = 1.7331485804877704:  95%|█████████▍| 157/166 [03:24<00:11,  1.32s/it]avg_loss = 1.7331485804877704:  95%|█████████▌| 158/166 [03:24<00:10,  1.32s/it]avg_loss = 1.7339121661845993:  95%|█████████▌| 158/166 [03:25<00:10,  1.32s/it]avg_loss = 1.7339121661845993:  96%|█████████▌| 159/166 [03:25<00:09,  1.32s/it]avg_loss = 1.735304044559598:  96%|█████████▌| 159/166 [03:26<00:09,  1.32s/it] avg_loss = 1.735304044559598:  96%|█████████▋| 160/166 [03:26<00:07,  1.32s/it]avg_loss = 1.7376387552444978:  96%|█████████▋| 160/166 [03:28<00:07,  1.32s/it]avg_loss = 1.7376387552444978:  97%|█████████▋| 161/166 [03:28<00:06,  1.32s/it]avg_loss = 1.7377784999064456:  97%|█████████▋| 161/166 [03:29<00:06,  1.32s/it]avg_loss = 1.7377784999064456:  98%|█████████▊| 162/166 [03:29<00:05,  1.32s/it]avg_loss = 1.7374076422738152:  98%|█████████▊| 162/166 [03:30<00:05,  1.32s/it]avg_loss = 1.7374076422738152:  98%|█████████▊| 163/166 [03:30<00:03,  1.32s/it]avg_loss = 1.7380194260579784:  98%|█████████▊| 163/166 [03:32<00:03,  1.32s/it]avg_loss = 1.7380194260579784:  99%|█████████▉| 164/166 [03:32<00:02,  1.32s/it]avg_loss = 1.7381758144407562:  99%|█████████▉| 164/166 [03:33<00:02,  1.32s/it]avg_loss = 1.7381758144407562:  99%|█████████▉| 165/166 [03:33<00:01,  1.32s/it]avg_loss = 1.7401241053299732:  99%|█████████▉| 165/166 [03:34<00:01,  1.32s/it]avg_loss = 1.7401241053299732: 100%|██████████| 166/166 [03:34<00:00,  1.32s/it]avg_loss = 1.7401241053299732: 100%|██████████| 166/166 [03:34<00:00,  1.29s/it]
I0402 14:34:42.639623 3229075 eval_ppl.py:107] wikitext2 perplexity: 5.698050498962402
wikitext2 perplexity: 5.698
