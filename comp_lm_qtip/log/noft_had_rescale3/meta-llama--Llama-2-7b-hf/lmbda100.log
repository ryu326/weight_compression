I0402 14:34:50.913706 3229179 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:34:50.913795 3229179 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:34:50.913833 3229179 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:34:51.232959 3229179 config.py:54] PyTorch version 2.6.0 available.
W0402 14:34:51.420807 3229179 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:34:52.050469 3229179 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  6.94it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.44it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.76it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.89it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  7.99it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.14it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.91it/s]
I0402 14:34:53.009340 3229179 quantize_finetune_llama.py:152] loaded model
I0402 14:34:53.393902 3229179 quantize_finetune_llama.py:190] loaded compression model
I0402 14:35:07.390910 3229179 quantize_finetune_llama.py:194] loaded dataset and devset
I0402 14:35:12.313558 3229179 quantize_finetune_llama.py:214] layer 0 gpu 0
I0402 14:35:14.795354 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 0 in 2.3096249103546143s
Use train scale and shift
tensor(2.2655e-07, device='cuda:0') tensor(0.0204, device='cuda:0')
tensor(0.0204, device='cuda:0') tensor(2.2655e-07, device='cuda:0')
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0402 14:35:26.985912 3229307 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:35:26.986000 3229307 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:35:26.986038 3229307 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:35:27.306879 3229307 config.py:54] PyTorch version 2.6.0 available.
W0402 14:35:27.494616 3229307 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:35:28.032529 3229307 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:35:28.036277 3229179 quantize_finetune_llama.py:214] layer 1 gpu 1
I0402 14:35:28.326930 3229307 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:35:30.547185 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 1 in 2.349637985229492s
I0402 14:35:34.227100 3229375 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:35:34.227196 3229375 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:35:34.227236 3229375 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:35:34.569708 3229375 config.py:54] PyTorch version 2.6.0 available.
W0402 14:35:34.782275 3229375 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:35:35.394286 3229375 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:35:35.398254 3229179 quantize_finetune_llama.py:214] layer 2 gpu 2
I0402 14:35:35.553858 3229375 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:35:37.889467 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 2 in 2.323256015777588s
I0402 14:35:41.742194 3229445 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:35:41.742303 3229445 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:35:41.742344 3229445 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:35:42.097429 3229445 config.py:54] PyTorch version 2.6.0 available.
W0402 14:35:42.331503 3229445 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:35:43.008441 3229445 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:35:43.012505 3229179 quantize_finetune_llama.py:214] layer 3 gpu 3
I0402 14:35:43.646494 3229445 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:35:45.822353 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 3 in 2.6351258754730225s
I0402 14:35:49.986260 3229515 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:35:49.986399 3229515 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:35:49.986442 3229515 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:35:50.382979 3229515 config.py:54] PyTorch version 2.6.0 available.
W0402 14:35:50.612148 3229515 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:35:51.315997 3229515 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:35:51.320561 3229179 quantize_finetune_llama.py:214] layer 4 gpu 0
I0402 14:35:51.879175 3229515 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_v proxy err 0.001693235826678574 tr(WHW.T) 971.8771362304688
bpp_loss 3.7097408771514893
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_q proxy err 8.256808359874412e-05 tr(WHW.T) 636425.375
bpp_loss 3.700573205947876
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_k proxy err 9.087290527531877e-05 tr(WHW.T) 398864.15625
bpp_loss 3.7990760803222656
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_o proxy err 0.00042753503657877445 tr(WHW.T) 15925.943359375
bpp_loss 3.547104835510254
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
0_up proxy err 0.003919536247849464 tr(WHW.T) 24117.18359375
bpp_loss 3.587505429290062
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
0_gate proxy err 0.00272839842364192 tr(WHW.T) 35427.40234375
bpp_loss 3.601860756097838
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
0_down proxy err 0.0019350239308550954 tr(WHW.T) 35796.33984375
bpp_loss 3.7924160402874616
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_v proxy err 0.00595423299819231 tr(WHW.T) 657.0241088867188
bpp_loss 3.655567765235901
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_q proxy err 0.00012789637548848987 tr(WHW.T) 195417.453125
bpp_loss 4.549970388412476
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_k proxy err 0.00012375530786812305 tr(WHW.T) 204295.5625
bpp_loss 4.5441272258758545
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_o proxy err 0.003020094009116292 tr(WHW.T) 4039.630859375
bpp_loss 3.525558590888977
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
1_up proxy err 0.0054254475980997086 tr(WHW.T) 23209.44921875
bpp_loss 3.6106161073196765
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
1_gate proxy err 0.002751826075837016 tr(WHW.T) 46954.0546875
bpp_loss 3.675045279569404
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
1_down proxy err 5.046987280366011e-05 tr(WHW.T) 40779.421875
bpp_loss 3.9670856387116187
I0402 14:36:33.382247 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 4 in 0.9532244205474854s
I0402 14:36:37.312768 3229583 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:36:37.312867 3229583 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:36:37.312908 3229583 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:36:37.653667 3229583 config.py:54] PyTorch version 2.6.0 available.
W0402 14:36:37.879932 3229583 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:36:38.498988 3229583 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:36:38.502971 3229179 quantize_finetune_llama.py:214] layer 5 gpu 1
I0402 14:36:38.946279 3229583 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_v proxy err 0.006120787002146244 tr(WHW.T) 2779.86376953125
bpp_loss 3.727813482284546
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_q proxy err 0.00017796871543396264 tr(WHW.T) 159508.203125
bpp_loss 4.526697874069214
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_k proxy err 0.00015052258095238358 tr(WHW.T) 210000.984375
bpp_loss 4.635161399841309
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_o proxy err 0.005678797140717506 tr(WHW.T) 5300.58984375
bpp_loss 3.595335364341736
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
2_up proxy err 0.006696783471852541 tr(WHW.T) 19933.5703125
bpp_loss 3.620072653127271
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
2_gate proxy err 0.004280506167560816 tr(WHW.T) 31660.716796875
bpp_loss 3.699474955714026
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
2_down proxy err 0.006624025758355856 tr(WHW.T) 17283.607421875
bpp_loss 3.6921889726505723
I0402 14:36:42.163804 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 5 in 0.9461495876312256s
I0402 14:36:46.037317 3229649 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:36:46.037422 3229649 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:36:46.037465 3229649 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:36:46.422745 3229649 config.py:54] PyTorch version 2.6.0 available.
W0402 14:36:46.646585 3229649 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:36:47.278679 3229649 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:36:47.282498 3229179 quantize_finetune_llama.py:214] layer 6 gpu 2
I0402 14:36:47.529208 3229649 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_v proxy err 0.008738046512007713 tr(WHW.T) 2978.910400390625
bpp_loss 3.6144392490386963
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_q proxy err 0.00042949747876264155 tr(WHW.T) 76174.0546875
bpp_loss 4.344726324081421
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_k proxy err 0.0003264763508923352 tr(WHW.T) 106360.5859375
bpp_loss 4.422906398773193
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_o proxy err 0.005071775987744331 tr(WHW.T) 5255.58154296875
bpp_loss 3.595747947692871
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
3_up proxy err 0.007526641245931387 tr(WHW.T) 17467.81640625
bpp_loss 3.6302041342092113
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
3_gate proxy err 0.0045574200339615345 tr(WHW.T) 29396.990234375
bpp_loss 3.7192237321720567
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
3_down proxy err 0.006635706406086683 tr(WHW.T) 16907.59375
bpp_loss 3.702711349309877
I0402 14:36:49.795653 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 6 in 0.9983267784118652s
I0402 14:36:53.572510 3229717 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:36:53.572629 3229717 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:36:53.572683 3229717 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:36:53.940507 3229717 config.py:54] PyTorch version 2.6.0 available.
W0402 14:36:54.157679 3229717 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:36:54.835169 3229717 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:36:54.839112 3229179 quantize_finetune_llama.py:214] layer 7 gpu 3
I0402 14:36:55.063866 3229717 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:36:56.222688 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 7 in 0.9205794334411621s
I0402 14:37:00.185396 3229785 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:37:00.185499 3229785 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:37:00.185545 3229785 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:37:00.554281 3229785 config.py:54] PyTorch version 2.6.0 available.
W0402 14:37:00.764066 3229785 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:37:01.391522 3229785 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:37:01.395672 3229179 quantize_finetune_llama.py:214] layer 8 gpu 0
I0402 14:37:01.657830 3229785 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_v proxy err 0.008034690283238888 tr(WHW.T) 3097.555419921875
bpp_loss 3.661249876022339
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_q proxy err 0.00039811129681766033 tr(WHW.T) 78746.46875
bpp_loss 4.4433979988098145
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_k proxy err 0.00028864751220680773 tr(WHW.T) 118660.75
bpp_loss 4.481977701187134
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_o proxy err 0.006246283184736967 tr(WHW.T) 5336.9765625
bpp_loss 3.573747158050537
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
4_up proxy err 0.007178436033427715 tr(WHW.T) 17664.685546875
bpp_loss 3.6280008360396985
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
4_gate proxy err 0.0035689324140548706 tr(WHW.T) 36588.140625
bpp_loss 3.750232075535974
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
4_down proxy err 0.00661882571876049 tr(WHW.T) 16830.9296875
bpp_loss 3.6924691976502886
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_v proxy err 0.008364742621779442 tr(WHW.T) 3166.572021484375
bpp_loss 3.666827440261841
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_q proxy err 0.00044755253475159407 tr(WHW.T) 72535.90625
bpp_loss 4.443850994110107
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_k proxy err 0.00030780976521782577 tr(WHW.T) 116207.296875
bpp_loss 4.5264973640441895
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_o proxy err 0.00713751558214426 tr(WHW.T) 3769.5478515625
bpp_loss 3.647276759147644
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
5_up proxy err 0.007107825018465519 tr(WHW.T) 18016.64453125
bpp_loss 3.6265427345453305
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
5_gate proxy err 0.00334566505625844 tr(WHW.T) 39417.078125
bpp_loss 3.7566255081531614
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
5_down proxy err 0.007251827511936426 tr(WHW.T) 15953.3203125
bpp_loss 3.679413684578829
I0402 14:37:45.082309 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 8 in 0.955507755279541s
I0402 14:37:49.065365 3229853 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:37:49.065479 3229853 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:37:49.065521 3229853 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:37:49.459185 3229853 config.py:54] PyTorch version 2.6.0 available.
W0402 14:37:49.685236 3229853 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:37:50.375813 3229853 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:37:50.379859 3229179 quantize_finetune_llama.py:214] layer 9 gpu 1
I0402 14:37:50.607237 3229853 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_v proxy err 0.00904811080545187 tr(WHW.T) 3180.833740234375
bpp_loss 3.5934990644454956
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_q proxy err 0.000626911292783916 tr(WHW.T) 54760.6953125
bpp_loss 4.301964282989502
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_k proxy err 0.0004752882232423872 tr(WHW.T) 75270.1171875
bpp_loss 4.33995795249939
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_o proxy err 0.007364899385720491 tr(WHW.T) 4041.38525390625
bpp_loss 3.576078176498413
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
6_up proxy err 0.007087125908583403 tr(WHW.T) 17971.6953125
bpp_loss 3.623615353606468
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
6_gate proxy err 0.002905606059357524 tr(WHW.T) 45430.91015625
bpp_loss 3.779686506404433
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
6_down proxy err 0.0074135796166956425 tr(WHW.T) 15434.75390625
bpp_loss 3.6773592039596203
I0402 14:37:53.632606 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 9 in 0.951667308807373s
I0402 14:37:57.569997 3229919 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:37:57.570101 3229919 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:37:57.570144 3229919 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:37:57.948851 3229919 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_v proxy err 0.008991335518658161 tr(WHW.T) 3250.746826171875
bpp_loss 3.5980461835861206
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_q proxy err 0.0006747555453330278 tr(WHW.T) 51300.6015625
bpp_loss 4.295262098312378
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_k proxy err 0.0005257643642835319 tr(WHW.T) 68225.015625
bpp_loss 4.302759408950806
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_o proxy err 0.008206319995224476 tr(WHW.T) 3528.39208984375
bpp_loss 3.588091015815735
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
7_up proxy err 0.006837897934019566 tr(WHW.T) 18230.072265625
bpp_loss 3.630671124125636
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
7_gate proxy err 0.0027778621297329664 tr(WHW.T) 46648.5078125
bpp_loss 3.7818536093068675
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
7_down proxy err 0.00755078811198473 tr(WHW.T) 15240.3759765625
bpp_loss 3.6770145948543105
W0402 14:37:58.170064 3229919 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:37:58.834978 3229919 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:37:58.838843 3229179 quantize_finetune_llama.py:214] layer 10 gpu 2
I0402 14:37:59.021232 3229919 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:38:00.112180 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 10 in 0.8174946308135986s
I0402 14:38:04.018171 3229987 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:38:04.018280 3229987 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:38:04.018319 3229987 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:38:04.359078 3229987 config.py:54] PyTorch version 2.6.0 available.
W0402 14:38:04.566165 3229987 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:38:05.176457 3229987 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:38:05.180640 3229179 quantize_finetune_llama.py:214] layer 11 gpu 3
I0402 14:38:05.348060 3229987 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:38:06.844237 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 11 in 1.1829578876495361s
I0402 14:38:10.933060 3230055 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:38:10.933188 3230055 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:38:10.933236 3230055 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:38:11.339983 3230055 config.py:54] PyTorch version 2.6.0 available.
W0402 14:38:11.580987 3230055 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:38:12.256669 3230055 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:38:12.261001 3229179 quantize_finetune_llama.py:214] layer 12 gpu 0
I0402 14:38:12.577871 3230055 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_v proxy err 0.008169375360012054 tr(WHW.T) 3470.401611328125
bpp_loss 3.6245731115341187
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_q proxy err 0.0006901785964146256 tr(WHW.T) 47592.55859375
bpp_loss 4.326882600784302
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_k proxy err 0.0004923168453387916 tr(WHW.T) 70099.9296875
bpp_loss 4.335008382797241
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_o proxy err 0.008983473293483257 tr(WHW.T) 3118.695556640625
bpp_loss 3.6159054040908813
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
8_up proxy err 0.0062067024409770966 tr(WHW.T) 19852.556640625
bpp_loss 3.645949075388354
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
8_gate proxy err 0.0028226503636687994 tr(WHW.T) 45311.83203125
bpp_loss 3.762263985567315
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
8_down proxy err 0.00756125058978796 tr(WHW.T) 15322.3779296875
bpp_loss 3.685777242793593
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_v proxy err 0.008155071176588535 tr(WHW.T) 3676.465087890625
bpp_loss 3.6250436305999756
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_q proxy err 0.0007450972916558385 tr(WHW.T) 45676.421875
bpp_loss 4.319205045700073
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_k proxy err 0.0005031723994761705 tr(WHW.T) 72055.03125
bpp_loss 4.364166021347046
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_o proxy err 0.00955642107874155 tr(WHW.T) 3148.60498046875
bpp_loss 3.613551139831543
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
9_up proxy err 0.005956617649644613 tr(WHW.T) 20605.642578125
bpp_loss 3.6540445726971296
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
9_gate proxy err 0.0028021540492773056 tr(WHW.T) 45406.61328125
bpp_loss 3.7502499957417332
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
9_down proxy err 0.0076865553855896 tr(WHW.T) 15324.0654296875
bpp_loss 3.687468417855196
I0402 14:38:57.229619 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 12 in 0.9196746349334717s
I0402 14:39:01.253924 3230123 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:39:01.254029 3230123 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:39:01.254072 3230123 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:39:01.649935 3230123 config.py:54] PyTorch version 2.6.0 available.
W0402 14:39:01.866788 3230123 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_v proxy err 0.008182377554476261 tr(WHW.T) 3651.62158203125
bpp_loss 3.6184805631637573
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_q proxy err 0.0007663531578145921 tr(WHW.T) 43935.53515625
bpp_loss 4.318160533905029
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_k proxy err 0.0005155936814844608 tr(WHW.T) 69912.859375
bpp_loss 4.373512506484985
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_o proxy err 0.010322223417460918 tr(WHW.T) 3058.283935546875
bpp_loss 3.597678542137146
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
10_up proxy err 0.005610890686511993 tr(WHW.T) 21904.43359375
bpp_loss 3.6658018245253454
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
10_gate proxy err 0.002766672521829605 tr(WHW.T) 45983.0625
bpp_loss 3.746359714241915
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
10_down proxy err 0.0072423480451107025 tr(WHW.T) 16111.1357421875
bpp_loss 3.7003200220507244
W0402 14:39:02.494428 3230123 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:39:02.498404 3229179 quantize_finetune_llama.py:214] layer 13 gpu 1
I0402 14:39:02.691891 3230123 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:39:04.334715 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 13 in 0.914074182510376s
I0402 14:39:08.298552 3230189 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:39:08.298652 3230189 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:39:08.298693 3230189 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:39:08.653638 3230189 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_v proxy err 0.008163771592080593 tr(WHW.T) 3890.81982421875
bpp_loss 3.634653329849243
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_q proxy err 0.0009303411934524775 tr(WHW.T) 38073.125
bpp_loss 4.204818964004517
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_k proxy err 0.0006477236747741699 tr(WHW.T) 56997.671875
bpp_loss 4.19495415687561
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_o proxy err 0.010560842230916023 tr(WHW.T) 3054.2177734375
bpp_loss 3.6226553916931152
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
11_up proxy err 0.005767535418272018 tr(WHW.T) 21548.3046875
bpp_loss 3.6739308556845023
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
11_gate proxy err 0.002832188503816724 tr(WHW.T) 45386.7265625
bpp_loss 3.739940554596657
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
11_down proxy err 0.007575002033263445 tr(WHW.T) 15730.783203125
bpp_loss 3.699757686881132
W0402 14:39:08.867470 3230189 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:39:09.626420 3230189 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:39:09.630392 3229179 quantize_finetune_llama.py:214] layer 14 gpu 2
I0402 14:39:09.949633 3230189 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:39:11.034419 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 14 in 0.9271457195281982s
I0402 14:39:15.003219 3230257 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:39:15.003306 3230257 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:39:15.003343 3230257 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:39:15.341801 3230257 config.py:54] PyTorch version 2.6.0 available.
W0402 14:39:15.532277 3230257 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:39:16.113422 3230257 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:39:16.117149 3229179 quantize_finetune_llama.py:214] layer 15 gpu 3
I0402 14:39:16.372648 3230257 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:39:17.573380 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 15 in 0.984907865524292s
I0402 14:39:21.627699 3230325 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:39:21.627813 3230325 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:39:21.627875 3230325 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:39:22.017756 3230325 config.py:54] PyTorch version 2.6.0 available.
W0402 14:39:22.238837 3230325 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:39:22.879946 3230325 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:39:22.883952 3229179 quantize_finetune_llama.py:214] layer 16 gpu 0
I0402 14:39:23.049558 3230325 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_v proxy err 0.008440960198640823 tr(WHW.T) 3807.353515625
bpp_loss 3.621417760848999
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_q proxy err 0.0009289112640544772 tr(WHW.T) 38406.0
bpp_loss 4.2409117221832275
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_k proxy err 0.0006319489912129939 tr(WHW.T) 59429.25
bpp_loss 4.295023202896118
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_o proxy err 0.011014861054718494 tr(WHW.T) 2997.118896484375
bpp_loss 3.607205390930176
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
12_up proxy err 0.0057397931814193726 tr(WHW.T) 21806.251953125
bpp_loss 3.683520472326944
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
12_gate proxy err 0.003041706280782819 tr(WHW.T) 42415.66015625
bpp_loss 3.7310822952625364
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
12_down proxy err 0.007495115045458078 tr(WHW.T) 15783.7470703125
bpp_loss 3.712369497432265
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_v proxy err 0.008705303072929382 tr(WHW.T) 3883.96826171875
bpp_loss 3.635472536087036
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_q proxy err 0.0009835215751081705 tr(WHW.T) 38091.8046875
bpp_loss 4.209425926208496
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_k proxy err 0.0006853897357359529 tr(WHW.T) 57141.765625
bpp_loss 4.238711595535278
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_o proxy err 0.009938386268913746 tr(WHW.T) 3392.266845703125
bpp_loss 3.6280637979507446
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
13_up proxy err 0.0055129812099039555 tr(WHW.T) 22688.076171875
bpp_loss 3.695659726165062
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
13_gate proxy err 0.002976104384288192 tr(WHW.T) 43286.52734375
bpp_loss 3.7268203025640445
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
13_down proxy err 0.0073104253970086575 tr(WHW.T) 15750.5673828125
bpp_loss 3.731195649435354
I0402 14:40:07.522077 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 16 in 0.9223964214324951s
I0402 14:40:11.424053 3230393 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:40:11.424151 3230393 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:40:11.424192 3230393 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:40:11.808929 3230393 config.py:54] PyTorch version 2.6.0 available.
W0402 14:40:12.032100 3230393 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_v proxy err 0.009231739677488804 tr(WHW.T) 3647.569580078125
bpp_loss 3.6213732957839966
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_q proxy err 0.001007142011076212 tr(WHW.T) 36835.87109375
bpp_loss 4.206510305404663
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_k proxy err 0.0006656762561760843 tr(WHW.T) 58844.5546875
bpp_loss 4.235136985778809
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_o proxy err 0.01133944932371378 tr(WHW.T) 3063.885498046875
bpp_loss 3.602959156036377
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
14_up proxy err 0.00564394798129797 tr(WHW.T) 22452.671875
bpp_loss 3.6948877378951672
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
14_gate proxy err 0.00315693742595613 tr(WHW.T) 41240.93359375
bpp_loss 3.722340783407522
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
14_down proxy err 0.007441171910613775 tr(WHW.T) 15409.48828125
bpp_loss 3.7339239564052846
W0402 14:40:12.638073 3230393 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:40:12.642058 3229179 quantize_finetune_llama.py:214] layer 17 gpu 1
I0402 14:40:12.809170 3230393 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:40:14.929458 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 17 in 1.1445763111114502s
I0402 14:40:18.861637 3230459 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:40:18.861739 3230459 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:40:18.861779 3230459 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:40:19.260574 3230459 config.py:54] PyTorch version 2.6.0 available.
W0402 14:40:19.471784 3230459 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_v proxy err 0.008249780163168907 tr(WHW.T) 4003.05126953125
bpp_loss 3.660256505012512
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_q proxy err 0.0009502822649665177 tr(WHW.T) 38353.34765625
bpp_loss 4.18962836265564
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_k proxy err 0.0006544085335917771 tr(WHW.T) 58600.94921875
bpp_loss 4.247640609741211
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_o proxy err 0.009750701487064362 tr(WHW.T) 3628.3759765625
bpp_loss 3.6279661655426025
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
15_up proxy err 0.0054826634004712105 tr(WHW.T) 23066.35546875
bpp_loss 3.7018611819245093
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
15_gate proxy err 0.003166450886055827 tr(WHW.T) 40946.2890625
bpp_loss 3.729723021041515
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
15_down proxy err 0.0072395033203065395 tr(WHW.T) 15429.0400390625
bpp_loss 3.747397555861362
W0402 14:40:20.088272 3230459 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:40:20.092315 3229179 quantize_finetune_llama.py:214] layer 18 gpu 2
I0402 14:40:20.296329 3230459 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:40:21.665584 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 18 in 0.8468878269195557s
I0402 14:40:25.619018 3230527 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:40:25.619103 3230527 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:40:25.619140 3230527 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:40:25.935632 3230527 config.py:54] PyTorch version 2.6.0 available.
W0402 14:40:26.150773 3230527 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:40:26.764559 3230527 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:40:26.768460 3229179 quantize_finetune_llama.py:214] layer 19 gpu 3
I0402 14:40:26.945064 3230527 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:40:28.312766 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 19 in 1.045344352722168s
I0402 14:40:32.325352 3230595 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:40:32.325463 3230595 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:40:32.325509 3230595 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:40:32.725013 3230595 config.py:54] PyTorch version 2.6.0 available.
W0402 14:40:32.949998 3230595 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:40:33.634628 3230595 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:40:33.638995 3229179 quantize_finetune_llama.py:214] layer 20 gpu 0
I0402 14:40:33.989396 3230595 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_v proxy err 0.008656393736600876 tr(WHW.T) 3988.751708984375
bpp_loss 3.6841976642608643
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_q proxy err 0.0010208392050117254 tr(WHW.T) 37064.6328125
bpp_loss 4.164309740066528
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_k proxy err 0.000665520376060158 tr(WHW.T) 59978.5
bpp_loss 4.206232786178589
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_o proxy err 0.008327513001859188 tr(WHW.T) 4695.22509765625
bpp_loss 3.635616660118103
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
16_up proxy err 0.005495195277035236 tr(WHW.T) 23603.60546875
bpp_loss 3.6971378769985463
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
16_gate proxy err 0.003150075441226363 tr(WHW.T) 42174.73828125
bpp_loss 3.7349372686341753
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
16_down proxy err 0.007360812276601791 tr(WHW.T) 15228.4970703125
bpp_loss 3.7454931126084436
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_v proxy err 0.008633073419332504 tr(WHW.T) 4263.80712890625
bpp_loss 3.6615631580352783
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_q proxy err 0.0011046465951949358 tr(WHW.T) 36392.74609375
bpp_loss 4.131662607192993
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_k proxy err 0.0007722616428509355 tr(WHW.T) 54388.6953125
bpp_loss 4.165876150131226
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_o proxy err 0.008954633958637714 tr(WHW.T) 4297.923828125
bpp_loss 3.6416646242141724
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
17_up proxy err 0.0061152298003435135 tr(WHW.T) 21615.3203125
bpp_loss 3.689490562261537
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
17_gate proxy err 0.0033507051412016153 tr(WHW.T) 40300.40234375
bpp_loss 3.7440591856490735
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
17_down proxy err 0.007538324221968651 tr(WHW.T) 15353.4287109375
bpp_loss 3.732543989669445
I0402 14:41:18.312652 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 20 in 0.9544727802276611s
I0402 14:41:22.333438 3230663 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:41:22.333543 3230663 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:41:22.333584 3230663 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:41:22.724422 3230663 config.py:54] PyTorch version 2.6.0 available.
W0402 14:41:22.936228 3230663 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_v proxy err 0.00821631494909525 tr(WHW.T) 4673.08837890625
bpp_loss 3.6929500102996826
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_q proxy err 0.0011773839360103011 tr(WHW.T) 35237.796875
bpp_loss 4.080315351486206
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_k proxy err 0.0008704978972673416 tr(WHW.T) 49119.20703125
bpp_loss 4.114444255828857
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_o proxy err 0.007666351739317179 tr(WHW.T) 4923.25390625
bpp_loss 3.6847177743911743
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
18_up proxy err 0.006544828414916992 tr(WHW.T) 20299.947265625
bpp_loss 3.685160614723383
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
18_gate proxy err 0.0035604198928922415 tr(WHW.T) 38072.26171875
bpp_loss 3.7551500187363733
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
18_down proxy err 0.007367471698671579 tr(WHW.T) 15266.072265625
bpp_loss 3.741484708564226
W0402 14:41:23.522994 3230663 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:41:23.526861 3229179 quantize_finetune_llama.py:214] layer 21 gpu 1
I0402 14:41:23.682414 3230663 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:41:25.641779 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 21 in 0.9197912216186523s
I0402 14:41:29.610245 3230729 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:41:29.610349 3230729 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:41:29.610391 3230729 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:41:29.980262 3230729 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_v proxy err 0.008091381751000881 tr(WHW.T) 4788.90576171875
bpp_loss 3.6989821195602417
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_q proxy err 0.0012699252692982554 tr(WHW.T) 32879.76953125
bpp_loss 4.056572437286377
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_k proxy err 0.0008700286853127182 tr(WHW.T) 49984.76171875
bpp_loss 4.083506107330322
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_o proxy err 0.00795661099255085 tr(WHW.T) 5005.189453125
bpp_loss 3.679247498512268
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
19_up proxy err 0.00660299276933074 tr(WHW.T) 20172.62109375
bpp_loss 3.6845795387445492
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
19_gate proxy err 0.00391008798032999 tr(WHW.T) 34698.6953125
bpp_loss 3.759586600370185
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
19_down proxy err 0.007191748823970556 tr(WHW.T) 15697.3017578125
bpp_loss 3.7428719498390377
W0402 14:41:30.192945 3230729 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:41:30.841851 3230729 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:41:30.845788 3229179 quantize_finetune_llama.py:214] layer 22 gpu 2
I0402 14:41:31.267511 3230729 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:41:32.213742 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 22 in 0.8882551193237305s
I0402 14:41:36.068719 3230797 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:41:36.068818 3230797 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:41:36.068859 3230797 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:41:36.405542 3230797 config.py:54] PyTorch version 2.6.0 available.
W0402 14:41:36.611686 3230797 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:41:37.201805 3230797 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:41:37.206061 3229179 quantize_finetune_llama.py:214] layer 23 gpu 3
I0402 14:41:37.476043 3230797 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:41:38.869500 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 23 in 1.1980619430541992s
I0402 14:41:42.798949 3230865 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:41:42.799053 3230865 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:41:42.799096 3230865 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:41:43.186662 3230865 config.py:54] PyTorch version 2.6.0 available.
W0402 14:41:43.408343 3230865 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:41:44.047879 3230865 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:41:44.052192 3229179 quantize_finetune_llama.py:214] layer 24 gpu 0
I0402 14:41:44.242793 3230865 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_v proxy err 0.008478746749460697 tr(WHW.T) 4645.8291015625
bpp_loss 3.7090909481048584
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_q proxy err 0.0012612289283424616 tr(WHW.T) 33815.2578125
bpp_loss 4.061246395111084
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_k proxy err 0.0008978248806670308 tr(WHW.T) 49149.9921875
bpp_loss 4.086984634399414
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_o proxy err 0.005768653471022844 tr(WHW.T) 6820.419921875
bpp_loss 3.7042139768600464
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
20_up proxy err 0.0065038870088756084 tr(WHW.T) 20603.083984375
bpp_loss 3.682919524436773
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
20_gate proxy err 0.003833151189610362 tr(WHW.T) 35509.98046875
bpp_loss 3.7662962092909704
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
20_down proxy err 0.007034846115857363 tr(WHW.T) 15803.095703125
bpp_loss 3.7484063658603404
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_v proxy err 0.008320020511746407 tr(WHW.T) 4862.75634765625
bpp_loss 3.733772873878479
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_q proxy err 0.0014304178766906261 tr(WHW.T) 30261.54296875
bpp_loss 4.0164854526519775
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_k proxy err 0.0010411784751340747 tr(WHW.T) 42768.43359375
bpp_loss 4.0296361446380615
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_o proxy err 0.006827849894762039 tr(WHW.T) 6399.25439453125
bpp_loss 3.690325975418091
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
21_up proxy err 0.006860719993710518 tr(WHW.T) 19562.884765625
bpp_loss 3.680630883505178
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
21_gate proxy err 0.004097566939890385 tr(WHW.T) 33251.44921875
bpp_loss 3.775273700093114
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
21_down proxy err 0.00718527939170599 tr(WHW.T) 15792.2314453125
bpp_loss 3.74080968457599
I0402 14:42:28.982384 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 24 in 0.9860310554504395s
I0402 14:42:32.928319 3230933 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:42:32.928422 3230933 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:42:32.928463 3230933 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:42:33.309256 3230933 config.py:54] PyTorch version 2.6.0 available.
W0402 14:42:33.526132 3230933 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_v proxy err 0.007851905189454556 tr(WHW.T) 5111.93212890625
bpp_loss 3.740551710128784
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_q proxy err 0.0013390129897743464 tr(WHW.T) 32103.412109375
bpp_loss 4.054039239883423
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_k proxy err 0.0010059855412691832 tr(WHW.T) 43969.734375
bpp_loss 4.071466684341431
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_o proxy err 0.005318836309015751 tr(WHW.T) 7627.48828125
bpp_loss 3.720674157142639
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
22_up proxy err 0.006924418732523918 tr(WHW.T) 19439.994140625
bpp_loss 3.6795634779819224
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
22_gate proxy err 0.004170595668256283 tr(WHW.T) 32731.19921875
bpp_loss 3.7828739964684774
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
22_down proxy err 0.007228257600218058 tr(WHW.T) 15874.603515625
bpp_loss 3.7373064839562704
W0402 14:42:34.153001 3230933 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:42:34.156874 3229179 quantize_finetune_llama.py:214] layer 25 gpu 1
I0402 14:42:34.359689 3230933 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:42:35.789510 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 25 in 0.8759090900421143s
I0402 14:42:39.662754 3230999 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:42:39.662861 3230999 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:42:39.662907 3230999 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:42:40.018391 3230999 config.py:54] PyTorch version 2.6.0 available.
W0402 14:42:40.228896 3230999 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_v proxy err 0.00741726765409112 tr(WHW.T) 5666.529296875
bpp_loss 3.7789417505264282
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_q proxy err 0.0015812444034963846 tr(WHW.T) 28221.720703125
bpp_loss 4.02826189994812
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_k proxy err 0.001189686357975006 tr(WHW.T) 38375.3828125
bpp_loss 4.038307428359985
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_o proxy err 0.006601111032068729 tr(WHW.T) 6345.388671875
bpp_loss 3.7654629945755005
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
23_up proxy err 0.00717095285654068 tr(WHW.T) 18753.931640625
bpp_loss 3.6855238093886267
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
23_gate proxy err 0.004473205655813217 tr(WHW.T) 30400.564453125
bpp_loss 3.7828546568404797
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
23_down proxy err 0.007263774052262306 tr(WHW.T) 15816.7724609375
bpp_loss 3.7438676745392554
W0402 14:42:40.828536 3230999 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:42:40.832273 3229179 quantize_finetune_llama.py:214] layer 26 gpu 2
I0402 14:42:41.120016 3230999 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:42:42.402011 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 26 in 0.872856855392456s
I0402 14:42:46.319861 3231067 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:42:46.319964 3231067 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:42:46.320007 3231067 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:42:46.693784 3231067 config.py:54] PyTorch version 2.6.0 available.
W0402 14:42:46.904749 3231067 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:42:47.501768 3231067 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:42:47.505520 3229179 quantize_finetune_llama.py:214] layer 27 gpu 3
I0402 14:42:47.682448 3231067 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:42:48.931546 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 27 in 0.9745528697967529s
I0402 14:42:52.859695 3231135 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:42:52.859801 3231135 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:42:52.859893 3231135 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:42:53.239244 3231135 config.py:54] PyTorch version 2.6.0 available.
W0402 14:42:53.480648 3231135 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:42:54.208876 3231135 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:42:54.212650 3229179 quantize_finetune_llama.py:214] layer 28 gpu 0
I0402 14:42:54.494759 3231135 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_v proxy err 0.007618045900017023 tr(WHW.T) 5324.72998046875
bpp_loss 3.7802207469940186
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_q proxy err 0.0015896436525508761 tr(WHW.T) 27011.373046875
bpp_loss 3.9935988187789917
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_k proxy err 0.0011163713643327355 tr(WHW.T) 39756.4296875
bpp_loss 3.9968162775039673
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_o proxy err 0.0051954882219433784 tr(WHW.T) 8038.615234375
bpp_loss 3.7502281665802
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
24_up proxy err 0.00727864122018218 tr(WHW.T) 18512.99609375
bpp_loss 3.6888825172601742
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
24_gate proxy err 0.0045188539661467075 tr(WHW.T) 30171.666015625
bpp_loss 3.7858295884243276
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
24_down proxy err 0.007248252164572477 tr(WHW.T) 15756.2470703125
bpp_loss 3.7503458954567135
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_v proxy err 0.0072670793160796165 tr(WHW.T) 5927.5078125
bpp_loss 3.8054044246673584
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_q proxy err 0.001809047651477158 tr(WHW.T) 25041.619140625
bpp_loss 3.9875305891036987
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_k proxy err 0.0013712455984205008 tr(WHW.T) 33632.96875
bpp_loss 3.9901342391967773
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_o proxy err 0.006717414129525423 tr(WHW.T) 6779.06689453125
bpp_loss 3.7675633430480957
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
25_up proxy err 0.007201113738119602 tr(WHW.T) 18624.029296875
bpp_loss 3.694805766260901
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
25_gate proxy err 0.00437531154602766 tr(WHW.T) 31053.6875
bpp_loss 3.7891561818677326
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
25_down proxy err 0.006862069945782423 tr(WHW.T) 15877.4140625
bpp_loss 3.772764582966649
I0402 14:43:39.573170 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 28 in 0.8840837478637695s
I0402 14:43:43.431904 3231203 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:43:43.432004 3231203 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:43:43.432049 3231203 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:43:43.812333 3231203 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_v proxy err 0.007083388976752758 tr(WHW.T) 5920.73828125
bpp_loss 3.8361440896987915
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_q proxy err 0.0016605930868536234 tr(WHW.T) 26711.5390625
bpp_loss 3.970595598220825
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_k proxy err 0.0012141187908127904 tr(WHW.T) 37533.6015625
bpp_loss 3.9789289236068726
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_o proxy err 0.0039056260138750076 tr(WHW.T) 9866.05859375
bpp_loss 3.8569819927215576
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
26_up proxy err 0.0067706359550356865 tr(WHW.T) 19856.3125
bpp_loss 3.6989793999250544
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
26_gate proxy err 0.004067108500748873 tr(WHW.T) 33454.33203125
bpp_loss 3.7933484454487645
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
26_down proxy err 0.006879225838929415 tr(WHW.T) 15425.537109375
bpp_loss 3.78537448616915
W0402 14:43:44.032245 3231203 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:43:44.660088 3231203 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:43:44.664146 3229179 quantize_finetune_llama.py:214] layer 29 gpu 1
I0402 14:43:44.882406 3231203 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:43:46.281742 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 29 in 1.069580316543579s
I0402 14:43:50.333690 3231269 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:43:50.333784 3231269 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:43:50.333825 3231269 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:43:50.689774 3231269 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_v proxy err 0.0068616075441241264 tr(WHW.T) 6537.79541015625
bpp_loss 3.8141943216323853
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_q proxy err 0.001681370660662651 tr(WHW.T) 28139.56640625
bpp_loss 4.007456064224243
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_k proxy err 0.0012341320980340242 tr(WHW.T) 38878.3359375
bpp_loss 4.021972179412842
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_o proxy err 0.0051594870164990425 tr(WHW.T) 7266.2978515625
bpp_loss 3.874127745628357
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
27_up proxy err 0.006151629611849785 tr(WHW.T) 21823.326171875
bpp_loss 3.7055988755337026
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
27_gate proxy err 0.00383000448346138 tr(WHW.T) 35491.0
bpp_loss 3.795248874398165
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
27_down proxy err 0.006579331588000059 tr(WHW.T) 15119.01171875
bpp_loss 3.8128437219664106
W0402 14:43:50.900767 3231269 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:43:51.492172 3231269 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:43:51.496167 3229179 quantize_finetune_llama.py:214] layer 30 gpu 2
I0402 14:43:51.718317 3231269 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:43:52.896959 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 30 in 0.9365684986114502s
I0402 14:43:56.779438 3231337 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:43:56.779546 3231337 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:43:56.779588 3231337 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:43:57.150957 3231337 config.py:54] PyTorch version 2.6.0 available.
W0402 14:43:57.356679 3231337 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:43:57.970063 3231337 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:43:57.973843 3229179 quantize_finetune_llama.py:214] layer 31 gpu 3
I0402 14:43:58.299906 3231337 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:43:59.378387 3229179 quantize_finetune_llama.py:245] computed original embedding for layer 31 in 0.9533250331878662s
I0402 14:44:03.366846 3231405 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:44:03.366946 3231405 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:44:03.366989 3231405 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:44:03.739712 3231405 config.py:54] PyTorch version 2.6.0 available.
W0402 14:44:03.974179 3231405 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:44:04.590501 3231405 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:44:04.824514 3231405 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_v proxy err 0.006364352069795132 tr(WHW.T) 7077.5078125
bpp_loss 3.856689453125
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_q proxy err 0.0017556622624397278 tr(WHW.T) 27002.65625
bpp_loss 3.959159731864929
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_k proxy err 0.0012943369802087545 tr(WHW.T) 37243.53125
bpp_loss 3.976539134979248
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_o proxy err 0.004380227066576481 tr(WHW.T) 8884.01171875
bpp_loss 3.9018694162368774
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
28_up proxy err 0.005118692293763161 tr(WHW.T) 26161.169921875
bpp_loss 3.7184077418127726
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
28_gate proxy err 0.0036802683025598526 tr(WHW.T) 36794.0625
bpp_loss 3.789921605309775
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
28_down proxy err 0.006008700467646122 tr(WHW.T) 14984.466796875
bpp_loss 3.849110714224882
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_v proxy err 0.0066510033793747425 tr(WHW.T) 6682.36328125
bpp_loss 3.8671154975891113
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_q proxy err 0.0017395366448909044 tr(WHW.T) 27006.056640625
bpp_loss 3.9215890169143677
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_k proxy err 0.0012167497770860791 tr(WHW.T) 39489.33984375
bpp_loss 3.9312063455581665
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_o proxy err 0.004163549747318029 tr(WHW.T) 10599.048828125
bpp_loss 3.876127600669861
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
29_up proxy err 0.004080773796886206 tr(WHW.T) 32884.11328125
bpp_loss 3.7302826282589936
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
29_gate proxy err 0.0033853817731142044 tr(WHW.T) 39942.984375
bpp_loss 3.7938908421716024
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
29_down proxy err 0.005467424169182777 tr(WHW.T) 14732.544921875
bpp_loss 3.8826083693393443
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_v proxy err 0.0057200901210308075 tr(WHW.T) 8207.525390625
bpp_loss 3.8810805082321167
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_q proxy err 0.0017302811611443758 tr(WHW.T) 28540.68359375
bpp_loss 3.912229061126709
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_k proxy err 0.0013077559415251017 tr(WHW.T) 38445.2421875
bpp_loss 3.9316511154174805
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_o proxy err 0.003768948372453451 tr(WHW.T) 10163.1123046875
bpp_loss 3.9598931074142456
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
30_up proxy err 0.002519673900678754 tr(WHW.T) 53873.95703125
bpp_loss 3.751476376555687
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
30_gate proxy err 0.0023104443680495024 tr(WHW.T) 59167.79296875
bpp_loss 3.825829261957213
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
30_down proxy err 0.001900072442367673 tr(WHW.T) 25842.498046875
bpp_loss 3.9622967520425485
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_v proxy err 0.0068998681381344795 tr(WHW.T) 6740.33837890625
bpp_loss 3.755069375038147
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_q proxy err 0.0013684816658496857 tr(WHW.T) 36698.0
bpp_loss 3.927793025970459
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_k proxy err 0.0009456839761696756 tr(WHW.T) 54793.80859375
bpp_loss 3.98345148563385
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_o proxy err 0.0023068850859999657 tr(WHW.T) 13121.5947265625
bpp_loss 3.906403064727783
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
31_up proxy err 0.001440908876247704 tr(WHW.T) 95778.3046875
bpp_loss 3.803802490234375
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
31_gate proxy err 0.0014200200093910098 tr(WHW.T) 97550.4453125
bpp_loss 3.8909559028093206
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
31_down proxy err 0.0009166894014924765 tr(WHW.T) 37026.6875
bpp_loss 4.019963619320891
I0402 14:45:11.068276 3231473 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:45:11.068429 3231473 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:45:11.068472 3231473 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:45:11.523182 3231473 config.py:54] PyTorch version 2.6.0 available.
W0402 14:45:11.734704 3231473 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0402 14:45:11.846159 3231473 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  3.75it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  4.94it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  5.63it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  6.10it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  6.37it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.62it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  6.00it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  4.34it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  4.73it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  5.39it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  5.66it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  5.85it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.85it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  5.56it/s]
I0402 14:45:15.098727 3231473 hfize_llama.py:153] loaded layer 0
I0402 14:45:15.934329 3231473 hfize_llama.py:153] loaded layer 1
I0402 14:45:16.788727 3231473 hfize_llama.py:153] loaded layer 2
I0402 14:45:17.659891 3231473 hfize_llama.py:153] loaded layer 3
I0402 14:45:18.501440 3231473 hfize_llama.py:153] loaded layer 4
I0402 14:45:19.341849 3231473 hfize_llama.py:153] loaded layer 5
I0402 14:45:20.213543 3231473 hfize_llama.py:153] loaded layer 6
I0402 14:45:21.094563 3231473 hfize_llama.py:153] loaded layer 7
I0402 14:45:21.982877 3231473 hfize_llama.py:153] loaded layer 8
I0402 14:45:22.846680 3231473 hfize_llama.py:153] loaded layer 9
I0402 14:45:23.705032 3231473 hfize_llama.py:153] loaded layer 10
I0402 14:45:24.578305 3231473 hfize_llama.py:153] loaded layer 11
I0402 14:45:25.446836 3231473 hfize_llama.py:153] loaded layer 12
I0402 14:45:26.316976 3231473 hfize_llama.py:153] loaded layer 13
I0402 14:45:27.177692 3231473 hfize_llama.py:153] loaded layer 14
I0402 14:45:28.071291 3231473 hfize_llama.py:153] loaded layer 15
I0402 14:45:28.916608 3231473 hfize_llama.py:153] loaded layer 16
I0402 14:45:29.770336 3231473 hfize_llama.py:153] loaded layer 17
I0402 14:45:30.634784 3231473 hfize_llama.py:153] loaded layer 18
I0402 14:45:31.505769 3231473 hfize_llama.py:153] loaded layer 19
I0402 14:45:32.376041 3231473 hfize_llama.py:153] loaded layer 20
I0402 14:45:33.228749 3231473 hfize_llama.py:153] loaded layer 21
I0402 14:45:34.089081 3231473 hfize_llama.py:153] loaded layer 22
I0402 14:45:34.967112 3231473 hfize_llama.py:153] loaded layer 23
I0402 14:45:35.844619 3231473 hfize_llama.py:153] loaded layer 24
I0402 14:45:36.712962 3231473 hfize_llama.py:153] loaded layer 25
I0402 14:45:37.588204 3231473 hfize_llama.py:153] loaded layer 26
I0402 14:45:38.468072 3231473 hfize_llama.py:153] loaded layer 27
I0402 14:45:39.333533 3231473 hfize_llama.py:153] loaded layer 28
I0402 14:45:40.212761 3231473 hfize_llama.py:153] loaded layer 29
I0402 14:45:41.106168 3231473 hfize_llama.py:153] loaded layer 30
I0402 14:45:41.996317 3231473 hfize_llama.py:153] loaded layer 31
I0402 14:45:41.996432 3231473 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.11s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.09it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.18it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.20it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.27it/s]
I0402 14:46:16.190058 3231473 hfize_llama.py:167] successfully loaded hfized model
I0402 14:46:20.603595 3231691 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:46:20.603750 3231691 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:46:20.603795 3231691 utils.py:162] NumExpr defaulting to 16 threads.
W0402 14:46:21.037746 3231691 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0402 14:46:21.502402 3231691 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.10s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:05,  1.30s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.13s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.04s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.03it/s]
I0402 14:46:27.419572 3231691 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/166 [00:00<?, ?it/s]avg_loss = 1.37944757938385:   0%|          | 0/166 [00:01<?, ?it/s]avg_loss = 1.37944757938385:   1%|          | 1/166 [00:01<04:30,  1.64s/it]avg_loss = 1.644959807395935:   1%|          | 1/166 [00:02<04:30,  1.64s/it]avg_loss = 1.644959807395935:   1%|          | 2/166 [00:02<03:46,  1.38s/it]avg_loss = 1.810960054397583:   1%|          | 2/166 [00:04<03:46,  1.38s/it]avg_loss = 1.810960054397583:   2%|▏         | 3/166 [00:04<03:32,  1.30s/it]avg_loss = 1.8424903452396393:   2%|▏         | 3/166 [00:05<03:32,  1.30s/it]avg_loss = 1.8424903452396393:   2%|▏         | 4/166 [00:05<03:25,  1.27s/it]avg_loss = 1.7751236438751221:   2%|▏         | 4/166 [00:06<03:25,  1.27s/it]avg_loss = 1.7751236438751221:   3%|▎         | 5/166 [00:06<03:20,  1.25s/it]avg_loss = 1.7511454621950786:   3%|▎         | 5/166 [00:07<03:20,  1.25s/it]avg_loss = 1.7511454621950786:   4%|▎         | 6/166 [00:07<03:17,  1.24s/it]avg_loss = 1.6916204690933228:   4%|▎         | 6/166 [00:08<03:17,  1.24s/it]avg_loss = 1.6916204690933228:   4%|▍         | 7/166 [00:08<03:15,  1.23s/it]avg_loss = 1.6350022554397583:   4%|▍         | 7/166 [00:10<03:15,  1.23s/it]avg_loss = 1.6350022554397583:   5%|▍         | 8/166 [00:10<03:13,  1.23s/it]avg_loss = 1.6298036177953084:   5%|▍         | 8/166 [00:11<03:13,  1.23s/it]avg_loss = 1.6298036177953084:   5%|▌         | 9/166 [00:11<03:12,  1.23s/it]avg_loss = 1.6358470797538758:   5%|▌         | 9/166 [00:12<03:12,  1.23s/it]avg_loss = 1.6358470797538758:   6%|▌         | 10/166 [00:12<03:11,  1.23s/it]avg_loss = 1.6526910933581265:   6%|▌         | 10/166 [00:13<03:11,  1.23s/it]avg_loss = 1.6526910933581265:   7%|▋         | 11/166 [00:13<03:09,  1.23s/it]avg_loss = 1.6622690359751384:   7%|▋         | 11/166 [00:15<03:09,  1.23s/it]avg_loss = 1.6622690359751384:   7%|▋         | 12/166 [00:15<03:08,  1.23s/it]avg_loss = 1.658031362753648:   7%|▋         | 12/166 [00:16<03:08,  1.23s/it] avg_loss = 1.658031362753648:   8%|▊         | 13/166 [00:16<03:07,  1.23s/it]avg_loss = 1.6704732945987157:   8%|▊         | 13/166 [00:17<03:07,  1.23s/it]avg_loss = 1.6704732945987157:   8%|▊         | 14/166 [00:17<03:06,  1.23s/it]avg_loss = 1.6879464070002237:   8%|▊         | 14/166 [00:18<03:06,  1.23s/it]avg_loss = 1.6879464070002237:   9%|▉         | 15/166 [00:18<03:06,  1.23s/it]avg_loss = 1.7073025032877922:   9%|▉         | 15/166 [00:19<03:06,  1.23s/it]avg_loss = 1.7073025032877922:  10%|▉         | 16/166 [00:19<03:05,  1.23s/it]avg_loss = 1.720851133851444:  10%|▉         | 16/166 [00:21<03:05,  1.23s/it] avg_loss = 1.720851133851444:  10%|█         | 17/166 [00:21<03:04,  1.24s/it]avg_loss = 1.7358645333184137:  10%|█         | 17/166 [00:22<03:04,  1.24s/it]avg_loss = 1.7358645333184137:  11%|█         | 18/166 [00:22<03:03,  1.24s/it]avg_loss = 1.7554806282645778:  11%|█         | 18/166 [00:23<03:03,  1.24s/it]avg_loss = 1.7554806282645778:  11%|█▏        | 19/166 [00:23<03:02,  1.24s/it]avg_loss = 1.7617598950862885:  11%|█▏        | 19/166 [00:24<03:02,  1.24s/it]avg_loss = 1.7617598950862885:  12%|█▏        | 20/166 [00:24<03:01,  1.24s/it]avg_loss = 1.762716327394758:  12%|█▏        | 20/166 [00:26<03:01,  1.24s/it] avg_loss = 1.762716327394758:  13%|█▎        | 21/166 [00:26<03:00,  1.24s/it]avg_loss = 1.7527602965181523:  13%|█▎        | 21/166 [00:27<03:00,  1.24s/it]avg_loss = 1.7527602965181523:  13%|█▎        | 22/166 [00:27<02:59,  1.25s/it]avg_loss = 1.7420773713485054:  13%|█▎        | 22/166 [00:28<02:59,  1.25s/it]avg_loss = 1.7420773713485054:  14%|█▍        | 23/166 [00:28<02:58,  1.25s/it]avg_loss = 1.7494674424330394:  14%|█▍        | 23/166 [00:29<02:58,  1.25s/it]avg_loss = 1.7494674424330394:  14%|█▍        | 24/166 [00:29<02:57,  1.25s/it]avg_loss = 1.7568629360198975:  14%|█▍        | 24/166 [00:31<02:57,  1.25s/it]avg_loss = 1.7568629360198975:  15%|█▌        | 25/166 [00:31<02:56,  1.25s/it]avg_loss = 1.7615684225009038:  15%|█▌        | 25/166 [00:32<02:56,  1.25s/it]avg_loss = 1.7615684225009038:  16%|█▌        | 26/166 [00:32<02:55,  1.25s/it]avg_loss = 1.76828196755162:  16%|█▌        | 26/166 [00:33<02:55,  1.25s/it]  avg_loss = 1.76828196755162:  16%|█▋        | 27/166 [00:33<02:54,  1.26s/it]avg_loss = 1.771583957331521:  16%|█▋        | 27/166 [00:34<02:54,  1.26s/it]avg_loss = 1.771583957331521:  17%|█▋        | 28/166 [00:34<02:53,  1.26s/it]avg_loss = 1.7812071093197526:  17%|█▋        | 28/166 [00:36<02:53,  1.26s/it]avg_loss = 1.7812071093197526:  17%|█▋        | 29/166 [00:36<02:52,  1.26s/it]avg_loss = 1.78129327694575:  17%|█▋        | 29/166 [00:37<02:52,  1.26s/it]  avg_loss = 1.78129327694575:  18%|█▊        | 30/166 [00:37<02:51,  1.26s/it]avg_loss = 1.7953993235864947:  18%|█▊        | 30/166 [00:38<02:51,  1.26s/it]avg_loss = 1.7953993235864947:  19%|█▊        | 31/166 [00:38<02:50,  1.26s/it]avg_loss = 1.8018131293356419:  19%|█▊        | 31/166 [00:40<02:50,  1.26s/it]avg_loss = 1.8018131293356419:  19%|█▉        | 32/166 [00:40<02:49,  1.26s/it]avg_loss = 1.8066494212006077:  19%|█▉        | 32/166 [00:41<02:49,  1.26s/it]avg_loss = 1.8066494212006077:  20%|█▉        | 33/166 [00:41<02:48,  1.26s/it]avg_loss = 1.8058958299019758:  20%|█▉        | 33/166 [00:42<02:48,  1.26s/it]avg_loss = 1.8058958299019758:  20%|██        | 34/166 [00:42<02:47,  1.27s/it]avg_loss = 1.7997426407677786:  20%|██        | 34/166 [00:43<02:47,  1.27s/it]avg_loss = 1.7997426407677786:  21%|██        | 35/166 [00:43<02:45,  1.27s/it]avg_loss = 1.7914152575863733:  21%|██        | 35/166 [00:45<02:45,  1.27s/it]avg_loss = 1.7914152575863733:  22%|██▏       | 36/166 [00:45<02:44,  1.27s/it]avg_loss = 1.7814804186692108:  22%|██▏       | 36/166 [00:46<02:44,  1.27s/it]avg_loss = 1.7814804186692108:  22%|██▏       | 37/166 [00:46<02:43,  1.27s/it]avg_loss = 1.779012105966869:  22%|██▏       | 37/166 [00:47<02:43,  1.27s/it] avg_loss = 1.779012105966869:  23%|██▎       | 38/166 [00:47<02:42,  1.27s/it]avg_loss = 1.7768186025130444:  23%|██▎       | 38/166 [00:48<02:42,  1.27s/it]avg_loss = 1.7768186025130444:  23%|██▎       | 39/166 [00:48<02:41,  1.27s/it]avg_loss = 1.7802881717681884:  23%|██▎       | 39/166 [00:50<02:41,  1.27s/it]avg_loss = 1.7802881717681884:  24%|██▍       | 40/166 [00:50<02:40,  1.27s/it]avg_loss = 1.780187775449055:  24%|██▍       | 40/166 [00:51<02:40,  1.27s/it] avg_loss = 1.780187775449055:  25%|██▍       | 41/166 [00:51<02:39,  1.27s/it]avg_loss = 1.7676204641660054:  25%|██▍       | 41/166 [00:52<02:39,  1.27s/it]avg_loss = 1.7676204641660054:  25%|██▌       | 42/166 [00:52<02:38,  1.28s/it]avg_loss = 1.7520372728968776:  25%|██▌       | 42/166 [00:54<02:38,  1.28s/it]avg_loss = 1.7520372728968776:  26%|██▌       | 43/166 [00:54<02:37,  1.28s/it]avg_loss = 1.7417010881684043:  26%|██▌       | 43/166 [00:55<02:37,  1.28s/it]avg_loss = 1.7417010881684043:  27%|██▋       | 44/166 [00:55<02:35,  1.28s/it]avg_loss = 1.7281068086624145:  27%|██▋       | 44/166 [00:56<02:35,  1.28s/it]avg_loss = 1.7281068086624145:  27%|██▋       | 45/166 [00:56<02:34,  1.28s/it]avg_loss = 1.7176533185917398:  27%|██▋       | 45/166 [00:57<02:34,  1.28s/it]avg_loss = 1.7176533185917398:  28%|██▊       | 46/166 [00:57<02:33,  1.28s/it]avg_loss = 1.7107547470863829:  28%|██▊       | 46/166 [00:59<02:33,  1.28s/it]avg_loss = 1.7107547470863829:  28%|██▊       | 47/166 [00:59<02:32,  1.28s/it]avg_loss = 1.7116422454516094:  28%|██▊       | 47/166 [01:00<02:32,  1.28s/it]avg_loss = 1.7116422454516094:  29%|██▉       | 48/166 [01:00<02:31,  1.28s/it]avg_loss = 1.722338511019337:  29%|██▉       | 48/166 [01:01<02:31,  1.28s/it] avg_loss = 1.722338511019337:  30%|██▉       | 49/166 [01:01<02:30,  1.28s/it]avg_loss = 1.7329386377334595:  30%|██▉       | 49/166 [01:03<02:30,  1.28s/it]avg_loss = 1.7329386377334595:  30%|███       | 50/166 [01:03<02:28,  1.28s/it]avg_loss = 1.7398692299337948:  30%|███       | 50/166 [01:04<02:28,  1.28s/it]avg_loss = 1.7398692299337948:  31%|███       | 51/166 [01:04<02:27,  1.29s/it]avg_loss = 1.744990179171929:  31%|███       | 51/166 [01:05<02:27,  1.29s/it] avg_loss = 1.744990179171929:  31%|███▏      | 52/166 [01:05<02:26,  1.29s/it]avg_loss = 1.748174638118384:  31%|███▏      | 52/166 [01:06<02:26,  1.29s/it]avg_loss = 1.748174638118384:  32%|███▏      | 53/166 [01:06<02:25,  1.29s/it]avg_loss = 1.74902578636452:  32%|███▏      | 53/166 [01:08<02:25,  1.29s/it] avg_loss = 1.74902578636452:  33%|███▎      | 54/166 [01:08<02:24,  1.29s/it]avg_loss = 1.7516374891454523:  33%|███▎      | 54/166 [01:09<02:24,  1.29s/it]avg_loss = 1.7516374891454523:  33%|███▎      | 55/166 [01:09<02:23,  1.29s/it]avg_loss = 1.755058846303395:  33%|███▎      | 55/166 [01:10<02:23,  1.29s/it] avg_loss = 1.755058846303395:  34%|███▎      | 56/166 [01:10<02:21,  1.29s/it]avg_loss = 1.750089166457193:  34%|███▎      | 56/166 [01:12<02:21,  1.29s/it]avg_loss = 1.750089166457193:  34%|███▍      | 57/166 [01:12<02:20,  1.29s/it]avg_loss = 1.7536517463881394:  34%|███▍      | 57/166 [01:13<02:20,  1.29s/it]avg_loss = 1.7536517463881394:  35%|███▍      | 58/166 [01:13<02:19,  1.29s/it]avg_loss = 1.751934546535298:  35%|███▍      | 58/166 [01:14<02:19,  1.29s/it] avg_loss = 1.751934546535298:  36%|███▌      | 59/166 [01:14<02:18,  1.29s/it]avg_loss = 1.7471734503904979:  36%|███▌      | 59/166 [01:15<02:18,  1.29s/it]avg_loss = 1.7471734503904979:  36%|███▌      | 60/166 [01:15<02:16,  1.29s/it]avg_loss = 1.7428559924735398:  36%|███▌      | 60/166 [01:17<02:16,  1.29s/it]avg_loss = 1.7428559924735398:  37%|███▋      | 61/166 [01:17<02:15,  1.29s/it]avg_loss = 1.7390051907108677:  37%|███▋      | 61/166 [01:18<02:15,  1.29s/it]avg_loss = 1.7390051907108677:  37%|███▋      | 62/166 [01:18<02:14,  1.29s/it]avg_loss = 1.7330795204828655:  37%|███▋      | 62/166 [01:19<02:14,  1.29s/it]avg_loss = 1.7330795204828655:  38%|███▊      | 63/166 [01:19<02:13,  1.30s/it]avg_loss = 1.7288548741489649:  38%|███▊      | 63/166 [01:21<02:13,  1.30s/it]avg_loss = 1.7288548741489649:  39%|███▊      | 64/166 [01:21<02:12,  1.30s/it]avg_loss = 1.7220313512361967:  39%|███▊      | 64/166 [01:22<02:12,  1.30s/it]avg_loss = 1.7220313512361967:  39%|███▉      | 65/166 [01:22<02:11,  1.30s/it]avg_loss = 1.7148070805000537:  39%|███▉      | 65/166 [01:23<02:11,  1.30s/it]avg_loss = 1.7148070805000537:  40%|███▉      | 66/166 [01:23<02:09,  1.30s/it]avg_loss = 1.709137046515052:  40%|███▉      | 66/166 [01:25<02:09,  1.30s/it] avg_loss = 1.709137046515052:  40%|████      | 67/166 [01:25<02:08,  1.30s/it]avg_loss = 1.7079902782159693:  40%|████      | 67/166 [01:26<02:08,  1.30s/it]avg_loss = 1.7079902782159693:  41%|████      | 68/166 [01:26<02:07,  1.30s/it]avg_loss = 1.7098780939544456:  41%|████      | 68/166 [01:27<02:07,  1.30s/it]avg_loss = 1.7098780939544456:  42%|████▏     | 69/166 [01:27<02:06,  1.30s/it]avg_loss = 1.7128122704369682:  42%|████▏     | 69/166 [01:28<02:06,  1.30s/it]avg_loss = 1.7128122704369682:  42%|████▏     | 70/166 [01:28<02:04,  1.30s/it]avg_loss = 1.7167950784656363:  42%|████▏     | 70/166 [01:30<02:04,  1.30s/it]avg_loss = 1.7167950784656363:  43%|████▎     | 71/166 [01:30<02:03,  1.30s/it]avg_loss = 1.7215962376859453:  43%|████▎     | 71/166 [01:31<02:03,  1.30s/it]avg_loss = 1.7215962376859453:  43%|████▎     | 72/166 [01:31<02:02,  1.30s/it]avg_loss = 1.7276532062112469:  43%|████▎     | 72/166 [01:32<02:02,  1.30s/it]avg_loss = 1.7276532062112469:  44%|████▍     | 73/166 [01:32<02:00,  1.30s/it]avg_loss = 1.7219763462607924:  44%|████▍     | 73/166 [01:34<02:00,  1.30s/it]avg_loss = 1.7219763462607924:  45%|████▍     | 74/166 [01:34<01:59,  1.30s/it]avg_loss = 1.717563673655192:  45%|████▍     | 74/166 [01:35<01:59,  1.30s/it] avg_loss = 1.717563673655192:  45%|████▌     | 75/166 [01:35<01:58,  1.30s/it]avg_loss = 1.7167440558734692:  45%|████▌     | 75/166 [01:36<01:58,  1.30s/it]avg_loss = 1.7167440558734692:  46%|████▌     | 76/166 [01:36<01:57,  1.30s/it]avg_loss = 1.7132188824864176:  46%|████▌     | 76/166 [01:38<01:57,  1.30s/it]avg_loss = 1.7132188824864176:  46%|████▋     | 77/166 [01:38<01:55,  1.30s/it]avg_loss = 1.7096746930709252:  46%|████▋     | 77/166 [01:39<01:55,  1.30s/it]avg_loss = 1.7096746930709252:  47%|████▋     | 78/166 [01:39<01:54,  1.30s/it]avg_loss = 1.7070488280887846:  47%|████▋     | 78/166 [01:40<01:54,  1.30s/it]avg_loss = 1.7070488280887846:  48%|████▊     | 79/166 [01:40<01:53,  1.30s/it]avg_loss = 1.7035989329218864:  48%|████▊     | 79/166 [01:41<01:53,  1.30s/it]avg_loss = 1.7035989329218864:  48%|████▊     | 80/166 [01:41<01:52,  1.30s/it]avg_loss = 1.6943461019315837:  48%|████▊     | 80/166 [01:43<01:52,  1.30s/it]avg_loss = 1.6943461019315837:  49%|████▉     | 81/166 [01:43<01:50,  1.30s/it]avg_loss = 1.696056901681714:  49%|████▉     | 81/166 [01:44<01:50,  1.30s/it] avg_loss = 1.696056901681714:  49%|████▉     | 82/166 [01:44<01:49,  1.30s/it]avg_loss = 1.6980427338416317:  49%|████▉     | 82/166 [01:45<01:49,  1.30s/it]avg_loss = 1.6980427338416317:  50%|█████     | 83/166 [01:45<01:48,  1.30s/it]avg_loss = 1.701004568310011:  50%|█████     | 83/166 [01:47<01:48,  1.30s/it] avg_loss = 1.701004568310011:  51%|█████     | 84/166 [01:47<01:46,  1.30s/it]avg_loss = 1.7027349465033588:  51%|█████     | 84/166 [01:48<01:46,  1.30s/it]avg_loss = 1.7027349465033588:  51%|█████     | 85/166 [01:48<01:45,  1.31s/it]avg_loss = 1.7017060663810997:  51%|█████     | 85/166 [01:49<01:45,  1.31s/it]avg_loss = 1.7017060663810997:  52%|█████▏    | 86/166 [01:49<01:44,  1.31s/it]avg_loss = 1.7020210349696807:  52%|█████▏    | 86/166 [01:51<01:44,  1.31s/it]avg_loss = 1.7020210349696807:  52%|█████▏    | 87/166 [01:51<01:43,  1.31s/it]avg_loss = 1.7022167003967545:  52%|█████▏    | 87/166 [01:52<01:43,  1.31s/it]avg_loss = 1.7022167003967545:  53%|█████▎    | 88/166 [01:52<01:41,  1.31s/it]avg_loss = 1.703447078720907:  53%|█████▎    | 88/166 [01:53<01:41,  1.31s/it] avg_loss = 1.703447078720907:  54%|█████▎    | 89/166 [01:53<01:40,  1.31s/it]avg_loss = 1.7032466166549258:  54%|█████▎    | 89/166 [01:55<01:40,  1.31s/it]avg_loss = 1.7032466166549258:  54%|█████▍    | 90/166 [01:55<01:39,  1.31s/it]avg_loss = 1.703687004335634:  54%|█████▍    | 90/166 [01:56<01:39,  1.31s/it] avg_loss = 1.703687004335634:  55%|█████▍    | 91/166 [01:56<01:38,  1.31s/it]avg_loss = 1.7047478433536447:  55%|█████▍    | 91/166 [01:57<01:38,  1.31s/it]avg_loss = 1.7047478433536447:  55%|█████▌    | 92/166 [01:57<01:36,  1.31s/it]avg_loss = 1.7086835093395685:  55%|█████▌    | 92/166 [01:58<01:36,  1.31s/it]avg_loss = 1.7086835093395685:  56%|█████▌    | 93/166 [01:58<01:35,  1.31s/it]avg_loss = 1.707771990527498:  56%|█████▌    | 93/166 [02:00<01:35,  1.31s/it] avg_loss = 1.707771990527498:  57%|█████▋    | 94/166 [02:00<01:34,  1.31s/it]avg_loss = 1.707078808859775:  57%|█████▋    | 94/166 [02:01<01:34,  1.31s/it]avg_loss = 1.707078808859775:  57%|█████▋    | 95/166 [02:01<01:32,  1.31s/it]avg_loss = 1.706754892443617:  57%|█████▋    | 95/166 [02:02<01:32,  1.31s/it]avg_loss = 1.706754892443617:  58%|█████▊    | 96/166 [02:02<01:31,  1.31s/it]avg_loss = 1.7067013653283267:  58%|█████▊    | 96/166 [02:04<01:31,  1.31s/it]avg_loss = 1.7067013653283267:  58%|█████▊    | 97/166 [02:04<01:30,  1.31s/it]avg_loss = 1.7049602142402105:  58%|█████▊    | 97/166 [02:05<01:30,  1.31s/it]avg_loss = 1.7049602142402105:  59%|█████▉    | 98/166 [02:05<01:29,  1.31s/it]avg_loss = 1.7025109451226514:  59%|█████▉    | 98/166 [02:06<01:29,  1.31s/it]avg_loss = 1.7025109451226514:  60%|█████▉    | 99/166 [02:06<01:27,  1.31s/it]avg_loss = 1.6998151606321334:  60%|█████▉    | 99/166 [02:08<01:27,  1.31s/it]avg_loss = 1.6998151606321334:  60%|██████    | 100/166 [02:08<01:26,  1.31s/it]avg_loss = 1.7002531048094873:  60%|██████    | 100/166 [02:09<01:26,  1.31s/it]avg_loss = 1.7002531048094873:  61%|██████    | 101/166 [02:09<01:25,  1.31s/it]avg_loss = 1.7012043706342286:  61%|██████    | 101/166 [02:10<01:25,  1.31s/it]avg_loss = 1.7012043706342286:  61%|██████▏   | 102/166 [02:10<01:23,  1.31s/it]avg_loss = 1.7023175786999822:  61%|██████▏   | 102/166 [02:12<01:23,  1.31s/it]avg_loss = 1.7023175786999822:  62%|██████▏   | 103/166 [02:12<01:22,  1.31s/it]avg_loss = 1.7044873300653238:  62%|██████▏   | 103/166 [02:13<01:22,  1.31s/it]avg_loss = 1.7044873300653238:  63%|██████▎   | 104/166 [02:13<01:21,  1.31s/it]avg_loss = 1.711149503503527:  63%|██████▎   | 104/166 [02:14<01:21,  1.31s/it] avg_loss = 1.711149503503527:  63%|██████▎   | 105/166 [02:14<01:20,  1.31s/it]avg_loss = 1.7163319514607482:  63%|██████▎   | 105/166 [02:15<01:20,  1.31s/it]avg_loss = 1.7163319514607482:  64%|██████▍   | 106/166 [02:15<01:18,  1.31s/it]avg_loss = 1.7198796199860973:  64%|██████▍   | 106/166 [02:17<01:18,  1.31s/it]avg_loss = 1.7198796199860973:  64%|██████▍   | 107/166 [02:17<01:17,  1.31s/it]avg_loss = 1.7230424500173993:  64%|██████▍   | 107/166 [02:18<01:17,  1.31s/it]avg_loss = 1.7230424500173993:  65%|██████▌   | 108/166 [02:18<01:16,  1.31s/it]avg_loss = 1.7277440997438693:  65%|██████▌   | 108/166 [02:19<01:16,  1.31s/it]avg_loss = 1.7277440997438693:  66%|██████▌   | 109/166 [02:19<01:14,  1.31s/it]avg_loss = 1.731196587194096:  66%|██████▌   | 109/166 [02:21<01:14,  1.31s/it] avg_loss = 1.731196587194096:  66%|██████▋   | 110/166 [02:21<01:13,  1.31s/it]avg_loss = 1.7327335820541725:  66%|██████▋   | 110/166 [02:22<01:13,  1.31s/it]avg_loss = 1.7327335820541725:  67%|██████▋   | 111/166 [02:22<01:12,  1.31s/it]avg_loss = 1.7339645789137907:  67%|██████▋   | 111/166 [02:23<01:12,  1.31s/it]avg_loss = 1.7339645789137907:  67%|██████▋   | 112/166 [02:23<01:10,  1.31s/it]avg_loss = 1.7342555897425762:  67%|██████▋   | 112/166 [02:25<01:10,  1.31s/it]avg_loss = 1.7342555897425762:  68%|██████▊   | 113/166 [02:25<01:09,  1.31s/it]avg_loss = 1.73564812541008:  68%|██████▊   | 113/166 [02:26<01:09,  1.31s/it]  avg_loss = 1.73564812541008:  69%|██████▊   | 114/166 [02:26<01:08,  1.31s/it]avg_loss = 1.7327187750650488:  69%|██████▊   | 114/166 [02:27<01:08,  1.31s/it]avg_loss = 1.7327187750650488:  69%|██████▉   | 115/166 [02:27<01:07,  1.31s/it]avg_loss = 1.7319948544790005:  69%|██████▉   | 115/166 [02:29<01:07,  1.31s/it]avg_loss = 1.7319948544790005:  70%|██████▉   | 116/166 [02:29<01:05,  1.31s/it]avg_loss = 1.7329226401117113:  70%|██████▉   | 116/166 [02:30<01:05,  1.31s/it]avg_loss = 1.7329226401117113:  70%|███████   | 117/166 [02:30<01:04,  1.31s/it]avg_loss = 1.733067367541588:  70%|███████   | 117/166 [02:31<01:04,  1.31s/it] avg_loss = 1.733067367541588:  71%|███████   | 118/166 [02:31<01:03,  1.32s/it]avg_loss = 1.7324418275296187:  71%|███████   | 118/166 [02:33<01:03,  1.32s/it]avg_loss = 1.7324418275296187:  72%|███████▏  | 119/166 [02:33<01:01,  1.32s/it]avg_loss = 1.7330391590793928:  72%|███████▏  | 119/166 [02:34<01:01,  1.32s/it]avg_loss = 1.7330391590793928:  72%|███████▏  | 120/166 [02:34<01:00,  1.32s/it]avg_loss = 1.7324510827537412:  72%|███████▏  | 120/166 [02:35<01:00,  1.32s/it]avg_loss = 1.7324510827537412:  73%|███████▎  | 121/166 [02:35<00:59,  1.32s/it]avg_loss = 1.7328388930344192:  73%|███████▎  | 121/166 [02:37<00:59,  1.32s/it]avg_loss = 1.7328388930344192:  73%|███████▎  | 122/166 [02:37<00:57,  1.32s/it]avg_loss = 1.7330526951851883:  73%|███████▎  | 122/166 [02:38<00:57,  1.32s/it]avg_loss = 1.7330526951851883:  74%|███████▍  | 123/166 [02:38<00:56,  1.32s/it]avg_loss = 1.7316301979364888:  74%|███████▍  | 123/166 [02:39<00:56,  1.32s/it]avg_loss = 1.7316301979364888:  75%|███████▍  | 124/166 [02:39<00:55,  1.32s/it]avg_loss = 1.729951111316681:  75%|███████▍  | 124/166 [02:40<00:55,  1.32s/it] avg_loss = 1.729951111316681:  75%|███████▌  | 125/166 [02:40<00:53,  1.32s/it]avg_loss = 1.7277352483499617:  75%|███████▌  | 125/166 [02:42<00:53,  1.32s/it]avg_loss = 1.7277352483499617:  76%|███████▌  | 126/166 [02:42<00:52,  1.32s/it]avg_loss = 1.7255222032389304:  76%|███████▌  | 126/166 [02:43<00:52,  1.32s/it]avg_loss = 1.7255222032389304:  77%|███████▋  | 127/166 [02:43<00:51,  1.32s/it]avg_loss = 1.7240827162750065:  77%|███████▋  | 127/166 [02:44<00:51,  1.32s/it]avg_loss = 1.7240827162750065:  77%|███████▋  | 128/166 [02:44<00:50,  1.32s/it]avg_loss = 1.7227939408878947:  77%|███████▋  | 128/166 [02:46<00:50,  1.32s/it]avg_loss = 1.7227939408878947:  78%|███████▊  | 129/166 [02:46<00:48,  1.32s/it]avg_loss = 1.7227461883654962:  78%|███████▊  | 129/166 [02:47<00:48,  1.32s/it]avg_loss = 1.7227461883654962:  78%|███████▊  | 130/166 [02:47<00:47,  1.32s/it]avg_loss = 1.723786097901468:  78%|███████▊  | 130/166 [02:48<00:47,  1.32s/it] avg_loss = 1.723786097901468:  79%|███████▉  | 131/166 [02:48<00:46,  1.32s/it]avg_loss = 1.7243291911753742:  79%|███████▉  | 131/166 [02:50<00:46,  1.32s/it]avg_loss = 1.7243291911753742:  80%|███████▉  | 132/166 [02:50<00:44,  1.32s/it]avg_loss = 1.7252774485071798:  80%|███████▉  | 132/166 [02:51<00:44,  1.32s/it]avg_loss = 1.7252774485071798:  80%|████████  | 133/166 [02:51<00:43,  1.32s/it]avg_loss = 1.7266033322953467:  80%|████████  | 133/166 [02:52<00:43,  1.32s/it]avg_loss = 1.7266033322953467:  81%|████████  | 134/166 [02:52<00:42,  1.32s/it]avg_loss = 1.7245401192594458:  81%|████████  | 134/166 [02:54<00:42,  1.32s/it]avg_loss = 1.7245401192594458:  81%|████████▏ | 135/166 [02:54<00:40,  1.32s/it]avg_loss = 1.7247767163550152:  81%|████████▏ | 135/166 [02:55<00:40,  1.32s/it]avg_loss = 1.7247767163550152:  82%|████████▏ | 136/166 [02:55<00:39,  1.32s/it]avg_loss = 1.7250537493803206:  82%|████████▏ | 136/166 [02:56<00:39,  1.32s/it]avg_loss = 1.7250537493803206:  83%|████████▎ | 137/166 [02:56<00:38,  1.32s/it]avg_loss = 1.7258660210215526:  83%|████████▎ | 137/166 [02:58<00:38,  1.32s/it]avg_loss = 1.7258660210215526:  83%|████████▎ | 138/166 [02:58<00:36,  1.32s/it]avg_loss = 1.7249899549449947:  83%|████████▎ | 138/166 [02:59<00:36,  1.32s/it]avg_loss = 1.7249899549449947:  84%|████████▎ | 139/166 [02:59<00:35,  1.32s/it]avg_loss = 1.7236685220684325:  84%|████████▎ | 139/166 [03:00<00:35,  1.32s/it]avg_loss = 1.7236685220684325:  84%|████████▍ | 140/166 [03:00<00:34,  1.32s/it]avg_loss = 1.7222913220419105:  84%|████████▍ | 140/166 [03:02<00:34,  1.32s/it]avg_loss = 1.7222913220419105:  85%|████████▍ | 141/166 [03:02<00:32,  1.32s/it]avg_loss = 1.7218622896872775:  85%|████████▍ | 141/166 [03:03<00:32,  1.32s/it]avg_loss = 1.7218622896872775:  86%|████████▌ | 142/166 [03:03<00:31,  1.32s/it]avg_loss = 1.7202282631313883:  86%|████████▌ | 142/166 [03:04<00:31,  1.32s/it]avg_loss = 1.7202282631313883:  86%|████████▌ | 143/166 [03:04<00:30,  1.32s/it]avg_loss = 1.7213841796749167:  86%|████████▌ | 143/166 [03:06<00:30,  1.32s/it]avg_loss = 1.7213841796749167:  87%|████████▋ | 144/166 [03:06<00:28,  1.32s/it]avg_loss = 1.72063665348908:  87%|████████▋ | 144/166 [03:07<00:28,  1.32s/it]  avg_loss = 1.72063665348908:  87%|████████▋ | 145/166 [03:07<00:27,  1.32s/it]avg_loss = 1.7205105672960412:  87%|████████▋ | 145/166 [03:08<00:27,  1.32s/it]avg_loss = 1.7205105672960412:  88%|████████▊ | 146/166 [03:08<00:26,  1.32s/it]avg_loss = 1.719357312536564:  88%|████████▊ | 146/166 [03:09<00:26,  1.32s/it] avg_loss = 1.719357312536564:  89%|████████▊ | 147/166 [03:09<00:25,  1.32s/it]avg_loss = 1.7184494544525404:  89%|████████▊ | 147/166 [03:11<00:25,  1.32s/it]avg_loss = 1.7184494544525404:  89%|████████▉ | 148/166 [03:11<00:23,  1.32s/it]avg_loss = 1.7167098878214024:  89%|████████▉ | 148/166 [03:12<00:23,  1.32s/it]avg_loss = 1.7167098878214024:  90%|████████▉ | 149/166 [03:12<00:22,  1.32s/it]avg_loss = 1.717659074862798:  90%|████████▉ | 149/166 [03:13<00:22,  1.32s/it] avg_loss = 1.717659074862798:  90%|█████████ | 150/166 [03:13<00:21,  1.32s/it]avg_loss = 1.7168019690260983:  90%|█████████ | 150/166 [03:15<00:21,  1.32s/it]avg_loss = 1.7168019690260983:  91%|█████████ | 151/166 [03:15<00:19,  1.32s/it]avg_loss = 1.7165884379493563:  91%|█████████ | 151/166 [03:16<00:19,  1.32s/it]avg_loss = 1.7165884379493563:  92%|█████████▏| 152/166 [03:16<00:18,  1.32s/it]avg_loss = 1.7163826379121518:  92%|█████████▏| 152/166 [03:17<00:18,  1.32s/it]avg_loss = 1.7163826379121518:  92%|█████████▏| 153/166 [03:17<00:17,  1.32s/it]avg_loss = 1.7179449986327777:  92%|█████████▏| 153/166 [03:19<00:17,  1.32s/it]avg_loss = 1.7179449986327777:  93%|█████████▎| 154/166 [03:19<00:15,  1.32s/it]avg_loss = 1.7174852321224827:  93%|█████████▎| 154/166 [03:20<00:15,  1.32s/it]avg_loss = 1.7174852321224827:  93%|█████████▎| 155/166 [03:20<00:14,  1.32s/it]avg_loss = 1.7173390430517685:  93%|█████████▎| 155/166 [03:21<00:14,  1.32s/it]avg_loss = 1.7173390430517685:  94%|█████████▍| 156/166 [03:21<00:13,  1.32s/it]avg_loss = 1.7155379189807138:  94%|█████████▍| 156/166 [03:23<00:13,  1.32s/it]avg_loss = 1.7155379189807138:  95%|█████████▍| 157/166 [03:23<00:11,  1.32s/it]avg_loss = 1.711279953959622:  95%|█████████▍| 157/166 [03:24<00:11,  1.32s/it] avg_loss = 1.711279953959622:  95%|█████████▌| 158/166 [03:24<00:10,  1.32s/it]avg_loss = 1.712068813027076:  95%|█████████▌| 158/166 [03:25<00:10,  1.32s/it]avg_loss = 1.712068813027076:  96%|█████████▌| 159/166 [03:25<00:09,  1.32s/it]avg_loss = 1.7134757045656444:  96%|█████████▌| 159/166 [03:27<00:09,  1.32s/it]avg_loss = 1.7134757045656444:  96%|█████████▋| 160/166 [03:27<00:07,  1.32s/it]avg_loss = 1.7158283820803861:  96%|█████████▋| 160/166 [03:28<00:07,  1.32s/it]avg_loss = 1.7158283820803861:  97%|█████████▋| 161/166 [03:28<00:06,  1.32s/it]avg_loss = 1.7159311377707822:  97%|█████████▋| 161/166 [03:29<00:06,  1.32s/it]avg_loss = 1.7159311377707822:  98%|█████████▊| 162/166 [03:29<00:05,  1.32s/it]avg_loss = 1.7155205424577913:  98%|█████████▊| 162/166 [03:31<00:05,  1.32s/it]avg_loss = 1.7155205424577913:  98%|█████████▊| 163/166 [03:31<00:03,  1.32s/it]avg_loss = 1.7161282704370777:  98%|█████████▊| 163/166 [03:32<00:03,  1.32s/it]avg_loss = 1.7161282704370777:  99%|█████████▉| 164/166 [03:32<00:02,  1.32s/it]avg_loss = 1.7162581743616045:  99%|█████████▉| 164/166 [03:33<00:02,  1.32s/it]avg_loss = 1.7162581743616045:  99%|█████████▉| 165/166 [03:33<00:01,  1.32s/it]avg_loss = 1.7182102099240544:  99%|█████████▉| 165/166 [03:35<00:01,  1.32s/it]avg_loss = 1.7182102099240544: 100%|██████████| 166/166 [03:35<00:00,  1.32s/it]avg_loss = 1.7182102099240544: 100%|██████████| 166/166 [03:35<00:00,  1.30s/it]
I0402 14:50:46.650935 3231691 eval_ppl.py:107] wikitext2 perplexity: 5.57454252243042
wikitext2 perplexity: 5.575
