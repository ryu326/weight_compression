I0402 14:50:54.850595 3231795 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:50:54.850684 3231795 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:50:54.850722 3231795 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:50:55.169661 3231795 config.py:54] PyTorch version 2.6.0 available.
W0402 14:50:55.356262 3231795 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:50:56.023395 3231795 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  6.82it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.46it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.79it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  7.98it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.08it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.22it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  7.97it/s]
I0402 14:50:56.977086 3231795 quantize_finetune_llama.py:152] loaded model
I0402 14:50:57.315621 3231795 quantize_finetune_llama.py:190] loaded compression model
I0402 14:51:11.581133 3231795 quantize_finetune_llama.py:194] loaded dataset and devset
I0402 14:51:16.599700 3231795 quantize_finetune_llama.py:214] layer 0 gpu 0
I0402 14:51:18.881196 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 0 in 2.106152296066284s
Use train scale and shift
tensor(2.2655e-07, device='cuda:0') tensor(0.0204, device='cuda:0')
tensor(0.0204, device='cuda:0') tensor(2.2655e-07, device='cuda:0')
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0402 14:51:30.668375 3231924 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:51:30.668461 3231924 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:51:30.668498 3231924 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:51:30.987089 3231924 config.py:54] PyTorch version 2.6.0 available.
W0402 14:51:31.173032 3231924 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:51:31.707292 3231924 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:51:31.711025 3231795 quantize_finetune_llama.py:214] layer 1 gpu 1
I0402 14:51:31.854545 3231924 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:51:34.147707 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 1 in 2.259171962738037s
I0402 14:51:37.823508 3231992 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:51:37.823599 3231992 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:51:37.823640 3231992 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:51:38.179258 3231992 config.py:54] PyTorch version 2.6.0 available.
W0402 14:51:38.384358 3231992 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:51:38.993574 3231992 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:51:38.997447 3231795 quantize_finetune_llama.py:214] layer 2 gpu 2
I0402 14:51:39.338208 3231992 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:51:41.496598 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 2 in 2.3416240215301514s
I0402 14:51:45.336575 3232062 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:51:45.336687 3232062 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:51:45.336728 3232062 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:51:45.694747 3232062 config.py:54] PyTorch version 2.6.0 available.
W0402 14:51:45.910575 3232062 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:51:46.538121 3232062 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:51:46.542098 3231795 quantize_finetune_llama.py:214] layer 3 gpu 3
I0402 14:51:46.721170 3232062 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:51:49.447791 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 3 in 2.7268142700195312s
I0402 14:51:53.461417 3232132 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:51:53.461527 3232132 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:51:53.461568 3232132 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:51:53.889842 3232132 config.py:54] PyTorch version 2.6.0 available.
W0402 14:51:54.115209 3232132 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:51:54.801487 3232132 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:51:54.805965 3231795 quantize_finetune_llama.py:214] layer 4 gpu 0
I0402 14:51:55.192640 3232132 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_v proxy err 0.0005595689290203154 tr(WHW.T) 971.8771362304688
bpp_loss 4.499108791351318
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_q proxy err 1.8269314750796184e-05 tr(WHW.T) 636425.375
bpp_loss 4.49021577835083
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_k proxy err 2.147238774341531e-05 tr(WHW.T) 398864.15625
bpp_loss 4.589315176010132
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
0_o proxy err 0.00013550589210353792 tr(WHW.T) 15925.943359375
bpp_loss 4.335294246673584
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
0_up proxy err 0.0013090443098917603 tr(WHW.T) 24117.18359375
bpp_loss 4.375725502191588
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
0_gate proxy err 0.0009081196039915085 tr(WHW.T) 35427.40234375
bpp_loss 4.390165639478107
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
0_down proxy err 0.0006411133799701929 tr(WHW.T) 35796.33984375
bpp_loss 4.582401275634766
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_v proxy err 0.0019981246441602707 tr(WHW.T) 657.0241088867188
bpp_loss 4.4444849491119385
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_q proxy err 3.629101047408767e-05 tr(WHW.T) 195417.453125
bpp_loss 5.3507795333862305
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_k proxy err 3.5116521758027375e-05 tr(WHW.T) 204295.5625
bpp_loss 5.344797849655151
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
1_o proxy err 0.0010074914898723364 tr(WHW.T) 4039.630859375
bpp_loss 4.313468933105469
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
1_up proxy err 0.0018162266351282597 tr(WHW.T) 23209.44921875
bpp_loss 4.39899480065634
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
1_gate proxy err 0.0009149222169071436 tr(WHW.T) 46954.0546875
bpp_loss 4.463932480922965
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
1_down proxy err 1.3048272194282617e-05 tr(WHW.T) 40779.421875
bpp_loss 4.798548742782238
I0402 14:52:37.319444 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 4 in 0.9273934364318848s
I0402 14:52:41.308379 3232200 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:52:41.308495 3232200 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:52:41.308538 3232200 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:52:41.696362 3232200 config.py:54] PyTorch version 2.6.0 available.
W0402 14:52:41.910912 3232200 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:52:42.546396 3232200 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:52:42.550475 3231795 quantize_finetune_llama.py:214] layer 5 gpu 1
I0402 14:52:42.797230 3232200 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_v proxy err 0.002049316419288516 tr(WHW.T) 2779.86376953125
bpp_loss 4.51720666885376
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_q proxy err 5.295327355270274e-05 tr(WHW.T) 159508.203125
bpp_loss 5.327187538146973
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_k proxy err 4.373368210508488e-05 tr(WHW.T) 210000.984375
bpp_loss 5.437328100204468
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
2_o proxy err 0.0019031642004847527 tr(WHW.T) 5300.58984375
bpp_loss 4.3835906982421875
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
2_up proxy err 0.0022445134818553925 tr(WHW.T) 19933.5703125
bpp_loss 4.408544318620549
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
2_gate proxy err 0.0014296835288405418 tr(WHW.T) 31660.716796875
bpp_loss 4.488577022108921
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
2_down proxy err 0.00221843458712101 tr(WHW.T) 17283.607421875
bpp_loss 4.481232066487157
I0402 14:52:45.306968 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 5 in 1.01776123046875s
I0402 14:52:49.373716 3232266 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:52:49.373826 3232266 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:52:49.373871 3232266 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:52:49.782602 3232266 config.py:54] PyTorch version 2.6.0 available.
W0402 14:52:50.008862 3232266 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:52:50.678914 3232266 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:52:50.683061 3231795 quantize_finetune_llama.py:214] layer 6 gpu 2
I0402 14:52:51.040155 3232266 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_v proxy err 0.0029313024133443832 tr(WHW.T) 2978.910400390625
bpp_loss 4.402873277664185
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_q proxy err 0.00013544279499910772 tr(WHW.T) 76174.0546875
bpp_loss 5.142285108566284
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_k proxy err 0.00010185441351495683 tr(WHW.T) 106360.5859375
bpp_loss 5.221749305725098
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
3_o proxy err 0.0016986749833449721 tr(WHW.T) 5255.58154296875
bpp_loss 4.383997440338135
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
3_up proxy err 0.0025238932576030493 tr(WHW.T) 17467.81640625
bpp_loss 4.418733818586483
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
3_gate proxy err 0.0015226975083351135 tr(WHW.T) 29396.990234375
bpp_loss 4.508506420046785
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
3_down proxy err 0.00222261855378747 tr(WHW.T) 16907.59375
bpp_loss 4.491852494173272
I0402 14:52:53.440273 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 6 in 0.9872736930847168s
I0402 14:52:57.249038 3232334 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:52:57.249130 3232334 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:52:57.249171 3232334 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:52:57.625596 3232334 config.py:54] PyTorch version 2.6.0 available.
W0402 14:52:57.846303 3232334 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:52:58.474939 3232334 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:52:58.478958 3231795 quantize_finetune_llama.py:214] layer 7 gpu 3
I0402 14:52:58.777863 3232334 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:52:59.859586 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 7 in 0.8957493305206299s
I0402 14:53:03.961247 3232402 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:53:03.961361 3232402 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:53:03.961410 3232402 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:53:04.336667 3232402 config.py:54] PyTorch version 2.6.0 available.
W0402 14:53:04.546192 3232402 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:53:05.273910 3232402 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:53:05.278623 3231795 quantize_finetune_llama.py:214] layer 8 gpu 0
I0402 14:53:05.434711 3232402 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_v proxy err 0.0026939441449940205 tr(WHW.T) 3097.555419921875
bpp_loss 4.450061082839966
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_q proxy err 0.0001264020975213498 tr(WHW.T) 78746.46875
bpp_loss 5.2426536083221436
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_k proxy err 8.964775042841211e-05 tr(WHW.T) 118660.75
bpp_loss 5.281831741333008
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
4_o proxy err 0.0020942094270139933 tr(WHW.T) 5336.9765625
bpp_loss 4.361860275268555
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
4_up proxy err 0.002406398532912135 tr(WHW.T) 17664.685546875
bpp_loss 4.41651277763899
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
4_gate proxy err 0.0011899196542799473 tr(WHW.T) 36588.140625
bpp_loss 4.53978693762491
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
4_down proxy err 0.0022170841693878174 tr(WHW.T) 16830.9296875
bpp_loss 4.4814938168193015
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_v proxy err 0.0028049591928720474 tr(WHW.T) 3166.572021484375
bpp_loss 4.455636262893677
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_q proxy err 0.00014262154581956565 tr(WHW.T) 72535.90625
bpp_loss 5.242923259735107
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_k proxy err 9.585712541593239e-05 tr(WHW.T) 116207.296875
bpp_loss 5.327037334442139
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
5_o proxy err 0.002393703907728195 tr(WHW.T) 3769.5478515625
bpp_loss 4.435952425003052
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
5_up proxy err 0.0023827007971704006 tr(WHW.T) 18016.64453125
bpp_loss 4.41504828874455
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
5_gate proxy err 0.0011153623927384615 tr(WHW.T) 39417.078125
bpp_loss 4.546243179676145
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
5_down proxy err 0.002430753316730261 tr(WHW.T) 15953.3203125
bpp_loss 4.468330782513286
I0402 14:53:48.543227 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 8 in 0.9019668102264404s
I0402 14:53:52.501757 3232470 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:53:52.501923 3232470 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:53:52.501965 3232470 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:53:52.893122 3232470 config.py:54] PyTorch version 2.6.0 available.
W0402 14:53:53.104445 3232470 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:53:53.749180 3232470 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:53:53.753210 3231795 quantize_finetune_llama.py:214] layer 9 gpu 1
I0402 14:53:54.022994 3232470 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_v proxy err 0.0030378219671547413 tr(WHW.T) 3180.833740234375
bpp_loss 4.3817596435546875
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_q proxy err 0.0002019055827986449 tr(WHW.T) 54760.6953125
bpp_loss 5.0988194942474365
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_k proxy err 0.00015072734095156193 tr(WHW.T) 75270.1171875
bpp_loss 5.137352705001831
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
6_o proxy err 0.0024713973980396986 tr(WHW.T) 4041.38525390625
bpp_loss 4.3642237186431885
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
6_up proxy err 0.0023755913134664297 tr(WHW.T) 17971.6953125
bpp_loss 4.412119843239008
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
6_gate proxy err 0.0009666201658546925 tr(WHW.T) 45430.91015625
bpp_loss 4.569519575252089
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
6_down proxy err 0.002484918339177966 tr(WHW.T) 15434.75390625
bpp_loss 4.466258470402208
I0402 14:53:57.105271 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 9 in 0.9366190433502197s
I0402 14:54:00.985383 3232536 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:54:00.985488 3232536 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:54:00.985530 3232536 utils.py:162] NumExpr defaulting to 16 threads.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_v proxy err 0.00301750679500401 tr(WHW.T) 3250.746826171875
bpp_loss 4.386363744735718
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_q proxy err 0.00021767902944702655 tr(WHW.T) 51300.6015625
bpp_loss 5.092097520828247
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_k proxy err 0.0001682146539678797 tr(WHW.T) 68225.015625
bpp_loss 5.0996575355529785
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
7_o proxy err 0.0027553874533623457 tr(WHW.T) 3528.39208984375
bpp_loss 4.3763017654418945
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
7_up proxy err 0.0022917238529771566 tr(WHW.T) 18230.072265625
bpp_loss 4.419218728708667
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
7_gate proxy err 0.0009237286867573857 tr(WHW.T) 46648.5078125
bpp_loss 4.571711872899255
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
7_down proxy err 0.00253120344132185 tr(WHW.T) 15240.3759765625
bpp_loss 4.4659143935802375
I0402 14:54:01.337332 3232536 config.py:54] PyTorch version 2.6.0 available.
W0402 14:54:01.545666 3232536 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:54:02.215740 3232536 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:54:02.219610 3231795 quantize_finetune_llama.py:214] layer 10 gpu 2
I0402 14:54:02.663478 3232536 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:54:03.438216 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 10 in 0.8195605278015137s
I0402 14:54:07.345201 3232604 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:54:07.345296 3232604 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:54:07.345337 3232604 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:54:07.674256 3232604 config.py:54] PyTorch version 2.6.0 available.
W0402 14:54:07.882102 3232604 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:54:08.572491 3232604 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:54:08.576745 3231795 quantize_finetune_llama.py:214] layer 11 gpu 3
I0402 14:54:08.826746 3232604 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:54:10.258935 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 11 in 1.2099363803863525s
I0402 14:54:14.198160 3232672 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:54:14.198264 3232672 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:54:14.198304 3232672 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:54:14.611998 3232672 config.py:54] PyTorch version 2.6.0 available.
W0402 14:54:14.830997 3232672 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:54:15.528747 3232672 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:54:15.532754 3231795 quantize_finetune_llama.py:214] layer 12 gpu 0
I0402 14:54:15.703162 3232672 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_v proxy err 0.002739672316238284 tr(WHW.T) 3470.401611328125
bpp_loss 4.413074731826782
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_q proxy err 0.00022319470008369535 tr(WHW.T) 47592.55859375
bpp_loss 5.124161005020142
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_k proxy err 0.00015691287990193814 tr(WHW.T) 70099.9296875
bpp_loss 5.132333278656006
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
8_o proxy err 0.003014658112078905 tr(WHW.T) 3118.695556640625
bpp_loss 4.404339790344238
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
8_up proxy err 0.002078756457194686 tr(WHW.T) 19852.556640625
bpp_loss 4.434613072594931
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
8_gate proxy err 0.0009390400373376906 tr(WHW.T) 45311.83203125
bpp_loss 4.551935417707576
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
8_down proxy err 0.0025346395559608936 tr(WHW.T) 15322.3779296875
bpp_loss 4.4747514724731445
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_v proxy err 0.0027355048805475235 tr(WHW.T) 3676.465087890625
bpp_loss 4.413552284240723
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_q proxy err 0.00024144876806531101 tr(WHW.T) 45676.421875
bpp_loss 5.1163177490234375
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_k proxy err 0.00016069690173026174 tr(WHW.T) 72055.03125
bpp_loss 5.16209864616394
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
9_o proxy err 0.003207473549991846 tr(WHW.T) 3148.60498046875
bpp_loss 4.401960849761963
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
9_up proxy err 0.0019947015680372715 tr(WHW.T) 20605.642578125
bpp_loss 4.442779541015625
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
9_gate proxy err 0.0009322107653133571 tr(WHW.T) 45406.61328125
bpp_loss 4.53980397069177
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
9_down proxy err 0.0025765318423509598 tr(WHW.T) 15324.0654296875
bpp_loss 4.476457507111306
I0402 14:55:00.240699 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 12 in 0.9964253902435303s
I0402 14:55:04.315018 3232740 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:55:04.315121 3232740 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:55:04.315167 3232740 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:55:04.703633 3232740 config.py:54] PyTorch version 2.6.0 available.
W0402 14:55:04.917387 3232740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_v proxy err 0.0027448986656963825 tr(WHW.T) 3651.62158203125
bpp_loss 4.406938791275024
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_q proxy err 0.0002492919738870114 tr(WHW.T) 43935.53515625
bpp_loss 5.11521315574646
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_k proxy err 0.00016486203821841627 tr(WHW.T) 69912.859375
bpp_loss 5.171551465988159
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
10_o proxy err 0.003466110909357667 tr(WHW.T) 3058.283935546875
bpp_loss 4.385939359664917
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
10_up proxy err 0.0018779980018734932 tr(WHW.T) 21904.43359375
bpp_loss 4.454627458439317
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
10_gate proxy err 0.0009202745277434587 tr(WHW.T) 45983.0625
bpp_loss 4.535877493924873
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
10_down proxy err 0.0024268862325698137 tr(WHW.T) 16111.1357421875
bpp_loss 4.489415390546932
W0402 14:55:05.624412 3232740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:55:05.628236 3231795 quantize_finetune_llama.py:214] layer 13 gpu 1
I0402 14:55:05.927466 3232740 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:55:07.320661 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 13 in 0.8959746360778809s
I0402 14:55:11.210355 3232806 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:55:11.210453 3232806 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:55:11.210496 3232806 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:55:11.550385 3232806 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_v proxy err 0.002738210139796138 tr(WHW.T) 3890.81982421875
bpp_loss 4.423220157623291
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_q proxy err 0.000302954955259338 tr(WHW.T) 38073.125
bpp_loss 5.000255346298218
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_k proxy err 0.0002093300863634795 tr(WHW.T) 56997.671875
bpp_loss 4.990163803100586
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
11_o proxy err 0.0035464689135551453 tr(WHW.T) 3054.2177734375
bpp_loss 4.411137580871582
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
11_up proxy err 0.0019306541653349996 tr(WHW.T) 21548.3046875
bpp_loss 4.462812734204669
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
11_gate proxy err 0.000941858917940408 tr(WHW.T) 45386.7265625
bpp_loss 4.529389847156613
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
11_down proxy err 0.002538481494411826 tr(WHW.T) 15730.783203125
bpp_loss 4.488870110622672
W0402 14:55:11.763262 3232806 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:55:12.367112 3232806 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:55:12.370838 3231795 quantize_finetune_llama.py:214] layer 14 gpu 2
I0402 14:55:12.533701 3232806 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:55:13.760329 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 14 in 0.8759055137634277s
I0402 14:55:17.737011 3232874 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:55:17.737119 3232874 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:55:17.737160 3232874 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:55:18.114016 3232874 config.py:54] PyTorch version 2.6.0 available.
W0402 14:55:18.317648 3232874 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:55:18.898758 3232874 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:55:18.902509 3231795 quantize_finetune_llama.py:214] layer 15 gpu 3
I0402 14:55:19.116575 3232874 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:55:20.376375 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 15 in 1.005969762802124s
I0402 14:55:24.466045 3232942 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:55:24.466148 3232942 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:55:24.466194 3232942 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:55:24.853905 3232942 config.py:54] PyTorch version 2.6.0 available.
W0402 14:55:25.074401 3232942 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:55:25.731880 3232942 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:55:25.736091 3231795 quantize_finetune_llama.py:214] layer 16 gpu 0
I0402 14:55:26.251516 3232942 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_v proxy err 0.002832495840266347 tr(WHW.T) 3807.353515625
bpp_loss 4.409913063049316
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_q proxy err 0.00030340172816067934 tr(WHW.T) 38406.0
bpp_loss 5.036783218383789
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_k proxy err 0.00020367173419799656 tr(WHW.T) 59429.25
bpp_loss 5.091764211654663
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
12_o proxy err 0.0036998481955379248 tr(WHW.T) 2997.118896484375
bpp_loss 4.395557641983032
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
12_up proxy err 0.0019211337203159928 tr(WHW.T) 21806.251953125
bpp_loss 4.4724917744481285
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
12_gate proxy err 0.001012371270917356 tr(WHW.T) 42415.66015625
bpp_loss 4.520470552666243
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
12_down proxy err 0.002511094557121396 tr(WHW.T) 15783.7470703125
bpp_loss 4.501594609992448
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_v proxy err 0.0029209477361291647 tr(WHW.T) 3883.96826171875
bpp_loss 4.424046039581299
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_q proxy err 0.000321191328112036 tr(WHW.T) 38091.8046875
bpp_loss 5.004804372787476
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_k proxy err 0.0002214026899309829 tr(WHW.T) 57141.765625
bpp_loss 5.034528017044067
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
13_o proxy err 0.0033364654518663883 tr(WHW.T) 3392.266845703125
bpp_loss 4.416593551635742
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
13_up proxy err 0.0018447859911248088 tr(WHW.T) 22688.076171875
bpp_loss 4.4847293232762535
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
13_gate proxy err 0.000990683096460998 tr(WHW.T) 43286.52734375
bpp_loss 4.51616526758948
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
13_down proxy err 0.002448909217491746 tr(WHW.T) 15750.5673828125
bpp_loss 4.520569845687511
I0402 14:56:11.067946 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 16 in 0.9446547031402588s
I0402 14:56:14.991594 3233010 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:56:14.991703 3233010 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:56:14.991747 3233010 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:56:15.372570 3233010 config.py:54] PyTorch version 2.6.0 available.
W0402 14:56:15.590272 3233010 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_v proxy err 0.003097633132711053 tr(WHW.T) 3647.569580078125
bpp_loss 4.409854888916016
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_q proxy err 0.000328898400766775 tr(WHW.T) 36835.87109375
bpp_loss 5.001990556716919
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_k proxy err 0.0002148717176169157 tr(WHW.T) 58844.5546875
bpp_loss 5.03103232383728
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
14_o proxy err 0.0038088951259851456 tr(WHW.T) 3063.885498046875
bpp_loss 4.39128303527832
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
14_up proxy err 0.0018888110062107444 tr(WHW.T) 22452.671875
bpp_loss 4.483962302984193
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
14_gate proxy err 0.001051250845193863 tr(WHW.T) 41240.93359375
bpp_loss 4.511650617732558
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
14_down proxy err 0.0024930136278271675 tr(WHW.T) 15409.48828125
bpp_loss 4.523329845694608
W0402 14:56:16.207799 3233010 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:56:16.211723 3231795 quantize_finetune_llama.py:214] layer 17 gpu 1
I0402 14:56:16.464090 3233010 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:56:18.019196 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 17 in 1.099677562713623s
I0402 14:56:22.064882 3233076 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:56:22.064986 3233076 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:56:22.065028 3233076 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:56:22.427840 3233076 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_v proxy err 0.0027669300325214863 tr(WHW.T) 4003.05126953125
bpp_loss 4.449018239974976
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_q proxy err 0.0003091175458393991 tr(WHW.T) 38353.34765625
bpp_loss 4.984824895858765
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_k proxy err 0.0002112955553457141 tr(WHW.T) 58600.94921875
bpp_loss 5.0436320304870605
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
15_o proxy err 0.0032725457567721605 tr(WHW.T) 3628.3759765625
bpp_loss 4.416482448577881
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
15_up proxy err 0.0018341144314035773 tr(WHW.T) 23066.35546875
bpp_loss 4.490983120230741
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
15_gate proxy err 0.001054607331752777 tr(WHW.T) 40946.2890625
bpp_loss 4.51910382647847
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
15_down proxy err 0.002425004495307803 tr(WHW.T) 15429.0400390625
bpp_loss 4.5369036253108534
W0402 14:56:22.641979 3233076 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:56:23.278436 3233076 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:56:23.282444 3231795 quantize_finetune_llama.py:214] layer 18 gpu 2
I0402 14:56:23.488304 3233076 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:56:24.738939 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 18 in 0.9227464199066162s
I0402 14:56:28.762768 3233144 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:56:28.762870 3233144 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:56:28.762913 3233144 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:56:29.120316 3233144 config.py:54] PyTorch version 2.6.0 available.
W0402 14:56:29.324912 3233144 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:56:29.963228 3233144 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:56:29.967135 3231795 quantize_finetune_llama.py:214] layer 19 gpu 3
I0402 14:56:30.185492 3233144 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:56:31.479971 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 19 in 1.0262017250061035s
I0402 14:56:35.568071 3233212 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:56:35.568216 3233212 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:56:35.568267 3233212 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:56:35.965392 3233212 config.py:54] PyTorch version 2.6.0 available.
W0402 14:56:36.183033 3233212 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:56:36.810336 3233212 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:56:36.814395 3231795 quantize_finetune_llama.py:214] layer 20 gpu 0
I0402 14:56:37.071686 3233212 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_v proxy err 0.0029028665740042925 tr(WHW.T) 3988.751708984375
bpp_loss 4.473174810409546
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_q proxy err 0.00033427064772695303 tr(WHW.T) 37064.6328125
bpp_loss 4.95918607711792
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_k proxy err 0.000215658379602246 tr(WHW.T) 59978.5
bpp_loss 5.001658916473389
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
16_o proxy err 0.002793135354295373 tr(WHW.T) 4695.22509765625
bpp_loss 4.424158334732056
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
16_up proxy err 0.0018384645227342844 tr(WHW.T) 23603.60546875
bpp_loss 4.486235152843387
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
16_gate proxy err 0.0010490396525710821 tr(WHW.T) 42174.73828125
bpp_loss 4.524339720260265
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
16_down proxy err 0.002465437864884734 tr(WHW.T) 15228.4970703125
bpp_loss 4.535018255544263
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_v proxy err 0.00289550912566483 tr(WHW.T) 4263.80712890625
bpp_loss 4.450387477874756
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_q proxy err 0.00036113447276875377 tr(WHW.T) 36392.74609375
bpp_loss 4.9259655475616455
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_k proxy err 0.00025029564858414233 tr(WHW.T) 54388.6953125
bpp_loss 4.960630655288696
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
17_o proxy err 0.003004859434440732 tr(WHW.T) 4297.923828125
bpp_loss 4.430271625518799
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
17_up proxy err 0.002047163899987936 tr(WHW.T) 21615.3203125
bpp_loss 4.478522544683412
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
17_gate proxy err 0.0011166190961375833 tr(WHW.T) 40300.40234375
bpp_loss 4.5335744813431145
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
17_down proxy err 0.002525579184293747 tr(WHW.T) 15353.4287109375
bpp_loss 4.52191978277162
I0402 14:57:21.983728 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 20 in 0.892695426940918s
I0402 14:57:25.969163 3233280 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:57:25.969264 3233280 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:57:25.969305 3233280 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:57:26.344406 3233280 config.py:54] PyTorch version 2.6.0 available.
W0402 14:57:26.552488 3233280 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_v proxy err 0.002754530403763056 tr(WHW.T) 4673.08837890625
bpp_loss 4.48203182220459
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_q proxy err 0.00038658667472191155 tr(WHW.T) 35237.796875
bpp_loss 4.873825788497925
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_k proxy err 0.0002840202650986612 tr(WHW.T) 49119.20703125
bpp_loss 4.908476114273071
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
18_o proxy err 0.002569629577919841 tr(WHW.T) 4923.25390625
bpp_loss 4.473707437515259
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
18_up proxy err 0.0021924087777733803 tr(WHW.T) 20299.947265625
bpp_loss 4.474140255950218
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
18_gate proxy err 0.001187309273518622 tr(WHW.T) 38072.26171875
bpp_loss 4.544739124386809
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
18_down proxy err 0.0024680583737790585 tr(WHW.T) 15266.072265625
bpp_loss 4.53094724167225
W0402 14:57:27.143687 3233280 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:57:27.147573 3231795 quantize_finetune_llama.py:214] layer 21 gpu 1
I0402 14:57:27.314609 3233280 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:57:28.756704 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 21 in 0.885908842086792s
I0402 14:57:32.704705 3233346 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:57:32.704797 3233346 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:57:32.704838 3233346 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:57:33.047020 3233346 config.py:54] PyTorch version 2.6.0 available.
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_v proxy err 0.0027124108746647835 tr(WHW.T) 4788.90576171875
bpp_loss 4.488080263137817
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_q proxy err 0.0004170876054558903 tr(WHW.T) 32879.76953125
bpp_loss 4.849790811538696
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_k proxy err 0.00028299386030994356 tr(WHW.T) 49984.76171875
bpp_loss 4.8770129680633545
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
19_o proxy err 0.0026683909818530083 tr(WHW.T) 5005.189453125
bpp_loss 4.468179225921631
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
19_up proxy err 0.0022118091583251953 tr(WHW.T) 20172.62109375
bpp_loss 4.473572664482649
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
19_gate proxy err 0.0013046414824202657 tr(WHW.T) 34698.6953125
bpp_loss 4.549223079237827
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
19_down proxy err 0.002408653497695923 tr(WHW.T) 15697.3017578125
bpp_loss 4.5323471024979
W0402 14:57:33.259663 3233346 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:57:33.838051 3233346 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:57:33.842017 3231795 quantize_finetune_llama.py:214] layer 22 gpu 2
I0402 14:57:34.113700 3233346 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:57:35.253613 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 22 in 0.9016473293304443s
I0402 14:57:39.113494 3233414 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:57:39.113602 3233414 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:57:39.113644 3233414 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:57:39.474393 3233414 config.py:54] PyTorch version 2.6.0 available.
W0402 14:57:39.682247 3233414 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:57:40.284044 3233414 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:57:40.287844 3231795 quantize_finetune_llama.py:214] layer 23 gpu 3
I0402 14:57:40.605435 3233414 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:57:41.908643 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 23 in 1.1872122287750244s
I0402 14:57:45.979235 3233482 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:57:45.979337 3233482 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:57:45.979379 3233482 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:57:46.378789 3233482 config.py:54] PyTorch version 2.6.0 available.
W0402 14:57:46.605053 3233482 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:57:47.288404 3233482 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:57:47.292537 3231795 quantize_finetune_llama.py:214] layer 24 gpu 0
I0402 14:57:47.582985 3233482 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_v proxy err 0.0028426949866116047 tr(WHW.T) 4645.8291015625
bpp_loss 4.498314142227173
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_q proxy err 0.0004134839109610766 tr(WHW.T) 33815.2578125
bpp_loss 4.854526042938232
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_k proxy err 0.00029182108119130135 tr(WHW.T) 49149.9921875
bpp_loss 4.880626916885376
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
20_o proxy err 0.0019314450910314918 tr(WHW.T) 6820.419921875
bpp_loss 4.493358612060547
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
20_up proxy err 0.002178397960960865 tr(WHW.T) 20603.083984375
bpp_loss 4.471885681152344
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
20_gate proxy err 0.0012785462895408273 tr(WHW.T) 35509.98046875
bpp_loss 4.5560082723928055
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
20_down proxy err 0.002356024459004402 tr(WHW.T) 15803.095703125
bpp_loss 4.537937075592751
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_v proxy err 0.0027881234418600798 tr(WHW.T) 4862.75634765625
bpp_loss 4.523179292678833
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_q proxy err 0.0004698726115748286 tr(WHW.T) 30261.54296875
bpp_loss 4.8091888427734375
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_k proxy err 0.0003397728723939508 tr(WHW.T) 42768.43359375
bpp_loss 4.8224852085113525
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
21_o proxy err 0.0022872877307236195 tr(WHW.T) 6399.25439453125
bpp_loss 4.479370355606079
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
21_up proxy err 0.0022984002716839314 tr(WHW.T) 19562.884765625
bpp_loss 4.469581426576126
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
21_gate proxy err 0.0013676665257662535 tr(WHW.T) 33251.44921875
bpp_loss 4.565062411995822
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
21_down proxy err 0.002406950341537595 tr(WHW.T) 15792.2314453125
bpp_loss 4.530282441959825
I0402 14:58:31.972746 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 24 in 0.9658782482147217s
I0402 14:58:35.835242 3233550 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:58:35.835346 3233550 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:58:35.835386 3233550 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:58:36.229150 3233550 config.py:54] PyTorch version 2.6.0 available.
W0402 14:58:36.450799 3233550 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_v proxy err 0.0026310505345463753 tr(WHW.T) 5111.93212890625
bpp_loss 4.530014991760254
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_q proxy err 0.0004404677893035114 tr(WHW.T) 32103.412109375
bpp_loss 4.84717869758606
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_k proxy err 0.00032852747244760394 tr(WHW.T) 43969.734375
bpp_loss 4.864871501922607
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
22_o proxy err 0.0017794693121686578 tr(WHW.T) 7627.48828125
bpp_loss 4.509942054748535
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
22_up proxy err 0.0023201326839625835 tr(WHW.T) 19439.994140625
bpp_loss 4.468500004258266
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
22_gate proxy err 0.0013921367935836315 tr(WHW.T) 32731.19921875
bpp_loss 4.5727489382721656
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
22_down proxy err 0.0024211143609136343 tr(WHW.T) 15874.603515625
bpp_loss 4.526748435441838
W0402 14:58:37.123852 3233550 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:58:37.128084 3231795 quantize_finetune_llama.py:214] layer 25 gpu 1
I0402 14:58:37.351280 3233550 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:58:38.646682 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 25 in 0.932457685470581s
I0402 14:58:42.625328 3233616 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:58:42.625429 3233616 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:58:42.625472 3233616 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:58:43.002088 3233616 config.py:54] PyTorch version 2.6.0 available.
W0402 14:58:43.221197 3233616 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:58:43.849441 3233616 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:58:43.853127 3231795 quantize_finetune_llama.py:214] layer 26 gpu 2
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_v proxy err 0.002483680611476302 tr(WHW.T) 5666.529296875
bpp_loss 4.568798303604126
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_q proxy err 0.0005202235770411789 tr(WHW.T) 28221.720703125
bpp_loss 4.821022272109985
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_k proxy err 0.00038982307887636125 tr(WHW.T) 38375.3828125
bpp_loss 4.831223726272583
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
23_o proxy err 0.0022099679335951805 tr(WHW.T) 6345.388671875
bpp_loss 4.555142641067505
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
23_up proxy err 0.0024026886094361544 tr(WHW.T) 18753.931640625
bpp_loss 4.474527403365734
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
23_gate proxy err 0.0014936868101358414 tr(WHW.T) 30400.564453125
bpp_loss 4.572709017021712
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
23_down proxy err 0.0024328893050551414 tr(WHW.T) 15816.7724609375
bpp_loss 4.533371703569279
I0402 14:58:44.228941 3233616 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:58:46.115088 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 26 in 0.9253859519958496s
I0402 14:58:50.347001 3233684 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:58:50.347104 3233684 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:58:50.347146 3233684 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:58:50.722712 3233684 config.py:54] PyTorch version 2.6.0 available.
W0402 14:58:50.930475 3233684 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:58:51.566680 3233684 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:58:51.570801 3231795 quantize_finetune_llama.py:214] layer 27 gpu 3
I0402 14:58:51.786424 3233684 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:58:53.014885 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 27 in 0.9497628211975098s
I0402 14:58:56.979408 3233752 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:58:56.979514 3233752 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:58:56.979559 3233752 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:58:57.348190 3233752 config.py:54] PyTorch version 2.6.0 available.
W0402 14:58:57.563606 3233752 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 14:58:58.172004 3233752 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:58:58.176209 3231795 quantize_finetune_llama.py:214] layer 28 gpu 0
I0402 14:58:58.396317 3233752 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_v proxy err 0.0025512403808534145 tr(WHW.T) 5324.72998046875
bpp_loss 4.5700647830963135
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_q proxy err 0.0005248499219305813 tr(WHW.T) 27011.373046875
bpp_loss 4.785984039306641
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_k proxy err 0.00036558444844558835 tr(WHW.T) 39756.4296875
bpp_loss 4.789240837097168
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
24_o proxy err 0.0017372972797602415 tr(WHW.T) 8038.615234375
bpp_loss 4.5397796630859375
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
24_up proxy err 0.002439298201352358 tr(WHW.T) 18512.99609375
bpp_loss 4.477881675542787
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
24_gate proxy err 0.0015088330255821347 tr(WHW.T) 30171.666015625
bpp_loss 4.5757258215615915
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
24_down proxy err 0.002427405444905162 tr(WHW.T) 15756.2470703125
bpp_loss 4.539908520011014
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_v proxy err 0.002432845765724778 tr(WHW.T) 5927.5078125
bpp_loss 4.595527410507202
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_q proxy err 0.0005977562977932394 tr(WHW.T) 25041.619140625
bpp_loss 4.7798707485198975
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_k proxy err 0.0004507612029556185 tr(WHW.T) 33632.96875
bpp_loss 4.782510757446289
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
25_o proxy err 0.0022496283054351807 tr(WHW.T) 6779.06689453125
bpp_loss 4.557281255722046
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
25_up proxy err 0.0024127454962581396 tr(WHW.T) 18624.029296875
bpp_loss 4.483857443166333
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
25_gate proxy err 0.0014610271900892258 tr(WHW.T) 31053.6875
bpp_loss 4.579064657521802
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
25_down proxy err 0.0022974528837949038 tr(WHW.T) 15877.4140625
bpp_loss 4.5625083391056505
I0402 14:59:42.307375 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 28 in 0.9273643493652344s
I0402 14:59:46.288354 3233820 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:59:46.288460 3233820 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:59:46.288504 3233820 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:59:46.667930 3233820 config.py:54] PyTorch version 2.6.0 available.
W0402 14:59:46.882233 3233820 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_v proxy err 0.002370392205193639 tr(WHW.T) 5920.73828125
bpp_loss 4.626529932022095
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_q proxy err 0.0005489102913998067 tr(WHW.T) 26711.5390625
bpp_loss 4.76270604133606
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_k proxy err 0.0003983970091212541 tr(WHW.T) 37533.6015625
bpp_loss 4.771124839782715
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
26_o proxy err 0.0013036903692409396 tr(WHW.T) 9866.05859375
bpp_loss 4.6475725173950195
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
26_up proxy err 0.002267642645165324 tr(WHW.T) 19856.3125
bpp_loss 4.488076320914335
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
26_gate proxy err 0.0013569605071097612 tr(WHW.T) 33454.33203125
bpp_loss 4.583338183025981
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
26_down proxy err 0.002302721841260791 tr(WHW.T) 15425.537109375
bpp_loss 4.575257545293764
W0402 14:59:47.523479 3233820 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:59:47.527390 3231795 quantize_finetune_llama.py:214] layer 29 gpu 1
I0402 14:59:47.767119 3233820 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:59:49.940803 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 29 in 1.1868257522583008s
I0402 14:59:53.919045 3233886 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 14:59:53.919149 3233886 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 14:59:53.919190 3233886 utils.py:162] NumExpr defaulting to 16 threads.
I0402 14:59:54.289376 3233886 config.py:54] PyTorch version 2.6.0 available.
W0402 14:59:54.504168 3233886 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_v proxy err 0.0022963040973991156 tr(WHW.T) 6537.79541015625
bpp_loss 4.604366779327393
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_q proxy err 0.0005553417722694576 tr(WHW.T) 28139.56640625
bpp_loss 4.799919128417969
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_k proxy err 0.000405927246902138 tr(WHW.T) 38878.3359375
bpp_loss 4.814615726470947
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
27_o proxy err 0.001723623019643128 tr(WHW.T) 7266.2978515625
bpp_loss 4.664920330047607
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
27_up proxy err 0.002059099730104208 tr(WHW.T) 21823.326171875
bpp_loss 4.494766767634902
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
27_gate proxy err 0.0012776092626154423 tr(WHW.T) 35491.0
bpp_loss 4.585243402525436
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
27_down proxy err 0.0022015648428350687 tr(WHW.T) 15119.01171875
bpp_loss 4.603000308192054
W0402 14:59:55.128004 3233886 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 14:59:55.132115 3231795 quantize_finetune_llama.py:214] layer 30 gpu 2
I0402 14:59:55.341385 3233886 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 14:59:56.588172 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 30 in 0.8395588397979736s
I0402 15:00:00.511273 3233954 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 15:00:00.511362 3233954 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 15:00:00.511400 3233954 utils.py:162] NumExpr defaulting to 16 threads.
I0402 15:00:00.833761 3233954 config.py:54] PyTorch version 2.6.0 available.
W0402 15:00:01.033111 3233954 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 15:00:01.679292 3233954 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 15:00:01.683590 3231795 quantize_finetune_llama.py:214] layer 31 gpu 3
I0402 15:00:01.918002 3233954 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 15:00:03.171601 3231795 quantize_finetune_llama.py:245] computed original embedding for layer 31 in 1.019219160079956s
I0402 15:00:07.128803 3234022 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 15:00:07.128895 3234022 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 15:00:07.128936 3234022 utils.py:162] NumExpr defaulting to 16 threads.
I0402 15:00:07.505990 3234022 config.py:54] PyTorch version 2.6.0 available.
W0402 15:00:07.720352 3234022 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0402 15:00:08.375041 3234022 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0402 15:00:08.611302 3234022 data_utils.py:336] using 256 training seqs, 128 validation seqs
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_v proxy err 0.002128256019204855 tr(WHW.T) 7077.5078125
bpp_loss 4.647335767745972
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_q proxy err 0.0005795261240564287 tr(WHW.T) 27002.65625
bpp_loss 4.751063108444214
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_k proxy err 0.00042551758815534413 tr(WHW.T) 37243.53125
bpp_loss 4.768657684326172
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
28_o proxy err 0.0014616699190810323 tr(WHW.T) 8884.01171875
bpp_loss 4.692949533462524
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
28_up proxy err 0.001711438875645399 tr(WHW.T) 26161.169921875
bpp_loss 4.507682977720749
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
28_gate proxy err 0.0012270071310922503 tr(WHW.T) 36794.0625
bpp_loss 4.579870889353198
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
28_down proxy err 0.0020086129661649466 tr(WHW.T) 14984.466796875
bpp_loss 4.639631581860919
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_v proxy err 0.002224845578894019 tr(WHW.T) 6682.36328125
bpp_loss 4.657835006713867
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_q proxy err 0.0005739820189774036 tr(WHW.T) 27006.056640625
bpp_loss 4.713114023208618
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_k proxy err 0.0003996596788056195 tr(WHW.T) 39489.33984375
bpp_loss 4.722820520401001
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
29_o proxy err 0.0013900792691856623 tr(WHW.T) 10599.048828125
bpp_loss 4.6669371128082275
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
29_up proxy err 0.0013623732374981046 tr(WHW.T) 32884.11328125
bpp_loss 4.519652078318042
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
29_gate proxy err 0.0011277684243395925 tr(WHW.T) 39942.984375
bpp_loss 4.583856981854106
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
29_down proxy err 0.0018274019239470363 tr(WHW.T) 14732.544921875
bpp_loss 4.673488239909327
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_v proxy err 0.001911734463647008 tr(WHW.T) 8207.525390625
bpp_loss 4.671957969665527
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_q proxy err 0.0005711871781386435 tr(WHW.T) 28540.68359375
bpp_loss 4.703515529632568
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_k proxy err 0.0004291575460229069 tr(WHW.T) 38445.2421875
bpp_loss 4.723242282867432
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
30_o proxy err 0.001256713061593473 tr(WHW.T) 10163.1123046875
bpp_loss 4.751617908477783
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
30_up proxy err 0.0008375839097425342 tr(WHW.T) 53873.95703125
bpp_loss 4.541169454885083
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
30_gate proxy err 0.0007678833208046854 tr(WHW.T) 59167.79296875
bpp_loss 4.616200646688772
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
30_down proxy err 0.0006298801163211465 tr(WHW.T) 25842.498046875
bpp_loss 4.759541955105094
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_v proxy err 0.0023104373831301928 tr(WHW.T) 6740.33837890625
bpp_loss 4.544708251953125
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_q proxy err 0.00045105276512913406 tr(WHW.T) 36698.0
bpp_loss 4.719304800033569
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_k proxy err 0.000308272079564631 tr(WHW.T) 54793.80859375
bpp_loss 4.775662899017334
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
31_o proxy err 0.0007665702141821384 tr(WHW.T) 13121.5947265625
bpp_loss 4.697741985321045
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
31_up proxy err 0.00047463507507927716 tr(WHW.T) 95778.3046875
bpp_loss 4.593903475029524
weight_block torch.Size([2048, 688, 16])
weight_block torch.Size([2048, 688, 16])
31_gate proxy err 0.000467333709821105 tr(WHW.T) 97550.4453125
bpp_loss 4.681926461153252
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([2048, 256, 16])
weight_block torch.Size([768, 256, 16])
31_down proxy err 0.00029875358450226486 tr(WHW.T) 37026.6875
bpp_loss 4.812281364618346
I0402 15:01:14.145826 3234090 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 15:01:14.145976 3234090 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 15:01:14.146014 3234090 utils.py:162] NumExpr defaulting to 16 threads.
I0402 15:01:14.535782 3234090 config.py:54] PyTorch version 2.6.0 available.
W0402 15:01:14.737187 3234090 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0402 15:01:14.855636 3234090 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  3.63it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  4.11it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  4.38it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  4.72it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  5.20it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.76it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  4.63it/s]
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  5.51it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.12it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  7.83it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.31it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.61it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.66it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.13it/s]
I0402 15:01:18.102508 3234090 hfize_llama.py:153] loaded layer 0
I0402 15:01:19.034015 3234090 hfize_llama.py:153] loaded layer 1
I0402 15:01:19.890922 3234090 hfize_llama.py:153] loaded layer 2
I0402 15:01:20.757328 3234090 hfize_llama.py:153] loaded layer 3
I0402 15:01:21.605789 3234090 hfize_llama.py:153] loaded layer 4
I0402 15:01:22.453654 3234090 hfize_llama.py:153] loaded layer 5
I0402 15:01:23.320045 3234090 hfize_llama.py:153] loaded layer 6
I0402 15:01:24.171886 3234090 hfize_llama.py:153] loaded layer 7
I0402 15:01:25.029173 3234090 hfize_llama.py:153] loaded layer 8
I0402 15:01:25.857194 3234090 hfize_llama.py:153] loaded layer 9
I0402 15:01:26.693886 3234090 hfize_llama.py:153] loaded layer 10
I0402 15:01:27.564609 3234090 hfize_llama.py:153] loaded layer 11
I0402 15:01:28.422298 3234090 hfize_llama.py:153] loaded layer 12
I0402 15:01:29.263538 3234090 hfize_llama.py:153] loaded layer 13
I0402 15:01:30.108593 3234090 hfize_llama.py:153] loaded layer 14
I0402 15:01:30.948266 3234090 hfize_llama.py:153] loaded layer 15
I0402 15:01:31.780606 3234090 hfize_llama.py:153] loaded layer 16
I0402 15:01:32.622754 3234090 hfize_llama.py:153] loaded layer 17
I0402 15:01:33.446462 3234090 hfize_llama.py:153] loaded layer 18
I0402 15:01:34.274655 3234090 hfize_llama.py:153] loaded layer 19
I0402 15:01:35.077668 3234090 hfize_llama.py:153] loaded layer 20
I0402 15:01:35.876064 3234090 hfize_llama.py:153] loaded layer 21
I0402 15:01:36.687334 3234090 hfize_llama.py:153] loaded layer 22
I0402 15:01:37.494794 3234090 hfize_llama.py:153] loaded layer 23
I0402 15:01:38.289608 3234090 hfize_llama.py:153] loaded layer 24
I0402 15:01:39.112394 3234090 hfize_llama.py:153] loaded layer 25
I0402 15:01:39.944665 3234090 hfize_llama.py:153] loaded layer 26
I0402 15:01:40.772742 3234090 hfize_llama.py:153] loaded layer 27
I0402 15:01:41.586202 3234090 hfize_llama.py:153] loaded layer 28
I0402 15:01:42.403670 3234090 hfize_llama.py:153] loaded layer 29
I0402 15:01:43.217049 3234090 hfize_llama.py:153] loaded layer 30
I0402 15:01:44.064588 3234090 hfize_llama.py:153] loaded layer 31
I0402 15:01:44.064700 3234090 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.02s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.12it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.20it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.24it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.47it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.31it/s]
I0402 15:02:16.995000 3234090 hfize_llama.py:167] successfully loaded hfized model
I0402 15:02:21.477355 3234308 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 15:02:21.477510 3234308 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 15:02:21.477551 3234308 utils.py:162] NumExpr defaulting to 16 threads.
W0402 15:02:21.944655 3234308 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0402 15:02:22.311971 3234308 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.01it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.02it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:02<00:02,  1.04it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:03<00:01,  1.06it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:04<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.14it/s]
I0402 15:02:27.693435 3234308 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/166 [00:00<?, ?it/s]avg_loss = 1.3655925989151:   0%|          | 0/166 [00:01<?, ?it/s]avg_loss = 1.3655925989151:   1%|          | 1/166 [00:01<04:23,  1.60s/it]avg_loss = 1.6321094632148743:   1%|          | 1/166 [00:02<04:23,  1.60s/it]avg_loss = 1.6321094632148743:   1%|          | 2/166 [00:02<03:43,  1.36s/it]avg_loss = 1.79675296942393:   1%|          | 2/166 [00:04<03:43,  1.36s/it]  avg_loss = 1.79675296942393:   2%|▏         | 3/166 [00:04<03:30,  1.29s/it]avg_loss = 1.827958106994629:   2%|▏         | 3/166 [00:05<03:30,  1.29s/it]avg_loss = 1.827958106994629:   2%|▏         | 4/166 [00:05<03:24,  1.26s/it]avg_loss = 1.7614055633544923:   2%|▏         | 4/166 [00:06<03:24,  1.26s/it]avg_loss = 1.7614055633544923:   3%|▎         | 5/166 [00:06<03:20,  1.24s/it]avg_loss = 1.7377322316169739:   3%|▎         | 5/166 [00:07<03:20,  1.24s/it]avg_loss = 1.7377322316169739:   4%|▎         | 6/166 [00:07<03:17,  1.23s/it]avg_loss = 1.677320156778608:   4%|▎         | 6/166 [00:08<03:17,  1.23s/it] avg_loss = 1.677320156778608:   4%|▍         | 7/166 [00:08<03:15,  1.23s/it]avg_loss = 1.619914948940277:   4%|▍         | 7/166 [00:10<03:15,  1.23s/it]avg_loss = 1.619914948940277:   5%|▍         | 8/166 [00:10<03:13,  1.22s/it]avg_loss = 1.6151425043741863:   5%|▍         | 8/166 [00:11<03:13,  1.22s/it]avg_loss = 1.6151425043741863:   5%|▌         | 9/166 [00:11<03:12,  1.22s/it]avg_loss = 1.6213433146476746:   5%|▌         | 9/166 [00:12<03:12,  1.22s/it]avg_loss = 1.6213433146476746:   6%|▌         | 10/166 [00:12<03:10,  1.22s/it]avg_loss = 1.6381223743612117:   6%|▌         | 10/166 [00:13<03:10,  1.22s/it]avg_loss = 1.6381223743612117:   7%|▋         | 11/166 [00:13<03:09,  1.22s/it]avg_loss = 1.6480615437030792:   7%|▋         | 11/166 [00:14<03:09,  1.22s/it]avg_loss = 1.6480615437030792:   7%|▋         | 12/166 [00:14<03:08,  1.23s/it]avg_loss = 1.6439417508932261:   7%|▋         | 12/166 [00:16<03:08,  1.23s/it]avg_loss = 1.6439417508932261:   8%|▊         | 13/166 [00:16<03:07,  1.23s/it]avg_loss = 1.6567716087613786:   8%|▊         | 13/166 [00:17<03:07,  1.23s/it]avg_loss = 1.6567716087613786:   8%|▊         | 14/166 [00:17<03:06,  1.23s/it]avg_loss = 1.6745468139648438:   8%|▊         | 14/166 [00:18<03:06,  1.23s/it]avg_loss = 1.6745468139648438:   9%|▉         | 15/166 [00:18<03:05,  1.23s/it]avg_loss = 1.6942612901329994:   9%|▉         | 15/166 [00:19<03:05,  1.23s/it]avg_loss = 1.6942612901329994:  10%|▉         | 16/166 [00:19<03:04,  1.23s/it]avg_loss = 1.7078876425238216:  10%|▉         | 16/166 [00:21<03:04,  1.23s/it]avg_loss = 1.7078876425238216:  10%|█         | 17/166 [00:21<03:03,  1.23s/it]avg_loss = 1.722709318002065:  10%|█         | 17/166 [00:22<03:03,  1.23s/it] avg_loss = 1.722709318002065:  11%|█         | 18/166 [00:22<03:03,  1.24s/it]avg_loss = 1.7426218923769499:  11%|█         | 18/166 [00:23<03:03,  1.24s/it]avg_loss = 1.7426218923769499:  11%|█▏        | 19/166 [00:23<03:02,  1.24s/it]avg_loss = 1.7491102933883667:  11%|█▏        | 19/166 [00:24<03:02,  1.24s/it]avg_loss = 1.7491102933883667:  12%|█▏        | 20/166 [00:24<03:01,  1.24s/it]avg_loss = 1.7499334585098993:  12%|█▏        | 20/166 [00:26<03:01,  1.24s/it]avg_loss = 1.7499334585098993:  13%|█▎        | 21/166 [00:26<03:00,  1.24s/it]avg_loss = 1.7400545206936924:  13%|█▎        | 21/166 [00:27<03:00,  1.24s/it]avg_loss = 1.7400545206936924:  13%|█▎        | 22/166 [00:27<02:59,  1.24s/it]avg_loss = 1.7291333519894143:  13%|█▎        | 22/166 [00:28<02:59,  1.24s/it]avg_loss = 1.7291333519894143:  14%|█▍        | 23/166 [00:28<02:58,  1.25s/it]avg_loss = 1.736560195684433:  14%|█▍        | 23/166 [00:29<02:58,  1.25s/it] avg_loss = 1.736560195684433:  14%|█▍        | 24/166 [00:29<02:57,  1.25s/it]avg_loss = 1.7440407800674438:  14%|█▍        | 24/166 [00:31<02:57,  1.25s/it]avg_loss = 1.7440407800674438:  15%|█▌        | 25/166 [00:31<02:56,  1.25s/it]avg_loss = 1.7488174851124103:  15%|█▌        | 25/166 [00:32<02:56,  1.25s/it]avg_loss = 1.7488174851124103:  16%|█▌        | 26/166 [00:32<02:55,  1.25s/it]avg_loss = 1.7555715243021648:  16%|█▌        | 26/166 [00:33<02:55,  1.25s/it]avg_loss = 1.7555715243021648:  16%|█▋        | 27/166 [00:33<02:54,  1.25s/it]avg_loss = 1.758726647921971:  16%|█▋        | 27/166 [00:34<02:54,  1.25s/it] avg_loss = 1.758726647921971:  17%|█▋        | 28/166 [00:34<02:53,  1.25s/it]avg_loss = 1.7683966899740284:  17%|█▋        | 28/166 [00:36<02:53,  1.25s/it]avg_loss = 1.7683966899740284:  17%|█▋        | 29/166 [00:36<02:51,  1.26s/it]avg_loss = 1.7684146682421367:  17%|█▋        | 29/166 [00:37<02:51,  1.26s/it]avg_loss = 1.7684146682421367:  18%|█▊        | 30/166 [00:37<02:50,  1.26s/it]avg_loss = 1.782548446809092:  18%|█▊        | 30/166 [00:38<02:50,  1.26s/it] avg_loss = 1.782548446809092:  19%|█▊        | 31/166 [00:38<02:49,  1.26s/it]avg_loss = 1.789053700864315:  19%|█▊        | 31/166 [00:39<02:49,  1.26s/it]avg_loss = 1.789053700864315:  19%|█▉        | 32/166 [00:39<02:48,  1.26s/it]avg_loss = 1.794069969292843:  19%|█▉        | 32/166 [00:41<02:48,  1.26s/it]avg_loss = 1.794069969292843:  20%|█▉        | 33/166 [00:41<02:47,  1.26s/it]avg_loss = 1.7932216595200932:  20%|█▉        | 33/166 [00:42<02:47,  1.26s/it]avg_loss = 1.7932216595200932:  20%|██        | 34/166 [00:42<02:46,  1.26s/it]avg_loss = 1.786930707522801:  20%|██        | 34/166 [00:43<02:46,  1.26s/it] avg_loss = 1.786930707522801:  21%|██        | 35/166 [00:43<02:45,  1.26s/it]avg_loss = 1.7784001065625086:  21%|██        | 35/166 [00:45<02:45,  1.26s/it]avg_loss = 1.7784001065625086:  22%|██▏       | 36/166 [00:45<02:44,  1.27s/it]avg_loss = 1.7682889925467002:  22%|██▏       | 36/166 [00:46<02:44,  1.27s/it]avg_loss = 1.7682889925467002:  22%|██▏       | 37/166 [00:46<02:43,  1.27s/it]avg_loss = 1.76561418018843:  22%|██▏       | 37/166 [00:47<02:43,  1.27s/it]  avg_loss = 1.76561418018843:  23%|██▎       | 38/166 [00:47<02:42,  1.27s/it]avg_loss = 1.7634730033385448:  23%|██▎       | 38/166 [00:48<02:42,  1.27s/it]avg_loss = 1.7634730033385448:  23%|██▎       | 39/166 [00:48<02:41,  1.27s/it]avg_loss = 1.7669345378875732:  23%|██▎       | 39/166 [00:50<02:41,  1.27s/it]avg_loss = 1.7669345378875732:  24%|██▍       | 40/166 [00:50<02:40,  1.27s/it]avg_loss = 1.7668203784198295:  24%|██▍       | 40/166 [00:51<02:40,  1.27s/it]avg_loss = 1.7668203784198295:  25%|██▍       | 41/166 [00:51<02:39,  1.27s/it]avg_loss = 1.7543354062806993:  25%|██▍       | 41/166 [00:52<02:39,  1.27s/it]avg_loss = 1.7543354062806993:  25%|██▌       | 42/166 [00:52<02:37,  1.27s/it]avg_loss = 1.738818922708201:  25%|██▌       | 42/166 [00:53<02:37,  1.27s/it] avg_loss = 1.738818922708201:  26%|██▌       | 43/166 [00:53<02:36,  1.28s/it]avg_loss = 1.7285396267067303:  26%|██▌       | 43/166 [00:55<02:36,  1.28s/it]avg_loss = 1.7285396267067303:  27%|██▋       | 44/166 [00:55<02:35,  1.28s/it]avg_loss = 1.7149664772881401:  27%|██▋       | 44/166 [00:56<02:35,  1.28s/it]avg_loss = 1.7149664772881401:  27%|██▋       | 45/166 [00:56<02:34,  1.28s/it]avg_loss = 1.704560759274856:  27%|██▋       | 45/166 [00:57<02:34,  1.28s/it] avg_loss = 1.704560759274856:  28%|██▊       | 46/166 [00:57<02:33,  1.28s/it]avg_loss = 1.6976023379792557:  28%|██▊       | 46/166 [00:59<02:33,  1.28s/it]avg_loss = 1.6976023379792557:  28%|██▊       | 47/166 [00:59<02:32,  1.28s/it]avg_loss = 1.698545182744662:  28%|██▊       | 47/166 [01:00<02:32,  1.28s/it] avg_loss = 1.698545182744662:  29%|██▉       | 48/166 [01:00<02:31,  1.28s/it]avg_loss = 1.709284091482357:  29%|██▉       | 48/166 [01:01<02:31,  1.28s/it]avg_loss = 1.709284091482357:  30%|██▉       | 49/166 [01:01<02:29,  1.28s/it]avg_loss = 1.7199563121795653:  30%|██▉       | 49/166 [01:02<02:29,  1.28s/it]avg_loss = 1.7199563121795653:  30%|███       | 50/166 [01:02<02:28,  1.28s/it]avg_loss = 1.7268915643879013:  30%|███       | 50/166 [01:04<02:28,  1.28s/it]avg_loss = 1.7268915643879013:  31%|███       | 51/166 [01:04<02:27,  1.28s/it]avg_loss = 1.7319506039986243:  31%|███       | 51/166 [01:05<02:27,  1.28s/it]avg_loss = 1.7319506039986243:  31%|███▏      | 52/166 [01:05<02:26,  1.28s/it]avg_loss = 1.7352229941566035:  31%|███▏      | 52/166 [01:06<02:26,  1.28s/it]avg_loss = 1.7352229941566035:  32%|███▏      | 53/166 [01:06<02:25,  1.29s/it]avg_loss = 1.7362063549183033:  32%|███▏      | 53/166 [01:08<02:25,  1.29s/it]avg_loss = 1.7362063549183033:  33%|███▎      | 54/166 [01:08<02:24,  1.29s/it]avg_loss = 1.7388621720400723:  33%|███▎      | 54/166 [01:09<02:24,  1.29s/it]avg_loss = 1.7388621720400723:  33%|███▎      | 55/166 [01:09<02:22,  1.29s/it]avg_loss = 1.7423615093742097:  33%|███▎      | 55/166 [01:10<02:22,  1.29s/it]avg_loss = 1.7423615093742097:  34%|███▎      | 56/166 [01:10<02:21,  1.29s/it]avg_loss = 1.7374196178034733:  34%|███▎      | 56/166 [01:11<02:21,  1.29s/it]avg_loss = 1.7374196178034733:  34%|███▍      | 57/166 [01:11<02:20,  1.29s/it]avg_loss = 1.741043277855577:  34%|███▍      | 57/166 [01:13<02:20,  1.29s/it] avg_loss = 1.741043277855577:  35%|███▍      | 58/166 [01:13<02:19,  1.29s/it]avg_loss = 1.7393267680022677:  35%|███▍      | 58/166 [01:14<02:19,  1.29s/it]avg_loss = 1.7393267680022677:  36%|███▌      | 59/166 [01:14<02:18,  1.29s/it]avg_loss = 1.734448136885961:  36%|███▌      | 59/166 [01:15<02:18,  1.29s/it] avg_loss = 1.734448136885961:  36%|███▌      | 60/166 [01:15<02:16,  1.29s/it]avg_loss = 1.7300921717628104:  36%|███▌      | 60/166 [01:17<02:16,  1.29s/it]avg_loss = 1.7300921717628104:  37%|███▋      | 61/166 [01:17<02:15,  1.29s/it]avg_loss = 1.7261866004236284:  37%|███▋      | 61/166 [01:18<02:15,  1.29s/it]avg_loss = 1.7261866004236284:  37%|███▋      | 62/166 [01:18<02:14,  1.29s/it]avg_loss = 1.72024000637115:  37%|███▋      | 62/166 [01:19<02:14,  1.29s/it]  avg_loss = 1.72024000637115:  38%|███▊      | 63/166 [01:19<02:13,  1.29s/it]avg_loss = 1.7159849517047405:  38%|███▊      | 63/166 [01:20<02:13,  1.29s/it]avg_loss = 1.7159849517047405:  39%|███▊      | 64/166 [01:20<02:12,  1.29s/it]avg_loss = 1.7092339589045598:  39%|███▊      | 64/166 [01:22<02:12,  1.29s/it]avg_loss = 1.7092339589045598:  39%|███▉      | 65/166 [01:22<02:10,  1.30s/it]avg_loss = 1.7020610932147864:  39%|███▉      | 65/166 [01:23<02:10,  1.30s/it]avg_loss = 1.7020610932147864:  40%|███▉      | 66/166 [01:23<02:09,  1.30s/it]avg_loss = 1.6961062808534992:  40%|███▉      | 66/166 [01:24<02:09,  1.30s/it]avg_loss = 1.6961062808534992:  40%|████      | 67/166 [01:24<02:08,  1.30s/it]avg_loss = 1.694958557100857:  40%|████      | 67/166 [01:26<02:08,  1.30s/it] avg_loss = 1.694958557100857:  41%|████      | 68/166 [01:26<02:07,  1.30s/it]avg_loss = 1.6968508302301601:  41%|████      | 68/166 [01:27<02:07,  1.30s/it]avg_loss = 1.6968508302301601:  42%|████▏     | 69/166 [01:27<02:06,  1.30s/it]avg_loss = 1.6998027154377529:  42%|████▏     | 69/166 [01:28<02:06,  1.30s/it]avg_loss = 1.6998027154377529:  42%|████▏     | 70/166 [01:28<02:04,  1.30s/it]avg_loss = 1.7038095014196046:  42%|████▏     | 70/166 [01:30<02:04,  1.30s/it]avg_loss = 1.7038095014196046:  43%|████▎     | 71/166 [01:30<02:03,  1.30s/it]avg_loss = 1.7086617334021463:  43%|████▎     | 71/166 [01:31<02:03,  1.30s/it]avg_loss = 1.7086617334021463:  43%|████▎     | 72/166 [01:31<02:02,  1.30s/it]avg_loss = 1.7147674250276121:  43%|████▎     | 72/166 [01:32<02:02,  1.30s/it]avg_loss = 1.7147674250276121:  44%|████▍     | 73/166 [01:32<02:00,  1.30s/it]avg_loss = 1.70907038933522:  44%|████▍     | 73/166 [01:33<02:00,  1.30s/it]  avg_loss = 1.70907038933522:  45%|████▍     | 74/166 [01:33<01:59,  1.30s/it]avg_loss = 1.7045930830637614:  45%|████▍     | 74/166 [01:35<01:59,  1.30s/it]avg_loss = 1.7045930830637614:  45%|████▌     | 75/166 [01:35<01:58,  1.30s/it]avg_loss = 1.7037502323326312:  45%|████▌     | 75/166 [01:36<01:58,  1.30s/it]avg_loss = 1.7037502323326312:  46%|████▌     | 76/166 [01:36<01:56,  1.30s/it]avg_loss = 1.7002551338889382:  46%|████▌     | 76/166 [01:37<01:56,  1.30s/it]avg_loss = 1.7002551338889382:  46%|████▋     | 77/166 [01:37<01:55,  1.30s/it]avg_loss = 1.69665754147065:  46%|████▋     | 77/166 [01:39<01:55,  1.30s/it]  avg_loss = 1.69665754147065:  47%|████▋     | 78/166 [01:39<01:54,  1.30s/it]avg_loss = 1.694034534164622:  47%|████▋     | 78/166 [01:40<01:54,  1.30s/it]avg_loss = 1.694034534164622:  48%|████▊     | 79/166 [01:40<01:53,  1.30s/it]avg_loss = 1.690567022562027:  48%|████▊     | 79/166 [01:41<01:53,  1.30s/it]avg_loss = 1.690567022562027:  48%|████▊     | 80/166 [01:41<01:51,  1.30s/it]avg_loss = 1.6812521322273914:  48%|████▊     | 80/166 [01:43<01:51,  1.30s/it]avg_loss = 1.6812521322273914:  49%|████▉     | 81/166 [01:43<01:50,  1.30s/it]avg_loss = 1.682980924117856:  49%|████▉     | 81/166 [01:44<01:50,  1.30s/it] avg_loss = 1.682980924117856:  49%|████▉     | 82/166 [01:44<01:49,  1.30s/it]avg_loss = 1.6849475251622947:  49%|████▉     | 82/166 [01:45<01:49,  1.30s/it]avg_loss = 1.6849475251622947:  50%|█████     | 83/166 [01:45<01:48,  1.30s/it]avg_loss = 1.6879772345225017:  50%|█████     | 83/166 [01:47<01:48,  1.30s/it]avg_loss = 1.6879772345225017:  51%|█████     | 84/166 [01:47<01:46,  1.30s/it]avg_loss = 1.6897699720719281:  51%|█████     | 84/166 [01:48<01:46,  1.30s/it]avg_loss = 1.6897699720719281:  51%|█████     | 85/166 [01:48<01:45,  1.30s/it]avg_loss = 1.6887494450391725:  51%|█████     | 85/166 [01:49<01:45,  1.30s/it]avg_loss = 1.6887494450391725:  52%|█████▏    | 86/166 [01:49<01:44,  1.30s/it]avg_loss = 1.689082004558081:  52%|█████▏    | 86/166 [01:50<01:44,  1.30s/it] avg_loss = 1.689082004558081:  52%|█████▏    | 87/166 [01:50<01:43,  1.31s/it]avg_loss = 1.6893122832883487:  52%|█████▏    | 87/166 [01:52<01:43,  1.31s/it]avg_loss = 1.6893122832883487:  53%|█████▎    | 88/166 [01:52<01:41,  1.30s/it]avg_loss = 1.6905524690499467:  53%|█████▎    | 88/166 [01:53<01:41,  1.30s/it]avg_loss = 1.6905524690499467:  54%|█████▎    | 89/166 [01:53<01:40,  1.31s/it]avg_loss = 1.6903771029578314:  54%|█████▎    | 89/166 [01:54<01:40,  1.31s/it]avg_loss = 1.6903771029578314:  54%|█████▍    | 90/166 [01:54<01:39,  1.31s/it]avg_loss = 1.6908479182274787:  54%|█████▍    | 90/166 [01:56<01:39,  1.31s/it]avg_loss = 1.6908479182274787:  55%|█████▍    | 91/166 [01:56<01:37,  1.31s/it]avg_loss = 1.6919717749823695:  55%|█████▍    | 91/166 [01:57<01:37,  1.31s/it]avg_loss = 1.6919717749823695:  55%|█████▌    | 92/166 [01:57<01:36,  1.31s/it]avg_loss = 1.6959039254855084:  55%|█████▌    | 92/166 [01:58<01:36,  1.31s/it]avg_loss = 1.6959039254855084:  56%|█████▌    | 93/166 [01:58<01:35,  1.31s/it]avg_loss = 1.695028431872104:  56%|█████▌    | 93/166 [02:00<01:35,  1.31s/it] avg_loss = 1.695028431872104:  57%|█████▋    | 94/166 [02:00<01:33,  1.31s/it]avg_loss = 1.6943254069278115:  57%|█████▋    | 94/166 [02:01<01:33,  1.31s/it]avg_loss = 1.6943254069278115:  57%|█████▋    | 95/166 [02:01<01:32,  1.31s/it]avg_loss = 1.6940336947639782:  57%|█████▋    | 95/166 [02:02<01:32,  1.31s/it]avg_loss = 1.6940336947639782:  58%|█████▊    | 96/166 [02:02<01:31,  1.31s/it]avg_loss = 1.693955206379448:  58%|█████▊    | 96/166 [02:03<01:31,  1.31s/it] avg_loss = 1.693955206379448:  58%|█████▊    | 97/166 [02:03<01:30,  1.31s/it]avg_loss = 1.6922661139040578:  58%|█████▊    | 97/166 [02:05<01:30,  1.31s/it]avg_loss = 1.6922661139040578:  59%|█████▉    | 98/166 [02:05<01:28,  1.31s/it]avg_loss = 1.6898492622857142:  59%|█████▉    | 98/166 [02:06<01:28,  1.31s/it]avg_loss = 1.6898492622857142:  60%|█████▉    | 99/166 [02:06<01:27,  1.31s/it]avg_loss = 1.6871560525894165:  60%|█████▉    | 99/166 [02:07<01:27,  1.31s/it]avg_loss = 1.6871560525894165:  60%|██████    | 100/166 [02:07<01:26,  1.31s/it]avg_loss = 1.6876015191030975:  60%|██████    | 100/166 [02:09<01:26,  1.31s/it]avg_loss = 1.6876015191030975:  61%|██████    | 101/166 [02:09<01:25,  1.31s/it]avg_loss = 1.6885807046703263:  61%|██████    | 101/166 [02:10<01:25,  1.31s/it]avg_loss = 1.6885807046703263:  61%|██████▏   | 102/166 [02:10<01:23,  1.31s/it]avg_loss = 1.689672688836033:  61%|██████▏   | 102/166 [02:11<01:23,  1.31s/it] avg_loss = 1.689672688836033:  62%|██████▏   | 103/166 [02:11<01:22,  1.31s/it]avg_loss = 1.691869196983484:  62%|██████▏   | 103/166 [02:13<01:22,  1.31s/it]avg_loss = 1.691869196983484:  63%|██████▎   | 104/166 [02:13<01:21,  1.31s/it]avg_loss = 1.6985294137682234:  63%|██████▎   | 104/166 [02:14<01:21,  1.31s/it]avg_loss = 1.6985294137682234:  63%|██████▎   | 105/166 [02:14<01:19,  1.31s/it]avg_loss = 1.7037274477616795:  63%|██████▎   | 105/166 [02:15<01:19,  1.31s/it]avg_loss = 1.7037274477616795:  64%|██████▍   | 106/166 [02:15<01:18,  1.31s/it]avg_loss = 1.7073260913385409:  64%|██████▍   | 106/166 [02:17<01:18,  1.31s/it]avg_loss = 1.7073260913385409:  64%|██████▍   | 107/166 [02:17<01:17,  1.31s/it]avg_loss = 1.7105010725833751:  64%|██████▍   | 107/166 [02:18<01:17,  1.31s/it]avg_loss = 1.7105010725833751:  65%|██████▌   | 108/166 [02:18<01:15,  1.31s/it]avg_loss = 1.7152067083831226:  65%|██████▌   | 108/166 [02:19<01:15,  1.31s/it]avg_loss = 1.7152067083831226:  66%|██████▌   | 109/166 [02:19<01:14,  1.31s/it]avg_loss = 1.7186673792925748:  66%|██████▌   | 109/166 [02:21<01:14,  1.31s/it]avg_loss = 1.7186673792925748:  66%|██████▋   | 110/166 [02:21<01:13,  1.31s/it]avg_loss = 1.7202091786238525:  66%|██████▋   | 110/166 [02:22<01:13,  1.31s/it]avg_loss = 1.7202091786238525:  67%|██████▋   | 111/166 [02:22<01:12,  1.31s/it]avg_loss = 1.7214867355568069:  67%|██████▋   | 111/166 [02:23<01:12,  1.31s/it]avg_loss = 1.7214867355568069:  67%|██████▋   | 112/166 [02:23<01:10,  1.31s/it]avg_loss = 1.7218334622087732:  67%|██████▋   | 112/166 [02:24<01:10,  1.31s/it]avg_loss = 1.7218334622087732:  68%|██████▊   | 113/166 [02:24<01:09,  1.31s/it]avg_loss = 1.72322814715536:  68%|██████▊   | 113/166 [02:26<01:09,  1.31s/it]  avg_loss = 1.72322814715536:  69%|██████▊   | 114/166 [02:26<01:08,  1.31s/it]avg_loss = 1.7201090045597243:  69%|██████▊   | 114/166 [02:27<01:08,  1.31s/it]avg_loss = 1.7201090045597243:  69%|██████▉   | 115/166 [02:27<01:06,  1.31s/it]avg_loss = 1.7193529944995354:  69%|██████▉   | 115/166 [02:28<01:06,  1.31s/it]avg_loss = 1.7193529944995354:  70%|██████▉   | 116/166 [02:28<01:05,  1.31s/it]avg_loss = 1.7203031849657369:  70%|██████▉   | 116/166 [02:30<01:05,  1.31s/it]avg_loss = 1.7203031849657369:  70%|███████   | 117/166 [02:30<01:04,  1.31s/it]avg_loss = 1.7204095498990204:  70%|███████   | 117/166 [02:31<01:04,  1.31s/it]avg_loss = 1.7204095498990204:  71%|███████   | 118/166 [02:31<01:03,  1.31s/it]avg_loss = 1.7197469492920308:  71%|███████   | 118/166 [02:32<01:03,  1.31s/it]avg_loss = 1.7197469492920308:  72%|███████▏  | 119/166 [02:32<01:01,  1.31s/it]avg_loss = 1.7203292777140935:  72%|███████▏  | 119/166 [02:34<01:01,  1.31s/it]avg_loss = 1.7203292777140935:  72%|███████▏  | 120/166 [02:34<01:00,  1.31s/it]avg_loss = 1.7196456794896402:  72%|███████▏  | 120/166 [02:35<01:00,  1.31s/it]avg_loss = 1.7196456794896402:  73%|███████▎  | 121/166 [02:35<00:59,  1.31s/it]avg_loss = 1.7199298377896919:  73%|███████▎  | 121/166 [02:36<00:59,  1.31s/it]avg_loss = 1.7199298377896919:  73%|███████▎  | 122/166 [02:36<00:57,  1.31s/it]avg_loss = 1.7201476678615664:  73%|███████▎  | 122/166 [02:38<00:57,  1.31s/it]avg_loss = 1.7201476678615664:  74%|███████▍  | 123/166 [02:38<00:56,  1.31s/it]avg_loss = 1.7186627320704921:  74%|███████▍  | 123/166 [02:39<00:56,  1.31s/it]avg_loss = 1.7186627320704921:  75%|███████▍  | 124/166 [02:39<00:55,  1.31s/it]avg_loss = 1.7169584121704102:  75%|███████▍  | 124/166 [02:40<00:55,  1.31s/it]avg_loss = 1.7169584121704102:  75%|███████▌  | 125/166 [02:40<00:53,  1.31s/it]avg_loss = 1.7147181790972512:  75%|███████▌  | 125/166 [02:42<00:53,  1.31s/it]avg_loss = 1.7147181790972512:  76%|███████▌  | 126/166 [02:42<00:52,  1.31s/it]avg_loss = 1.7124850834448506:  76%|███████▌  | 126/166 [02:43<00:52,  1.31s/it]avg_loss = 1.7124850834448506:  77%|███████▋  | 127/166 [02:43<00:51,  1.31s/it]avg_loss = 1.710994465276599:  77%|███████▋  | 127/166 [02:44<00:51,  1.31s/it] avg_loss = 1.710994465276599:  77%|███████▋  | 128/166 [02:44<00:49,  1.31s/it]avg_loss = 1.7096765290859133:  77%|███████▋  | 128/166 [02:45<00:49,  1.31s/it]avg_loss = 1.7096765290859133:  78%|███████▊  | 129/166 [02:45<00:48,  1.31s/it]avg_loss = 1.709582061950977:  78%|███████▊  | 129/166 [02:47<00:48,  1.31s/it] avg_loss = 1.709582061950977:  78%|███████▊  | 130/166 [02:47<00:47,  1.31s/it]avg_loss = 1.710612160558919:  78%|███████▊  | 130/166 [02:48<00:47,  1.31s/it]avg_loss = 1.710612160558919:  79%|███████▉  | 131/166 [02:48<00:46,  1.32s/it]avg_loss = 1.7111687262852986:  79%|███████▉  | 131/166 [02:49<00:46,  1.32s/it]avg_loss = 1.7111687262852986:  80%|███████▉  | 132/166 [02:49<00:44,  1.31s/it]avg_loss = 1.7120863557758188:  80%|███████▉  | 132/166 [02:51<00:44,  1.31s/it]avg_loss = 1.7120863557758188:  80%|████████  | 133/166 [02:51<00:43,  1.32s/it]avg_loss = 1.713415112068404:  80%|████████  | 133/166 [02:52<00:43,  1.32s/it] avg_loss = 1.713415112068404:  81%|████████  | 134/166 [02:52<00:42,  1.32s/it]avg_loss = 1.7113660936002377:  81%|████████  | 134/166 [02:53<00:42,  1.32s/it]avg_loss = 1.7113660936002377:  81%|████████▏ | 135/166 [02:53<00:40,  1.31s/it]avg_loss = 1.71163508208359:  81%|████████▏ | 135/166 [02:55<00:40,  1.31s/it]  avg_loss = 1.71163508208359:  82%|████████▏ | 136/166 [02:55<00:39,  1.32s/it]avg_loss = 1.711904017594609:  82%|████████▏ | 136/166 [02:56<00:39,  1.32s/it]avg_loss = 1.711904017594609:  83%|████████▎ | 137/166 [02:56<00:38,  1.32s/it]avg_loss = 1.7126996629480002:  83%|████████▎ | 137/166 [02:57<00:38,  1.32s/it]avg_loss = 1.7126996629480002:  83%|████████▎ | 138/166 [02:57<00:36,  1.32s/it]avg_loss = 1.711840855131904:  83%|████████▎ | 138/166 [02:59<00:36,  1.32s/it] avg_loss = 1.711840855131904:  84%|████████▎ | 139/166 [02:59<00:35,  1.32s/it]avg_loss = 1.7105412219251905:  84%|████████▎ | 139/166 [03:00<00:35,  1.32s/it]avg_loss = 1.7105412219251905:  84%|████████▍ | 140/166 [03:00<00:34,  1.32s/it]avg_loss = 1.70919853406595:  84%|████████▍ | 140/166 [03:01<00:34,  1.32s/it]  avg_loss = 1.70919853406595:  85%|████████▍ | 141/166 [03:01<00:32,  1.32s/it]avg_loss = 1.7087668197255739:  85%|████████▍ | 141/166 [03:03<00:32,  1.32s/it]avg_loss = 1.7087668197255739:  86%|████████▌ | 142/166 [03:03<00:31,  1.32s/it]avg_loss = 1.7071381423856828:  86%|████████▌ | 142/166 [03:04<00:31,  1.32s/it]avg_loss = 1.7071381423856828:  86%|████████▌ | 143/166 [03:04<00:30,  1.32s/it]avg_loss = 1.7083140023880534:  86%|████████▌ | 143/166 [03:05<00:30,  1.32s/it]avg_loss = 1.7083140023880534:  87%|████████▋ | 144/166 [03:05<00:28,  1.32s/it]avg_loss = 1.707567938442888:  87%|████████▋ | 144/166 [03:07<00:28,  1.32s/it] avg_loss = 1.707567938442888:  87%|████████▋ | 145/166 [03:07<00:27,  1.32s/it]avg_loss = 1.7074756042598045:  87%|████████▋ | 145/166 [03:08<00:27,  1.32s/it]avg_loss = 1.7074756042598045:  88%|████████▊ | 146/166 [03:08<00:26,  1.32s/it]avg_loss = 1.706328448795137:  88%|████████▊ | 146/166 [03:09<00:26,  1.32s/it] avg_loss = 1.706328448795137:  89%|████████▊ | 147/166 [03:09<00:25,  1.32s/it]avg_loss = 1.7054140922185537:  89%|████████▊ | 147/166 [03:10<00:25,  1.32s/it]avg_loss = 1.7054140922185537:  89%|████████▉ | 148/166 [03:10<00:23,  1.32s/it]avg_loss = 1.703705846863305:  89%|████████▉ | 148/166 [03:12<00:23,  1.32s/it] avg_loss = 1.703705846863305:  90%|████████▉ | 149/166 [03:12<00:22,  1.32s/it]avg_loss = 1.7046634332338968:  90%|████████▉ | 149/166 [03:13<00:22,  1.32s/it]avg_loss = 1.7046634332338968:  90%|█████████ | 150/166 [03:13<00:21,  1.32s/it]avg_loss = 1.7037963393508204:  90%|█████████ | 150/166 [03:14<00:21,  1.32s/it]avg_loss = 1.7037963393508204:  91%|█████████ | 151/166 [03:14<00:19,  1.32s/it]avg_loss = 1.7035824068282779:  91%|█████████ | 151/166 [03:16<00:19,  1.32s/it]avg_loss = 1.7035824068282779:  92%|█████████▏| 152/166 [03:16<00:18,  1.32s/it]avg_loss = 1.7033903271544213:  92%|█████████▏| 152/166 [03:17<00:18,  1.32s/it]avg_loss = 1.7033903271544213:  92%|█████████▏| 153/166 [03:17<00:17,  1.32s/it]avg_loss = 1.704914017157121:  92%|█████████▏| 153/166 [03:18<00:17,  1.32s/it] avg_loss = 1.704914017157121:  93%|█████████▎| 154/166 [03:18<00:15,  1.32s/it]avg_loss = 1.7044310931236513:  93%|█████████▎| 154/166 [03:20<00:15,  1.32s/it]avg_loss = 1.7044310931236513:  93%|█████████▎| 155/166 [03:20<00:14,  1.32s/it]avg_loss = 1.7043058436650496:  93%|█████████▎| 155/166 [03:21<00:14,  1.32s/it]avg_loss = 1.7043058436650496:  94%|█████████▍| 156/166 [03:21<00:13,  1.32s/it]avg_loss = 1.7025256453046373:  94%|█████████▍| 156/166 [03:22<00:13,  1.32s/it]avg_loss = 1.7025256453046373:  95%|█████████▍| 157/166 [03:22<00:11,  1.32s/it]avg_loss = 1.6982938843437387:  95%|█████████▍| 157/166 [03:24<00:11,  1.32s/it]avg_loss = 1.6982938843437387:  95%|█████████▌| 158/166 [03:24<00:10,  1.32s/it]avg_loss = 1.6991081065351858:  95%|█████████▌| 158/166 [03:25<00:10,  1.32s/it]avg_loss = 1.6991081065351858:  96%|█████████▌| 159/166 [03:25<00:09,  1.32s/it]avg_loss = 1.7005192019045352:  96%|█████████▌| 159/166 [03:26<00:09,  1.32s/it]avg_loss = 1.7005192019045352:  96%|█████████▋| 160/166 [03:26<00:07,  1.32s/it]avg_loss = 1.7028842779420177:  96%|█████████▋| 160/166 [03:28<00:07,  1.32s/it]avg_loss = 1.7028842779420177:  97%|█████████▋| 161/166 [03:28<00:06,  1.32s/it]avg_loss = 1.7029786860501324:  97%|█████████▋| 161/166 [03:29<00:06,  1.32s/it]avg_loss = 1.7029786860501324:  98%|█████████▊| 162/166 [03:29<00:05,  1.32s/it]avg_loss = 1.7025546559526876:  98%|█████████▊| 162/166 [03:30<00:05,  1.32s/it]avg_loss = 1.7025546559526876:  98%|█████████▊| 163/166 [03:30<00:03,  1.32s/it]avg_loss = 1.7031541931919936:  98%|█████████▊| 163/166 [03:32<00:03,  1.32s/it]avg_loss = 1.7031541931919936:  99%|█████████▉| 164/166 [03:32<00:02,  1.32s/it]avg_loss = 1.7032687360590155:  99%|█████████▉| 164/166 [03:33<00:02,  1.32s/it]avg_loss = 1.7032687360590155:  99%|█████████▉| 165/166 [03:33<00:01,  1.32s/it]avg_loss = 1.7052175740161575:  99%|█████████▉| 165/166 [03:34<00:01,  1.32s/it]avg_loss = 1.7052175740161575: 100%|██████████| 166/166 [03:34<00:00,  1.32s/it]avg_loss = 1.7052175740161575: 100%|██████████| 166/166 [03:34<00:00,  1.29s/it]
I0402 15:06:47.275247 3234308 eval_ppl.py:107] wikitext2 perplexity: 5.502583026885986
wikitext2 perplexity: 5.503
