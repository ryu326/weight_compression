I0312 00:00:50.987405 1756366 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.32it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.52it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 10.34it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.36it/s]
I0312 00:00:52.747893 1756366 quantize_finetune_llama.py:134] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:22,  1.37it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:20,  1.45it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:19,  1.48it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:19,  1.46it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:18,  1.46it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:17,  1.45it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:17,  1.47it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:16,  1.47it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:15,  1.45it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:06<00:15,  1.45it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:07<00:14,  1.45it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:13,  1.45it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:08<00:13,  1.45it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:09<00:12,  1.45it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:10<00:11,  1.46it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:10<00:10,  1.46it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:11<00:10,  1.46it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:12<00:09,  1.46it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:08,  1.47it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:13<00:07,  1.50it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:14<00:07,  1.51it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:14<00:06,  1.51it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:15<00:06,  1.48it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:16<00:05,  1.46it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:04,  1.45it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:17<00:04,  1.43it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:18<00:03,  1.42it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:19<00:02,  1.42it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:19<00:02,  1.42it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:20<00:01,  1.42it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:21<00:00,  1.42it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.42it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.45it/s]
I0312 00:01:24.944469 1756366 quantize_finetune_llama.py:159] loaded compression model
I0312 00:01:38.395593 1756366 quantize_finetune_llama.py:163] loaded dataset and devset
I0312 00:01:43.443717 1756366 quantize_finetune_llama.py:183] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:02:47.527744 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 0 in 63.969282150268555s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0312 00:03:06.411170 1756480 config.py:54] PyTorch version 2.1.1 available.
I0312 00:03:07.337360 1756366 quantize_finetune_llama.py:183] layer 1 gpu 1
I0312 00:03:07.402338 1756480 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:03:15.527656 1756480 finetune.py:45] layer 0_v initial loss 5.415984674073115e-07
I0312 00:03:47.711338 1756480 finetune.py:68] layer 0_v @ epoch 0 new loss 2.055208483398019e-07 old loss 5.415984674073115e-07 BETTER
I0312 00:04:16.610123 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 1 in 69.13196682929993s
I0312 00:04:24.103067 1756480 finetune.py:68] layer 0_v @ epoch 1 new loss 9.951413915132434e-08 old loss 2.055208483398019e-07 BETTER
I0312 00:04:27.796072 1756598 config.py:54] PyTorch version 2.1.1 available.
I0312 00:04:28.823420 1756366 quantize_finetune_llama.py:183] layer 2 gpu 2
I0312 00:04:28.908858 1756598 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:04:37.003637 1756598 finetune.py:45] layer 1_v initial loss 1.0443455721542705e-05
I0312 00:04:57.774967 1756480 finetune.py:68] layer 0_v @ epoch 2 new loss 6.185787526646891e-08 old loss 9.951413915132434e-08 BETTER
I0312 00:05:08.058419 1756598 finetune.py:68] layer 1_v @ epoch 0 new loss 4.65170751340338e-06 old loss 1.0443455721542705e-05 BETTER
I0312 00:05:31.822594 1756480 finetune.py:68] layer 0_v @ epoch 3 new loss 4.690610566626674e-08 old loss 6.185787526646891e-08 BETTER
I0312 00:05:39.458136 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 2 in 70.49598002433777s
I0312 00:05:40.016229 1756598 finetune.py:76] layer 1_v @ epoch 1 new loss 5.338584742275998e-06 old loss 4.65170751340338e-06 WORSE
I0312 00:05:50.917621 1756716 config.py:54] PyTorch version 2.1.1 available.
I0312 00:05:51.947638 1756366 quantize_finetune_llama.py:183] layer 3 gpu 3
I0312 00:05:52.015553 1756716 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:06:00.340797 1756716 finetune.py:45] layer 2_v initial loss 5.192221124161733e-06
I0312 00:06:06.374367 1756480 finetune.py:68] layer 0_v @ epoch 4 new loss 4.002814080195094e-08 old loss 4.690610566626674e-08 BETTER
I0312 00:06:12.521416 1756598 finetune.py:68] layer 1_v @ epoch 2 new loss 3.989765900769271e-06 old loss 4.65170751340338e-06 BETTER
I0312 00:06:15.769670 1756480 finetune.py:45] layer 0_q initial loss 8.157136477393578e-08
I0312 00:06:31.629280 1756716 finetune.py:68] layer 2_v @ epoch 0 new loss 3.4876611607614905e-06 old loss 5.192221124161733e-06 BETTER
I0312 00:06:45.154630 1756598 finetune.py:68] layer 1_v @ epoch 3 new loss 2.502605411791592e-06 old loss 3.989765900769271e-06 BETTER
I0312 00:06:48.944330 1756480 finetune.py:68] layer 0_q @ epoch 0 new loss 3.8181042327778414e-08 old loss 8.157136477393578e-08 BETTER
I0312 00:07:03.976721 1756716 finetune.py:68] layer 2_v @ epoch 1 new loss 2.6503962544666138e-06 old loss 3.4876611607614905e-06 BETTER
I0312 00:07:04.417901 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 3 in 72.34269332885742s
I0312 00:07:15.265519 1756834 config.py:54] PyTorch version 2.1.1 available.
I0312 00:07:16.385428 1756366 quantize_finetune_llama.py:183] layer 4 gpu 0
I0312 00:07:16.460944 1756834 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 00:07:18.023900 1756598 finetune.py:68] layer 1_v @ epoch 4 new loss 2.3942970983625855e-06 old loss 2.502605411791592e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:07:23.193311 1756480 finetune.py:68] layer 0_q @ epoch 1 new loss 3.472329623832593e-08 old loss 3.8181042327778414e-08 BETTER
I0312 00:07:25.201311 1756834 finetune.py:45] layer 3_v initial loss 8.993712981464341e-06
I0312 00:07:27.698528 1756598 finetune.py:45] layer 1_q initial loss 3.8117427720862906e-06
I0312 00:07:36.743321 1756716 finetune.py:68] layer 2_v @ epoch 2 new loss 2.2223741780180717e-06 old loss 2.6503962544666138e-06 BETTER
I0312 00:07:56.213473 1756834 finetune.py:68] layer 3_v @ epoch 0 new loss 4.953927145834314e-06 old loss 8.993712981464341e-06 BETTER
I0312 00:07:57.582872 1756480 finetune.py:68] layer 0_q @ epoch 2 new loss 3.237134293954114e-08 old loss 3.472329623832593e-08 BETTER
I0312 00:07:59.308831 1756598 finetune.py:68] layer 1_q @ epoch 0 new loss 2.203692702096305e-06 old loss 3.8117427720862906e-06 BETTER
I0312 00:08:09.593518 1756716 finetune.py:68] layer 2_v @ epoch 3 new loss 1.9873598375852453e-06 old loss 2.2223741780180717e-06 BETTER
I0312 00:08:28.285852 1756834 finetune.py:68] layer 3_v @ epoch 1 new loss 3.864648988383124e-06 old loss 4.953927145834314e-06 BETTER
I0312 00:08:31.787442 1756598 finetune.py:76] layer 1_q @ epoch 1 new loss 2.3303637135541067e-06 old loss 2.203692702096305e-06 WORSE
I0312 00:08:31.848466 1756480 finetune.py:68] layer 0_q @ epoch 3 new loss 3.056894826158896e-08 old loss 3.237134293954114e-08 BETTER
I0312 00:08:42.883482 1756716 finetune.py:68] layer 2_v @ epoch 4 new loss 1.8496275515644811e-06 old loss 1.9873598375852453e-06 BETTER
I0312 00:08:51.999730 1756716 finetune.py:45] layer 2_q initial loss 2.134814621967962e-06
I0312 00:09:00.718694 1756834 finetune.py:68] layer 3_v @ epoch 2 new loss 3.4856632282753708e-06 old loss 3.864648988383124e-06 BETTER
I0312 00:09:03.571323 1756598 finetune.py:76] layer 1_q @ epoch 2 new loss 7.971982086019125e-06 old loss 2.203692702096305e-06 WORSE
I0312 00:09:06.130790 1756480 finetune.py:68] layer 0_q @ epoch 4 new loss 2.9074065821532713e-08 old loss 3.056894826158896e-08 BETTER
I0312 00:09:15.353296 1756480 finetune.py:45] layer 0_k initial loss 1.2466422560919455e-07
I0312 00:09:23.772633 1756716 finetune.py:68] layer 2_q @ epoch 0 new loss 1.887403300315782e-06 old loss 2.134814621967962e-06 BETTER
I0312 00:09:33.289812 1756834 finetune.py:68] layer 3_v @ epoch 3 new loss 3.2950933928077575e-06 old loss 3.4856632282753708e-06 BETTER
I0312 00:09:35.662119 1756598 finetune.py:76] layer 1_q @ epoch 3 new loss 3.905902303813491e-06 old loss 2.203692702096305e-06 WORSE
I0312 00:09:47.993618 1756480 finetune.py:68] layer 0_k @ epoch 0 new loss 4.148121490743506e-08 old loss 1.2466422560919455e-07 BETTER
I0312 00:09:56.258627 1756716 finetune.py:68] layer 2_q @ epoch 1 new loss 1.8091822084898013e-06 old loss 1.887403300315782e-06 BETTER
I0312 00:10:06.135904 1756834 finetune.py:68] layer 3_v @ epoch 4 new loss 3.171856405970175e-06 old loss 3.2950933928077575e-06 BETTER
I0312 00:10:07.872269 1756598 finetune.py:76] layer 1_q @ epoch 4 new loss 5.19129616805003e-06 old loss 2.203692702096305e-06 WORSE
I0312 00:10:15.522970 1756834 finetune.py:45] layer 3_q initial loss 3.937600013159681e-06
I0312 00:10:16.690606 1756598 finetune.py:45] layer 1_k initial loss 2.7202372621104587e-06
I0312 00:10:21.601074 1756480 finetune.py:68] layer 0_k @ epoch 1 new loss 3.632143119602915e-08 old loss 4.148121490743506e-08 BETTER
I0312 00:10:28.613216 1756716 finetune.py:68] layer 2_q @ epoch 2 new loss 1.755156290528248e-06 old loss 1.8091822084898013e-06 BETTER
I0312 00:10:47.062374 1756834 finetune.py:68] layer 3_q @ epoch 0 new loss 3.55625797965331e-06 old loss 3.937600013159681e-06 BETTER
I0312 00:10:48.189668 1756598 finetune.py:68] layer 1_k @ epoch 0 new loss 2.3108525510906475e-06 old loss 2.7202372621104587e-06 BETTER
I0312 00:10:55.349081 1756480 finetune.py:68] layer 0_k @ epoch 2 new loss 3.390136171788072e-08 old loss 3.632143119602915e-08 BETTER
I0312 00:11:01.260082 1756716 finetune.py:68] layer 2_q @ epoch 3 new loss 1.713035885586578e-06 old loss 1.755156290528248e-06 BETTER
I0312 00:11:19.444572 1756834 finetune.py:68] layer 3_q @ epoch 1 new loss 3.44393356499495e-06 old loss 3.55625797965331e-06 BETTER
I0312 00:11:20.311708 1756598 finetune.py:76] layer 1_k @ epoch 1 new loss 2.3658647023694357e-06 old loss 2.3108525510906475e-06 WORSE
I0312 00:11:29.010551 1756480 finetune.py:68] layer 0_k @ epoch 3 new loss 3.227906830716165e-08 old loss 3.390136171788072e-08 BETTER
I0312 00:11:33.692944 1756716 finetune.py:68] layer 2_q @ epoch 4 new loss 1.6781158365120064e-06 old loss 1.713035885586578e-06 BETTER
I0312 00:11:42.971221 1756716 finetune.py:45] layer 2_k initial loss 1.9913732103304937e-06
I0312 00:11:51.626111 1756834 finetune.py:68] layer 3_q @ epoch 2 new loss 3.360620894454769e-06 old loss 3.44393356499495e-06 BETTER
I0312 00:11:51.899703 1756598 finetune.py:68] layer 1_k @ epoch 2 new loss 2.0424515696504386e-06 old loss 2.3108525510906475e-06 BETTER
I0312 00:12:02.935112 1756480 finetune.py:68] layer 0_k @ epoch 4 new loss 3.105562029759312e-08 old loss 3.227906830716165e-08 BETTER
I0312 00:12:12.463381 1756480 finetune.py:45] layer 0_o initial loss 2.258835394286507e-07
I0312 00:12:14.379350 1756716 finetune.py:68] layer 2_k @ epoch 0 new loss 1.9164242530678166e-06 old loss 1.9913732103304937e-06 BETTER
I0312 00:12:23.737938 1756834 finetune.py:68] layer 3_q @ epoch 3 new loss 3.294932639619219e-06 old loss 3.360620894454769e-06 BETTER
I0312 00:12:24.029705 1756598 finetune.py:76] layer 1_k @ epoch 3 new loss 2.441389369778335e-06 old loss 2.0424515696504386e-06 WORSE
I0312 00:12:45.053920 1756480 finetune.py:68] layer 0_o @ epoch 0 new loss 2.069597115905708e-07 old loss 2.258835394286507e-07 BETTER
I0312 00:12:46.495207 1756716 finetune.py:68] layer 2_k @ epoch 1 new loss 1.8752244841380161e-06 old loss 1.9164242530678166e-06 BETTER
I0312 00:12:56.236754 1756598 finetune.py:76] layer 1_k @ epoch 4 new loss 2.17242086364422e-06 old loss 2.0424515696504386e-06 WORSE
I0312 00:12:56.589775 1756834 finetune.py:68] layer 3_q @ epoch 4 new loss 3.2398741041106405e-06 old loss 3.294932639619219e-06 BETTER
I0312 00:13:04.984777 1756598 finetune.py:45] layer 1_o initial loss 6.897012553963577e-06
I0312 00:13:05.948856 1756834 finetune.py:45] layer 3_k initial loss 3.875323272950482e-06
I0312 00:13:18.327330 1756480 finetune.py:68] layer 0_o @ epoch 1 new loss 1.9274357043741475e-07 old loss 2.069597115905708e-07 BETTER
I0312 00:13:18.758547 1756716 finetune.py:68] layer 2_k @ epoch 2 new loss 1.8423925212118775e-06 old loss 1.8752244841380161e-06 BETTER
I0312 00:13:35.935676 1756598 finetune.py:68] layer 1_o @ epoch 0 new loss 4.289062417228706e-06 old loss 6.897012553963577e-06 BETTER
I0312 00:13:37.071933 1756834 finetune.py:68] layer 3_k @ epoch 0 new loss 3.762801725315512e-06 old loss 3.875323272950482e-06 BETTER
I0312 00:13:51.127978 1756716 finetune.py:68] layer 2_k @ epoch 3 new loss 1.8143138049708796e-06 old loss 1.8423925212118775e-06 BETTER
I0312 00:13:51.619373 1756480 finetune.py:68] layer 0_o @ epoch 2 new loss 1.8144918101370422e-07 old loss 1.9274357043741475e-07 BETTER
I0312 00:14:07.657900 1756598 finetune.py:68] layer 1_o @ epoch 1 new loss 3.937750989280175e-06 old loss 4.289062417228706e-06 BETTER
I0312 00:14:09.028485 1756834 finetune.py:68] layer 3_k @ epoch 1 new loss 3.706677489390131e-06 old loss 3.762801725315512e-06 BETTER
I0312 00:14:23.473648 1756716 finetune.py:68] layer 2_k @ epoch 4 new loss 1.7894977872856543e-06 old loss 1.8143138049708796e-06 BETTER
I0312 00:14:25.190283 1756480 finetune.py:68] layer 0_o @ epoch 3 new loss 1.7229569948540302e-07 old loss 1.8144918101370422e-07 BETTER
I0312 00:14:32.795925 1756716 finetune.py:45] layer 2_o initial loss 4.81152437714627e-06
I0312 00:14:39.287908 1756598 finetune.py:68] layer 1_o @ epoch 2 new loss 3.836997620965121e-06 old loss 3.937750989280175e-06 BETTER
I0312 00:14:40.978362 1756834 finetune.py:68] layer 3_k @ epoch 2 new loss 3.660755055534537e-06 old loss 3.706677489390131e-06 BETTER
I0312 00:14:58.500025 1756480 finetune.py:68] layer 0_o @ epoch 4 new loss 1.648243852514497e-07 old loss 1.7229569948540302e-07 BETTER
I0312 00:15:03.786140 1756716 finetune.py:68] layer 2_o @ epoch 0 new loss 4.7565376917191315e-06 old loss 4.81152437714627e-06 BETTER
I0312 00:15:11.030456 1756598 finetune.py:68] layer 1_o @ epoch 3 new loss 3.8022135413484648e-06 old loss 3.836997620965121e-06 BETTER
I0312 00:15:12.973346 1756834 finetune.py:68] layer 3_k @ epoch 3 new loss 3.6213175462762592e-06 old loss 3.660755055534537e-06 BETTER
I0312 00:15:14.165624 1756480 finetune.py:45] layer 0_up initial loss 2.2182084080668574e-07
I0312 00:15:35.537556 1756716 finetune.py:68] layer 2_o @ epoch 1 new loss 4.715534487331752e-06 old loss 4.7565376917191315e-06 BETTER
I0312 00:15:42.820347 1756598 finetune.py:68] layer 1_o @ epoch 4 new loss 3.750792984646978e-06 old loss 3.8022135413484648e-06 BETTER
I0312 00:15:44.722180 1756480 finetune.py:68] layer 0_up @ epoch 0 new loss 2.101373723917277e-07 old loss 2.2182084080668574e-07 BETTER
I0312 00:15:44.877665 1756834 finetune.py:68] layer 3_k @ epoch 4 new loss 3.5866237340087537e-06 old loss 3.6213175462762592e-06 BETTER
I0312 00:15:53.971560 1756834 finetune.py:45] layer 3_o initial loss 9.083226359507535e-06
I0312 00:15:58.100639 1756598 finetune.py:45] layer 1_up initial loss 5.977856289973715e-06
I0312 00:16:07.280649 1756716 finetune.py:68] layer 2_o @ epoch 2 new loss 4.680856818595203e-06 old loss 4.715534487331752e-06 BETTER
I0312 00:16:16.214791 1756480 finetune.py:68] layer 0_up @ epoch 1 new loss 2.0428961988727679e-07 old loss 2.101373723917277e-07 BETTER
I0312 00:16:24.230614 1756834 finetune.py:68] layer 3_o @ epoch 0 new loss 8.9041750470642e-06 old loss 9.083226359507535e-06 BETTER
I0312 00:16:26.938413 1756598 finetune.py:68] layer 1_up @ epoch 0 new loss 4.356813406047877e-06 old loss 5.977856289973715e-06 BETTER
I0312 00:16:39.058731 1756716 finetune.py:68] layer 2_o @ epoch 3 new loss 4.650551090890076e-06 old loss 4.680856818595203e-06 BETTER
I0312 00:16:47.800340 1756480 finetune.py:68] layer 0_up @ epoch 2 new loss 1.9974656595422857e-07 old loss 2.0428961988727679e-07 BETTER
I0312 00:16:55.275862 1756834 finetune.py:68] layer 3_o @ epoch 1 new loss 8.784592864685692e-06 old loss 8.9041750470642e-06 BETTER
I0312 00:16:56.579198 1756598 finetune.py:76] layer 1_up @ epoch 1 new loss 4.404832452564733e-06 old loss 4.356813406047877e-06 WORSE
I0312 00:17:10.929028 1756716 finetune.py:68] layer 2_o @ epoch 4 new loss 4.623440418072278e-06 old loss 4.650551090890076e-06 BETTER
I0312 00:17:19.379233 1756480 finetune.py:68] layer 0_up @ epoch 3 new loss 1.9603957923663984e-07 old loss 1.9974656595422857e-07 BETTER
I0312 00:17:25.891495 1756598 finetune.py:68] layer 1_up @ epoch 2 new loss 4.214798082102789e-06 old loss 4.356813406047877e-06 BETTER
I0312 00:17:26.319563 1756716 finetune.py:45] layer 2_up initial loss 6.8251365519245155e-06
I0312 00:17:26.603345 1756834 finetune.py:68] layer 3_o @ epoch 2 new loss 8.695769793121144e-06 old loss 8.784592864685692e-06 BETTER
I0312 00:17:51.074360 1756480 finetune.py:68] layer 0_up @ epoch 4 new loss 1.9291523756237439e-07 old loss 1.9603957923663984e-07 BETTER
I0312 00:17:55.399930 1756716 finetune.py:68] layer 2_up @ epoch 0 new loss 6.7861888055631425e-06 old loss 6.8251365519245155e-06 BETTER
I0312 00:17:55.965528 1756598 finetune.py:76] layer 1_up @ epoch 3 new loss 4.27853274231893e-06 old loss 4.214798082102789e-06 WORSE
I0312 00:17:57.804681 1756834 finetune.py:68] layer 3_o @ epoch 3 new loss 8.621839697298128e-06 old loss 8.695769793121144e-06 BETTER
I0312 00:18:06.770984 1756480 finetune.py:45] layer 0_gate initial loss 2.43814923805985e-07
I0312 00:18:25.268535 1756598 finetune.py:76] layer 1_up @ epoch 4 new loss 4.2464644138817675e-06 old loss 4.214798082102789e-06 WORSE
I0312 00:18:25.403002 1756716 finetune.py:68] layer 2_up @ epoch 1 new loss 6.758415111107752e-06 old loss 6.7861888055631425e-06 BETTER
I0312 00:18:28.827738 1756834 finetune.py:68] layer 3_o @ epoch 4 new loss 8.558453373552766e-06 old loss 8.621839697298128e-06 BETTER
I0312 00:18:35.728466 1756480 finetune.py:68] layer 0_gate @ epoch 0 new loss 2.347467642493939e-07 old loss 2.43814923805985e-07 BETTER
I0312 00:18:40.047047 1756598 finetune.py:45] layer 1_gate initial loss 5.142995632922975e-06
I0312 00:18:44.475782 1756834 finetune.py:45] layer 3_up initial loss 1.3641181794810109e-05
I0312 00:18:55.273882 1756716 finetune.py:68] layer 2_up @ epoch 2 new loss 6.734472663083579e-06 old loss 6.758415111107752e-06 BETTER
I0312 00:19:05.621198 1756480 finetune.py:68] layer 0_gate @ epoch 1 new loss 2.3149641492636874e-07 old loss 2.347467642493939e-07 BETTER
I0312 00:19:07.603527 1756598 finetune.py:68] layer 1_gate @ epoch 0 new loss 4.681357040681178e-06 old loss 5.142995632922975e-06 BETTER
I0312 00:19:13.130939 1756834 finetune.py:68] layer 3_up @ epoch 0 new loss 1.3543552086048294e-05 old loss 1.3641181794810109e-05 BETTER
I0312 00:19:25.147226 1756716 finetune.py:68] layer 2_up @ epoch 3 new loss 6.712119557050755e-06 old loss 6.734472663083579e-06 BETTER
I0312 00:19:35.467337 1756480 finetune.py:68] layer 0_gate @ epoch 2 new loss 2.2923130416074855e-07 old loss 2.3149641492636874e-07 BETTER
I0312 00:19:35.754903 1756598 finetune.py:68] layer 1_gate @ epoch 1 new loss 4.647053629014408e-06 old loss 4.681357040681178e-06 BETTER
I0312 00:19:42.805377 1756834 finetune.py:68] layer 3_up @ epoch 1 new loss 1.3472907994582783e-05 old loss 1.3543552086048294e-05 BETTER
I0312 00:19:55.210448 1756716 finetune.py:68] layer 2_up @ epoch 4 new loss 6.691167982353363e-06 old loss 6.712119557050755e-06 BETTER
I0312 00:20:04.146391 1756598 finetune.py:76] layer 1_gate @ epoch 2 new loss 4.6876543819962535e-06 old loss 4.647053629014408e-06 WORSE
I0312 00:20:05.487795 1756480 finetune.py:68] layer 0_gate @ epoch 3 new loss 2.273354908766123e-07 old loss 2.2923130416074855e-07 BETTER
I0312 00:20:10.529146 1756716 finetune.py:45] layer 2_gate initial loss 8.255680768343154e-06
I0312 00:20:12.353185 1756834 finetune.py:68] layer 3_up @ epoch 2 new loss 1.3411221516435035e-05 old loss 1.3472907994582783e-05 BETTER
I0312 00:20:31.838305 1756598 finetune.py:68] layer 1_gate @ epoch 3 new loss 4.617732429323951e-06 old loss 4.647053629014408e-06 BETTER
I0312 00:20:35.447917 1756480 finetune.py:68] layer 0_gate @ epoch 4 new loss 2.2570961277779134e-07 old loss 2.273354908766123e-07 BETTER
I0312 00:20:38.102381 1756716 finetune.py:68] layer 2_gate @ epoch 0 new loss 8.219832125178073e-06 old loss 8.255680768343154e-06 BETTER
I0312 00:20:41.787150 1756834 finetune.py:68] layer 3_up @ epoch 3 new loss 1.3354307156987488e-05 old loss 1.3411221516435035e-05 BETTER
I0312 00:20:51.699432 1756480 finetune.py:45] layer 0_down initial loss 5.227490760262299e-07
I0312 00:21:00.089721 1756598 finetune.py:68] layer 1_gate @ epoch 4 new loss 4.588371211866615e-06 old loss 4.617732429323951e-06 BETTER
I0312 00:21:06.195154 1756716 finetune.py:68] layer 2_gate @ epoch 1 new loss 8.197863280656748e-06 old loss 8.219832125178073e-06 BETTER
I0312 00:21:11.256577 1756834 finetune.py:68] layer 3_up @ epoch 4 new loss 1.3301900253281929e-05 old loss 1.3354307156987488e-05 BETTER
I0312 00:21:15.760498 1756598 finetune.py:45] layer 1_down initial loss 0.00029049653676338494
I0312 00:21:18.796265 1756480 finetune.py:68] layer 0_down @ epoch 0 new loss 5.213012741478451e-07 old loss 5.227490760262299e-07 BETTER
I0312 00:21:26.470824 1756834 finetune.py:45] layer 3_gate initial loss 1.69258019013796e-05
I0312 00:21:34.176785 1756716 finetune.py:68] layer 2_gate @ epoch 2 new loss 8.17946329334518e-06 old loss 8.197863280656748e-06 BETTER
I0312 00:21:41.638317 1756598 finetune.py:68] layer 1_down @ epoch 0 new loss 0.00022409412486013025 old loss 0.00029049653676338494 BETTER
I0312 00:21:47.174809 1756480 finetune.py:68] layer 0_down @ epoch 1 new loss 5.210708877712023e-07 old loss 5.213012741478451e-07 BETTER
I0312 00:21:53.505502 1756834 finetune.py:68] layer 3_gate @ epoch 0 new loss 1.6848711311467923e-05 old loss 1.69258019013796e-05 BETTER
I0312 00:22:02.538080 1756716 finetune.py:68] layer 2_gate @ epoch 3 new loss 8.162682206602767e-06 old loss 8.17946329334518e-06 BETTER
I0312 00:22:08.426420 1756598 finetune.py:68] layer 1_down @ epoch 1 new loss 0.0002029275638051331 old loss 0.00022409412486013025 BETTER
I0312 00:22:15.592659 1756480 finetune.py:68] layer 0_down @ epoch 2 new loss 5.209024607211177e-07 old loss 5.210708877712023e-07 BETTER
I0312 00:22:21.691907 1756834 finetune.py:68] layer 3_gate @ epoch 1 new loss 1.679697379586287e-05 old loss 1.6848711311467923e-05 BETTER
I0312 00:22:31.109569 1756716 finetune.py:68] layer 2_gate @ epoch 4 new loss 8.146877917170059e-06 old loss 8.162682206602767e-06 BETTER
I0312 00:22:35.313304 1756598 finetune.py:68] layer 1_down @ epoch 2 new loss 0.000198328765691258 old loss 0.0002029275638051331 BETTER
I0312 00:22:44.130757 1756480 finetune.py:68] layer 0_down @ epoch 3 new loss 5.208267452871951e-07 old loss 5.209024607211177e-07 BETTER
I0312 00:22:47.273658 1756716 finetune.py:45] layer 2_down initial loss 1.2338096894382033e-05
I0312 00:22:50.007408 1756834 finetune.py:68] layer 3_gate @ epoch 2 new loss 1.6752070223446935e-05 old loss 1.679697379586287e-05 BETTER
I0312 00:23:02.310240 1756598 finetune.py:68] layer 1_down @ epoch 3 new loss 0.000197622983250767 old loss 0.000198328765691258 BETTER
I0312 00:23:12.623144 1756480 finetune.py:68] layer 0_down @ epoch 4 new loss 5.207093067838287e-07 old loss 5.208267452871951e-07 BETTER
I0312 00:23:13.513794 1756716 finetune.py:68] layer 2_down @ epoch 0 new loss 1.2335372957750224e-05 old loss 1.2338096894382033e-05 BETTER
0_v proxy err 0.006401833612471819 tr(WHW.T) 4.225186347961426
0_q proxy err 0.000227922952035442 tr(WHW.T) 2710.36865234375
0_k proxy err 0.00022219093807507306 tr(WHW.T) 1698.9349365234375
0_o proxy err 0.0007687349570915103 tr(WHW.T) 0.9715777039527893
0_up proxy err 0.0032181646674871445 tr(WHW.T) 43.269927978515625
0_gate proxy err 0.00224390160292387 tr(WHW.T) 63.4742317199707
0_down proxy err 0.0019518521148711443 tr(WHW.T) 0.6573374271392822
I0312 00:23:18.319274 1756834 finetune.py:68] layer 3_gate @ epoch 3 new loss 1.6711514035705477e-05 old loss 1.6752070223446935e-05 BETTER
I0312 00:23:29.632536 1756598 finetune.py:68] layer 1_down @ epoch 4 new loss 0.0001974658516701311 old loss 0.000197622983250767 BETTER
1_v proxy err 0.013439243659377098 tr(WHW.T) 16.465883255004883
1_q proxy err 0.00026921925018541515 tr(WHW.T) 4779.0654296875
1_k proxy err 0.0002875957579817623 tr(WHW.T) 4997.22607421875
1_o proxy err 0.005587121471762657 tr(WHW.T) 1.1129372119903564
1_up proxy err 0.004406759049743414 tr(WHW.T) 109.74827575683594
1_gate proxy err 0.002234946470707655 tr(WHW.T) 221.324951171875
1_down proxy err 0.0005872164038009942 tr(WHW.T) 2041.9344482421875
I0312 00:23:41.006806 1756716 finetune.py:68] layer 2_down @ epoch 1 new loss 1.2334196981100831e-05 old loss 1.2335372957750224e-05 BETTER
I0312 00:23:47.430207 1756834 finetune.py:68] layer 3_gate @ epoch 4 new loss 1.667344622546807e-05 old loss 1.6711514035705477e-05 BETTER
I0312 00:24:02.882230 1756834 finetune.py:45] layer 3_down initial loss 2.5303861548309214e-05
I0312 00:24:08.097447 1756716 finetune.py:68] layer 2_down @ epoch 2 new loss 1.233332386618713e-05 old loss 1.2334196981100831e-05 BETTER
I0312 00:24:28.395977 1756834 finetune.py:68] layer 3_down @ epoch 0 new loss 2.5299148546764627e-05 old loss 2.5303861548309214e-05 BETTER
I0312 00:24:34.931747 1756716 finetune.py:68] layer 2_down @ epoch 3 new loss 1.2332667211012449e-05 old loss 1.233332386618713e-05 BETTER
I0312 00:24:48.912335 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 4 in 73.63554573059082s
I0312 00:24:52.164252 1756950 config.py:54] PyTorch version 2.1.1 available.
I0312 00:24:53.235899 1756366 quantize_finetune_llama.py:183] layer 5 gpu 1
I0312 00:24:53.324096 1756950 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 00:24:54.850355 1756834 finetune.py:68] layer 3_down @ epoch 1 new loss 2.5297777028754354e-05 old loss 2.5299148546764627e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:25:01.747130 1756950 finetune.py:45] layer 4_v initial loss 1.1982473552052397e-05
I0312 00:25:01.762649 1756716 finetune.py:68] layer 2_down @ epoch 4 new loss 1.2332147889537737e-05 old loss 1.2332667211012449e-05 BETTER
2_v proxy err 0.0059209526516497135 tr(WHW.T) 136.67332458496094
2_q proxy err 0.00023512453481089324 tr(WHW.T) 7753.50927734375
2_k proxy err 0.00020907062571495771 tr(WHW.T) 10205.427734375
2_o proxy err 0.00647016754373908 tr(WHW.T) 1.4628925323486328
2_up proxy err 0.0054064723663032055 tr(WHW.T) 193.44754028320312
2_gate proxy err 0.003448514034971595 tr(WHW.T) 306.6561584472656
2_down proxy err 0.005730494856834412 tr(WHW.T) 3.0135765075683594
I0312 00:25:21.341607 1756834 finetune.py:68] layer 3_down @ epoch 2 new loss 2.529663106543012e-05 old loss 2.5297777028754354e-05 BETTER
I0312 00:25:34.871825 1756950 finetune.py:68] layer 4_v @ epoch 0 new loss 6.36391951047699e-06 old loss 1.1982473552052397e-05 BETTER
I0312 00:25:47.872654 1756834 finetune.py:68] layer 3_down @ epoch 3 new loss 2.5295765226474032e-05 old loss 2.529663106543012e-05 BETTER
I0312 00:26:09.098626 1756950 finetune.py:68] layer 4_v @ epoch 1 new loss 5.282082383928355e-06 old loss 6.36391951047699e-06 BETTER
I0312 00:26:14.325253 1756834 finetune.py:68] layer 3_down @ epoch 4 new loss 2.529509765736293e-05 old loss 2.5295765226474032e-05 BETTER
3_v proxy err 0.007880429737269878 tr(WHW.T) 284.77557373046875
3_q proxy err 0.00043460202869027853 tr(WHW.T) 7218.57080078125
3_k proxy err 0.0003437307314015925 tr(WHW.T) 10076.9990234375
3_o proxy err 0.005594324786216021 tr(WHW.T) 3.3601245880126953
3_up proxy err 0.006068466696888208 tr(WHW.T) 284.77593994140625
3_gate proxy err 0.0036677978932857513 tr(WHW.T) 478.13671875
3_down proxy err 0.005850514862686396 tr(WHW.T) 6.139037609100342
I0312 00:26:17.346555 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 5 in 70.3269190788269s
I0312 00:26:20.478737 1757066 config.py:54] PyTorch version 2.1.1 available.
I0312 00:26:21.422492 1756366 quantize_finetune_llama.py:183] layer 6 gpu 2
I0312 00:26:21.488230 1757066 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:26:29.758576 1757066 finetune.py:45] layer 5_v initial loss 1.7723519704304636e-05
I0312 00:26:43.754939 1756950 finetune.py:68] layer 4_v @ epoch 2 new loss 4.873236321145669e-06 old loss 5.282082383928355e-06 BETTER
I0312 00:27:01.082428 1757066 finetune.py:68] layer 5_v @ epoch 0 new loss 1.0694883712858427e-05 old loss 1.7723519704304636e-05 BETTER
I0312 00:27:18.598749 1756950 finetune.py:68] layer 4_v @ epoch 3 new loss 4.636695393855916e-06 old loss 4.873236321145669e-06 BETTER
I0312 00:27:31.859588 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 6 in 70.03189396858215s
I0312 00:27:33.423652 1757066 finetune.py:68] layer 5_v @ epoch 1 new loss 9.51474066823721e-06 old loss 1.0694883712858427e-05 BETTER
I0312 00:27:35.122522 1757182 config.py:54] PyTorch version 2.1.1 available.
I0312 00:27:36.259462 1756366 quantize_finetune_llama.py:183] layer 7 gpu 3
I0312 00:27:36.326702 1757182 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:27:44.819475 1757182 finetune.py:45] layer 6_v initial loss 2.3755985239404254e-05
I0312 00:27:53.456776 1756950 finetune.py:68] layer 4_v @ epoch 4 new loss 4.474199613468954e-06 old loss 4.636695393855916e-06 BETTER
I0312 00:28:02.768869 1756950 finetune.py:45] layer 4_q initial loss 5.5993209571170155e-06
I0312 00:28:06.015970 1757066 finetune.py:68] layer 5_v @ epoch 2 new loss 8.974134289019275e-06 old loss 9.51474066823721e-06 BETTER
I0312 00:28:16.211988 1757182 finetune.py:68] layer 6_v @ epoch 0 new loss 1.3358585420064628e-05 old loss 2.3755985239404254e-05 BETTER
I0312 00:28:36.314246 1756950 finetune.py:68] layer 4_q @ epoch 0 new loss 5.063384378445335e-06 old loss 5.5993209571170155e-06 BETTER
I0312 00:28:38.544375 1757066 finetune.py:68] layer 5_v @ epoch 3 new loss 8.625045666121878e-06 old loss 8.974134289019275e-06 BETTER
I0312 00:28:47.580487 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 7 in 70.90984606742859s
I0312 00:28:48.689547 1757182 finetune.py:68] layer 6_v @ epoch 1 new loss 1.1967053069383837e-05 old loss 1.3358585420064628e-05 BETTER
I0312 00:28:50.897486 1757298 config.py:54] PyTorch version 2.1.1 available.
I0312 00:28:52.069966 1756366 quantize_finetune_llama.py:183] layer 8 gpu 0
I0312 00:28:52.156737 1757298 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:29:00.497339 1757298 finetune.py:45] layer 7_v initial loss 3.1200350349536166e-05
I0312 00:29:10.915520 1756950 finetune.py:68] layer 4_q @ epoch 1 new loss 4.901693500869442e-06 old loss 5.063384378445335e-06 BETTER
I0312 00:29:11.593820 1757066 finetune.py:68] layer 5_v @ epoch 4 new loss 8.372903721465264e-06 old loss 8.625045666121878e-06 BETTER
I0312 00:29:20.827975 1757066 finetune.py:45] layer 5_q initial loss 9.867567314358894e-06
I0312 00:29:21.540042 1757182 finetune.py:68] layer 6_v @ epoch 2 new loss 1.1285001164651476e-05 old loss 1.1967053069383837e-05 BETTER
I0312 00:29:31.493999 1757298 finetune.py:68] layer 7_v @ epoch 0 new loss 1.839601645770017e-05 old loss 3.1200350349536166e-05 BETTER
I0312 00:29:45.407717 1756950 finetune.py:68] layer 4_q @ epoch 2 new loss 4.783229996974114e-06 old loss 4.901693500869442e-06 BETTER
I0312 00:29:52.509592 1757066 finetune.py:68] layer 5_q @ epoch 0 new loss 9.17296620173147e-06 old loss 9.867567314358894e-06 BETTER
I0312 00:29:54.425564 1757182 finetune.py:68] layer 6_v @ epoch 3 new loss 1.0861249393201433e-05 old loss 1.1285001164651476e-05 BETTER
I0312 00:30:03.584113 1757298 finetune.py:68] layer 7_v @ epoch 1 new loss 1.6689727999619208e-05 old loss 1.839601645770017e-05 BETTER
I0312 00:30:20.134052 1756950 finetune.py:68] layer 4_q @ epoch 3 new loss 4.688075478043174e-06 old loss 4.783229996974114e-06 BETTER
I0312 00:30:25.212260 1757066 finetune.py:68] layer 5_q @ epoch 1 new loss 8.92823936737841e-06 old loss 9.17296620173147e-06 BETTER
I0312 00:30:27.781583 1757182 finetune.py:68] layer 6_v @ epoch 4 new loss 1.0551801096880808e-05 old loss 1.0861249393201433e-05 BETTER
I0312 00:30:35.682954 1757298 finetune.py:68] layer 7_v @ epoch 2 new loss 1.5844652807572857e-05 old loss 1.6689727999619208e-05 BETTER
I0312 00:30:37.348603 1757182 finetune.py:45] layer 6_q initial loss 1.381014044454787e-05
I0312 00:30:54.900086 1756950 finetune.py:68] layer 4_q @ epoch 4 new loss 4.608777999237645e-06 old loss 4.688075478043174e-06 BETTER
I0312 00:30:57.745501 1757066 finetune.py:68] layer 5_q @ epoch 2 new loss 8.747665560804307e-06 old loss 8.92823936737841e-06 BETTER
I0312 00:31:04.339719 1756950 finetune.py:45] layer 4_k initial loss 5.424001756182406e-06
I0312 00:31:08.030734 1757298 finetune.py:68] layer 7_v @ epoch 3 new loss 1.527832864667289e-05 old loss 1.5844652807572857e-05 BETTER
I0312 00:31:09.159012 1757182 finetune.py:68] layer 6_q @ epoch 0 new loss 1.2460056495910976e-05 old loss 1.381014044454787e-05 BETTER
I0312 00:31:30.393302 1757066 finetune.py:68] layer 5_q @ epoch 3 new loss 8.59906413097633e-06 old loss 8.747665560804307e-06 BETTER
I0312 00:31:37.301262 1756950 finetune.py:68] layer 4_k @ epoch 0 new loss 5.272762791719288e-06 old loss 5.424001756182406e-06 BETTER
I0312 00:31:40.698804 1757298 finetune.py:68] layer 7_v @ epoch 4 new loss 1.487636018282501e-05 old loss 1.527832864667289e-05 BETTER
I0312 00:31:41.678405 1757182 finetune.py:68] layer 6_q @ epoch 1 new loss 1.2106871508876793e-05 old loss 1.2460056495910976e-05 BETTER
I0312 00:31:49.779982 1757298 finetune.py:45] layer 7_q initial loss 1.9115750546916388e-05
I0312 00:32:03.151257 1757066 finetune.py:68] layer 5_q @ epoch 4 new loss 8.470639841107186e-06 old loss 8.59906413097633e-06 BETTER
I0312 00:32:11.258787 1756950 finetune.py:68] layer 4_k @ epoch 1 new loss 5.195807261770824e-06 old loss 5.272762791719288e-06 BETTER
I0312 00:32:12.339430 1757066 finetune.py:45] layer 5_k initial loss 9.62180729402462e-06
I0312 00:32:14.149831 1757182 finetune.py:68] layer 6_q @ epoch 2 new loss 1.1848878784803674e-05 old loss 1.2106871508876793e-05 BETTER
I0312 00:32:21.254873 1757298 finetune.py:68] layer 7_q @ epoch 0 new loss 1.759542283252813e-05 old loss 1.9115750546916388e-05 BETTER
I0312 00:32:43.926511 1757066 finetune.py:68] layer 5_k @ epoch 0 new loss 9.251948540622834e-06 old loss 9.62180729402462e-06 BETTER
I0312 00:32:45.138123 1756950 finetune.py:68] layer 4_k @ epoch 2 new loss 5.132290425535757e-06 old loss 5.195807261770824e-06 BETTER
I0312 00:32:46.576521 1757182 finetune.py:68] layer 6_q @ epoch 3 new loss 1.164147488452727e-05 old loss 1.1848878784803674e-05 BETTER
I0312 00:32:53.300620 1757298 finetune.py:68] layer 7_q @ epoch 1 new loss 1.709541174932383e-05 old loss 1.759542283252813e-05 BETTER
I0312 00:33:16.157384 1757066 finetune.py:68] layer 5_k @ epoch 1 new loss 9.117802619584836e-06 old loss 9.251948540622834e-06 BETTER
I0312 00:33:19.192455 1756950 finetune.py:68] layer 4_k @ epoch 3 new loss 5.077372406958602e-06 old loss 5.132290425535757e-06 BETTER
I0312 00:33:19.489054 1757182 finetune.py:68] layer 6_q @ epoch 4 new loss 1.146982867794577e-05 old loss 1.164147488452727e-05 BETTER
I0312 00:33:25.351480 1757298 finetune.py:68] layer 7_q @ epoch 2 new loss 1.6731471987441182e-05 old loss 1.709541174932383e-05 BETTER
I0312 00:33:28.884602 1757182 finetune.py:45] layer 6_k initial loss 1.400090604875004e-05
I0312 00:33:48.350915 1757066 finetune.py:68] layer 5_k @ epoch 2 new loss 9.01275961950887e-06 old loss 9.117802619584836e-06 BETTER
I0312 00:33:53.280135 1756950 finetune.py:68] layer 4_k @ epoch 4 new loss 5.028532541473396e-06 old loss 5.077372406958602e-06 BETTER
I0312 00:33:57.563044 1757298 finetune.py:68] layer 7_q @ epoch 3 new loss 1.644289295654744e-05 old loss 1.6731471987441182e-05 BETTER
I0312 00:34:00.375529 1757182 finetune.py:68] layer 6_k @ epoch 0 new loss 1.3245070476841647e-05 old loss 1.400090604875004e-05 BETTER
I0312 00:34:03.371365 1756950 finetune.py:45] layer 4_o initial loss 1.4076439583732281e-05
I0312 00:34:20.581566 1757066 finetune.py:68] layer 5_k @ epoch 3 new loss 8.922434062696993e-06 old loss 9.01275961950887e-06 BETTER
I0312 00:34:29.808818 1757298 finetune.py:68] layer 7_q @ epoch 4 new loss 1.620297734916676e-05 old loss 1.644289295654744e-05 BETTER
I0312 00:34:32.644655 1757182 finetune.py:68] layer 6_k @ epoch 1 new loss 1.305334717471851e-05 old loss 1.3245070476841647e-05 BETTER
I0312 00:34:35.817865 1756950 finetune.py:68] layer 4_o @ epoch 0 new loss 1.3665928236150648e-05 old loss 1.4076439583732281e-05 BETTER
I0312 00:34:39.426372 1757298 finetune.py:45] layer 7_k initial loss 1.9091392459813505e-05
I0312 00:34:52.771522 1757066 finetune.py:68] layer 5_k @ epoch 4 new loss 8.84542259882437e-06 old loss 8.922434062696993e-06 BETTER
I0312 00:35:02.124850 1757066 finetune.py:45] layer 5_o initial loss 2.3365299057331868e-05
I0312 00:35:04.857679 1757182 finetune.py:68] layer 6_k @ epoch 2 new loss 1.2904941286251415e-05 old loss 1.305334717471851e-05 BETTER
I0312 00:35:09.185753 1756950 finetune.py:68] layer 4_o @ epoch 1 new loss 1.3459334695653524e-05 old loss 1.3665928236150648e-05 BETTER
I0312 00:35:10.398097 1757298 finetune.py:68] layer 7_k @ epoch 0 new loss 1.8532175090513192e-05 old loss 1.9091392459813505e-05 BETTER
I0312 00:35:32.750473 1757066 finetune.py:68] layer 5_o @ epoch 0 new loss 2.246995791210793e-05 old loss 2.3365299057331868e-05 BETTER
I0312 00:35:37.126933 1757182 finetune.py:68] layer 6_k @ epoch 3 new loss 1.2780567885783967e-05 old loss 1.2904941286251415e-05 BETTER
I0312 00:35:42.287052 1757298 finetune.py:68] layer 7_k @ epoch 1 new loss 1.8305110643268563e-05 old loss 1.8532175090513192e-05 BETTER
I0312 00:35:42.429618 1756950 finetune.py:68] layer 4_o @ epoch 2 new loss 1.3300688806339167e-05 old loss 1.3459334695653524e-05 BETTER
I0312 00:36:04.170200 1757066 finetune.py:68] layer 5_o @ epoch 1 new loss 2.1984673367114738e-05 old loss 2.246995791210793e-05 BETTER
I0312 00:36:09.237251 1757182 finetune.py:68] layer 6_k @ epoch 4 new loss 1.2677070117206313e-05 old loss 1.2780567885783967e-05 BETTER
I0312 00:36:14.651606 1757298 finetune.py:68] layer 7_k @ epoch 2 new loss 1.8117962099495344e-05 old loss 1.8305110643268563e-05 BETTER
I0312 00:36:16.181719 1756950 finetune.py:68] layer 4_o @ epoch 3 new loss 1.3168430086807348e-05 old loss 1.3300688806339167e-05 BETTER
I0312 00:36:19.518264 1757182 finetune.py:45] layer 6_o initial loss 3.466366251814179e-05
I0312 00:36:35.687791 1757066 finetune.py:68] layer 5_o @ epoch 2 new loss 2.1610143448924646e-05 old loss 2.1984673367114738e-05 BETTER
I0312 00:36:46.599527 1757298 finetune.py:68] layer 7_k @ epoch 3 new loss 1.796395008568652e-05 old loss 1.8117962099495344e-05 BETTER
I0312 00:36:49.691285 1756950 finetune.py:68] layer 4_o @ epoch 4 new loss 1.3051309906586539e-05 old loss 1.3168430086807348e-05 BETTER
I0312 00:36:50.484443 1757182 finetune.py:68] layer 6_o @ epoch 0 new loss 3.3175649150507525e-05 old loss 3.466366251814179e-05 BETTER
I0312 00:37:05.037675 1756950 finetune.py:45] layer 4_up initial loss 2.2504309526993893e-05
I0312 00:37:07.170780 1757066 finetune.py:68] layer 5_o @ epoch 3 new loss 2.1298501451383345e-05 old loss 2.1610143448924646e-05 BETTER
I0312 00:37:18.500000 1757298 finetune.py:68] layer 7_k @ epoch 4 new loss 1.782629442459438e-05 old loss 1.796395008568652e-05 BETTER
I0312 00:37:22.134778 1757182 finetune.py:68] layer 6_o @ epoch 1 new loss 3.24152315442916e-05 old loss 3.3175649150507525e-05 BETTER
I0312 00:37:27.597935 1757298 finetune.py:45] layer 7_o initial loss 4.812651968677528e-05
I0312 00:37:35.489213 1756950 finetune.py:68] layer 4_up @ epoch 0 new loss 2.226588003395591e-05 old loss 2.2504309526993893e-05 BETTER
I0312 00:37:38.601217 1757066 finetune.py:68] layer 5_o @ epoch 4 new loss 2.1035362806287594e-05 old loss 2.1298501451383345e-05 BETTER
I0312 00:37:53.548746 1757066 finetune.py:45] layer 5_up initial loss 3.630094215623103e-05
I0312 00:37:53.717933 1757182 finetune.py:68] layer 6_o @ epoch 2 new loss 3.1846564525039867e-05 old loss 3.24152315442916e-05 BETTER
I0312 00:37:57.850441 1757298 finetune.py:68] layer 7_o @ epoch 0 new loss 4.550842641037889e-05 old loss 4.812651968677528e-05 BETTER
I0312 00:38:07.025281 1756950 finetune.py:68] layer 4_up @ epoch 1 new loss 2.2103435185272247e-05 old loss 2.226588003395591e-05 BETTER
I0312 00:38:22.426523 1757066 finetune.py:68] layer 5_up @ epoch 0 new loss 3.582399585866369e-05 old loss 3.630094215623103e-05 BETTER
I0312 00:38:25.415509 1757182 finetune.py:68] layer 6_o @ epoch 3 new loss 3.139091859338805e-05 old loss 3.1846564525039867e-05 BETTER
I0312 00:38:28.777105 1757298 finetune.py:68] layer 7_o @ epoch 1 new loss 4.423307836987078e-05 old loss 4.550842641037889e-05 BETTER
I0312 00:38:38.449173 1756950 finetune.py:68] layer 4_up @ epoch 2 new loss 2.1965479390928522e-05 old loss 2.2103435185272247e-05 BETTER
I0312 00:38:52.278649 1757066 finetune.py:68] layer 5_up @ epoch 1 new loss 3.548424137989059e-05 old loss 3.582399585866369e-05 BETTER
I0312 00:38:57.094315 1757182 finetune.py:68] layer 6_o @ epoch 4 new loss 3.100814137724228e-05 old loss 3.139091859338805e-05 BETTER
I0312 00:38:59.745711 1757298 finetune.py:68] layer 7_o @ epoch 2 new loss 4.3316540541127324e-05 old loss 4.423307836987078e-05 BETTER
I0312 00:39:09.947854 1756950 finetune.py:68] layer 4_up @ epoch 3 new loss 2.184037475672085e-05 old loss 2.1965479390928522e-05 BETTER
I0312 00:39:12.498313 1757182 finetune.py:45] layer 6_up initial loss 5.4426051065092906e-05
I0312 00:39:22.149995 1757066 finetune.py:68] layer 5_up @ epoch 2 new loss 3.5192064387956634e-05 old loss 3.548424137989059e-05 BETTER
I0312 00:39:30.940538 1757298 finetune.py:68] layer 7_o @ epoch 3 new loss 4.2598610889399424e-05 old loss 4.3316540541127324e-05 BETTER
I0312 00:39:41.670992 1756950 finetune.py:68] layer 4_up @ epoch 4 new loss 2.1723790268879384e-05 old loss 2.184037475672085e-05 BETTER
I0312 00:39:41.769959 1757182 finetune.py:68] layer 6_up @ epoch 0 new loss 5.353706728783436e-05 old loss 5.4426051065092906e-05 BETTER
I0312 00:39:51.960592 1757066 finetune.py:68] layer 5_up @ epoch 3 new loss 3.492835821816698e-05 old loss 3.5192064387956634e-05 BETTER
I0312 00:39:57.327792 1756950 finetune.py:45] layer 4_gate initial loss 2.7821959520224482e-05
I0312 00:40:01.989879 1757298 finetune.py:68] layer 7_o @ epoch 4 new loss 4.200047260383144e-05 old loss 4.2598610889399424e-05 BETTER
I0312 00:40:11.482571 1757182 finetune.py:68] layer 6_up @ epoch 1 new loss 5.295006849337369e-05 old loss 5.353706728783436e-05 BETTER
I0312 00:40:16.947822 1757298 finetune.py:45] layer 7_up initial loss 7.366012141574174e-05
I0312 00:40:21.891256 1757066 finetune.py:68] layer 5_up @ epoch 4 new loss 3.469139846856706e-05 old loss 3.492835821816698e-05 BETTER
I0312 00:40:26.322846 1756950 finetune.py:68] layer 4_gate @ epoch 0 new loss 2.7650283300317824e-05 old loss 2.7821959520224482e-05 BETTER
I0312 00:40:37.559431 1757066 finetune.py:45] layer 5_gate initial loss 4.39868526882492e-05
I0312 00:40:41.493809 1757182 finetune.py:68] layer 6_up @ epoch 2 new loss 5.245644933893345e-05 old loss 5.295006849337369e-05 BETTER
I0312 00:40:45.831182 1757298 finetune.py:68] layer 7_up @ epoch 0 new loss 7.218690006993711e-05 old loss 7.366012141574174e-05 BETTER
I0312 00:40:56.098235 1756950 finetune.py:68] layer 4_gate @ epoch 1 new loss 2.753223634499591e-05 old loss 2.7650283300317824e-05 BETTER
I0312 00:41:05.096381 1757066 finetune.py:68] layer 5_gate @ epoch 0 new loss 4.3685016862582415e-05 old loss 4.39868526882492e-05 BETTER
I0312 00:41:11.375311 1757182 finetune.py:68] layer 6_up @ epoch 3 new loss 5.2019207942066714e-05 old loss 5.245644933893345e-05 BETTER
I0312 00:41:15.289768 1757298 finetune.py:68] layer 7_up @ epoch 1 new loss 7.125925912987441e-05 old loss 7.218690006993711e-05 BETTER
I0312 00:41:25.836206 1756950 finetune.py:68] layer 4_gate @ epoch 2 new loss 2.7432020942796953e-05 old loss 2.753223634499591e-05 BETTER
I0312 00:41:33.307202 1757066 finetune.py:68] layer 5_gate @ epoch 1 new loss 4.3459440348669887e-05 old loss 4.3685016862582415e-05 BETTER
I0312 00:41:41.275392 1757182 finetune.py:68] layer 6_up @ epoch 4 new loss 5.163028254173696e-05 old loss 5.2019207942066714e-05 BETTER
I0312 00:41:44.727691 1757298 finetune.py:68] layer 7_up @ epoch 2 new loss 7.048598490655422e-05 old loss 7.125925912987441e-05 BETTER
I0312 00:41:55.742013 1756950 finetune.py:68] layer 4_gate @ epoch 3 new loss 2.734097870416008e-05 old loss 2.7432020942796953e-05 BETTER
I0312 00:41:56.837210 1757182 finetune.py:45] layer 6_gate initial loss 6.466873310273513e-05
I0312 00:42:01.697784 1757066 finetune.py:68] layer 5_gate @ epoch 2 new loss 4.326416819822043e-05 old loss 4.3459440348669887e-05 BETTER
I0312 00:42:14.273475 1757298 finetune.py:68] layer 7_up @ epoch 3 new loss 6.981959450058639e-05 old loss 7.048598490655422e-05 BETTER
I0312 00:42:24.179142 1757182 finetune.py:68] layer 6_gate @ epoch 0 new loss 6.415619282051921e-05 old loss 6.466873310273513e-05 BETTER
I0312 00:42:25.683553 1756950 finetune.py:68] layer 4_gate @ epoch 4 new loss 2.72554225375643e-05 old loss 2.734097870416008e-05 BETTER
I0312 00:42:30.011080 1757066 finetune.py:68] layer 5_gate @ epoch 3 new loss 4.308394272811711e-05 old loss 4.326416819822043e-05 BETTER
I0312 00:42:41.878365 1756950 finetune.py:45] layer 4_down initial loss 4.350824747234583e-05
I0312 00:42:43.824061 1757298 finetune.py:68] layer 7_up @ epoch 4 new loss 6.922992179170251e-05 old loss 6.981959450058639e-05 BETTER
I0312 00:42:52.314169 1757182 finetune.py:68] layer 6_gate @ epoch 1 new loss 6.378858961397782e-05 old loss 6.415619282051921e-05 BETTER
I0312 00:42:58.337895 1757066 finetune.py:68] layer 5_gate @ epoch 4 new loss 4.291894947527908e-05 old loss 4.308394272811711e-05 BETTER
I0312 00:42:58.936393 1757298 finetune.py:45] layer 7_gate initial loss 8.681688632350415e-05
I0312 00:43:09.142841 1756950 finetune.py:68] layer 4_down @ epoch 0 new loss 4.349975642981008e-05 old loss 4.350824747234583e-05 BETTER
I0312 00:43:14.193828 1757066 finetune.py:45] layer 5_down initial loss 6.708748696837574e-05
I0312 00:43:20.550081 1757182 finetune.py:68] layer 6_gate @ epoch 2 new loss 6.346713053062558e-05 old loss 6.378858961397782e-05 BETTER
I0312 00:43:26.209349 1757298 finetune.py:68] layer 7_gate @ epoch 0 new loss 8.604310278315097e-05 old loss 8.681688632350415e-05 BETTER
I0312 00:43:37.645859 1756950 finetune.py:68] layer 4_down @ epoch 1 new loss 4.3497057049535215e-05 old loss 4.349975642981008e-05 BETTER
I0312 00:43:40.009928 1757066 finetune.py:68] layer 5_down @ epoch 0 new loss 6.708074943162501e-05 old loss 6.708748696837574e-05 BETTER
I0312 00:43:48.731113 1757182 finetune.py:68] layer 6_gate @ epoch 3 new loss 6.318088708212599e-05 old loss 6.346713053062558e-05 BETTER
I0312 00:43:54.357485 1757298 finetune.py:68] layer 7_gate @ epoch 1 new loss 8.548876212444156e-05 old loss 8.604310278315097e-05 BETTER
I0312 00:44:06.156763 1756950 finetune.py:68] layer 4_down @ epoch 2 new loss 4.349568916950375e-05 old loss 4.3497057049535215e-05 BETTER
I0312 00:44:07.081096 1757066 finetune.py:68] layer 5_down @ epoch 1 new loss 6.707812281092629e-05 old loss 6.708074943162501e-05 BETTER
I0312 00:44:17.006830 1757182 finetune.py:68] layer 6_gate @ epoch 4 new loss 6.292023317655548e-05 old loss 6.318088708212599e-05 BETTER
I0312 00:44:22.472517 1757298 finetune.py:68] layer 7_gate @ epoch 2 new loss 8.500814874423668e-05 old loss 8.548876212444156e-05 BETTER
I0312 00:44:32.774907 1757182 finetune.py:45] layer 6_down initial loss 9.915410191752017e-05
I0312 00:44:34.072753 1757066 finetune.py:68] layer 5_down @ epoch 2 new loss 6.707626744173467e-05 old loss 6.707812281092629e-05 BETTER
I0312 00:44:34.719566 1756950 finetune.py:68] layer 4_down @ epoch 3 new loss 4.349473601905629e-05 old loss 4.349568916950375e-05 BETTER
I0312 00:44:50.502478 1757298 finetune.py:68] layer 7_gate @ epoch 3 new loss 8.458452066406608e-05 old loss 8.500814874423668e-05 BETTER
I0312 00:44:58.831644 1757182 finetune.py:68] layer 6_down @ epoch 0 new loss 9.914204565575346e-05 old loss 9.915410191752017e-05 BETTER
I0312 00:45:01.097630 1757066 finetune.py:68] layer 5_down @ epoch 3 new loss 6.707494321744889e-05 old loss 6.707626744173467e-05 BETTER
I0312 00:45:03.214901 1756950 finetune.py:68] layer 4_down @ epoch 4 new loss 4.349393930169754e-05 old loss 4.349473601905629e-05 BETTER
4_v proxy err 0.0072509367018938065 tr(WHW.T) 274.6131286621094
4_q proxy err 0.00041974533814936876 tr(WHW.T) 6918.38916015625
4_k proxy err 0.0003241068625357002 tr(WHW.T) 10420.8330078125
4_o proxy err 0.006493786349892616 tr(WHW.T) 5.141382694244385
4_up proxy err 0.005806088913232088 tr(WHW.T) 397.7056884765625
4_gate proxy err 0.0028943116776645184 tr(WHW.T) 820.4850463867188
4_down proxy err 0.005882387049496174 tr(WHW.T) 11.596039772033691
I0312 00:45:19.251645 1757298 finetune.py:68] layer 7_gate @ epoch 4 new loss 8.4202409198042e-05 old loss 8.458452066406608e-05 BETTER
I0312 00:45:26.676709 1757182 finetune.py:68] layer 6_down @ epoch 1 new loss 9.913613030221313e-05 old loss 9.914204565575346e-05 BETTER
I0312 00:45:28.976137 1757066 finetune.py:68] layer 5_down @ epoch 4 new loss 6.707399734295905e-05 old loss 6.707494321744889e-05 BETTER
5_v proxy err 0.007453184574842453 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0004421189078129828 tr(WHW.T) 6772.736328125
5_k proxy err 0.00031917734304443 tr(WHW.T) 10845.810546875
5_o proxy err 0.007344821002334356 tr(WHW.T) 7.947500228881836
5_up proxy err 0.005738828331232071 tr(WHW.T) 506.68951416015625
5_gate proxy err 0.0026987241581082344 tr(WHW.T) 1103.9569091796875
5_down proxy err 0.006396979559212923 tr(WHW.T) 15.686481475830078
I0312 00:45:35.465981 1757298 finetune.py:45] layer 7_down initial loss 0.0001328094076598063
I0312 00:45:53.556544 1757182 finetune.py:68] layer 6_down @ epoch 2 new loss 9.913254325510934e-05 old loss 9.913613030221313e-05 BETTER
I0312 00:46:00.939494 1757298 finetune.py:68] layer 7_down @ epoch 0 new loss 0.00013279500126373023 old loss 0.0001328094076598063 BETTER
I0312 00:46:20.491170 1757182 finetune.py:68] layer 6_down @ epoch 3 new loss 9.912981477100402e-05 old loss 9.913254325510934e-05 BETTER
I0312 00:46:27.155328 1757298 finetune.py:68] layer 7_down @ epoch 1 new loss 0.00013278952974360436 old loss 0.00013279500126373023 BETTER
I0312 00:46:46.982459 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 8 in 73.21090483665466s
I0312 00:46:47.439201 1757182 finetune.py:68] layer 6_down @ epoch 4 new loss 9.912770474329591e-05 old loss 9.912981477100402e-05 BETTER
6_v proxy err 0.008018909022212029 tr(WHW.T) 443.5464782714844
6_q proxy err 0.0005812071613036096 tr(WHW.T) 7578.0322265625
6_k proxy err 0.0004480172647163272 tr(WHW.T) 10413.71484375
6_o proxy err 0.008344328962266445 tr(WHW.T) 11.603019714355469
6_up proxy err 0.005729906260967255 tr(WHW.T) 617.2357177734375
6_gate proxy err 0.0023416734766215086 tr(WHW.T) 1554.1236572265625
6_down proxy err 0.006550812628120184 tr(WHW.T) 23.044519424438477
I0312 00:46:50.258641 1757414 config.py:54] PyTorch version 2.1.1 available.
I0312 00:46:51.511209 1756366 quantize_finetune_llama.py:183] layer 9 gpu 1
I0312 00:46:51.584745 1757414 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 00:46:53.541099 1757298 finetune.py:68] layer 7_down @ epoch 2 new loss 0.00013278625556267798 old loss 0.00013278952974360436 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:47:00.144355 1757414 finetune.py:45] layer 8_v initial loss 4.43690296378918e-05
I0312 00:47:19.863687 1757298 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00013278370897751302 old loss 0.00013278625556267798 BETTER
I0312 00:47:32.947734 1757414 finetune.py:68] layer 8_v @ epoch 0 new loss 2.626961941132322e-05 old loss 4.43690296378918e-05 BETTER
I0312 00:47:46.351813 1757298 finetune.py:68] layer 7_down @ epoch 4 new loss 0.00013278168626129627 old loss 0.00013278370897751302 BETTER
7_v proxy err 0.007961800321936607 tr(WHW.T) 489.9357604980469
7_q proxy err 0.0006123605417087674 tr(WHW.T) 7674.8642578125
7_k proxy err 0.00048225338105112314 tr(WHW.T) 10205.091796875
7_o proxy err 0.009268563240766525 tr(WHW.T) 15.177536010742188
7_up proxy err 0.005541543941944838 tr(WHW.T) 735.8356323242188
7_gate proxy err 0.0022510234266519547 tr(WHW.T) 1874.8826904296875
7_down proxy err 0.0066261691972613335 tr(WHW.T) 30.661376953125
I0312 00:48:02.630398 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 9 in 70.62331581115723s
I0312 00:48:05.797741 1757530 config.py:54] PyTorch version 2.1.1 available.
I0312 00:48:06.762080 1756366 quantize_finetune_llama.py:183] layer 10 gpu 2
I0312 00:48:06.826108 1757530 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 00:48:07.098029 1757414 finetune.py:68] layer 8_v @ epoch 1 new loss 2.3795912056812085e-05 old loss 2.626961941132322e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:48:14.732302 1757530 finetune.py:45] layer 9_v initial loss 4.0110131521942094e-05
I0312 00:48:41.741507 1757414 finetune.py:68] layer 8_v @ epoch 2 new loss 2.2550833818968385e-05 old loss 2.3795912056812085e-05 BETTER
I0312 00:48:46.010202 1757530 finetune.py:68] layer 9_v @ epoch 0 new loss 3.0508286727126688e-05 old loss 4.0110131521942094e-05 BETTER
I0312 00:49:15.445086 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 10 in 68.25816798210144s
I0312 00:49:16.636439 1757414 finetune.py:68] layer 8_v @ epoch 3 new loss 2.175062218157109e-05 old loss 2.2550833818968385e-05 BETTER
I0312 00:49:18.262709 1757530 finetune.py:68] layer 9_v @ epoch 1 new loss 2.8524347726488486e-05 old loss 3.0508286727126688e-05 BETTER
I0312 00:49:18.632227 1757646 config.py:54] PyTorch version 2.1.1 available.
I0312 00:49:19.667239 1756366 quantize_finetune_llama.py:183] layer 11 gpu 3
I0312 00:49:19.744606 1757646 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:49:27.754285 1757646 finetune.py:45] layer 10_v initial loss 5.8350793551653624e-05
I0312 00:49:50.832823 1757530 finetune.py:68] layer 9_v @ epoch 2 new loss 2.735385533014778e-05 old loss 2.8524347726488486e-05 BETTER
I0312 00:49:51.757927 1757414 finetune.py:68] layer 8_v @ epoch 4 new loss 2.1177154849283397e-05 old loss 2.175062218157109e-05 BETTER
I0312 00:49:59.293269 1757646 finetune.py:68] layer 10_v @ epoch 0 new loss 4.417888339958154e-05 old loss 5.8350793551653624e-05 BETTER
I0312 00:50:01.150089 1757414 finetune.py:45] layer 8_q initial loss 2.707691237446852e-05
I0312 00:50:23.649494 1757530 finetune.py:68] layer 9_v @ epoch 3 new loss 2.6535171855357476e-05 old loss 2.735385533014778e-05 BETTER
I0312 00:50:30.012689 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 11 in 69.87295603752136s
I0312 00:50:31.570551 1757646 finetune.py:68] layer 10_v @ epoch 1 new loss 4.1036761103896424e-05 old loss 4.417888339958154e-05 BETTER
I0312 00:50:33.394257 1757762 config.py:54] PyTorch version 2.1.1 available.
I0312 00:50:34.438275 1756366 quantize_finetune_llama.py:183] layer 12 gpu 0
I0312 00:50:34.512802 1757762 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 00:50:34.519724 1757414 finetune.py:68] layer 8_q @ epoch 0 new loss 2.46482522925362e-05 old loss 2.707691237446852e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 00:50:42.652105 1757762 finetune.py:45] layer 11_v initial loss 6.0047739680157974e-05
I0312 00:50:56.416475 1757530 finetune.py:68] layer 9_v @ epoch 4 new loss 2.5909490432241e-05 old loss 2.6535171855357476e-05 BETTER
I0312 00:51:04.162291 1757646 finetune.py:68] layer 10_v @ epoch 2 new loss 3.9150701923063025e-05 old loss 4.1036761103896424e-05 BETTER
I0312 00:51:05.651670 1757530 finetune.py:45] layer 9_q initial loss 3.266282146796584e-05
I0312 00:51:08.960343 1757414 finetune.py:68] layer 8_q @ epoch 1 new loss 2.3941960535012186e-05 old loss 2.46482522925362e-05 BETTER
I0312 00:51:13.745423 1757762 finetune.py:68] layer 11_v @ epoch 0 new loss 4.4890646677231416e-05 old loss 6.0047739680157974e-05 BETTER
I0312 00:51:37.497804 1757530 finetune.py:68] layer 9_q @ epoch 0 new loss 3.0275479730335064e-05 old loss 3.266282146796584e-05 BETTER
I0312 00:51:37.684606 1757646 finetune.py:68] layer 10_v @ epoch 3 new loss 3.7809084460604936e-05 old loss 3.9150701923063025e-05 BETTER
I0312 00:51:43.591942 1757414 finetune.py:68] layer 8_q @ epoch 2 new loss 2.343293817830272e-05 old loss 2.3941960535012186e-05 BETTER
I0312 00:51:46.067207 1757762 finetune.py:68] layer 11_v @ epoch 1 new loss 4.179001189186238e-05 old loss 4.4890646677231416e-05 BETTER
I0312 00:52:09.923709 1757530 finetune.py:68] layer 9_q @ epoch 1 new loss 2.9507466024369933e-05 old loss 3.0275479730335064e-05 BETTER
I0312 00:52:10.742549 1757646 finetune.py:68] layer 10_v @ epoch 4 new loss 3.678586654132232e-05 old loss 3.7809084460604936e-05 BETTER
I0312 00:52:18.293243 1757414 finetune.py:68] layer 8_q @ epoch 3 new loss 2.3013288227957673e-05 old loss 2.343293817830272e-05 BETTER
I0312 00:52:18.382220 1757762 finetune.py:68] layer 11_v @ epoch 2 new loss 3.998867032350972e-05 old loss 4.179001189186238e-05 BETTER
I0312 00:52:20.318259 1757646 finetune.py:45] layer 10_q initial loss 4.4234475353732705e-05
I0312 00:52:42.584801 1757530 finetune.py:68] layer 9_q @ epoch 2 new loss 2.8932032364537008e-05 old loss 2.9507466024369933e-05 BETTER
I0312 00:52:51.096679 1757762 finetune.py:68] layer 11_v @ epoch 3 new loss 3.873511013807729e-05 old loss 3.998867032350972e-05 BETTER
I0312 00:52:52.095186 1757646 finetune.py:68] layer 10_q @ epoch 0 new loss 4.1594597860239446e-05 old loss 4.4234475353732705e-05 BETTER
I0312 00:52:53.334026 1757414 finetune.py:68] layer 8_q @ epoch 4 new loss 2.267364106955938e-05 old loss 2.3013288227957673e-05 BETTER
I0312 00:53:02.798039 1757414 finetune.py:45] layer 8_k initial loss 2.752220825641416e-05
I0312 00:53:14.914525 1757530 finetune.py:68] layer 9_q @ epoch 3 new loss 2.847056202881504e-05 old loss 2.8932032364537008e-05 BETTER
I0312 00:53:23.786585 1757762 finetune.py:68] layer 11_v @ epoch 4 new loss 3.783561987802386e-05 old loss 3.873511013807729e-05 BETTER
I0312 00:53:24.500654 1757646 finetune.py:68] layer 10_q @ epoch 1 new loss 4.0519385947845876e-05 old loss 4.1594597860239446e-05 BETTER
I0312 00:53:32.961782 1757762 finetune.py:45] layer 11_q initial loss 4.6000197471585125e-05
I0312 00:53:35.724928 1757414 finetune.py:68] layer 8_k @ epoch 0 new loss 2.5985673346440308e-05 old loss 2.752220825641416e-05 BETTER
I0312 00:53:47.665030 1757530 finetune.py:68] layer 9_q @ epoch 4 new loss 2.8091029889765196e-05 old loss 2.847056202881504e-05 BETTER
I0312 00:53:56.763702 1757530 finetune.py:45] layer 9_k initial loss 3.300669413874857e-05
I0312 00:53:57.215896 1757646 finetune.py:68] layer 10_q @ epoch 2 new loss 3.964122515753843e-05 old loss 4.0519385947845876e-05 BETTER
I0312 00:54:04.321270 1757762 finetune.py:68] layer 11_q @ epoch 0 new loss 4.3567157263169065e-05 old loss 4.6000197471585125e-05 BETTER
I0312 00:54:09.802636 1757414 finetune.py:68] layer 8_k @ epoch 1 new loss 2.5620858650654554e-05 old loss 2.5985673346440308e-05 BETTER
I0312 00:54:28.291251 1757530 finetune.py:68] layer 9_k @ epoch 0 new loss 3.188570190104656e-05 old loss 3.300669413874857e-05 BETTER
I0312 00:54:29.835715 1757646 finetune.py:68] layer 10_q @ epoch 3 new loss 3.89537526643835e-05 old loss 3.964122515753843e-05 BETTER
I0312 00:54:36.733901 1757762 finetune.py:68] layer 11_q @ epoch 1 new loss 4.252239887136966e-05 old loss 4.3567157263169065e-05 BETTER
I0312 00:54:43.646695 1757414 finetune.py:68] layer 8_k @ epoch 2 new loss 2.5340908905491233e-05 old loss 2.5620858650654554e-05 BETTER
I0312 00:55:00.607786 1757530 finetune.py:68] layer 9_k @ epoch 1 new loss 3.1503393984166905e-05 old loss 3.188570190104656e-05 BETTER
I0312 00:55:02.443713 1757646 finetune.py:68] layer 10_q @ epoch 4 new loss 3.837675467366353e-05 old loss 3.89537526643835e-05 BETTER
I0312 00:55:08.772925 1757762 finetune.py:68] layer 11_q @ epoch 2 new loss 4.171762338955887e-05 old loss 4.252239887136966e-05 BETTER
I0312 00:55:11.973215 1757646 finetune.py:45] layer 10_k initial loss 4.4247433834243566e-05
I0312 00:55:17.582558 1757414 finetune.py:68] layer 8_k @ epoch 3 new loss 2.5104605811065994e-05 old loss 2.5340908905491233e-05 BETTER
I0312 00:55:32.811469 1757530 finetune.py:68] layer 9_k @ epoch 2 new loss 3.1196184863802046e-05 old loss 3.1503393984166905e-05 BETTER
I0312 00:55:41.066442 1757762 finetune.py:68] layer 11_q @ epoch 3 new loss 4.1068520658882335e-05 old loss 4.171762338955887e-05 BETTER
I0312 00:55:43.676960 1757646 finetune.py:68] layer 10_k @ epoch 0 new loss 4.2808911530300975e-05 old loss 4.4247433834243566e-05 BETTER
I0312 00:55:51.427176 1757414 finetune.py:68] layer 8_k @ epoch 4 new loss 2.4907438273658045e-05 old loss 2.5104605811065994e-05 BETTER
I0312 00:56:00.935247 1757414 finetune.py:45] layer 8_o initial loss 7.00799937476404e-05
I0312 00:56:04.799968 1757530 finetune.py:68] layer 9_k @ epoch 3 new loss 3.0936247640056536e-05 old loss 3.1196184863802046e-05 BETTER
I0312 00:56:13.154308 1757762 finetune.py:68] layer 11_q @ epoch 4 new loss 4.054020610055886e-05 old loss 4.1068520658882335e-05 BETTER
I0312 00:56:15.826602 1757646 finetune.py:68] layer 10_k @ epoch 1 new loss 4.2232422856613994e-05 old loss 4.2808911530300975e-05 BETTER
I0312 00:56:22.387418 1757762 finetune.py:45] layer 11_k initial loss 4.6806464524706826e-05
I0312 00:56:33.654577 1757414 finetune.py:68] layer 8_o @ epoch 0 new loss 6.588213727809489e-05 old loss 7.00799937476404e-05 BETTER
I0312 00:56:36.862882 1757530 finetune.py:68] layer 9_k @ epoch 4 new loss 3.0717917979927734e-05 old loss 3.0936247640056536e-05 BETTER
I0312 00:56:46.275555 1757530 finetune.py:45] layer 9_o initial loss 8.92408934305422e-05
I0312 00:56:47.998257 1757646 finetune.py:68] layer 10_k @ epoch 2 new loss 4.178221570327878e-05 old loss 4.2232422856613994e-05 BETTER
I0312 00:56:53.615463 1757762 finetune.py:68] layer 11_k @ epoch 0 new loss 4.569794691633433e-05 old loss 4.6806464524706826e-05 BETTER
I0312 00:57:07.416085 1757414 finetune.py:68] layer 8_o @ epoch 1 new loss 6.377071258611977e-05 old loss 6.588213727809489e-05 BETTER
I0312 00:57:17.210941 1757530 finetune.py:68] layer 9_o @ epoch 0 new loss 8.38337728055194e-05 old loss 8.92408934305422e-05 BETTER
I0312 00:57:20.337373 1757646 finetune.py:68] layer 10_k @ epoch 3 new loss 4.140706732869148e-05 old loss 4.178221570327878e-05 BETTER
I0312 00:57:25.513562 1757762 finetune.py:68] layer 11_k @ epoch 1 new loss 4.519512003753334e-05 old loss 4.569794691633433e-05 BETTER
I0312 00:57:40.734803 1757414 finetune.py:68] layer 8_o @ epoch 2 new loss 6.226796540431678e-05 old loss 6.377071258611977e-05 BETTER
I0312 00:57:48.846025 1757530 finetune.py:68] layer 9_o @ epoch 1 new loss 8.102877472992986e-05 old loss 8.38337728055194e-05 BETTER
I0312 00:57:52.518882 1757646 finetune.py:68] layer 10_k @ epoch 4 new loss 4.105917832930572e-05 old loss 4.140706732869148e-05 BETTER
I0312 00:57:57.465822 1757762 finetune.py:68] layer 11_k @ epoch 2 new loss 4.478327900869772e-05 old loss 4.519512003753334e-05 BETTER
I0312 00:58:02.208921 1757646 finetune.py:45] layer 10_o initial loss 0.00012349938333500177
I0312 00:58:14.281137 1757414 finetune.py:68] layer 8_o @ epoch 3 new loss 6.110266986070201e-05 old loss 6.226796540431678e-05 BETTER
I0312 00:58:20.540982 1757530 finetune.py:68] layer 9_o @ epoch 2 new loss 7.905027450760826e-05 old loss 8.102877472992986e-05 BETTER
I0312 00:58:29.460104 1757762 finetune.py:68] layer 11_k @ epoch 3 new loss 4.445118611329235e-05 old loss 4.478327900869772e-05 BETTER
I0312 00:58:33.165938 1757646 finetune.py:68] layer 10_o @ epoch 0 new loss 0.00011559958511497825 old loss 0.00012349938333500177 BETTER
I0312 00:58:47.812354 1757414 finetune.py:68] layer 8_o @ epoch 4 new loss 6.015437611495145e-05 old loss 6.110266986070201e-05 BETTER
I0312 00:58:52.070269 1757530 finetune.py:68] layer 9_o @ epoch 3 new loss 7.750831719022244e-05 old loss 7.905027450760826e-05 BETTER
I0312 00:59:01.379317 1757762 finetune.py:68] layer 11_k @ epoch 4 new loss 4.4177246309118345e-05 old loss 4.445118611329235e-05 BETTER
I0312 00:59:03.598321 1757414 finetune.py:45] layer 8_up initial loss 9.766600123839453e-05
I0312 00:59:04.856577 1757646 finetune.py:68] layer 10_o @ epoch 1 new loss 0.00011137001274619251 old loss 0.00011559958511497825 BETTER
I0312 00:59:11.291472 1757762 finetune.py:45] layer 11_o initial loss 0.00012800794502254575
I0312 00:59:23.820547 1757530 finetune.py:68] layer 9_o @ epoch 4 new loss 7.625129364896566e-05 old loss 7.750831719022244e-05 BETTER
I0312 00:59:34.773127 1757414 finetune.py:68] layer 8_up @ epoch 0 new loss 9.56151052378118e-05 old loss 9.766600123839453e-05 BETTER
I0312 00:59:37.039019 1757646 finetune.py:68] layer 10_o @ epoch 2 new loss 0.00010835658758878708 old loss 0.00011137001274619251 BETTER
I0312 00:59:40.029129 1757530 finetune.py:45] layer 9_up initial loss 0.00011954569345107302
I0312 00:59:41.713817 1757762 finetune.py:68] layer 11_o @ epoch 0 new loss 0.00011999746493529528 old loss 0.00012800794502254575 BETTER
I0312 01:00:06.308352 1757414 finetune.py:68] layer 8_up @ epoch 1 new loss 9.429061901755631e-05 old loss 9.56151052378118e-05 BETTER
I0312 01:00:08.880483 1757646 finetune.py:68] layer 10_o @ epoch 3 new loss 0.00010599800589261577 old loss 0.00010835658758878708 BETTER
I0312 01:00:09.163860 1757530 finetune.py:68] layer 9_up @ epoch 0 new loss 0.00011697116133291274 old loss 0.00011954569345107302 BETTER
I0312 01:00:12.794893 1757762 finetune.py:68] layer 11_o @ epoch 1 new loss 0.00011591691145440564 old loss 0.00011999746493529528 BETTER
I0312 01:00:38.189206 1757414 finetune.py:68] layer 8_up @ epoch 2 new loss 9.320210665464401e-05 old loss 9.429061901755631e-05 BETTER
I0312 01:00:39.188878 1757530 finetune.py:68] layer 9_up @ epoch 1 new loss 0.00011532766075106338 old loss 0.00011697116133291274 BETTER
I0312 01:00:40.885015 1757646 finetune.py:68] layer 10_o @ epoch 4 new loss 0.00010409406968392432 old loss 0.00010599800589261577 BETTER
I0312 01:00:43.814386 1757762 finetune.py:68] layer 11_o @ epoch 2 new loss 0.00011306593660265207 old loss 0.00011591691145440564 BETTER
I0312 01:00:56.196473 1757646 finetune.py:45] layer 10_up initial loss 0.0001531788584543392
I0312 01:01:09.305608 1757530 finetune.py:68] layer 9_up @ epoch 2 new loss 0.00011396538320695981 old loss 0.00011532766075106338 BETTER
I0312 01:01:10.098531 1757414 finetune.py:68] layer 8_up @ epoch 3 new loss 9.227114787790924e-05 old loss 9.320210665464401e-05 BETTER
I0312 01:01:15.012671 1757762 finetune.py:68] layer 11_o @ epoch 3 new loss 0.0001108435244532302 old loss 0.00011306593660265207 BETTER
I0312 01:01:25.170086 1757646 finetune.py:68] layer 10_up @ epoch 0 new loss 0.00014977328828535974 old loss 0.0001531788584543392 BETTER
I0312 01:01:39.345502 1757530 finetune.py:68] layer 9_up @ epoch 3 new loss 0.00011280333274044096 old loss 0.00011396538320695981 BETTER
I0312 01:01:41.883884 1757414 finetune.py:68] layer 8_up @ epoch 4 new loss 9.145923831965774e-05 old loss 9.227114787790924e-05 BETTER
I0312 01:01:46.154452 1757762 finetune.py:68] layer 11_o @ epoch 4 new loss 0.00010907270916504785 old loss 0.0001108435244532302 BETTER
I0312 01:01:55.047219 1757646 finetune.py:68] layer 10_up @ epoch 1 new loss 0.00014756152813788503 old loss 0.00014977328828535974 BETTER
I0312 01:01:57.325029 1757414 finetune.py:45] layer 8_gate initial loss 0.00011436708155088127
I0312 01:02:01.462629 1757762 finetune.py:45] layer 11_up initial loss 0.00016459340986330062
I0312 01:02:09.390541 1757530 finetune.py:68] layer 9_up @ epoch 4 new loss 0.00011179091961821541 old loss 0.00011280333274044096 BETTER
I0312 01:02:24.506321 1757530 finetune.py:45] layer 9_gate initial loss 0.0001393943530274555
I0312 01:02:25.222904 1757646 finetune.py:68] layer 10_up @ epoch 2 new loss 0.00014573785301763564 old loss 0.00014756152813788503 BETTER
I0312 01:02:26.429726 1757414 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.00011321965575916693 old loss 0.00011436708155088127 BETTER
I0312 01:02:30.242552 1757762 finetune.py:68] layer 11_up @ epoch 0 new loss 0.0001611717598279938 old loss 0.00016459340986330062 BETTER
I0312 01:02:52.244107 1757530 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0001380163594149053 old loss 0.0001393943530274555 BETTER
I0312 01:02:55.441860 1757646 finetune.py:68] layer 10_up @ epoch 3 new loss 0.00014419051876757294 old loss 0.00014573785301763564 BETTER
I0312 01:02:56.381568 1757414 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.0001124365662690252 old loss 0.00011321965575916693 BETTER
I0312 01:02:59.772650 1757762 finetune.py:68] layer 11_up @ epoch 1 new loss 0.00015891868679318577 old loss 0.0001611717598279938 BETTER
I0312 01:03:20.646671 1757530 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.0001370468089589849 old loss 0.0001380163594149053 BETTER
I0312 01:03:25.880043 1757646 finetune.py:68] layer 10_up @ epoch 4 new loss 0.00014282340998761356 old loss 0.00014419051876757294 BETTER
I0312 01:03:26.310345 1757414 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.00011178149725310504 old loss 0.0001124365662690252 BETTER
I0312 01:03:29.310071 1757762 finetune.py:68] layer 11_up @ epoch 2 new loss 0.00015708878345321864 old loss 0.00015891868679318577 BETTER
I0312 01:03:41.328052 1757646 finetune.py:45] layer 10_gate initial loss 0.000175846042111516
I0312 01:03:48.638883 1757530 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.00013622544065583497 old loss 0.0001370468089589849 BETTER
I0312 01:03:56.326257 1757414 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.00011119939881609753 old loss 0.00011178149725310504 BETTER
I0312 01:03:58.985776 1757762 finetune.py:68] layer 11_up @ epoch 3 new loss 0.00015553261619061232 old loss 0.00015708878345321864 BETTER
I0312 01:04:08.761634 1757646 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.00017404431127943099 old loss 0.000175846042111516 BETTER
I0312 01:04:16.649254 1757530 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.0001355013228021562 old loss 0.00013622544065583497 BETTER
I0312 01:04:26.464156 1757414 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.00011067909508710727 old loss 0.00011119939881609753 BETTER
I0312 01:04:28.417470 1757762 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00015416809765156358 old loss 0.00015553261619061232 BETTER
I0312 01:04:36.974104 1757646 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.00017277341976296157 old loss 0.00017404431127943099 BETTER
I0312 01:04:43.006042 1757414 finetune.py:45] layer 8_down initial loss 0.0001692409859970212
I0312 01:04:43.994968 1757762 finetune.py:45] layer 11_gate initial loss 0.00019171799067407846
I0312 01:04:45.157282 1757530 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.00013486020907294005 old loss 0.0001355013228021562 BETTER
I0312 01:05:00.852137 1757530 finetune.py:45] layer 9_down initial loss 0.0002034111093962565
I0312 01:05:05.184650 1757646 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.00017170071077998728 old loss 0.00017277341976296157 BETTER
I0312 01:05:10.185743 1757414 finetune.py:68] layer 8_down @ epoch 0 new loss 0.00016922413487918675 old loss 0.0001692409859970212 BETTER
I0312 01:05:11.128978 1757762 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.00018985485075972974 old loss 0.00019171799067407846 BETTER
I0312 01:05:26.725658 1757530 finetune.py:68] layer 9_down @ epoch 0 new loss 0.0002033948403550312 old loss 0.0002034111093962565 BETTER
I0312 01:05:33.196316 1757646 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.00017075141659006476 old loss 0.00017170071077998728 BETTER
I0312 01:05:38.649319 1757414 finetune.py:68] layer 8_down @ epoch 1 new loss 0.00016921614587772638 old loss 0.00016922413487918675 BETTER
I0312 01:05:39.160549 1757762 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.0001885706587927416 old loss 0.00018985485075972974 BETTER
I0312 01:05:53.399620 1757530 finetune.py:68] layer 9_down @ epoch 1 new loss 0.00020338724425528198 old loss 0.0002033948403550312 BETTER
I0312 01:06:01.365773 1757646 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.00016990544099826366 old loss 0.00017075141659006476 BETTER
I0312 01:06:07.141776 1757414 finetune.py:68] layer 8_down @ epoch 2 new loss 0.00016921188216656446 old loss 0.00016921614587772638 BETTER
I0312 01:06:07.374644 1757762 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.0001874755253084004 old loss 0.0001885706587927416 BETTER
I0312 01:06:17.718101 1757646 finetune.py:45] layer 10_down initial loss 0.00024972285609692335
I0312 01:06:20.211608 1757530 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00020338148169685155 old loss 0.00020338724425528198 BETTER
I0312 01:06:35.587371 1757762 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.00018651557911653072 old loss 0.0001874755253084004 BETTER
I0312 01:06:35.708019 1757414 finetune.py:68] layer 8_down @ epoch 3 new loss 0.00016920824418775737 old loss 0.00016921188216656446 BETTER
I0312 01:06:43.632512 1757646 finetune.py:68] layer 10_down @ epoch 0 new loss 0.00024969392688944936 old loss 0.00024972285609692335 BETTER
I0312 01:06:47.144558 1757530 finetune.py:68] layer 9_down @ epoch 3 new loss 0.00020337804744485766 old loss 0.00020338148169685155 BETTER
I0312 01:07:03.737024 1757762 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.00018566587823443115 old loss 0.00018651557911653072 BETTER
I0312 01:07:04.180015 1757414 finetune.py:68] layer 8_down @ epoch 4 new loss 0.0001692055957391858 old loss 0.00016920824418775737 BETTER
8_v proxy err 0.007206937298178673 tr(WHW.T) 530.9967041015625
8_q proxy err 0.0006466276827268302 tr(WHW.T) 7230.701171875
8_k proxy err 0.0004800581082236022 tr(WHW.T) 10648.2275390625
8_o proxy err 0.010487079620361328 tr(WHW.T) 20.19572639465332
8_up proxy err 0.005047373473644257 tr(WHW.T) 866.0263061523438
8_gate proxy err 0.0022902502678334713 tr(WHW.T) 1969.784423828125
8_down proxy err 0.0065858433954417706 tr(WHW.T) 37.25826644897461
I0312 01:07:11.533221 1757646 finetune.py:68] layer 10_down @ epoch 1 new loss 0.00024968062643893063 old loss 0.00024969392688944936 BETTER
I0312 01:07:14.645638 1757530 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00020337579189799726 old loss 0.00020337804744485766 BETTER
9_v proxy err 0.007106396369636059 tr(WHW.T) 565.0663452148438
9_q proxy err 0.0006967269000597298 tr(WHW.T) 6972.60205078125
9_k proxy err 0.00048062155838124454 tr(WHW.T) 10995.431640625
9_o proxy err 0.010707796551287174 tr(WHW.T) 25.691715240478516
9_up proxy err 0.004849821794778109 tr(WHW.T) 970.8540649414062
9_gate proxy err 0.002273304620757699 tr(WHW.T) 2133.108154296875
9_down proxy err 0.006671368610113859 tr(WHW.T) 43.09311294555664
I0312 01:07:19.929651 1757762 finetune.py:45] layer 11_down initial loss 0.0002730061823967844
I0312 01:07:38.497914 1757646 finetune.py:68] layer 10_down @ epoch 2 new loss 0.000249673321377486 old loss 0.00024968062643893063 BETTER
I0312 01:07:45.428123 1757762 finetune.py:68] layer 11_down @ epoch 0 new loss 0.00027298094937577844 old loss 0.0002730061823967844 BETTER
I0312 01:08:05.369237 1757646 finetune.py:68] layer 10_down @ epoch 3 new loss 0.00024966863566078246 old loss 0.000249673321377486 BETTER
I0312 01:08:11.783489 1757762 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0002729679399635643 old loss 0.00027298094937577844 BETTER
I0312 01:08:32.025887 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 12 in 72.63511538505554s
I0312 01:08:32.393780 1757646 finetune.py:68] layer 10_down @ epoch 4 new loss 0.0002496648521628231 old loss 0.00024966863566078246 BETTER
10_v proxy err 0.0071396296843886375 tr(WHW.T) 578.807373046875
10_q proxy err 0.0007079142960719764 tr(WHW.T) 6918.291015625
10_k proxy err 0.0004883556393906474 tr(WHW.T) 11006.7041015625
10_o proxy err 0.01111744437366724 tr(WHW.T) 35.31458282470703
10_up proxy err 0.00456750625744462 tr(WHW.T) 1080.1455078125
10_gate proxy err 0.0022400948219001293 tr(WHW.T) 2263.16650390625
10_down proxy err 0.006370333954691887 tr(WHW.T) 52.461395263671875
I0312 01:08:35.258434 1757878 config.py:54] PyTorch version 2.1.1 available.
I0312 01:08:36.272425 1756366 quantize_finetune_llama.py:183] layer 13 gpu 1
I0312 01:08:36.341956 1757878 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 01:08:38.110156 1757762 finetune.py:68] layer 11_down @ epoch 2 new loss 0.00027296034386381507 old loss 0.0002729679399635643 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:08:44.824413 1757878 finetune.py:45] layer 12_v initial loss 6.266315176617354e-05
I0312 01:09:04.498465 1757762 finetune.py:68] layer 11_down @ epoch 3 new loss 0.00027295458130538464 old loss 0.00027296034386381507 BETTER
I0312 01:09:17.905573 1757878 finetune.py:68] layer 12_v @ epoch 0 new loss 4.809252277482301e-05 old loss 6.266315176617354e-05 BETTER
I0312 01:09:30.872916 1757762 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0002729496918618679 old loss 0.00027295458130538464 BETTER
11_v proxy err 0.007051039021462202 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0008268057717941701 tr(WHW.T) 7029.64501953125
11_k proxy err 0.0005923152784816921 tr(WHW.T) 10520.322265625
11_o proxy err 0.011110136285424232 tr(WHW.T) 36.81632614135742
11_up proxy err 0.00468094227835536 tr(WHW.T) 1139.903076171875
11_gate proxy err 0.002286630915477872 tr(WHW.T) 2394.754150390625
11_down proxy err 0.00649378914386034 tr(WHW.T) 56.25359344482422
I0312 01:09:47.351434 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 13 in 70.57204079627991s
I0312 01:09:50.560231 1757994 config.py:54] PyTorch version 2.1.1 available.
I0312 01:09:51.685867 1756366 quantize_finetune_llama.py:183] layer 14 gpu 2
I0312 01:09:51.763413 1757994 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 01:09:52.113637 1757878 finetune.py:68] layer 12_v @ epoch 1 new loss 4.500933573581278e-05 old loss 4.809252277482301e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:09:59.989759 1757994 finetune.py:45] layer 13_v initial loss 6.674598262179643e-05
I0312 01:10:26.774742 1757878 finetune.py:68] layer 12_v @ epoch 2 new loss 4.318317223805934e-05 old loss 4.500933573581278e-05 BETTER
I0312 01:10:31.367756 1757994 finetune.py:68] layer 13_v @ epoch 0 new loss 5.147774572833441e-05 old loss 6.674598262179643e-05 BETTER
I0312 01:11:01.570813 1757878 finetune.py:68] layer 12_v @ epoch 3 new loss 4.1879306081682444e-05 old loss 4.318317223805934e-05 BETTER
I0312 01:11:02.265269 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 14 in 70.14979815483093s
I0312 01:11:03.880709 1757994 finetune.py:68] layer 13_v @ epoch 1 new loss 4.8253543354803696e-05 old loss 5.147774572833441e-05 BETTER
I0312 01:11:05.587963 1758110 config.py:54] PyTorch version 2.1.1 available.
I0312 01:11:06.652297 1756366 quantize_finetune_llama.py:183] layer 15 gpu 3
I0312 01:11:06.720598 1758110 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:11:14.970410 1758110 finetune.py:45] layer 14_v initial loss 8.756606985116377e-05
I0312 01:11:36.212197 1757994 finetune.py:68] layer 13_v @ epoch 2 new loss 4.637086749426089e-05 old loss 4.8253543354803696e-05 BETTER
I0312 01:11:36.592438 1757878 finetune.py:68] layer 12_v @ epoch 4 new loss 4.09309686801862e-05 old loss 4.1879306081682444e-05 BETTER
I0312 01:11:45.949666 1757878 finetune.py:45] layer 12_q initial loss 5.083933501737192e-05
I0312 01:11:46.397522 1758110 finetune.py:68] layer 14_v @ epoch 0 new loss 6.776432564947754e-05 old loss 8.756606985116377e-05 BETTER
I0312 01:12:09.221014 1757994 finetune.py:68] layer 13_v @ epoch 3 new loss 4.497791451285593e-05 old loss 4.637086749426089e-05 BETTER
I0312 01:12:16.947363 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 15 in 69.70866322517395s
I0312 01:12:18.748192 1758110 finetune.py:68] layer 14_v @ epoch 1 new loss 6.304936687229201e-05 old loss 6.776432564947754e-05 BETTER
I0312 01:12:19.554071 1757878 finetune.py:68] layer 12_q @ epoch 0 new loss 4.781507595907897e-05 old loss 5.083933501737192e-05 BETTER
I0312 01:12:20.283053 1758226 config.py:54] PyTorch version 2.1.1 available.
I0312 01:12:21.302881 1756366 quantize_finetune_llama.py:183] layer 16 gpu 0
I0312 01:12:21.370812 1758226 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:12:29.362068 1758226 finetune.py:45] layer 15_v initial loss 8.664940833114088e-05
I0312 01:12:42.156237 1757994 finetune.py:68] layer 13_v @ epoch 4 new loss 4.4053318561054766e-05 old loss 4.497791451285593e-05 BETTER
I0312 01:12:51.175117 1757994 finetune.py:45] layer 13_q initial loss 5.434869308373891e-05
I0312 01:12:51.458464 1758110 finetune.py:68] layer 14_v @ epoch 2 new loss 6.0219288570806384e-05 old loss 6.304936687229201e-05 BETTER
I0312 01:12:53.917469 1757878 finetune.py:68] layer 12_q @ epoch 1 new loss 4.661814818973653e-05 old loss 4.781507595907897e-05 BETTER
I0312 01:13:00.600154 1758226 finetune.py:68] layer 15_v @ epoch 0 new loss 6.635297177126631e-05 old loss 8.664940833114088e-05 BETTER
I0312 01:13:22.628183 1757994 finetune.py:68] layer 13_q @ epoch 0 new loss 5.075936496723443e-05 old loss 5.434869308373891e-05 BETTER
I0312 01:13:24.310162 1758110 finetune.py:68] layer 14_v @ epoch 3 new loss 5.818529461976141e-05 old loss 6.0219288570806384e-05 BETTER
I0312 01:13:28.424729 1757878 finetune.py:68] layer 12_q @ epoch 2 new loss 4.573736805468798e-05 old loss 4.661814818973653e-05 BETTER
I0312 01:13:32.595990 1758226 finetune.py:68] layer 15_v @ epoch 1 new loss 6.193428271217272e-05 old loss 6.635297177126631e-05 BETTER
I0312 01:13:54.928416 1757994 finetune.py:68] layer 13_q @ epoch 1 new loss 4.943797830492258e-05 old loss 5.075936496723443e-05 BETTER
I0312 01:13:57.580060 1758110 finetune.py:68] layer 14_v @ epoch 4 new loss 5.664397758664563e-05 old loss 5.818529461976141e-05 BETTER
I0312 01:14:03.059613 1757878 finetune.py:68] layer 12_q @ epoch 3 new loss 4.502102092374116e-05 old loss 4.573736805468798e-05 BETTER
I0312 01:14:04.924129 1758226 finetune.py:68] layer 15_v @ epoch 2 new loss 5.922948912484571e-05 old loss 6.193428271217272e-05 BETTER
I0312 01:14:07.288962 1758110 finetune.py:45] layer 14_q initial loss 6.840245623607188e-05
I0312 01:14:27.501996 1757994 finetune.py:68] layer 13_q @ epoch 2 new loss 4.849674951401539e-05 old loss 4.943797830492258e-05 BETTER
I0312 01:14:37.890961 1758226 finetune.py:68] layer 15_v @ epoch 3 new loss 5.7232035032939166e-05 old loss 5.922948912484571e-05 BETTER
I0312 01:14:38.030293 1757878 finetune.py:68] layer 12_q @ epoch 4 new loss 4.440675184014253e-05 old loss 4.502102092374116e-05 BETTER
I0312 01:14:39.567792 1758110 finetune.py:68] layer 14_q @ epoch 0 new loss 6.458286225097254e-05 old loss 6.840245623607188e-05 BETTER
I0312 01:14:47.616425 1757878 finetune.py:45] layer 12_k initial loss 5.165795664652251e-05
I0312 01:15:00.249600 1757994 finetune.py:68] layer 13_q @ epoch 3 new loss 4.7694466047687456e-05 old loss 4.849674951401539e-05 BETTER
I0312 01:15:10.621004 1758226 finetune.py:68] layer 15_v @ epoch 4 new loss 5.5744552810210735e-05 old loss 5.7232035032939166e-05 BETTER
I0312 01:15:12.037286 1758110 finetune.py:68] layer 14_q @ epoch 1 new loss 6.28610941930674e-05 old loss 6.458286225097254e-05 BETTER
I0312 01:15:19.883385 1758226 finetune.py:45] layer 15_q initial loss 6.906107591930777e-05
I0312 01:15:20.723690 1757878 finetune.py:68] layer 12_k @ epoch 0 new loss 5.0132046453654766e-05 old loss 5.165795664652251e-05 BETTER
I0312 01:15:32.980864 1757994 finetune.py:68] layer 13_q @ epoch 4 new loss 4.709431232186034e-05 old loss 4.7694466047687456e-05 BETTER
I0312 01:15:42.134314 1757994 finetune.py:45] layer 13_k initial loss 5.4443476983578876e-05
I0312 01:15:44.489145 1758110 finetune.py:68] layer 14_q @ epoch 2 new loss 6.153237336548045e-05 old loss 6.28610941930674e-05 BETTER
I0312 01:15:51.395292 1758226 finetune.py:68] layer 15_q @ epoch 0 new loss 6.450018554460257e-05 old loss 6.906107591930777e-05 BETTER
I0312 01:15:54.408327 1757878 finetune.py:68] layer 12_k @ epoch 1 new loss 4.9545989895705134e-05 old loss 5.0132046453654766e-05 BETTER
I0312 01:16:13.559996 1757994 finetune.py:68] layer 13_k @ epoch 0 new loss 5.312194844009355e-05 old loss 5.4443476983578876e-05 BETTER
I0312 01:16:16.903419 1758110 finetune.py:68] layer 14_q @ epoch 3 new loss 6.0449878219515085e-05 old loss 6.153237336548045e-05 BETTER
I0312 01:16:23.485844 1758226 finetune.py:68] layer 15_q @ epoch 1 new loss 6.266868876991794e-05 old loss 6.450018554460257e-05 BETTER
I0312 01:16:28.247189 1757878 finetune.py:68] layer 12_k @ epoch 2 new loss 4.909280687570572e-05 old loss 4.9545989895705134e-05 BETTER
I0312 01:16:45.685394 1757994 finetune.py:68] layer 13_k @ epoch 1 new loss 5.25034120073542e-05 old loss 5.312194844009355e-05 BETTER
I0312 01:16:49.307004 1758110 finetune.py:68] layer 14_q @ epoch 4 new loss 5.954979860689491e-05 old loss 6.0449878219515085e-05 BETTER
I0312 01:16:55.836623 1758226 finetune.py:68] layer 15_q @ epoch 2 new loss 6.130451947683468e-05 old loss 6.266868876991794e-05 BETTER
I0312 01:16:59.023914 1758110 finetune.py:45] layer 14_k initial loss 6.888998905196786e-05
I0312 01:17:02.416398 1757878 finetune.py:68] layer 12_k @ epoch 3 new loss 4.8693913413444534e-05 old loss 4.909280687570572e-05 BETTER
I0312 01:17:17.870177 1757994 finetune.py:68] layer 13_k @ epoch 2 new loss 5.2039071306353435e-05 old loss 5.25034120073542e-05 BETTER
I0312 01:17:27.966474 1758226 finetune.py:68] layer 15_q @ epoch 3 new loss 6.0210517403902486e-05 old loss 6.130451947683468e-05 BETTER
I0312 01:17:30.554230 1758110 finetune.py:68] layer 14_k @ epoch 0 new loss 6.695702904835343e-05 old loss 6.888998905196786e-05 BETTER
I0312 01:17:36.738919 1757878 finetune.py:68] layer 12_k @ epoch 4 new loss 4.836521839024499e-05 old loss 4.8693913413444534e-05 BETTER
I0312 01:17:46.244709 1757878 finetune.py:45] layer 12_o initial loss 0.0001426267990609631
I0312 01:17:50.056995 1757994 finetune.py:68] layer 13_k @ epoch 3 new loss 5.162419256521389e-05 old loss 5.2039071306353435e-05 BETTER
I0312 01:18:00.063870 1758226 finetune.py:68] layer 15_q @ epoch 4 new loss 5.9301939472788945e-05 old loss 6.0210517403902486e-05 BETTER
I0312 01:18:02.644521 1758110 finetune.py:68] layer 14_k @ epoch 1 new loss 6.611559365410358e-05 old loss 6.695702904835343e-05 BETTER
I0312 01:18:09.365720 1758226 finetune.py:45] layer 15_k initial loss 6.917127029737458e-05
I0312 01:18:19.006439 1757878 finetune.py:68] layer 12_o @ epoch 0 new loss 0.0001340252929367125 old loss 0.0001426267990609631 BETTER
I0312 01:18:22.170502 1757994 finetune.py:68] layer 13_k @ epoch 4 new loss 5.127174881636165e-05 old loss 5.162419256521389e-05 BETTER
I0312 01:18:31.323144 1757994 finetune.py:45] layer 13_o initial loss 0.00014827963605057448
I0312 01:18:34.736379 1758110 finetune.py:68] layer 14_k @ epoch 2 new loss 6.546961230924353e-05 old loss 6.611559365410358e-05 BETTER
I0312 01:18:40.403522 1758226 finetune.py:68] layer 15_k @ epoch 0 new loss 6.719642988173291e-05 old loss 6.917127029737458e-05 BETTER
I0312 01:18:52.529969 1757878 finetune.py:68] layer 12_o @ epoch 1 new loss 0.00012939619773533195 old loss 0.0001340252929367125 BETTER
I0312 01:19:02.255200 1757994 finetune.py:68] layer 13_o @ epoch 0 new loss 0.00013831623073201627 old loss 0.00014827963605057448 BETTER
I0312 01:19:06.805607 1758110 finetune.py:68] layer 14_k @ epoch 3 new loss 6.490891246357933e-05 old loss 6.546961230924353e-05 BETTER
I0312 01:19:12.249386 1758226 finetune.py:68] layer 15_k @ epoch 1 new loss 6.640249921474606e-05 old loss 6.719642988173291e-05 BETTER
I0312 01:19:26.133571 1757878 finetune.py:68] layer 12_o @ epoch 2 new loss 0.00012616011372301728 old loss 0.00012939619773533195 BETTER
I0312 01:19:33.985831 1757994 finetune.py:68] layer 13_o @ epoch 1 new loss 0.00013323799066711217 old loss 0.00013831623073201627 BETTER
I0312 01:19:39.054001 1758110 finetune.py:68] layer 14_k @ epoch 4 new loss 6.441924051614478e-05 old loss 6.490891246357933e-05 BETTER
I0312 01:19:44.083971 1758226 finetune.py:68] layer 15_k @ epoch 2 new loss 6.57519995002076e-05 old loss 6.640249921474606e-05 BETTER
I0312 01:19:48.390868 1758110 finetune.py:45] layer 14_o initial loss 0.00019029290706384927
I0312 01:19:59.778475 1757878 finetune.py:68] layer 12_o @ epoch 3 new loss 0.00012363922724034637 old loss 0.00012616011372301728 BETTER
I0312 01:20:05.695763 1757994 finetune.py:68] layer 13_o @ epoch 2 new loss 0.00012971025716979057 old loss 0.00013323799066711217 BETTER
I0312 01:20:15.907115 1758226 finetune.py:68] layer 15_k @ epoch 3 new loss 6.521153409266844e-05 old loss 6.57519995002076e-05 BETTER
I0312 01:20:19.301845 1758110 finetune.py:68] layer 14_o @ epoch 0 new loss 0.00017845723778009415 old loss 0.00019029290706384927 BETTER
I0312 01:20:33.482080 1757878 finetune.py:68] layer 12_o @ epoch 4 new loss 0.0001216187301906757 old loss 0.00012363922724034637 BETTER
I0312 01:20:37.407897 1757994 finetune.py:68] layer 13_o @ epoch 3 new loss 0.00012703906395472586 old loss 0.00012971025716979057 BETTER
I0312 01:20:47.899713 1758226 finetune.py:68] layer 15_k @ epoch 4 new loss 6.475920235970989e-05 old loss 6.521153409266844e-05 BETTER
I0312 01:20:49.332121 1757878 finetune.py:45] layer 12_up initial loss 0.00018391975027043372
I0312 01:20:51.290035 1758110 finetune.py:68] layer 14_o @ epoch 1 new loss 0.00017221548478119075 old loss 0.00017845723778009415 BETTER
I0312 01:20:57.318327 1758226 finetune.py:45] layer 15_o initial loss 0.00018870107305701822
I0312 01:21:09.097816 1757994 finetune.py:68] layer 13_o @ epoch 4 new loss 0.00012488813081290573 old loss 0.00012703906395472586 BETTER
I0312 01:21:19.949339 1757878 finetune.py:68] layer 12_up @ epoch 0 new loss 0.0001800121390260756 old loss 0.00018391975027043372 BETTER
I0312 01:21:23.140042 1758110 finetune.py:68] layer 14_o @ epoch 2 new loss 0.00016786310879979283 old loss 0.00017221548478119075 BETTER
I0312 01:21:24.448728 1757994 finetune.py:45] layer 13_up initial loss 0.00019897607853636146
I0312 01:21:27.552721 1758226 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0001749573857523501 old loss 0.00018870107305701822 BETTER
I0312 01:21:51.816405 1757878 finetune.py:68] layer 12_up @ epoch 1 new loss 0.0001774462580215186 old loss 0.0001800121390260756 BETTER
I0312 01:21:53.679876 1757994 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00019402700127102435 old loss 0.00019897607853636146 BETTER
I0312 01:21:55.115162 1758110 finetune.py:68] layer 14_o @ epoch 3 new loss 0.00016451667761430144 old loss 0.00016786310879979283 BETTER
I0312 01:21:58.775078 1758226 finetune.py:68] layer 15_o @ epoch 1 new loss 0.00016840639000292867 old loss 0.0001749573857523501 BETTER
I0312 01:22:23.727682 1757994 finetune.py:68] layer 13_up @ epoch 1 new loss 0.0001908818376250565 old loss 0.00019402700127102435 BETTER
I0312 01:22:23.879257 1757878 finetune.py:68] layer 12_up @ epoch 2 new loss 0.00017535769438836724 old loss 0.0001774462580215186 BETTER
I0312 01:22:27.103160 1758110 finetune.py:68] layer 14_o @ epoch 4 new loss 0.00016181408136617392 old loss 0.00016451667761430144 BETTER
I0312 01:22:29.734388 1758226 finetune.py:68] layer 15_o @ epoch 2 new loss 0.00016392534598708153 old loss 0.00016840639000292867 BETTER
I0312 01:22:42.475140 1758110 finetune.py:45] layer 14_up initial loss 0.0002446470607537776
I0312 01:22:54.217296 1757994 finetune.py:68] layer 13_up @ epoch 2 new loss 0.00018838487449102104 old loss 0.0001908818376250565 BETTER
I0312 01:22:55.926270 1757878 finetune.py:68] layer 12_up @ epoch 3 new loss 0.00017358515469823033 old loss 0.00017535769438836724 BETTER
I0312 01:23:00.803144 1758226 finetune.py:68] layer 15_o @ epoch 3 new loss 0.00016056621097959578 old loss 0.00016392534598708153 BETTER
I0312 01:23:11.528986 1758110 finetune.py:68] layer 14_up @ epoch 0 new loss 0.00023919947852846235 old loss 0.0002446470607537776 BETTER
I0312 01:23:24.206596 1757994 finetune.py:68] layer 13_up @ epoch 3 new loss 0.00018630384874995798 old loss 0.00018838487449102104 BETTER
I0312 01:23:27.874089 1757878 finetune.py:68] layer 12_up @ epoch 4 new loss 0.0001720478176139295 old loss 0.00017358515469823033 BETTER
I0312 01:23:31.846389 1758226 finetune.py:68] layer 15_o @ epoch 4 new loss 0.0001579083618707955 old loss 0.00016056621097959578 BETTER
I0312 01:23:41.439898 1758110 finetune.py:68] layer 14_up @ epoch 1 new loss 0.00023571474594064057 old loss 0.00023919947852846235 BETTER
I0312 01:23:43.526691 1757878 finetune.py:45] layer 12_gate initial loss 0.0002159697178285569
I0312 01:23:47.119988 1758226 finetune.py:45] layer 15_up initial loss 0.000258653104538098
I0312 01:23:54.425018 1757994 finetune.py:68] layer 13_up @ epoch 4 new loss 0.00018452912627253681 old loss 0.00018630384874995798 BETTER
I0312 01:24:09.613655 1757994 finetune.py:45] layer 13_gate initial loss 0.00023759441683068871
I0312 01:24:11.487905 1758110 finetune.py:68] layer 14_up @ epoch 2 new loss 0.00023294305719900876 old loss 0.00023571474594064057 BETTER
I0312 01:24:12.730808 1757878 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.00021383495186455548 old loss 0.0002159697178285569 BETTER
I0312 01:24:15.890666 1758226 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0002515401865821332 old loss 0.000258653104538098 BETTER
I0312 01:24:37.251292 1757994 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.0002349757414776832 old loss 0.00023759441683068871 BETTER
I0312 01:24:41.534230 1758110 finetune.py:68] layer 14_up @ epoch 3 new loss 0.00023060489911586046 old loss 0.00023294305719900876 BETTER
I0312 01:24:42.743591 1757878 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0002123405138263479 old loss 0.00021383495186455548 BETTER
I0312 01:24:45.470566 1758226 finetune.py:68] layer 15_up @ epoch 1 new loss 0.0002473881177138537 old loss 0.0002515401865821332 BETTER
I0312 01:25:05.383636 1757994 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0002331582800252363 old loss 0.0002349757414776832 BETTER
I0312 01:25:11.538507 1758110 finetune.py:68] layer 14_up @ epoch 4 new loss 0.00022858857118990272 old loss 0.00023060489911586046 BETTER
I0312 01:25:12.734054 1757878 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.00021109134831931442 old loss 0.0002123405138263479 BETTER
I0312 01:25:15.021333 1758226 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0002441674587316811 old loss 0.0002473881177138537 BETTER
I0312 01:25:26.865005 1758110 finetune.py:45] layer 14_gate initial loss 0.0002909080358222127
I0312 01:25:33.375540 1757994 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.00023166669416241348 old loss 0.0002331582800252363 BETTER
I0312 01:25:42.723278 1757878 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.00021000443666707724 old loss 0.00021109134831931442 BETTER
I0312 01:25:44.626306 1758226 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0002414777991361916 old loss 0.0002441674587316811 BETTER
I0312 01:25:54.436876 1758110 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.00028791913064196706 old loss 0.0002909080358222127 BETTER
I0312 01:26:01.700158 1757994 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.00023037374194245785 old loss 0.00023166669416241348 BETTER
I0312 01:26:12.924159 1757878 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.00020904067787341774 old loss 0.00021000443666707724 BETTER
I0312 01:26:14.183073 1758226 finetune.py:68] layer 15_up @ epoch 4 new loss 0.0002392412134213373 old loss 0.0002414777991361916 BETTER
I0312 01:26:22.732769 1758110 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0002858662628568709 old loss 0.00028791913064196706 BETTER
I0312 01:26:29.361634 1757878 finetune.py:45] layer 12_down initial loss 0.0003081296163145453
I0312 01:26:29.946847 1758226 finetune.py:45] layer 15_gate initial loss 0.0003158294130116701
I0312 01:26:30.517143 1757994 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0002292429853696376 old loss 0.00023037374194245785 BETTER
I0312 01:26:46.128253 1757994 finetune.py:45] layer 13_down initial loss 0.0003496935241855681
I0312 01:26:51.021505 1758110 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.00028419573209248483 old loss 0.0002858662628568709 BETTER
I0312 01:26:56.741396 1757878 finetune.py:68] layer 12_down @ epoch 0 new loss 0.0003080994647461921 old loss 0.0003081296163145453 BETTER
I0312 01:26:57.218971 1758226 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.00031205880804918706 old loss 0.0003158294130116701 BETTER
I0312 01:27:12.070950 1757994 finetune.py:68] layer 13_down @ epoch 0 new loss 0.0003496603458188474 old loss 0.0003496935241855681 BETTER
I0312 01:27:19.392142 1758110 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.00028275299700908363 old loss 0.00028419573209248483 BETTER
I0312 01:27:25.221288 1757878 finetune.py:68] layer 12_down @ epoch 1 new loss 0.00030808450537733734 old loss 0.0003080994647461921 BETTER
I0312 01:27:25.342157 1758226 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.000309605646179989 old loss 0.00031205880804918706 BETTER
I0312 01:27:39.054885 1757994 finetune.py:68] layer 13_down @ epoch 1 new loss 0.00034964195219799876 old loss 0.0003496603458188474 BETTER
I0312 01:27:47.244334 1758110 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.00028146596741862595 old loss 0.00028275299700908363 BETTER
I0312 01:27:53.611516 1758226 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.00030762547976337373 old loss 0.000309605646179989 BETTER
I0312 01:27:53.783730 1757878 finetune.py:68] layer 12_down @ epoch 2 new loss 0.00030807661823928356 old loss 0.00030808450537733734 BETTER
I0312 01:28:03.295561 1758110 finetune.py:45] layer 14_down initial loss 0.00042108146590180695
I0312 01:28:06.027255 1757994 finetune.py:68] layer 13_down @ epoch 2 new loss 0.00034963106736540794 old loss 0.00034964195219799876 BETTER
I0312 01:28:21.769821 1758226 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0003059339360333979 old loss 0.00030762547976337373 BETTER
I0312 01:28:22.275636 1757878 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0003080704773310572 old loss 0.00030807661823928356 BETTER
I0312 01:28:29.289140 1758110 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0004210395854897797 old loss 0.00042108146590180695 BETTER
I0312 01:28:33.019679 1757994 finetune.py:68] layer 13_down @ epoch 3 new loss 0.00034962367499247193 old loss 0.00034963106736540794 BETTER
I0312 01:28:50.015485 1758226 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0003044716431759298 old loss 0.0003059339360333979 BETTER
I0312 01:28:50.754970 1757878 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0003080652095377445 old loss 0.0003080704773310572 BETTER
12_v proxy err 0.007286907639354467 tr(WHW.T) 703.318603515625
12_q proxy err 0.0008316043531522155 tr(WHW.T) 7048.1455078125
12_k proxy err 0.0005803576787002385 tr(WHW.T) 10903.951171875
12_o proxy err 0.0116191441193223 tr(WHW.T) 39.45016860961914
12_up proxy err 0.004654474090784788 tr(WHW.T) 1228.7242431640625
12_gate proxy err 0.002452707849442959 tr(WHW.T) 2384.668701171875
12_down proxy err 0.006435390096157789 tr(WHW.T) 64.3212890625
I0312 01:28:56.210991 1758110 finetune.py:68] layer 14_down @ epoch 1 new loss 0.00042101877625100315 old loss 0.0004210395854897797 BETTER
I0312 01:29:00.729804 1757994 finetune.py:68] layer 13_down @ epoch 4 new loss 0.00034961733035743237 old loss 0.00034962367499247193 BETTER
13_v proxy err 0.007450896315276623 tr(WHW.T) 714.5677490234375
13_q proxy err 0.0008662822656333447 tr(WHW.T) 6958.51904296875
13_k proxy err 0.0006110031972639263 tr(WHW.T) 10437.3876953125
13_o proxy err 0.01022540032863617 tr(WHW.T) 46.032135009765625
13_up proxy err 0.004468437284231186 tr(WHW.T) 1367.4476318359375
13_gate proxy err 0.0023991873022168875 tr(WHW.T) 2602.9013671875
13_down proxy err 0.006346619687974453 tr(WHW.T) 79.57452392578125
I0312 01:29:06.473574 1758226 finetune.py:45] layer 15_down initial loss 0.0004787757352460176
I0312 01:29:24.091913 1758110 finetune.py:68] layer 14_down @ epoch 2 new loss 0.00042100640712305903 old loss 0.00042101877625100315 BETTER
I0312 01:29:31.841675 1758226 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0004787331272382289 old loss 0.0004787757352460176 BETTER
I0312 01:29:50.865851 1758110 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0004209962789900601 old loss 0.00042100640712305903 BETTER
I0312 01:29:58.048703 1758226 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00047871016431599855 old loss 0.0004787331272382289 BETTER
I0312 01:30:17.538653 1758110 finetune.py:68] layer 14_down @ epoch 4 new loss 0.0004209900216665119 old loss 0.0004209962789900601 BETTER
I0312 01:30:18.801364 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 16 in 73.290611743927s
14_v proxy err 0.00789466593414545 tr(WHW.T) 706.1612548828125
14_q proxy err 0.0008866646094247699 tr(WHW.T) 7079.6640625
14_k proxy err 0.0005943930591456592 tr(WHW.T) 11305.572265625
14_o proxy err 0.011678318493068218 tr(WHW.T) 51.12453842163086
14_up proxy err 0.004566384945064783 tr(WHW.T) 1465.138427734375
14_gate proxy err 0.0025360221043229103 tr(WHW.T) 2686.191162109375
14_down proxy err 0.006441282574087381 tr(WHW.T) 90.51762390136719
I0312 01:30:22.014323 1758342 config.py:54] PyTorch version 2.1.1 available.
I0312 01:30:23.052564 1756366 quantize_finetune_llama.py:183] layer 17 gpu 1
I0312 01:30:23.128829 1758342 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 01:30:24.460366 1758226 finetune.py:68] layer 15_down @ epoch 2 new loss 0.0004786961944773793 old loss 0.00047871016431599855 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:30:31.563530 1758342 finetune.py:45] layer 16_v initial loss 0.00010705670865718275
I0312 01:30:50.880867 1758226 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0004786866484209895 old loss 0.0004786961944773793 BETTER
I0312 01:31:04.682864 1758342 finetune.py:68] layer 16_v @ epoch 0 new loss 8.400312071898952e-05 old loss 0.00010705670865718275 BETTER
I0312 01:31:17.335813 1758226 finetune.py:68] layer 15_down @ epoch 4 new loss 0.00047867937246337533 old loss 0.0004786866484209895 BETTER
15_v proxy err 0.007064803037792444 tr(WHW.T) 762.7275390625
15_q proxy err 0.0008446080610156059 tr(WHW.T) 7254.5126953125
15_k proxy err 0.0005867670406587422 tr(WHW.T) 11081.603515625
15_o proxy err 0.009599240496754646 tr(WHW.T) 59.839786529541016
15_up proxy err 0.004435971844941378 tr(WHW.T) 1640.675537109375
15_gate proxy err 0.002543187700212002 tr(WHW.T) 2904.663818359375
15_down proxy err 0.006362580694258213 tr(WHW.T) 114.4164810180664
I0312 01:31:33.721232 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 17 in 70.20992922782898s
I0312 01:31:36.987460 1758458 config.py:54] PyTorch version 2.1.1 available.
I0312 01:31:37.988839 1756366 quantize_finetune_llama.py:183] layer 18 gpu 2
I0312 01:31:38.056324 1758458 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 01:31:39.225939 1758342 finetune.py:68] layer 16_v @ epoch 1 new loss 7.8517863585148e-05 old loss 8.400312071898952e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:31:46.224506 1758458 finetune.py:45] layer 17_v initial loss 9.041000885190442e-05
I0312 01:32:14.069899 1758342 finetune.py:68] layer 16_v @ epoch 2 new loss 7.512173033319414e-05 old loss 7.8517863585148e-05 BETTER
I0312 01:32:17.617031 1758458 finetune.py:68] layer 17_v @ epoch 0 new loss 7.142353570088744e-05 old loss 9.041000885190442e-05 BETTER
I0312 01:32:49.058555 1758342 finetune.py:68] layer 16_v @ epoch 3 new loss 7.267655018949881e-05 old loss 7.512173033319414e-05 BETTER
I0312 01:32:49.145713 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 18 in 70.68797326087952s
I0312 01:32:50.016230 1758458 finetune.py:68] layer 17_v @ epoch 1 new loss 6.69813816784881e-05 old loss 7.142353570088744e-05 BETTER
I0312 01:32:52.416001 1758574 config.py:54] PyTorch version 2.1.1 available.
I0312 01:32:53.421097 1756366 quantize_finetune_llama.py:183] layer 19 gpu 3
I0312 01:32:53.485693 1758574 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:33:01.733109 1758574 finetune.py:45] layer 18_v initial loss 8.857593638822436e-05
I0312 01:33:22.688770 1758458 finetune.py:68] layer 17_v @ epoch 2 new loss 6.422684236895293e-05 old loss 6.69813816784881e-05 BETTER
I0312 01:33:24.022200 1758342 finetune.py:68] layer 16_v @ epoch 4 new loss 7.084941898938268e-05 old loss 7.267655018949881e-05 BETTER
I0312 01:33:33.479327 1758574 finetune.py:68] layer 18_v @ epoch 0 new loss 7.082309457473457e-05 old loss 8.857593638822436e-05 BETTER
I0312 01:33:33.511248 1758342 finetune.py:45] layer 16_q initial loss 8.594340033596382e-05
I0312 01:33:55.599215 1758458 finetune.py:68] layer 17_v @ epoch 3 new loss 6.22616644250229e-05 old loss 6.422684236895293e-05 BETTER
I0312 01:34:05.048992 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 19 in 71.23301887512207s
I0312 01:34:05.925117 1758574 finetune.py:68] layer 18_v @ epoch 1 new loss 6.694237526971847e-05 old loss 7.082309457473457e-05 BETTER
I0312 01:34:07.096227 1758342 finetune.py:68] layer 16_q @ epoch 0 new loss 8.121728751575574e-05 old loss 8.594340033596382e-05 BETTER
I0312 01:34:08.297321 1758690 config.py:54] PyTorch version 2.1.1 available.
I0312 01:34:09.329015 1756366 quantize_finetune_llama.py:183] layer 20 gpu 0
I0312 01:34:09.397678 1758690 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:34:17.390066 1758690 finetune.py:45] layer 19_v initial loss 8.685893408255652e-05
I0312 01:34:28.952485 1758458 finetune.py:68] layer 17_v @ epoch 4 new loss 6.0852595197502524e-05 old loss 6.22616644250229e-05 BETTER
I0312 01:34:38.109941 1758458 finetune.py:45] layer 17_q initial loss 7.575120980618522e-05
I0312 01:34:38.953817 1758574 finetune.py:68] layer 18_v @ epoch 2 new loss 6.464124453486875e-05 old loss 6.694237526971847e-05 BETTER
I0312 01:34:41.600411 1758342 finetune.py:68] layer 16_q @ epoch 1 new loss 7.910254498710856e-05 old loss 8.121728751575574e-05 BETTER
I0312 01:34:48.567076 1758690 finetune.py:68] layer 19_v @ epoch 0 new loss 6.971072434680536e-05 old loss 8.685893408255652e-05 BETTER
I0312 01:35:09.782215 1758458 finetune.py:68] layer 17_q @ epoch 0 new loss 7.097283378243446e-05 old loss 7.575120980618522e-05 BETTER
I0312 01:35:12.024432 1758574 finetune.py:68] layer 18_v @ epoch 3 new loss 6.288626173045486e-05 old loss 6.464124453486875e-05 BETTER
I0312 01:35:16.096091 1758342 finetune.py:68] layer 16_q @ epoch 2 new loss 7.751520752208307e-05 old loss 7.910254498710856e-05 BETTER
I0312 01:35:20.552279 1758690 finetune.py:68] layer 19_v @ epoch 1 new loss 6.607596151297912e-05 old loss 6.971072434680536e-05 BETTER
I0312 01:35:42.246141 1758458 finetune.py:68] layer 17_q @ epoch 1 new loss 6.906056660227478e-05 old loss 7.097283378243446e-05 BETTER
I0312 01:35:45.056407 1758574 finetune.py:68] layer 18_v @ epoch 4 new loss 6.165273953229189e-05 old loss 6.288626173045486e-05 BETTER
I0312 01:35:50.604256 1758342 finetune.py:68] layer 16_q @ epoch 3 new loss 7.628422463312745e-05 old loss 7.751520752208307e-05 BETTER
I0312 01:35:52.939121 1758690 finetune.py:68] layer 19_v @ epoch 2 new loss 6.37709308648482e-05 old loss 6.607596151297912e-05 BETTER
I0312 01:35:54.631805 1758574 finetune.py:45] layer 18_q initial loss 8.069296745816246e-05
I0312 01:36:14.716235 1758458 finetune.py:68] layer 17_q @ epoch 2 new loss 6.757707160431892e-05 old loss 6.906056660227478e-05 BETTER
I0312 01:36:25.360681 1758342 finetune.py:68] layer 16_q @ epoch 4 new loss 7.523855310864747e-05 old loss 7.628422463312745e-05 BETTER
I0312 01:36:25.911366 1758690 finetune.py:68] layer 19_v @ epoch 3 new loss 6.215965549927205e-05 old loss 6.37709308648482e-05 BETTER
I0312 01:36:26.483175 1758574 finetune.py:68] layer 18_q @ epoch 0 new loss 7.559141522506252e-05 old loss 8.069296745816246e-05 BETTER
I0312 01:36:34.628320 1758342 finetune.py:45] layer 16_k initial loss 8.656184945721179e-05
I0312 01:36:47.046315 1758458 finetune.py:68] layer 17_q @ epoch 3 new loss 6.645242683589458e-05 old loss 6.757707160431892e-05 BETTER
I0312 01:36:58.087680 1758690 finetune.py:68] layer 19_v @ epoch 4 new loss 6.098347148508765e-05 old loss 6.215965549927205e-05 BETTER
I0312 01:36:58.774065 1758574 finetune.py:68] layer 18_q @ epoch 1 new loss 7.352961256401613e-05 old loss 7.559141522506252e-05 BETTER
I0312 01:37:07.136544 1758690 finetune.py:45] layer 19_q initial loss 7.817996811354533e-05
I0312 01:37:07.435271 1758342 finetune.py:68] layer 16_k @ epoch 0 new loss 8.487869490636513e-05 old loss 8.656184945721179e-05 BETTER
I0312 01:37:19.694653 1758458 finetune.py:68] layer 17_q @ epoch 4 new loss 6.551685510203242e-05 old loss 6.645242683589458e-05 BETTER
I0312 01:37:28.831796 1758458 finetune.py:45] layer 17_k initial loss 7.671723869862035e-05
I0312 01:37:31.204709 1758574 finetune.py:68] layer 18_q @ epoch 2 new loss 7.204017310868949e-05 old loss 7.352961256401613e-05 BETTER
I0312 01:37:38.529275 1758690 finetune.py:68] layer 19_q @ epoch 0 new loss 7.344421464949846e-05 old loss 7.817996811354533e-05 BETTER
I0312 01:37:41.442567 1758342 finetune.py:68] layer 16_k @ epoch 1 new loss 8.396475459448993e-05 old loss 8.487869490636513e-05 BETTER
I0312 01:38:00.560873 1758458 finetune.py:68] layer 17_k @ epoch 0 new loss 7.468528201570734e-05 old loss 7.671723869862035e-05 BETTER
I0312 01:38:04.014484 1758574 finetune.py:68] layer 18_q @ epoch 3 new loss 7.091838779160753e-05 old loss 7.204017310868949e-05 BETTER
I0312 01:38:10.904229 1758690 finetune.py:68] layer 19_q @ epoch 1 new loss 7.166416617110372e-05 old loss 7.344421464949846e-05 BETTER
I0312 01:38:15.334003 1758342 finetune.py:68] layer 16_k @ epoch 2 new loss 8.326762326760218e-05 old loss 8.396475459448993e-05 BETTER
I0312 01:38:32.793841 1758458 finetune.py:68] layer 17_k @ epoch 1 new loss 7.381934119621292e-05 old loss 7.468528201570734e-05 BETTER
I0312 01:38:36.564977 1758574 finetune.py:68] layer 18_q @ epoch 4 new loss 7.001247286098078e-05 old loss 7.091838779160753e-05 BETTER
I0312 01:38:42.955024 1758690 finetune.py:68] layer 19_q @ epoch 2 new loss 7.035257294774055e-05 old loss 7.166416617110372e-05 BETTER
I0312 01:38:45.879046 1758574 finetune.py:45] layer 18_k initial loss 8.436966163571924e-05
I0312 01:38:49.306747 1758342 finetune.py:68] layer 16_k @ epoch 3 new loss 8.268202509498224e-05 old loss 8.326762326760218e-05 BETTER
I0312 01:39:04.902668 1758458 finetune.py:68] layer 17_k @ epoch 2 new loss 7.31689651729539e-05 old loss 7.381934119621292e-05 BETTER
I0312 01:39:15.072547 1758690 finetune.py:68] layer 19_q @ epoch 3 new loss 6.933838449185714e-05 old loss 7.035257294774055e-05 BETTER
I0312 01:39:17.291399 1758574 finetune.py:68] layer 18_k @ epoch 0 new loss 8.240077295340598e-05 old loss 8.436966163571924e-05 BETTER
I0312 01:39:23.296822 1758342 finetune.py:68] layer 16_k @ epoch 4 new loss 8.219948358600959e-05 old loss 8.268202509498224e-05 BETTER
I0312 01:39:32.714274 1758342 finetune.py:45] layer 16_o initial loss 0.00023695359413977712
I0312 01:39:36.992995 1758458 finetune.py:68] layer 17_k @ epoch 3 new loss 7.263101724674925e-05 old loss 7.31689651729539e-05 BETTER
I0312 01:39:47.193696 1758690 finetune.py:68] layer 19_q @ epoch 4 new loss 6.850757199572399e-05 old loss 6.933838449185714e-05 BETTER
I0312 01:39:49.381029 1758574 finetune.py:68] layer 18_k @ epoch 1 new loss 8.154314127750695e-05 old loss 8.240077295340598e-05 BETTER
I0312 01:39:56.341845 1758690 finetune.py:45] layer 19_k initial loss 8.15925159258768e-05
I0312 01:40:05.319350 1758342 finetune.py:68] layer 16_o @ epoch 0 new loss 0.00022187776630744338 old loss 0.00023695359413977712 BETTER
I0312 01:40:09.084397 1758458 finetune.py:68] layer 17_k @ epoch 4 new loss 7.219784311018884e-05 old loss 7.263101724674925e-05 BETTER
I0312 01:40:18.597899 1758458 finetune.py:45] layer 17_o initial loss 0.00018540500605013222
I0312 01:40:21.390927 1758574 finetune.py:68] layer 18_k @ epoch 2 new loss 8.092639473034069e-05 old loss 8.154314127750695e-05 BETTER
I0312 01:40:27.381749 1758690 finetune.py:68] layer 19_k @ epoch 0 new loss 7.980820373632014e-05 old loss 8.15925159258768e-05 BETTER
I0312 01:40:38.610513 1758342 finetune.py:68] layer 16_o @ epoch 1 new loss 0.00021412991918623447 old loss 0.00022187776630744338 BETTER
I0312 01:40:49.570604 1758458 finetune.py:68] layer 17_o @ epoch 0 new loss 0.00017529402975924313 old loss 0.00018540500605013222 BETTER
I0312 01:40:53.522042 1758574 finetune.py:68] layer 18_k @ epoch 3 new loss 8.040694228839129e-05 old loss 8.092639473034069e-05 BETTER
I0312 01:40:59.266966 1758690 finetune.py:68] layer 19_k @ epoch 1 new loss 7.901620119810104e-05 old loss 7.980820373632014e-05 BETTER
I0312 01:41:12.064054 1758342 finetune.py:68] layer 16_o @ epoch 2 new loss 0.0002086467866320163 old loss 0.00021412991918623447 BETTER
I0312 01:41:21.200267 1758458 finetune.py:68] layer 17_o @ epoch 1 new loss 0.0001702203298918903 old loss 0.00017529402975924313 BETTER
I0312 01:41:25.632285 1758574 finetune.py:68] layer 18_k @ epoch 4 new loss 8.000547677511349e-05 old loss 8.040694228839129e-05 BETTER
I0312 01:41:31.062661 1758690 finetune.py:68] layer 19_k @ epoch 2 new loss 7.84388612373732e-05 old loss 7.901620119810104e-05 BETTER
I0312 01:41:34.947989 1758574 finetune.py:45] layer 18_o initial loss 0.0001999309315579012
I0312 01:41:45.485405 1758342 finetune.py:68] layer 16_o @ epoch 3 new loss 0.00020446650160010904 old loss 0.0002086467866320163 BETTER
I0312 01:41:52.851806 1758458 finetune.py:68] layer 17_o @ epoch 2 new loss 0.00016665496514178813 old loss 0.0001702203298918903 BETTER
I0312 01:42:02.874858 1758690 finetune.py:68] layer 19_k @ epoch 3 new loss 7.796980207785964e-05 old loss 7.84388612373732e-05 BETTER
I0312 01:42:05.879318 1758574 finetune.py:68] layer 18_o @ epoch 0 new loss 0.00018964477931149304 old loss 0.0001999309315579012 BETTER
I0312 01:42:19.083122 1758342 finetune.py:68] layer 16_o @ epoch 4 new loss 0.00020106887677684426 old loss 0.00020446650160010904 BETTER
I0312 01:42:24.402302 1758458 finetune.py:68] layer 17_o @ epoch 3 new loss 0.00016394778504036367 old loss 0.00016665496514178813 BETTER
I0312 01:42:34.403841 1758342 finetune.py:45] layer 16_up initial loss 0.0003317836672067642
I0312 01:42:34.824883 1758690 finetune.py:68] layer 19_k @ epoch 4 new loss 7.759362779324874e-05 old loss 7.796980207785964e-05 BETTER
I0312 01:42:37.629497 1758574 finetune.py:68] layer 18_o @ epoch 1 new loss 0.000184647724381648 old loss 0.00018964477931149304 BETTER
I0312 01:42:44.017528 1758690 finetune.py:45] layer 19_o initial loss 0.00018917725537903607
I0312 01:42:56.030102 1758458 finetune.py:68] layer 17_o @ epoch 4 new loss 0.00016180312377400696 old loss 0.00016394778504036367 BETTER
I0312 01:43:05.003207 1758342 finetune.py:68] layer 16_up @ epoch 0 new loss 0.00032328456290997565 old loss 0.0003317836672067642 BETTER
I0312 01:43:09.440063 1758574 finetune.py:68] layer 18_o @ epoch 2 new loss 0.0001811583060771227 old loss 0.000184647724381648 BETTER
I0312 01:43:11.220245 1758458 finetune.py:45] layer 17_up initial loss 0.0003093916457146406
I0312 01:43:14.431258 1758690 finetune.py:68] layer 19_o @ epoch 0 new loss 0.0001801023754524067 old loss 0.00018917725537903607 BETTER
I0312 01:43:36.821787 1758342 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0003181323118042201 old loss 0.00032328456290997565 BETTER
I0312 01:43:40.275135 1758458 finetune.py:68] layer 17_up @ epoch 0 new loss 0.00030160456662997603 old loss 0.0003093916457146406 BETTER
I0312 01:43:41.250404 1758574 finetune.py:68] layer 18_o @ epoch 3 new loss 0.00017854757606983185 old loss 0.0001811583060771227 BETTER
I0312 01:43:45.521530 1758690 finetune.py:68] layer 19_o @ epoch 1 new loss 0.0001759569568093866 old loss 0.0001801023754524067 BETTER
I0312 01:44:08.621786 1758342 finetune.py:68] layer 16_up @ epoch 2 new loss 0.00031413466786034405 old loss 0.0003181323118042201 BETTER
I0312 01:44:10.201021 1758458 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0002969661436509341 old loss 0.00030160456662997603 BETTER
I0312 01:44:13.231183 1758574 finetune.py:68] layer 18_o @ epoch 4 new loss 0.000176468602148816 old loss 0.00017854757606983185 BETTER
I0312 01:44:16.432310 1758690 finetune.py:68] layer 19_o @ epoch 2 new loss 0.00017315076547674835 old loss 0.0001759569568093866 BETTER
I0312 01:44:28.490042 1758574 finetune.py:45] layer 18_up initial loss 0.0003544247301761061
I0312 01:44:39.970411 1758458 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0002934686199296266 old loss 0.0002969661436509341 BETTER
I0312 01:44:40.562029 1758342 finetune.py:68] layer 16_up @ epoch 3 new loss 0.00031083152862265706 old loss 0.00031413466786034405 BETTER
I0312 01:44:47.641915 1758690 finetune.py:68] layer 19_o @ epoch 3 new loss 0.00017113419016823173 old loss 0.00017315076547674835 BETTER
I0312 01:44:57.524704 1758574 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0003454477118793875 old loss 0.0003544247301761061 BETTER
I0312 01:45:09.915242 1758458 finetune.py:68] layer 17_up @ epoch 3 new loss 0.00029057671781629324 old loss 0.0002934686199296266 BETTER
I0312 01:45:12.358722 1758342 finetune.py:68] layer 16_up @ epoch 4 new loss 0.0003080572350881994 old loss 0.00031083152862265706 BETTER
I0312 01:45:18.674215 1758690 finetune.py:68] layer 19_o @ epoch 4 new loss 0.00016952006262727082 old loss 0.00017113419016823173 BETTER
I0312 01:45:27.495745 1758574 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0003402859147172421 old loss 0.0003454477118793875 BETTER
I0312 01:45:27.528345 1758342 finetune.py:45] layer 16_gate initial loss 0.00040805793832987547
I0312 01:45:33.919578 1758690 finetune.py:45] layer 19_up initial loss 0.0003704972332343459
I0312 01:45:39.840194 1758458 finetune.py:68] layer 17_up @ epoch 4 new loss 0.00028817058773711324 old loss 0.00029057671781629324 BETTER
I0312 01:45:54.938738 1758458 finetune.py:45] layer 17_gate initial loss 0.00040040211752057076
I0312 01:45:56.887886 1758342 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.00040339890983887017 old loss 0.00040805793832987547 BETTER
I0312 01:45:57.906189 1758574 finetune.py:68] layer 18_up @ epoch 2 new loss 0.0003363890282344073 old loss 0.0003402859147172421 BETTER
I0312 01:46:02.876928 1758690 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0003611119755078107 old loss 0.0003704972332343459 BETTER
I0312 01:46:22.597538 1758458 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0003962461487390101 old loss 0.00040040211752057076 BETTER
I0312 01:46:26.931159 1758342 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.00040022513712756336 old loss 0.00040339890983887017 BETTER
I0312 01:46:28.157574 1758574 finetune.py:68] layer 18_up @ epoch 3 new loss 0.000333203817717731 old loss 0.0003363890282344073 BETTER
I0312 01:46:32.481504 1758690 finetune.py:68] layer 19_up @ epoch 1 new loss 0.00035592366475611925 old loss 0.0003611119755078107 BETTER
I0312 01:46:50.903362 1758458 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0003933805273845792 old loss 0.0003962461487390101 BETTER
I0312 01:46:56.883538 1758342 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.0003976555890403688 old loss 0.00040022513712756336 BETTER
I0312 01:46:58.268696 1758574 finetune.py:68] layer 18_up @ epoch 4 new loss 0.000330598239088431 old loss 0.000333203817717731 BETTER
I0312 01:47:01.941892 1758690 finetune.py:68] layer 19_up @ epoch 2 new loss 0.000352012604707852 old loss 0.00035592366475611925 BETTER
I0312 01:47:13.560952 1758574 finetune.py:45] layer 18_gate initial loss 0.0004629182512871921
I0312 01:47:19.207715 1758458 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.00039114063838496804 old loss 0.0003933805273845792 BETTER
I0312 01:47:27.003073 1758342 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0003955320571549237 old loss 0.0003976555890403688 BETTER
I0312 01:47:31.432948 1758690 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0003489632217679173 old loss 0.000352012604707852 BETTER
I0312 01:47:40.993811 1758574 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.00045855354983359575 old loss 0.0004629182512871921 BETTER
I0312 01:47:47.520567 1758458 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.00038925334229134023 old loss 0.00039114063838496804 BETTER
I0312 01:47:57.136711 1758342 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.00039368291618302464 old loss 0.0003955320571549237 BETTER
I0312 01:48:01.236837 1758690 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0003464886685833335 old loss 0.0003489632217679173 BETTER
I0312 01:48:09.166672 1758574 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.00045552814844995737 old loss 0.00045855354983359575 BETTER
I0312 01:48:13.428057 1758342 finetune.py:45] layer 16_down initial loss 0.0006306415307335556
I0312 01:48:15.632776 1758458 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.00038764666533097625 old loss 0.00038925334229134023 BETTER
I0312 01:48:16.557964 1758690 finetune.py:45] layer 19_gate initial loss 0.0005025275750085711
I0312 01:48:31.123392 1758458 finetune.py:45] layer 17_down initial loss 0.000649015128146857
I0312 01:48:37.457354 1758574 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0004531376762315631 old loss 0.00045552814844995737 BETTER
I0312 01:48:40.841395 1758342 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0006305769784376025 old loss 0.0006306415307335556 BETTER
I0312 01:48:43.529259 1758690 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.000497783999890089 old loss 0.0005025275750085711 BETTER
I0312 01:48:57.197596 1758458 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0006489501101896167 old loss 0.000649015128146857 BETTER
I0312 01:49:05.769346 1758574 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0004511693841777742 old loss 0.0004531376762315631 BETTER
I0312 01:49:09.346418 1758342 finetune.py:68] layer 16_down @ epoch 1 new loss 0.00063054368365556 old loss 0.0006305769784376025 BETTER
I0312 01:49:11.477999 1758690 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0004946101107634604 old loss 0.000497783999890089 BETTER
I0312 01:49:24.201635 1758458 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0006489189108833671 old loss 0.0006489501101896167 BETTER
I0312 01:49:34.117991 1758574 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.00044950522715225816 old loss 0.0004511693841777742 BETTER
I0312 01:49:37.830409 1758342 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0006305210408754647 old loss 0.00063054368365556 BETTER
I0312 01:49:39.651443 1758690 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0004921623039990664 old loss 0.0004946101107634604 BETTER
I0312 01:49:50.171127 1758574 finetune.py:45] layer 18_down initial loss 0.0007582684047520161
I0312 01:49:51.162359 1758458 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0006488985964097083 old loss 0.0006489189108833671 BETTER
I0312 01:50:06.409686 1758342 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0006305074202828109 old loss 0.0006305210408754647 BETTER
I0312 01:50:07.864306 1758690 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.000490134465508163 old loss 0.0004921623039990664 BETTER
I0312 01:50:16.377965 1758574 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0007581770769320428 old loss 0.0007582684047520161 BETTER
I0312 01:50:18.223157 1758458 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0006488841027021408 old loss 0.0006488985964097083 BETTER
I0312 01:50:34.916928 1758342 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0006304930429905653 old loss 0.0006305074202828109 BETTER
I0312 01:50:35.936074 1758690 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.000488476303871721 old loss 0.000490134465508163 BETTER
16_v proxy err 0.007361093070358038 tr(WHW.T) 780.7407836914062
16_q proxy err 0.0008922195411287248 tr(WHW.T) 7194.41357421875
16_k proxy err 0.0005931815248914063 tr(WHW.T) 11641.75390625
16_o proxy err 0.007875450886785984 tr(WHW.T) 88.48251342773438
16_up proxy err 0.004426144994795322 tr(WHW.T) 1890.81591796875
16_gate proxy err 0.002515359316021204 tr(WHW.T) 3371.869873046875
16_down proxy err 0.006461497861891985 tr(WHW.T) 152.50811767578125
I0312 01:50:44.095949 1758574 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0007581285899505019 old loss 0.0007581770769320428 BETTER
I0312 01:50:45.864591 1758458 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0006488724029622972 old loss 0.0006488841027021408 BETTER
17_v proxy err 0.007281939964741468 tr(WHW.T) 845.7654418945312
17_q proxy err 0.0009471628582105041 tr(WHW.T) 7166.22314453125
17_k proxy err 0.0006665933760814369 tr(WHW.T) 10708.3408203125
17_o proxy err 0.008511574007570744 tr(WHW.T) 58.38168716430664
17_up proxy err 0.004914411809295416 tr(WHW.T) 1921.2313232421875
17_gate proxy err 0.002672491827979684 tr(WHW.T) 3573.928466796875
17_down proxy err 0.00656451191753149 tr(WHW.T) 165.91204833984375
I0312 01:50:52.492759 1758690 finetune.py:45] layer 19_down initial loss 0.0008254709537141025
I0312 01:51:11.069219 1758574 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0007580992532894015 old loss 0.0007581285899505019 BETTER
I0312 01:51:17.936755 1758690 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0008253922569565475 old loss 0.0008254709537141025 BETTER
I0312 01:51:37.893582 1758574 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0007580792298540473 old loss 0.0007580992532894015 BETTER
I0312 01:51:44.305528 1758690 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0008253490086644888 old loss 0.0008253922569565475 BETTER
I0312 01:52:03.638737 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 20 in 72.91693472862244s
I0312 01:52:04.953117 1758574 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0007580626988783479 old loss 0.0007580792298540473 BETTER
18_v proxy err 0.006877053063362837 tr(WHW.T) 1003.7705078125
18_q proxy err 0.0009988718666136265 tr(WHW.T) 7512.1611328125
18_k proxy err 0.0007451533456332982 tr(WHW.T) 10471.822265625
18_o proxy err 0.007323000114411116 tr(WHW.T) 70.20858764648438
18_up proxy err 0.0052555026486516 tr(WHW.T) 2023.194091796875
18_gate proxy err 0.0028467813972383738 tr(WHW.T) 3782.12158203125
18_down proxy err 0.006438018754124641 tr(WHW.T) 199.1576690673828
I0312 01:52:06.779601 1758806 config.py:54] PyTorch version 2.1.1 available.
I0312 01:52:07.843204 1756366 quantize_finetune_llama.py:183] layer 21 gpu 1
I0312 01:52:07.917967 1758806 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 01:52:10.886468 1758690 finetune.py:68] layer 19_down @ epoch 2 new loss 0.000825326016638428 old loss 0.0008253490086644888 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:52:16.514118 1758806 finetune.py:45] layer 20_v initial loss 0.00010040303459390998
I0312 01:52:37.297945 1758690 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0008253070991486311 old loss 0.000825326016638428 BETTER
I0312 01:52:49.439277 1758806 finetune.py:68] layer 20_v @ epoch 0 new loss 7.960469520185143e-05 old loss 0.00010040303459390998 BETTER
I0312 01:53:03.809510 1758690 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0008252935949712992 old loss 0.0008253070991486311 BETTER
19_v proxy err 0.006767058279365301 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.0010709847556427121 tr(WHW.T) 6947.0595703125
19_k proxy err 0.0007354429108090699 tr(WHW.T) 10560.619140625
19_o proxy err 0.007336319889873266 tr(WHW.T) 62.51328659057617
19_up proxy err 0.005299847573041916 tr(WHW.T) 2149.5078125
19_gate proxy err 0.0031224950216710567 tr(WHW.T) 3688.74169921875
19_down proxy err 0.006273739505559206 tr(WHW.T) 223.6105194091797
I0312 01:53:19.046243 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 21 in 70.02265954017639s
I0312 01:53:22.277142 1758922 config.py:54] PyTorch version 2.1.1 available.
I0312 01:53:23.282945 1756366 quantize_finetune_llama.py:183] layer 22 gpu 2
I0312 01:53:23.349330 1758922 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 01:53:23.775295 1758806 finetune.py:68] layer 20_v @ epoch 1 new loss 7.554543117294088e-05 old loss 7.960469520185143e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:53:31.238090 1758922 finetune.py:45] layer 21_v initial loss 8.844846888678148e-05
I0312 01:53:58.539433 1758806 finetune.py:68] layer 20_v @ epoch 2 new loss 7.317958807107061e-05 old loss 7.554543117294088e-05 BETTER
I0312 01:54:02.550812 1758922 finetune.py:68] layer 21_v @ epoch 0 new loss 7.227725291159004e-05 old loss 8.844846888678148e-05 BETTER
I0312 01:54:33.308609 1758806 finetune.py:68] layer 20_v @ epoch 3 new loss 7.161228859331459e-05 old loss 7.317958807107061e-05 BETTER
I0312 01:54:33.583657 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 22 in 69.87219858169556s
I0312 01:54:34.911982 1758922 finetune.py:68] layer 21_v @ epoch 1 new loss 6.929491792107001e-05 old loss 7.227725291159004e-05 BETTER
I0312 01:54:36.802292 1759038 config.py:54] PyTorch version 2.1.1 available.
I0312 01:54:37.808813 1756366 quantize_finetune_llama.py:183] layer 23 gpu 3
I0312 01:54:37.885001 1759038 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:54:46.191442 1759038 finetune.py:45] layer 22_v initial loss 0.00010855759319383651
I0312 01:55:07.383224 1758922 finetune.py:68] layer 21_v @ epoch 2 new loss 6.750778265995905e-05 old loss 6.929491792107001e-05 BETTER
I0312 01:55:08.517162 1758806 finetune.py:68] layer 20_v @ epoch 4 new loss 7.030554115772247e-05 old loss 7.161228859331459e-05 BETTER
I0312 01:55:17.594462 1759038 finetune.py:68] layer 22_v @ epoch 0 new loss 8.969594637164846e-05 old loss 0.00010855759319383651 BETTER
I0312 01:55:18.095273 1758806 finetune.py:45] layer 20_q initial loss 9.044938633451238e-05
I0312 01:55:40.294247 1758922 finetune.py:68] layer 21_v @ epoch 3 new loss 6.634285091422498e-05 old loss 6.750778265995905e-05 BETTER
I0312 01:55:49.412349 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 23 in 71.19070100784302s
I0312 01:55:50.160093 1759038 finetune.py:68] layer 22_v @ epoch 1 new loss 8.584489114582539e-05 old loss 8.969594637164846e-05 BETTER
I0312 01:55:51.764395 1758806 finetune.py:68] layer 20_q @ epoch 0 new loss 8.576493564760312e-05 old loss 9.044938633451238e-05 BETTER
I0312 01:55:52.817034 1759154 config.py:54] PyTorch version 2.1.1 available.
I0312 01:55:54.112840 1756366 quantize_finetune_llama.py:183] layer 24 gpu 0
I0312 01:55:54.181062 1759154 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 01:56:02.309718 1759154 finetune.py:45] layer 23_v initial loss 0.00010983955871779472
I0312 01:56:13.030966 1758922 finetune.py:68] layer 21_v @ epoch 4 new loss 6.539362948387861e-05 old loss 6.634285091422498e-05 BETTER
I0312 01:56:22.079116 1758922 finetune.py:45] layer 21_q initial loss 8.277378947241232e-05
I0312 01:56:22.892782 1759038 finetune.py:68] layer 22_v @ epoch 2 new loss 8.367263217223808e-05 old loss 8.584489114582539e-05 BETTER
I0312 01:56:26.103351 1758806 finetune.py:68] layer 20_q @ epoch 1 new loss 8.371831063413993e-05 old loss 8.576493564760312e-05 BETTER
I0312 01:56:33.476872 1759154 finetune.py:68] layer 23_v @ epoch 0 new loss 9.118392335949466e-05 old loss 0.00010983955871779472 BETTER
I0312 01:56:53.831580 1758922 finetune.py:68] layer 21_q @ epoch 0 new loss 7.885081868153065e-05 old loss 8.277378947241232e-05 BETTER
I0312 01:56:56.182684 1759038 finetune.py:68] layer 22_v @ epoch 3 new loss 8.213393448386341e-05 old loss 8.367263217223808e-05 BETTER
I0312 01:57:00.692908 1758806 finetune.py:68] layer 20_q @ epoch 2 new loss 8.228769729612395e-05 old loss 8.371831063413993e-05 BETTER
I0312 01:57:05.685003 1759154 finetune.py:68] layer 23_v @ epoch 1 new loss 8.752082794671878e-05 old loss 9.118392335949466e-05 BETTER
I0312 01:57:26.219872 1758922 finetune.py:68] layer 21_q @ epoch 1 new loss 7.736411498626694e-05 old loss 7.885081868153065e-05 BETTER
I0312 01:57:29.314314 1759038 finetune.py:68] layer 22_v @ epoch 4 new loss 8.08531476650387e-05 old loss 8.213393448386341e-05 BETTER
I0312 01:57:35.369312 1758806 finetune.py:68] layer 20_q @ epoch 3 new loss 8.114362572086975e-05 old loss 8.228769729612395e-05 BETTER
I0312 01:57:38.238396 1759154 finetune.py:68] layer 23_v @ epoch 2 new loss 8.535906817996874e-05 old loss 8.752082794671878e-05 BETTER
I0312 01:57:39.014072 1759038 finetune.py:45] layer 22_q initial loss 0.00010978830687236041
I0312 01:57:58.849278 1758922 finetune.py:68] layer 21_q @ epoch 2 new loss 7.628243474755436e-05 old loss 7.736411498626694e-05 BETTER
I0312 01:58:10.324269 1758806 finetune.py:68] layer 20_q @ epoch 4 new loss 8.025605347938836e-05 old loss 8.114362572086975e-05 BETTER
I0312 01:58:10.940650 1759038 finetune.py:68] layer 22_q @ epoch 0 new loss 0.00010304283205186948 old loss 0.00010978830687236041 BETTER
I0312 01:58:11.075368 1759154 finetune.py:68] layer 23_v @ epoch 3 new loss 8.384015382034704e-05 old loss 8.535906817996874e-05 BETTER
I0312 01:58:19.739187 1758806 finetune.py:45] layer 20_k initial loss 9.467033669352531e-05
I0312 01:58:31.021368 1758922 finetune.py:68] layer 21_q @ epoch 3 new loss 7.542872481280938e-05 old loss 7.628243474755436e-05 BETTER
I0312 01:58:43.404139 1759154 finetune.py:68] layer 23_v @ epoch 4 new loss 8.255252032540739e-05 old loss 8.384015382034704e-05 BETTER
I0312 01:58:43.789602 1759038 finetune.py:68] layer 22_q @ epoch 1 new loss 0.00010049065895145759 old loss 0.00010304283205186948 BETTER
I0312 01:58:52.405711 1759154 finetune.py:45] layer 23_q initial loss 0.00010659710824256763
I0312 01:58:52.855625 1758806 finetune.py:68] layer 20_k @ epoch 0 new loss 9.293569746660069e-05 old loss 9.467033669352531e-05 BETTER
I0312 01:59:03.712592 1758922 finetune.py:68] layer 21_q @ epoch 4 new loss 7.472798461094499e-05 old loss 7.542872481280938e-05 BETTER
I0312 01:59:12.987503 1758922 finetune.py:45] layer 21_k initial loss 8.956968667916954e-05
I0312 01:59:16.326663 1759038 finetune.py:68] layer 22_q @ epoch 2 new loss 9.86203522188589e-05 old loss 0.00010049065895145759 BETTER
I0312 01:59:23.792396 1759154 finetune.py:68] layer 23_q @ epoch 0 new loss 0.00010114551696460694 old loss 0.00010659710824256763 BETTER
I0312 01:59:26.499240 1758806 finetune.py:68] layer 20_k @ epoch 1 new loss 9.213525481754914e-05 old loss 9.293569746660069e-05 BETTER
I0312 01:59:44.498963 1758922 finetune.py:68] layer 21_k @ epoch 0 new loss 8.772196451900527e-05 old loss 8.956968667916954e-05 BETTER
I0312 01:59:48.670703 1759038 finetune.py:68] layer 22_q @ epoch 3 new loss 9.718237561173737e-05 old loss 9.86203522188589e-05 BETTER
I0312 01:59:55.863302 1759154 finetune.py:68] layer 23_q @ epoch 1 new loss 9.914999100146815e-05 old loss 0.00010114551696460694 BETTER
I0312 02:00:00.290564 1758806 finetune.py:68] layer 20_k @ epoch 2 new loss 9.155880252365023e-05 old loss 9.213525481754914e-05 BETTER
I0312 02:00:16.628920 1758922 finetune.py:68] layer 21_k @ epoch 1 new loss 8.712957060197368e-05 old loss 8.772196451900527e-05 BETTER
I0312 02:00:21.100511 1759038 finetune.py:68] layer 22_q @ epoch 4 new loss 9.599062468623742e-05 old loss 9.718237561173737e-05 BETTER
I0312 02:00:27.893916 1759154 finetune.py:68] layer 23_q @ epoch 2 new loss 9.776608203537762e-05 old loss 9.914999100146815e-05 BETTER
I0312 02:00:30.276176 1759038 finetune.py:45] layer 22_k initial loss 0.00011783061927417293
I0312 02:00:34.299231 1758806 finetune.py:68] layer 20_k @ epoch 3 new loss 9.110268001677468e-05 old loss 9.155880252365023e-05 BETTER
I0312 02:00:48.738072 1758922 finetune.py:68] layer 21_k @ epoch 2 new loss 8.674741548020393e-05 old loss 8.712957060197368e-05 BETTER
I0312 02:00:59.944137 1759154 finetune.py:68] layer 23_q @ epoch 3 new loss 9.666220284998417e-05 old loss 9.776608203537762e-05 BETTER
I0312 02:01:01.784554 1759038 finetune.py:68] layer 22_k @ epoch 0 new loss 0.00011575112876016647 old loss 0.00011783061927417293 BETTER
I0312 02:01:08.215018 1758806 finetune.py:68] layer 20_k @ epoch 4 new loss 9.077019058167934e-05 old loss 9.110268001677468e-05 BETTER
I0312 02:01:17.748073 1758806 finetune.py:45] layer 20_o initial loss 0.00022399198496714234
I0312 02:01:20.871649 1758922 finetune.py:68] layer 21_k @ epoch 3 new loss 8.640122541692108e-05 old loss 8.674741548020393e-05 BETTER
I0312 02:01:32.205390 1759154 finetune.py:68] layer 23_q @ epoch 4 new loss 9.5783980214037e-05 old loss 9.666220284998417e-05 BETTER
I0312 02:01:33.898586 1759038 finetune.py:68] layer 22_k @ epoch 1 new loss 0.00011478966189315543 old loss 0.00011575112876016647 BETTER
I0312 02:01:41.429321 1759154 finetune.py:45] layer 23_k initial loss 0.00011589159112190828
I0312 02:01:50.681684 1758806 finetune.py:68] layer 20_o @ epoch 0 new loss 0.00021297212515491992 old loss 0.00022399198496714234 BETTER
I0312 02:01:53.138610 1758922 finetune.py:68] layer 21_k @ epoch 4 new loss 8.612269448349252e-05 old loss 8.640122541692108e-05 BETTER
I0312 02:02:02.292423 1758922 finetune.py:45] layer 21_o initial loss 0.00019909694674424827
I0312 02:02:06.126303 1759038 finetune.py:68] layer 22_k @ epoch 2 new loss 0.00011407095735194162 old loss 0.00011478966189315543 BETTER
I0312 02:02:12.382754 1759154 finetune.py:68] layer 23_k @ epoch 0 new loss 0.00011445078416727483 old loss 0.00011589159112190828 BETTER
I0312 02:02:24.087994 1758806 finetune.py:68] layer 20_o @ epoch 1 new loss 0.000207867196877487 old loss 0.00021297212515491992 BETTER
I0312 02:02:33.205654 1758922 finetune.py:68] layer 21_o @ epoch 0 new loss 0.00019213212362956256 old loss 0.00019909694674424827 BETTER
I0312 02:02:38.258464 1759038 finetune.py:68] layer 22_k @ epoch 3 new loss 0.00011350510612828657 old loss 0.00011407095735194162 BETTER
I0312 02:02:44.111295 1759154 finetune.py:68] layer 23_k @ epoch 1 new loss 0.00011376261682016775 old loss 0.00011445078416727483 BETTER
I0312 02:02:57.577927 1758806 finetune.py:68] layer 20_o @ epoch 2 new loss 0.00020432425662875175 old loss 0.000207867196877487 BETTER
I0312 02:03:04.776604 1758922 finetune.py:68] layer 21_o @ epoch 1 new loss 0.00018934946274384856 old loss 0.00019213212362956256 BETTER
I0312 02:03:10.465991 1759038 finetune.py:68] layer 22_k @ epoch 4 new loss 0.00011307344539090991 old loss 0.00011350510612828657 BETTER
I0312 02:03:16.242007 1759154 finetune.py:68] layer 23_k @ epoch 2 new loss 0.00011324165825499222 old loss 0.00011376261682016775 BETTER
I0312 02:03:20.183230 1759038 finetune.py:45] layer 22_o initial loss 0.00025245407596230507
I0312 02:03:31.464613 1758806 finetune.py:68] layer 20_o @ epoch 3 new loss 0.00020177883561700583 old loss 0.00020432425662875175 BETTER
I0312 02:03:36.586485 1758922 finetune.py:68] layer 21_o @ epoch 2 new loss 0.00018749649461824447 old loss 0.00018934946274384856 BETTER
I0312 02:03:48.011662 1759154 finetune.py:68] layer 23_k @ epoch 3 new loss 0.00011286555672995746 old loss 0.00011324165825499222 BETTER
I0312 02:03:51.102128 1759038 finetune.py:68] layer 22_o @ epoch 0 new loss 0.00024244619999080896 old loss 0.00025245407596230507 BETTER
I0312 02:04:05.061343 1758806 finetune.py:68] layer 20_o @ epoch 4 new loss 0.00019971212896052748 old loss 0.00020177883561700583 BETTER
I0312 02:04:08.096740 1758922 finetune.py:68] layer 21_o @ epoch 3 new loss 0.0001861618657130748 old loss 0.00018749649461824447 BETTER
I0312 02:04:20.160091 1759154 finetune.py:68] layer 23_k @ epoch 4 new loss 0.00011251780233578756 old loss 0.00011286555672995746 BETTER
I0312 02:04:20.860276 1758806 finetune.py:45] layer 20_up initial loss 0.00043753813952207565
I0312 02:04:23.084971 1759038 finetune.py:68] layer 22_o @ epoch 1 new loss 0.00023819191846996546 old loss 0.00024244619999080896 BETTER
I0312 02:04:29.705759 1759154 finetune.py:45] layer 23_o initial loss 0.0002368007117183879
I0312 02:04:39.867858 1758922 finetune.py:68] layer 21_o @ epoch 4 new loss 0.00018517038552090526 old loss 0.0001861618657130748 BETTER
I0312 02:04:51.694448 1758806 finetune.py:68] layer 20_up @ epoch 0 new loss 0.0004271274956408888 old loss 0.00043753813952207565 BETTER
I0312 02:04:55.140842 1759038 finetune.py:68] layer 22_o @ epoch 2 new loss 0.00023527268785983324 old loss 0.00023819191846996546 BETTER
I0312 02:04:55.657079 1758922 finetune.py:45] layer 21_up initial loss 0.0004456333408597857
I0312 02:04:59.918891 1759154 finetune.py:68] layer 23_o @ epoch 0 new loss 0.00022976635955274105 old loss 0.0002368007117183879 BETTER
I0312 02:05:23.794455 1758806 finetune.py:68] layer 20_up @ epoch 1 new loss 0.0004212359490338713 old loss 0.0004271274956408888 BETTER
I0312 02:05:24.792043 1758922 finetune.py:68] layer 21_up @ epoch 0 new loss 0.00043640585499815643 old loss 0.0004456333408597857 BETTER
I0312 02:05:27.133987 1759038 finetune.py:68] layer 22_o @ epoch 3 new loss 0.00023314615827985108 old loss 0.00023527268785983324 BETTER
I0312 02:05:30.915158 1759154 finetune.py:68] layer 23_o @ epoch 1 new loss 0.0002266705851070583 old loss 0.00022976635955274105 BETTER
I0312 02:05:54.540790 1758922 finetune.py:68] layer 21_up @ epoch 1 new loss 0.00043127202661708 old loss 0.00043640585499815643 BETTER
I0312 02:05:55.735875 1758806 finetune.py:68] layer 20_up @ epoch 2 new loss 0.0004168371087871492 old loss 0.0004212359490338713 BETTER
I0312 02:05:59.033225 1759038 finetune.py:68] layer 22_o @ epoch 4 new loss 0.00023156356473919004 old loss 0.00023314615827985108 BETTER
I0312 02:06:01.951089 1759154 finetune.py:68] layer 23_o @ epoch 2 new loss 0.00022469113173428923 old loss 0.0002266705851070583 BETTER
I0312 02:06:14.085965 1759038 finetune.py:45] layer 22_up initial loss 0.0005288042593747377
I0312 02:06:24.412469 1758922 finetune.py:68] layer 21_up @ epoch 2 new loss 0.0004274480161257088 old loss 0.00043127202661708 BETTER
I0312 02:06:27.569617 1758806 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0004132782050874084 old loss 0.0004168371087871492 BETTER
I0312 02:06:32.981509 1759154 finetune.py:68] layer 23_o @ epoch 3 new loss 0.0002231929829576984 old loss 0.00022469113173428923 BETTER
I0312 02:06:43.200791 1759038 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0005190125084482133 old loss 0.0005288042593747377 BETTER
I0312 02:06:54.203191 1758922 finetune.py:68] layer 21_up @ epoch 3 new loss 0.00042444898281246424 old loss 0.0004274480161257088 BETTER
I0312 02:06:59.412758 1758806 finetune.py:68] layer 20_up @ epoch 4 new loss 0.000410407898016274 old loss 0.0004132782050874084 BETTER
I0312 02:07:04.049337 1759154 finetune.py:68] layer 23_o @ epoch 4 new loss 0.00022216542856767774 old loss 0.0002231929829576984 BETTER
I0312 02:07:13.162962 1759038 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0005133908707648516 old loss 0.0005190125084482133 BETTER
I0312 02:07:15.032545 1758806 finetune.py:45] layer 20_gate initial loss 0.0005936622619628906
I0312 02:07:19.600137 1759154 finetune.py:45] layer 23_up initial loss 0.0005504727596417069
I0312 02:07:23.993049 1758922 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0004219614202156663 old loss 0.00042444898281246424 BETTER
I0312 02:07:39.107458 1758922 finetune.py:45] layer 21_gate initial loss 0.0006216553738340735
I0312 02:07:43.096819 1759038 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0005092010833323002 old loss 0.0005133908707648516 BETTER
I0312 02:07:44.195441 1758806 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.000588370836339891 old loss 0.0005936622619628906 BETTER
I0312 02:07:48.313483 1759154 finetune.py:68] layer 23_up @ epoch 0 new loss 0.0005407719290815294 old loss 0.0005504727596417069 BETTER
I0312 02:08:06.721122 1758922 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0006171682616695762 old loss 0.0006216553738340735 BETTER
I0312 02:08:13.172129 1759038 finetune.py:68] layer 22_up @ epoch 3 new loss 0.0005058894748799503 old loss 0.0005092010833323002 BETTER
I0312 02:08:14.122566 1758806 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0005847346037626266 old loss 0.000588370836339891 BETTER
I0312 02:08:17.805419 1759154 finetune.py:68] layer 23_up @ epoch 1 new loss 0.000535016821231693 old loss 0.0005407719290815294 BETTER
I0312 02:08:34.933414 1758922 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0006141134654171765 old loss 0.0006171682616695762 BETTER
I0312 02:08:43.462903 1759038 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0005032523768022656 old loss 0.0005058894748799503 BETTER
I0312 02:08:44.207552 1758806 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.0005818240460939705 old loss 0.0005847346037626266 BETTER
I0312 02:08:47.529309 1759154 finetune.py:68] layer 23_up @ epoch 2 new loss 0.0005309522384777665 old loss 0.000535016821231693 BETTER
I0312 02:08:58.835091 1759038 finetune.py:45] layer 22_gate initial loss 0.0007313170935958624
I0312 02:09:03.404942 1758922 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0006116604781709611 old loss 0.0006141134654171765 BETTER
I0312 02:09:14.344462 1758806 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0005795378820039332 old loss 0.0005818240460939705 BETTER
I0312 02:09:17.050717 1759154 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0005276474985294044 old loss 0.0005309522384777665 BETTER
I0312 02:09:26.449456 1759038 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0007264231098815799 old loss 0.0007313170935958624 BETTER
I0312 02:09:31.808938 1758922 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.0006097509758546948 old loss 0.0006116604781709611 BETTER
I0312 02:09:44.309825 1758806 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.0005775835597887635 old loss 0.0005795378820039332 BETTER
I0312 02:09:46.700809 1759154 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0005250665708445013 old loss 0.0005276474985294044 BETTER
I0312 02:09:54.932346 1759038 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.0007230921182781458 old loss 0.0007264231098815799 BETTER
I0312 02:10:01.143027 1758922 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.0006081538158468902 old loss 0.0006097509758546948 BETTER
I0312 02:10:01.915775 1758806 finetune.py:45] layer 20_down initial loss 0.0009906655177474022
I0312 02:10:03.752126 1759154 finetune.py:45] layer 23_gate initial loss 0.0007830907125025988
I0312 02:10:18.699813 1758922 finetune.py:45] layer 21_down initial loss 0.0010350922821089625
I0312 02:10:23.168225 1759038 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0007205085712485015 old loss 0.0007230921182781458 BETTER
I0312 02:10:29.394364 1758806 finetune.py:68] layer 20_down @ epoch 0 new loss 0.000990516971796751 old loss 0.0009906655177474022 BETTER
I0312 02:10:31.085017 1759154 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0007785474881529808 old loss 0.0007830907125025988 BETTER
I0312 02:10:44.330833 1758922 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0010349915828555822 old loss 0.0010350922821089625 BETTER
I0312 02:10:51.416078 1759038 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.0007183791021816432 old loss 0.0007205085712485015 BETTER
I0312 02:10:58.180625 1758806 finetune.py:68] layer 20_down @ epoch 1 new loss 0.0009904296603053808 old loss 0.000990516971796751 BETTER
I0312 02:10:59.459670 1759154 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.0007752547389827669 old loss 0.0007785474881529808 BETTER
I0312 02:11:11.121384 1758922 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0010349301155656576 old loss 0.0010349915828555822 BETTER
I0312 02:11:19.677177 1759038 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.000716765527613461 old loss 0.0007183791021816432 BETTER
I0312 02:11:26.896599 1758806 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0009903787868097425 old loss 0.0009904296603053808 BETTER
I0312 02:11:27.818995 1759154 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0007727292249910533 old loss 0.0007752547389827669 BETTER
I0312 02:11:35.768289 1759038 finetune.py:45] layer 22_down initial loss 0.0012009001802653074
I0312 02:11:37.977905 1758922 finetune.py:68] layer 21_down @ epoch 2 new loss 0.0010348896030336618 old loss 0.0010349301155656576 BETTER
I0312 02:11:55.449096 1758806 finetune.py:68] layer 20_down @ epoch 3 new loss 0.0009903445607051253 old loss 0.0009903787868097425 BETTER
I0312 02:11:56.215010 1759154 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.0007706430042162538 old loss 0.0007727292249910533 BETTER
I0312 02:12:01.870777 1759038 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0012007775949314237 old loss 0.0012009001802653074 BETTER
I0312 02:12:04.868730 1758922 finetune.py:68] layer 21_down @ epoch 3 new loss 0.0010348617797717452 old loss 0.0010348896030336618 BETTER
I0312 02:12:24.060144 1758806 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0009903201134875417 old loss 0.0009903445607051253 BETTER
I0312 02:12:24.572144 1759154 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0007690295460633934 old loss 0.0007706430042162538 BETTER
20_v proxy err 0.007082038559019566 tr(WHW.T) 990.5983276367188
20_q proxy err 0.0010648652678355575 tr(WHW.T) 7152.29150390625
20_k proxy err 0.0007632228662259877 tr(WHW.T) 10398.732421875
20_o proxy err 0.0052785724401474 tr(WHW.T) 100.48408508300781
20_up proxy err 0.005221380386501551 tr(WHW.T) 2340.9853515625
20_gate proxy err 0.0030698745977133512 tr(WHW.T) 4026.307861328125
20_down proxy err 0.006188323721289635 tr(WHW.T) 275.766845703125
I0312 02:12:28.655553 1759038 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0012006994802504778 old loss 0.0012007775949314237 BETTER
I0312 02:12:32.439420 1758922 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0010348405921831727 old loss 0.0010348617797717452 BETTER
21_v proxy err 0.006908171344548464 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.001198877114802599 tr(WHW.T) 7067.49951171875
21_k proxy err 0.0008783741504885256 tr(WHW.T) 9988.7412109375
21_o proxy err 0.005991117563098669 tr(WHW.T) 75.64232635498047
21_up proxy err 0.005514419637620449 tr(WHW.T) 2361.5546875
21_gate proxy err 0.003287335392087698 tr(WHW.T) 4003.6318359375
21_down proxy err 0.006335053127259016 tr(WHW.T) 277.4043273925781
I0312 02:12:41.065311 1759154 finetune.py:45] layer 23_down initial loss 0.0012670076685026288
I0312 02:12:56.266927 1759038 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0012006523320451379 old loss 0.0012006994802504778 BETTER
I0312 02:13:06.533461 1759154 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0012669020798057318 old loss 0.0012670076685026288 BETTER
I0312 02:13:23.118639 1759038 finetune.py:68] layer 22_down @ epoch 3 new loss 0.00120061868801713 old loss 0.0012006523320451379 BETTER
I0312 02:13:32.949068 1759154 finetune.py:68] layer 23_down @ epoch 1 new loss 0.001266838051378727 old loss 0.0012669020798057318 BETTER
I0312 02:13:49.959716 1759038 finetune.py:68] layer 22_down @ epoch 4 new loss 0.001200598431751132 old loss 0.00120061868801713 BETTER
I0312 02:13:51.467736 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 24 in 74.26895785331726s
22_v proxy err 0.006535535212606192 tr(WHW.T) 1243.2529296875
22_q proxy err 0.0011302108177915215 tr(WHW.T) 7749.41162109375
22_k proxy err 0.0008530050981789827 tr(WHW.T) 10615.54296875
22_o proxy err 0.004794903099536896 tr(WHW.T) 114.59138488769531
22_up proxy err 0.005564041901379824 tr(WHW.T) 2474.980712890625
22_gate proxy err 0.0033532418310642242 tr(WHW.T) 4156.49169921875
22_down proxy err 0.00632837787270546 tr(WHW.T) 312.798828125
I0312 02:13:54.767694 1759270 config.py:54] PyTorch version 2.1.1 available.
I0312 02:13:55.813442 1756366 quantize_finetune_llama.py:183] layer 25 gpu 1
I0312 02:13:55.881052 1759270 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 02:13:59.343975 1759154 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0012667961418628693 old loss 0.001266838051378727 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 02:14:04.630692 1759270 finetune.py:45] layer 24_v initial loss 0.00012309853627812117
I0312 02:14:25.951210 1759154 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0012667697155848145 old loss 0.0012667961418628693 BETTER
I0312 02:14:37.594656 1759270 finetune.py:68] layer 24_v @ epoch 0 new loss 0.00010225169535260648 old loss 0.00012309853627812117 BETTER
I0312 02:14:52.287481 1759154 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0012667515547946095 old loss 0.0012667697155848145 BETTER
23_v proxy err 0.006143088918179274 tr(WHW.T) 1486.037353515625
23_q proxy err 0.0013165734708309174 tr(WHW.T) 7348.82763671875
23_k proxy err 0.000994664034806192 tr(WHW.T) 9993.3017578125
23_o proxy err 0.0057291071861982346 tr(WHW.T) 85.39492797851562
23_up proxy err 0.005759736523032188 tr(WHW.T) 2533.771240234375
23_gate proxy err 0.003601272590458393 tr(WHW.T) 4097.85693359375
23_down proxy err 0.006323087960481644 tr(WHW.T) 322.2168273925781
I0312 02:15:07.670357 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 25 in 71.37378740310669s
I0312 02:15:10.859169 1759386 config.py:54] PyTorch version 2.1.1 available.
I0312 02:15:11.904666 1759270 finetune.py:68] layer 24_v @ epoch 1 new loss 9.835704986471683e-05 old loss 0.00010225169535260648 BETTER
I0312 02:15:12.075444 1756366 quantize_finetune_llama.py:183] layer 26 gpu 2
I0312 02:15:12.139705 1759386 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 02:15:20.373118 1759386 finetune.py:45] layer 25_v initial loss 0.00011307303793728352
I0312 02:15:46.513642 1759270 finetune.py:68] layer 24_v @ epoch 2 new loss 9.603161015547812e-05 old loss 9.835704986471683e-05 BETTER
I0312 02:15:51.747873 1759386 finetune.py:68] layer 25_v @ epoch 0 new loss 9.042351302923635e-05 old loss 0.00011307303793728352 BETTER
I0312 02:16:21.198876 1759270 finetune.py:68] layer 24_v @ epoch 3 new loss 9.43235500017181e-05 old loss 9.603161015547812e-05 BETTER
I0312 02:16:22.399811 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 26 in 69.92259407043457s
I0312 02:16:23.904059 1759386 finetune.py:68] layer 25_v @ epoch 1 new loss 8.726070518605411e-05 old loss 9.042351302923635e-05 BETTER
I0312 02:16:25.585521 1759502 config.py:54] PyTorch version 2.1.1 available.
I0312 02:16:26.608467 1756366 quantize_finetune_llama.py:183] layer 27 gpu 3
I0312 02:16:26.673341 1759502 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 02:16:34.965018 1759502 finetune.py:45] layer 26_v initial loss 0.00017487506556790322
I0312 02:16:56.053699 1759270 finetune.py:68] layer 24_v @ epoch 4 new loss 9.307586879003793e-05 old loss 9.43235500017181e-05 BETTER
I0312 02:16:56.276835 1759386 finetune.py:68] layer 25_v @ epoch 2 new loss 8.555455860914662e-05 old loss 8.726070518605411e-05 BETTER
I0312 02:17:05.189937 1759270 finetune.py:45] layer 24_q initial loss 0.0001214125077240169
I0312 02:17:06.425943 1759502 finetune.py:68] layer 26_v @ epoch 0 new loss 0.00014287341036833823 old loss 0.00017487506556790322 BETTER
I0312 02:17:28.987879 1759386 finetune.py:68] layer 25_v @ epoch 3 new loss 8.423411782132462e-05 old loss 8.555455860914662e-05 BETTER
I0312 02:17:37.016367 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 27 in 69.9573347568512s
I0312 02:17:38.667427 1759270 finetune.py:68] layer 24_q @ epoch 0 new loss 0.00011605085455812514 old loss 0.0001214125077240169 BETTER
I0312 02:17:39.035859 1759502 finetune.py:68] layer 26_v @ epoch 1 new loss 0.00013743515592068434 old loss 0.00014287341036833823 BETTER
I0312 02:17:40.366742 1759618 config.py:54] PyTorch version 2.1.1 available.
I0312 02:17:41.410696 1756366 quantize_finetune_llama.py:183] layer 28 gpu 0
I0312 02:17:41.484310 1759618 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 02:17:49.550673 1759618 finetune.py:45] layer 27_v initial loss 0.00014893301704432815
I0312 02:18:01.798451 1759386 finetune.py:68] layer 25_v @ epoch 4 new loss 8.339733176399022e-05 old loss 8.423411782132462e-05 BETTER
I0312 02:18:10.795263 1759386 finetune.py:45] layer 25_q initial loss 0.00010979139915434644
I0312 02:18:11.476287 1759502 finetune.py:68] layer 26_v @ epoch 2 new loss 0.00013405297067947686 old loss 0.00013743515592068434 BETTER
I0312 02:18:12.947233 1759270 finetune.py:68] layer 24_q @ epoch 1 new loss 0.00011392270971555263 old loss 0.00011605085455812514 BETTER
I0312 02:18:20.433129 1759618 finetune.py:68] layer 27_v @ epoch 0 new loss 0.00012551831605378538 old loss 0.00014893301704432815 BETTER
I0312 02:18:42.270490 1759386 finetune.py:68] layer 25_q @ epoch 0 new loss 0.00010477076284587383 old loss 0.00010979139915434644 BETTER
I0312 02:18:44.417980 1759502 finetune.py:68] layer 26_v @ epoch 3 new loss 0.00013164945994503796 old loss 0.00013405297067947686 BETTER
I0312 02:18:47.451506 1759270 finetune.py:68] layer 24_q @ epoch 2 new loss 0.00011245072528254241 old loss 0.00011392270971555263 BETTER
I0312 02:18:52.489641 1759618 finetune.py:68] layer 27_v @ epoch 1 new loss 0.00012124778004363179 old loss 0.00012551831605378538 BETTER
I0312 02:19:14.771798 1759386 finetune.py:68] layer 25_q @ epoch 1 new loss 0.00010305634350515902 old loss 0.00010477076284587383 BETTER
I0312 02:19:17.428563 1759502 finetune.py:68] layer 26_v @ epoch 4 new loss 0.0001300578733207658 old loss 0.00013164945994503796 BETTER
I0312 02:19:22.265374 1759270 finetune.py:68] layer 24_q @ epoch 3 new loss 0.00011122212163172662 old loss 0.00011245072528254241 BETTER
I0312 02:19:24.970459 1759618 finetune.py:68] layer 27_v @ epoch 2 new loss 0.00011885280400747433 old loss 0.00012124778004363179 BETTER
I0312 02:19:27.343996 1759502 finetune.py:45] layer 26_q initial loss 0.00017016327183227986
I0312 02:19:47.365654 1759386 finetune.py:68] layer 25_q @ epoch 2 new loss 0.00010180782555835322 old loss 0.00010305634350515902 BETTER
I0312 02:19:57.092861 1759270 finetune.py:68] layer 24_q @ epoch 4 new loss 0.00011033847113139927 old loss 0.00011122212163172662 BETTER
I0312 02:19:57.653406 1759618 finetune.py:68] layer 27_v @ epoch 3 new loss 0.00011697976151481271 old loss 0.00011885280400747433 BETTER
I0312 02:19:59.054087 1759502 finetune.py:68] layer 26_q @ epoch 0 new loss 0.00016246398445218801 old loss 0.00017016327183227986 BETTER
I0312 02:20:06.757708 1759270 finetune.py:45] layer 24_k initial loss 0.00013424588541965932
I0312 02:20:19.894866 1759386 finetune.py:68] layer 25_q @ epoch 3 new loss 0.00010097611811943352 old loss 0.00010180782555835322 BETTER
I0312 02:20:30.069815 1759618 finetune.py:68] layer 27_v @ epoch 4 new loss 0.00011553129297681153 old loss 0.00011697976151481271 BETTER
I0312 02:20:31.626525 1759502 finetune.py:68] layer 26_q @ epoch 1 new loss 0.0001591776090208441 old loss 0.00016246398445218801 BETTER
I0312 02:20:39.409625 1759618 finetune.py:45] layer 27_q initial loss 0.00015537934086751193
I0312 02:20:39.725413 1759270 finetune.py:68] layer 24_k @ epoch 0 new loss 0.00013235065853223205 old loss 0.00013424588541965932 BETTER
I0312 02:20:52.565539 1759386 finetune.py:68] layer 25_q @ epoch 4 new loss 0.00010040163761004806 old loss 0.00010097611811943352 BETTER
I0312 02:21:01.832330 1759386 finetune.py:45] layer 25_k initial loss 0.0001235950767295435
I0312 02:21:04.235534 1759502 finetune.py:68] layer 26_q @ epoch 2 new loss 0.00015689461724832654 old loss 0.0001591776090208441 BETTER
I0312 02:21:10.771248 1759618 finetune.py:68] layer 27_q @ epoch 0 new loss 0.00014677300350740552 old loss 0.00015537934086751193 BETTER
I0312 02:21:13.736901 1759270 finetune.py:68] layer 24_k @ epoch 1 new loss 0.00013153327745385468 old loss 0.00013235065853223205 BETTER
I0312 02:21:33.500634 1759386 finetune.py:68] layer 25_k @ epoch 0 new loss 0.00012193033035146073 old loss 0.0001235950767295435 BETTER
I0312 02:21:37.305897 1759502 finetune.py:68] layer 26_q @ epoch 3 new loss 0.00015503840404562652 old loss 0.00015689461724832654 BETTER
I0312 02:21:42.961294 1759618 finetune.py:68] layer 27_q @ epoch 1 new loss 0.00014382469817064703 old loss 0.00014677300350740552 BETTER
I0312 02:21:47.917856 1759270 finetune.py:68] layer 24_k @ epoch 2 new loss 0.00013095175381749868 old loss 0.00013153327745385468 BETTER
I0312 02:22:05.762671 1759386 finetune.py:68] layer 25_k @ epoch 1 new loss 0.0001212119241245091 old loss 0.00012193033035146073 BETTER
I0312 02:22:09.857278 1759502 finetune.py:68] layer 26_q @ epoch 4 new loss 0.0001536203926661983 old loss 0.00015503840404562652 BETTER
I0312 02:22:15.034012 1759618 finetune.py:68] layer 27_q @ epoch 2 new loss 0.00014176929835230112 old loss 0.00014382469817064703 BETTER
I0312 02:22:19.231612 1759502 finetune.py:45] layer 26_k initial loss 0.0001854257716331631
I0312 02:22:22.116367 1759270 finetune.py:68] layer 24_k @ epoch 3 new loss 0.00013054862210992724 old loss 0.00013095175381749868 BETTER
I0312 02:22:37.876232 1759386 finetune.py:68] layer 25_k @ epoch 2 new loss 0.00012078083818778396 old loss 0.0001212119241245091 BETTER
I0312 02:22:47.143523 1759618 finetune.py:68] layer 27_q @ epoch 3 new loss 0.0001400104520143941 old loss 0.00014176929835230112 BETTER
I0312 02:22:50.723186 1759502 finetune.py:68] layer 26_k @ epoch 0 new loss 0.00018355862994212657 old loss 0.0001854257716331631 BETTER
I0312 02:22:56.367808 1759270 finetune.py:68] layer 24_k @ epoch 4 new loss 0.0001301937154494226 old loss 0.00013054862210992724 BETTER
I0312 02:23:05.713053 1759270 finetune.py:45] layer 24_o initial loss 0.00028241955442354083
I0312 02:23:09.884155 1759386 finetune.py:68] layer 25_k @ epoch 3 new loss 0.0001204393629450351 old loss 0.00012078083818778396 BETTER
I0312 02:23:19.303596 1759618 finetune.py:68] layer 27_q @ epoch 4 new loss 0.00013875112927053124 old loss 0.0001400104520143941 BETTER
I0312 02:23:22.753689 1759502 finetune.py:68] layer 26_k @ epoch 1 new loss 0.0001825259387260303 old loss 0.00018355862994212657 BETTER
I0312 02:23:28.643968 1759618 finetune.py:45] layer 27_k initial loss 0.00017186798504553735
I0312 02:23:38.376271 1759270 finetune.py:68] layer 24_o @ epoch 0 new loss 0.000274216930847615 old loss 0.00028241955442354083 BETTER
I0312 02:23:41.974383 1759386 finetune.py:68] layer 25_k @ epoch 4 new loss 0.00012023343151668087 old loss 0.0001204393629450351 BETTER
I0312 02:23:51.208142 1759386 finetune.py:45] layer 25_o initial loss 0.00024126908101607114
I0312 02:23:54.803548 1759502 finetune.py:68] layer 26_k @ epoch 2 new loss 0.00018179256585426629 old loss 0.0001825259387260303 BETTER
I0312 02:23:59.700209 1759618 finetune.py:68] layer 27_k @ epoch 0 new loss 0.00016838549345266074 old loss 0.00017186798504553735 BETTER
I0312 02:24:11.700322 1759270 finetune.py:68] layer 24_o @ epoch 1 new loss 0.0002710454282350838 old loss 0.000274216930847615 BETTER
I0312 02:24:22.100193 1759386 finetune.py:68] layer 25_o @ epoch 0 new loss 0.0002358542406000197 old loss 0.00024126908101607114 BETTER
I0312 02:24:26.875205 1759502 finetune.py:68] layer 26_k @ epoch 3 new loss 0.00018119644664693624 old loss 0.00018179256585426629 BETTER
I0312 02:24:31.391094 1759618 finetune.py:68] layer 27_k @ epoch 1 new loss 0.00016731341020204127 old loss 0.00016838549345266074 BETTER
I0312 02:24:45.475650 1759270 finetune.py:68] layer 24_o @ epoch 2 new loss 0.0002689668326638639 old loss 0.0002710454282350838 BETTER
I0312 02:24:53.769494 1759386 finetune.py:68] layer 25_o @ epoch 1 new loss 0.00023348491231445223 old loss 0.0002358542406000197 BETTER
I0312 02:24:59.325157 1759502 finetune.py:68] layer 26_k @ epoch 4 new loss 0.00018072077364195138 old loss 0.00018119644664693624 BETTER
I0312 02:25:03.161304 1759618 finetune.py:68] layer 27_k @ epoch 2 new loss 0.00016654233331792057 old loss 0.00016731341020204127 BETTER
I0312 02:25:08.900037 1759502 finetune.py:45] layer 26_o initial loss 0.00036537842242978513
I0312 02:25:18.820909 1759270 finetune.py:68] layer 24_o @ epoch 3 new loss 0.00026757296291179955 old loss 0.0002689668326638639 BETTER
I0312 02:25:25.518667 1759386 finetune.py:68] layer 25_o @ epoch 2 new loss 0.00023210991639643908 old loss 0.00023348491231445223 BETTER
I0312 02:25:35.064713 1759618 finetune.py:68] layer 27_k @ epoch 3 new loss 0.00016607223369646817 old loss 0.00016654233331792057 BETTER
I0312 02:25:39.830968 1759502 finetune.py:68] layer 26_o @ epoch 0 new loss 0.0003520722675602883 old loss 0.00036537842242978513 BETTER
I0312 02:25:52.503143 1759270 finetune.py:68] layer 24_o @ epoch 4 new loss 0.00026656078989617527 old loss 0.00026757296291179955 BETTER
I0312 02:25:57.130935 1759386 finetune.py:68] layer 25_o @ epoch 3 new loss 0.00023112972849048674 old loss 0.00023210991639643908 BETTER
I0312 02:26:06.842247 1759618 finetune.py:68] layer 27_k @ epoch 4 new loss 0.00016555139154661447 old loss 0.00016607223369646817 BETTER
I0312 02:26:08.020466 1759270 finetune.py:45] layer 24_up initial loss 0.0006215915200300515
I0312 02:26:11.559494 1759502 finetune.py:68] layer 26_o @ epoch 1 new loss 0.00034767601755447686 old loss 0.0003520722675602883 BETTER
I0312 02:26:16.177323 1759618 finetune.py:45] layer 27_o initial loss 0.0003237781347706914
I0312 02:26:28.803553 1759386 finetune.py:68] layer 25_o @ epoch 4 new loss 0.0002304234803887084 old loss 0.00023112972849048674 BETTER
I0312 02:26:38.705504 1759270 finetune.py:68] layer 24_up @ epoch 0 new loss 0.0006121848709881306 old loss 0.0006215915200300515 BETTER
I0312 02:26:43.501157 1759502 finetune.py:68] layer 26_o @ epoch 2 new loss 0.0003447250637691468 old loss 0.00034767601755447686 BETTER
I0312 02:26:44.192002 1759386 finetune.py:45] layer 25_up initial loss 0.0006276344647631049
I0312 02:26:46.545352 1759618 finetune.py:68] layer 27_o @ epoch 0 new loss 0.00031262237462215126 old loss 0.0003237781347706914 BETTER
I0312 02:27:10.484871 1759270 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0006068545044399798 old loss 0.0006121848709881306 BETTER
I0312 02:27:13.244338 1759386 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0006168257095851004 old loss 0.0006276344647631049 BETTER
I0312 02:27:15.401458 1759502 finetune.py:68] layer 26_o @ epoch 3 new loss 0.0003427002520766109 old loss 0.0003447250637691468 BETTER
I0312 02:27:17.620730 1759618 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0003079355519730598 old loss 0.00031262237462215126 BETTER
I0312 02:27:42.476561 1759270 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0006029299111105502 old loss 0.0006068545044399798 BETTER
I0312 02:27:43.155958 1759386 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0006111487746238708 old loss 0.0006168257095851004 BETTER
I0312 02:27:47.210084 1759502 finetune.py:68] layer 26_o @ epoch 4 new loss 0.00034122494980692863 old loss 0.0003427002520766109 BETTER
I0312 02:27:48.873666 1759618 finetune.py:68] layer 27_o @ epoch 2 new loss 0.00030495630926452577 old loss 0.0003079355519730598 BETTER
I0312 02:28:02.602863 1759502 finetune.py:45] layer 26_up initial loss 0.0007754735415801406
I0312 02:28:13.868679 1759386 finetune.py:68] layer 25_up @ epoch 2 new loss 0.0006068858783692122 old loss 0.0006111487746238708 BETTER
I0312 02:28:14.790030 1759270 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0005998515989631414 old loss 0.0006029299111105502 BETTER
I0312 02:28:19.881164 1759618 finetune.py:68] layer 27_o @ epoch 3 new loss 0.00030288181733340025 old loss 0.00030495630926452577 BETTER
I0312 02:28:31.651782 1759502 finetune.py:68] layer 26_up @ epoch 0 new loss 0.0007631946937181056 old loss 0.0007754735415801406 BETTER
I0312 02:28:43.983978 1759386 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0006036113481968641 old loss 0.0006068858783692122 BETTER
I0312 02:28:46.805768 1759270 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0005974094965495169 old loss 0.0005998515989631414 BETTER
I0312 02:28:51.050834 1759618 finetune.py:68] layer 27_o @ epoch 4 new loss 0.00030141585739329457 old loss 0.00030288181733340025 BETTER
I0312 02:29:01.930442 1759502 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0007567128050141037 old loss 0.0007631946937181056 BETTER
I0312 02:29:03.624200 1759270 finetune.py:45] layer 24_gate initial loss 0.0008841983508318663
I0312 02:29:07.484819 1759618 finetune.py:45] layer 27_up initial loss 0.0007949542487040162
I0312 02:29:14.190692 1759386 finetune.py:68] layer 25_up @ epoch 4 new loss 0.0006009257631376386 old loss 0.0006036113481968641 BETTER
I0312 02:29:29.722790 1759386 finetune.py:45] layer 25_gate initial loss 0.0009208205156028271
I0312 02:29:32.123773 1759502 finetune.py:68] layer 26_up @ epoch 2 new loss 0.0007518907659687102 old loss 0.0007567128050141037 BETTER
I0312 02:29:32.843324 1759270 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0008799006463959813 old loss 0.0008841983508318663 BETTER
I0312 02:29:36.317559 1759618 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0007781945168972015 old loss 0.0007949542487040162 BETTER
I0312 02:29:57.334039 1759386 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.0009162467904388905 old loss 0.0009208205156028271 BETTER
I0312 02:30:02.403584 1759502 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0007481737993657589 old loss 0.0007518907659687102 BETTER
I0312 02:30:02.991429 1759270 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0008766724495217204 old loss 0.0008799006463959813 BETTER
I0312 02:30:05.885608 1759618 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0007697620894759893 old loss 0.0007781945168972015 BETTER
I0312 02:30:25.527593 1759386 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.0009130248217843473 old loss 0.0009162467904388905 BETTER
I0312 02:30:32.670114 1759502 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0007451054407283664 old loss 0.0007481737993657589 BETTER
I0312 02:30:33.411557 1759270 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.0008742131758481264 old loss 0.0008766724495217204 BETTER
I0312 02:30:35.596883 1759618 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0007637654198333621 old loss 0.0007697620894759893 BETTER
I0312 02:30:48.372045 1759502 finetune.py:45] layer 26_gate initial loss 0.0011059108655899763
I0312 02:30:53.707946 1759386 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.0009105266071856022 old loss 0.0009130248217843473 BETTER
I0312 02:31:03.494732 1759270 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0008723218343220651 old loss 0.0008742131758481264 BETTER
I0312 02:31:05.252309 1759618 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0007592156762257218 old loss 0.0007637654198333621 BETTER
I0312 02:31:15.986802 1759502 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0011005832348018885 old loss 0.0011059108655899763 BETTER
I0312 02:31:21.907072 1759386 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.0009085350902751088 old loss 0.0009105266071856022 BETTER
I0312 02:31:33.803524 1759270 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.0008706701337359846 old loss 0.0008723218343220651 BETTER
I0312 02:31:35.037856 1759618 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0007556350319646299 old loss 0.0007592156762257218 BETTER
I0312 02:31:44.319133 1759502 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.001096828724257648 old loss 0.0011005832348018885 BETTER
I0312 02:31:50.308229 1759386 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0009068932267837226 old loss 0.0009085350902751088 BETTER
I0312 02:31:50.596594 1759270 finetune.py:45] layer 24_down initial loss 0.0013948872219771147
I0312 02:31:50.973472 1759618 finetune.py:45] layer 27_gate initial loss 0.0011704793432727456
I0312 02:32:06.391871 1759386 finetune.py:45] layer 25_down initial loss 0.0014566475292667747
I0312 02:32:12.669890 1759502 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0010939728235825896 old loss 0.001096828724257648 BETTER
I0312 02:32:18.277084 1759618 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.001163498149253428 old loss 0.0011704793432727456 BETTER
I0312 02:32:18.330482 1759270 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0013947683619335294 old loss 0.0013948872219771147 BETTER
I0312 02:32:32.408218 1759386 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0014564947923645377 old loss 0.0014566475292667747 BETTER
I0312 02:32:41.175231 1759502 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0010916241444647312 old loss 0.0010939728235825896 BETTER
I0312 02:32:46.578937 1759618 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.0011590045178309083 old loss 0.001163498149253428 BETTER
I0312 02:32:46.870132 1759270 finetune.py:68] layer 24_down @ epoch 1 new loss 0.001394693274050951 old loss 0.0013947683619335294 BETTER
I0312 02:32:59.280430 1759386 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0014564129523932934 old loss 0.0014564947923645377 BETTER
I0312 02:33:09.592073 1759502 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0010897447355091572 old loss 0.0010916241444647312 BETTER
I0312 02:33:14.905650 1759618 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.0011555912205949426 old loss 0.0011590045178309083 BETTER
I0312 02:33:15.489838 1759270 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0013946432154625654 old loss 0.001394693274050951 BETTER
I0312 02:33:25.879908 1759502 finetune.py:45] layer 26_down initial loss 0.0016964038368314505
I0312 02:33:26.190775 1759386 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0014563597505912185 old loss 0.0014564129523932934 BETTER
I0312 02:33:43.641199 1759618 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0011527532478794456 old loss 0.0011555912205949426 BETTER
I0312 02:33:44.366347 1759270 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0013946070102974772 old loss 0.0013946432154625654 BETTER
I0312 02:33:52.104275 1759502 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0016962733352556825 old loss 0.0016964038368314505 BETTER
I0312 02:33:53.119338 1759386 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0014563206350430846 old loss 0.0014563597505912185 BETTER
I0312 02:34:12.311195 1759618 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.0011505152797326446 old loss 0.0011527532478794456 BETTER
I0312 02:34:13.041450 1759270 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0013945784885436296 old loss 0.0013946070102974772 BETTER
24_v proxy err 0.006331878714263439 tr(WHW.T) 1394.900634765625
24_q proxy err 0.0013301887083798647 tr(WHW.T) 7022.65380859375
24_k proxy err 0.0009393668151460588 tr(WHW.T) 10336.728515625
24_o proxy err 0.004477076698094606 tr(WHW.T) 134.20050048828125
24_up proxy err 0.00585375539958477 tr(WHW.T) 2622.057373046875
24_gate proxy err 0.0036373159382492304 tr(WHW.T) 4263.806640625
24_down proxy err 0.006261556409299374 tr(WHW.T) 341.1719055175781
I0312 02:34:19.284395 1759502 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0016961847431957722 old loss 0.0016962733352556825 BETTER
I0312 02:34:21.121799 1759386 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0014562904834747314 old loss 0.0014563206350430846 BETTER
25_v proxy err 0.006010179873555899 tr(WHW.T) 1707.664794921875
25_q proxy err 0.001501676277257502 tr(WHW.T) 7163.91845703125
25_k proxy err 0.0011433502659201622 tr(WHW.T) 9622.1611328125
25_o proxy err 0.005634216591715813 tr(WHW.T) 83.79444122314453
25_up proxy err 0.005792727693915367 tr(WHW.T) 2805.831298828125
25_gate proxy err 0.0035234892275184393 tr(WHW.T) 4665.751953125
25_down proxy err 0.005997583270072937 tr(WHW.T) 374.5528259277344
I0312 02:34:28.821790 1759618 finetune.py:45] layer 27_down initial loss 0.0018410965567454696
I0312 02:34:46.882498 1759502 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0016961193177849054 old loss 0.0016961847431957722 BETTER
I0312 02:34:54.316180 1759618 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0018409470794722438 old loss 0.0018410965567454696 BETTER
I0312 02:35:13.714316 1759502 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0016960703069344163 old loss 0.0016961193177849054 BETTER
I0312 02:35:20.447837 1759618 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0018408398609608412 old loss 0.0018409470794722438 BETTER
I0312 02:35:40.305632 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 28 in 74.22283339500427s
I0312 02:35:40.526090 1759502 finetune.py:68] layer 26_down @ epoch 4 new loss 0.0016960329376161098 old loss 0.0016960703069344163 BETTER
26_v proxy err 0.0058634462766349316 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.001385150826536119 tr(WHW.T) 7472.06201171875
26_k proxy err 0.0010111238807439804 tr(WHW.T) 10499.830078125
26_o proxy err 0.003553612157702446 tr(WHW.T) 203.2406005859375
26_up proxy err 0.005437277723103762 tr(WHW.T) 3155.699951171875
26_gate proxy err 0.003270841436460614 tr(WHW.T) 5303.89599609375
26_down proxy err 0.006160906050354242 tr(WHW.T) 402.44744873046875
I0312 02:35:43.386329 1759734 config.py:54] PyTorch version 2.1.1 available.
I0312 02:35:44.392518 1756366 quantize_finetune_llama.py:183] layer 29 gpu 1
I0312 02:35:44.472253 1759734 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 02:35:46.784648 1759618 finetune.py:68] layer 27_down @ epoch 2 new loss 0.001840759301558137 old loss 0.0018408398609608412 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 02:35:52.908693 1759734 finetune.py:45] layer 28_v initial loss 0.00025031334371306
I0312 02:36:13.216036 1759618 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0018407009774819016 old loss 0.001840759301558137 BETTER
I0312 02:36:25.887771 1759734 finetune.py:68] layer 28_v @ epoch 0 new loss 0.0001645251759327948 old loss 0.00025031334371306 BETTER
I0312 02:36:39.704701 1759618 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0018406545277684927 old loss 0.0018407009774819016 BETTER
27_v proxy err 0.005653576459735632 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.001382572459988296 tr(WHW.T) 7692.443359375
27_k proxy err 0.0010181459365412593 tr(WHW.T) 10632.544921875
27_o proxy err 0.004724405240267515 tr(WHW.T) 126.65715789794922
27_up proxy err 0.004939764738082886 tr(WHW.T) 3691.172607421875
27_gate proxy err 0.0030756539199501276 tr(WHW.T) 5989.3779296875
27_down proxy err 0.006058477330952883 tr(WHW.T) 468.25787353515625
I0312 02:36:55.377495 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 29 in 70.50820994377136s
I0312 02:36:58.571465 1759850 config.py:54] PyTorch version 2.1.1 available.
I0312 02:36:59.575440 1756366 quantize_finetune_llama.py:183] layer 30 gpu 2
I0312 02:36:59.644026 1759850 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 02:37:00.236380 1759734 finetune.py:68] layer 28_v @ epoch 1 new loss 0.0001574353373143822 old loss 0.0001645251759327948 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 02:37:08.269734 1759850 finetune.py:45] layer 29_v initial loss 0.0002502982097212225
I0312 02:37:34.892736 1759734 finetune.py:68] layer 28_v @ epoch 2 new loss 0.00015363884449470788 old loss 0.0001574353373143822 BETTER
I0312 02:37:39.607690 1759850 finetune.py:68] layer 29_v @ epoch 0 new loss 0.00018342646944802254 old loss 0.0002502982097212225 BETTER
I0312 02:38:09.621627 1759734 finetune.py:68] layer 28_v @ epoch 3 new loss 0.00015092964167706668 old loss 0.00015363884449470788 BETTER
I0312 02:38:10.957874 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 30 in 70.96653318405151s
I0312 02:38:11.899415 1759850 finetune.py:68] layer 29_v @ epoch 1 new loss 0.00017552990175317973 old loss 0.00018342646944802254 BETTER
I0312 02:38:14.431557 1759966 config.py:54] PyTorch version 2.1.1 available.
I0312 02:38:15.451380 1756366 quantize_finetune_llama.py:183] layer 31 gpu 3
I0312 02:38:15.532603 1759966 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 02:38:24.182364 1759966 finetune.py:45] layer 30_v initial loss 0.000259707507211715
I0312 02:38:44.300409 1759850 finetune.py:68] layer 29_v @ epoch 2 new loss 0.0001714313984848559 old loss 0.00017552990175317973 BETTER
I0312 02:38:44.670984 1759734 finetune.py:68] layer 28_v @ epoch 4 new loss 0.0001485745597165078 old loss 0.00015092964167706668 BETTER
I0312 02:38:54.084257 1759734 finetune.py:45] layer 28_q initial loss 0.00019706922466866672
I0312 02:38:55.693840 1759966 finetune.py:68] layer 30_v @ epoch 0 new loss 0.0001737125712679699 old loss 0.000259707507211715 BETTER
I0312 02:39:17.063312 1759850 finetune.py:68] layer 29_v @ epoch 3 new loss 0.00016886430967133492 old loss 0.0001714313984848559 BETTER
I0312 02:39:27.672645 1756366 quantize_finetune_llama.py:210] computed original embedding for layer 31 in 71.80352330207825s
I0312 02:39:27.763182 1759734 finetune.py:68] layer 28_q @ epoch 0 new loss 0.00018714358157012612 old loss 0.00019706922466866672 BETTER
I0312 02:39:28.353284 1759966 finetune.py:68] layer 30_v @ epoch 1 new loss 0.0001663644943619147 old loss 0.0001737125712679699 BETTER
I0312 02:39:31.018465 1760082 config.py:54] PyTorch version 2.1.1 available.
I0312 02:39:32.156868 1760082 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 02:39:40.153237 1760082 finetune.py:45] layer 31_v initial loss 0.0004026898823212832
I0312 02:39:49.911778 1759850 finetune.py:68] layer 29_v @ epoch 4 new loss 0.0001663910661591217 old loss 0.00016886430967133492 BETTER
I0312 02:39:59.026383 1759850 finetune.py:45] layer 29_q initial loss 0.00021047079644631594
I0312 02:40:01.404321 1759966 finetune.py:68] layer 30_v @ epoch 2 new loss 0.00016355873958673328 old loss 0.0001663644943619147 BETTER
I0312 02:40:02.700476 1759734 finetune.py:68] layer 28_q @ epoch 1 new loss 0.000183508571353741 old loss 0.00018714358157012612 BETTER
I0312 02:40:11.488339 1760082 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0002741625066846609 old loss 0.0004026898823212832 BETTER
I0312 02:40:30.780517 1759850 finetune.py:68] layer 29_q @ epoch 0 new loss 0.0002010602329391986 old loss 0.00021047079644631594 BETTER
I0312 02:40:34.804578 1759966 finetune.py:68] layer 30_v @ epoch 3 new loss 0.00015981378965079784 old loss 0.00016355873958673328 BETTER
I0312 02:40:37.217095 1759734 finetune.py:68] layer 28_q @ epoch 2 new loss 0.00018091750098392367 old loss 0.000183508571353741 BETTER
I0312 02:40:43.573567 1760082 finetune.py:68] layer 31_v @ epoch 1 new loss 0.00027028369368053973 old loss 0.0002741625066846609 BETTER
I0312 02:41:03.071882 1759850 finetune.py:68] layer 29_q @ epoch 1 new loss 0.00019773092935793102 old loss 0.0002010602329391986 BETTER
I0312 02:41:07.881385 1759966 finetune.py:68] layer 30_v @ epoch 4 new loss 0.0001583090197527781 old loss 0.00015981378965079784 BETTER
I0312 02:41:11.845769 1759734 finetune.py:68] layer 28_q @ epoch 3 new loss 0.00017900859529618174 old loss 0.00018091750098392367 BETTER
I0312 02:41:15.753905 1760082 finetune.py:68] layer 31_v @ epoch 2 new loss 0.00025953276781365275 old loss 0.00027028369368053973 BETTER
I0312 02:41:17.419619 1759966 finetune.py:45] layer 30_q initial loss 0.00021859072148799896
I0312 02:41:35.180989 1759850 finetune.py:68] layer 29_q @ epoch 2 new loss 0.00019519953639246523 old loss 0.00019773092935793102 BETTER
I0312 02:41:46.616456 1759734 finetune.py:68] layer 28_q @ epoch 4 new loss 0.00017725551151670516 old loss 0.00017900859529618174 BETTER
I0312 02:41:48.391091 1760082 finetune.py:76] layer 31_v @ epoch 3 new loss 0.00026484846603125334 old loss 0.00025953276781365275 WORSE
I0312 02:41:49.111573 1759966 finetune.py:68] layer 30_q @ epoch 0 new loss 0.00020408447016961873 old loss 0.00021859072148799896 BETTER
I0312 02:41:56.190266 1759734 finetune.py:45] layer 28_k initial loss 0.00021943073079455644
I0312 02:42:07.691612 1759850 finetune.py:68] layer 29_q @ epoch 3 new loss 0.0001933988241944462 old loss 0.00019519953639246523 BETTER
I0312 02:42:20.426556 1760082 finetune.py:68] layer 31_v @ epoch 4 new loss 0.00025584938703104854 old loss 0.00025953276781365275 BETTER
I0312 02:42:21.422511 1759966 finetune.py:68] layer 30_q @ epoch 1 new loss 0.00019964054808951914 old loss 0.00020408447016961873 BETTER
I0312 02:42:29.323026 1759734 finetune.py:68] layer 28_k @ epoch 0 new loss 0.0002156424307031557 old loss 0.00021943073079455644 BETTER
I0312 02:42:29.808818 1760082 finetune.py:45] layer 31_q initial loss 0.00048428509035147727
I0312 02:42:40.401092 1759850 finetune.py:68] layer 29_q @ epoch 4 new loss 0.00019173847977072 old loss 0.0001933988241944462 BETTER
I0312 02:42:49.714741 1759850 finetune.py:45] layer 29_k initial loss 0.00023189802595879883
I0312 02:42:54.024204 1759966 finetune.py:68] layer 30_q @ epoch 2 new loss 0.00019684429571498185 old loss 0.00019964054808951914 BETTER
I0312 02:43:01.301758 1760082 finetune.py:68] layer 31_q @ epoch 0 new loss 0.00041160729597322643 old loss 0.00048428509035147727 BETTER
I0312 02:43:03.119555 1759734 finetune.py:68] layer 28_k @ epoch 1 new loss 0.00021435812232084572 old loss 0.0002156424307031557 BETTER
I0312 02:43:21.247955 1759850 finetune.py:68] layer 29_k @ epoch 0 new loss 0.0002288619289174676 old loss 0.00023189802595879883 BETTER
I0312 02:43:26.534017 1759966 finetune.py:68] layer 30_q @ epoch 3 new loss 0.0001947441924130544 old loss 0.00019684429571498185 BETTER
I0312 02:43:33.522924 1760082 finetune.py:68] layer 31_q @ epoch 1 new loss 0.00039428859599865973 old loss 0.00041160729597322643 BETTER
I0312 02:43:37.222147 1759734 finetune.py:68] layer 28_k @ epoch 2 new loss 0.00021332214237190783 old loss 0.00021435812232084572 BETTER
I0312 02:43:53.362935 1759850 finetune.py:68] layer 29_k @ epoch 1 new loss 0.0002274915314046666 old loss 0.0002288619289174676 BETTER
I0312 02:43:58.845427 1759966 finetune.py:68] layer 30_q @ epoch 4 new loss 0.00019223810522817075 old loss 0.0001947441924130544 BETTER
I0312 02:44:05.591246 1760082 finetune.py:68] layer 31_q @ epoch 2 new loss 0.000388686778023839 old loss 0.00039428859599865973 BETTER
I0312 02:44:08.168034 1759966 finetune.py:45] layer 30_k initial loss 0.0002412952744634822
I0312 02:44:11.271860 1759734 finetune.py:68] layer 28_k @ epoch 3 new loss 0.00021270517027005553 old loss 0.00021332214237190783 BETTER
I0312 02:44:25.448316 1759850 finetune.py:68] layer 29_k @ epoch 2 new loss 0.00022650005121249706 old loss 0.0002274915314046666 BETTER
I0312 02:44:37.793952 1760082 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0003783470601774752 old loss 0.000388686778023839 BETTER
I0312 02:44:39.639155 1759966 finetune.py:68] layer 30_k @ epoch 0 new loss 0.00023706446518190205 old loss 0.0002412952744634822 BETTER
I0312 02:44:45.439759 1759734 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0002121074649039656 old loss 0.00021270517027005553 BETTER
I0312 02:44:54.994953 1759734 finetune.py:45] layer 28_o initial loss 0.0004062113002873957
I0312 02:44:57.508235 1759850 finetune.py:68] layer 29_k @ epoch 3 new loss 0.0002258067106595263 old loss 0.00022650005121249706 BETTER
I0312 02:45:10.200088 1760082 finetune.py:68] layer 31_q @ epoch 4 new loss 0.0003772207419387996 old loss 0.0003783470601774752 BETTER
I0312 02:45:12.110110 1759966 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0002352842566324398 old loss 0.00023706446518190205 BETTER
I0312 02:45:19.737541 1760082 finetune.py:45] layer 31_k initial loss 0.0005024091224186122
I0312 02:45:27.830793 1759734 finetune.py:68] layer 28_o @ epoch 0 new loss 0.0003916103742085397 old loss 0.0004062113002873957 BETTER
I0312 02:45:29.608034 1759850 finetune.py:68] layer 29_k @ epoch 4 new loss 0.00022526420070789754 old loss 0.0002258067106595263 BETTER
I0312 02:45:38.902727 1759850 finetune.py:45] layer 29_o initial loss 0.000420205295085907
I0312 02:45:44.179622 1759966 finetune.py:68] layer 30_k @ epoch 2 new loss 0.00023451574088539928 old loss 0.0002352842566324398 BETTER
I0312 02:45:50.674796 1760082 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0004315145197324455 old loss 0.0005024091224186122 BETTER
I0312 02:46:01.415800 1759734 finetune.py:68] layer 28_o @ epoch 1 new loss 0.00038617104291915894 old loss 0.0003916103742085397 BETTER
I0312 02:46:09.635085 1759850 finetune.py:68] layer 29_o @ epoch 0 new loss 0.00040535442531108856 old loss 0.000420205295085907 BETTER
I0312 02:46:16.229944 1759966 finetune.py:68] layer 30_k @ epoch 3 new loss 0.00023365591187030077 old loss 0.00023451574088539928 BETTER
I0312 02:46:22.409190 1760082 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0004281038709450513 old loss 0.0004315145197324455 BETTER
I0312 02:46:34.989963 1759734 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0003824777959380299 old loss 0.00038617104291915894 BETTER
I0312 02:46:41.105675 1759850 finetune.py:68] layer 29_o @ epoch 1 new loss 0.00040057438309304416 old loss 0.00040535442531108856 BETTER
I0312 02:46:48.278311 1759966 finetune.py:68] layer 30_k @ epoch 4 new loss 0.00023300245811697096 old loss 0.00023365591187030077 BETTER
I0312 02:46:54.213506 1760082 finetune.py:68] layer 31_k @ epoch 2 new loss 0.00042192815453745425 old loss 0.0004281038709450513 BETTER
I0312 02:46:57.697614 1759966 finetune.py:45] layer 30_o initial loss 0.000466333469375968
I0312 02:47:08.575165 1759734 finetune.py:68] layer 28_o @ epoch 3 new loss 0.000380005338229239 old loss 0.0003824777959380299 BETTER
I0312 02:47:12.554392 1759850 finetune.py:68] layer 29_o @ epoch 2 new loss 0.0003976341395173222 old loss 0.00040057438309304416 BETTER
I0312 02:47:26.009435 1760082 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0004159111122135073 old loss 0.00042192815453745425 BETTER
I0312 02:47:28.667576 1759966 finetune.py:68] layer 30_o @ epoch 0 new loss 0.00043717684457078576 old loss 0.000466333469375968 BETTER
I0312 02:47:42.165874 1759734 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0003781018313020468 old loss 0.000380005338229239 BETTER
I0312 02:47:44.037140 1759850 finetune.py:68] layer 29_o @ epoch 3 new loss 0.00039572163950651884 old loss 0.0003976341395173222 BETTER
I0312 02:47:57.594953 1759734 finetune.py:45] layer 28_up initial loss 0.0009537040605209768
I0312 02:47:58.027265 1760082 finetune.py:68] layer 31_k @ epoch 4 new loss 0.00041345274075865746 old loss 0.0004159111122135073 BETTER
I0312 02:48:00.398489 1759966 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0004277099797036499 old loss 0.00043717684457078576 BETTER
I0312 02:48:07.131930 1760082 finetune.py:45] layer 31_o initial loss 0.0007854927680455148
I0312 02:48:15.516951 1759850 finetune.py:68] layer 29_o @ epoch 4 new loss 0.0003944162162952125 old loss 0.00039572163950651884 BETTER
I0312 02:48:28.236737 1759734 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0009303420665673912 old loss 0.0009537040605209768 BETTER
I0312 02:48:30.490633 1759850 finetune.py:45] layer 29_up initial loss 0.0010749491630122066
I0312 02:48:32.184299 1759966 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0004222571733407676 old loss 0.0004277099797036499 BETTER
I0312 02:48:37.407976 1760082 finetune.py:68] layer 31_o @ epoch 0 new loss 0.000674710376188159 old loss 0.0007854927680455148 BETTER
I0312 02:48:59.421736 1759850 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0010388302616775036 old loss 0.0010749491630122066 BETTER
I0312 02:48:59.919869 1759734 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0009194529848173261 old loss 0.0009303420665673912 BETTER
I0312 02:49:03.798596 1759966 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0004182474222034216 old loss 0.0004222571733407676 BETTER
I0312 02:49:08.536126 1760082 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0006495826528407633 old loss 0.000674710376188159 BETTER
I0312 02:49:29.112275 1759850 finetune.py:68] layer 29_up @ epoch 1 new loss 0.001024537137709558 old loss 0.0010388302616775036 BETTER
I0312 02:49:31.729044 1759734 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0009117868030443788 old loss 0.0009194529848173261 BETTER
I0312 02:49:35.505549 1759966 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0004158720257692039 old loss 0.0004182474222034216 BETTER
I0312 02:49:39.663740 1760082 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0006366664310917258 old loss 0.0006495826528407633 BETTER
I0312 02:49:50.820183 1759966 finetune.py:45] layer 30_up initial loss 0.0015176207525655627
I0312 02:49:59.022828 1759850 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0010145962005481124 old loss 0.001024537137709558 BETTER
I0312 02:50:03.702275 1759734 finetune.py:68] layer 28_up @ epoch 3 new loss 0.000905957305803895 old loss 0.0009117868030443788 BETTER
I0312 02:50:11.038406 1760082 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0006275151390582323 old loss 0.0006366664310917258 BETTER
I0312 02:50:19.814173 1759966 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0014032123144716024 old loss 0.0015176207525655627 BETTER
I0312 02:50:28.975258 1759850 finetune.py:68] layer 29_up @ epoch 3 new loss 0.001007204526104033 old loss 0.0010145962005481124 BETTER
I0312 02:50:35.734630 1759734 finetune.py:68] layer 28_up @ epoch 4 new loss 0.0009011828224174678 old loss 0.000905957305803895 BETTER
I0312 02:50:42.249636 1760082 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0006216343026608229 old loss 0.0006275151390582323 BETTER
I0312 02:50:49.918774 1759966 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0013651306508108974 old loss 0.0014032123144716024 BETTER
I0312 02:50:51.362139 1759734 finetune.py:45] layer 28_gate initial loss 0.0013972301967442036
I0312 02:50:57.718493 1760082 finetune.py:45] layer 31_up initial loss 0.003525156294927001
I0312 02:50:59.065864 1759850 finetune.py:68] layer 29_up @ epoch 4 new loss 0.001001331489533186 old loss 0.001007204526104033 BETTER
I0312 02:51:14.036813 1759850 finetune.py:45] layer 29_gate initial loss 0.0015911718364804983
I0312 02:51:20.175078 1759966 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0013397895963862538 old loss 0.0013651306508108974 BETTER
I0312 02:51:20.517504 1759734 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0013873308198526502 old loss 0.0013972301967442036 BETTER
I0312 02:51:26.234782 1760082 finetune.py:68] layer 31_up @ epoch 0 new loss 0.002844428177922964 old loss 0.003525156294927001 BETTER
I0312 02:51:41.659308 1759850 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.001576066017150879 old loss 0.0015911718364804983 BETTER
I0312 02:51:50.207876 1759966 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0013201204128563404 old loss 0.0013397895963862538 BETTER
I0312 02:51:50.408022 1759734 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.00138116127345711 old loss 0.0013873308198526502 BETTER
I0312 02:51:55.647007 1760082 finetune.py:68] layer 31_up @ epoch 1 new loss 0.002680110279470682 old loss 0.002844428177922964 BETTER
I0312 02:52:09.763382 1759850 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.0015677737537771463 old loss 0.001576066017150879 BETTER
I0312 02:52:20.408968 1759966 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0013047130778431892 old loss 0.0013201204128563404 BETTER
I0312 02:52:20.489908 1759734 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.0013764607720077038 old loss 0.00138116127345711 BETTER
I0312 02:52:25.158009 1760082 finetune.py:68] layer 31_up @ epoch 2 new loss 0.0025677806697785854 old loss 0.002680110279470682 BETTER
I0312 02:52:35.796288 1759966 finetune.py:45] layer 30_gate initial loss 0.0020408849231898785
I0312 02:52:38.128717 1759850 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.001561480574309826 old loss 0.0015677737537771463 BETTER
I0312 02:52:50.645183 1759734 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0013727714540436864 old loss 0.0013764607720077038 BETTER
I0312 02:52:54.749584 1760082 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0024797068908810616 old loss 0.0025677806697785854 BETTER
I0312 02:53:03.368895 1759966 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0019988026469945908 old loss 0.0020408849231898785 BETTER
I0312 02:53:06.446694 1759850 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0015566061483696103 old loss 0.001561480574309826 BETTER
I0312 02:53:20.791609 1759734 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.0013697355752810836 old loss 0.0013727714540436864 BETTER
I0312 02:53:24.435296 1760082 finetune.py:68] layer 31_up @ epoch 4 new loss 0.002409445820376277 old loss 0.0024797068908810616 BETTER
I0312 02:53:31.903831 1759966 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.0019803144969046116 old loss 0.0019988026469945908 BETTER
I0312 02:53:35.187707 1759850 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0015527617651969194 old loss 0.0015566061483696103 BETTER
I0312 02:53:38.111183 1759734 finetune.py:45] layer 28_down initial loss 0.0022117309272289276
I0312 02:53:41.360287 1760082 finetune.py:45] layer 31_gate initial loss 0.0036958043929189444
I0312 02:53:52.755836 1759850 finetune.py:45] layer 29_down initial loss 0.002615941222757101
I0312 02:54:00.167724 1759966 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0019670501351356506 old loss 0.0019803144969046116 BETTER
I0312 02:54:05.644565 1759734 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0022115150932222605 old loss 0.0022117309272289276 BETTER
I0312 02:54:08.524899 1760082 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.003465145593509078 old loss 0.0036958043929189444 BETTER
I0312 02:54:18.823991 1759850 finetune.py:68] layer 29_down @ epoch 0 new loss 0.002615564037114382 old loss 0.002615941222757101 BETTER
I0312 02:54:28.619128 1759966 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.001956874504685402 old loss 0.0019670501351356506 BETTER
I0312 02:54:34.133523 1759734 finetune.py:68] layer 28_down @ epoch 1 new loss 0.002211346523836255 old loss 0.0022115150932222605 BETTER
I0312 02:54:36.437542 1760082 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.003397585591301322 old loss 0.003465145593509078 BETTER
I0312 02:54:45.694418 1759850 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0026152855716645718 old loss 0.002615564037114382 BETTER
I0312 02:54:56.789930 1759966 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.00194814323913306 old loss 0.001956874504685402 BETTER
I0312 02:55:02.609264 1759734 finetune.py:68] layer 28_down @ epoch 2 new loss 0.00221121683716774 old loss 0.002211346523836255 BETTER
I0312 02:55:04.629162 1760082 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.0033518450800329447 old loss 0.003397585591301322 BETTER
I0312 02:55:12.404571 1759850 finetune.py:68] layer 29_down @ epoch 2 new loss 0.002615068107843399 old loss 0.0026152855716645718 BETTER
I0312 02:55:12.681573 1759966 finetune.py:45] layer 30_down initial loss 0.003833916736766696
I0312 02:55:31.321256 1759734 finetune.py:68] layer 28_down @ epoch 3 new loss 0.002211116487160325 old loss 0.00221121683716774 BETTER
I0312 02:55:33.148224 1760082 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.0033144678454846144 old loss 0.0033518450800329447 BETTER
I0312 02:55:39.014601 1759966 finetune.py:68] layer 30_down @ epoch 0 new loss 0.003824340645223856 old loss 0.003833916736766696 BETTER
I0312 02:55:39.299564 1759850 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0026148853357881308 old loss 0.002615068107843399 BETTER
I0312 02:55:59.898360 1759734 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0022110319696366787 old loss 0.002211116487160325 BETTER
I0312 02:56:01.562078 1760082 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.0032824683003127575 old loss 0.0033144678454846144 BETTER
28_v proxy err 0.005230090115219355 tr(WHW.T) 2018.944091796875
28_q proxy err 0.0014309940161183476 tr(WHW.T) 7651.74853515625
28_k proxy err 0.0010591144673526287 tr(WHW.T) 10560.865234375
28_o proxy err 0.003867080435156822 tr(WHW.T) 195.41537475585938
28_up proxy err 0.004112891852855682 tr(WHW.T) 4661.03515625
28_gate proxy err 0.0029510187450796366 tr(WHW.T) 6545.50634765625
28_down proxy err 0.005769421346485615 tr(WHW.T) 606.1038818359375
I0312 02:56:05.807138 1759966 finetune.py:68] layer 30_down @ epoch 1 new loss 0.0038162204436957836 old loss 0.003824340645223856 BETTER
I0312 02:56:06.249204 1759850 finetune.py:68] layer 29_down @ epoch 4 new loss 0.002614725846797228 old loss 0.0026148853357881308 BETTER
29_v proxy err 0.005485535599291325 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.0014230977976694703 tr(WHW.T) 7228.62109375
29_k proxy err 0.0009958231821656227 tr(WHW.T) 10577.5498046875
29_o proxy err 0.003498748643323779 tr(WHW.T) 208.2514190673828
29_up proxy err 0.003272428410127759 tr(WHW.T) 6072.6025390625
29_gate proxy err 0.0027093926910310984 tr(WHW.T) 7366.52734375
29_down proxy err 0.005608075764030218 tr(WHW.T) 785.9426879882812
I0312 02:56:18.326073 1760082 finetune.py:45] layer 31_down initial loss 0.008369491435587406
I0312 02:56:32.639810 1759966 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0038103703409433365 old loss 0.0038162204436957836 BETTER
I0312 02:56:43.577545 1760082 finetune.py:68] layer 31_down @ epoch 0 new loss 0.008341132663190365 old loss 0.008369491435587406 BETTER
I0312 02:56:59.411567 1759966 finetune.py:68] layer 30_down @ epoch 3 new loss 0.0038051253650337458 old loss 0.0038103703409433365 BETTER
I0312 02:57:09.790117 1760082 finetune.py:68] layer 31_down @ epoch 1 new loss 0.008326913230121136 old loss 0.008341132663190365 BETTER
I0312 02:57:26.383373 1759966 finetune.py:68] layer 30_down @ epoch 4 new loss 0.0038009784184396267 old loss 0.0038051253650337458 BETTER
30_v proxy err 0.004697178490459919 tr(WHW.T) 2261.489501953125
30_q proxy err 0.0014077882515266538 tr(WHW.T) 7817.02880859375
30_k proxy err 0.0010623019188642502 tr(WHW.T) 10540.3916015625
30_o proxy err 0.003377942368388176 tr(WHW.T) 252.57742309570312
30_up proxy err 0.002017547143623233 tr(WHW.T) 10018.1279296875
30_gate proxy err 0.0018466680776327848 tr(WHW.T) 10994.1376953125
30_down proxy err 0.0022022733464837074 tr(WHW.T) 3596.3974609375
I0312 02:57:36.151474 1760082 finetune.py:68] layer 31_down @ epoch 2 new loss 0.00831947848200798 old loss 0.008326913230121136 BETTER
I0312 02:58:02.405527 1760082 finetune.py:68] layer 31_down @ epoch 3 new loss 0.00831546913832426 old loss 0.00831947848200798 BETTER
I0312 02:58:28.863154 1760082 finetune.py:68] layer 31_down @ epoch 4 new loss 0.008312812075018883 old loss 0.00831546913832426 BETTER
31_v proxy err 0.005681284237653017 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.0011165090836584568 tr(WHW.T) 6859.72998046875
31_k proxy err 0.0007651387713849545 tr(WHW.T) 10260.1337890625
31_o proxy err 0.002423656638711691 tr(WHW.T) 458.900634765625
31_up proxy err 0.0011649641674011946 tr(WHW.T) 14563.1416015625
31_gate proxy err 0.0011480620596557856 tr(WHW.T) 14837.025390625
31_down proxy err 0.001207097084261477 tr(WHW.T) 17989.1796875
