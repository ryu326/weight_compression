I0311 13:46:08.563298 1536994 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.22it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 10.01it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00, 10.53it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.44it/s]
I0311 13:46:10.322104 1536994 quantize_finetune_llama.py:134] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:27,  1.12it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:23,  1.28it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:21,  1.36it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:20,  1.39it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:19,  1.41it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:18,  1.43it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:05<00:17,  1.42it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:16,  1.43it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:16,  1.43it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:07<00:15,  1.43it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:07<00:14,  1.43it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:13,  1.44it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:13,  1.44it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:09<00:12,  1.44it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:10<00:11,  1.44it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:11,  1.44it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:12<00:10,  1.44it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:12<00:09,  1.45it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:08,  1.45it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:14<00:08,  1.45it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:14<00:07,  1.45it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:06,  1.45it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:06,  1.45it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:16<00:05,  1.46it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:04,  1.44it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.43it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:18<00:03,  1.43it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:19<00:02,  1.43it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:02,  1.42it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.43it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:21<00:00,  1.43it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.43it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.43it/s]
I0311 13:46:42.728087 1536994 quantize_finetune_llama.py:159] loaded compression model
I0311 13:46:56.403621 1536994 quantize_finetune_llama.py:163] loaded dataset and devset
I0311 13:47:01.491024 1536994 quantize_finetune_llama.py:183] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 13:48:37.187477 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 0 in 95.5641667842865s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0311 13:49:03.140241 1538674 config.py:54] PyTorch version 2.1.1 available.
I0311 13:49:04.099658 1536994 quantize_finetune_llama.py:183] layer 1 gpu 1
I0311 13:49:04.173175 1538674 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 13:49:12.463757 1538674 finetune.py:45] layer 0_v initial loss 2.554325192249962e-06
I0311 13:49:45.109457 1538674 finetune.py:68] layer 0_v @ epoch 0 new loss 9.581951871950878e-07 old loss 2.554325192249962e-06 BETTER
I0311 13:50:13.899135 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 1 in 69.60238862037659s
I0311 13:50:22.203405 1538674 finetune.py:68] layer 0_v @ epoch 1 new loss 4.713891144092486e-07 old loss 9.581951871950878e-07 BETTER
I0311 13:50:24.851878 1539540 config.py:54] PyTorch version 2.1.1 available.
I0311 13:50:25.798046 1536994 quantize_finetune_llama.py:183] layer 2 gpu 2
I0311 13:50:25.864589 1539540 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 13:50:33.853701 1539540 finetune.py:45] layer 1_v initial loss 8.537031681044027e-05
I0311 13:50:56.003768 1538674 finetune.py:68] layer 0_v @ epoch 2 new loss 3.0105837822702597e-07 old loss 4.713891144092486e-07 BETTER
I0311 13:51:05.106000 1539540 finetune.py:68] layer 1_v @ epoch 0 new loss 2.2812351744505577e-05 old loss 8.537031681044027e-05 BETTER
I0311 13:51:30.154995 1538674 finetune.py:68] layer 0_v @ epoch 3 new loss 2.3368900770037726e-07 old loss 3.0105837822702597e-07 BETTER
I0311 13:51:35.777836 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 2 in 69.81201505661011s
I0311 13:51:38.326927 1539540 finetune.py:68] layer 1_v @ epoch 1 new loss 2.0658464563894086e-05 old loss 2.2812351744505577e-05 BETTER
I0311 13:51:44.309855 1540351 config.py:54] PyTorch version 2.1.1 available.
I0311 13:51:45.361315 1536994 quantize_finetune_llama.py:183] layer 3 gpu 3
I0311 13:51:45.440520 1540351 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 13:51:53.480049 1540351 finetune.py:45] layer 2_v initial loss 2.7380670871934853e-05
I0311 13:52:04.451865 1538674 finetune.py:68] layer 0_v @ epoch 4 new loss 2.0246579879312776e-07 old loss 2.3368900770037726e-07 BETTER
I0311 13:52:11.034907 1539540 finetune.py:68] layer 1_v @ epoch 2 new loss 1.488140787841985e-05 old loss 2.0658464563894086e-05 BETTER
I0311 13:52:13.606986 1538674 finetune.py:45] layer 0_q initial loss 2.1053342891264037e-07
I0311 13:52:24.775015 1540351 finetune.py:68] layer 2_v @ epoch 0 new loss 1.676382998994086e-05 old loss 2.7380670871934853e-05 BETTER
I0311 13:52:43.127427 1539540 finetune.py:68] layer 1_v @ epoch 3 new loss 1.3756360203842632e-05 old loss 1.488140787841985e-05 BETTER
I0311 13:52:46.642287 1538674 finetune.py:68] layer 0_q @ epoch 0 new loss 1.8325560802168184e-07 old loss 2.1053342891264037e-07 BETTER
I0311 13:52:56.970613 1540351 finetune.py:68] layer 2_v @ epoch 1 new loss 1.2897296073788311e-05 old loss 1.676382998994086e-05 BETTER
I0311 13:52:57.283926 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 3 in 71.78146433830261s
I0311 13:53:08.068797 1541217 config.py:54] PyTorch version 2.1.1 available.
I0311 13:53:09.130120 1536994 quantize_finetune_llama.py:183] layer 4 gpu 0
I0311 13:53:09.177489 1541217 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 13:53:16.291174 1539540 finetune.py:68] layer 1_v @ epoch 4 new loss 1.150364005297888e-05 old loss 1.3756360203842632e-05 BETTER
I0311 13:53:17.429744 1541217 finetune.py:45] layer 3_v initial loss 4.745550177176483e-05
I0311 13:53:20.845325 1538674 finetune.py:68] layer 0_q @ epoch 1 new loss 1.6924479950830573e-07 old loss 1.8325560802168184e-07 BETTER
I0311 13:53:25.591138 1539540 finetune.py:45] layer 1_q initial loss 3.6798526707571e-05
I0311 13:53:29.701709 1540351 finetune.py:68] layer 2_v @ epoch 2 new loss 1.1219344742130488e-05 old loss 1.2897296073788311e-05 BETTER
I0311 13:53:48.306721 1541217 finetune.py:68] layer 3_v @ epoch 0 new loss 2.5970599381253123e-05 old loss 4.745550177176483e-05 BETTER
I0311 13:53:54.942947 1538674 finetune.py:68] layer 0_q @ epoch 2 new loss 1.5879489012604608e-07 old loss 1.6924479950830573e-07 BETTER
I0311 13:53:56.984475 1539540 finetune.py:68] layer 1_q @ epoch 0 new loss 1.2693595635937527e-05 old loss 3.6798526707571e-05 BETTER
I0311 13:54:02.711965 1540351 finetune.py:68] layer 2_v @ epoch 3 new loss 1.0392825060989708e-05 old loss 1.1219344742130488e-05 BETTER
I0311 13:54:20.222663 1541217 finetune.py:68] layer 3_v @ epoch 1 new loss 2.067692912532948e-05 old loss 2.5970599381253123e-05 BETTER
I0311 13:54:29.088779 1538674 finetune.py:68] layer 0_q @ epoch 3 new loss 1.5078775561505608e-07 old loss 1.5879489012604608e-07 BETTER
I0311 13:54:29.197912 1539540 finetune.py:68] layer 1_q @ epoch 1 new loss 1.1348237421771046e-05 old loss 1.2693595635937527e-05 BETTER
I0311 13:54:35.723104 1540351 finetune.py:68] layer 2_v @ epoch 4 new loss 9.926455277309287e-06 old loss 1.0392825060989708e-05 BETTER
I0311 13:54:44.718986 1540351 finetune.py:45] layer 2_q initial loss 1.1310109584883321e-05
I0311 13:54:52.307001 1541217 finetune.py:68] layer 3_v @ epoch 2 new loss 1.8931725207949057e-05 old loss 2.067692912532948e-05 BETTER
I0311 13:55:01.525924 1539540 finetune.py:76] layer 1_q @ epoch 2 new loss 1.4154073141980916e-05 old loss 1.1348237421771046e-05 WORSE
I0311 13:55:03.138832 1538674 finetune.py:68] layer 0_q @ epoch 4 new loss 1.439081813714438e-07 old loss 1.5078775561505608e-07 BETTER
I0311 13:55:12.235424 1538674 finetune.py:45] layer 0_k initial loss 2.1443572961743484e-07
I0311 13:55:16.270736 1540351 finetune.py:68] layer 2_q @ epoch 0 new loss 1.0405561624793336e-05 old loss 1.1310109584883321e-05 BETTER
I0311 13:55:24.545871 1541217 finetune.py:68] layer 3_v @ epoch 3 new loss 1.811190668377094e-05 old loss 1.8931725207949057e-05 BETTER
I0311 13:55:33.235131 1539540 finetune.py:68] layer 1_q @ epoch 3 new loss 9.599357326806057e-06 old loss 1.1348237421771046e-05 BETTER
I0311 13:55:45.018891 1538674 finetune.py:68] layer 0_k @ epoch 0 new loss 1.4453696906002733e-07 old loss 2.1443572961743484e-07 BETTER
I0311 13:55:48.740222 1540351 finetune.py:68] layer 2_q @ epoch 1 new loss 1.010378218779806e-05 old loss 1.0405561624793336e-05 BETTER
I0311 13:55:56.890446 1541217 finetune.py:68] layer 3_v @ epoch 4 new loss 1.7594536984688602e-05 old loss 1.811190668377094e-05 BETTER
I0311 13:56:05.545403 1539540 finetune.py:76] layer 1_q @ epoch 4 new loss 2.1086214474053122e-05 old loss 9.599357326806057e-06 WORSE
I0311 13:56:05.945664 1541217 finetune.py:45] layer 3_q initial loss 2.1854095393791795e-05
I0311 13:56:14.100771 1539540 finetune.py:45] layer 1_k initial loss 1.971181518456433e-05
I0311 13:56:18.635544 1538674 finetune.py:68] layer 0_k @ epoch 1 new loss 1.385124193120646e-07 old loss 1.4453696906002733e-07 BETTER
I0311 13:56:21.370636 1540351 finetune.py:68] layer 2_q @ epoch 2 new loss 9.877398952085059e-06 old loss 1.010378218779806e-05 BETTER
I0311 13:56:37.322814 1541217 finetune.py:68] layer 3_q @ epoch 0 new loss 2.023902561631985e-05 old loss 2.1854095393791795e-05 BETTER
I0311 13:56:45.651445 1539540 finetune.py:68] layer 1_k @ epoch 0 new loss 1.40840420499444e-05 old loss 1.971181518456433e-05 BETTER
I0311 13:56:52.048100 1538674 finetune.py:68] layer 0_k @ epoch 2 new loss 1.338733852662699e-07 old loss 1.385124193120646e-07 BETTER
I0311 13:56:53.680162 1540351 finetune.py:68] layer 2_q @ epoch 3 new loss 9.688966201792937e-06 old loss 9.877398952085059e-06 BETTER
I0311 13:57:09.354455 1541217 finetune.py:68] layer 3_q @ epoch 1 new loss 1.9725161109818146e-05 old loss 2.023902561631985e-05 BETTER
I0311 13:57:17.702966 1539540 finetune.py:68] layer 1_k @ epoch 1 new loss 1.1195525985385757e-05 old loss 1.40840420499444e-05 BETTER
I0311 13:57:25.533612 1538674 finetune.py:68] layer 0_k @ epoch 3 new loss 1.2976244079254684e-07 old loss 1.338733852662699e-07 BETTER
I0311 13:57:26.269791 1540351 finetune.py:68] layer 2_q @ epoch 4 new loss 9.524875167699065e-06 old loss 9.688966201792937e-06 BETTER
I0311 13:57:35.227087 1540351 finetune.py:45] layer 2_k initial loss 1.0744372048065998e-05
I0311 13:57:41.344363 1541217 finetune.py:68] layer 3_q @ epoch 2 new loss 1.9328619600855745e-05 old loss 1.9725161109818146e-05 BETTER
I0311 13:57:49.692291 1539540 finetune.py:76] layer 1_k @ epoch 2 new loss 1.1211853234271985e-05 old loss 1.1195525985385757e-05 WORSE
I0311 13:57:59.003753 1538674 finetune.py:68] layer 0_k @ epoch 4 new loss 1.2633229573566496e-07 old loss 1.2976244079254684e-07 BETTER
I0311 13:58:06.656712 1540351 finetune.py:68] layer 2_k @ epoch 0 new loss 1.0366266906203236e-05 old loss 1.0744372048065998e-05 BETTER
I0311 13:58:08.218078 1538674 finetune.py:45] layer 0_o initial loss 1.4481456673820503e-06
I0311 13:58:13.353898 1541217 finetune.py:68] layer 3_q @ epoch 3 new loss 1.900163260870613e-05 old loss 1.9328619600855745e-05 BETTER
I0311 13:58:21.103229 1539540 finetune.py:76] layer 1_k @ epoch 3 new loss 1.1399045433790889e-05 old loss 1.1195525985385757e-05 WORSE
I0311 13:58:38.848448 1540351 finetune.py:68] layer 2_k @ epoch 1 new loss 1.018615239445353e-05 old loss 1.0366266906203236e-05 BETTER
I0311 13:58:40.563357 1538674 finetune.py:68] layer 0_o @ epoch 0 new loss 1.334167791355867e-06 old loss 1.4481456673820503e-06 BETTER
I0311 13:58:45.384083 1541217 finetune.py:68] layer 3_q @ epoch 4 new loss 1.871901986305602e-05 old loss 1.900163260870613e-05 BETTER
I0311 13:58:52.513241 1539540 finetune.py:76] layer 1_k @ epoch 4 new loss 1.2100872481823899e-05 old loss 1.1195525985385757e-05 WORSE
I0311 13:58:54.350560 1541217 finetune.py:45] layer 3_k initial loss 2.2277337848208845e-05
I0311 13:59:01.168328 1539540 finetune.py:45] layer 1_o initial loss 4.732710658572614e-05
I0311 13:59:10.918063 1540351 finetune.py:68] layer 2_k @ epoch 2 new loss 1.0040797860710882e-05 old loss 1.018615239445353e-05 BETTER
I0311 13:59:13.472146 1538674 finetune.py:68] layer 0_o @ epoch 1 new loss 1.242406938217755e-06 old loss 1.334167791355867e-06 BETTER
I0311 13:59:25.308469 1541217 finetune.py:68] layer 3_k @ epoch 0 new loss 2.1448335246532224e-05 old loss 2.2277337848208845e-05 BETTER
I0311 13:59:31.720336 1539540 finetune.py:68] layer 1_o @ epoch 0 new loss 2.984016464324668e-05 old loss 4.732710658572614e-05 BETTER
I0311 13:59:43.067531 1540351 finetune.py:68] layer 2_k @ epoch 3 new loss 9.914127076626755e-06 old loss 1.0040797860710882e-05 BETTER
I0311 13:59:46.470557 1538674 finetune.py:68] layer 0_o @ epoch 2 new loss 1.1657219829430687e-06 old loss 1.242406938217755e-06 BETTER
I0311 13:59:56.939860 1541217 finetune.py:68] layer 3_k @ epoch 1 new loss 2.110928289766889e-05 old loss 2.1448335246532224e-05 BETTER
I0311 14:00:03.070289 1539540 finetune.py:68] layer 1_o @ epoch 1 new loss 2.7940219297306612e-05 old loss 2.984016464324668e-05 BETTER
I0311 14:00:15.297348 1540351 finetune.py:68] layer 2_k @ epoch 4 new loss 9.799839062907267e-06 old loss 9.914127076626755e-06 BETTER
I0311 14:00:19.305202 1538674 finetune.py:68] layer 0_o @ epoch 3 new loss 1.1001690154444077e-06 old loss 1.1657219829430687e-06 BETTER
I0311 14:00:24.292977 1540351 finetune.py:45] layer 2_o initial loss 2.800000220304355e-05
I0311 14:00:28.551704 1541217 finetune.py:68] layer 3_k @ epoch 2 new loss 2.0843290258198977e-05 old loss 2.110928289766889e-05 BETTER
I0311 14:00:34.321022 1539540 finetune.py:68] layer 1_o @ epoch 2 new loss 2.713051981118042e-05 old loss 2.7940219297306612e-05 BETTER
I0311 14:00:52.489652 1538674 finetune.py:68] layer 0_o @ epoch 4 new loss 1.0435865078761708e-06 old loss 1.1001690154444077e-06 BETTER
I0311 14:00:55.283279 1540351 finetune.py:68] layer 2_o @ epoch 0 new loss 2.7428610337665305e-05 old loss 2.800000220304355e-05 BETTER
I0311 14:01:00.322438 1541217 finetune.py:68] layer 3_k @ epoch 3 new loss 2.06155855266843e-05 old loss 2.0843290258198977e-05 BETTER
I0311 14:01:05.632058 1539540 finetune.py:68] layer 1_o @ epoch 3 new loss 2.630930976010859e-05 old loss 2.713051981118042e-05 BETTER
I0311 14:01:07.780411 1538674 finetune.py:45] layer 0_up initial loss 1.582181084813783e-06
I0311 14:01:26.973995 1540351 finetune.py:68] layer 2_o @ epoch 1 new loss 2.7028976546716876e-05 old loss 2.7428610337665305e-05 BETTER
I0311 14:01:32.301142 1541217 finetune.py:68] layer 3_k @ epoch 4 new loss 2.041749939962756e-05 old loss 2.06155855266843e-05 BETTER
I0311 14:01:37.017696 1539540 finetune.py:68] layer 1_o @ epoch 4 new loss 2.5572780941729434e-05 old loss 2.630930976010859e-05 BETTER
I0311 14:01:38.179356 1538674 finetune.py:68] layer 0_up @ epoch 0 new loss 1.4438849120779196e-06 old loss 1.582181084813783e-06 BETTER
I0311 14:01:41.546240 1541217 finetune.py:45] layer 3_o initial loss 5.232490730122663e-05
I0311 14:01:51.941570 1539540 finetune.py:45] layer 1_up initial loss 6.510927778435871e-05
I0311 14:01:58.605548 1540351 finetune.py:68] layer 2_o @ epoch 2 new loss 2.6726645955932327e-05 old loss 2.7028976546716876e-05 BETTER
I0311 14:02:09.419912 1538674 finetune.py:68] layer 0_up @ epoch 1 new loss 1.3439632766676368e-06 old loss 1.4438849120779196e-06 BETTER
I0311 14:02:11.739866 1541217 finetune.py:68] layer 3_o @ epoch 0 new loss 5.084694203105755e-05 old loss 5.232490730122663e-05 BETTER
I0311 14:02:20.826081 1539540 finetune.py:68] layer 1_up @ epoch 0 new loss 3.256223499192856e-05 old loss 6.510927778435871e-05 BETTER
I0311 14:02:30.274631 1540351 finetune.py:68] layer 2_o @ epoch 3 new loss 2.6481753593543544e-05 old loss 2.6726645955932327e-05 BETTER
I0311 14:02:40.711348 1538674 finetune.py:68] layer 0_up @ epoch 2 new loss 1.2735147265630076e-06 old loss 1.3439632766676368e-06 BETTER
I0311 14:02:42.690331 1541217 finetune.py:68] layer 3_o @ epoch 1 new loss 4.990080196876079e-05 old loss 5.084694203105755e-05 BETTER
I0311 14:02:50.378314 1539540 finetune.py:68] layer 1_up @ epoch 1 new loss 3.0245511879911646e-05 old loss 3.256223499192856e-05 BETTER
I0311 14:03:01.898676 1540351 finetune.py:68] layer 2_o @ epoch 4 new loss 2.6274707124684937e-05 old loss 2.6481753593543544e-05 BETTER
I0311 14:03:12.004865 1538674 finetune.py:68] layer 0_up @ epoch 3 new loss 1.2209818578412523e-06 old loss 1.2735147265630076e-06 BETTER
I0311 14:03:13.635203 1541217 finetune.py:68] layer 3_o @ epoch 2 new loss 4.923102824250236e-05 old loss 4.990080196876079e-05 BETTER
I0311 14:03:18.375500 1540351 finetune.py:45] layer 2_up initial loss 3.9893398934509605e-05
I0311 14:03:19.839117 1539540 finetune.py:68] layer 1_up @ epoch 2 new loss 2.9903594622737728e-05 old loss 3.0245511879911646e-05 BETTER
I0311 14:03:43.316956 1538674 finetune.py:68] layer 0_up @ epoch 4 new loss 1.1809405577878351e-06 old loss 1.2209818578412523e-06 BETTER
I0311 14:03:44.612107 1541217 finetune.py:68] layer 3_o @ epoch 3 new loss 4.8707261157687753e-05 old loss 4.923102824250236e-05 BETTER
I0311 14:03:47.367907 1540351 finetune.py:68] layer 2_up @ epoch 0 new loss 3.949504753109068e-05 old loss 3.9893398934509605e-05 BETTER
I0311 14:03:49.441088 1539540 finetune.py:68] layer 1_up @ epoch 3 new loss 2.9380249543464743e-05 old loss 2.9903594622737728e-05 BETTER
I0311 14:03:58.806915 1538674 finetune.py:45] layer 0_gate initial loss 1.826443394747912e-06
I0311 14:04:15.832836 1541217 finetune.py:68] layer 3_o @ epoch 4 new loss 4.8279387556249276e-05 old loss 4.8707261157687753e-05 BETTER
I0311 14:04:17.381049 1540351 finetune.py:68] layer 2_up @ epoch 1 new loss 3.919771916116588e-05 old loss 3.949504753109068e-05 BETTER
I0311 14:04:19.190003 1539540 finetune.py:68] layer 1_up @ epoch 4 new loss 2.905377186834812e-05 old loss 2.9380249543464743e-05 BETTER
I0311 14:04:27.329363 1538674 finetune.py:68] layer 0_gate @ epoch 0 new loss 1.6960344737526611e-06 old loss 1.826443394747912e-06 BETTER
I0311 14:04:31.077510 1541217 finetune.py:45] layer 3_up initial loss 7.927886326797307e-05
I0311 14:04:34.457972 1539540 finetune.py:45] layer 1_gate initial loss 5.38173844688572e-05
I0311 14:04:47.351697 1540351 finetune.py:68] layer 2_up @ epoch 2 new loss 3.896427733707242e-05 old loss 3.919771916116588e-05 BETTER
I0311 14:04:57.032274 1538674 finetune.py:68] layer 0_gate @ epoch 1 new loss 1.596762331246282e-06 old loss 1.6960344737526611e-06 BETTER
I0311 14:04:59.444129 1541217 finetune.py:68] layer 3_up @ epoch 0 new loss 7.842981722205877e-05 old loss 7.927886326797307e-05 BETTER
I0311 14:05:01.875378 1539540 finetune.py:68] layer 1_gate @ epoch 0 new loss 3.539460522006266e-05 old loss 5.38173844688572e-05 BETTER
I0311 14:05:17.383244 1540351 finetune.py:68] layer 2_up @ epoch 3 new loss 3.876722257700749e-05 old loss 3.896427733707242e-05 BETTER
I0311 14:05:26.816439 1538674 finetune.py:68] layer 0_gate @ epoch 2 new loss 1.5206155694613699e-06 old loss 1.596762331246282e-06 BETTER
I0311 14:05:28.936993 1541217 finetune.py:68] layer 3_up @ epoch 1 new loss 7.780904707033187e-05 old loss 7.842981722205877e-05 BETTER
I0311 14:05:30.208595 1539540 finetune.py:68] layer 1_gate @ epoch 1 new loss 3.419983841013163e-05 old loss 3.539460522006266e-05 BETTER
I0311 14:05:47.334811 1540351 finetune.py:68] layer 2_up @ epoch 4 new loss 3.8596375816268846e-05 old loss 3.876722257700749e-05 BETTER
I0311 14:05:56.727923 1538674 finetune.py:68] layer 0_gate @ epoch 3 new loss 1.4624141613239772e-06 old loss 1.5206155694613699e-06 BETTER
I0311 14:05:58.412997 1539540 finetune.py:68] layer 1_gate @ epoch 2 new loss 3.373447907506488e-05 old loss 3.419983841013163e-05 BETTER
I0311 14:05:58.670000 1541217 finetune.py:68] layer 3_up @ epoch 2 new loss 7.732522499281913e-05 old loss 7.780904707033187e-05 BETTER
I0311 14:06:02.985209 1540351 finetune.py:45] layer 2_gate initial loss 4.9131504056276754e-05
I0311 14:06:26.734266 1538674 finetune.py:68] layer 0_gate @ epoch 4 new loss 1.4176753211359028e-06 old loss 1.4624141613239772e-06 BETTER
I0311 14:06:26.865825 1539540 finetune.py:68] layer 1_gate @ epoch 3 new loss 3.3244046790059656e-05 old loss 3.373447907506488e-05 BETTER
I0311 14:06:28.820418 1541217 finetune.py:68] layer 3_up @ epoch 3 new loss 7.691763312323019e-05 old loss 7.732522499281913e-05 BETTER
I0311 14:06:30.817210 1540351 finetune.py:68] layer 2_gate @ epoch 0 new loss 4.876563252764754e-05 old loss 4.9131504056276754e-05 BETTER
I0311 14:06:43.117090 1538674 finetune.py:45] layer 0_down initial loss 3.1916385978547623e-06
I0311 14:06:55.297360 1539540 finetune.py:68] layer 1_gate @ epoch 4 new loss 3.2881773222470656e-05 old loss 3.3244046790059656e-05 BETTER
I0311 14:06:58.291562 1541217 finetune.py:68] layer 3_up @ epoch 4 new loss 7.656443631276488e-05 old loss 7.691763312323019e-05 BETTER
I0311 14:06:58.831640 1540351 finetune.py:68] layer 2_gate @ epoch 1 new loss 4.8473677452420816e-05 old loss 4.876563252764754e-05 BETTER
I0311 14:07:10.174100 1538674 finetune.py:68] layer 0_down @ epoch 0 new loss 3.1339864108304027e-06 old loss 3.1916385978547623e-06 BETTER
I0311 14:07:11.111922 1539540 finetune.py:45] layer 1_down initial loss 0.0007299852441065013
I0311 14:07:13.533656 1541217 finetune.py:45] layer 3_gate initial loss 9.997876622946933e-05
I0311 14:07:26.902726 1540351 finetune.py:68] layer 2_gate @ epoch 2 new loss 4.822894698008895e-05 old loss 4.8473677452420816e-05 BETTER
I0311 14:07:36.875015 1539540 finetune.py:68] layer 1_down @ epoch 0 new loss 0.0007277233526110649 old loss 0.0007299852441065013 BETTER
I0311 14:07:38.362671 1538674 finetune.py:68] layer 0_down @ epoch 1 new loss 3.093722398261889e-06 old loss 3.1339864108304027e-06 BETTER
I0311 14:07:40.738705 1541217 finetune.py:68] layer 3_gate @ epoch 0 new loss 9.932941611623392e-05 old loss 9.997876622946933e-05 BETTER
I0311 14:07:54.813791 1540351 finetune.py:68] layer 2_gate @ epoch 3 new loss 4.8017111112130806e-05 old loss 4.822894698008895e-05 BETTER
I0311 14:08:03.541129 1539540 finetune.py:68] layer 1_down @ epoch 1 new loss 0.0007275078096427023 old loss 0.0007277233526110649 BETTER
I0311 14:08:06.687011 1538674 finetune.py:68] layer 0_down @ epoch 2 new loss 3.066145154662081e-06 old loss 3.093722398261889e-06 BETTER
I0311 14:08:08.867661 1541217 finetune.py:68] layer 3_gate @ epoch 1 new loss 9.878475248115137e-05 old loss 9.932941611623392e-05 BETTER
I0311 14:08:22.884467 1540351 finetune.py:68] layer 2_gate @ epoch 4 new loss 4.7830304538365453e-05 old loss 4.8017111112130806e-05 BETTER
I0311 14:08:30.130945 1539540 finetune.py:68] layer 1_down @ epoch 2 new loss 0.0007267529726959765 old loss 0.0007275078096427023 BETTER
I0311 14:08:34.968874 1538674 finetune.py:68] layer 0_down @ epoch 3 new loss 3.0461849291896215e-06 old loss 3.066145154662081e-06 BETTER
I0311 14:08:37.040599 1541217 finetune.py:68] layer 3_gate @ epoch 2 new loss 9.831952047534287e-05 old loss 9.878475248115137e-05 BETTER
I0311 14:08:38.876793 1540351 finetune.py:45] layer 2_down initial loss 7.173252379288897e-05
I0311 14:08:56.884276 1539540 finetune.py:68] layer 1_down @ epoch 3 new loss 0.0007266403408721089 old loss 0.0007267529726959765 BETTER
I0311 14:09:03.331304 1538674 finetune.py:68] layer 0_down @ epoch 4 new loss 3.0309229259728454e-06 old loss 3.0461849291896215e-06 BETTER
I0311 14:09:05.140823 1540351 finetune.py:68] layer 2_down @ epoch 0 new loss 7.15988571755588e-05 old loss 7.173252379288897e-05 BETTER
0_v proxy err 0.032212838530540466 tr(WHW.T) 4.225186347961426
0_q proxy err 0.00017769930127542466 tr(WHW.T) 2711.06396484375
0_k proxy err 0.0001966454874491319 tr(WHW.T) 1699.275146484375
0_o proxy err 0.003652466693893075 tr(WHW.T) 0.9741519689559937
0_up proxy err 0.018997296690940857 tr(WHW.T) 43.276519775390625
0_gate proxy err 0.013092026114463806 tr(WHW.T) 63.4854621887207
0_down proxy err 0.011192004196345806 tr(WHW.T) 0.6611024141311646
I0311 14:09:05.191652 1541217 finetune.py:68] layer 3_gate @ epoch 3 new loss 9.791607590159401e-05 old loss 9.831952047534287e-05 BETTER
I0311 14:09:24.355323 1539540 finetune.py:68] layer 1_down @ epoch 4 new loss 0.0007262344006448984 old loss 0.0007266403408721089 BETTER
1_v proxy err 0.06959035992622375 tr(WHW.T) 16.465883255004883
1_q proxy err 0.000378705415641889 tr(WHW.T) 4781.2998046875
1_k proxy err 0.0003918800794053823 tr(WHW.T) 5002.65869140625
1_o proxy err 0.030111214146018028 tr(WHW.T) 1.1131401062011719
1_up proxy err 0.026185104623436928 tr(WHW.T) 109.84740447998047
1_gate proxy err 0.013483848422765732 tr(WHW.T) 221.14865112304688
1_down proxy err 0.0015495202969759703 tr(WHW.T) 2043.2489013671875
I0311 14:09:32.541431 1540351 finetune.py:68] layer 2_down @ epoch 1 new loss 7.149099837988615e-05 old loss 7.15988571755588e-05 BETTER
I0311 14:09:33.943464 1541217 finetune.py:68] layer 3_gate @ epoch 4 new loss 9.755900100572035e-05 old loss 9.791607590159401e-05 BETTER
I0311 14:09:49.153733 1541217 finetune.py:45] layer 3_down initial loss 0.00014637099229730666
I0311 14:09:59.495890 1540351 finetune.py:68] layer 2_down @ epoch 2 new loss 7.140096568036824e-05 old loss 7.149099837988615e-05 BETTER
I0311 14:10:14.567664 1541217 finetune.py:68] layer 3_down @ epoch 0 new loss 0.00014616255066357553 old loss 0.00014637099229730666 BETTER
I0311 14:10:26.246783 1540351 finetune.py:68] layer 2_down @ epoch 3 new loss 7.13250701664947e-05 old loss 7.140096568036824e-05 BETTER
I0311 14:10:41.281569 1541217 finetune.py:68] layer 3_down @ epoch 1 new loss 0.00014600175200030208 old loss 0.00014616255066357553 BETTER
I0311 14:10:43.103965 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 4 in 72.8058729171753s
I0311 14:10:46.486244 1550761 config.py:54] PyTorch version 2.1.1 available.
I0311 14:10:47.562173 1536994 quantize_finetune_llama.py:183] layer 5 gpu 1
I0311 14:10:47.636425 1550761 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:10:53.279157 1540351 finetune.py:68] layer 2_down @ epoch 4 new loss 7.125977572286502e-05 old loss 7.13250701664947e-05 BETTER
2_v proxy err 0.033577438443899155 tr(WHW.T) 136.67332458496094
2_q proxy err 0.0007389130187220871 tr(WHW.T) 7757.05908203125
2_k proxy err 0.0005793090676888824 tr(WHW.T) 10217.833984375
2_o proxy err 0.036622464656829834 tr(WHW.T) 1.467541217803955
2_up proxy err 0.03198898211121559 tr(WHW.T) 193.5021209716797
2_gate proxy err 0.020822172984480858 tr(WHW.T) 306.3603515625
2_down proxy err 0.033385127782821655 tr(WHW.T) 3.029202938079834
I0311 14:10:57.017527 1550761 finetune.py:45] layer 4_v initial loss 6.788036989746615e-05
I0311 14:11:07.652300 1541217 finetune.py:68] layer 3_down @ epoch 2 new loss 0.00014587261830456555 old loss 0.00014600175200030208 BETTER
I0311 14:11:30.363571 1550761 finetune.py:68] layer 4_v @ epoch 0 new loss 3.684565672301687e-05 old loss 6.788036989746615e-05 BETTER
I0311 14:11:34.006133 1541217 finetune.py:68] layer 3_down @ epoch 3 new loss 0.00014576489047612995 old loss 0.00014587261830456555 BETTER
I0311 14:12:00.594083 1541217 finetune.py:68] layer 3_down @ epoch 4 new loss 0.00014567218022421002 old loss 0.00014576489047612995 BETTER
3_v proxy err 0.0442550852894783 tr(WHW.T) 284.77557373046875
3_q proxy err 0.002071764087304473 tr(WHW.T) 7221.4306640625
3_k proxy err 0.0015196071472018957 tr(WHW.T) 10089.3046875
3_o proxy err 0.030966689810156822 tr(WHW.T) 3.3733444213867188
3_up proxy err 0.03573264181613922 tr(WHW.T) 284.8477478027344
3_gate proxy err 0.02192867547273636 tr(WHW.T) 477.5670166015625
3_down proxy err 0.03400355577468872 tr(WHW.T) 6.184110641479492
I0311 14:12:04.984058 1550761 finetune.py:68] layer 4_v @ epoch 1 new loss 3.037237547687255e-05 old loss 3.684565672301687e-05 BETTER
I0311 14:12:09.202630 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 5 in 70.23426151275635s
I0311 14:12:12.324862 1551641 config.py:54] PyTorch version 2.1.1 available.
I0311 14:12:13.288599 1536994 quantize_finetune_llama.py:183] layer 6 gpu 2
I0311 14:12:13.352885 1551641 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:12:22.264710 1551641 finetune.py:45] layer 5_v initial loss 0.00010091376316267997
I0311 14:12:40.474579 1550761 finetune.py:68] layer 4_v @ epoch 2 new loss 2.8151189326308668e-05 old loss 3.037237547687255e-05 BETTER
I0311 14:12:54.229129 1551641 finetune.py:68] layer 5_v @ epoch 0 new loss 6.267023127293214e-05 old loss 0.00010091376316267997 BETTER
I0311 14:13:15.611653 1550761 finetune.py:68] layer 4_v @ epoch 3 new loss 2.694515023904387e-05 old loss 2.8151189326308668e-05 BETTER
I0311 14:13:27.248087 1551641 finetune.py:68] layer 5_v @ epoch 1 new loss 5.518334364751354e-05 old loss 6.267023127293214e-05 BETTER
I0311 14:13:27.440671 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 6 in 73.7412178516388s
I0311 14:13:30.821698 1552465 config.py:54] PyTorch version 2.1.1 available.
I0311 14:13:31.882822 1536994 quantize_finetune_llama.py:183] layer 7 gpu 3
I0311 14:13:31.952757 1552465 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:13:41.560528 1552465 finetune.py:45] layer 6_v initial loss 0.00014004916010890156
I0311 14:13:50.710978 1550761 finetune.py:68] layer 4_v @ epoch 4 new loss 2.6119927497347817e-05 old loss 2.694515023904387e-05 BETTER
I0311 14:14:00.241914 1550761 finetune.py:45] layer 4_q initial loss 3.1693794880993664e-05
I0311 14:14:00.344892 1551641 finetune.py:68] layer 5_v @ epoch 2 new loss 5.2117728046141565e-05 old loss 5.518334364751354e-05 BETTER
I0311 14:14:13.230438 1552465 finetune.py:68] layer 6_v @ epoch 0 new loss 8.06806783657521e-05 old loss 0.00014004916010890156 BETTER
I0311 14:14:33.333591 1551641 finetune.py:68] layer 5_v @ epoch 3 new loss 5.016841532778926e-05 old loss 5.2117728046141565e-05 BETTER
I0311 14:14:34.011743 1550761 finetune.py:68] layer 4_q @ epoch 0 new loss 2.9845383323845454e-05 old loss 3.1693794880993664e-05 BETTER
I0311 14:14:43.174317 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 7 in 70.83095049858093s
I0311 14:14:46.422423 1552465 finetune.py:68] layer 6_v @ epoch 1 new loss 7.10831445758231e-05 old loss 8.06806783657521e-05 BETTER
I0311 14:14:46.527283 1553244 config.py:54] PyTorch version 2.1.1 available.
I0311 14:14:47.573112 1536994 quantize_finetune_llama.py:183] layer 8 gpu 0
I0311 14:14:47.655291 1553244 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:14:55.841251 1553244 finetune.py:45] layer 7_v initial loss 0.00018061231821775436
I0311 14:15:06.407080 1551641 finetune.py:68] layer 5_v @ epoch 4 new loss 4.874038859270513e-05 old loss 5.016841532778926e-05 BETTER
I0311 14:15:08.480296 1550761 finetune.py:68] layer 4_q @ epoch 1 new loss 2.9018669010838494e-05 old loss 2.9845383323845454e-05 BETTER
I0311 14:15:15.562731 1551641 finetune.py:45] layer 5_q initial loss 5.691890692105517e-05
I0311 14:15:19.335496 1552465 finetune.py:68] layer 6_v @ epoch 2 new loss 6.707712600473315e-05 old loss 7.10831445758231e-05 BETTER
I0311 14:15:26.923729 1553244 finetune.py:68] layer 7_v @ epoch 0 new loss 0.00011065234866691753 old loss 0.00018061231821775436 BETTER
I0311 14:15:43.231094 1550761 finetune.py:68] layer 4_q @ epoch 2 new loss 2.838645741576329e-05 old loss 2.9018669010838494e-05 BETTER
I0311 14:15:47.294695 1551641 finetune.py:68] layer 5_q @ epoch 0 new loss 5.3947456763125956e-05 old loss 5.691890692105517e-05 BETTER
I0311 14:15:52.571053 1552465 finetune.py:68] layer 6_v @ epoch 3 new loss 6.45686304778792e-05 old loss 6.707712600473315e-05 BETTER
I0311 14:15:58.837075 1553244 finetune.py:68] layer 7_v @ epoch 1 new loss 9.95077716652304e-05 old loss 0.00011065234866691753 BETTER
I0311 14:16:17.855121 1550761 finetune.py:68] layer 4_q @ epoch 3 new loss 2.7863652576343156e-05 old loss 2.838645741576329e-05 BETTER
I0311 14:16:19.644873 1551641 finetune.py:68] layer 5_q @ epoch 1 new loss 5.2529951062751934e-05 old loss 5.3947456763125956e-05 BETTER
I0311 14:16:25.960619 1552465 finetune.py:68] layer 6_v @ epoch 4 new loss 6.274517363635823e-05 old loss 6.45686304778792e-05 BETTER
I0311 14:16:30.830878 1553244 finetune.py:68] layer 7_v @ epoch 2 new loss 9.445744217373431e-05 old loss 9.95077716652304e-05 BETTER
I0311 14:16:35.324935 1552465 finetune.py:45] layer 6_q initial loss 8.015005005290732e-05
I0311 14:16:52.355102 1551641 finetune.py:68] layer 5_q @ epoch 2 new loss 5.144715032656677e-05 old loss 5.2529951062751934e-05 BETTER
I0311 14:16:52.631077 1550761 finetune.py:68] layer 4_q @ epoch 4 new loss 2.7418887839303352e-05 old loss 2.7863652576343156e-05 BETTER
I0311 14:17:02.362385 1550761 finetune.py:45] layer 4_k initial loss 3.160337655572221e-05
I0311 14:17:03.676211 1553244 finetune.py:68] layer 7_v @ epoch 3 new loss 9.113884152611718e-05 old loss 9.445744217373431e-05 BETTER
I0311 14:17:07.090533 1552465 finetune.py:68] layer 6_q @ epoch 0 new loss 7.47038793633692e-05 old loss 8.015005005290732e-05 BETTER
I0311 14:17:24.983461 1551641 finetune.py:68] layer 5_q @ epoch 3 new loss 5.05310163134709e-05 old loss 5.144715032656677e-05 BETTER
I0311 14:17:35.340886 1550761 finetune.py:68] layer 4_k @ epoch 0 new loss 3.089660458499566e-05 old loss 3.160337655572221e-05 BETTER
I0311 14:17:36.173853 1553244 finetune.py:68] layer 7_v @ epoch 4 new loss 8.869735029293224e-05 old loss 9.113884152611718e-05 BETTER
I0311 14:17:39.612263 1552465 finetune.py:68] layer 6_q @ epoch 1 new loss 7.267110049724579e-05 old loss 7.47038793633692e-05 BETTER
I0311 14:17:45.344321 1553244 finetune.py:45] layer 7_q initial loss 0.00011357215407770127
I0311 14:17:57.419928 1551641 finetune.py:68] layer 5_q @ epoch 4 new loss 4.973580871592276e-05 old loss 5.05310163134709e-05 BETTER
I0311 14:18:06.451454 1551641 finetune.py:45] layer 5_k initial loss 5.5142569181043655e-05
I0311 14:18:09.422967 1550761 finetune.py:68] layer 4_k @ epoch 1 new loss 3.0457056709565222e-05 old loss 3.089660458499566e-05 BETTER
I0311 14:18:12.882347 1552465 finetune.py:68] layer 6_q @ epoch 2 new loss 7.113735773600638e-05 old loss 7.267110049724579e-05 BETTER
I0311 14:18:16.691760 1553244 finetune.py:68] layer 7_q @ epoch 0 new loss 0.00010589144949335605 old loss 0.00011357215407770127 BETTER
I0311 14:18:37.882238 1551641 finetune.py:68] layer 5_k @ epoch 0 new loss 5.386818156694062e-05 old loss 5.5142569181043655e-05 BETTER
I0311 14:18:43.497113 1550761 finetune.py:68] layer 4_k @ epoch 2 new loss 3.0087454433669336e-05 old loss 3.0457056709565222e-05 BETTER
I0311 14:18:45.890258 1552465 finetune.py:68] layer 6_q @ epoch 3 new loss 6.987808592384681e-05 old loss 7.113735773600638e-05 BETTER
I0311 14:18:48.710915 1553244 finetune.py:68] layer 7_q @ epoch 1 new loss 0.00010299196583218873 old loss 0.00010589144949335605 BETTER
I0311 14:19:10.128829 1551641 finetune.py:68] layer 5_k @ epoch 1 new loss 5.3130534070078284e-05 old loss 5.386818156694062e-05 BETTER
I0311 14:19:17.444227 1550761 finetune.py:68] layer 4_k @ epoch 3 new loss 2.9764232749585062e-05 old loss 3.0087454433669336e-05 BETTER
I0311 14:19:18.859534 1552465 finetune.py:68] layer 6_q @ epoch 4 new loss 6.88175787217915e-05 old loss 6.987808592384681e-05 BETTER
I0311 14:19:20.760385 1553244 finetune.py:68] layer 7_q @ epoch 2 new loss 0.00010081366053782403 old loss 0.00010299196583218873 BETTER
I0311 14:19:28.155241 1552465 finetune.py:45] layer 6_k initial loss 8.118676487356424e-05
I0311 14:19:42.349119 1551641 finetune.py:68] layer 5_k @ epoch 2 new loss 5.250573667581193e-05 old loss 5.3130534070078284e-05 BETTER
I0311 14:19:51.596259 1550761 finetune.py:68] layer 4_k @ epoch 4 new loss 2.9475200790329836e-05 old loss 2.9764232749585062e-05 BETTER
I0311 14:19:52.791646 1553244 finetune.py:68] layer 7_q @ epoch 3 new loss 9.905509068630636e-05 old loss 0.00010081366053782403 BETTER
I0311 14:19:59.653864 1552465 finetune.py:68] layer 6_k @ epoch 0 new loss 7.821157487342134e-05 old loss 8.118676487356424e-05 BETTER
I0311 14:20:01.032750 1550761 finetune.py:45] layer 4_o initial loss 8.285084913950413e-05
I0311 14:20:14.478789 1551641 finetune.py:68] layer 5_k @ epoch 3 new loss 5.1950370107078925e-05 old loss 5.250573667581193e-05 BETTER
I0311 14:20:24.816148 1553244 finetune.py:68] layer 7_q @ epoch 4 new loss 9.755686915013939e-05 old loss 9.905509068630636e-05 BETTER
I0311 14:20:32.138785 1552465 finetune.py:68] layer 6_k @ epoch 1 new loss 7.713191007496789e-05 old loss 7.821157487342134e-05 BETTER
I0311 14:20:33.713368 1550761 finetune.py:68] layer 4_o @ epoch 0 new loss 7.929273851914331e-05 old loss 8.285084913950413e-05 BETTER
I0311 14:20:35.249785 1553244 finetune.py:45] layer 7_k initial loss 0.00011467006697785109
I0311 14:20:47.421574 1551641 finetune.py:68] layer 5_k @ epoch 4 new loss 5.145645991433412e-05 old loss 5.1950370107078925e-05 BETTER
I0311 14:20:56.717588 1551641 finetune.py:45] layer 5_o initial loss 0.00013552226300816983
I0311 14:21:05.136117 1552465 finetune.py:68] layer 6_k @ epoch 2 new loss 7.623159035574645e-05 old loss 7.713191007496789e-05 BETTER
I0311 14:21:06.350050 1553244 finetune.py:68] layer 7_k @ epoch 0 new loss 0.00011157719563925639 old loss 0.00011467006697785109 BETTER
I0311 14:21:07.389679 1550761 finetune.py:68] layer 4_o @ epoch 1 new loss 7.761348388157785e-05 old loss 7.929273851914331e-05 BETTER
I0311 14:21:27.811139 1551641 finetune.py:68] layer 5_o @ epoch 0 new loss 0.00012949973461218178 old loss 0.00013552226300816983 BETTER
I0311 14:21:37.947010 1552465 finetune.py:68] layer 6_k @ epoch 3 new loss 7.546663982793689e-05 old loss 7.623159035574645e-05 BETTER
I0311 14:21:38.340007 1553244 finetune.py:68] layer 7_k @ epoch 1 new loss 0.00011004183761542663 old loss 0.00011157719563925639 BETTER
I0311 14:21:41.158313 1550761 finetune.py:68] layer 4_o @ epoch 2 new loss 7.650953193660825e-05 old loss 7.761348388157785e-05 BETTER
I0311 14:21:59.777446 1551641 finetune.py:68] layer 5_o @ epoch 1 new loss 0.00012656382750719786 old loss 0.00012949973461218178 BETTER
I0311 14:22:10.596531 1553244 finetune.py:68] layer 7_k @ epoch 2 new loss 0.00010876554733840749 old loss 0.00011004183761542663 BETTER
I0311 14:22:10.936320 1552465 finetune.py:68] layer 6_k @ epoch 4 new loss 7.480399654014036e-05 old loss 7.546663982793689e-05 BETTER
I0311 14:22:15.064250 1550761 finetune.py:68] layer 4_o @ epoch 3 new loss 7.566190470242873e-05 old loss 7.650953193660825e-05 BETTER
I0311 14:22:20.384451 1552465 finetune.py:45] layer 6_o initial loss 0.0001990932651096955
I0311 14:22:31.630252 1551641 finetune.py:68] layer 5_o @ epoch 2 new loss 0.00012447297922335565 old loss 0.00012656382750719786 BETTER
I0311 14:22:42.620556 1553244 finetune.py:68] layer 7_k @ epoch 3 new loss 0.00010770127846626565 old loss 0.00010876554733840749 BETTER
I0311 14:22:48.714229 1550761 finetune.py:68] layer 4_o @ epoch 4 new loss 7.494245801353827e-05 old loss 7.566190470242873e-05 BETTER
I0311 14:22:51.835181 1552465 finetune.py:68] layer 6_o @ epoch 0 new loss 0.00018922355957329273 old loss 0.0001990932651096955 BETTER
I0311 14:23:03.520093 1551641 finetune.py:68] layer 5_o @ epoch 3 new loss 0.00012276288180146366 old loss 0.00012447297922335565 BETTER
I0311 14:23:04.248660 1550761 finetune.py:45] layer 4_up initial loss 0.00013242126442492008
I0311 14:23:14.491799 1553244 finetune.py:68] layer 7_k @ epoch 4 new loss 0.00010675166413420811 old loss 0.00010770127846626565 BETTER
I0311 14:23:23.685506 1553244 finetune.py:45] layer 7_o initial loss 0.00027399632381275296
I0311 14:23:23.861199 1552465 finetune.py:68] layer 6_o @ epoch 1 new loss 0.0001848982647061348 old loss 0.00018922355957329273 BETTER
I0311 14:23:35.384699 1551641 finetune.py:68] layer 5_o @ epoch 4 new loss 0.0001213180148624815 old loss 0.00012276288180146366 BETTER
I0311 14:23:35.519548 1550761 finetune.py:68] layer 4_up @ epoch 0 new loss 0.00013068149564787745 old loss 0.00013242126442492008 BETTER
I0311 14:23:50.266799 1551641 finetune.py:45] layer 5_up initial loss 0.00021301890956237912
I0311 14:23:53.943785 1553244 finetune.py:68] layer 7_o @ epoch 0 new loss 0.00025970122078433633 old loss 0.00027399632381275296 BETTER
I0311 14:23:55.829274 1552465 finetune.py:68] layer 6_o @ epoch 2 new loss 0.00018184453074354678 old loss 0.0001848982647061348 BETTER
I0311 14:24:07.407220 1550761 finetune.py:68] layer 4_up @ epoch 1 new loss 0.00012948070070706308 old loss 0.00013068149564787745 BETTER
I0311 14:24:19.392703 1551641 finetune.py:68] layer 5_up @ epoch 0 new loss 0.00021002440189477056 old loss 0.00021301890956237912 BETTER
I0311 14:24:25.069644 1553244 finetune.py:68] layer 7_o @ epoch 1 new loss 0.00025323560112155974 old loss 0.00025970122078433633 BETTER
I0311 14:24:27.925582 1552465 finetune.py:68] layer 6_o @ epoch 3 new loss 0.00017939104873221368 old loss 0.00018184453074354678 BETTER
I0311 14:24:39.406734 1550761 finetune.py:68] layer 4_up @ epoch 2 new loss 0.00012853179941885173 old loss 0.00012948070070706308 BETTER
I0311 14:24:49.519692 1551641 finetune.py:68] layer 5_up @ epoch 1 new loss 0.0002079920086544007 old loss 0.00021002440189477056 BETTER
I0311 14:24:56.116991 1553244 finetune.py:68] layer 7_o @ epoch 2 new loss 0.00024857823154889047 old loss 0.00025323560112155974 BETTER
I0311 14:24:59.895845 1552465 finetune.py:68] layer 6_o @ epoch 4 new loss 0.0001773124822648242 old loss 0.00017939104873221368 BETTER
I0311 14:25:11.435513 1550761 finetune.py:68] layer 4_up @ epoch 3 new loss 0.00012771565525326878 old loss 0.00012853179941885173 BETTER
I0311 14:25:15.116173 1552465 finetune.py:45] layer 6_up initial loss 0.00031729534384794533
I0311 14:25:19.423950 1551641 finetune.py:68] layer 5_up @ epoch 2 new loss 0.00020630218205042183 old loss 0.0002079920086544007 BETTER
I0311 14:25:27.232781 1553244 finetune.py:68] layer 7_o @ epoch 3 new loss 0.00024486149777658284 old loss 0.00024857823154889047 BETTER
I0311 14:25:43.638202 1550761 finetune.py:68] layer 4_up @ epoch 4 new loss 0.00012697746569756418 old loss 0.00012771565525326878 BETTER
I0311 14:25:44.760899 1552465 finetune.py:68] layer 6_up @ epoch 0 new loss 0.0003123259521089494 old loss 0.00031729534384794533 BETTER
I0311 14:25:49.663800 1551641 finetune.py:68] layer 5_up @ epoch 3 new loss 0.00020479357044678181 old loss 0.00020630218205042183 BETTER
I0311 14:25:58.240730 1553244 finetune.py:68] layer 7_o @ epoch 4 new loss 0.0002416973002254963 old loss 0.00024486149777658284 BETTER
I0311 14:25:59.235333 1550761 finetune.py:45] layer 4_gate initial loss 0.00016619812231510878
I0311 14:26:13.225687 1553244 finetune.py:45] layer 7_up initial loss 0.00043088538222946227
I0311 14:26:14.738768 1552465 finetune.py:68] layer 6_up @ epoch 1 new loss 0.0003090934478677809 old loss 0.0003123259521089494 BETTER
I0311 14:26:19.699624 1551641 finetune.py:68] layer 5_up @ epoch 4 new loss 0.00020344210497569293 old loss 0.00020479357044678181 BETTER
I0311 14:26:28.442827 1550761 finetune.py:68] layer 4_gate @ epoch 0 new loss 0.00016493596194777638 old loss 0.00016619812231510878 BETTER
I0311 14:26:36.010680 1551641 finetune.py:45] layer 5_gate initial loss 0.00026219821302220225
I0311 14:26:45.456930 1553244 finetune.py:68] layer 7_up @ epoch 0 new loss 0.00042298203334212303 old loss 0.00043088538222946227 BETTER
I0311 14:26:47.971854 1552465 finetune.py:68] layer 6_up @ epoch 2 new loss 0.00030641868943348527 old loss 0.0003090934478677809 BETTER
I0311 14:27:00.645641 1550761 finetune.py:68] layer 4_gate @ epoch 1 new loss 0.0001639278925722465 old loss 0.00016493596194777638 BETTER
I0311 14:27:05.473103 1551641 finetune.py:68] layer 5_gate @ epoch 0 new loss 0.00026013157912530005 old loss 0.00026219821302220225 BETTER
I0311 14:27:16.565514 1553244 finetune.py:68] layer 7_up @ epoch 1 new loss 0.00041805923683568835 old loss 0.00042298203334212303 BETTER
I0311 14:27:20.241592 1552465 finetune.py:68] layer 6_up @ epoch 3 new loss 0.0003040457086171955 old loss 0.00030641868943348527 BETTER
I0311 14:27:33.239845 1550761 finetune.py:68] layer 4_gate @ epoch 2 new loss 0.00016305242024827749 old loss 0.0001639278925722465 BETTER
I0311 14:27:35.896185 1551641 finetune.py:68] layer 5_gate @ epoch 1 new loss 0.00025852644466795027 old loss 0.00026013157912530005 BETTER
I0311 14:27:48.132428 1553244 finetune.py:68] layer 7_up @ epoch 2 new loss 0.000413994217524305 old loss 0.00041805923683568835 BETTER
I0311 14:27:52.773961 1552465 finetune.py:68] layer 6_up @ epoch 4 new loss 0.00030191525002010167 old loss 0.0003040457086171955 BETTER
I0311 14:28:05.525098 1550761 finetune.py:68] layer 4_gate @ epoch 3 new loss 0.00016226305160671473 old loss 0.00016305242024827749 BETTER
I0311 14:28:05.935701 1551641 finetune.py:68] layer 5_gate @ epoch 2 new loss 0.0002571352815721184 old loss 0.00025852644466795027 BETTER
I0311 14:28:12.964825 1552465 finetune.py:45] layer 6_gate initial loss 0.00038402617792598903
I0311 14:28:19.156527 1553244 finetune.py:68] layer 7_up @ epoch 3 new loss 0.0004104433464817703 old loss 0.000413994217524305 BETTER
I0311 14:28:34.655159 1551641 finetune.py:68] layer 5_gate @ epoch 3 new loss 0.00025585113326087594 old loss 0.0002571352815721184 BETTER
I0311 14:28:35.745339 1550761 finetune.py:68] layer 4_gate @ epoch 4 new loss 0.0001615389483049512 old loss 0.00016226305160671473 BETTER
I0311 14:28:40.563263 1552465 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.0003807597968261689 old loss 0.00038402617792598903 BETTER
I0311 14:28:50.688329 1553244 finetune.py:68] layer 7_up @ epoch 4 new loss 0.0004072537994943559 old loss 0.0004104433464817703 BETTER
I0311 14:28:54.405699 1550761 finetune.py:45] layer 4_down initial loss 0.0002528518671169877
I0311 14:29:04.577363 1551641 finetune.py:68] layer 5_gate @ epoch 4 new loss 0.00025466311490163207 old loss 0.00025585113326087594 BETTER
I0311 14:29:10.074612 1552465 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.00037835657713003457 old loss 0.0003807597968261689 BETTER
I0311 14:29:10.998525 1553244 finetune.py:45] layer 7_gate initial loss 0.0005166867631487548
I0311 14:29:23.305490 1550761 finetune.py:68] layer 4_down @ epoch 0 new loss 0.000252518366323784 old loss 0.0002528518671169877 BETTER
I0311 14:29:25.291940 1551641 finetune.py:45] layer 5_down initial loss 0.00039013216155581176
I0311 14:29:39.622619 1553244 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.0005120061105117202 old loss 0.0005166867631487548 BETTER
I0311 14:29:40.642321 1552465 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.000376241747289896 old loss 0.00037835657713003457 BETTER
I0311 14:29:52.910426 1551641 finetune.py:68] layer 5_down @ epoch 0 new loss 0.00038976219366304576 old loss 0.00039013216155581176 BETTER
I0311 14:29:53.863489 1550761 finetune.py:68] layer 4_down @ epoch 1 new loss 0.00025223122793249786 old loss 0.000252518366323784 BETTER
I0311 14:30:09.974173 1553244 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.0005085091106593609 old loss 0.0005120061105117202 BETTER
I0311 14:30:11.685364 1552465 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.0003743186825886369 old loss 0.000376241747289896 BETTER
I0311 14:30:21.870256 1551641 finetune.py:68] layer 5_down @ epoch 1 new loss 0.00038943643448874354 old loss 0.00038976219366304576 BETTER
I0311 14:30:24.361500 1550761 finetune.py:68] layer 4_down @ epoch 2 new loss 0.0002519836707506329 old loss 0.00025223122793249786 BETTER
I0311 14:30:39.553713 1553244 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.0005054384819231927 old loss 0.0005085091106593609 BETTER
I0311 14:30:42.197463 1552465 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.00037253592745400965 old loss 0.0003743186825886369 BETTER
I0311 14:30:50.354149 1551641 finetune.py:68] layer 5_down @ epoch 2 new loss 0.0003891479573212564 old loss 0.00038943643448874354 BETTER
I0311 14:30:54.202961 1550761 finetune.py:68] layer 4_down @ epoch 3 new loss 0.0002517657121643424 old loss 0.0002519836707506329 BETTER
I0311 14:31:03.297062 1552465 finetune.py:45] layer 6_down initial loss 0.0005741051863878965
I0311 14:31:09.060210 1553244 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.0005026796134188771 old loss 0.0005054384819231927 BETTER
I0311 14:31:18.212297 1551641 finetune.py:68] layer 5_down @ epoch 3 new loss 0.00038889344432391226 old loss 0.0003891479573212564 BETTER
I0311 14:31:23.328350 1550761 finetune.py:68] layer 4_down @ epoch 4 new loss 0.0002515759551897645 old loss 0.0002517657121643424 BETTER
4_v proxy err 0.040983956307172775 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0019199256785213947 tr(WHW.T) 6921.12255859375
4_k proxy err 0.0012984536588191986 tr(WHW.T) 10432.1162109375
4_o proxy err 0.036423832178115845 tr(WHW.T) 5.165080547332764
4_up proxy err 0.03412218391895294 tr(WHW.T) 397.73974609375
4_gate proxy err 0.017212621867656708 tr(WHW.T) 818.2290649414062
4_down proxy err 0.03392038866877556 tr(WHW.T) 11.74367904663086
I0311 14:31:29.415033 1552465 finetune.py:68] layer 6_down @ epoch 0 new loss 0.0005736214225180447 old loss 0.0005741051863878965 BETTER
I0311 14:31:37.853229 1553244 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.000500156544148922 old loss 0.0005026796134188771 BETTER
I0311 14:31:45.734693 1551641 finetune.py:68] layer 5_down @ epoch 4 new loss 0.00038866978138685226 old loss 0.00038889344432391226 BETTER
5_v proxy err 0.042242348194122314 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0022029478568583727 tr(WHW.T) 6775.00830078125
5_k proxy err 0.0014106185408309102 tr(WHW.T) 10861.1201171875
5_o proxy err 0.040858540683984756 tr(WHW.T) 8.017535209655762
5_up proxy err 0.03371672332286835 tr(WHW.T) 506.7348937988281
5_gate proxy err 0.016145609319210052 tr(WHW.T) 1100.5321044921875
5_down proxy err 0.03695795312523842 tr(WHW.T) 15.882037162780762
I0311 14:31:52.967620 1553244 finetune.py:45] layer 7_down initial loss 0.0007707990007475019
I0311 14:31:57.220099 1552465 finetune.py:68] layer 6_down @ epoch 1 new loss 0.0005731923156417906 old loss 0.0005736214225180447 BETTER
I0311 14:32:18.402000 1553244 finetune.py:68] layer 7_down @ epoch 0 new loss 0.0007702031871303916 old loss 0.0007707990007475019 BETTER
I0311 14:32:24.456413 1552465 finetune.py:68] layer 6_down @ epoch 2 new loss 0.000572813383769244 old loss 0.0005731923156417906 BETTER
I0311 14:32:44.474323 1553244 finetune.py:68] layer 7_down @ epoch 1 new loss 0.0007696817046962678 old loss 0.0007702031871303916 BETTER
I0311 14:32:51.508678 1552465 finetune.py:68] layer 6_down @ epoch 3 new loss 0.0005724784568883479 old loss 0.000572813383769244 BETTER
I0311 14:33:02.224920 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 8 in 71.90961956977844s
I0311 14:33:05.498784 1563227 config.py:54] PyTorch version 2.1.1 available.
I0311 14:33:06.522386 1536994 quantize_finetune_llama.py:183] layer 9 gpu 1
I0311 14:33:06.589432 1563227 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:33:10.881393 1553244 finetune.py:68] layer 7_down @ epoch 2 new loss 0.0007692220387980342 old loss 0.0007696817046962678 BETTER
I0311 14:33:16.499900 1563227 finetune.py:45] layer 8_v initial loss 0.00024923167075030506
I0311 14:33:19.382946 1552465 finetune.py:68] layer 6_down @ epoch 4 new loss 0.0005721820052713156 old loss 0.0005724784568883479 BETTER
6_v proxy err 0.04513479769229889 tr(WHW.T) 443.5464782714844
6_q proxy err 0.0031223823316395283 tr(WHW.T) 7580.2275390625
6_k proxy err 0.0023019411601126194 tr(WHW.T) 10433.2880859375
6_o proxy err 0.046061549335718155 tr(WHW.T) 11.718099594116211
6_up proxy err 0.03362122178077698 tr(WHW.T) 616.49951171875
6_gate proxy err 0.014018029905855656 tr(WHW.T) 1550.1190185546875
6_down proxy err 0.037756066769361496 tr(WHW.T) 23.322439193725586
I0311 14:33:37.560208 1553244 finetune.py:68] layer 7_down @ epoch 3 new loss 0.0007688112091273069 old loss 0.0007692220387980342 BETTER
I0311 14:33:49.843255 1563227 finetune.py:68] layer 8_v @ epoch 0 new loss 0.00015788775635883212 old loss 0.00024923167075030506 BETTER
I0311 14:34:04.011946 1553244 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0007684502052143216 old loss 0.0007688112091273069 BETTER
7_v proxy err 0.044920701533555984 tr(WHW.T) 489.9357604980469
7_q proxy err 0.0033742496743798256 tr(WHW.T) 7679.64453125
7_k proxy err 0.002562527311965823 tr(WHW.T) 10234.0390625
7_o proxy err 0.050910212099552155 tr(WHW.T) 15.365532875061035
7_up proxy err 0.032510906457901 tr(WHW.T) 735.1640625
7_gate proxy err 0.013371803797781467 tr(WHW.T) 1870.893798828125
7_down proxy err 0.03821277618408203 tr(WHW.T) 31.04166603088379
I0311 14:34:24.436151 1563227 finetune.py:68] layer 8_v @ epoch 1 new loss 0.00014222868776414543 old loss 0.00015788775635883212 BETTER
I0311 14:34:34.745221 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 9 in 70.41727614402771s
I0311 14:34:37.719132 1564168 config.py:54] PyTorch version 2.1.1 available.
I0311 14:34:38.805884 1536994 quantize_finetune_llama.py:183] layer 10 gpu 2
I0311 14:34:38.873764 1564168 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:34:46.931891 1564168 finetune.py:45] layer 9_v initial loss 0.00022670417092740536
I0311 14:34:59.353288 1563227 finetune.py:68] layer 8_v @ epoch 2 new loss 0.00013497080362867564 old loss 0.00014222868776414543 BETTER
I0311 14:35:18.267658 1564168 finetune.py:68] layer 9_v @ epoch 0 new loss 0.00018266464758198708 old loss 0.00022670417092740536 BETTER
I0311 14:35:34.459348 1563227 finetune.py:68] layer 8_v @ epoch 3 new loss 0.00013020489132031798 old loss 0.00013497080362867564 BETTER
I0311 14:35:48.004160 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 10 in 68.8109405040741s
I0311 14:35:50.602713 1564168 finetune.py:68] layer 9_v @ epoch 1 new loss 0.00017139861301984638 old loss 0.00018266464758198708 BETTER
I0311 14:35:51.218167 1564938 config.py:54] PyTorch version 2.1.1 available.
I0311 14:35:52.219604 1536994 quantize_finetune_llama.py:183] layer 11 gpu 3
I0311 14:35:52.286904 1564938 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:36:00.392596 1564938 finetune.py:45] layer 10_v initial loss 0.0003305099962744862
I0311 14:36:09.530361 1563227 finetune.py:68] layer 8_v @ epoch 4 new loss 0.00012665307440329343 old loss 0.00013020489132031798 BETTER
I0311 14:36:18.961143 1563227 finetune.py:45] layer 8_q initial loss 0.00015796255320310593
I0311 14:36:22.991778 1564168 finetune.py:68] layer 9_v @ epoch 2 new loss 0.0001647275930736214 old loss 0.00017139861301984638 BETTER
I0311 14:36:32.006458 1564938 finetune.py:68] layer 10_v @ epoch 0 new loss 0.00026493537006899714 old loss 0.0003305099962744862 BETTER
I0311 14:36:52.892922 1563227 finetune.py:68] layer 8_q @ epoch 0 new loss 0.00014838263450656086 old loss 0.00015796255320310593 BETTER
I0311 14:36:55.781886 1564168 finetune.py:68] layer 9_v @ epoch 3 new loss 0.00015992371481843293 old loss 0.0001647275930736214 BETTER
I0311 14:37:01.891719 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 11 in 69.26910591125488s
I0311 14:37:04.488984 1564938 finetune.py:68] layer 10_v @ epoch 1 new loss 0.000247857125941664 old loss 0.00026493537006899714 BETTER
I0311 14:37:05.209194 1565707 config.py:54] PyTorch version 2.1.1 available.
I0311 14:37:06.229966 1536994 quantize_finetune_llama.py:183] layer 12 gpu 0
I0311 14:37:06.295495 1565707 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:37:17.042630 1565707 finetune.py:45] layer 11_v initial loss 0.00033467731554992497
I0311 14:37:29.192744 1563227 finetune.py:68] layer 8_q @ epoch 1 new loss 0.00014433858450502157 old loss 0.00014838263450656086 BETTER
I0311 14:37:30.270741 1564168 finetune.py:68] layer 9_v @ epoch 4 new loss 0.0001561813842272386 old loss 0.00015992371481843293 BETTER
I0311 14:37:37.000622 1564938 finetune.py:68] layer 10_v @ epoch 2 new loss 0.00023733582929708064 old loss 0.000247857125941664 BETTER
I0311 14:37:41.997013 1564168 finetune.py:45] layer 9_q initial loss 0.00019521042122505605
I0311 14:37:49.128338 1565707 finetune.py:68] layer 11_v @ epoch 0 new loss 0.000270200107479468 old loss 0.00033467731554992497 BETTER
I0311 14:38:04.394015 1563227 finetune.py:68] layer 8_q @ epoch 2 new loss 0.00014129253395367414 old loss 0.00014433858450502157 BETTER
I0311 14:38:11.001616 1564938 finetune.py:68] layer 10_v @ epoch 3 new loss 0.00022966587857808918 old loss 0.00023733582929708064 BETTER
I0311 14:38:13.737236 1564168 finetune.py:68] layer 9_q @ epoch 0 new loss 0.0001838664902606979 old loss 0.00019521042122505605 BETTER
I0311 14:38:21.417554 1565707 finetune.py:68] layer 11_v @ epoch 1 new loss 0.0002532681974116713 old loss 0.000270200107479468 BETTER
I0311 14:38:38.908426 1563227 finetune.py:68] layer 8_q @ epoch 3 new loss 0.00013877995661459863 old loss 0.00014129253395367414 BETTER
I0311 14:38:44.238307 1564938 finetune.py:68] layer 10_v @ epoch 4 new loss 0.0002236534346593544 old loss 0.00022966587857808918 BETTER
I0311 14:38:45.967621 1564168 finetune.py:68] layer 9_q @ epoch 1 new loss 0.0001792001276044175 old loss 0.0001838664902606979 BETTER
I0311 14:38:53.567048 1564938 finetune.py:45] layer 10_q initial loss 0.0002710901899263263
I0311 14:38:53.817742 1565707 finetune.py:68] layer 11_v @ epoch 2 new loss 0.00024292666057590395 old loss 0.0002532681974116713 BETTER
I0311 14:39:13.756683 1563227 finetune.py:68] layer 8_q @ epoch 4 new loss 0.00013666617451235652 old loss 0.00013877995661459863 BETTER
I0311 14:39:18.362556 1564168 finetune.py:68] layer 9_q @ epoch 2 new loss 0.00017566124734003097 old loss 0.0001792001276044175 BETTER
I0311 14:39:23.702953 1563227 finetune.py:45] layer 8_k initial loss 0.00015870943025220186
I0311 14:39:25.650291 1564938 finetune.py:68] layer 10_q @ epoch 0 new loss 0.0002548223128542304 old loss 0.0002710901899263263 BETTER
I0311 14:39:26.528191 1565707 finetune.py:68] layer 11_v @ epoch 3 new loss 0.00023554016661364585 old loss 0.00024292666057590395 BETTER
I0311 14:39:50.800584 1564168 finetune.py:68] layer 9_q @ epoch 3 new loss 0.00017277838196605444 old loss 0.00017566124734003097 BETTER
I0311 14:39:57.451168 1563227 finetune.py:68] layer 8_k @ epoch 0 new loss 0.00015450315549969673 old loss 0.00015870943025220186 BETTER
I0311 14:39:58.877549 1564938 finetune.py:68] layer 10_q @ epoch 1 new loss 0.00024822205887176096 old loss 0.0002548223128542304 BETTER
I0311 14:39:59.757513 1565707 finetune.py:68] layer 11_v @ epoch 4 new loss 0.00022982567315921187 old loss 0.00023554016661364585 BETTER
I0311 14:40:09.314476 1565707 finetune.py:45] layer 11_q initial loss 0.00028182900859974325
I0311 14:40:23.397162 1564168 finetune.py:68] layer 9_q @ epoch 4 new loss 0.0001702512672636658 old loss 0.00017277838196605444 BETTER
I0311 14:40:31.839690 1564938 finetune.py:68] layer 10_q @ epoch 2 new loss 0.00024309521540999413 old loss 0.00024822205887176096 BETTER
I0311 14:40:31.863674 1563227 finetune.py:68] layer 8_k @ epoch 1 new loss 0.00015241741493809968 old loss 0.00015450315549969673 BETTER
I0311 14:40:33.405947 1564168 finetune.py:45] layer 9_k initial loss 0.00019882970082107931
I0311 14:40:41.066199 1565707 finetune.py:68] layer 11_q @ epoch 0 new loss 0.00026656617410480976 old loss 0.00028182900859974325 BETTER
I0311 14:41:05.228292 1564938 finetune.py:68] layer 10_q @ epoch 3 new loss 0.00023878384672570974 old loss 0.00024309521540999413 BETTER
I0311 14:41:05.391676 1564168 finetune.py:68] layer 9_k @ epoch 0 new loss 0.0001933276653289795 old loss 0.00019882970082107931 BETTER
I0311 14:41:06.296258 1563227 finetune.py:68] layer 8_k @ epoch 2 new loss 0.0001506620174041018 old loss 0.00015241741493809968 BETTER
I0311 14:41:13.459332 1565707 finetune.py:68] layer 11_q @ epoch 1 new loss 0.0002600566658657044 old loss 0.00026656617410480976 BETTER
I0311 14:41:38.238763 1564168 finetune.py:68] layer 9_k @ epoch 1 new loss 0.00019072092254646122 old loss 0.0001933276653289795 BETTER
I0311 14:41:38.547882 1564938 finetune.py:68] layer 10_q @ epoch 4 new loss 0.00023508307640440762 old loss 0.00023878384672570974 BETTER
I0311 14:41:40.519372 1563227 finetune.py:68] layer 8_k @ epoch 3 new loss 0.00014915694191586226 old loss 0.0001506620174041018 BETTER
I0311 14:41:46.003895 1565707 finetune.py:68] layer 11_q @ epoch 2 new loss 0.0002551354991737753 old loss 0.0002600566658657044 BETTER
I0311 14:41:48.380522 1564938 finetune.py:45] layer 10_k initial loss 0.0002732666616793722
I0311 14:42:10.412173 1564168 finetune.py:68] layer 9_k @ epoch 2 new loss 0.00018855811504181474 old loss 0.00019072092254646122 BETTER
I0311 14:42:14.458254 1563227 finetune.py:68] layer 8_k @ epoch 4 new loss 0.00014782819198444486 old loss 0.00014915694191586226 BETTER
I0311 14:42:18.572421 1565707 finetune.py:68] layer 11_q @ epoch 3 new loss 0.0002510564518161118 old loss 0.0002551354991737753 BETTER
I0311 14:42:19.898142 1564938 finetune.py:68] layer 10_k @ epoch 0 new loss 0.00026223412714898586 old loss 0.0002732666616793722 BETTER
I0311 14:42:24.621881 1563227 finetune.py:45] layer 8_o initial loss 0.000394378206692636
I0311 14:42:42.514817 1564168 finetune.py:68] layer 9_k @ epoch 3 new loss 0.0001866780803538859 old loss 0.00018855811504181474 BETTER
I0311 14:42:51.462804 1565707 finetune.py:68] layer 11_q @ epoch 4 new loss 0.0002475723158568144 old loss 0.0002510564518161118 BETTER
I0311 14:42:52.184810 1564938 finetune.py:68] layer 10_k @ epoch 1 new loss 0.00025838936562649906 old loss 0.00026223412714898586 BETTER
I0311 14:42:57.311760 1563227 finetune.py:68] layer 8_o @ epoch 0 new loss 0.00037366198375821114 old loss 0.000394378206692636 BETTER
I0311 14:43:00.902137 1565707 finetune.py:45] layer 11_k initial loss 0.0002863561094272882
I0311 14:43:14.783514 1564168 finetune.py:68] layer 9_k @ epoch 4 new loss 0.00018502770399209112 old loss 0.0001866780803538859 BETTER
I0311 14:43:24.409997 1564938 finetune.py:68] layer 10_k @ epoch 2 new loss 0.0002552323858253658 old loss 0.00025838936562649906 BETTER
I0311 14:43:25.332129 1564168 finetune.py:45] layer 9_o initial loss 0.0005007997388020158
I0311 14:43:30.812657 1563227 finetune.py:68] layer 8_o @ epoch 1 new loss 0.0003635999746620655 old loss 0.00037366198375821114 BETTER
I0311 14:43:32.465827 1565707 finetune.py:68] layer 11_k @ epoch 0 new loss 0.0002796178450807929 old loss 0.0002863561094272882 BETTER
I0311 14:43:56.146076 1564168 finetune.py:68] layer 9_o @ epoch 0 new loss 0.00047688366612419486 old loss 0.0005007997388020158 BETTER
I0311 14:43:56.853200 1564938 finetune.py:68] layer 10_k @ epoch 3 new loss 0.0002524697338230908 old loss 0.0002552323858253658 BETTER
I0311 14:44:04.389725 1563227 finetune.py:68] layer 8_o @ epoch 2 new loss 0.00035629357444122434 old loss 0.0003635999746620655 BETTER
I0311 14:44:04.503364 1565707 finetune.py:68] layer 11_k @ epoch 1 new loss 0.0002760510833468288 old loss 0.0002796178450807929 BETTER
I0311 14:44:27.915570 1564168 finetune.py:68] layer 9_o @ epoch 1 new loss 0.0004643596475943923 old loss 0.00047688366612419486 BETTER
I0311 14:44:29.379827 1564938 finetune.py:68] layer 10_k @ epoch 4 new loss 0.0002500017290003598 old loss 0.0002524697338230908 BETTER
I0311 14:44:36.701388 1565707 finetune.py:68] layer 11_k @ epoch 2 new loss 0.00027307102573104203 old loss 0.0002760510833468288 BETTER
I0311 14:44:37.880900 1563227 finetune.py:68] layer 8_o @ epoch 3 new loss 0.0003504485066514462 old loss 0.00035629357444122434 BETTER
I0311 14:44:39.069432 1564938 finetune.py:45] layer 10_o initial loss 0.000695542199537158
I0311 14:44:59.668441 1564168 finetune.py:68] layer 9_o @ epoch 2 new loss 0.0004550919693429023 old loss 0.0004643596475943923 BETTER
I0311 14:45:09.518836 1565707 finetune.py:68] layer 11_k @ epoch 3 new loss 0.0002704815997276455 old loss 0.00027307102573104203 BETTER
I0311 14:45:10.545709 1564938 finetune.py:68] layer 10_o @ epoch 0 new loss 0.0006618489278480411 old loss 0.000695542199537158 BETTER
I0311 14:45:11.811133 1563227 finetune.py:68] layer 8_o @ epoch 4 new loss 0.000345569133060053 old loss 0.0003504485066514462 BETTER
I0311 14:45:27.388294 1563227 finetune.py:45] layer 8_up initial loss 0.0005699531175196171
I0311 14:45:31.201771 1564168 finetune.py:68] layer 9_o @ epoch 3 new loss 0.0004475967725738883 old loss 0.0004550919693429023 BETTER
I0311 14:45:42.291026 1565707 finetune.py:68] layer 11_k @ epoch 4 new loss 0.0002681539626792073 old loss 0.0002704815997276455 BETTER
I0311 14:45:42.416759 1564938 finetune.py:68] layer 10_o @ epoch 1 new loss 0.0006436561816371977 old loss 0.0006618489278480411 BETTER
I0311 14:45:51.429240 1565707 finetune.py:45] layer 11_o initial loss 0.0007171679171733558
I0311 14:45:58.325704 1563227 finetune.py:68] layer 8_up @ epoch 0 new loss 0.0005593090318143368 old loss 0.0005699531175196171 BETTER
I0311 14:46:02.806279 1564168 finetune.py:68] layer 9_o @ epoch 4 new loss 0.0004412972484715283 old loss 0.0004475967725738883 BETTER
I0311 14:46:14.201082 1564938 finetune.py:68] layer 10_o @ epoch 2 new loss 0.0006299627711996436 old loss 0.0006436561816371977 BETTER
I0311 14:46:18.163320 1564168 finetune.py:45] layer 9_up initial loss 0.0007009116816334426
I0311 14:46:22.262555 1565707 finetune.py:68] layer 11_o @ epoch 0 new loss 0.000684326165355742 old loss 0.0007171679171733558 BETTER
I0311 14:46:30.347733 1563227 finetune.py:68] layer 8_up @ epoch 1 new loss 0.0005524875596165657 old loss 0.0005593090318143368 BETTER
I0311 14:46:46.113893 1564938 finetune.py:68] layer 10_o @ epoch 3 new loss 0.0006187024409882724 old loss 0.0006299627711996436 BETTER
I0311 14:46:47.236979 1564168 finetune.py:68] layer 9_up @ epoch 0 new loss 0.0006880137952975929 old loss 0.0007009116816334426 BETTER
I0311 14:46:53.831787 1565707 finetune.py:68] layer 11_o @ epoch 1 new loss 0.0006670031580142677 old loss 0.000684326165355742 BETTER
I0311 14:47:02.291375 1563227 finetune.py:68] layer 8_up @ epoch 2 new loss 0.0005468425806611776 old loss 0.0005524875596165657 BETTER
I0311 14:47:17.000440 1564168 finetune.py:68] layer 9_up @ epoch 1 new loss 0.0006797548849135637 old loss 0.0006880137952975929 BETTER
I0311 14:47:18.114345 1564938 finetune.py:68] layer 10_o @ epoch 4 new loss 0.0006092849071137607 old loss 0.0006187024409882724 BETTER
I0311 14:47:25.213726 1565707 finetune.py:68] layer 11_o @ epoch 2 new loss 0.0006540039903484285 old loss 0.0006670031580142677 BETTER
I0311 14:47:33.451163 1564938 finetune.py:45] layer 10_up initial loss 0.0009048468200489879
I0311 14:47:34.318298 1563227 finetune.py:68] layer 8_up @ epoch 3 new loss 0.0005419153021648526 old loss 0.0005468425806611776 BETTER
I0311 14:47:46.817746 1564168 finetune.py:68] layer 9_up @ epoch 2 new loss 0.0006727844593115151 old loss 0.0006797548849135637 BETTER
I0311 14:47:56.778169 1565707 finetune.py:68] layer 11_o @ epoch 3 new loss 0.0006433774251490831 old loss 0.0006540039903484285 BETTER
I0311 14:48:02.549180 1564938 finetune.py:68] layer 10_up @ epoch 0 new loss 0.0008880887180566788 old loss 0.0009048468200489879 BETTER
I0311 14:48:06.503909 1563227 finetune.py:68] layer 8_up @ epoch 4 new loss 0.000537536630872637 old loss 0.0005419153021648526 BETTER
I0311 14:48:16.769681 1564168 finetune.py:68] layer 9_up @ epoch 3 new loss 0.0006666888948529959 old loss 0.0006727844593115151 BETTER
I0311 14:48:22.065814 1563227 finetune.py:45] layer 8_gate initial loss 0.0006781825213693082
I0311 14:48:28.175554 1565707 finetune.py:68] layer 11_o @ epoch 4 new loss 0.0006345462752506137 old loss 0.0006433774251490831 BETTER
I0311 14:48:32.749569 1564938 finetune.py:68] layer 10_up @ epoch 1 new loss 0.0008771326974965632 old loss 0.0008880887180566788 BETTER
I0311 14:48:43.935483 1565707 finetune.py:45] layer 11_up initial loss 0.0009685785043984652
I0311 14:48:46.676750 1564168 finetune.py:68] layer 9_up @ epoch 4 new loss 0.0006612705765292048 old loss 0.0006666888948529959 BETTER
I0311 14:48:51.349221 1563227 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.0006718260119669139 old loss 0.0006781825213693082 BETTER
I0311 14:49:02.323298 1564168 finetune.py:45] layer 9_gate initial loss 0.0008307162206619978
I0311 14:49:02.888885 1564938 finetune.py:68] layer 10_up @ epoch 2 new loss 0.0008678198792040348 old loss 0.0008771326974965632 BETTER
I0311 14:49:13.054605 1565707 finetune.py:68] layer 11_up @ epoch 0 new loss 0.0009521625470370054 old loss 0.0009685785043984652 BETTER
I0311 14:49:21.290560 1563227 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.0006671337760053575 old loss 0.0006718260119669139 BETTER
I0311 14:49:30.099075 1564168 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0008231420069932938 old loss 0.0008307162206619978 BETTER
I0311 14:49:32.914228 1564938 finetune.py:68] layer 10_up @ epoch 3 new loss 0.0008596935658715665 old loss 0.0008678198792040348 BETTER
I0311 14:49:42.619776 1565707 finetune.py:68] layer 11_up @ epoch 1 new loss 0.0009411571663804352 old loss 0.0009521625470370054 BETTER
I0311 14:49:51.470141 1563227 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.0006631126161664724 old loss 0.0006671337760053575 BETTER
I0311 14:49:58.476741 1564168 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.0008174435934051871 old loss 0.0008231420069932938 BETTER
I0311 14:50:03.030884 1564938 finetune.py:68] layer 10_up @ epoch 4 new loss 0.0008523659780621529 old loss 0.0008596935658715665 BETTER
I0311 14:50:12.369395 1565707 finetune.py:68] layer 11_up @ epoch 2 new loss 0.0009319609380327165 old loss 0.0009411571663804352 BETTER
I0311 14:50:18.240056 1564938 finetune.py:45] layer 10_gate initial loss 0.0010544996475800872
I0311 14:50:21.539360 1563227 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.0006594611331820488 old loss 0.0006631126161664724 BETTER
I0311 14:50:26.808554 1564168 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.0008124824380502105 old loss 0.0008174435934051871 BETTER
I0311 14:50:42.653562 1565707 finetune.py:68] layer 11_up @ epoch 3 new loss 0.0009238585480488837 old loss 0.0009319609380327165 BETTER
I0311 14:50:46.518433 1564938 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.0010447513777762651 old loss 0.0010544996475800872 BETTER
I0311 14:50:52.377795 1563227 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.0006561307236552238 old loss 0.0006594611331820488 BETTER
I0311 14:50:56.109353 1564168 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.0008079948020167649 old loss 0.0008124824380502105 BETTER
I0311 14:51:13.398985 1563227 finetune.py:45] layer 8_down initial loss 0.0009825134184211493
I0311 14:51:16.456265 1565707 finetune.py:68] layer 11_up @ epoch 4 new loss 0.0009166167001239955 old loss 0.0009238585480488837 BETTER
I0311 14:51:17.967873 1564938 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.0010375735582783818 old loss 0.0010447513777762651 BETTER
I0311 14:51:26.081768 1564168 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0008039044332690537 old loss 0.0008079948020167649 BETTER
I0311 14:51:36.547065 1565707 finetune.py:45] layer 11_gate initial loss 0.001145548070780933
I0311 14:51:42.388388 1563227 finetune.py:68] layer 8_down @ epoch 0 new loss 0.0009818279650062323 old loss 0.0009825134184211493 BETTER
I0311 14:51:47.768974 1564168 finetune.py:45] layer 9_down initial loss 0.0011861585080623627
I0311 14:51:48.136798 1564938 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.0010312565136700869 old loss 0.0010375735582783818 BETTER
I0311 14:52:05.651564 1565707 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.001135794329456985 old loss 0.001145548070780933 BETTER
I0311 14:52:13.351406 1563227 finetune.py:68] layer 8_down @ epoch 1 new loss 0.000981218065135181 old loss 0.0009818279650062323 BETTER
I0311 14:52:16.180437 1564168 finetune.py:68] layer 9_down @ epoch 0 new loss 0.0011854004114866257 old loss 0.0011861585080623627 BETTER
I0311 14:52:18.829874 1564938 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.0010254671797156334 old loss 0.0010312565136700869 BETTER
I0311 14:52:34.266024 1565707 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.0011285686632618308 old loss 0.001135794329456985 BETTER
I0311 14:52:41.876998 1563227 finetune.py:68] layer 8_down @ epoch 2 new loss 0.0009806804591789842 old loss 0.000981218065135181 BETTER
I0311 14:52:43.063615 1564168 finetune.py:68] layer 9_down @ epoch 1 new loss 0.001184725435450673 old loss 0.0011854004114866257 BETTER
I0311 14:52:47.318878 1564938 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.0010201465338468552 old loss 0.0010254671797156334 BETTER
I0311 14:53:02.577265 1565707 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.0011221645399928093 old loss 0.0011285686632618308 BETTER
I0311 14:53:02.888109 1564938 finetune.py:45] layer 10_down initial loss 0.0014659257140010595
I0311 14:53:10.016249 1564168 finetune.py:68] layer 9_down @ epoch 2 new loss 0.0011841162340715528 old loss 0.001184725435450673 BETTER
I0311 14:53:10.612954 1563227 finetune.py:68] layer 8_down @ epoch 3 new loss 0.000980200944468379 old loss 0.0009806804591789842 BETTER
I0311 14:53:29.131719 1564938 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0014650224475190043 old loss 0.0014659257140010595 BETTER
I0311 14:53:30.709786 1565707 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.001116331433877349 old loss 0.0011221645399928093 BETTER
I0311 14:53:36.790422 1564168 finetune.py:68] layer 9_down @ epoch 3 new loss 0.0011835757177323103 old loss 0.0011841162340715528 BETTER
I0311 14:53:39.264286 1563227 finetune.py:68] layer 8_down @ epoch 4 new loss 0.000979771139100194 old loss 0.000980200944468379 BETTER
8_v proxy err 0.04084034636616707 tr(WHW.T) 530.9967041015625
8_q proxy err 0.0034765249583870173 tr(WHW.T) 7237.3818359375
8_k proxy err 0.0023906389251351357 tr(WHW.T) 10677.5751953125
8_o proxy err 0.05761590972542763 tr(WHW.T) 20.46475601196289
8_up proxy err 0.029726896435022354 tr(WHW.T) 864.7178955078125
8_gate proxy err 0.013540580868721008 tr(WHW.T) 1965.3179931640625
8_down proxy err 0.03807944431900978 tr(WHW.T) 37.690040588378906
I0311 14:53:56.587635 1564938 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0014642055612057447 old loss 0.0014650224475190043 BETTER
I0311 14:53:59.620641 1565707 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.0011109847109764814 old loss 0.001116331433877349 BETTER
I0311 14:54:04.242604 1564168 finetune.py:68] layer 9_down @ epoch 4 new loss 0.0011830935254693031 old loss 0.0011835757177323103 BETTER
9_v proxy err 0.04044404998421669 tr(WHW.T) 565.0663452148438
9_q proxy err 0.003802116261795163 tr(WHW.T) 6979.27978515625
9_k proxy err 0.0024487164337188005 tr(WHW.T) 11028.33203125
9_o proxy err 0.05904010683298111 tr(WHW.T) 26.124874114990234
9_up proxy err 0.028609419241547585 tr(WHW.T) 969.3399047851562
9_gate proxy err 0.013435935601592064 tr(WHW.T) 2128.3251953125
9_down proxy err 0.0386616550385952 tr(WHW.T) 43.54729080200195
I0311 14:54:15.160169 1565707 finetune.py:45] layer 11_down initial loss 0.0015997468726709485
I0311 14:54:23.509955 1564938 finetune.py:68] layer 10_down @ epoch 2 new loss 0.0014634750550612807 old loss 0.0014642055612057447 BETTER
I0311 14:54:40.886781 1565707 finetune.py:68] layer 11_down @ epoch 0 new loss 0.001598769100382924 old loss 0.0015997468726709485 BETTER
I0311 14:54:50.282627 1564938 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0014628228964284062 old loss 0.0014634750550612807 BETTER
I0311 14:55:07.440520 1565707 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0015978936571627855 old loss 0.001598769100382924 BETTER
I0311 14:55:17.114795 1564938 finetune.py:68] layer 10_down @ epoch 4 new loss 0.0014622242888435721 old loss 0.0014628228964284062 BETTER
10_v proxy err 0.040516629815101624 tr(WHW.T) 578.807373046875
10_q proxy err 0.003922618925571442 tr(WHW.T) 6925.2001953125
10_k proxy err 0.002502951305359602 tr(WHW.T) 11039.5244140625
10_o proxy err 0.06173630431294441 tr(WHW.T) 35.780616760253906
10_up proxy err 0.027007021009922028 tr(WHW.T) 1078.0770263671875
10_gate proxy err 0.013242971152067184 tr(WHW.T) 2258.95361328125
10_down proxy err 0.03693334385752678 tr(WHW.T) 53.09566116333008
I0311 14:55:22.245653 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 12 in 73.24269199371338s
I0311 14:55:25.385202 1575633 config.py:54] PyTorch version 2.1.1 available.
I0311 14:55:26.362168 1536994 quantize_finetune_llama.py:183] layer 13 gpu 1
I0311 14:55:26.428202 1575633 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:55:34.192715 1565707 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0015971125103533268 old loss 0.0015978936571627855 BETTER
I0311 14:55:34.845316 1575633 finetune.py:45] layer 12_v initial loss 0.000348270230460912
I0311 14:56:00.864571 1565707 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0015963994665071368 old loss 0.0015971125103533268 BETTER
I0311 14:56:07.675413 1575633 finetune.py:68] layer 12_v @ epoch 0 new loss 0.00028748810291290283 old loss 0.000348270230460912 BETTER
I0311 14:56:27.455738 1565707 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0015957651194185019 old loss 0.0015963994665071368 BETTER
11_v proxy err 0.040251556783914566 tr(WHW.T) 723.1956176757812
11_q proxy err 0.004654189571738243 tr(WHW.T) 7037.51708984375
11_k proxy err 0.003141202963888645 tr(WHW.T) 10563.1630859375
11_o proxy err 0.06152389943599701 tr(WHW.T) 37.343482971191406
11_up proxy err 0.027691904455423355 tr(WHW.T) 1138.8359375
11_gate proxy err 0.01352313719689846 tr(WHW.T) 2390.7666015625
11_down proxy err 0.03771854564547539 tr(WHW.T) 56.866600036621094
I0311 14:56:36.144740 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 13 in 69.33619332313538s
I0311 14:56:39.333798 1576405 config.py:54] PyTorch version 2.1.1 available.
I0311 14:56:40.396148 1536994 quantize_finetune_llama.py:183] layer 14 gpu 2
I0311 14:56:40.462357 1576405 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 14:56:41.887292 1575633 finetune.py:68] layer 12_v @ epoch 1 new loss 0.0002709942345973104 old loss 0.00028748810291290283 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:56:48.739278 1576405 finetune.py:45] layer 13_v initial loss 0.0003795594675466418
I0311 14:57:16.688172 1575633 finetune.py:68] layer 12_v @ epoch 2 new loss 0.00026085495483130217 old loss 0.0002709942345973104 BETTER
I0311 14:57:20.018894 1576405 finetune.py:68] layer 13_v @ epoch 0 new loss 0.00030997837893664837 old loss 0.0003795594675466418 BETTER
I0311 14:57:50.202983 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 14 in 69.33906197547913s
I0311 14:57:51.572531 1575633 finetune.py:68] layer 12_v @ epoch 3 new loss 0.00025345399626530707 old loss 0.00026085495483130217 BETTER
I0311 14:57:52.429549 1576405 finetune.py:68] layer 13_v @ epoch 1 new loss 0.0002926674787886441 old loss 0.00030997837893664837 BETTER
I0311 14:57:53.362321 1577178 config.py:54] PyTorch version 2.1.1 available.
I0311 14:57:54.385385 1536994 quantize_finetune_llama.py:183] layer 15 gpu 3
I0311 14:57:54.461995 1577178 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:58:03.039936 1577178 finetune.py:45] layer 14_v initial loss 0.0004942230880260468
I0311 14:58:24.781794 1576405 finetune.py:68] layer 13_v @ epoch 2 new loss 0.0002819033106788993 old loss 0.0002926674787886441 BETTER
I0311 14:58:26.363600 1575633 finetune.py:68] layer 12_v @ epoch 4 new loss 0.00024758875952102244 old loss 0.00025345399626530707 BETTER
I0311 14:58:34.494350 1577178 finetune.py:68] layer 14_v @ epoch 0 new loss 0.000409281172323972 old loss 0.0004942230880260468 BETTER
I0311 14:58:35.613866 1575633 finetune.py:45] layer 12_q initial loss 0.00030681019416078925
I0311 14:58:57.312116 1576405 finetune.py:68] layer 13_v @ epoch 3 new loss 0.00027401067200116813 old loss 0.0002819033106788993 BETTER
I0311 14:59:03.737067 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 15 in 68.62611079216003s
I0311 14:59:07.013517 1577178 finetune.py:68] layer 14_v @ epoch 1 new loss 0.0003845857281703502 old loss 0.000409281172323972 BETTER
I0311 14:59:07.124094 1577947 config.py:54] PyTorch version 2.1.1 available.
I0311 14:59:08.187752 1536994 quantize_finetune_llama.py:183] layer 16 gpu 0
I0311 14:59:08.289047 1577947 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 14:59:09.265383 1575633 finetune.py:68] layer 12_q @ epoch 0 new loss 0.0002913748030550778 old loss 0.00030681019416078925 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 14:59:18.128096 1577947 finetune.py:45] layer 15_v initial loss 0.0004942127852700651
I0311 14:59:29.922379 1576405 finetune.py:68] layer 13_v @ epoch 4 new loss 0.0002677149313967675 old loss 0.00027401067200116813 BETTER
I0311 14:59:39.305238 1576405 finetune.py:45] layer 13_q initial loss 0.0003299752133898437
I0311 14:59:39.884549 1577178 finetune.py:68] layer 14_v @ epoch 2 new loss 0.0003690082812681794 old loss 0.0003845857281703502 BETTER
I0311 14:59:43.576096 1575633 finetune.py:68] layer 12_q @ epoch 1 new loss 0.00028447137447074056 old loss 0.0002913748030550778 BETTER
I0311 14:59:49.157574 1577947 finetune.py:68] layer 15_v @ epoch 0 new loss 0.00040290202014148235 old loss 0.0004942127852700651 BETTER
I0311 15:00:10.909057 1576405 finetune.py:68] layer 13_q @ epoch 0 new loss 0.00031232443870976567 old loss 0.0003299752133898437 BETTER
I0311 15:00:12.840132 1577178 finetune.py:68] layer 14_v @ epoch 3 new loss 0.0003575152950361371 old loss 0.0003690082812681794 BETTER
I0311 15:00:17.886139 1575633 finetune.py:68] layer 12_q @ epoch 2 new loss 0.0002791364095173776 old loss 0.00028447137447074056 BETTER
I0311 15:00:21.334954 1577947 finetune.py:68] layer 15_v @ epoch 1 new loss 0.000379784032702446 old loss 0.00040290202014148235 BETTER
I0311 15:00:43.208612 1576405 finetune.py:68] layer 13_q @ epoch 1 new loss 0.00030485406750813127 old loss 0.00031232443870976567 BETTER
I0311 15:00:46.092558 1577178 finetune.py:68] layer 14_v @ epoch 4 new loss 0.00034834633697755635 old loss 0.0003575152950361371 BETTER
I0311 15:00:52.573551 1575633 finetune.py:68] layer 12_q @ epoch 3 new loss 0.0002747610560618341 old loss 0.0002791364095173776 BETTER
I0311 15:00:53.920097 1577947 finetune.py:68] layer 15_v @ epoch 2 new loss 0.0003650457947514951 old loss 0.000379784032702446 BETTER
I0311 15:00:56.461041 1577178 finetune.py:45] layer 14_q initial loss 0.0004210224433336407
I0311 15:01:15.404049 1576405 finetune.py:68] layer 13_q @ epoch 2 new loss 0.0002991085348185152 old loss 0.00030485406750813127 BETTER
I0311 15:01:26.788552 1577947 finetune.py:68] layer 15_v @ epoch 3 new loss 0.00035391226992942393 old loss 0.0003650457947514951 BETTER
I0311 15:01:27.237690 1575633 finetune.py:68] layer 12_q @ epoch 4 new loss 0.00027094382676295936 old loss 0.0002747610560618341 BETTER
I0311 15:01:28.620126 1577178 finetune.py:68] layer 14_q @ epoch 0 new loss 0.0003989887482021004 old loss 0.0004210224433336407 BETTER
I0311 15:01:36.790247 1575633 finetune.py:45] layer 12_k initial loss 0.0003119361062999815
I0311 15:01:47.680691 1576405 finetune.py:68] layer 13_q @ epoch 3 new loss 0.0002942734572570771 old loss 0.0002991085348185152 BETTER
I0311 15:01:59.448949 1577947 finetune.py:68] layer 15_v @ epoch 4 new loss 0.00034511025296524167 old loss 0.00035391226992942393 BETTER
I0311 15:02:01.409218 1577178 finetune.py:68] layer 14_q @ epoch 1 new loss 0.0003888216451741755 old loss 0.0003989887482021004 BETTER
I0311 15:02:09.690243 1577947 finetune.py:45] layer 15_q initial loss 0.00042649699025787413
I0311 15:02:10.040725 1575633 finetune.py:68] layer 12_k @ epoch 0 new loss 0.000304811546811834 old loss 0.0003119361062999815 BETTER
I0311 15:02:20.249619 1576405 finetune.py:68] layer 13_q @ epoch 4 new loss 0.00029019135399721563 old loss 0.0002942734572570771 BETTER
I0311 15:02:29.185475 1576405 finetune.py:45] layer 13_k initial loss 0.00033461907878518105
I0311 15:02:33.980559 1577178 finetune.py:68] layer 14_q @ epoch 2 new loss 0.0003808572655543685 old loss 0.0003888216451741755 BETTER
I0311 15:02:41.238909 1577947 finetune.py:68] layer 15_q @ epoch 0 new loss 0.00040281875408254564 old loss 0.00042649699025787413 BETTER
I0311 15:02:43.629346 1575633 finetune.py:68] layer 12_k @ epoch 1 new loss 0.00030114149558357894 old loss 0.000304811546811834 BETTER
I0311 15:03:00.558450 1576405 finetune.py:68] layer 13_k @ epoch 0 new loss 0.00032682844903320074 old loss 0.00033461907878518105 BETTER
I0311 15:03:06.630323 1577178 finetune.py:68] layer 14_q @ epoch 3 new loss 0.0003742361441254616 old loss 0.0003808572655543685 BETTER
I0311 15:03:13.495314 1577947 finetune.py:68] layer 15_q @ epoch 1 new loss 0.0003923917538486421 old loss 0.00040281875408254564 BETTER
I0311 15:03:17.420145 1575633 finetune.py:68] layer 12_k @ epoch 2 new loss 0.00029804027872160077 old loss 0.00030114149558357894 BETTER
I0311 15:03:32.619493 1576405 finetune.py:68] layer 13_k @ epoch 1 new loss 0.0003228411078453064 old loss 0.00032682844903320074 BETTER
I0311 15:03:39.339786 1577178 finetune.py:68] layer 14_q @ epoch 4 new loss 0.0003685523697640747 old loss 0.0003742361441254616 BETTER
I0311 15:03:45.827483 1577947 finetune.py:68] layer 15_q @ epoch 2 new loss 0.00038419084739871323 old loss 0.0003923917538486421 BETTER
I0311 15:03:48.888544 1577178 finetune.py:45] layer 14_k initial loss 0.0004224047006573528
I0311 15:03:51.178991 1575633 finetune.py:68] layer 12_k @ epoch 3 new loss 0.00029532023472711444 old loss 0.00029804027872160077 BETTER
I0311 15:04:04.800115 1576405 finetune.py:68] layer 13_k @ epoch 2 new loss 0.0003194486489519477 old loss 0.0003228411078453064 BETTER
I0311 15:04:17.896283 1577947 finetune.py:68] layer 15_q @ epoch 3 new loss 0.0003773444623220712 old loss 0.00038419084739871323 BETTER
I0311 15:04:20.307803 1577178 finetune.py:68] layer 14_k @ epoch 0 new loss 0.0004128398431930691 old loss 0.0004224047006573528 BETTER
I0311 15:04:24.860745 1575633 finetune.py:68] layer 12_k @ epoch 4 new loss 0.0002929199254140258 old loss 0.00029532023472711444 BETTER
I0311 15:04:34.288490 1575633 finetune.py:45] layer 12_o initial loss 0.0007937581394799054
I0311 15:04:36.789942 1576405 finetune.py:68] layer 13_k @ epoch 3 new loss 0.0003164679219480604 old loss 0.0003194486489519477 BETTER
I0311 15:04:49.984663 1577947 finetune.py:68] layer 15_q @ epoch 4 new loss 0.00037145314854569733 old loss 0.0003773444623220712 BETTER
I0311 15:04:52.459652 1577178 finetune.py:68] layer 14_k @ epoch 1 new loss 0.000407351297326386 old loss 0.0004128398431930691 BETTER
I0311 15:04:59.010554 1577947 finetune.py:45] layer 15_k initial loss 0.0004285725299268961
I0311 15:05:06.958832 1575633 finetune.py:68] layer 12_o @ epoch 0 new loss 0.0007603344856761396 old loss 0.0007937581394799054 BETTER
I0311 15:05:08.963988 1576405 finetune.py:68] layer 13_k @ epoch 4 new loss 0.00031383728492073715 old loss 0.0003164679219480604 BETTER
I0311 15:05:18.274856 1576405 finetune.py:45] layer 13_o initial loss 0.0008363928063772619
I0311 15:05:24.508079 1577178 finetune.py:68] layer 14_k @ epoch 2 new loss 0.00040275853825733066 old loss 0.000407351297326386 BETTER
I0311 15:05:30.092147 1577947 finetune.py:68] layer 15_k @ epoch 0 new loss 0.0004188508610241115 old loss 0.0004285725299268961 BETTER
I0311 15:05:40.581987 1575633 finetune.py:68] layer 12_o @ epoch 1 new loss 0.0007412756094709039 old loss 0.0007603344856761396 BETTER
I0311 15:05:48.967595 1576405 finetune.py:68] layer 13_o @ epoch 0 new loss 0.0007977588102221489 old loss 0.0008363928063772619 BETTER
I0311 15:05:56.659386 1577178 finetune.py:68] layer 14_k @ epoch 3 new loss 0.00039866683073341846 old loss 0.00040275853825733066 BETTER
I0311 15:06:01.863411 1577947 finetune.py:68] layer 15_k @ epoch 1 new loss 0.00041335332207381725 old loss 0.0004188508610241115 BETTER
I0311 15:06:13.624442 1575633 finetune.py:68] layer 12_o @ epoch 2 new loss 0.000726811122149229 old loss 0.0007412756094709039 BETTER
I0311 15:06:20.429632 1576405 finetune.py:68] layer 13_o @ epoch 1 new loss 0.0007764894980937243 old loss 0.0007977588102221489 BETTER
I0311 15:06:28.708770 1577178 finetune.py:68] layer 14_k @ epoch 4 new loss 0.00039500711136497557 old loss 0.00039866683073341846 BETTER
I0311 15:06:33.673430 1577947 finetune.py:68] layer 15_k @ epoch 2 new loss 0.0004087011329829693 old loss 0.00041335332207381725 BETTER
I0311 15:06:38.219808 1577178 finetune.py:45] layer 14_o initial loss 0.0010658287210389972
I0311 15:06:47.066848 1575633 finetune.py:68] layer 12_o @ epoch 3 new loss 0.0007149604498408735 old loss 0.000726811122149229 BETTER
I0311 15:06:51.888358 1576405 finetune.py:68] layer 13_o @ epoch 2 new loss 0.0007603785488754511 old loss 0.0007764894980937243 BETTER
I0311 15:07:05.423132 1577947 finetune.py:68] layer 15_k @ epoch 3 new loss 0.0004046449903398752 old loss 0.0004087011329829693 BETTER
I0311 15:07:09.077366 1577178 finetune.py:68] layer 14_o @ epoch 0 new loss 0.0010219841497018933 old loss 0.0010658287210389972 BETTER
I0311 15:07:20.203332 1575633 finetune.py:68] layer 12_o @ epoch 4 new loss 0.0007050234125927091 old loss 0.0007149604498408735 BETTER
I0311 15:07:23.257388 1576405 finetune.py:68] layer 13_o @ epoch 3 new loss 0.0007474119192920625 old loss 0.0007603785488754511 BETTER
I0311 15:07:35.466562 1575633 finetune.py:45] layer 12_up initial loss 0.001079296343959868
I0311 15:07:37.234179 1577947 finetune.py:68] layer 15_k @ epoch 4 new loss 0.0004010344564449042 old loss 0.0004046449903398752 BETTER
I0311 15:07:40.706241 1577178 finetune.py:68] layer 14_o @ epoch 1 new loss 0.0009969008387997746 old loss 0.0010219841497018933 BETTER
I0311 15:07:46.584387 1577947 finetune.py:45] layer 15_o initial loss 0.001073388964869082
I0311 15:07:54.626410 1576405 finetune.py:68] layer 13_o @ epoch 4 new loss 0.0007365159108303487 old loss 0.0007474119192920625 BETTER
I0311 15:08:05.863309 1575633 finetune.py:68] layer 12_up @ epoch 0 new loss 0.001061096554622054 old loss 0.001079296343959868 BETTER
I0311 15:08:09.860152 1576405 finetune.py:45] layer 13_up initial loss 0.0011822048109024763
I0311 15:08:12.402933 1577178 finetune.py:68] layer 14_o @ epoch 2 new loss 0.0009777841623872519 old loss 0.0009969008387997746 BETTER
I0311 15:08:16.734457 1577947 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0010221096454188228 old loss 0.001073388964869082 BETTER
I0311 15:08:37.293454 1575633 finetune.py:68] layer 12_up @ epoch 1 new loss 0.001048666425049305 old loss 0.001061096554622054 BETTER
I0311 15:08:38.753122 1576405 finetune.py:68] layer 13_up @ epoch 0 new loss 0.0011593201197683811 old loss 0.0011822048109024763 BETTER
I0311 15:08:44.065900 1577178 finetune.py:68] layer 14_o @ epoch 3 new loss 0.000962188933044672 old loss 0.0009777841623872519 BETTER
I0311 15:08:47.687992 1577947 finetune.py:68] layer 15_o @ epoch 1 new loss 0.0009941004682332277 old loss 0.0010221096454188228 BETTER
I0311 15:09:08.677533 1576405 finetune.py:68] layer 13_up @ epoch 1 new loss 0.0011441393289715052 old loss 0.0011593201197683811 BETTER
I0311 15:09:08.974517 1575633 finetune.py:68] layer 12_up @ epoch 2 new loss 0.0010381987085565925 old loss 0.001048666425049305 BETTER
I0311 15:09:16.242185 1577178 finetune.py:68] layer 14_o @ epoch 4 new loss 0.0009489537915214896 old loss 0.000962188933044672 BETTER
I0311 15:09:18.901159 1577947 finetune.py:68] layer 15_o @ epoch 2 new loss 0.0009732613689266145 old loss 0.0009941004682332277 BETTER
I0311 15:09:31.482872 1577178 finetune.py:45] layer 14_up initial loss 0.0014498380478471518
I0311 15:09:38.473681 1576405 finetune.py:68] layer 13_up @ epoch 2 new loss 0.001131572062149644 old loss 0.0011441393289715052 BETTER
I0311 15:09:40.575149 1575633 finetune.py:68] layer 12_up @ epoch 3 new loss 0.0010290322825312614 old loss 0.0010381987085565925 BETTER
I0311 15:09:49.994253 1577947 finetune.py:68] layer 15_o @ epoch 3 new loss 0.0009564990759827197 old loss 0.0009732613689266145 BETTER
I0311 15:10:00.682652 1577178 finetune.py:68] layer 14_up @ epoch 0 new loss 0.0014244922203943133 old loss 0.0014498380478471518 BETTER
I0311 15:10:08.345099 1576405 finetune.py:68] layer 13_up @ epoch 3 new loss 0.001120684202760458 old loss 0.001131572062149644 BETTER
I0311 15:10:12.323876 1575633 finetune.py:68] layer 12_up @ epoch 4 new loss 0.001020882511511445 old loss 0.0010290322825312614 BETTER
I0311 15:10:21.201205 1577947 finetune.py:68] layer 15_o @ epoch 4 new loss 0.0009425749885849655 old loss 0.0009564990759827197 BETTER
I0311 15:10:27.691354 1575633 finetune.py:45] layer 12_gate initial loss 0.0012872129445895553
I0311 15:10:30.535658 1577178 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0014075092040002346 old loss 0.0014244922203943133 BETTER
I0311 15:10:36.499391 1577947 finetune.py:45] layer 15_up initial loss 0.0015542443143203855
I0311 15:10:38.219104 1576405 finetune.py:68] layer 13_up @ epoch 4 new loss 0.0011110911145806313 old loss 0.001120684202760458 BETTER
I0311 15:10:53.184081 1576405 finetune.py:45] layer 13_gate initial loss 0.0014357073232531548
I0311 15:10:56.726551 1575633 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.001276487484574318 old loss 0.0012872129445895553 BETTER
I0311 15:11:00.581893 1577178 finetune.py:68] layer 14_up @ epoch 2 new loss 0.001393408514559269 old loss 0.0014075092040002346 BETTER
I0311 15:11:05.355408 1577947 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0015205985400825739 old loss 0.0015542443143203855 BETTER
I0311 15:11:20.832124 1576405 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.0014222513418644667 old loss 0.0014357073232531548 BETTER
I0311 15:11:26.554599 1575633 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0012683377135545015 old loss 0.001276487484574318 BETTER
I0311 15:11:30.736224 1577178 finetune.py:68] layer 14_up @ epoch 3 new loss 0.001381151145324111 old loss 0.001393408514559269 BETTER
I0311 15:11:34.821774 1577947 finetune.py:68] layer 15_up @ epoch 1 new loss 0.0014996543759480119 old loss 0.0015205985400825739 BETTER
I0311 15:11:48.875332 1576405 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0014122424181550741 old loss 0.0014222513418644667 BETTER
I0311 15:11:56.686846 1575633 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.0012610823614522815 old loss 0.0012683377135545015 BETTER
I0311 15:12:01.098894 1577178 finetune.py:68] layer 14_up @ epoch 4 new loss 0.001370210899040103 old loss 0.001381151145324111 BETTER
I0311 15:12:04.418654 1577947 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0014827661216259003 old loss 0.0014996543759480119 BETTER
I0311 15:12:16.716187 1577178 finetune.py:45] layer 14_gate initial loss 0.0017536025261506438
I0311 15:12:16.827273 1576405 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0014034092891961336 old loss 0.0014122424181550741 BETTER
I0311 15:12:27.609916 1575633 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.001254469738341868 old loss 0.0012610823614522815 BETTER
I0311 15:12:34.065186 1577947 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0014680952299386263 old loss 0.0014827661216259003 BETTER
I0311 15:12:44.312366 1577178 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0017385869286954403 old loss 0.0017536025261506438 BETTER
I0311 15:12:44.878801 1576405 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.0013953729066997766 old loss 0.0014034092891961336 BETTER
I0311 15:12:57.710106 1575633 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.0012484006583690643 old loss 0.001254469738341868 BETTER
I0311 15:13:03.675004 1577947 finetune.py:68] layer 15_up @ epoch 4 new loss 0.001455407589673996 old loss 0.0014680952299386263 BETTER
I0311 15:13:13.318090 1577178 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0017272070981562138 old loss 0.0017385869286954403 BETTER
I0311 15:13:13.576631 1576405 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.001388039207085967 old loss 0.0013953729066997766 BETTER
I0311 15:13:14.720457 1575633 finetune.py:45] layer 12_down initial loss 0.001803407445549965
I0311 15:13:19.701401 1577947 finetune.py:45] layer 15_gate initial loss 0.001928843674249947
I0311 15:13:29.621404 1576405 finetune.py:45] layer 13_down initial loss 0.0020660331938415766
I0311 15:13:41.733591 1577178 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.0017170619685202837 old loss 0.0017272070981562138 BETTER
I0311 15:13:41.940043 1575633 finetune.py:68] layer 12_down @ epoch 0 new loss 0.0018023314187303185 old loss 0.001803407445549965 BETTER
I0311 15:13:46.903041 1577947 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.0019103054655715823 old loss 0.001928843674249947 BETTER
I0311 15:13:55.555566 1576405 finetune.py:68] layer 13_down @ epoch 0 new loss 0.0020646785851567984 old loss 0.0020660331938415766 BETTER
I0311 15:14:10.364110 1577178 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.0017078490927815437 old loss 0.0017170619685202837 BETTER
I0311 15:14:10.427374 1575633 finetune.py:68] layer 12_down @ epoch 1 new loss 0.0018013506196439266 old loss 0.0018023314187303185 BETTER
I0311 15:14:15.113012 1577947 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.001896345755085349 old loss 0.0019103054655715823 BETTER
I0311 15:14:22.574872 1576405 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0020634671673178673 old loss 0.0020646785851567984 BETTER
I0311 15:14:39.039687 1577178 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0016993236495181918 old loss 0.0017078490927815437 BETTER
I0311 15:14:39.103955 1575633 finetune.py:68] layer 12_down @ epoch 2 new loss 0.0018004694720730186 old loss 0.0018013506196439266 BETTER
I0311 15:14:43.403649 1577947 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.001884053461253643 old loss 0.001896345755085349 BETTER
I0311 15:14:49.592458 1576405 finetune.py:68] layer 13_down @ epoch 2 new loss 0.002062369603663683 old loss 0.0020634671673178673 BETTER
I0311 15:14:55.718516 1577178 finetune.py:45] layer 14_down initial loss 0.002486141864210367
I0311 15:15:07.617192 1575633 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0017996645765379071 old loss 0.0018004694720730186 BETTER
I0311 15:15:11.675698 1577947 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.001872854307293892 old loss 0.001884053461253643 BETTER
I0311 15:15:16.571158 1576405 finetune.py:68] layer 13_down @ epoch 3 new loss 0.0020613661035895348 old loss 0.002062369603663683 BETTER
I0311 15:15:21.983855 1577178 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0024844480212777853 old loss 0.002486141864210367 BETTER
I0311 15:15:36.082173 1575633 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0017989428015425801 old loss 0.0017996645765379071 BETTER
12_v proxy err 0.04149024933576584 tr(WHW.T) 703.318603515625
12_q proxy err 0.0047173211351037025 tr(WHW.T) 7056.8876953125
12_k proxy err 0.003103000810369849 tr(WHW.T) 10935.4111328125
12_o proxy err 0.06423943489789963 tr(WHW.T) 40.046443939208984
12_up proxy err 0.02758028544485569 tr(WHW.T) 1227.033447265625
12_gate proxy err 0.014499066397547722 tr(WHW.T) 2381.40625
12_down proxy err 0.037406910210847855 tr(WHW.T) 65.04229736328125
I0311 15:15:39.950162 1577947 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0018626338569447398 old loss 0.001872854307293892 BETTER
I0311 15:15:44.457288 1576405 finetune.py:68] layer 13_down @ epoch 4 new loss 0.0020604520104825497 old loss 0.0020613661035895348 BETTER
13_v proxy err 0.04254117235541344 tr(WHW.T) 714.5677490234375
13_q proxy err 0.004948507994413376 tr(WHW.T) 6966.46240234375
13_k proxy err 0.003337798174470663 tr(WHW.T) 10473.9990234375
13_o proxy err 0.05700257793068886 tr(WHW.T) 46.695068359375
13_up proxy err 0.026525409892201424 tr(WHW.T) 1365.25537109375
13_gate proxy err 0.014198075979948044 tr(WHW.T) 2598.737060546875
13_down proxy err 0.036874424666166306 tr(WHW.T) 80.51404571533203
I0311 15:15:49.655375 1577178 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0024829271715134382 old loss 0.0024844480212777853 BETTER
I0311 15:15:56.733340 1577947 finetune.py:45] layer 15_down initial loss 0.002846842398867011
I0311 15:16:16.587044 1577178 finetune.py:68] layer 14_down @ epoch 2 new loss 0.002481549745425582 old loss 0.0024829271715134382 BETTER
I0311 15:16:22.272749 1577947 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0028448484372347593 old loss 0.002846842398867011 BETTER
I0311 15:16:43.338633 1577178 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0024803003761917353 old loss 0.002481549745425582 BETTER
I0311 15:16:48.582117 1577947 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00284304260276258 old loss 0.0028448484372347593 BETTER
I0311 15:17:00.805332 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 16 in 71.463134765625s
I0311 15:17:03.914636 1587662 config.py:54] PyTorch version 2.1.1 available.
I0311 15:17:04.922261 1536994 quantize_finetune_llama.py:183] layer 17 gpu 1
I0311 15:17:04.993718 1587662 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 15:17:10.111149 1577178 finetune.py:68] layer 14_down @ epoch 4 new loss 0.002479142975062132 old loss 0.0024803003761917353 BETTER
14_v proxy err 0.04495638981461525 tr(WHW.T) 706.1612548828125
14_q proxy err 0.005110954400151968 tr(WHW.T) 7088.3603515625
14_k proxy err 0.0032439178321510553 tr(WHW.T) 11348.1826171875
14_o proxy err 0.06480539590120316 tr(WHW.T) 51.981632232666016
14_up proxy err 0.02709369733929634 tr(WHW.T) 1464.0675048828125
14_gate proxy err 0.01502235047519207 tr(WHW.T) 2685.004150390625
14_down proxy err 0.03739309310913086 tr(WHW.T) 91.6231460571289
I0311 15:17:13.472026 1587662 finetune.py:45] layer 16_v initial loss 0.0006297969375737011
I0311 15:17:15.037378 1577947 finetune.py:68] layer 15_down @ epoch 2 new loss 0.0028413881082087755 old loss 0.00284304260276258 BETTER
I0311 15:17:41.488932 1577947 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0028398875147104263 old loss 0.0028413881082087755 BETTER
I0311 15:17:46.700341 1587662 finetune.py:68] layer 16_v @ epoch 0 new loss 0.0005177049315534532 old loss 0.0006297969375737011 BETTER
I0311 15:18:07.901827 1577947 finetune.py:68] layer 15_down @ epoch 4 new loss 0.002838509390130639 old loss 0.0028398875147104263 BETTER
15_v proxy err 0.040570322424173355 tr(WHW.T) 762.7275390625
15_q proxy err 0.00479736877605319 tr(WHW.T) 7262.89208984375
15_k proxy err 0.003198613179847598 tr(WHW.T) 11126.546875
15_o proxy err 0.05371202528476715 tr(WHW.T) 60.82242202758789
15_up proxy err 0.026341672986745834 tr(WHW.T) 1640.2774658203125
15_gate proxy err 0.015122942626476288 tr(WHW.T) 2903.925048828125
15_down proxy err 0.036876313388347626 tr(WHW.T) 115.8710708618164
I0311 15:18:21.250456 1587662 finetune.py:68] layer 16_v @ epoch 1 new loss 0.0004892504075542092 old loss 0.0005177049315534532 BETTER
I0311 15:18:24.766581 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 17 in 69.8687391281128s
I0311 15:18:27.825399 1588527 config.py:54] PyTorch version 2.1.1 available.
I0311 15:18:28.802974 1536994 quantize_finetune_llama.py:183] layer 18 gpu 2
I0311 15:18:28.875803 1588527 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 15:18:37.269985 1588527 finetune.py:45] layer 17_v initial loss 0.0005185242043808103
I0311 15:18:55.997817 1587662 finetune.py:68] layer 16_v @ epoch 2 new loss 0.00047065128455869853 old loss 0.0004892504075542092 BETTER
I0311 15:19:08.588955 1588527 finetune.py:68] layer 17_v @ epoch 0 new loss 0.00043397233821451664 old loss 0.0005185242043808103 BETTER
I0311 15:19:30.839921 1587662 finetune.py:68] layer 16_v @ epoch 3 new loss 0.00045672967098653316 old loss 0.00047065128455869853 BETTER
I0311 15:19:38.780889 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 18 in 69.5661973953247s
I0311 15:19:40.966179 1588527 finetune.py:68] layer 17_v @ epoch 1 new loss 0.00041158864041790366 old loss 0.00043397233821451664 BETTER
I0311 15:19:41.972763 1589299 config.py:54] PyTorch version 2.1.1 available.
I0311 15:19:42.978610 1536994 quantize_finetune_llama.py:183] layer 19 gpu 3
I0311 15:19:43.058582 1589299 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 15:19:51.513212 1589299 finetune.py:45] layer 18_v initial loss 0.0005170235526748002
I0311 15:20:05.775666 1587662 finetune.py:68] layer 16_v @ epoch 4 new loss 0.0004456340684555471 old loss 0.00045672967098653316 BETTER
I0311 15:20:13.873543 1588527 finetune.py:68] layer 17_v @ epoch 2 new loss 0.00039671099511906505 old loss 0.00041158864041790366 BETTER
I0311 15:20:15.633286 1587662 finetune.py:45] layer 16_q initial loss 0.0005452942568808794
I0311 15:20:23.120041 1589299 finetune.py:68] layer 18_v @ epoch 0 new loss 0.00043226085836067796 old loss 0.0005170235526748002 BETTER
I0311 15:20:46.711170 1588527 finetune.py:68] layer 17_v @ epoch 3 new loss 0.00038544100243598223 old loss 0.00039671099511906505 BETTER
I0311 15:20:49.129972 1587662 finetune.py:68] layer 16_q @ epoch 0 new loss 0.0005141095025464892 old loss 0.0005452942568808794 BETTER
I0311 15:20:54.736306 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 19 in 71.35695147514343s
I0311 15:20:55.697721 1589299 finetune.py:68] layer 18_v @ epoch 1 new loss 0.0004125661216676235 old loss 0.00043226085836067796 BETTER
I0311 15:20:58.040084 1590088 config.py:54] PyTorch version 2.1.1 available.
I0311 15:20:59.108684 1536994 quantize_finetune_llama.py:183] layer 20 gpu 0
I0311 15:20:59.174325 1590088 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 15:21:07.252710 1590088 finetune.py:45] layer 19_v initial loss 0.0005042898119427264
I0311 15:21:19.635969 1588527 finetune.py:68] layer 17_v @ epoch 4 new loss 0.00037647184217348695 old loss 0.00038544100243598223 BETTER
I0311 15:21:23.488789 1587662 finetune.py:68] layer 16_q @ epoch 1 new loss 0.0005007392028346658 old loss 0.0005141095025464892 BETTER
I0311 15:21:28.571240 1589299 finetune.py:68] layer 18_v @ epoch 2 new loss 0.00039939937414601445 old loss 0.0004125661216676235 BETTER
I0311 15:21:29.133017 1588527 finetune.py:45] layer 17_q initial loss 0.000472637708298862
I0311 15:21:38.290502 1590088 finetune.py:68] layer 19_v @ epoch 0 new loss 0.0004251995705999434 old loss 0.0005042898119427264 BETTER
I0311 15:21:58.021738 1587662 finetune.py:68] layer 16_q @ epoch 2 new loss 0.0004903480876237154 old loss 0.0005007392028346658 BETTER
I0311 15:22:00.899648 1588527 finetune.py:68] layer 17_q @ epoch 0 new loss 0.00044394441647455096 old loss 0.000472637708298862 BETTER
I0311 15:22:01.739874 1589299 finetune.py:68] layer 18_v @ epoch 3 new loss 0.00038929007132537663 old loss 0.00039939937414601445 BETTER
I0311 15:22:10.319960 1590088 finetune.py:68] layer 19_v @ epoch 1 new loss 0.00040659657679498196 old loss 0.0004251995705999434 BETTER
I0311 15:22:32.742035 1587662 finetune.py:68] layer 16_q @ epoch 3 new loss 0.0004817589942831546 old loss 0.0004903480876237154 BETTER
I0311 15:22:33.296083 1588527 finetune.py:68] layer 17_q @ epoch 1 new loss 0.0004320515727158636 old loss 0.00044394441647455096 BETTER
I0311 15:22:35.249391 1589299 finetune.py:68] layer 18_v @ epoch 4 new loss 0.0003814048832282424 old loss 0.00038929007132537663 BETTER
I0311 15:22:42.625353 1590088 finetune.py:68] layer 19_v @ epoch 2 new loss 0.0003942773037124425 old loss 0.00040659657679498196 BETTER
I0311 15:22:44.624623 1589299 finetune.py:45] layer 18_q initial loss 0.0005117990076541901
I0311 15:23:05.785974 1588527 finetune.py:68] layer 17_q @ epoch 2 new loss 0.00042270636186003685 old loss 0.0004320515727158636 BETTER
I0311 15:23:07.300489 1587662 finetune.py:68] layer 16_q @ epoch 4 new loss 0.0004745346086565405 old loss 0.0004817589942831546 BETTER
I0311 15:23:15.139864 1590088 finetune.py:68] layer 19_v @ epoch 3 new loss 0.00038489836151711643 old loss 0.0003942773037124425 BETTER
I0311 15:23:16.436082 1589299 finetune.py:68] layer 18_q @ epoch 0 new loss 0.0004744555044453591 old loss 0.0005117990076541901 BETTER
I0311 15:23:17.055505 1587662 finetune.py:45] layer 16_k initial loss 0.0005466553848236799
I0311 15:23:38.494530 1588527 finetune.py:68] layer 17_q @ epoch 3 new loss 0.000415138085372746 old loss 0.00042270636186003685 BETTER
I0311 15:23:47.880946 1590088 finetune.py:68] layer 19_v @ epoch 4 new loss 0.00037745287409052253 old loss 0.00038489836151711643 BETTER
I0311 15:23:49.167765 1589299 finetune.py:68] layer 18_q @ epoch 1 new loss 0.00046186806866899133 old loss 0.0004744555044453591 BETTER
I0311 15:23:49.961510 1587662 finetune.py:68] layer 16_k @ epoch 0 new loss 0.0005341893411241472 old loss 0.0005466553848236799 BETTER
I0311 15:23:57.411149 1590088 finetune.py:45] layer 19_q initial loss 0.0004961986560374498
I0311 15:24:11.085188 1588527 finetune.py:68] layer 17_q @ epoch 4 new loss 0.00040871367673389614 old loss 0.000415138085372746 BETTER
I0311 15:24:20.180863 1588527 finetune.py:45] layer 17_k initial loss 0.0004821206093765795
I0311 15:24:21.676757 1589299 finetune.py:68] layer 18_q @ epoch 2 new loss 0.00045243988279253244 old loss 0.00046186806866899133 BETTER
I0311 15:24:23.697039 1587662 finetune.py:68] layer 16_k @ epoch 1 new loss 0.0005270669353194535 old loss 0.0005341893411241472 BETTER
I0311 15:24:29.076014 1590088 finetune.py:68] layer 19_q @ epoch 0 new loss 0.00046218300121836364 old loss 0.0004961986560374498 BETTER
I0311 15:24:51.783020 1588527 finetune.py:68] layer 17_k @ epoch 0 new loss 0.0004700471763499081 old loss 0.0004821206093765795 BETTER
I0311 15:24:54.264318 1589299 finetune.py:68] layer 18_q @ epoch 3 new loss 0.00044479413190856576 old loss 0.00045243988279253244 BETTER
I0311 15:24:57.282045 1587662 finetune.py:68] layer 16_k @ epoch 2 new loss 0.0005210914532653987 old loss 0.0005270669353194535 BETTER
I0311 15:25:01.090541 1590088 finetune.py:68] layer 19_q @ epoch 1 new loss 0.00045083428267389536 old loss 0.00046218300121836364 BETTER
I0311 15:25:23.802745 1588527 finetune.py:68] layer 17_k @ epoch 1 new loss 0.0004635334189515561 old loss 0.0004700471763499081 BETTER
I0311 15:25:27.160369 1589299 finetune.py:68] layer 18_q @ epoch 4 new loss 0.000438385148299858 old loss 0.00044479413190856576 BETTER
I0311 15:25:31.016604 1587662 finetune.py:68] layer 16_k @ epoch 3 new loss 0.0005159209249541163 old loss 0.0005210914532653987 BETTER
I0311 15:25:33.162901 1590088 finetune.py:68] layer 19_q @ epoch 2 new loss 0.00044237321708351374 old loss 0.00045083428267389536 BETTER
I0311 15:25:36.711428 1589299 finetune.py:45] layer 18_k initial loss 0.0005352809675969183
I0311 15:25:55.912175 1588527 finetune.py:68] layer 17_k @ epoch 2 new loss 0.00045812572352588177 old loss 0.0004635334189515561 BETTER
I0311 15:26:04.679428 1587662 finetune.py:68] layer 16_k @ epoch 4 new loss 0.0005113253137096763 old loss 0.0005159209249541163 BETTER
I0311 15:26:05.351366 1590088 finetune.py:68] layer 19_q @ epoch 3 new loss 0.00043551321141421795 old loss 0.00044237321708351374 BETTER
I0311 15:26:08.182337 1589299 finetune.py:68] layer 18_k @ epoch 0 new loss 0.000518471235409379 old loss 0.0005352809675969183 BETTER
I0311 15:26:14.265478 1587662 finetune.py:45] layer 16_o initial loss 0.0013743805466219783
I0311 15:26:27.965380 1588527 finetune.py:68] layer 17_k @ epoch 3 new loss 0.00045347053674049675 old loss 0.00045812572352588177 BETTER
I0311 15:26:37.463503 1590088 finetune.py:68] layer 19_q @ epoch 4 new loss 0.0004296926490496844 old loss 0.00043551321141421795 BETTER
I0311 15:26:40.294859 1589299 finetune.py:68] layer 18_k @ epoch 1 new loss 0.0005119797424413264 old loss 0.000518471235409379 BETTER
I0311 15:26:46.293163 1587662 finetune.py:68] layer 16_o @ epoch 0 new loss 0.0013138205977156758 old loss 0.0013743805466219783 BETTER
I0311 15:26:46.638442 1590088 finetune.py:45] layer 19_k initial loss 0.0005216113058850169
I0311 15:27:00.032544 1588527 finetune.py:68] layer 17_k @ epoch 4 new loss 0.0004493959713727236 old loss 0.00045347053674049675 BETTER
I0311 15:27:09.187211 1588527 finetune.py:45] layer 17_o initial loss 0.001070949831046164
I0311 15:27:12.404606 1589299 finetune.py:68] layer 18_k @ epoch 2 new loss 0.0005066809244453907 old loss 0.0005119797424413264 BETTER
I0311 15:27:17.487018 1590088 finetune.py:68] layer 19_k @ epoch 0 new loss 0.0005079135880805552 old loss 0.0005216113058850169 BETTER
I0311 15:27:19.323798 1587662 finetune.py:68] layer 16_o @ epoch 1 new loss 0.0012812672648578882 old loss 0.0013138205977156758 BETTER
I0311 15:27:39.780024 1588527 finetune.py:68] layer 17_o @ epoch 0 new loss 0.00103322125505656 old loss 0.001070949831046164 BETTER
I0311 15:27:44.434612 1589299 finetune.py:68] layer 18_k @ epoch 3 new loss 0.0005020708194933832 old loss 0.0005066809244453907 BETTER
I0311 15:27:49.120528 1590088 finetune.py:68] layer 19_k @ epoch 1 new loss 0.0005015680799260736 old loss 0.0005079135880805552 BETTER
I0311 15:27:52.474571 1587662 finetune.py:68] layer 16_o @ epoch 2 new loss 0.0012564283097162843 old loss 0.0012812672648578882 BETTER
I0311 15:28:11.131225 1588527 finetune.py:68] layer 17_o @ epoch 1 new loss 0.0010118732461705804 old loss 0.00103322125505656 BETTER
I0311 15:28:16.465370 1589299 finetune.py:68] layer 18_k @ epoch 4 new loss 0.0004981046658940613 old loss 0.0005020708194933832 BETTER
I0311 15:28:20.719541 1590088 finetune.py:68] layer 19_k @ epoch 2 new loss 0.0004963592509739101 old loss 0.0005015680799260736 BETTER
I0311 15:28:25.645068 1587662 finetune.py:68] layer 16_o @ epoch 3 new loss 0.0012363739078864455 old loss 0.0012564283097162843 BETTER
I0311 15:28:25.931957 1589299 finetune.py:45] layer 18_o initial loss 0.001165540423244238
I0311 15:28:42.442015 1588527 finetune.py:68] layer 17_o @ epoch 2 new loss 0.000995409907773137 old loss 0.0010118732461705804 BETTER
I0311 15:28:52.376730 1590088 finetune.py:68] layer 19_k @ epoch 3 new loss 0.0004918604390695691 old loss 0.0004963592509739101 BETTER
I0311 15:28:56.712317 1589299 finetune.py:68] layer 18_o @ epoch 0 new loss 0.0011250718962401152 old loss 0.001165540423244238 BETTER
I0311 15:28:59.012832 1587662 finetune.py:68] layer 16_o @ epoch 4 new loss 0.0012192341964691877 old loss 0.0012363739078864455 BETTER
I0311 15:29:13.870842 1588527 finetune.py:68] layer 17_o @ epoch 3 new loss 0.0009819524129852653 old loss 0.000995409907773137 BETTER
I0311 15:29:14.198932 1587662 finetune.py:45] layer 16_up initial loss 0.0020176954567432404
I0311 15:29:24.134694 1590088 finetune.py:68] layer 19_k @ epoch 4 new loss 0.00048802595119923353 old loss 0.0004918604390695691 BETTER
I0311 15:29:28.283671 1589299 finetune.py:68] layer 18_o @ epoch 1 new loss 0.0011037009535357356 old loss 0.0011250718962401152 BETTER
I0311 15:29:33.473150 1590088 finetune.py:45] layer 19_o initial loss 0.0011107849422842264
I0311 15:29:45.017657 1587662 finetune.py:68] layer 16_up @ epoch 0 new loss 0.0019774979446083307 old loss 0.0020176954567432404 BETTER
I0311 15:29:45.663258 1588527 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0009705738048069179 old loss 0.0009819524129852653 BETTER
I0311 15:29:59.907695 1589299 finetune.py:68] layer 18_o @ epoch 2 new loss 0.0010873693972826004 old loss 0.0011037009535357356 BETTER
I0311 15:30:00.585949 1588527 finetune.py:45] layer 17_up initial loss 0.001869307947345078
I0311 15:30:04.013166 1590088 finetune.py:68] layer 19_o @ epoch 0 new loss 0.0010750207584351301 old loss 0.0011107849422842264 BETTER
I0311 15:30:16.553878 1587662 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0019518013577908278 old loss 0.0019774979446083307 BETTER
I0311 15:30:29.388756 1588527 finetune.py:68] layer 17_up @ epoch 0 new loss 0.0018325846176594496 old loss 0.001869307947345078 BETTER
I0311 15:30:31.607039 1589299 finetune.py:68] layer 18_o @ epoch 3 new loss 0.001074084430001676 old loss 0.0010873693972826004 BETTER
I0311 15:30:35.027896 1590088 finetune.py:68] layer 19_o @ epoch 1 new loss 0.0010566541459411383 old loss 0.0010750207584351301 BETTER
I0311 15:30:48.124326 1587662 finetune.py:68] layer 16_up @ epoch 2 new loss 0.0019310470670461655 old loss 0.0019518013577908278 BETTER
I0311 15:30:58.981365 1588527 finetune.py:68] layer 17_up @ epoch 1 new loss 0.00180988444481045 old loss 0.0018325846176594496 BETTER
I0311 15:31:03.294024 1589299 finetune.py:68] layer 18_o @ epoch 4 new loss 0.0010628055315464735 old loss 0.001074084430001676 BETTER
I0311 15:31:06.004850 1590088 finetune.py:68] layer 19_o @ epoch 2 new loss 0.0010426775552332401 old loss 0.0010566541459411383 BETTER
I0311 15:31:18.246612 1589299 finetune.py:45] layer 18_up initial loss 0.0021467339247465134
I0311 15:31:19.772921 1587662 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0019132053712382913 old loss 0.0019310470670461655 BETTER
I0311 15:31:28.792005 1588527 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0017918930388987064 old loss 0.00180988444481045 BETTER
I0311 15:31:37.015515 1590088 finetune.py:68] layer 19_o @ epoch 3 new loss 0.0010316262487322092 old loss 0.0010426775552332401 BETTER
I0311 15:31:47.322274 1589299 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0021034071687608957 old loss 0.0021467339247465134 BETTER
I0311 15:31:51.463482 1587662 finetune.py:68] layer 16_up @ epoch 4 new loss 0.0018975116545334458 old loss 0.0019132053712382913 BETTER
I0311 15:31:58.564107 1588527 finetune.py:68] layer 17_up @ epoch 3 new loss 0.0017763307550922036 old loss 0.0017918930388987064 BETTER
I0311 15:32:06.838374 1587662 finetune.py:45] layer 16_gate initial loss 0.0025262716226279736
I0311 15:32:08.017284 1590088 finetune.py:68] layer 19_o @ epoch 4 new loss 0.0010221947450190783 old loss 0.0010316262487322092 BETTER
I0311 15:32:17.335178 1589299 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0020778102334588766 old loss 0.0021034071687608957 BETTER
I0311 15:32:22.856918 1590088 finetune.py:45] layer 19_up initial loss 0.002242823364213109
I0311 15:32:28.435068 1588527 finetune.py:68] layer 17_up @ epoch 4 new loss 0.001762759406119585 old loss 0.0017763307550922036 BETTER
I0311 15:32:35.787672 1587662 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.002503741765394807 old loss 0.0025262716226279736 BETTER
I0311 15:32:43.364175 1588527 finetune.py:45] layer 17_gate initial loss 0.002464757766574621
I0311 15:32:47.333438 1589299 finetune.py:68] layer 18_up @ epoch 2 new loss 0.00205761706456542 old loss 0.0020778102334588766 BETTER
I0311 15:32:51.405013 1590088 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0021976397838443518 old loss 0.002242823364213109 BETTER
I0311 15:33:05.579936 1587662 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0024860932026058435 old loss 0.002503741765394807 BETTER
I0311 15:33:10.981801 1588527 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0024447357282042503 old loss 0.002464757766574621 BETTER
I0311 15:33:17.356089 1589299 finetune.py:68] layer 18_up @ epoch 3 new loss 0.002040460705757141 old loss 0.00205761706456542 BETTER
I0311 15:33:20.940705 1590088 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0021716835908591747 old loss 0.0021976397838443518 BETTER
I0311 15:33:35.516664 1587662 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.002470307983458042 old loss 0.0024860932026058435 BETTER
I0311 15:33:39.112756 1588527 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.002428743988275528 old loss 0.0024447357282042503 BETTER
I0311 15:33:47.447728 1589299 finetune.py:68] layer 18_up @ epoch 4 new loss 0.002025595400482416 old loss 0.002040460705757141 BETTER
I0311 15:33:50.375256 1590088 finetune.py:68] layer 19_up @ epoch 2 new loss 0.002151266671717167 old loss 0.0021716835908591747 BETTER
I0311 15:34:02.375727 1589299 finetune.py:45] layer 18_gate initial loss 0.0028548857662826777
I0311 15:34:05.484867 1587662 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.00245598703622818 old loss 0.002470307983458042 BETTER
I0311 15:34:07.325141 1588527 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.0024145415518432856 old loss 0.002428743988275528 BETTER
I0311 15:34:19.886656 1590088 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0021342698018997908 old loss 0.002151266671717167 BETTER
I0311 15:34:29.873667 1589299 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0028330401983112097 old loss 0.0028548857662826777 BETTER
I0311 15:34:35.378014 1588527 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.0024015503004193306 old loss 0.0024145415518432856 BETTER
I0311 15:34:35.511585 1587662 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.002442803466692567 old loss 0.00245598703622818 BETTER
I0311 15:34:49.815335 1590088 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0021196543239057064 old loss 0.0021342698018997908 BETTER
I0311 15:34:51.963695 1587662 finetune.py:45] layer 16_down initial loss 0.003785001114010811
I0311 15:34:58.265034 1589299 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.0028157506603747606 old loss 0.0028330401983112097 BETTER
I0311 15:35:03.980040 1588527 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.002389688743278384 old loss 0.0024015503004193306 BETTER
I0311 15:35:06.041005 1590088 finetune.py:45] layer 19_gate initial loss 0.003088458441197872
I0311 15:35:19.344079 1587662 finetune.py:68] layer 16_down @ epoch 0 new loss 0.003782151034101844 old loss 0.003785001114010811 BETTER
I0311 15:35:20.314225 1588527 finetune.py:45] layer 17_down initial loss 0.0038686261978000402
I0311 15:35:26.992165 1589299 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0028003358747810125 old loss 0.0028157506603747606 BETTER
I0311 15:35:33.121219 1590088 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.0030655215959995985 old loss 0.003088458441197872 BETTER
I0311 15:35:46.197911 1588527 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0038657733239233494 old loss 0.0038686261978000402 BETTER
I0311 15:35:48.097712 1587662 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0037795272655785084 old loss 0.003782151034101844 BETTER
I0311 15:35:55.347113 1589299 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.002786303171887994 old loss 0.0028003358747810125 BETTER
I0311 15:36:01.215888 1590088 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0030475601088255644 old loss 0.0030655215959995985 BETTER
I0311 15:36:12.964156 1588527 finetune.py:68] layer 17_down @ epoch 1 new loss 0.00386320473626256 old loss 0.0038657733239233494 BETTER
I0311 15:36:16.836016 1587662 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0037771030329167843 old loss 0.0037795272655785084 BETTER
I0311 15:36:23.641818 1589299 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.0027734681498259306 old loss 0.002786303171887994 BETTER
I0311 15:36:29.367322 1590088 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0030316675547510386 old loss 0.0030475601088255644 BETTER
I0311 15:36:39.551535 1589299 finetune.py:45] layer 18_down initial loss 0.004516745917499065
I0311 15:36:39.839663 1588527 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0038608438335359097 old loss 0.00386320473626256 BETTER
I0311 15:36:45.537126 1587662 finetune.py:68] layer 16_down @ epoch 3 new loss 0.003774854354560375 old loss 0.0037771030329167843 BETTER
I0311 15:36:57.638724 1590088 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0030171594116836786 old loss 0.0030316675547510386 BETTER
I0311 15:37:05.724300 1589299 finetune.py:68] layer 18_down @ epoch 0 new loss 0.004513618070632219 old loss 0.004516745917499065 BETTER
I0311 15:37:06.710367 1588527 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0038586610462516546 old loss 0.0038608438335359097 BETTER
I0311 15:37:14.179022 1587662 finetune.py:68] layer 16_down @ epoch 4 new loss 0.003772761905565858 old loss 0.003774854354560375 BETTER
16_v proxy err 0.042409323155879974 tr(WHW.T) 780.7407836914062
16_q proxy err 0.005144336726516485 tr(WHW.T) 7199.0302734375
16_k proxy err 0.0032291768584400415 tr(WHW.T) 11688.103515625
16_o proxy err 0.04470239579677582 tr(WHW.T) 89.4837875366211
16_up proxy err 0.026426449418067932 tr(WHW.T) 1886.0467529296875
16_gate proxy err 0.015162128955125809 tr(WHW.T) 3356.37353515625
16_down proxy err 0.03745148330926895 tr(WHW.T) 154.46612548828125
I0311 15:37:26.313293 1590088 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.003004054306074977 old loss 0.0030171594116836786 BETTER
I0311 15:37:33.312327 1589299 finetune.py:68] layer 18_down @ epoch 1 new loss 0.004510731901973486 old loss 0.004513618070632219 BETTER
I0311 15:37:34.002474 1588527 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0038566605653613806 old loss 0.0038586610462516546 BETTER
17_v proxy err 0.04198663681745529 tr(WHW.T) 845.7654418945312
17_q proxy err 0.005548038985580206 tr(WHW.T) 7176.6962890625
17_k proxy err 0.0037666333373636007 tr(WHW.T) 10749.1826171875
17_o proxy err 0.048193685710430145 tr(WHW.T) 59.082706451416016
17_up proxy err 0.02927013672888279 tr(WHW.T) 1919.52783203125
17_gate proxy err 0.016180317848920822 tr(WHW.T) 3559.953369140625
17_down proxy err 0.0381222739815712 tr(WHW.T) 167.84591674804688
I0311 15:37:42.164044 1590088 finetune.py:45] layer 19_down initial loss 0.004912159405648708
I0311 15:38:00.089953 1589299 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0045080725103616714 old loss 0.004510731901973486 BETTER
I0311 15:38:07.875641 1590088 finetune.py:68] layer 19_down @ epoch 0 new loss 0.004908821079879999 old loss 0.004912159405648708 BETTER
I0311 15:38:26.942300 1589299 finetune.py:68] layer 18_down @ epoch 3 new loss 0.004505601711571217 old loss 0.0045080725103616714 BETTER
I0311 15:38:34.329492 1590088 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0049057574942708015 old loss 0.004908821079879999 BETTER
I0311 15:38:49.831665 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 20 in 71.16280817985535s
I0311 15:38:53.056899 1599780 config.py:54] PyTorch version 2.1.1 available.
I0311 15:38:53.771728 1589299 finetune.py:68] layer 18_down @ epoch 4 new loss 0.004503278061747551 old loss 0.004505601711571217 BETTER
I0311 15:38:54.060622 1536994 quantize_finetune_llama.py:183] layer 21 gpu 1
I0311 15:38:54.128940 1599780 data_utils.py:336] using 256 training seqs, 128 validation seqs
18_v proxy err 0.03991661220788956 tr(WHW.T) 1003.7705078125
18_q proxy err 0.005919240415096283 tr(WHW.T) 7519.2216796875
18_k proxy err 0.004293662495911121 tr(WHW.T) 10512.6787109375
18_o proxy err 0.04171053320169449 tr(WHW.T) 71.00541687011719
18_up proxy err 0.031289756298065186 tr(WHW.T) 2019.85693359375
18_gate proxy err 0.017311763018369675 tr(WHW.T) 3755.48291015625
18_down proxy err 0.03728007152676582 tr(WHW.T) 201.599609375
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 15:39:00.881103 1590088 finetune.py:68] layer 19_down @ epoch 2 new loss 0.004902941174805164 old loss 0.0049057574942708015 BETTER
I0311 15:39:02.721212 1599780 finetune.py:45] layer 20_v initial loss 0.0005980739369988441
I0311 15:39:27.554830 1590088 finetune.py:68] layer 19_down @ epoch 3 new loss 0.00490031111985445 old loss 0.004902941174805164 BETTER
I0311 15:39:36.016546 1599780 finetune.py:68] layer 20_v @ epoch 0 new loss 0.0004923595697619021 old loss 0.0005980739369988441 BETTER
I0311 15:39:54.126592 1590088 finetune.py:68] layer 19_down @ epoch 4 new loss 0.004897888284176588 old loss 0.00490031111985445 BETTER
19_v proxy err 0.03940030187368393 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.006329081021249294 tr(WHW.T) 6958.40380859375
19_k proxy err 0.004211918450891972 tr(WHW.T) 10605.7626953125
19_o proxy err 0.04207388684153557 tr(WHW.T) 63.15861892700195
19_up proxy err 0.031495776027441025 tr(WHW.T) 2148.056396484375
19_gate proxy err 0.018904121592640877 tr(WHW.T) 3673.416015625
19_down proxy err 0.03643454983830452 tr(WHW.T) 226.2574462890625
I0311 15:40:07.827040 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 21 in 69.82157850265503s
I0311 15:40:10.407535 1599780 finetune.py:68] layer 20_v @ epoch 1 new loss 0.00046982726780697703 old loss 0.0004923595697619021 BETTER
I0311 15:40:11.065195 1600594 config.py:54] PyTorch version 2.1.1 available.
I0311 15:40:12.038728 1536994 quantize_finetune_llama.py:183] layer 22 gpu 2
I0311 15:40:12.112665 1600594 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 15:40:20.456053 1600594 finetune.py:45] layer 21_v initial loss 0.000514050479978323
I0311 15:40:44.997925 1599780 finetune.py:68] layer 20_v @ epoch 2 new loss 0.0004555706982500851 old loss 0.00046982726780697703 BETTER
I0311 15:40:51.813753 1600594 finetune.py:68] layer 21_v @ epoch 0 new loss 0.00044095265911892056 old loss 0.000514050479978323 BETTER
I0311 15:41:19.671521 1599780 finetune.py:68] layer 20_v @ epoch 3 new loss 0.0004450856940820813 old loss 0.0004555706982500851 BETTER
I0311 15:41:22.537333 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 22 in 70.16494131088257s
I0311 15:41:24.188881 1600594 finetune.py:68] layer 21_v @ epoch 1 new loss 0.0004251628997735679 old loss 0.00044095265911892056 BETTER
I0311 15:41:25.776942 1601381 config.py:54] PyTorch version 2.1.1 available.
I0311 15:41:26.773632 1536994 quantize_finetune_llama.py:183] layer 23 gpu 3
I0311 15:41:26.841790 1601381 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 15:41:35.271110 1601381 finetune.py:45] layer 22_v initial loss 0.0006403723964467645
I0311 15:41:54.521941 1599780 finetune.py:68] layer 20_v @ epoch 4 new loss 0.0004367475921753794 old loss 0.0004450856940820813 BETTER
I0311 15:41:56.721305 1600594 finetune.py:68] layer 21_v @ epoch 2 new loss 0.000414817885030061 old loss 0.0004251628997735679 BETTER
I0311 15:42:03.935333 1599780 finetune.py:45] layer 20_q initial loss 0.0005744384252466261
I0311 15:42:06.820425 1601381 finetune.py:68] layer 22_v @ epoch 0 new loss 0.0005486112786456943 old loss 0.0006403723964467645 BETTER
I0311 15:42:29.352486 1600594 finetune.py:68] layer 21_v @ epoch 3 new loss 0.00040721448021940887 old loss 0.000414817885030061 BETTER
I0311 15:42:37.346092 1599780 finetune.py:68] layer 20_q @ epoch 0 new loss 0.0005380018847063184 old loss 0.0005744384252466261 BETTER
I0311 15:42:38.351913 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 23 in 71.17746376991272s
I0311 15:42:39.222191 1601381 finetune.py:68] layer 22_v @ epoch 1 new loss 0.0005282931961119175 old loss 0.0005486112786456943 BETTER
I0311 15:42:41.544831 1602170 config.py:54] PyTorch version 2.1.1 available.
I0311 15:42:42.571239 1536994 quantize_finetune_llama.py:183] layer 24 gpu 0
I0311 15:42:42.640951 1602170 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 15:42:51.047777 1602170 finetune.py:45] layer 23_v initial loss 0.0006430287612602115
I0311 15:43:02.160063 1600594 finetune.py:68] layer 21_v @ epoch 4 new loss 0.0004012359422631562 old loss 0.00040721448021940887 BETTER
I0311 15:43:11.530220 1600594 finetune.py:45] layer 21_q initial loss 0.000518284214194864
I0311 15:43:11.677344 1599780 finetune.py:68] layer 20_q @ epoch 1 new loss 0.0005248402012512088 old loss 0.0005380018847063184 BETTER
I0311 15:43:12.212254 1601381 finetune.py:68] layer 22_v @ epoch 2 new loss 0.0005146979819983244 old loss 0.0005282931961119175 BETTER
I0311 15:43:22.218869 1602170 finetune.py:68] layer 23_v @ epoch 0 new loss 0.0005568735650740564 old loss 0.0006430287612602115 BETTER
I0311 15:43:43.380043 1600594 finetune.py:68] layer 21_q @ epoch 0 new loss 0.0004918593331240118 old loss 0.000518284214194864 BETTER
I0311 15:43:45.098653 1601381 finetune.py:68] layer 22_v @ epoch 3 new loss 0.0005047069280408323 old loss 0.0005146979819983244 BETTER
I0311 15:43:46.031949 1599780 finetune.py:68] layer 20_q @ epoch 2 new loss 0.0005151326768100262 old loss 0.0005248402012512088 BETTER
I0311 15:43:54.134456 1602170 finetune.py:68] layer 23_v @ epoch 1 new loss 0.0005380407674238086 old loss 0.0005568735650740564 BETTER
I0311 15:44:15.993993 1600594 finetune.py:68] layer 21_q @ epoch 1 new loss 0.00048197194701060653 old loss 0.0004918593331240118 BETTER
I0311 15:44:18.173279 1601381 finetune.py:68] layer 22_v @ epoch 4 new loss 0.0004966119886375964 old loss 0.0005047069280408323 BETTER
I0311 15:44:20.459459 1599780 finetune.py:68] layer 20_q @ epoch 3 new loss 0.0005071301129646599 old loss 0.0005151326768100262 BETTER
I0311 15:44:26.302890 1602170 finetune.py:68] layer 23_v @ epoch 2 new loss 0.0005257318844087422 old loss 0.0005380407674238086 BETTER
I0311 15:44:27.583183 1601381 finetune.py:45] layer 22_q initial loss 0.0006981647456996143
I0311 15:44:48.567755 1600594 finetune.py:68] layer 21_q @ epoch 2 new loss 0.0004746026825159788 old loss 0.00048197194701060653 BETTER
I0311 15:44:54.740240 1599780 finetune.py:68] layer 20_q @ epoch 4 new loss 0.0005006238352507353 old loss 0.0005071301129646599 BETTER
I0311 15:44:58.726288 1602170 finetune.py:68] layer 23_v @ epoch 3 new loss 0.0005163372843526304 old loss 0.0005257318844087422 BETTER
I0311 15:44:59.285817 1601381 finetune.py:68] layer 22_q @ epoch 0 new loss 0.0006497958092950284 old loss 0.0006981647456996143 BETTER
I0311 15:45:04.662130 1599780 finetune.py:45] layer 20_k initial loss 0.0006028215866535902
I0311 15:45:20.919096 1600594 finetune.py:68] layer 21_q @ epoch 3 new loss 0.0004685973690357059 old loss 0.0004746026825159788 BETTER
I0311 15:45:30.973263 1602170 finetune.py:68] layer 23_v @ epoch 4 new loss 0.0005087900208309293 old loss 0.0005163372843526304 BETTER
I0311 15:45:31.572543 1601381 finetune.py:68] layer 22_q @ epoch 1 new loss 0.0006336885853670537 old loss 0.0006497958092950284 BETTER
I0311 15:45:37.511931 1599780 finetune.py:68] layer 20_k @ epoch 0 new loss 0.0005858495132997632 old loss 0.0006028215866535902 BETTER
I0311 15:45:40.438745 1602170 finetune.py:45] layer 23_q initial loss 0.0006717011565342546
I0311 15:45:52.909035 1600594 finetune.py:68] layer 21_q @ epoch 4 new loss 0.0004636146768461913 old loss 0.0004685973690357059 BETTER
I0311 15:46:02.016475 1600594 finetune.py:45] layer 21_k initial loss 0.0005688091623596847
I0311 15:46:04.142466 1601381 finetune.py:68] layer 22_q @ epoch 2 new loss 0.0006213808082975447 old loss 0.0006336885853670537 BETTER
I0311 15:46:11.184133 1599780 finetune.py:68] layer 20_k @ epoch 1 new loss 0.0005790858995169401 old loss 0.0005858495132997632 BETTER
I0311 15:46:11.947031 1602170 finetune.py:68] layer 23_q @ epoch 0 new loss 0.0006338406819850206 old loss 0.0006717011565342546 BETTER
I0311 15:46:33.537450 1600594 finetune.py:68] layer 21_k @ epoch 0 new loss 0.0005580345168709755 old loss 0.0005688091623596847 BETTER
I0311 15:46:36.772307 1601381 finetune.py:68] layer 22_q @ epoch 3 new loss 0.0006114613497629762 old loss 0.0006213808082975447 BETTER
I0311 15:46:44.039717 1602170 finetune.py:68] layer 23_q @ epoch 1 new loss 0.0006203366792760789 old loss 0.0006338406819850206 BETTER
I0311 15:46:44.781157 1599780 finetune.py:68] layer 20_k @ epoch 2 new loss 0.0005734181613661349 old loss 0.0005790858995169401 BETTER
I0311 15:47:05.664538 1600594 finetune.py:68] layer 21_k @ epoch 1 new loss 0.0005525511223822832 old loss 0.0005580345168709755 BETTER
I0311 15:47:09.442642 1601381 finetune.py:68] layer 22_q @ epoch 4 new loss 0.0006031274679116905 old loss 0.0006114613497629762 BETTER
I0311 15:47:16.410761 1602170 finetune.py:68] layer 23_q @ epoch 2 new loss 0.0006108585512265563 old loss 0.0006203366792760789 BETTER
I0311 15:47:18.512093 1599780 finetune.py:68] layer 20_k @ epoch 3 new loss 0.0005685638752765954 old loss 0.0005734181613661349 BETTER
I0311 15:47:19.276987 1601381 finetune.py:45] layer 22_k initial loss 0.0007556631462648511
I0311 15:47:37.786684 1600594 finetune.py:68] layer 21_k @ epoch 2 new loss 0.0005479779792949557 old loss 0.0005525511223822832 BETTER
I0311 15:47:48.888026 1602170 finetune.py:68] layer 23_q @ epoch 3 new loss 0.0006030373624525964 old loss 0.0006108585512265563 BETTER
I0311 15:47:51.097551 1601381 finetune.py:68] layer 22_k @ epoch 0 new loss 0.0007369825034402311 old loss 0.0007556631462648511 BETTER
I0311 15:47:52.334219 1599780 finetune.py:68] layer 20_k @ epoch 4 new loss 0.0005643506883643568 old loss 0.0005685638752765954 BETTER
I0311 15:48:01.885104 1599780 finetune.py:45] layer 20_o initial loss 0.001323430915363133
I0311 15:48:09.915726 1600594 finetune.py:68] layer 21_k @ epoch 3 new loss 0.0005439174128696322 old loss 0.0005479779792949557 BETTER
I0311 15:48:21.245522 1602170 finetune.py:68] layer 23_q @ epoch 4 new loss 0.0005965263699181378 old loss 0.0006030373624525964 BETTER
I0311 15:48:23.340872 1601381 finetune.py:68] layer 22_k @ epoch 1 new loss 0.0007291187648661435 old loss 0.0007369825034402311 BETTER
I0311 15:48:30.654614 1602170 finetune.py:45] layer 23_k initial loss 0.0007379123126156628
I0311 15:48:34.332307 1599780 finetune.py:68] layer 20_o @ epoch 0 new loss 0.0012762793339788914 old loss 0.001323430915363133 BETTER
I0311 15:48:41.995899 1600594 finetune.py:68] layer 21_k @ epoch 4 new loss 0.0005404534749686718 old loss 0.0005439174128696322 BETTER
I0311 15:48:51.298272 1600594 finetune.py:45] layer 21_o initial loss 0.0011771742720156908
I0311 15:48:55.383587 1601381 finetune.py:68] layer 22_k @ epoch 2 new loss 0.0007225570734590292 old loss 0.0007291187648661435 BETTER
I0311 15:49:01.566390 1602170 finetune.py:68] layer 23_k @ epoch 0 new loss 0.0007272131624631584 old loss 0.0007379123126156628 BETTER
I0311 15:49:07.233532 1599780 finetune.py:68] layer 20_o @ epoch 1 new loss 0.0012535426067188382 old loss 0.0012762793339788914 BETTER
I0311 15:49:22.002322 1600594 finetune.py:68] layer 21_o @ epoch 0 new loss 0.0011482001282274723 old loss 0.0011771742720156908 BETTER
I0311 15:49:27.539793 1601381 finetune.py:68] layer 22_k @ epoch 3 new loss 0.0007168781012296677 old loss 0.0007225570734590292 BETTER
I0311 15:49:33.149864 1602170 finetune.py:68] layer 23_k @ epoch 1 new loss 0.000720816315151751 old loss 0.0007272131624631584 BETTER
I0311 15:49:40.297644 1599780 finetune.py:68] layer 20_o @ epoch 2 new loss 0.0012363336281850934 old loss 0.0012535426067188382 BETTER
I0311 15:49:53.354375 1600594 finetune.py:68] layer 21_o @ epoch 1 new loss 0.0011352163273841143 old loss 0.0011482001282274723 BETTER
I0311 15:49:59.690392 1601381 finetune.py:68] layer 22_k @ epoch 4 new loss 0.0007119340589269996 old loss 0.0007168781012296677 BETTER
I0311 15:50:04.750683 1602170 finetune.py:68] layer 23_k @ epoch 2 new loss 0.0007154779159463942 old loss 0.000720816315151751 BETTER
I0311 15:50:09.256582 1601381 finetune.py:45] layer 22_o initial loss 0.001495529431849718
I0311 15:50:13.507139 1599780 finetune.py:68] layer 20_o @ epoch 3 new loss 0.001222627586685121 old loss 0.0012363336281850934 BETTER
I0311 15:50:24.685682 1600594 finetune.py:68] layer 21_o @ epoch 2 new loss 0.00112529459875077 old loss 0.0011352163273841143 BETTER
I0311 15:50:36.670870 1602170 finetune.py:68] layer 23_k @ epoch 3 new loss 0.000710940221324563 old loss 0.0007154779159463942 BETTER
I0311 15:50:40.318089 1601381 finetune.py:68] layer 22_o @ epoch 0 new loss 0.001455893274396658 old loss 0.001495529431849718 BETTER
I0311 15:50:46.616046 1599780 finetune.py:68] layer 20_o @ epoch 4 new loss 0.001210725400596857 old loss 0.001222627586685121 BETTER
I0311 15:50:56.175213 1600594 finetune.py:68] layer 21_o @ epoch 3 new loss 0.0011173039674758911 old loss 0.00112529459875077 BETTER
I0311 15:51:02.028453 1599780 finetune.py:45] layer 20_up initial loss 0.0026669360231608152
I0311 15:51:08.440721 1602170 finetune.py:68] layer 23_k @ epoch 4 new loss 0.0007069332059472799 old loss 0.000710940221324563 BETTER
I0311 15:51:12.098166 1601381 finetune.py:68] layer 22_o @ epoch 1 new loss 0.0014372681034728885 old loss 0.001455893274396658 BETTER
I0311 15:51:17.975523 1602170 finetune.py:45] layer 23_o initial loss 0.0014126739697530866
I0311 15:51:27.685952 1600594 finetune.py:68] layer 21_o @ epoch 4 new loss 0.0011107329046353698 old loss 0.0011173039674758911 BETTER
I0311 15:51:32.380852 1599780 finetune.py:68] layer 20_up @ epoch 0 new loss 0.002615481847897172 old loss 0.0026669360231608152 BETTER
I0311 15:51:43.389278 1600594 finetune.py:45] layer 21_up initial loss 0.0026934901252388954
I0311 15:51:43.976198 1601381 finetune.py:68] layer 22_o @ epoch 2 new loss 0.0014224783517420292 old loss 0.0014372681034728885 BETTER
I0311 15:51:48.080204 1602170 finetune.py:68] layer 23_o @ epoch 0 new loss 0.0013833472039550543 old loss 0.0014126739697530866 BETTER
I0311 15:52:03.881300 1599780 finetune.py:68] layer 20_up @ epoch 1 new loss 0.0025850553065538406 old loss 0.002615481847897172 BETTER
I0311 15:52:12.312105 1600594 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0026480250526219606 old loss 0.0026934901252388954 BETTER
I0311 15:52:15.903095 1601381 finetune.py:68] layer 22_o @ epoch 3 new loss 0.0014104163274168968 old loss 0.0014224783517420292 BETTER
I0311 15:52:19.084202 1602170 finetune.py:68] layer 23_o @ epoch 1 new loss 0.0013691469794139266 old loss 0.0013833472039550543 BETTER
I0311 15:52:35.517331 1599780 finetune.py:68] layer 20_up @ epoch 2 new loss 0.00256131193600595 old loss 0.0025850553065538406 BETTER
I0311 15:52:42.083583 1600594 finetune.py:68] layer 21_up @ epoch 1 new loss 0.0026222183369100094 old loss 0.0026480250526219606 BETTER
I0311 15:52:47.952040 1601381 finetune.py:68] layer 22_o @ epoch 4 new loss 0.0014004093827679753 old loss 0.0014104163274168968 BETTER
I0311 15:52:50.181885 1602170 finetune.py:68] layer 23_o @ epoch 2 new loss 0.0013584275729954243 old loss 0.0013691469794139266 BETTER
I0311 15:53:03.204846 1601381 finetune.py:45] layer 22_up initial loss 0.0032047859858721495
I0311 15:53:07.200468 1599780 finetune.py:68] layer 20_up @ epoch 3 new loss 0.002541070803999901 old loss 0.00256131193600595 BETTER
I0311 15:53:11.876853 1600594 finetune.py:68] layer 21_up @ epoch 2 new loss 0.0026018894277513027 old loss 0.0026222183369100094 BETTER
I0311 15:53:21.123601 1602170 finetune.py:68] layer 23_o @ epoch 3 new loss 0.0013494094600901008 old loss 0.0013584275729954243 BETTER
I0311 15:53:32.143500 1601381 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0031568477861583233 old loss 0.0032047859858721495 BETTER
I0311 15:53:38.977166 1599780 finetune.py:68] layer 20_up @ epoch 4 new loss 0.0025239272508770227 old loss 0.002541070803999901 BETTER
I0311 15:53:41.725902 1600594 finetune.py:68] layer 21_up @ epoch 3 new loss 0.0025851218961179256 old loss 0.0026018894277513027 BETTER
I0311 15:53:52.149358 1602170 finetune.py:68] layer 23_o @ epoch 4 new loss 0.0013422784395515919 old loss 0.0013494094600901008 BETTER
I0311 15:53:54.573477 1599780 finetune.py:45] layer 20_gate initial loss 0.003678733017295599
I0311 15:54:01.955709 1601381 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0031281919218599796 old loss 0.0031568477861583233 BETTER
I0311 15:54:07.297646 1602170 finetune.py:45] layer 23_up initial loss 0.0033259433694183826
I0311 15:54:11.436156 1600594 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0025703543797135353 old loss 0.0025851218961179256 BETTER
I0311 15:54:23.642860 1599780 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.003652420360594988 old loss 0.003678733017295599 BETTER
I0311 15:54:26.574085 1600594 finetune.py:45] layer 21_gate initial loss 0.003813620889559388
I0311 15:54:32.137966 1601381 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0031057510059326887 old loss 0.0031281919218599796 BETTER
I0311 15:54:36.050136 1602170 finetune.py:68] layer 23_up @ epoch 0 new loss 0.003279993310570717 old loss 0.0033259433694183826 BETTER
I0311 15:54:53.582753 1599780 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0036317307967692614 old loss 0.003652420360594988 BETTER
I0311 15:54:54.361664 1600594 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0037907189689576626 old loss 0.003813620889559388 BETTER
I0311 15:55:02.354037 1601381 finetune.py:68] layer 22_up @ epoch 3 new loss 0.003086790908128023 old loss 0.0031057510059326887 BETTER
I0311 15:55:05.548410 1602170 finetune.py:68] layer 23_up @ epoch 1 new loss 0.003251408226788044 old loss 0.003279993310570717 BETTER
I0311 15:55:22.748494 1600594 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.003773049684241414 old loss 0.0037907189689576626 BETTER
I0311 15:55:23.517154 1599780 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.0036127143539488316 old loss 0.0036317307967692614 BETTER
I0311 15:55:32.712180 1601381 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0030706506222486496 old loss 0.003086790908128023 BETTER
I0311 15:55:35.215775 1602170 finetune.py:68] layer 23_up @ epoch 2 new loss 0.003229953581467271 old loss 0.003251408226788044 BETTER
I0311 15:55:48.181412 1601381 finetune.py:45] layer 22_gate initial loss 0.004486118908971548
I0311 15:55:51.184868 1600594 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.003756751073524356 old loss 0.003773049684241414 BETTER
I0311 15:55:53.620261 1599780 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0035958902444690466 old loss 0.0036127143539488316 BETTER
I0311 15:56:04.730859 1602170 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0032113336492329836 old loss 0.003229953581467271 BETTER
I0311 15:56:15.882740 1601381 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0044614169746637344 old loss 0.004486118908971548 BETTER
I0311 15:56:19.422028 1600594 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.003742283210158348 old loss 0.003756751073524356 BETTER
I0311 15:56:23.652482 1599780 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.003580023068934679 old loss 0.0035958902444690466 BETTER
I0311 15:56:34.242884 1602170 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0031956119928508997 old loss 0.0032113336492329836 BETTER
I0311 15:56:39.745683 1599780 finetune.py:45] layer 20_down initial loss 0.005926838144659996
I0311 15:56:44.025040 1601381 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.004441976081579924 old loss 0.0044614169746637344 BETTER
I0311 15:56:47.687999 1600594 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.003728926181793213 old loss 0.003742283210158348 BETTER
I0311 15:56:49.658566 1602170 finetune.py:45] layer 23_gate initial loss 0.004777360707521439
I0311 15:57:03.480487 1600594 finetune.py:45] layer 21_down initial loss 0.006143960170447826
I0311 15:57:06.974298 1599780 finetune.py:68] layer 20_down @ epoch 0 new loss 0.005922190845012665 old loss 0.005926838144659996 BETTER
I0311 15:57:12.478014 1601381 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.004424382001161575 old loss 0.004441976081579924 BETTER
I0311 15:57:16.729070 1602170 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.004754901397973299 old loss 0.004777360707521439 BETTER
I0311 15:57:29.614434 1600594 finetune.py:68] layer 21_down @ epoch 0 new loss 0.006139889359474182 old loss 0.006143960170447826 BETTER
I0311 15:57:35.294644 1599780 finetune.py:68] layer 20_down @ epoch 1 new loss 0.005917841102927923 old loss 0.005922190845012665 BETTER
I0311 15:57:40.949562 1601381 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.004408159293234348 old loss 0.004424382001161575 BETTER
I0311 15:57:44.938702 1602170 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.004736173432320356 old loss 0.004754901397973299 BETTER
I0311 15:57:56.570062 1600594 finetune.py:68] layer 21_down @ epoch 1 new loss 0.006136097013950348 old loss 0.006139889359474182 BETTER
I0311 15:58:03.725561 1599780 finetune.py:68] layer 20_down @ epoch 2 new loss 0.00591376144438982 old loss 0.005917841102927923 BETTER
I0311 15:58:09.193372 1601381 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.004393972456455231 old loss 0.004408159293234348 BETTER
I0311 15:58:13.131882 1602170 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.004719344899058342 old loss 0.004736173432320356 BETTER
I0311 15:58:23.566097 1600594 finetune.py:68] layer 21_down @ epoch 2 new loss 0.006132540293037891 old loss 0.006136097013950348 BETTER
I0311 15:58:25.332730 1601381 finetune.py:45] layer 22_down initial loss 0.007145529612898827
I0311 15:58:32.172930 1599780 finetune.py:68] layer 20_down @ epoch 3 new loss 0.005909930914640427 old loss 0.00591376144438982 BETTER
I0311 15:58:41.363580 1602170 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.004703708924353123 old loss 0.004719344899058342 BETTER
I0311 15:58:50.509665 1600594 finetune.py:68] layer 21_down @ epoch 3 new loss 0.006129248533397913 old loss 0.006132540293037891 BETTER
I0311 15:58:51.505175 1601381 finetune.py:68] layer 22_down @ epoch 0 new loss 0.007141202688217163 old loss 0.007145529612898827 BETTER
I0311 15:59:00.754654 1599780 finetune.py:68] layer 20_down @ epoch 4 new loss 0.005906385835260153 old loss 0.005909930914640427 BETTER
20_v proxy err 0.0412762351334095 tr(WHW.T) 990.5983276367188
20_q proxy err 0.006242011673748493 tr(WHW.T) 7159.43017578125
20_k proxy err 0.004335413686931133 tr(WHW.T) 10438.7890625
20_o proxy err 0.030421840026974678 tr(WHW.T) 101.3912124633789
20_up proxy err 0.03108850307762623 tr(WHW.T) 2339.262451171875
20_gate proxy err 0.01868932507932186 tr(WHW.T) 4005.98486328125
20_down proxy err 0.035910848528146744 tr(WHW.T) 279.022216796875
I0311 15:59:10.534583 1602170 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.00468983082100749 old loss 0.004703708924353123 BETTER
I0311 15:59:17.881024 1600594 finetune.py:68] layer 21_down @ epoch 4 new loss 0.006126132793724537 old loss 0.006129248533397913 BETTER
I0311 15:59:18.997977 1601381 finetune.py:68] layer 22_down @ epoch 1 new loss 0.007137134205549955 old loss 0.007141202688217163 BETTER
21_v proxy err 0.040393564850091934 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.0070396047085523605 tr(WHW.T) 7078.546875
21_k proxy err 0.005010839551687241 tr(WHW.T) 10026.41796875
21_o proxy err 0.034717291593551636 tr(WHW.T) 76.26946258544922
21_up proxy err 0.0327606201171875 tr(WHW.T) 2360.252685546875
21_gate proxy err 0.019992614164948463 tr(WHW.T) 3986.4833984375
21_down proxy err 0.03681646287441254 tr(WHW.T) 280.3690490722656
I0311 15:59:26.027348 1602170 finetune.py:45] layer 23_down initial loss 0.007502207066863775
I0311 15:59:45.911010 1601381 finetune.py:68] layer 22_down @ epoch 2 new loss 0.007133321836590767 old loss 0.007137134205549955 BETTER
I0311 15:59:51.512650 1602170 finetune.py:68] layer 23_down @ epoch 0 new loss 0.007498177699744701 old loss 0.007502207066863775 BETTER
I0311 16:00:12.703785 1601381 finetune.py:68] layer 22_down @ epoch 3 new loss 0.007129725068807602 old loss 0.007133321836590767 BETTER
I0311 16:00:17.693163 1602170 finetune.py:68] layer 23_down @ epoch 1 new loss 0.007494412828236818 old loss 0.007498177699744701 BETTER
I0311 16:00:35.147356 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 24 in 72.45715594291687s
I0311 16:00:38.494297 1611960 config.py:54] PyTorch version 2.1.1 available.
I0311 16:00:39.563901 1536994 quantize_finetune_llama.py:183] layer 25 gpu 1
I0311 16:00:39.628796 1611960 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 16:00:39.652194 1601381 finetune.py:68] layer 22_down @ epoch 4 new loss 0.007126326207071543 old loss 0.007129725068807602 BETTER
22_v proxy err 0.03830263391137123 tr(WHW.T) 1243.2529296875
22_q proxy err 0.0066377040930092335 tr(WHW.T) 7759.54052734375
22_k proxy err 0.0048833852633833885 tr(WHW.T) 10648.55859375
22_o proxy err 0.027708260342478752 tr(WHW.T) 115.4854965209961
22_up proxy err 0.03303239122033119 tr(WHW.T) 2475.44775390625
22_gate proxy err 0.02038896456360817 tr(WHW.T) 4142.22021484375
22_down proxy err 0.036871545016765594 tr(WHW.T) 315.7555236816406
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 16:00:44.227894 1602170 finetune.py:68] layer 23_down @ epoch 2 new loss 0.007490852382034063 old loss 0.007494412828236818 BETTER
I0311 16:00:47.993049 1611960 finetune.py:45] layer 24_v initial loss 0.000723765348084271
I0311 16:01:10.516077 1602170 finetune.py:68] layer 23_down @ epoch 3 new loss 0.007487575989216566 old loss 0.007490852382034063 BETTER
I0311 16:01:20.941786 1611960 finetune.py:68] layer 24_v @ epoch 0 new loss 0.000626296503469348 old loss 0.000723765348084271 BETTER
I0311 16:01:37.169684 1602170 finetune.py:68] layer 23_down @ epoch 4 new loss 0.007484497036784887 old loss 0.007487575989216566 BETTER
23_v proxy err 0.03632229566574097 tr(WHW.T) 1486.037353515625
23_q proxy err 0.0078093912452459335 tr(WHW.T) 7357.4482421875
23_k proxy err 0.005775608588010073 tr(WHW.T) 10028.6923828125
23_o proxy err 0.033433493226766586 tr(WHW.T) 86.15970611572266
23_up proxy err 0.03419233113527298 tr(WHW.T) 2534.245849609375
23_gate proxy err 0.021844688802957535 tr(WHW.T) 4087.8125
23_down proxy err 0.03683754801750183 tr(WHW.T) 325.2161865234375
I0311 16:01:53.614583 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 25 in 69.17639327049255s
I0311 16:01:55.227216 1611960 finetune.py:68] layer 24_v @ epoch 1 new loss 0.0006050567608326674 old loss 0.000626296503469348 BETTER
I0311 16:01:56.630921 1612766 config.py:54] PyTorch version 2.1.1 available.
I0311 16:01:57.614038 1536994 quantize_finetune_llama.py:183] layer 26 gpu 2
I0311 16:01:57.688842 1612766 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 16:02:05.674310 1612766 finetune.py:45] layer 25_v initial loss 0.0006656742189079523
I0311 16:02:29.773549 1611960 finetune.py:68] layer 24_v @ epoch 2 new loss 0.0005914914654567838 old loss 0.0006050567608326674 BETTER
I0311 16:02:36.986159 1612766 finetune.py:68] layer 25_v @ epoch 0 new loss 0.0005556317628361285 old loss 0.0006656742189079523 BETTER
I0311 16:03:04.382097 1611960 finetune.py:68] layer 24_v @ epoch 3 new loss 0.0005812383606098592 old loss 0.0005914914654567838 BETTER
I0311 16:03:07.432563 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 26 in 69.41203045845032s
I0311 16:03:09.174620 1612766 finetune.py:68] layer 25_v @ epoch 1 new loss 0.0005379894864745438 old loss 0.0005556317628361285 BETTER
I0311 16:03:10.727895 1613538 config.py:54] PyTorch version 2.1.1 available.
I0311 16:03:11.730834 1536994 quantize_finetune_llama.py:183] layer 27 gpu 3
I0311 16:03:11.797729 1613538 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 16:03:20.356665 1613538 finetune.py:45] layer 26_v initial loss 0.0010352294193580747
I0311 16:03:39.096187 1611960 finetune.py:68] layer 24_v @ epoch 4 new loss 0.0005733605357818305 old loss 0.0005812383606098592 BETTER
I0311 16:03:41.678752 1612766 finetune.py:68] layer 25_v @ epoch 2 new loss 0.0005271698464639485 old loss 0.0005379894864745438 BETTER
I0311 16:03:48.666638 1611960 finetune.py:45] layer 24_q initial loss 0.0007551867747679353
I0311 16:03:51.907916 1613538 finetune.py:68] layer 26_v @ epoch 0 new loss 0.0008790179272182286 old loss 0.0010352294193580747 BETTER
I0311 16:04:14.483207 1612766 finetune.py:68] layer 25_v @ epoch 3 new loss 0.0005192040698602796 old loss 0.0005271698464639485 BETTER
I0311 16:04:22.006411 1611960 finetune.py:68] layer 24_q @ epoch 0 new loss 0.0007237175013870001 old loss 0.0007551867747679353 BETTER
I0311 16:04:22.943615 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 27 in 70.78475880622864s
I0311 16:04:24.469863 1613538 finetune.py:68] layer 26_v @ epoch 1 new loss 0.0008516423986293375 old loss 0.0008790179272182286 BETTER
I0311 16:04:26.171390 1614343 config.py:54] PyTorch version 2.1.1 available.
I0311 16:04:27.153007 1536994 quantize_finetune_llama.py:183] layer 28 gpu 0
I0311 16:04:27.218432 1614343 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 16:04:35.261753 1614343 finetune.py:45] layer 27_v initial loss 0.000884927692823112
I0311 16:04:47.472235 1612766 finetune.py:68] layer 25_v @ epoch 4 new loss 0.0005131754442118108 old loss 0.0005192040698602796 BETTER
I0311 16:04:56.083603 1611960 finetune.py:68] layer 24_q @ epoch 1 new loss 0.0007103794487193227 old loss 0.0007237175013870001 BETTER
I0311 16:04:56.769315 1612766 finetune.py:45] layer 25_q initial loss 0.0007030636188574135
I0311 16:04:57.053003 1613538 finetune.py:68] layer 26_v @ epoch 2 new loss 0.0008326722891069949 old loss 0.0008516423986293375 BETTER
I0311 16:05:06.288384 1614343 finetune.py:68] layer 27_v @ epoch 0 new loss 0.0007693428779020905 old loss 0.000884927692823112 BETTER
I0311 16:05:28.462019 1612766 finetune.py:68] layer 25_q @ epoch 0 new loss 0.0006566632073372602 old loss 0.0007030636188574135 BETTER
I0311 16:05:29.994521 1613538 finetune.py:68] layer 26_v @ epoch 3 new loss 0.0008186331833712757 old loss 0.0008326722891069949 BETTER
I0311 16:05:30.360833 1611960 finetune.py:68] layer 24_q @ epoch 2 new loss 0.0007002706406638026 old loss 0.0007103794487193227 BETTER
I0311 16:05:38.179584 1614343 finetune.py:68] layer 27_v @ epoch 1 new loss 0.0007478820043615997 old loss 0.0007693428779020905 BETTER
I0311 16:06:00.804409 1612766 finetune.py:68] layer 25_q @ epoch 1 new loss 0.000644630054011941 old loss 0.0006566632073372602 BETTER
I0311 16:06:03.220771 1613538 finetune.py:68] layer 26_v @ epoch 4 new loss 0.0008075226214714348 old loss 0.0008186331833712757 BETTER
I0311 16:06:04.872057 1611960 finetune.py:68] layer 24_q @ epoch 3 new loss 0.0006920896121300757 old loss 0.0007002706406638026 BETTER
I0311 16:06:10.457966 1614343 finetune.py:68] layer 27_v @ epoch 2 new loss 0.0007331311935558915 old loss 0.0007478820043615997 BETTER
I0311 16:06:12.845379 1613538 finetune.py:45] layer 26_q initial loss 0.0010618441738188267
I0311 16:06:33.263308 1612766 finetune.py:68] layer 25_q @ epoch 2 new loss 0.0006360553670674562 old loss 0.000644630054011941 BETTER
I0311 16:06:38.991181 1611960 finetune.py:68] layer 24_q @ epoch 4 new loss 0.0006854207604192197 old loss 0.0006920896121300757 BETTER
I0311 16:06:42.910968 1614343 finetune.py:68] layer 27_v @ epoch 3 new loss 0.0007221025298349559 old loss 0.0007331311935558915 BETTER
I0311 16:06:44.496912 1613538 finetune.py:68] layer 26_q @ epoch 0 new loss 0.0010185992578044534 old loss 0.0010618441738188267 BETTER
I0311 16:06:48.647355 1611960 finetune.py:45] layer 24_k initial loss 0.0008468813030049205
I0311 16:07:05.470443 1612766 finetune.py:68] layer 25_q @ epoch 3 new loss 0.0006293674232438207 old loss 0.0006360553670674562 BETTER
I0311 16:07:15.456305 1614343 finetune.py:68] layer 27_v @ epoch 4 new loss 0.0007130090962164104 old loss 0.0007221025298349559 BETTER
I0311 16:07:16.822186 1613538 finetune.py:68] layer 26_q @ epoch 1 new loss 0.000999051844701171 old loss 0.0010185992578044534 BETTER
I0311 16:07:21.593490 1611960 finetune.py:68] layer 24_k @ epoch 0 new loss 0.0008345453534275293 old loss 0.0008468813030049205 BETTER
I0311 16:07:25.116176 1614343 finetune.py:45] layer 27_q initial loss 0.000994955305941403
I0311 16:07:37.683201 1612766 finetune.py:68] layer 25_q @ epoch 4 new loss 0.0006237396155484021 old loss 0.0006293674232438207 BETTER
I0311 16:07:46.658927 1612766 finetune.py:45] layer 25_k initial loss 0.0008008512668311596
I0311 16:07:49.557580 1613538 finetune.py:68] layer 26_q @ epoch 2 new loss 0.0009846710599958897 old loss 0.000999051844701171 BETTER
I0311 16:07:55.236573 1611960 finetune.py:68] layer 24_k @ epoch 1 new loss 0.000827521551400423 old loss 0.0008345453534275293 BETTER
I0311 16:07:56.468430 1614343 finetune.py:68] layer 27_q @ epoch 0 new loss 0.0009287609718739986 old loss 0.000994955305941403 BETTER
I0311 16:08:18.069911 1612766 finetune.py:68] layer 25_k @ epoch 0 new loss 0.0007856862503103912 old loss 0.0008008512668311596 BETTER
I0311 16:08:21.946068 1613538 finetune.py:68] layer 26_q @ epoch 3 new loss 0.000972936162725091 old loss 0.0009846710599958897 BETTER
I0311 16:08:29.812594 1614343 finetune.py:68] layer 27_q @ epoch 1 new loss 0.0009088715305551887 old loss 0.0009287609718739986 BETTER
I0311 16:08:30.129441 1611960 finetune.py:68] layer 24_k @ epoch 2 new loss 0.0008217174909077585 old loss 0.000827521551400423 BETTER
I0311 16:08:50.198561 1612766 finetune.py:68] layer 25_k @ epoch 1 new loss 0.0007788356160745025 old loss 0.0007856862503103912 BETTER
I0311 16:08:54.537302 1613538 finetune.py:68] layer 26_q @ epoch 4 new loss 0.0009630899876356125 old loss 0.000972936162725091 BETTER
I0311 16:09:02.387943 1614343 finetune.py:68] layer 27_q @ epoch 2 new loss 0.0008945165318436921 old loss 0.0009088715305551887 BETTER
I0311 16:09:03.874217 1611960 finetune.py:68] layer 24_k @ epoch 3 new loss 0.0008167022024281323 old loss 0.0008217174909077585 BETTER
I0311 16:09:04.392033 1613538 finetune.py:45] layer 26_k initial loss 0.0011918377131223679
I0311 16:09:22.330476 1612766 finetune.py:68] layer 25_k @ epoch 2 new loss 0.0007733994862064719 old loss 0.0007788356160745025 BETTER
I0311 16:09:34.852110 1614343 finetune.py:68] layer 27_q @ epoch 3 new loss 0.0008831806480884552 old loss 0.0008945165318436921 BETTER
I0311 16:09:36.133904 1613538 finetune.py:68] layer 26_k @ epoch 0 new loss 0.001170723931863904 old loss 0.0011918377131223679 BETTER
I0311 16:09:37.842829 1611960 finetune.py:68] layer 24_k @ epoch 4 new loss 0.0008122173603624105 old loss 0.0008167022024281323 BETTER
I0311 16:09:47.160210 1611960 finetune.py:45] layer 24_o initial loss 0.0016737922560423613
I0311 16:09:54.397352 1612766 finetune.py:68] layer 25_k @ epoch 3 new loss 0.0007687794859521091 old loss 0.0007733994862064719 BETTER
I0311 16:10:07.086058 1614343 finetune.py:68] layer 27_q @ epoch 4 new loss 0.000873730459716171 old loss 0.0008831806480884552 BETTER
I0311 16:10:08.274222 1613538 finetune.py:68] layer 26_k @ epoch 1 new loss 0.0011615226976573467 old loss 0.001170723931863904 BETTER
I0311 16:10:16.080367 1614343 finetune.py:45] layer 27_k initial loss 0.0011074305512011051
I0311 16:10:19.635810 1611960 finetune.py:68] layer 24_o @ epoch 0 new loss 0.001641209702938795 old loss 0.0016737922560423613 BETTER
I0311 16:10:26.449779 1612766 finetune.py:68] layer 25_k @ epoch 4 new loss 0.0007647714810445905 old loss 0.0007687794859521091 BETTER
I0311 16:10:35.665411 1612766 finetune.py:45] layer 25_o initial loss 0.001461040461435914
I0311 16:10:40.363827 1613538 finetune.py:68] layer 26_k @ epoch 2 new loss 0.0011539107654243708 old loss 0.0011615226976573467 BETTER
I0311 16:10:47.074210 1614343 finetune.py:68] layer 27_k @ epoch 0 new loss 0.001078915549442172 old loss 0.0011074305512011051 BETTER
I0311 16:10:52.924041 1611960 finetune.py:68] layer 24_o @ epoch 1 new loss 0.0016267532482743263 old loss 0.001641209702938795 BETTER
I0311 16:11:06.297792 1612766 finetune.py:68] layer 25_o @ epoch 0 new loss 0.0014365272363647819 old loss 0.001461040461435914 BETTER
I0311 16:11:12.493774 1613538 finetune.py:68] layer 26_k @ epoch 3 new loss 0.0011474533239379525 old loss 0.0011539107654243708 BETTER
I0311 16:11:19.064924 1614343 finetune.py:68] layer 27_k @ epoch 1 new loss 0.0010703103616833687 old loss 0.001078915549442172 BETTER
I0311 16:11:26.373432 1611960 finetune.py:68] layer 24_o @ epoch 2 new loss 0.0016153784235939384 old loss 0.0016267532482743263 BETTER
I0311 16:11:37.727695 1612766 finetune.py:68] layer 25_o @ epoch 1 new loss 0.0014247886138036847 old loss 0.0014365272363647819 BETTER
I0311 16:11:44.703230 1613538 finetune.py:68] layer 26_k @ epoch 4 new loss 0.0011413081083446741 old loss 0.0011474533239379525 BETTER
I0311 16:11:50.814048 1614343 finetune.py:68] layer 27_k @ epoch 2 new loss 0.0010632274206727743 old loss 0.0010703103616833687 BETTER
I0311 16:11:54.079376 1613538 finetune.py:45] layer 26_o initial loss 0.00218786858022213
I0311 16:11:59.767172 1611960 finetune.py:68] layer 24_o @ epoch 3 new loss 0.0016064628725871444 old loss 0.0016153784235939384 BETTER
I0311 16:12:09.155141 1612766 finetune.py:68] layer 25_o @ epoch 2 new loss 0.0014160540886223316 old loss 0.0014247886138036847 BETTER
I0311 16:12:22.596225 1614343 finetune.py:68] layer 27_k @ epoch 3 new loss 0.0010573535691946745 old loss 0.0010632274206727743 BETTER
I0311 16:12:25.152840 1613538 finetune.py:68] layer 26_o @ epoch 0 new loss 0.0021345196291804314 old loss 0.00218786858022213 BETTER
I0311 16:12:33.220031 1611960 finetune.py:68] layer 24_o @ epoch 4 new loss 0.0015990856336429715 old loss 0.0016064628725871444 BETTER
I0311 16:12:40.566673 1612766 finetune.py:68] layer 25_o @ epoch 3 new loss 0.001408913522027433 old loss 0.0014160540886223316 BETTER
I0311 16:12:48.399502 1611960 finetune.py:45] layer 24_up initial loss 0.0037449104711413383
I0311 16:12:54.613991 1614343 finetune.py:68] layer 27_k @ epoch 4 new loss 0.0010520898504182696 old loss 0.0010573535691946745 BETTER
I0311 16:12:56.780613 1613538 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0021144768688827753 old loss 0.0021345196291804314 BETTER
I0311 16:13:03.748617 1614343 finetune.py:45] layer 27_o initial loss 0.0019520219648256898
I0311 16:13:12.017142 1612766 finetune.py:68] layer 25_o @ epoch 4 new loss 0.0014030324527993798 old loss 0.001408913522027433 BETTER
I0311 16:13:18.961788 1611960 finetune.py:68] layer 24_up @ epoch 0 new loss 0.0036999378353357315 old loss 0.0037449104711413383 BETTER
I0311 16:13:26.749984 1612766 finetune.py:45] layer 25_up initial loss 0.0037963411305099726
I0311 16:13:28.551369 1613538 finetune.py:68] layer 26_o @ epoch 2 new loss 0.0020986099261790514 old loss 0.0021144768688827753 BETTER
I0311 16:13:33.982443 1614343 finetune.py:68] layer 27_o @ epoch 0 new loss 0.001907415920868516 old loss 0.0019520219648256898 BETTER
I0311 16:13:50.450139 1611960 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0036732337903231382 old loss 0.0036999378353357315 BETTER
I0311 16:13:57.120117 1612766 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0037463654298335314 old loss 0.0037963411305099726 BETTER
I0311 16:14:00.383607 1613538 finetune.py:68] layer 26_o @ epoch 3 new loss 0.002085872460156679 old loss 0.0020986099261790514 BETTER
I0311 16:14:05.033574 1614343 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0018857496324926615 old loss 0.001907415920868516 BETTER
I0311 16:14:22.292779 1611960 finetune.py:68] layer 24_up @ epoch 2 new loss 0.003652320709079504 old loss 0.0036732337903231382 BETTER
I0311 16:14:27.094967 1612766 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0037184767425060272 old loss 0.0037463654298335314 BETTER
I0311 16:14:32.091767 1613538 finetune.py:68] layer 26_o @ epoch 4 new loss 0.0020755729638040066 old loss 0.002085872460156679 BETTER
I0311 16:14:37.377604 1614343 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0018694885075092316 old loss 0.0018857496324926615 BETTER
I0311 16:14:47.207500 1613538 finetune.py:45] layer 26_up initial loss 0.004696906544268131
I0311 16:14:54.317955 1611960 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0036348223220556974 old loss 0.003652320709079504 BETTER
I0311 16:14:57.244381 1612766 finetune.py:68] layer 25_up @ epoch 2 new loss 0.0036962353624403477 old loss 0.0037184767425060272 BETTER
I0311 16:15:08.433660 1614343 finetune.py:68] layer 27_o @ epoch 3 new loss 0.0018565176287665963 old loss 0.0018694885075092316 BETTER
I0311 16:15:16.259043 1613538 finetune.py:68] layer 26_up @ epoch 0 new loss 0.00463716359809041 old loss 0.004696906544268131 BETTER
I0311 16:15:26.072624 1611960 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0036196953151375055 old loss 0.0036348223220556974 BETTER
I0311 16:15:26.995756 1612766 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0036776461638510227 old loss 0.0036962353624403477 BETTER
I0311 16:15:39.504443 1614343 finetune.py:68] layer 27_o @ epoch 4 new loss 0.0018457567784935236 old loss 0.0018565176287665963 BETTER
I0311 16:15:41.286008 1611960 finetune.py:45] layer 24_gate initial loss 0.005367084871977568
I0311 16:15:46.087508 1613538 finetune.py:68] layer 26_up @ epoch 1 new loss 0.004604465793818235 old loss 0.00463716359809041 BETTER
I0311 16:15:54.647566 1614343 finetune.py:45] layer 27_up initial loss 0.00481881620362401
I0311 16:15:58.372229 1612766 finetune.py:68] layer 25_up @ epoch 4 new loss 0.0036613999400287867 old loss 0.0036776461638510227 BETTER
I0311 16:16:10.497762 1611960 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.005345775280147791 old loss 0.005367084871977568 BETTER
I0311 16:16:16.132050 1612766 finetune.py:45] layer 25_gate initial loss 0.005593706388026476
I0311 16:16:16.926272 1613538 finetune.py:68] layer 26_up @ epoch 2 new loss 0.004578690975904465 old loss 0.004604465793818235 BETTER
I0311 16:16:23.941410 1614343 finetune.py:68] layer 27_up @ epoch 0 new loss 0.00473921000957489 old loss 0.00481881620362401 BETTER
I0311 16:16:41.326592 1611960 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.005327662918716669 old loss 0.005345775280147791 BETTER
I0311 16:16:45.295487 1612766 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.005571460351347923 old loss 0.005593706388026476 BETTER
I0311 16:16:48.240635 1613538 finetune.py:68] layer 26_up @ epoch 3 new loss 0.004557123873382807 old loss 0.004578690975904465 BETTER
I0311 16:16:55.050106 1614343 finetune.py:68] layer 27_up @ epoch 1 new loss 0.004697395954281092 old loss 0.00473921000957489 BETTER
I0311 16:17:12.493803 1611960 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.00531141459941864 old loss 0.005327662918716669 BETTER
I0311 16:17:15.351208 1612766 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.00555346580222249 old loss 0.005571460351347923 BETTER
I0311 16:17:19.622965 1613538 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0045381225645542145 old loss 0.004557123873382807 BETTER
I0311 16:17:25.553002 1614343 finetune.py:68] layer 27_up @ epoch 2 new loss 0.004665724467486143 old loss 0.004697395954281092 BETTER
I0311 16:17:38.678493 1613538 finetune.py:45] layer 26_gate initial loss 0.006711076479405165
I0311 16:17:43.938883 1611960 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.005296871066093445 old loss 0.00531141459941864 BETTER
I0311 16:17:45.408867 1612766 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.005537169985473156 old loss 0.00555346580222249 BETTER
I0311 16:17:55.240834 1614343 finetune.py:68] layer 27_up @ epoch 3 new loss 0.004639653488993645 old loss 0.004665724467486143 BETTER
I0311 16:18:06.382380 1613538 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.006683980114758015 old loss 0.006711076479405165 BETTER
I0311 16:18:14.045909 1612766 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.0055223992094397545 old loss 0.005537169985473156 BETTER
I0311 16:18:14.372276 1611960 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.005283115431666374 old loss 0.005296871066093445 BETTER
I0311 16:18:24.753953 1614343 finetune.py:68] layer 27_up @ epoch 4 new loss 0.00461750291287899 old loss 0.004639653488993645 BETTER
I0311 16:18:30.487291 1611960 finetune.py:45] layer 24_down initial loss 0.008236996829509735
I0311 16:18:34.435504 1613538 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.006662988103926182 old loss 0.006683980114758015 BETTER
I0311 16:18:40.165285 1614343 finetune.py:45] layer 27_gate initial loss 0.0071127149276435375
I0311 16:18:42.194952 1612766 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.005508885253220797 old loss 0.0055223992094397545 BETTER
I0311 16:18:57.845726 1611960 finetune.py:68] layer 24_down @ epoch 0 new loss 0.00823299027979374 old loss 0.008236996829509735 BETTER
I0311 16:18:58.103130 1612766 finetune.py:45] layer 25_down initial loss 0.008590232580900192
I0311 16:19:02.726352 1613538 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.006644250359386206 old loss 0.006662988103926182 BETTER
I0311 16:19:07.506420 1614343 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0070779453963041306 old loss 0.0071127149276435375 BETTER
I0311 16:19:24.110810 1612766 finetune.py:68] layer 25_down @ epoch 0 new loss 0.008586579002439976 old loss 0.008590232580900192 BETTER
I0311 16:19:26.371119 1611960 finetune.py:68] layer 24_down @ epoch 1 new loss 0.00822928361594677 old loss 0.00823299027979374 BETTER
I0311 16:19:31.102201 1613538 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.006627026479691267 old loss 0.006644250359386206 BETTER
I0311 16:19:35.566707 1614343 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.007052513770759106 old loss 0.0070779453963041306 BETTER
I0311 16:19:51.024524 1612766 finetune.py:68] layer 25_down @ epoch 1 new loss 0.008583166636526585 old loss 0.008586579002439976 BETTER
I0311 16:19:54.972300 1611960 finetune.py:68] layer 24_down @ epoch 2 new loss 0.008225813508033752 old loss 0.00822928361594677 BETTER
I0311 16:19:59.462878 1613538 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.006611625663936138 old loss 0.006627026479691267 BETTER
I0311 16:20:03.679990 1614343 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.00703031150624156 old loss 0.007052513770759106 BETTER
I0311 16:20:15.424679 1613538 finetune.py:45] layer 26_down initial loss 0.010013877414166927
I0311 16:20:17.899709 1612766 finetune.py:68] layer 25_down @ epoch 2 new loss 0.008580018766224384 old loss 0.008583166636526585 BETTER
I0311 16:20:23.614782 1611960 finetune.py:68] layer 24_down @ epoch 3 new loss 0.008222595788538456 old loss 0.008225813508033752 BETTER
I0311 16:20:31.816362 1614343 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.007009997498244047 old loss 0.00703031150624156 BETTER
I0311 16:20:41.692626 1613538 finetune.py:68] layer 26_down @ epoch 0 new loss 0.010009868070483208 old loss 0.010013877414166927 BETTER
I0311 16:20:44.733758 1612766 finetune.py:68] layer 25_down @ epoch 3 new loss 0.008577050641179085 old loss 0.008580018766224384 BETTER
I0311 16:20:52.101045 1611960 finetune.py:68] layer 24_down @ epoch 4 new loss 0.008219550363719463 old loss 0.008222595788538456 BETTER
24_v proxy err 0.037304703146219254 tr(WHW.T) 1394.900634765625
24_q proxy err 0.007812194991856813 tr(WHW.T) 7031.6953125
24_k proxy err 0.005331703927367926 tr(WHW.T) 10375.625
24_o proxy err 0.025963231921195984 tr(WHW.T) 135.12815856933594
24_up proxy err 0.03472987934947014 tr(WHW.T) 2623.0439453125
24_gate proxy err 0.02204018644988537 tr(WHW.T) 4254.1962890625
24_down proxy err 0.036495141685009 tr(WHW.T) 344.1007995605469
I0311 16:21:00.899245 1614343 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.006991836242377758 old loss 0.007009997498244047 BETTER
I0311 16:21:09.401142 1613538 finetune.py:68] layer 26_down @ epoch 1 new loss 0.010006221942603588 old loss 0.010009868070483208 BETTER
I0311 16:21:12.200173 1612766 finetune.py:68] layer 25_down @ epoch 4 new loss 0.008574261330068111 old loss 0.008577050641179085 BETTER
25_v proxy err 0.035663578659296036 tr(WHW.T) 1707.664794921875
25_q proxy err 0.008922163397073746 tr(WHW.T) 7172.78955078125
25_k proxy err 0.006669242400676012 tr(WHW.T) 9655.5107421875
25_o proxy err 0.03308733180165291 tr(WHW.T) 84.49485778808594
25_up proxy err 0.0344255156815052 tr(WHW.T) 2804.75537109375
25_gate proxy err 0.02133563533425331 tr(WHW.T) 4652.5185546875
25_down proxy err 0.03486227989196777 tr(WHW.T) 377.815673828125
I0311 16:21:16.991030 1614343 finetune.py:45] layer 27_down initial loss 0.010853518731892109
I0311 16:21:36.499068 1613538 finetune.py:68] layer 26_down @ epoch 2 new loss 0.01000278815627098 old loss 0.010006221942603588 BETTER
I0311 16:21:42.409065 1614343 finetune.py:68] layer 27_down @ epoch 0 new loss 0.010849494487047195 old loss 0.010853518731892109 BETTER
I0311 16:22:03.247938 1613538 finetune.py:68] layer 26_down @ epoch 3 new loss 0.009999576956033707 old loss 0.01000278815627098 BETTER
I0311 16:22:08.860731 1614343 finetune.py:68] layer 27_down @ epoch 1 new loss 0.01084580086171627 old loss 0.010849494487047195 BETTER
I0311 16:22:29.270956 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 28 in 71.83440637588501s
I0311 16:22:30.221044 1613538 finetune.py:68] layer 26_down @ epoch 4 new loss 0.009996612556278706 old loss 0.009999576956033707 BETTER
26_v proxy err 0.034871187061071396 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.008168192580342293 tr(WHW.T) 7482.02978515625
26_k proxy err 0.005861873738467693 tr(WHW.T) 10541.9140625
26_o proxy err 0.020645413547754288 tr(WHW.T) 204.6439208984375
26_up proxy err 0.032326411455869675 tr(WHW.T) 3157.627685546875
26_gate proxy err 0.019877836108207703 tr(WHW.T) 5282.8017578125
26_down proxy err 0.03582771122455597 tr(WHW.T) 406.22039794921875
I0311 16:22:32.457977 1624142 config.py:54] PyTorch version 2.1.1 available.
I0311 16:22:33.518389 1536994 quantize_finetune_llama.py:183] layer 29 gpu 1
I0311 16:22:33.595975 1624142 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0311 16:22:35.157292 1614343 finetune.py:68] layer 27_down @ epoch 2 new loss 0.010842306539416313 old loss 0.01084580086171627 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 16:22:42.116448 1624142 finetune.py:45] layer 28_v initial loss 0.001470252638682723
I0311 16:23:01.587522 1614343 finetune.py:68] layer 27_down @ epoch 3 new loss 0.010839113034307957 old loss 0.010842306539416313 BETTER
I0311 16:23:15.102216 1624142 finetune.py:68] layer 28_v @ epoch 0 new loss 0.001029341947287321 old loss 0.001470252638682723 BETTER
I0311 16:23:28.439710 1614343 finetune.py:68] layer 27_down @ epoch 4 new loss 0.010836079716682434 old loss 0.010839113034307957 BETTER
27_v proxy err 0.03370501473546028 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.008418211713433266 tr(WHW.T) 7697.06396484375
27_k proxy err 0.0061556207947432995 tr(WHW.T) 10681.6298828125
27_o proxy err 0.027562731876969337 tr(WHW.T) 127.93907165527344
27_up proxy err 0.02949732355773449 tr(WHW.T) 3687.431640625
27_gate proxy err 0.018709763884544373 tr(WHW.T) 5962.8564453125
27_down proxy err 0.03513258323073387 tr(WHW.T) 473.1248779296875
I0311 16:23:45.572945 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 29 in 71.1293535232544s
I0311 16:23:48.737031 1624928 config.py:54] PyTorch version 2.1.1 available.
I0311 16:23:49.415630 1624142 finetune.py:68] layer 28_v @ epoch 1 new loss 0.000988817890174687 old loss 0.001029341947287321 BETTER
I0311 16:23:49.685705 1536994 quantize_finetune_llama.py:183] layer 30 gpu 2
I0311 16:23:49.752432 1624928 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 16:23:57.849701 1624928 finetune.py:45] layer 29_v initial loss 0.0014714414719492197
I0311 16:24:23.943923 1624142 finetune.py:68] layer 28_v @ epoch 2 new loss 0.0009648274863138795 old loss 0.000988817890174687 BETTER
I0311 16:24:29.195961 1624928 finetune.py:68] layer 29_v @ epoch 0 new loss 0.001135165337473154 old loss 0.0014714414719492197 BETTER
I0311 16:24:58.135024 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 30 in 68.06640577316284s
I0311 16:24:58.771320 1624142 finetune.py:68] layer 28_v @ epoch 3 new loss 0.0009469606447964907 old loss 0.0009648274863138795 BETTER
I0311 16:25:01.275472 1625689 config.py:54] PyTorch version 2.1.1 available.
I0311 16:25:01.292346 1624928 finetune.py:68] layer 29_v @ epoch 1 new loss 0.001094407751224935 old loss 0.001135165337473154 BETTER
I0311 16:25:02.292966 1536994 quantize_finetune_llama.py:183] layer 31 gpu 3
I0311 16:25:02.368602 1625689 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 16:25:10.457869 1625689 finetune.py:45] layer 30_v initial loss 0.0015634740702807903
I0311 16:25:33.680972 1624142 finetune.py:68] layer 28_v @ epoch 4 new loss 0.0009329327149316669 old loss 0.0009469606447964907 BETTER
I0311 16:25:33.958511 1624928 finetune.py:68] layer 29_v @ epoch 2 new loss 0.0010682397987693548 old loss 0.001094407751224935 BETTER
I0311 16:25:42.266115 1625689 finetune.py:68] layer 30_v @ epoch 0 new loss 0.001088671269826591 old loss 0.0015634740702807903 BETTER
I0311 16:25:43.339121 1624142 finetune.py:45] layer 28_q initial loss 0.0012695805635303259
I0311 16:26:06.673902 1624928 finetune.py:68] layer 29_v @ epoch 3 new loss 0.0010496943723410368 old loss 0.0010682397987693548 BETTER
I0311 16:26:14.012991 1536994 quantize_finetune_llama.py:210] computed original embedding for layer 31 in 71.27741742134094s
I0311 16:26:14.709511 1625689 finetune.py:68] layer 30_v @ epoch 1 new loss 0.0010462894570082426 old loss 0.001088671269826591 BETTER
I0311 16:26:16.820387 1624142 finetune.py:68] layer 28_q @ epoch 0 new loss 0.0011918899836018682 old loss 0.0012695805635303259 BETTER
I0311 16:26:17.331528 1626490 config.py:54] PyTorch version 2.1.1 available.
I0311 16:26:18.439465 1626490 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0311 16:26:26.562785 1626490 finetune.py:45] layer 31_v initial loss 0.0026420955546200275
I0311 16:26:39.484555 1624928 finetune.py:68] layer 29_v @ epoch 4 new loss 0.0010345096234232187 old loss 0.0010496943723410368 BETTER
I0311 16:26:47.657889 1625689 finetune.py:68] layer 30_v @ epoch 2 new loss 0.0010199378011748195 old loss 0.0010462894570082426 BETTER
I0311 16:26:48.790786 1624928 finetune.py:45] layer 29_q initial loss 0.0013538184575736523
I0311 16:26:51.025193 1624142 finetune.py:68] layer 28_q @ epoch 1 new loss 0.0011676254216581583 old loss 0.0011918899836018682 BETTER
I0311 16:26:57.585767 1626490 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0016915727173909545 old loss 0.0026420955546200275 BETTER
I0311 16:27:20.430873 1624928 finetune.py:68] layer 29_q @ epoch 0 new loss 0.0012764568673446774 old loss 0.0013538184575736523 BETTER
I0311 16:27:20.879799 1625689 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0010008919052779675 old loss 0.0010199378011748195 BETTER
I0311 16:27:25.486221 1624142 finetune.py:68] layer 28_q @ epoch 2 new loss 0.001149676856584847 old loss 0.0011676254216581583 BETTER
I0311 16:27:29.940828 1626490 finetune.py:68] layer 31_v @ epoch 1 new loss 0.0015927593922242522 old loss 0.0016915727173909545 BETTER
I0311 16:27:52.660613 1624928 finetune.py:68] layer 29_q @ epoch 1 new loss 0.0012521803146228194 old loss 0.0012764568673446774 BETTER
I0311 16:27:53.979337 1625689 finetune.py:68] layer 30_v @ epoch 4 new loss 0.000986682833172381 old loss 0.0010008919052779675 BETTER
I0311 16:28:00.311459 1624142 finetune.py:68] layer 28_q @ epoch 3 new loss 0.0011350108543410897 old loss 0.001149676856584847 BETTER
I0311 16:28:02.295355 1626490 finetune.py:68] layer 31_v @ epoch 2 new loss 0.0015289174625650048 old loss 0.0015927593922242522 BETTER
I0311 16:28:03.990584 1625689 finetune.py:45] layer 30_q initial loss 0.001544338185340166
I0311 16:28:24.792467 1624928 finetune.py:68] layer 29_q @ epoch 2 new loss 0.0012341799447312951 old loss 0.0012521803146228194 BETTER
I0311 16:28:35.053221 1626490 finetune.py:68] layer 31_v @ epoch 3 new loss 0.001493659452535212 old loss 0.0015289174625650048 BETTER
I0311 16:28:35.209196 1624142 finetune.py:68] layer 28_q @ epoch 4 new loss 0.0011228312505409122 old loss 0.0011350108543410897 BETTER
I0311 16:28:36.026754 1625689 finetune.py:68] layer 30_q @ epoch 0 new loss 0.001317522139288485 old loss 0.001544338185340166 BETTER
I0311 16:28:44.549643 1624142 finetune.py:45] layer 28_k initial loss 0.0014318390749394894
I0311 16:28:57.202115 1624928 finetune.py:68] layer 29_q @ epoch 3 new loss 0.0012201210483908653 old loss 0.0012341799447312951 BETTER
I0311 16:29:07.778153 1626490 finetune.py:68] layer 31_v @ epoch 4 new loss 0.00146143336314708 old loss 0.001493659452535212 BETTER
I0311 16:29:08.657929 1625689 finetune.py:68] layer 30_q @ epoch 1 new loss 0.0012836827663704753 old loss 0.001317522139288485 BETTER
I0311 16:29:17.297634 1624142 finetune.py:68] layer 28_k @ epoch 0 new loss 0.0013936037430539727 old loss 0.0014318390749394894 BETTER
I0311 16:29:17.446874 1626490 finetune.py:45] layer 31_q initial loss 0.004313416313380003
I0311 16:29:29.851900 1624928 finetune.py:68] layer 29_q @ epoch 4 new loss 0.001208337489515543 old loss 0.0012201210483908653 BETTER
I0311 16:29:39.048670 1624928 finetune.py:45] layer 29_k initial loss 0.0015057373093441129
I0311 16:29:41.247229 1625689 finetune.py:68] layer 30_q @ epoch 2 new loss 0.0012609767727553844 old loss 0.0012836827663704753 BETTER
I0311 16:29:48.898150 1626490 finetune.py:68] layer 31_q @ epoch 0 new loss 0.002714760834351182 old loss 0.004313416313380003 BETTER
I0311 16:29:50.988129 1624142 finetune.py:68] layer 28_k @ epoch 1 new loss 0.0013818840961903334 old loss 0.0013936037430539727 BETTER
I0311 16:30:10.749953 1624928 finetune.py:68] layer 29_k @ epoch 0 new loss 0.0014772003050893545 old loss 0.0015057373093441129 BETTER
I0311 16:30:14.096390 1625689 finetune.py:68] layer 30_q @ epoch 3 new loss 0.001242502243258059 old loss 0.0012609767727553844 BETTER
I0311 16:30:21.538667 1626490 finetune.py:68] layer 31_q @ epoch 1 new loss 0.002596193226054311 old loss 0.002714760834351182 BETTER
I0311 16:30:25.305029 1624142 finetune.py:68] layer 28_k @ epoch 2 new loss 0.0013727005571126938 old loss 0.0013818840961903334 BETTER
I0311 16:30:44.975856 1624928 finetune.py:68] layer 29_k @ epoch 1 new loss 0.001465753186494112 old loss 0.0014772003050893545 BETTER
I0311 16:30:47.865337 1625689 finetune.py:68] layer 30_q @ epoch 4 new loss 0.0012273970060050488 old loss 0.001242502243258059 BETTER
I0311 16:30:53.952886 1626490 finetune.py:68] layer 31_q @ epoch 2 new loss 0.002521534450352192 old loss 0.002596193226054311 BETTER
I0311 16:30:59.514196 1625689 finetune.py:45] layer 30_k initial loss 0.0016931187128648162
I0311 16:31:00.411646 1624142 finetune.py:68] layer 28_k @ epoch 3 new loss 0.0013648868771269917 old loss 0.0013727005571126938 BETTER
I0311 16:31:18.134332 1624928 finetune.py:68] layer 29_k @ epoch 2 new loss 0.001456105150282383 old loss 0.001465753186494112 BETTER
I0311 16:31:28.280156 1626490 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0024638324975967407 old loss 0.002521534450352192 BETTER
I0311 16:31:33.593399 1625689 finetune.py:68] layer 30_k @ epoch 0 new loss 0.0015679501229897141 old loss 0.0016931187128648162 BETTER
I0311 16:31:36.301907 1624142 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0013577728532254696 old loss 0.0013648868771269917 BETTER
I0311 16:31:47.609006 1624142 finetune.py:45] layer 28_o initial loss 0.002475489629432559
I0311 16:31:50.668597 1624928 finetune.py:68] layer 29_k @ epoch 3 new loss 0.001447973307222128 old loss 0.001456105150282383 BETTER
I0311 16:32:01.100604 1626490 finetune.py:68] layer 31_q @ epoch 4 new loss 0.0024129212833940983 old loss 0.0024638324975967407 BETTER
I0311 16:32:05.971799 1625689 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0015514985425397754 old loss 0.0015679501229897141 BETTER
I0311 16:32:10.491050 1626490 finetune.py:45] layer 31_k initial loss 0.003915322013199329
I0311 16:32:19.528053 1624142 finetune.py:68] layer 28_o @ epoch 0 new loss 0.002406826475635171 old loss 0.002475489629432559 BETTER
I0311 16:32:22.719040 1624928 finetune.py:68] layer 29_k @ epoch 4 new loss 0.001440847641788423 old loss 0.001447973307222128 BETTER
I0311 16:32:31.961973 1624928 finetune.py:45] layer 29_o initial loss 0.0025725909508764744
I0311 16:32:38.153639 1625689 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0015393447829410434 old loss 0.0015514985425397754 BETTER
I0311 16:32:41.286792 1626490 finetune.py:68] layer 31_k @ epoch 0 new loss 0.002952235983684659 old loss 0.003915322013199329 BETTER
I0311 16:32:52.600760 1624142 finetune.py:68] layer 28_o @ epoch 1 new loss 0.002381301252171397 old loss 0.002406826475635171 BETTER
I0311 16:33:02.664503 1624928 finetune.py:68] layer 29_o @ epoch 0 new loss 0.002501083305105567 old loss 0.0025725909508764744 BETTER
I0311 16:33:10.222175 1625689 finetune.py:68] layer 30_k @ epoch 3 new loss 0.0015294195618480444 old loss 0.0015393447829410434 BETTER
I0311 16:33:12.934185 1626490 finetune.py:68] layer 31_k @ epoch 1 new loss 0.002885986352339387 old loss 0.002952235983684659 BETTER
I0311 16:33:25.922599 1624142 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0023616997059434652 old loss 0.002381301252171397 BETTER
I0311 16:33:34.275301 1624928 finetune.py:68] layer 29_o @ epoch 1 new loss 0.002477015135809779 old loss 0.002501083305105567 BETTER
I0311 16:33:42.394752 1625689 finetune.py:68] layer 30_k @ epoch 4 new loss 0.001520298421382904 old loss 0.0015294195618480444 BETTER
I0311 16:33:44.786970 1626490 finetune.py:68] layer 31_k @ epoch 2 new loss 0.0028411441016942263 old loss 0.002885986352339387 BETTER
I0311 16:33:51.654774 1625689 finetune.py:45] layer 30_o initial loss 0.002868145238608122
I0311 16:33:59.182987 1624142 finetune.py:68] layer 28_o @ epoch 3 new loss 0.0023462562821805477 old loss 0.0023616997059434652 BETTER
I0311 16:34:05.844288 1624928 finetune.py:68] layer 29_o @ epoch 2 new loss 0.002459369832649827 old loss 0.002477015135809779 BETTER
I0311 16:34:16.559108 1626490 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0028062150813639164 old loss 0.0028411441016942263 BETTER
I0311 16:34:22.637891 1625689 finetune.py:68] layer 30_o @ epoch 0 new loss 0.002737758681178093 old loss 0.002868145238608122 BETTER
I0311 16:34:32.403354 1624142 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0023328897077590227 old loss 0.0023462562821805477 BETTER
I0311 16:34:37.476782 1624928 finetune.py:68] layer 29_o @ epoch 3 new loss 0.002445379039272666 old loss 0.002459369832649827 BETTER
I0311 16:34:47.709447 1624142 finetune.py:45] layer 28_up initial loss 0.005815127864480019
I0311 16:34:48.309216 1626490 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0027803918346762657 old loss 0.0028062150813639164 BETTER
I0311 16:34:54.370768 1625689 finetune.py:68] layer 30_o @ epoch 1 new loss 0.002690619323402643 old loss 0.002737758681178093 BETTER
I0311 16:34:57.265365 1626490 finetune.py:45] layer 31_o initial loss 0.004909228533506393
I0311 16:35:09.105963 1624928 finetune.py:68] layer 29_o @ epoch 4 new loss 0.0024339458905160427 old loss 0.002445379039272666 BETTER
I0311 16:35:18.178069 1624142 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0057001360692083836 old loss 0.005815127864480019 BETTER
I0311 16:35:23.879127 1624928 finetune.py:45] layer 29_up initial loss 0.0065590995363891125
I0311 16:35:26.140969 1625689 finetune.py:68] layer 30_o @ epoch 2 new loss 0.002657964127138257 old loss 0.002690619323402643 BETTER
I0311 16:35:27.470611 1626490 finetune.py:68] layer 31_o @ epoch 0 new loss 0.004351934418082237 old loss 0.004909228533506393 BETTER
I0311 16:35:52.260569 1624142 finetune.py:68] layer 28_up @ epoch 1 new loss 0.005644968245178461 old loss 0.0057001360692083836 BETTER
I0311 16:35:55.876259 1624928 finetune.py:68] layer 29_up @ epoch 0 new loss 0.006377690006047487 old loss 0.0065590995363891125 BETTER
I0311 16:36:00.674514 1625689 finetune.py:68] layer 30_o @ epoch 3 new loss 0.002632258692756295 old loss 0.002657964127138257 BETTER
I0311 16:36:00.967712 1626490 finetune.py:68] layer 31_o @ epoch 1 new loss 0.004229474347084761 old loss 0.004351934418082237 BETTER
I0311 16:36:25.630652 1624142 finetune.py:68] layer 28_up @ epoch 2 new loss 0.00560370460152626 old loss 0.005644968245178461 BETTER
I0311 16:36:27.929690 1624928 finetune.py:68] layer 29_up @ epoch 1 new loss 0.006303874775767326 old loss 0.006377690006047487 BETTER
I0311 16:36:34.632826 1626490 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0041521633975207806 old loss 0.004229474347084761 BETTER
I0311 16:36:34.973621 1625689 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0026124780997633934 old loss 0.002632258692756295 BETTER
I0311 16:36:54.286782 1625689 finetune.py:45] layer 30_up initial loss 0.009405023418366909
I0311 16:36:59.741177 1624142 finetune.py:68] layer 28_up @ epoch 3 new loss 0.0055700126104056835 old loss 0.00560370460152626 BETTER
I0311 16:37:00.292716 1624928 finetune.py:68] layer 29_up @ epoch 2 new loss 0.006251263432204723 old loss 0.006303874775767326 BETTER
I0311 16:37:07.318359 1626490 finetune.py:68] layer 31_o @ epoch 3 new loss 0.00409414479508996 old loss 0.0041521633975207806 BETTER
I0311 16:37:24.518365 1625689 finetune.py:68] layer 30_up @ epoch 0 new loss 0.008766382932662964 old loss 0.009405023418366909 BETTER
I0311 16:37:31.503523 1624928 finetune.py:68] layer 29_up @ epoch 3 new loss 0.006208185106515884 old loss 0.006251263432204723 BETTER
I0311 16:37:32.199294 1624142 finetune.py:68] layer 28_up @ epoch 4 new loss 0.005540945567190647 old loss 0.0055700126104056835 BETTER
I0311 16:37:38.577196 1626490 finetune.py:68] layer 31_o @ epoch 4 new loss 0.004048873670399189 old loss 0.00409414479508996 BETTER
I0311 16:37:47.328778 1624142 finetune.py:45] layer 28_gate initial loss 0.008524547331035137
I0311 16:37:53.673309 1626490 finetune.py:45] layer 31_up initial loss 0.024272840470075607
I0311 16:37:54.656038 1625689 finetune.py:68] layer 30_up @ epoch 1 new loss 0.008572149090468884 old loss 0.008766382932662964 BETTER
I0311 16:38:01.473664 1624928 finetune.py:68] layer 29_up @ epoch 4 new loss 0.006171770393848419 old loss 0.006208185106515884 BETTER
I0311 16:38:16.263305 1624142 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.008473960682749748 old loss 0.008524547331035137 BETTER
I0311 16:38:16.387034 1624928 finetune.py:45] layer 29_gate initial loss 0.009725837036967278
I0311 16:38:22.207253 1626490 finetune.py:68] layer 31_up @ epoch 0 new loss 0.019353080540895462 old loss 0.024272840470075607 BETTER
I0311 16:38:24.765865 1625689 finetune.py:68] layer 30_up @ epoch 2 new loss 0.008440334349870682 old loss 0.008572149090468884 BETTER
I0311 16:38:43.941136 1624928 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.009653385728597641 old loss 0.009725837036967278 BETTER
I0311 16:38:46.040444 1624142 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.008439119905233383 old loss 0.008473960682749748 BETTER
I0311 16:38:51.715320 1626490 finetune.py:68] layer 31_up @ epoch 1 new loss 0.018221542239189148 old loss 0.019353080540895462 BETTER
I0311 16:38:54.717629 1625689 finetune.py:68] layer 30_up @ epoch 3 new loss 0.008334590122103691 old loss 0.008440334349870682 BETTER
I0311 16:39:12.212094 1624928 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.009605745784938335 old loss 0.009653385728597641 BETTER
I0311 16:39:15.885744 1624142 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.008409162051975727 old loss 0.008439119905233383 BETTER
I0311 16:39:21.227037 1626490 finetune.py:68] layer 31_up @ epoch 2 new loss 0.017546938732266426 old loss 0.018221542239189148 BETTER
I0311 16:39:24.868724 1625689 finetune.py:68] layer 30_up @ epoch 4 new loss 0.008247660472989082 old loss 0.008334590122103691 BETTER
I0311 16:39:39.934419 1625689 finetune.py:45] layer 30_gate initial loss 0.012791195884346962
I0311 16:39:40.302164 1624928 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.009565790183842182 old loss 0.009605745784938335 BETTER
I0311 16:39:45.909361 1624142 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.008382813073694706 old loss 0.008409162051975727 BETTER
I0311 16:39:50.798319 1626490 finetune.py:68] layer 31_up @ epoch 3 new loss 0.017048122361302376 old loss 0.017546938732266426 BETTER
I0311 16:40:07.533533 1625689 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.012571115046739578 old loss 0.012791195884346962 BETTER
I0311 16:40:08.237685 1624928 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0095309903845191 old loss 0.009565790183842182 BETTER
I0311 16:40:15.994117 1624142 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.008358773775398731 old loss 0.008382813073694706 BETTER
I0311 16:40:20.329994 1626490 finetune.py:68] layer 31_up @ epoch 4 new loss 0.016645541414618492 old loss 0.017048122361302376 BETTER
I0311 16:40:32.056350 1624142 finetune.py:45] layer 28_down initial loss 0.013071473687887192
I0311 16:40:35.484356 1626490 finetune.py:45] layer 31_gate initial loss 0.02720918506383896
I0311 16:40:35.826538 1625689 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.012456961907446384 old loss 0.012571115046739578 BETTER
I0311 16:40:36.148222 1624928 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.009500079788267612 old loss 0.0095309903845191 BETTER
I0311 16:40:51.580301 1624928 finetune.py:45] layer 29_down initial loss 0.015424405224621296
I0311 16:40:59.298258 1624142 finetune.py:68] layer 28_down @ epoch 0 new loss 0.013066417537629604 old loss 0.013071473687887192 BETTER
I0311 16:41:02.694992 1626490 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.02528577484190464 old loss 0.02720918506383896 BETTER
I0311 16:41:03.998653 1625689 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.012366623617708683 old loss 0.012456961907446384 BETTER
I0311 16:41:17.833725 1624928 finetune.py:68] layer 29_down @ epoch 0 new loss 0.015418099239468575 old loss 0.015424405224621296 BETTER
I0311 16:41:30.422406 1624142 finetune.py:68] layer 28_down @ epoch 1 new loss 0.013061754405498505 old loss 0.013066417537629604 BETTER
I0311 16:41:33.999801 1626490 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.024462230503559113 old loss 0.02528577484190464 BETTER
I0311 16:41:34.864505 1625689 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.012290893122553825 old loss 0.012366623617708683 BETTER
I0311 16:41:46.609194 1624928 finetune.py:68] layer 29_down @ epoch 1 new loss 0.015412233769893646 old loss 0.015418099239468575 BETTER
I0311 16:42:01.008894 1624142 finetune.py:68] layer 28_down @ epoch 2 new loss 0.013057396747171879 old loss 0.013061754405498505 BETTER
I0311 16:42:04.604537 1626490 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.023892048746347427 old loss 0.024462230503559113 BETTER
I0311 16:42:05.175872 1625689 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.012223645113408566 old loss 0.012290893122553825 BETTER
I0311 16:42:14.955083 1624928 finetune.py:68] layer 29_down @ epoch 2 new loss 0.015406787395477295 old loss 0.015412233769893646 BETTER
I0311 16:42:25.406716 1625689 finetune.py:45] layer 30_down initial loss 0.021948857232928276
I0311 16:42:31.425205 1624142 finetune.py:68] layer 28_down @ epoch 3 new loss 0.013053272850811481 old loss 0.013057396747171879 BETTER
I0311 16:42:34.474998 1626490 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.023454641923308372 old loss 0.023892048746347427 BETTER
I0311 16:42:43.441949 1624928 finetune.py:68] layer 29_down @ epoch 3 new loss 0.015401660464704037 old loss 0.015406787395477295 BETTER
I0311 16:42:52.603567 1625689 finetune.py:68] layer 30_down @ epoch 0 new loss 0.02193337492644787 old loss 0.021948857232928276 BETTER
I0311 16:43:01.878421 1624142 finetune.py:68] layer 28_down @ epoch 4 new loss 0.01304942462593317 old loss 0.013053272850811481 BETTER
28_v proxy err 0.03133852779865265 tr(WHW.T) 2018.944091796875
28_q proxy err 0.008734781295061111 tr(WHW.T) 7655.96923828125
28_k proxy err 0.0063970014452934265 tr(WHW.T) 10618.134765625
28_o proxy err 0.022666560485959053 tr(WHW.T) 196.9017791748047
28_up proxy err 0.02469988912343979 tr(WHW.T) 4657.10107421875
28_gate proxy err 0.017930708825588226 tr(WHW.T) 6515.46875
28_down proxy err 0.03335944935679436 tr(WHW.T) 613.605224609375
I0311 16:43:04.587500 1626490 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.023103322833776474 old loss 0.023454641923308372 BETTER
I0311 16:43:10.458324 1624928 finetune.py:68] layer 29_down @ epoch 4 new loss 0.01539683435112238 old loss 0.015401660464704037 BETTER
29_v proxy err 0.03289547562599182 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.008589781820774078 tr(WHW.T) 7239.0849609375
29_k proxy err 0.0059535568580031395 tr(WHW.T) 10655.103515625
29_o proxy err 0.02070845663547516 tr(WHW.T) 209.1800079345703
29_up proxy err 0.019805463030934334 tr(WHW.T) 6071.62109375
29_gate proxy err 0.016439126804471016 tr(WHW.T) 7342.3427734375
29_down proxy err 0.032177090644836426 tr(WHW.T) 797.9815063476562
I0311 16:43:19.716596 1625689 finetune.py:68] layer 30_down @ epoch 1 new loss 0.02192014642059803 old loss 0.02193337492644787 BETTER
I0311 16:43:21.290997 1626490 finetune.py:45] layer 31_down initial loss 0.05039461329579353
I0311 16:43:46.544283 1626490 finetune.py:68] layer 31_down @ epoch 0 new loss 0.05017662048339844 old loss 0.05039461329579353 BETTER
I0311 16:43:46.767192 1625689 finetune.py:68] layer 30_down @ epoch 2 new loss 0.02190861850976944 old loss 0.02192014642059803 BETTER
I0311 16:44:12.653050 1626490 finetune.py:68] layer 31_down @ epoch 1 new loss 0.04997960850596428 old loss 0.05017662048339844 BETTER
I0311 16:44:13.800065 1625689 finetune.py:68] layer 30_down @ epoch 3 new loss 0.021898433566093445 old loss 0.02190861850976944 BETTER
I0311 16:44:38.926629 1626490 finetune.py:68] layer 31_down @ epoch 2 new loss 0.04980269819498062 old loss 0.04997960850596428 BETTER
I0311 16:44:40.612539 1625689 finetune.py:68] layer 30_down @ epoch 4 new loss 0.021889710798859596 old loss 0.021898433566093445 BETTER
30_v proxy err 0.028350556269288063 tr(WHW.T) 2261.489501953125
30_q proxy err 0.008643733337521553 tr(WHW.T) 7822.43701171875
30_k proxy err 0.006522524636238813 tr(WHW.T) 10610.681640625
30_o proxy err 0.019820112735033035 tr(WHW.T) 254.31590270996094
30_up proxy err 0.01243609469383955 tr(WHW.T) 9991.2431640625
30_gate proxy err 0.011306362226605415 tr(WHW.T) 10943.60546875
30_down proxy err 0.01165764033794403 tr(WHW.T) 3645.014892578125
I0311 16:45:05.261424 1626490 finetune.py:68] layer 31_down @ epoch 3 new loss 0.04964674264192581 old loss 0.04980269819498062 BETTER
I0311 16:45:31.673576 1626490 finetune.py:68] layer 31_down @ epoch 4 new loss 0.04950493201613426 old loss 0.04964674264192581 BETTER
31_v proxy err 0.033838700503110886 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.007098570000380278 tr(WHW.T) 6866.12109375
31_k proxy err 0.0048778424970805645 tr(WHW.T) 10357.7744140625
31_o proxy err 0.013943527825176716 tr(WHW.T) 461.5377197265625
31_up proxy err 0.007506371941417456 tr(WHW.T) 14520.82421875
31_gate proxy err 0.007313143461942673 tr(WHW.T) 14846.041015625
31_down proxy err 0.006371241062879562 tr(WHW.T) 18565.78515625
