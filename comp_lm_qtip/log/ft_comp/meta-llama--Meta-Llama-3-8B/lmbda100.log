I0407 08:03:27.885163 22419 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:03:27.885263 22419 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:03:27.885305 22419 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:03:28.212818 22419 config.py:54] PyTorch version 2.6.0 available.
W0407 08:03:28.401553 22419 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:03:29.255066 22419 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.06it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.41it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.66it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.73it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.83it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.90it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.02it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.82it/s]
I0407 08:03:30.737318 22419 quantize_finetune_llama.py:157] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:00<00:00, 16432.14it/s]
I0407 08:03:34.568958 22419 quantize_finetune_llama.py:195] loaded compression model
I0407 08:03:52.470925 22419 quantize_finetune_llama.py:199] loaded dataset and devset
I0407 08:03:58.210418 22419 quantize_finetune_llama.py:219] layer 0 gpu 0
I0407 08:04:00.878769 22419 quantize_finetune_llama.py:250] computed original embedding for layer 0 in 2.497047185897827s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0407 08:04:14.784091 23057 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:04:14.784188 23057 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:04:14.784230 23057 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:04:15.113542 23057 config.py:54] PyTorch version 2.6.0 available.
W0407 08:04:15.308017 23057 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:04:15.871469 23057 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:04:15.875492 22419 quantize_finetune_llama.py:219] layer 1 gpu 1
I0407 08:04:15.888695 23057 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:50,  1.96it/s]  2%|▏         | 2/100 [00:00<00:34,  2.85it/s]  3%|▎         | 3/100 [00:00<00:28,  3.36it/s]  4%|▍         | 4/100 [00:01<00:26,  3.65it/s]  5%|▌         | 5/100 [00:01<00:24,  3.83it/s]  6%|▌         | 6/100 [00:01<00:23,  3.93it/s]  7%|▋         | 7/100 [00:01<00:23,  4.01it/s]  8%|▊         | 8/100 [00:02<00:22,  4.06it/s]  9%|▉         | 9/100 [00:02<00:22,  4.09it/s] 10%|█         | 10/100 [00:02<00:21,  4.12it/s] 11%|█         | 11/100 [00:02<00:21,  4.13it/s]I0407 08:04:20.283061 22419 quantize_finetune_llama.py:250] computed original embedding for layer 1 in 4.24025297164917s
 12%|█▏        | 12/100 [00:03<00:21,  4.14it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.15it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.16it/s] 15%|█▌        | 15/100 [00:03<00:20,  4.14it/s] 16%|█▌        | 16/100 [00:04<00:20,  4.16it/s] 17%|█▋        | 17/100 [00:04<00:19,  4.16it/s] 18%|█▊        | 18/100 [00:04<00:19,  4.17it/s] 19%|█▉        | 19/100 [00:04<00:19,  4.17it/s] 20%|██        | 20/100 [00:05<00:19,  4.15it/s] 21%|██        | 21/100 [00:05<00:18,  4.16it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.16it/s] 23%|██▎       | 23/100 [00:05<00:18,  4.16it/s] 24%|██▍       | 24/100 [00:06<00:18,  4.17it/s] 25%|██▌       | 25/100 [00:06<00:18,  4.16it/s] 26%|██▌       | 26/100 [00:06<00:17,  4.17it/s]I0407 08:04:23.854094 23225 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:04:23.854193 23225 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:04:23.854233 23225 utils.py:162] NumExpr defaulting to 16 threads.
 27%|██▋       | 27/100 [00:06<00:17,  4.17it/s]I0407 08:04:24.181247 23225 config.py:54] PyTorch version 2.6.0 available.
 28%|██▊       | 28/100 [00:06<00:17,  4.17it/s]W0407 08:04:24.381798 23225 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 29%|██▉       | 29/100 [00:07<00:17,  4.16it/s] 30%|███       | 30/100 [00:07<00:16,  4.17it/s] 31%|███       | 31/100 [00:07<00:16,  4.17it/s] 32%|███▏      | 32/100 [00:07<00:16,  4.17it/s]W0407 08:04:25.179157 23225 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:04:25.183108 22419 quantize_finetune_llama.py:219] layer 2 gpu 2
I0407 08:04:25.197925 23225 data_utils.py:336] using 256 training seqs, 128 validation seqs
 33%|███▎      | 33/100 [00:08<00:16,  4.16it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.16it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.16it/s]  0%|          | 0/100 [00:00<?, ?it/s] 36%|███▌      | 36/100 [00:08<00:15,  4.17it/s] 37%|███▋      | 37/100 [00:09<00:15,  4.15it/s]  1%|          | 1/100 [00:00<00:50,  1.96it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.14it/s]  2%|▏         | 2/100 [00:00<00:35,  2.77it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.13it/s]  3%|▎         | 3/100 [00:01<00:29,  3.30it/s] 40%|████      | 40/100 [00:09<00:14,  4.15it/s]  4%|▍         | 4/100 [00:01<00:26,  3.63it/s] 41%|████      | 41/100 [00:10<00:14,  4.15it/s]  5%|▌         | 5/100 [00:01<00:24,  3.84it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.15it/s]  6%|▌         | 6/100 [00:01<00:23,  3.98it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.15it/s]  7%|▋         | 7/100 [00:01<00:22,  4.08it/s] 44%|████▍     | 44/100 [00:10<00:13,  4.15it/s]  8%|▊         | 8/100 [00:02<00:22,  4.16it/s] 45%|████▌     | 45/100 [00:11<00:13,  4.16it/s]  9%|▉         | 9/100 [00:02<00:21,  4.19it/s]I0407 08:04:28.347114 22419 quantize_finetune_llama.py:250] computed original embedding for layer 2 in 3.009570598602295s
 46%|████▌     | 46/100 [00:11<00:13,  4.14it/s] 10%|█         | 10/100 [00:02<00:21,  4.19it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.13it/s] 11%|█         | 11/100 [00:02<00:21,  4.16it/s] 48%|████▊     | 48/100 [00:11<00:12,  4.14it/s] 12%|█▏        | 12/100 [00:03<00:20,  4.20it/s] 49%|████▉     | 49/100 [00:12<00:12,  4.15it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.21it/s] 50%|█████     | 50/100 [00:12<00:12,  4.15it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.24it/s] 51%|█████     | 51/100 [00:12<00:11,  4.15it/s] 15%|█▌        | 15/100 [00:03<00:20,  4.24it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.15it/s] 16%|█▌        | 16/100 [00:04<00:19,  4.26it/s] 17%|█▋        | 17/100 [00:04<00:19,  4.27it/s] 53%|█████▎    | 53/100 [00:13<00:11,  4.14it/s] 18%|█▊        | 18/100 [00:04<00:19,  4.27it/s] 54%|█████▍    | 54/100 [00:13<00:11,  4.12it/s] 19%|█▉        | 19/100 [00:04<00:18,  4.28it/s] 55%|█████▌    | 55/100 [00:13<00:10,  4.12it/s] 20%|██        | 20/100 [00:04<00:18,  4.27it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.13it/s] 21%|██        | 21/100 [00:05<00:18,  4.26it/s] 57%|█████▋    | 57/100 [00:13<00:10,  4.12it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.23it/s] 58%|█████▊    | 58/100 [00:14<00:10,  4.11it/s] 23%|██▎       | 23/100 [00:05<00:18,  4.23it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.14it/s] 24%|██▍       | 24/100 [00:05<00:17,  4.25it/s] 60%|██████    | 60/100 [00:14<00:09,  4.14it/s]I0407 08:04:31.982822 23393 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:04:31.982926 23393 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:04:31.982967 23393 utils.py:162] NumExpr defaulting to 16 threads.
 25%|██▌       | 25/100 [00:06<00:17,  4.25it/s] 61%|██████    | 61/100 [00:14<00:09,  4.13it/s]I0407 08:04:32.322217 23393 config.py:54] PyTorch version 2.6.0 available.
 26%|██▌       | 26/100 [00:06<00:17,  4.24it/s] 62%|██████▏   | 62/100 [00:15<00:09,  4.12it/s]W0407 08:04:32.517599 23393 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 27%|██▋       | 27/100 [00:06<00:17,  4.25it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.13it/s] 28%|██▊       | 28/100 [00:06<00:16,  4.26it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.13it/s] 29%|██▉       | 29/100 [00:07<00:16,  4.27it/s] 65%|██████▌   | 65/100 [00:15<00:08,  4.13it/s] 30%|███       | 30/100 [00:07<00:16,  4.27it/s] 66%|██████▌   | 66/100 [00:16<00:08,  4.12it/s] 31%|███       | 31/100 [00:07<00:16,  4.28it/s] 67%|██████▋   | 67/100 [00:16<00:07,  4.13it/s] 32%|███▏      | 32/100 [00:07<00:15,  4.27it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.14it/s]W0407 08:04:33.922910 23393 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:04:33.926740 22419 quantize_finetune_llama.py:219] layer 3 gpu 3
I0407 08:04:33.940878 23393 data_utils.py:336] using 256 training seqs, 128 validation seqs
 33%|███▎      | 33/100 [00:08<00:15,  4.27it/s] 69%|██████▉   | 69/100 [00:16<00:07,  4.13it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.25it/s] 70%|███████   | 70/100 [00:17<00:07,  4.12it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.24it/s]  0%|          | 0/100 [00:00<?, ?it/s] 71%|███████   | 71/100 [00:17<00:07,  4.12it/s] 36%|███▌      | 36/100 [00:08<00:15,  4.25it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.12it/s] 37%|███▋      | 37/100 [00:08<00:14,  4.25it/s] 73%|███████▎  | 73/100 [00:17<00:06,  4.11it/s]  1%|          | 1/100 [00:00<01:04,  1.55it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.26it/s] 74%|███████▍  | 74/100 [00:18<00:06,  4.11it/s]  2%|▏         | 2/100 [00:00<00:39,  2.48it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.27it/s] 75%|███████▌  | 75/100 [00:18<00:06,  4.11it/s]  3%|▎         | 3/100 [00:01<00:31,  3.06it/s] 40%|████      | 40/100 [00:09<00:14,  4.26it/s]  4%|▍         | 4/100 [00:01<00:27,  3.45it/s] 76%|███████▌  | 76/100 [00:18<00:05,  4.11it/s] 41%|████      | 41/100 [00:09<00:13,  4.27it/s]  5%|▌         | 5/100 [00:01<00:25,  3.70it/s] 77%|███████▋  | 77/100 [00:18<00:05,  4.11it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.26it/s]  6%|▌         | 6/100 [00:01<00:24,  3.88it/s] 78%|███████▊  | 78/100 [00:19<00:05,  4.10it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.25it/s]  7%|▋         | 7/100 [00:02<00:23,  4.01it/s] 79%|███████▉  | 79/100 [00:19<00:05,  4.10it/s] 44%|████▍     | 44/100 [00:10<00:13,  4.25it/s]I0407 08:04:36.618069 22419 quantize_finetune_llama.py:250] computed original embedding for layer 3 in 2.530045986175537s
  8%|▊         | 8/100 [00:02<00:22,  4.08it/s] 80%|████████  | 80/100 [00:19<00:04,  4.08it/s] 45%|████▌     | 45/100 [00:10<00:12,  4.25it/s]  9%|▉         | 9/100 [00:02<00:22,  4.09it/s] 81%|████████  | 81/100 [00:19<00:04,  4.08it/s] 46%|████▌     | 46/100 [00:11<00:12,  4.26it/s] 10%|█         | 10/100 [00:02<00:21,  4.14it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.24it/s] 82%|████████▏ | 82/100 [00:20<00:04,  4.08it/s] 11%|█         | 11/100 [00:03<00:21,  4.17it/s] 48%|████▊     | 48/100 [00:11<00:12,  4.24it/s] 83%|████████▎ | 83/100 [00:20<00:04,  4.09it/s] 12%|█▏        | 12/100 [00:03<00:20,  4.19it/s] 49%|████▉     | 49/100 [00:11<00:12,  4.25it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.09it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.22it/s] 50%|█████     | 50/100 [00:12<00:11,  4.25it/s] 85%|████████▌ | 85/100 [00:20<00:03,  4.10it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.22it/s] 51%|█████     | 51/100 [00:12<00:11,  4.25it/s] 86%|████████▌ | 86/100 [00:21<00:03,  4.10it/s] 15%|█▌        | 15/100 [00:03<00:20,  4.25it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.25it/s] 87%|████████▋ | 87/100 [00:21<00:03,  4.09it/s] 16%|█▌        | 16/100 [00:04<00:19,  4.24it/s] 53%|█████▎    | 53/100 [00:12<00:11,  4.25it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.08it/s] 17%|█▋        | 17/100 [00:04<00:19,  4.24it/s] 54%|█████▍    | 54/100 [00:12<00:10,  4.26it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.08it/s] 18%|█▊        | 18/100 [00:04<00:19,  4.23it/s] 55%|█████▌    | 55/100 [00:13<00:10,  4.24it/s] 90%|█████████ | 90/100 [00:22<00:02,  4.08it/s] 19%|█▉        | 19/100 [00:04<00:19,  4.24it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.25it/s] 91%|█████████ | 91/100 [00:22<00:02,  4.09it/s] 20%|██        | 20/100 [00:05<00:18,  4.23it/s] 57%|█████▋    | 57/100 [00:13<00:10,  4.23it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.10it/s] 21%|██        | 21/100 [00:05<00:18,  4.24it/s] 58%|█████▊    | 58/100 [00:13<00:09,  4.21it/s] 93%|█████████▎| 93/100 [00:22<00:01,  4.09it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.25it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.22it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.09it/s]I0407 08:04:40.232035 23558 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:04:40.232129 23558 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:04:40.232169 23558 utils.py:162] NumExpr defaulting to 16 threads.
 23%|██▎       | 23/100 [00:05<00:18,  4.24it/s] 60%|██████    | 60/100 [00:14<00:09,  4.23it/s] 95%|█████████▌| 95/100 [00:23<00:01,  4.10it/s] 24%|██▍       | 24/100 [00:06<00:17,  4.26it/s] 61%|██████    | 61/100 [00:14<00:09,  4.23it/s]I0407 08:04:40.574350 23558 config.py:54] PyTorch version 2.6.0 available.
 96%|█████████▌| 96/100 [00:23<00:00,  4.09it/s] 25%|██▌       | 25/100 [00:06<00:17,  4.24it/s]W0407 08:04:40.770486 23558 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 62%|██████▏   | 62/100 [00:14<00:08,  4.22it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.09it/s] 26%|██▌       | 26/100 [00:06<00:17,  4.26it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.23it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.07it/s] 27%|██▋       | 27/100 [00:06<00:17,  4.25it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.23it/s]W0407 08:04:41.353424 23558 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:04:41.357316 22419 quantize_finetune_llama.py:219] layer 4 gpu 0
I0407 08:04:41.370698 23558 data_utils.py:336] using 256 training seqs, 128 validation seqs
 99%|█████████▉| 99/100 [00:24<00:00,  4.07it/s] 28%|██▊       | 28/100 [00:07<00:16,  4.25it/s] 65%|██████▌   | 65/100 [00:15<00:08,  4.24it/s]100%|██████████| 100/100 [00:24<00:00,  4.07it/s]100%|██████████| 100/100 [00:24<00:00,  4.09it/s]
 29%|██▉       | 29/100 [00:07<00:16,  4.25it/s] 66%|██████▌   | 66/100 [00:15<00:08,  4.24it/s] 30%|███       | 30/100 [00:07<00:16,  4.23it/s] 67%|██████▋   | 67/100 [00:16<00:07,  4.24it/s]  0%|          | 0/100 [00:00<?, ?it/s] 31%|███       | 31/100 [00:07<00:16,  4.23it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.24it/s] 32%|███▏      | 32/100 [00:07<00:16,  4.22it/s] 69%|██████▉   | 69/100 [00:16<00:07,  4.22it/s] 33%|███▎      | 33/100 [00:08<00:15,  4.21it/s] 70%|███████   | 70/100 [00:16<00:07,  4.21it/s]  1%|          | 1/100 [00:00<01:15,  1.31it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.21it/s] 71%|███████   | 71/100 [00:17<00:06,  4.24it/s]  2%|▏         | 2/100 [00:00<00:44,  2.21it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.22it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.23it/s]  3%|▎         | 3/100 [00:01<00:34,  2.84it/s] 36%|███▌      | 36/100 [00:08<00:15,  4.23it/s] 73%|███████▎  | 73/100 [00:17<00:06,  4.22it/s]  4%|▍         | 4/100 [00:01<00:29,  3.29it/s] 37%|███▋      | 37/100 [00:09<00:14,  4.21it/s] 74%|███████▍  | 74/100 [00:17<00:06,  4.23it/s]0_v proxy err 0.04660177603363991 err 2.837435245513916 tr(WHW.T) 60.88684844970703
bpp_loss 3.2455804347991943
  0%|          | 0/100 [00:00<?, ?it/s]  5%|▌         | 5/100 [00:01<00:26,  3.58it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.21it/s] 75%|███████▌  | 75/100 [00:17<00:05,  4.22it/s]  6%|▌         | 6/100 [00:01<00:24,  3.79it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.22it/s] 76%|███████▌  | 76/100 [00:18<00:05,  4.22it/s]  7%|▋         | 7/100 [00:02<00:23,  3.95it/s] 40%|████      | 40/100 [00:09<00:14,  4.24it/s] 77%|███████▋  | 77/100 [00:18<00:05,  4.23it/s]  8%|▊         | 8/100 [00:02<00:22,  4.05it/s] 41%|████      | 41/100 [00:10<00:13,  4.25it/s] 78%|███████▊  | 78/100 [00:18<00:05,  4.23it/s]  9%|▉         | 9/100 [00:02<00:22,  4.12it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.24it/s] 79%|███████▉  | 79/100 [00:18<00:04,  4.22it/s] 10%|█         | 10/100 [00:02<00:21,  4.18it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.24it/s] 80%|████████  | 80/100 [00:19<00:04,  4.23it/s]  1%|          | 1/100 [00:01<02:26,  1.48s/it] 11%|█         | 11/100 [00:03<00:21,  4.22it/s] 44%|████▍     | 44/100 [00:10<00:13,  4.23it/s] 81%|████████  | 81/100 [00:19<00:04,  4.22it/s] 12%|█▏        | 12/100 [00:03<00:20,  4.24it/s] 45%|████▌     | 45/100 [00:11<00:13,  4.21it/s] 82%|████████▏ | 82/100 [00:19<00:04,  4.21it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.27it/s] 46%|████▌     | 46/100 [00:11<00:12,  4.21it/s] 83%|████████▎ | 83/100 [00:19<00:04,  4.20it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.25it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.22it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.20it/s] 15%|█▌        | 15/100 [00:04<00:19,  4.25it/s]  2%|▏         | 2/100 [00:02<01:56,  1.19s/it] 48%|████▊     | 48/100 [00:11<00:12,  4.22it/s] 85%|████████▌ | 85/100 [00:20<00:03,  4.19it/s] 16%|█▌        | 16/100 [00:04<00:19,  4.27it/s] 49%|████▉     | 49/100 [00:11<00:12,  4.21it/s] 86%|████████▌ | 86/100 [00:20<00:03,  4.20it/s] 17%|█▋        | 17/100 [00:04<00:19,  4.27it/s] 50%|█████     | 50/100 [00:12<00:11,  4.21it/s] 87%|████████▋ | 87/100 [00:20<00:03,  4.21it/s] 18%|█▊        | 18/100 [00:04<00:19,  4.28it/s] 51%|█████     | 51/100 [00:12<00:11,  4.23it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.21it/s] 19%|█▉        | 19/100 [00:04<00:18,  4.29it/s]  3%|▎         | 3/100 [00:03<01:46,  1.10s/it] 52%|█████▏    | 52/100 [00:12<00:11,  4.21it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.20it/s] 20%|██        | 20/100 [00:05<00:18,  4.30it/s] 53%|█████▎    | 53/100 [00:12<00:11,  4.21it/s] 90%|█████████ | 90/100 [00:21<00:02,  4.19it/s] 21%|██        | 21/100 [00:05<00:18,  4.30it/s] 54%|█████▍    | 54/100 [00:13<00:10,  4.21it/s] 91%|█████████ | 91/100 [00:21<00:02,  4.18it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.30it/s] 55%|█████▌    | 55/100 [00:13<00:10,  4.21it/s] 92%|█████████▏| 92/100 [00:21<00:01,  4.17it/s] 23%|██▎       | 23/100 [00:05<00:17,  4.29it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.22it/s]  4%|▍         | 4/100 [00:04<01:41,  1.06s/it] 93%|█████████▎| 93/100 [00:22<00:01,  4.17it/s] 24%|██▍       | 24/100 [00:06<00:17,  4.30it/s] 57%|█████▋    | 57/100 [00:13<00:10,  4.21it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.18it/s] 25%|██▌       | 25/100 [00:06<00:17,  4.29it/s] 58%|█████▊    | 58/100 [00:14<00:09,  4.21it/s] 95%|█████████▌| 95/100 [00:22<00:01,  4.20it/s] 26%|██▌       | 26/100 [00:06<00:17,  4.29it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.23it/s] 96%|█████████▌| 96/100 [00:22<00:00,  4.20it/s] 27%|██▋       | 27/100 [00:06<00:16,  4.30it/s] 60%|██████    | 60/100 [00:14<00:09,  4.21it/s]  5%|▌         | 5/100 [00:05<01:37,  1.03s/it] 28%|██▊       | 28/100 [00:07<00:16,  4.29it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.19it/s] 61%|██████    | 61/100 [00:14<00:09,  4.20it/s] 29%|██▉       | 29/100 [00:07<00:16,  4.28it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.19it/s] 62%|██████▏   | 62/100 [00:15<00:09,  4.20it/s] 30%|███       | 30/100 [00:07<00:16,  4.27it/s] 99%|█████████▉| 99/100 [00:23<00:00,  4.18it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.21it/s] 31%|███       | 31/100 [00:07<00:16,  4.28it/s]100%|██████████| 100/100 [00:23<00:00,  4.18it/s]100%|██████████| 100/100 [00:23<00:00,  4.18it/s]
 64%|██████▍   | 64/100 [00:15<00:08,  4.22it/s] 32%|███▏      | 32/100 [00:07<00:15,  4.29it/s]  6%|▌         | 6/100 [00:06<01:35,  1.02s/it] 65%|██████▌   | 65/100 [00:15<00:08,  4.20it/s] 33%|███▎      | 33/100 [00:08<00:15,  4.29it/s] 66%|██████▌   | 66/100 [00:16<00:08,  4.20it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.28it/s] 67%|██████▋   | 67/100 [00:16<00:07,  4.18it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.28it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.18it/s] 36%|███▌      | 36/100 [00:08<00:14,  4.28it/s]  7%|▋         | 7/100 [00:07<01:33,  1.01s/it] 69%|██████▉   | 69/100 [00:16<00:07,  4.18it/s] 37%|███▋      | 37/100 [00:09<00:14,  4.28it/s] 70%|███████   | 70/100 [00:16<00:07,  4.18it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.29it/s] 71%|███████   | 71/100 [00:17<00:06,  4.17it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.28it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.17it/s] 40%|████      | 40/100 [00:09<00:14,  4.29it/s]  8%|▊         | 8/100 [00:08<01:32,  1.00s/it] 73%|███████▎  | 73/100 [00:17<00:06,  4.17it/s] 41%|████      | 41/100 [00:10<00:13,  4.27it/s]1_v proxy err 0.03300020471215248 err 3.5993640422821045 tr(WHW.T) 109.07096099853516
bpp_loss 3.360320568084717
  0%|          | 0/100 [00:00<?, ?it/s] 74%|███████▍  | 74/100 [00:17<00:06,  4.17it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.27it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.27it/s] 75%|███████▌  | 75/100 [00:18<00:06,  4.16it/s] 44%|████▍     | 44/100 [00:10<00:13,  4.26it/s] 76%|███████▌  | 76/100 [00:18<00:05,  4.17it/s]  9%|▉         | 9/100 [00:09<01:31,  1.00s/it] 45%|████▌     | 45/100 [00:11<00:12,  4.25it/s] 77%|███████▋  | 77/100 [00:18<00:05,  4.14it/s] 46%|████▌     | 46/100 [00:11<00:12,  4.25it/s] 78%|███████▊  | 78/100 [00:18<00:05,  4.17it/s]  1%|          | 1/100 [00:01<01:59,  1.21s/it] 47%|████▋     | 47/100 [00:11<00:12,  4.26it/s] 79%|███████▉  | 79/100 [00:19<00:05,  4.18it/s] 48%|████▊     | 48/100 [00:11<00:12,  4.27it/s] 80%|████████  | 80/100 [00:19<00:04,  4.18it/s] 49%|████▉     | 49/100 [00:11<00:11,  4.29it/s] 10%|█         | 10/100 [00:10<01:29,  1.00it/s] 81%|████████  | 81/100 [00:19<00:04,  4.17it/s] 50%|█████     | 50/100 [00:12<00:11,  4.29it/s] 82%|████████▏ | 82/100 [00:19<00:04,  4.17it/s]  2%|▏         | 2/100 [00:02<01:43,  1.06s/it] 51%|█████     | 51/100 [00:12<00:11,  4.26it/s] 83%|████████▎ | 83/100 [00:20<00:04,  4.17it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.25it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.17it/s] 53%|█████▎    | 53/100 [00:12<00:11,  4.26it/s] 85%|████████▌ | 85/100 [00:20<00:03,  4.17it/s] 11%|█         | 11/100 [00:11<01:28,  1.00it/s] 54%|█████▍    | 54/100 [00:13<00:10,  4.24it/s] 86%|████████▌ | 86/100 [00:20<00:03,  4.16it/s] 55%|█████▌    | 55/100 [00:13<00:10,  4.24it/s]  3%|▎         | 3/100 [00:03<01:38,  1.01s/it] 87%|████████▋ | 87/100 [00:21<00:03,  4.17it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.26it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.17it/s] 57%|█████▋    | 57/100 [00:13<00:10,  4.27it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.16it/s] 12%|█▏        | 12/100 [00:12<01:27,  1.00it/s] 58%|█████▊    | 58/100 [00:14<00:09,  4.27it/s] 90%|█████████ | 90/100 [00:21<00:02,  4.17it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.27it/s]  4%|▍         | 4/100 [00:04<01:34,  1.01it/s] 91%|█████████ | 91/100 [00:22<00:02,  4.16it/s] 60%|██████    | 60/100 [00:14<00:09,  4.28it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.16it/s] 61%|██████    | 61/100 [00:14<00:09,  4.28it/s] 93%|█████████▎| 93/100 [00:22<00:01,  4.17it/s] 13%|█▎        | 13/100 [00:13<01:26,  1.00it/s] 62%|██████▏   | 62/100 [00:15<00:08,  4.28it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.18it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.26it/s]  5%|▌         | 5/100 [00:05<01:32,  1.02it/s] 95%|█████████▌| 95/100 [00:22<00:01,  4.16it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.26it/s] 96%|█████████▌| 96/100 [00:23<00:00,  4.17it/s] 65%|██████▌   | 65/100 [00:15<00:08,  4.26it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.17it/s] 66%|██████▌   | 66/100 [00:15<00:07,  4.27it/s] 14%|█▍        | 14/100 [00:14<01:25,  1.00it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.14it/s] 67%|██████▋   | 67/100 [00:16<00:07,  4.28it/s]  6%|▌         | 6/100 [00:05<01:31,  1.03it/s] 99%|█████████▉| 99/100 [00:23<00:00,  4.17it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.27it/s]100%|██████████| 100/100 [00:24<00:00,  4.17it/s]100%|██████████| 100/100 [00:24<00:00,  4.14it/s]
 69%|██████▉   | 69/100 [00:16<00:07,  4.28it/s] 70%|███████   | 70/100 [00:16<00:07,  4.27it/s] 15%|█▌        | 15/100 [00:15<01:24,  1.00it/s] 71%|███████   | 71/100 [00:17<00:06,  4.26it/s]  7%|▋         | 7/100 [00:06<01:29,  1.04it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.26it/s] 73%|███████▎  | 73/100 [00:17<00:06,  4.26it/s] 74%|███████▍  | 74/100 [00:17<00:06,  4.26it/s] 16%|█▌        | 16/100 [00:16<01:23,  1.00it/s] 75%|███████▌  | 75/100 [00:18<00:05,  4.26it/s]  8%|▊         | 8/100 [00:07<01:28,  1.04it/s] 76%|███████▌  | 76/100 [00:18<00:05,  4.26it/s] 77%|███████▋  | 77/100 [00:18<00:05,  4.26it/s] 78%|███████▊  | 78/100 [00:18<00:05,  4.25it/s]2_v proxy err 0.01920217275619507 err 2.9947614669799805 tr(WHW.T) 155.95950317382812
bpp_loss 3.3033230304718018
  0%|          | 0/100 [00:00<?, ?it/s] 17%|█▋        | 17/100 [00:17<01:22,  1.00it/s] 79%|███████▉  | 79/100 [00:19<00:04,  4.24it/s]  9%|▉         | 9/100 [00:08<01:27,  1.04it/s] 80%|████████  | 80/100 [00:19<00:04,  4.24it/s] 81%|████████  | 81/100 [00:19<00:04,  4.25it/s] 82%|████████▏ | 82/100 [00:19<00:04,  4.26it/s]  1%|          | 1/100 [00:01<01:39,  1.01s/it] 83%|████████▎ | 83/100 [00:19<00:03,  4.27it/s] 18%|█▊        | 18/100 [00:18<01:22,  1.00s/it] 10%|█         | 10/100 [00:09<01:26,  1.04it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.26it/s] 85%|████████▌ | 85/100 [00:20<00:03,  4.27it/s] 86%|████████▌ | 86/100 [00:20<00:03,  4.26it/s]  2%|▏         | 2/100 [00:01<01:36,  1.02it/s] 87%|████████▋ | 87/100 [00:20<00:03,  4.26it/s] 19%|█▉        | 19/100 [00:19<01:21,  1.00s/it] 11%|█         | 11/100 [00:10<01:25,  1.04it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.22it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.22it/s] 90%|█████████ | 90/100 [00:21<00:02,  4.23it/s]  3%|▎         | 3/100 [00:02<01:34,  1.03it/s] 91%|█████████ | 91/100 [00:21<00:02,  4.24it/s] 20%|██        | 20/100 [00:20<01:20,  1.00s/it] 12%|█▏        | 12/100 [00:11<01:24,  1.04it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.24it/s] 93%|█████████▎| 93/100 [00:22<00:01,  4.22it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.22it/s]  4%|▍         | 4/100 [00:03<01:33,  1.03it/s] 95%|█████████▌| 95/100 [00:22<00:01,  4.21it/s] 13%|█▎        | 13/100 [00:12<01:23,  1.04it/s] 21%|██        | 21/100 [00:21<01:19,  1.00s/it] 96%|█████████▌| 96/100 [00:23<00:00,  4.19it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.19it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.18it/s]  5%|▌         | 5/100 [00:04<01:32,  1.03it/s] 99%|█████████▉| 99/100 [00:23<00:00,  4.20it/s] 14%|█▍        | 14/100 [00:13<01:22,  1.04it/s]100%|██████████| 100/100 [00:23<00:00,  4.22it/s]100%|██████████| 100/100 [00:23<00:00,  4.17it/s]
 22%|██▏       | 22/100 [00:22<01:18,  1.00s/it]  6%|▌         | 6/100 [00:05<01:30,  1.03it/s] 15%|█▌        | 15/100 [00:14<01:21,  1.04it/s] 23%|██▎       | 23/100 [00:23<01:17,  1.00s/it]  7%|▋         | 7/100 [00:06<01:29,  1.03it/s] 16%|█▌        | 16/100 [00:15<01:20,  1.04it/s] 24%|██▍       | 24/100 [00:24<01:16,  1.00s/it]3_v proxy err 0.012037408538162708 err 3.482821464538574 tr(WHW.T) 289.3331604003906
bpp_loss 3.401644468307495
  0%|          | 0/100 [00:00<?, ?it/s]  8%|▊         | 8/100 [00:07<01:28,  1.03it/s] 17%|█▋        | 17/100 [00:16<01:19,  1.04it/s] 25%|██▌       | 25/100 [00:25<01:15,  1.00s/it]  1%|          | 1/100 [00:00<01:32,  1.07it/s]  9%|▉         | 9/100 [00:08<01:28,  1.03it/s] 18%|█▊        | 18/100 [00:17<01:18,  1.04it/s] 26%|██▌       | 26/100 [00:26<01:14,  1.01s/it]  2%|▏         | 2/100 [00:01<01:32,  1.06it/s] 10%|█         | 10/100 [00:09<01:27,  1.03it/s] 19%|█▉        | 19/100 [00:18<01:18,  1.04it/s] 27%|██▋       | 27/100 [00:27<01:13,  1.01s/it]  3%|▎         | 3/100 [00:02<01:31,  1.06it/s] 11%|█         | 11/100 [00:10<01:26,  1.03it/s] 20%|██        | 20/100 [00:19<01:17,  1.04it/s] 28%|██▊       | 28/100 [00:28<01:12,  1.01s/it]  4%|▍         | 4/100 [00:03<01:30,  1.06it/s] 12%|█▏        | 12/100 [00:11<01:25,  1.03it/s] 21%|██        | 21/100 [00:20<01:16,  1.04it/s] 29%|██▉       | 29/100 [00:29<01:11,  1.01s/it]  5%|▌         | 5/100 [00:04<01:29,  1.06it/s] 13%|█▎        | 13/100 [00:12<01:24,  1.03it/s] 22%|██▏       | 22/100 [00:21<01:15,  1.03it/s]  6%|▌         | 6/100 [00:05<01:28,  1.06it/s] 30%|███       | 30/100 [00:30<01:10,  1.01s/it] 14%|█▍        | 14/100 [00:13<01:23,  1.03it/s] 23%|██▎       | 23/100 [00:22<01:14,  1.03it/s]  7%|▋         | 7/100 [00:06<01:28,  1.05it/s] 31%|███       | 31/100 [00:31<01:09,  1.01s/it] 15%|█▌        | 15/100 [00:14<01:22,  1.03it/s] 24%|██▍       | 24/100 [00:23<01:13,  1.03it/s]  8%|▊         | 8/100 [00:07<01:27,  1.05it/s] 32%|███▏      | 32/100 [00:32<01:08,  1.01s/it] 16%|█▌        | 16/100 [00:15<01:21,  1.03it/s] 25%|██▌       | 25/100 [00:24<01:12,  1.03it/s]  9%|▉         | 9/100 [00:08<01:26,  1.05it/s] 33%|███▎      | 33/100 [00:33<01:07,  1.01s/it] 17%|█▋        | 17/100 [00:16<01:20,  1.03it/s] 26%|██▌       | 26/100 [00:25<01:11,  1.03it/s] 10%|█         | 10/100 [00:09<01:25,  1.05it/s] 34%|███▍      | 34/100 [00:34<01:06,  1.01s/it] 18%|█▊        | 18/100 [00:17<01:19,  1.03it/s] 27%|██▋       | 27/100 [00:26<01:10,  1.03it/s] 11%|█         | 11/100 [00:10<01:24,  1.05it/s] 35%|███▌      | 35/100 [00:35<01:05,  1.01s/it] 19%|█▉        | 19/100 [00:18<01:19,  1.02it/s] 28%|██▊       | 28/100 [00:27<01:09,  1.03it/s] 12%|█▏        | 12/100 [00:11<01:23,  1.05it/s] 36%|███▌      | 36/100 [00:36<01:04,  1.01s/it] 20%|██        | 20/100 [00:19<01:18,  1.02it/s] 29%|██▉       | 29/100 [00:28<01:08,  1.03it/s] 13%|█▎        | 13/100 [00:12<01:23,  1.05it/s] 37%|███▋      | 37/100 [00:37<01:03,  1.01s/it] 21%|██        | 21/100 [00:20<01:17,  1.02it/s] 30%|███       | 30/100 [00:29<01:07,  1.03it/s] 14%|█▍        | 14/100 [00:13<01:22,  1.05it/s] 38%|███▊      | 38/100 [00:38<01:02,  1.01s/it] 22%|██▏       | 22/100 [00:21<01:16,  1.02it/s] 31%|███       | 31/100 [00:30<01:07,  1.03it/s] 15%|█▌        | 15/100 [00:14<01:21,  1.05it/s] 39%|███▉      | 39/100 [00:39<01:01,  1.01s/it] 23%|██▎       | 23/100 [00:22<01:15,  1.02it/s] 32%|███▏      | 32/100 [00:31<01:06,  1.03it/s] 16%|█▌        | 16/100 [00:15<01:20,  1.05it/s] 40%|████      | 40/100 [00:40<01:00,  1.01s/it] 24%|██▍       | 24/100 [00:23<01:14,  1.02it/s] 33%|███▎      | 33/100 [00:32<01:05,  1.03it/s] 17%|█▋        | 17/100 [00:16<01:19,  1.05it/s] 25%|██▌       | 25/100 [00:24<01:13,  1.02it/s] 41%|████      | 41/100 [00:41<00:59,  1.02s/it] 34%|███▍      | 34/100 [00:33<01:04,  1.03it/s] 18%|█▊        | 18/100 [00:17<01:18,  1.05it/s] 26%|██▌       | 26/100 [00:25<01:12,  1.02it/s] 42%|████▏     | 42/100 [00:42<00:58,  1.02s/it] 35%|███▌      | 35/100 [00:34<01:03,  1.03it/s] 19%|█▉        | 19/100 [00:18<01:17,  1.04it/s] 27%|██▋       | 27/100 [00:26<01:11,  1.02it/s] 36%|███▌      | 36/100 [00:34<01:02,  1.03it/s] 43%|████▎     | 43/100 [00:43<00:57,  1.02s/it] 20%|██        | 20/100 [00:19<01:16,  1.04it/s] 28%|██▊       | 28/100 [00:27<01:10,  1.02it/s] 37%|███▋      | 37/100 [00:35<01:01,  1.03it/s] 44%|████▍     | 44/100 [00:44<00:56,  1.02s/it] 21%|██        | 21/100 [00:20<01:15,  1.04it/s] 29%|██▉       | 29/100 [00:28<01:09,  1.02it/s] 38%|███▊      | 38/100 [00:36<01:00,  1.03it/s] 45%|████▌     | 45/100 [00:45<00:55,  1.02s/it] 22%|██▏       | 22/100 [00:20<01:14,  1.04it/s] 30%|███       | 30/100 [00:29<01:08,  1.02it/s] 39%|███▉      | 39/100 [00:37<00:59,  1.03it/s] 23%|██▎       | 23/100 [00:21<01:13,  1.04it/s] 46%|████▌     | 46/100 [00:46<00:54,  1.02s/it] 31%|███       | 31/100 [00:30<01:07,  1.02it/s] 40%|████      | 40/100 [00:38<00:58,  1.02it/s] 24%|██▍       | 24/100 [00:22<01:12,  1.04it/s] 47%|████▋     | 47/100 [00:47<00:54,  1.02s/it] 32%|███▏      | 32/100 [00:31<01:06,  1.02it/s] 41%|████      | 41/100 [00:39<00:57,  1.03it/s] 25%|██▌       | 25/100 [00:23<01:12,  1.04it/s] 48%|████▊     | 48/100 [00:48<00:53,  1.02s/it] 33%|███▎      | 33/100 [00:32<01:05,  1.02it/s] 42%|████▏     | 42/100 [00:40<00:56,  1.02it/s] 26%|██▌       | 26/100 [00:24<01:11,  1.04it/s] 49%|████▉     | 49/100 [00:49<00:52,  1.02s/it] 34%|███▍      | 34/100 [00:33<01:04,  1.02it/s] 43%|████▎     | 43/100 [00:41<00:55,  1.02it/s] 27%|██▋       | 27/100 [00:25<01:10,  1.04it/s] 50%|█████     | 50/100 [00:50<00:51,  1.02s/it] 35%|███▌      | 35/100 [00:34<01:03,  1.02it/s] 44%|████▍     | 44/100 [00:42<00:54,  1.02it/s] 28%|██▊       | 28/100 [00:26<01:09,  1.04it/s] 51%|█████     | 51/100 [00:51<00:50,  1.02s/it] 36%|███▌      | 36/100 [00:35<01:02,  1.02it/s] 45%|████▌     | 45/100 [00:43<00:53,  1.02it/s] 29%|██▉       | 29/100 [00:27<01:08,  1.04it/s] 52%|█████▏    | 52/100 [00:52<00:49,  1.02s/it] 37%|███▋      | 37/100 [00:36<01:01,  1.02it/s] 30%|███       | 30/100 [00:28<01:07,  1.04it/s] 46%|████▌     | 46/100 [00:44<00:52,  1.02it/s] 53%|█████▎    | 53/100 [00:53<00:48,  1.02s/it] 38%|███▊      | 38/100 [00:37<01:00,  1.02it/s] 31%|███       | 31/100 [00:29<01:06,  1.04it/s] 47%|████▋     | 47/100 [00:45<00:51,  1.02it/s] 54%|█████▍    | 54/100 [00:54<00:47,  1.02s/it] 39%|███▉      | 39/100 [00:38<00:59,  1.02it/s] 32%|███▏      | 32/100 [00:30<01:05,  1.04it/s] 48%|████▊     | 48/100 [00:46<00:50,  1.02it/s] 55%|█████▌    | 55/100 [00:55<00:46,  1.02s/it] 40%|████      | 40/100 [00:39<00:58,  1.02it/s] 33%|███▎      | 33/100 [00:31<01:04,  1.03it/s] 49%|████▉     | 49/100 [00:47<00:49,  1.02it/s] 56%|█████▌    | 56/100 [00:56<00:45,  1.02s/it] 41%|████      | 41/100 [00:40<00:57,  1.02it/s] 34%|███▍      | 34/100 [00:32<01:03,  1.03it/s] 50%|█████     | 50/100 [00:48<00:48,  1.02it/s] 57%|█████▋    | 57/100 [00:57<00:44,  1.02s/it] 42%|████▏     | 42/100 [00:40<00:56,  1.02it/s] 35%|███▌      | 35/100 [00:33<01:02,  1.03it/s] 51%|█████     | 51/100 [00:49<00:48,  1.02it/s] 58%|█████▊    | 58/100 [00:58<00:43,  1.02s/it] 43%|████▎     | 43/100 [00:41<00:55,  1.02it/s] 36%|███▌      | 36/100 [00:34<01:01,  1.03it/s] 52%|█████▏    | 52/100 [00:50<00:47,  1.02it/s] 59%|█████▉    | 59/100 [01:00<00:42,  1.02s/it] 37%|███▋      | 37/100 [00:35<01:00,  1.03it/s] 44%|████▍     | 44/100 [00:42<00:54,  1.02it/s] 53%|█████▎    | 53/100 [00:51<00:46,  1.02it/s] 60%|██████    | 60/100 [01:01<00:41,  1.03s/it] 38%|███▊      | 38/100 [00:36<01:00,  1.03it/s] 45%|████▌     | 45/100 [00:43<00:54,  1.02it/s] 54%|█████▍    | 54/100 [00:52<00:45,  1.02it/s] 61%|██████    | 61/100 [01:02<00:40,  1.03s/it] 39%|███▉      | 39/100 [00:37<00:59,  1.03it/s] 46%|████▌     | 46/100 [00:44<00:53,  1.02it/s] 55%|█████▌    | 55/100 [00:53<00:44,  1.02it/s] 40%|████      | 40/100 [00:38<00:58,  1.03it/s] 62%|██████▏   | 62/100 [01:03<00:39,  1.03s/it] 47%|████▋     | 47/100 [00:45<00:52,  1.02it/s] 56%|█████▌    | 56/100 [00:54<00:43,  1.02it/s] 41%|████      | 41/100 [00:39<00:57,  1.03it/s] 48%|████▊     | 48/100 [00:46<00:51,  1.02it/s] 63%|██████▎   | 63/100 [01:04<00:38,  1.03s/it] 57%|█████▋    | 57/100 [00:55<00:42,  1.02it/s] 42%|████▏     | 42/100 [00:40<00:56,  1.03it/s] 49%|████▉     | 49/100 [00:47<00:50,  1.01it/s] 64%|██████▍   | 64/100 [01:05<00:36,  1.03s/it] 58%|█████▊    | 58/100 [00:56<00:41,  1.02it/s] 43%|████▎     | 43/100 [00:41<00:55,  1.03it/s] 50%|█████     | 50/100 [00:48<00:49,  1.01it/s] 59%|█████▉    | 59/100 [00:57<00:40,  1.02it/s] 65%|██████▌   | 65/100 [01:06<00:35,  1.03s/it] 44%|████▍     | 44/100 [00:42<00:54,  1.03it/s] 51%|█████     | 51/100 [00:49<00:48,  1.01it/s] 60%|██████    | 60/100 [00:58<00:39,  1.02it/s] 66%|██████▌   | 66/100 [01:07<00:34,  1.03s/it] 45%|████▌     | 45/100 [00:43<00:53,  1.03it/s] 52%|█████▏    | 52/100 [00:50<00:47,  1.01it/s] 61%|██████    | 61/100 [00:59<00:38,  1.02it/s] 67%|██████▋   | 67/100 [01:08<00:33,  1.03s/it] 46%|████▌     | 46/100 [00:44<00:52,  1.03it/s] 53%|█████▎    | 53/100 [00:51<00:46,  1.01it/s] 62%|██████▏   | 62/100 [01:00<00:37,  1.02it/s] 68%|██████▊   | 68/100 [01:09<00:32,  1.03s/it] 47%|████▋     | 47/100 [00:45<00:51,  1.03it/s] 54%|█████▍    | 54/100 [00:52<00:45,  1.01it/s] 63%|██████▎   | 63/100 [01:01<00:36,  1.02it/s] 69%|██████▉   | 69/100 [01:10<00:31,  1.03s/it] 48%|████▊     | 48/100 [00:46<00:50,  1.03it/s] 55%|█████▌    | 55/100 [00:53<00:44,  1.01it/s] 64%|██████▍   | 64/100 [01:02<00:35,  1.02it/s] 70%|███████   | 70/100 [01:11<00:30,  1.03s/it] 49%|████▉     | 49/100 [00:47<00:49,  1.03it/s] 56%|█████▌    | 56/100 [00:54<00:43,  1.01it/s] 65%|██████▌   | 65/100 [01:03<00:34,  1.02it/s] 71%|███████   | 71/100 [01:12<00:29,  1.03s/it] 50%|█████     | 50/100 [00:48<00:48,  1.03it/s] 57%|█████▋    | 57/100 [00:55<00:42,  1.01it/s] 66%|██████▌   | 66/100 [01:04<00:33,  1.02it/s] 72%|███████▏  | 72/100 [01:13<00:28,  1.03s/it] 51%|█████     | 51/100 [00:49<00:47,  1.03it/s] 58%|█████▊    | 58/100 [00:56<00:41,  1.01it/s] 67%|██████▋   | 67/100 [01:05<00:32,  1.02it/s] 73%|███████▎  | 73/100 [01:14<00:27,  1.03s/it] 52%|█████▏    | 52/100 [00:50<00:46,  1.03it/s] 68%|██████▊   | 68/100 [01:06<00:31,  1.02it/s] 59%|█████▉    | 59/100 [00:57<00:40,  1.01it/s] 74%|███████▍  | 74/100 [01:15<00:26,  1.03s/it] 53%|█████▎    | 53/100 [00:50<00:45,  1.03it/s] 69%|██████▉   | 69/100 [01:07<00:30,  1.01it/s] 60%|██████    | 60/100 [00:58<00:39,  1.01it/s] 75%|███████▌  | 75/100 [01:16<00:25,  1.03s/it] 54%|█████▍    | 54/100 [00:51<00:44,  1.03it/s] 70%|███████   | 70/100 [01:08<00:29,  1.01it/s] 61%|██████    | 61/100 [00:59<00:38,  1.01it/s] 76%|███████▌  | 76/100 [01:17<00:24,  1.03s/it] 55%|█████▌    | 55/100 [00:52<00:43,  1.03it/s] 71%|███████   | 71/100 [01:09<00:28,  1.02it/s] 62%|██████▏   | 62/100 [01:00<00:37,  1.01it/s] 77%|███████▋  | 77/100 [01:18<00:23,  1.03s/it] 56%|█████▌    | 56/100 [00:53<00:42,  1.03it/s] 72%|███████▏  | 72/100 [01:10<00:27,  1.02it/s] 63%|██████▎   | 63/100 [01:01<00:36,  1.01it/s] 78%|███████▊  | 78/100 [01:19<00:22,  1.03s/it] 57%|█████▋    | 57/100 [00:54<00:41,  1.03it/s] 73%|███████▎  | 73/100 [01:11<00:26,  1.02it/s] 64%|██████▍   | 64/100 [01:02<00:35,  1.01it/s] 58%|█████▊    | 58/100 [00:55<00:40,  1.02it/s] 79%|███████▉  | 79/100 [01:20<00:21,  1.03s/it] 74%|███████▍  | 74/100 [01:12<00:25,  1.02it/s] 65%|██████▌   | 65/100 [01:03<00:34,  1.01it/s] 59%|█████▉    | 59/100 [00:56<00:39,  1.03it/s] 80%|████████  | 80/100 [01:21<00:20,  1.03s/it] 75%|███████▌  | 75/100 [01:13<00:24,  1.02it/s] 66%|██████▌   | 66/100 [01:04<00:33,  1.01it/s] 60%|██████    | 60/100 [00:57<00:38,  1.03it/s] 81%|████████  | 81/100 [01:22<00:19,  1.03s/it] 76%|███████▌  | 76/100 [01:14<00:23,  1.01it/s] 67%|██████▋   | 67/100 [01:05<00:32,  1.01it/s] 61%|██████    | 61/100 [00:58<00:37,  1.03it/s] 82%|████████▏ | 82/100 [01:23<00:18,  1.03s/it] 77%|███████▋  | 77/100 [01:15<00:22,  1.01it/s] 68%|██████▊   | 68/100 [01:06<00:31,  1.01it/s] 62%|██████▏   | 62/100 [00:59<00:37,  1.02it/s] 83%|████████▎ | 83/100 [01:24<00:17,  1.03s/it] 78%|███████▊  | 78/100 [01:16<00:21,  1.01it/s] 69%|██████▉   | 69/100 [01:07<00:30,  1.01it/s] 63%|██████▎   | 63/100 [01:00<00:36,  1.02it/s] 84%|████████▍ | 84/100 [01:25<00:16,  1.03s/it] 79%|███████▉  | 79/100 [01:17<00:20,  1.01it/s] 70%|███████   | 70/100 [01:08<00:29,  1.01it/s] 64%|██████▍   | 64/100 [01:01<00:35,  1.02it/s] 85%|████████▌ | 85/100 [01:26<00:15,  1.03s/it] 80%|████████  | 80/100 [01:18<00:19,  1.01it/s] 71%|███████   | 71/100 [01:09<00:28,  1.01it/s] 65%|██████▌   | 65/100 [01:02<00:34,  1.02it/s] 81%|████████  | 81/100 [01:19<00:18,  1.01it/s] 86%|████████▌ | 86/100 [01:27<00:14,  1.03s/it] 72%|███████▏  | 72/100 [01:10<00:27,  1.01it/s] 66%|██████▌   | 66/100 [01:03<00:33,  1.02it/s] 82%|████████▏ | 82/100 [01:20<00:17,  1.01it/s] 73%|███████▎  | 73/100 [01:11<00:26,  1.01it/s] 87%|████████▋ | 87/100 [01:28<00:13,  1.03s/it] 67%|██████▋   | 67/100 [01:04<00:32,  1.03it/s] 83%|████████▎ | 83/100 [01:21<00:16,  1.01it/s] 74%|███████▍  | 74/100 [01:12<00:25,  1.01it/s] 88%|████████▊ | 88/100 [01:29<00:12,  1.04s/it] 68%|██████▊   | 68/100 [01:05<00:31,  1.02it/s] 84%|████████▍ | 84/100 [01:22<00:15,  1.01it/s] 75%|███████▌  | 75/100 [01:13<00:24,  1.01it/s] 89%|████████▉ | 89/100 [01:30<00:11,  1.04s/it] 69%|██████▉   | 69/100 [01:06<00:30,  1.02it/s] 85%|████████▌ | 85/100 [01:23<00:14,  1.01it/s] 76%|███████▌  | 76/100 [01:14<00:23,  1.01it/s] 90%|█████████ | 90/100 [01:32<00:10,  1.04s/it] 70%|███████   | 70/100 [01:07<00:29,  1.02it/s] 86%|████████▌ | 86/100 [01:24<00:13,  1.01it/s] 77%|███████▋  | 77/100 [01:15<00:22,  1.01it/s] 91%|█████████ | 91/100 [01:33<00:09,  1.04s/it] 71%|███████   | 71/100 [01:08<00:28,  1.02it/s] 87%|████████▋ | 87/100 [01:25<00:12,  1.01it/s] 78%|███████▊  | 78/100 [01:16<00:21,  1.00it/s] 92%|█████████▏| 92/100 [01:34<00:08,  1.04s/it] 72%|███████▏  | 72/100 [01:09<00:27,  1.02it/s] 88%|████████▊ | 88/100 [01:26<00:11,  1.01it/s] 79%|███████▉  | 79/100 [01:17<00:20,  1.00it/s] 93%|█████████▎| 93/100 [01:35<00:07,  1.04s/it] 73%|███████▎  | 73/100 [01:10<00:26,  1.02it/s] 89%|████████▉ | 89/100 [01:27<00:10,  1.01it/s] 80%|████████  | 80/100 [01:18<00:19,  1.01it/s] 94%|█████████▍| 94/100 [01:36<00:06,  1.04s/it] 74%|███████▍  | 74/100 [01:11<00:25,  1.02it/s] 90%|█████████ | 90/100 [01:28<00:09,  1.01it/s] 81%|████████  | 81/100 [01:19<00:18,  1.01it/s] 75%|███████▌  | 75/100 [01:12<00:24,  1.02it/s] 95%|█████████▌| 95/100 [01:37<00:05,  1.04s/it] 91%|█████████ | 91/100 [01:29<00:08,  1.01it/s] 82%|████████▏ | 82/100 [01:20<00:17,  1.01it/s] 76%|███████▌  | 76/100 [01:13<00:23,  1.02it/s] 96%|█████████▌| 96/100 [01:38<00:04,  1.04s/it] 92%|█████████▏| 92/100 [01:30<00:07,  1.01it/s] 83%|████████▎ | 83/100 [01:21<00:16,  1.01it/s] 77%|███████▋  | 77/100 [01:14<00:22,  1.02it/s] 97%|█████████▋| 97/100 [01:39<00:03,  1.04s/it] 93%|█████████▎| 93/100 [01:31<00:06,  1.01it/s] 84%|████████▍ | 84/100 [01:22<00:15,  1.01it/s] 78%|███████▊  | 78/100 [01:15<00:21,  1.02it/s] 98%|█████████▊| 98/100 [01:40<00:02,  1.04s/it] 94%|█████████▍| 94/100 [01:32<00:05,  1.01it/s] 85%|████████▌ | 85/100 [01:23<00:14,  1.00it/s] 79%|███████▉  | 79/100 [01:16<00:20,  1.02it/s] 99%|█████████▉| 99/100 [01:41<00:01,  1.04s/it] 95%|█████████▌| 95/100 [01:33<00:04,  1.01it/s] 86%|████████▌ | 86/100 [01:24<00:13,  1.00it/s] 80%|████████  | 80/100 [01:17<00:19,  1.02it/s]100%|██████████| 100/100 [01:42<00:00,  1.04s/it]100%|██████████| 100/100 [01:42<00:00,  1.02s/it]
 96%|█████████▌| 96/100 [01:34<00:03,  1.01it/s] 87%|████████▋ | 87/100 [01:25<00:12,  1.00it/s] 81%|████████  | 81/100 [01:18<00:18,  1.02it/s] 97%|█████████▋| 97/100 [01:35<00:02,  1.01it/s] 88%|████████▊ | 88/100 [01:26<00:11,  1.00it/s] 82%|████████▏ | 82/100 [01:19<00:17,  1.02it/s] 98%|█████████▊| 98/100 [01:36<00:01,  1.01it/s] 89%|████████▉ | 89/100 [01:27<00:10,  1.00it/s] 83%|████████▎ | 83/100 [01:20<00:16,  1.02it/s]0_q proxy err 8.849925507092848e-05 err 25.49616241455078 tr(WHW.T) 288094.65625
bpp_loss 3.9640876054763794
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:30,  3.24it/s] 99%|█████████▉| 99/100 [01:36<00:00,  1.01it/s]  2%|▏         | 2/100 [00:00<00:27,  3.57it/s] 90%|█████████ | 90/100 [01:28<00:09,  1.00it/s]  3%|▎         | 3/100 [00:00<00:26,  3.69it/s] 84%|████████▍ | 84/100 [01:21<00:15,  1.02it/s]  4%|▍         | 4/100 [00:01<00:25,  3.76it/s]  5%|▌         | 5/100 [00:01<00:24,  3.82it/s]100%|██████████| 100/100 [01:37<00:00,  1.01it/s]100%|██████████| 100/100 [01:37<00:00,  1.02it/s]
  6%|▌         | 6/100 [00:01<00:24,  3.83it/s] 91%|█████████ | 91/100 [01:29<00:08,  1.00it/s] 85%|████████▌ | 85/100 [01:22<00:14,  1.02it/s]  7%|▋         | 7/100 [00:01<00:24,  3.83it/s]  8%|▊         | 8/100 [00:02<00:23,  3.84it/s]  9%|▉         | 9/100 [00:02<00:23,  3.84it/s] 10%|█         | 10/100 [00:02<00:23,  3.84it/s] 92%|█████████▏| 92/100 [01:30<00:07,  1.00it/s] 86%|████████▌ | 86/100 [01:23<00:13,  1.02it/s] 11%|█         | 11/100 [00:02<00:23,  3.84it/s] 12%|█▏        | 12/100 [00:03<00:22,  3.84it/s] 13%|█▎        | 13/100 [00:03<00:22,  3.85it/s] 93%|█████████▎| 93/100 [01:31<00:07,  1.00s/it] 14%|█▍        | 14/100 [00:03<00:22,  3.85it/s] 87%|████████▋ | 87/100 [01:24<00:12,  1.02it/s] 15%|█▌        | 15/100 [00:03<00:22,  3.84it/s]1_q proxy err 0.0001581137184984982 err 22.8990535736084 tr(WHW.T) 144826.484375
bpp_loss 4.179770469665527
  0%|          | 0/100 [00:00<?, ?it/s] 16%|█▌        | 16/100 [00:04<00:21,  3.85it/s]  1%|          | 1/100 [00:00<00:23,  4.13it/s] 17%|█▋        | 17/100 [00:04<00:21,  3.85it/s] 94%|█████████▍| 94/100 [01:32<00:05,  1.00it/s]  2%|▏         | 2/100 [00:00<00:24,  4.02it/s] 18%|█▊        | 18/100 [00:04<00:21,  3.85it/s] 88%|████████▊ | 88/100 [01:25<00:11,  1.02it/s]  3%|▎         | 3/100 [00:00<00:24,  4.02it/s] 19%|█▉        | 19/100 [00:04<00:21,  3.84it/s]  4%|▍         | 4/100 [00:00<00:23,  4.05it/s] 20%|██        | 20/100 [00:05<00:20,  3.85it/s]  5%|▌         | 5/100 [00:01<00:23,  4.06it/s] 21%|██        | 21/100 [00:05<00:20,  3.84it/s]  6%|▌         | 6/100 [00:01<00:23,  4.05it/s] 95%|█████████▌| 95/100 [01:33<00:04,  1.00it/s] 22%|██▏       | 22/100 [00:05<00:20,  3.84it/s] 89%|████████▉ | 89/100 [01:26<00:10,  1.02it/s]  7%|▋         | 7/100 [00:01<00:22,  4.06it/s] 23%|██▎       | 23/100 [00:06<00:20,  3.83it/s]  8%|▊         | 8/100 [00:01<00:22,  4.06it/s] 24%|██▍       | 24/100 [00:06<00:19,  3.84it/s]  9%|▉         | 9/100 [00:02<00:22,  4.05it/s] 25%|██▌       | 25/100 [00:06<00:19,  3.84it/s] 10%|█         | 10/100 [00:02<00:22,  4.04it/s] 96%|█████████▌| 96/100 [01:34<00:03,  1.00it/s] 90%|█████████ | 90/100 [01:27<00:09,  1.02it/s] 26%|██▌       | 26/100 [00:06<00:19,  3.84it/s] 11%|█         | 11/100 [00:02<00:21,  4.05it/s] 27%|██▋       | 27/100 [00:07<00:19,  3.84it/s] 12%|█▏        | 12/100 [00:02<00:21,  4.05it/s] 28%|██▊       | 28/100 [00:07<00:18,  3.85it/s] 13%|█▎        | 13/100 [00:03<00:21,  4.06it/s] 29%|██▉       | 29/100 [00:07<00:18,  3.82it/s] 14%|█▍        | 14/100 [00:03<00:21,  4.06it/s] 97%|█████████▋| 97/100 [01:35<00:02,  1.00it/s] 91%|█████████ | 91/100 [01:28<00:08,  1.02it/s] 30%|███       | 30/100 [00:07<00:18,  3.85it/s] 15%|█▌        | 15/100 [00:03<00:20,  4.06it/s] 31%|███       | 31/100 [00:08<00:17,  3.85it/s] 16%|█▌        | 16/100 [00:03<00:20,  4.07it/s] 32%|███▏      | 32/100 [00:08<00:17,  3.84it/s] 17%|█▋        | 17/100 [00:04<00:20,  4.06it/s] 18%|█▊        | 18/100 [00:04<00:20,  4.05it/s] 33%|███▎      | 33/100 [00:08<00:17,  3.85it/s] 98%|█████████▊| 98/100 [01:36<00:01,  1.00it/s] 92%|█████████▏| 92/100 [01:29<00:07,  1.02it/s] 19%|█▉        | 19/100 [00:04<00:19,  4.05it/s] 34%|███▍      | 34/100 [00:08<00:17,  3.85it/s] 20%|██        | 20/100 [00:04<00:19,  4.05it/s] 35%|███▌      | 35/100 [00:09<00:16,  3.85it/s] 21%|██        | 21/100 [00:05<00:19,  4.05it/s] 36%|███▌      | 36/100 [00:09<00:16,  3.85it/s] 22%|██▏       | 22/100 [00:05<00:19,  4.05it/s] 99%|█████████▉| 99/100 [01:37<00:01,  1.00s/it] 37%|███▋      | 37/100 [00:09<00:16,  3.84it/s] 93%|█████████▎| 93/100 [01:30<00:06,  1.02it/s] 23%|██▎       | 23/100 [00:05<00:19,  4.05it/s] 38%|███▊      | 38/100 [00:09<00:16,  3.85it/s] 24%|██▍       | 24/100 [00:05<00:18,  4.03it/s] 39%|███▉      | 39/100 [00:10<00:15,  3.85it/s] 25%|██▌       | 25/100 [00:06<00:18,  4.02it/s] 40%|████      | 40/100 [00:10<00:15,  3.85it/s] 26%|██▌       | 26/100 [00:06<00:18,  4.02it/s]100%|██████████| 100/100 [01:38<00:00,  1.00it/s]100%|██████████| 100/100 [01:38<00:00,  1.01it/s]
 94%|█████████▍| 94/100 [01:31<00:05,  1.02it/s] 41%|████      | 41/100 [00:10<00:15,  3.85it/s] 27%|██▋       | 27/100 [00:06<00:18,  4.01it/s] 42%|████▏     | 42/100 [00:10<00:15,  3.85it/s] 28%|██▊       | 28/100 [00:06<00:17,  4.03it/s] 43%|████▎     | 43/100 [00:11<00:14,  3.85it/s] 29%|██▉       | 29/100 [00:07<00:17,  4.05it/s] 44%|████▍     | 44/100 [00:11<00:14,  3.84it/s] 30%|███       | 30/100 [00:07<00:17,  4.05it/s] 95%|█████████▌| 95/100 [01:32<00:04,  1.02it/s] 45%|████▌     | 45/100 [00:11<00:14,  3.85it/s] 31%|███       | 31/100 [00:07<00:17,  4.04it/s] 46%|████▌     | 46/100 [00:12<00:14,  3.85it/s] 32%|███▏      | 32/100 [00:07<00:16,  4.04it/s] 47%|████▋     | 47/100 [00:12<00:13,  3.85it/s] 33%|███▎      | 33/100 [00:08<00:16,  4.05it/s] 48%|████▊     | 48/100 [00:12<00:13,  3.85it/s] 34%|███▍      | 34/100 [00:08<00:16,  4.04it/s] 96%|█████████▌| 96/100 [01:33<00:03,  1.02it/s] 49%|████▉     | 49/100 [00:12<00:13,  3.84it/s] 35%|███▌      | 35/100 [00:08<00:16,  4.05it/s] 50%|█████     | 50/100 [00:13<00:13,  3.85it/s] 36%|███▌      | 36/100 [00:08<00:15,  4.05it/s] 51%|█████     | 51/100 [00:13<00:12,  3.85it/s] 37%|███▋      | 37/100 [00:09<00:15,  4.06it/s]2_q proxy err 0.0005427078576758504 err 22.508893966674805 tr(WHW.T) 41475.15625
bpp_loss 4.145637035369873
  0%|          | 0/100 [00:00<?, ?it/s] 52%|█████▏    | 52/100 [00:13<00:12,  3.85it/s] 38%|███▊      | 38/100 [00:09<00:15,  4.02it/s] 97%|█████████▋| 97/100 [01:34<00:02,  1.02it/s] 39%|███▉      | 39/100 [00:09<00:15,  4.04it/s] 53%|█████▎    | 53/100 [00:13<00:12,  3.85it/s]  1%|          | 1/100 [00:00<00:47,  2.07it/s] 40%|████      | 40/100 [00:09<00:14,  4.04it/s] 54%|█████▍    | 54/100 [00:14<00:11,  3.85it/s]  2%|▏         | 2/100 [00:00<00:33,  2.91it/s] 41%|████      | 41/100 [00:10<00:14,  4.03it/s] 55%|█████▌    | 55/100 [00:14<00:11,  3.85it/s]  3%|▎         | 3/100 [00:00<00:29,  3.33it/s] 42%|████▏     | 42/100 [00:10<00:14,  4.05it/s] 56%|█████▌    | 56/100 [00:14<00:11,  3.85it/s]  4%|▍         | 4/100 [00:01<00:26,  3.58it/s] 98%|█████████▊| 98/100 [01:35<00:01,  1.02it/s] 43%|████▎     | 43/100 [00:10<00:14,  4.04it/s] 57%|█████▋    | 57/100 [00:14<00:11,  3.84it/s]  5%|▌         | 5/100 [00:01<00:25,  3.74it/s] 44%|████▍     | 44/100 [00:10<00:13,  4.03it/s]  6%|▌         | 6/100 [00:01<00:24,  3.83it/s] 58%|█████▊    | 58/100 [00:15<00:10,  3.84it/s] 45%|████▌     | 45/100 [00:11<00:13,  4.04it/s]  7%|▋         | 7/100 [00:01<00:23,  3.89it/s] 59%|█████▉    | 59/100 [00:15<00:10,  3.83it/s] 46%|████▌     | 46/100 [00:11<00:13,  4.03it/s] 99%|█████████▉| 99/100 [01:36<00:00,  1.02it/s]  8%|▊         | 8/100 [00:02<00:23,  3.93it/s] 60%|██████    | 60/100 [00:15<00:10,  3.84it/s] 47%|████▋     | 47/100 [00:11<00:13,  4.05it/s]  9%|▉         | 9/100 [00:02<00:22,  3.97it/s] 61%|██████    | 61/100 [00:15<00:10,  3.85it/s] 48%|████▊     | 48/100 [00:11<00:12,  4.06it/s] 10%|█         | 10/100 [00:02<00:22,  3.99it/s] 62%|██████▏   | 62/100 [00:16<00:09,  3.85it/s] 49%|████▉     | 49/100 [00:12<00:12,  4.05it/s] 11%|█         | 11/100 [00:02<00:22,  4.00it/s] 63%|██████▎   | 63/100 [00:16<00:09,  3.85it/s] 50%|█████     | 50/100 [00:12<00:12,  4.04it/s]100%|██████████| 100/100 [01:37<00:00,  1.02it/s]100%|██████████| 100/100 [01:37<00:00,  1.03it/s]
 12%|█▏        | 12/100 [00:03<00:22,  4.00it/s] 64%|██████▍   | 64/100 [00:16<00:09,  3.84it/s] 51%|█████     | 51/100 [00:12<00:12,  4.04it/s] 13%|█▎        | 13/100 [00:03<00:21,  4.01it/s] 65%|██████▌   | 65/100 [00:16<00:09,  3.85it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.03it/s] 14%|█▍        | 14/100 [00:03<00:21,  4.02it/s] 66%|██████▌   | 66/100 [00:17<00:08,  3.85it/s] 53%|█████▎    | 53/100 [00:13<00:11,  4.04it/s] 15%|█▌        | 15/100 [00:03<00:21,  4.01it/s] 67%|██████▋   | 67/100 [00:17<00:08,  3.83it/s] 54%|█████▍    | 54/100 [00:13<00:11,  4.04it/s] 16%|█▌        | 16/100 [00:04<00:20,  4.02it/s] 68%|██████▊   | 68/100 [00:17<00:08,  3.84it/s] 55%|█████▌    | 55/100 [00:13<00:11,  4.04it/s] 17%|█▋        | 17/100 [00:04<00:20,  4.02it/s] 69%|██████▉   | 69/100 [00:17<00:08,  3.85it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.04it/s] 18%|█▊        | 18/100 [00:04<00:20,  4.02it/s] 70%|███████   | 70/100 [00:18<00:07,  3.84it/s] 57%|█████▋    | 57/100 [00:14<00:10,  4.03it/s] 19%|█▉        | 19/100 [00:04<00:20,  4.02it/s] 71%|███████   | 71/100 [00:18<00:07,  3.85it/s] 58%|█████▊    | 58/100 [00:14<00:10,  4.03it/s] 20%|██        | 20/100 [00:05<00:20,  4.00it/s] 59%|█████▉    | 59/100 [00:14<00:10,  4.03it/s] 72%|███████▏  | 72/100 [00:18<00:07,  3.83it/s] 21%|██        | 21/100 [00:05<00:19,  4.01it/s] 60%|██████    | 60/100 [00:14<00:09,  4.03it/s] 73%|███████▎  | 73/100 [00:19<00:07,  3.84it/s] 22%|██▏       | 22/100 [00:05<00:19,  4.01it/s] 61%|██████    | 61/100 [00:15<00:09,  4.03it/s]3_q proxy err 0.0004803110787179321 err 22.851726531982422 tr(WHW.T) 47576.9296875
bpp_loss 4.159604787826538
  0%|          | 0/100 [00:00<?, ?it/s] 74%|███████▍  | 74/100 [00:19<00:06,  3.84it/s] 23%|██▎       | 23/100 [00:05<00:19,  4.00it/s] 62%|██████▏   | 62/100 [00:15<00:09,  4.03it/s] 75%|███████▌  | 75/100 [00:19<00:06,  3.85it/s] 24%|██▍       | 24/100 [00:06<00:18,  4.01it/s] 63%|██████▎   | 63/100 [00:15<00:09,  4.02it/s]  1%|          | 1/100 [00:00<00:51,  1.92it/s] 76%|███████▌  | 76/100 [00:19<00:06,  3.84it/s] 25%|██▌       | 25/100 [00:06<00:18,  4.01it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.03it/s]  2%|▏         | 2/100 [00:00<00:35,  2.79it/s] 77%|███████▋  | 77/100 [00:20<00:05,  3.86it/s] 26%|██▌       | 26/100 [00:06<00:18,  4.01it/s] 65%|██████▌   | 65/100 [00:16<00:08,  4.04it/s]  3%|▎         | 3/100 [00:01<00:29,  3.27it/s] 78%|███████▊  | 78/100 [00:20<00:05,  3.86it/s] 27%|██▋       | 27/100 [00:06<00:18,  4.02it/s] 66%|██████▌   | 66/100 [00:16<00:08,  4.03it/s]  4%|▍         | 4/100 [00:01<00:26,  3.56it/s] 79%|███████▉  | 79/100 [00:20<00:05,  3.85it/s] 28%|██▊       | 28/100 [00:07<00:17,  4.02it/s] 67%|██████▋   | 67/100 [00:16<00:08,  4.04it/s]  5%|▌         | 5/100 [00:01<00:25,  3.73it/s] 29%|██▉       | 29/100 [00:07<00:17,  4.01it/s] 80%|████████  | 80/100 [00:20<00:05,  3.85it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.04it/s]  6%|▌         | 6/100 [00:01<00:24,  3.86it/s] 30%|███       | 30/100 [00:07<00:17,  4.02it/s] 81%|████████  | 81/100 [00:21<00:04,  3.85it/s] 69%|██████▉   | 69/100 [00:17<00:07,  4.05it/s]  7%|▋         | 7/100 [00:01<00:23,  3.95it/s] 31%|███       | 31/100 [00:07<00:17,  4.02it/s] 82%|████████▏ | 82/100 [00:21<00:04,  3.85it/s]  8%|▊         | 8/100 [00:02<00:23,  3.99it/s] 70%|███████   | 70/100 [00:17<00:07,  4.03it/s] 32%|███▏      | 32/100 [00:08<00:16,  4.02it/s] 83%|████████▎ | 83/100 [00:21<00:04,  3.84it/s]  9%|▉         | 9/100 [00:02<00:22,  4.03it/s] 71%|███████   | 71/100 [00:17<00:07,  4.03it/s] 33%|███▎      | 33/100 [00:08<00:16,  4.02it/s] 84%|████████▍ | 84/100 [00:21<00:04,  3.85it/s] 10%|█         | 10/100 [00:02<00:22,  4.06it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.02it/s] 34%|███▍      | 34/100 [00:08<00:16,  4.00it/s] 85%|████████▌ | 85/100 [00:22<00:03,  3.85it/s] 11%|█         | 11/100 [00:02<00:21,  4.06it/s] 73%|███████▎  | 73/100 [00:18<00:06,  4.02it/s] 35%|███▌      | 35/100 [00:08<00:16,  4.01it/s] 86%|████████▌ | 86/100 [00:22<00:03,  3.85it/s] 12%|█▏        | 12/100 [00:03<00:21,  4.08it/s] 74%|███████▍  | 74/100 [00:18<00:06,  4.02it/s] 36%|███▌      | 36/100 [00:09<00:15,  4.00it/s] 87%|████████▋ | 87/100 [00:22<00:03,  3.85it/s] 13%|█▎        | 13/100 [00:03<00:21,  4.10it/s] 75%|███████▌  | 75/100 [00:18<00:06,  4.03it/s] 37%|███▋      | 37/100 [00:09<00:15,  4.01it/s] 88%|████████▊ | 88/100 [00:22<00:03,  3.85it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.11it/s] 76%|███████▌  | 76/100 [00:18<00:05,  4.03it/s] 38%|███▊      | 38/100 [00:09<00:15,  4.01it/s] 89%|████████▉ | 89/100 [00:23<00:02,  3.85it/s] 15%|█▌        | 15/100 [00:03<00:20,  4.09it/s] 77%|███████▋  | 77/100 [00:19<00:05,  4.03it/s] 39%|███▉      | 39/100 [00:09<00:15,  4.00it/s] 16%|█▌        | 16/100 [00:04<00:20,  4.10it/s] 90%|█████████ | 90/100 [00:23<00:02,  3.85it/s] 78%|███████▊  | 78/100 [00:19<00:05,  4.01it/s] 40%|████      | 40/100 [00:10<00:15,  4.00it/s] 17%|█▋        | 17/100 [00:04<00:20,  4.09it/s] 91%|█████████ | 91/100 [00:23<00:02,  3.85it/s] 79%|███████▉  | 79/100 [00:19<00:05,  4.02it/s] 41%|████      | 41/100 [00:10<00:14,  4.00it/s] 18%|█▊        | 18/100 [00:04<00:20,  4.10it/s] 92%|█████████▏| 92/100 [00:23<00:02,  3.85it/s] 80%|████████  | 80/100 [00:19<00:04,  4.02it/s] 42%|████▏     | 42/100 [00:10<00:14,  4.00it/s] 19%|█▉        | 19/100 [00:04<00:19,  4.10it/s] 93%|█████████▎| 93/100 [00:24<00:01,  3.84it/s] 81%|████████  | 81/100 [00:20<00:04,  4.01it/s] 43%|████▎     | 43/100 [00:10<00:14,  4.00it/s] 20%|██        | 20/100 [00:05<00:19,  4.10it/s] 94%|█████████▍| 94/100 [00:24<00:01,  3.85it/s] 82%|████████▏ | 82/100 [00:20<00:04,  4.00it/s] 44%|████▍     | 44/100 [00:11<00:13,  4.00it/s] 21%|██        | 21/100 [00:05<00:19,  4.09it/s] 83%|████████▎ | 83/100 [00:20<00:04,  4.01it/s] 95%|█████████▌| 95/100 [00:24<00:01,  3.83it/s] 45%|████▌     | 45/100 [00:11<00:13,  4.00it/s] 22%|██▏       | 22/100 [00:05<00:19,  4.09it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.02it/s] 96%|█████████▌| 96/100 [00:25<00:01,  3.85it/s] 46%|████▌     | 46/100 [00:11<00:13,  3.99it/s] 23%|██▎       | 23/100 [00:05<00:18,  4.09it/s] 85%|████████▌ | 85/100 [00:21<00:03,  4.03it/s] 97%|█████████▋| 97/100 [00:25<00:00,  3.85it/s] 47%|████▋     | 47/100 [00:11<00:13,  4.01it/s] 24%|██▍       | 24/100 [00:06<00:18,  4.08it/s] 86%|████████▌ | 86/100 [00:21<00:03,  4.03it/s] 98%|█████████▊| 98/100 [00:25<00:00,  3.85it/s] 48%|████▊     | 48/100 [00:12<00:12,  4.00it/s] 25%|██▌       | 25/100 [00:06<00:18,  4.07it/s] 87%|████████▋ | 87/100 [00:21<00:03,  4.03it/s] 99%|█████████▉| 99/100 [00:25<00:00,  3.85it/s] 49%|████▉     | 49/100 [00:12<00:12,  4.00it/s] 26%|██▌       | 26/100 [00:06<00:18,  4.08it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.03it/s]100%|██████████| 100/100 [00:26<00:00,  3.85it/s]100%|██████████| 100/100 [00:26<00:00,  3.84it/s]
 50%|█████     | 50/100 [00:12<00:12,  3.99it/s] 27%|██▋       | 27/100 [00:06<00:17,  4.07it/s] 89%|████████▉ | 89/100 [00:22<00:02,  4.04it/s] 51%|█████     | 51/100 [00:12<00:12,  4.01it/s] 28%|██▊       | 28/100 [00:07<00:17,  4.07it/s] 90%|█████████ | 90/100 [00:22<00:02,  4.03it/s] 52%|█████▏    | 52/100 [00:13<00:12,  4.00it/s] 29%|██▉       | 29/100 [00:07<00:17,  4.08it/s] 91%|█████████ | 91/100 [00:22<00:02,  4.03it/s] 53%|█████▎    | 53/100 [00:13<00:11,  4.01it/s] 30%|███       | 30/100 [00:07<00:17,  4.10it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.01it/s] 54%|█████▍    | 54/100 [00:13<00:11,  4.00it/s] 31%|███       | 31/100 [00:07<00:16,  4.09it/s] 93%|█████████▎| 93/100 [00:23<00:01,  4.03it/s] 55%|█████▌    | 55/100 [00:13<00:11,  4.00it/s] 32%|███▏      | 32/100 [00:08<00:16,  4.10it/s] 94%|█████████▍| 94/100 [00:23<00:01,  4.03it/s] 56%|█████▌    | 56/100 [00:14<00:10,  4.01it/s] 33%|███▎      | 33/100 [00:08<00:16,  4.10it/s] 95%|█████████▌| 95/100 [00:23<00:01,  4.02it/s] 57%|█████▋    | 57/100 [00:14<00:10,  4.01it/s] 34%|███▍      | 34/100 [00:08<00:16,  4.09it/s] 96%|█████████▌| 96/100 [00:23<00:00,  4.02it/s] 58%|█████▊    | 58/100 [00:14<00:10,  4.00it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.10it/s]0_k proxy err 7.051334978314117e-05 err 7.064079284667969 tr(WHW.T) 100180.734375
bpp_loss 4.585677146911621
  0%|          | 0/100 [00:00<?, ?it/s] 97%|█████████▋| 97/100 [00:24<00:00,  4.01it/s] 36%|███▌      | 36/100 [00:09<00:15,  4.10it/s] 59%|█████▉    | 59/100 [00:14<00:10,  3.99it/s] 98%|█████████▊| 98/100 [00:24<00:00,  4.01it/s] 37%|███▋      | 37/100 [00:09<00:15,  4.09it/s] 60%|██████    | 60/100 [00:15<00:10,  4.00it/s] 99%|█████████▉| 99/100 [00:24<00:00,  4.00it/s] 38%|███▊      | 38/100 [00:09<00:15,  4.08it/s] 61%|██████    | 61/100 [00:15<00:09,  4.00it/s]100%|██████████| 100/100 [00:24<00:00,  4.01it/s]100%|██████████| 100/100 [00:24<00:00,  4.03it/s]
 39%|███▉      | 39/100 [00:09<00:14,  4.09it/s] 62%|██████▏   | 62/100 [00:15<00:09,  4.01it/s]  1%|          | 1/100 [00:01<01:42,  1.03s/it] 40%|████      | 40/100 [00:10<00:14,  4.10it/s] 63%|██████▎   | 63/100 [00:15<00:09,  4.00it/s] 41%|████      | 41/100 [00:10<00:14,  4.07it/s] 64%|██████▍   | 64/100 [00:16<00:08,  4.00it/s] 42%|████▏     | 42/100 [00:10<00:14,  4.07it/s] 65%|██████▌   | 65/100 [00:16<00:08,  3.99it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.08it/s] 66%|██████▌   | 66/100 [00:16<00:08,  4.00it/s]  2%|▏         | 2/100 [00:02<01:41,  1.04s/it] 44%|████▍     | 44/100 [00:11<00:13,  4.07it/s] 67%|██████▋   | 67/100 [00:16<00:08,  4.00it/s] 45%|████▌     | 45/100 [00:11<00:13,  4.07it/s] 68%|██████▊   | 68/100 [00:17<00:07,  4.00it/s]1_k proxy err 8.689800597494468e-05 err 6.564192771911621 tr(WHW.T) 75539.046875
bpp_loss 4.896854400634766
  0%|          | 0/100 [00:00<?, ?it/s] 46%|████▌     | 46/100 [00:11<00:13,  4.08it/s] 69%|██████▉   | 69/100 [00:17<00:07,  4.01it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.09it/s] 70%|███████   | 70/100 [00:17<00:07,  4.01it/s]  3%|▎         | 3/100 [00:03<01:40,  1.04s/it] 48%|████▊     | 48/100 [00:12<00:12,  4.08it/s] 71%|███████   | 71/100 [00:17<00:07,  4.01it/s] 49%|████▉     | 49/100 [00:12<00:12,  4.08it/s] 72%|███████▏  | 72/100 [00:18<00:06,  4.01it/s]  1%|          | 1/100 [00:00<01:37,  1.01it/s] 50%|█████     | 50/100 [00:12<00:12,  4.06it/s] 73%|███████▎  | 73/100 [00:18<00:06,  4.01it/s] 51%|█████     | 51/100 [00:12<00:12,  4.05it/s] 74%|███████▍  | 74/100 [00:18<00:06,  4.01it/s]  4%|▍         | 4/100 [00:04<01:39,  1.04s/it] 52%|█████▏    | 52/100 [00:12<00:11,  4.05it/s] 75%|███████▌  | 75/100 [00:18<00:06,  4.01it/s] 53%|█████▎    | 53/100 [00:13<00:11,  4.06it/s] 76%|███████▌  | 76/100 [00:19<00:05,  4.01it/s]  2%|▏         | 2/100 [00:01<01:37,  1.01it/s] 54%|█████▍    | 54/100 [00:13<00:11,  4.06it/s] 77%|███████▋  | 77/100 [00:19<00:05,  3.99it/s] 55%|█████▌    | 55/100 [00:13<00:11,  4.05it/s] 78%|███████▊  | 78/100 [00:19<00:05,  4.01it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.07it/s]  5%|▌         | 5/100 [00:05<01:38,  1.04s/it] 79%|███████▉  | 79/100 [00:19<00:05,  4.00it/s] 57%|█████▋    | 57/100 [00:14<00:10,  4.05it/s]  3%|▎         | 3/100 [00:02<01:35,  1.01it/s] 80%|████████  | 80/100 [00:20<00:04,  4.00it/s] 58%|█████▊    | 58/100 [00:14<00:10,  4.04it/s] 81%|████████  | 81/100 [00:20<00:04,  4.00it/s] 59%|█████▉    | 59/100 [00:14<00:10,  4.05it/s] 82%|████████▏ | 82/100 [00:20<00:04,  4.00it/s] 60%|██████    | 60/100 [00:14<00:09,  4.06it/s] 83%|████████▎ | 83/100 [00:20<00:04,  3.98it/s]  6%|▌         | 6/100 [00:06<01:37,  1.04s/it] 61%|██████    | 61/100 [00:15<00:09,  4.06it/s]  4%|▍         | 4/100 [00:03<01:35,  1.01it/s] 84%|████████▍ | 84/100 [00:21<00:04,  4.00it/s] 62%|██████▏   | 62/100 [00:15<00:09,  4.08it/s] 85%|████████▌ | 85/100 [00:21<00:03,  4.00it/s] 63%|██████▎   | 63/100 [00:15<00:09,  4.09it/s] 86%|████████▌ | 86/100 [00:21<00:03,  4.00it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.07it/s] 87%|████████▋ | 87/100 [00:21<00:03,  4.00it/s]  7%|▋         | 7/100 [00:07<01:36,  1.04s/it] 65%|██████▌   | 65/100 [00:16<00:08,  4.06it/s]  5%|▌         | 5/100 [00:04<01:34,  1.01it/s] 88%|████████▊ | 88/100 [00:22<00:02,  4.00it/s] 66%|██████▌   | 66/100 [00:16<00:08,  4.08it/s] 89%|████████▉ | 89/100 [00:22<00:02,  4.00it/s] 67%|██████▋   | 67/100 [00:16<00:08,  4.08it/s] 90%|█████████ | 90/100 [00:22<00:02,  4.00it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.06it/s] 91%|█████████ | 91/100 [00:22<00:02,  4.01it/s]  8%|▊         | 8/100 [00:08<01:35,  1.04s/it] 69%|██████▉   | 69/100 [00:17<00:07,  4.08it/s]  6%|▌         | 6/100 [00:05<01:33,  1.01it/s] 92%|█████████▏| 92/100 [00:23<00:01,  4.00it/s] 70%|███████   | 70/100 [00:17<00:07,  4.09it/s] 93%|█████████▎| 93/100 [00:23<00:01,  4.00it/s] 71%|███████   | 71/100 [00:17<00:07,  4.07it/s] 94%|█████████▍| 94/100 [00:23<00:01,  4.00it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.07it/s] 95%|█████████▌| 95/100 [00:23<00:01,  4.00it/s] 73%|███████▎  | 73/100 [00:18<00:06,  4.08it/s]  9%|▉         | 9/100 [00:09<01:34,  1.04s/it]  7%|▋         | 7/100 [00:06<01:32,  1.01it/s] 96%|█████████▌| 96/100 [00:24<00:01,  4.00it/s] 74%|███████▍  | 74/100 [00:18<00:06,  4.09it/s] 97%|█████████▋| 97/100 [00:24<00:00,  4.00it/s] 75%|███████▌  | 75/100 [00:18<00:06,  4.08it/s] 98%|█████████▊| 98/100 [00:24<00:00,  4.00it/s] 76%|███████▌  | 76/100 [00:18<00:05,  4.08it/s] 99%|█████████▉| 99/100 [00:24<00:00,  4.00it/s] 77%|███████▋  | 77/100 [00:19<00:05,  4.08it/s] 10%|█         | 10/100 [00:10<01:33,  1.04s/it]  8%|▊         | 8/100 [00:07<01:31,  1.01it/s]100%|██████████| 100/100 [00:25<00:00,  4.00it/s]100%|██████████| 100/100 [00:25<00:00,  3.97it/s]
 78%|███████▊  | 78/100 [00:19<00:05,  4.09it/s] 79%|███████▉  | 79/100 [00:19<00:05,  4.08it/s] 80%|████████  | 80/100 [00:19<00:04,  4.07it/s] 81%|████████  | 81/100 [00:20<00:04,  4.06it/s]  9%|▉         | 9/100 [00:08<01:30,  1.01it/s] 11%|█         | 11/100 [00:11<01:32,  1.04s/it] 82%|████████▏ | 82/100 [00:20<00:04,  4.07it/s] 83%|████████▎ | 83/100 [00:20<00:04,  4.08it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.08it/s] 85%|████████▌ | 85/100 [00:21<00:03,  4.07it/s]2_k proxy err 0.00028480502078309655 err 6.440732479095459 tr(WHW.T) 22614.533203125
bpp_loss 5.013985633850098
  0%|          | 0/100 [00:00<?, ?it/s] 10%|█         | 10/100 [00:09<01:29,  1.01it/s] 12%|█▏        | 12/100 [00:12<01:31,  1.04s/it] 86%|████████▌ | 86/100 [00:21<00:03,  4.07it/s] 87%|████████▋ | 87/100 [00:21<00:03,  4.08it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.08it/s] 89%|████████▉ | 89/100 [00:22<00:02,  4.09it/s]  1%|          | 1/100 [00:01<01:40,  1.01s/it] 11%|█         | 11/100 [00:10<01:28,  1.01it/s] 90%|█████████ | 90/100 [00:22<00:02,  4.07it/s] 13%|█▎        | 13/100 [00:13<01:30,  1.04s/it] 91%|█████████ | 91/100 [00:22<00:02,  4.06it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.07it/s] 93%|█████████▎| 93/100 [00:23<00:01,  4.07it/s] 12%|█▏        | 12/100 [00:11<01:27,  1.01it/s]  2%|▏         | 2/100 [00:02<01:38,  1.01s/it] 94%|█████████▍| 94/100 [00:23<00:01,  4.08it/s] 14%|█▍        | 14/100 [00:14<01:29,  1.04s/it] 95%|█████████▌| 95/100 [00:23<00:01,  4.08it/s] 96%|█████████▌| 96/100 [00:23<00:00,  4.08it/s] 97%|█████████▋| 97/100 [00:24<00:00,  4.07it/s] 13%|█▎        | 13/100 [00:12<01:26,  1.01it/s]  3%|▎         | 3/100 [00:03<01:37,  1.00s/it] 98%|█████████▊| 98/100 [00:24<00:00,  4.06it/s] 15%|█▌        | 15/100 [00:15<01:28,  1.04s/it] 99%|█████████▉| 99/100 [00:24<00:00,  4.07it/s]100%|██████████| 100/100 [00:24<00:00,  4.06it/s]100%|██████████| 100/100 [00:24<00:00,  4.04it/s]
 14%|█▍        | 14/100 [00:13<01:25,  1.01it/s]  4%|▍         | 4/100 [00:04<01:36,  1.00s/it] 16%|█▌        | 16/100 [00:16<01:27,  1.04s/it] 15%|█▌        | 15/100 [00:14<01:24,  1.01it/s]  5%|▌         | 5/100 [00:05<01:34,  1.00it/s]3_k proxy err 0.00025002655456773937 err 6.544323921203613 tr(WHW.T) 26174.515625
bpp_loss 5.078547954559326
  0%|          | 0/100 [00:00<?, ?it/s] 17%|█▋        | 17/100 [00:17<01:26,  1.04s/it] 16%|█▌        | 16/100 [00:15<01:23,  1.01it/s]  6%|▌         | 6/100 [00:05<01:33,  1.00it/s]  1%|          | 1/100 [00:00<01:38,  1.00it/s] 18%|█▊        | 18/100 [00:18<01:25,  1.04s/it] 17%|█▋        | 17/100 [00:16<01:22,  1.01it/s]  7%|▋         | 7/100 [00:06<01:32,  1.00it/s]  2%|▏         | 2/100 [00:01<01:36,  1.01it/s] 19%|█▉        | 19/100 [00:19<01:24,  1.04s/it] 18%|█▊        | 18/100 [00:17<01:21,  1.01it/s]  8%|▊         | 8/100 [00:07<01:31,  1.00it/s]  3%|▎         | 3/100 [00:02<01:35,  1.02it/s] 20%|██        | 20/100 [00:20<01:23,  1.04s/it] 19%|█▉        | 19/100 [00:18<01:20,  1.01it/s]  9%|▉         | 9/100 [00:08<01:30,  1.00it/s]  4%|▍         | 4/100 [00:03<01:34,  1.02it/s] 21%|██        | 21/100 [00:21<01:22,  1.04s/it] 20%|██        | 20/100 [00:19<01:19,  1.01it/s] 10%|█         | 10/100 [00:09<01:29,  1.00it/s]  5%|▌         | 5/100 [00:04<01:33,  1.02it/s] 22%|██▏       | 22/100 [00:22<01:21,  1.04s/it] 21%|██        | 21/100 [00:20<01:18,  1.01it/s] 11%|█         | 11/100 [00:10<01:28,  1.00it/s]  6%|▌         | 6/100 [00:05<01:32,  1.02it/s] 23%|██▎       | 23/100 [00:23<01:20,  1.04s/it] 22%|██▏       | 22/100 [00:21<01:17,  1.01it/s] 12%|█▏        | 12/100 [00:11<01:27,  1.00it/s]  7%|▋         | 7/100 [00:06<01:31,  1.02it/s] 24%|██▍       | 24/100 [00:24<01:19,  1.04s/it] 23%|██▎       | 23/100 [00:22<01:16,  1.01it/s] 13%|█▎        | 13/100 [00:12<01:26,  1.00it/s]  8%|▊         | 8/100 [00:07<01:30,  1.02it/s] 25%|██▌       | 25/100 [00:25<01:18,  1.04s/it] 24%|██▍       | 24/100 [00:23<01:15,  1.01it/s]  9%|▉         | 9/100 [00:08<01:29,  1.02it/s] 14%|█▍        | 14/100 [00:13<01:25,  1.00it/s] 26%|██▌       | 26/100 [00:27<01:17,  1.04s/it] 25%|██▌       | 25/100 [00:24<01:14,  1.01it/s] 10%|█         | 10/100 [00:09<01:28,  1.02it/s] 15%|█▌        | 15/100 [00:14<01:24,  1.00it/s] 27%|██▋       | 27/100 [00:28<01:16,  1.04s/it] 26%|██▌       | 26/100 [00:25<01:13,  1.01it/s] 11%|█         | 11/100 [00:10<01:27,  1.02it/s] 16%|█▌        | 16/100 [00:15<01:23,  1.00it/s] 28%|██▊       | 28/100 [00:29<01:15,  1.04s/it] 27%|██▋       | 27/100 [00:26<01:12,  1.01it/s] 12%|█▏        | 12/100 [00:11<01:26,  1.02it/s] 17%|█▋        | 17/100 [00:16<01:22,  1.00it/s] 29%|██▉       | 29/100 [00:30<01:14,  1.04s/it] 28%|██▊       | 28/100 [00:27<01:11,  1.01it/s] 13%|█▎        | 13/100 [00:12<01:25,  1.02it/s] 18%|█▊        | 18/100 [00:17<01:21,  1.00it/s] 30%|███       | 30/100 [00:31<01:13,  1.04s/it] 14%|█▍        | 14/100 [00:13<01:24,  1.02it/s] 29%|██▉       | 29/100 [00:28<01:10,  1.01it/s] 19%|█▉        | 19/100 [00:18<01:20,  1.00it/s] 15%|█▌        | 15/100 [00:14<01:23,  1.02it/s] 30%|███       | 30/100 [00:29<01:09,  1.01it/s] 31%|███       | 31/100 [00:32<01:12,  1.05s/it] 20%|██        | 20/100 [00:19<01:19,  1.00it/s] 16%|█▌        | 16/100 [00:15<01:22,  1.02it/s] 31%|███       | 31/100 [00:30<01:08,  1.01it/s] 32%|███▏      | 32/100 [00:33<01:11,  1.04s/it] 21%|██        | 21/100 [00:20<01:18,  1.00it/s] 17%|█▋        | 17/100 [00:16<01:21,  1.02it/s] 32%|███▏      | 32/100 [00:31<01:07,  1.01it/s] 22%|██▏       | 22/100 [00:21<01:17,  1.00it/s] 33%|███▎      | 33/100 [00:34<01:10,  1.04s/it] 18%|█▊        | 18/100 [00:17<01:20,  1.02it/s] 33%|███▎      | 33/100 [00:32<01:06,  1.01it/s] 23%|██▎       | 23/100 [00:22<01:16,  1.00it/s] 34%|███▍      | 34/100 [00:35<01:08,  1.04s/it] 19%|█▉        | 19/100 [00:18<01:19,  1.02it/s] 34%|███▍      | 34/100 [00:33<01:05,  1.01it/s] 24%|██▍       | 24/100 [00:23<01:15,  1.00it/s] 35%|███▌      | 35/100 [00:36<01:07,  1.05s/it] 20%|██        | 20/100 [00:19<01:18,  1.02it/s] 35%|███▌      | 35/100 [00:34<01:04,  1.01it/s] 25%|██▌       | 25/100 [00:24<01:14,  1.00it/s] 36%|███▌      | 36/100 [00:37<01:06,  1.05s/it] 21%|██        | 21/100 [00:20<01:17,  1.02it/s] 36%|███▌      | 36/100 [00:35<01:03,  1.00it/s] 26%|██▌       | 26/100 [00:25<01:13,  1.00it/s] 37%|███▋      | 37/100 [00:38<01:05,  1.05s/it] 22%|██▏       | 22/100 [00:21<01:16,  1.02it/s] 37%|███▋      | 37/100 [00:36<01:02,  1.00it/s] 27%|██▋       | 27/100 [00:26<01:12,  1.00it/s] 38%|███▊      | 38/100 [00:39<01:04,  1.05s/it] 23%|██▎       | 23/100 [00:22<01:15,  1.02it/s] 38%|███▊      | 38/100 [00:37<01:01,  1.00it/s] 28%|██▊       | 28/100 [00:27<01:11,  1.00it/s] 39%|███▉      | 39/100 [00:40<01:03,  1.05s/it] 24%|██▍       | 24/100 [00:23<01:14,  1.02it/s] 39%|███▉      | 39/100 [00:38<01:00,  1.01it/s] 29%|██▉       | 29/100 [00:28<01:10,  1.00it/s] 40%|████      | 40/100 [00:41<01:02,  1.05s/it] 25%|██▌       | 25/100 [00:24<01:13,  1.02it/s] 40%|████      | 40/100 [00:39<00:59,  1.00it/s] 30%|███       | 30/100 [00:29<01:09,  1.00it/s] 41%|████      | 41/100 [00:42<01:01,  1.05s/it] 26%|██▌       | 26/100 [00:25<01:12,  1.02it/s] 41%|████      | 41/100 [00:40<00:58,  1.00it/s] 31%|███       | 31/100 [00:30<01:08,  1.00it/s] 42%|████▏     | 42/100 [00:43<01:00,  1.05s/it] 27%|██▋       | 27/100 [00:26<01:11,  1.02it/s] 42%|████▏     | 42/100 [00:41<00:57,  1.00it/s] 32%|███▏      | 32/100 [00:31<01:07,  1.00it/s] 43%|████▎     | 43/100 [00:44<00:59,  1.05s/it] 28%|██▊       | 28/100 [00:27<01:10,  1.02it/s] 43%|████▎     | 43/100 [00:42<00:56,  1.00it/s] 33%|███▎      | 33/100 [00:32<01:06,  1.00it/s] 44%|████▍     | 44/100 [00:45<00:58,  1.05s/it] 29%|██▉       | 29/100 [00:28<01:09,  1.02it/s] 44%|████▍     | 44/100 [00:43<00:55,  1.01it/s] 34%|███▍      | 34/100 [00:33<01:05,  1.00it/s] 45%|████▌     | 45/100 [00:46<00:57,  1.05s/it] 30%|███       | 30/100 [00:29<01:08,  1.02it/s] 45%|████▌     | 45/100 [00:44<00:54,  1.00it/s] 35%|███▌      | 35/100 [00:34<01:04,  1.00it/s] 31%|███       | 31/100 [00:30<01:07,  1.02it/s] 46%|████▌     | 46/100 [00:47<00:56,  1.05s/it] 46%|████▌     | 46/100 [00:45<00:53,  1.00it/s] 36%|███▌      | 36/100 [00:35<01:03,  1.00it/s] 32%|███▏      | 32/100 [00:31<01:06,  1.02it/s] 47%|████▋     | 47/100 [00:48<00:55,  1.05s/it] 47%|████▋     | 47/100 [00:46<00:52,  1.00it/s] 37%|███▋      | 37/100 [00:36<01:02,  1.00it/s] 33%|███▎      | 33/100 [00:32<01:05,  1.02it/s] 48%|████▊     | 48/100 [00:50<00:54,  1.05s/it] 48%|████▊     | 48/100 [00:47<00:51,  1.00it/s] 38%|███▊      | 38/100 [00:37<01:01,  1.00it/s] 34%|███▍      | 34/100 [00:33<01:04,  1.02it/s] 49%|████▉     | 49/100 [00:51<00:53,  1.05s/it] 49%|████▉     | 49/100 [00:48<00:50,  1.00it/s] 39%|███▉      | 39/100 [00:38<01:00,  1.00it/s] 35%|███▌      | 35/100 [00:34<01:03,  1.02it/s] 50%|█████     | 50/100 [00:49<00:49,  1.00it/s] 50%|█████     | 50/100 [00:52<00:52,  1.05s/it] 40%|████      | 40/100 [00:39<00:59,  1.00it/s] 36%|███▌      | 36/100 [00:35<01:02,  1.02it/s] 51%|█████     | 51/100 [00:50<00:48,  1.00it/s] 51%|█████     | 51/100 [00:53<00:51,  1.05s/it] 41%|████      | 41/100 [00:40<00:58,  1.00it/s] 37%|███▋      | 37/100 [00:36<01:01,  1.02it/s] 52%|█████▏    | 52/100 [00:51<00:47,  1.00it/s] 52%|█████▏    | 52/100 [00:54<00:50,  1.05s/it] 42%|████▏     | 42/100 [00:41<00:57,  1.00it/s] 38%|███▊      | 38/100 [00:37<01:00,  1.02it/s] 53%|█████▎    | 53/100 [00:52<00:46,  1.00it/s] 53%|█████▎    | 53/100 [00:55<00:49,  1.05s/it] 43%|████▎     | 43/100 [00:42<00:57,  1.00s/it] 39%|███▉      | 39/100 [00:38<00:59,  1.02it/s] 54%|█████▍    | 54/100 [00:53<00:45,  1.00it/s] 44%|████▍     | 44/100 [00:43<00:56,  1.00s/it] 54%|█████▍    | 54/100 [00:56<00:48,  1.05s/it] 40%|████      | 40/100 [00:39<00:58,  1.02it/s] 55%|█████▌    | 55/100 [00:54<00:44,  1.00it/s] 45%|████▌     | 45/100 [00:44<00:55,  1.00s/it] 55%|█████▌    | 55/100 [00:57<00:47,  1.05s/it] 41%|████      | 41/100 [00:40<00:58,  1.02it/s] 56%|█████▌    | 56/100 [00:55<00:43,  1.00it/s] 46%|████▌     | 46/100 [00:45<00:54,  1.00s/it] 56%|█████▌    | 56/100 [00:58<00:46,  1.05s/it] 42%|████▏     | 42/100 [00:41<00:57,  1.02it/s] 57%|█████▋    | 57/100 [00:56<00:42,  1.00it/s] 47%|████▋     | 47/100 [00:46<00:53,  1.00s/it] 57%|█████▋    | 57/100 [00:59<00:45,  1.05s/it] 43%|████▎     | 43/100 [00:42<00:56,  1.02it/s] 58%|█████▊    | 58/100 [00:57<00:41,  1.00it/s] 48%|████▊     | 48/100 [00:47<00:52,  1.00s/it] 58%|█████▊    | 58/100 [01:00<00:44,  1.05s/it] 44%|████▍     | 44/100 [00:43<00:55,  1.02it/s] 59%|█████▉    | 59/100 [00:58<00:40,  1.00it/s] 49%|████▉     | 49/100 [00:48<00:51,  1.00s/it] 59%|█████▉    | 59/100 [01:01<00:43,  1.05s/it] 45%|████▌     | 45/100 [00:44<00:53,  1.02it/s] 60%|██████    | 60/100 [00:59<00:39,  1.00it/s] 50%|█████     | 50/100 [00:49<00:49,  1.00it/s] 46%|████▌     | 46/100 [00:45<00:52,  1.02it/s] 60%|██████    | 60/100 [01:02<00:41,  1.05s/it] 61%|██████    | 61/100 [01:00<00:38,  1.00it/s] 51%|█████     | 51/100 [00:50<00:48,  1.00it/s] 47%|████▋     | 47/100 [00:46<00:52,  1.02it/s] 61%|██████    | 61/100 [01:03<00:40,  1.05s/it] 62%|██████▏   | 62/100 [01:01<00:37,  1.00it/s] 52%|█████▏    | 52/100 [00:51<00:47,  1.00it/s] 48%|████▊     | 48/100 [00:47<00:51,  1.02it/s] 62%|██████▏   | 62/100 [01:04<00:39,  1.05s/it] 63%|██████▎   | 63/100 [01:02<00:36,  1.00it/s] 53%|█████▎    | 53/100 [00:52<00:46,  1.00it/s] 49%|████▉     | 49/100 [00:48<00:50,  1.02it/s] 63%|██████▎   | 63/100 [01:05<00:38,  1.05s/it] 64%|██████▍   | 64/100 [01:03<00:35,  1.00it/s] 54%|█████▍    | 54/100 [00:53<00:45,  1.00it/s] 50%|█████     | 50/100 [00:49<00:49,  1.02it/s] 64%|██████▍   | 64/100 [01:06<00:37,  1.05s/it] 65%|██████▌   | 65/100 [01:04<00:34,  1.00it/s] 55%|█████▌    | 55/100 [00:54<00:44,  1.00it/s] 51%|█████     | 51/100 [00:50<00:48,  1.02it/s] 65%|██████▌   | 65/100 [01:07<00:36,  1.05s/it] 66%|██████▌   | 66/100 [01:05<00:34,  1.00s/it] 56%|█████▌    | 56/100 [00:55<00:43,  1.00it/s] 52%|█████▏    | 52/100 [00:51<00:47,  1.02it/s] 66%|██████▌   | 66/100 [01:08<00:35,  1.05s/it] 67%|██████▋   | 67/100 [01:06<00:32,  1.00it/s] 57%|█████▋    | 57/100 [00:56<00:42,  1.00it/s] 53%|█████▎    | 53/100 [00:52<00:46,  1.02it/s] 67%|██████▋   | 67/100 [01:09<00:34,  1.05s/it] 68%|██████▊   | 68/100 [01:07<00:31,  1.00it/s] 58%|█████▊    | 58/100 [00:57<00:42,  1.00s/it] 54%|█████▍    | 54/100 [00:53<00:45,  1.02it/s] 68%|██████▊   | 68/100 [01:11<00:33,  1.05s/it] 69%|██████▉   | 69/100 [01:08<00:30,  1.00it/s] 59%|█████▉    | 59/100 [00:58<00:40,  1.00it/s] 55%|█████▌    | 55/100 [00:53<00:44,  1.02it/s] 70%|███████   | 70/100 [01:09<00:29,  1.00it/s] 69%|██████▉   | 69/100 [01:12<00:32,  1.05s/it] 60%|██████    | 60/100 [00:59<00:39,  1.00it/s] 56%|█████▌    | 56/100 [00:54<00:43,  1.02it/s] 71%|███████   | 71/100 [01:10<00:28,  1.00it/s] 70%|███████   | 70/100 [01:13<00:31,  1.05s/it] 61%|██████    | 61/100 [01:00<00:38,  1.00it/s] 57%|█████▋    | 57/100 [00:55<00:42,  1.01it/s] 72%|███████▏  | 72/100 [01:11<00:27,  1.00it/s] 71%|███████   | 71/100 [01:14<00:30,  1.05s/it] 62%|██████▏   | 62/100 [01:01<00:38,  1.00s/it] 58%|█████▊    | 58/100 [00:56<00:41,  1.02it/s] 73%|███████▎  | 73/100 [01:12<00:26,  1.00it/s] 72%|███████▏  | 72/100 [01:15<00:29,  1.05s/it] 63%|██████▎   | 63/100 [01:02<00:37,  1.00s/it] 59%|█████▉    | 59/100 [00:57<00:40,  1.02it/s] 74%|███████▍  | 74/100 [01:13<00:25,  1.00it/s] 73%|███████▎  | 73/100 [01:16<00:28,  1.05s/it] 64%|██████▍   | 64/100 [01:03<00:36,  1.00s/it] 60%|██████    | 60/100 [00:58<00:39,  1.01it/s] 75%|███████▌  | 75/100 [01:14<00:24,  1.00it/s] 65%|██████▌   | 65/100 [01:04<00:35,  1.00s/it] 74%|███████▍  | 74/100 [01:17<00:27,  1.05s/it] 61%|██████    | 61/100 [00:59<00:38,  1.01it/s] 76%|███████▌  | 76/100 [01:15<00:23,  1.00it/s] 66%|██████▌   | 66/100 [01:05<00:34,  1.00s/it] 62%|██████▏   | 62/100 [01:00<00:37,  1.01it/s] 75%|███████▌  | 75/100 [01:18<00:26,  1.05s/it] 77%|███████▋  | 77/100 [01:16<00:22,  1.00it/s] 67%|██████▋   | 67/100 [01:06<00:33,  1.00s/it] 63%|██████▎   | 63/100 [01:01<00:36,  1.01it/s] 76%|███████▌  | 76/100 [01:19<00:25,  1.05s/it] 78%|███████▊  | 78/100 [01:17<00:21,  1.00it/s] 68%|██████▊   | 68/100 [01:07<00:32,  1.00s/it] 64%|██████▍   | 64/100 [01:02<00:35,  1.01it/s] 77%|███████▋  | 77/100 [01:20<00:24,  1.05s/it] 79%|███████▉  | 79/100 [01:18<00:20,  1.00it/s] 69%|██████▉   | 69/100 [01:08<00:31,  1.00s/it] 65%|██████▌   | 65/100 [01:03<00:34,  1.01it/s] 78%|███████▊  | 78/100 [01:21<00:23,  1.05s/it] 80%|████████  | 80/100 [01:19<00:19,  1.00it/s] 66%|██████▌   | 66/100 [01:04<00:33,  1.02it/s] 70%|███████   | 70/100 [01:09<00:30,  1.00s/it] 79%|███████▉  | 79/100 [01:22<00:22,  1.05s/it] 81%|████████  | 81/100 [01:20<00:18,  1.00it/s] 67%|██████▋   | 67/100 [01:05<00:32,  1.02it/s] 71%|███████   | 71/100 [01:10<00:29,  1.00s/it] 80%|████████  | 80/100 [01:23<00:21,  1.05s/it] 82%|████████▏ | 82/100 [01:21<00:17,  1.00it/s] 68%|██████▊   | 68/100 [01:06<00:31,  1.02it/s] 72%|███████▏  | 72/100 [01:11<00:28,  1.00s/it] 81%|████████  | 81/100 [01:24<00:20,  1.05s/it] 83%|████████▎ | 83/100 [01:22<00:16,  1.00it/s] 69%|██████▉   | 69/100 [01:07<00:30,  1.02it/s] 73%|███████▎  | 73/100 [01:12<00:27,  1.00s/it] 82%|████████▏ | 82/100 [01:25<00:18,  1.05s/it] 84%|████████▍ | 84/100 [01:23<00:15,  1.00it/s] 70%|███████   | 70/100 [01:08<00:29,  1.02it/s] 74%|███████▍  | 74/100 [01:13<00:26,  1.00s/it] 83%|████████▎ | 83/100 [01:26<00:17,  1.05s/it] 85%|████████▌ | 85/100 [01:24<00:14,  1.00it/s] 71%|███████   | 71/100 [01:09<00:28,  1.02it/s] 75%|███████▌  | 75/100 [01:14<00:25,  1.00s/it] 84%|████████▍ | 84/100 [01:27<00:16,  1.05s/it] 86%|████████▌ | 86/100 [01:25<00:13,  1.00it/s] 72%|███████▏  | 72/100 [01:10<00:27,  1.02it/s] 76%|███████▌  | 76/100 [01:15<00:24,  1.00s/it] 85%|████████▌ | 85/100 [01:28<00:15,  1.05s/it] 87%|████████▋ | 87/100 [01:26<00:12,  1.00it/s] 73%|███████▎  | 73/100 [01:11<00:26,  1.02it/s] 77%|███████▋  | 77/100 [01:16<00:23,  1.00s/it] 86%|████████▌ | 86/100 [01:30<00:14,  1.06s/it] 88%|████████▊ | 88/100 [01:27<00:11,  1.00it/s] 74%|███████▍  | 74/100 [01:12<00:25,  1.02it/s] 78%|███████▊  | 78/100 [01:17<00:22,  1.00s/it] 87%|████████▋ | 87/100 [01:31<00:13,  1.06s/it] 89%|████████▉ | 89/100 [01:28<00:11,  1.00s/it] 75%|███████▌  | 75/100 [01:13<00:24,  1.02it/s] 79%|███████▉  | 79/100 [01:18<00:21,  1.00s/it] 90%|█████████ | 90/100 [01:29<00:09,  1.00it/s] 88%|████████▊ | 88/100 [01:32<00:12,  1.06s/it] 76%|███████▌  | 76/100 [01:14<00:23,  1.02it/s] 80%|████████  | 80/100 [01:19<00:20,  1.00s/it] 91%|█████████ | 91/100 [01:30<00:08,  1.00it/s] 77%|███████▋  | 77/100 [01:15<00:22,  1.02it/s] 89%|████████▉ | 89/100 [01:33<00:11,  1.06s/it] 81%|████████  | 81/100 [01:20<00:19,  1.00s/it] 92%|█████████▏| 92/100 [01:31<00:07,  1.00it/s] 78%|███████▊  | 78/100 [01:16<00:21,  1.02it/s] 90%|█████████ | 90/100 [01:34<00:10,  1.05s/it] 82%|████████▏ | 82/100 [01:21<00:18,  1.00s/it] 93%|█████████▎| 93/100 [01:32<00:06,  1.00it/s] 79%|███████▉  | 79/100 [01:17<00:20,  1.02it/s] 91%|█████████ | 91/100 [01:35<00:09,  1.05s/it] 83%|████████▎ | 83/100 [01:22<00:17,  1.00s/it] 94%|█████████▍| 94/100 [01:33<00:05,  1.00it/s] 80%|████████  | 80/100 [01:18<00:19,  1.02it/s] 92%|█████████▏| 92/100 [01:36<00:08,  1.05s/it] 84%|████████▍ | 84/100 [01:23<00:16,  1.00s/it] 81%|████████  | 81/100 [01:19<00:18,  1.02it/s] 95%|█████████▌| 95/100 [01:34<00:04,  1.00it/s] 85%|████████▌ | 85/100 [01:24<00:15,  1.00s/it] 93%|█████████▎| 93/100 [01:37<00:07,  1.05s/it] 82%|████████▏ | 82/100 [01:20<00:17,  1.02it/s] 96%|█████████▌| 96/100 [01:35<00:04,  1.00s/it] 86%|████████▌ | 86/100 [01:25<00:14,  1.00s/it] 94%|█████████▍| 94/100 [01:38<00:06,  1.05s/it] 83%|████████▎ | 83/100 [01:21<00:16,  1.02it/s] 97%|█████████▋| 97/100 [01:36<00:02,  1.00it/s] 87%|████████▋ | 87/100 [01:26<00:13,  1.00s/it] 95%|█████████▌| 95/100 [01:39<00:05,  1.05s/it] 84%|████████▍ | 84/100 [01:22<00:15,  1.02it/s] 98%|█████████▊| 98/100 [01:37<00:01,  1.00it/s] 88%|████████▊ | 88/100 [01:27<00:12,  1.00s/it] 96%|█████████▌| 96/100 [01:40<00:04,  1.05s/it] 85%|████████▌ | 85/100 [01:23<00:14,  1.02it/s] 99%|█████████▉| 99/100 [01:38<00:00,  1.00it/s] 89%|████████▉ | 89/100 [01:28<00:11,  1.00s/it] 97%|█████████▋| 97/100 [01:41<00:03,  1.05s/it] 86%|████████▌ | 86/100 [01:24<00:13,  1.02it/s]100%|██████████| 100/100 [01:39<00:00,  1.00it/s]100%|██████████| 100/100 [01:39<00:00,  1.00it/s]
 90%|█████████ | 90/100 [01:29<00:10,  1.00s/it] 98%|█████████▊| 98/100 [01:42<00:02,  1.05s/it] 87%|████████▋ | 87/100 [01:25<00:12,  1.02it/s] 91%|█████████ | 91/100 [01:30<00:09,  1.00s/it] 99%|█████████▉| 99/100 [01:43<00:01,  1.05s/it] 88%|████████▊ | 88/100 [01:26<00:11,  1.02it/s] 92%|█████████▏| 92/100 [01:32<00:08,  1.00s/it]100%|██████████| 100/100 [01:44<00:00,  1.05s/it]100%|██████████| 100/100 [01:44<00:00,  1.05s/it]
 89%|████████▉ | 89/100 [01:27<00:10,  1.02it/s]1_o proxy err 0.007924924604594707 err 15.721719741821289 tr(WHW.T) 1983.8321533203125
bpp_loss 3.3446362018585205
  0%|          | 0/100 [00:00<?, ?it/s] 93%|█████████▎| 93/100 [01:33<00:07,  1.00s/it] 90%|█████████ | 90/100 [01:28<00:09,  1.01it/s] 94%|█████████▍| 94/100 [01:34<00:06,  1.00s/it] 91%|█████████ | 91/100 [01:29<00:08,  1.01it/s] 95%|█████████▌| 95/100 [01:35<00:05,  1.01s/it] 92%|█████████▏| 92/100 [01:30<00:07,  1.02it/s]0_o proxy err 0.004550734534859657 err 14.212933540344238 tr(WHW.T) 3123.217529296875
bpp_loss 3.2557839155197144
  0%|          | 0/100 [00:00<?, ?it/s] 96%|█████████▌| 96/100 [01:36<00:04,  1.01s/it]  1%|          | 1/100 [00:03<05:51,  3.55s/it] 93%|█████████▎| 93/100 [01:31<00:06,  1.01it/s] 97%|█████████▋| 97/100 [01:37<00:03,  1.01s/it] 94%|█████████▍| 94/100 [01:32<00:05,  1.01it/s] 98%|█████████▊| 98/100 [01:38<00:02,  1.00s/it] 95%|█████████▌| 95/100 [01:33<00:04,  1.02it/s] 99%|█████████▉| 99/100 [01:39<00:01,  1.00s/it] 96%|█████████▌| 96/100 [01:34<00:03,  1.02it/s]  1%|          | 1/100 [00:04<06:41,  4.05s/it]  2%|▏         | 2/100 [00:07<05:46,  3.53s/it]100%|██████████| 100/100 [01:40<00:00,  1.00s/it]100%|██████████| 100/100 [01:40<00:00,  1.00s/it]
 97%|█████████▋| 97/100 [01:35<00:02,  1.01it/s] 98%|█████████▊| 98/100 [01:36<00:01,  1.01it/s] 99%|█████████▉| 99/100 [01:37<00:00,  1.01it/s]2_o proxy err 0.007517730817198753 err 14.789801597595215 tr(WHW.T) 1967.3226318359375
bpp_loss 3.284485936164856
  0%|          | 0/100 [00:00<?, ?it/s]  3%|▎         | 3/100 [00:10<05:41,  3.53s/it]  2%|▏         | 2/100 [00:07<06:17,  3.86s/it]100%|██████████| 100/100 [01:38<00:00,  1.01it/s]100%|██████████| 100/100 [01:38<00:00,  1.02it/s]
3_o proxy err 0.009105652570724487 err 16.910633087158203 tr(WHW.T) 1857.15771484375
bpp_loss 3.402063488960266
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:03<06:04,  3.68s/it]  4%|▍         | 4/100 [00:14<05:39,  3.53s/it]  3%|▎         | 3/100 [00:11<06:07,  3.79s/it]  1%|          | 1/100 [00:03<05:43,  3.47s/it]  2%|▏         | 2/100 [00:07<05:52,  3.60s/it]  5%|▌         | 5/100 [00:17<05:35,  3.53s/it]  4%|▍         | 4/100 [00:15<06:01,  3.76s/it]  2%|▏         | 2/100 [00:06<05:40,  3.48s/it]  3%|▎         | 3/100 [00:10<05:46,  3.57s/it]  6%|▌         | 6/100 [00:21<05:31,  3.53s/it]  5%|▌         | 5/100 [00:18<05:55,  3.75s/it]  3%|▎         | 3/100 [00:10<05:38,  3.49s/it]  4%|▍         | 4/100 [00:14<05:42,  3.56s/it]  7%|▋         | 7/100 [00:24<05:28,  3.53s/it]  6%|▌         | 6/100 [00:22<05:50,  3.73s/it]  4%|▍         | 4/100 [00:13<05:34,  3.48s/it]  5%|▌         | 5/100 [00:17<05:37,  3.56s/it]  8%|▊         | 8/100 [00:28<05:24,  3.53s/it]  7%|▋         | 7/100 [00:26<05:46,  3.73s/it]  5%|▌         | 5/100 [00:17<05:30,  3.48s/it]  9%|▉         | 9/100 [00:31<05:21,  3.53s/it]  6%|▌         | 6/100 [00:21<05:34,  3.56s/it]  8%|▊         | 8/100 [00:30<05:43,  3.73s/it]  6%|▌         | 6/100 [00:20<05:27,  3.48s/it] 10%|█         | 10/100 [00:35<05:17,  3.53s/it]  7%|▋         | 7/100 [00:24<05:30,  3.56s/it]  9%|▉         | 9/100 [00:33<05:39,  3.73s/it]  7%|▋         | 7/100 [00:24<05:23,  3.48s/it] 11%|█         | 11/100 [00:38<05:14,  3.53s/it]  8%|▊         | 8/100 [00:28<05:27,  3.56s/it] 10%|█         | 10/100 [00:37<05:36,  3.74s/it]  8%|▊         | 8/100 [00:27<05:20,  3.48s/it] 12%|█▏        | 12/100 [00:42<05:10,  3.53s/it]  9%|▉         | 9/100 [00:32<05:23,  3.56s/it] 11%|█         | 11/100 [00:41<05:32,  3.74s/it]  9%|▉         | 9/100 [00:31<05:16,  3.48s/it] 13%|█▎        | 13/100 [00:45<05:07,  3.53s/it] 10%|█         | 10/100 [00:35<05:20,  3.56s/it] 12%|█▏        | 12/100 [00:45<05:28,  3.73s/it] 10%|█         | 10/100 [00:34<05:12,  3.48s/it] 14%|█▍        | 14/100 [00:49<05:03,  3.53s/it] 11%|█         | 11/100 [00:39<05:16,  3.55s/it] 13%|█▎        | 13/100 [00:48<05:24,  3.73s/it] 11%|█         | 11/100 [00:38<05:09,  3.48s/it] 15%|█▌        | 15/100 [00:52<04:59,  3.53s/it] 12%|█▏        | 12/100 [00:42<05:13,  3.56s/it] 14%|█▍        | 14/100 [00:52<05:20,  3.73s/it] 12%|█▏        | 12/100 [00:41<05:05,  3.48s/it] 16%|█▌        | 16/100 [00:56<04:57,  3.54s/it] 13%|█▎        | 13/100 [00:46<05:09,  3.56s/it] 13%|█▎        | 13/100 [00:45<05:02,  3.48s/it] 15%|█▌        | 15/100 [00:56<05:17,  3.73s/it] 17%|█▋        | 17/100 [01:00<04:53,  3.54s/it] 14%|█▍        | 14/100 [00:49<05:06,  3.56s/it] 14%|█▍        | 14/100 [00:48<04:59,  3.48s/it] 16%|█▌        | 16/100 [00:59<05:13,  3.73s/it] 18%|█▊        | 18/100 [01:03<04:50,  3.54s/it] 15%|█▌        | 15/100 [00:53<05:02,  3.56s/it] 15%|█▌        | 15/100 [00:52<04:55,  3.48s/it] 17%|█▋        | 17/100 [01:03<05:10,  3.74s/it] 19%|█▉        | 19/100 [01:07<04:46,  3.54s/it] 16%|█▌        | 16/100 [00:57<04:58,  3.56s/it] 16%|█▌        | 16/100 [00:55<04:52,  3.48s/it] 18%|█▊        | 18/100 [01:07<05:06,  3.73s/it] 20%|██        | 20/100 [01:10<04:42,  3.54s/it] 17%|█▋        | 17/100 [01:00<04:55,  3.56s/it] 17%|█▋        | 17/100 [00:59<04:48,  3.48s/it] 19%|█▉        | 19/100 [01:11<05:02,  3.74s/it] 21%|██        | 21/100 [01:14<04:39,  3.54s/it] 18%|█▊        | 18/100 [01:04<04:51,  3.56s/it] 18%|█▊        | 18/100 [01:02<04:45,  3.48s/it] 22%|██▏       | 22/100 [01:17<04:36,  3.55s/it] 20%|██        | 20/100 [01:14<04:59,  3.74s/it] 19%|█▉        | 19/100 [01:07<04:47,  3.55s/it] 19%|█▉        | 19/100 [01:06<04:41,  3.48s/it] 23%|██▎       | 23/100 [01:21<04:32,  3.54s/it] 20%|██        | 20/100 [01:11<04:44,  3.55s/it] 21%|██        | 21/100 [01:18<04:55,  3.75s/it] 20%|██        | 20/100 [01:09<04:38,  3.48s/it] 24%|██▍       | 24/100 [01:24<04:30,  3.55s/it] 21%|██        | 21/100 [01:14<04:40,  3.55s/it] 22%|██▏       | 22/100 [01:22<04:52,  3.75s/it] 21%|██        | 21/100 [01:13<04:35,  3.48s/it] 25%|██▌       | 25/100 [01:28<04:26,  3.56s/it] 22%|██▏       | 22/100 [01:18<04:36,  3.55s/it] 23%|██▎       | 23/100 [01:26<04:48,  3.75s/it] 22%|██▏       | 22/100 [01:16<04:32,  3.49s/it] 26%|██▌       | 26/100 [01:32<04:22,  3.55s/it] 23%|██▎       | 23/100 [01:21<04:33,  3.55s/it] 24%|██▍       | 24/100 [01:29<04:44,  3.75s/it] 23%|██▎       | 23/100 [01:20<04:28,  3.49s/it] 27%|██▋       | 27/100 [01:35<04:19,  3.55s/it] 24%|██▍       | 24/100 [01:25<04:30,  3.55s/it] 25%|██▌       | 25/100 [01:33<04:41,  3.75s/it] 24%|██▍       | 24/100 [01:23<04:25,  3.49s/it] 28%|██▊       | 28/100 [01:39<04:15,  3.55s/it] 25%|██▌       | 25/100 [01:28<04:26,  3.55s/it] 26%|██▌       | 26/100 [01:37<04:37,  3.75s/it] 25%|██▌       | 25/100 [01:27<04:21,  3.49s/it] 29%|██▉       | 29/100 [01:42<04:12,  3.55s/it] 26%|██▌       | 26/100 [01:32<04:23,  3.56s/it] 27%|██▋       | 27/100 [01:41<04:34,  3.76s/it] 26%|██▌       | 26/100 [01:30<04:18,  3.49s/it] 30%|███       | 30/100 [01:46<04:08,  3.55s/it] 27%|██▋       | 27/100 [01:36<04:20,  3.56s/it] 27%|██▋       | 27/100 [01:34<04:14,  3.49s/it] 28%|██▊       | 28/100 [01:44<04:30,  3.76s/it] 31%|███       | 31/100 [01:49<04:04,  3.54s/it] 28%|██▊       | 28/100 [01:39<04:16,  3.56s/it] 28%|██▊       | 28/100 [01:37<04:11,  3.50s/it] 29%|██▉       | 29/100 [01:48<04:26,  3.76s/it] 32%|███▏      | 32/100 [01:53<04:01,  3.55s/it] 29%|██▉       | 29/100 [01:43<04:13,  3.56s/it] 29%|██▉       | 29/100 [01:41<04:07,  3.49s/it] 30%|███       | 30/100 [01:52<04:22,  3.75s/it] 33%|███▎      | 33/100 [01:56<03:57,  3.54s/it] 30%|███       | 30/100 [01:46<04:09,  3.57s/it] 30%|███       | 30/100 [01:44<04:04,  3.50s/it] 31%|███       | 31/100 [01:56<04:18,  3.75s/it] 34%|███▍      | 34/100 [02:00<03:53,  3.54s/it] 31%|███       | 31/100 [01:50<04:06,  3.57s/it] 31%|███       | 31/100 [01:48<04:01,  3.50s/it] 32%|███▏      | 32/100 [01:59<04:15,  3.75s/it] 35%|███▌      | 35/100 [02:03<03:50,  3.54s/it] 32%|███▏      | 32/100 [01:53<04:02,  3.57s/it] 32%|███▏      | 32/100 [01:51<03:58,  3.50s/it] 33%|███▎      | 33/100 [02:03<04:11,  3.75s/it] 36%|███▌      | 36/100 [02:07<03:46,  3.54s/it] 33%|███▎      | 33/100 [01:57<03:58,  3.57s/it] 33%|███▎      | 33/100 [01:55<03:54,  3.49s/it] 34%|███▍      | 34/100 [02:07<04:07,  3.75s/it] 37%|███▋      | 37/100 [02:11<03:43,  3.55s/it] 34%|███▍      | 34/100 [02:01<03:56,  3.58s/it] 34%|███▍      | 34/100 [01:58<03:50,  3.49s/it] 35%|███▌      | 35/100 [02:11<04:03,  3.75s/it] 38%|███▊      | 38/100 [02:14<03:40,  3.55s/it] 35%|███▌      | 35/100 [02:04<03:52,  3.57s/it] 35%|███▌      | 35/100 [02:01<03:46,  3.48s/it] 36%|███▌      | 36/100 [02:14<04:00,  3.75s/it] 39%|███▉      | 39/100 [02:18<03:36,  3.55s/it] 36%|███▌      | 36/100 [02:08<03:48,  3.57s/it] 36%|███▌      | 36/100 [02:05<03:42,  3.48s/it] 37%|███▋      | 37/100 [02:18<03:56,  3.75s/it] 40%|████      | 40/100 [02:21<03:33,  3.55s/it] 37%|███▋      | 37/100 [02:11<03:44,  3.57s/it] 37%|███▋      | 37/100 [02:08<03:39,  3.48s/it] 41%|████      | 41/100 [02:25<03:29,  3.55s/it] 38%|███▊      | 38/100 [02:22<03:52,  3.75s/it] 38%|███▊      | 38/100 [02:15<03:41,  3.57s/it] 38%|███▊      | 38/100 [02:12<03:35,  3.48s/it] 42%|████▏     | 42/100 [02:28<03:25,  3.55s/it] 39%|███▉      | 39/100 [02:26<03:49,  3.75s/it] 39%|███▉      | 39/100 [02:18<03:37,  3.57s/it] 39%|███▉      | 39/100 [02:15<03:32,  3.48s/it] 43%|████▎     | 43/100 [02:32<03:22,  3.55s/it] 40%|████      | 40/100 [02:22<03:33,  3.56s/it] 40%|████      | 40/100 [02:30<03:45,  3.76s/it] 40%|████      | 40/100 [02:19<03:29,  3.49s/it] 44%|████▍     | 44/100 [02:35<03:18,  3.55s/it] 41%|████      | 41/100 [02:26<03:30,  3.57s/it] 41%|████      | 41/100 [02:22<03:25,  3.48s/it] 41%|████      | 41/100 [02:33<03:41,  3.76s/it] 45%|████▌     | 45/100 [02:39<03:14,  3.54s/it] 42%|████▏     | 42/100 [02:29<03:26,  3.57s/it] 42%|████▏     | 42/100 [02:26<03:22,  3.49s/it] 42%|████▏     | 42/100 [02:37<03:37,  3.76s/it] 46%|████▌     | 46/100 [02:42<03:11,  3.55s/it] 43%|████▎     | 43/100 [02:29<03:19,  3.50s/it] 43%|████▎     | 43/100 [02:33<03:23,  3.57s/it] 43%|████▎     | 43/100 [02:41<03:34,  3.76s/it] 47%|████▋     | 47/100 [02:46<03:08,  3.55s/it] 44%|████▍     | 44/100 [02:33<03:15,  3.49s/it] 44%|████▍     | 44/100 [02:36<03:19,  3.57s/it] 44%|████▍     | 44/100 [02:45<03:30,  3.76s/it] 48%|████▊     | 48/100 [02:50<03:05,  3.56s/it] 45%|████▌     | 45/100 [02:36<03:12,  3.49s/it] 45%|████▌     | 45/100 [02:40<03:16,  3.56s/it] 45%|████▌     | 45/100 [02:48<03:26,  3.76s/it] 49%|████▉     | 49/100 [02:53<03:01,  3.56s/it] 46%|████▌     | 46/100 [02:40<03:08,  3.50s/it] 46%|████▌     | 46/100 [02:43<03:12,  3.57s/it] 46%|████▌     | 46/100 [02:52<03:22,  3.76s/it] 50%|█████     | 50/100 [02:57<02:57,  3.56s/it] 47%|████▋     | 47/100 [02:43<03:05,  3.49s/it] 47%|████▋     | 47/100 [02:47<03:09,  3.57s/it] 47%|████▋     | 47/100 [02:56<03:19,  3.76s/it] 51%|█████     | 51/100 [03:00<02:54,  3.56s/it] 48%|████▊     | 48/100 [02:47<03:01,  3.49s/it] 48%|████▊     | 48/100 [02:51<03:05,  3.56s/it] 48%|████▊     | 48/100 [03:00<03:15,  3.76s/it] 52%|█████▏    | 52/100 [03:04<02:50,  3.55s/it] 49%|████▉     | 49/100 [02:50<02:57,  3.48s/it] 49%|████▉     | 49/100 [02:54<03:01,  3.57s/it] 49%|████▉     | 49/100 [03:03<03:11,  3.76s/it] 53%|█████▎    | 53/100 [03:07<02:46,  3.55s/it] 50%|█████     | 50/100 [02:54<02:54,  3.50s/it] 50%|█████     | 50/100 [02:58<02:58,  3.57s/it] 50%|█████     | 50/100 [03:07<03:07,  3.76s/it] 54%|█████▍    | 54/100 [03:11<02:43,  3.55s/it] 51%|█████     | 51/100 [02:57<02:51,  3.49s/it] 51%|█████     | 51/100 [03:01<02:54,  3.57s/it] 51%|█████     | 51/100 [03:11<03:04,  3.76s/it] 55%|█████▌    | 55/100 [03:14<02:39,  3.55s/it] 52%|█████▏    | 52/100 [03:01<02:47,  3.49s/it] 52%|█████▏    | 52/100 [03:05<02:51,  3.58s/it] 52%|█████▏    | 52/100 [03:15<03:00,  3.76s/it] 53%|█████▎    | 53/100 [03:04<02:43,  3.48s/it] 56%|█████▌    | 56/100 [03:18<02:36,  3.55s/it] 53%|█████▎    | 53/100 [03:08<02:47,  3.57s/it] 53%|█████▎    | 53/100 [03:18<02:56,  3.76s/it] 54%|█████▍    | 54/100 [03:08<02:40,  3.48s/it] 57%|█████▋    | 57/100 [03:22<02:32,  3.55s/it] 54%|█████▍    | 54/100 [03:12<02:44,  3.57s/it] 55%|█████▌    | 55/100 [03:11<02:36,  3.48s/it] 54%|█████▍    | 54/100 [03:22<02:52,  3.76s/it] 58%|█████▊    | 58/100 [03:25<02:28,  3.54s/it] 55%|█████▌    | 55/100 [03:16<02:40,  3.57s/it] 56%|█████▌    | 56/100 [03:15<02:33,  3.48s/it] 59%|█████▉    | 59/100 [03:29<02:25,  3.54s/it] 55%|█████▌    | 55/100 [03:26<02:49,  3.76s/it] 56%|█████▌    | 56/100 [03:19<02:37,  3.57s/it] 57%|█████▋    | 57/100 [03:18<02:29,  3.48s/it] 60%|██████    | 60/100 [03:32<02:21,  3.54s/it] 56%|█████▌    | 56/100 [03:30<02:45,  3.76s/it] 57%|█████▋    | 57/100 [03:23<02:33,  3.57s/it] 58%|█████▊    | 58/100 [03:22<02:26,  3.48s/it] 61%|██████    | 61/100 [03:36<02:18,  3.55s/it] 57%|█████▋    | 57/100 [03:33<02:41,  3.76s/it] 58%|█████▊    | 58/100 [03:26<02:29,  3.57s/it] 59%|█████▉    | 59/100 [03:25<02:22,  3.49s/it] 62%|██████▏   | 62/100 [03:39<02:14,  3.55s/it] 58%|█████▊    | 58/100 [03:37<02:37,  3.76s/it] 59%|█████▉    | 59/100 [03:30<02:26,  3.57s/it] 60%|██████    | 60/100 [03:29<02:19,  3.50s/it] 63%|██████▎   | 63/100 [03:43<02:11,  3.55s/it] 60%|██████    | 60/100 [03:33<02:22,  3.57s/it] 59%|█████▉    | 59/100 [03:41<02:34,  3.76s/it] 61%|██████    | 61/100 [03:32<02:16,  3.49s/it] 64%|██████▍   | 64/100 [03:46<02:07,  3.56s/it] 61%|██████    | 61/100 [03:37<02:19,  3.58s/it] 60%|██████    | 60/100 [03:45<02:30,  3.76s/it] 62%|██████▏   | 62/100 [03:36<02:12,  3.49s/it] 65%|██████▌   | 65/100 [03:50<02:04,  3.55s/it] 62%|██████▏   | 62/100 [03:41<02:15,  3.57s/it] 61%|██████    | 61/100 [03:48<02:26,  3.76s/it] 63%|██████▎   | 63/100 [03:39<02:09,  3.49s/it] 66%|██████▌   | 66/100 [03:53<02:00,  3.55s/it] 63%|██████▎   | 63/100 [03:44<02:12,  3.57s/it] 62%|██████▏   | 62/100 [03:52<02:22,  3.76s/it] 64%|██████▍   | 64/100 [03:43<02:05,  3.48s/it] 67%|██████▋   | 67/100 [03:57<01:56,  3.54s/it] 64%|██████▍   | 64/100 [03:48<02:08,  3.57s/it] 63%|██████▎   | 63/100 [03:56<02:18,  3.75s/it] 65%|██████▌   | 65/100 [03:46<02:02,  3.49s/it] 68%|██████▊   | 68/100 [04:01<01:53,  3.54s/it] 65%|██████▌   | 65/100 [03:51<02:04,  3.56s/it] 64%|██████▍   | 64/100 [04:00<02:15,  3.77s/it] 66%|██████▌   | 66/100 [03:50<01:58,  3.49s/it] 69%|██████▉   | 69/100 [04:04<01:49,  3.54s/it] 66%|██████▌   | 66/100 [03:55<02:01,  3.56s/it] 65%|██████▌   | 65/100 [04:03<02:11,  3.76s/it] 67%|██████▋   | 67/100 [03:53<01:55,  3.49s/it] 70%|███████   | 70/100 [04:08<01:46,  3.55s/it] 67%|██████▋   | 67/100 [03:58<01:57,  3.56s/it] 66%|██████▌   | 66/100 [04:07<02:07,  3.76s/it] 68%|██████▊   | 68/100 [03:57<01:51,  3.49s/it] 71%|███████   | 71/100 [04:11<01:42,  3.54s/it] 68%|██████▊   | 68/100 [04:02<01:54,  3.56s/it] 69%|██████▉   | 69/100 [04:00<01:48,  3.49s/it] 67%|██████▋   | 67/100 [04:11<02:04,  3.76s/it] 72%|███████▏  | 72/100 [04:15<01:39,  3.55s/it] 69%|██████▉   | 69/100 [04:05<01:50,  3.57s/it] 70%|███████   | 70/100 [04:04<01:44,  3.49s/it] 68%|██████▊   | 68/100 [04:15<02:00,  3.76s/it] 73%|███████▎  | 73/100 [04:18<01:35,  3.55s/it] 70%|███████   | 70/100 [04:09<01:47,  3.57s/it] 71%|███████   | 71/100 [04:07<01:41,  3.49s/it] 69%|██████▉   | 69/100 [04:18<01:56,  3.76s/it] 74%|███████▍  | 74/100 [04:22<01:32,  3.55s/it] 71%|███████   | 71/100 [04:13<01:43,  3.57s/it] 72%|███████▏  | 72/100 [04:11<01:37,  3.49s/it] 70%|███████   | 70/100 [04:22<01:52,  3.76s/it] 75%|███████▌  | 75/100 [04:25<01:28,  3.55s/it] 72%|███████▏  | 72/100 [04:16<01:39,  3.57s/it] 73%|███████▎  | 73/100 [04:14<01:34,  3.49s/it] 76%|███████▌  | 76/100 [04:29<01:25,  3.54s/it] 71%|███████   | 71/100 [04:26<01:48,  3.76s/it] 73%|███████▎  | 73/100 [04:20<01:36,  3.57s/it] 74%|███████▍  | 74/100 [04:18<01:30,  3.49s/it] 77%|███████▋  | 77/100 [04:32<01:21,  3.55s/it] 72%|███████▏  | 72/100 [04:30<01:45,  3.76s/it] 74%|███████▍  | 74/100 [04:23<01:32,  3.57s/it] 75%|███████▌  | 75/100 [04:21<01:27,  3.49s/it] 78%|███████▊  | 78/100 [04:36<01:18,  3.55s/it] 73%|███████▎  | 73/100 [04:34<01:41,  3.76s/it] 75%|███████▌  | 75/100 [04:27<01:29,  3.57s/it] 76%|███████▌  | 76/100 [04:25<01:23,  3.49s/it] 79%|███████▉  | 79/100 [04:40<01:14,  3.54s/it] 74%|███████▍  | 74/100 [04:37<01:37,  3.76s/it] 76%|███████▌  | 76/100 [04:30<01:25,  3.57s/it] 77%|███████▋  | 77/100 [04:28<01:20,  3.49s/it] 80%|████████  | 80/100 [04:43<01:10,  3.54s/it] 75%|███████▌  | 75/100 [04:41<01:33,  3.76s/it] 77%|███████▋  | 77/100 [04:34<01:22,  3.57s/it] 78%|███████▊  | 78/100 [04:31<01:16,  3.49s/it] 81%|████████  | 81/100 [04:47<01:07,  3.54s/it] 76%|███████▌  | 76/100 [04:45<01:30,  3.76s/it] 78%|███████▊  | 78/100 [04:38<01:18,  3.58s/it] 79%|███████▉  | 79/100 [04:35<01:13,  3.50s/it] 82%|████████▏ | 82/100 [04:50<01:03,  3.55s/it] 77%|███████▋  | 77/100 [04:49<01:26,  3.76s/it] 79%|███████▉  | 79/100 [04:41<01:15,  3.58s/it] 80%|████████  | 80/100 [04:39<01:09,  3.50s/it] 83%|████████▎ | 83/100 [04:54<01:00,  3.54s/it] 80%|████████  | 80/100 [04:45<01:11,  3.58s/it] 78%|███████▊  | 78/100 [04:52<01:22,  3.76s/it] 81%|████████  | 81/100 [04:42<01:06,  3.50s/it] 84%|████████▍ | 84/100 [04:57<00:56,  3.54s/it] 81%|████████  | 81/100 [04:48<01:07,  3.57s/it] 79%|███████▉  | 79/100 [04:56<01:18,  3.76s/it] 82%|████████▏ | 82/100 [04:45<01:02,  3.49s/it] 85%|████████▌ | 85/100 [05:01<00:53,  3.54s/it] 82%|████████▏ | 82/100 [04:52<01:04,  3.57s/it] 83%|████████▎ | 83/100 [04:49<00:59,  3.49s/it] 80%|████████  | 80/100 [05:00<01:15,  3.76s/it] 86%|████████▌ | 86/100 [05:04<00:49,  3.55s/it] 83%|████████▎ | 83/100 [04:56<01:00,  3.57s/it] 84%|████████▍ | 84/100 [04:52<00:55,  3.50s/it] 81%|████████  | 81/100 [05:04<01:11,  3.76s/it] 87%|████████▋ | 87/100 [05:08<00:46,  3.55s/it] 84%|████████▍ | 84/100 [04:59<00:57,  3.57s/it] 85%|████████▌ | 85/100 [04:56<00:52,  3.49s/it] 82%|████████▏ | 82/100 [05:07<01:07,  3.77s/it] 88%|████████▊ | 88/100 [05:11<00:42,  3.55s/it] 85%|████████▌ | 85/100 [05:03<00:53,  3.57s/it] 86%|████████▌ | 86/100 [04:59<00:48,  3.49s/it] 83%|████████▎ | 83/100 [05:11<01:03,  3.76s/it] 89%|████████▉ | 89/100 [05:15<00:39,  3.55s/it] 86%|████████▌ | 86/100 [05:06<00:50,  3.57s/it] 87%|████████▋ | 87/100 [05:03<00:45,  3.49s/it] 84%|████████▍ | 84/100 [05:15<01:00,  3.76s/it] 90%|█████████ | 90/100 [05:19<00:35,  3.55s/it] 88%|████████▊ | 88/100 [05:06<00:41,  3.48s/it] 87%|████████▋ | 87/100 [05:10<00:46,  3.58s/it] 85%|████████▌ | 85/100 [05:19<00:56,  3.76s/it] 91%|█████████ | 91/100 [05:22<00:31,  3.54s/it] 89%|████████▉ | 89/100 [05:10<00:38,  3.49s/it] 88%|████████▊ | 88/100 [05:13<00:42,  3.58s/it] 86%|████████▌ | 86/100 [05:22<00:52,  3.76s/it] 92%|█████████▏| 92/100 [05:26<00:28,  3.55s/it] 90%|█████████ | 90/100 [05:13<00:34,  3.49s/it] 89%|████████▉ | 89/100 [05:17<00:39,  3.57s/it] 87%|████████▋ | 87/100 [05:26<00:48,  3.76s/it] 93%|█████████▎| 93/100 [05:29<00:24,  3.55s/it] 91%|█████████ | 91/100 [05:17<00:31,  3.48s/it] 90%|█████████ | 90/100 [05:21<00:35,  3.57s/it] 94%|█████████▍| 94/100 [05:33<00:21,  3.54s/it] 88%|████████▊ | 88/100 [05:30<00:45,  3.76s/it] 92%|█████████▏| 92/100 [05:20<00:27,  3.49s/it] 91%|█████████ | 91/100 [05:24<00:32,  3.57s/it] 95%|█████████▌| 95/100 [05:36<00:17,  3.54s/it] 89%|████████▉ | 89/100 [05:34<00:41,  3.76s/it] 93%|█████████▎| 93/100 [05:24<00:24,  3.49s/it] 92%|█████████▏| 92/100 [05:28<00:28,  3.57s/it] 96%|█████████▌| 96/100 [05:40<00:14,  3.54s/it] 90%|█████████ | 90/100 [05:37<00:37,  3.76s/it] 94%|█████████▍| 94/100 [05:27<00:20,  3.49s/it] 93%|█████████▎| 93/100 [05:31<00:24,  3.57s/it] 97%|█████████▋| 97/100 [05:43<00:10,  3.55s/it] 91%|█████████ | 91/100 [05:41<00:33,  3.76s/it] 95%|█████████▌| 95/100 [05:31<00:17,  3.49s/it] 94%|█████████▍| 94/100 [05:35<00:21,  3.57s/it] 98%|█████████▊| 98/100 [05:47<00:07,  3.55s/it] 92%|█████████▏| 92/100 [05:45<00:29,  3.75s/it] 96%|█████████▌| 96/100 [05:34<00:13,  3.49s/it] 95%|█████████▌| 95/100 [05:38<00:17,  3.57s/it] 99%|█████████▉| 99/100 [05:50<00:03,  3.54s/it] 97%|█████████▋| 97/100 [05:38<00:10,  3.49s/it] 93%|█████████▎| 93/100 [05:49<00:26,  3.75s/it] 96%|█████████▌| 96/100 [05:42<00:14,  3.57s/it]100%|██████████| 100/100 [05:54<00:00,  3.54s/it]100%|██████████| 100/100 [05:54<00:00,  3.54s/it]
 98%|█████████▊| 98/100 [05:41<00:06,  3.49s/it] 94%|█████████▍| 94/100 [05:52<00:22,  3.75s/it] 97%|█████████▋| 97/100 [05:46<00:10,  3.57s/it] 99%|█████████▉| 99/100 [05:45<00:03,  3.51s/it] 95%|█████████▌| 95/100 [05:56<00:18,  3.75s/it] 98%|█████████▊| 98/100 [05:49<00:07,  3.57s/it]100%|██████████| 100/100 [05:48<00:00,  3.51s/it]100%|██████████| 100/100 [05:48<00:00,  3.49s/it]
 96%|█████████▌| 96/100 [06:00<00:15,  3.76s/it] 99%|█████████▉| 99/100 [05:53<00:03,  3.57s/it]1_up proxy err 0.008183712139725685 err 67.3586654663086 tr(WHW.T) 8230.8203125
bpp_loss 3.580225331442697
  0%|          | 0/100 [00:00<?, ?it/s]100%|██████████| 100/100 [05:56<00:00,  3.57s/it]100%|██████████| 100/100 [05:56<00:00,  3.57s/it]
 97%|█████████▋| 97/100 [06:04<00:11,  3.76s/it]  1%|          | 1/100 [00:03<05:43,  3.47s/it] 98%|█████████▊| 98/100 [06:07<00:07,  3.76s/it]  2%|▏         | 2/100 [00:07<05:44,  3.52s/it]3_up proxy err 0.008926007896661758 err 67.27054595947266 tr(WHW.T) 7536.46484375
bpp_loss 3.548671211515154
  0%|          | 0/100 [00:00<?, ?it/s]  3%|▎         | 3/100 [00:10<05:39,  3.50s/it] 99%|█████████▉| 99/100 [06:11<00:03,  3.76s/it]  1%|          | 1/100 [00:03<05:42,  3.46s/it]2_up proxy err 0.008847074583172798 err 67.2419662475586 tr(WHW.T) 7600.474609375
bpp_loss 3.5744149344308034
  0%|          | 0/100 [00:00<?, ?it/s]  4%|▍         | 4/100 [00:13<05:35,  3.49s/it]100%|██████████| 100/100 [06:15<00:00,  3.76s/it]100%|██████████| 100/100 [06:15<00:00,  3.75s/it]
  2%|▏         | 2/100 [00:06<05:38,  3.46s/it]  1%|          | 1/100 [00:03<05:47,  3.51s/it]  5%|▌         | 5/100 [00:17<05:31,  3.49s/it]  3%|▎         | 3/100 [00:10<05:36,  3.46s/it]  2%|▏         | 2/100 [00:07<05:46,  3.54s/it]  6%|▌         | 6/100 [00:20<05:28,  3.49s/it]  4%|▍         | 4/100 [00:13<05:32,  3.46s/it]  3%|▎         | 3/100 [00:10<05:42,  3.53s/it]0_up proxy err 0.007544700987637043 err 67.33231353759766 tr(WHW.T) 8924.451171875
bpp_loss 3.561313901628767
  0%|          | 0/100 [00:00<?, ?it/s]  7%|▋         | 7/100 [00:24<05:25,  3.50s/it]  5%|▌         | 5/100 [00:17<05:29,  3.47s/it]  4%|▍         | 4/100 [00:14<05:38,  3.52s/it]  1%|          | 1/100 [00:03<06:11,  3.76s/it]  8%|▊         | 8/100 [00:27<05:22,  3.50s/it]  6%|▌         | 6/100 [00:20<05:25,  3.47s/it]  5%|▌         | 5/100 [00:17<05:35,  3.53s/it]  9%|▉         | 9/100 [00:31<05:18,  3.50s/it]  2%|▏         | 2/100 [00:07<06:03,  3.71s/it]  7%|▋         | 7/100 [00:24<05:22,  3.47s/it]  6%|▌         | 6/100 [00:21<05:32,  3.54s/it] 10%|█         | 10/100 [00:34<05:15,  3.51s/it]  3%|▎         | 3/100 [00:11<05:58,  3.69s/it]  8%|▊         | 8/100 [00:27<05:19,  3.47s/it]  7%|▋         | 7/100 [00:24<05:28,  3.53s/it] 11%|█         | 11/100 [00:38<05:12,  3.51s/it]  4%|▍         | 4/100 [00:14<05:54,  3.69s/it]  9%|▉         | 9/100 [00:31<05:16,  3.47s/it]  8%|▊         | 8/100 [00:28<05:25,  3.54s/it] 12%|█▏        | 12/100 [00:42<05:09,  3.51s/it]  5%|▌         | 5/100 [00:18<05:51,  3.70s/it] 10%|█         | 10/100 [00:34<05:13,  3.49s/it]  9%|▉         | 9/100 [00:31<05:21,  3.54s/it] 13%|█▎        | 13/100 [00:45<05:05,  3.51s/it]  6%|▌         | 6/100 [00:22<05:47,  3.70s/it] 11%|█         | 11/100 [00:38<05:09,  3.48s/it] 10%|█         | 10/100 [00:35<05:18,  3.54s/it] 14%|█▍        | 14/100 [00:49<05:01,  3.51s/it] 12%|█▏        | 12/100 [00:41<05:06,  3.48s/it]  7%|▋         | 7/100 [00:25<05:44,  3.70s/it] 11%|█         | 11/100 [00:38<05:15,  3.55s/it] 15%|█▌        | 15/100 [00:52<04:58,  3.51s/it] 13%|█▎        | 13/100 [00:45<05:02,  3.48s/it]  8%|▊         | 8/100 [00:29<05:40,  3.71s/it] 12%|█▏        | 12/100 [00:42<05:12,  3.55s/it] 16%|█▌        | 16/100 [00:56<04:55,  3.51s/it] 14%|█▍        | 14/100 [00:48<04:59,  3.48s/it]  9%|▉         | 9/100 [00:33<05:37,  3.71s/it] 13%|█▎        | 13/100 [00:45<05:08,  3.54s/it] 17%|█▋        | 17/100 [00:59<04:51,  3.52s/it] 15%|█▌        | 15/100 [00:52<04:55,  3.48s/it] 10%|█         | 10/100 [00:37<05:33,  3.71s/it] 14%|█▍        | 14/100 [00:49<05:04,  3.55s/it] 18%|█▊        | 18/100 [01:03<04:48,  3.52s/it] 16%|█▌        | 16/100 [00:55<04:52,  3.48s/it] 11%|█         | 11/100 [00:40<05:30,  3.71s/it] 15%|█▌        | 15/100 [00:53<05:00,  3.54s/it] 19%|█▉        | 19/100 [01:06<04:46,  3.53s/it] 17%|█▋        | 17/100 [00:59<04:48,  3.47s/it] 12%|█▏        | 12/100 [00:44<05:26,  3.71s/it] 16%|█▌        | 16/100 [00:56<04:57,  3.54s/it] 20%|██        | 20/100 [01:10<04:42,  3.54s/it] 18%|█▊        | 18/100 [01:02<04:44,  3.48s/it] 13%|█▎        | 13/100 [00:48<05:22,  3.71s/it] 17%|█▋        | 17/100 [01:00<04:54,  3.55s/it] 21%|██        | 21/100 [01:13<04:39,  3.54s/it] 19%|█▉        | 19/100 [01:05<04:41,  3.47s/it] 14%|█▍        | 14/100 [00:51<05:19,  3.72s/it] 18%|█▊        | 18/100 [01:03<04:50,  3.54s/it] 22%|██▏       | 22/100 [01:17<04:35,  3.54s/it] 20%|██        | 20/100 [01:09<04:38,  3.48s/it] 19%|█▉        | 19/100 [01:07<04:47,  3.55s/it] 15%|█▌        | 15/100 [00:55<05:16,  3.73s/it] 23%|██▎       | 23/100 [01:20<04:32,  3.54s/it] 21%|██        | 21/100 [01:12<04:34,  3.48s/it] 20%|██        | 20/100 [01:10<04:44,  3.55s/it] 16%|█▌        | 16/100 [00:59<05:13,  3.73s/it] 24%|██▍       | 24/100 [01:24<04:28,  3.54s/it] 22%|██▏       | 22/100 [01:16<04:31,  3.48s/it] 21%|██        | 21/100 [01:14<04:40,  3.55s/it] 17%|█▋        | 17/100 [01:03<05:09,  3.73s/it] 25%|██▌       | 25/100 [01:27<04:25,  3.54s/it] 23%|██▎       | 23/100 [01:19<04:27,  3.48s/it] 22%|██▏       | 22/100 [01:17<04:36,  3.55s/it] 18%|█▊        | 18/100 [01:06<05:06,  3.74s/it] 26%|██▌       | 26/100 [01:31<04:21,  3.54s/it] 24%|██▍       | 24/100 [01:23<04:24,  3.48s/it] 23%|██▎       | 23/100 [01:21<04:33,  3.55s/it] 19%|█▉        | 19/100 [01:10<05:03,  3.74s/it] 25%|██▌       | 25/100 [01:26<04:20,  3.48s/it] 27%|██▋       | 27/100 [01:35<04:18,  3.54s/it] 24%|██▍       | 24/100 [01:25<04:29,  3.55s/it] 26%|██▌       | 26/100 [01:30<04:17,  3.48s/it] 20%|██        | 20/100 [01:14<04:59,  3.74s/it] 28%|██▊       | 28/100 [01:38<04:14,  3.54s/it] 25%|██▌       | 25/100 [01:28<04:26,  3.55s/it] 27%|██▋       | 27/100 [01:33<04:14,  3.48s/it] 29%|██▉       | 29/100 [01:42<04:11,  3.54s/it] 21%|██        | 21/100 [01:18<04:55,  3.74s/it] 26%|██▌       | 26/100 [01:32<04:23,  3.56s/it] 28%|██▊       | 28/100 [01:37<04:10,  3.48s/it] 30%|███       | 30/100 [01:45<04:07,  3.54s/it] 22%|██▏       | 22/100 [01:21<04:52,  3.75s/it] 27%|██▋       | 27/100 [01:35<04:19,  3.55s/it] 29%|██▉       | 29/100 [01:40<04:07,  3.49s/it] 31%|███       | 31/100 [01:49<04:04,  3.55s/it] 23%|██▎       | 23/100 [01:25<04:48,  3.74s/it] 28%|██▊       | 28/100 [01:39<04:16,  3.56s/it] 30%|███       | 30/100 [01:44<04:03,  3.48s/it] 32%|███▏      | 32/100 [01:52<04:00,  3.54s/it] 24%|██▍       | 24/100 [01:29<04:44,  3.74s/it] 29%|██▉       | 29/100 [01:42<04:12,  3.56s/it] 31%|███       | 31/100 [01:47<04:01,  3.49s/it] 33%|███▎      | 33/100 [01:56<03:56,  3.54s/it] 25%|██▌       | 25/100 [01:33<04:40,  3.74s/it] 30%|███       | 30/100 [01:46<04:09,  3.56s/it] 32%|███▏      | 32/100 [01:51<03:57,  3.49s/it] 34%|███▍      | 34/100 [01:59<03:53,  3.54s/it] 26%|██▌       | 26/100 [01:36<04:36,  3.74s/it] 31%|███       | 31/100 [01:49<04:05,  3.56s/it] 33%|███▎      | 33/100 [01:54<03:53,  3.48s/it] 35%|███▌      | 35/100 [02:03<03:49,  3.54s/it] 27%|██▋       | 27/100 [01:40<04:33,  3.74s/it] 32%|███▏      | 32/100 [01:53<04:02,  3.56s/it] 34%|███▍      | 34/100 [01:58<03:49,  3.48s/it] 36%|███▌      | 36/100 [02:06<03:46,  3.54s/it] 28%|██▊       | 28/100 [01:44<04:29,  3.75s/it] 33%|███▎      | 33/100 [01:57<03:58,  3.56s/it] 35%|███▌      | 35/100 [02:01<03:46,  3.48s/it] 37%|███▋      | 37/100 [02:10<03:42,  3.54s/it] 29%|██▉       | 29/100 [01:48<04:26,  3.75s/it] 34%|███▍      | 34/100 [02:00<03:55,  3.56s/it] 36%|███▌      | 36/100 [02:05<03:42,  3.48s/it] 38%|███▊      | 38/100 [02:13<03:39,  3.54s/it] 30%|███       | 30/100 [01:51<04:22,  3.75s/it] 35%|███▌      | 35/100 [02:04<03:51,  3.56s/it] 37%|███▋      | 37/100 [02:08<03:39,  3.48s/it] 39%|███▉      | 39/100 [02:17<03:35,  3.54s/it] 31%|███       | 31/100 [01:55<04:19,  3.75s/it] 36%|███▌      | 36/100 [02:07<03:47,  3.56s/it] 38%|███▊      | 38/100 [02:12<03:35,  3.48s/it] 40%|████      | 40/100 [02:21<03:32,  3.54s/it] 32%|███▏      | 32/100 [01:59<04:15,  3.76s/it] 37%|███▋      | 37/100 [02:11<03:44,  3.56s/it] 39%|███▉      | 39/100 [02:15<03:32,  3.48s/it] 41%|████      | 41/100 [02:24<03:28,  3.54s/it] 40%|████      | 40/100 [02:19<03:28,  3.48s/it] 33%|███▎      | 33/100 [02:03<04:12,  3.77s/it] 38%|███▊      | 38/100 [02:14<03:40,  3.56s/it] 42%|████▏     | 42/100 [02:28<03:25,  3.54s/it] 41%|████      | 41/100 [02:22<03:25,  3.48s/it] 39%|███▉      | 39/100 [02:18<03:37,  3.56s/it] 34%|███▍      | 34/100 [02:06<04:08,  3.76s/it] 43%|████▎     | 43/100 [02:31<03:22,  3.55s/it] 42%|████▏     | 42/100 [02:26<03:22,  3.48s/it] 40%|████      | 40/100 [02:21<03:33,  3.56s/it] 35%|███▌      | 35/100 [02:10<04:04,  3.76s/it] 44%|████▍     | 44/100 [02:35<03:18,  3.54s/it] 43%|████▎     | 43/100 [02:29<03:18,  3.48s/it] 41%|████      | 41/100 [02:25<03:30,  3.56s/it] 36%|███▌      | 36/100 [02:14<04:00,  3.76s/it] 45%|████▌     | 45/100 [02:38<03:14,  3.54s/it] 44%|████▍     | 44/100 [02:33<03:15,  3.49s/it] 42%|████▏     | 42/100 [02:29<03:26,  3.56s/it] 37%|███▋      | 37/100 [02:18<03:56,  3.75s/it] 46%|████▌     | 46/100 [02:42<03:11,  3.54s/it] 45%|████▌     | 45/100 [02:36<03:12,  3.49s/it] 43%|████▎     | 43/100 [02:32<03:23,  3.57s/it] 47%|████▋     | 47/100 [02:45<03:07,  3.54s/it] 38%|███▊      | 38/100 [02:21<03:53,  3.76s/it] 46%|████▌     | 46/100 [02:40<03:08,  3.50s/it] 44%|████▍     | 44/100 [02:36<03:20,  3.57s/it] 48%|████▊     | 48/100 [02:49<03:04,  3.54s/it] 39%|███▉      | 39/100 [02:25<03:49,  3.76s/it] 47%|████▋     | 47/100 [02:43<03:05,  3.50s/it] 45%|████▌     | 45/100 [02:39<03:16,  3.57s/it] 49%|████▉     | 49/100 [02:52<03:00,  3.54s/it] 40%|████      | 40/100 [02:29<03:45,  3.75s/it] 48%|████▊     | 48/100 [02:47<03:01,  3.50s/it] 46%|████▌     | 46/100 [02:43<03:12,  3.57s/it] 50%|█████     | 50/100 [02:56<02:57,  3.55s/it] 41%|████      | 41/100 [02:33<03:41,  3.76s/it] 49%|████▉     | 49/100 [02:50<02:58,  3.50s/it] 47%|████▋     | 47/100 [02:46<03:09,  3.57s/it] 51%|█████     | 51/100 [03:00<02:53,  3.55s/it] 42%|████▏     | 42/100 [02:36<03:37,  3.76s/it] 50%|█████     | 50/100 [02:54<02:54,  3.50s/it] 48%|████▊     | 48/100 [02:50<03:05,  3.57s/it] 52%|█████▏    | 52/100 [03:03<02:50,  3.54s/it] 43%|████▎     | 43/100 [02:40<03:33,  3.75s/it] 51%|█████     | 51/100 [02:57<02:51,  3.50s/it] 49%|████▉     | 49/100 [02:54<03:02,  3.57s/it] 53%|█████▎    | 53/100 [03:07<02:46,  3.54s/it] 44%|████▍     | 44/100 [02:44<03:30,  3.75s/it] 52%|█████▏    | 52/100 [03:01<02:48,  3.50s/it] 50%|█████     | 50/100 [02:57<02:58,  3.57s/it] 54%|█████▍    | 54/100 [03:10<02:42,  3.54s/it] 45%|████▌     | 45/100 [02:48<03:26,  3.76s/it] 53%|█████▎    | 53/100 [03:04<02:44,  3.50s/it] 51%|█████     | 51/100 [03:01<02:54,  3.56s/it] 55%|█████▌    | 55/100 [03:14<02:39,  3.54s/it] 46%|████▌     | 46/100 [02:52<03:23,  3.76s/it] 54%|█████▍    | 54/100 [03:08<02:40,  3.50s/it] 52%|█████▏    | 52/100 [03:04<02:51,  3.56s/it] 56%|█████▌    | 56/100 [03:17<02:35,  3.54s/it] 55%|█████▌    | 55/100 [03:11<02:37,  3.50s/it] 47%|████▋     | 47/100 [02:55<03:19,  3.76s/it] 53%|█████▎    | 53/100 [03:08<02:48,  3.58s/it] 57%|█████▋    | 57/100 [03:21<02:32,  3.55s/it] 56%|█████▌    | 56/100 [03:15<02:34,  3.52s/it] 48%|████▊     | 48/100 [02:59<03:16,  3.78s/it] 54%|█████▍    | 54/100 [03:11<02:44,  3.57s/it] 58%|█████▊    | 58/100 [03:24<02:29,  3.56s/it] 57%|█████▋    | 57/100 [03:18<02:30,  3.51s/it] 49%|████▉     | 49/100 [03:03<03:12,  3.77s/it] 55%|█████▌    | 55/100 [03:15<02:40,  3.57s/it] 59%|█████▉    | 59/100 [03:28<02:25,  3.55s/it] 58%|█████▊    | 58/100 [03:22<02:26,  3.50s/it] 50%|█████     | 50/100 [03:07<03:08,  3.78s/it] 56%|█████▌    | 56/100 [03:19<02:37,  3.57s/it] 60%|██████    | 60/100 [03:31<02:22,  3.55s/it] 59%|█████▉    | 59/100 [03:25<02:23,  3.51s/it] 51%|█████     | 51/100 [03:10<03:04,  3.77s/it] 57%|█████▋    | 57/100 [03:22<02:33,  3.57s/it] 61%|██████    | 61/100 [03:35<02:18,  3.55s/it] 60%|██████    | 60/100 [03:29<02:20,  3.50s/it] 58%|█████▊    | 58/100 [03:26<02:29,  3.57s/it] 52%|█████▏    | 52/100 [03:14<03:00,  3.77s/it] 62%|██████▏   | 62/100 [03:39<02:14,  3.55s/it] 61%|██████    | 61/100 [03:32<02:16,  3.50s/it] 59%|█████▉    | 59/100 [03:29<02:26,  3.56s/it] 53%|█████▎    | 53/100 [03:18<02:56,  3.76s/it] 63%|██████▎   | 63/100 [03:42<02:11,  3.54s/it] 62%|██████▏   | 62/100 [03:36<02:13,  3.50s/it] 60%|██████    | 60/100 [03:33<02:26,  3.65s/it] 64%|██████▍   | 64/100 [03:46<02:09,  3.60s/it] 54%|█████▍    | 54/100 [03:22<02:54,  3.80s/it] 63%|██████▎   | 63/100 [03:39<02:12,  3.58s/it] 61%|██████    | 61/100 [03:37<02:22,  3.65s/it] 55%|█████▌    | 55/100 [03:26<02:51,  3.81s/it] 65%|██████▌   | 65/100 [03:50<02:09,  3.70s/it] 64%|██████▍   | 64/100 [03:43<02:08,  3.58s/it] 62%|██████▏   | 62/100 [03:40<02:18,  3.65s/it] 66%|██████▌   | 66/100 [03:53<02:05,  3.68s/it] 56%|█████▌    | 56/100 [03:29<02:48,  3.82s/it] 65%|██████▌   | 65/100 [03:46<02:04,  3.55s/it] 63%|██████▎   | 63/100 [03:44<02:14,  3.63s/it] 67%|██████▋   | 67/100 [03:57<02:00,  3.64s/it] 57%|█████▋    | 57/100 [03:33<02:43,  3.81s/it] 66%|██████▌   | 66/100 [03:50<02:00,  3.54s/it] 64%|██████▍   | 64/100 [03:48<02:11,  3.66s/it] 68%|██████▊   | 68/100 [04:00<01:55,  3.61s/it] 58%|█████▊    | 58/100 [03:37<02:39,  3.79s/it] 67%|██████▋   | 67/100 [03:54<01:57,  3.56s/it] 65%|██████▌   | 65/100 [03:51<02:07,  3.64s/it] 69%|██████▉   | 69/100 [04:04<01:51,  3.60s/it] 59%|█████▉    | 59/100 [03:41<02:37,  3.84s/it] 68%|██████▊   | 68/100 [03:57<01:56,  3.63s/it] 66%|██████▌   | 66/100 [03:55<02:03,  3.62s/it] 70%|███████   | 70/100 [04:08<01:47,  3.58s/it] 60%|██████    | 60/100 [03:45<02:32,  3.82s/it] 69%|██████▉   | 69/100 [04:01<01:51,  3.59s/it] 67%|██████▋   | 67/100 [03:58<01:58,  3.60s/it] 71%|███████   | 71/100 [04:11<01:43,  3.57s/it] 70%|███████   | 70/100 [04:04<01:46,  3.56s/it] 61%|██████    | 61/100 [03:48<02:28,  3.80s/it] 68%|██████▊   | 68/100 [04:02<01:54,  3.59s/it] 72%|███████▏  | 72/100 [04:15<01:39,  3.56s/it] 71%|███████   | 71/100 [04:08<01:42,  3.53s/it] 62%|██████▏   | 62/100 [03:52<02:25,  3.83s/it] 69%|██████▉   | 69/100 [04:06<01:51,  3.58s/it] 73%|███████▎  | 73/100 [04:18<01:36,  3.56s/it] 72%|███████▏  | 72/100 [04:11<01:38,  3.52s/it] 63%|██████▎   | 63/100 [03:56<02:20,  3.81s/it] 70%|███████   | 70/100 [04:09<01:47,  3.58s/it] 74%|███████▍  | 74/100 [04:22<01:34,  3.63s/it] 73%|███████▎  | 73/100 [04:15<01:34,  3.51s/it] 64%|██████▍   | 64/100 [04:00<02:16,  3.79s/it] 71%|███████   | 71/100 [04:13<01:43,  3.58s/it] 75%|███████▌  | 75/100 [04:26<01:30,  3.60s/it] 74%|███████▍  | 74/100 [04:18<01:31,  3.50s/it] 65%|██████▌   | 65/100 [04:04<02:12,  3.78s/it] 72%|███████▏  | 72/100 [04:16<01:40,  3.57s/it] 76%|███████▌  | 76/100 [04:29<01:25,  3.58s/it] 75%|███████▌  | 75/100 [04:22<01:27,  3.50s/it] 66%|██████▌   | 66/100 [04:07<02:08,  3.77s/it] 73%|███████▎  | 73/100 [04:20<01:36,  3.57s/it] 77%|███████▋  | 77/100 [04:33<01:22,  3.57s/it] 76%|███████▌  | 76/100 [04:25<01:23,  3.49s/it] 67%|██████▋   | 67/100 [04:11<02:04,  3.77s/it] 74%|███████▍  | 74/100 [04:23<01:33,  3.58s/it] 78%|███████▊  | 78/100 [04:36<01:18,  3.56s/it] 77%|███████▋  | 77/100 [04:29<01:20,  3.50s/it] 68%|██████▊   | 68/100 [04:15<02:00,  3.78s/it] 75%|███████▌  | 75/100 [04:27<01:29,  3.57s/it] 79%|███████▉  | 79/100 [04:40<01:14,  3.56s/it] 78%|███████▊  | 78/100 [04:32<01:17,  3.51s/it] 69%|██████▉   | 69/100 [04:19<01:56,  3.77s/it] 76%|███████▌  | 76/100 [04:31<01:25,  3.57s/it] 80%|████████  | 80/100 [04:43<01:11,  3.56s/it] 79%|███████▉  | 79/100 [04:36<01:13,  3.50s/it] 70%|███████   | 70/100 [04:22<01:52,  3.76s/it] 77%|███████▋  | 77/100 [04:34<01:22,  3.57s/it] 81%|████████  | 81/100 [04:47<01:07,  3.56s/it] 80%|████████  | 80/100 [04:39<01:09,  3.50s/it] 78%|███████▊  | 78/100 [04:38<01:18,  3.57s/it] 71%|███████   | 71/100 [04:26<01:49,  3.76s/it] 82%|████████▏ | 82/100 [04:50<01:03,  3.55s/it] 81%|████████  | 81/100 [04:43<01:06,  3.49s/it] 79%|███████▉  | 79/100 [04:41<01:14,  3.57s/it] 83%|████████▎ | 83/100 [04:54<01:00,  3.55s/it] 72%|███████▏  | 72/100 [04:30<01:45,  3.76s/it] 82%|████████▏ | 82/100 [04:46<01:02,  3.49s/it] 80%|████████  | 80/100 [04:45<01:11,  3.57s/it] 84%|████████▍ | 84/100 [04:57<00:56,  3.54s/it] 73%|███████▎  | 73/100 [04:34<01:41,  3.76s/it] 83%|████████▎ | 83/100 [04:50<00:59,  3.49s/it] 81%|████████  | 81/100 [04:48<01:07,  3.57s/it] 85%|████████▌ | 85/100 [05:01<00:53,  3.54s/it] 84%|████████▍ | 84/100 [04:53<00:55,  3.49s/it] 74%|███████▍  | 74/100 [04:37<01:37,  3.76s/it] 82%|████████▏ | 82/100 [04:52<01:04,  3.57s/it] 86%|████████▌ | 86/100 [05:04<00:49,  3.54s/it] 85%|████████▌ | 85/100 [04:57<00:52,  3.49s/it] 75%|███████▌  | 75/100 [04:41<01:34,  3.77s/it] 83%|████████▎ | 83/100 [04:56<01:00,  3.57s/it] 87%|████████▋ | 87/100 [05:08<00:46,  3.55s/it] 86%|████████▌ | 86/100 [05:00<00:48,  3.48s/it] 76%|███████▌  | 76/100 [04:45<01:30,  3.77s/it] 84%|████████▍ | 84/100 [04:59<00:57,  3.57s/it] 88%|████████▊ | 88/100 [05:12<00:42,  3.55s/it] 87%|████████▋ | 87/100 [05:04<00:45,  3.49s/it] 77%|███████▋  | 77/100 [04:49<01:26,  3.76s/it] 85%|████████▌ | 85/100 [05:03<00:53,  3.57s/it] 89%|████████▉ | 89/100 [05:15<00:39,  3.55s/it] 88%|████████▊ | 88/100 [05:07<00:41,  3.49s/it] 78%|███████▊  | 78/100 [04:53<01:22,  3.76s/it] 90%|█████████ | 90/100 [05:19<00:35,  3.55s/it] 86%|████████▌ | 86/100 [05:06<00:49,  3.57s/it] 89%|████████▉ | 89/100 [05:11<00:38,  3.49s/it] 79%|███████▉  | 79/100 [04:56<01:18,  3.76s/it] 90%|█████████ | 90/100 [05:14<00:34,  3.48s/it] 91%|█████████ | 91/100 [05:22<00:31,  3.55s/it] 87%|████████▋ | 87/100 [05:10<00:46,  3.57s/it] 80%|████████  | 80/100 [05:00<01:15,  3.75s/it] 91%|█████████ | 91/100 [05:18<00:31,  3.48s/it] 92%|█████████▏| 92/100 [05:26<00:28,  3.55s/it] 88%|████████▊ | 88/100 [05:13<00:42,  3.57s/it] 81%|████████  | 81/100 [05:04<01:11,  3.75s/it] 92%|█████████▏| 92/100 [05:21<00:27,  3.48s/it] 93%|█████████▎| 93/100 [05:29<00:24,  3.55s/it] 89%|████████▉ | 89/100 [05:17<00:39,  3.57s/it] 82%|████████▏ | 82/100 [05:08<01:07,  3.75s/it] 93%|█████████▎| 93/100 [05:25<00:24,  3.48s/it] 94%|█████████▍| 94/100 [05:33<00:21,  3.54s/it] 90%|█████████ | 90/100 [05:21<00:35,  3.57s/it] 83%|████████▎ | 83/100 [05:11<01:03,  3.76s/it] 94%|█████████▍| 94/100 [05:28<00:20,  3.48s/it] 95%|█████████▌| 95/100 [05:36<00:17,  3.56s/it] 91%|█████████ | 91/100 [05:24<00:32,  3.57s/it] 84%|████████▍ | 84/100 [05:15<01:00,  3.76s/it] 95%|█████████▌| 95/100 [05:32<00:17,  3.49s/it] 96%|█████████▌| 96/100 [05:40<00:14,  3.55s/it] 92%|█████████▏| 92/100 [05:28<00:28,  3.57s/it] 85%|████████▌ | 85/100 [05:19<00:56,  3.76s/it] 96%|█████████▌| 96/100 [05:35<00:14,  3.50s/it] 97%|█████████▋| 97/100 [05:44<00:10,  3.55s/it] 93%|█████████▎| 93/100 [05:31<00:24,  3.57s/it] 86%|████████▌ | 86/100 [05:23<00:52,  3.76s/it] 97%|█████████▋| 97/100 [05:39<00:10,  3.49s/it] 98%|█████████▊| 98/100 [05:47<00:07,  3.55s/it] 94%|█████████▍| 94/100 [05:35<00:21,  3.57s/it] 98%|█████████▊| 98/100 [05:42<00:06,  3.49s/it] 87%|████████▋ | 87/100 [05:26<00:48,  3.76s/it] 99%|█████████▉| 99/100 [05:51<00:03,  3.55s/it] 95%|█████████▌| 95/100 [05:38<00:17,  3.57s/it] 99%|█████████▉| 99/100 [05:46<00:03,  3.49s/it] 88%|████████▊ | 88/100 [05:30<00:45,  3.75s/it]100%|██████████| 100/100 [05:54<00:00,  3.55s/it]100%|██████████| 100/100 [05:54<00:00,  3.55s/it]
 96%|█████████▌| 96/100 [05:42<00:14,  3.57s/it]100%|██████████| 100/100 [05:49<00:00,  3.49s/it]100%|██████████| 100/100 [05:49<00:00,  3.49s/it]
 89%|████████▉ | 89/100 [05:34<00:41,  3.75s/it] 97%|█████████▋| 97/100 [05:46<00:10,  3.57s/it] 98%|█████████▊| 98/100 [05:49<00:07,  3.58s/it] 90%|█████████ | 90/100 [05:38<00:37,  3.76s/it]Process Process-2:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
1_gate proxy err 0.004864327143877745 err 67.86886596679688 tr(WHW.T) 13952.365234375
bpp_loss 3.710291930607387
Process Process-4:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
3_gate proxy err 0.003257677424699068 err 68.03446960449219 tr(WHW.T) 20884.34765625
bpp_loss 3.834317513874599
 99%|█████████▉| 99/100 [05:53<00:03,  3.58s/it] 91%|█████████ | 91/100 [05:41<00:33,  3.76s/it]100%|██████████| 100/100 [05:56<00:00,  3.58s/it]100%|██████████| 100/100 [05:56<00:00,  3.57s/it]
 92%|█████████▏| 92/100 [05:45<00:30,  3.77s/it] 93%|█████████▎| 93/100 [05:49<00:26,  3.77s/it] 94%|█████████▍| 94/100 [05:53<00:22,  3.79s/it]Process Process-3:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
2_gate proxy err 0.004483124706894159 err 67.7548828125 tr(WHW.T) 15113.31640625
bpp_loss 3.750558069774083
 95%|█████████▌| 95/100 [05:57<00:18,  3.77s/it] 96%|█████████▌| 96/100 [06:00<00:15,  3.77s/it] 97%|█████████▋| 97/100 [06:04<00:11,  3.77s/it] 98%|█████████▊| 98/100 [06:08<00:07,  3.76s/it] 99%|█████████▉| 99/100 [06:12<00:03,  3.76s/it]100%|██████████| 100/100 [06:15<00:00,  3.77s/it]100%|██████████| 100/100 [06:15<00:00,  3.76s/it]
Process Process-1:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
0_gate proxy err 0.004296762403100729 err 67.79718017578125 tr(WHW.T) 15778.666015625
bpp_loss 3.6910736901419505
I0407 08:21:36.190248 22419 quantize_finetune_llama.py:250] computed original embedding for layer 4 in 0.8574151992797852s
I0407 08:21:39.679577 35711 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:21:39.679669 35711 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:21:39.679707 35711 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:21:40.009691 35711 config.py:54] PyTorch version 2.6.0 available.
W0407 08:21:40.198136 35711 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:21:40.872017 35711 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:21:40.875781 22419 quantize_finetune_llama.py:219] layer 5 gpu 1
I0407 08:21:40.888785 35711 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:00<00:51,  1.94it/s]I0407 08:21:42.209956 22419 quantize_finetune_llama.py:250] computed original embedding for layer 5 in 0.8915565013885498s
  2%|▏         | 2/100 [00:00<00:35,  2.80it/s]  3%|▎         | 3/100 [00:01<00:30,  3.19it/s]  4%|▍         | 4/100 [00:01<00:27,  3.48it/s]  5%|▌         | 5/100 [00:01<00:26,  3.64it/s]  6%|▌         | 6/100 [00:01<00:25,  3.75it/s]  7%|▋         | 7/100 [00:02<00:24,  3.82it/s]  8%|▊         | 8/100 [00:02<00:23,  3.87it/s]  9%|▉         | 9/100 [00:02<00:23,  3.90it/s] 10%|█         | 10/100 [00:02<00:22,  3.92it/s] 11%|█         | 11/100 [00:03<00:22,  3.95it/s] 12%|█▏        | 12/100 [00:03<00:22,  3.96it/s] 13%|█▎        | 13/100 [00:03<00:21,  3.96it/s] 14%|█▍        | 14/100 [00:03<00:21,  3.96it/s] 15%|█▌        | 15/100 [00:04<00:21,  3.97it/s]I0407 08:21:45.776366 35884 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:21:45.776468 35884 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:21:45.776512 35884 utils.py:162] NumExpr defaulting to 16 threads.
 16%|█▌        | 16/100 [00:04<00:21,  3.96it/s]I0407 08:21:46.108033 35884 config.py:54] PyTorch version 2.6.0 available.
 17%|█▋        | 17/100 [00:04<00:20,  3.97it/s]W0407 08:21:46.304051 35884 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 18%|█▊        | 18/100 [00:04<00:20,  3.96it/s] 19%|█▉        | 19/100 [00:05<00:20,  3.96it/s] 20%|██        | 20/100 [00:05<00:20,  3.95it/s]W0407 08:21:47.186542 35884 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:21:47.190513 22419 quantize_finetune_llama.py:219] layer 6 gpu 2
I0407 08:21:47.204855 35884 data_utils.py:336] using 256 training seqs, 128 validation seqs
 21%|██        | 21/100 [00:05<00:19,  3.96it/s] 22%|██▏       | 22/100 [00:05<00:19,  3.94it/s] 23%|██▎       | 23/100 [00:06<00:19,  3.96it/s] 24%|██▍       | 24/100 [00:06<00:19,  3.94it/s]  0%|          | 0/100 [00:00<?, ?it/s] 25%|██▌       | 25/100 [00:06<00:19,  3.93it/s] 26%|██▌       | 26/100 [00:06<00:18,  3.95it/s]I0407 08:21:48.504018 22419 quantize_finetune_llama.py:250] computed original embedding for layer 6 in 0.9076581001281738s
 27%|██▋       | 27/100 [00:07<00:18,  3.95it/s]  1%|          | 1/100 [00:00<01:14,  1.32it/s] 28%|██▊       | 28/100 [00:07<00:18,  3.94it/s]  2%|▏         | 2/100 [00:00<00:43,  2.23it/s] 29%|██▉       | 29/100 [00:07<00:18,  3.93it/s]  3%|▎         | 3/100 [00:01<00:33,  2.85it/s] 30%|███       | 30/100 [00:07<00:17,  3.95it/s]  4%|▍         | 4/100 [00:01<00:29,  3.28it/s]  5%|▌         | 5/100 [00:01<00:26,  3.59it/s] 31%|███       | 31/100 [00:08<00:17,  3.95it/s]  6%|▌         | 6/100 [00:01<00:24,  3.80it/s] 32%|███▏      | 32/100 [00:08<00:17,  3.93it/s]  7%|▋         | 7/100 [00:02<00:23,  3.94it/s] 33%|███▎      | 33/100 [00:08<00:17,  3.92it/s]  8%|▊         | 8/100 [00:02<00:22,  4.05it/s] 34%|███▍      | 34/100 [00:08<00:16,  3.93it/s]  9%|▉         | 9/100 [00:02<00:22,  4.12it/s] 35%|███▌      | 35/100 [00:09<00:16,  3.93it/s] 10%|█         | 10/100 [00:02<00:21,  4.16it/s] 36%|███▌      | 36/100 [00:09<00:16,  3.93it/s] 11%|█         | 11/100 [00:03<00:21,  4.20it/s] 37%|███▋      | 37/100 [00:09<00:15,  3.94it/s] 12%|█▏        | 12/100 [00:03<00:20,  4.21it/s] 38%|███▊      | 38/100 [00:09<00:15,  3.95it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.21it/s] 39%|███▉      | 39/100 [00:10<00:15,  3.94it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.23it/s] 40%|████      | 40/100 [00:10<00:15,  3.95it/s] 15%|█▌        | 15/100 [00:04<00:19,  4.25it/s]I0407 08:21:52.133642 36031 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:21:52.133733 36031 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:21:52.133776 36031 utils.py:162] NumExpr defaulting to 16 threads.
 41%|████      | 41/100 [00:10<00:14,  3.95it/s] 16%|█▌        | 16/100 [00:04<00:19,  4.24it/s]I0407 08:21:52.458906 36031 config.py:54] PyTorch version 2.6.0 available.
 42%|████▏     | 42/100 [00:10<00:14,  3.95it/s] 17%|█▋        | 17/100 [00:04<00:19,  4.23it/s]W0407 08:21:52.650914 36031 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 18%|█▊        | 18/100 [00:04<00:19,  4.25it/s] 43%|████▎     | 43/100 [00:11<00:14,  3.93it/s] 19%|█▉        | 19/100 [00:04<00:19,  4.23it/s] 44%|████▍     | 44/100 [00:11<00:14,  3.94it/s] 20%|██        | 20/100 [00:05<00:18,  4.23it/s] 45%|████▌     | 45/100 [00:11<00:14,  3.92it/s] 21%|██        | 21/100 [00:05<00:18,  4.22it/s] 46%|████▌     | 46/100 [00:11<00:13,  3.93it/s]W0407 08:21:53.670297 36031 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:21:53.674287 22419 quantize_finetune_llama.py:219] layer 7 gpu 3
I0407 08:21:53.688100 36031 data_utils.py:336] using 256 training seqs, 128 validation seqs
 22%|██▏       | 22/100 [00:05<00:18,  4.21it/s] 47%|████▋     | 47/100 [00:12<00:13,  3.93it/s] 23%|██▎       | 23/100 [00:05<00:18,  4.24it/s] 48%|████▊     | 48/100 [00:12<00:13,  3.93it/s]  0%|          | 0/100 [00:00<?, ?it/s] 24%|██▍       | 24/100 [00:06<00:17,  4.24it/s] 49%|████▉     | 49/100 [00:12<00:12,  3.92it/s] 25%|██▌       | 25/100 [00:06<00:17,  4.25it/s] 50%|█████     | 50/100 [00:12<00:12,  3.92it/s] 26%|██▌       | 26/100 [00:06<00:17,  4.25it/s] 51%|█████     | 51/100 [00:13<00:12,  3.92it/s] 27%|██▋       | 27/100 [00:06<00:17,  4.26it/s]  1%|          | 1/100 [00:00<01:13,  1.35it/s]I0407 08:21:55.042776 22419 quantize_finetune_llama.py:250] computed original embedding for layer 7 in 0.9186463356018066s
 52%|█████▏    | 52/100 [00:13<00:12,  3.93it/s] 28%|██▊       | 28/100 [00:07<00:16,  4.24it/s]  2%|▏         | 2/100 [00:00<00:43,  2.26it/s] 53%|█████▎    | 53/100 [00:13<00:11,  3.92it/s] 29%|██▉       | 29/100 [00:07<00:16,  4.24it/s]  3%|▎         | 3/100 [00:01<00:34,  2.82it/s] 54%|█████▍    | 54/100 [00:13<00:11,  3.92it/s] 30%|███       | 30/100 [00:07<00:16,  4.26it/s]  4%|▍         | 4/100 [00:01<00:29,  3.26it/s] 31%|███       | 31/100 [00:07<00:16,  4.27it/s] 55%|█████▌    | 55/100 [00:14<00:11,  3.92it/s]  5%|▌         | 5/100 [00:01<00:26,  3.57it/s] 32%|███▏      | 32/100 [00:08<00:15,  4.26it/s]  6%|▌         | 6/100 [00:01<00:24,  3.78it/s] 56%|█████▌    | 56/100 [00:14<00:11,  3.93it/s] 33%|███▎      | 33/100 [00:08<00:15,  4.24it/s]  7%|▋         | 7/100 [00:02<00:23,  3.93it/s] 57%|█████▋    | 57/100 [00:14<00:10,  3.91it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.26it/s]  8%|▊         | 8/100 [00:02<00:22,  4.02it/s] 58%|█████▊    | 58/100 [00:14<00:10,  3.91it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.27it/s]  9%|▉         | 9/100 [00:02<00:22,  4.10it/s] 59%|█████▉    | 59/100 [00:15<00:10,  3.92it/s] 36%|███▌      | 36/100 [00:08<00:15,  4.26it/s] 10%|█         | 10/100 [00:02<00:21,  4.13it/s] 60%|██████    | 60/100 [00:15<00:10,  3.91it/s] 37%|███▋      | 37/100 [00:09<00:14,  4.25it/s] 11%|█         | 11/100 [00:03<00:21,  4.17it/s] 61%|██████    | 61/100 [00:15<00:09,  3.91it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.24it/s] 12%|█▏        | 12/100 [00:03<00:20,  4.19it/s] 62%|██████▏   | 62/100 [00:15<00:09,  3.91it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.27it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.22it/s] 63%|██████▎   | 63/100 [00:16<00:09,  3.91it/s] 40%|████      | 40/100 [00:09<00:14,  4.25it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.23it/s] 64%|██████▍   | 64/100 [00:16<00:09,  3.91it/s] 41%|████      | 41/100 [00:10<00:13,  4.25it/s] 15%|█▌        | 15/100 [00:04<00:20,  4.23it/s] 65%|██████▌   | 65/100 [00:16<00:08,  3.90it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.26it/s] 16%|█▌        | 16/100 [00:04<00:20,  4.19it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.25it/s] 66%|██████▌   | 66/100 [00:17<00:08,  3.91it/s] 17%|█▋        | 17/100 [00:04<00:19,  4.21it/s]I0407 08:21:58.736865 36172 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:21:58.736958 36172 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:21:58.736999 36172 utils.py:162] NumExpr defaulting to 16 threads.
 44%|████▍     | 44/100 [00:10<00:13,  4.26it/s] 67%|██████▋   | 67/100 [00:17<00:08,  3.92it/s] 18%|█▊        | 18/100 [00:04<00:19,  4.20it/s]I0407 08:21:59.084146 36172 config.py:54] PyTorch version 2.6.0 available.
 45%|████▌     | 45/100 [00:11<00:12,  4.25it/s] 19%|█▉        | 19/100 [00:04<00:19,  4.21it/s] 68%|██████▊   | 68/100 [00:17<00:08,  3.92it/s]W0407 08:21:59.285748 36172 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 46%|████▌     | 46/100 [00:11<00:12,  4.26it/s] 20%|██        | 20/100 [00:05<00:18,  4.22it/s] 69%|██████▉   | 69/100 [00:17<00:07,  3.92it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.23it/s] 21%|██        | 21/100 [00:05<00:18,  4.22it/s] 70%|███████   | 70/100 [00:18<00:07,  3.92it/s] 48%|████▊     | 48/100 [00:11<00:12,  4.21it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.21it/s]W0407 08:21:59.894533 36172 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:21:59.898669 22419 quantize_finetune_llama.py:219] layer 8 gpu 0
I0407 08:21:59.913200 36172 data_utils.py:336] using 256 training seqs, 128 validation seqs
 71%|███████   | 71/100 [00:18<00:07,  3.91it/s] 49%|████▉     | 49/100 [00:12<00:12,  4.20it/s] 23%|██▎       | 23/100 [00:05<00:18,  4.21it/s] 72%|███████▏  | 72/100 [00:18<00:07,  3.92it/s] 50%|█████     | 50/100 [00:12<00:11,  4.20it/s] 24%|██▍       | 24/100 [00:06<00:18,  4.17it/s] 73%|███████▎  | 73/100 [00:18<00:06,  3.93it/s] 51%|█████     | 51/100 [00:12<00:11,  4.21it/s] 25%|██▌       | 25/100 [00:06<00:17,  4.20it/s] 74%|███████▍  | 74/100 [00:19<00:06,  3.92it/s]  0%|          | 0/100 [00:00<?, ?it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.23it/s] 26%|██▌       | 26/100 [00:06<00:17,  4.21it/s] 75%|███████▌  | 75/100 [00:19<00:06,  3.92it/s] 53%|█████▎    | 53/100 [00:12<00:11,  4.22it/s] 27%|██▋       | 27/100 [00:06<00:17,  4.20it/s] 76%|███████▌  | 76/100 [00:19<00:06,  3.91it/s] 54%|█████▍    | 54/100 [00:13<00:10,  4.25it/s] 28%|██▊       | 28/100 [00:07<00:17,  4.21it/s]  1%|          | 1/100 [00:00<00:59,  1.68it/s] 77%|███████▋  | 77/100 [00:19<00:05,  3.91it/s] 55%|█████▌    | 55/100 [00:13<00:10,  4.25it/s] 29%|██▉       | 29/100 [00:07<00:17,  4.18it/s]  2%|▏         | 2/100 [00:00<00:37,  2.63it/s] 78%|███████▊  | 78/100 [00:20<00:05,  3.92it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.22it/s] 30%|███       | 30/100 [00:07<00:16,  4.20it/s]  3%|▎         | 3/100 [00:01<00:30,  3.20it/s] 57%|█████▋    | 57/100 [00:13<00:10,  4.24it/s] 79%|███████▉  | 79/100 [00:20<00:05,  3.92it/s]  4%|▍         | 4/100 [00:01<00:26,  3.57it/s] 31%|███       | 31/100 [00:07<00:16,  4.18it/s] 58%|█████▊    | 58/100 [00:14<00:09,  4.22it/s] 80%|████████  | 80/100 [00:20<00:05,  3.91it/s]  5%|▌         | 5/100 [00:01<00:24,  3.81it/s] 32%|███▏      | 32/100 [00:08<00:16,  4.19it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.19it/s]  6%|▌         | 6/100 [00:01<00:23,  3.97it/s] 33%|███▎      | 33/100 [00:08<00:15,  4.19it/s] 81%|████████  | 81/100 [00:20<00:04,  3.90it/s] 60%|██████    | 60/100 [00:14<00:09,  4.20it/s]  7%|▋         | 7/100 [00:01<00:22,  4.08it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.21it/s] 82%|████████▏ | 82/100 [00:21<00:04,  3.89it/s] 61%|██████    | 61/100 [00:14<00:09,  4.21it/s]  8%|▊         | 8/100 [00:02<00:22,  4.16it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.21it/s] 83%|████████▎ | 83/100 [00:21<00:04,  3.90it/s] 62%|██████▏   | 62/100 [00:15<00:08,  4.23it/s]  9%|▉         | 9/100 [00:02<00:21,  4.22it/s] 36%|███▌      | 36/100 [00:09<00:15,  4.20it/s] 84%|████████▍ | 84/100 [00:21<00:04,  3.90it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.23it/s] 10%|█         | 10/100 [00:02<00:21,  4.26it/s] 37%|███▋      | 37/100 [00:09<00:15,  4.17it/s] 85%|████████▌ | 85/100 [00:21<00:03,  3.91it/s] 11%|█         | 11/100 [00:02<00:20,  4.28it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.22it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.16it/s] 86%|████████▌ | 86/100 [00:22<00:03,  3.91it/s] 12%|█▏        | 12/100 [00:03<00:20,  4.30it/s] 65%|██████▌   | 65/100 [00:15<00:08,  4.20it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.19it/s] 87%|████████▋ | 87/100 [00:22<00:03,  3.90it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.32it/s] 66%|██████▌   | 66/100 [00:16<00:08,  4.22it/s] 40%|████      | 40/100 [00:10<00:14,  4.19it/s] 88%|████████▊ | 88/100 [00:22<00:03,  3.91it/s] 14%|█▍        | 14/100 [00:03<00:19,  4.33it/s] 67%|██████▋   | 67/100 [00:16<00:07,  4.22it/s] 41%|████      | 41/100 [00:10<00:14,  4.20it/s] 89%|████████▉ | 89/100 [00:22<00:02,  3.91it/s] 15%|█▌        | 15/100 [00:03<00:19,  4.33it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.24it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.20it/s] 16%|█▌        | 16/100 [00:04<00:19,  4.34it/s] 90%|█████████ | 90/100 [00:23<00:02,  3.92it/s] 69%|██████▉   | 69/100 [00:16<00:07,  4.23it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.19it/s] 17%|█▋        | 17/100 [00:04<00:19,  4.34it/s] 70%|███████   | 70/100 [00:17<00:07,  4.24it/s] 91%|█████████ | 91/100 [00:23<00:02,  3.91it/s] 44%|████▍     | 44/100 [00:10<00:13,  4.18it/s] 18%|█▊        | 18/100 [00:04<00:18,  4.32it/s] 71%|███████   | 71/100 [00:17<00:06,  4.23it/s] 92%|█████████▏| 92/100 [00:23<00:02,  3.91it/s] 45%|████▌     | 45/100 [00:11<00:13,  4.19it/s] 19%|█▉        | 19/100 [00:04<00:18,  4.33it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.23it/s] 93%|█████████▎| 93/100 [00:23<00:01,  3.91it/s] 46%|████▌     | 46/100 [00:11<00:12,  4.16it/s] 20%|██        | 20/100 [00:04<00:18,  4.31it/s] 73%|███████▎  | 73/100 [00:17<00:06,  4.23it/s] 94%|█████████▍| 94/100 [00:24<00:01,  3.91it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.17it/s] 21%|██        | 21/100 [00:05<00:18,  4.29it/s] 74%|███████▍  | 74/100 [00:17<00:06,  4.23it/s] 95%|█████████▌| 95/100 [00:24<00:01,  3.92it/s] 48%|████▊     | 48/100 [00:11<00:12,  4.19it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.31it/s] 75%|███████▌  | 75/100 [00:18<00:05,  4.19it/s] 49%|████▉     | 49/100 [00:12<00:12,  4.19it/s] 96%|█████████▌| 96/100 [00:24<00:01,  3.91it/s] 23%|██▎       | 23/100 [00:05<00:17,  4.30it/s] 76%|███████▌  | 76/100 [00:18<00:05,  4.18it/s] 50%|█████     | 50/100 [00:12<00:11,  4.19it/s] 97%|█████████▋| 97/100 [00:24<00:00,  3.89it/s] 24%|██▍       | 24/100 [00:05<00:17,  4.31it/s] 77%|███████▋  | 77/100 [00:18<00:05,  4.19it/s] 51%|█████     | 51/100 [00:12<00:11,  4.19it/s] 98%|█████████▊| 98/100 [00:25<00:00,  3.90it/s] 25%|██▌       | 25/100 [00:06<00:17,  4.32it/s] 78%|███████▊  | 78/100 [00:18<00:05,  4.17it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.20it/s] 26%|██▌       | 26/100 [00:06<00:17,  4.32it/s] 99%|█████████▉| 99/100 [00:25<00:00,  3.90it/s] 79%|███████▉  | 79/100 [00:19<00:05,  4.17it/s] 53%|█████▎    | 53/100 [00:13<00:11,  4.21it/s] 27%|██▋       | 27/100 [00:06<00:16,  4.31it/s]100%|██████████| 100/100 [00:25<00:00,  3.90it/s]100%|██████████| 100/100 [00:25<00:00,  3.89it/s]
 80%|████████  | 80/100 [00:19<00:04,  4.19it/s] 54%|█████▍    | 54/100 [00:13<00:11,  4.18it/s] 28%|██▊       | 28/100 [00:06<00:16,  4.29it/s] 81%|████████  | 81/100 [00:19<00:04,  4.18it/s] 55%|█████▌    | 55/100 [00:13<00:10,  4.18it/s] 29%|██▉       | 29/100 [00:07<00:16,  4.30it/s] 82%|████████▏ | 82/100 [00:19<00:04,  4.19it/s]Process Process-5:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
4_v proxy err 0.013127539306879044 err 3.7453806400299072 tr(WHW.T) 285.30712890625
bpp_loss 3.435328722000122
 56%|█████▌    | 56/100 [00:13<00:10,  4.19it/s] 30%|███       | 30/100 [00:07<00:16,  4.29it/s] 83%|████████▎ | 83/100 [00:20<00:04,  4.18it/s] 57%|█████▋    | 57/100 [00:14<00:10,  4.18it/s] 31%|███       | 31/100 [00:07<00:16,  4.30it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.18it/s] 58%|█████▊    | 58/100 [00:14<00:10,  4.17it/s] 32%|███▏      | 32/100 [00:07<00:15,  4.28it/s] 85%|████████▌ | 85/100 [00:20<00:03,  4.20it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.17it/s] 33%|███▎      | 33/100 [00:08<00:15,  4.29it/s] 86%|████████▌ | 86/100 [00:20<00:03,  4.21it/s] 60%|██████    | 60/100 [00:14<00:09,  4.18it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.30it/s] 87%|████████▋ | 87/100 [00:21<00:03,  4.21it/s] 61%|██████    | 61/100 [00:15<00:09,  4.15it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.29it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.21it/s] 36%|███▌      | 36/100 [00:08<00:14,  4.29it/s] 62%|██████▏   | 62/100 [00:15<00:09,  4.17it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.22it/s] 37%|███▋      | 37/100 [00:08<00:14,  4.29it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.17it/s] 90%|█████████ | 90/100 [00:21<00:02,  4.20it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.30it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.14it/s] 91%|█████████ | 91/100 [00:22<00:02,  4.18it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.31it/s] 65%|██████▌   | 65/100 [00:15<00:08,  4.16it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.20it/s] 40%|████      | 40/100 [00:09<00:13,  4.32it/s] 66%|██████▌   | 66/100 [00:16<00:08,  4.18it/s] 93%|█████████▎| 93/100 [00:22<00:01,  4.20it/s] 41%|████      | 41/100 [00:09<00:13,  4.32it/s] 67%|██████▋   | 67/100 [00:16<00:07,  4.18it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.21it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.31it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.17it/s] 95%|█████████▌| 95/100 [00:22<00:01,  4.20it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.28it/s] 69%|██████▉   | 69/100 [00:16<00:07,  4.17it/s] 96%|█████████▌| 96/100 [00:23<00:00,  4.21it/s] 44%|████▍     | 44/100 [00:10<00:13,  4.29it/s] 70%|███████   | 70/100 [00:17<00:07,  4.17it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.20it/s] 45%|████▌     | 45/100 [00:10<00:12,  4.28it/s] 71%|███████   | 71/100 [00:17<00:06,  4.17it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.19it/s] 46%|████▌     | 46/100 [00:11<00:12,  4.30it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.17it/s] 99%|█████████▉| 99/100 [00:23<00:00,  4.20it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.29it/s] 73%|███████▎  | 73/100 [00:17<00:06,  4.15it/s]100%|██████████| 100/100 [00:24<00:00,  4.22it/s]100%|██████████| 100/100 [00:24<00:00,  4.14it/s]
 48%|████▊     | 48/100 [00:11<00:12,  4.29it/s] 74%|███████▍  | 74/100 [00:18<00:06,  4.17it/s] 49%|████▉     | 49/100 [00:11<00:11,  4.28it/s] 75%|███████▌  | 75/100 [00:18<00:05,  4.18it/s] 50%|█████     | 50/100 [00:11<00:11,  4.27it/s]Process Process-6:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
5_v proxy err 0.015470629557967186 err 3.2305750846862793 tr(WHW.T) 208.81988525390625
bpp_loss 3.3319993019104004
 76%|███████▌  | 76/100 [00:18<00:05,  4.17it/s] 51%|█████     | 51/100 [00:12<00:11,  4.27it/s] 77%|███████▋  | 77/100 [00:18<00:05,  4.15it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.27it/s] 78%|███████▊  | 78/100 [00:19<00:05,  4.16it/s] 53%|█████▎    | 53/100 [00:12<00:10,  4.28it/s] 79%|███████▉  | 79/100 [00:19<00:05,  4.18it/s] 54%|█████▍    | 54/100 [00:12<00:10,  4.28it/s] 80%|████████  | 80/100 [00:19<00:04,  4.17it/s] 55%|█████▌    | 55/100 [00:13<00:10,  4.27it/s] 81%|████████  | 81/100 [00:19<00:04,  4.17it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.28it/s] 82%|████████▏ | 82/100 [00:20<00:04,  4.15it/s] 57%|█████▋    | 57/100 [00:13<00:10,  4.29it/s] 83%|████████▎ | 83/100 [00:20<00:04,  4.17it/s] 58%|█████▊    | 58/100 [00:13<00:09,  4.28it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.17it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.28it/s]I0407 08:22:14.861744 22419 quantize_finetune_llama.py:250] computed original embedding for layer 8 in 1.036193609237671s
 85%|████████▌ | 85/100 [00:20<00:03,  4.17it/s] 60%|██████    | 60/100 [00:14<00:09,  4.29it/s] 86%|████████▌ | 86/100 [00:21<00:03,  4.17it/s] 61%|██████    | 61/100 [00:14<00:09,  4.25it/s] 87%|████████▋ | 87/100 [00:21<00:03,  4.16it/s] 62%|██████▏   | 62/100 [00:14<00:08,  4.25it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.16it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.27it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.17it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.28it/s] 90%|█████████ | 90/100 [00:21<00:02,  4.17it/s] 65%|██████▌   | 65/100 [00:15<00:08,  4.24it/s] 91%|█████████ | 91/100 [00:22<00:02,  4.15it/s] 66%|██████▌   | 66/100 [00:15<00:08,  4.25it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.16it/s] 67%|██████▋   | 67/100 [00:15<00:07,  4.26it/s] 93%|█████████▎| 93/100 [00:22<00:01,  4.18it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.28it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.18it/s] 69%|██████▉   | 69/100 [00:16<00:07,  4.25it/s] 95%|█████████▌| 95/100 [00:23<00:01,  4.17it/s] 70%|███████   | 70/100 [00:16<00:07,  4.25it/s] 96%|█████████▌| 96/100 [00:23<00:00,  4.17it/s] 71%|███████   | 71/100 [00:16<00:06,  4.26it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.17it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.25it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.17it/s] 73%|███████▎  | 73/100 [00:17<00:06,  4.25it/s] 99%|█████████▉| 99/100 [00:24<00:00,  4.17it/s] 74%|███████▍  | 74/100 [00:17<00:06,  4.24it/s]I0407 08:22:18.450006 36466 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:22:18.450100 36466 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:22:18.450138 36466 utils.py:162] NumExpr defaulting to 16 threads.
100%|██████████| 100/100 [00:24<00:00,  4.17it/s]100%|██████████| 100/100 [00:24<00:00,  4.10it/s]
 75%|███████▌  | 75/100 [00:17<00:05,  4.23it/s]I0407 08:22:18.783796 36466 config.py:54] PyTorch version 2.6.0 available.
 76%|███████▌  | 76/100 [00:18<00:05,  4.23it/s]W0407 08:22:18.977802 36466 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 77%|███████▋  | 77/100 [00:18<00:05,  4.23it/s]Process Process-7:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
6_v proxy err 0.013537196442484856 err 3.43349027633667 tr(WHW.T) 253.63377380371094
bpp_loss 3.3858790397644043
 78%|███████▊  | 78/100 [00:18<00:05,  4.23it/s] 79%|███████▉  | 79/100 [00:18<00:04,  4.25it/s]W0407 08:22:19.564050 36466 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:22:19.568295 22419 quantize_finetune_llama.py:219] layer 9 gpu 1
I0407 08:22:19.584773 36466 data_utils.py:336] using 256 training seqs, 128 validation seqs
 80%|████████  | 80/100 [00:19<00:04,  4.26it/s] 81%|████████  | 81/100 [00:19<00:04,  4.26it/s] 82%|████████▏ | 82/100 [00:19<00:04,  4.27it/s] 83%|████████▎ | 83/100 [00:19<00:03,  4.27it/s]  0%|          | 0/100 [00:00<?, ?it/s] 84%|████████▍ | 84/100 [00:19<00:03,  4.26it/s] 85%|████████▌ | 85/100 [00:20<00:03,  4.27it/s] 86%|████████▌ | 86/100 [00:20<00:03,  4.26it/s]I0407 08:22:21.181688 22419 quantize_finetune_llama.py:250] computed original embedding for layer 9 in 0.8745450973510742s
  1%|          | 1/100 [00:00<01:11,  1.38it/s] 87%|████████▋ | 87/100 [00:20<00:03,  4.23it/s]  2%|▏         | 2/100 [00:00<00:43,  2.26it/s] 88%|████████▊ | 88/100 [00:20<00:02,  4.12it/s]  3%|▎         | 3/100 [00:01<00:34,  2.82it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.17it/s]  4%|▍         | 4/100 [00:01<00:30,  3.20it/s] 90%|█████████ | 90/100 [00:21<00:02,  4.19it/s]  5%|▌         | 5/100 [00:01<00:27,  3.45it/s] 91%|█████████ | 91/100 [00:21<00:02,  4.22it/s] 92%|█████████▏| 92/100 [00:21<00:01,  4.24it/s]  6%|▌         | 6/100 [00:01<00:25,  3.62it/s] 93%|█████████▎| 93/100 [00:22<00:01,  4.23it/s]  7%|▋         | 7/100 [00:02<00:24,  3.74it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.25it/s]  8%|▊         | 8/100 [00:02<00:24,  3.82it/s] 95%|█████████▌| 95/100 [00:22<00:01,  4.25it/s]  9%|▉         | 9/100 [00:02<00:23,  3.88it/s] 96%|█████████▌| 96/100 [00:22<00:00,  4.25it/s] 10%|█         | 10/100 [00:02<00:23,  3.91it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.22it/s] 11%|█         | 11/100 [00:03<00:22,  3.94it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.23it/s] 12%|█▏        | 12/100 [00:03<00:22,  3.96it/s] 99%|█████████▉| 99/100 [00:23<00:00,  4.23it/s] 13%|█▎        | 13/100 [00:03<00:21,  3.97it/s]100%|██████████| 100/100 [00:23<00:00,  4.25it/s]100%|██████████| 100/100 [00:23<00:00,  4.21it/s]
 14%|█▍        | 14/100 [00:03<00:21,  3.98it/s] 15%|█▌        | 15/100 [00:04<00:21,  3.99it/s]I0407 08:22:24.891804 36617 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:22:24.891920 36617 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:22:24.891956 36617 utils.py:162] NumExpr defaulting to 16 threads.
Process Process-8:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
7_v proxy err 0.010988185182213783 err 3.4000422954559326 tr(WHW.T) 309.4270935058594
bpp_loss 3.380939245223999
 16%|█▌        | 16/100 [00:04<00:21,  3.99it/s]I0407 08:22:25.256963 36617 config.py:54] PyTorch version 2.6.0 available.
 17%|█▋        | 17/100 [00:04<00:20,  4.00it/s]W0407 08:22:25.457376 36617 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 18%|█▊        | 18/100 [00:04<00:20,  3.97it/s] 19%|█▉        | 19/100 [00:05<00:20,  4.00it/s]W0407 08:22:26.040346 36617 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:22:26.044344 22419 quantize_finetune_llama.py:219] layer 10 gpu 2
I0407 08:22:26.061029 36617 data_utils.py:336] using 256 training seqs, 128 validation seqs
 20%|██        | 20/100 [00:05<00:20,  3.99it/s] 21%|██        | 21/100 [00:05<00:19,  4.00it/s] 22%|██▏       | 22/100 [00:05<00:19,  3.99it/s] 23%|██▎       | 23/100 [00:06<00:19,  3.97it/s]  0%|          | 0/100 [00:00<?, ?it/s] 24%|██▍       | 24/100 [00:06<00:19,  3.96it/s] 25%|██▌       | 25/100 [00:06<00:18,  3.96it/s]I0407 08:22:27.448582 22419 quantize_finetune_llama.py:250] computed original embedding for layer 10 in 0.9820122718811035s
 26%|██▌       | 26/100 [00:06<00:18,  3.98it/s]  1%|          | 1/100 [00:00<01:05,  1.50it/s] 27%|██▋       | 27/100 [00:07<00:18,  3.96it/s]  2%|▏         | 2/100 [00:00<00:40,  2.44it/s] 28%|██▊       | 28/100 [00:07<00:18,  3.96it/s]  3%|▎         | 3/100 [00:01<00:32,  3.02it/s] 29%|██▉       | 29/100 [00:07<00:17,  3.97it/s]  4%|▍         | 4/100 [00:01<00:28,  3.40it/s] 30%|███       | 30/100 [00:07<00:17,  3.95it/s]  5%|▌         | 5/100 [00:01<00:26,  3.65it/s] 31%|███       | 31/100 [00:08<00:17,  3.97it/s]  6%|▌         | 6/100 [00:01<00:24,  3.84it/s] 32%|███▏      | 32/100 [00:08<00:17,  3.98it/s]  7%|▋         | 7/100 [00:02<00:23,  3.96it/s] 33%|███▎      | 33/100 [00:08<00:16,  3.98it/s]  8%|▊         | 8/100 [00:02<00:22,  4.05it/s] 34%|███▍      | 34/100 [00:08<00:16,  3.96it/s]  9%|▉         | 9/100 [00:02<00:22,  4.11it/s] 35%|███▌      | 35/100 [00:09<00:16,  3.97it/s] 10%|█         | 10/100 [00:02<00:21,  4.16it/s] 11%|█         | 11/100 [00:03<00:21,  4.19it/s] 36%|███▌      | 36/100 [00:09<00:16,  3.97it/s] 12%|█▏        | 12/100 [00:03<00:20,  4.21it/s] 37%|███▋      | 37/100 [00:09<00:15,  3.96it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.20it/s] 38%|███▊      | 38/100 [00:10<00:15,  3.95it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.21it/s] 39%|███▉      | 39/100 [00:10<00:15,  3.97it/s] 15%|█▌        | 15/100 [00:03<00:20,  4.23it/s]I0407 08:22:31.107134 36753 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:22:31.107222 36753 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:22:31.107260 36753 utils.py:162] NumExpr defaulting to 16 threads.
 40%|████      | 40/100 [00:10<00:15,  3.98it/s] 16%|█▌        | 16/100 [00:04<00:19,  4.23it/s] 41%|████      | 41/100 [00:10<00:14,  3.98it/s]I0407 08:22:31.441906 36753 config.py:54] PyTorch version 2.6.0 available.
 17%|█▋        | 17/100 [00:04<00:19,  4.24it/s] 42%|████▏     | 42/100 [00:11<00:14,  3.96it/s]W0407 08:22:31.641732 36753 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 18%|█▊        | 18/100 [00:04<00:19,  4.24it/s] 43%|████▎     | 43/100 [00:11<00:14,  3.94it/s] 19%|█▉        | 19/100 [00:04<00:19,  4.25it/s] 44%|████▍     | 44/100 [00:11<00:14,  3.94it/s]W0407 08:22:32.228630 36753 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

 20%|██        | 20/100 [00:05<00:18,  4.24it/s]I0407 08:22:32.232498 22419 quantize_finetune_llama.py:219] layer 11 gpu 3
I0407 08:22:32.245608 36753 data_utils.py:336] using 256 training seqs, 128 validation seqs
 45%|████▌     | 45/100 [00:11<00:13,  3.96it/s] 21%|██        | 21/100 [00:05<00:18,  4.21it/s] 46%|████▌     | 46/100 [00:12<00:13,  3.93it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.21it/s] 47%|████▋     | 47/100 [00:12<00:13,  3.95it/s] 23%|██▎       | 23/100 [00:05<00:18,  4.21it/s] 48%|████▊     | 48/100 [00:12<00:13,  3.96it/s] 24%|██▍       | 24/100 [00:06<00:18,  4.22it/s]  0%|          | 0/100 [00:00<?, ?it/s] 49%|████▉     | 49/100 [00:12<00:12,  3.99it/s] 25%|██▌       | 25/100 [00:06<00:17,  4.20it/s] 50%|█████     | 50/100 [00:13<00:12,  3.99it/s] 26%|██▌       | 26/100 [00:06<00:17,  4.20it/s]I0407 08:22:33.713376 22419 quantize_finetune_llama.py:250] computed original embedding for layer 11 in 1.0597927570343018s
 27%|██▋       | 27/100 [00:06<00:17,  4.20it/s] 51%|█████     | 51/100 [00:13<00:12,  3.96it/s]  1%|          | 1/100 [00:00<01:02,  1.59it/s] 28%|██▊       | 28/100 [00:07<00:17,  4.10it/s] 52%|█████▏    | 52/100 [00:13<00:12,  3.90it/s]  2%|▏         | 2/100 [00:00<00:39,  2.47it/s] 29%|██▉       | 29/100 [00:07<00:17,  4.13it/s] 53%|█████▎    | 53/100 [00:13<00:11,  3.93it/s]  3%|▎         | 3/100 [00:01<00:31,  3.05it/s] 30%|███       | 30/100 [00:07<00:16,  4.17it/s] 54%|█████▍    | 54/100 [00:14<00:11,  3.95it/s]  4%|▍         | 4/100 [00:01<00:28,  3.43it/s] 31%|███       | 31/100 [00:07<00:16,  4.18it/s] 55%|█████▌    | 55/100 [00:14<00:11,  3.94it/s]  5%|▌         | 5/100 [00:01<00:25,  3.68it/s] 32%|███▏      | 32/100 [00:08<00:16,  4.20it/s]  6%|▌         | 6/100 [00:01<00:24,  3.84it/s] 56%|█████▌    | 56/100 [00:14<00:11,  3.96it/s] 33%|███▎      | 33/100 [00:08<00:15,  4.19it/s]  7%|▋         | 7/100 [00:02<00:23,  3.92it/s] 57%|█████▋    | 57/100 [00:14<00:10,  3.95it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.20it/s]  8%|▊         | 8/100 [00:02<00:22,  4.00it/s] 58%|█████▊    | 58/100 [00:15<00:10,  3.92it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.20it/s]  9%|▉         | 9/100 [00:02<00:22,  4.05it/s] 59%|█████▉    | 59/100 [00:15<00:10,  3.94it/s] 36%|███▌      | 36/100 [00:08<00:15,  4.18it/s] 10%|█         | 10/100 [00:02<00:21,  4.09it/s] 60%|██████    | 60/100 [00:15<00:10,  3.94it/s] 37%|███▋      | 37/100 [00:09<00:15,  4.18it/s] 11%|█         | 11/100 [00:03<00:21,  4.12it/s] 61%|██████    | 61/100 [00:15<00:09,  3.95it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.20it/s] 12%|█▏        | 12/100 [00:03<00:21,  4.14it/s] 62%|██████▏   | 62/100 [00:16<00:09,  3.95it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.22it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.17it/s] 63%|██████▎   | 63/100 [00:16<00:09,  3.96it/s] 40%|████      | 40/100 [00:09<00:14,  4.21it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.19it/s] 64%|██████▍   | 64/100 [00:16<00:09,  3.96it/s] 41%|████      | 41/100 [00:10<00:14,  4.20it/s] 15%|█▌        | 15/100 [00:03<00:20,  4.18it/s]I0407 08:22:37.390494 36894 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:22:37.390593 36894 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:22:37.390632 36894 utils.py:162] NumExpr defaulting to 16 threads.
 65%|██████▌   | 65/100 [00:16<00:08,  3.97it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.22it/s] 16%|█▌        | 16/100 [00:04<00:20,  4.17it/s] 66%|██████▌   | 66/100 [00:17<00:08,  3.97it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.21it/s]I0407 08:22:37.728153 36894 config.py:54] PyTorch version 2.6.0 available.
 17%|█▋        | 17/100 [00:04<00:19,  4.17it/s]W0407 08:22:37.924295 36894 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 44%|████▍     | 44/100 [00:10<00:13,  4.19it/s] 67%|██████▋   | 67/100 [00:17<00:08,  3.96it/s] 18%|█▊        | 18/100 [00:04<00:19,  4.17it/s] 45%|████▌     | 45/100 [00:11<00:13,  4.19it/s] 68%|██████▊   | 68/100 [00:17<00:08,  3.96it/s] 19%|█▉        | 19/100 [00:04<00:19,  4.16it/s] 46%|████▌     | 46/100 [00:11<00:12,  4.17it/s] 69%|██████▉   | 69/100 [00:17<00:07,  3.97it/s]W0407 08:22:38.494659 36894 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:22:38.498568 22419 quantize_finetune_llama.py:219] layer 12 gpu 0
I0407 08:22:38.511888 36894 data_utils.py:336] using 256 training seqs, 128 validation seqs
 20%|██        | 20/100 [00:05<00:19,  4.18it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.18it/s] 70%|███████   | 70/100 [00:18<00:07,  3.97it/s] 21%|██        | 21/100 [00:05<00:18,  4.18it/s] 48%|████▊     | 48/100 [00:11<00:12,  4.19it/s] 71%|███████   | 71/100 [00:18<00:07,  3.95it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.17it/s]  0%|          | 0/100 [00:00<?, ?it/s] 49%|████▉     | 49/100 [00:12<00:12,  4.21it/s] 72%|███████▏  | 72/100 [00:18<00:07,  3.96it/s] 23%|██▎       | 23/100 [00:05<00:18,  4.16it/s] 50%|█████     | 50/100 [00:12<00:11,  4.21it/s] 73%|███████▎  | 73/100 [00:18<00:06,  3.96it/s] 24%|██▍       | 24/100 [00:06<00:18,  4.16it/s] 51%|█████     | 51/100 [00:12<00:11,  4.21it/s]  1%|          | 1/100 [00:00<01:02,  1.59it/s] 25%|██▌       | 25/100 [00:06<00:18,  4.16it/s] 74%|███████▍  | 74/100 [00:19<00:06,  3.96it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.21it/s]  2%|▏         | 2/100 [00:00<00:38,  2.52it/s] 26%|██▌       | 26/100 [00:06<00:17,  4.15it/s] 75%|███████▌  | 75/100 [00:19<00:06,  3.94it/s] 53%|█████▎    | 53/100 [00:13<00:11,  4.20it/s]  3%|▎         | 3/100 [00:01<00:31,  3.11it/s] 27%|██▋       | 27/100 [00:06<00:17,  4.16it/s] 76%|███████▌  | 76/100 [00:19<00:06,  3.95it/s] 54%|█████▍    | 54/100 [00:13<00:10,  4.21it/s]  4%|▍         | 4/100 [00:01<00:27,  3.48it/s] 28%|██▊       | 28/100 [00:07<00:17,  4.18it/s] 77%|███████▋  | 77/100 [00:19<00:05,  3.94it/s]  5%|▌         | 5/100 [00:01<00:25,  3.73it/s] 55%|█████▌    | 55/100 [00:13<00:10,  4.21it/s] 29%|██▉       | 29/100 [00:07<00:17,  4.18it/s] 78%|███████▊  | 78/100 [00:20<00:05,  3.95it/s]  6%|▌         | 6/100 [00:01<00:24,  3.91it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.20it/s] 30%|███       | 30/100 [00:07<00:16,  4.17it/s] 79%|███████▉  | 79/100 [00:20<00:05,  3.95it/s]  7%|▋         | 7/100 [00:02<00:23,  4.01it/s] 57%|█████▋    | 57/100 [00:13<00:10,  4.21it/s] 31%|███       | 31/100 [00:07<00:16,  4.15it/s] 80%|████████  | 80/100 [00:20<00:05,  3.95it/s]  8%|▊         | 8/100 [00:02<00:22,  4.10it/s] 58%|█████▊    | 58/100 [00:14<00:10,  4.20it/s] 32%|███▏      | 32/100 [00:08<00:16,  4.17it/s]  9%|▉         | 9/100 [00:02<00:21,  4.17it/s] 81%|████████  | 81/100 [00:20<00:04,  3.95it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.21it/s] 33%|███▎      | 33/100 [00:08<00:16,  4.17it/s] 10%|█         | 10/100 [00:02<00:21,  4.20it/s] 82%|████████▏ | 82/100 [00:21<00:04,  3.95it/s] 60%|██████    | 60/100 [00:14<00:09,  4.21it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.19it/s] 11%|█         | 11/100 [00:02<00:21,  4.22it/s] 61%|██████    | 61/100 [00:14<00:09,  4.20it/s] 83%|████████▎ | 83/100 [00:21<00:04,  3.94it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.20it/s] 12%|█▏        | 12/100 [00:03<00:20,  4.24it/s] 62%|██████▏   | 62/100 [00:15<00:09,  4.18it/s] 84%|████████▍ | 84/100 [00:21<00:04,  3.95it/s] 36%|███▌      | 36/100 [00:09<00:15,  4.19it/s] 13%|█▎        | 13/100 [00:03<00:20,  4.22it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.14it/s] 85%|████████▌ | 85/100 [00:21<00:03,  3.91it/s] 37%|███▋      | 37/100 [00:09<00:15,  4.17it/s] 14%|█▍        | 14/100 [00:03<00:20,  4.22it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.16it/s] 86%|████████▌ | 86/100 [00:22<00:03,  3.94it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.19it/s] 15%|█▌        | 15/100 [00:03<00:19,  4.25it/s] 65%|██████▌   | 65/100 [00:15<00:08,  4.18it/s] 87%|████████▋ | 87/100 [00:22<00:03,  3.92it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.18it/s] 16%|█▌        | 16/100 [00:04<00:19,  4.27it/s] 66%|██████▌   | 66/100 [00:16<00:08,  4.18it/s] 88%|████████▊ | 88/100 [00:22<00:03,  3.92it/s] 40%|████      | 40/100 [00:09<00:14,  4.17it/s] 17%|█▋        | 17/100 [00:04<00:19,  4.24it/s] 67%|██████▋   | 67/100 [00:16<00:07,  4.17it/s] 89%|████████▉ | 89/100 [00:22<00:02,  3.90it/s] 41%|████      | 41/100 [00:10<00:14,  4.16it/s] 18%|█▊        | 18/100 [00:04<00:19,  4.24it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.16it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.16it/s] 90%|█████████ | 90/100 [00:23<00:02,  3.92it/s] 19%|█▉        | 19/100 [00:04<00:19,  4.24it/s] 69%|██████▉   | 69/100 [00:16<00:07,  4.18it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.18it/s] 91%|█████████ | 91/100 [00:23<00:02,  3.93it/s] 20%|██        | 20/100 [00:05<00:18,  4.25it/s] 70%|███████   | 70/100 [00:17<00:07,  4.17it/s] 44%|████▍     | 44/100 [00:10<00:13,  4.18it/s] 92%|█████████▏| 92/100 [00:23<00:02,  3.93it/s] 21%|██        | 21/100 [00:05<00:18,  4.26it/s] 71%|███████   | 71/100 [00:17<00:06,  4.15it/s] 45%|████▌     | 45/100 [00:11<00:13,  4.18it/s] 22%|██▏       | 22/100 [00:05<00:18,  4.26it/s] 93%|█████████▎| 93/100 [00:23<00:01,  3.93it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.18it/s] 46%|████▌     | 46/100 [00:11<00:12,  4.16it/s] 23%|██▎       | 23/100 [00:05<00:18,  4.25it/s] 94%|█████████▍| 94/100 [00:24<00:01,  3.92it/s] 73%|███████▎  | 73/100 [00:17<00:06,  4.18it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.17it/s] 24%|██▍       | 24/100 [00:06<00:17,  4.25it/s] 95%|█████████▌| 95/100 [00:24<00:01,  3.94it/s] 74%|███████▍  | 74/100 [00:18<00:06,  4.17it/s] 48%|████▊     | 48/100 [00:11<00:12,  4.16it/s] 25%|██▌       | 25/100 [00:06<00:17,  4.24it/s] 96%|█████████▌| 96/100 [00:24<00:01,  3.93it/s] 75%|███████▌  | 75/100 [00:18<00:06,  4.17it/s] 49%|████▉     | 49/100 [00:12<00:12,  4.14it/s] 26%|██▌       | 26/100 [00:06<00:17,  4.26it/s] 97%|█████████▋| 97/100 [00:24<00:00,  3.93it/s] 76%|███████▌  | 76/100 [00:18<00:05,  4.17it/s] 50%|█████     | 50/100 [00:12<00:12,  4.15it/s] 27%|██▋       | 27/100 [00:06<00:17,  4.27it/s] 98%|█████████▊| 98/100 [00:25<00:00,  3.94it/s] 77%|███████▋  | 77/100 [00:18<00:05,  4.17it/s] 51%|█████     | 51/100 [00:12<00:11,  4.14it/s] 28%|██▊       | 28/100 [00:06<00:16,  4.25it/s] 78%|███████▊  | 78/100 [00:18<00:05,  4.17it/s] 99%|█████████▉| 99/100 [00:25<00:00,  3.94it/s] 29%|██▉       | 29/100 [00:07<00:16,  4.25it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.15it/s] 79%|███████▉  | 79/100 [00:19<00:05,  4.17it/s]100%|██████████| 100/100 [00:25<00:00,  3.93it/s]100%|██████████| 100/100 [00:25<00:00,  3.89it/s]
 30%|███       | 30/100 [00:07<00:16,  4.24it/s] 53%|█████▎    | 53/100 [00:13<00:11,  4.16it/s] 80%|████████  | 80/100 [00:19<00:04,  4.16it/s] 31%|███       | 31/100 [00:07<00:16,  4.22it/s] 54%|█████▍    | 54/100 [00:13<00:11,  4.16it/s] 81%|████████  | 81/100 [00:19<00:04,  4.17it/s] 32%|███▏      | 32/100 [00:07<00:16,  4.22it/s]Process Process-9:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
 55%|█████▌    | 55/100 [00:13<00:10,  4.15it/s]8_v proxy err 0.013607196509838104 err 3.506645679473877 tr(WHW.T) 257.7052307128906
bpp_loss 3.3984456062316895
 82%|████████▏ | 82/100 [00:19<00:04,  4.17it/s] 33%|███▎      | 33/100 [00:08<00:15,  4.25it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.17it/s] 83%|████████▎ | 83/100 [00:20<00:04,  4.17it/s] 34%|███▍      | 34/100 [00:08<00:15,  4.23it/s] 57%|█████▋    | 57/100 [00:14<00:10,  4.15it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.15it/s] 35%|███▌      | 35/100 [00:08<00:15,  4.26it/s] 58%|█████▊    | 58/100 [00:14<00:10,  4.15it/s] 85%|████████▌ | 85/100 [00:20<00:03,  4.18it/s] 36%|███▌      | 36/100 [00:08<00:15,  4.26it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.18it/s] 86%|████████▌ | 86/100 [00:20<00:03,  4.17it/s] 37%|███▋      | 37/100 [00:09<00:14,  4.21it/s] 60%|██████    | 60/100 [00:14<00:09,  4.16it/s] 87%|████████▋ | 87/100 [00:21<00:03,  4.17it/s] 38%|███▊      | 38/100 [00:09<00:14,  4.24it/s] 61%|██████    | 61/100 [00:15<00:09,  4.18it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.17it/s] 39%|███▉      | 39/100 [00:09<00:14,  4.24it/s] 62%|██████▏   | 62/100 [00:15<00:09,  4.18it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.17it/s] 40%|████      | 40/100 [00:09<00:14,  4.24it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.18it/s] 90%|█████████ | 90/100 [00:21<00:02,  4.17it/s] 41%|████      | 41/100 [00:10<00:13,  4.24it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.16it/s] 91%|█████████ | 91/100 [00:22<00:02,  4.17it/s] 42%|████▏     | 42/100 [00:10<00:13,  4.26it/s] 65%|██████▌   | 65/100 [00:15<00:08,  4.17it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.17it/s] 43%|████▎     | 43/100 [00:10<00:13,  4.27it/s] 66%|██████▌   | 66/100 [00:16<00:08,  4.17it/s] 93%|█████████▎| 93/100 [00:22<00:01,  4.15it/s] 44%|████▍     | 44/100 [00:10<00:13,  4.25it/s] 67%|██████▋   | 67/100 [00:16<00:07,  4.17it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.18it/s] 45%|████▌     | 45/100 [00:10<00:12,  4.26it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.17it/s] 95%|█████████▌| 95/100 [00:23<00:01,  4.17it/s] 46%|████▌     | 46/100 [00:11<00:12,  4.26it/s] 69%|██████▉   | 69/100 [00:16<00:07,  4.16it/s] 96%|█████████▌| 96/100 [00:23<00:00,  4.18it/s] 47%|████▋     | 47/100 [00:11<00:12,  4.27it/s] 70%|███████   | 70/100 [00:17<00:07,  4.17it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.17it/s] 48%|████▊     | 48/100 [00:11<00:12,  4.26it/s] 71%|███████   | 71/100 [00:17<00:06,  4.17it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.15it/s] 49%|████▉     | 49/100 [00:11<00:12,  4.24it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.16it/s] 99%|█████████▉| 99/100 [00:24<00:00,  4.17it/s] 50%|█████     | 50/100 [00:12<00:11,  4.26it/s] 73%|███████▎  | 73/100 [00:17<00:06,  4.17it/s]100%|██████████| 100/100 [00:24<00:00,  4.16it/s]100%|██████████| 100/100 [00:24<00:00,  4.12it/s]
 51%|█████     | 51/100 [00:12<00:11,  4.24it/s] 74%|███████▍  | 74/100 [00:18<00:06,  4.17it/s] 52%|█████▏    | 52/100 [00:12<00:11,  4.24it/s] 75%|███████▌  | 75/100 [00:18<00:06,  4.15it/s] 53%|█████▎    | 53/100 [00:12<00:11,  4.25it/s]Process Process-10:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
9_v proxy err 0.011013888753950596 err 3.8705978393554688 tr(WHW.T) 351.4288024902344
bpp_loss 3.485344886779785
 76%|███████▌  | 76/100 [00:18<00:05,  4.16it/s] 54%|█████▍    | 54/100 [00:13<00:10,  4.26it/s] 77%|███████▋  | 77/100 [00:18<00:05,  4.15it/s] 55%|█████▌    | 55/100 [00:13<00:10,  4.27it/s] 78%|███████▊  | 78/100 [00:19<00:05,  4.17it/s] 56%|█████▌    | 56/100 [00:13<00:10,  4.25it/s] 79%|███████▉  | 79/100 [00:19<00:05,  4.16it/s] 57%|█████▋    | 57/100 [00:13<00:10,  4.22it/s] 80%|████████  | 80/100 [00:19<00:04,  4.15it/s] 58%|█████▊    | 58/100 [00:14<00:09,  4.23it/s] 81%|████████  | 81/100 [00:19<00:04,  4.16it/s] 59%|█████▉    | 59/100 [00:14<00:09,  4.24it/s] 82%|████████▏ | 82/100 [00:20<00:04,  4.16it/s] 60%|██████    | 60/100 [00:14<00:09,  4.23it/s] 83%|████████▎ | 83/100 [00:20<00:04,  4.14it/s] 61%|██████    | 61/100 [00:14<00:09,  4.20it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.15it/s] 62%|██████▏   | 62/100 [00:14<00:09,  4.21it/s]I0407 08:22:54.031769 22419 quantize_finetune_llama.py:250] computed original embedding for layer 12 in 0.9985017776489258s
 85%|████████▌ | 85/100 [00:20<00:03,  4.15it/s] 63%|██████▎   | 63/100 [00:15<00:08,  4.23it/s] 86%|████████▌ | 86/100 [00:21<00:03,  4.12it/s] 64%|██████▍   | 64/100 [00:15<00:08,  4.21it/s] 87%|████████▋ | 87/100 [00:21<00:03,  4.12it/s] 65%|██████▌   | 65/100 [00:15<00:08,  4.22it/s] 88%|████████▊ | 88/100 [00:21<00:02,  4.13it/s] 66%|██████▌   | 66/100 [00:15<00:08,  4.21it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.14it/s] 67%|██████▋   | 67/100 [00:16<00:07,  4.23it/s] 90%|█████████ | 90/100 [00:21<00:02,  4.15it/s] 68%|██████▊   | 68/100 [00:16<00:07,  4.24it/s] 91%|█████████ | 91/100 [00:22<00:02,  4.14it/s] 69%|██████▉   | 69/100 [00:16<00:07,  4.24it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.14it/s] 70%|███████   | 70/100 [00:16<00:07,  4.23it/s] 93%|█████████▎| 93/100 [00:22<00:01,  4.15it/s] 71%|███████   | 71/100 [00:17<00:06,  4.22it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.15it/s] 72%|███████▏  | 72/100 [00:17<00:06,  4.24it/s] 95%|█████████▌| 95/100 [00:23<00:01,  4.15it/s] 73%|███████▎  | 73/100 [00:17<00:06,  4.24it/s] 96%|█████████▌| 96/100 [00:23<00:00,  4.15it/s] 74%|███████▍  | 74/100 [00:17<00:06,  4.24it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.14it/s] 75%|███████▌  | 75/100 [00:18<00:05,  4.23it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.13it/s] 76%|███████▌  | 76/100 [00:18<00:05,  4.23it/s] 99%|█████████▉| 99/100 [00:24<00:00,  4.14it/s] 77%|███████▋  | 77/100 [00:18<00:05,  4.24it/s]I0407 08:22:57.600495 37188 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:22:57.600585 37188 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:22:57.600627 37188 utils.py:162] NumExpr defaulting to 16 threads.
100%|██████████| 100/100 [00:24<00:00,  4.16it/s]100%|██████████| 100/100 [00:24<00:00,  4.10it/s]
 78%|███████▊  | 78/100 [00:18<00:05,  4.23it/s]I0407 08:22:57.928728 37188 config.py:54] PyTorch version 2.6.0 available.
 79%|███████▉  | 79/100 [00:18<00:04,  4.24it/s]W0407 08:22:58.128638 37188 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

 80%|████████  | 80/100 [00:19<00:04,  4.24it/s]Process Process-11:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
10_v proxy err 0.013661154545843601 err 3.4404101371765137 tr(WHW.T) 251.83889770507812
bpp_loss 3.38456392288208
 81%|████████  | 81/100 [00:19<00:04,  4.22it/s] 82%|████████▏ | 82/100 [00:19<00:04,  4.19it/s]W0407 08:22:58.720682 37188 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:22:58.724611 22419 quantize_finetune_llama.py:219] layer 13 gpu 1
I0407 08:22:58.738019 37188 data_utils.py:336] using 256 training seqs, 128 validation seqs
 83%|████████▎ | 83/100 [00:19<00:04,  4.19it/s] 84%|████████▍ | 84/100 [00:20<00:03,  4.20it/s] 85%|████████▌ | 85/100 [00:20<00:03,  4.20it/s]  0%|          | 0/100 [00:00<?, ?it/s] 86%|████████▌ | 86/100 [00:20<00:03,  4.22it/s] 87%|████████▋ | 87/100 [00:20<00:03,  4.22it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-13:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
 88%|████████▊ | 88/100 [00:21<00:02,  4.22it/s] 89%|████████▉ | 89/100 [00:21<00:02,  4.22it/s]I0407 08:23:00.388594 22419 quantize_finetune_llama.py:250] computed original embedding for layer 13 in 0.9587218761444092s
 90%|█████████ | 90/100 [00:21<00:02,  4.19it/s] 91%|█████████ | 91/100 [00:21<00:02,  4.20it/s] 92%|█████████▏| 92/100 [00:22<00:01,  4.20it/s] 93%|█████████▎| 93/100 [00:22<00:01,  4.21it/s] 94%|█████████▍| 94/100 [00:22<00:01,  4.22it/s] 95%|█████████▌| 95/100 [00:22<00:01,  4.21it/s] 96%|█████████▌| 96/100 [00:23<00:00,  4.23it/s] 97%|█████████▋| 97/100 [00:23<00:00,  4.23it/s] 98%|█████████▊| 98/100 [00:23<00:00,  4.21it/s] 99%|█████████▉| 99/100 [00:23<00:00,  4.16it/s]100%|██████████| 100/100 [00:23<00:00,  4.18it/s]100%|██████████| 100/100 [00:23<00:00,  4.17it/s]
Process Process-12:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 149, in compress_finetune_decoder_layer
    torch.save(
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 943, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 810, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 781, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name, _compute_crc32))
RuntimeError: Parent directory ../hf_model_comp/comp_qtip/ckpt/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100 does not exist.
11_v proxy err 0.010862651281058788 err 3.4713680744171143 tr(WHW.T) 319.5691223144531
bpp_loss 3.3941667079925537
I0407 08:23:03.925413 37325 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:23:03.925512 37325 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:23:03.925556 37325 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:23:04.256826 37325 config.py:54] PyTorch version 2.6.0 available.
W0407 08:23:04.451638 37325 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:23:05.031271 37325 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:23:05.035038 22419 quantize_finetune_llama.py:219] layer 14 gpu 2
I0407 08:23:05.048310 37325 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-14:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:23:06.713532 22419 quantize_finetune_llama.py:250] computed original embedding for layer 14 in 1.2820796966552734s
I0407 08:23:10.389011 37538 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:23:10.389117 37538 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:23:10.389161 37538 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:23:10.777419 37538 config.py:54] PyTorch version 2.6.0 available.
W0407 08:23:10.996140 37538 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:23:11.645245 37538 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:23:11.649599 22419 quantize_finetune_llama.py:219] layer 15 gpu 3
I0407 08:23:11.676445 37538 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-15:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:23:13.749575 22419 quantize_finetune_llama.py:250] computed original embedding for layer 15 in 1.6238617897033691s
I0407 08:23:17.287729 37740 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:23:17.287842 37740 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:23:17.287883 37740 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:23:17.610838 37740 config.py:54] PyTorch version 2.6.0 available.
W0407 08:23:17.798741 37740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:23:18.678431 37740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:23:18.682645 22419 quantize_finetune_llama.py:219] layer 16 gpu 0
I0407 08:23:18.704422 37740 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-16:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:23:20.106982 22419 quantize_finetune_llama.py:250] computed original embedding for layer 16 in 1.0078954696655273s
I0407 08:23:23.794294 37929 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:23:23.794391 37929 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:23:23.794431 37929 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:23:24.120304 37929 config.py:54] PyTorch version 2.6.0 available.
W0407 08:23:24.326305 37929 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:23:25.165595 37929 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:23:25.169176 22419 quantize_finetune_llama.py:219] layer 17 gpu 1
I0407 08:23:25.181791 37929 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-17:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:23:26.695732 22419 quantize_finetune_llama.py:250] computed original embedding for layer 17 in 1.0929715633392334s
I0407 08:23:30.353518 38081 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:23:30.353617 38081 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:23:30.353661 38081 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:23:30.680899 38081 config.py:54] PyTorch version 2.6.0 available.
W0407 08:23:30.878275 38081 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:23:31.528988 38081 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:23:31.533097 22419 quantize_finetune_llama.py:219] layer 18 gpu 2
I0407 08:23:31.547520 38081 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-18:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:23:33.057630 22419 quantize_finetune_llama.py:250] computed original embedding for layer 18 in 1.08085036277771s
I0407 08:23:37.219048 38215 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:23:37.219146 38215 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:23:37.219187 38215 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:23:37.640980 38215 config.py:54] PyTorch version 2.6.0 available.
W0407 08:23:37.903150 38215 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:23:38.902792 38215 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:23:38.907141 22419 quantize_finetune_llama.py:219] layer 19 gpu 3
I0407 08:23:38.980232 38215 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-19:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:23:41.338822 22419 quantize_finetune_llama.py:250] computed original embedding for layer 19 in 1.6887476444244385s
I0407 08:23:44.848418 38386 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:23:44.848501 38386 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:23:44.848539 38386 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:23:45.172620 38386 config.py:54] PyTorch version 2.6.0 available.
W0407 08:23:45.357954 38386 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:23:45.915339 38386 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:23:45.919068 22419 quantize_finetune_llama.py:219] layer 20 gpu 0
I0407 08:23:45.932322 38386 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-20:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:23:47.455153 22419 quantize_finetune_llama.py:250] computed original embedding for layer 20 in 1.1194896697998047s
I0407 08:23:51.456081 38548 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:23:51.456181 38548 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:23:51.456223 38548 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:23:51.875114 38548 config.py:54] PyTorch version 2.6.0 available.
W0407 08:23:52.088472 38548 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:23:53.306259 38548 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:23:53.310245 22419 quantize_finetune_llama.py:219] layer 21 gpu 1
I0407 08:23:53.344259 38548 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-21:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:23:54.752925 22419 quantize_finetune_llama.py:250] computed original embedding for layer 21 in 1.0219922065734863s
I0407 08:23:58.324015 38710 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:23:58.324112 38710 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:23:58.324153 38710 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:23:58.660928 38710 config.py:54] PyTorch version 2.6.0 available.
W0407 08:23:58.849794 38710 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:23:59.726395 38710 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:23:59.730377 22419 quantize_finetune_llama.py:219] layer 22 gpu 2
I0407 08:23:59.743451 38710 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-22:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:01.265266 22419 quantize_finetune_llama.py:250] computed original embedding for layer 22 in 1.130134105682373s
I0407 08:24:04.875377 38943 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:24:04.875469 38943 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:24:04.875510 38943 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:24:05.209352 38943 config.py:54] PyTorch version 2.6.0 available.
W0407 08:24:05.400906 38943 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:24:06.241119 38943 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:24:06.245299 22419 quantize_finetune_llama.py:219] layer 23 gpu 3
I0407 08:24:06.259371 38943 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-23:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:07.488446 22419 quantize_finetune_llama.py:250] computed original embedding for layer 23 in 0.827059268951416s
I0407 08:24:11.053143 39137 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:24:11.053228 39137 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:24:11.053269 39137 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:24:11.374689 39137 config.py:54] PyTorch version 2.6.0 available.
W0407 08:24:11.561896 39137 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:24:12.173277 39137 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:24:12.177175 22419 quantize_finetune_llama.py:219] layer 24 gpu 0
I0407 08:24:12.189918 39137 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-24:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:13.538746 22419 quantize_finetune_llama.py:250] computed original embedding for layer 24 in 0.950514554977417s
I0407 08:24:17.075717 39306 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:24:17.075825 39306 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:24:17.075870 39306 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:24:17.408955 39306 config.py:54] PyTorch version 2.6.0 available.
W0407 08:24:17.600207 39306 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:24:18.192875 39306 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:24:18.196727 22419 quantize_finetune_llama.py:219] layer 25 gpu 1
I0407 08:24:18.209788 39306 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-25:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:19.618503 22419 quantize_finetune_llama.py:250] computed original embedding for layer 25 in 0.9900939464569092s
I0407 08:24:23.178517 39526 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:24:23.178618 39526 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:24:23.178660 39526 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:24:23.512583 39526 config.py:54] PyTorch version 2.6.0 available.
W0407 08:24:23.700167 39526 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:24:24.534345 39526 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:24:24.538017 22419 quantize_finetune_llama.py:219] layer 26 gpu 2
I0407 08:24:24.551108 39526 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-26:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:26.131475 22419 quantize_finetune_llama.py:250] computed original embedding for layer 26 in 1.147925615310669s
I0407 08:24:29.721639 39763 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:24:29.721726 39763 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:24:29.721770 39763 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:24:30.046523 39763 config.py:54] PyTorch version 2.6.0 available.
W0407 08:24:30.235117 39763 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:24:31.119324 39763 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:24:31.123127 22419 quantize_finetune_llama.py:219] layer 27 gpu 3
I0407 08:24:31.136845 39763 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-27:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:32.550855 22419 quantize_finetune_llama.py:250] computed original embedding for layer 27 in 0.993776798248291s
I0407 08:24:36.067152 39981 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:24:36.067238 39981 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:24:36.067275 39981 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:24:36.402413 39981 config.py:54] PyTorch version 2.6.0 available.
W0407 08:24:36.595188 39981 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:24:37.375488 39981 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:24:37.379105 22419 quantize_finetune_llama.py:219] layer 28 gpu 0
I0407 08:24:37.391841 39981 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-28:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:38.680958 22419 quantize_finetune_llama.py:250] computed original embedding for layer 28 in 0.8725841045379639s
I0407 08:24:42.185303 40142 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:24:42.185393 40142 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:24:42.185432 40142 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:24:42.520097 40142 config.py:54] PyTorch version 2.6.0 available.
W0407 08:24:42.720807 40142 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:24:43.862740 40142 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:24:43.866417 22419 quantize_finetune_llama.py:219] layer 29 gpu 1
I0407 08:24:43.879342 40142 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-29:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:45.244745 22419 quantize_finetune_llama.py:250] computed original embedding for layer 29 in 0.9654033184051514s
I0407 08:24:49.083091 40360 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:24:49.083209 40360 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:24:49.083250 40360 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:24:49.470512 40360 config.py:54] PyTorch version 2.6.0 available.
W0407 08:24:49.821101 40360 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:24:50.732911 40360 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:24:50.736568 22419 quantize_finetune_llama.py:219] layer 30 gpu 2
I0407 08:24:50.750176 40360 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-30:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:52.832839 22419 quantize_finetune_llama.py:250] computed original embedding for layer 30 in 1.6546339988708496s
I0407 08:24:56.264528 40577 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:24:56.264618 40577 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:24:56.264660 40577 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:24:56.610448 40577 config.py:54] PyTorch version 2.6.0 available.
W0407 08:24:56.799851 40577 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:24:57.422876 40577 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:24:57.426486 22419 quantize_finetune_llama.py:219] layer 31 gpu 3
I0407 08:24:57.440057 40577 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-31:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:24:59.422211 22419 quantize_finetune_llama.py:250] computed original embedding for layer 31 in 1.569209098815918s
I0407 08:25:02.983253 40771 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:25:02.983341 40771 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:25:02.983382 40771 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:25:03.325638 40771 config.py:54] PyTorch version 2.6.0 available.
W0407 08:25:03.516410 40771 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0407 08:25:04.222351 40771 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0407 08:25:04.239270 40771 data_utils.py:336] using 256 training seqs, 128 validation seqs
  0%|          | 0/100 [00:00<?, ?it/s]  0%|          | 0/100 [00:00<?, ?it/s]
Process Process-32:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/quantize_finetune_llama.py", line 125, in compress_llama_decoder
    finetune.compress_finetune_decoder_layer(layer, quant_order, idx, comp_model, ql_i, args,
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py", line 130, in compress_finetune_decoder_layer
    W_hat, bpp_loss_sum, num_pixels, SU, SV, scaleWH, ft_result = nwc.compress_linear(W.clone(), HR, comp_model, ql, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 231, in compress_linear
    comp_model_ft, ft_result, pe = fine_tune_comp_model(comp_model_ft, W, H, ql, lstats, args, device)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 132, in fine_tune_comp_model
    loss = loss_fn(batch.to(device), out, ori_shape)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/workspace/Weight_compression/comp_lm_qtip/lib/algo/nwc.py", line 42, in forward
    if isinstance(out["likelihoods"], dict):
KeyError: 'likelihoods'
I0407 08:25:16.253276 41009 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0407 08:25:16.253420 41009 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0407 08:25:16.253465 41009 utils.py:162] NumExpr defaulting to 16 threads.
I0407 08:25:16.601057 41009 config.py:54] PyTorch version 2.6.0 available.
W0407 08:25:16.817348 41009 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 194, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 24, in main
    assert os.path.exists(args.quantized_path)
AssertionError
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
  File "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../hf_model_comp/comp_qtip/hf/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 124, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 71, in main
    model, model_str = model_from_hf_path(
  File "/workspace/Weight_compression/comp_lm_qtip/eval_ppl.py", line 34, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
  File "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1021, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py", line 590, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py", line 649, in _get_config_dict
    resolved_config_file = cached_file(
  File "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: '../hf_model_comp/comp_qtip/hf/ft_comp/meta-llama--Meta-Llama-3-8B/lmbda100'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
