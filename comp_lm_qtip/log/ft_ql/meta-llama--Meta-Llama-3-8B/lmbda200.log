I0326 06:08:02.392101 1418472 config.py:54] PyTorch version 2.6.0 available.
W0326 06:08:02.673579 1418472 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 06:08:03.561103 1418472 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.63it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.05it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.52it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.83it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.02it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.14it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.32it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.07it/s]
I0326 06:08:05.001626 1418472 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:23,  1.34it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:18,  1.64it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.78it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.85it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.87it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.89it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.91it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.92it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:11,  1.93it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.93it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:10,  1.92it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.93it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:09,  1.93it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.93it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:08,  1.94it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.95it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:08<00:07,  1.96it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.98it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:09<00:06,  1.97it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.96it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.96it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.97it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.97it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.97it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.95it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.94it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.95it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.95it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.96it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.98it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.96it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.95it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.93it/s]
I0326 06:08:27.338209 1418472 quantize_finetune_llama.py:185] loaded compression model
I0326 06:08:46.019263 1418472 quantize_finetune_llama.py:189] loaded dataset and devset
I0326 06:08:48.998406 1418472 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 06:09:42.539944 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 53.39955544471741s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0326 06:10:00.113847 1419557 config.py:54] PyTorch version 2.6.0 available.
W0326 06:10:00.396051 1419557 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 06:10:01.322139 1419557 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 06:10:01.326233 1418472 quantize_finetune_llama.py:209] layer 1 gpu 1
I0326 06:10:01.339794 1419557 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 06:10:08.592005 1419557 finetune.py:45] layer 0_v initial loss 1.5955945400492055e-06
W0326 06:10:08.592195 1419557 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 06:10:44.316621 1419557 finetune.py:68] layer 0_v @ epoch 0 new loss 1.036292360367952e-06 old loss 1.5955945400492055e-06 BETTER
I0326 06:10:55.877101 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 54.37011957168579s
I0326 06:11:07.611979 1420159 config.py:54] PyTorch version 2.6.0 available.
W0326 06:11:07.884718 1420159 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 06:11:08.761534 1420159 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 06:11:08.765558 1418472 quantize_finetune_llama.py:209] layer 2 gpu 0
I0326 06:11:08.778540 1420159 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 06:11:15.613543 1420159 finetune.py:45] layer 1_v initial loss 3.4561303436930757e-06
W0326 06:11:15.613911 1420159 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 06:11:21.044637 1419557 finetune.py:68] layer 0_v @ epoch 1 new loss 8.932905757319531e-07 old loss 1.036292360367952e-06 BETTER
I0326 06:11:48.667341 1420159 finetune.py:68] layer 1_v @ epoch 0 new loss 1.743710640766949e-06 old loss 3.4561303436930757e-06 BETTER
I0326 06:11:57.819420 1419557 finetune.py:68] layer 0_v @ epoch 2 new loss 8.302000082949235e-07 old loss 8.932905757319531e-07 BETTER
I0326 06:12:22.863293 1420159 finetune.py:68] layer 1_v @ epoch 1 new loss 1.321550826105522e-06 old loss 1.743710640766949e-06 BETTER
I0326 06:12:34.711211 1419557 finetune.py:68] layer 0_v @ epoch 3 new loss 7.93802371390484e-07 old loss 8.302000082949235e-07 BETTER
I0326 06:12:57.375471 1420159 finetune.py:68] layer 1_v @ epoch 2 new loss 1.1738401326510939e-06 old loss 1.321550826105522e-06 BETTER
I0326 06:13:11.798000 1419557 finetune.py:68] layer 0_v @ epoch 4 new loss 7.691926953157235e-07 old loss 7.93802371390484e-07 BETTER
I0326 06:13:21.471139 1419557 finetune.py:45] layer 0_q initial loss 7.729444746473746e-07
I0326 06:13:32.066297 1420159 finetune.py:68] layer 1_v @ epoch 3 new loss 1.099366841117444e-06 old loss 1.1738401326510939e-06 BETTER
I0326 06:13:57.438769 1419557 finetune.py:68] layer 0_q @ epoch 0 new loss 7.510560067203187e-07 old loss 7.729444746473746e-07 BETTER
I0326 06:14:06.863118 1420159 finetune.py:68] layer 1_v @ epoch 4 new loss 1.0518167528061895e-06 old loss 1.099366841117444e-06 BETTER
I0326 06:14:16.278695 1420159 finetune.py:45] layer 1_q initial loss 1.095494553737808e-06
I0326 06:14:34.165210 1419557 finetune.py:68] layer 0_q @ epoch 1 new loss 7.345618087128969e-07 old loss 7.510560067203187e-07 BETTER
I0326 06:14:50.095554 1420159 finetune.py:68] layer 1_q @ epoch 0 new loss 1.0370024483563611e-06 old loss 1.095494553737808e-06 BETTER
I0326 06:15:11.043970 1419557 finetune.py:68] layer 0_q @ epoch 2 new loss 7.209252430584456e-07 old loss 7.345618087128969e-07 BETTER
I0326 06:15:24.599751 1420159 finetune.py:68] layer 1_q @ epoch 1 new loss 1.0009086963691516e-06 old loss 1.0370024483563611e-06 BETTER
I0326 06:15:47.814024 1419557 finetune.py:68] layer 0_q @ epoch 3 new loss 7.091593943187036e-07 old loss 7.209252430584456e-07 BETTER
I0326 06:15:59.134690 1420159 finetune.py:68] layer 1_q @ epoch 2 new loss 9.728987606649753e-07 old loss 1.0009086963691516e-06 BETTER
I0326 06:16:24.544396 1419557 finetune.py:68] layer 0_q @ epoch 4 new loss 6.987125971136265e-07 old loss 7.091593943187036e-07 BETTER
I0326 06:16:32.373325 1419557 finetune.py:45] layer 0_k initial loss 7.11339168901759e-07
I0326 06:16:33.731987 1420159 finetune.py:68] layer 1_q @ epoch 3 new loss 9.492798653809587e-07 old loss 9.728987606649753e-07 BETTER
I0326 06:17:08.229455 1420159 finetune.py:68] layer 1_q @ epoch 4 new loss 9.299774887949752e-07 old loss 9.492798653809587e-07 BETTER
I0326 06:17:08.285286 1419557 finetune.py:68] layer 0_k @ epoch 0 new loss 6.986348921600438e-07 old loss 7.11339168901759e-07 BETTER
I0326 06:17:15.824321 1420159 finetune.py:45] layer 1_k initial loss 9.516580234958383e-07
I0326 06:17:44.874483 1419557 finetune.py:68] layer 0_k @ epoch 1 new loss 6.896759146002296e-07 old loss 6.986348921600438e-07 BETTER
I0326 06:17:49.548901 1420159 finetune.py:68] layer 1_k @ epoch 0 new loss 9.30062128645659e-07 old loss 9.516580234958383e-07 BETTER
I0326 06:18:21.553443 1419557 finetune.py:68] layer 0_k @ epoch 2 new loss 6.817452344876074e-07 old loss 6.896759146002296e-07 BETTER
I0326 06:18:23.914322 1420159 finetune.py:68] layer 1_k @ epoch 1 new loss 9.132330092143093e-07 old loss 9.30062128645659e-07 BETTER
I0326 06:18:58.308528 1420159 finetune.py:68] layer 1_k @ epoch 2 new loss 8.983060979517177e-07 old loss 9.132330092143093e-07 BETTER
I0326 06:18:58.366075 1419557 finetune.py:68] layer 0_k @ epoch 3 new loss 6.74563182201382e-07 old loss 6.817452344876074e-07 BETTER
I0326 06:19:32.819559 1420159 finetune.py:68] layer 1_k @ epoch 3 new loss 8.847059120853373e-07 old loss 8.983060979517177e-07 BETTER
I0326 06:19:35.004479 1419557 finetune.py:68] layer 0_k @ epoch 4 new loss 6.679156854261237e-07 old loss 6.74563182201382e-07 BETTER
I0326 06:19:44.687596 1419557 finetune.py:45] layer 0_o initial loss 1.1085801361332415e-06
I0326 06:20:07.330541 1420159 finetune.py:68] layer 1_k @ epoch 4 new loss 8.709444614396489e-07 old loss 8.847059120853373e-07 BETTER
I0326 06:20:16.759555 1420159 finetune.py:45] layer 1_o initial loss 2.0518275505310157e-06
I0326 06:20:19.960075 1419557 finetune.py:68] layer 0_o @ epoch 0 new loss 1.0959157634715666e-06 old loss 1.1085801361332415e-06 BETTER
I0326 06:20:50.008580 1420159 finetune.py:68] layer 1_o @ epoch 0 new loss 2.015175141423242e-06 old loss 2.0518275505310157e-06 BETTER
I0326 06:20:56.254548 1419557 finetune.py:68] layer 0_o @ epoch 1 new loss 1.0855842447199393e-06 old loss 1.0959157634715666e-06 BETTER
I0326 06:21:23.905959 1420159 finetune.py:68] layer 1_o @ epoch 1 new loss 1.9876822534570238e-06 old loss 2.015175141423242e-06 BETTER
I0326 06:21:32.432132 1419557 finetune.py:68] layer 0_o @ epoch 2 new loss 1.076798412213975e-06 old loss 1.0855842447199393e-06 BETTER
I0326 06:21:57.756034 1420159 finetune.py:68] layer 1_o @ epoch 2 new loss 1.9658932615129743e-06 old loss 1.9876822534570238e-06 BETTER
I0326 06:22:08.540644 1419557 finetune.py:68] layer 0_o @ epoch 3 new loss 1.0691358056647005e-06 old loss 1.076798412213975e-06 BETTER
I0326 06:22:31.488692 1420159 finetune.py:68] layer 1_o @ epoch 3 new loss 1.9480930859572254e-06 old loss 1.9658932615129743e-06 BETTER
I0326 06:22:44.823095 1419557 finetune.py:68] layer 0_o @ epoch 4 new loss 1.0625138884279295e-06 old loss 1.0691358056647005e-06 BETTER
I0326 06:23:05.590922 1420159 finetune.py:68] layer 1_o @ epoch 4 new loss 1.933141675181105e-06 old loss 1.9480930859572254e-06 BETTER
I0326 06:23:06.422719 1419557 finetune.py:45] layer 0_up initial loss 1.3953940651845187e-06
I0326 06:23:27.038247 1420159 finetune.py:45] layer 1_up initial loss 2.8208467028889572e-06
I0326 06:23:38.637943 1419557 finetune.py:68] layer 0_up @ epoch 0 new loss 1.3866479093849193e-06 old loss 1.3953940651845187e-06 BETTER
I0326 06:23:57.765617 1420159 finetune.py:68] layer 1_up @ epoch 0 new loss 2.680748366401531e-06 old loss 2.8208467028889572e-06 BETTER
I0326 06:24:11.854614 1419557 finetune.py:68] layer 0_up @ epoch 1 new loss 1.380279741169943e-06 old loss 1.3866479093849193e-06 BETTER
I0326 06:24:29.330097 1420159 finetune.py:68] layer 1_up @ epoch 1 new loss 2.6653888198779896e-06 old loss 2.680748366401531e-06 BETTER
I0326 06:24:45.478894 1419557 finetune.py:68] layer 0_up @ epoch 2 new loss 1.3750948255619733e-06 old loss 1.380279741169943e-06 BETTER
I0326 06:25:01.343878 1420159 finetune.py:68] layer 1_up @ epoch 2 new loss 2.655398020579014e-06 old loss 2.6653888198779896e-06 BETTER
I0326 06:25:18.905431 1419557 finetune.py:68] layer 0_up @ epoch 3 new loss 1.3706438721783343e-06 old loss 1.3750948255619733e-06 BETTER
I0326 06:25:33.463969 1420159 finetune.py:68] layer 1_up @ epoch 3 new loss 2.646369921421865e-06 old loss 2.655398020579014e-06 BETTER
I0326 06:25:52.575334 1419557 finetune.py:68] layer 0_up @ epoch 4 new loss 1.3666253835253883e-06 old loss 1.3706438721783343e-06 BETTER
I0326 06:26:05.602172 1420159 finetune.py:68] layer 1_up @ epoch 4 new loss 2.6384927878098097e-06 old loss 2.646369921421865e-06 BETTER
I0326 06:26:14.343617 1419557 finetune.py:45] layer 0_gate initial loss 1.582087975293689e-06
I0326 06:26:27.202244 1420159 finetune.py:45] layer 1_gate initial loss 3.2083801215776475e-06
I0326 06:26:44.637009 1419557 finetune.py:68] layer 0_gate @ epoch 0 new loss 1.575517103447055e-06 old loss 1.582087975293689e-06 BETTER
I0326 06:26:55.864068 1420159 finetune.py:68] layer 1_gate @ epoch 0 new loss 3.1462946026294958e-06 old loss 3.2083801215776475e-06 BETTER
I0326 06:27:16.120344 1419557 finetune.py:68] layer 0_gate @ epoch 1 new loss 1.5704966926932684e-06 old loss 1.575517103447055e-06 BETTER
I0326 06:27:25.398813 1420159 finetune.py:68] layer 1_gate @ epoch 1 new loss 3.1387048693432007e-06 old loss 3.1462946026294958e-06 BETTER
I0326 06:27:47.780504 1419557 finetune.py:68] layer 0_gate @ epoch 2 new loss 1.5664247712265933e-06 old loss 1.5704966926932684e-06 BETTER
I0326 06:27:55.554208 1420159 finetune.py:68] layer 1_gate @ epoch 2 new loss 3.132497795377276e-06 old loss 3.1387048693432007e-06 BETTER
I0326 06:28:19.845933 1419557 finetune.py:68] layer 0_gate @ epoch 3 new loss 1.5630348570994101e-06 old loss 1.5664247712265933e-06 BETTER
I0326 06:28:25.288934 1420159 finetune.py:68] layer 1_gate @ epoch 3 new loss 3.1268509701476432e-06 old loss 3.132497795377276e-06 BETTER
I0326 06:28:51.482714 1419557 finetune.py:68] layer 0_gate @ epoch 4 new loss 1.560083546792157e-06 old loss 1.5630348570994101e-06 BETTER
I0326 06:28:55.102073 1420159 finetune.py:68] layer 1_gate @ epoch 4 new loss 3.1215954550134484e-06 old loss 3.1268509701476432e-06 BETTER
I0326 06:29:14.392093 1419557 finetune.py:45] layer 0_down initial loss 2.315428673682618e-06
I0326 06:29:17.616247 1420159 finetune.py:45] layer 1_down initial loss 1.1374769201211166e-05
I0326 06:29:42.359396 1419557 finetune.py:68] layer 0_down @ epoch 0 new loss 2.3140175926528173e-06 old loss 2.315428673682618e-06 BETTER
I0326 06:29:44.611083 1420159 finetune.py:68] layer 1_down @ epoch 0 new loss 1.1324817023705691e-05 old loss 1.1374769201211166e-05 BETTER
I0326 06:30:11.415529 1419557 finetune.py:68] layer 0_down @ epoch 1 new loss 2.3131158286560094e-06 old loss 2.3140175926528173e-06 BETTER
I0326 06:30:12.279132 1420159 finetune.py:68] layer 1_down @ epoch 1 new loss 1.1309485671517905e-05 old loss 1.1324817023705691e-05 BETTER
I0326 06:30:39.937851 1420159 finetune.py:68] layer 1_down @ epoch 2 new loss 1.1308940884191543e-05 old loss 1.1309485671517905e-05 BETTER
I0326 06:30:40.627402 1419557 finetune.py:68] layer 0_down @ epoch 2 new loss 2.312451215402689e-06 old loss 2.3131158286560094e-06 BETTER
I0326 06:31:07.637771 1420159 finetune.py:68] layer 1_down @ epoch 3 new loss 1.1308500688755885e-05 old loss 1.1308940884191543e-05 BETTER
I0326 06:31:10.124621 1419557 finetune.py:68] layer 0_down @ epoch 3 new loss 2.3118486751627643e-06 old loss 2.312451215402689e-06 BETTER
I0326 06:31:35.402926 1420159 finetune.py:68] layer 1_down @ epoch 4 new loss 1.1307511158520356e-05 old loss 1.1308500688755885e-05 BETTER
1_v proxy err 0.024711724370718002 tr(WHW.T) 109.07096099853516
bpp_loss 3.1700652837753296
1_q proxy err 0.00011817130143754184 tr(WHW.T) 144819.125
bpp_loss 4.1716179847717285
1_k proxy err 7.989264850039035e-05 tr(WHW.T) 75522.546875
bpp_loss 4.901546955108643
1_o proxy err 0.009590848349034786 tr(WHW.T) 1989.222412109375
bpp_loss 3.2411099672317505
1_up proxy err 0.01010889746248722 tr(WHW.T) 8231.25
bpp_loss 3.465435028076172
1_gate proxy err 0.006030398420989513 tr(WHW.T) 13948.4658203125
bpp_loss 3.574693271092006
1_down proxy err 0.0009993364801630378 tr(WHW.T) 14001.3701171875
bpp_loss 3.460106134414673
I0326 06:31:39.546600 1419557 finetune.py:68] layer 0_down @ epoch 4 new loss 2.3113434508559294e-06 old loss 2.3118486751627643e-06 BETTER
0_v proxy err 0.0404244139790535 tr(WHW.T) 60.88684844970703
bpp_loss 3.0664148330688477
0_q proxy err 0.00012371236516628414 tr(WHW.T) 288071.375
bpp_loss 3.9163628816604614
0_k proxy err 0.0001290869404328987 tr(WHW.T) 100142.34375
bpp_loss 4.4970715045928955
0_o proxy err 0.00480948481708765 tr(WHW.T) 3148.996337890625
bpp_loss 3.1619110107421875
0_up proxy err 0.009213156066834927 tr(WHW.T) 8921.8056640625
bpp_loss 3.450939723423549
0_gate proxy err 0.00527568394318223 tr(WHW.T) 15776.587890625
bpp_loss 3.5634662083217075
0_down proxy err 0.006527810823172331 tr(WHW.T) 10842.3427734375
bpp_loss 3.4444305215563094
I0326 06:32:49.894495 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 64.62336683273315s
I0326 06:32:53.140459 1430829 config.py:54] PyTorch version 2.6.0 available.
W0326 06:32:53.421659 1430829 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 06:32:54.298417 1430829 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 06:32:54.302251 1418472 quantize_finetune_llama.py:209] layer 3 gpu 1
I0326 06:32:54.314563 1430829 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 06:33:01.586698 1430829 finetune.py:45] layer 2_v initial loss 6.901613232912496e-06
W0326 06:33:01.586897 1430829 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 06:33:37.770742 1430829 finetune.py:68] layer 2_v @ epoch 0 new loss 2.4346170448552584e-06 old loss 6.901613232912496e-06 BETTER
I0326 06:33:55.785675 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 61.065021276474s
I0326 06:33:59.137384 1431411 config.py:54] PyTorch version 2.6.0 available.
W0326 06:33:59.427395 1431411 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 06:34:00.316891 1431411 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 06:34:00.321159 1418472 quantize_finetune_llama.py:209] layer 4 gpu 0
I0326 06:34:00.334641 1431411 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 06:34:07.092459 1431411 finetune.py:45] layer 3_v initial loss 8.160947800206486e-06
W0326 06:34:07.092672 1431411 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 06:34:14.930150 1430829 finetune.py:68] layer 2_v @ epoch 1 new loss 1.7518184449727414e-06 old loss 2.4346170448552584e-06 BETTER
I0326 06:34:40.612260 1431411 finetune.py:68] layer 3_v @ epoch 0 new loss 3.0671478725707857e-06 old loss 8.160947800206486e-06 BETTER
I0326 06:34:52.035366 1430829 finetune.py:68] layer 2_v @ epoch 2 new loss 1.5686825918237446e-06 old loss 1.7518184449727414e-06 BETTER
I0326 06:35:15.106866 1431411 finetune.py:68] layer 3_v @ epoch 1 new loss 2.331898713237024e-06 old loss 3.0671478725707857e-06 BETTER
I0326 06:35:29.278394 1430829 finetune.py:68] layer 2_v @ epoch 3 new loss 1.4838373090242385e-06 old loss 1.5686825918237446e-06 BETTER
I0326 06:35:49.914650 1431411 finetune.py:68] layer 3_v @ epoch 2 new loss 2.09179415833205e-06 old loss 2.331898713237024e-06 BETTER
I0326 06:36:06.465841 1430829 finetune.py:68] layer 2_v @ epoch 4 new loss 1.4289928458310897e-06 old loss 1.4838373090242385e-06 BETTER
I0326 06:36:16.180071 1430829 finetune.py:45] layer 2_q initial loss 1.6399758351326454e-06
I0326 06:36:24.779184 1431411 finetune.py:68] layer 3_v @ epoch 3 new loss 1.967638809219352e-06 old loss 2.09179415833205e-06 BETTER
I0326 06:36:52.278811 1430829 finetune.py:68] layer 2_q @ epoch 0 new loss 1.5670018456148682e-06 old loss 1.6399758351326454e-06 BETTER
I0326 06:36:59.821386 1431411 finetune.py:68] layer 3_v @ epoch 4 new loss 1.8882003587350482e-06 old loss 1.967638809219352e-06 BETTER
I0326 06:37:09.293695 1431411 finetune.py:45] layer 3_q initial loss 2.2943759176996537e-06
I0326 06:37:29.210592 1430829 finetune.py:68] layer 2_q @ epoch 1 new loss 1.5245761915139155e-06 old loss 1.5670018456148682e-06 BETTER
I0326 06:37:43.137519 1431411 finetune.py:68] layer 3_q @ epoch 0 new loss 2.1499922695511486e-06 old loss 2.2943759176996537e-06 BETTER
I0326 06:38:06.043719 1430829 finetune.py:68] layer 2_q @ epoch 2 new loss 1.4920119610906113e-06 old loss 1.5245761915139155e-06 BETTER
I0326 06:38:17.833482 1431411 finetune.py:68] layer 3_q @ epoch 1 new loss 2.0908773876726627e-06 old loss 2.1499922695511486e-06 BETTER
I0326 06:38:42.958952 1430829 finetune.py:68] layer 2_q @ epoch 3 new loss 1.465581249249226e-06 old loss 1.4920119610906113e-06 BETTER
I0326 06:38:52.448485 1431411 finetune.py:68] layer 3_q @ epoch 2 new loss 2.047516090897261e-06 old loss 2.0908773876726627e-06 BETTER
I0326 06:39:19.837910 1430829 finetune.py:68] layer 2_q @ epoch 4 new loss 1.4434112927119713e-06 old loss 1.465581249249226e-06 BETTER
I0326 06:39:27.220907 1431411 finetune.py:68] layer 3_q @ epoch 3 new loss 2.011098331422545e-06 old loss 2.047516090897261e-06 BETTER
I0326 06:39:27.732263 1430829 finetune.py:45] layer 2_k initial loss 1.5184969015535899e-06
I0326 06:40:01.985071 1431411 finetune.py:68] layer 3_q @ epoch 4 new loss 1.9824260562018026e-06 old loss 2.011098331422545e-06 BETTER
I0326 06:40:03.868522 1430829 finetune.py:68] layer 2_k @ epoch 0 new loss 1.4944171198294498e-06 old loss 1.5184969015535899e-06 BETTER
I0326 06:40:09.750307 1431411 finetune.py:45] layer 3_k initial loss 2.1377306893555215e-06
I0326 06:40:40.574135 1430829 finetune.py:68] layer 2_k @ epoch 1 new loss 1.4771464975638082e-06 old loss 1.4944171198294498e-06 BETTER
I0326 06:40:43.589157 1431411 finetune.py:68] layer 3_k @ epoch 0 new loss 2.0951110855094157e-06 old loss 2.1377306893555215e-06 BETTER
I0326 06:41:17.328830 1430829 finetune.py:68] layer 2_k @ epoch 2 new loss 1.4619877219956834e-06 old loss 1.4771464975638082e-06 BETTER
I0326 06:41:17.914954 1431411 finetune.py:68] layer 3_k @ epoch 1 new loss 2.069688434858108e-06 old loss 2.0951110855094157e-06 BETTER
I0326 06:41:52.370694 1431411 finetune.py:68] layer 3_k @ epoch 2 new loss 2.0483466869336553e-06 old loss 2.069688434858108e-06 BETTER
I0326 06:41:54.036298 1430829 finetune.py:68] layer 2_k @ epoch 3 new loss 1.44845205340971e-06 old loss 1.4619877219956834e-06 BETTER
I0326 06:42:26.861117 1431411 finetune.py:68] layer 3_k @ epoch 3 new loss 2.0295749436627375e-06 old loss 2.0483466869336553e-06 BETTER
I0326 06:42:30.603763 1430829 finetune.py:68] layer 2_k @ epoch 4 new loss 1.435943318028876e-06 old loss 1.44845205340971e-06 BETTER
I0326 06:42:40.318165 1430829 finetune.py:45] layer 2_o initial loss 3.2027417091740062e-06
I0326 06:43:01.331393 1431411 finetune.py:68] layer 3_k @ epoch 4 new loss 2.012417326113791e-06 old loss 2.0295749436627375e-06 BETTER
I0326 06:43:10.718565 1431411 finetune.py:45] layer 3_o initial loss 5.318813236954156e-06
I0326 06:43:15.527270 1430829 finetune.py:68] layer 2_o @ epoch 0 new loss 3.105655650870176e-06 old loss 3.2027417091740062e-06 BETTER
I0326 06:43:43.821763 1431411 finetune.py:68] layer 3_o @ epoch 0 new loss 5.101684564579045e-06 old loss 5.318813236954156e-06 BETTER
I0326 06:43:51.430199 1430829 finetune.py:68] layer 2_o @ epoch 1 new loss 3.0471765057882294e-06 old loss 3.105655650870176e-06 BETTER
I0326 06:44:17.631995 1431411 finetune.py:68] layer 3_o @ epoch 1 new loss 5.0044031922880094e-06 old loss 5.101684564579045e-06 BETTER
I0326 06:44:27.533109 1430829 finetune.py:68] layer 2_o @ epoch 2 new loss 3.0077244446147233e-06 old loss 3.0471765057882294e-06 BETTER
I0326 06:44:51.467498 1431411 finetune.py:68] layer 3_o @ epoch 2 new loss 4.937465291732224e-06 old loss 5.0044031922880094e-06 BETTER
I0326 06:45:03.507729 1430829 finetune.py:68] layer 2_o @ epoch 3 new loss 2.978236125272815e-06 old loss 3.0077244446147233e-06 BETTER
I0326 06:45:25.464787 1431411 finetune.py:68] layer 3_o @ epoch 3 new loss 4.8829178922460414e-06 old loss 4.937465291732224e-06 BETTER
I0326 06:45:39.523601 1430829 finetune.py:68] layer 2_o @ epoch 4 new loss 2.954455794679234e-06 old loss 2.978236125272815e-06 BETTER
I0326 06:45:59.600438 1431411 finetune.py:68] layer 3_o @ epoch 4 new loss 4.835837444261415e-06 old loss 4.8829178922460414e-06 BETTER
I0326 06:46:01.132152 1430829 finetune.py:45] layer 2_up initial loss 4.737651579489466e-06
I0326 06:46:21.052387 1431411 finetune.py:45] layer 3_up initial loss 8.95269840839319e-06
I0326 06:46:33.882424 1430829 finetune.py:68] layer 2_up @ epoch 0 new loss 4.708142114395741e-06 old loss 4.737651579489466e-06 BETTER
I0326 06:46:51.792380 1431411 finetune.py:68] layer 3_up @ epoch 0 new loss 8.872063517628703e-06 old loss 8.95269840839319e-06 BETTER
I0326 06:47:07.140415 1430829 finetune.py:68] layer 2_up @ epoch 1 new loss 4.684981831815094e-06 old loss 4.708142114395741e-06 BETTER
I0326 06:47:23.895025 1431411 finetune.py:68] layer 3_up @ epoch 1 new loss 8.813969543552957e-06 old loss 8.872063517628703e-06 BETTER
I0326 06:47:40.732478 1430829 finetune.py:68] layer 2_up @ epoch 2 new loss 4.665031156037003e-06 old loss 4.684981831815094e-06 BETTER
I0326 06:47:56.063138 1431411 finetune.py:68] layer 3_up @ epoch 2 new loss 8.764458470977843e-06 old loss 8.813969543552957e-06 BETTER
I0326 06:48:14.510206 1430829 finetune.py:68] layer 2_up @ epoch 3 new loss 4.647395144274924e-06 old loss 4.665031156037003e-06 BETTER
I0326 06:48:28.199257 1431411 finetune.py:68] layer 3_up @ epoch 3 new loss 8.719347533769906e-06 old loss 8.764458470977843e-06 BETTER
I0326 06:48:48.351099 1430829 finetune.py:68] layer 2_up @ epoch 4 new loss 4.631158844858874e-06 old loss 4.647395144274924e-06 BETTER
I0326 06:49:00.342844 1431411 finetune.py:68] layer 3_up @ epoch 4 new loss 8.678096492076293e-06 old loss 8.719347533769906e-06 BETTER
I0326 06:49:10.000864 1430829 finetune.py:45] layer 2_gate initial loss 5.738705567637226e-06
I0326 06:49:21.921659 1431411 finetune.py:45] layer 3_gate initial loss 1.0600283530948218e-05
I0326 06:49:40.435749 1430829 finetune.py:68] layer 2_gate @ epoch 0 new loss 5.72020417166641e-06 old loss 5.738705567637226e-06 BETTER
I0326 06:49:50.833389 1431411 finetune.py:68] layer 3_gate @ epoch 0 new loss 1.055194570653839e-05 old loss 1.0600283530948218e-05 BETTER
I0326 06:50:11.792114 1430829 finetune.py:68] layer 2_gate @ epoch 1 new loss 5.704562681785319e-06 old loss 5.72020417166641e-06 BETTER
I0326 06:50:20.445093 1431411 finetune.py:68] layer 3_gate @ epoch 1 new loss 1.0513249435462058e-05 old loss 1.055194570653839e-05 BETTER
I0326 06:50:43.204952 1430829 finetune.py:68] layer 2_gate @ epoch 2 new loss 5.690582838724367e-06 old loss 5.704562681785319e-06 BETTER
I0326 06:50:50.239275 1431411 finetune.py:68] layer 3_gate @ epoch 2 new loss 1.0478027434146497e-05 old loss 1.0513249435462058e-05 BETTER
I0326 06:51:14.734328 1430829 finetune.py:68] layer 2_gate @ epoch 3 new loss 5.677864464814775e-06 old loss 5.690582838724367e-06 BETTER
I0326 06:51:20.084574 1431411 finetune.py:68] layer 3_gate @ epoch 3 new loss 1.0445789484947454e-05 old loss 1.0478027434146497e-05 BETTER
I0326 06:51:46.355797 1430829 finetune.py:68] layer 2_gate @ epoch 4 new loss 5.6660092013771646e-06 old loss 5.677864464814775e-06 BETTER
I0326 06:51:49.932212 1431411 finetune.py:68] layer 3_gate @ epoch 4 new loss 1.0415386896056589e-05 old loss 1.0445789484947454e-05 BETTER
I0326 06:52:09.119374 1430829 finetune.py:45] layer 2_down initial loss 8.45867361931596e-06
I0326 06:52:12.244881 1431411 finetune.py:45] layer 3_down initial loss 1.6067639080574736e-05
I0326 06:52:37.185841 1430829 finetune.py:68] layer 2_down @ epoch 0 new loss 8.457266631012317e-06 old loss 8.45867361931596e-06 BETTER
I0326 06:52:39.075016 1431411 finetune.py:68] layer 3_down @ epoch 0 new loss 1.6065980162238702e-05 old loss 1.6067639080574736e-05 BETTER
I0326 06:53:06.256987 1430829 finetune.py:68] layer 2_down @ epoch 1 new loss 8.456343493890017e-06 old loss 8.457266631012317e-06 BETTER
I0326 06:53:06.650444 1431411 finetune.py:68] layer 3_down @ epoch 1 new loss 1.606483965588268e-05 old loss 1.6065980162238702e-05 BETTER
I0326 06:53:34.224847 1431411 finetune.py:68] layer 3_down @ epoch 2 new loss 1.6063859220594168e-05 old loss 1.606483965588268e-05 BETTER
I0326 06:53:35.388236 1430829 finetune.py:68] layer 2_down @ epoch 2 new loss 8.455699571641162e-06 old loss 8.456343493890017e-06 BETTER
I0326 06:54:01.998130 1431411 finetune.py:68] layer 3_down @ epoch 3 new loss 1.6063237126218155e-05 old loss 1.6063859220594168e-05 BETTER
I0326 06:54:04.641874 1430829 finetune.py:68] layer 2_down @ epoch 3 new loss 8.45520116854459e-06 old loss 8.455699571641162e-06 BETTER
I0326 06:54:29.729644 1431411 finetune.py:68] layer 3_down @ epoch 4 new loss 1.60625932039693e-05 old loss 1.6063237126218155e-05 BETTER
3_v proxy err 0.016142353415489197 tr(WHW.T) 289.3331604003906
bpp_loss 3.178705930709839
3_q proxy err 0.00045584756298922 tr(WHW.T) 47573.1640625
bpp_loss 4.156601905822754
3_k proxy err 0.00023218136630021036 tr(WHW.T) 26181.767578125
bpp_loss 5.081831455230713
3_o proxy err 0.01114659197628498 tr(WHW.T) 1858.6953125
bpp_loss 3.2933233976364136
3_up proxy err 0.011155663058161736 tr(WHW.T) 7538.0390625
bpp_loss 3.4393445423671176
3_gate proxy err 0.004129308741539717 tr(WHW.T) 20875.96875
bpp_loss 3.6849749428885326
3_down proxy err 0.011295834556221962 tr(WHW.T) 7024.896484375
bpp_loss 3.4387419564383372
I0326 06:54:34.054185 1430829 finetune.py:68] layer 2_down @ epoch 4 new loss 8.454780981992371e-06 old loss 8.45520116854459e-06 BETTER
2_v proxy err 0.024582769721746445 tr(WHW.T) 155.95950317382812
bpp_loss 3.08063280582428
2_q proxy err 0.0004273267986718565 tr(WHW.T) 41462.046875
bpp_loss 4.115429878234863
2_k proxy err 0.0002284699585288763 tr(WHW.T) 22620.56640625
bpp_loss 5.001641035079956
2_o proxy err 0.008728100918233395 tr(WHW.T) 1969.3255615234375
bpp_loss 3.195391058921814
2_up proxy err 0.011188705451786518 tr(WHW.T) 7603.00439453125
bpp_loss 3.4574947357177734
2_gate proxy err 0.005710917059332132 tr(WHW.T) 15119.0078125
bpp_loss 3.6086338588169644
2_down proxy err 0.00984286144375801 tr(WHW.T) 7746.94287109375
bpp_loss 3.4626264572143555
I0326 06:55:43.672953 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 63.991899490356445s
I0326 06:55:46.934257 1442072 config.py:54] PyTorch version 2.6.0 available.
W0326 06:55:47.216213 1442072 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 06:55:48.099296 1442072 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 06:55:48.103288 1418472 quantize_finetune_llama.py:209] layer 5 gpu 1
I0326 06:55:48.119209 1442072 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 06:55:55.482130 1442072 finetune.py:45] layer 4_v initial loss 9.082802534976508e-06
W0326 06:55:55.482332 1442072 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 06:56:31.946490 1442072 finetune.py:68] layer 4_v @ epoch 0 new loss 3.6393748814589344e-06 old loss 9.082802534976508e-06 BETTER
I0326 06:56:49.194164 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 60.6654167175293s
I0326 06:56:52.659251 1442682 config.py:54] PyTorch version 2.6.0 available.
W0326 06:56:52.951592 1442682 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 06:56:53.831133 1442682 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 06:56:53.835613 1418472 quantize_finetune_llama.py:209] layer 6 gpu 0
I0326 06:56:53.850314 1442682 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 06:57:00.636211 1442682 finetune.py:45] layer 5_v initial loss 9.67882078839466e-06
W0326 06:57:00.636411 1442682 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 06:57:08.943925 1442072 finetune.py:68] layer 4_v @ epoch 1 new loss 2.993354428326711e-06 old loss 3.6393748814589344e-06 BETTER
I0326 06:57:34.084197 1442682 finetune.py:68] layer 5_v @ epoch 0 new loss 4.794893811777001e-06 old loss 9.67882078839466e-06 BETTER
I0326 06:57:46.019940 1442072 finetune.py:68] layer 4_v @ epoch 2 new loss 2.741460775723681e-06 old loss 2.993354428326711e-06 BETTER
I0326 06:58:08.519357 1442682 finetune.py:68] layer 5_v @ epoch 1 new loss 4.228666966810124e-06 old loss 4.794893811777001e-06 BETTER
I0326 06:58:23.446191 1442072 finetune.py:68] layer 4_v @ epoch 3 new loss 2.5984711555793183e-06 old loss 2.741460775723681e-06 BETTER
I0326 06:58:43.310737 1442682 finetune.py:68] layer 5_v @ epoch 2 new loss 3.978542736149393e-06 old loss 4.228666966810124e-06 BETTER
I0326 06:59:00.984248 1442072 finetune.py:68] layer 4_v @ epoch 4 new loss 2.5026877210621024e-06 old loss 2.5984711555793183e-06 BETTER
I0326 06:59:11.038559 1442072 finetune.py:45] layer 4_q initial loss 3.051078692806186e-06
I0326 06:59:18.532962 1442682 finetune.py:68] layer 5_v @ epoch 3 new loss 3.826440661214292e-06 old loss 3.978542736149393e-06 BETTER
I0326 06:59:47.241131 1442072 finetune.py:68] layer 4_q @ epoch 0 new loss 2.8848808142356575e-06 old loss 3.051078692806186e-06 BETTER
I0326 06:59:53.759720 1442682 finetune.py:68] layer 5_v @ epoch 4 new loss 3.7204431464488152e-06 old loss 3.826440661214292e-06 BETTER
I0326 07:00:03.177705 1442682 finetune.py:45] layer 5_q initial loss 4.573347268888028e-06
I0326 07:00:24.240463 1442072 finetune.py:68] layer 4_q @ epoch 1 new loss 2.799937192321522e-06 old loss 2.8848808142356575e-06 BETTER
I0326 07:00:37.129434 1442682 finetune.py:68] layer 5_q @ epoch 0 new loss 4.3548166104301345e-06 old loss 4.573347268888028e-06 BETTER
I0326 07:01:01.178783 1442072 finetune.py:68] layer 4_q @ epoch 2 new loss 2.737237991823349e-06 old loss 2.799937192321522e-06 BETTER
I0326 07:01:11.740435 1442682 finetune.py:68] layer 5_q @ epoch 1 new loss 4.252823600836564e-06 old loss 4.3548166104301345e-06 BETTER
I0326 07:01:38.021502 1442072 finetune.py:68] layer 4_q @ epoch 3 new loss 2.6860950583795784e-06 old loss 2.737237991823349e-06 BETTER
I0326 07:01:46.538894 1442682 finetune.py:68] layer 5_q @ epoch 2 new loss 4.17477031078306e-06 old loss 4.252823600836564e-06 BETTER
I0326 07:02:14.899595 1442072 finetune.py:68] layer 4_q @ epoch 4 new loss 2.643128937052097e-06 old loss 2.6860950583795784e-06 BETTER
I0326 07:02:21.338150 1442682 finetune.py:68] layer 5_q @ epoch 3 new loss 4.111972430109745e-06 old loss 4.17477031078306e-06 BETTER
I0326 07:02:22.775631 1442072 finetune.py:45] layer 4_k initial loss 2.839611170202261e-06
I0326 07:02:56.128112 1442682 finetune.py:68] layer 5_q @ epoch 4 new loss 4.058524154970655e-06 old loss 4.111972430109745e-06 BETTER
I0326 07:02:58.965118 1442072 finetune.py:68] layer 4_k @ epoch 0 new loss 2.786231107165804e-06 old loss 2.839611170202261e-06 BETTER
I0326 07:03:03.787901 1442682 finetune.py:45] layer 5_k initial loss 4.342427018855233e-06
I0326 07:03:35.728487 1442072 finetune.py:68] layer 4_k @ epoch 1 new loss 2.7507164759299485e-06 old loss 2.786231107165804e-06 BETTER
I0326 07:03:37.524883 1442682 finetune.py:68] layer 5_k @ epoch 0 new loss 4.225442808092339e-06 old loss 4.342427018855233e-06 BETTER
I0326 07:04:11.984754 1442682 finetune.py:68] layer 5_k @ epoch 1 new loss 4.179570169071667e-06 old loss 4.225442808092339e-06 BETTER
I0326 07:04:12.661432 1442072 finetune.py:68] layer 4_k @ epoch 2 new loss 2.720633347053081e-06 old loss 2.7507164759299485e-06 BETTER
I0326 07:04:46.530673 1442682 finetune.py:68] layer 5_k @ epoch 2 new loss 4.141726549278246e-06 old loss 4.179570169071667e-06 BETTER
I0326 07:04:49.467578 1442072 finetune.py:68] layer 4_k @ epoch 3 new loss 2.694700697247754e-06 old loss 2.720633347053081e-06 BETTER
I0326 07:05:21.142560 1442682 finetune.py:68] layer 5_k @ epoch 3 new loss 4.108277607883792e-06 old loss 4.141726549278246e-06 BETTER
I0326 07:05:26.220152 1442072 finetune.py:68] layer 4_k @ epoch 4 new loss 2.670390131243039e-06 old loss 2.694700697247754e-06 BETTER
I0326 07:05:35.907032 1442072 finetune.py:45] layer 4_o initial loss 7.68100653658621e-06
I0326 07:05:55.740002 1442682 finetune.py:68] layer 5_k @ epoch 4 new loss 4.077791800227715e-06 old loss 4.108277607883792e-06 BETTER
I0326 07:06:05.041780 1442682 finetune.py:45] layer 5_o initial loss 1.1167472621309571e-05
I0326 07:06:11.111998 1442072 finetune.py:68] layer 4_o @ epoch 0 new loss 7.238061243697302e-06 old loss 7.68100653658621e-06 BETTER
I0326 07:06:38.638774 1442682 finetune.py:68] layer 5_o @ epoch 0 new loss 1.045718636305537e-05 old loss 1.1167472621309571e-05 BETTER
I0326 07:06:47.052228 1442072 finetune.py:68] layer 4_o @ epoch 1 new loss 7.0410715125035495e-06 old loss 7.238061243697302e-06 BETTER
I0326 07:07:12.423363 1442682 finetune.py:68] layer 5_o @ epoch 1 new loss 1.0141189704881981e-05 old loss 1.045718636305537e-05 BETTER
I0326 07:07:22.920452 1442072 finetune.py:68] layer 4_o @ epoch 2 new loss 6.894780653965427e-06 old loss 7.0410715125035495e-06 BETTER
I0326 07:07:46.341449 1442682 finetune.py:68] layer 5_o @ epoch 2 new loss 9.917494026012719e-06 old loss 1.0141189704881981e-05 BETTER
I0326 07:07:58.961389 1442072 finetune.py:68] layer 4_o @ epoch 3 new loss 6.778909664717503e-06 old loss 6.894780653965427e-06 BETTER
I0326 07:08:20.337478 1442682 finetune.py:68] layer 5_o @ epoch 3 new loss 9.743576811160892e-06 old loss 9.917494026012719e-06 BETTER
I0326 07:08:35.019956 1442072 finetune.py:68] layer 4_o @ epoch 4 new loss 6.6801080720324535e-06 old loss 6.778909664717503e-06 BETTER
I0326 07:08:54.008032 1442682 finetune.py:68] layer 5_o @ epoch 4 new loss 9.599765689927153e-06 old loss 9.743576811160892e-06 BETTER
I0326 07:08:56.657376 1442072 finetune.py:45] layer 4_up initial loss 1.4271948202804197e-05
I0326 07:09:15.426404 1442682 finetune.py:45] layer 5_up initial loss 2.0543679056572728e-05
I0326 07:09:28.677963 1442072 finetune.py:68] layer 4_up @ epoch 0 new loss 1.4024091797182336e-05 old loss 1.4271948202804197e-05 BETTER
I0326 07:09:46.016142 1442682 finetune.py:68] layer 5_up @ epoch 0 new loss 2.0093637431273237e-05 old loss 2.0543679056572728e-05 BETTER
I0326 07:10:01.999389 1442072 finetune.py:68] layer 4_up @ epoch 1 new loss 1.3863725143892225e-05 old loss 1.4024091797182336e-05 BETTER
I0326 07:10:17.771188 1442682 finetune.py:68] layer 5_up @ epoch 1 new loss 1.9813503968180157e-05 old loss 2.0093637431273237e-05 BETTER
I0326 07:10:35.652173 1442072 finetune.py:68] layer 4_up @ epoch 2 new loss 1.3729480997426435e-05 old loss 1.3863725143892225e-05 BETTER
I0326 07:10:49.942577 1442682 finetune.py:68] layer 5_up @ epoch 2 new loss 1.957948916242458e-05 old loss 1.9813503968180157e-05 BETTER
I0326 07:11:09.468918 1442072 finetune.py:68] layer 4_up @ epoch 3 new loss 1.3608098015538417e-05 old loss 1.3729480997426435e-05 BETTER
I0326 07:11:22.040954 1442682 finetune.py:68] layer 5_up @ epoch 3 new loss 1.9375152987777255e-05 old loss 1.957948916242458e-05 BETTER
I0326 07:11:43.324390 1442072 finetune.py:68] layer 4_up @ epoch 4 new loss 1.3498512998921797e-05 old loss 1.3608098015538417e-05 BETTER
I0326 07:11:54.241120 1442682 finetune.py:68] layer 5_up @ epoch 4 new loss 1.919181704579387e-05 old loss 1.9375152987777255e-05 BETTER
I0326 07:12:05.013761 1442072 finetune.py:45] layer 4_gate initial loss 1.619002432562411e-05
I0326 07:12:15.816069 1442682 finetune.py:45] layer 5_gate initial loss 2.3022410459816456e-05
I0326 07:12:35.543991 1442072 finetune.py:68] layer 4_gate @ epoch 0 new loss 1.606684054422658e-05 old loss 1.619002432562411e-05 BETTER
I0326 07:12:44.581554 1442682 finetune.py:68] layer 5_gate @ epoch 0 new loss 2.2814998374087736e-05 old loss 2.3022410459816456e-05 BETTER
I0326 07:13:06.951382 1442072 finetune.py:68] layer 4_gate @ epoch 1 new loss 1.5966281353030354e-05 old loss 1.606684054422658e-05 BETTER
I0326 07:13:14.222235 1442682 finetune.py:68] layer 5_gate @ epoch 1 new loss 2.2648620870313607e-05 old loss 2.2814998374087736e-05 BETTER
I0326 07:13:38.432468 1442072 finetune.py:68] layer 4_gate @ epoch 2 new loss 1.587574661243707e-05 old loss 1.5966281353030354e-05 BETTER
I0326 07:13:43.986667 1442682 finetune.py:68] layer 5_gate @ epoch 2 new loss 2.2497652025776915e-05 old loss 2.2648620870313607e-05 BETTER
I0326 07:14:10.301205 1442072 finetune.py:68] layer 4_gate @ epoch 3 new loss 1.579246782057453e-05 old loss 1.587574661243707e-05 BETTER
I0326 07:14:13.760916 1442682 finetune.py:68] layer 5_gate @ epoch 3 new loss 2.236341060779523e-05 old loss 2.2497652025776915e-05 BETTER
I0326 07:14:42.010736 1442072 finetune.py:68] layer 4_gate @ epoch 4 new loss 1.5714635082986206e-05 old loss 1.579246782057453e-05 BETTER
I0326 07:14:43.626205 1442682 finetune.py:68] layer 5_gate @ epoch 4 new loss 2.223549017799087e-05 old loss 2.236341060779523e-05 BETTER
I0326 07:15:04.644747 1442072 finetune.py:45] layer 4_down initial loss 2.5993029339588247e-05
I0326 07:15:05.842710 1442682 finetune.py:45] layer 5_down initial loss 3.66690474038478e-05
I0326 07:15:32.701159 1442682 finetune.py:68] layer 5_down @ epoch 0 new loss 3.666424163384363e-05 old loss 3.66690474038478e-05 BETTER
I0326 07:15:32.789375 1442072 finetune.py:68] layer 4_down @ epoch 0 new loss 2.5989569621742703e-05 old loss 2.5993029339588247e-05 BETTER
I0326 07:16:00.336475 1442682 finetune.py:68] layer 5_down @ epoch 1 new loss 3.6662022466771305e-05 old loss 3.666424163384363e-05 BETTER
I0326 07:16:02.087600 1442072 finetune.py:68] layer 4_down @ epoch 1 new loss 2.598775790829677e-05 old loss 2.5989569621742703e-05 BETTER
I0326 07:16:28.052093 1442682 finetune.py:68] layer 5_down @ epoch 2 new loss 3.6660207115346566e-05 old loss 3.6662022466771305e-05 BETTER
I0326 07:16:31.556694 1442072 finetune.py:68] layer 4_down @ epoch 2 new loss 2.5986231776187196e-05 old loss 2.598775790829677e-05 BETTER
I0326 07:16:55.761410 1442682 finetune.py:68] layer 5_down @ epoch 3 new loss 3.665867552626878e-05 old loss 3.6660207115346566e-05 BETTER
I0326 07:17:00.878407 1442072 finetune.py:68] layer 4_down @ epoch 3 new loss 2.5985051252064295e-05 old loss 2.5986231776187196e-05 BETTER
I0326 07:17:23.528761 1442682 finetune.py:68] layer 5_down @ epoch 4 new loss 3.6657405871665105e-05 old loss 3.665867552626878e-05 BETTER
5_v proxy err 0.02068202942609787 tr(WHW.T) 208.81988525390625
bpp_loss 3.111508011817932
5_q proxy err 0.0005461539840325713 tr(WHW.T) 35997.47265625
bpp_loss 4.095146656036377
5_k proxy err 0.00024998540175147355 tr(WHW.T) 23006.609375
bpp_loss 5.033259391784668
5_o proxy err 0.017148608341813087 tr(WHW.T) 1060.9193115234375
bpp_loss 3.250158429145813
5_up proxy err 0.010661046020686626 tr(WHW.T) 7656.89990234375
bpp_loss 3.4173408235822404
5_gate proxy err 0.00278554018586874 tr(WHW.T) 30429.095703125
bpp_loss 3.760830879211426
5_down proxy err 0.012533740140497684 tr(WHW.T) 6436.05029296875
bpp_loss 3.418966395514352
I0326 07:17:30.186318 1442072 finetune.py:68] layer 4_down @ epoch 4 new loss 2.5984087187680416e-05 old loss 2.5985051252064295e-05 BETTER
4_v proxy err 0.014951963908970356 tr(WHW.T) 285.30712890625
bpp_loss 3.224624752998352
4_q proxy err 0.00039874043432064354 tr(WHW.T) 50126.56640625
bpp_loss 4.120109558105469
4_k proxy err 0.00020207167835906148 tr(WHW.T) 29317.939453125
bpp_loss 5.060348272323608
4_o proxy err 0.014647758565843105 tr(WHW.T) 1303.58447265625
bpp_loss 3.303091883659363
4_up proxy err 0.011124557815492153 tr(WHW.T) 7383.4482421875
bpp_loss 3.412043026515416
4_gate proxy err 0.002928723581135273 tr(WHW.T) 29111.291015625
bpp_loss 3.7583387919834683
4_down proxy err 0.012746881693601608 tr(WHW.T) 6420.53515625
bpp_loss 3.4136626379830495
I0326 07:18:39.369242 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 64.30427718162537s
I0326 07:18:42.781757 1453886 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 07:18:42.781858 1453886 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 07:18:42.781897 1453886 utils.py:162] NumExpr defaulting to 16 threads.
I0326 07:18:43.104938 1453886 config.py:54] PyTorch version 2.6.0 available.
W0326 07:18:43.292858 1453886 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 07:18:43.829644 1453886 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 07:18:43.833225 1418472 quantize_finetune_llama.py:209] layer 7 gpu 1
I0326 07:18:43.845682 1453886 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 07:18:51.120227 1453886 finetune.py:45] layer 6_v initial loss 9.746408068167511e-06
W0326 07:18:51.120429 1453886 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 07:19:27.407155 1453886 finetune.py:68] layer 6_v @ epoch 0 new loss 5.564364073507022e-06 old loss 9.746408068167511e-06 BETTER
I0326 07:19:43.836622 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 59.628376722335815s
I0326 07:19:47.478261 1454523 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 07:19:47.478363 1454523 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 07:19:47.478401 1454523 utils.py:162] NumExpr defaulting to 16 threads.
I0326 07:19:47.798422 1454523 config.py:54] PyTorch version 2.6.0 available.
W0326 07:19:47.991607 1454523 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 07:19:48.544425 1454523 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 07:19:48.548107 1418472 quantize_finetune_llama.py:209] layer 8 gpu 0
I0326 07:19:48.560653 1454523 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 07:19:55.273822 1454523 finetune.py:45] layer 7_v initial loss 1.0869171092053875e-05
W0326 07:19:55.274058 1454523 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 07:20:04.374118 1453886 finetune.py:68] layer 6_v @ epoch 1 new loss 5.027772203902714e-06 old loss 5.564364073507022e-06 BETTER
I0326 07:20:28.743398 1454523 finetune.py:68] layer 7_v @ epoch 0 new loss 6.805229077144759e-06 old loss 1.0869171092053875e-05 BETTER
I0326 07:20:41.340714 1453886 finetune.py:68] layer 6_v @ epoch 2 new loss 4.769201495946618e-06 old loss 5.027772203902714e-06 BETTER
I0326 07:21:03.176595 1454523 finetune.py:68] layer 7_v @ epoch 1 new loss 6.25114898866741e-06 old loss 6.805229077144759e-06 BETTER
I0326 07:21:18.851008 1453886 finetune.py:68] layer 6_v @ epoch 3 new loss 4.6044710870774e-06 old loss 4.769201495946618e-06 BETTER
I0326 07:21:37.842389 1454523 finetune.py:68] layer 7_v @ epoch 2 new loss 5.964430329186143e-06 old loss 6.25114898866741e-06 BETTER
I0326 07:21:56.066763 1453886 finetune.py:68] layer 6_v @ epoch 4 new loss 4.484676537686028e-06 old loss 4.6044710870774e-06 BETTER
I0326 07:22:05.772632 1453886 finetune.py:45] layer 6_q initial loss 5.543543466046685e-06
I0326 07:22:12.688503 1454523 finetune.py:68] layer 7_v @ epoch 3 new loss 5.77296032133745e-06 old loss 5.964430329186143e-06 BETTER
I0326 07:22:41.676546 1453886 finetune.py:68] layer 6_q @ epoch 0 new loss 5.2571745072782505e-06 old loss 5.543543466046685e-06 BETTER
I0326 07:22:47.708711 1454523 finetune.py:68] layer 7_v @ epoch 4 new loss 5.632443389913533e-06 old loss 5.77296032133745e-06 BETTER
I0326 07:22:57.878571 1454523 finetune.py:45] layer 7_q initial loss 7.191823897301219e-06
I0326 07:23:18.710105 1453886 finetune.py:68] layer 6_q @ epoch 1 new loss 5.135017090651672e-06 old loss 5.2571745072782505e-06 BETTER
I0326 07:23:31.624116 1454523 finetune.py:68] layer 7_q @ epoch 0 new loss 6.814464995841263e-06 old loss 7.191823897301219e-06 BETTER
I0326 07:23:55.371179 1453886 finetune.py:68] layer 6_q @ epoch 2 new loss 5.040375071985181e-06 old loss 5.135017090651672e-06 BETTER
I0326 07:24:06.095558 1454523 finetune.py:68] layer 7_q @ epoch 1 new loss 6.663076874247054e-06 old loss 6.814464995841263e-06 BETTER
I0326 07:24:31.881852 1453886 finetune.py:68] layer 6_q @ epoch 3 new loss 4.9623554332356434e-06 old loss 5.040375071985181e-06 BETTER
I0326 07:24:40.597991 1454523 finetune.py:68] layer 7_q @ epoch 2 new loss 6.545986707351403e-06 old loss 6.663076874247054e-06 BETTER
I0326 07:25:08.763704 1453886 finetune.py:68] layer 6_q @ epoch 4 new loss 4.897164217254613e-06 old loss 4.9623554332356434e-06 BETTER
I0326 07:25:15.041675 1454523 finetune.py:68] layer 7_q @ epoch 3 new loss 6.451911758631468e-06 old loss 6.545986707351403e-06 BETTER
I0326 07:25:16.714880 1453886 finetune.py:45] layer 6_k initial loss 5.2723548833455425e-06
I0326 07:25:49.732310 1454523 finetune.py:68] layer 7_q @ epoch 4 new loss 6.371581093844725e-06 old loss 6.451911758631468e-06 BETTER
I0326 07:25:52.477159 1453886 finetune.py:68] layer 6_k @ epoch 0 new loss 5.120259174873354e-06 old loss 5.2723548833455425e-06 BETTER
I0326 07:25:57.504483 1454523 finetune.py:45] layer 7_k initial loss 6.786638095945818e-06
I0326 07:26:28.888788 1453886 finetune.py:68] layer 6_k @ epoch 1 new loss 5.0651115088840015e-06 old loss 5.120259174873354e-06 BETTER
I0326 07:26:31.097991 1454523 finetune.py:68] layer 7_k @ epoch 0 new loss 6.6583552325027995e-06 old loss 6.786638095945818e-06 BETTER
I0326 07:27:05.490681 1453886 finetune.py:68] layer 6_k @ epoch 2 new loss 5.019058789912378e-06 old loss 5.0651115088840015e-06 BETTER
I0326 07:27:05.500226 1454523 finetune.py:68] layer 7_k @ epoch 1 new loss 6.592842964892043e-06 old loss 6.6583552325027995e-06 BETTER
I0326 07:27:39.872450 1454523 finetune.py:68] layer 7_k @ epoch 2 new loss 6.534432941407431e-06 old loss 6.592842964892043e-06 BETTER
I0326 07:27:42.185769 1453886 finetune.py:68] layer 6_k @ epoch 3 new loss 4.976459877070738e-06 old loss 5.019058789912378e-06 BETTER
I0326 07:28:14.466763 1454523 finetune.py:68] layer 7_k @ epoch 3 new loss 6.4840678533073515e-06 old loss 6.534432941407431e-06 BETTER
I0326 07:28:19.035832 1453886 finetune.py:68] layer 6_k @ epoch 4 new loss 4.939742666465463e-06 old loss 4.976459877070738e-06 BETTER
I0326 07:28:28.712336 1453886 finetune.py:45] layer 6_o initial loss 1.5394309230032377e-05
I0326 07:28:49.112661 1454523 finetune.py:68] layer 7_k @ epoch 4 new loss 6.435493560275063e-06 old loss 6.4840678533073515e-06 BETTER
I0326 07:28:58.431404 1454523 finetune.py:45] layer 7_o initial loss 2.1972020476823673e-05
I0326 07:29:04.267499 1453886 finetune.py:68] layer 6_o @ epoch 0 new loss 1.4290095350588672e-05 old loss 1.5394309230032377e-05 BETTER
I0326 07:29:31.727055 1454523 finetune.py:68] layer 7_o @ epoch 0 new loss 1.996241189772263e-05 old loss 2.1972020476823673e-05 BETTER
I0326 07:29:40.291213 1453886 finetune.py:68] layer 6_o @ epoch 1 new loss 1.378443448629696e-05 old loss 1.4290095350588672e-05 BETTER
I0326 07:30:05.927501 1454523 finetune.py:68] layer 7_o @ epoch 1 new loss 1.9041832274524495e-05 old loss 1.996241189772263e-05 BETTER
I0326 07:30:16.286605 1453886 finetune.py:68] layer 6_o @ epoch 2 new loss 1.3429645150608849e-05 old loss 1.378443448629696e-05 BETTER
I0326 07:30:39.762198 1454523 finetune.py:68] layer 7_o @ epoch 2 new loss 1.842237907112576e-05 old loss 1.9041832274524495e-05 BETTER
I0326 07:30:52.313621 1453886 finetune.py:68] layer 6_o @ epoch 3 new loss 1.3153409781807568e-05 old loss 1.3429645150608849e-05 BETTER
I0326 07:31:13.838866 1454523 finetune.py:68] layer 7_o @ epoch 3 new loss 1.7949023458641022e-05 old loss 1.842237907112576e-05 BETTER
I0326 07:31:28.647485 1453886 finetune.py:68] layer 6_o @ epoch 4 new loss 1.2924762813781854e-05 old loss 1.3153409781807568e-05 BETTER
I0326 07:31:47.896998 1454523 finetune.py:68] layer 7_o @ epoch 4 new loss 1.7570961063029245e-05 old loss 1.7949023458641022e-05 BETTER
I0326 07:31:51.135500 1453886 finetune.py:45] layer 6_up initial loss 2.6946034267893992e-05
I0326 07:32:09.987527 1454523 finetune.py:45] layer 7_up initial loss 3.2799776818137616e-05
I0326 07:32:23.644045 1453886 finetune.py:68] layer 6_up @ epoch 0 new loss 2.6209581847069785e-05 old loss 2.6946034267893992e-05 BETTER
I0326 07:32:40.870677 1454523 finetune.py:68] layer 7_up @ epoch 0 new loss 3.1862451578490436e-05 old loss 3.2799776818137616e-05 BETTER
I0326 07:32:57.066393 1453886 finetune.py:68] layer 6_up @ epoch 1 new loss 2.5758057745406404e-05 old loss 2.6209581847069785e-05 BETTER
I0326 07:33:12.914707 1454523 finetune.py:68] layer 7_up @ epoch 1 new loss 3.128236494376324e-05 old loss 3.1862451578490436e-05 BETTER
I0326 07:33:30.763939 1453886 finetune.py:68] layer 6_up @ epoch 2 new loss 2.5387664209119976e-05 old loss 2.5758057745406404e-05 BETTER
I0326 07:33:45.087834 1454523 finetune.py:68] layer 7_up @ epoch 2 new loss 3.0805342248640954e-05 old loss 3.128236494376324e-05 BETTER
I0326 07:34:04.595329 1453886 finetune.py:68] layer 6_up @ epoch 3 new loss 2.5068657123483717e-05 old loss 2.5387664209119976e-05 BETTER
I0326 07:34:17.286598 1454523 finetune.py:68] layer 7_up @ epoch 3 new loss 3.0395673093153164e-05 old loss 3.0805342248640954e-05 BETTER
I0326 07:34:38.520742 1453886 finetune.py:68] layer 6_up @ epoch 4 new loss 2.4782371838227846e-05 old loss 2.5068657123483717e-05 BETTER
I0326 07:34:49.443721 1454523 finetune.py:68] layer 7_up @ epoch 4 new loss 3.0030414563952945e-05 old loss 3.0395673093153164e-05 BETTER
I0326 07:35:00.778083 1453886 finetune.py:45] layer 6_gate initial loss 2.9397420803434215e-05
I0326 07:35:11.634526 1454523 finetune.py:45] layer 7_gate initial loss 3.568767715478316e-05
I0326 07:35:31.492540 1453886 finetune.py:68] layer 6_gate @ epoch 0 new loss 2.9070712116663344e-05 old loss 2.9397420803434215e-05 BETTER
I0326 07:35:40.437288 1454523 finetune.py:68] layer 7_gate @ epoch 0 new loss 3.528217348502949e-05 old loss 3.568767715478316e-05 BETTER
I0326 07:36:02.919643 1453886 finetune.py:68] layer 6_gate @ epoch 1 new loss 2.881695399992168e-05 old loss 2.9070712116663344e-05 BETTER
I0326 07:36:10.176916 1454523 finetune.py:68] layer 7_gate @ epoch 1 new loss 3.496677891234867e-05 old loss 3.528217348502949e-05 BETTER
I0326 07:36:34.577724 1453886 finetune.py:68] layer 6_gate @ epoch 2 new loss 2.85916084976634e-05 old loss 2.881695399992168e-05 BETTER
I0326 07:36:40.012854 1454523 finetune.py:68] layer 7_gate @ epoch 2 new loss 3.468887371127494e-05 old loss 3.496677891234867e-05 BETTER
I0326 07:37:06.260981 1453886 finetune.py:68] layer 6_gate @ epoch 3 new loss 2.838568798324559e-05 old loss 2.85916084976634e-05 BETTER
I0326 07:37:09.917486 1454523 finetune.py:68] layer 7_gate @ epoch 3 new loss 3.4435419365763664e-05 old loss 3.468887371127494e-05 BETTER
I0326 07:37:37.998871 1453886 finetune.py:68] layer 6_gate @ epoch 4 new loss 2.8196291168569587e-05 old loss 2.838568798324559e-05 BETTER
I0326 07:37:39.770449 1454523 finetune.py:68] layer 7_gate @ epoch 4 new loss 3.420284338062629e-05 old loss 3.4435419365763664e-05 BETTER
I0326 07:38:01.288829 1453886 finetune.py:45] layer 6_down initial loss 4.636013909475878e-05
I0326 07:38:02.562398 1454523 finetune.py:45] layer 7_down initial loss 5.41445697308518e-05
I0326 07:38:29.498274 1453886 finetune.py:68] layer 6_down @ epoch 0 new loss 4.6353186917258427e-05 old loss 4.636013909475878e-05 BETTER
I0326 07:38:29.521129 1454523 finetune.py:68] layer 7_down @ epoch 0 new loss 5.4138708946993575e-05 old loss 5.41445697308518e-05 BETTER
I0326 07:38:57.205126 1454523 finetune.py:68] layer 7_down @ epoch 1 new loss 5.413439066614956e-05 old loss 5.4138708946993575e-05 BETTER
I0326 07:38:58.772236 1453886 finetune.py:68] layer 6_down @ epoch 1 new loss 4.634977085515857e-05 old loss 4.6353186917258427e-05 BETTER
I0326 07:39:25.060178 1454523 finetune.py:68] layer 7_down @ epoch 2 new loss 5.413104736362584e-05 old loss 5.413439066614956e-05 BETTER
I0326 07:39:28.026237 1453886 finetune.py:68] layer 6_down @ epoch 2 new loss 4.6346147428266704e-05 old loss 4.634977085515857e-05 BETTER
I0326 07:39:52.872177 1454523 finetune.py:68] layer 7_down @ epoch 3 new loss 5.412810787674971e-05 old loss 5.413104736362584e-05 BETTER
I0326 07:39:57.459908 1453886 finetune.py:68] layer 6_down @ epoch 3 new loss 4.6343644498847425e-05 old loss 4.6346147428266704e-05 BETTER
I0326 07:40:20.569175 1454523 finetune.py:68] layer 7_down @ epoch 4 new loss 5.412590326159261e-05 old loss 5.412810787674971e-05 BETTER
7_v proxy err 0.014306409284472466 tr(WHW.T) 309.4270935058594
bpp_loss 3.1550577878952026
7_q proxy err 0.0005743923247791827 tr(WHW.T) 35156.5859375
bpp_loss 4.065508842468262
7_k proxy err 0.00021996816212777048 tr(WHW.T) 26871.984375
bpp_loss 5.131833553314209
7_o proxy err 0.019894970580935478 tr(WHW.T) 966.2494506835938
bpp_loss 3.294402837753296
7_up proxy err 0.009156322106719017 tr(WHW.T) 8627.970703125
bpp_loss 3.4298116139003207
7_gate proxy err 0.0023562940768897533 tr(WHW.T) 34903.71484375
bpp_loss 3.7365305764334544
7_down proxy err 0.012507088482379913 tr(WHW.T) 6564.23046875
bpp_loss 3.433756555829729
I0326 07:40:26.846851 1453886 finetune.py:68] layer 6_down @ epoch 4 new loss 4.634152355720289e-05 old loss 4.6343644498847425e-05 BETTER
6_v proxy err 0.01702660322189331 tr(WHW.T) 253.63377380371094
bpp_loss 3.156864285469055
6_q proxy err 0.0005537159158848226 tr(WHW.T) 35659.73046875
bpp_loss 4.14307713508606
6_k proxy err 0.0002181626477977261 tr(WHW.T) 26187.330078125
bpp_loss 5.104706764221191
6_o proxy err 0.01951623149216175 tr(WHW.T) 1016.862548828125
bpp_loss 3.2831037044525146
6_up proxy err 0.010098018683493137 tr(WHW.T) 7923.77197265625
bpp_loss 3.4164551326206754
6_gate proxy err 0.0023367826361209154 tr(WHW.T) 35749.9453125
bpp_loss 3.766104289463588
6_down proxy err 0.01244769711047411 tr(WHW.T) 6512.5849609375
bpp_loss 3.419085843222482
I0326 07:41:36.292386 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 64.43338561058044s
I0326 07:41:39.848668 1469253 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 07:41:39.848771 1469253 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 07:41:39.848809 1469253 utils.py:162] NumExpr defaulting to 16 threads.
I0326 07:41:40.174428 1469253 config.py:54] PyTorch version 2.6.0 available.
W0326 07:41:40.368129 1469253 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 07:41:40.929663 1469253 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 07:41:40.933710 1418472 quantize_finetune_llama.py:209] layer 9 gpu 1
I0326 07:41:40.946598 1469253 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 07:41:48.361370 1469253 finetune.py:45] layer 8_v initial loss 1.1750380508601665e-05
W0326 07:41:48.361580 1469253 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 07:42:24.760068 1469253 finetune.py:68] layer 8_v @ epoch 0 new loss 7.710257705184631e-06 old loss 1.1750380508601665e-05 BETTER
I0326 07:42:42.953605 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 61.628867387771606s
I0326 07:42:46.509453 1469949 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 07:42:46.509555 1469949 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 07:42:46.509594 1469949 utils.py:162] NumExpr defaulting to 16 threads.
I0326 07:42:46.835685 1469949 config.py:54] PyTorch version 2.6.0 available.
W0326 07:42:47.023128 1469949 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 07:42:47.577811 1469949 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 07:42:47.581311 1418472 quantize_finetune_llama.py:209] layer 10 gpu 0
I0326 07:42:47.593850 1469949 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 07:42:54.478188 1469949 finetune.py:45] layer 9_v initial loss 1.3335170478967484e-05
W0326 07:42:54.478479 1469949 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 07:43:01.804868 1469253 finetune.py:68] layer 8_v @ epoch 1 new loss 7.108402769517852e-06 old loss 7.710257705184631e-06 BETTER
I0326 07:43:27.840070 1469949 finetune.py:68] layer 9_v @ epoch 0 new loss 8.00343605078524e-06 old loss 1.3335170478967484e-05 BETTER
I0326 07:43:38.741293 1469253 finetune.py:68] layer 8_v @ epoch 2 new loss 6.788655355194351e-06 old loss 7.108402769517852e-06 BETTER
I0326 07:44:02.335870 1469949 finetune.py:68] layer 9_v @ epoch 1 new loss 7.284728781087324e-06 old loss 8.00343605078524e-06 BETTER
I0326 07:44:15.856531 1469253 finetune.py:68] layer 8_v @ epoch 3 new loss 6.575173301825998e-06 old loss 6.788655355194351e-06 BETTER
I0326 07:44:37.223949 1469949 finetune.py:68] layer 9_v @ epoch 2 new loss 6.92938374413643e-06 old loss 7.284728781087324e-06 BETTER
I0326 07:44:52.901311 1469253 finetune.py:68] layer 8_v @ epoch 4 new loss 6.417257736757165e-06 old loss 6.575173301825998e-06 BETTER
I0326 07:45:03.305565 1469253 finetune.py:45] layer 8_q initial loss 8.085164154181257e-06
I0326 07:45:12.578045 1469949 finetune.py:68] layer 9_v @ epoch 3 new loss 6.691268481517909e-06 old loss 6.92938374413643e-06 BETTER
I0326 07:45:39.669482 1469253 finetune.py:68] layer 8_q @ epoch 0 new loss 7.713269042142201e-06 old loss 8.085164154181257e-06 BETTER
I0326 07:45:47.952252 1469949 finetune.py:68] layer 9_v @ epoch 4 new loss 6.523097908939235e-06 old loss 6.691268481517909e-06 BETTER
I0326 07:45:57.392427 1469949 finetune.py:45] layer 9_q initial loss 8.339541636814829e-06
I0326 07:46:16.559268 1469253 finetune.py:68] layer 8_q @ epoch 1 new loss 7.541194463556167e-06 old loss 7.713269042142201e-06 BETTER
I0326 07:46:31.187733 1469949 finetune.py:68] layer 9_q @ epoch 0 new loss 7.9411383921979e-06 old loss 8.339541636814829e-06 BETTER
I0326 07:46:53.501916 1469253 finetune.py:68] layer 8_q @ epoch 2 new loss 7.415282198053319e-06 old loss 7.541194463556167e-06 BETTER
I0326 07:47:05.739742 1469949 finetune.py:68] layer 9_q @ epoch 1 new loss 7.76115120970644e-06 old loss 7.9411383921979e-06 BETTER
I0326 07:47:30.301220 1469253 finetune.py:68] layer 8_q @ epoch 3 new loss 7.3050578066613525e-06 old loss 7.415282198053319e-06 BETTER
I0326 07:47:40.380818 1469949 finetune.py:68] layer 9_q @ epoch 2 new loss 7.63282150728628e-06 old loss 7.76115120970644e-06 BETTER
I0326 07:48:07.172343 1469253 finetune.py:68] layer 8_q @ epoch 4 new loss 7.218371138151269e-06 old loss 7.3050578066613525e-06 BETTER
I0326 07:48:14.993422 1469949 finetune.py:68] layer 9_q @ epoch 3 new loss 7.5320594987715594e-06 old loss 7.63282150728628e-06 BETTER
I0326 07:48:15.113341 1469253 finetune.py:45] layer 8_k initial loss 7.617925803060643e-06
I0326 07:48:49.646874 1469949 finetune.py:68] layer 9_q @ epoch 4 new loss 7.439190994773526e-06 old loss 7.5320594987715594e-06 BETTER
I0326 07:48:51.128834 1469253 finetune.py:68] layer 8_k @ epoch 0 new loss 7.498024842789164e-06 old loss 7.617925803060643e-06 BETTER
I0326 07:48:57.461650 1469949 finetune.py:45] layer 9_k initial loss 7.990976882865652e-06
I0326 07:49:27.682191 1469253 finetune.py:68] layer 8_k @ epoch 1 new loss 7.426848242175765e-06 old loss 7.498024842789164e-06 BETTER
I0326 07:49:31.245141 1469949 finetune.py:68] layer 9_k @ epoch 0 new loss 7.815011485945433e-06 old loss 7.990976882865652e-06 BETTER
I0326 07:50:04.237647 1469253 finetune.py:68] layer 8_k @ epoch 2 new loss 7.365526471403427e-06 old loss 7.426848242175765e-06 BETTER
I0326 07:50:05.800093 1469949 finetune.py:68] layer 9_k @ epoch 1 new loss 7.723135240667034e-06 old loss 7.815011485945433e-06 BETTER
I0326 07:50:40.261905 1469949 finetune.py:68] layer 9_k @ epoch 2 new loss 7.661753443244379e-06 old loss 7.723135240667034e-06 BETTER
I0326 07:50:41.192796 1469253 finetune.py:68] layer 8_k @ epoch 3 new loss 7.311500667128712e-06 old loss 7.365526471403427e-06 BETTER
I0326 07:51:14.941931 1469949 finetune.py:68] layer 9_k @ epoch 3 new loss 7.603005087730708e-06 old loss 7.661753443244379e-06 BETTER
I0326 07:51:18.061730 1469253 finetune.py:68] layer 8_k @ epoch 4 new loss 7.266123247973155e-06 old loss 7.311500667128712e-06 BETTER
I0326 07:51:27.963497 1469253 finetune.py:45] layer 8_o initial loss 2.7503823730512522e-05
I0326 07:51:49.597553 1469949 finetune.py:68] layer 9_k @ epoch 4 new loss 7.544575055362657e-06 old loss 7.603005087730708e-06 BETTER
I0326 07:51:59.086457 1469949 finetune.py:45] layer 9_o initial loss 2.9676153644686565e-05
I0326 07:52:03.534065 1469253 finetune.py:68] layer 8_o @ epoch 0 new loss 2.501809285604395e-05 old loss 2.7503823730512522e-05 BETTER
I0326 07:52:32.405818 1469949 finetune.py:68] layer 9_o @ epoch 0 new loss 2.6815079763764516e-05 old loss 2.9676153644686565e-05 BETTER
I0326 07:52:39.678808 1469253 finetune.py:68] layer 8_o @ epoch 1 new loss 2.386706546531059e-05 old loss 2.501809285604395e-05 BETTER
I0326 07:53:06.388374 1469949 finetune.py:68] layer 9_o @ epoch 1 new loss 2.5501525669824332e-05 old loss 2.6815079763764516e-05 BETTER
I0326 07:53:15.838964 1469253 finetune.py:68] layer 8_o @ epoch 2 new loss 2.307011527591385e-05 old loss 2.386706546531059e-05 BETTER
I0326 07:53:40.231675 1469949 finetune.py:68] layer 9_o @ epoch 2 new loss 2.4598912204965018e-05 old loss 2.5501525669824332e-05 BETTER
I0326 07:53:51.918695 1469253 finetune.py:68] layer 8_o @ epoch 3 new loss 2.2462123524746858e-05 old loss 2.307011527591385e-05 BETTER
I0326 07:54:14.175455 1469949 finetune.py:68] layer 9_o @ epoch 3 new loss 2.3919394152471796e-05 old loss 2.4598912204965018e-05 BETTER
I0326 07:54:28.054539 1469253 finetune.py:68] layer 8_o @ epoch 4 new loss 2.1965210180496797e-05 old loss 2.2462123524746858e-05 BETTER
I0326 07:54:48.219640 1469949 finetune.py:68] layer 9_o @ epoch 4 new loss 2.3368707843474112e-05 old loss 2.3919394152471796e-05 BETTER
I0326 07:54:49.886979 1469253 finetune.py:45] layer 8_up initial loss 3.853219823213294e-05
I0326 07:55:09.683757 1469949 finetune.py:45] layer 9_up initial loss 4.160083699389361e-05
I0326 07:55:22.377879 1469253 finetune.py:68] layer 8_up @ epoch 0 new loss 3.7481626350199804e-05 old loss 3.853219823213294e-05 BETTER
I0326 07:55:40.361363 1469949 finetune.py:68] layer 9_up @ epoch 0 new loss 4.035874371766113e-05 old loss 4.160083699389361e-05 BETTER
I0326 07:55:55.864011 1469253 finetune.py:68] layer 8_up @ epoch 1 new loss 3.681166344904341e-05 old loss 3.7481626350199804e-05 BETTER
I0326 07:56:12.150312 1469949 finetune.py:68] layer 9_up @ epoch 1 new loss 3.9582373574376106e-05 old loss 4.035874371766113e-05 BETTER
I0326 07:56:29.602020 1469253 finetune.py:68] layer 8_up @ epoch 2 new loss 3.625572571763769e-05 old loss 3.681166344904341e-05 BETTER
I0326 07:56:44.125280 1469949 finetune.py:68] layer 9_up @ epoch 2 new loss 3.894956171279773e-05 old loss 3.9582373574376106e-05 BETTER
I0326 07:57:03.545837 1469253 finetune.py:68] layer 8_up @ epoch 3 new loss 3.5778797609964386e-05 old loss 3.625572571763769e-05 BETTER
I0326 07:57:16.227766 1469949 finetune.py:68] layer 9_up @ epoch 3 new loss 3.840046701952815e-05 old loss 3.894956171279773e-05 BETTER
I0326 07:57:37.404034 1469253 finetune.py:68] layer 8_up @ epoch 4 new loss 3.534864663379267e-05 old loss 3.5778797609964386e-05 BETTER
I0326 07:57:48.295847 1469949 finetune.py:68] layer 9_up @ epoch 4 new loss 3.792046118178405e-05 old loss 3.840046701952815e-05 BETTER
I0326 07:57:59.300257 1469253 finetune.py:45] layer 8_gate initial loss 4.141039607929997e-05
I0326 07:58:09.847786 1469949 finetune.py:45] layer 9_gate initial loss 4.4473123125499114e-05
I0326 07:58:29.616149 1469253 finetune.py:68] layer 8_gate @ epoch 0 new loss 4.094267205800861e-05 old loss 4.141039607929997e-05 BETTER
I0326 07:58:38.405106 1469949 finetune.py:68] layer 9_gate @ epoch 0 new loss 4.3937598093179986e-05 old loss 4.4473123125499114e-05 BETTER
I0326 07:59:00.988942 1469253 finetune.py:68] layer 8_gate @ epoch 1 new loss 4.057377009303309e-05 old loss 4.094267205800861e-05 BETTER
I0326 07:59:07.918914 1469949 finetune.py:68] layer 9_gate @ epoch 1 new loss 4.352645919425413e-05 old loss 4.3937598093179986e-05 BETTER
I0326 07:59:32.548007 1469253 finetune.py:68] layer 8_gate @ epoch 2 new loss 4.0245391573989764e-05 old loss 4.057377009303309e-05 BETTER
I0326 07:59:37.579621 1469949 finetune.py:68] layer 9_gate @ epoch 2 new loss 4.316091872169636e-05 old loss 4.352645919425413e-05 BETTER
I0326 08:00:04.234762 1469253 finetune.py:68] layer 8_gate @ epoch 3 new loss 3.9950366044649854e-05 old loss 4.0245391573989764e-05 BETTER
I0326 08:00:07.394641 1469949 finetune.py:68] layer 9_gate @ epoch 3 new loss 4.282595182303339e-05 old loss 4.316091872169636e-05 BETTER
I0326 08:00:35.964432 1469253 finetune.py:68] layer 8_gate @ epoch 4 new loss 3.967463271692395e-05 old loss 3.9950366044649854e-05 BETTER
I0326 08:00:37.189300 1469949 finetune.py:68] layer 9_gate @ epoch 4 new loss 4.2523766751401126e-05 old loss 4.282595182303339e-05 BETTER
I0326 08:00:58.702038 1469253 finetune.py:45] layer 8_down initial loss 6.132208363851532e-05
I0326 08:00:59.508868 1469949 finetune.py:45] layer 9_down initial loss 6.664502870989963e-05
I0326 08:01:26.240908 1469949 finetune.py:68] layer 9_down @ epoch 0 new loss 6.663617386948317e-05 old loss 6.664502870989963e-05 BETTER
I0326 08:01:26.704530 1469253 finetune.py:68] layer 8_down @ epoch 0 new loss 6.131451664259657e-05 old loss 6.132208363851532e-05 BETTER
I0326 08:01:53.926038 1469949 finetune.py:68] layer 9_down @ epoch 1 new loss 6.662945088464767e-05 old loss 6.663617386948317e-05 BETTER
I0326 08:01:55.865657 1469253 finetune.py:68] layer 8_down @ epoch 1 new loss 6.130809924798086e-05 old loss 6.131451664259657e-05 BETTER
I0326 08:02:21.632653 1469949 finetune.py:68] layer 9_down @ epoch 2 new loss 6.662395753664896e-05 old loss 6.662945088464767e-05 BETTER
I0326 08:02:25.250030 1469253 finetune.py:68] layer 8_down @ epoch 2 new loss 6.130366818979383e-05 old loss 6.130809924798086e-05 BETTER
I0326 08:02:49.265620 1469949 finetune.py:68] layer 9_down @ epoch 3 new loss 6.661944644292817e-05 old loss 6.662395753664896e-05 BETTER
I0326 08:02:54.557129 1469253 finetune.py:68] layer 8_down @ epoch 3 new loss 6.130048132035881e-05 old loss 6.130366818979383e-05 BETTER
I0326 08:03:17.063450 1469949 finetune.py:68] layer 9_down @ epoch 4 new loss 6.661564839305356e-05 old loss 6.661944644292817e-05 BETTER
9_v proxy err 0.012007802724838257 tr(WHW.T) 351.4288024902344
bpp_loss 3.2787158489227295
9_q proxy err 0.0007381910108961165 tr(WHW.T) 25661.96875
bpp_loss 4.0639564990997314
9_k proxy err 0.00026926645659841597 tr(WHW.T) 20954.623046875
bpp_loss 5.059642791748047
9_o proxy err 0.024682581424713135 tr(WHW.T) 783.2474365234375
bpp_loss 3.3624905347824097
9_up proxy err 0.008798019960522652 tr(WHW.T) 8974.6630859375
bpp_loss 3.4349705832345143
9_gate proxy err 0.002093138173222542 tr(WHW.T) 39452.5703125
bpp_loss 3.756009374346052
9_down proxy err 0.012942097149789333 tr(WHW.T) 6303.84765625
bpp_loss 3.433614798954555
I0326 08:03:24.015591 1469253 finetune.py:68] layer 8_down @ epoch 4 new loss 6.129687244538218e-05 old loss 6.130048132035881e-05 BETTER
8_v proxy err 0.016324814409017563 tr(WHW.T) 257.7052307128906
bpp_loss 3.1773582696914673
8_q proxy err 0.000713444547727704 tr(WHW.T) 26607.513671875
bpp_loss 4.051312685012817
8_k proxy err 0.00025347204064019024 tr(WHW.T) 22533.984375
bpp_loss 5.038534164428711
8_o proxy err 0.0261460579931736 tr(WHW.T) 751.9638061523438
bpp_loss 3.3058509826660156
8_up proxy err 0.009258062578737736 tr(WHW.T) 8497.6298828125
bpp_loss 3.425464766366141
8_gate proxy err 0.002203594893217087 tr(WHW.T) 37304.83203125
bpp_loss 3.7415798732212613
8_down proxy err 0.012767679058015347 tr(WHW.T) 6517.2978515625
bpp_loss 3.4320526804242815
I0326 08:04:32.076926 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 63.11225724220276s
I0326 08:04:35.620660 1486189 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 08:04:35.620760 1486189 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 08:04:35.620801 1486189 utils.py:162] NumExpr defaulting to 16 threads.
I0326 08:04:35.939006 1486189 config.py:54] PyTorch version 2.6.0 available.
W0326 08:04:36.127523 1486189 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 08:04:36.659011 1486189 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 08:04:36.662658 1418472 quantize_finetune_llama.py:209] layer 11 gpu 1
I0326 08:04:36.675463 1486189 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 08:04:44.134079 1486189 finetune.py:45] layer 10_v initial loss 1.5661422366974875e-05
W0326 08:04:44.134279 1486189 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 08:05:20.086874 1486189 finetune.py:68] layer 10_v @ epoch 0 new loss 1.0561539056652691e-05 old loss 1.5661422366974875e-05 BETTER
I0326 08:05:38.398718 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 61.28597354888916s
I0326 08:05:42.085053 1487211 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 08:05:42.085161 1487211 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 08:05:42.085202 1487211 utils.py:162] NumExpr defaulting to 16 threads.
I0326 08:05:42.405223 1487211 config.py:54] PyTorch version 2.6.0 available.
W0326 08:05:42.609399 1487211 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 08:05:43.185949 1487211 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 08:05:43.189395 1418472 quantize_finetune_llama.py:209] layer 12 gpu 0
I0326 08:05:43.201681 1487211 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 08:05:50.035263 1487211 finetune.py:45] layer 11_v initial loss 1.4273948181653395e-05
W0326 08:05:50.035656 1487211 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 08:05:57.089214 1486189 finetune.py:68] layer 10_v @ epoch 1 new loss 9.763493835635018e-06 old loss 1.0561539056652691e-05 BETTER
I0326 08:06:23.376860 1487211 finetune.py:68] layer 11_v @ epoch 0 new loss 9.350610525871161e-06 old loss 1.4273948181653395e-05 BETTER
I0326 08:06:34.021483 1486189 finetune.py:68] layer 10_v @ epoch 2 new loss 9.345429134555161e-06 old loss 9.763493835635018e-06 BETTER
I0326 08:06:57.858760 1487211 finetune.py:68] layer 11_v @ epoch 1 new loss 8.626692761026789e-06 old loss 9.350610525871161e-06 BETTER
I0326 08:07:11.153703 1486189 finetune.py:68] layer 10_v @ epoch 3 new loss 9.052445420820732e-06 old loss 9.345429134555161e-06 BETTER
I0326 08:07:32.755018 1487211 finetune.py:68] layer 11_v @ epoch 2 new loss 8.254580279754009e-06 old loss 8.626692761026789e-06 BETTER
I0326 08:07:48.346725 1486189 finetune.py:68] layer 10_v @ epoch 4 new loss 8.835807420837227e-06 old loss 9.052445420820732e-06 BETTER
I0326 08:07:58.273823 1486189 finetune.py:45] layer 10_q initial loss 1.0946031579806004e-05
I0326 08:08:07.740334 1487211 finetune.py:68] layer 11_v @ epoch 3 new loss 8.008171789697371e-06 old loss 8.254580279754009e-06 BETTER
I0326 08:08:34.529120 1486189 finetune.py:68] layer 10_q @ epoch 0 new loss 1.0453003596921917e-05 old loss 1.0946031579806004e-05 BETTER
I0326 08:08:42.838088 1487211 finetune.py:68] layer 11_v @ epoch 4 new loss 7.822593943274114e-06 old loss 8.008171789697371e-06 BETTER
I0326 08:08:52.378521 1487211 finetune.py:45] layer 11_q initial loss 1.025756137096323e-05
I0326 08:09:11.522480 1486189 finetune.py:68] layer 10_q @ epoch 1 new loss 1.0227809980278835e-05 old loss 1.0453003596921917e-05 BETTER
I0326 08:09:26.127985 1487211 finetune.py:68] layer 11_q @ epoch 0 new loss 9.79420747171389e-06 old loss 1.025756137096323e-05 BETTER
I0326 08:09:48.466051 1486189 finetune.py:68] layer 10_q @ epoch 2 new loss 1.0049013326351997e-05 old loss 1.0227809980278835e-05 BETTER
I0326 08:10:00.673037 1487211 finetune.py:68] layer 11_q @ epoch 1 new loss 9.576086085871793e-06 old loss 9.79420747171389e-06 BETTER
I0326 08:10:25.346006 1486189 finetune.py:68] layer 10_q @ epoch 3 new loss 9.909115760819986e-06 old loss 1.0049013326351997e-05 BETTER
I0326 08:10:35.364592 1487211 finetune.py:68] layer 11_q @ epoch 2 new loss 9.4012621048023e-06 old loss 9.576086085871793e-06 BETTER
I0326 08:11:02.181173 1486189 finetune.py:68] layer 10_q @ epoch 4 new loss 9.784418580238707e-06 old loss 9.909115760819986e-06 BETTER
I0326 08:11:09.955417 1487211 finetune.py:68] layer 11_q @ epoch 3 new loss 9.286008207709529e-06 old loss 9.4012621048023e-06 BETTER
I0326 08:11:10.131445 1486189 finetune.py:45] layer 10_k initial loss 1.0284516974934377e-05
I0326 08:11:44.545734 1487211 finetune.py:68] layer 11_q @ epoch 4 new loss 9.18186651688302e-06 old loss 9.286008207709529e-06 BETTER
I0326 08:11:46.332106 1486189 finetune.py:68] layer 10_k @ epoch 0 new loss 1.0136691344087012e-05 old loss 1.0284516974934377e-05 BETTER
I0326 08:11:52.295693 1487211 finetune.py:45] layer 11_k initial loss 9.739961569721345e-06
I0326 08:12:23.148381 1486189 finetune.py:68] layer 10_k @ epoch 1 new loss 1.0038816071755718e-05 old loss 1.0136691344087012e-05 BETTER
I0326 08:12:26.027143 1487211 finetune.py:68] layer 11_k @ epoch 0 new loss 9.553465133649297e-06 old loss 9.739961569721345e-06 BETTER
I0326 08:13:00.007472 1486189 finetune.py:68] layer 10_k @ epoch 2 new loss 9.957638212654274e-06 old loss 1.0038816071755718e-05 BETTER
I0326 08:13:00.443263 1487211 finetune.py:68] layer 11_k @ epoch 1 new loss 9.456118277739733e-06 old loss 9.553465133649297e-06 BETTER
I0326 08:13:34.919161 1487211 finetune.py:68] layer 11_k @ epoch 2 new loss 9.400125236425083e-06 old loss 9.456118277739733e-06 BETTER
I0326 08:13:36.805382 1486189 finetune.py:68] layer 10_k @ epoch 3 new loss 9.88332976703532e-06 old loss 9.957638212654274e-06 BETTER
I0326 08:14:09.320002 1487211 finetune.py:68] layer 11_k @ epoch 3 new loss 9.31623981159646e-06 old loss 9.400125236425083e-06 BETTER
I0326 08:14:13.590363 1486189 finetune.py:68] layer 10_k @ epoch 4 new loss 9.818696526053827e-06 old loss 9.88332976703532e-06 BETTER
I0326 08:14:23.387861 1486189 finetune.py:45] layer 10_o initial loss 3.474069308140315e-05
I0326 08:14:43.669777 1487211 finetune.py:68] layer 11_k @ epoch 4 new loss 9.242759915650822e-06 old loss 9.31623981159646e-06 BETTER
I0326 08:14:53.182930 1487211 finetune.py:45] layer 11_o initial loss 3.840986391878687e-05
I0326 08:14:58.529651 1486189 finetune.py:68] layer 10_o @ epoch 0 new loss 3.139249383821152e-05 old loss 3.474069308140315e-05 BETTER
I0326 08:15:26.097362 1487211 finetune.py:68] layer 11_o @ epoch 0 new loss 3.417272091610357e-05 old loss 3.840986391878687e-05 BETTER
I0326 08:15:34.526244 1486189 finetune.py:68] layer 10_o @ epoch 1 new loss 2.981851685035508e-05 old loss 3.139249383821152e-05 BETTER
I0326 08:15:59.887049 1487211 finetune.py:68] layer 11_o @ epoch 1 new loss 3.22538944601547e-05 old loss 3.417272091610357e-05 BETTER
I0326 08:16:10.292618 1486189 finetune.py:68] layer 10_o @ epoch 2 new loss 2.8759372071363032e-05 old loss 2.981851685035508e-05 BETTER
I0326 08:16:33.564465 1487211 finetune.py:68] layer 11_o @ epoch 2 new loss 3.098700472037308e-05 old loss 3.22538944601547e-05 BETTER
I0326 08:16:46.358146 1486189 finetune.py:68] layer 10_o @ epoch 3 new loss 2.7949326977250166e-05 old loss 2.8759372071363032e-05 BETTER
I0326 08:17:07.391411 1487211 finetune.py:68] layer 11_o @ epoch 3 new loss 3.003980418725405e-05 old loss 3.098700472037308e-05 BETTER
I0326 08:17:22.513835 1486189 finetune.py:68] layer 10_o @ epoch 4 new loss 2.730050437094178e-05 old loss 2.7949326977250166e-05 BETTER
I0326 08:17:41.480700 1487211 finetune.py:68] layer 11_o @ epoch 4 new loss 2.928098001575563e-05 old loss 3.003980418725405e-05 BETTER
I0326 08:17:44.355404 1486189 finetune.py:45] layer 10_up initial loss 4.596056533046067e-05
I0326 08:18:03.160252 1487211 finetune.py:45] layer 11_up initial loss 4.921701111015864e-05
I0326 08:18:16.751930 1486189 finetune.py:68] layer 10_up @ epoch 0 new loss 4.4627984607359394e-05 old loss 4.596056533046067e-05 BETTER
I0326 08:18:33.787886 1487211 finetune.py:68] layer 11_up @ epoch 0 new loss 4.765818448504433e-05 old loss 4.921701111015864e-05 BETTER
I0326 08:18:50.266222 1486189 finetune.py:68] layer 10_up @ epoch 1 new loss 4.3783198634628206e-05 old loss 4.4627984607359394e-05 BETTER
I0326 08:19:05.700391 1487211 finetune.py:68] layer 11_up @ epoch 1 new loss 4.6694309276062995e-05 old loss 4.765818448504433e-05 BETTER
I0326 08:19:24.121310 1486189 finetune.py:68] layer 10_up @ epoch 2 new loss 4.310467920731753e-05 old loss 4.3783198634628206e-05 BETTER
I0326 08:19:37.700107 1487211 finetune.py:68] layer 11_up @ epoch 2 new loss 4.591560718836263e-05 old loss 4.6694309276062995e-05 BETTER
I0326 08:19:58.047203 1486189 finetune.py:68] layer 10_up @ epoch 3 new loss 4.25100552092772e-05 old loss 4.310467920731753e-05 BETTER
I0326 08:20:09.801520 1487211 finetune.py:68] layer 11_up @ epoch 3 new loss 4.5238273742143065e-05 old loss 4.591560718836263e-05 BETTER
I0326 08:20:31.967664 1486189 finetune.py:68] layer 10_up @ epoch 4 new loss 4.198837268631905e-05 old loss 4.25100552092772e-05 BETTER
I0326 08:20:42.031681 1487211 finetune.py:68] layer 11_up @ epoch 4 new loss 4.464442827156745e-05 old loss 4.5238273742143065e-05 BETTER
I0326 08:20:53.847155 1486189 finetune.py:45] layer 10_gate initial loss 4.9208181735593826e-05
I0326 08:21:03.741024 1487211 finetune.py:45] layer 11_gate initial loss 5.245732972980477e-05
I0326 08:21:24.333906 1486189 finetune.py:68] layer 10_gate @ epoch 0 new loss 4.86201279272791e-05 old loss 4.9208181735593826e-05 BETTER
I0326 08:21:32.352275 1487211 finetune.py:68] layer 11_gate @ epoch 0 new loss 5.180330845178105e-05 old loss 5.245732972980477e-05 BETTER
I0326 08:21:55.665890 1486189 finetune.py:68] layer 10_gate @ epoch 1 new loss 4.81723909615539e-05 old loss 4.86201279272791e-05 BETTER
I0326 08:22:01.949798 1487211 finetune.py:68] layer 11_gate @ epoch 1 new loss 5.1301591156516224e-05 old loss 5.180330845178105e-05 BETTER
I0326 08:22:27.258872 1486189 finetune.py:68] layer 10_gate @ epoch 2 new loss 4.7775814891792834e-05 old loss 4.81723909615539e-05 BETTER
I0326 08:22:31.664457 1487211 finetune.py:68] layer 11_gate @ epoch 2 new loss 5.086718738311902e-05 old loss 5.1301591156516224e-05 BETTER
I0326 08:22:58.945323 1486189 finetune.py:68] layer 10_gate @ epoch 3 new loss 4.742103556054644e-05 old loss 4.7775814891792834e-05 BETTER
I0326 08:23:02.072073 1487211 finetune.py:68] layer 11_gate @ epoch 3 new loss 5.046866135671735e-05 old loss 5.086718738311902e-05 BETTER
I0326 08:23:31.044270 1486189 finetune.py:68] layer 10_gate @ epoch 4 new loss 4.7089131840039045e-05 old loss 4.742103556054644e-05 BETTER
I0326 08:23:31.834261 1487211 finetune.py:68] layer 11_gate @ epoch 4 new loss 5.010699533158913e-05 old loss 5.046866135671735e-05 BETTER
I0326 08:23:53.746688 1486189 finetune.py:45] layer 10_down initial loss 7.181982073234394e-05
I0326 08:23:54.308199 1487211 finetune.py:45] layer 11_down initial loss 7.593019836349413e-05
I0326 08:24:21.022582 1487211 finetune.py:68] layer 11_down @ epoch 0 new loss 7.591862959088758e-05 old loss 7.593019836349413e-05 BETTER
I0326 08:24:21.748180 1486189 finetune.py:68] layer 10_down @ epoch 0 new loss 7.180924876593053e-05 old loss 7.181982073234394e-05 BETTER
I0326 08:24:48.711488 1487211 finetune.py:68] layer 11_down @ epoch 1 new loss 7.591026951558888e-05 old loss 7.591862959088758e-05 BETTER
I0326 08:24:50.743978 1486189 finetune.py:68] layer 10_down @ epoch 1 new loss 7.1801790909376e-05 old loss 7.180924876593053e-05 BETTER
I0326 08:25:16.401220 1487211 finetune.py:68] layer 11_down @ epoch 2 new loss 7.590315362904221e-05 old loss 7.591026951558888e-05 BETTER
I0326 08:25:19.939810 1486189 finetune.py:68] layer 10_down @ epoch 2 new loss 7.17956863809377e-05 old loss 7.1801790909376e-05 BETTER
I0326 08:25:44.117464 1487211 finetune.py:68] layer 11_down @ epoch 3 new loss 7.589605229441077e-05 old loss 7.590315362904221e-05 BETTER
I0326 08:25:49.291345 1486189 finetune.py:68] layer 10_down @ epoch 3 new loss 7.178993109846488e-05 old loss 7.17956863809377e-05 BETTER
I0326 08:26:11.821597 1487211 finetune.py:68] layer 11_down @ epoch 4 new loss 7.58904789108783e-05 old loss 7.589605229441077e-05 BETTER
11_v proxy err 0.013019203208386898 tr(WHW.T) 319.5691223144531
bpp_loss 3.172617197036743
11_q proxy err 0.0008333611767739058 tr(WHW.T) 22201.669921875
bpp_loss 4.002202272415161
11_k proxy err 0.0003038606373593211 tr(WHW.T) 18028.123046875
bpp_loss 5.060727119445801
11_o proxy err 0.031568579375743866 tr(WHW.T) 571.0537109375
bpp_loss 3.3179677724838257
11_up proxy err 0.008515489287674427 tr(WHW.T) 9200.5283203125
bpp_loss 3.4567926951817105
11_gate proxy err 0.00220289989374578 tr(WHW.T) 36932.2109375
bpp_loss 3.703681673322405
11_down proxy err 0.012033102102577686 tr(WHW.T) 6691.1015625
bpp_loss 3.4577672140938893
I0326 08:26:18.695849 1486189 finetune.py:68] layer 10_down @ epoch 4 new loss 7.178494706749916e-05 old loss 7.178993109846488e-05 BETTER
10_v proxy err 0.015952985733747482 tr(WHW.T) 251.83889770507812
bpp_loss 3.1676626205444336
10_q proxy err 0.0007724261959083378 tr(WHW.T) 23346.0859375
bpp_loss 4.065527439117432
10_k proxy err 0.00027982014580629766 tr(WHW.T) 19757.8359375
bpp_loss 5.060917615890503
10_o proxy err 0.027655506506562233 tr(WHW.T) 687.6930541992188
bpp_loss 3.2986196279525757
10_up proxy err 0.008656089194118977 tr(WHW.T) 9201.2275390625
bpp_loss 3.4519897188459123
10_gate proxy err 0.002212874125689268 tr(WHW.T) 37450.52734375
bpp_loss 3.72481632232666
10_down proxy err 0.012490804307162762 tr(WHW.T) 6555.20361328125
bpp_loss 3.450423036302839
I0326 08:27:26.756504 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 63.02720332145691s
I0326 08:27:30.332354 1504349 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 08:27:30.332454 1504349 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 08:27:30.332492 1504349 utils.py:162] NumExpr defaulting to 16 threads.
I0326 08:27:30.656516 1504349 config.py:54] PyTorch version 2.6.0 available.
W0326 08:27:30.845085 1504349 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 08:27:31.409290 1504349 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 08:27:31.412832 1418472 quantize_finetune_llama.py:209] layer 13 gpu 1
I0326 08:27:31.425541 1504349 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 08:27:38.678209 1504349 finetune.py:45] layer 12_v initial loss 1.5359844837803394e-05
W0326 08:27:38.678428 1504349 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 08:28:14.403095 1504349 finetune.py:68] layer 12_v @ epoch 0 new loss 1.0228020983049646e-05 old loss 1.5359844837803394e-05 BETTER
I0326 08:28:33.459307 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 61.6092095375061s
I0326 08:28:37.114066 1505300 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 08:28:37.114166 1505300 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 08:28:37.114204 1505300 utils.py:162] NumExpr defaulting to 16 threads.
I0326 08:28:37.458096 1505300 config.py:54] PyTorch version 2.6.0 available.
W0326 08:28:37.646293 1505300 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 08:28:38.206974 1505300 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 08:28:38.210453 1418472 quantize_finetune_llama.py:209] layer 14 gpu 0
I0326 08:28:38.222928 1505300 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 08:28:45.096295 1505300 finetune.py:45] layer 13_v initial loss 1.673100450716447e-05
W0326 08:28:45.096505 1505300 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 08:28:51.434075 1504349 finetune.py:68] layer 12_v @ epoch 1 new loss 9.427008990314789e-06 old loss 1.0228020983049646e-05 BETTER
I0326 08:29:18.377607 1505300 finetune.py:68] layer 13_v @ epoch 0 new loss 1.1253090633545071e-05 old loss 1.673100450716447e-05 BETTER
I0326 08:29:28.362321 1504349 finetune.py:68] layer 12_v @ epoch 2 new loss 8.967812391347252e-06 old loss 9.427008990314789e-06 BETTER
I0326 08:29:52.735749 1505300 finetune.py:68] layer 13_v @ epoch 1 new loss 1.0379689229012001e-05 old loss 1.1253090633545071e-05 BETTER
I0326 08:30:05.366810 1504349 finetune.py:68] layer 12_v @ epoch 3 new loss 8.65843230712926e-06 old loss 8.967812391347252e-06 BETTER
I0326 08:30:27.457195 1505300 finetune.py:68] layer 13_v @ epoch 2 new loss 9.906648301694077e-06 old loss 1.0379689229012001e-05 BETTER
I0326 08:30:42.332822 1504349 finetune.py:68] layer 12_v @ epoch 4 new loss 8.455445822619367e-06 old loss 8.65843230712926e-06 BETTER
I0326 08:30:52.194020 1504349 finetune.py:45] layer 12_q initial loss 1.0361532986280508e-05
I0326 08:31:02.313991 1505300 finetune.py:68] layer 13_v @ epoch 3 new loss 9.591124580765609e-06 old loss 9.906648301694077e-06 BETTER
I0326 08:31:28.234353 1504349 finetune.py:68] layer 12_q @ epoch 0 new loss 9.81508674158249e-06 old loss 1.0361532986280508e-05 BETTER
I0326 08:31:37.450884 1505300 finetune.py:68] layer 13_v @ epoch 4 new loss 9.372967724630143e-06 old loss 9.591124580765609e-06 BETTER
I0326 08:31:46.938051 1505300 finetune.py:45] layer 13_q initial loss 1.1657301911327522e-05
I0326 08:32:05.170095 1504349 finetune.py:68] layer 12_q @ epoch 1 new loss 9.568924724590033e-06 old loss 9.81508674158249e-06 BETTER
I0326 08:32:20.684859 1505300 finetune.py:68] layer 13_q @ epoch 0 new loss 1.1195962542842608e-05 old loss 1.1657301911327522e-05 BETTER
I0326 08:32:42.210128 1504349 finetune.py:68] layer 12_q @ epoch 2 new loss 9.387603313371073e-06 old loss 9.568924724590033e-06 BETTER
I0326 08:32:55.540246 1505300 finetune.py:68] layer 13_q @ epoch 1 new loss 1.0974928045470733e-05 old loss 1.1195962542842608e-05 BETTER
I0326 08:33:19.128720 1504349 finetune.py:68] layer 12_q @ epoch 3 new loss 9.233568562194705e-06 old loss 9.387603313371073e-06 BETTER
I0326 08:33:30.031068 1505300 finetune.py:68] layer 13_q @ epoch 2 new loss 1.0802431461343076e-05 old loss 1.0974928045470733e-05 BETTER
I0326 08:33:56.022760 1504349 finetune.py:68] layer 12_q @ epoch 4 new loss 9.12163704924751e-06 old loss 9.233568562194705e-06 BETTER
I0326 08:34:04.092578 1504349 finetune.py:45] layer 12_k initial loss 9.82020083029056e-06
I0326 08:34:04.709339 1505300 finetune.py:68] layer 13_q @ epoch 3 new loss 1.0668100003385916e-05 old loss 1.0802431461343076e-05 BETTER
I0326 08:34:39.459666 1505300 finetune.py:68] layer 13_q @ epoch 4 new loss 1.05442040876369e-05 old loss 1.0668100003385916e-05 BETTER
I0326 08:34:40.351484 1504349 finetune.py:68] layer 12_k @ epoch 0 new loss 9.59417775447946e-06 old loss 9.82020083029056e-06 BETTER
I0326 08:34:47.221722 1505300 finetune.py:45] layer 13_k initial loss 1.1103304132120684e-05
I0326 08:35:17.157705 1504349 finetune.py:68] layer 12_k @ epoch 1 new loss 9.479356776864734e-06 old loss 9.59417775447946e-06 BETTER
I0326 08:35:21.093199 1505300 finetune.py:68] layer 13_k @ epoch 0 new loss 1.0968364222208038e-05 old loss 1.1103304132120684e-05 BETTER
I0326 08:35:54.160765 1504349 finetune.py:68] layer 12_k @ epoch 2 new loss 9.394682820129674e-06 old loss 9.479356776864734e-06 BETTER
I0326 08:35:55.634816 1505300 finetune.py:68] layer 13_k @ epoch 1 new loss 1.0869356628973037e-05 old loss 1.0968364222208038e-05 BETTER
I0326 08:36:30.202676 1505300 finetune.py:68] layer 13_k @ epoch 2 new loss 1.0788746294565499e-05 old loss 1.0869356628973037e-05 BETTER
I0326 08:36:31.401413 1504349 finetune.py:68] layer 12_k @ epoch 3 new loss 9.308819244324695e-06 old loss 9.394682820129674e-06 BETTER
I0326 08:37:04.889735 1505300 finetune.py:68] layer 13_k @ epoch 3 new loss 1.0712955372582655e-05 old loss 1.0788746294565499e-05 BETTER
I0326 08:37:08.575849 1504349 finetune.py:68] layer 12_k @ epoch 4 new loss 9.235781362804119e-06 old loss 9.308819244324695e-06 BETTER
I0326 08:37:18.316241 1504349 finetune.py:45] layer 12_o initial loss 3.663433017209172e-05
I0326 08:37:39.722745 1505300 finetune.py:68] layer 13_k @ epoch 4 new loss 1.0652855053194799e-05 old loss 1.0712955372582655e-05 BETTER
I0326 08:37:49.230530 1505300 finetune.py:45] layer 13_o initial loss 4.9478388973511755e-05
I0326 08:37:54.008524 1504349 finetune.py:68] layer 12_o @ epoch 0 new loss 3.29317626892589e-05 old loss 3.663433017209172e-05 BETTER
I0326 08:38:22.533598 1505300 finetune.py:68] layer 13_o @ epoch 0 new loss 4.351393727120012e-05 old loss 4.9478388973511755e-05 BETTER
I0326 08:38:30.331584 1504349 finetune.py:68] layer 12_o @ epoch 1 new loss 3.1237890652846545e-05 old loss 3.29317626892589e-05 BETTER
I0326 08:38:56.570600 1505300 finetune.py:68] layer 13_o @ epoch 1 new loss 4.0814760723151267e-05 old loss 4.351393727120012e-05 BETTER
I0326 08:39:06.792863 1504349 finetune.py:68] layer 12_o @ epoch 2 new loss 3.0055092793190852e-05 old loss 3.1237890652846545e-05 BETTER
I0326 08:39:30.825831 1505300 finetune.py:68] layer 13_o @ epoch 2 new loss 3.8997666706563905e-05 old loss 4.0814760723151267e-05 BETTER
I0326 08:39:43.170665 1504349 finetune.py:68] layer 12_o @ epoch 3 new loss 2.914978722401429e-05 old loss 3.0055092793190852e-05 BETTER
I0326 08:40:04.857043 1505300 finetune.py:68] layer 13_o @ epoch 3 new loss 3.7638623325619847e-05 old loss 3.8997666706563905e-05 BETTER
I0326 08:40:19.349731 1504349 finetune.py:68] layer 12_o @ epoch 4 new loss 2.8410682716639712e-05 old loss 2.914978722401429e-05 BETTER
I0326 08:40:38.867853 1505300 finetune.py:68] layer 13_o @ epoch 4 new loss 3.6562349123414606e-05 old loss 3.7638623325619847e-05 BETTER
I0326 08:40:41.168218 1504349 finetune.py:45] layer 12_up initial loss 4.9106260121334344e-05
I0326 08:41:00.355916 1505300 finetune.py:45] layer 13_up initial loss 6.073738040868193e-05
I0326 08:41:13.755318 1504349 finetune.py:68] layer 12_up @ epoch 0 new loss 4.7356774302897975e-05 old loss 4.9106260121334344e-05 BETTER
I0326 08:41:31.373046 1505300 finetune.py:68] layer 13_up @ epoch 0 new loss 5.8482397435000166e-05 old loss 6.073738040868193e-05 BETTER
I0326 08:41:47.306799 1504349 finetune.py:68] layer 12_up @ epoch 1 new loss 4.631056799553335e-05 old loss 4.7356774302897975e-05 BETTER
I0326 08:42:03.408879 1505300 finetune.py:68] layer 13_up @ epoch 1 new loss 5.714508370147087e-05 old loss 5.8482397435000166e-05 BETTER
I0326 08:42:21.075902 1504349 finetune.py:68] layer 12_up @ epoch 2 new loss 4.546184209175408e-05 old loss 4.631056799553335e-05 BETTER
I0326 08:42:35.459642 1505300 finetune.py:68] layer 13_up @ epoch 2 new loss 5.60686930839438e-05 old loss 5.714508370147087e-05 BETTER
I0326 08:42:54.934581 1504349 finetune.py:68] layer 12_up @ epoch 3 new loss 4.4739590521203354e-05 old loss 4.546184209175408e-05 BETTER
I0326 08:43:07.629949 1505300 finetune.py:68] layer 13_up @ epoch 3 new loss 5.514633085113019e-05 old loss 5.60686930839438e-05 BETTER
I0326 08:43:28.751552 1504349 finetune.py:68] layer 12_up @ epoch 4 new loss 4.4103206164436415e-05 old loss 4.4739590521203354e-05 BETTER
I0326 08:43:39.697732 1505300 finetune.py:68] layer 13_up @ epoch 4 new loss 5.4344320233212784e-05 old loss 5.514633085113019e-05 BETTER
I0326 08:43:50.798009 1504349 finetune.py:45] layer 12_gate initial loss 5.297333700582385e-05
I0326 08:44:01.423996 1505300 finetune.py:45] layer 13_gate initial loss 6.427224434446543e-05
I0326 08:44:21.779044 1504349 finetune.py:68] layer 12_gate @ epoch 0 new loss 5.22104419360403e-05 old loss 5.297333700582385e-05 BETTER
I0326 08:44:30.055258 1505300 finetune.py:68] layer 13_gate @ epoch 0 new loss 6.335860234685242e-05 old loss 6.427224434446543e-05 BETTER
I0326 08:44:52.988613 1504349 finetune.py:68] layer 12_gate @ epoch 1 new loss 5.1646562496898696e-05 old loss 5.22104419360403e-05 BETTER
I0326 08:44:59.601144 1505300 finetune.py:68] layer 13_gate @ epoch 1 new loss 6.266076525207609e-05 old loss 6.335860234685242e-05 BETTER
I0326 08:45:24.423491 1504349 finetune.py:68] layer 12_gate @ epoch 2 new loss 5.115929525345564e-05 old loss 5.1646562496898696e-05 BETTER
I0326 08:45:29.181680 1505300 finetune.py:68] layer 13_gate @ epoch 2 new loss 6.205253157531843e-05 old loss 6.266076525207609e-05 BETTER
I0326 08:45:55.951942 1504349 finetune.py:68] layer 12_gate @ epoch 3 new loss 5.071729901828803e-05 old loss 5.115929525345564e-05 BETTER
I0326 08:45:59.012160 1505300 finetune.py:68] layer 13_gate @ epoch 3 new loss 6.150306580821052e-05 old loss 6.205253157531843e-05 BETTER
I0326 08:46:27.480471 1504349 finetune.py:68] layer 12_gate @ epoch 4 new loss 5.0316819397266954e-05 old loss 5.071729901828803e-05 BETTER
I0326 08:46:28.761047 1505300 finetune.py:68] layer 13_gate @ epoch 4 new loss 6.1012247897451743e-05 old loss 6.150306580821052e-05 BETTER
I0326 08:46:50.206010 1504349 finetune.py:45] layer 12_down initial loss 7.786018977640197e-05
I0326 08:46:51.286082 1505300 finetune.py:45] layer 13_down initial loss 9.41264588618651e-05
I0326 08:47:17.736903 1505300 finetune.py:68] layer 13_down @ epoch 0 new loss 9.411282371729612e-05 old loss 9.41264588618651e-05 BETTER
I0326 08:47:17.934505 1504349 finetune.py:68] layer 12_down @ epoch 0 new loss 7.785010529914871e-05 old loss 7.786018977640197e-05 BETTER
I0326 08:47:45.283005 1505300 finetune.py:68] layer 13_down @ epoch 1 new loss 9.410261554876342e-05 old loss 9.411282371729612e-05 BETTER
I0326 08:47:46.764707 1504349 finetune.py:68] layer 12_down @ epoch 1 new loss 7.78422036091797e-05 old loss 7.785010529914871e-05 BETTER
I0326 08:48:13.001343 1505300 finetune.py:68] layer 13_down @ epoch 2 new loss 9.409433550899848e-05 old loss 9.410261554876342e-05 BETTER
I0326 08:48:15.826564 1504349 finetune.py:68] layer 12_down @ epoch 2 new loss 7.783472392475232e-05 old loss 7.78422036091797e-05 BETTER
I0326 08:48:40.716977 1505300 finetune.py:68] layer 13_down @ epoch 3 new loss 9.408609912497923e-05 old loss 9.409433550899848e-05 BETTER
I0326 08:48:44.910143 1504349 finetune.py:68] layer 12_down @ epoch 3 new loss 7.782920874888077e-05 old loss 7.783472392475232e-05 BETTER
I0326 08:49:08.447832 1505300 finetune.py:68] layer 13_down @ epoch 4 new loss 9.408043115399778e-05 old loss 9.408609912497923e-05 BETTER
13_v proxy err 0.014822857454419136 tr(WHW.T) 280.153564453125
bpp_loss 3.2312371730804443
13_q proxy err 0.0008754286100156605 tr(WHW.T) 20902.72265625
bpp_loss 4.036683797836304
13_k proxy err 0.00030923160375095904 tr(WHW.T) 17803.13671875
bpp_loss 5.074627637863159
13_o proxy err 0.02782292477786541 tr(WHW.T) 679.9859008789062
bpp_loss 3.3463926315307617
13_up proxy err 0.00766040850430727 tr(WHW.T) 10018.0234375
bpp_loss 3.4800402777535573
13_gate proxy err 0.0020397985354065895 tr(WHW.T) 38958.47265625
bpp_loss 3.688772337777274
13_down proxy err 0.011964432895183563 tr(WHW.T) 6594.9970703125
bpp_loss 3.470076867512294
I0326 08:49:14.222911 1504349 finetune.py:68] layer 12_down @ epoch 4 new loss 7.78241956140846e-05 old loss 7.782920874888077e-05 BETTER
12_v proxy err 0.012057974003255367 tr(WHW.T) 363.6233825683594
bpp_loss 3.291145920753479
12_q proxy err 0.0005806816043332219 tr(WHW.T) 34118.27734375
bpp_loss 4.061237573623657
12_k proxy err 0.00024624759680591524 tr(WHW.T) 23070.939453125
bpp_loss 5.057183265686035
12_o proxy err 0.024705111980438232 tr(WHW.T) 785.8966674804688
bpp_loss 3.3638460636138916
12_up proxy err 0.0076360139064490795 tr(WHW.T) 10031.4560546875
bpp_loss 3.4766107286725725
12_gate proxy err 0.002125188708305359 tr(WHW.T) 37379.26953125
bpp_loss 3.6840278080531528
12_down proxy err 0.011559599079191685 tr(WHW.T) 6853.38623046875
bpp_loss 3.4704275812421526
I0326 08:50:15.167454 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 55.93619084358215s
I0326 08:50:18.526988 1522252 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 08:50:18.527081 1522252 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 08:50:18.527120 1522252 utils.py:162] NumExpr defaulting to 16 threads.
I0326 08:50:18.851781 1522252 config.py:54] PyTorch version 2.6.0 available.
W0326 08:50:19.042539 1522252 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 08:50:19.595739 1522252 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 08:50:19.599342 1418472 quantize_finetune_llama.py:209] layer 15 gpu 1
I0326 08:50:19.612095 1522252 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 08:50:27.046856 1522252 finetune.py:45] layer 14_v initial loss 1.7211232261615805e-05
W0326 08:50:27.047088 1522252 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 08:51:03.279221 1522252 finetune.py:68] layer 14_v @ epoch 0 new loss 1.2181486454210244e-05 old loss 1.7211232261615805e-05 BETTER
I0326 08:51:20.704520 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 60.677000761032104s
I0326 08:51:24.257445 1523174 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 08:51:24.257539 1523174 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 08:51:24.257581 1523174 utils.py:162] NumExpr defaulting to 16 threads.
I0326 08:51:24.583780 1523174 config.py:54] PyTorch version 2.6.0 available.
W0326 08:51:24.778460 1523174 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 08:51:25.343158 1523174 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 08:51:25.346821 1418472 quantize_finetune_llama.py:209] layer 16 gpu 0
I0326 08:51:25.359290 1523174 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 08:51:32.253569 1523174 finetune.py:45] layer 15_v initial loss 1.9045848603127524e-05
W0326 08:51:32.253779 1523174 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 08:51:40.382894 1522252 finetune.py:68] layer 14_v @ epoch 1 new loss 1.1308181456115562e-05 old loss 1.2181486454210244e-05 BETTER
I0326 08:52:05.794573 1523174 finetune.py:68] layer 15_v @ epoch 0 new loss 1.3084607417113148e-05 old loss 1.9045848603127524e-05 BETTER
I0326 08:52:17.609819 1522252 finetune.py:68] layer 14_v @ epoch 2 new loss 1.0816199392138515e-05 old loss 1.1308181456115562e-05 BETTER
I0326 08:52:40.383851 1523174 finetune.py:68] layer 15_v @ epoch 1 new loss 1.2144748325226828e-05 old loss 1.3084607417113148e-05 BETTER
I0326 08:52:55.068025 1522252 finetune.py:68] layer 14_v @ epoch 3 new loss 1.0484427548362873e-05 old loss 1.0816199392138515e-05 BETTER
I0326 08:53:15.423257 1523174 finetune.py:68] layer 15_v @ epoch 2 new loss 1.1626137165876571e-05 old loss 1.2144748325226828e-05 BETTER
I0326 08:53:32.726085 1522252 finetune.py:68] layer 14_v @ epoch 4 new loss 1.0234401997877285e-05 old loss 1.0484427548362873e-05 BETTER
I0326 08:53:42.579159 1522252 finetune.py:45] layer 14_q initial loss 1.3398410374065861e-05
I0326 08:53:50.595148 1523174 finetune.py:68] layer 15_v @ epoch 3 new loss 1.127299947256688e-05 old loss 1.1626137165876571e-05 BETTER
I0326 08:54:18.947552 1522252 finetune.py:68] layer 14_q @ epoch 0 new loss 1.2730123671644833e-05 old loss 1.3398410374065861e-05 BETTER
I0326 08:54:25.802967 1523174 finetune.py:68] layer 15_v @ epoch 4 new loss 1.1011814422090538e-05 old loss 1.127299947256688e-05 BETTER
I0326 08:54:35.347847 1523174 finetune.py:45] layer 15_q initial loss 1.2867128134530503e-05
I0326 08:54:56.006765 1522252 finetune.py:68] layer 14_q @ epoch 1 new loss 1.2439175407052971e-05 old loss 1.2730123671644833e-05 BETTER
I0326 08:55:09.353645 1523174 finetune.py:68] layer 15_q @ epoch 0 new loss 1.243427595909452e-05 old loss 1.2867128134530503e-05 BETTER
I0326 08:55:33.279856 1522252 finetune.py:68] layer 14_q @ epoch 2 new loss 1.2236542715982068e-05 old loss 1.2439175407052971e-05 BETTER
I0326 08:55:44.111158 1523174 finetune.py:68] layer 15_q @ epoch 1 new loss 1.2193822840345092e-05 old loss 1.243427595909452e-05 BETTER
I0326 08:56:10.840578 1522252 finetune.py:68] layer 14_q @ epoch 3 new loss 1.2057679668942e-05 old loss 1.2236542715982068e-05 BETTER
I0326 08:56:18.874473 1523174 finetune.py:68] layer 15_q @ epoch 2 new loss 1.2006914403173141e-05 old loss 1.2193822840345092e-05 BETTER
I0326 08:56:47.959557 1522252 finetune.py:68] layer 14_q @ epoch 4 new loss 1.1908861779375002e-05 old loss 1.2057679668942e-05 BETTER
I0326 08:56:53.577425 1523174 finetune.py:68] layer 15_q @ epoch 3 new loss 1.1853660907945596e-05 old loss 1.2006914403173141e-05 BETTER
I0326 08:56:56.214959 1522252 finetune.py:45] layer 14_k initial loss 1.283961955778068e-05
I0326 08:57:28.367687 1523174 finetune.py:68] layer 15_q @ epoch 4 new loss 1.1743280992959626e-05 old loss 1.1853660907945596e-05 BETTER
I0326 08:57:32.585577 1522252 finetune.py:68] layer 14_k @ epoch 0 new loss 1.2626954230654519e-05 old loss 1.283961955778068e-05 BETTER
I0326 08:57:36.418975 1523174 finetune.py:45] layer 15_k initial loss 1.2451891052478459e-05
I0326 08:58:09.741371 1522252 finetune.py:68] layer 14_k @ epoch 1 new loss 1.2502218851295765e-05 old loss 1.2626954230654519e-05 BETTER
I0326 08:58:11.057878 1523174 finetune.py:68] layer 15_k @ epoch 0 new loss 1.226675885845907e-05 old loss 1.2451891052478459e-05 BETTER
I0326 08:58:45.539249 1523174 finetune.py:68] layer 15_k @ epoch 1 new loss 1.2159422112745233e-05 old loss 1.226675885845907e-05 BETTER
I0326 08:58:46.818992 1522252 finetune.py:68] layer 14_k @ epoch 2 new loss 1.2401405001583043e-05 old loss 1.2502218851295765e-05 BETTER
I0326 08:59:20.060651 1523174 finetune.py:68] layer 15_k @ epoch 2 new loss 1.2059640539519023e-05 old loss 1.2159422112745233e-05 BETTER
I0326 08:59:23.681561 1522252 finetune.py:68] layer 14_k @ epoch 3 new loss 1.231046007887926e-05 old loss 1.2401405001583043e-05 BETTER
I0326 08:59:54.483079 1523174 finetune.py:68] layer 15_k @ epoch 3 new loss 1.199575126520358e-05 old loss 1.2059640539519023e-05 BETTER
I0326 09:00:00.678559 1522252 finetune.py:68] layer 14_k @ epoch 4 new loss 1.2242409866303205e-05 old loss 1.231046007887926e-05 BETTER
I0326 09:00:10.616546 1522252 finetune.py:45] layer 14_o initial loss 5.2411738579394296e-05
I0326 09:00:28.951099 1523174 finetune.py:68] layer 15_k @ epoch 4 new loss 1.192708259623032e-05 old loss 1.199575126520358e-05 BETTER
I0326 09:00:38.607900 1523174 finetune.py:45] layer 15_o initial loss 4.502663796301931e-05
I0326 09:00:45.891501 1522252 finetune.py:68] layer 14_o @ epoch 0 new loss 4.65970479126554e-05 old loss 5.2411738579394296e-05 BETTER
I0326 09:01:11.668491 1523174 finetune.py:68] layer 15_o @ epoch 0 new loss 4.032775905216113e-05 old loss 4.502663796301931e-05 BETTER
I0326 09:01:22.197124 1522252 finetune.py:68] layer 14_o @ epoch 1 new loss 4.3918935261899605e-05 old loss 4.65970479126554e-05 BETTER
I0326 09:01:45.658995 1523174 finetune.py:68] layer 15_o @ epoch 1 new loss 3.820209894911386e-05 old loss 4.032775905216113e-05 BETTER
I0326 09:01:58.468970 1522252 finetune.py:68] layer 14_o @ epoch 2 new loss 4.209206599625759e-05 old loss 4.3918935261899605e-05 BETTER
I0326 09:02:19.601150 1523174 finetune.py:68] layer 15_o @ epoch 2 new loss 3.6729172279592603e-05 old loss 3.820209894911386e-05 BETTER
I0326 09:02:34.660569 1522252 finetune.py:68] layer 14_o @ epoch 3 new loss 4.0712839108891785e-05 old loss 4.209206599625759e-05 BETTER
I0326 09:02:53.634102 1523174 finetune.py:68] layer 15_o @ epoch 3 new loss 3.561634730431251e-05 old loss 3.6729172279592603e-05 BETTER
I0326 09:03:10.984647 1522252 finetune.py:68] layer 14_o @ epoch 4 new loss 3.9616399590158835e-05 old loss 4.0712839108891785e-05 BETTER
I0326 09:03:27.401782 1523174 finetune.py:68] layer 15_o @ epoch 4 new loss 3.471489253570326e-05 old loss 3.561634730431251e-05 BETTER
I0326 09:03:33.330200 1522252 finetune.py:45] layer 14_up initial loss 6.87210849719122e-05
I0326 09:03:49.781620 1523174 finetune.py:45] layer 15_up initial loss 6.974991993047297e-05
I0326 09:04:05.571627 1522252 finetune.py:68] layer 14_up @ epoch 0 new loss 6.599196785828099e-05 old loss 6.87210849719122e-05 BETTER
I0326 09:04:20.547506 1523174 finetune.py:68] layer 15_up @ epoch 0 new loss 6.648735870840028e-05 old loss 6.974991993047297e-05 BETTER
I0326 09:04:39.166644 1522252 finetune.py:68] layer 14_up @ epoch 1 new loss 6.440479774028063e-05 old loss 6.599196785828099e-05 BETTER
I0326 09:04:52.549886 1523174 finetune.py:68] layer 15_up @ epoch 1 new loss 6.468177889473736e-05 old loss 6.648735870840028e-05 BETTER
I0326 09:05:12.785576 1522252 finetune.py:68] layer 14_up @ epoch 2 new loss 6.314716301858425e-05 old loss 6.440479774028063e-05 BETTER
I0326 09:05:24.642267 1523174 finetune.py:68] layer 15_up @ epoch 2 new loss 6.326819129753858e-05 old loss 6.468177889473736e-05 BETTER
I0326 09:05:46.589399 1522252 finetune.py:68] layer 14_up @ epoch 3 new loss 6.208533886820078e-05 old loss 6.314716301858425e-05 BETTER
I0326 09:05:56.821136 1523174 finetune.py:68] layer 15_up @ epoch 3 new loss 6.208562990650535e-05 old loss 6.326819129753858e-05 BETTER
I0326 09:06:20.436856 1522252 finetune.py:68] layer 14_up @ epoch 4 new loss 6.115840369602665e-05 old loss 6.208533886820078e-05 BETTER
I0326 09:06:29.000955 1523174 finetune.py:68] layer 15_up @ epoch 4 new loss 6.105698412284255e-05 old loss 6.208562990650535e-05 BETTER
I0326 09:06:42.789495 1522252 finetune.py:45] layer 14_gate initial loss 7.182137778727338e-05
I0326 09:06:51.251095 1523174 finetune.py:45] layer 15_gate initial loss 7.278375414898619e-05
I0326 09:07:13.135829 1522252 finetune.py:68] layer 14_gate @ epoch 0 new loss 7.076455221977085e-05 old loss 7.182137778727338e-05 BETTER
I0326 09:07:19.897026 1523174 finetune.py:68] layer 15_gate @ epoch 0 new loss 7.159226515796036e-05 old loss 7.278375414898619e-05 BETTER
I0326 09:07:44.397929 1522252 finetune.py:68] layer 14_gate @ epoch 1 new loss 6.996484444243833e-05 old loss 7.076455221977085e-05 BETTER
I0326 09:07:49.422213 1523174 finetune.py:68] layer 15_gate @ epoch 1 new loss 7.066799298627302e-05 old loss 7.159226515796036e-05 BETTER
I0326 09:08:15.974663 1522252 finetune.py:68] layer 14_gate @ epoch 2 new loss 6.927312642801553e-05 old loss 6.996484444243833e-05 BETTER
I0326 09:08:19.184360 1523174 finetune.py:68] layer 15_gate @ epoch 2 new loss 6.987484084675089e-05 old loss 7.066799298627302e-05 BETTER
I0326 09:08:47.744027 1522252 finetune.py:68] layer 14_gate @ epoch 3 new loss 6.865226168883964e-05 old loss 6.927312642801553e-05 BETTER
I0326 09:08:49.166362 1523174 finetune.py:68] layer 15_gate @ epoch 3 new loss 6.917103746673092e-05 old loss 6.987484084675089e-05 BETTER
I0326 09:09:19.586795 1522252 finetune.py:68] layer 14_gate @ epoch 4 new loss 6.808988109696656e-05 old loss 6.865226168883964e-05 BETTER
I0326 09:09:20.467765 1523174 finetune.py:68] layer 15_gate @ epoch 4 new loss 6.854056846350431e-05 old loss 6.917103746673092e-05 BETTER
I0326 09:09:42.941745 1522252 finetune.py:45] layer 14_down initial loss 0.00010668732284102589
I0326 09:09:43.935891 1523174 finetune.py:45] layer 15_down initial loss 0.00011743378126993775
I0326 09:10:10.446568 1523174 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0001174190838355571 old loss 0.00011743378126993775 BETTER
I0326 09:10:10.545992 1522252 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0001066699405782856 old loss 0.00010668732284102589 BETTER
I0326 09:10:38.091365 1523174 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00011740782065317035 old loss 0.0001174190838355571 BETTER
I0326 09:10:39.295926 1522252 finetune.py:68] layer 14_down @ epoch 1 new loss 0.00010665604349924251 old loss 0.0001066699405782856 BETTER
I0326 09:11:05.845560 1523174 finetune.py:68] layer 15_down @ epoch 2 new loss 0.00011739827459678054 old loss 0.00011740782065317035 BETTER
I0326 09:11:08.287590 1522252 finetune.py:68] layer 14_down @ epoch 2 new loss 0.00010664405999705195 old loss 0.00010665604349924251 BETTER
I0326 09:11:33.668440 1523174 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0001173897398984991 old loss 0.00011739827459678054 BETTER
I0326 09:11:37.580404 1522252 finetune.py:68] layer 14_down @ epoch 3 new loss 0.00010663326975191012 old loss 0.00010664405999705195 BETTER
I0326 09:12:01.550339 1523174 finetune.py:68] layer 15_down @ epoch 4 new loss 0.00011738321336451918 old loss 0.0001173897398984991 BETTER
15_v proxy err 0.015088754706084728 tr(WHW.T) 284.0271301269531
bpp_loss 3.287685513496399
15_q proxy err 0.000685372855514288 tr(WHW.T) 28086.8046875
bpp_loss 4.1460466384887695
15_k proxy err 0.00029707045177929103 tr(WHW.T) 18874.1640625
bpp_loss 5.066547870635986
15_o proxy err 0.024838266894221306 tr(WHW.T) 830.3876953125
bpp_loss 3.3685067892074585
15_up proxy err 0.008545543998479843 tr(WHW.T) 9000.7783203125
bpp_loss 3.4667600904192244
15_gate proxy err 0.0017373842420056462 tr(WHW.T) 46196.51171875
bpp_loss 3.7584920610700334
15_down proxy err 0.012367986142635345 tr(WHW.T) 6436.9296875
bpp_loss 3.460219757897513
I0326 09:12:06.769189 1522252 finetune.py:68] layer 14_down @ epoch 4 new loss 0.00010662466229405254 old loss 0.00010663326975191012 BETTER
14_v proxy err 0.014259537681937218 tr(WHW.T) 281.3382873535156
bpp_loss 3.223863363265991
14_q proxy err 0.0008502139244228601 tr(WHW.T) 20888.255859375
bpp_loss 4.003415822982788
14_k proxy err 0.0002903678978327662 tr(WHW.T) 18625.314453125
bpp_loss 5.01973032951355
14_o proxy err 0.028766846284270287 tr(WHW.T) 690.725830078125
bpp_loss 3.337791085243225
14_up proxy err 0.008317044004797935 tr(WHW.T) 9173.6591796875
bpp_loss 3.474534170968192
14_gate proxy err 0.0018932503880932927 tr(WHW.T) 41859.2734375
bpp_loss 3.7206625257219588
14_down proxy err 0.01223068218678236 tr(WHW.T) 6425.34521484375
bpp_loss 3.465884038380214
I0326 09:13:15.792073 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 63.97015738487244s
I0326 09:13:19.265777 1536852 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 09:13:19.265869 1536852 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 09:13:19.265908 1536852 utils.py:162] NumExpr defaulting to 16 threads.
I0326 09:13:19.594856 1536852 config.py:54] PyTorch version 2.6.0 available.
W0326 09:13:19.784854 1536852 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 09:13:20.400896 1536852 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 09:13:20.404754 1418472 quantize_finetune_llama.py:209] layer 17 gpu 1
I0326 09:13:20.419089 1536852 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 09:13:28.314419 1536852 finetune.py:45] layer 16_v initial loss 2.1335534256650135e-05
W0326 09:13:28.314787 1536852 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 09:14:04.053579 1536852 finetune.py:68] layer 16_v @ epoch 0 new loss 1.4427174392039888e-05 old loss 2.1335534256650135e-05 BETTER
I0326 09:14:22.340507 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 61.515018701553345s
I0326 09:14:26.020180 1537554 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 09:14:26.020283 1537554 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 09:14:26.020326 1537554 utils.py:162] NumExpr defaulting to 16 threads.
I0326 09:14:26.360853 1537554 config.py:54] PyTorch version 2.6.0 available.
W0326 09:14:26.568095 1537554 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 09:14:27.140321 1537554 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 09:14:27.144511 1418472 quantize_finetune_llama.py:209] layer 18 gpu 0
I0326 09:14:27.156938 1537554 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 09:14:34.056600 1537554 finetune.py:45] layer 17_v initial loss 1.9817032807623036e-05
W0326 09:14:34.056794 1537554 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 09:14:41.008219 1536852 finetune.py:68] layer 16_v @ epoch 1 new loss 1.3274577213451266e-05 old loss 1.4427174392039888e-05 BETTER
I0326 09:15:07.612836 1537554 finetune.py:68] layer 17_v @ epoch 0 new loss 1.269687527383212e-05 old loss 1.9817032807623036e-05 BETTER
I0326 09:15:18.244181 1536852 finetune.py:68] layer 16_v @ epoch 2 new loss 1.2617640095413662e-05 old loss 1.3274577213451266e-05 BETTER
I0326 09:15:42.261450 1537554 finetune.py:68] layer 17_v @ epoch 1 new loss 1.1697583431669045e-05 old loss 1.269687527383212e-05 BETTER
I0326 09:15:55.255739 1536852 finetune.py:68] layer 16_v @ epoch 3 new loss 1.2159772268205415e-05 old loss 1.2617640095413662e-05 BETTER
I0326 09:16:17.252303 1537554 finetune.py:68] layer 17_v @ epoch 2 new loss 1.1150516002089716e-05 old loss 1.1697583431669045e-05 BETTER
I0326 09:16:32.350629 1536852 finetune.py:68] layer 16_v @ epoch 4 new loss 1.182382766273804e-05 old loss 1.2159772268205415e-05 BETTER
I0326 09:16:42.342913 1536852 finetune.py:45] layer 16_q initial loss 1.4137947800918482e-05
I0326 09:16:52.387539 1537554 finetune.py:68] layer 17_v @ epoch 3 new loss 1.0803681107063312e-05 old loss 1.1150516002089716e-05 BETTER
I0326 09:17:18.474586 1536852 finetune.py:68] layer 16_q @ epoch 0 new loss 1.3489874618244357e-05 old loss 1.4137947800918482e-05 BETTER
I0326 09:17:27.615159 1537554 finetune.py:68] layer 17_v @ epoch 4 new loss 1.057430654327618e-05 old loss 1.0803681107063312e-05 BETTER
I0326 09:17:37.400758 1537554 finetune.py:45] layer 17_q initial loss 1.2458949640858918e-05
I0326 09:17:55.450923 1536852 finetune.py:68] layer 16_q @ epoch 1 new loss 1.3157023204257712e-05 old loss 1.3489874618244357e-05 BETTER
I0326 09:18:11.294478 1537554 finetune.py:68] layer 17_q @ epoch 0 new loss 1.1964624718530104e-05 old loss 1.2458949640858918e-05 BETTER
I0326 09:18:32.350151 1536852 finetune.py:68] layer 16_q @ epoch 2 new loss 1.2900771253043786e-05 old loss 1.3157023204257712e-05 BETTER
I0326 09:18:45.965340 1537554 finetune.py:68] layer 17_q @ epoch 1 new loss 1.1710828403010964e-05 old loss 1.1964624718530104e-05 BETTER
I0326 09:19:09.410575 1536852 finetune.py:68] layer 16_q @ epoch 3 new loss 1.2689105460594874e-05 old loss 1.2900771253043786e-05 BETTER
I0326 09:19:20.797960 1537554 finetune.py:68] layer 17_q @ epoch 2 new loss 1.1518510291352868e-05 old loss 1.1710828403010964e-05 BETTER
I0326 09:19:46.429321 1536852 finetune.py:68] layer 16_q @ epoch 4 new loss 1.2514380614447873e-05 old loss 1.2689105460594874e-05 BETTER
I0326 09:19:54.582299 1536852 finetune.py:45] layer 16_k initial loss 1.3290265087562148e-05
I0326 09:19:55.556797 1537554 finetune.py:68] layer 17_q @ epoch 3 new loss 1.1352680303389207e-05 old loss 1.1518510291352868e-05 BETTER
I0326 09:20:30.286174 1537554 finetune.py:68] layer 17_q @ epoch 4 new loss 1.1221103704883717e-05 old loss 1.1352680303389207e-05 BETTER
I0326 09:20:30.328276 1536852 finetune.py:68] layer 16_k @ epoch 0 new loss 1.3028829016548116e-05 old loss 1.3290265087562148e-05 BETTER
I0326 09:20:38.212861 1537554 finetune.py:45] layer 17_k initial loss 1.1857891877298243e-05
I0326 09:21:06.838192 1536852 finetune.py:68] layer 16_k @ epoch 1 new loss 1.288611201744061e-05 old loss 1.3028829016548116e-05 BETTER
I0326 09:21:12.009973 1537554 finetune.py:68] layer 17_k @ epoch 0 new loss 1.1705764336511493e-05 old loss 1.1857891877298243e-05 BETTER
I0326 09:21:43.445541 1536852 finetune.py:68] layer 16_k @ epoch 2 new loss 1.2771293768309988e-05 old loss 1.288611201744061e-05 BETTER
I0326 09:21:46.432165 1537554 finetune.py:68] layer 17_k @ epoch 1 new loss 1.1589130735956132e-05 old loss 1.1705764336511493e-05 BETTER
I0326 09:22:20.350608 1536852 finetune.py:68] layer 16_k @ epoch 3 new loss 1.2681522093771491e-05 old loss 1.2771293768309988e-05 BETTER
I0326 09:22:20.982328 1537554 finetune.py:68] layer 17_k @ epoch 2 new loss 1.1507361705298536e-05 old loss 1.1589130735956132e-05 BETTER
I0326 09:22:55.554512 1537554 finetune.py:68] layer 17_k @ epoch 3 new loss 1.1419159818615299e-05 old loss 1.1507361705298536e-05 BETTER
I0326 09:22:57.276024 1536852 finetune.py:68] layer 16_k @ epoch 4 new loss 1.2583787793118972e-05 old loss 1.2681522093771491e-05 BETTER
I0326 09:23:07.419017 1536852 finetune.py:45] layer 16_o initial loss 4.5350956497713923e-05
I0326 09:23:30.317426 1537554 finetune.py:68] layer 17_k @ epoch 4 new loss 1.1365292266418692e-05 old loss 1.1419159818615299e-05 BETTER
I0326 09:23:40.116748 1537554 finetune.py:45] layer 17_o initial loss 3.749405368580483e-05
I0326 09:23:42.783344 1536852 finetune.py:68] layer 16_o @ epoch 0 new loss 4.0616621845401824e-05 old loss 4.5350956497713923e-05 BETTER
I0326 09:24:13.368306 1537554 finetune.py:68] layer 17_o @ epoch 0 new loss 3.385299351066351e-05 old loss 3.749405368580483e-05 BETTER
I0326 09:24:18.808098 1536852 finetune.py:68] layer 16_o @ epoch 1 new loss 3.851657311315648e-05 old loss 4.0616621845401824e-05 BETTER
I0326 09:24:47.455710 1537554 finetune.py:68] layer 17_o @ epoch 1 new loss 3.22250671160873e-05 old loss 3.385299351066351e-05 BETTER
I0326 09:24:54.903711 1536852 finetune.py:68] layer 16_o @ epoch 2 new loss 3.710476812557317e-05 old loss 3.851657311315648e-05 BETTER
I0326 09:25:21.482495 1537554 finetune.py:68] layer 17_o @ epoch 2 new loss 3.114874562015757e-05 old loss 3.22250671160873e-05 BETTER
I0326 09:25:31.113651 1536852 finetune.py:68] layer 16_o @ epoch 3 new loss 3.6020846891915426e-05 old loss 3.710476812557317e-05 BETTER
I0326 09:25:55.517363 1537554 finetune.py:68] layer 17_o @ epoch 3 new loss 3.034517249034252e-05 old loss 3.114874562015757e-05 BETTER
I0326 09:26:07.513328 1536852 finetune.py:68] layer 16_o @ epoch 4 new loss 3.5163579013897106e-05 old loss 3.6020846891915426e-05 BETTER
I0326 09:26:29.418110 1537554 finetune.py:68] layer 17_o @ epoch 4 new loss 2.96980761049781e-05 old loss 3.034517249034252e-05 BETTER
I0326 09:26:29.717001 1536852 finetune.py:45] layer 16_up initial loss 7.23416669643484e-05
I0326 09:26:51.193453 1537554 finetune.py:45] layer 17_up initial loss 7.246190216392279e-05
I0326 09:27:01.887142 1536852 finetune.py:68] layer 16_up @ epoch 0 new loss 6.924945773789659e-05 old loss 7.23416669643484e-05 BETTER
I0326 09:27:22.086929 1537554 finetune.py:68] layer 17_up @ epoch 0 new loss 6.896886043250561e-05 old loss 7.246190216392279e-05 BETTER
I0326 09:27:35.282440 1536852 finetune.py:68] layer 16_up @ epoch 1 new loss 6.752329500159249e-05 old loss 6.924945773789659e-05 BETTER
I0326 09:27:54.131077 1537554 finetune.py:68] layer 17_up @ epoch 1 new loss 6.708137516397983e-05 old loss 6.896886043250561e-05 BETTER
I0326 09:28:08.809704 1536852 finetune.py:68] layer 16_up @ epoch 2 new loss 6.615291204070672e-05 old loss 6.752329500159249e-05 BETTER
I0326 09:28:26.331191 1537554 finetune.py:68] layer 17_up @ epoch 2 new loss 6.564358773175627e-05 old loss 6.708137516397983e-05 BETTER
I0326 09:28:42.642782 1536852 finetune.py:68] layer 16_up @ epoch 3 new loss 6.50108777335845e-05 old loss 6.615291204070672e-05 BETTER
I0326 09:28:58.662263 1537554 finetune.py:68] layer 17_up @ epoch 3 new loss 6.445228063967079e-05 old loss 6.564358773175627e-05 BETTER
I0326 09:29:16.702498 1536852 finetune.py:68] layer 16_up @ epoch 4 new loss 6.401525752153248e-05 old loss 6.50108777335845e-05 BETTER
I0326 09:29:30.981819 1537554 finetune.py:68] layer 17_up @ epoch 4 new loss 6.343908898998052e-05 old loss 6.445228063967079e-05 BETTER
I0326 09:29:39.193224 1536852 finetune.py:45] layer 16_gate initial loss 7.625814760103822e-05
I0326 09:29:53.181521 1537554 finetune.py:45] layer 17_gate initial loss 7.764635665807873e-05
I0326 09:30:09.594958 1536852 finetune.py:68] layer 16_gate @ epoch 0 new loss 7.514216122217476e-05 old loss 7.625814760103822e-05 BETTER
I0326 09:30:21.824674 1537554 finetune.py:68] layer 17_gate @ epoch 0 new loss 7.641758566023782e-05 old loss 7.764635665807873e-05 BETTER
I0326 09:30:40.988343 1536852 finetune.py:68] layer 16_gate @ epoch 1 new loss 7.427509990520775e-05 old loss 7.514216122217476e-05 BETTER
I0326 09:30:51.508617 1537554 finetune.py:68] layer 17_gate @ epoch 1 new loss 7.55009168642573e-05 old loss 7.641758566023782e-05 BETTER
I0326 09:31:12.731367 1536852 finetune.py:68] layer 16_gate @ epoch 2 new loss 7.351709791691974e-05 old loss 7.427509990520775e-05 BETTER
I0326 09:31:21.520092 1537554 finetune.py:68] layer 17_gate @ epoch 2 new loss 7.471477874787524e-05 old loss 7.55009168642573e-05 BETTER
I0326 09:31:44.382834 1536852 finetune.py:68] layer 16_gate @ epoch 3 new loss 7.284701860044152e-05 old loss 7.351709791691974e-05 BETTER
I0326 09:31:51.424107 1537554 finetune.py:68] layer 17_gate @ epoch 3 new loss 7.40124742151238e-05 old loss 7.471477874787524e-05 BETTER
I0326 09:32:16.282862 1536852 finetune.py:68] layer 16_gate @ epoch 4 new loss 7.224971341202036e-05 old loss 7.284701860044152e-05 BETTER
I0326 09:32:21.421951 1537554 finetune.py:68] layer 17_gate @ epoch 4 new loss 7.339943840634078e-05 old loss 7.40124742151238e-05 BETTER
I0326 09:32:39.896304 1536852 finetune.py:45] layer 16_down initial loss 0.000124176440294832
I0326 09:32:44.812705 1537554 finetune.py:45] layer 17_down initial loss 0.00013611801841761917
I0326 09:33:07.985944 1536852 finetune.py:68] layer 16_down @ epoch 0 new loss 0.00012416501704137772 old loss 0.000124176440294832 BETTER
I0326 09:33:11.668832 1537554 finetune.py:68] layer 17_down @ epoch 0 new loss 0.00013610481983050704 old loss 0.00013611801841761917 BETTER
I0326 09:33:37.359377 1536852 finetune.py:68] layer 16_down @ epoch 1 new loss 0.00012415517994668335 old loss 0.00012416501704137772 BETTER
I0326 09:33:39.535802 1537554 finetune.py:68] layer 17_down @ epoch 1 new loss 0.00013609560846816748 old loss 0.00013610481983050704 BETTER
I0326 09:34:06.962399 1536852 finetune.py:68] layer 16_down @ epoch 2 new loss 0.00012414749653544277 old loss 0.00012415517994668335 BETTER
I0326 09:34:07.298156 1537554 finetune.py:68] layer 17_down @ epoch 2 new loss 0.00013608727022074163 old loss 0.00013609560846816748 BETTER
I0326 09:34:35.118168 1537554 finetune.py:68] layer 17_down @ epoch 3 new loss 0.00013608061999548227 old loss 0.00013608727022074163 BETTER
I0326 09:34:36.579526 1536852 finetune.py:68] layer 16_down @ epoch 3 new loss 0.00012414094817359 old loss 0.00012414749653544277 BETTER
I0326 09:35:03.128223 1537554 finetune.py:68] layer 17_down @ epoch 4 new loss 0.00013607491564471275 old loss 0.00013608061999548227 BETTER
17_v proxy err 0.015160074457526207 tr(WHW.T) 283.9730224609375
bpp_loss 3.314234972000122
17_q proxy err 0.0006923260516487062 tr(WHW.T) 27575.0234375
bpp_loss 4.135690212249756
17_k proxy err 0.0003234746982343495 tr(WHW.T) 17433.1640625
bpp_loss 5.087945938110352
17_o proxy err 0.018870877102017403 tr(WHW.T) 1110.962646484375
bpp_loss 3.3785678148269653
17_up proxy err 0.009435235522687435 tr(WHW.T) 8457.21875
bpp_loss 3.451406615121024
17_gate proxy err 0.0019968838896602392 tr(WHW.T) 41718.65234375
bpp_loss 3.8072136470249722
17_down proxy err 0.012615937739610672 tr(WHW.T) 6244.89501953125
bpp_loss 3.442265748977661
I0326 09:35:06.257508 1536852 finetune.py:68] layer 16_down @ epoch 4 new loss 0.00012413434160407633 old loss 0.00012414094817359 BETTER
16_v proxy err 0.014549891464412212 tr(WHW.T) 274.28167724609375
bpp_loss 3.244860291481018
16_q proxy err 0.0007312270463444293 tr(WHW.T) 24487.236328125
bpp_loss 4.11927604675293
16_k proxy err 0.00027880107518285513 tr(WHW.T) 19511.158203125
bpp_loss 5.058336496353149
16_o proxy err 0.020785391330718994 tr(WHW.T) 972.67724609375
bpp_loss 3.3490097522735596
16_up proxy err 0.009473119862377644 tr(WHW.T) 8334.07421875
bpp_loss 3.45381041935512
16_gate proxy err 0.0019974110182374716 tr(WHW.T) 41200.171875
bpp_loss 3.792910167149135
16_down proxy err 0.012396926991641521 tr(WHW.T) 6322.041015625
bpp_loss 3.445849588939122
I0326 09:36:15.871481 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 64.47752165794373s
I0326 09:36:19.430093 1551846 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 09:36:19.430189 1551846 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 09:36:19.430227 1551846 utils.py:162] NumExpr defaulting to 16 threads.
I0326 09:36:19.768057 1551846 config.py:54] PyTorch version 2.6.0 available.
W0326 09:36:19.958893 1551846 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 09:36:20.519364 1551846 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 09:36:20.523017 1418472 quantize_finetune_llama.py:209] layer 19 gpu 1
I0326 09:36:20.535801 1551846 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 09:36:28.077448 1551846 finetune.py:45] layer 18_v initial loss 1.721486660244409e-05
W0326 09:36:28.077674 1551846 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 09:37:04.042467 1551846 finetune.py:68] layer 18_v @ epoch 0 new loss 1.0288178600603715e-05 old loss 1.721486660244409e-05 BETTER
I0326 09:37:23.698473 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 62.773667335510254s
I0326 09:37:27.446646 1552809 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 09:37:27.446739 1552809 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 09:37:27.446777 1552809 utils.py:162] NumExpr defaulting to 16 threads.
I0326 09:37:27.797437 1552809 config.py:54] PyTorch version 2.6.0 available.
W0326 09:37:27.995406 1552809 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 09:37:28.569100 1552809 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 09:37:28.573083 1418472 quantize_finetune_llama.py:209] layer 20 gpu 0
I0326 09:37:28.586242 1552809 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 09:37:35.631256 1552809 finetune.py:45] layer 19_v initial loss 2.0718396626762114e-05
W0326 09:37:35.631499 1552809 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 09:37:41.523797 1551846 finetune.py:68] layer 18_v @ epoch 1 new loss 9.554863027005922e-06 old loss 1.0288178600603715e-05 BETTER
I0326 09:38:09.178411 1552809 finetune.py:68] layer 19_v @ epoch 0 new loss 1.0771065717563033e-05 old loss 2.0718396626762114e-05 BETTER
I0326 09:38:18.956859 1551846 finetune.py:68] layer 18_v @ epoch 2 new loss 9.136158041656017e-06 old loss 9.554863027005922e-06 BETTER
I0326 09:38:43.795666 1552809 finetune.py:68] layer 19_v @ epoch 1 new loss 9.90920489130076e-06 old loss 1.0771065717563033e-05 BETTER
I0326 09:38:56.451071 1551846 finetune.py:68] layer 18_v @ epoch 3 new loss 8.859959962137509e-06 old loss 9.136158041656017e-06 BETTER
I0326 09:39:18.823398 1552809 finetune.py:68] layer 19_v @ epoch 2 new loss 9.490531738265418e-06 old loss 9.90920489130076e-06 BETTER
I0326 09:39:34.096051 1551846 finetune.py:68] layer 18_v @ epoch 4 new loss 8.666546818858478e-06 old loss 8.859959962137509e-06 BETTER
I0326 09:39:44.243145 1551846 finetune.py:45] layer 18_q initial loss 1.0640771506587043e-05
I0326 09:39:53.845080 1552809 finetune.py:68] layer 19_v @ epoch 3 new loss 9.212751137965824e-06 old loss 9.490531738265418e-06 BETTER
I0326 09:40:20.539243 1551846 finetune.py:68] layer 18_q @ epoch 0 new loss 1.0134420335816685e-05 old loss 1.0640771506587043e-05 BETTER
I0326 09:40:28.998940 1552809 finetune.py:68] layer 19_v @ epoch 4 new loss 9.022443919093348e-06 old loss 9.212751137965824e-06 BETTER
I0326 09:40:38.692974 1552809 finetune.py:45] layer 19_q initial loss 1.068664641934447e-05
I0326 09:40:57.509772 1551846 finetune.py:68] layer 18_q @ epoch 1 new loss 9.910755579767283e-06 old loss 1.0134420335816685e-05 BETTER
I0326 09:41:12.470505 1552809 finetune.py:68] layer 19_q @ epoch 0 new loss 1.0233718967356253e-05 old loss 1.068664641934447e-05 BETTER
I0326 09:41:34.558143 1551846 finetune.py:68] layer 18_q @ epoch 2 new loss 9.753511221788358e-06 old loss 9.910755579767283e-06 BETTER
I0326 09:41:47.633152 1552809 finetune.py:68] layer 19_q @ epoch 1 new loss 1.0034875231212936e-05 old loss 1.0233718967356253e-05 BETTER
I0326 09:42:11.638738 1551846 finetune.py:68] layer 18_q @ epoch 3 new loss 9.62291051109787e-06 old loss 9.753511221788358e-06 BETTER
I0326 09:42:22.327450 1552809 finetune.py:68] layer 19_q @ epoch 2 new loss 9.863877494353801e-06 old loss 1.0034875231212936e-05 BETTER
I0326 09:42:48.850015 1551846 finetune.py:68] layer 18_q @ epoch 4 new loss 9.516881618765183e-06 old loss 9.62291051109787e-06 BETTER
I0326 09:42:56.944637 1552809 finetune.py:68] layer 19_q @ epoch 3 new loss 9.755997780303005e-06 old loss 9.863877494353801e-06 BETTER
I0326 09:42:57.088704 1551846 finetune.py:45] layer 18_k initial loss 1.0095236575580202e-05
I0326 09:43:31.654177 1552809 finetune.py:68] layer 19_q @ epoch 4 new loss 9.632439287088346e-06 old loss 9.755997780303005e-06 BETTER
I0326 09:43:33.307356 1551846 finetune.py:68] layer 18_k @ epoch 0 new loss 9.932283319358248e-06 old loss 1.0095236575580202e-05 BETTER
I0326 09:43:39.667681 1552809 finetune.py:45] layer 19_k initial loss 1.0277904038957786e-05
I0326 09:44:10.326461 1551846 finetune.py:68] layer 18_k @ epoch 1 new loss 9.842837243922986e-06 old loss 9.932283319358248e-06 BETTER
I0326 09:44:13.462667 1552809 finetune.py:68] layer 19_k @ epoch 0 new loss 1.0084312634717207e-05 old loss 1.0277904038957786e-05 BETTER
I0326 09:44:47.325722 1551846 finetune.py:68] layer 18_k @ epoch 2 new loss 9.775205398909748e-06 old loss 9.842837243922986e-06 BETTER
I0326 09:44:47.912086 1552809 finetune.py:68] layer 19_k @ epoch 1 new loss 9.999590474762954e-06 old loss 1.0084312634717207e-05 BETTER
I0326 09:45:22.462400 1552809 finetune.py:68] layer 19_k @ epoch 2 new loss 9.938752555171959e-06 old loss 9.999590474762954e-06 BETTER
I0326 09:45:24.347957 1551846 finetune.py:68] layer 18_k @ epoch 3 new loss 9.707960089144763e-06 old loss 9.775205398909748e-06 BETTER
I0326 09:45:56.972279 1552809 finetune.py:68] layer 19_k @ epoch 3 new loss 9.869743735180236e-06 old loss 9.938752555171959e-06 BETTER
I0326 09:46:02.633082 1551846 finetune.py:68] layer 18_k @ epoch 4 new loss 9.645606041885912e-06 old loss 9.707960089144763e-06 BETTER
I0326 09:46:12.556886 1551846 finetune.py:45] layer 18_o initial loss 2.815617153828498e-05
I0326 09:46:31.497782 1552809 finetune.py:68] layer 19_k @ epoch 4 new loss 9.8131931736134e-06 old loss 9.869743735180236e-06 BETTER
I0326 09:46:41.331562 1552809 finetune.py:45] layer 19_o initial loss 2.668365959834773e-05
I0326 09:46:47.700511 1551846 finetune.py:68] layer 18_o @ epoch 0 new loss 2.5848035875242203e-05 old loss 2.815617153828498e-05 BETTER
I0326 09:47:14.501045 1552809 finetune.py:68] layer 19_o @ epoch 0 new loss 2.452265289321076e-05 old loss 2.668365959834773e-05 BETTER
I0326 09:47:23.750511 1551846 finetune.py:68] layer 18_o @ epoch 1 new loss 2.4940283765317872e-05 old loss 2.5848035875242203e-05 BETTER
I0326 09:47:48.490387 1552809 finetune.py:68] layer 19_o @ epoch 1 new loss 2.370262154727243e-05 old loss 2.452265289321076e-05 BETTER
I0326 09:47:59.822769 1551846 finetune.py:68] layer 18_o @ epoch 2 new loss 2.4339644369320013e-05 old loss 2.4940283765317872e-05 BETTER
I0326 09:48:22.493281 1552809 finetune.py:68] layer 19_o @ epoch 2 new loss 2.3168171537690796e-05 old loss 2.370262154727243e-05 BETTER
I0326 09:48:36.090504 1551846 finetune.py:68] layer 18_o @ epoch 3 new loss 2.3881986635387875e-05 old loss 2.4339644369320013e-05 BETTER
I0326 09:48:56.462286 1552809 finetune.py:68] layer 19_o @ epoch 3 new loss 2.276122359035071e-05 old loss 2.3168171537690796e-05 BETTER
I0326 09:49:12.450432 1551846 finetune.py:68] layer 18_o @ epoch 4 new loss 2.3531003535026684e-05 old loss 2.3881986635387875e-05 BETTER
I0326 09:49:30.730340 1552809 finetune.py:68] layer 19_o @ epoch 4 new loss 2.244300048914738e-05 old loss 2.276122359035071e-05 BETTER
I0326 09:49:34.653781 1551846 finetune.py:45] layer 18_up initial loss 6.694476178381592e-05
I0326 09:49:52.959291 1552809 finetune.py:45] layer 19_up initial loss 6.905217742314562e-05
I0326 09:50:07.187592 1551846 finetune.py:68] layer 18_up @ epoch 0 new loss 6.397913966793567e-05 old loss 6.694476178381592e-05 BETTER
I0326 09:50:23.686774 1552809 finetune.py:68] layer 19_up @ epoch 0 new loss 6.60318837617524e-05 old loss 6.905217742314562e-05 BETTER
I0326 09:50:40.518464 1551846 finetune.py:68] layer 18_up @ epoch 1 new loss 6.237011984921992e-05 old loss 6.397913966793567e-05 BETTER
I0326 09:50:55.636577 1552809 finetune.py:68] layer 19_up @ epoch 1 new loss 6.440072320401669e-05 old loss 6.60318837617524e-05 BETTER
I0326 09:51:13.837241 1551846 finetune.py:68] layer 18_up @ epoch 2 new loss 6.115223368396983e-05 old loss 6.237011984921992e-05 BETTER
I0326 09:51:27.811543 1552809 finetune.py:68] layer 19_up @ epoch 2 new loss 6.314697384368628e-05 old loss 6.440072320401669e-05 BETTER
I0326 09:51:47.412594 1551846 finetune.py:68] layer 18_up @ epoch 3 new loss 6.012921221554279e-05 old loss 6.115223368396983e-05 BETTER
I0326 09:52:00.013922 1552809 finetune.py:68] layer 19_up @ epoch 3 new loss 6.21307481196709e-05 old loss 6.314697384368628e-05 BETTER
I0326 09:52:21.094581 1551846 finetune.py:68] layer 18_up @ epoch 4 new loss 5.9273555962136015e-05 old loss 6.012921221554279e-05 BETTER
I0326 09:52:32.263037 1552809 finetune.py:68] layer 19_up @ epoch 4 new loss 6.126370863057673e-05 old loss 6.21307481196709e-05 BETTER
I0326 09:52:43.698314 1551846 finetune.py:45] layer 18_gate initial loss 7.424830255331472e-05
I0326 09:52:54.371102 1552809 finetune.py:45] layer 19_gate initial loss 7.742371235508472e-05
I0326 09:53:14.062015 1551846 finetune.py:68] layer 18_gate @ epoch 0 new loss 7.322301826206967e-05 old loss 7.424830255331472e-05 BETTER
I0326 09:53:23.032012 1552809 finetune.py:68] layer 19_gate @ epoch 0 new loss 7.641615957254544e-05 old loss 7.742371235508472e-05 BETTER
I0326 09:53:45.363465 1551846 finetune.py:68] layer 18_gate @ epoch 1 new loss 7.244686275953427e-05 old loss 7.322301826206967e-05 BETTER
I0326 09:53:52.740465 1552809 finetune.py:68] layer 19_gate @ epoch 1 new loss 7.564692350570112e-05 old loss 7.641615957254544e-05 BETTER
I0326 09:54:16.925481 1551846 finetune.py:68] layer 18_gate @ epoch 2 new loss 7.17801449354738e-05 old loss 7.244686275953427e-05 BETTER
I0326 09:54:22.665624 1552809 finetune.py:68] layer 19_gate @ epoch 2 new loss 7.497399201383814e-05 old loss 7.564692350570112e-05 BETTER
I0326 09:54:48.349023 1551846 finetune.py:68] layer 18_gate @ epoch 3 new loss 7.119503425201401e-05 old loss 7.17801449354738e-05 BETTER
I0326 09:54:52.583923 1552809 finetune.py:68] layer 19_gate @ epoch 3 new loss 7.439032196998596e-05 old loss 7.497399201383814e-05 BETTER
I0326 09:55:19.801148 1551846 finetune.py:68] layer 18_gate @ epoch 4 new loss 7.066683610901237e-05 old loss 7.119503425201401e-05 BETTER
I0326 09:55:22.494474 1552809 finetune.py:68] layer 19_gate @ epoch 4 new loss 7.387647201539949e-05 old loss 7.439032196998596e-05 BETTER
I0326 09:55:43.762466 1551846 finetune.py:45] layer 18_down initial loss 0.00013197965745348483
I0326 09:55:45.526177 1552809 finetune.py:45] layer 19_down initial loss 0.00013727039913646877
I0326 09:56:11.544783 1551846 finetune.py:68] layer 18_down @ epoch 0 new loss 0.00013197335647419095 old loss 0.00013197965745348483 BETTER
I0326 09:56:12.448498 1552809 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0001372612314298749 old loss 0.00013727039913646877 BETTER
I0326 09:56:40.293327 1552809 finetune.py:68] layer 19_down @ epoch 1 new loss 0.00013725468306802213 old loss 0.0001372612314298749 BETTER
I0326 09:56:40.775160 1551846 finetune.py:68] layer 18_down @ epoch 1 new loss 0.00013196692452766 old loss 0.00013197335647419095 BETTER
I0326 09:57:08.087446 1552809 finetune.py:68] layer 19_down @ epoch 2 new loss 0.00013724954624194652 old loss 0.00013725468306802213 BETTER
I0326 09:57:10.028111 1551846 finetune.py:68] layer 18_down @ epoch 2 new loss 0.00013196130748838186 old loss 0.00013196692452766 BETTER
I0326 09:57:35.896784 1552809 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0001372448168694973 old loss 0.00013724954624194652 BETTER
I0326 09:57:39.446780 1551846 finetune.py:68] layer 18_down @ epoch 3 new loss 0.00013195679639466107 old loss 0.00013196130748838186 BETTER
I0326 09:58:03.859096 1552809 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0001372411788906902 old loss 0.0001372448168694973 BETTER
19_v proxy err 0.013266714289784431 tr(WHW.T) 341.0596618652344
bpp_loss 3.284932494163513
19_q proxy err 0.0008279287139885128 tr(WHW.T) 24051.1328125
bpp_loss 4.134205341339111
19_k proxy err 0.00036484183510765433 tr(WHW.T) 15565.09765625
bpp_loss 5.067872524261475
19_o proxy err 0.017105303704738617 tr(WHW.T) 1172.093994140625
bpp_loss 3.3707947731018066
19_up proxy err 0.010735801421105862 tr(WHW.T) 7651.876953125
bpp_loss 3.4441592352730885
19_gate proxy err 0.0026073171757161617 tr(WHW.T) 32779.2265625
bpp_loss 3.8264899935041154
19_down proxy err 0.01241519395262003 tr(WHW.T) 6215.4150390625
bpp_loss 3.439789056777954
I0326 09:58:08.897672 1551846 finetune.py:68] layer 18_down @ epoch 4 new loss 0.00013195232895668596 old loss 0.00013195679639466107 BETTER
18_v proxy err 0.015256739221513271 tr(WHW.T) 287.61376953125
bpp_loss 3.239068627357483
18_q proxy err 0.0008666437352076173 tr(WHW.T) 22415.798828125
bpp_loss 4.131968259811401
18_k proxy err 0.0003264903207309544 tr(WHW.T) 17408.953125
bpp_loss 5.16942834854126
18_o proxy err 0.016722382977604866 tr(WHW.T) 1206.979736328125
bpp_loss 3.3574260473251343
18_up proxy err 0.010228934697806835 tr(WHW.T) 7984.4111328125
bpp_loss 3.4476243427821567
18_gate proxy err 0.0024187073577195406 tr(WHW.T) 35151.6484375
bpp_loss 3.8151662009102956
18_down proxy err 0.01247180625796318 tr(WHW.T) 6259.91162109375
bpp_loss 3.441496270043509
I0326 09:59:09.894764 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 55.566545248031616s
I0326 09:59:13.385495 1566920 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 09:59:13.385587 1566920 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 09:59:13.385627 1566920 utils.py:162] NumExpr defaulting to 16 threads.
I0326 09:59:13.716451 1566920 config.py:54] PyTorch version 2.6.0 available.
W0326 09:59:13.911259 1566920 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 09:59:14.591889 1566920 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 09:59:14.595380 1418472 quantize_finetune_llama.py:209] layer 21 gpu 1
I0326 09:59:14.611084 1566920 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 09:59:22.179173 1566920 finetune.py:45] layer 20_v initial loss 1.8547047147876583e-05
W0326 09:59:22.179400 1566920 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 09:59:58.162922 1566920 finetune.py:68] layer 20_v @ epoch 0 new loss 1.1253514458076097e-05 old loss 1.8547047147876583e-05 BETTER
I0326 10:00:17.933958 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 62.90145683288574s
I0326 10:00:21.914103 1567627 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 10:00:21.914201 1567627 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 10:00:21.914244 1567627 utils.py:162] NumExpr defaulting to 16 threads.
I0326 10:00:22.255758 1567627 config.py:54] PyTorch version 2.6.0 available.
W0326 10:00:22.459711 1567627 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 10:00:23.026288 1567627 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 10:00:23.030005 1418472 quantize_finetune_llama.py:209] layer 22 gpu 0
I0326 10:00:23.042792 1567627 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 10:00:30.372407 1567627 finetune.py:45] layer 21_v initial loss 2.1539313820539974e-05
W0326 10:00:30.372596 1567627 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 10:00:35.427844 1566920 finetune.py:68] layer 20_v @ epoch 1 new loss 1.0466159437783062e-05 old loss 1.1253514458076097e-05 BETTER
I0326 10:01:03.919886 1567627 finetune.py:68] layer 21_v @ epoch 0 new loss 1.3436354493023828e-05 old loss 2.1539313820539974e-05 BETTER
I0326 10:01:12.458496 1566920 finetune.py:68] layer 20_v @ epoch 2 new loss 1.003560009849025e-05 old loss 1.0466159437783062e-05 BETTER
I0326 10:01:38.693864 1567627 finetune.py:68] layer 21_v @ epoch 1 new loss 1.2383165085338987e-05 old loss 1.3436354493023828e-05 BETTER
I0326 10:01:49.437219 1566920 finetune.py:68] layer 20_v @ epoch 3 new loss 9.744721864990424e-06 old loss 1.003560009849025e-05 BETTER
I0326 10:02:13.967452 1567627 finetune.py:68] layer 21_v @ epoch 2 new loss 1.17942072392907e-05 old loss 1.2383165085338987e-05 BETTER
I0326 10:02:26.442227 1566920 finetune.py:68] layer 20_v @ epoch 4 new loss 9.524496817903128e-06 old loss 9.744721864990424e-06 BETTER
I0326 10:02:36.519525 1566920 finetune.py:45] layer 20_q initial loss 1.145587521023117e-05
I0326 10:02:49.198394 1567627 finetune.py:68] layer 21_v @ epoch 3 new loss 1.1378127965144813e-05 old loss 1.17942072392907e-05 BETTER
I0326 10:03:12.554539 1566920 finetune.py:68] layer 20_q @ epoch 0 new loss 1.0938300874840934e-05 old loss 1.145587521023117e-05 BETTER
I0326 10:03:24.458595 1567627 finetune.py:68] layer 21_v @ epoch 4 new loss 1.1064790669479407e-05 old loss 1.1378127965144813e-05 BETTER
I0326 10:03:34.274110 1567627 finetune.py:45] layer 21_q initial loss 1.4264584933698643e-05
I0326 10:03:49.597945 1566920 finetune.py:68] layer 20_q @ epoch 1 new loss 1.070473444997333e-05 old loss 1.0938300874840934e-05 BETTER
I0326 10:04:08.183566 1567627 finetune.py:68] layer 21_q @ epoch 0 new loss 1.3411847248789854e-05 old loss 1.4264584933698643e-05 BETTER
I0326 10:04:26.407707 1566920 finetune.py:68] layer 20_q @ epoch 2 new loss 1.0531562111282256e-05 old loss 1.070473444997333e-05 BETTER
I0326 10:04:42.916519 1567627 finetune.py:68] layer 21_q @ epoch 1 new loss 1.3054104783805087e-05 old loss 1.3411847248789854e-05 BETTER
I0326 10:05:03.441266 1566920 finetune.py:68] layer 20_q @ epoch 3 new loss 1.0387441761849914e-05 old loss 1.0531562111282256e-05 BETTER
I0326 10:05:17.575709 1567627 finetune.py:68] layer 21_q @ epoch 2 new loss 1.2790002983820159e-05 old loss 1.3054104783805087e-05 BETTER
I0326 10:05:40.217190 1566920 finetune.py:68] layer 20_q @ epoch 4 new loss 1.0274110536556691e-05 old loss 1.0387441761849914e-05 BETTER
I0326 10:05:48.460643 1566920 finetune.py:45] layer 20_k initial loss 1.0957244739984162e-05
I0326 10:05:52.274598 1567627 finetune.py:68] layer 21_q @ epoch 3 new loss 1.2566617442644201e-05 old loss 1.2790002983820159e-05 BETTER
I0326 10:06:24.338106 1566920 finetune.py:68] layer 20_k @ epoch 0 new loss 1.0744526662165299e-05 old loss 1.0957244739984162e-05 BETTER
I0326 10:06:26.912348 1567627 finetune.py:68] layer 21_q @ epoch 4 new loss 1.2387784408929292e-05 old loss 1.2566617442644201e-05 BETTER
I0326 10:06:34.711901 1567627 finetune.py:45] layer 21_k initial loss 1.3386368664214388e-05
I0326 10:07:01.119857 1566920 finetune.py:68] layer 20_k @ epoch 1 new loss 1.0649308933352586e-05 old loss 1.0744526662165299e-05 BETTER
I0326 10:07:08.561614 1567627 finetune.py:68] layer 21_k @ epoch 0 new loss 1.314810651820153e-05 old loss 1.3386368664214388e-05 BETTER
I0326 10:07:37.971896 1566920 finetune.py:68] layer 20_k @ epoch 2 new loss 1.057278859661892e-05 old loss 1.0649308933352586e-05 BETTER
I0326 10:07:43.271730 1567627 finetune.py:68] layer 21_k @ epoch 1 new loss 1.3023142855672631e-05 old loss 1.314810651820153e-05 BETTER
I0326 10:08:14.926653 1566920 finetune.py:68] layer 20_k @ epoch 3 new loss 1.0501397810003255e-05 old loss 1.057278859661892e-05 BETTER
I0326 10:08:17.962350 1567627 finetune.py:68] layer 21_k @ epoch 2 new loss 1.2912959391542245e-05 old loss 1.3023142855672631e-05 BETTER
I0326 10:08:52.074347 1566920 finetune.py:68] layer 20_k @ epoch 4 new loss 1.0443547580507584e-05 old loss 1.0501397810003255e-05 BETTER
I0326 10:08:52.718107 1567627 finetune.py:68] layer 21_k @ epoch 3 new loss 1.2820471056329552e-05 old loss 1.2912959391542245e-05 BETTER
I0326 10:09:02.118383 1566920 finetune.py:45] layer 20_o initial loss 2.8106302124797367e-05
I0326 10:09:27.366572 1567627 finetune.py:68] layer 21_k @ epoch 4 new loss 1.274824626307236e-05 old loss 1.2820471056329552e-05 BETTER
I0326 10:09:37.008086 1567627 finetune.py:45] layer 21_o initial loss 4.071506191394292e-05
I0326 10:09:37.421543 1566920 finetune.py:68] layer 20_o @ epoch 0 new loss 2.598725404823199e-05 old loss 2.8106302124797367e-05 BETTER
I0326 10:10:10.572042 1567627 finetune.py:68] layer 21_o @ epoch 0 new loss 3.616460890043527e-05 old loss 4.071506191394292e-05 BETTER
I0326 10:10:13.469707 1566920 finetune.py:68] layer 20_o @ epoch 1 new loss 2.5145849576801993e-05 old loss 2.598725404823199e-05 BETTER
I0326 10:10:44.698741 1567627 finetune.py:68] layer 21_o @ epoch 1 new loss 3.435462713241577e-05 old loss 3.616460890043527e-05 BETTER
I0326 10:10:49.489020 1566920 finetune.py:68] layer 20_o @ epoch 2 new loss 2.459024835843593e-05 old loss 2.5145849576801993e-05 BETTER
I0326 10:11:18.672060 1567627 finetune.py:68] layer 21_o @ epoch 2 new loss 3.3171378163388e-05 old loss 3.435462713241577e-05 BETTER
I0326 10:11:25.473865 1566920 finetune.py:68] layer 20_o @ epoch 3 new loss 2.4161954570445232e-05 old loss 2.459024835843593e-05 BETTER
I0326 10:11:52.731839 1567627 finetune.py:68] layer 21_o @ epoch 3 new loss 3.230113361496478e-05 old loss 3.3171378163388e-05 BETTER
I0326 10:12:01.438060 1566920 finetune.py:68] layer 20_o @ epoch 4 new loss 2.3828590201446787e-05 old loss 2.4161954570445232e-05 BETTER
I0326 10:12:23.644808 1566920 finetune.py:45] layer 20_up initial loss 7.368939986918122e-05
I0326 10:12:26.782794 1567627 finetune.py:68] layer 21_o @ epoch 4 new loss 3.162894063279964e-05 old loss 3.230113361496478e-05 BETTER
I0326 10:12:49.021927 1567627 finetune.py:45] layer 21_up initial loss 8.956655074143782e-05
I0326 10:12:55.810952 1566920 finetune.py:68] layer 20_up @ epoch 0 new loss 7.052656292216852e-05 old loss 7.368939986918122e-05 BETTER
I0326 10:13:19.775625 1567627 finetune.py:68] layer 21_up @ epoch 0 new loss 8.539997361367568e-05 old loss 8.956655074143782e-05 BETTER
I0326 10:13:29.173347 1566920 finetune.py:68] layer 20_up @ epoch 1 new loss 6.885852781124413e-05 old loss 7.052656292216852e-05 BETTER
I0326 10:13:51.681855 1567627 finetune.py:68] layer 21_up @ epoch 1 new loss 8.325264934683219e-05 old loss 8.539997361367568e-05 BETTER
I0326 10:14:02.553059 1566920 finetune.py:68] layer 20_up @ epoch 2 new loss 6.7583066993393e-05 old loss 6.885852781124413e-05 BETTER
I0326 10:14:23.906575 1567627 finetune.py:68] layer 21_up @ epoch 2 new loss 8.162409358192235e-05 old loss 8.325264934683219e-05 BETTER
I0326 10:14:36.195839 1566920 finetune.py:68] layer 20_up @ epoch 3 new loss 6.652990123257041e-05 old loss 6.7583066993393e-05 BETTER
I0326 10:14:56.097434 1567627 finetune.py:68] layer 21_up @ epoch 3 new loss 8.029384480323642e-05 old loss 8.162409358192235e-05 BETTER
I0326 10:15:09.802139 1566920 finetune.py:68] layer 20_up @ epoch 4 new loss 6.565749936271459e-05 old loss 6.652990123257041e-05 BETTER
I0326 10:15:28.338582 1567627 finetune.py:68] layer 21_up @ epoch 4 new loss 7.917901530163363e-05 old loss 8.029384480323642e-05 BETTER
I0326 10:15:32.893397 1566920 finetune.py:45] layer 20_gate initial loss 8.390696893911809e-05
I0326 10:15:51.182398 1567627 finetune.py:45] layer 21_gate initial loss 0.00010024864604929462
I0326 10:16:03.002402 1566920 finetune.py:68] layer 20_gate @ epoch 0 new loss 8.284331852337345e-05 old loss 8.390696893911809e-05 BETTER
I0326 10:16:19.752701 1567627 finetune.py:68] layer 21_gate @ epoch 0 new loss 9.883423626888543e-05 old loss 0.00010024864604929462 BETTER
I0326 10:16:34.237900 1566920 finetune.py:68] layer 20_gate @ epoch 1 new loss 8.204525511246175e-05 old loss 8.284331852337345e-05 BETTER
I0326 10:16:49.272813 1567627 finetune.py:68] layer 21_gate @ epoch 1 new loss 9.781504195416346e-05 old loss 9.883423626888543e-05 BETTER
I0326 10:17:05.590974 1566920 finetune.py:68] layer 20_gate @ epoch 2 new loss 8.135652751661837e-05 old loss 8.204525511246175e-05 BETTER
I0326 10:17:19.016740 1567627 finetune.py:68] layer 21_gate @ epoch 2 new loss 9.694893378764391e-05 old loss 9.781504195416346e-05 BETTER
I0326 10:17:37.161435 1566920 finetune.py:68] layer 20_gate @ epoch 3 new loss 8.075599907897413e-05 old loss 8.135652751661837e-05 BETTER
I0326 10:17:48.822447 1567627 finetune.py:68] layer 21_gate @ epoch 3 new loss 9.619746560929343e-05 old loss 9.694893378764391e-05 BETTER
I0326 10:18:08.645201 1566920 finetune.py:68] layer 20_gate @ epoch 4 new loss 8.02259091869928e-05 old loss 8.075599907897413e-05 BETTER
I0326 10:18:18.829212 1567627 finetune.py:68] layer 21_gate @ epoch 4 new loss 9.552839765092358e-05 old loss 9.619746560929343e-05 BETTER
I0326 10:18:31.826546 1566920 finetune.py:45] layer 20_down initial loss 0.00014744664076715708
I0326 10:18:41.809073 1567627 finetune.py:45] layer 21_down initial loss 0.00017534883227199316
I0326 10:18:59.552386 1566920 finetune.py:68] layer 20_down @ epoch 0 new loss 0.0001474384916946292 old loss 0.00014744664076715708 BETTER
I0326 10:19:08.557551 1567627 finetune.py:68] layer 21_down @ epoch 0 new loss 0.00017533560458105057 old loss 0.00017534883227199316 BETTER
I0326 10:19:28.396397 1566920 finetune.py:68] layer 20_down @ epoch 1 new loss 0.00014743364590685815 old loss 0.0001474384916946292 BETTER
I0326 10:19:36.398685 1567627 finetune.py:68] layer 21_down @ epoch 1 new loss 0.00017532681522425264 old loss 0.00017533560458105057 BETTER
I0326 10:19:57.435448 1566920 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0001474295713705942 old loss 0.00014743364590685815 BETTER
I0326 10:20:04.156003 1567627 finetune.py:68] layer 21_down @ epoch 2 new loss 0.00017531888443045318 old loss 0.00017532681522425264 BETTER
I0326 10:20:26.871059 1566920 finetune.py:68] layer 20_down @ epoch 3 new loss 0.00014742615167051554 old loss 0.0001474295713705942 BETTER
I0326 10:20:31.964600 1567627 finetune.py:68] layer 21_down @ epoch 3 new loss 0.00017531249613966793 old loss 0.00017531888443045318 BETTER
I0326 10:20:56.295207 1566920 finetune.py:68] layer 20_down @ epoch 4 new loss 0.00014742327039130032 old loss 0.00014742615167051554 BETTER
20_v proxy err 0.013214855454862118 tr(WHW.T) 330.192138671875
bpp_loss 3.319178581237793
20_q proxy err 0.0009269337169826031 tr(WHW.T) 20749.7734375
bpp_loss 4.10558009147644
20_k proxy err 0.00036450455081649125 tr(WHW.T) 15411.595703125
bpp_loss 5.0208117961883545
20_o proxy err 0.01709637977182865 tr(WHW.T) 1208.6466064453125
bpp_loss 3.3575438261032104
20_up proxy err 0.010861322283744812 tr(WHW.T) 7601.41943359375
bpp_loss 3.4480528150285994
20_gate proxy err 0.0027991917449980974 tr(WHW.T) 30655.51953125
bpp_loss 3.8286387579781667
20_down proxy err 0.012142256833612919 tr(WHW.T) 6331.10009765625
bpp_loss 3.444054569516863
I0326 10:20:59.721712 1567627 finetune.py:68] layer 21_down @ epoch 4 new loss 0.00017530677723698318 old loss 0.00017531249613966793 BETTER
21_v proxy err 0.012230061925947666 tr(WHW.T) 362.87310791015625
bpp_loss 3.344658374786377
21_q proxy err 0.0007542734383605421 tr(WHW.T) 25846.15234375
bpp_loss 4.102846384048462
21_k proxy err 0.0003357938549015671 tr(WHW.T) 16795.578125
bpp_loss 5.063458681106567
21_o proxy err 0.014507082290947437 tr(WHW.T) 1269.88916015625
bpp_loss 3.3753538131713867
21_up proxy err 0.010591784492135048 tr(WHW.T) 7777.201171875
bpp_loss 3.4509409495762418
21_gate proxy err 0.002714334987103939 tr(WHW.T) 31569.7734375
bpp_loss 3.8404938834054128
21_down proxy err 0.011705962009727955 tr(WHW.T) 6388.232421875
bpp_loss 3.4450200966426303
I0326 10:22:08.401982 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 63.87534523010254s
I0326 10:22:11.966354 1579999 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 10:22:11.966470 1579999 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 10:22:11.966513 1579999 utils.py:162] NumExpr defaulting to 16 threads.
I0326 10:22:12.304377 1579999 config.py:54] PyTorch version 2.6.0 available.
W0326 10:22:12.496897 1579999 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 10:22:13.071133 1579999 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 10:22:13.074916 1418472 quantize_finetune_llama.py:209] layer 23 gpu 1
I0326 10:22:13.087910 1579999 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 10:22:20.758877 1579999 finetune.py:45] layer 22_v initial loss 2.094898809446022e-05
W0326 10:22:20.759112 1579999 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 10:22:56.314782 1579999 finetune.py:68] layer 22_v @ epoch 0 new loss 1.0902873327722773e-05 old loss 2.094898809446022e-05 BETTER
I0326 10:23:16.075729 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 62.60438561439514s
I0326 10:23:19.726881 1580704 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 10:23:19.727000 1580704 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 10:23:19.727045 1580704 utils.py:162] NumExpr defaulting to 16 threads.
I0326 10:23:20.073481 1580704 config.py:54] PyTorch version 2.6.0 available.
W0326 10:23:20.276169 1580704 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 10:23:20.892834 1580704 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 10:23:20.896490 1418472 quantize_finetune_llama.py:209] layer 24 gpu 0
I0326 10:23:20.909462 1580704 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 10:23:28.633464 1580704 finetune.py:45] layer 23_v initial loss 2.0156632672296837e-05
W0326 10:23:28.633689 1580704 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 10:23:33.609950 1579999 finetune.py:68] layer 22_v @ epoch 1 new loss 1.004772730084369e-05 old loss 1.0902873327722773e-05 BETTER
I0326 10:24:02.171994 1580704 finetune.py:68] layer 23_v @ epoch 0 new loss 1.078575587598607e-05 old loss 2.0156632672296837e-05 BETTER
I0326 10:24:10.728624 1579999 finetune.py:68] layer 22_v @ epoch 2 new loss 9.60654051596066e-06 old loss 1.004772730084369e-05 BETTER
I0326 10:24:36.815946 1580704 finetune.py:68] layer 23_v @ epoch 1 new loss 9.964938726625405e-06 old loss 1.078575587598607e-05 BETTER
I0326 10:24:47.926347 1579999 finetune.py:68] layer 22_v @ epoch 3 new loss 9.318506272393279e-06 old loss 9.60654051596066e-06 BETTER
I0326 10:25:11.860216 1580704 finetune.py:68] layer 23_v @ epoch 2 new loss 9.555829819873907e-06 old loss 9.964938726625405e-06 BETTER
I0326 10:25:25.519210 1579999 finetune.py:68] layer 22_v @ epoch 4 new loss 9.117375157075003e-06 old loss 9.318506272393279e-06 BETTER
I0326 10:25:35.558647 1579999 finetune.py:45] layer 22_q initial loss 1.1887311302416492e-05
I0326 10:25:47.105258 1580704 finetune.py:68] layer 23_v @ epoch 3 new loss 9.298278200731147e-06 old loss 9.555829819873907e-06 BETTER
I0326 10:26:11.629153 1579999 finetune.py:68] layer 22_q @ epoch 0 new loss 1.123052879847819e-05 old loss 1.1887311302416492e-05 BETTER
I0326 10:26:22.391074 1580704 finetune.py:68] layer 23_v @ epoch 4 new loss 9.128990313911345e-06 old loss 9.298278200731147e-06 BETTER
I0326 10:26:32.146610 1580704 finetune.py:45] layer 23_q initial loss 1.1568266927497461e-05
I0326 10:26:48.594989 1579999 finetune.py:68] layer 22_q @ epoch 1 new loss 1.0984449545503594e-05 old loss 1.123052879847819e-05 BETTER
I0326 10:27:06.057041 1580704 finetune.py:68] layer 23_q @ epoch 0 new loss 1.10070241134963e-05 old loss 1.1568266927497461e-05 BETTER
I0326 10:27:25.547346 1579999 finetune.py:68] layer 22_q @ epoch 2 new loss 1.0800993550219573e-05 old loss 1.0984449545503594e-05 BETTER
I0326 10:27:40.608600 1580704 finetune.py:68] layer 23_q @ epoch 1 new loss 1.0783833204186521e-05 old loss 1.10070241134963e-05 BETTER
I0326 10:28:02.546754 1579999 finetune.py:68] layer 22_q @ epoch 3 new loss 1.0660407497198321e-05 old loss 1.0800993550219573e-05 BETTER
I0326 10:28:15.387690 1580704 finetune.py:68] layer 23_q @ epoch 2 new loss 1.0615475730446633e-05 old loss 1.0783833204186521e-05 BETTER
I0326 10:28:39.462363 1579999 finetune.py:68] layer 22_q @ epoch 4 new loss 1.054593303706497e-05 old loss 1.0660407497198321e-05 BETTER
I0326 10:28:47.822227 1579999 finetune.py:45] layer 22_k initial loss 1.1592861483222805e-05
I0326 10:28:50.127250 1580704 finetune.py:68] layer 23_q @ epoch 3 new loss 1.0488377483852673e-05 old loss 1.0615475730446633e-05 BETTER
I0326 10:29:23.612326 1579999 finetune.py:68] layer 22_k @ epoch 0 new loss 1.1387753147573676e-05 old loss 1.1592861483222805e-05 BETTER
I0326 10:29:24.807455 1580704 finetune.py:68] layer 23_q @ epoch 4 new loss 1.0385314453742467e-05 old loss 1.0488377483852673e-05 BETTER
I0326 10:29:32.713516 1580704 finetune.py:45] layer 23_k initial loss 1.1403552889532875e-05
I0326 10:30:00.159179 1579999 finetune.py:68] layer 22_k @ epoch 1 new loss 1.128759868151974e-05 old loss 1.1387753147573676e-05 BETTER
I0326 10:30:06.483070 1580704 finetune.py:68] layer 23_k @ epoch 0 new loss 1.1258483027631883e-05 old loss 1.1403552889532875e-05 BETTER
I0326 10:30:36.700479 1579999 finetune.py:68] layer 22_k @ epoch 2 new loss 1.1205870578123722e-05 old loss 1.128759868151974e-05 BETTER
I0326 10:30:40.987042 1580704 finetune.py:68] layer 23_k @ epoch 1 new loss 1.116869316319935e-05 old loss 1.1258483027631883e-05 BETTER
I0326 10:31:13.355841 1579999 finetune.py:68] layer 22_k @ epoch 3 new loss 1.1141246432089247e-05 old loss 1.1205870578123722e-05 BETTER
I0326 10:31:15.383357 1580704 finetune.py:68] layer 23_k @ epoch 2 new loss 1.1093000466644298e-05 old loss 1.116869316319935e-05 BETTER
I0326 10:31:49.969762 1580704 finetune.py:68] layer 23_k @ epoch 3 new loss 1.1033956980099902e-05 old loss 1.1093000466644298e-05 BETTER
I0326 10:31:50.054615 1579999 finetune.py:68] layer 22_k @ epoch 4 new loss 1.1083677236456424e-05 old loss 1.1141246432089247e-05 BETTER
I0326 10:32:00.000562 1579999 finetune.py:45] layer 22_o initial loss 3.376552922418341e-05
I0326 10:32:24.461138 1580704 finetune.py:68] layer 23_k @ epoch 4 new loss 1.098021493817214e-05 old loss 1.1033956980099902e-05 BETTER
I0326 10:32:34.088193 1580704 finetune.py:45] layer 23_o initial loss 3.0240007617976516e-05
I0326 10:32:35.066658 1579999 finetune.py:68] layer 22_o @ epoch 0 new loss 3.1235336791723967e-05 old loss 3.376552922418341e-05 BETTER
I0326 10:33:07.307949 1580704 finetune.py:68] layer 23_o @ epoch 0 new loss 2.8190643206471577e-05 old loss 3.0240007617976516e-05 BETTER
I0326 10:33:10.824569 1579999 finetune.py:68] layer 22_o @ epoch 1 new loss 3.0242277716752142e-05 old loss 3.1235336791723967e-05 BETTER
I0326 10:33:41.249794 1580704 finetune.py:68] layer 23_o @ epoch 1 new loss 2.746288009802811e-05 old loss 2.8190643206471577e-05 BETTER
I0326 10:33:46.672102 1579999 finetune.py:68] layer 22_o @ epoch 2 new loss 2.957803189929109e-05 old loss 3.0242277716752142e-05 BETTER
I0326 10:34:15.394776 1580704 finetune.py:68] layer 23_o @ epoch 2 new loss 2.6967945814249106e-05 old loss 2.746288009802811e-05 BETTER
I0326 10:34:22.813764 1579999 finetune.py:68] layer 22_o @ epoch 3 new loss 2.9087779694236815e-05 old loss 2.957803189929109e-05 BETTER
I0326 10:34:49.526341 1580704 finetune.py:68] layer 23_o @ epoch 3 new loss 2.660775135154836e-05 old loss 2.6967945814249106e-05 BETTER
I0326 10:34:58.875397 1579999 finetune.py:68] layer 22_o @ epoch 4 new loss 2.869628951884806e-05 old loss 2.9087779694236815e-05 BETTER
I0326 10:35:21.433809 1579999 finetune.py:45] layer 22_up initial loss 8.912950579542667e-05
I0326 10:35:23.663588 1580704 finetune.py:68] layer 23_o @ epoch 4 new loss 2.633395342854783e-05 old loss 2.660775135154836e-05 BETTER
I0326 10:35:46.093116 1580704 finetune.py:45] layer 23_up initial loss 9.19875456020236e-05
I0326 10:35:53.694505 1579999 finetune.py:68] layer 22_up @ epoch 0 new loss 8.519706898368895e-05 old loss 8.912950579542667e-05 BETTER
I0326 10:36:16.989497 1580704 finetune.py:68] layer 23_up @ epoch 0 new loss 8.823645475786179e-05 old loss 9.19875456020236e-05 BETTER
I0326 10:36:27.361052 1579999 finetune.py:68] layer 22_up @ epoch 1 new loss 8.325809903908521e-05 old loss 8.519706898368895e-05 BETTER
I0326 10:36:49.177732 1580704 finetune.py:68] layer 23_up @ epoch 1 new loss 8.632695971755311e-05 old loss 8.823645475786179e-05 BETTER
I0326 10:37:01.208438 1579999 finetune.py:68] layer 22_up @ epoch 2 new loss 8.177482959581539e-05 old loss 8.325809903908521e-05 BETTER
I0326 10:37:21.378791 1580704 finetune.py:68] layer 23_up @ epoch 2 new loss 8.490154868923128e-05 old loss 8.632695971755311e-05 BETTER
I0326 10:37:35.227034 1579999 finetune.py:68] layer 22_up @ epoch 3 new loss 8.057660306803882e-05 old loss 8.177482959581539e-05 BETTER
I0326 10:37:53.697729 1580704 finetune.py:68] layer 23_up @ epoch 3 new loss 8.374382741749287e-05 old loss 8.490154868923128e-05 BETTER
I0326 10:38:09.405989 1579999 finetune.py:68] layer 22_up @ epoch 4 new loss 7.956296030897647e-05 old loss 8.057660306803882e-05 BETTER
I0326 10:38:25.950814 1580704 finetune.py:68] layer 23_up @ epoch 4 new loss 8.278337918454781e-05 old loss 8.374382741749287e-05 BETTER
I0326 10:38:31.668687 1579999 finetune.py:45] layer 22_gate initial loss 0.00010222125274594873
I0326 10:38:48.156895 1580704 finetune.py:45] layer 23_gate initial loss 0.00010813028347911313
I0326 10:39:01.937314 1579999 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.00010095354809891433 old loss 0.00010222125274594873 BETTER
I0326 10:39:16.894147 1580704 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.00010697169636841863 old loss 0.00010813028347911313 BETTER
I0326 10:39:33.227196 1579999 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.00010002769704442471 old loss 0.00010095354809891433 BETTER
I0326 10:39:46.643745 1580704 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.00010612363985273987 old loss 0.00010697169636841863 BETTER
I0326 10:40:04.769151 1579999 finetune.py:68] layer 22_gate @ epoch 2 new loss 9.924486221279949e-05 old loss 0.00010002769704442471 BETTER
I0326 10:40:16.536109 1580704 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.00010539423965383321 old loss 0.00010612363985273987 BETTER
I0326 10:40:36.387101 1579999 finetune.py:68] layer 22_gate @ epoch 3 new loss 9.856082760961726e-05 old loss 9.924486221279949e-05 BETTER
I0326 10:40:46.571285 1580704 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.00010475911403773353 old loss 0.00010539423965383321 BETTER
I0326 10:41:08.059631 1579999 finetune.py:68] layer 22_gate @ epoch 4 new loss 9.796149970497936e-05 old loss 9.856082760961726e-05 BETTER
I0326 10:41:16.733655 1580704 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.00010421113984193653 old loss 0.00010475911403773353 BETTER
I0326 10:41:31.458365 1579999 finetune.py:45] layer 22_down initial loss 0.00018123281188309193
I0326 10:41:39.602255 1580704 finetune.py:45] layer 23_down initial loss 0.00019043177599087358
I0326 10:41:59.300595 1579999 finetune.py:68] layer 22_down @ epoch 0 new loss 0.00018122035544365644 old loss 0.00018123281188309193 BETTER
I0326 10:42:06.276392 1580704 finetune.py:68] layer 23_down @ epoch 0 new loss 0.00019042109488509595 old loss 0.00019043177599087358 BETTER
I0326 10:42:28.202601 1579999 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0001812130940379575 old loss 0.00018122035544365644 BETTER
I0326 10:42:33.968513 1580704 finetune.py:68] layer 23_down @ epoch 1 new loss 0.00019041349878534675 old loss 0.00019042109488509595 BETTER
I0326 10:42:57.226723 1579999 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0001812070404412225 old loss 0.0001812130940379575 BETTER
I0326 10:43:01.962469 1580704 finetune.py:68] layer 23_down @ epoch 2 new loss 0.00019040795450564474 old loss 0.00019041349878534675 BETTER
I0326 10:43:26.403933 1579999 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0001812014088500291 old loss 0.0001812070404412225 BETTER
I0326 10:43:29.741650 1580704 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0001904033706523478 old loss 0.00019040795450564474 BETTER
I0326 10:43:55.700138 1579999 finetune.py:68] layer 22_down @ epoch 4 new loss 0.00018119654851034284 old loss 0.0001812014088500291 BETTER
22_v proxy err 0.01233911607414484 tr(WHW.T) 346.0789489746094
bpp_loss 3.3950884342193604
22_q proxy err 0.0009114883723668754 tr(WHW.T) 20330.353515625
bpp_loss 4.062069654464722
22_k proxy err 0.00036918159457854927 tr(WHW.T) 14730.443359375
bpp_loss 4.997945308685303
22_o proxy err 0.01720157265663147 tr(WHW.T) 1222.597412109375
bpp_loss 3.405357599258423
22_up proxy err 0.010968799702823162 tr(WHW.T) 7551.494140625
bpp_loss 3.455313410077776
22_gate proxy err 0.002916420577093959 tr(WHW.T) 29516.3984375
bpp_loss 3.8451403209141324
22_down proxy err 0.011609680019319057 tr(WHW.T) 6568.17578125
bpp_loss 3.450918367930821
I0326 10:43:57.446878 1580704 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0001903991651488468 old loss 0.0001904033706523478 BETTER
23_v proxy err 0.011445276439189911 tr(WHW.T) 397.90704345703125
bpp_loss 3.44738507270813
23_q proxy err 0.0008701403276063502 tr(WHW.T) 22621.8828125
bpp_loss 4.074288845062256
23_k proxy err 0.0003812793584074825 tr(WHW.T) 14864.9404296875
bpp_loss 5.000295400619507
23_o proxy err 0.012149941176176071 tr(WHW.T) 1747.522705078125
bpp_loss 3.4286749362945557
23_up proxy err 0.011168349534273148 tr(WHW.T) 7424.31298828125
bpp_loss 3.4592911856515065
23_gate proxy err 0.0031585812103003263 tr(WHW.T) 27256.03515625
bpp_loss 3.8474721908569336
23_down proxy err 0.011316330172121525 tr(WHW.T) 6698.42919921875
bpp_loss 3.4562435490744456
I0326 10:45:05.822166 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 63.660369634628296s
I0326 10:45:09.274720 1593037 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 10:45:09.274811 1593037 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 10:45:09.274848 1593037 utils.py:162] NumExpr defaulting to 16 threads.
I0326 10:45:09.610166 1593037 config.py:54] PyTorch version 2.6.0 available.
W0326 10:45:09.802688 1593037 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 10:45:10.411970 1593037 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 10:45:10.415542 1418472 quantize_finetune_llama.py:209] layer 25 gpu 1
I0326 10:45:10.428456 1593037 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 10:45:17.903905 1593037 finetune.py:45] layer 24_v initial loss 2.012379809457343e-05
W0326 10:45:17.904145 1593037 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 10:45:54.031222 1593037 finetune.py:68] layer 24_v @ epoch 0 new loss 1.0828581253008451e-05 old loss 2.012379809457343e-05 BETTER
I0326 10:46:13.386625 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 62.537214040756226s
I0326 10:46:17.052592 1593742 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 10:46:17.052701 1593742 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 10:46:17.052746 1593742 utils.py:162] NumExpr defaulting to 16 threads.
I0326 10:46:17.389488 1593742 config.py:54] PyTorch version 2.6.0 available.
W0326 10:46:17.602100 1593742 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 10:46:18.197349 1593742 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 10:46:18.200940 1418472 quantize_finetune_llama.py:209] layer 26 gpu 0
I0326 10:46:18.214088 1593742 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 10:46:25.449156 1593742 finetune.py:45] layer 25_v initial loss 2.3224498363560997e-05
W0326 10:46:25.449415 1593742 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 10:46:31.080701 1593037 finetune.py:68] layer 24_v @ epoch 1 new loss 1.009908828564221e-05 old loss 1.0828581253008451e-05 BETTER
I0326 10:46:58.982493 1593742 finetune.py:68] layer 25_v @ epoch 0 new loss 1.2829701518057846e-05 old loss 2.3224498363560997e-05 BETTER
I0326 10:47:08.283079 1593037 finetune.py:68] layer 24_v @ epoch 2 new loss 9.749394848768134e-06 old loss 1.009908828564221e-05 BETTER
I0326 10:47:33.833320 1593742 finetune.py:68] layer 25_v @ epoch 1 new loss 1.1974689186899923e-05 old loss 1.2829701518057846e-05 BETTER
I0326 10:47:45.744544 1593037 finetune.py:68] layer 24_v @ epoch 3 new loss 9.545355169393588e-06 old loss 9.749394848768134e-06 BETTER
I0326 10:48:08.803221 1593742 finetune.py:68] layer 25_v @ epoch 2 new loss 1.1525063200679142e-05 old loss 1.1974689186899923e-05 BETTER
I0326 10:48:23.300932 1593037 finetune.py:68] layer 24_v @ epoch 4 new loss 9.378621143696364e-06 old loss 9.545355169393588e-06 BETTER
I0326 10:48:33.410680 1593037 finetune.py:45] layer 24_q initial loss 1.2268252248759381e-05
I0326 10:48:43.876621 1593742 finetune.py:68] layer 25_v @ epoch 3 new loss 1.1389639439585153e-05 old loss 1.1525063200679142e-05 BETTER
I0326 10:49:09.656339 1593037 finetune.py:68] layer 24_q @ epoch 0 new loss 1.1741587513824925e-05 old loss 1.2268252248759381e-05 BETTER
I0326 10:49:19.925713 1593742 finetune.py:68] layer 25_v @ epoch 4 new loss 1.1139940397697501e-05 old loss 1.1389639439585153e-05 BETTER
I0326 10:49:29.655461 1593742 finetune.py:45] layer 25_q initial loss 1.674683517194353e-05
I0326 10:49:46.561810 1593037 finetune.py:68] layer 24_q @ epoch 1 new loss 1.1501737390062772e-05 old loss 1.1741587513824925e-05 BETTER
I0326 10:50:03.515076 1593742 finetune.py:68] layer 25_q @ epoch 0 new loss 1.5269537470885552e-05 old loss 1.674683517194353e-05 BETTER
I0326 10:50:23.692203 1593037 finetune.py:68] layer 24_q @ epoch 2 new loss 1.1341109711793251e-05 old loss 1.1501737390062772e-05 BETTER
I0326 10:50:38.091531 1593742 finetune.py:68] layer 25_q @ epoch 1 new loss 1.4785096027480904e-05 old loss 1.5269537470885552e-05 BETTER
I0326 10:51:00.563667 1593037 finetune.py:68] layer 24_q @ epoch 3 new loss 1.1229838491999544e-05 old loss 1.1341109711793251e-05 BETTER
I0326 10:51:12.668884 1593742 finetune.py:68] layer 25_q @ epoch 2 new loss 1.4479235687758774e-05 old loss 1.4785096027480904e-05 BETTER
I0326 10:51:37.483685 1593037 finetune.py:68] layer 24_q @ epoch 4 new loss 1.1120996532554273e-05 old loss 1.1229838491999544e-05 BETTER
I0326 10:51:45.617297 1593037 finetune.py:45] layer 24_k initial loss 1.251537923963042e-05
I0326 10:51:47.417240 1593742 finetune.py:68] layer 25_q @ epoch 3 new loss 1.4244042176869698e-05 old loss 1.4479235687758774e-05 BETTER
I0326 10:52:21.955327 1593037 finetune.py:68] layer 24_k @ epoch 0 new loss 1.2389484254526906e-05 old loss 1.251537923963042e-05 BETTER
I0326 10:52:22.151506 1593742 finetune.py:68] layer 25_q @ epoch 4 new loss 1.4026046301296446e-05 old loss 1.4244042176869698e-05 BETTER
I0326 10:52:29.995913 1593742 finetune.py:45] layer 25_k initial loss 1.6523685189895332e-05
I0326 10:52:58.974980 1593037 finetune.py:68] layer 24_k @ epoch 1 new loss 1.2307379620324355e-05 old loss 1.2389484254526906e-05 BETTER
I0326 10:53:03.914028 1593742 finetune.py:68] layer 25_k @ epoch 0 new loss 1.6226233128691092e-05 old loss 1.6523685189895332e-05 BETTER
I0326 10:53:36.153231 1593037 finetune.py:68] layer 24_k @ epoch 2 new loss 1.2250899999344256e-05 old loss 1.2307379620324355e-05 BETTER
I0326 10:53:38.595061 1593742 finetune.py:68] layer 25_k @ epoch 1 new loss 1.606856494618114e-05 old loss 1.6226233128691092e-05 BETTER
I0326 10:54:13.227019 1593037 finetune.py:68] layer 24_k @ epoch 3 new loss 1.220255307998741e-05 old loss 1.2250899999344256e-05 BETTER
I0326 10:54:13.272229 1593742 finetune.py:68] layer 25_k @ epoch 2 new loss 1.5947623978718184e-05 old loss 1.606856494618114e-05 BETTER
I0326 10:54:48.079990 1593742 finetune.py:68] layer 25_k @ epoch 3 new loss 1.5849951523705386e-05 old loss 1.5947623978718184e-05 BETTER
I0326 10:54:50.301666 1593037 finetune.py:68] layer 24_k @ epoch 4 new loss 1.2147122106398456e-05 old loss 1.220255307998741e-05 BETTER
I0326 10:55:00.417835 1593037 finetune.py:45] layer 24_o initial loss 3.4617562050698325e-05
I0326 10:55:22.901710 1593742 finetune.py:68] layer 25_k @ epoch 4 new loss 1.578796400281135e-05 old loss 1.5849951523705386e-05 BETTER
I0326 10:55:32.720834 1593742 finetune.py:45] layer 25_o initial loss 4.025770613225177e-05
I0326 10:55:35.989883 1593037 finetune.py:68] layer 24_o @ epoch 0 new loss 3.2532385375816375e-05 old loss 3.4617562050698325e-05 BETTER
I0326 10:56:06.359985 1593742 finetune.py:68] layer 25_o @ epoch 0 new loss 3.737457518582232e-05 old loss 4.025770613225177e-05 BETTER
I0326 10:56:12.454871 1593037 finetune.py:68] layer 24_o @ epoch 1 new loss 3.177957842126489e-05 old loss 3.2532385375816375e-05 BETTER
I0326 10:56:40.637631 1593742 finetune.py:68] layer 25_o @ epoch 1 new loss 3.6392153560882434e-05 old loss 3.737457518582232e-05 BETTER
I0326 10:56:49.014309 1593037 finetune.py:68] layer 24_o @ epoch 2 new loss 3.1282561394618824e-05 old loss 3.177957842126489e-05 BETTER
I0326 10:57:14.719928 1593742 finetune.py:68] layer 25_o @ epoch 2 new loss 3.576394010451622e-05 old loss 3.6392153560882434e-05 BETTER
I0326 10:57:25.634311 1593037 finetune.py:68] layer 24_o @ epoch 3 new loss 3.090869722655043e-05 old loss 3.1282561394618824e-05 BETTER
I0326 10:57:48.799201 1593742 finetune.py:68] layer 25_o @ epoch 3 new loss 3.531541733536869e-05 old loss 3.576394010451622e-05 BETTER
I0326 10:58:02.239614 1593037 finetune.py:68] layer 24_o @ epoch 4 new loss 3.0614854040322825e-05 old loss 3.090869722655043e-05 BETTER
I0326 10:58:23.275537 1593742 finetune.py:68] layer 25_o @ epoch 4 new loss 3.4983626392204314e-05 old loss 3.531541733536869e-05 BETTER
I0326 10:58:25.238650 1593037 finetune.py:45] layer 24_up initial loss 0.00010052723519038409
I0326 10:58:46.673694 1593742 finetune.py:45] layer 25_up initial loss 0.00011281738261459395
I0326 10:58:57.761123 1593037 finetune.py:68] layer 24_up @ epoch 0 new loss 9.677101479610428e-05 old loss 0.00010052723519038409 BETTER
I0326 10:59:17.828652 1593742 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0001084537580027245 old loss 0.00011281738261459395 BETTER
I0326 10:59:31.938951 1593037 finetune.py:68] layer 24_up @ epoch 1 new loss 9.491403034189716e-05 old loss 9.677101479610428e-05 BETTER
I0326 10:59:50.267894 1593742 finetune.py:68] layer 25_up @ epoch 1 new loss 0.00010641779226716608 old loss 0.0001084537580027245 BETTER
I0326 11:00:06.002675 1593037 finetune.py:68] layer 24_up @ epoch 2 new loss 9.353824134450406e-05 old loss 9.491403034189716e-05 BETTER
I0326 11:00:22.675897 1593742 finetune.py:68] layer 25_up @ epoch 2 new loss 0.00010490553540876135 old loss 0.00010641779226716608 BETTER
I0326 11:00:40.594621 1593037 finetune.py:68] layer 24_up @ epoch 3 new loss 9.240921644959599e-05 old loss 9.353824134450406e-05 BETTER
I0326 11:00:55.233957 1593742 finetune.py:68] layer 25_up @ epoch 3 new loss 0.00010369436495238915 old loss 0.00010490553540876135 BETTER
I0326 11:01:15.128060 1593037 finetune.py:68] layer 24_up @ epoch 4 new loss 9.148239041678607e-05 old loss 9.240921644959599e-05 BETTER
I0326 11:01:29.034437 1593742 finetune.py:68] layer 25_up @ epoch 4 new loss 0.00010268247569911182 old loss 0.00010369436495238915 BETTER
I0326 11:01:37.547246 1593037 finetune.py:45] layer 24_gate initial loss 0.00011996843386441469
I0326 11:01:52.405701 1593742 finetune.py:45] layer 25_gate initial loss 0.00013540305371861905
I0326 11:02:07.985059 1593037 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.00011885565618285909 old loss 0.00011996843386441469 BETTER
I0326 11:02:21.071306 1593742 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.00013417865557130426 old loss 0.00013540305371861905 BETTER
I0326 11:02:39.525930 1593037 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0001180210747406818 old loss 0.00011885565618285909 BETTER
I0326 11:02:50.760711 1593742 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.00013327105261851102 old loss 0.00013417865557130426 BETTER
I0326 11:03:11.078994 1593037 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.00011732841085176915 old loss 0.0001180210747406818 BETTER
I0326 11:03:20.727984 1593742 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.000132525252411142 old loss 0.00013327105261851102 BETTER
I0326 11:03:42.847458 1593037 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.00011671070387819782 old loss 0.00011732841085176915 BETTER
I0326 11:03:50.633443 1593742 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.00013187441800255328 old loss 0.000132525252411142 BETTER
I0326 11:04:14.536959 1593037 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.00011618647840805352 old loss 0.00011671070387819782 BETTER
I0326 11:04:20.655707 1593742 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0001313124957960099 old loss 0.00013187441800255328 BETTER
I0326 11:04:38.729030 1593037 finetune.py:45] layer 24_down initial loss 0.00020514873904176056
I0326 11:04:45.353077 1593742 finetune.py:45] layer 25_down initial loss 0.00022693884966429323
I0326 11:05:06.538645 1593037 finetune.py:68] layer 24_down @ epoch 0 new loss 0.00020514032803475857 old loss 0.00020514873904176056 BETTER
I0326 11:05:11.994336 1593742 finetune.py:68] layer 25_down @ epoch 0 new loss 0.00022692521451972425 old loss 0.00022693884966429323 BETTER
I0326 11:05:35.653091 1593037 finetune.py:68] layer 24_down @ epoch 1 new loss 0.00020513293566182256 old loss 0.00020514032803475857 BETTER
I0326 11:05:40.087282 1593742 finetune.py:68] layer 25_down @ epoch 1 new loss 0.00022691367485094815 old loss 0.00022692521451972425 BETTER
I0326 11:06:04.932950 1593037 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0002051270566880703 old loss 0.00020513293566182256 BETTER
I0326 11:06:07.976730 1593742 finetune.py:68] layer 25_down @ epoch 2 new loss 0.00022690484183840454 old loss 0.00022691367485094815 BETTER
I0326 11:06:34.151916 1593037 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0002051210612989962 old loss 0.0002051270566880703 BETTER
I0326 11:06:36.060889 1593742 finetune.py:68] layer 25_down @ epoch 3 new loss 0.00022689613979309797 old loss 0.00022690484183840454 BETTER
I0326 11:07:03.724646 1593037 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0002051166957244277 old loss 0.0002051210612989962 BETTER
I0326 11:07:04.076236 1593742 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0002268889220431447 old loss 0.00022689613979309797 BETTER
24_v proxy err 0.009586886502802372 tr(WHW.T) 467.2783508300781
bpp_loss 3.5463290214538574
24_q proxy err 0.0008681469480507076 tr(WHW.T) 22456.8046875
bpp_loss 4.04053521156311
24_k proxy err 0.0003946317592635751 tr(WHW.T) 14195.0400390625
bpp_loss 4.827976226806641
24_o proxy err 0.012675441801548004 tr(WHW.T) 1592.9439697265625
bpp_loss 3.470295548439026
24_up proxy err 0.011374974623322487 tr(WHW.T) 7311.04443359375
bpp_loss 3.463962963649205
24_gate proxy err 0.0033390780445188284 tr(WHW.T) 25848.8203125
bpp_loss 3.853785651070731
24_down proxy err 0.010999160818755627 tr(WHW.T) 6778.68359375
bpp_loss 3.461681297847203
25_v proxy err 0.008220585063099861 tr(WHW.T) 557.8086547851562
bpp_loss 3.556941866874695
25_q proxy err 0.0007628875318914652 tr(WHW.T) 26135.0546875
bpp_loss 4.0192039012908936
25_k proxy err 0.00038906201370991766 tr(WHW.T) 14429.23828125
bpp_loss 4.810135126113892
25_o proxy err 0.010048780590295792 tr(WHW.T) 1994.8509521484375
bpp_loss 3.4694483280181885
25_up proxy err 0.011218104511499405 tr(WHW.T) 7384.822265625
bpp_loss 3.4734061104910716
25_gate proxy err 0.0032735213171690702 tr(WHW.T) 26261.30078125
bpp_loss 3.8631153106689453
25_down proxy err 0.010852326638996601 tr(WHW.T) 6653.2275390625
bpp_loss 3.4707864352634976
I0326 11:08:05.786629 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 56.47094535827637s
I0326 11:08:09.229195 1606089 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 11:08:09.229293 1606089 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 11:08:09.229334 1606089 utils.py:162] NumExpr defaulting to 16 threads.
I0326 11:08:09.565258 1606089 config.py:54] PyTorch version 2.6.0 available.
W0326 11:08:09.758525 1606089 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 11:08:10.345723 1606089 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 11:08:10.349829 1418472 quantize_finetune_llama.py:209] layer 27 gpu 1
I0326 11:08:10.369761 1606089 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 11:08:17.808168 1606089 finetune.py:45] layer 26_v initial loss 3.094775456702337e-05
W0326 11:08:17.808409 1606089 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 11:08:53.549059 1606089 finetune.py:68] layer 26_v @ epoch 0 new loss 1.8706234186538495e-05 old loss 3.094775456702337e-05 BETTER
I0326 11:09:13.179102 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 62.42156219482422s
I0326 11:09:16.839849 1606794 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 11:09:16.839968 1606794 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 11:09:16.840015 1606794 utils.py:162] NumExpr defaulting to 16 threads.
I0326 11:09:17.184431 1606794 config.py:54] PyTorch version 2.6.0 available.
W0326 11:09:17.396025 1606794 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 11:09:17.971453 1606794 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 11:09:17.975153 1418472 quantize_finetune_llama.py:209] layer 28 gpu 0
I0326 11:09:17.988120 1606794 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 11:09:25.236637 1606794 finetune.py:45] layer 27_v initial loss 3.0279625207185745e-05
W0326 11:09:25.236897 1606794 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 11:09:30.433362 1606089 finetune.py:68] layer 26_v @ epoch 1 new loss 1.7379647033521906e-05 old loss 1.8706234186538495e-05 BETTER
I0326 11:09:58.794642 1606794 finetune.py:68] layer 27_v @ epoch 0 new loss 1.7196263797814026e-05 old loss 3.0279625207185745e-05 BETTER
I0326 11:10:07.342279 1606089 finetune.py:68] layer 26_v @ epoch 2 new loss 1.6707585018593818e-05 old loss 1.7379647033521906e-05 BETTER
I0326 11:10:33.360713 1606794 finetune.py:68] layer 27_v @ epoch 1 new loss 1.6193187548196875e-05 old loss 1.7196263797814026e-05 BETTER
I0326 11:10:44.689707 1606089 finetune.py:68] layer 26_v @ epoch 3 new loss 1.624721335247159e-05 old loss 1.6707585018593818e-05 BETTER
I0326 11:11:08.498267 1606794 finetune.py:68] layer 27_v @ epoch 2 new loss 1.5805957445991226e-05 old loss 1.6193187548196875e-05 BETTER
I0326 11:11:21.644470 1606089 finetune.py:68] layer 26_v @ epoch 4 new loss 1.5900157450232655e-05 old loss 1.624721335247159e-05 BETTER
I0326 11:11:31.595753 1606089 finetune.py:45] layer 26_q initial loss 2.0472469259402715e-05
I0326 11:11:43.520233 1606794 finetune.py:68] layer 27_v @ epoch 3 new loss 1.5500732843065634e-05 old loss 1.5805957445991226e-05 BETTER
I0326 11:12:07.471721 1606089 finetune.py:68] layer 26_q @ epoch 0 new loss 1.9400600649532862e-05 old loss 2.0472469259402715e-05 BETTER
I0326 11:12:18.655444 1606794 finetune.py:68] layer 27_v @ epoch 4 new loss 1.5062014426803216e-05 old loss 1.5500732843065634e-05 BETTER
I0326 11:12:28.417146 1606794 finetune.py:45] layer 27_q initial loss 2.1943360479781404e-05
I0326 11:12:44.162418 1606089 finetune.py:68] layer 26_q @ epoch 1 new loss 1.8938442735816352e-05 old loss 1.9400600649532862e-05 BETTER
I0326 11:13:02.203705 1606794 finetune.py:68] layer 27_q @ epoch 0 new loss 2.07486555154901e-05 old loss 2.1943360479781404e-05 BETTER
I0326 11:13:20.894174 1606089 finetune.py:68] layer 26_q @ epoch 2 new loss 1.8621774870553054e-05 old loss 1.8938442735816352e-05 BETTER
I0326 11:13:36.696684 1606794 finetune.py:68] layer 27_q @ epoch 1 new loss 2.0256598872947507e-05 old loss 2.07486555154901e-05 BETTER
I0326 11:13:57.606827 1606089 finetune.py:68] layer 26_q @ epoch 3 new loss 1.8359185560257174e-05 old loss 1.8621774870553054e-05 BETTER
I0326 11:14:11.246755 1606794 finetune.py:68] layer 27_q @ epoch 2 new loss 1.998310472117737e-05 old loss 2.0256598872947507e-05 BETTER
I0326 11:14:34.373223 1606089 finetune.py:68] layer 26_q @ epoch 4 new loss 1.817024894990027e-05 old loss 1.8359185560257174e-05 BETTER
I0326 11:14:42.455075 1606089 finetune.py:45] layer 26_k initial loss 1.9976792827947065e-05
I0326 11:14:46.150108 1606794 finetune.py:68] layer 27_q @ epoch 3 new loss 1.9776469343923964e-05 old loss 1.998310472117737e-05 BETTER
I0326 11:15:18.446988 1606089 finetune.py:68] layer 26_k @ epoch 0 new loss 1.9657958546304144e-05 old loss 1.9976792827947065e-05 BETTER
I0326 11:15:20.852626 1606794 finetune.py:68] layer 27_q @ epoch 4 new loss 1.9606286514317617e-05 old loss 1.9776469343923964e-05 BETTER
I0326 11:15:28.766388 1606794 finetune.py:45] layer 27_k initial loss 2.2986569092608988e-05
I0326 11:15:55.208050 1606089 finetune.py:68] layer 26_k @ epoch 1 new loss 1.952177808561828e-05 old loss 1.9657958546304144e-05 BETTER
I0326 11:16:02.626437 1606794 finetune.py:68] layer 27_k @ epoch 0 new loss 2.2656495275441557e-05 old loss 2.2986569092608988e-05 BETTER
I0326 11:16:32.150162 1606089 finetune.py:68] layer 26_k @ epoch 2 new loss 1.9405153580009937e-05 old loss 1.952177808561828e-05 BETTER
I0326 11:16:37.220439 1606794 finetune.py:68] layer 27_k @ epoch 1 new loss 2.2415148123400286e-05 old loss 2.2656495275441557e-05 BETTER
I0326 11:17:09.174383 1606089 finetune.py:68] layer 26_k @ epoch 3 new loss 1.9324783352203667e-05 old loss 1.9405153580009937e-05 BETTER
I0326 11:17:11.901707 1606794 finetune.py:68] layer 27_k @ epoch 2 new loss 2.2303753212327138e-05 old loss 2.2415148123400286e-05 BETTER
I0326 11:17:46.174216 1606089 finetune.py:68] layer 26_k @ epoch 4 new loss 1.923193303809967e-05 old loss 1.9324783352203667e-05 BETTER
I0326 11:17:46.597651 1606794 finetune.py:68] layer 27_k @ epoch 3 new loss 2.2220308892428875e-05 old loss 2.2303753212327138e-05 BETTER
I0326 11:17:56.213509 1606089 finetune.py:45] layer 26_o initial loss 5.834858166053891e-05
I0326 11:18:21.321326 1606794 finetune.py:68] layer 27_k @ epoch 4 new loss 2.2166201233631e-05 old loss 2.2220308892428875e-05 BETTER
I0326 11:18:31.154440 1606794 finetune.py:45] layer 27_o initial loss 6.799836410209537e-05
I0326 11:18:31.634636 1606089 finetune.py:68] layer 26_o @ epoch 0 new loss 5.3647429012926295e-05 old loss 5.834858166053891e-05 BETTER
I0326 11:19:04.436816 1606794 finetune.py:68] layer 27_o @ epoch 0 new loss 6.280683010118082e-05 old loss 6.799836410209537e-05 BETTER
I0326 11:19:07.710722 1606089 finetune.py:68] layer 26_o @ epoch 1 new loss 5.204832996241748e-05 old loss 5.3647429012926295e-05 BETTER
I0326 11:19:38.720283 1606794 finetune.py:68] layer 27_o @ epoch 1 new loss 6.109872629167512e-05 old loss 6.280683010118082e-05 BETTER
I0326 11:19:43.790240 1606089 finetune.py:68] layer 26_o @ epoch 2 new loss 5.100025373394601e-05 old loss 5.204832996241748e-05 BETTER
I0326 11:20:13.042252 1606794 finetune.py:68] layer 27_o @ epoch 2 new loss 6.0007445426890627e-05 old loss 6.109872629167512e-05 BETTER
I0326 11:20:20.035312 1606089 finetune.py:68] layer 26_o @ epoch 3 new loss 5.020222670282237e-05 old loss 5.100025373394601e-05 BETTER
I0326 11:20:47.159995 1606794 finetune.py:68] layer 27_o @ epoch 3 new loss 5.9176687500439584e-05 old loss 6.0007445426890627e-05 BETTER
I0326 11:20:56.397361 1606089 finetune.py:68] layer 26_o @ epoch 4 new loss 4.956722114002332e-05 old loss 5.020222670282237e-05 BETTER
I0326 11:21:18.659674 1606089 finetune.py:45] layer 26_up initial loss 0.00014002708485350013
I0326 11:21:21.147850 1606794 finetune.py:68] layer 27_o @ epoch 4 new loss 5.856506686541252e-05 old loss 5.9176687500439584e-05 BETTER
I0326 11:21:43.859051 1606794 finetune.py:45] layer 27_up initial loss 0.0001635500811971724
I0326 11:21:50.819389 1606089 finetune.py:68] layer 26_up @ epoch 0 new loss 0.0001348655205219984 old loss 0.00014002708485350013 BETTER
I0326 11:22:14.699799 1606794 finetune.py:68] layer 27_up @ epoch 0 new loss 0.00015692636952735484 old loss 0.0001635500811971724 BETTER
I0326 11:22:24.490901 1606089 finetune.py:68] layer 26_up @ epoch 1 new loss 0.00013238015526439995 old loss 0.0001348655205219984 BETTER
I0326 11:22:46.710276 1606794 finetune.py:68] layer 27_up @ epoch 1 new loss 0.00015392558998428285 old loss 0.00015692636952735484 BETTER
I0326 11:22:58.291942 1606089 finetune.py:68] layer 26_up @ epoch 2 new loss 0.00013053476868662983 old loss 0.00013238015526439995 BETTER
I0326 11:23:19.026020 1606794 finetune.py:68] layer 27_up @ epoch 2 new loss 0.00015178092871792614 old loss 0.00015392558998428285 BETTER
I0326 11:23:32.125341 1606089 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0001290370273636654 old loss 0.00013053476868662983 BETTER
I0326 11:23:51.349767 1606794 finetune.py:68] layer 27_up @ epoch 3 new loss 0.00015004505985416472 old loss 0.00015178092871792614 BETTER
I0326 11:24:05.994753 1606089 finetune.py:68] layer 26_up @ epoch 4 new loss 0.00012779330427292734 old loss 0.0001290370273636654 BETTER
I0326 11:24:23.721798 1606794 finetune.py:68] layer 27_up @ epoch 4 new loss 0.00014860345982015133 old loss 0.00015004505985416472 BETTER
I0326 11:24:29.061583 1606089 finetune.py:45] layer 26_gate initial loss 0.00016636212239973247
I0326 11:24:46.761147 1606794 finetune.py:45] layer 27_gate initial loss 0.00019616447389125824
I0326 11:24:59.291309 1606089 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.00016487494576722383 old loss 0.00016636212239973247 BETTER
I0326 11:25:15.620920 1606794 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0001941697410074994 old loss 0.00019616447389125824 BETTER
I0326 11:25:31.052807 1606089 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.00016378525469917804 old loss 0.00016487494576722383 BETTER
I0326 11:25:45.710958 1606794 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.00019282891298644245 old loss 0.0001941697410074994 BETTER
I0326 11:26:02.525747 1606089 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.00016288711049128324 old loss 0.00016378525469917804 BETTER
I0326 11:26:15.552616 1606794 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.00019173405598849058 old loss 0.00019282891298644245 BETTER
I0326 11:26:34.008350 1606089 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.00016208982560783625 old loss 0.00016288711049128324 BETTER
I0326 11:26:45.455039 1606794 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0001908119156723842 old loss 0.00019173405598849058 BETTER
I0326 11:27:05.870399 1606089 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.00016141626110766083 old loss 0.00016208982560783625 BETTER
I0326 11:27:15.304970 1606794 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.00019001305918209255 old loss 0.0001908119156723842 BETTER
I0326 11:27:29.878891 1606089 finetune.py:45] layer 26_down initial loss 0.00027246231911703944
I0326 11:27:39.231585 1606794 finetune.py:45] layer 27_down initial loss 0.0003270410525146872
I0326 11:27:57.585220 1606089 finetune.py:68] layer 26_down @ epoch 0 new loss 0.00027244692319072783 old loss 0.00027246231911703944 BETTER
I0326 11:28:05.941758 1606794 finetune.py:68] layer 27_down @ epoch 0 new loss 0.00032699588336981833 old loss 0.0003270410525146872 BETTER
I0326 11:28:26.522597 1606089 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0002724346413742751 old loss 0.00027244692319072783 BETTER
I0326 11:28:34.076517 1606794 finetune.py:68] layer 27_down @ epoch 1 new loss 0.00032696613925509155 old loss 0.00032699588336981833 BETTER
I0326 11:28:55.526859 1606089 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0002724238729570061 old loss 0.0002724346413742751 BETTER
I0326 11:29:01.976402 1606794 finetune.py:68] layer 27_down @ epoch 2 new loss 0.00032694326364435256 old loss 0.00032696613925509155 BETTER
I0326 11:29:24.569590 1606089 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0002724146470427513 old loss 0.0002724238729570061 BETTER
I0326 11:29:29.817496 1606794 finetune.py:68] layer 27_down @ epoch 3 new loss 0.00032692577224224806 old loss 0.00032694326364435256 BETTER
I0326 11:29:53.582659 1606089 finetune.py:68] layer 26_down @ epoch 4 new loss 0.000272405770374462 old loss 0.0002724146470427513 BETTER
26_v proxy err 0.010048234835267067 tr(WHW.T) 434.54583740234375
bpp_loss 3.6081546545028687
26_q proxy err 0.0008838603971526027 tr(WHW.T) 21410.17578125
bpp_loss 4.029122829437256
26_k proxy err 0.00036247490788809955 tr(WHW.T) 15437.82421875
bpp_loss 4.890638828277588
26_o proxy err 0.008397391065955162 tr(WHW.T) 2389.67919921875
bpp_loss 3.4870121479034424
26_up proxy err 0.010810315608978271 tr(WHW.T) 7650.71484375
bpp_loss 3.4827428545270647
26_gate proxy err 0.00297367456369102 tr(WHW.T) 28890.7109375
bpp_loss 3.8691930770874023
26_down proxy err 0.010941782034933567 tr(WHW.T) 6649.96728515625
bpp_loss 3.4786155905042375
I0326 11:29:57.603376 1606794 finetune.py:68] layer 27_down @ epoch 4 new loss 0.00032690787338651717 old loss 0.00032692577224224806 BETTER
27_v proxy err 0.0068633584305644035 tr(WHW.T) 677.69384765625
bpp_loss 3.7003650665283203
27_q proxy err 0.0009282991522923112 tr(WHW.T) 21313.927734375
bpp_loss 3.9985508918762207
27_k proxy err 0.0004102946841157973 tr(WHW.T) 14027.966796875
bpp_loss 4.84430456161499
27_o proxy err 0.00954514741897583 tr(WHW.T) 2162.180419921875
bpp_loss 3.528235912322998
27_up proxy err 0.009818699210882187 tr(WHW.T) 8475.908203125
bpp_loss 3.4984050478254045
27_gate proxy err 0.0026385639794170856 tr(WHW.T) 32794.28125
bpp_loss 3.8797480719430104
27_down proxy err 0.00956171564757824 tr(WHW.T) 6585.123046875
bpp_loss 3.4897244657788957
I0326 11:31:05.866257 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 63.44418978691101s
I0326 11:31:09.245688 1619151 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 11:31:09.245783 1619151 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 11:31:09.245820 1619151 utils.py:162] NumExpr defaulting to 16 threads.
I0326 11:31:09.582207 1619151 config.py:54] PyTorch version 2.6.0 available.
W0326 11:31:09.784558 1619151 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 11:31:10.347493 1619151 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 11:31:10.351114 1418472 quantize_finetune_llama.py:209] layer 29 gpu 1
I0326 11:31:10.363950 1619151 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 11:31:17.888643 1619151 finetune.py:45] layer 28_v initial loss 4.720447395811789e-05
W0326 11:31:17.888868 1619151 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 11:31:53.384241 1619151 finetune.py:68] layer 28_v @ epoch 0 new loss 2.4671908249729313e-05 old loss 4.720447395811789e-05 BETTER
I0326 11:32:13.570327 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 62.80340242385864s
I0326 11:32:17.266019 1619878 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 11:32:17.266108 1619878 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 11:32:17.266145 1619878 utils.py:162] NumExpr defaulting to 16 threads.
I0326 11:32:17.603719 1619878 config.py:54] PyTorch version 2.6.0 available.
W0326 11:32:17.809622 1619878 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 11:32:18.421598 1619878 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 11:32:18.425150 1418472 quantize_finetune_llama.py:209] layer 30 gpu 0
I0326 11:32:18.437371 1619878 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 11:32:25.919682 1619878 finetune.py:45] layer 29_v initial loss 4.56212910648901e-05
W0326 11:32:25.919967 1619878 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 11:32:30.133004 1619151 finetune.py:68] layer 28_v @ epoch 1 new loss 2.3010745280771516e-05 old loss 2.4671908249729313e-05 BETTER
I0326 11:32:59.323435 1619878 finetune.py:68] layer 29_v @ epoch 0 new loss 2.7013620638172142e-05 old loss 4.56212910648901e-05 BETTER
I0326 11:33:07.245345 1619151 finetune.py:68] layer 28_v @ epoch 2 new loss 2.2431384422816336e-05 old loss 2.3010745280771516e-05 BETTER
I0326 11:33:33.843425 1619878 finetune.py:68] layer 29_v @ epoch 1 new loss 2.571425102360081e-05 old loss 2.7013620638172142e-05 BETTER
I0326 11:33:44.235711 1619151 finetune.py:68] layer 28_v @ epoch 3 new loss 2.1735975678893737e-05 old loss 2.2431384422816336e-05 BETTER
I0326 11:34:08.696179 1619878 finetune.py:68] layer 29_v @ epoch 2 new loss 2.5251551051042043e-05 old loss 2.571425102360081e-05 BETTER
I0326 11:34:21.194332 1619151 finetune.py:68] layer 28_v @ epoch 4 new loss 2.158567622245755e-05 old loss 2.1735975678893737e-05 BETTER
I0326 11:34:31.392229 1619151 finetune.py:45] layer 28_q initial loss 2.9870891012251377e-05
I0326 11:34:43.888754 1619878 finetune.py:76] layer 29_v @ epoch 3 new loss 2.53187827183865e-05 old loss 2.5251551051042043e-05 WORSE
I0326 11:35:07.222902 1619151 finetune.py:68] layer 28_q @ epoch 0 new loss 2.7723574021365494e-05 old loss 2.9870891012251377e-05 BETTER
I0326 11:35:18.367963 1619878 finetune.py:68] layer 29_v @ epoch 4 new loss 2.4505581677658483e-05 old loss 2.5251551051042043e-05 BETTER
I0326 11:35:28.697033 1619878 finetune.py:45] layer 29_q initial loss 4.6645334805361927e-05
I0326 11:35:44.045793 1619151 finetune.py:68] layer 28_q @ epoch 1 new loss 2.7045292881666683e-05 old loss 2.7723574021365494e-05 BETTER
I0326 11:36:02.501812 1619878 finetune.py:68] layer 29_q @ epoch 0 new loss 4.0620536310598254e-05 old loss 4.6645334805361927e-05 BETTER
I0326 11:36:21.161141 1619151 finetune.py:68] layer 28_q @ epoch 2 new loss 2.6538315069046803e-05 old loss 2.7045292881666683e-05 BETTER
I0326 11:36:37.174270 1619878 finetune.py:68] layer 29_q @ epoch 1 new loss 3.8849309930810705e-05 old loss 4.0620536310598254e-05 BETTER
I0326 11:36:58.180481 1619151 finetune.py:68] layer 28_q @ epoch 3 new loss 2.616180427139625e-05 old loss 2.6538315069046803e-05 BETTER
I0326 11:37:11.894282 1619878 finetune.py:68] layer 29_q @ epoch 2 new loss 3.7735859223175794e-05 old loss 3.8849309930810705e-05 BETTER
I0326 11:37:35.151314 1619151 finetune.py:68] layer 28_q @ epoch 4 new loss 2.5918583560269326e-05 old loss 2.616180427139625e-05 BETTER
I0326 11:37:43.497095 1619151 finetune.py:45] layer 28_k initial loss 2.9193244699854404e-05
I0326 11:37:46.674888 1619878 finetune.py:68] layer 29_q @ epoch 3 new loss 3.6849443858955055e-05 old loss 3.7735859223175794e-05 BETTER
I0326 11:38:19.716665 1619151 finetune.py:68] layer 28_k @ epoch 0 new loss 2.8557475161505863e-05 old loss 2.9193244699854404e-05 BETTER
I0326 11:38:21.550811 1619878 finetune.py:68] layer 29_q @ epoch 4 new loss 3.637455665739253e-05 old loss 3.6849443858955055e-05 BETTER
I0326 11:38:29.498333 1619878 finetune.py:45] layer 29_k initial loss 4.088005152880214e-05
I0326 11:38:56.830594 1619151 finetune.py:68] layer 28_k @ epoch 1 new loss 2.8293097784626298e-05 old loss 2.8557475161505863e-05 BETTER
I0326 11:39:03.395139 1619878 finetune.py:68] layer 29_k @ epoch 0 new loss 4.0068000089377165e-05 old loss 4.088005152880214e-05 BETTER
I0326 11:39:33.811041 1619151 finetune.py:68] layer 28_k @ epoch 2 new loss 2.8175996703794226e-05 old loss 2.8293097784626298e-05 BETTER
I0326 11:39:38.183319 1619878 finetune.py:68] layer 29_k @ epoch 1 new loss 3.950094469473697e-05 old loss 4.0068000089377165e-05 BETTER
I0326 11:40:10.867797 1619151 finetune.py:68] layer 28_k @ epoch 3 new loss 2.794781539705582e-05 old loss 2.8175996703794226e-05 BETTER
I0326 11:40:13.061786 1619878 finetune.py:68] layer 29_k @ epoch 2 new loss 3.9308641134994105e-05 old loss 3.950094469473697e-05 BETTER
I0326 11:40:47.870835 1619878 finetune.py:68] layer 29_k @ epoch 3 new loss 3.8950918678892776e-05 old loss 3.9308641134994105e-05 BETTER
I0326 11:40:47.923780 1619151 finetune.py:68] layer 28_k @ epoch 4 new loss 2.7929683710681275e-05 old loss 2.794781539705582e-05 BETTER
I0326 11:40:58.109197 1619151 finetune.py:45] layer 28_o initial loss 8.808436541585252e-05
I0326 11:41:22.704232 1619878 finetune.py:68] layer 29_k @ epoch 4 new loss 3.888216451741755e-05 old loss 3.8950918678892776e-05 BETTER
I0326 11:41:32.429993 1619878 finetune.py:45] layer 29_o initial loss 0.00010290452337358147
I0326 11:41:33.685115 1619151 finetune.py:68] layer 28_o @ epoch 0 new loss 7.997813372639939e-05 old loss 8.808436541585252e-05 BETTER
I0326 11:42:05.734122 1619878 finetune.py:68] layer 29_o @ epoch 0 new loss 9.328194573754445e-05 old loss 0.00010290452337358147 BETTER
I0326 11:42:09.941319 1619151 finetune.py:68] layer 28_o @ epoch 1 new loss 7.718259439570829e-05 old loss 7.997813372639939e-05 BETTER
I0326 11:42:39.957988 1619878 finetune.py:68] layer 29_o @ epoch 1 new loss 9.041017619892955e-05 old loss 9.328194573754445e-05 BETTER
I0326 11:42:46.392163 1619151 finetune.py:68] layer 28_o @ epoch 2 new loss 7.528808055212721e-05 old loss 7.718259439570829e-05 BETTER
I0326 11:43:14.482556 1619878 finetune.py:68] layer 29_o @ epoch 2 new loss 8.860640082275495e-05 old loss 9.041017619892955e-05 BETTER
I0326 11:43:22.986835 1619151 finetune.py:68] layer 28_o @ epoch 3 new loss 7.38707312848419e-05 old loss 7.528808055212721e-05 BETTER
I0326 11:43:49.011560 1619878 finetune.py:68] layer 29_o @ epoch 3 new loss 8.728831016924232e-05 old loss 8.860640082275495e-05 BETTER
I0326 11:43:59.372872 1619151 finetune.py:68] layer 28_o @ epoch 4 new loss 7.276324322447181e-05 old loss 7.38707312848419e-05 BETTER
I0326 11:44:21.486005 1619151 finetune.py:45] layer 28_up initial loss 0.00020570465130731463
I0326 11:44:23.523802 1619878 finetune.py:68] layer 29_o @ epoch 4 new loss 8.613881800556555e-05 old loss 8.728831016924232e-05 BETTER
I0326 11:44:45.613256 1619878 finetune.py:45] layer 29_up initial loss 0.00027394804055802524
I0326 11:44:54.058178 1619151 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0001953964092535898 old loss 0.00020570465130731463 BETTER
I0326 11:45:16.616954 1619878 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0002575726539362222 old loss 0.00027394804055802524 BETTER
I0326 11:45:28.170819 1619151 finetune.py:68] layer 28_up @ epoch 1 new loss 0.00019119572243653238 old loss 0.0001953964092535898 BETTER
I0326 11:45:48.885597 1619878 finetune.py:68] layer 29_up @ epoch 1 new loss 0.0002512015344109386 old loss 0.0002575726539362222 BETTER
I0326 11:46:02.375846 1619151 finetune.py:68] layer 28_up @ epoch 2 new loss 0.00018822975107468665 old loss 0.00019119572243653238 BETTER
I0326 11:46:21.103195 1619878 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0002467298472765833 old loss 0.0002512015344109386 BETTER
I0326 11:46:36.624240 1619151 finetune.py:68] layer 28_up @ epoch 3 new loss 0.00018584974168334156 old loss 0.00018822975107468665 BETTER
I0326 11:46:53.443402 1619878 finetune.py:68] layer 29_up @ epoch 3 new loss 0.00024318111536558717 old loss 0.0002467298472765833 BETTER
I0326 11:47:10.992724 1619151 finetune.py:68] layer 28_up @ epoch 4 new loss 0.00018388575699646026 old loss 0.00018584974168334156 BETTER
I0326 11:47:25.621337 1619878 finetune.py:68] layer 29_up @ epoch 4 new loss 0.00024029337510000914 old loss 0.00024318111536558717 BETTER
I0326 11:47:33.432581 1619151 finetune.py:45] layer 28_gate initial loss 0.0002463526325300336
I0326 11:47:47.733349 1619878 finetune.py:45] layer 29_gate initial loss 0.0003261326055508107
I0326 11:48:03.899835 1619151 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.00024347193539142609 old loss 0.0002463526325300336 BETTER
I0326 11:48:16.424785 1619878 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0003215133910998702 old loss 0.0003261326055508107 BETTER
I0326 11:48:35.399943 1619151 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.00024158359155990183 old loss 0.00024347193539142609 BETTER
I0326 11:48:46.362643 1619878 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.00031873592524789274 old loss 0.0003215133910998702 BETTER
I0326 11:49:06.994463 1619151 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.00023996901290956885 old loss 0.00024158359155990183 BETTER
I0326 11:49:16.241960 1619878 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0003165799716953188 old loss 0.00031873592524789274 BETTER
I0326 11:49:38.832840 1619151 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.00023871731536928564 old loss 0.00023996901290956885 BETTER
I0326 11:49:46.147949 1619878 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0003147193929180503 old loss 0.0003165799716953188 BETTER
I0326 11:50:10.660971 1619151 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.00023754262656439096 old loss 0.00023871731536928564 BETTER
I0326 11:50:16.030467 1619878 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0003131878620479256 old loss 0.0003147193929180503 BETTER
I0326 11:50:33.984744 1619151 finetune.py:45] layer 28_down initial loss 0.00041918433271348476
I0326 11:50:38.981440 1619878 finetune.py:45] layer 29_down initial loss 0.0005712900892831385
I0326 11:51:02.122000 1619151 finetune.py:68] layer 28_down @ epoch 0 new loss 0.00041914626490324736 old loss 0.00041918433271348476 BETTER
I0326 11:51:06.178020 1619878 finetune.py:68] layer 29_down @ epoch 0 new loss 0.0005711889825761318 old loss 0.0005712900892831385 BETTER
I0326 11:51:31.253150 1619151 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0004191146872472018 old loss 0.00041914626490324736 BETTER
I0326 11:51:33.946766 1619878 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0005711168050765991 old loss 0.0005711889825761318 BETTER
I0326 11:52:00.525204 1619151 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0004190866893623024 old loss 0.0004191146872472018 BETTER
I0326 11:52:01.887773 1619878 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0005710581317543983 old loss 0.0005711168050765991 BETTER
I0326 11:52:29.704925 1619878 finetune.py:68] layer 29_down @ epoch 3 new loss 0.000570999807678163 old loss 0.0005710581317543983 BETTER
I0326 11:52:30.165077 1619151 finetune.py:68] layer 28_down @ epoch 3 new loss 0.00041905971011146903 old loss 0.0004190866893623024 BETTER
I0326 11:52:57.519452 1619878 finetune.py:68] layer 29_down @ epoch 4 new loss 0.0005709496908821166 old loss 0.000570999807678163 BETTER
29_v proxy err 0.005947357974946499 tr(WHW.T) 850.4290161132812
bpp_loss 3.8228073120117188
29_q proxy err 0.0010259191039949656 tr(WHW.T) 20655.251953125
bpp_loss 3.9971492290496826
29_k proxy err 0.000366565102012828 tr(WHW.T) 16384.21875
bpp_loss 4.889170169830322
29_o proxy err 0.0056828698143363 tr(WHW.T) 3107.243896484375
bpp_loss 3.5935298204421997
29_up proxy err 0.00643570814281702 tr(WHW.T) 12867.5634765625
bpp_loss 3.554354122706822
29_gate proxy err 0.002231830498203635 tr(WHW.T) 38346.5703125
bpp_loss 3.8561314174107144
29_down proxy err 0.008214056491851807 tr(WHW.T) 7520.69091796875
bpp_loss 3.5196927956172397
I0326 11:52:59.786769 1619151 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0004190350591670722 old loss 0.00041905971011146903 BETTER
28_v proxy err 0.007672456093132496 tr(WHW.T) 601.4844360351562
bpp_loss 3.75329053401947
28_q proxy err 0.0008547243196517229 tr(WHW.T) 23173.1484375
bpp_loss 4.008108139038086
28_k proxy err 0.00037581066135317087 tr(WHW.T) 15011.8984375
bpp_loss 4.799073219299316
28_o proxy err 0.008365590125322342 tr(WHW.T) 2513.666015625
bpp_loss 3.5579367876052856
28_up proxy err 0.008045201189815998 tr(WHW.T) 10249.111328125
bpp_loss 3.5208330154418945
28_gate proxy err 0.002379304263740778 tr(WHW.T) 35908.44921875
bpp_loss 3.8656699316842213
28_down proxy err 0.009392094798386097 tr(WHW.T) 7263.72900390625
bpp_loss 3.5050761018480574
I0326 11:54:08.942774 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 64.12636399269104s
I0326 11:54:12.471254 1632250 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 11:54:12.471362 1632250 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 11:54:12.471406 1632250 utils.py:162] NumExpr defaulting to 16 threads.
I0326 11:54:12.818143 1632250 config.py:54] PyTorch version 2.6.0 available.
W0326 11:54:13.009637 1632250 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 11:54:13.593610 1632250 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 11:54:13.597150 1418472 quantize_finetune_llama.py:209] layer 31 gpu 1
I0326 11:54:13.610038 1632250 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 11:54:21.238214 1632250 finetune.py:45] layer 30_v initial loss 8.280927431769669e-05
W0326 11:54:21.238442 1632250 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 11:54:56.965701 1632250 finetune.py:68] layer 30_v @ epoch 0 new loss 4.9762231355998665e-05 old loss 8.280927431769669e-05 BETTER
I0326 11:55:16.488447 1418472 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 62.45974659919739s
I0326 11:55:20.214196 1632963 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 11:55:20.214296 1632963 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 11:55:20.214335 1632963 utils.py:162] NumExpr defaulting to 16 threads.
I0326 11:55:20.589749 1632963 config.py:54] PyTorch version 2.6.0 available.
W0326 11:55:20.793744 1632963 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 11:55:21.396612 1632963 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 11:55:21.413351 1632963 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 11:55:29.027918 1632963 finetune.py:45] layer 31_v initial loss 0.00017040014790836722
W0326 11:55:29.028141 1632963 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 11:55:33.978595 1632250 finetune.py:68] layer 30_v @ epoch 1 new loss 4.629016257240437e-05 old loss 4.9762231355998665e-05 BETTER
I0326 11:56:02.584960 1632963 finetune.py:68] layer 31_v @ epoch 0 new loss 8.930262265494093e-05 old loss 0.00017040014790836722 BETTER
I0326 11:56:11.059699 1632250 finetune.py:68] layer 30_v @ epoch 2 new loss 4.548013748717494e-05 old loss 4.629016257240437e-05 BETTER
I0326 11:56:37.294785 1632963 finetune.py:76] layer 31_v @ epoch 1 new loss 0.00011106433521490544 old loss 8.930262265494093e-05 WORSE
I0326 11:56:48.165979 1632250 finetune.py:68] layer 30_v @ epoch 3 new loss 4.52630192739889e-05 old loss 4.548013748717494e-05 BETTER
I0326 11:57:11.761046 1632963 finetune.py:76] layer 31_v @ epoch 2 new loss 0.00011189726501470432 old loss 8.930262265494093e-05 WORSE
I0326 11:57:25.296399 1632250 finetune.py:68] layer 30_v @ epoch 4 new loss 4.3468284275149927e-05 old loss 4.52630192739889e-05 BETTER
I0326 11:57:35.452862 1632250 finetune.py:45] layer 30_q initial loss 5.946285818936303e-05
I0326 11:57:46.258561 1632963 finetune.py:76] layer 31_v @ epoch 3 new loss 0.0001022523210849613 old loss 8.930262265494093e-05 WORSE
I0326 11:58:11.694848 1632250 finetune.py:68] layer 30_q @ epoch 0 new loss 5.5232660088222474e-05 old loss 5.946285818936303e-05 BETTER
I0326 11:58:20.918684 1632963 finetune.py:76] layer 31_v @ epoch 4 new loss 0.00011996610555797815 old loss 8.930262265494093e-05 WORSE
I0326 11:58:30.180768 1632963 finetune.py:45] layer 31_q initial loss 0.00014301869668997824
I0326 11:58:48.430225 1632250 finetune.py:68] layer 30_q @ epoch 1 new loss 5.3736257541459054e-05 old loss 5.5232660088222474e-05 BETTER
I0326 11:59:04.329761 1632963 finetune.py:68] layer 31_q @ epoch 0 new loss 0.00011775812163250521 old loss 0.00014301869668997824 BETTER
I0326 11:59:25.294936 1632250 finetune.py:68] layer 30_q @ epoch 2 new loss 5.29528442712035e-05 old loss 5.3736257541459054e-05 BETTER
I0326 11:59:39.030605 1632963 finetune.py:68] layer 31_q @ epoch 1 new loss 0.0001143598128692247 old loss 0.00011775812163250521 BETTER
I0326 12:00:02.306104 1632250 finetune.py:68] layer 30_q @ epoch 3 new loss 5.2379517001099885e-05 old loss 5.29528442712035e-05 BETTER
I0326 12:00:13.639248 1632963 finetune.py:68] layer 31_q @ epoch 2 new loss 9.663479431765154e-05 old loss 0.0001143598128692247 BETTER
I0326 12:00:39.340109 1632250 finetune.py:68] layer 30_q @ epoch 4 new loss 5.2231171139283106e-05 old loss 5.2379517001099885e-05 BETTER
I0326 12:00:47.486301 1632250 finetune.py:45] layer 30_k initial loss 6.05734130658675e-05
I0326 12:00:48.376042 1632963 finetune.py:76] layer 31_q @ epoch 3 new loss 9.756016515893862e-05 old loss 9.663479431765154e-05 WORSE
I0326 12:01:22.477272 1632963 finetune.py:76] layer 31_q @ epoch 4 new loss 9.880368452286348e-05 old loss 9.663479431765154e-05 WORSE
I0326 12:01:23.504711 1632250 finetune.py:68] layer 30_k @ epoch 0 new loss 5.920784315094352e-05 old loss 6.05734130658675e-05 BETTER
I0326 12:01:29.965001 1632963 finetune.py:45] layer 31_k initial loss 0.00012210548447910696
I0326 12:02:00.406917 1632250 finetune.py:76] layer 30_k @ epoch 1 new loss 5.950260674580932e-05 old loss 5.920784315094352e-05 WORSE
I0326 12:02:03.815740 1632963 finetune.py:68] layer 31_k @ epoch 0 new loss 0.00011043257836718112 old loss 0.00012210548447910696 BETTER
I0326 12:02:36.724680 1632250 finetune.py:76] layer 30_k @ epoch 2 new loss 6.235248292796314e-05 old loss 5.920784315094352e-05 WORSE
I0326 12:02:38.319287 1632963 finetune.py:68] layer 31_k @ epoch 1 new loss 0.00010675959492800757 old loss 0.00011043257836718112 BETTER
I0326 12:03:13.033803 1632963 finetune.py:68] layer 31_k @ epoch 2 new loss 0.00010476598254172131 old loss 0.00010675959492800757 BETTER
I0326 12:03:13.041078 1632250 finetune.py:68] layer 30_k @ epoch 3 new loss 5.840417725266889e-05 old loss 5.920784315094352e-05 BETTER
I0326 12:03:47.743386 1632963 finetune.py:68] layer 31_k @ epoch 3 new loss 0.00010297722474206239 old loss 0.00010476598254172131 BETTER
I0326 12:03:49.775532 1632250 finetune.py:76] layer 30_k @ epoch 4 new loss 5.841050733579323e-05 old loss 5.840417725266889e-05 WORSE
I0326 12:03:59.298040 1632250 finetune.py:45] layer 30_o initial loss 0.0001734410907374695
I0326 12:04:22.554160 1632963 finetune.py:76] layer 31_k @ epoch 4 new loss 0.00011339969205437228 old loss 0.00010297722474206239 WORSE
I0326 12:04:31.766917 1632963 finetune.py:45] layer 31_o initial loss 0.0004656174860429019
I0326 12:04:34.666219 1632250 finetune.py:68] layer 30_o @ epoch 0 new loss 0.00015463409363292158 old loss 0.0001734410907374695 BETTER
I0326 12:05:05.221965 1632963 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0002845473063644022 old loss 0.0004656174860429019 BETTER
I0326 12:05:10.808164 1632250 finetune.py:68] layer 30_o @ epoch 1 new loss 0.00014968789764679968 old loss 0.00015463409363292158 BETTER
I0326 12:05:39.430460 1632963 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0002599581493996084 old loss 0.0002845473063644022 BETTER
I0326 12:05:46.997533 1632250 finetune.py:68] layer 30_o @ epoch 2 new loss 0.00014640833251178265 old loss 0.00014968789764679968 BETTER
I0326 12:06:14.161108 1632963 finetune.py:68] layer 31_o @ epoch 2 new loss 0.00024659704649820924 old loss 0.0002599581493996084 BETTER
I0326 12:06:23.812501 1632250 finetune.py:68] layer 30_o @ epoch 3 new loss 0.00014409337018150836 old loss 0.00014640833251178265 BETTER
I0326 12:06:48.479842 1632963 finetune.py:68] layer 31_o @ epoch 3 new loss 0.00023706906358711421 old loss 0.00024659704649820924 BETTER
I0326 12:07:00.081405 1632250 finetune.py:68] layer 30_o @ epoch 4 new loss 0.00014199210272636265 old loss 0.00014409337018150836 BETTER
I0326 12:07:22.406198 1632250 finetune.py:45] layer 30_up initial loss 0.0005331843276508152
I0326 12:07:22.551237 1632963 finetune.py:68] layer 31_o @ epoch 4 new loss 0.00023019003856461495 old loss 0.00023706906358711421 BETTER
I0326 12:07:44.305983 1632963 finetune.py:45] layer 31_up initial loss 0.0017553716897964478
I0326 12:07:55.049736 1632250 finetune.py:68] layer 30_up @ epoch 0 new loss 0.00047891971189528704 old loss 0.0005331843276508152 BETTER
I0326 12:08:15.579056 1632963 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0013589446898549795 old loss 0.0017553716897964478 BETTER
I0326 12:08:28.917918 1632250 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0004610179748851806 old loss 0.00047891971189528704 BETTER
I0326 12:08:47.788074 1632963 finetune.py:68] layer 31_up @ epoch 1 new loss 0.001254499889910221 old loss 0.0013589446898549795 BETTER
I0326 12:09:02.910311 1632250 finetune.py:68] layer 30_up @ epoch 2 new loss 0.00044846307719126344 old loss 0.0004610179748851806 BETTER
I0326 12:09:20.105729 1632963 finetune.py:68] layer 31_up @ epoch 2 new loss 0.0011832768796011806 old loss 0.001254499889910221 BETTER
I0326 12:09:37.105454 1632250 finetune.py:68] layer 30_up @ epoch 3 new loss 0.00043879804434254766 old loss 0.00044846307719126344 BETTER
I0326 12:09:52.436256 1632963 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0011292205890640616 old loss 0.0011832768796011806 BETTER
I0326 12:10:11.291028 1632250 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0004309500800445676 old loss 0.00043879804434254766 BETTER
I0326 12:10:24.806143 1632963 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0010844371281564236 old loss 0.0011292205890640616 BETTER
I0326 12:10:33.764885 1632250 finetune.py:45] layer 30_gate initial loss 0.0005641950992867351
I0326 12:10:46.848985 1632963 finetune.py:45] layer 31_gate initial loss 0.0013926553074270487
I0326 12:11:04.336812 1632250 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0005486245499923825 old loss 0.0005641950992867351 BETTER
I0326 12:11:15.811038 1632963 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.0012949537485837936 old loss 0.0013926553074270487 BETTER
I0326 12:11:36.055449 1632250 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.000541489280294627 old loss 0.0005486245499923825 BETTER
I0326 12:11:45.657374 1632963 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.0012592399725690484 old loss 0.0012949537485837936 BETTER
I0326 12:12:07.802614 1632250 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0005358119960874319 old loss 0.000541489280294627 BETTER
I0326 12:12:15.613408 1632963 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.0012324204435572028 old loss 0.0012592399725690484 BETTER
I0326 12:12:39.734050 1632250 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.0005310302367433906 old loss 0.0005358119960874319 BETTER
I0326 12:12:45.590170 1632963 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.001209488371387124 old loss 0.0012324204435572028 BETTER
I0326 12:13:11.610866 1632250 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0005269655957818031 old loss 0.0005310302367433906 BETTER
I0326 12:13:15.505262 1632963 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.0011891856556758285 old loss 0.001209488371387124 BETTER
I0326 12:13:34.874235 1632250 finetune.py:45] layer 30_down initial loss 0.0010386384092271328
I0326 12:13:38.393678 1632963 finetune.py:45] layer 31_down initial loss 0.003723717061802745
I0326 12:14:02.964274 1632250 finetune.py:68] layer 30_down @ epoch 0 new loss 0.0010381363099440932 old loss 0.0010386384092271328 BETTER
I0326 12:14:05.230255 1632963 finetune.py:68] layer 31_down @ epoch 0 new loss 0.0037192925810813904 old loss 0.003723717061802745 BETTER
I0326 12:14:32.074250 1632250 finetune.py:68] layer 30_down @ epoch 1 new loss 0.0010377638973295689 old loss 0.0010381363099440932 BETTER
I0326 12:14:32.940966 1632963 finetune.py:68] layer 31_down @ epoch 1 new loss 0.0037167544942349195 old loss 0.0037192925810813904 BETTER
I0326 12:15:00.715114 1632963 finetune.py:68] layer 31_down @ epoch 2 new loss 0.0037149516865611076 old loss 0.0037167544942349195 BETTER
I0326 12:15:01.430654 1632250 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0010374881094321609 old loss 0.0010377638973295689 BETTER
I0326 12:15:28.470212 1632963 finetune.py:68] layer 31_down @ epoch 3 new loss 0.003713415702804923 old loss 0.0037149516865611076 BETTER
I0326 12:15:31.039813 1632250 finetune.py:68] layer 30_down @ epoch 3 new loss 0.001037266687490046 old loss 0.0010374881094321609 BETTER
I0326 12:15:56.217427 1632963 finetune.py:68] layer 31_down @ epoch 4 new loss 0.003712061559781432 old loss 0.003713415702804923 BETTER
31_v proxy err 0.0030281543731689453 tr(WHW.T) 1808.723876953125
bpp_loss 3.925965189933777
31_q proxy err 0.0005212674732320011 tr(WHW.T) 46184.3359375
bpp_loss 4.052123785018921
31_k proxy err 0.00032347021624445915 tr(WHW.T) 20501.271484375
bpp_loss 4.780230522155762
31_o proxy err 0.005287112668156624 tr(WHW.T) 2222.580322265625
bpp_loss 3.6373718976974487
31_up proxy err 0.0013339227298274636 tr(WHW.T) 68984.703125
bpp_loss 3.781655584062849
31_gate proxy err 0.000669417786411941 tr(WHW.T) 144046.59375
bpp_loss 4.1237896510532925
31_down proxy err 0.0036932944785803556 tr(WHW.T) 10070.1357421875
bpp_loss 3.545023577553885
I0326 12:16:00.736243 1632250 finetune.py:68] layer 30_down @ epoch 4 new loss 0.0010370705276727676 old loss 0.001037266687490046 BETTER
30_v proxy err 0.005509731359779835 tr(WHW.T) 863.060791015625
bpp_loss 4.108954429626465
30_q proxy err 0.0008269520476460457 tr(WHW.T) 24059.419921875
bpp_loss 3.9001612663269043
30_k proxy err 0.0004049604758620262 tr(WHW.T) 14051.2158203125
bpp_loss 4.544769763946533
30_o proxy err 0.004396217409521341 tr(WHW.T) 4878.39697265625
bpp_loss 3.680566191673279
30_up proxy err 0.003915965557098389 tr(WHW.T) 21696.828125
bpp_loss 3.5866077968052457
30_gate proxy err 0.0016942479414865375 tr(WHW.T) 51809.29296875
bpp_loss 3.906299046107701
30_down proxy err 0.0058082956820726395 tr(WHW.T) 8865.865234375
bpp_loss 3.520352806363787
I0326 12:16:20.243369 1644915 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 12:16:20.243501 1644915 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 12:16:20.243544 1644915 utils.py:162] NumExpr defaulting to 16 threads.
I0326 12:16:20.578548 1644915 config.py:54] PyTorch version 2.6.0 available.
W0326 12:16:20.794290 1644915 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0326 12:16:20.909175 1644915 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.40it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.76it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.84it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.77it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.14it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.47it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.79it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.31it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.31it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.38it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  6.95it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.25it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.38it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.45it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.86it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.57it/s]
I0326 12:16:24.366185 1644915 hfize_llama.py:153] loaded layer 0
I0326 12:16:25.211241 1644915 hfize_llama.py:153] loaded layer 1
I0326 12:16:26.048615 1644915 hfize_llama.py:153] loaded layer 2
I0326 12:16:26.813266 1644915 hfize_llama.py:153] loaded layer 3
I0326 12:16:28.028296 1644915 hfize_llama.py:153] loaded layer 4
I0326 12:16:29.126076 1644915 hfize_llama.py:153] loaded layer 5
I0326 12:16:30.440870 1644915 hfize_llama.py:153] loaded layer 6
I0326 12:16:31.608548 1644915 hfize_llama.py:153] loaded layer 7
I0326 12:16:32.791054 1644915 hfize_llama.py:153] loaded layer 8
I0326 12:16:33.877329 1644915 hfize_llama.py:153] loaded layer 9
I0326 12:16:34.985840 1644915 hfize_llama.py:153] loaded layer 10
I0326 12:16:36.024464 1644915 hfize_llama.py:153] loaded layer 11
I0326 12:16:37.284269 1644915 hfize_llama.py:153] loaded layer 12
I0326 12:16:38.517647 1644915 hfize_llama.py:153] loaded layer 13
I0326 12:16:39.627785 1644915 hfize_llama.py:153] loaded layer 14
I0326 12:16:40.694238 1644915 hfize_llama.py:153] loaded layer 15
I0326 12:16:41.813409 1644915 hfize_llama.py:153] loaded layer 16
I0326 12:16:42.868538 1644915 hfize_llama.py:153] loaded layer 17
I0326 12:16:44.073664 1644915 hfize_llama.py:153] loaded layer 18
I0326 12:16:45.098152 1644915 hfize_llama.py:153] loaded layer 19
I0326 12:16:46.173751 1644915 hfize_llama.py:153] loaded layer 20
I0326 12:16:47.227886 1644915 hfize_llama.py:153] loaded layer 21
I0326 12:16:48.324132 1644915 hfize_llama.py:153] loaded layer 22
I0326 12:16:49.351687 1644915 hfize_llama.py:153] loaded layer 23
I0326 12:16:50.420471 1644915 hfize_llama.py:153] loaded layer 24
I0326 12:16:51.489796 1644915 hfize_llama.py:153] loaded layer 25
I0326 12:16:52.521954 1644915 hfize_llama.py:153] loaded layer 26
I0326 12:16:53.590985 1644915 hfize_llama.py:153] loaded layer 27
I0326 12:16:54.705564 1644915 hfize_llama.py:153] loaded layer 28
I0326 12:16:55.836076 1644915 hfize_llama.py:153] loaded layer 29
I0326 12:16:56.932738 1644915 hfize_llama.py:153] loaded layer 30
I0326 12:16:57.977694 1644915 hfize_llama.py:153] loaded layer 31
I0326 12:16:57.977843 1644915 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.71s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.30s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.15s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.08s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.03s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.01s/it]
I0326 12:17:53.217580 1644915 hfize_llama.py:167] successfully loaded hfized model
I0326 12:17:58.479415 1645976 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 12:17:58.479530 1645976 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 12:17:58.479573 1645976 utils.py:162] NumExpr defaulting to 16 threads.
W0326 12:17:58.884115 1645976 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0326 12:17:59.401870 1645976 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.23s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.24s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.21s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.22s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.12s/it]
I0326 12:18:07.358652 1645976 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.6432569026947021:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.6432569026947021:   1%|          | 1/141 [00:01<04:25,  1.90s/it]avg_loss = 1.9369181394577026:   1%|          | 1/141 [00:03<04:25,  1.90s/it]avg_loss = 1.9369181394577026:   1%|▏         | 2/141 [00:03<03:51,  1.66s/it]avg_loss = 2.060457785924276:   1%|▏         | 2/141 [00:04<03:51,  1.66s/it] avg_loss = 2.060457785924276:   2%|▏         | 3/141 [00:04<03:40,  1.60s/it]avg_loss = 2.006759375333786:   2%|▏         | 3/141 [00:06<03:40,  1.60s/it]avg_loss = 2.006759375333786:   3%|▎         | 4/141 [00:06<03:34,  1.56s/it]avg_loss = 1.9593597888946532:   3%|▎         | 4/141 [00:07<03:34,  1.56s/it]avg_loss = 1.9593597888946532:   4%|▎         | 5/141 [00:07<03:30,  1.55s/it]avg_loss = 1.862325926621755:   4%|▎         | 5/141 [00:09<03:30,  1.55s/it] avg_loss = 1.862325926621755:   4%|▍         | 6/141 [00:09<03:28,  1.54s/it]avg_loss = 1.8010075773511613:   4%|▍         | 6/141 [00:11<03:28,  1.54s/it]avg_loss = 1.8010075773511613:   5%|▍         | 7/141 [00:11<03:26,  1.54s/it]avg_loss = 1.7968452125787735:   5%|▍         | 7/141 [00:12<03:26,  1.54s/it]avg_loss = 1.7968452125787735:   6%|▌         | 8/141 [00:12<03:24,  1.54s/it]avg_loss = 1.82945888572269:   6%|▌         | 8/141 [00:14<03:24,  1.54s/it]  avg_loss = 1.82945888572269:   6%|▋         | 9/141 [00:14<03:23,  1.54s/it]avg_loss = 1.8307670474052429:   6%|▋         | 9/141 [00:15<03:23,  1.54s/it]avg_loss = 1.8307670474052429:   7%|▋         | 10/141 [00:15<03:22,  1.54s/it]avg_loss = 1.8258349136872725:   7%|▋         | 10/141 [00:17<03:22,  1.54s/it]avg_loss = 1.8258349136872725:   8%|▊         | 11/141 [00:17<03:20,  1.54s/it]avg_loss = 1.8481419583161671:   8%|▊         | 11/141 [00:18<03:20,  1.54s/it]avg_loss = 1.8481419583161671:   9%|▊         | 12/141 [00:18<03:19,  1.55s/it]avg_loss = 1.8601197737913866:   9%|▊         | 12/141 [00:20<03:19,  1.55s/it]avg_loss = 1.8601197737913866:   9%|▉         | 13/141 [00:20<03:18,  1.55s/it]avg_loss = 1.8784409846578325:   9%|▉         | 13/141 [00:21<03:18,  1.55s/it]avg_loss = 1.8784409846578325:  10%|▉         | 14/141 [00:21<03:17,  1.56s/it]avg_loss = 1.8881614446640014:  10%|▉         | 14/141 [00:23<03:17,  1.56s/it]avg_loss = 1.8881614446640014:  11%|█         | 15/141 [00:23<03:16,  1.56s/it]avg_loss = 1.9116944447159767:  11%|█         | 15/141 [00:25<03:16,  1.56s/it]avg_loss = 1.9116944447159767:  11%|█▏        | 16/141 [00:25<03:15,  1.57s/it]avg_loss = 1.9147540190640617:  11%|█▏        | 16/141 [00:26<03:15,  1.57s/it]avg_loss = 1.9147540190640617:  12%|█▏        | 17/141 [00:26<03:14,  1.57s/it]avg_loss = 1.9173260463608637:  12%|█▏        | 17/141 [00:28<03:14,  1.57s/it]avg_loss = 1.9173260463608637:  13%|█▎        | 18/141 [00:28<03:13,  1.57s/it]avg_loss = 1.9031446607489335:  13%|█▎        | 18/141 [00:29<03:13,  1.57s/it]avg_loss = 1.9031446607489335:  13%|█▎        | 19/141 [00:29<03:12,  1.58s/it]avg_loss = 1.9020485997200012:  13%|█▎        | 19/141 [00:31<03:12,  1.58s/it]avg_loss = 1.9020485997200012:  14%|█▍        | 20/141 [00:31<03:11,  1.58s/it]avg_loss = 1.9067991006942022:  14%|█▍        | 20/141 [00:32<03:11,  1.58s/it]avg_loss = 1.9067991006942022:  15%|█▍        | 21/141 [00:32<03:10,  1.59s/it]avg_loss = 1.908513524315574:  15%|█▍        | 21/141 [00:34<03:10,  1.59s/it] avg_loss = 1.908513524315574:  16%|█▌        | 22/141 [00:34<03:09,  1.59s/it]avg_loss = 1.9099360341611116:  16%|█▌        | 22/141 [00:36<03:09,  1.59s/it]avg_loss = 1.9099360341611116:  16%|█▋        | 23/141 [00:36<03:08,  1.60s/it]avg_loss = 1.914885511000951:  16%|█▋        | 23/141 [00:37<03:08,  1.60s/it] avg_loss = 1.914885511000951:  17%|█▋        | 24/141 [00:37<03:07,  1.60s/it]avg_loss = 1.9201317405700684:  17%|█▋        | 24/141 [00:39<03:07,  1.60s/it]avg_loss = 1.9201317405700684:  18%|█▊        | 25/141 [00:39<03:05,  1.60s/it]avg_loss = 1.9313991711689875:  18%|█▊        | 25/141 [00:41<03:05,  1.60s/it]avg_loss = 1.9313991711689875:  18%|█▊        | 26/141 [00:41<03:04,  1.61s/it]avg_loss = 1.9441044948719166:  18%|█▊        | 26/141 [00:42<03:04,  1.61s/it]avg_loss = 1.9441044948719166:  19%|█▉        | 27/141 [00:42<03:03,  1.61s/it]avg_loss = 1.9500564251627241:  19%|█▉        | 27/141 [00:44<03:03,  1.61s/it]avg_loss = 1.9500564251627241:  20%|█▉        | 28/141 [00:44<03:02,  1.61s/it]avg_loss = 1.946835078042129:  20%|█▉        | 28/141 [00:45<03:02,  1.61s/it] avg_loss = 1.946835078042129:  21%|██        | 29/141 [00:45<03:01,  1.62s/it]avg_loss = 1.9376268982887268:  21%|██        | 29/141 [00:47<03:01,  1.62s/it]avg_loss = 1.9376268982887268:  21%|██▏       | 30/141 [00:47<02:59,  1.62s/it]avg_loss = 1.9237422058659215:  21%|██▏       | 30/141 [00:49<02:59,  1.62s/it]avg_loss = 1.9237422058659215:  22%|██▏       | 31/141 [00:49<02:58,  1.62s/it]avg_loss = 1.9119734726846218:  22%|██▏       | 31/141 [00:50<02:58,  1.62s/it]avg_loss = 1.9119734726846218:  23%|██▎       | 32/141 [00:50<02:56,  1.62s/it]avg_loss = 1.9103813388130881:  23%|██▎       | 32/141 [00:52<02:56,  1.62s/it]avg_loss = 1.9103813388130881:  23%|██▎       | 33/141 [00:52<02:55,  1.63s/it]avg_loss = 1.9090723886209375:  23%|██▎       | 33/141 [00:54<02:55,  1.63s/it]avg_loss = 1.9090723886209375:  24%|██▍       | 34/141 [00:54<02:54,  1.63s/it]avg_loss = 1.9115953411374773:  24%|██▍       | 34/141 [00:55<02:54,  1.63s/it]avg_loss = 1.9115953411374773:  25%|██▍       | 35/141 [00:55<02:53,  1.63s/it]avg_loss = 1.8951515191131167:  25%|██▍       | 35/141 [00:57<02:53,  1.63s/it]avg_loss = 1.8951515191131167:  26%|██▌       | 36/141 [00:57<02:51,  1.63s/it]avg_loss = 1.8798098306398134:  26%|██▌       | 36/141 [00:58<02:51,  1.63s/it]avg_loss = 1.8798098306398134:  26%|██▌       | 37/141 [00:58<02:50,  1.64s/it]avg_loss = 1.8648508034254376:  26%|██▌       | 37/141 [01:00<02:50,  1.64s/it]avg_loss = 1.8648508034254376:  27%|██▋       | 38/141 [01:00<02:48,  1.64s/it]avg_loss = 1.8502376415790656:  27%|██▋       | 38/141 [01:02<02:48,  1.64s/it]avg_loss = 1.8502376415790656:  28%|██▊       | 39/141 [01:02<02:47,  1.64s/it]avg_loss = 1.8413657248020172:  28%|██▊       | 39/141 [01:03<02:47,  1.64s/it]avg_loss = 1.8413657248020172:  28%|██▊       | 40/141 [01:03<02:46,  1.64s/it]avg_loss = 1.8467829750805367:  28%|██▊       | 40/141 [01:05<02:46,  1.64s/it]avg_loss = 1.8467829750805367:  29%|██▉       | 41/141 [01:05<02:44,  1.65s/it]avg_loss = 1.8634853930700392:  29%|██▉       | 41/141 [01:07<02:44,  1.65s/it]avg_loss = 1.8634853930700392:  30%|██▉       | 42/141 [01:07<02:43,  1.65s/it]avg_loss = 1.8794717067895934:  30%|██▉       | 42/141 [01:08<02:43,  1.65s/it]avg_loss = 1.8794717067895934:  30%|███       | 43/141 [01:08<02:41,  1.65s/it]avg_loss = 1.8836229064247825:  30%|███       | 43/141 [01:10<02:41,  1.65s/it]avg_loss = 1.8836229064247825:  31%|███       | 44/141 [01:10<02:40,  1.65s/it]avg_loss = 1.8879571172926162:  31%|███       | 44/141 [01:12<02:40,  1.65s/it]avg_loss = 1.8879571172926162:  32%|███▏      | 45/141 [01:12<02:39,  1.66s/it]avg_loss = 1.8927765670030012:  32%|███▏      | 45/141 [01:13<02:39,  1.66s/it]avg_loss = 1.8927765670030012:  33%|███▎      | 46/141 [01:13<02:37,  1.66s/it]avg_loss = 1.899097001298945:  33%|███▎      | 46/141 [01:15<02:37,  1.66s/it] avg_loss = 1.899097001298945:  33%|███▎      | 47/141 [01:15<02:36,  1.66s/it]avg_loss = 1.902248019973437:  33%|███▎      | 47/141 [01:17<02:36,  1.66s/it]avg_loss = 1.902248019973437:  34%|███▍      | 48/141 [01:17<02:34,  1.66s/it]avg_loss = 1.901432526354887:  34%|███▍      | 48/141 [01:18<02:34,  1.66s/it]avg_loss = 1.901432526354887:  35%|███▍      | 49/141 [01:18<02:33,  1.67s/it]avg_loss = 1.9013284397125245:  35%|███▍      | 49/141 [01:20<02:33,  1.67s/it]avg_loss = 1.9013284397125245:  35%|███▌      | 50/141 [01:20<02:31,  1.67s/it]avg_loss = 1.8955668094111424:  35%|███▌      | 50/141 [01:22<02:31,  1.67s/it]avg_loss = 1.8955668094111424:  36%|███▌      | 51/141 [01:22<02:30,  1.67s/it]avg_loss = 1.8916269334462972:  36%|███▌      | 51/141 [01:23<02:30,  1.67s/it]avg_loss = 1.8916269334462972:  37%|███▋      | 52/141 [01:23<02:28,  1.67s/it]avg_loss = 1.8848632124235045:  37%|███▋      | 52/141 [01:25<02:28,  1.67s/it]avg_loss = 1.8848632124235045:  38%|███▊      | 53/141 [01:25<02:27,  1.67s/it]avg_loss = 1.8816943323170696:  38%|███▊      | 53/141 [01:27<02:27,  1.67s/it]avg_loss = 1.8816943323170696:  38%|███▊      | 54/141 [01:27<02:25,  1.67s/it]avg_loss = 1.8738520622253418:  38%|███▊      | 54/141 [01:28<02:25,  1.67s/it]avg_loss = 1.8738520622253418:  39%|███▉      | 55/141 [01:28<02:24,  1.68s/it]avg_loss = 1.8658053704670496:  39%|███▉      | 55/141 [01:30<02:24,  1.68s/it]avg_loss = 1.8658053704670496:  40%|███▉      | 56/141 [01:30<02:22,  1.68s/it]avg_loss = 1.860339133363021:  40%|███▉      | 56/141 [01:32<02:22,  1.68s/it] avg_loss = 1.860339133363021:  40%|████      | 57/141 [01:32<02:20,  1.68s/it]avg_loss = 1.8573226373771141:  40%|████      | 57/141 [01:33<02:20,  1.68s/it]avg_loss = 1.8573226373771141:  41%|████      | 58/141 [01:33<02:19,  1.68s/it]avg_loss = 1.8594256740505413:  41%|████      | 58/141 [01:35<02:19,  1.68s/it]avg_loss = 1.8594256740505413:  42%|████▏     | 59/141 [01:35<02:18,  1.68s/it]avg_loss = 1.8649359862009685:  42%|████▏     | 59/141 [01:37<02:18,  1.68s/it]avg_loss = 1.8649359862009685:  43%|████▎     | 60/141 [01:37<02:16,  1.69s/it]avg_loss = 1.8704568792562015:  43%|████▎     | 60/141 [01:39<02:16,  1.69s/it]avg_loss = 1.8704568792562015:  43%|████▎     | 61/141 [01:39<02:14,  1.69s/it]avg_loss = 1.877599916150493:  43%|████▎     | 61/141 [01:40<02:14,  1.69s/it] avg_loss = 1.877599916150493:  44%|████▍     | 62/141 [01:40<02:13,  1.69s/it]avg_loss = 1.868762697492327:  44%|████▍     | 62/141 [01:42<02:13,  1.69s/it]avg_loss = 1.868762697492327:  45%|████▍     | 63/141 [01:42<02:11,  1.69s/it]avg_loss = 1.8663504738360643:  45%|████▍     | 63/141 [01:44<02:11,  1.69s/it]avg_loss = 1.8663504738360643:  45%|████▌     | 64/141 [01:44<02:10,  1.69s/it]avg_loss = 1.8638782812998844:  45%|████▌     | 64/141 [01:45<02:10,  1.69s/it]avg_loss = 1.8638782812998844:  46%|████▌     | 65/141 [01:45<02:08,  1.69s/it]avg_loss = 1.8576538887890903:  46%|████▌     | 65/141 [01:47<02:08,  1.69s/it]avg_loss = 1.8576538887890903:  47%|████▋     | 66/141 [01:47<02:06,  1.69s/it]avg_loss = 1.854980481204702:  47%|████▋     | 66/141 [01:49<02:06,  1.69s/it] avg_loss = 1.854980481204702:  48%|████▊     | 67/141 [01:49<02:05,  1.69s/it]avg_loss = 1.8517268300056458:  48%|████▊     | 67/141 [01:50<02:05,  1.69s/it]avg_loss = 1.8517268300056458:  48%|████▊     | 68/141 [01:50<02:03,  1.69s/it]avg_loss = 1.8489515936892966:  48%|████▊     | 68/141 [01:52<02:03,  1.69s/it]avg_loss = 1.8489515936892966:  49%|████▉     | 69/141 [01:52<02:01,  1.69s/it]avg_loss = 1.8500170605523245:  49%|████▉     | 69/141 [01:54<02:01,  1.69s/it]avg_loss = 1.8500170605523245:  50%|████▉     | 70/141 [01:54<02:00,  1.69s/it]avg_loss = 1.8536334037780762:  50%|████▉     | 70/141 [01:55<02:00,  1.69s/it]avg_loss = 1.8536334037780762:  50%|█████     | 71/141 [01:55<01:58,  1.69s/it]avg_loss = 1.85614479581515:  50%|█████     | 71/141 [01:57<01:58,  1.69s/it]  avg_loss = 1.85614479581515:  51%|█████     | 72/141 [01:57<01:56,  1.69s/it]avg_loss = 1.8547477232266778:  51%|█████     | 72/141 [01:59<01:56,  1.69s/it]avg_loss = 1.8547477232266778:  52%|█████▏    | 73/141 [01:59<01:55,  1.69s/it]avg_loss = 1.856253696454538:  52%|█████▏    | 73/141 [02:01<01:55,  1.69s/it] avg_loss = 1.856253696454538:  52%|█████▏    | 74/141 [02:01<01:53,  1.70s/it]avg_loss = 1.8563543351491292:  52%|█████▏    | 74/141 [02:02<01:53,  1.70s/it]avg_loss = 1.8563543351491292:  53%|█████▎    | 75/141 [02:02<01:51,  1.70s/it]avg_loss = 1.8553482168599178:  53%|█████▎    | 75/141 [02:04<01:51,  1.70s/it]avg_loss = 1.8553482168599178:  54%|█████▍    | 76/141 [02:04<01:50,  1.70s/it]avg_loss = 1.8566544365573239:  54%|█████▍    | 76/141 [02:06<01:50,  1.70s/it]avg_loss = 1.8566544365573239:  55%|█████▍    | 77/141 [02:06<01:48,  1.70s/it]avg_loss = 1.8589654305042365:  55%|█████▍    | 77/141 [02:07<01:48,  1.70s/it]avg_loss = 1.8589654305042365:  55%|█████▌    | 78/141 [02:07<01:46,  1.70s/it]avg_loss = 1.8628978367093243:  55%|█████▌    | 78/141 [02:09<01:46,  1.70s/it]avg_loss = 1.8628978367093243:  56%|█████▌    | 79/141 [02:09<01:49,  1.77s/it]avg_loss = 1.8598870068788529:  56%|█████▌    | 79/141 [02:11<01:49,  1.77s/it]avg_loss = 1.8598870068788529:  57%|█████▋    | 80/141 [02:11<01:46,  1.74s/it]avg_loss = 1.8588574697941909:  57%|█████▋    | 80/141 [02:13<01:46,  1.74s/it]avg_loss = 1.8588574697941909:  57%|█████▋    | 81/141 [02:13<01:43,  1.73s/it]avg_loss = 1.8580179592458212:  57%|█████▋    | 81/141 [02:14<01:43,  1.73s/it]avg_loss = 1.8580179592458212:  58%|█████▊    | 82/141 [02:14<01:41,  1.72s/it]avg_loss = 1.856012331434043:  58%|█████▊    | 82/141 [02:16<01:41,  1.72s/it] avg_loss = 1.856012331434043:  59%|█████▉    | 83/141 [02:16<01:39,  1.71s/it]avg_loss = 1.8539060141359056:  59%|█████▉    | 83/141 [02:18<01:39,  1.71s/it]avg_loss = 1.8539060141359056:  60%|█████▉    | 84/141 [02:18<01:37,  1.71s/it]avg_loss = 1.8515523391611435:  60%|█████▉    | 84/141 [02:19<01:37,  1.71s/it]avg_loss = 1.8515523391611435:  60%|██████    | 85/141 [02:19<01:35,  1.71s/it]avg_loss = 1.8531136249386988:  60%|██████    | 85/141 [02:21<01:35,  1.71s/it]avg_loss = 1.8531136249386988:  61%|██████    | 86/141 [02:21<01:33,  1.71s/it]avg_loss = 1.8550266630348118:  61%|██████    | 86/141 [02:23<01:33,  1.71s/it]avg_loss = 1.8550266630348118:  62%|██████▏   | 87/141 [02:23<01:32,  1.70s/it]avg_loss = 1.8556171032515438:  62%|██████▏   | 87/141 [02:25<01:32,  1.70s/it]avg_loss = 1.8556171032515438:  62%|██████▏   | 88/141 [02:25<01:30,  1.70s/it]avg_loss = 1.8643699603134327:  62%|██████▏   | 88/141 [02:26<01:30,  1.70s/it]avg_loss = 1.8643699603134327:  63%|██████▎   | 89/141 [02:26<01:28,  1.70s/it]avg_loss = 1.8718336317274304:  63%|██████▎   | 89/141 [02:28<01:28,  1.70s/it]avg_loss = 1.8718336317274304:  64%|██████▍   | 90/141 [02:28<01:26,  1.70s/it]avg_loss = 1.8750827076670888:  64%|██████▍   | 90/141 [02:30<01:26,  1.70s/it]avg_loss = 1.8750827076670888:  65%|██████▍   | 91/141 [02:30<01:25,  1.70s/it]avg_loss = 1.8801205650619839:  65%|██████▍   | 91/141 [02:31<01:25,  1.70s/it]avg_loss = 1.8801205650619839:  65%|██████▌   | 92/141 [02:31<01:23,  1.70s/it]avg_loss = 1.8850900537224227:  65%|██████▌   | 92/141 [02:33<01:23,  1.70s/it]avg_loss = 1.8850900537224227:  66%|██████▌   | 93/141 [02:33<01:21,  1.70s/it]avg_loss = 1.8859888277155288:  66%|██████▌   | 93/141 [02:35<01:21,  1.70s/it]avg_loss = 1.8859888277155288:  67%|██████▋   | 94/141 [02:35<01:20,  1.70s/it]avg_loss = 1.8898107014204326:  67%|██████▋   | 94/141 [02:36<01:20,  1.70s/it]avg_loss = 1.8898107014204326:  67%|██████▋   | 95/141 [02:36<01:18,  1.70s/it]avg_loss = 1.8906786205867927:  67%|██████▋   | 95/141 [02:38<01:18,  1.70s/it]avg_loss = 1.8906786205867927:  68%|██████▊   | 96/141 [02:38<01:16,  1.70s/it]avg_loss = 1.8925376294814433:  68%|██████▊   | 96/141 [02:40<01:16,  1.70s/it]avg_loss = 1.8925376294814433:  69%|██████▉   | 97/141 [02:40<01:15,  1.71s/it]avg_loss = 1.889193360902825:  69%|██████▉   | 97/141 [02:42<01:15,  1.71s/it] avg_loss = 1.889193360902825:  70%|██████▉   | 98/141 [02:42<01:13,  1.71s/it]avg_loss = 1.8901979092395667:  70%|██████▉   | 98/141 [02:43<01:13,  1.71s/it]avg_loss = 1.8901979092395667:  70%|███████   | 99/141 [02:43<01:11,  1.71s/it]avg_loss = 1.8923584043979644:  70%|███████   | 99/141 [02:45<01:11,  1.71s/it]avg_loss = 1.8923584043979644:  71%|███████   | 100/141 [02:45<01:10,  1.71s/it]avg_loss = 1.8912149549710868:  71%|███████   | 100/141 [02:47<01:10,  1.71s/it]avg_loss = 1.8912149549710868:  72%|███████▏  | 101/141 [02:47<01:08,  1.71s/it]avg_loss = 1.8915459408479578:  72%|███████▏  | 101/141 [02:48<01:08,  1.71s/it]avg_loss = 1.8915459408479578:  72%|███████▏  | 102/141 [02:48<01:06,  1.71s/it]avg_loss = 1.8900463326463421:  72%|███████▏  | 102/141 [02:50<01:06,  1.71s/it]avg_loss = 1.8900463326463421:  73%|███████▎  | 103/141 [02:50<01:04,  1.71s/it]avg_loss = 1.8925996308143322:  73%|███████▎  | 103/141 [02:52<01:04,  1.71s/it]avg_loss = 1.8925996308143322:  74%|███████▍  | 104/141 [02:52<01:03,  1.71s/it]avg_loss = 1.891310199101766:  74%|███████▍  | 104/141 [02:54<01:03,  1.71s/it] avg_loss = 1.891310199101766:  74%|███████▍  | 105/141 [02:54<01:01,  1.71s/it]avg_loss = 1.8904886987974059:  74%|███████▍  | 105/141 [02:55<01:01,  1.71s/it]avg_loss = 1.8904886987974059:  75%|███████▌  | 106/141 [02:55<00:59,  1.71s/it]avg_loss = 1.888258813697601:  75%|███████▌  | 106/141 [02:57<00:59,  1.71s/it] avg_loss = 1.888258813697601:  76%|███████▌  | 107/141 [02:57<00:58,  1.71s/it]avg_loss = 1.8860220754588093:  76%|███████▌  | 107/141 [02:59<00:58,  1.71s/it]avg_loss = 1.8860220754588093:  77%|███████▋  | 108/141 [02:59<00:56,  1.71s/it]avg_loss = 1.8836312884584478:  77%|███████▋  | 108/141 [03:00<00:56,  1.71s/it]avg_loss = 1.8836312884584478:  77%|███████▋  | 109/141 [03:00<00:54,  1.71s/it]avg_loss = 1.8811617677861994:  77%|███████▋  | 109/141 [03:02<00:54,  1.71s/it]avg_loss = 1.8811617677861994:  78%|███████▊  | 110/141 [03:02<00:53,  1.71s/it]avg_loss = 1.8835598778080296:  78%|███████▊  | 110/141 [03:04<00:53,  1.71s/it]avg_loss = 1.8835598778080296:  79%|███████▊  | 111/141 [03:04<00:51,  1.71s/it]avg_loss = 1.8832861525671822:  79%|███████▊  | 111/141 [03:06<00:51,  1.71s/it]avg_loss = 1.8832861525671822:  79%|███████▉  | 112/141 [03:06<00:49,  1.71s/it]avg_loss = 1.8843238374828237:  79%|███████▉  | 112/141 [03:07<00:49,  1.71s/it]avg_loss = 1.8843238374828237:  80%|████████  | 113/141 [03:07<00:48,  1.71s/it]avg_loss = 1.885338308518393:  80%|████████  | 113/141 [03:09<00:48,  1.71s/it] avg_loss = 1.885338308518393:  81%|████████  | 114/141 [03:09<00:46,  1.71s/it]avg_loss = 1.8846187550088633:  81%|████████  | 114/141 [03:11<00:46,  1.71s/it]avg_loss = 1.8846187550088633:  82%|████████▏ | 115/141 [03:11<00:44,  1.71s/it]avg_loss = 1.8833028653572346:  82%|████████▏ | 115/141 [03:12<00:44,  1.71s/it]avg_loss = 1.8833028653572346:  82%|████████▏ | 116/141 [03:12<00:42,  1.71s/it]avg_loss = 1.8854834809262528:  82%|████████▏ | 116/141 [03:14<00:42,  1.71s/it]avg_loss = 1.8854834809262528:  83%|████████▎ | 117/141 [03:14<00:41,  1.71s/it]avg_loss = 1.8850579120345035:  83%|████████▎ | 117/141 [03:16<00:41,  1.71s/it]avg_loss = 1.8850579120345035:  84%|████████▎ | 118/141 [03:16<00:39,  1.71s/it]avg_loss = 1.8836813373725956:  84%|████████▎ | 118/141 [03:18<00:39,  1.71s/it]avg_loss = 1.8836813373725956:  84%|████████▍ | 119/141 [03:18<00:37,  1.72s/it]avg_loss = 1.8818646013736724:  84%|████████▍ | 119/141 [03:19<00:37,  1.72s/it]avg_loss = 1.8818646013736724:  85%|████████▌ | 120/141 [03:19<00:36,  1.71s/it]avg_loss = 1.881785478473695:  85%|████████▌ | 120/141 [03:21<00:36,  1.71s/it] avg_loss = 1.881785478473695:  86%|████████▌ | 121/141 [03:21<00:34,  1.72s/it]avg_loss = 1.882292266751899:  86%|████████▌ | 121/141 [03:23<00:34,  1.72s/it]avg_loss = 1.882292266751899:  87%|████████▋ | 122/141 [03:23<00:32,  1.72s/it]avg_loss = 1.8820745295625392:  87%|████████▋ | 122/141 [03:24<00:32,  1.72s/it]avg_loss = 1.8820745295625392:  87%|████████▋ | 123/141 [03:24<00:30,  1.72s/it]avg_loss = 1.8823148627435007:  87%|████████▋ | 123/141 [03:26<00:30,  1.72s/it]avg_loss = 1.8823148627435007:  88%|████████▊ | 124/141 [03:26<00:29,  1.72s/it]avg_loss = 1.881035400390625:  88%|████████▊ | 124/141 [03:28<00:29,  1.72s/it] avg_loss = 1.881035400390625:  89%|████████▊ | 125/141 [03:28<00:27,  1.72s/it]avg_loss = 1.8814012862387157:  89%|████████▊ | 125/141 [03:30<00:27,  1.72s/it]avg_loss = 1.8814012862387157:  89%|████████▉ | 126/141 [03:30<00:25,  1.72s/it]avg_loss = 1.8811010591627106:  89%|████████▉ | 126/141 [03:31<00:25,  1.72s/it]avg_loss = 1.8811010591627106:  90%|█████████ | 127/141 [03:31<00:24,  1.72s/it]avg_loss = 1.8798941429704428:  90%|█████████ | 127/141 [03:33<00:24,  1.72s/it]avg_loss = 1.8798941429704428:  91%|█████████ | 128/141 [03:33<00:22,  1.72s/it]avg_loss = 1.8800356951794883:  91%|█████████ | 128/141 [03:35<00:22,  1.72s/it]avg_loss = 1.8800356951794883:  91%|█████████▏| 129/141 [03:35<00:20,  1.72s/it]avg_loss = 1.8809137766177837:  91%|█████████▏| 129/141 [03:36<00:20,  1.72s/it]avg_loss = 1.8809137766177837:  92%|█████████▏| 130/141 [03:36<00:18,  1.72s/it]avg_loss = 1.881962182867618:  92%|█████████▏| 130/141 [03:38<00:18,  1.72s/it] avg_loss = 1.881962182867618:  93%|█████████▎| 131/141 [03:38<00:17,  1.72s/it]avg_loss = 1.8825835892648408:  93%|█████████▎| 131/141 [03:40<00:17,  1.72s/it]avg_loss = 1.8825835892648408:  94%|█████████▎| 132/141 [03:40<00:15,  1.72s/it]avg_loss = 1.87962920325143:  94%|█████████▎| 132/141 [03:42<00:15,  1.72s/it]  avg_loss = 1.87962920325143:  94%|█████████▍| 133/141 [03:42<00:13,  1.72s/it]avg_loss = 1.8751313508446537:  94%|█████████▍| 133/141 [03:43<00:13,  1.72s/it]avg_loss = 1.8751313508446537:  95%|█████████▌| 134/141 [03:43<00:12,  1.72s/it]avg_loss = 1.8775543000963:  95%|█████████▌| 134/141 [03:45<00:12,  1.72s/it]   avg_loss = 1.8775543000963:  96%|█████████▌| 135/141 [03:45<00:10,  1.72s/it]avg_loss = 1.881065968204947:  96%|█████████▌| 135/141 [03:47<00:10,  1.72s/it]avg_loss = 1.881065968204947:  96%|█████████▋| 136/141 [03:47<00:08,  1.72s/it]avg_loss = 1.8822661055265553:  96%|█████████▋| 136/141 [03:49<00:08,  1.72s/it]avg_loss = 1.8822661055265553:  97%|█████████▋| 137/141 [03:49<00:06,  1.72s/it]avg_loss = 1.8809849291607954:  97%|█████████▋| 137/141 [03:50<00:06,  1.72s/it]avg_loss = 1.8809849291607954:  98%|█████████▊| 138/141 [03:50<00:05,  1.72s/it]avg_loss = 1.8812832635083645:  98%|█████████▊| 138/141 [03:52<00:05,  1.72s/it]avg_loss = 1.8812832635083645:  99%|█████████▊| 139/141 [03:52<00:03,  1.72s/it]avg_loss = 1.8819454721042088:  99%|█████████▊| 139/141 [03:54<00:03,  1.72s/it]avg_loss = 1.8819454721042088:  99%|█████████▉| 140/141 [03:54<00:01,  1.73s/it]avg_loss = 1.8832529013883983:  99%|█████████▉| 140/141 [03:55<00:01,  1.73s/it]avg_loss = 1.8832529013883983: 100%|██████████| 141/141 [03:55<00:00,  1.73s/it]avg_loss = 1.8832529013883983: 100%|██████████| 141/141 [03:55<00:00,  1.67s/it]
I0326 12:22:26.356842 1645976 eval_ppl.py:107] wikitext2 perplexity: 6.574857234954834
wikitext2 perplexity: 6.575
