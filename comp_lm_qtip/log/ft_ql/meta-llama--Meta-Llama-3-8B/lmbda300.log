I0326 12:22:34.865562 1649289 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 12:22:34.865668 1649289 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 12:22:34.865712 1649289 utils.py:162] NumExpr defaulting to 16 threads.
I0326 12:22:35.211266 1649289 config.py:54] PyTorch version 2.6.0 available.
W0326 12:22:35.414804 1649289 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 12:22:35.987485 1649289 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.96it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.42it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.14it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.38it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.67it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.50it/s]
I0326 12:22:37.521131 1649289 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.43it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:18,  1.66it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.78it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.84it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.87it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.89it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.90it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.90it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:12,  1.91it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.91it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:10,  1.91it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.91it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:09,  1.91it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.91it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:08,  1.93it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.93it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.94it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.95it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.95it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.94it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.94it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.94it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.93it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.93it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.93it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.95it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.94it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.94it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.95it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.96it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]
I0326 12:23:01.405819 1649289 quantize_finetune_llama.py:185] loaded compression model
I0326 12:23:21.431748 1649289 quantize_finetune_llama.py:189] loaded dataset and devset
I0326 12:23:24.461666 1649289 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 12:24:18.668698 1649289 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 54.06260967254639s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0326 12:24:36.492646 1650599 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 12:24:36.492748 1650599 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 12:24:36.492787 1650599 utils.py:162] NumExpr defaulting to 16 threads.
I0326 12:24:36.826886 1650599 config.py:54] PyTorch version 2.6.0 available.
W0326 12:24:37.028156 1650599 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 12:24:37.636099 1650599 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 12:24:37.639740 1649289 quantize_finetune_llama.py:209] layer 1 gpu 1
I0326 12:24:37.653179 1650599 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 12:24:45.162784 1650599 finetune.py:45] layer 0_v initial loss 1.1523158036652603e-06
W0326 12:24:45.163059 1650599 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 12:25:19.976029 1650599 finetune.py:68] layer 0_v @ epoch 0 new loss 7.348430699494202e-07 old loss 1.1523158036652603e-06 BETTER
I0326 12:25:42.355406 1649289 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 64.54698848724365s
I0326 12:25:51.182445 1651368 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 12:25:51.182539 1651368 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 12:25:51.182582 1651368 utils.py:162] NumExpr defaulting to 16 threads.
I0326 12:25:51.534537 1651368 config.py:54] PyTorch version 2.6.0 available.
W0326 12:25:51.733904 1651368 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 12:25:52.608433 1651368 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 12:25:52.612148 1649289 quantize_finetune_llama.py:209] layer 2 gpu 0
I0326 12:25:52.625617 1651368 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 12:25:56.529390 1650599 finetune.py:68] layer 0_v @ epoch 1 new loss 6.260116833800566e-07 old loss 7.348430699494202e-07 BETTER
I0326 12:25:59.769933 1651368 finetune.py:45] layer 1_v initial loss 2.4209448383771814e-06
W0326 12:25:59.770142 1651368 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 12:26:33.070012 1651368 finetune.py:68] layer 1_v @ epoch 0 new loss 1.2789613492714125e-06 old loss 2.4209448383771814e-06 BETTER
I0326 12:26:33.640803 1650599 finetune.py:68] layer 0_v @ epoch 2 new loss 5.777552019026189e-07 old loss 6.260116833800566e-07 BETTER
I0326 12:27:07.545046 1651368 finetune.py:68] layer 1_v @ epoch 1 new loss 9.534336413707933e-07 old loss 1.2789613492714125e-06 BETTER
I0326 12:27:10.606966 1650599 finetune.py:68] layer 0_v @ epoch 3 new loss 5.506079219230742e-07 old loss 5.777552019026189e-07 BETTER
I0326 12:27:42.388479 1651368 finetune.py:68] layer 1_v @ epoch 2 new loss 8.32743580758688e-07 old loss 9.534336413707933e-07 BETTER
I0326 12:27:47.903988 1650599 finetune.py:68] layer 0_v @ epoch 4 new loss 5.325701408764871e-07 old loss 5.506079219230742e-07 BETTER
I0326 12:27:57.969891 1650599 finetune.py:45] layer 0_q initial loss 5.357571239983372e-07
I0326 12:28:17.493438 1651368 finetune.py:68] layer 1_v @ epoch 3 new loss 7.691714927204885e-07 old loss 8.32743580758688e-07 BETTER
I0326 12:28:34.243870 1650599 finetune.py:68] layer 0_q @ epoch 0 new loss 5.199691486268421e-07 old loss 5.357571239983372e-07 BETTER
I0326 12:28:52.997763 1651368 finetune.py:68] layer 1_v @ epoch 4 new loss 7.305376925614837e-07 old loss 7.691714927204885e-07 BETTER
I0326 12:29:02.771753 1651368 finetune.py:45] layer 1_q initial loss 7.618788799845788e-07
I0326 12:29:11.255395 1650599 finetune.py:68] layer 0_q @ epoch 1 new loss 5.082691814095597e-07 old loss 5.199691486268421e-07 BETTER
I0326 12:29:36.660828 1651368 finetune.py:68] layer 1_q @ epoch 0 new loss 7.20048717539612e-07 old loss 7.618788799845788e-07 BETTER
I0326 12:29:48.509571 1650599 finetune.py:68] layer 0_q @ epoch 2 new loss 4.98678218718851e-07 old loss 5.082691814095597e-07 BETTER
I0326 12:30:11.412953 1651368 finetune.py:68] layer 1_q @ epoch 1 new loss 6.944487154214585e-07 old loss 7.20048717539612e-07 BETTER
I0326 12:30:25.526566 1650599 finetune.py:68] layer 0_q @ epoch 3 new loss 4.90476281811425e-07 old loss 4.98678218718851e-07 BETTER
I0326 12:30:46.223322 1651368 finetune.py:68] layer 1_q @ epoch 2 new loss 6.740282856299018e-07 old loss 6.944487154214585e-07 BETTER
I0326 12:31:02.577661 1650599 finetune.py:68] layer 0_q @ epoch 4 new loss 4.831791216020065e-07 old loss 4.90476281811425e-07 BETTER
I0326 12:31:10.940703 1650599 finetune.py:45] layer 0_k initial loss 4.927273380417319e-07
I0326 12:31:21.056978 1651368 finetune.py:68] layer 1_q @ epoch 3 new loss 6.568194521605619e-07 old loss 6.740282856299018e-07 BETTER
I0326 12:31:47.271338 1650599 finetune.py:68] layer 0_k @ epoch 0 new loss 4.834738547287998e-07 old loss 4.927273380417319e-07 BETTER
I0326 12:31:55.838931 1651368 finetune.py:68] layer 1_q @ epoch 4 new loss 6.420399358830764e-07 old loss 6.568194521605619e-07 BETTER
I0326 12:32:03.656600 1651368 finetune.py:45] layer 1_k initial loss 6.739128934896144e-07
I0326 12:32:24.197919 1650599 finetune.py:68] layer 0_k @ epoch 1 new loss 4.769767656398471e-07 old loss 4.834738547287998e-07 BETTER
I0326 12:32:37.497693 1651368 finetune.py:68] layer 1_k @ epoch 0 new loss 6.547829229930358e-07 old loss 6.739128934896144e-07 BETTER
I0326 12:33:00.808341 1650599 finetune.py:68] layer 0_k @ epoch 2 new loss 4.713162127245596e-07 old loss 4.769767656398471e-07 BETTER
I0326 12:33:11.997624 1651368 finetune.py:68] layer 1_k @ epoch 1 new loss 6.418536031560507e-07 old loss 6.547829229930358e-07 BETTER
I0326 12:33:37.474253 1650599 finetune.py:68] layer 0_k @ epoch 3 new loss 4.662551305045781e-07 old loss 4.713162127245596e-07 BETTER
I0326 12:33:46.716811 1651368 finetune.py:68] layer 1_k @ epoch 2 new loss 6.306789828158799e-07 old loss 6.418536031560507e-07 BETTER
I0326 12:34:14.023308 1650599 finetune.py:68] layer 0_k @ epoch 4 new loss 4.61569811704976e-07 old loss 4.662551305045781e-07 BETTER
I0326 12:34:21.453140 1651368 finetune.py:68] layer 1_k @ epoch 3 new loss 6.205968929862138e-07 old loss 6.306789828158799e-07 BETTER
I0326 12:34:24.246821 1650599 finetune.py:45] layer 0_o initial loss 7.566269459857722e-07
I0326 12:34:56.079459 1651368 finetune.py:68] layer 1_k @ epoch 4 new loss 6.114157145020727e-07 old loss 6.205968929862138e-07 BETTER
I0326 12:34:59.498118 1650599 finetune.py:68] layer 0_o @ epoch 0 new loss 7.488123401344637e-07 old loss 7.566269459857722e-07 BETTER
I0326 12:35:06.037997 1651368 finetune.py:45] layer 1_o initial loss 1.3990363640914438e-06
I0326 12:35:35.945937 1650599 finetune.py:68] layer 0_o @ epoch 1 new loss 7.425259127558093e-07 old loss 7.488123401344637e-07 BETTER
I0326 12:35:39.292464 1651368 finetune.py:68] layer 1_o @ epoch 0 new loss 1.378134925289487e-06 old loss 1.3990363640914438e-06 BETTER
I0326 12:36:12.019345 1650599 finetune.py:68] layer 0_o @ epoch 2 new loss 7.371410220002872e-07 old loss 7.425259127558093e-07 BETTER
I0326 12:36:13.237869 1651368 finetune.py:68] layer 1_o @ epoch 1 new loss 1.3621001926367171e-06 old loss 1.378134925289487e-06 BETTER
I0326 12:36:47.126858 1651368 finetune.py:68] layer 1_o @ epoch 2 new loss 1.3492831385519821e-06 old loss 1.3621001926367171e-06 BETTER
I0326 12:36:48.061689 1650599 finetune.py:68] layer 0_o @ epoch 3 new loss 7.324919693019183e-07 old loss 7.371410220002872e-07 BETTER
I0326 12:37:20.984632 1651368 finetune.py:68] layer 1_o @ epoch 3 new loss 1.3386312502916553e-06 old loss 1.3492831385519821e-06 BETTER
I0326 12:37:24.333924 1650599 finetune.py:68] layer 0_o @ epoch 4 new loss 7.283908871613676e-07 old loss 7.324919693019183e-07 BETTER
I0326 12:37:46.751886 1650599 finetune.py:45] layer 0_up initial loss 9.52396476350259e-07
I0326 12:37:54.909718 1651368 finetune.py:68] layer 1_o @ epoch 4 new loss 1.3295639291754924e-06 old loss 1.3386312502916553e-06 BETTER
I0326 12:38:16.893334 1651368 finetune.py:45] layer 1_up initial loss 1.8554660528025124e-06
I0326 12:38:18.851327 1650599 finetune.py:68] layer 0_up @ epoch 0 new loss 9.474196644987387e-07 old loss 9.52396476350259e-07 BETTER
I0326 12:38:47.645223 1651368 finetune.py:68] layer 1_up @ epoch 0 new loss 1.833902842918178e-06 old loss 1.8554660528025124e-06 BETTER
I0326 12:38:52.172425 1650599 finetune.py:68] layer 0_up @ epoch 1 new loss 9.437864036954124e-07 old loss 9.474196644987387e-07 BETTER
I0326 12:39:19.517899 1651368 finetune.py:68] layer 1_up @ epoch 1 new loss 1.82800829406915e-06 old loss 1.833902842918178e-06 BETTER
I0326 12:39:25.644142 1650599 finetune.py:68] layer 0_up @ epoch 2 new loss 9.406538197254122e-07 old loss 9.437864036954124e-07 BETTER
I0326 12:39:51.631235 1651368 finetune.py:68] layer 1_up @ epoch 2 new loss 1.8198059024143731e-06 old loss 1.82800829406915e-06 BETTER
I0326 12:39:59.146265 1650599 finetune.py:68] layer 0_up @ epoch 3 new loss 9.3797041245125e-07 old loss 9.406538197254122e-07 BETTER
I0326 12:40:23.794762 1651368 finetune.py:68] layer 1_up @ epoch 3 new loss 1.8141305417884723e-06 old loss 1.8198059024143731e-06 BETTER
I0326 12:40:32.766286 1650599 finetune.py:68] layer 0_up @ epoch 4 new loss 9.354923236060131e-07 old loss 9.3797041245125e-07 BETTER
I0326 12:40:54.895677 1650599 finetune.py:45] layer 0_gate initial loss 1.0788996860355837e-06
I0326 12:40:55.914255 1651368 finetune.py:68] layer 1_up @ epoch 4 new loss 1.8089151581079932e-06 old loss 1.8141305417884723e-06 BETTER
I0326 12:41:17.839501 1651368 finetune.py:45] layer 1_gate initial loss 2.188116241086391e-06
I0326 12:41:25.041806 1650599 finetune.py:68] layer 0_gate @ epoch 0 new loss 1.0751422223620466e-06 old loss 1.0788996860355837e-06 BETTER
I0326 12:41:46.682414 1651368 finetune.py:68] layer 1_gate @ epoch 0 new loss 2.1511598333745496e-06 old loss 2.188116241086391e-06 BETTER
I0326 12:41:56.239080 1650599 finetune.py:68] layer 0_gate @ epoch 1 new loss 1.072356099030003e-06 old loss 1.0751422223620466e-06 BETTER
I0326 12:42:16.440447 1651368 finetune.py:68] layer 1_gate @ epoch 1 new loss 2.14671490539331e-06 old loss 2.1511598333745496e-06 BETTER
I0326 12:42:27.591620 1650599 finetune.py:68] layer 0_gate @ epoch 2 new loss 1.070101802724821e-06 old loss 1.072356099030003e-06 BETTER
I0326 12:42:46.261422 1651368 finetune.py:68] layer 1_gate @ epoch 2 new loss 2.1438984276755946e-06 old loss 2.14671490539331e-06 BETTER
I0326 12:42:58.972328 1650599 finetune.py:68] layer 0_gate @ epoch 3 new loss 1.0680854529709904e-06 old loss 1.070101802724821e-06 BETTER
I0326 12:43:16.141702 1651368 finetune.py:68] layer 1_gate @ epoch 3 new loss 2.14192868952523e-06 old loss 2.1438984276755946e-06 BETTER
I0326 12:43:30.512070 1650599 finetune.py:68] layer 0_gate @ epoch 4 new loss 1.0663560487955692e-06 old loss 1.0680854529709904e-06 BETTER
I0326 12:43:45.922709 1651368 finetune.py:68] layer 1_gate @ epoch 4 new loss 2.136825969500933e-06 old loss 2.14192868952523e-06 BETTER
I0326 12:43:53.599587 1650599 finetune.py:45] layer 0_down initial loss 1.583953803674376e-06
I0326 12:44:08.786232 1651368 finetune.py:45] layer 1_down initial loss 8.467865882266778e-06
I0326 12:44:21.590469 1650599 finetune.py:68] layer 0_down @ epoch 0 new loss 1.5830368056413135e-06 old loss 1.583953803674376e-06 BETTER
I0326 12:44:35.441512 1651368 finetune.py:68] layer 1_down @ epoch 0 new loss 8.260503818746656e-06 old loss 8.467865882266778e-06 BETTER
I0326 12:44:50.568837 1650599 finetune.py:68] layer 0_down @ epoch 1 new loss 1.5825753507670015e-06 old loss 1.5830368056413135e-06 BETTER
I0326 12:45:03.130670 1651368 finetune.py:68] layer 1_down @ epoch 1 new loss 8.115004675346427e-06 old loss 8.260503818746656e-06 BETTER
I0326 12:45:19.884071 1650599 finetune.py:68] layer 0_down @ epoch 2 new loss 1.5822037084944895e-06 old loss 1.5825753507670015e-06 BETTER
I0326 12:45:30.819838 1651368 finetune.py:68] layer 1_down @ epoch 2 new loss 8.049891221162397e-06 old loss 8.115004675346427e-06 BETTER
I0326 12:45:49.280720 1650599 finetune.py:68] layer 0_down @ epoch 3 new loss 1.5819114196347073e-06 old loss 1.5822037084944895e-06 BETTER
I0326 12:45:58.969314 1651368 finetune.py:68] layer 1_down @ epoch 3 new loss 8.028318916331045e-06 old loss 8.049891221162397e-06 BETTER
I0326 12:46:19.326555 1650599 finetune.py:68] layer 0_down @ epoch 4 new loss 1.5816092400200432e-06 old loss 1.5819114196347073e-06 BETTER
0_v proxy err 0.027930868789553642 tr(WHW.T) 60.88684844970703
bpp_loss 3.3574801683425903
0_q proxy err 0.00014515793009195477 tr(WHW.T) 288077.59375
bpp_loss 4.222598314285278
0_k proxy err 0.0001570137101225555 tr(WHW.T) 100157.5546875
bpp_loss 4.8328166007995605
0_o proxy err 0.003340364433825016 tr(WHW.T) 3149.369873046875
bpp_loss 3.452609419822693
0_up proxy err 0.006254592910408974 tr(WHW.T) 8921.7724609375
bpp_loss 3.739729745047433
0_gate proxy err 0.0035946634598076344 tr(WHW.T) 15776.701171875
bpp_loss 3.8542628969464983
0_down proxy err 0.004468719474971294 tr(WHW.T) 10831.6611328125
bpp_loss 3.7336863449641635
I0326 12:46:27.283211 1651368 finetune.py:68] layer 1_down @ epoch 4 new loss 8.01392161520198e-06 old loss 8.028318916331045e-06 BETTER
1_v proxy err 0.017024755477905273 tr(WHW.T) 109.07096099853516
bpp_loss 3.459620952606201
1_q proxy err 0.0001460738421883434 tr(WHW.T) 144823.5625
bpp_loss 4.493837594985962
1_k proxy err 0.0001231122441822663 tr(WHW.T) 75519.6640625
bpp_loss 5.27608585357666
1_o proxy err 0.006613825913518667 tr(WHW.T) 1985.3060302734375
bpp_loss 3.5332638025283813
1_up proxy err 0.006854985374957323 tr(WHW.T) 8231.8486328125
bpp_loss 3.7541572025844028
1_gate proxy err 0.004088650457561016 tr(WHW.T) 13949.521484375
bpp_loss 3.864859308515276
1_down proxy err 0.00076234748121351 tr(WHW.T) 13992.5458984375
bpp_loss 3.7488588946206227
I0326 12:47:37.310029 1649289 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 64.7509171962738s
I0326 12:47:40.855846 1663736 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 12:47:40.855942 1663736 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 12:47:40.855983 1663736 utils.py:162] NumExpr defaulting to 16 threads.
I0326 12:47:41.193920 1663736 config.py:54] PyTorch version 2.6.0 available.
W0326 12:47:41.401417 1663736 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 12:47:42.074762 1663736 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 12:47:42.078805 1649289 quantize_finetune_llama.py:209] layer 3 gpu 1
I0326 12:47:42.092344 1663736 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 12:47:49.469401 1663736 finetune.py:45] layer 2_v initial loss 4.635903223970672e-06
W0326 12:47:49.469640 1663736 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 12:48:25.039344 1663736 finetune.py:68] layer 2_v @ epoch 0 new loss 1.7476287439421867e-06 old loss 4.635903223970672e-06 BETTER
I0326 12:48:45.145748 1649289 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 62.60923218727112s
I0326 12:48:48.818289 1664448 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 12:48:48.818458 1664448 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 12:48:48.818524 1664448 utils.py:162] NumExpr defaulting to 16 threads.
I0326 12:48:49.155851 1664448 config.py:54] PyTorch version 2.6.0 available.
W0326 12:48:49.361930 1664448 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 12:48:50.010508 1664448 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 12:48:50.014140 1649289 quantize_finetune_llama.py:209] layer 4 gpu 0
I0326 12:48:50.027678 1664448 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 12:48:57.342878 1664448 finetune.py:45] layer 3_v initial loss 5.596438313659746e-06
W0326 12:48:57.343346 1664448 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 12:49:01.914677 1663736 finetune.py:68] layer 2_v @ epoch 1 new loss 1.2423711268638726e-06 old loss 1.7476287439421867e-06 BETTER
I0326 12:49:30.845657 1664448 finetune.py:68] layer 3_v @ epoch 0 new loss 2.1878643110539997e-06 old loss 5.596438313659746e-06 BETTER
I0326 12:49:38.932973 1663736 finetune.py:68] layer 2_v @ epoch 2 new loss 1.0994192507496336e-06 old loss 1.2423711268638726e-06 BETTER
I0326 12:50:05.419215 1664448 finetune.py:68] layer 3_v @ epoch 1 new loss 1.6491366068294155e-06 old loss 2.1878643110539997e-06 BETTER
I0326 12:50:15.882379 1663736 finetune.py:68] layer 2_v @ epoch 3 new loss 1.032637328535202e-06 old loss 1.0994192507496336e-06 BETTER
I0326 12:50:40.419220 1664448 finetune.py:68] layer 3_v @ epoch 2 new loss 1.4630938949267147e-06 old loss 1.6491366068294155e-06 BETTER
I0326 12:50:52.988447 1663736 finetune.py:68] layer 2_v @ epoch 4 new loss 9.901498287945287e-07 old loss 1.032637328535202e-06 BETTER
I0326 12:51:02.943938 1663736 finetune.py:45] layer 2_q initial loss 1.1446686585259158e-06
I0326 12:51:15.540832 1664448 finetune.py:68] layer 3_v @ epoch 3 new loss 1.3667056464328198e-06 old loss 1.4630938949267147e-06 BETTER
I0326 12:51:38.882079 1663736 finetune.py:68] layer 2_q @ epoch 0 new loss 1.0856011840587598e-06 old loss 1.1446686585259158e-06 BETTER
I0326 12:51:50.843563 1664448 finetune.py:68] layer 3_v @ epoch 4 new loss 1.3058066770099686e-06 old loss 1.3667056464328198e-06 BETTER
I0326 12:52:00.549172 1664448 finetune.py:45] layer 3_q initial loss 1.6195479020097991e-06
I0326 12:52:15.640122 1663736 finetune.py:68] layer 2_q @ epoch 1 new loss 1.0533210570429219e-06 old loss 1.0856011840587598e-06 BETTER
I0326 12:52:34.484329 1664448 finetune.py:68] layer 3_q @ epoch 0 new loss 1.4862994248687755e-06 old loss 1.6195479020097991e-06 BETTER
I0326 12:52:52.638352 1663736 finetune.py:68] layer 2_q @ epoch 2 new loss 1.0291420267094509e-06 old loss 1.0533210570429219e-06 BETTER
I0326 12:53:09.461207 1664448 finetune.py:68] layer 3_q @ epoch 1 new loss 1.441587983208592e-06 old loss 1.4862994248687755e-06 BETTER
I0326 12:53:29.488413 1663736 finetune.py:68] layer 2_q @ epoch 3 new loss 1.00971976735309e-06 old loss 1.0291420267094509e-06 BETTER
I0326 12:53:44.130179 1664448 finetune.py:68] layer 3_q @ epoch 2 new loss 1.4090935565036489e-06 old loss 1.441587983208592e-06 BETTER
I0326 12:54:06.294051 1663736 finetune.py:68] layer 2_q @ epoch 4 new loss 9.93580897556967e-07 old loss 1.00971976735309e-06 BETTER
I0326 12:54:14.676718 1663736 finetune.py:45] layer 2_k initial loss 1.0740621974036912e-06
I0326 12:54:18.925081 1664448 finetune.py:68] layer 3_q @ epoch 3 new loss 1.3821537550029461e-06 old loss 1.4090935565036489e-06 BETTER
I0326 12:54:50.458539 1663736 finetune.py:68] layer 2_k @ epoch 0 new loss 1.0493166655578534e-06 old loss 1.0740621974036912e-06 BETTER
I0326 12:54:53.648972 1664448 finetune.py:68] layer 3_q @ epoch 4 new loss 1.3610496125693317e-06 old loss 1.3821537550029461e-06 BETTER
I0326 12:55:01.592765 1664448 finetune.py:45] layer 3_k initial loss 1.499770064583572e-06
I0326 12:55:26.944170 1663736 finetune.py:68] layer 2_k @ epoch 1 new loss 1.0357706514696474e-06 old loss 1.0493166655578534e-06 BETTER
I0326 12:55:35.497368 1664448 finetune.py:68] layer 3_k @ epoch 0 new loss 1.4549643765349174e-06 old loss 1.499770064583572e-06 BETTER
I0326 12:56:03.486637 1663736 finetune.py:68] layer 2_k @ epoch 2 new loss 1.0242011967420694e-06 old loss 1.0357706514696474e-06 BETTER
I0326 12:56:10.017561 1664448 finetune.py:68] layer 3_k @ epoch 1 new loss 1.436574279978231e-06 old loss 1.4549643765349174e-06 BETTER
I0326 12:56:40.083575 1663736 finetune.py:68] layer 2_k @ epoch 3 new loss 1.0140979611605871e-06 old loss 1.0242011967420694e-06 BETTER
I0326 12:56:44.675584 1664448 finetune.py:68] layer 3_k @ epoch 2 new loss 1.4206478908818099e-06 old loss 1.436574279978231e-06 BETTER
I0326 12:57:16.649042 1663736 finetune.py:68] layer 2_k @ epoch 4 new loss 1.0048031526821433e-06 old loss 1.0140979611605871e-06 BETTER
I0326 12:57:19.072671 1664448 finetune.py:68] layer 3_k @ epoch 3 new loss 1.406795149705431e-06 old loss 1.4206478908818099e-06 BETTER
I0326 12:57:26.567955 1663736 finetune.py:45] layer 2_o initial loss 2.196833520429209e-06
I0326 12:57:53.695806 1664448 finetune.py:68] layer 3_k @ epoch 4 new loss 1.3941019005869748e-06 old loss 1.406795149705431e-06 BETTER
I0326 12:58:01.654214 1663736 finetune.py:68] layer 2_o @ epoch 0 new loss 2.1369448859331897e-06 old loss 2.196833520429209e-06 BETTER
I0326 12:58:03.312632 1664448 finetune.py:45] layer 3_o initial loss 3.6229823763278546e-06
I0326 12:58:36.451602 1664448 finetune.py:68] layer 3_o @ epoch 0 new loss 3.49205720340251e-06 old loss 3.6229823763278546e-06 BETTER
I0326 12:58:37.488965 1663736 finetune.py:68] layer 2_o @ epoch 1 new loss 2.1011028366046958e-06 old loss 2.1369448859331897e-06 BETTER
I0326 12:59:10.367631 1664448 finetune.py:68] layer 3_o @ epoch 1 new loss 3.431017148614046e-06 old loss 3.49205720340251e-06 BETTER
I0326 12:59:13.259226 1663736 finetune.py:68] layer 2_o @ epoch 2 new loss 2.076761575153796e-06 old loss 2.1011028366046958e-06 BETTER
I0326 12:59:44.246391 1664448 finetune.py:68] layer 3_o @ epoch 2 new loss 3.3874589462357108e-06 old loss 3.431017148614046e-06 BETTER
I0326 12:59:49.190389 1663736 finetune.py:68] layer 2_o @ epoch 3 new loss 2.058204245258821e-06 old loss 2.076761575153796e-06 BETTER
I0326 13:00:18.358779 1664448 finetune.py:68] layer 3_o @ epoch 3 new loss 3.351232180648367e-06 old loss 3.3874589462357108e-06 BETTER
I0326 13:00:24.967291 1663736 finetune.py:68] layer 2_o @ epoch 4 new loss 2.042989990513888e-06 old loss 2.058204245258821e-06 BETTER
I0326 13:00:47.034101 1663736 finetune.py:45] layer 2_up initial loss 3.2474422368977685e-06
I0326 13:00:52.264539 1664448 finetune.py:68] layer 3_o @ epoch 4 new loss 3.3198293749592267e-06 old loss 3.351232180648367e-06 BETTER
I0326 13:01:13.827169 1664448 finetune.py:45] layer 3_up initial loss 6.107360604801215e-06
I0326 13:01:18.902756 1663736 finetune.py:68] layer 2_up @ epoch 0 new loss 3.2285206543747336e-06 old loss 3.2474422368977685e-06 BETTER
I0326 13:01:44.514523 1664448 finetune.py:68] layer 3_up @ epoch 0 new loss 6.052610387996538e-06 old loss 6.107360604801215e-06 BETTER
I0326 13:01:52.212351 1663736 finetune.py:68] layer 2_up @ epoch 1 new loss 3.213487616449129e-06 old loss 3.2285206543747336e-06 BETTER
I0326 13:02:16.691465 1664448 finetune.py:68] layer 3_up @ epoch 1 new loss 6.013288384565385e-06 old loss 6.052610387996538e-06 BETTER
I0326 13:02:25.708219 1663736 finetune.py:68] layer 2_up @ epoch 2 new loss 3.2003651995182736e-06 old loss 3.213487616449129e-06 BETTER
I0326 13:02:48.841163 1664448 finetune.py:68] layer 3_up @ epoch 2 new loss 5.979663001198787e-06 old loss 6.013288384565385e-06 BETTER
I0326 13:02:59.202973 1663736 finetune.py:68] layer 2_up @ epoch 3 new loss 3.188590198988095e-06 old loss 3.2003651995182736e-06 BETTER
I0326 13:03:20.913409 1664448 finetune.py:68] layer 3_up @ epoch 3 new loss 5.948902980890125e-06 old loss 5.979663001198787e-06 BETTER
I0326 13:03:32.806915 1663736 finetune.py:68] layer 2_up @ epoch 4 new loss 3.177672169840662e-06 old loss 3.188590198988095e-06 BETTER
I0326 13:03:53.106828 1664448 finetune.py:68] layer 3_up @ epoch 4 new loss 5.920625426369952e-06 old loss 5.948902980890125e-06 BETTER
I0326 13:03:55.103495 1663736 finetune.py:45] layer 2_gate initial loss 3.9244450817932375e-06
I0326 13:04:14.953330 1664448 finetune.py:45] layer 3_gate initial loss 7.2230691330332775e-06
I0326 13:04:25.379686 1663736 finetune.py:68] layer 2_gate @ epoch 0 new loss 3.912480678991415e-06 old loss 3.9244450817932375e-06 BETTER
I0326 13:04:43.566647 1664448 finetune.py:68] layer 3_gate @ epoch 0 new loss 7.191035820142133e-06 old loss 7.2230691330332775e-06 BETTER
I0326 13:04:56.649171 1663736 finetune.py:68] layer 2_gate @ epoch 1 new loss 3.902242497133557e-06 old loss 3.912480678991415e-06 BETTER
I0326 13:05:13.280048 1664448 finetune.py:68] layer 3_gate @ epoch 1 new loss 7.165081569837639e-06 old loss 7.191035820142133e-06 BETTER
I0326 13:05:27.963248 1663736 finetune.py:68] layer 2_gate @ epoch 2 new loss 3.893110715580406e-06 old loss 3.902242497133557e-06 BETTER
I0326 13:05:43.100013 1664448 finetune.py:68] layer 3_gate @ epoch 2 new loss 7.141369678720366e-06 old loss 7.165081569837639e-06 BETTER
I0326 13:05:59.397985 1663736 finetune.py:68] layer 2_gate @ epoch 3 new loss 3.88468970413669e-06 old loss 3.893110715580406e-06 BETTER
I0326 13:06:13.015024 1664448 finetune.py:68] layer 3_gate @ epoch 3 new loss 7.1195513555721845e-06 old loss 7.141369678720366e-06 BETTER
I0326 13:06:30.925052 1663736 finetune.py:68] layer 2_gate @ epoch 4 new loss 3.876783921441529e-06 old loss 3.88468970413669e-06 BETTER
I0326 13:06:43.016250 1664448 finetune.py:68] layer 3_gate @ epoch 4 new loss 7.098774858604884e-06 old loss 7.1195513555721845e-06 BETTER
I0326 13:06:54.282532 1663736 finetune.py:45] layer 2_down initial loss 5.786586370959412e-06
I0326 13:07:06.160073 1664448 finetune.py:45] layer 3_down initial loss 1.0961309271806385e-05
I0326 13:07:22.395297 1663736 finetune.py:68] layer 2_down @ epoch 0 new loss 5.785746452602325e-06 old loss 5.786586370959412e-06 BETTER
I0326 13:07:33.143198 1664448 finetune.py:68] layer 3_down @ epoch 0 new loss 1.0960544386762194e-05 old loss 1.0961309271806385e-05 BETTER
I0326 13:07:51.436563 1663736 finetune.py:68] layer 2_down @ epoch 1 new loss 5.7852917052514385e-06 old loss 5.785746452602325e-06 BETTER
I0326 13:08:00.817824 1664448 finetune.py:68] layer 3_down @ epoch 1 new loss 1.0959891369566321e-05 old loss 1.0960544386762194e-05 BETTER
I0326 13:08:20.886188 1663736 finetune.py:68] layer 2_down @ epoch 2 new loss 5.784944733022712e-06 old loss 5.7852917052514385e-06 BETTER
I0326 13:08:28.553539 1664448 finetune.py:68] layer 3_down @ epoch 2 new loss 1.0959356586681679e-05 old loss 1.0959891369566321e-05 BETTER
I0326 13:08:50.431560 1663736 finetune.py:68] layer 2_down @ epoch 3 new loss 5.784684162790654e-06 old loss 5.784944733022712e-06 BETTER
I0326 13:08:56.484688 1664448 finetune.py:68] layer 3_down @ epoch 3 new loss 1.0958985512843356e-05 old loss 1.0959356586681679e-05 BETTER
I0326 13:09:20.059166 1663736 finetune.py:68] layer 2_down @ epoch 4 new loss 5.784451786894351e-06 old loss 5.784684162790654e-06 BETTER
2_v proxy err 0.016818512231111526 tr(WHW.T) 155.95950317382812
bpp_loss 3.3706352710723877
2_q proxy err 0.0003383910225238651 tr(WHW.T) 41460.203125
bpp_loss 4.4201884269714355
2_k proxy err 0.0002000767271965742 tr(WHW.T) 22619.369140625
bpp_loss 5.34093976020813
2_o proxy err 0.006057393737137318 tr(WHW.T) 1967.6676025390625
bpp_loss 3.484947681427002
2_up proxy err 0.007577922660857439 tr(WHW.T) 7602.30712890625
bpp_loss 3.7459936141967773
2_gate proxy err 0.0038726890925318003 tr(WHW.T) 15119.6220703125
bpp_loss 3.898982048034668
2_down proxy err 0.006699463352560997 tr(WHW.T) 7739.8232421875
bpp_loss 3.7510322502681186
I0326 13:09:24.583225 1664448 finetune.py:68] layer 3_down @ epoch 4 new loss 1.0958617167489138e-05 old loss 1.0958985512843356e-05 BETTER
3_v proxy err 0.01105935126543045 tr(WHW.T) 289.3331604003906
bpp_loss 3.4679596424102783
3_q proxy err 0.0003638871421571821 tr(WHW.T) 47574.9140625
bpp_loss 4.461025953292847
3_k proxy err 0.00021263404050841928 tr(WHW.T) 26183.37890625
bpp_loss 5.424307823181152
3_o proxy err 0.00765560707077384 tr(WHW.T) 1860.3123779296875
bpp_loss 3.5817102193832397
3_up proxy err 0.0075625148601830006 tr(WHW.T) 7537.10302734375
bpp_loss 3.727771895272391
3_gate proxy err 0.002803236711770296 tr(WHW.T) 20879.384765625
bpp_loss 3.9771812983921597
3_down proxy err 0.007685179356485605 tr(WHW.T) 7016.3173828125
bpp_loss 3.726980754307338
I0326 13:10:36.524126 1649289 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 65.92144536972046s
I0326 13:10:40.210237 1676865 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 13:10:40.210336 1676865 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 13:10:40.210377 1676865 utils.py:162] NumExpr defaulting to 16 threads.
I0326 13:10:40.539609 1676865 config.py:54] PyTorch version 2.6.0 available.
W0326 13:10:40.729820 1676865 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 13:10:41.313154 1676865 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 13:10:41.316850 1649289 quantize_finetune_llama.py:209] layer 5 gpu 1
I0326 13:10:41.330240 1676865 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 13:10:49.200792 1676865 finetune.py:45] layer 4_v initial loss 6.151760317152366e-06
W0326 13:10:49.201014 1676865 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 13:11:24.942986 1676865 finetune.py:68] layer 4_v @ epoch 0 new loss 2.5092429041251307e-06 old loss 6.151760317152366e-06 BETTER
I0326 13:11:44.437466 1649289 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 62.667630195617676s
I0326 13:11:48.229979 1677576 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0326 13:11:48.230071 1677576 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0326 13:11:48.230111 1677576 utils.py:162] NumExpr defaulting to 16 threads.
I0326 13:11:48.579756 1677576 config.py:54] PyTorch version 2.6.0 available.
W0326 13:11:48.785272 1677576 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 13:11:49.365973 1677576 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 13:11:49.369701 1649289 quantize_finetune_llama.py:209] layer 6 gpu 0
I0326 13:11:49.383026 1677576 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 13:11:56.398214 1677576 finetune.py:45] layer 5_v initial loss 6.5102863118227106e-06
W0326 13:11:56.398421 1677576 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 13:12:01.902322 1676865 finetune.py:68] layer 4_v @ epoch 1 new loss 2.0604595647455426e-06 old loss 2.5092429041251307e-06 BETTER
I0326 13:12:29.878245 1677576 finetune.py:68] layer 5_v @ epoch 0 new loss 3.2721125080570346e-06 old loss 6.5102863118227106e-06 BETTER
I0326 13:12:38.887511 1676865 finetune.py:68] layer 4_v @ epoch 2 new loss 1.8806875914378907e-06 old loss 2.0604595647455426e-06 BETTER
I0326 13:13:04.511756 1677576 finetune.py:68] layer 5_v @ epoch 1 new loss 2.8820850275224075e-06 old loss 3.2721125080570346e-06 BETTER
I0326 13:13:16.080677 1676865 finetune.py:68] layer 4_v @ epoch 3 new loss 1.7783124803827377e-06 old loss 1.8806875914378907e-06 BETTER
I0326 13:13:39.438332 1677576 finetune.py:68] layer 5_v @ epoch 2 new loss 2.7083506211056374e-06 old loss 2.8820850275224075e-06 BETTER
I0326 13:13:53.290240 1676865 finetune.py:68] layer 4_v @ epoch 4 new loss 1.710536935206619e-06 old loss 1.7783124803827377e-06 BETTER
I0326 13:14:03.346823 1676865 finetune.py:45] layer 4_q initial loss 2.128630967490608e-06
I0326 13:14:14.629283 1677576 finetune.py:68] layer 5_v @ epoch 3 new loss 2.6032882942672586e-06 old loss 2.7083506211056374e-06 BETTER
I0326 13:14:39.317186 1676865 finetune.py:68] layer 4_q @ epoch 0 new loss 1.9790463738900144e-06 old loss 2.128630967490608e-06 BETTER
I0326 13:14:49.891196 1677576 finetune.py:68] layer 5_v @ epoch 4 new loss 2.5306298994109966e-06 old loss 2.6032882942672586e-06 BETTER
