I0325 17:40:46.437921 975333 config.py:54] PyTorch version 2.6.0 available.
W0325 17:40:46.721624 975333 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 17:40:47.629146 975333 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.76it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.52it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.71it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.00it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.15it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.22it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.38it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.13it/s]
I0325 17:40:49.082146 975333 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:23,  1.33it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:18,  1.60it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.74it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.85it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.87it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.90it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.92it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.94it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:11,  1.94it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.93it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:10,  1.94it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.95it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:09,  1.93it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.92it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:07<00:08,  1.92it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.94it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:08<00:07,  1.94it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.96it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:09<00:06,  1.96it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.95it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.96it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.96it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.97it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.98it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.97it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.97it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.97it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  2.00it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:14<00:01,  2.07it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:00,  2.12it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:15<00:00,  2.16it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  2.20it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.96it/s]
I0325 17:41:11.034651 975333 quantize_finetune_llama.py:185] loaded compression model
I0325 17:41:29.514837 975333 quantize_finetune_llama.py:189] loaded dataset and devset
I0325 17:41:32.495230 975333 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 17:42:31.185911 975333 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 58.54845666885376s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0325 17:42:52.378069 977220 config.py:54] PyTorch version 2.6.0 available.
W0325 17:42:52.659446 977220 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 17:42:53.544766 977220 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 17:42:53.548788 975333 quantize_finetune_llama.py:209] layer 1 gpu 1
I0325 17:42:53.562290 977220 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 17:43:00.587758 977220 finetune.py:45] layer 0_v initial loss 5.621147465717513e-06
W0325 17:43:00.588006 977220 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 17:43:34.909290 977220 finetune.py:68] layer 0_v @ epoch 0 new loss 3.6180272218189202e-06 old loss 5.621147465717513e-06 BETTER
I0325 17:43:55.726962 975333 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 62.01337790489197s
I0325 17:44:08.136610 978317 config.py:54] PyTorch version 2.6.0 available.
W0325 17:44:08.455423 978317 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 17:44:09.444340 978317 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 17:44:09.448494 975333 quantize_finetune_llama.py:209] layer 2 gpu 0
I0325 17:44:09.462012 978317 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0325 17:44:10.967206 977220 finetune.py:68] layer 0_v @ epoch 1 new loss 3.2421210107713705e-06 old loss 3.6180272218189202e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 17:44:16.454463 978317 finetune.py:45] layer 1_v initial loss 1.3288282389112283e-05
W0325 17:44:16.454804 978317 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 17:44:47.860804 977220 finetune.py:68] layer 0_v @ epoch 2 new loss 3.081809836658067e-06 old loss 3.2421210107713705e-06 BETTER
I0325 17:44:49.587191 978317 finetune.py:68] layer 1_v @ epoch 0 new loss 5.716759005736094e-06 old loss 1.3288282389112283e-05 BETTER
I0325 17:45:23.982397 978317 finetune.py:68] layer 1_v @ epoch 1 new loss 4.723411620943807e-06 old loss 5.716759005736094e-06 BETTER
I0325 17:45:25.117643 977220 finetune.py:68] layer 0_v @ epoch 3 new loss 2.9778070711472537e-06 old loss 3.081809836658067e-06 BETTER
I0325 17:45:58.744127 978317 finetune.py:68] layer 1_v @ epoch 2 new loss 4.390801677800482e-06 old loss 4.723411620943807e-06 BETTER
I0325 17:46:02.135451 977220 finetune.py:68] layer 0_v @ epoch 4 new loss 2.8988731628487585e-06 old loss 2.9778070711472537e-06 BETTER
I0325 17:46:11.867406 977220 finetune.py:45] layer 0_q initial loss 2.9023403840255924e-06
I0325 17:46:33.902255 978317 finetune.py:68] layer 1_v @ epoch 3 new loss 4.183095370535739e-06 old loss 4.390801677800482e-06 BETTER
I0325 17:46:47.811948 977220 finetune.py:68] layer 0_q @ epoch 0 new loss 2.8289648525969824e-06 old loss 2.9023403840255924e-06 BETTER
I0325 17:47:08.932065 978317 finetune.py:68] layer 1_v @ epoch 4 new loss 4.024080226372462e-06 old loss 4.183095370535739e-06 BETTER
I0325 17:47:18.401998 978317 finetune.py:45] layer 1_q initial loss 4.143425940128509e-06
I0325 17:47:24.623982 977220 finetune.py:68] layer 0_q @ epoch 1 new loss 2.7690634851751383e-06 old loss 2.8289648525969824e-06 BETTER
I0325 17:47:52.253746 978317 finetune.py:68] layer 1_q @ epoch 0 new loss 3.96862515117391e-06 old loss 4.143425940128509e-06 BETTER
I0325 17:48:01.422013 977220 finetune.py:68] layer 0_q @ epoch 2 new loss 2.7176715775567573e-06 old loss 2.7690634851751383e-06 BETTER
I0325 17:48:26.709267 978317 finetune.py:68] layer 1_q @ epoch 1 new loss 3.8455023059214e-06 old loss 3.96862515117391e-06 BETTER
I0325 17:48:38.308787 977220 finetune.py:68] layer 0_q @ epoch 3 new loss 2.6724808321887394e-06 old loss 2.7176715775567573e-06 BETTER
I0325 17:49:01.452865 978317 finetune.py:68] layer 1_q @ epoch 2 new loss 3.7419765703816665e-06 old loss 3.8455023059214e-06 BETTER
I0325 17:49:15.280366 977220 finetune.py:68] layer 0_q @ epoch 4 new loss 2.6314082788303494e-06 old loss 2.6724808321887394e-06 BETTER
I0325 17:49:23.267405 977220 finetune.py:45] layer 0_k initial loss 2.6384525426692562e-06
I0325 17:49:36.305108 978317 finetune.py:68] layer 1_q @ epoch 3 new loss 3.653118255897425e-06 old loss 3.7419765703816665e-06 BETTER
I0325 17:49:59.625802 977220 finetune.py:68] layer 0_k @ epoch 0 new loss 2.600747620817856e-06 old loss 2.6384525426692562e-06 BETTER
I0325 17:50:11.038433 978317 finetune.py:68] layer 1_q @ epoch 4 new loss 3.575589516913169e-06 old loss 3.653118255897425e-06 BETTER
I0325 17:50:18.745127 978317 finetune.py:45] layer 1_k initial loss 3.7182733194640605e-06
I0325 17:50:36.675273 977220 finetune.py:68] layer 0_k @ epoch 1 new loss 2.5671249659353634e-06 old loss 2.600747620817856e-06 BETTER
I0325 17:50:52.664075 978317 finetune.py:68] layer 1_k @ epoch 0 new loss 3.606662858146592e-06 old loss 3.7182733194640605e-06 BETTER
I0325 17:51:14.116445 977220 finetune.py:68] layer 0_k @ epoch 2 new loss 2.5365329747728538e-06 old loss 2.5671249659353634e-06 BETTER
I0325 17:51:27.675904 978317 finetune.py:68] layer 1_k @ epoch 1 new loss 3.5366317661100766e-06 old loss 3.606662858146592e-06 BETTER
I0325 17:51:51.190333 977220 finetune.py:68] layer 0_k @ epoch 3 new loss 2.5082354113692418e-06 old loss 2.5365329747728538e-06 BETTER
I0325 17:52:02.413808 978317 finetune.py:68] layer 1_k @ epoch 2 new loss 3.476158099147142e-06 old loss 3.5366317661100766e-06 BETTER
I0325 17:52:28.130643 977220 finetune.py:68] layer 0_k @ epoch 4 new loss 2.4819485133775743e-06 old loss 2.5082354113692418e-06 BETTER
I0325 17:52:37.257056 978317 finetune.py:68] layer 1_k @ epoch 3 new loss 3.4216923268104438e-06 old loss 3.476158099147142e-06 BETTER
I0325 17:52:38.123242 977220 finetune.py:45] layer 0_o initial loss 4.547932348941686e-06
I0325 17:53:12.056965 978317 finetune.py:68] layer 1_k @ epoch 4 new loss 3.3712190088408533e-06 old loss 3.4216923268104438e-06 BETTER
I0325 17:53:13.418133 977220 finetune.py:68] layer 0_o @ epoch 0 new loss 4.457602699403651e-06 old loss 4.547932348941686e-06 BETTER
I0325 17:53:21.718665 978317 finetune.py:45] layer 1_o initial loss 8.891690413292963e-06
I0325 17:53:49.462138 977220 finetune.py:68] layer 0_o @ epoch 1 new loss 4.386071395856561e-06 old loss 4.457602699403651e-06 BETTER
I0325 17:53:55.363705 978317 finetune.py:68] layer 1_o @ epoch 0 new loss 8.578963388572447e-06 old loss 8.891690413292963e-06 BETTER
I0325 17:54:25.684276 977220 finetune.py:68] layer 0_o @ epoch 2 new loss 4.325282134232111e-06 old loss 4.386071395856561e-06 BETTER
I0325 17:54:29.396621 978317 finetune.py:68] layer 1_o @ epoch 1 new loss 8.341195098182652e-06 old loss 8.578963388572447e-06 BETTER
I0325 17:55:02.038028 977220 finetune.py:68] layer 0_o @ epoch 3 new loss 4.272320893505821e-06 old loss 4.325282134232111e-06 BETTER
I0325 17:55:03.748113 978317 finetune.py:68] layer 1_o @ epoch 2 new loss 8.154562237905338e-06 old loss 8.341195098182652e-06 BETTER
I0325 17:55:38.099887 978317 finetune.py:68] layer 1_o @ epoch 3 new loss 8.004324627108872e-06 old loss 8.154562237905338e-06 BETTER
I0325 17:55:38.578789 977220 finetune.py:68] layer 0_o @ epoch 4 new loss 4.2265101001248695e-06 old loss 4.272320893505821e-06 BETTER
I0325 17:56:00.479319 977220 finetune.py:45] layer 0_up initial loss 5.641726147587178e-06
I0325 17:56:12.106115 978317 finetune.py:68] layer 1_o @ epoch 4 new loss 7.881014425947797e-06 old loss 8.004324627108872e-06 BETTER
I0325 17:56:32.923008 977220 finetune.py:68] layer 0_up @ epoch 0 new loss 5.5763061936886515e-06 old loss 5.641726147587178e-06 BETTER
I0325 17:56:33.704083 978317 finetune.py:45] layer 1_up initial loss 1.0963314707623795e-05
I0325 17:57:04.503944 978317 finetune.py:68] layer 1_up @ epoch 0 new loss 1.084258019545814e-05 old loss 1.0963314707623795e-05 BETTER
I0325 17:57:06.571674 977220 finetune.py:68] layer 0_up @ epoch 1 new loss 5.524540938495193e-06 old loss 5.5763061936886515e-06 BETTER
I0325 17:57:36.548656 978317 finetune.py:68] layer 1_up @ epoch 1 new loss 1.0751961781352293e-05 old loss 1.084258019545814e-05 BETTER
I0325 17:57:40.303647 977220 finetune.py:68] layer 0_up @ epoch 2 new loss 5.481366770254681e-06 old loss 5.524540938495193e-06 BETTER
I0325 17:58:08.765787 978317 finetune.py:68] layer 1_up @ epoch 2 new loss 1.0676482816052157e-05 old loss 1.0751961781352293e-05 BETTER
I0325 17:58:14.161705 977220 finetune.py:68] layer 0_up @ epoch 3 new loss 5.445106125989696e-06 old loss 5.481366770254681e-06 BETTER
I0325 17:58:40.941268 978317 finetune.py:68] layer 1_up @ epoch 3 new loss 1.0610958270262927e-05 old loss 1.0676482816052157e-05 BETTER
I0325 17:58:48.211490 977220 finetune.py:68] layer 0_up @ epoch 4 new loss 5.413542567112017e-06 old loss 5.445106125989696e-06 BETTER
I0325 17:59:10.079108 977220 finetune.py:45] layer 0_gate initial loss 6.494747140095569e-06
I0325 17:59:13.035886 978317 finetune.py:68] layer 1_up @ epoch 4 new loss 1.0553542779234704e-05 old loss 1.0610958270262927e-05 BETTER
I0325 17:59:34.680333 978317 finetune.py:45] layer 1_gate initial loss 1.3190422578190919e-05
I0325 17:59:40.696882 977220 finetune.py:68] layer 0_gate @ epoch 0 new loss 6.4421237766509876e-06 old loss 6.494747140095569e-06 BETTER
I0325 18:00:03.390738 978317 finetune.py:68] layer 1_gate @ epoch 0 new loss 1.2878291272500064e-05 old loss 1.3190422578190919e-05 BETTER
I0325 18:00:12.316655 977220 finetune.py:68] layer 0_gate @ epoch 1 new loss 6.396313438017387e-06 old loss 6.4421237766509876e-06 BETTER
I0325 18:00:33.089921 978317 finetune.py:68] layer 1_gate @ epoch 1 new loss 1.2692625205090735e-05 old loss 1.2878291272500064e-05 BETTER
I0325 18:00:43.940785 977220 finetune.py:68] layer 0_gate @ epoch 2 new loss 6.356307494570501e-06 old loss 6.396313438017387e-06 BETTER
I0325 18:01:02.864208 978317 finetune.py:68] layer 1_gate @ epoch 2 new loss 1.2623651855392382e-05 old loss 1.2692625205090735e-05 BETTER
I0325 18:01:15.687141 977220 finetune.py:68] layer 0_gate @ epoch 3 new loss 6.320898137346376e-06 old loss 6.356307494570501e-06 BETTER
I0325 18:01:32.742623 978317 finetune.py:68] layer 1_gate @ epoch 3 new loss 1.2561284165713005e-05 old loss 1.2623651855392382e-05 BETTER
I0325 18:01:47.510541 977220 finetune.py:68] layer 0_gate @ epoch 4 new loss 6.289783414104022e-06 old loss 6.320898137346376e-06 BETTER
I0325 18:02:02.640403 978317 finetune.py:68] layer 1_gate @ epoch 4 new loss 1.252314086741535e-05 old loss 1.2561284165713005e-05 BETTER
I0325 18:02:10.439090 977220 finetune.py:45] layer 0_down initial loss 9.148151548288297e-06
I0325 18:02:25.600941 978317 finetune.py:45] layer 1_down initial loss 3.027824823220726e-05
I0325 18:02:38.538983 977220 finetune.py:68] layer 0_down @ epoch 0 new loss 9.12380164663773e-06 old loss 9.148151548288297e-06 BETTER
I0325 18:02:52.335913 978317 finetune.py:68] layer 1_down @ epoch 0 new loss 3.022084456461016e-05 old loss 3.027824823220726e-05 BETTER
I0325 18:03:07.739478 977220 finetune.py:68] layer 0_down @ epoch 1 new loss 9.103599040827248e-06 old loss 9.12380164663773e-06 BETTER
I0325 18:03:20.029849 978317 finetune.py:68] layer 1_down @ epoch 1 new loss 3.0206738301785663e-05 old loss 3.022084456461016e-05 BETTER
I0325 18:03:37.187870 977220 finetune.py:68] layer 0_down @ epoch 2 new loss 9.086581485462375e-06 old loss 9.103599040827248e-06 BETTER
I0325 18:03:47.761894 978317 finetune.py:68] layer 1_down @ epoch 2 new loss 3.0193632483133115e-05 old loss 3.0206738301785663e-05 BETTER
I0325 18:04:06.500553 977220 finetune.py:68] layer 0_down @ epoch 3 new loss 9.072155080502853e-06 old loss 9.086581485462375e-06 BETTER
I0325 18:04:15.565057 978317 finetune.py:68] layer 1_down @ epoch 3 new loss 3.018419010913931e-05 old loss 3.0193632483133115e-05 BETTER
I0325 18:04:36.057597 977220 finetune.py:68] layer 0_down @ epoch 4 new loss 9.05929300643038e-06 old loss 9.072155080502853e-06 BETTER
0_v proxy err 0.15039101243019104 tr(WHW.T) 60.88684844970703
bpp_loss 2.0790116786956787
0_q proxy err 0.000249220262048766 tr(WHW.T) 288016.625
bpp_loss 2.863135814666748
0_k proxy err 0.00021839558030478656 tr(WHW.T) 100059.3515625
bpp_loss 3.3719804286956787
0_o proxy err 0.017987040802836418 tr(WHW.T) 3166.459228515625
bpp_loss 2.175145983695984
0_up proxy err 0.036218881607055664 tr(WHW.T) 8921.349609375
bpp_loss 2.4610022136143277
0_gate proxy err 0.021075889468193054 tr(WHW.T) 15775.9619140625
bpp_loss 2.567375728062221
0_down proxy err 0.025318559259176254 tr(WHW.T) 10913.2998046875
bpp_loss 2.454772744859968
I0325 18:04:43.741226 978317 finetune.py:68] layer 1_down @ epoch 4 new loss 3.0173985578585416e-05 old loss 3.018419010913931e-05 BETTER
1_v proxy err 0.09375762939453125 tr(WHW.T) 109.07096099853516
bpp_loss 2.1860607862472534
1_q proxy err 0.0003748484596144408 tr(WHW.T) 144831.546875
bpp_loss 3.1074461936950684
1_k proxy err 0.00020892791508231312 tr(WHW.T) 75537.0390625
bpp_loss 3.7294903993606567
1_o proxy err 0.036320991814136505 tr(WHW.T) 1991.9486083984375
bpp_loss 2.252762794494629
1_up proxy err 0.039728619158267975 tr(WHW.T) 8236.1650390625
bpp_loss 2.475348881312779
1_gate proxy err 0.024170242249965668 tr(WHW.T) 13943.5078125
bpp_loss 2.5783568790980746
1_down proxy err 0.002167976461350918 tr(WHW.T) 14017.146484375
bpp_loss 2.4699298994881764
I0325 18:05:52.431074 975333 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 63.64042139053345s
I0325 18:05:55.670930 995524 config.py:54] PyTorch version 2.6.0 available.
W0325 18:05:55.952842 995524 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 18:05:56.830777 995524 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 18:05:56.834721 975333 quantize_finetune_llama.py:209] layer 3 gpu 1
I0325 18:05:56.848204 995524 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 18:06:04.012591 995524 finetune.py:45] layer 2_v initial loss 2.5442866899538785e-05
W0325 18:06:04.013178 995524 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 18:06:39.837775 995524 finetune.py:68] layer 2_v @ epoch 0 new loss 7.998723958735354e-06 old loss 2.5442866899538785e-05 BETTER
I0325 18:06:58.405625 975333 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 61.13070893287659s
I0325 18:07:01.807242 996452 config.py:54] PyTorch version 2.6.0 available.
W0325 18:07:02.097547 996452 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 18:07:03.010456 996452 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 18:07:03.014195 975333 quantize_finetune_llama.py:209] layer 4 gpu 0
I0325 18:07:03.027888 996452 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 18:07:10.077780 996452 finetune.py:45] layer 3_v initial loss 3.0852086638333276e-05
W0325 18:07:10.078063 996452 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 18:07:16.714782 995524 finetune.py:68] layer 2_v @ epoch 1 new loss 6.152033620310249e-06 old loss 7.998723958735354e-06 BETTER
I0325 18:07:43.624862 996452 finetune.py:68] layer 3_v @ epoch 0 new loss 1.0906520401476882e-05 old loss 3.0852086638333276e-05 BETTER
I0325 18:07:53.947253 995524 finetune.py:68] layer 2_v @ epoch 2 new loss 5.708580829377752e-06 old loss 6.152033620310249e-06 BETTER
I0325 18:08:18.270325 996452 finetune.py:68] layer 3_v @ epoch 1 new loss 8.604320100857876e-06 old loss 1.0906520401476882e-05 BETTER
I0325 18:08:31.008098 995524 finetune.py:68] layer 2_v @ epoch 3 new loss 5.4885776989976875e-06 old loss 5.708580829377752e-06 BETTER
I0325 18:08:53.296569 996452 finetune.py:68] layer 3_v @ epoch 2 new loss 7.920863936305977e-06 old loss 8.604320100857876e-06 BETTER
I0325 18:09:08.230087 995524 finetune.py:68] layer 2_v @ epoch 4 new loss 5.3333274081524e-06 old loss 5.4885776989976875e-06 BETTER
I0325 18:09:18.171000 995524 finetune.py:45] layer 2_q initial loss 6.268493962124921e-06
I0325 18:09:28.451210 996452 finetune.py:68] layer 3_v @ epoch 3 new loss 7.5560624281934e-06 old loss 7.920863936305977e-06 BETTER
I0325 18:09:54.446863 995524 finetune.py:68] layer 2_q @ epoch 0 new loss 5.9738545132859144e-06 old loss 6.268493962124921e-06 BETTER
I0325 18:10:03.719976 996452 finetune.py:68] layer 3_v @ epoch 4 new loss 7.303518032131251e-06 old loss 7.5560624281934e-06 BETTER
I0325 18:10:13.236326 996452 finetune.py:45] layer 3_q initial loss 8.889685886970256e-06
I0325 18:10:31.447062 995524 finetune.py:68] layer 2_q @ epoch 1 new loss 5.821663762617391e-06 old loss 5.9738545132859144e-06 BETTER
I0325 18:10:47.212216 996452 finetune.py:68] layer 3_q @ epoch 0 new loss 8.457621333946008e-06 old loss 8.889685886970256e-06 BETTER
I0325 18:11:08.363142 995524 finetune.py:68] layer 2_q @ epoch 2 new loss 5.703790066036163e-06 old loss 5.821663762617391e-06 BETTER
I0325 18:11:21.816349 996452 finetune.py:68] layer 3_q @ epoch 1 new loss 8.233458174800035e-06 old loss 8.457621333946008e-06 BETTER
I0325 18:11:45.338172 995524 finetune.py:68] layer 2_q @ epoch 3 new loss 5.6052326726785395e-06 old loss 5.703790066036163e-06 BETTER
I0325 18:11:56.417150 996452 finetune.py:68] layer 3_q @ epoch 2 new loss 8.063213499553967e-06 old loss 8.233458174800035e-06 BETTER
I0325 18:12:22.361510 995524 finetune.py:68] layer 2_q @ epoch 4 new loss 5.520834747585468e-06 old loss 5.6052326726785395e-06 BETTER
I0325 18:12:30.341709 995524 finetune.py:45] layer 2_k initial loss 5.864153990842169e-06
I0325 18:12:31.153262 996452 finetune.py:68] layer 3_q @ epoch 3 new loss 7.921144060674123e-06 old loss 8.063213499553967e-06 BETTER
I0325 18:13:06.061050 996452 finetune.py:68] layer 3_q @ epoch 4 new loss 7.801775609550532e-06 old loss 7.921144060674123e-06 BETTER
I0325 18:13:06.613582 995524 finetune.py:68] layer 2_k @ epoch 0 new loss 5.748660896642832e-06 old loss 5.864153990842169e-06 BETTER
I0325 18:13:13.926529 996452 finetune.py:45] layer 3_k initial loss 8.478473318973556e-06
I0325 18:13:43.532577 995524 finetune.py:68] layer 2_k @ epoch 1 new loss 5.679537935066037e-06 old loss 5.748660896642832e-06 BETTER
I0325 18:13:47.895252 996452 finetune.py:68] layer 3_k @ epoch 0 new loss 8.316414096043445e-06 old loss 8.478473318973556e-06 BETTER
I0325 18:14:20.618838 995524 finetune.py:68] layer 2_k @ epoch 2 new loss 5.618996056000469e-06 old loss 5.679537935066037e-06 BETTER
I0325 18:14:22.494537 996452 finetune.py:68] layer 3_k @ epoch 1 new loss 8.216786227421835e-06 old loss 8.316414096043445e-06 BETTER
I0325 18:14:57.125533 996452 finetune.py:68] layer 3_k @ epoch 2 new loss 8.12836424302077e-06 old loss 8.216786227421835e-06 BETTER
I0325 18:14:57.605164 995524 finetune.py:68] layer 2_k @ epoch 3 new loss 5.5645427892159205e-06 old loss 5.618996056000469e-06 BETTER
I0325 18:15:31.642033 996452 finetune.py:68] layer 3_k @ epoch 3 new loss 8.051526492636185e-06 old loss 8.12836424302077e-06 BETTER
I0325 18:15:34.553948 995524 finetune.py:68] layer 2_k @ epoch 4 new loss 5.513303676707437e-06 old loss 5.5645427892159205e-06 BETTER
I0325 18:15:44.402679 995524 finetune.py:45] layer 2_o initial loss 1.3550233234127518e-05
I0325 18:16:06.208745 996452 finetune.py:68] layer 3_k @ epoch 4 new loss 7.98044402472442e-06 old loss 8.051526492636185e-06 BETTER
I0325 18:16:15.704294 996452 finetune.py:45] layer 3_o initial loss 2.222879084001761e-05
I0325 18:16:19.598442 995524 finetune.py:68] layer 2_o @ epoch 0 new loss 1.2832613720092922e-05 old loss 1.3550233234127518e-05 BETTER
I0325 18:16:49.061392 996452 finetune.py:68] layer 3_o @ epoch 0 new loss 2.063868123514112e-05 old loss 2.222879084001761e-05 BETTER
I0325 18:16:55.506609 995524 finetune.py:68] layer 2_o @ epoch 1 new loss 1.2367726412776392e-05 old loss 1.2832613720092922e-05 BETTER
I0325 18:17:23.159188 996452 finetune.py:68] layer 3_o @ epoch 1 new loss 1.9938925106544048e-05 old loss 2.063868123514112e-05 BETTER
I0325 18:17:31.580332 995524 finetune.py:68] layer 2_o @ epoch 2 new loss 1.204303862323286e-05 old loss 1.2367726412776392e-05 BETTER
I0325 18:17:57.071014 996452 finetune.py:68] layer 3_o @ epoch 2 new loss 1.9519569832482375e-05 old loss 1.9938925106544048e-05 BETTER
I0325 18:18:07.678004 995524 finetune.py:68] layer 2_o @ epoch 3 new loss 1.180671915790299e-05 old loss 1.204303862323286e-05 BETTER
I0325 18:18:31.189299 996452 finetune.py:68] layer 3_o @ epoch 3 new loss 1.9210816390113905e-05 old loss 1.9519569832482375e-05 BETTER
I0325 18:18:43.742250 995524 finetune.py:68] layer 2_o @ epoch 4 new loss 1.1623806130955927e-05 old loss 1.180671915790299e-05 BETTER
I0325 18:19:05.166953 996452 finetune.py:68] layer 3_o @ epoch 4 new loss 1.8960652596433647e-05 old loss 1.9210816390113905e-05 BETTER
I0325 18:19:05.759365 995524 finetune.py:45] layer 2_up initial loss 1.8787859517033212e-05
I0325 18:19:26.766248 996452 finetune.py:45] layer 3_up initial loss 3.521588587318547e-05
I0325 18:19:38.268714 995524 finetune.py:68] layer 2_up @ epoch 0 new loss 1.858535506471526e-05 old loss 1.8787859517033212e-05 BETTER
I0325 18:19:57.522815 996452 finetune.py:68] layer 3_up @ epoch 0 new loss 3.482088868622668e-05 old loss 3.521588587318547e-05 BETTER
I0325 18:20:12.046270 995524 finetune.py:68] layer 2_up @ epoch 1 new loss 1.84291711775586e-05 old loss 1.858535506471526e-05 BETTER
I0325 18:20:29.662792 996452 finetune.py:68] layer 3_up @ epoch 1 new loss 3.454359102761373e-05 old loss 3.482088868622668e-05 BETTER
I0325 18:20:45.923681 995524 finetune.py:68] layer 2_up @ epoch 2 new loss 1.8299755538464524e-05 old loss 1.84291711775586e-05 BETTER
I0325 18:21:01.796497 996452 finetune.py:68] layer 3_up @ epoch 2 new loss 3.431586446822621e-05 old loss 3.454359102761373e-05 BETTER
I0325 18:21:19.755336 995524 finetune.py:68] layer 2_up @ epoch 3 new loss 1.8190175978816114e-05 old loss 1.8299755538464524e-05 BETTER
I0325 18:21:33.944427 996452 finetune.py:68] layer 3_up @ epoch 3 new loss 3.411426951061003e-05 old loss 3.431586446822621e-05 BETTER
I0325 18:21:53.710506 995524 finetune.py:68] layer 2_up @ epoch 4 new loss 1.809380773920566e-05 old loss 1.8190175978816114e-05 BETTER
I0325 18:22:06.172034 996452 finetune.py:68] layer 3_up @ epoch 4 new loss 3.393288352526724e-05 old loss 3.411426951061003e-05 BETTER
I0325 18:22:15.892410 995524 finetune.py:45] layer 2_gate initial loss 2.2825883206678554e-05
I0325 18:22:28.074698 996452 finetune.py:45] layer 3_gate initial loss 4.2046151065733284e-05
I0325 18:22:46.453465 995524 finetune.py:68] layer 2_gate @ epoch 0 new loss 2.2721202185493894e-05 old loss 2.2825883206678554e-05 BETTER
I0325 18:22:56.914187 996452 finetune.py:68] layer 3_gate @ epoch 0 new loss 4.181476833764464e-05 old loss 4.2046151065733284e-05 BETTER
I0325 18:23:17.810365 995524 finetune.py:68] layer 2_gate @ epoch 1 new loss 2.26277388719609e-05 old loss 2.2721202185493894e-05 BETTER
I0325 18:23:26.715623 996452 finetune.py:68] layer 3_gate @ epoch 1 new loss 4.162869299761951e-05 old loss 4.181476833764464e-05 BETTER
I0325 18:23:49.346275 995524 finetune.py:68] layer 2_gate @ epoch 2 new loss 2.2543958039022982e-05 old loss 2.26277388719609e-05 BETTER
I0325 18:23:56.655515 996452 finetune.py:68] layer 3_gate @ epoch 2 new loss 4.1461978980805725e-05 old loss 4.162869299761951e-05 BETTER
I0325 18:24:20.859807 995524 finetune.py:68] layer 2_gate @ epoch 3 new loss 2.2466232621809468e-05 old loss 2.2543958039022982e-05 BETTER
I0325 18:24:26.526921 996452 finetune.py:68] layer 3_gate @ epoch 3 new loss 4.130857632844709e-05 old loss 4.1461978980805725e-05 BETTER
I0325 18:24:52.485291 995524 finetune.py:68] layer 2_gate @ epoch 4 new loss 2.2394853658624925e-05 old loss 2.2466232621809468e-05 BETTER
I0325 18:24:56.335196 996452 finetune.py:68] layer 3_gate @ epoch 4 new loss 4.1165254515362903e-05 old loss 4.130857632844709e-05 BETTER
I0325 18:25:15.562557 995524 finetune.py:45] layer 2_down initial loss 3.2782529160613194e-05
I0325 18:25:18.977511 996452 finetune.py:45] layer 3_down initial loss 6.196788308443502e-05
I0325 18:25:43.609115 995524 finetune.py:68] layer 2_down @ epoch 0 new loss 3.27533416566439e-05 old loss 3.2782529160613194e-05 BETTER
I0325 18:25:45.763708 996452 finetune.py:68] layer 3_down @ epoch 0 new loss 6.19360725977458e-05 old loss 6.196788308443502e-05 BETTER
I0325 18:26:12.785068 995524 finetune.py:68] layer 2_down @ epoch 1 new loss 3.2727413781685755e-05 old loss 3.27533416566439e-05 BETTER
I0325 18:26:13.361924 996452 finetune.py:68] layer 3_down @ epoch 1 new loss 6.1907856434118e-05 old loss 6.19360725977458e-05 BETTER
I0325 18:26:41.152780 996452 finetune.py:68] layer 3_down @ epoch 2 new loss 6.188260886119679e-05 old loss 6.1907856434118e-05 BETTER
I0325 18:26:42.143866 995524 finetune.py:68] layer 2_down @ epoch 2 new loss 3.2704509067116305e-05 old loss 3.2727413781685755e-05 BETTER
I0325 18:27:08.869569 996452 finetune.py:68] layer 3_down @ epoch 3 new loss 6.185990059748292e-05 old loss 6.188260886119679e-05 BETTER
I0325 18:27:11.626233 995524 finetune.py:68] layer 2_down @ epoch 3 new loss 3.2683845347492024e-05 old loss 3.2704509067116305e-05 BETTER
I0325 18:27:36.642088 996452 finetune.py:68] layer 3_down @ epoch 4 new loss 6.183963705552742e-05 old loss 6.185990059748292e-05 BETTER
3_v proxy err 0.06173185631632805 tr(WHW.T) 289.3331604003906
bpp_loss 2.1951277256011963
3_q proxy err 0.0018804307328537107 tr(WHW.T) 47605.546875
bpp_loss 3.101874351501465
3_k proxy err 0.0009668255224823952 tr(WHW.T) 26221.521484375
bpp_loss 3.9052869081497192
3_o proxy err 0.04207272082567215 tr(WHW.T) 1870.858642578125
bpp_loss 2.309359908103943
3_up proxy err 0.04366324096918106 tr(WHW.T) 7535.90869140625
bpp_loss 2.450122424534389
3_gate proxy err 0.01679229363799095 tr(WHW.T) 20833.98046875
bpp_loss 2.680433682032994
3_down proxy err 0.04358566924929619 tr(WHW.T) 7112.01708984375
bpp_loss 2.4505063125065396
I0325 18:27:41.140441 995524 finetune.py:68] layer 2_down @ epoch 4 new loss 3.266494968556799e-05 old loss 3.2683845347492024e-05 BETTER
2_v proxy err 0.0924566239118576 tr(WHW.T) 155.95950317382812
bpp_loss 2.0966715812683105
2_q proxy err 0.0017978769028559327 tr(WHW.T) 41475.65234375
bpp_loss 3.0607393980026245
2_k proxy err 0.0009237978374585509 tr(WHW.T) 22654.228515625
bpp_loss 3.831114888191223
2_o proxy err 0.03242920711636543 tr(WHW.T) 1988.307861328125
bpp_loss 2.2107067108154297
2_up proxy err 0.04387854412198067 tr(WHW.T) 7603.0830078125
bpp_loss 2.467463493347168
2_gate proxy err 0.02291720360517502 tr(WHW.T) 15111.109375
bpp_loss 2.609787259783064
2_down proxy err 0.038191236555576324 tr(WHW.T) 7815.7607421875
bpp_loss 2.4724317278180803
I0325 18:28:49.617294 975333 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 62.84923005104065s
I0325 18:28:52.987273 1013719 config.py:54] PyTorch version 2.6.0 available.
W0325 18:28:53.270816 1013719 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 18:28:54.155983 1013719 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 18:28:54.159919 975333 quantize_finetune_llama.py:209] layer 5 gpu 1
I0325 18:28:54.173177 1013719 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 18:29:01.363516 1013719 finetune.py:45] layer 4_v initial loss 3.228884452255443e-05
W0325 18:29:01.363715 1013719 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 18:29:37.241803 1013719 finetune.py:68] layer 4_v @ epoch 0 new loss 1.3212391422712244e-05 old loss 3.228884452255443e-05 BETTER
I0325 18:29:56.185875 975333 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 61.596468687057495s
I0325 18:29:59.775523 1014653 config.py:54] PyTorch version 2.6.0 available.
W0325 18:30:00.066059 1014653 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 18:30:01.011522 1014653 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 18:30:01.016306 975333 quantize_finetune_llama.py:209] layer 6 gpu 0
I0325 18:30:01.032611 1014653 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 18:30:08.157652 1014653 finetune.py:45] layer 5_v initial loss 3.6036759411217645e-05
W0325 18:30:08.157900 1014653 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 18:30:14.694971 1013719 finetune.py:68] layer 4_v @ epoch 1 new loss 1.1185204130015336e-05 old loss 1.3212391422712244e-05 BETTER
I0325 18:30:41.650346 1014653 finetune.py:68] layer 5_v @ epoch 0 new loss 1.8148786693927832e-05 old loss 3.6036759411217645e-05 BETTER
I0325 18:30:51.797582 1013719 finetune.py:68] layer 4_v @ epoch 2 new loss 1.0401953659311403e-05 old loss 1.1185204130015336e-05 BETTER
I0325 18:31:16.549876 1014653 finetune.py:68] layer 5_v @ epoch 1 new loss 1.616895133338403e-05 old loss 1.8148786693927832e-05 BETTER
I0325 18:31:29.058604 1013719 finetune.py:68] layer 4_v @ epoch 3 new loss 9.943209988705348e-06 old loss 1.0401953659311403e-05 BETTER
I0325 18:31:51.721230 1014653 finetune.py:68] layer 5_v @ epoch 2 new loss 1.5290028386516497e-05 old loss 1.616895133338403e-05 BETTER
I0325 18:32:07.245353 1013719 finetune.py:68] layer 4_v @ epoch 4 new loss 9.620187483960763e-06 old loss 9.943209988705348e-06 BETTER
I0325 18:32:17.489227 1013719 finetune.py:45] layer 4_q initial loss 1.2063094800396357e-05
I0325 18:32:27.457745 1014653 finetune.py:68] layer 5_v @ epoch 3 new loss 1.4734349861100782e-05 old loss 1.5290028386516497e-05 BETTER
I0325 18:32:53.913685 1013719 finetune.py:68] layer 4_q @ epoch 0 new loss 1.1293709576420952e-05 old loss 1.2063094800396357e-05 BETTER
I0325 18:33:02.780640 1014653 finetune.py:68] layer 5_v @ epoch 4 new loss 1.433646866644267e-05 old loss 1.4734349861100782e-05 BETTER
I0325 18:33:12.382987 1014653 finetune.py:45] layer 5_q initial loss 1.8269105567014776e-05
I0325 18:33:31.226032 1013719 finetune.py:68] layer 4_q @ epoch 1 new loss 1.0972486052196473e-05 old loss 1.1293709576420952e-05 BETTER
I0325 18:33:46.412473 1014653 finetune.py:68] layer 5_q @ epoch 0 new loss 1.706614602881018e-05 old loss 1.8269105567014776e-05 BETTER
I0325 18:34:08.623321 1013719 finetune.py:68] layer 4_q @ epoch 2 new loss 1.0725720130722038e-05 old loss 1.0972486052196473e-05 BETTER
I0325 18:34:21.167236 1014653 finetune.py:68] layer 5_q @ epoch 1 new loss 1.6631969629088417e-05 old loss 1.706614602881018e-05 BETTER
I0325 18:34:46.022559 1013719 finetune.py:68] layer 4_q @ epoch 3 new loss 1.05239923868794e-05 old loss 1.0725720130722038e-05 BETTER
I0325 18:34:56.005437 1014653 finetune.py:68] layer 5_q @ epoch 2 new loss 1.6302286894642748e-05 old loss 1.6631969629088417e-05 BETTER
I0325 18:35:23.358488 1013719 finetune.py:68] layer 4_q @ epoch 4 new loss 1.0355117410654202e-05 old loss 1.05239923868794e-05 BETTER
I0325 18:35:31.137128 1014653 finetune.py:68] layer 5_q @ epoch 3 new loss 1.6037245586630888e-05 old loss 1.6302286894642748e-05 BETTER
I0325 18:35:31.421098 1013719 finetune.py:45] layer 4_k initial loss 1.1172506674483884e-05
I0325 18:36:06.292487 1014653 finetune.py:68] layer 5_q @ epoch 4 new loss 1.5810523109394126e-05 old loss 1.6037245586630888e-05 BETTER
I0325 18:36:07.870488 1013719 finetune.py:68] layer 4_k @ epoch 0 new loss 1.0953856872220058e-05 old loss 1.1172506674483884e-05 BETTER
I0325 18:36:14.068450 1014653 finetune.py:45] layer 5_k initial loss 1.6903062714845873e-05
I0325 18:36:45.017080 1013719 finetune.py:68] layer 4_k @ epoch 1 new loss 1.081036953110015e-05 old loss 1.0953856872220058e-05 BETTER
I0325 18:36:47.887143 1014653 finetune.py:68] layer 5_k @ epoch 0 new loss 1.6511785361217335e-05 old loss 1.6903062714845873e-05 BETTER
I0325 18:37:22.167507 1013719 finetune.py:68] layer 4_k @ epoch 2 new loss 1.0688285328797065e-05 old loss 1.081036953110015e-05 BETTER
I0325 18:37:22.327644 1014653 finetune.py:68] layer 5_k @ epoch 1 new loss 1.6326970580848865e-05 old loss 1.6511785361217335e-05 BETTER
I0325 18:37:56.856615 1014653 finetune.py:68] layer 5_k @ epoch 2 new loss 1.6165558918146417e-05 old loss 1.6326970580848865e-05 BETTER
I0325 18:37:59.272823 1013719 finetune.py:68] layer 4_k @ epoch 3 new loss 1.0581456990621518e-05 old loss 1.0688285328797065e-05 BETTER
I0325 18:38:31.492753 1014653 finetune.py:68] layer 5_k @ epoch 3 new loss 1.6023295756895095e-05 old loss 1.6165558918146417e-05 BETTER
I0325 18:38:36.314307 1013719 finetune.py:68] layer 4_k @ epoch 4 new loss 1.0481868230272084e-05 old loss 1.0581456990621518e-05 BETTER
I0325 18:38:46.101814 1013719 finetune.py:45] layer 4_o initial loss 3.0880619306117296e-05
I0325 18:39:06.305454 1014653 finetune.py:68] layer 5_k @ epoch 4 new loss 1.589393286849372e-05 old loss 1.6023295756895095e-05 BETTER
I0325 18:39:15.806868 1014653 finetune.py:45] layer 5_o initial loss 4.327335409470834e-05
I0325 18:39:21.583895 1013719 finetune.py:68] layer 4_o @ epoch 0 new loss 2.8311764253885485e-05 old loss 3.0880619306117296e-05 BETTER
I0325 18:39:48.961798 1014653 finetune.py:68] layer 5_o @ epoch 0 new loss 3.998269676230848e-05 old loss 4.327335409470834e-05 BETTER
I0325 18:39:57.518342 1013719 finetune.py:68] layer 4_o @ epoch 1 new loss 2.7360694730305113e-05 old loss 2.8311764253885485e-05 BETTER
I0325 18:40:22.938574 1014653 finetune.py:68] layer 5_o @ epoch 1 new loss 3.866020779241808e-05 old loss 3.998269676230848e-05 BETTER
I0325 18:40:33.538865 1013719 finetune.py:68] layer 4_o @ epoch 2 new loss 2.6710100428317674e-05 old loss 2.7360694730305113e-05 BETTER
I0325 18:40:56.950218 1014653 finetune.py:68] layer 5_o @ epoch 2 new loss 3.7746114685432985e-05 old loss 3.866020779241808e-05 BETTER
I0325 18:41:09.603533 1013719 finetune.py:68] layer 4_o @ epoch 3 new loss 2.6205749236396514e-05 old loss 2.6710100428317674e-05 BETTER
I0325 18:41:31.301579 1014653 finetune.py:68] layer 5_o @ epoch 3 new loss 3.70415800716728e-05 old loss 3.7746114685432985e-05 BETTER
I0325 18:41:45.949126 1013719 finetune.py:68] layer 4_o @ epoch 4 new loss 2.5784533136175014e-05 old loss 2.6205749236396514e-05 BETTER
I0325 18:42:05.629938 1014653 finetune.py:68] layer 5_o @ epoch 4 new loss 3.646331970230676e-05 old loss 3.70415800716728e-05 BETTER
I0325 18:42:08.243197 1013719 finetune.py:45] layer 4_up initial loss 5.5621589126531035e-05
I0325 18:42:27.241377 1014653 finetune.py:45] layer 5_up initial loss 7.930571882752702e-05
I0325 18:42:40.855483 1013719 finetune.py:68] layer 4_up @ epoch 0 new loss 5.4603478929493576e-05 old loss 5.5621589126531035e-05 BETTER
I0325 18:42:58.131631 1014653 finetune.py:68] layer 5_up @ epoch 0 new loss 7.760903099551797e-05 old loss 7.930571882752702e-05 BETTER
I0325 18:43:14.614933 1013719 finetune.py:68] layer 4_up @ epoch 1 new loss 5.394858817453496e-05 old loss 5.4603478929493576e-05 BETTER
I0325 18:43:30.310335 1014653 finetune.py:68] layer 5_up @ epoch 1 new loss 7.654218643438071e-05 old loss 7.760903099551797e-05 BETTER
I0325 18:43:48.556857 1013719 finetune.py:68] layer 4_up @ epoch 2 new loss 5.341652286006138e-05 old loss 5.394858817453496e-05 BETTER
I0325 18:44:02.486068 1014653 finetune.py:68] layer 5_up @ epoch 2 new loss 7.566790736746043e-05 old loss 7.654218643438071e-05 BETTER
I0325 18:44:22.789780 1013719 finetune.py:68] layer 4_up @ epoch 3 new loss 5.2943061746191233e-05 old loss 5.341652286006138e-05 BETTER
I0325 18:44:34.629944 1014653 finetune.py:68] layer 5_up @ epoch 3 new loss 7.490628922823817e-05 old loss 7.566790736746043e-05 BETTER
I0325 18:44:57.076493 1013719 finetune.py:68] layer 4_up @ epoch 4 new loss 5.251634502201341e-05 old loss 5.2943061746191233e-05 BETTER
I0325 18:45:06.825503 1014653 finetune.py:68] layer 5_up @ epoch 4 new loss 7.42222327971831e-05 old loss 7.490628922823817e-05 BETTER
I0325 18:45:19.146279 1013719 finetune.py:45] layer 4_gate initial loss 6.386825407389551e-05
I0325 18:45:28.687693 1014653 finetune.py:45] layer 5_gate initial loss 9.015598334372044e-05
I0325 18:45:49.732847 1013719 finetune.py:68] layer 4_gate @ epoch 0 new loss 6.33526433375664e-05 old loss 6.386825407389551e-05 BETTER
I0325 18:45:57.653548 1014653 finetune.py:68] layer 5_gate @ epoch 0 new loss 8.934803190641105e-05 old loss 9.015598334372044e-05 BETTER
I0325 18:46:21.249751 1013719 finetune.py:68] layer 4_gate @ epoch 1 new loss 6.293490150710568e-05 old loss 6.33526433375664e-05 BETTER
I0325 18:46:27.300456 1014653 finetune.py:68] layer 5_gate @ epoch 1 new loss 8.869826706359163e-05 old loss 8.934803190641105e-05 BETTER
I0325 18:46:52.881247 1013719 finetune.py:68] layer 4_gate @ epoch 2 new loss 6.255831249291077e-05 old loss 6.293490150710568e-05 BETTER
I0325 18:46:57.145149 1014653 finetune.py:68] layer 5_gate @ epoch 2 new loss 8.810997678665444e-05 old loss 8.869826706359163e-05 BETTER
I0325 18:47:24.602878 1013719 finetune.py:68] layer 4_gate @ epoch 3 new loss 6.220980867510661e-05 old loss 6.255831249291077e-05 BETTER
I0325 18:47:26.880127 1014653 finetune.py:68] layer 5_gate @ epoch 3 new loss 8.75817277119495e-05 old loss 8.810997678665444e-05 BETTER
I0325 18:47:56.402215 1013719 finetune.py:68] layer 4_gate @ epoch 4 new loss 6.188157567521557e-05 old loss 6.220980867510661e-05 BETTER
I0325 18:47:56.707630 1014653 finetune.py:68] layer 5_gate @ epoch 4 new loss 8.707670349394903e-05 old loss 8.75817277119495e-05 BETTER
I0325 18:48:19.212696 1014653 finetune.py:45] layer 5_down initial loss 0.00014006171841174364
I0325 18:48:19.317843 1013719 finetune.py:45] layer 4_down initial loss 9.974834392778575e-05
I0325 18:48:45.943536 1014653 finetune.py:68] layer 5_down @ epoch 0 new loss 0.00014000418013893068 old loss 0.00014006171841174364 BETTER
I0325 18:48:47.634891 1013719 finetune.py:68] layer 4_down @ epoch 0 new loss 9.969433449441567e-05 old loss 9.974834392778575e-05 BETTER
I0325 18:49:13.701368 1014653 finetune.py:68] layer 5_down @ epoch 1 new loss 0.00013996152847539634 old loss 0.00014000418013893068 BETTER
I0325 18:49:17.069581 1013719 finetune.py:68] layer 4_down @ epoch 1 new loss 9.965390927391127e-05 old loss 9.969433449441567e-05 BETTER
I0325 18:49:41.459510 1014653 finetune.py:68] layer 5_down @ epoch 2 new loss 0.00013992541062179953 old loss 0.00013996152847539634 BETTER
I0325 18:49:46.541628 1013719 finetune.py:68] layer 4_down @ epoch 2 new loss 9.961829346138984e-05 old loss 9.965390927391127e-05 BETTER
I0325 18:50:09.230237 1014653 finetune.py:68] layer 5_down @ epoch 3 new loss 0.0001398944586981088 old loss 0.00013992541062179953 BETTER
I0325 18:50:16.088360 1013719 finetune.py:68] layer 4_down @ epoch 3 new loss 9.958790906239301e-05 old loss 9.961829346138984e-05 BETTER
I0325 18:50:36.973871 1014653 finetune.py:68] layer 5_down @ epoch 4 new loss 0.0001398660388076678 old loss 0.0001398944586981088 BETTER
5_v proxy err 0.07773909717798233 tr(WHW.T) 208.81988525390625
bpp_loss 2.127954602241516
5_q proxy err 0.002258944557979703 tr(WHW.T) 36022.1640625
bpp_loss 3.043466806411743
5_k proxy err 0.0010005689691752195 tr(WHW.T) 23058.359375
bpp_loss 3.862603783607483
5_o proxy err 0.06373060494661331 tr(WHW.T) 1073.214599609375
bpp_loss 2.2662819623947144
5_up proxy err 0.04156552627682686 tr(WHW.T) 7657.8603515625
bpp_loss 2.428450311933245
5_gate proxy err 0.011399442330002785 tr(WHW.T) 30361.607421875
bpp_loss 2.75093936920166
5_down proxy err 0.04813840240240097 tr(WHW.T) 6513.04736328125
bpp_loss 2.4316818714141846
I0325 18:50:45.837931 1013719 finetune.py:68] layer 4_down @ epoch 4 new loss 9.956116991816089e-05 old loss 9.958790906239301e-05 BETTER
4_v proxy err 0.05734242498874664 tr(WHW.T) 285.30712890625
bpp_loss 2.239655017852783
4_q proxy err 0.0016044947551563382 tr(WHW.T) 50140.01171875
bpp_loss 3.067779064178467
4_k proxy err 0.0007879872573539615 tr(WHW.T) 29373.05859375
bpp_loss 3.889027237892151
4_o proxy err 0.05486854538321495 tr(WHW.T) 1315.539794921875
bpp_loss 2.3179253339767456
4_up proxy err 0.04336943477392197 tr(WHW.T) 7379.947265625
bpp_loss 2.423159599304199
4_gate proxy err 0.01200755313038826 tr(WHW.T) 29030.220703125
bpp_loss 2.74795286996024
4_down proxy err 0.049013275653123856 tr(WHW.T) 6495.33935546875
bpp_loss 2.4266456195286343
I0325 18:51:53.969283 975333 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 63.192827463150024s
I0325 18:51:57.366436 1032025 config.py:54] PyTorch version 2.6.0 available.
W0325 18:51:57.650493 1032025 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 18:51:58.518127 1032025 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 18:51:58.522214 975333 quantize_finetune_llama.py:209] layer 7 gpu 1
I0325 18:51:58.536007 1032025 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 18:52:05.715501 1032025 finetune.py:45] layer 6_v initial loss 3.920683229807764e-05
W0325 18:52:05.715716 1032025 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 18:52:41.529625 1032025 finetune.py:68] layer 6_v @ epoch 0 new loss 2.172294625779614e-05 old loss 3.920683229807764e-05 BETTER
I0325 18:52:58.693390 975333 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 59.74563646316528s
I0325 18:53:02.146214 1032944 config.py:54] PyTorch version 2.6.0 available.
W0325 18:53:02.461645 1032944 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 18:53:03.389233 1032944 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 18:53:03.393336 975333 quantize_finetune_llama.py:209] layer 8 gpu 0
I0325 18:53:03.407372 1032944 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 18:53:10.344550 1032944 finetune.py:45] layer 7_v initial loss 4.138965596212074e-05
W0325 18:53:10.344841 1032944 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 18:53:18.486480 1032025 finetune.py:68] layer 6_v @ epoch 1 new loss 1.9663702914840542e-05 old loss 2.172294625779614e-05 BETTER
I0325 18:53:43.777329 1032944 finetune.py:68] layer 7_v @ epoch 0 new loss 2.672136906767264e-05 old loss 4.138965596212074e-05 BETTER
I0325 18:53:55.582138 1032025 finetune.py:68] layer 6_v @ epoch 2 new loss 1.8672124497243203e-05 old loss 1.9663702914840542e-05 BETTER
I0325 18:54:18.251896 1032944 finetune.py:68] layer 7_v @ epoch 1 new loss 2.458219569234643e-05 old loss 2.672136906767264e-05 BETTER
I0325 18:54:32.852166 1032025 finetune.py:68] layer 6_v @ epoch 3 new loss 1.8025193639914505e-05 old loss 1.8672124497243203e-05 BETTER
I0325 18:54:53.285606 1032944 finetune.py:68] layer 7_v @ epoch 2 new loss 2.3446131308446638e-05 old loss 2.458219569234643e-05 BETTER
I0325 18:55:10.093417 1032025 finetune.py:68] layer 6_v @ epoch 4 new loss 1.754479671944864e-05 old loss 1.8025193639914505e-05 BETTER
I0325 18:55:19.970181 1032025 finetune.py:45] layer 6_q initial loss 2.244582719868049e-05
I0325 18:55:28.456938 1032944 finetune.py:68] layer 7_v @ epoch 3 new loss 2.2682210328639485e-05 old loss 2.3446131308446638e-05 BETTER
I0325 18:55:56.281929 1032025 finetune.py:68] layer 6_q @ epoch 0 new loss 2.083602339553181e-05 old loss 2.244582719868049e-05 BETTER
I0325 18:56:03.633363 1032944 finetune.py:68] layer 7_v @ epoch 4 new loss 2.2097598048276268e-05 old loss 2.2682210328639485e-05 BETTER
I0325 18:56:13.153730 1032944 finetune.py:45] layer 7_q initial loss 2.8915466828038916e-05
I0325 18:56:33.279200 1032025 finetune.py:68] layer 6_q @ epoch 1 new loss 2.0301016775192693e-05 old loss 2.083602339553181e-05 BETTER
I0325 18:56:47.078269 1032944 finetune.py:68] layer 7_q @ epoch 0 new loss 2.7050466087530367e-05 old loss 2.8915466828038916e-05 BETTER
I0325 18:57:10.306807 1032025 finetune.py:68] layer 6_q @ epoch 2 new loss 1.990660712181125e-05 old loss 2.0301016775192693e-05 BETTER
I0325 18:57:21.657429 1032944 finetune.py:68] layer 7_q @ epoch 1 new loss 2.6375304514658637e-05 old loss 2.7050466087530367e-05 BETTER
I0325 18:57:47.306854 1032025 finetune.py:68] layer 6_q @ epoch 3 new loss 1.9568764400901273e-05 old loss 1.990660712181125e-05 BETTER
I0325 18:57:56.380609 1032944 finetune.py:68] layer 7_q @ epoch 2 new loss 2.586587652331218e-05 old loss 2.6375304514658637e-05 BETTER
I0325 18:58:24.273421 1032025 finetune.py:68] layer 6_q @ epoch 4 new loss 1.9290795535198413e-05 old loss 1.9568764400901273e-05 BETTER
I0325 18:58:31.121271 1032944 finetune.py:68] layer 7_q @ epoch 3 new loss 2.5457322408328764e-05 old loss 2.586587652331218e-05 BETTER
I0325 18:58:32.310539 1032025 finetune.py:45] layer 6_k initial loss 2.0770794435520656e-05
I0325 18:59:06.000525 1032944 finetune.py:68] layer 7_q @ epoch 4 new loss 2.510133526811842e-05 old loss 2.5457322408328764e-05 BETTER
I0325 18:59:08.706094 1032025 finetune.py:68] layer 6_k @ epoch 0 new loss 2.0252027752576396e-05 old loss 2.0770794435520656e-05 BETTER
I0325 18:59:13.826228 1032944 finetune.py:45] layer 7_k initial loss 2.6997597160516307e-05
I0325 18:59:45.772196 1032025 finetune.py:68] layer 6_k @ epoch 1 new loss 2.0017971110064536e-05 old loss 2.0252027752576396e-05 BETTER
I0325 18:59:47.754987 1032944 finetune.py:68] layer 7_k @ epoch 0 new loss 2.635491000546608e-05 old loss 2.6997597160516307e-05 BETTER
I0325 19:00:22.317558 1032944 finetune.py:68] layer 7_k @ epoch 1 new loss 2.6055960915982723e-05 old loss 2.635491000546608e-05 BETTER
I0325 19:00:22.647833 1032025 finetune.py:68] layer 6_k @ epoch 2 new loss 1.9818318833131343e-05 old loss 2.0017971110064536e-05 BETTER
I0325 19:00:56.950589 1032944 finetune.py:68] layer 7_k @ epoch 2 new loss 2.5804969482123852e-05 old loss 2.6055960915982723e-05 BETTER
I0325 19:00:59.365048 1032025 finetune.py:68] layer 6_k @ epoch 3 new loss 1.9633036572486162e-05 old loss 1.9818318833131343e-05 BETTER
I0325 19:01:31.627433 1032944 finetune.py:68] layer 7_k @ epoch 3 new loss 2.5581821319065057e-05 old loss 2.5804969482123852e-05 BETTER
I0325 19:01:36.359801 1032025 finetune.py:68] layer 6_k @ epoch 4 new loss 1.9472658095764928e-05 old loss 1.9633036572486162e-05 BETTER
I0325 19:01:46.136247 1032025 finetune.py:45] layer 6_o initial loss 5.951813000137918e-05
I0325 19:02:06.401598 1032944 finetune.py:68] layer 7_k @ epoch 4 new loss 2.5373006792506203e-05 old loss 2.5581821319065057e-05 BETTER
I0325 19:02:16.025905 1032944 finetune.py:45] layer 7_o initial loss 8.360311767319217e-05
I0325 19:02:21.441277 1032025 finetune.py:68] layer 6_o @ epoch 0 new loss 5.477566082845442e-05 old loss 5.951813000137918e-05 BETTER
I0325 19:02:49.292329 1032944 finetune.py:68] layer 7_o @ epoch 0 new loss 7.618695963174105e-05 old loss 8.360311767319217e-05 BETTER
I0325 19:02:57.495453 1032025 finetune.py:68] layer 6_o @ epoch 1 new loss 5.280285040498711e-05 old loss 5.477566082845442e-05 BETTER
I0325 19:03:23.377238 1032944 finetune.py:68] layer 7_o @ epoch 1 new loss 7.283170998562127e-05 old loss 7.618695963174105e-05 BETTER
I0325 19:03:33.575932 1032025 finetune.py:68] layer 6_o @ epoch 2 new loss 5.143634552950971e-05 old loss 5.280285040498711e-05 BETTER
I0325 19:03:57.685052 1032944 finetune.py:68] layer 7_o @ epoch 2 new loss 7.053541048662737e-05 old loss 7.283170998562127e-05 BETTER
I0325 19:04:09.702615 1032025 finetune.py:68] layer 6_o @ epoch 3 new loss 5.03693227074109e-05 old loss 5.143634552950971e-05 BETTER
I0325 19:04:31.971307 1032944 finetune.py:68] layer 7_o @ epoch 3 new loss 6.875807594042271e-05 old loss 7.053541048662737e-05 BETTER
I0325 19:04:45.705533 1032025 finetune.py:68] layer 6_o @ epoch 4 new loss 4.948751666233875e-05 old loss 5.03693227074109e-05 BETTER
I0325 19:05:06.109677 1032944 finetune.py:68] layer 7_o @ epoch 4 new loss 6.732530164299533e-05 old loss 6.875807594042271e-05 BETTER
I0325 19:05:07.614903 1032025 finetune.py:45] layer 6_up initial loss 0.00010444714280311018
I0325 19:05:27.920471 1032944 finetune.py:45] layer 7_up initial loss 0.000127179388073273
I0325 19:05:39.988678 1032025 finetune.py:68] layer 6_up @ epoch 0 new loss 0.00010177090007346123 old loss 0.00010444714280311018 BETTER
I0325 19:05:58.694753 1032944 finetune.py:68] layer 7_up @ epoch 0 new loss 0.0001237930409843102 old loss 0.000127179388073273 BETTER
I0325 19:06:13.570129 1032025 finetune.py:68] layer 6_up @ epoch 1 new loss 0.00010010897676693276 old loss 0.00010177090007346123 BETTER
I0325 19:06:30.635095 1032944 finetune.py:68] layer 7_up @ epoch 1 new loss 0.00012166858505224809 old loss 0.0001237930409843102 BETTER
I0325 19:06:47.302062 1032025 finetune.py:68] layer 6_up @ epoch 2 new loss 9.876002150122076e-05 old loss 0.00010010897676693276 BETTER
I0325 19:07:02.695122 1032944 finetune.py:68] layer 7_up @ epoch 2 new loss 0.00011991854989901185 old loss 0.00012166858505224809 BETTER
I0325 19:07:21.032016 1032025 finetune.py:68] layer 6_up @ epoch 3 new loss 9.758987289387733e-05 old loss 9.876002150122076e-05 BETTER
I0325 19:07:34.869655 1032944 finetune.py:68] layer 7_up @ epoch 3 new loss 0.00011840374645544216 old loss 0.00011991854989901185 BETTER
I0325 19:07:55.020021 1032025 finetune.py:68] layer 6_up @ epoch 4 new loss 9.65354047366418e-05 old loss 9.758987289387733e-05 BETTER
I0325 19:08:07.079863 1032944 finetune.py:68] layer 7_up @ epoch 4 new loss 0.00011703977361321449 old loss 0.00011840374645544216 BETTER
I0325 19:08:17.021183 1032025 finetune.py:45] layer 6_gate initial loss 0.0001155393329099752
I0325 19:08:28.846766 1032944 finetune.py:45] layer 7_gate initial loss 0.00014003347314428538
I0325 19:08:47.452686 1032025 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.00011430440645199269 old loss 0.0001155393329099752 BETTER
I0325 19:08:57.562170 1032944 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.00013845441571902484 old loss 0.00014003347314428538 BETTER
I0325 19:09:18.955609 1032025 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.00011333282600389794 old loss 0.00011430440645199269 BETTER
I0325 19:09:27.293143 1032944 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.000137238297611475 old loss 0.00013845441571902484 BETTER
I0325 19:09:50.605078 1032025 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.00011247166548855603 old loss 0.00011333282600389794 BETTER
I0325 19:09:57.096277 1032944 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.00013616473006550223 old loss 0.000137238297611475 BETTER
I0325 19:10:22.459086 1032025 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.00011167940101586282 old loss 0.00011247166548855603 BETTER
I0325 19:10:27.080910 1032944 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.0001351822866126895 old loss 0.00013616473006550223 BETTER
I0325 19:10:54.099165 1032025 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.00011094680667156354 old loss 0.00011167940101586282 BETTER
I0325 19:10:57.083652 1032944 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.0001342740433756262 old loss 0.0001351822866126895 BETTER
I0325 19:11:17.844140 1032025 finetune.py:45] layer 6_down initial loss 0.00017765266238711774
I0325 19:11:20.253141 1032944 finetune.py:45] layer 7_down initial loss 0.00020763570501003414
I0325 19:11:46.056252 1032025 finetune.py:68] layer 6_down @ epoch 0 new loss 0.00017758290050551295 old loss 0.00017765266238711774 BETTER
I0325 19:11:47.085730 1032944 finetune.py:68] layer 7_down @ epoch 0 new loss 0.00020756496815010905 old loss 0.00020763570501003414 BETTER
I0325 19:12:14.788561 1032944 finetune.py:68] layer 7_down @ epoch 1 new loss 0.000207501943805255 old loss 0.00020756496815010905 BETTER
I0325 19:12:15.094538 1032025 finetune.py:68] layer 6_down @ epoch 1 new loss 0.00017752959684003145 old loss 0.00017758290050551295 BETTER
I0325 19:12:42.519264 1032944 finetune.py:68] layer 7_down @ epoch 2 new loss 0.0002074462827295065 old loss 0.000207501943805255 BETTER
I0325 19:12:44.593548 1032025 finetune.py:68] layer 6_down @ epoch 2 new loss 0.00017748524260241538 old loss 0.00017752959684003145 BETTER
I0325 19:13:10.331737 1032944 finetune.py:68] layer 7_down @ epoch 3 new loss 0.0002073958603432402 old loss 0.0002074462827295065 BETTER
I0325 19:13:14.052599 1032025 finetune.py:68] layer 6_down @ epoch 3 new loss 0.00017744582146406174 old loss 0.00017748524260241538 BETTER
I0325 19:13:38.188870 1032944 finetune.py:68] layer 7_down @ epoch 4 new loss 0.00020734932331833988 old loss 0.0002073958603432402 BETTER
7_v proxy err 0.054532043635845184 tr(WHW.T) 309.4270935058594
bpp_loss 2.1714601516723633
7_q proxy err 0.0024560000747442245 tr(WHW.T) 35181.96484375
bpp_loss 3.0170546770095825
7_k proxy err 0.0009348637540824711 tr(WHW.T) 26960.984375
bpp_loss 3.951054573059082
7_o proxy err 0.07389151304960251 tr(WHW.T) 981.849365234375
bpp_loss 2.309838652610779
7_up proxy err 0.03588808700442314 tr(WHW.T) 8623.71484375
bpp_loss 2.4399345942905972
7_gate proxy err 0.009593779221177101 tr(WHW.T) 34868.63671875
bpp_loss 2.727545601981027
7_down proxy err 0.048095621168613434 tr(WHW.T) 6640.62109375
bpp_loss 2.4461565698896135
I0325 19:13:43.454021 1032025 finetune.py:68] layer 6_down @ epoch 4 new loss 0.00017741079500410706 old loss 0.00017744582146406174 BETTER
6_v proxy err 0.06482826918363571 tr(WHW.T) 253.63377380371094
bpp_loss 2.17340886592865
6_q proxy err 0.002338375197723508 tr(WHW.T) 35684.02734375
bpp_loss 3.0881214141845703
6_k proxy err 0.0009182809153571725 tr(WHW.T) 26253.4296875
bpp_loss 3.930781602859497
6_o proxy err 0.07259651273488998 tr(WHW.T) 1031.54248046875
bpp_loss 2.299301266670227
6_up proxy err 0.03946142643690109 tr(WHW.T) 7918.51220703125
bpp_loss 2.4275030408586775
6_gate proxy err 0.009568019770085812 tr(WHW.T) 35638.78515625
bpp_loss 2.7551210948399136
6_down proxy err 0.04776019975543022 tr(WHW.T) 6592.70654296875
bpp_loss 2.4321036679404124
I0325 19:14:51.923362 975333 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 63.62218260765076s
I0325 19:14:55.130372 1050187 config.py:54] PyTorch version 2.6.0 available.
W0325 19:14:55.409335 1050187 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 19:14:56.281130 1050187 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 19:14:56.285000 975333 quantize_finetune_llama.py:209] layer 9 gpu 1
I0325 19:14:56.297447 1050187 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 19:15:03.542625 1050187 finetune.py:45] layer 8_v initial loss 4.510521830525249e-05
W0325 19:15:03.542813 1050187 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 19:15:39.727927 1050187 finetune.py:68] layer 8_v @ epoch 0 new loss 2.9989045287948102e-05 old loss 4.510521830525249e-05 BETTER
I0325 19:15:56.693643 975333 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 59.99244999885559s
I0325 19:16:00.128747 1051098 config.py:54] PyTorch version 2.6.0 available.
W0325 19:16:00.453221 1051098 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 19:16:01.389371 1051098 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 19:16:01.393482 975333 quantize_finetune_llama.py:209] layer 10 gpu 0
I0325 19:16:01.407036 1051098 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 19:16:08.375711 1051098 finetune.py:45] layer 9_v initial loss 5.051923290011473e-05
W0325 19:16:08.376014 1051098 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 19:16:17.120390 1050187 finetune.py:68] layer 8_v @ epoch 1 new loss 2.7679554477799684e-05 old loss 2.9989045287948102e-05 BETTER
I0325 19:16:42.011245 1051098 finetune.py:68] layer 9_v @ epoch 0 new loss 3.124740396742709e-05 old loss 5.051923290011473e-05 BETTER
I0325 19:16:54.670264 1050187 finetune.py:68] layer 8_v @ epoch 2 new loss 2.6441633963258937e-05 old loss 2.7679554477799684e-05 BETTER
I0325 19:17:16.576918 1051098 finetune.py:68] layer 9_v @ epoch 1 new loss 2.8496655431808904e-05 old loss 3.124740396742709e-05 BETTER
I0325 19:17:32.223545 1050187 finetune.py:68] layer 8_v @ epoch 3 new loss 2.5592946258257143e-05 old loss 2.6441633963258937e-05 BETTER
I0325 19:17:51.555356 1051098 finetune.py:68] layer 9_v @ epoch 2 new loss 2.7095911718788557e-05 old loss 2.8496655431808904e-05 BETTER
I0325 19:18:09.597938 1050187 finetune.py:68] layer 8_v @ epoch 4 new loss 2.4952161766123027e-05 old loss 2.5592946258257143e-05 BETTER
I0325 19:18:19.438506 1050187 finetune.py:45] layer 8_q initial loss 3.182974978699349e-05
I0325 19:18:26.722610 1051098 finetune.py:68] layer 9_v @ epoch 3 new loss 2.614280310808681e-05 old loss 2.7095911718788557e-05 BETTER
I0325 19:18:55.864397 1050187 finetune.py:68] layer 8_q @ epoch 0 new loss 3.0263348890002817e-05 old loss 3.182974978699349e-05 BETTER
I0325 19:19:02.089019 1051098 finetune.py:68] layer 9_v @ epoch 4 new loss 2.546066571085248e-05 old loss 2.614280310808681e-05 BETTER
I0325 19:19:11.556706 1051098 finetune.py:45] layer 9_q initial loss 3.3175412681885064e-05
I0325 19:19:32.841985 1050187 finetune.py:68] layer 8_q @ epoch 1 new loss 2.9545659344876185e-05 old loss 3.0263348890002817e-05 BETTER
I0325 19:19:45.640830 1051098 finetune.py:68] layer 9_q @ epoch 0 new loss 3.131805351586081e-05 old loss 3.3175412681885064e-05 BETTER
I0325 19:20:09.897376 1050187 finetune.py:68] layer 8_q @ epoch 2 new loss 2.9011909646214917e-05 old loss 2.9545659344876185e-05 BETTER
I0325 19:20:20.507762 1051098 finetune.py:68] layer 9_q @ epoch 1 new loss 3.057130015804432e-05 old loss 3.131805351586081e-05 BETTER
I0325 19:20:47.318838 1050187 finetune.py:68] layer 8_q @ epoch 3 new loss 2.8561378712765872e-05 old loss 2.9011909646214917e-05 BETTER
I0325 19:20:55.372310 1051098 finetune.py:68] layer 9_q @ epoch 2 new loss 3.0009003239683807e-05 old loss 3.057130015804432e-05 BETTER
I0325 19:21:24.380373 1050187 finetune.py:68] layer 8_q @ epoch 4 new loss 2.8186323106638156e-05 old loss 2.8561378712765872e-05 BETTER
I0325 19:21:30.194558 1051098 finetune.py:68] layer 9_q @ epoch 3 new loss 2.955306990770623e-05 old loss 3.0009003239683807e-05 BETTER
I0325 19:21:32.577898 1050187 finetune.py:45] layer 8_k initial loss 2.9782351703033783e-05
I0325 19:22:05.049670 1051098 finetune.py:68] layer 9_q @ epoch 4 new loss 2.915675941039808e-05 old loss 2.955306990770623e-05 BETTER
I0325 19:22:08.795745 1050187 finetune.py:68] layer 8_k @ epoch 0 new loss 2.9340237233554944e-05 old loss 2.9782351703033783e-05 BETTER
I0325 19:22:13.003489 1051098 finetune.py:45] layer 9_k initial loss 3.120631299680099e-05
I0325 19:22:45.858213 1050187 finetune.py:68] layer 8_k @ epoch 1 new loss 2.9042077585472725e-05 old loss 2.9340237233554944e-05 BETTER
I0325 19:22:47.023763 1051098 finetune.py:68] layer 9_k @ epoch 0 new loss 3.0668819817947224e-05 old loss 3.120631299680099e-05 BETTER
I0325 19:23:21.670083 1051098 finetune.py:68] layer 9_k @ epoch 1 new loss 3.0341188903548755e-05 old loss 3.0668819817947224e-05 BETTER
I0325 19:23:22.795733 1050187 finetune.py:68] layer 8_k @ epoch 2 new loss 2.87836210191017e-05 old loss 2.9042077585472725e-05 BETTER
I0325 19:23:56.376663 1051098 finetune.py:68] layer 9_k @ epoch 2 new loss 3.0052533475100063e-05 old loss 3.0341188903548755e-05 BETTER
I0325 19:23:59.819335 1050187 finetune.py:68] layer 8_k @ epoch 3 new loss 2.8543297958094627e-05 old loss 2.87836210191017e-05 BETTER
I0325 19:24:30.956276 1051098 finetune.py:68] layer 9_k @ epoch 3 new loss 2.980964200105518e-05 old loss 3.0052533475100063e-05 BETTER
I0325 19:24:36.855162 1050187 finetune.py:68] layer 8_k @ epoch 4 new loss 2.833421058312524e-05 old loss 2.8543297958094627e-05 BETTER
I0325 19:24:46.730080 1050187 finetune.py:45] layer 8_o initial loss 0.00010370928066549823
I0325 19:25:05.437929 1051098 finetune.py:68] layer 9_k @ epoch 4 new loss 2.957843753392808e-05 old loss 2.980964200105518e-05 BETTER
I0325 19:25:15.011584 1051098 finetune.py:45] layer 9_o initial loss 0.00011254390847170725
I0325 19:25:22.020995 1050187 finetune.py:68] layer 8_o @ epoch 0 new loss 9.471078374190256e-05 old loss 0.00010370928066549823 BETTER
I0325 19:25:48.298398 1051098 finetune.py:68] layer 9_o @ epoch 0 new loss 0.00010245268640574068 old loss 0.00011254390847170725 BETTER
I0325 19:25:58.020246 1050187 finetune.py:68] layer 8_o @ epoch 1 new loss 9.054299880517647e-05 old loss 9.471078374190256e-05 BETTER
I0325 19:26:22.149513 1051098 finetune.py:68] layer 9_o @ epoch 1 new loss 9.780843538464978e-05 old loss 0.00010245268640574068 BETTER
I0325 19:26:34.097289 1050187 finetune.py:68] layer 8_o @ epoch 2 new loss 8.761283970670775e-05 old loss 9.054299880517647e-05 BETTER
I0325 19:26:56.107042 1051098 finetune.py:68] layer 9_o @ epoch 2 new loss 9.450974175706506e-05 old loss 9.780843538464978e-05 BETTER
I0325 19:27:10.415658 1050187 finetune.py:68] layer 8_o @ epoch 3 new loss 8.53516103234142e-05 old loss 8.761283970670775e-05 BETTER
I0325 19:27:30.245663 1051098 finetune.py:68] layer 9_o @ epoch 3 new loss 9.197894542012364e-05 old loss 9.450974175706506e-05 BETTER
I0325 19:27:46.891693 1050187 finetune.py:68] layer 8_o @ epoch 4 new loss 8.348748087882996e-05 old loss 8.53516103234142e-05 BETTER
I0325 19:28:04.370097 1051098 finetune.py:68] layer 9_o @ epoch 4 new loss 8.990569040179253e-05 old loss 9.197894542012364e-05 BETTER
I0325 19:28:08.966263 1050187 finetune.py:45] layer 8_up initial loss 0.00014874187763780355
I0325 19:28:26.134168 1051098 finetune.py:45] layer 9_up initial loss 0.0001618801470613107
I0325 19:28:41.555083 1050187 finetune.py:68] layer 8_up @ epoch 0 new loss 0.00014499473036266863 old loss 0.00014874187763780355 BETTER
I0325 19:28:56.999848 1051098 finetune.py:68] layer 9_up @ epoch 0 new loss 0.00015741889365017414 old loss 0.0001618801470613107 BETTER
I0325 19:29:15.288307 1050187 finetune.py:68] layer 8_up @ epoch 1 new loss 0.00014253435074351728 old loss 0.00014499473036266863 BETTER
I0325 19:29:29.144844 1051098 finetune.py:68] layer 9_up @ epoch 1 new loss 0.0001546290295664221 old loss 0.00015741889365017414 BETTER
I0325 19:29:49.073028 1050187 finetune.py:68] layer 8_up @ epoch 2 new loss 0.00014049725723452866 old loss 0.00014253435074351728 BETTER
I0325 19:30:01.254441 1051098 finetune.py:68] layer 9_up @ epoch 2 new loss 0.0001523244718555361 old loss 0.0001546290295664221 BETTER
I0325 19:30:22.966331 1050187 finetune.py:68] layer 8_up @ epoch 3 new loss 0.0001387349475407973 old loss 0.00014049725723452866 BETTER
I0325 19:30:33.407144 1051098 finetune.py:68] layer 9_up @ epoch 3 new loss 0.00015030161011964083 old loss 0.0001523244718555361 BETTER
I0325 19:30:56.821821 1050187 finetune.py:68] layer 8_up @ epoch 4 new loss 0.00013712942018173635 old loss 0.0001387349475407973 BETTER
I0325 19:31:05.549453 1051098 finetune.py:68] layer 9_up @ epoch 4 new loss 0.0001485208049416542 old loss 0.00015030161011964083 BETTER
I0325 19:31:18.897929 1050187 finetune.py:45] layer 8_gate initial loss 0.000161669246153906
I0325 19:31:27.497117 1051098 finetune.py:45] layer 9_gate initial loss 0.0001750728697516024
I0325 19:31:49.225713 1050187 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.00015994231216609478 old loss 0.000161669246153906 BETTER
I0325 19:31:56.016860 1051098 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.00017302623018622398 old loss 0.0001750728697516024 BETTER
I0325 19:32:20.574535 1050187 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.00015853146032895893 old loss 0.00015994231216609478 BETTER
I0325 19:32:25.659250 1051098 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.00017145175661426038 old loss 0.00017302623018622398 BETTER
I0325 19:32:52.271136 1050187 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.00015726499259471893 old loss 0.00015853146032895893 BETTER
I0325 19:32:55.387620 1051098 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.00017005170229822397 old loss 0.00017145175661426038 BETTER
I0325 19:33:24.058739 1050187 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.0001561224489705637 old loss 0.00015726499259471893 BETTER
I0325 19:33:25.178843 1051098 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.00016875786241143942 old loss 0.00017005170229822397 BETTER
I0325 19:33:55.024635 1051098 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.00016757867706473917 old loss 0.00016875786241143942 BETTER
I0325 19:33:55.784087 1050187 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.00015504540351685137 old loss 0.0001561224489705637 BETTER
I0325 19:34:17.524569 1051098 finetune.py:45] layer 9_down initial loss 0.00025627430295571685
I0325 19:34:18.750167 1050187 finetune.py:45] layer 8_down initial loss 0.00023461849195882678
I0325 19:34:44.179332 1051098 finetune.py:68] layer 9_down @ epoch 0 new loss 0.00025617826031520963 old loss 0.00025627430295571685 BETTER
I0325 19:34:46.764914 1050187 finetune.py:68] layer 8_down @ epoch 0 new loss 0.00023452355526387691 old loss 0.00023461849195882678 BETTER
I0325 19:35:11.908783 1051098 finetune.py:68] layer 9_down @ epoch 1 new loss 0.00025609589647501707 old loss 0.00025617826031520963 BETTER
I0325 19:35:15.879829 1050187 finetune.py:68] layer 8_down @ epoch 1 new loss 0.0002344499807804823 old loss 0.00023452355526387691 BETTER
I0325 19:35:39.683382 1051098 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00025602325331419706 old loss 0.00025609589647501707 BETTER
I0325 19:35:44.970952 1050187 finetune.py:68] layer 8_down @ epoch 2 new loss 0.00023438752396032214 old loss 0.0002344499807804823 BETTER
I0325 19:36:07.508553 1051098 finetune.py:68] layer 9_down @ epoch 3 new loss 0.0002559587883297354 old loss 0.00025602325331419706 BETTER
I0325 19:36:14.102077 1050187 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0002343253290746361 old loss 0.00023438752396032214 BETTER
I0325 19:36:35.304229 1051098 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00025589161668904126 old loss 0.0002559587883297354 BETTER
9_v proxy err 0.04611710086464882 tr(WHW.T) 351.4288024902344
bpp_loss 2.289726138114929
9_q proxy err 0.003011728636920452 tr(WHW.T) 25671.98046875
bpp_loss 3.015762448310852
9_k proxy err 0.0010697467951104045 tr(WHW.T) 21006.1953125
bpp_loss 3.887578845024109
9_o proxy err 0.09196604043245316 tr(WHW.T) 793.4456787109375
bpp_loss 2.3742374181747437
9_up proxy err 0.03448847681283951 tr(WHW.T) 8978.7431640625
bpp_loss 2.444945744105748
9_gate proxy err 0.008502190001308918 tr(WHW.T) 39521.7421875
bpp_loss 2.74454471043178
9_down proxy err 0.04968275874853134 tr(WHW.T) 6384.7373046875
bpp_loss 2.445909261703491
I0325 19:36:43.491438 1050187 finetune.py:68] layer 8_down @ epoch 4 new loss 0.00023427122505381703 old loss 0.0002343253290746361 BETTER
8_v proxy err 0.06214619800448418 tr(WHW.T) 257.7052307128906
bpp_loss 2.192232131958008
8_q proxy err 0.00291129550896585 tr(WHW.T) 26617.953125
bpp_loss 3.003813862800598
8_k proxy err 0.0009972058469429612 tr(WHW.T) 22572.70703125
bpp_loss 3.8681777715682983
8_o proxy err 0.09676873683929443 tr(WHW.T) 763.6322631835938
bpp_loss 2.320492744445801
8_up proxy err 0.03627150505781174 tr(WHW.T) 8498.564453125
bpp_loss 2.435897554670061
8_gate proxy err 0.00893675908446312 tr(WHW.T) 37315.92578125
bpp_loss 2.7317748750959123
8_down proxy err 0.04904872179031372 tr(WHW.T) 6595.2783203125
bpp_loss 2.4445875031607494
I0325 19:37:51.429064 975333 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 63.030911922454834s
I0325 19:37:54.817072 1068416 config.py:54] PyTorch version 2.6.0 available.
W0325 19:37:55.100414 1068416 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 19:37:55.974529 1068416 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 19:37:55.978537 975333 quantize_finetune_llama.py:209] layer 11 gpu 1
I0325 19:37:55.991960 1068416 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 19:38:03.221140 1068416 finetune.py:45] layer 10_v initial loss 5.8538251323625445e-05
W0325 19:38:03.221485 1068416 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 19:38:38.825032 1068416 finetune.py:68] layer 10_v @ epoch 0 new loss 4.103605533600785e-05 old loss 5.8538251323625445e-05 BETTER
I0325 19:38:56.363198 975333 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 59.94272232055664s
I0325 19:38:59.882295 1069334 config.py:54] PyTorch version 2.6.0 available.
W0325 19:39:00.200054 1069334 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 19:39:01.132262 1069334 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 19:39:01.136325 975333 quantize_finetune_llama.py:209] layer 12 gpu 0
I0325 19:39:01.149960 1069334 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 19:39:08.127812 1069334 finetune.py:45] layer 11_v initial loss 5.2230192522984e-05
W0325 19:39:08.128098 1069334 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 19:39:15.675118 1068416 finetune.py:68] layer 10_v @ epoch 1 new loss 3.790959090110846e-05 old loss 4.103605533600785e-05 BETTER
I0325 19:39:41.444032 1069334 finetune.py:68] layer 11_v @ epoch 0 new loss 3.615567766246386e-05 old loss 5.2230192522984e-05 BETTER
I0325 19:39:52.605503 1068416 finetune.py:68] layer 10_v @ epoch 2 new loss 3.62045357178431e-05 old loss 3.790959090110846e-05 BETTER
I0325 19:40:15.992965 1069334 finetune.py:68] layer 11_v @ epoch 1 new loss 3.344250944792293e-05 old loss 3.615567766246386e-05 BETTER
I0325 19:40:29.703691 1068416 finetune.py:68] layer 10_v @ epoch 3 new loss 3.5028984711971134e-05 old loss 3.62045357178431e-05 BETTER
I0325 19:40:50.868730 1069334 finetune.py:68] layer 11_v @ epoch 2 new loss 3.195723911630921e-05 old loss 3.344250944792293e-05 BETTER
I0325 19:41:06.612915 1068416 finetune.py:68] layer 10_v @ epoch 4 new loss 3.4124383091693744e-05 old loss 3.5028984711971134e-05 BETTER
I0325 19:41:16.484627 1068416 finetune.py:45] layer 10_q initial loss 4.3317602830938995e-05
I0325 19:41:25.929040 1069334 finetune.py:68] layer 11_v @ epoch 3 new loss 3.0930594220990315e-05 old loss 3.195723911630921e-05 BETTER
I0325 19:41:52.843613 1068416 finetune.py:68] layer 10_q @ epoch 0 new loss 4.089179856237024e-05 old loss 4.3317602830938995e-05 BETTER
I0325 19:42:01.064662 1069334 finetune.py:68] layer 11_v @ epoch 4 new loss 3.015665424754843e-05 old loss 3.0930594220990315e-05 BETTER
I0325 19:42:10.473441 1069334 finetune.py:45] layer 11_q initial loss 4.006002927781083e-05
I0325 19:42:29.750978 1068416 finetune.py:68] layer 10_q @ epoch 1 new loss 3.9909122278913856e-05 old loss 4.089179856237024e-05 BETTER
I0325 19:42:44.417795 1069334 finetune.py:68] layer 11_q @ epoch 0 new loss 3.806612585322e-05 old loss 4.006002927781083e-05 BETTER
I0325 19:43:06.720166 1068416 finetune.py:68] layer 10_q @ epoch 2 new loss 3.915916749974713e-05 old loss 3.9909122278913856e-05 BETTER
I0325 19:43:19.049746 1069334 finetune.py:68] layer 11_q @ epoch 1 new loss 3.721442772075534e-05 old loss 3.806612585322e-05 BETTER
I0325 19:43:43.675390 1068416 finetune.py:68] layer 10_q @ epoch 3 new loss 3.8549056625925004e-05 old loss 3.915916749974713e-05 BETTER
I0325 19:43:53.727932 1069334 finetune.py:68] layer 11_q @ epoch 2 new loss 3.6541223380481824e-05 old loss 3.721442772075534e-05 BETTER
I0325 19:44:20.699727 1068416 finetune.py:68] layer 10_q @ epoch 4 new loss 3.8032907468732446e-05 old loss 3.8549056625925004e-05 BETTER
I0325 19:44:28.455408 1069334 finetune.py:68] layer 11_q @ epoch 3 new loss 3.601282151066698e-05 old loss 3.6541223380481824e-05 BETTER
I0325 19:44:28.620648 1068416 finetune.py:45] layer 10_k initial loss 3.9996448322199285e-05
I0325 19:45:03.140810 1069334 finetune.py:68] layer 11_q @ epoch 4 new loss 3.555543298716657e-05 old loss 3.601282151066698e-05 BETTER
I0325 19:45:04.769274 1068416 finetune.py:68] layer 10_k @ epoch 0 new loss 3.9356837078230456e-05 old loss 3.9996448322199285e-05 BETTER
I0325 19:45:10.910568 1069334 finetune.py:45] layer 11_k initial loss 3.783916690736078e-05
I0325 19:45:41.617912 1068416 finetune.py:68] layer 10_k @ epoch 1 new loss 3.894443216267973e-05 old loss 3.9356837078230456e-05 BETTER
I0325 19:45:44.706773 1069334 finetune.py:68] layer 11_k @ epoch 0 new loss 3.709695738507435e-05 old loss 3.783916690736078e-05 BETTER
I0325 19:46:18.560986 1068416 finetune.py:68] layer 10_k @ epoch 2 new loss 3.8581016269745305e-05 old loss 3.894443216267973e-05 BETTER
I0325 19:46:19.149769 1069334 finetune.py:68] layer 11_k @ epoch 1 new loss 3.668449426186271e-05 old loss 3.709695738507435e-05 BETTER
I0325 19:46:53.653423 1069334 finetune.py:68] layer 11_k @ epoch 2 new loss 3.635285247582942e-05 old loss 3.668449426186271e-05 BETTER
I0325 19:46:55.455407 1068416 finetune.py:68] layer 10_k @ epoch 3 new loss 3.8256232073763385e-05 old loss 3.8581016269745305e-05 BETTER
I0325 19:47:28.271618 1069334 finetune.py:68] layer 11_k @ epoch 3 new loss 3.6045290471520275e-05 old loss 3.635285247582942e-05 BETTER
I0325 19:47:32.355168 1068416 finetune.py:68] layer 10_k @ epoch 4 new loss 3.79675293515902e-05 old loss 3.8256232073763385e-05 BETTER
I0325 19:47:42.152249 1068416 finetune.py:45] layer 10_o initial loss 0.00012817322567570955
I0325 19:48:02.786511 1069334 finetune.py:68] layer 11_k @ epoch 4 new loss 3.577351162675768e-05 old loss 3.6045290471520275e-05 BETTER
I0325 19:48:12.225265 1069334 finetune.py:45] layer 11_o initial loss 0.00014214625116437674
I0325 19:48:17.472664 1068416 finetune.py:68] layer 10_o @ epoch 0 new loss 0.00011708412785083055 old loss 0.00012817322567570955 BETTER
I0325 19:48:45.398291 1069334 finetune.py:68] layer 11_o @ epoch 0 new loss 0.00012812622298952192 old loss 0.00014214625116437674 BETTER
I0325 19:48:53.494803 1068416 finetune.py:68] layer 10_o @ epoch 1 new loss 0.00011167436605319381 old loss 0.00011708412785083055 BETTER
I0325 19:49:19.183377 1069334 finetune.py:68] layer 11_o @ epoch 1 new loss 0.00012142302875872701 old loss 0.00012812622298952192 BETTER
I0325 19:49:29.547352 1068416 finetune.py:68] layer 10_o @ epoch 2 new loss 0.00010793218825710937 old loss 0.00011167436605319381 BETTER
I0325 19:49:53.045980 1069334 finetune.py:68] layer 11_o @ epoch 2 new loss 0.00011682182230288163 old loss 0.00012142302875872701 BETTER
I0325 19:50:05.647614 1068416 finetune.py:68] layer 10_o @ epoch 3 new loss 0.00010502548684598878 old loss 0.00010793218825710937 BETTER
I0325 19:50:27.146635 1069334 finetune.py:68] layer 11_o @ epoch 3 new loss 0.00011332579742884263 old loss 0.00011682182230288163 BETTER
I0325 19:50:41.690749 1068416 finetune.py:68] layer 10_o @ epoch 4 new loss 0.00010265370656270534 old loss 0.00010502548684598878 BETTER
I0325 19:51:00.935459 1069334 finetune.py:68] layer 11_o @ epoch 4 new loss 0.00011048281157854944 old loss 0.00011332579742884263 BETTER
I0325 19:51:04.220121 1068416 finetune.py:45] layer 10_up initial loss 0.0001761667081154883
I0325 19:51:23.123260 1069334 finetune.py:45] layer 11_up initial loss 0.00018926964548882097
I0325 19:51:36.370343 1068416 finetune.py:68] layer 10_up @ epoch 0 new loss 0.000171486652106978 old loss 0.0001761667081154883 BETTER
I0325 19:51:53.896512 1069334 finetune.py:68] layer 11_up @ epoch 0 new loss 0.00018373692000750452 old loss 0.00018926964548882097 BETTER
I0325 19:52:09.667155 1068416 finetune.py:68] layer 10_up @ epoch 1 new loss 0.00016846197831910104 old loss 0.000171486652106978 BETTER
I0325 19:52:25.882647 1069334 finetune.py:68] layer 11_up @ epoch 1 new loss 0.00018025323515757918 old loss 0.00018373692000750452 BETTER
I0325 19:52:43.297652 1068416 finetune.py:68] layer 10_up @ epoch 2 new loss 0.00016600257367826998 old loss 0.00016846197831910104 BETTER
I0325 19:52:58.020704 1069334 finetune.py:68] layer 11_up @ epoch 2 new loss 0.0001773992698872462 old loss 0.00018025323515757918 BETTER
I0325 19:53:17.072116 1068416 finetune.py:68] layer 10_up @ epoch 3 new loss 0.0001638194953557104 old loss 0.00016600257367826998 BETTER
I0325 19:53:30.064318 1069334 finetune.py:68] layer 11_up @ epoch 3 new loss 0.0001748862850945443 old loss 0.0001773992698872462 BETTER
I0325 19:53:50.931462 1068416 finetune.py:68] layer 10_up @ epoch 4 new loss 0.00016189126472454518 old loss 0.0001638194953557104 BETTER
I0325 19:54:02.224807 1069334 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00017264674534089863 old loss 0.0001748862850945443 BETTER
I0325 19:54:12.846914 1068416 finetune.py:45] layer 10_gate initial loss 0.0001909642160171643
I0325 19:54:23.945290 1069334 finetune.py:45] layer 11_gate initial loss 0.00020398831111378968
I0325 19:54:43.264007 1068416 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.00018879931303672493 old loss 0.0001909642160171643 BETTER
I0325 19:54:52.510602 1069334 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.00020152205252088606 old loss 0.00020398831111378968 BETTER
I0325 19:55:14.864780 1068416 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.00018709014693740755 old loss 0.00018879931303672493 BETTER
I0325 19:55:22.186434 1069334 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.000199575224542059 old loss 0.00020152205252088606 BETTER
I0325 19:55:46.696251 1068416 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.0001855664886534214 old loss 0.00018709014693740755 BETTER
I0325 19:55:51.957545 1069334 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.0001978722430067137 old loss 0.000199575224542059 BETTER
I0325 19:56:18.320829 1068416 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.000184192496817559 old loss 0.0001855664886534214 BETTER
I0325 19:56:21.712693 1069334 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.00019630012684501708 old loss 0.0001978722430067137 BETTER
I0325 19:56:50.111909 1068416 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.00018289666331838816 old loss 0.000184192496817559 BETTER
I0325 19:56:51.486844 1069334 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.00019486229575704783 old loss 0.00019630012684501708 BETTER
I0325 19:57:13.001271 1068416 finetune.py:45] layer 10_down initial loss 0.0002738760376814753
I0325 19:57:14.159986 1069334 finetune.py:45] layer 11_down initial loss 0.000290036405203864
I0325 19:57:40.795699 1069334 finetune.py:68] layer 11_down @ epoch 0 new loss 0.00028992307488806546 old loss 0.000290036405203864 BETTER
I0325 19:57:41.006971 1068416 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0002737642207648605 old loss 0.0002738760376814753 BETTER
I0325 19:58:08.320652 1069334 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0002898258389905095 old loss 0.00028992307488806546 BETTER
I0325 19:58:10.131433 1068416 finetune.py:68] layer 10_down @ epoch 1 new loss 0.000273669371381402 old loss 0.0002737642207648605 BETTER
I0325 19:58:36.069201 1069334 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0002897384692914784 old loss 0.0002898258389905095 BETTER
I0325 19:58:39.633449 1068416 finetune.py:68] layer 10_down @ epoch 2 new loss 0.00027357760700397193 old loss 0.000273669371381402 BETTER
I0325 19:59:03.859242 1069334 finetune.py:68] layer 11_down @ epoch 3 new loss 0.00028964984812773764 old loss 0.0002897384692914784 BETTER
I0325 19:59:09.202993 1068416 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0002734992594923824 old loss 0.00027357760700397193 BETTER
I0325 19:59:31.614520 1069334 finetune.py:68] layer 11_down @ epoch 4 new loss 0.00028957388713024557 old loss 0.00028964984812773764 BETTER
11_v proxy err 0.04961176961660385 tr(WHW.T) 319.5691223144531
bpp_loss 2.189155340194702
11_q proxy err 0.0034291003830730915 tr(WHW.T) 22218.671875
bpp_loss 2.9593585729599
11_k proxy err 0.0012373273493722081 tr(WHW.T) 18054.130859375
bpp_loss 3.886049747467041
11_o proxy err 0.1162354126572609 tr(WHW.T) 582.8819580078125
bpp_loss 2.3320361375808716
11_up proxy err 0.03346918523311615 tr(WHW.T) 9211.2783203125
bpp_loss 2.466017041887556
11_gate proxy err 0.00889735296368599 tr(WHW.T) 37061.48046875
bpp_loss 2.6959312983921597
11_down proxy err 0.04624433070421219 tr(WHW.T) 6780.79833984375
bpp_loss 2.4693946157182967
I0325 19:59:38.578315 1068416 finetune.py:68] layer 10_down @ epoch 4 new loss 0.00027342865359969437 old loss 0.0002734992594923824 BETTER
10_v proxy err 0.060503650456666946 tr(WHW.T) 251.83889770507812
bpp_loss 2.1825788021087646
10_q proxy err 0.0032048067077994347 tr(WHW.T) 23358.640625
bpp_loss 3.012948513031006
10_k proxy err 0.0010965680703520775 tr(WHW.T) 19800.75390625
bpp_loss 3.8855940103530884
10_o proxy err 0.10187327861785889 tr(WHW.T) 700.62060546875
bpp_loss 2.3130873441696167
10_up proxy err 0.03403979539871216 tr(WHW.T) 9204.58203125
bpp_loss 2.461797305515834
10_gate proxy err 0.008989299647510052 tr(WHW.T) 37459.6640625
bpp_loss 2.7164813450404575
10_down proxy err 0.04799157753586769 tr(WHW.T) 6645.3671875
bpp_loss 2.4624810559409007
I0325 20:00:46.705200 975333 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 63.326326847076416s
I0325 20:00:49.968181 1086587 config.py:54] PyTorch version 2.6.0 available.
W0325 20:00:50.247359 1086587 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 20:00:51.126190 1086587 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 20:00:51.130117 975333 quantize_finetune_llama.py:209] layer 13 gpu 1
I0325 20:00:51.143506 1086587 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 20:00:58.354989 1086587 finetune.py:45] layer 12_v initial loss 5.890406100661494e-05
W0325 20:00:58.355267 1086587 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 20:01:34.181840 1086587 finetune.py:68] layer 12_v @ epoch 0 new loss 4.0501210605725646e-05 old loss 5.890406100661494e-05 BETTER
I0325 20:01:51.306668 975333 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 59.76357436180115s
I0325 20:01:54.804511 1087498 config.py:54] PyTorch version 2.6.0 available.
W0325 20:01:55.092959 1087498 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 20:01:55.975593 1087498 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 20:01:55.979668 975333 quantize_finetune_llama.py:209] layer 14 gpu 0
I0325 20:01:55.992856 1087498 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 20:02:02.936690 1087498 finetune.py:45] layer 13_v initial loss 6.442031008191407e-05
W0325 20:02:02.936935 1087498 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 20:02:11.172044 1086587 finetune.py:68] layer 12_v @ epoch 1 new loss 3.730610842467286e-05 old loss 4.0501210605725646e-05 BETTER
I0325 20:02:36.408523 1087498 finetune.py:68] layer 13_v @ epoch 0 new loss 4.482049189391546e-05 old loss 6.442031008191407e-05 BETTER
I0325 20:02:48.204956 1086587 finetune.py:68] layer 12_v @ epoch 2 new loss 3.5473716707201675e-05 old loss 3.730610842467286e-05 BETTER
I0325 20:03:10.920164 1087498 finetune.py:68] layer 13_v @ epoch 1 new loss 4.1181578126270324e-05 old loss 4.482049189391546e-05 BETTER
I0325 20:03:25.316323 1086587 finetune.py:68] layer 12_v @ epoch 3 new loss 3.416096660657786e-05 old loss 3.5473716707201675e-05 BETTER
I0325 20:03:45.735189 1087498 finetune.py:68] layer 13_v @ epoch 2 new loss 3.9203325286507607e-05 old loss 4.1181578126270324e-05 BETTER
I0325 20:04:02.675028 1086587 finetune.py:68] layer 12_v @ epoch 4 new loss 3.31591181748081e-05 old loss 3.416096660657786e-05 BETTER
I0325 20:04:12.475783 1086587 finetune.py:45] layer 12_q initial loss 4.1229450289392844e-05
I0325 20:04:20.793166 1087498 finetune.py:68] layer 13_v @ epoch 3 new loss 3.786815796047449e-05 old loss 3.9203325286507607e-05 BETTER
I0325 20:04:48.684145 1086587 finetune.py:68] layer 12_q @ epoch 0 new loss 3.911488602170721e-05 old loss 4.1229450289392844e-05 BETTER
I0325 20:04:56.017559 1087498 finetune.py:68] layer 13_v @ epoch 4 new loss 3.688659126055427e-05 old loss 3.786815796047449e-05 BETTER
I0325 20:05:05.487533 1087498 finetune.py:45] layer 13_q initial loss 4.657899626181461e-05
I0325 20:05:25.709318 1086587 finetune.py:68] layer 12_q @ epoch 1 new loss 3.8078978832345456e-05 old loss 3.911488602170721e-05 BETTER
I0325 20:05:39.377132 1087498 finetune.py:68] layer 13_q @ epoch 0 new loss 4.4361393520375714e-05 old loss 4.657899626181461e-05 BETTER
I0325 20:06:02.764562 1086587 finetune.py:68] layer 12_q @ epoch 2 new loss 3.728449883055873e-05 old loss 3.8078978832345456e-05 BETTER
I0325 20:06:14.090264 1087498 finetune.py:68] layer 13_q @ epoch 1 new loss 4.341779640526511e-05 old loss 4.4361393520375714e-05 BETTER
I0325 20:06:39.744271 1086587 finetune.py:68] layer 12_q @ epoch 3 new loss 3.661011578515172e-05 old loss 3.728449883055873e-05 BETTER
I0325 20:06:48.782126 1087498 finetune.py:68] layer 13_q @ epoch 2 new loss 4.268460179446265e-05 old loss 4.341779640526511e-05 BETTER
I0325 20:07:16.731198 1086587 finetune.py:68] layer 12_q @ epoch 4 new loss 3.603078221203759e-05 old loss 3.661011578515172e-05 BETTER
I0325 20:07:23.464206 1087498 finetune.py:68] layer 13_q @ epoch 3 new loss 4.207164965919219e-05 old loss 4.268460179446265e-05 BETTER
I0325 20:07:24.738667 1086587 finetune.py:45] layer 12_k initial loss 3.9513150113634765e-05
I0325 20:07:58.180545 1087498 finetune.py:68] layer 13_q @ epoch 4 new loss 4.154468115302734e-05 old loss 4.207164965919219e-05 BETTER
I0325 20:08:00.993445 1086587 finetune.py:68] layer 12_k @ epoch 0 new loss 3.8188481994438916e-05 old loss 3.9513150113634765e-05 BETTER
I0325 20:08:06.007480 1087498 finetune.py:45] layer 13_k initial loss 4.387504668557085e-05
I0325 20:08:37.799707 1086587 finetune.py:68] layer 12_k @ epoch 1 new loss 3.769538307096809e-05 old loss 3.8188481994438916e-05 BETTER
I0325 20:08:39.967220 1087498 finetune.py:68] layer 13_k @ epoch 0 new loss 4.320639345678501e-05 old loss 4.387504668557085e-05 BETTER
I0325 20:09:14.436871 1087498 finetune.py:68] layer 13_k @ epoch 1 new loss 4.2784595279954374e-05 old loss 4.320639345678501e-05 BETTER
I0325 20:09:14.776695 1086587 finetune.py:68] layer 12_k @ epoch 2 new loss 3.727251532836817e-05 old loss 3.769538307096809e-05 BETTER
I0325 20:09:48.975118 1087498 finetune.py:68] layer 13_k @ epoch 2 new loss 4.2424871935509145e-05 old loss 4.2784595279954374e-05 BETTER
I0325 20:09:51.587065 1086587 finetune.py:68] layer 12_k @ epoch 3 new loss 3.691627352964133e-05 old loss 3.727251532836817e-05 BETTER
I0325 20:10:23.614126 1087498 finetune.py:68] layer 13_k @ epoch 3 new loss 4.2097148252651095e-05 old loss 4.2424871935509145e-05 BETTER
I0325 20:10:28.519311 1086587 finetune.py:68] layer 12_k @ epoch 4 new loss 3.656651824712753e-05 old loss 3.691627352964133e-05 BETTER
I0325 20:10:38.501197 1086587 finetune.py:45] layer 12_o initial loss 0.00013830210082232952
I0325 20:10:58.160932 1087498 finetune.py:68] layer 13_k @ epoch 4 new loss 4.180489122518338e-05 old loss 4.2097148252651095e-05 BETTER
I0325 20:11:07.615160 1087498 finetune.py:45] layer 13_o initial loss 0.00018525360792409629
I0325 20:11:13.947821 1086587 finetune.py:68] layer 12_o @ epoch 0 new loss 0.00012567474914249033 old loss 0.00013830210082232952 BETTER
I0325 20:11:40.947626 1087498 finetune.py:68] layer 13_o @ epoch 0 new loss 0.00016611622413620353 old loss 0.00018525360792409629 BETTER
I0325 20:11:50.053911 1086587 finetune.py:68] layer 12_o @ epoch 1 new loss 0.00011976577661698684 old loss 0.00012567474914249033 BETTER
I0325 20:12:14.953834 1087498 finetune.py:68] layer 13_o @ epoch 1 new loss 0.0001568093430250883 old loss 0.00016611622413620353 BETTER
I0325 20:12:26.104334 1086587 finetune.py:68] layer 12_o @ epoch 2 new loss 0.00011553469084901735 old loss 0.00011976577661698684 BETTER
I0325 20:12:48.879365 1087498 finetune.py:68] layer 13_o @ epoch 2 new loss 0.0001502626109868288 old loss 0.0001568093430250883 BETTER
I0325 20:13:02.138834 1086587 finetune.py:68] layer 12_o @ epoch 3 new loss 0.00011221475870115682 old loss 0.00011553469084901735 BETTER
I0325 20:13:23.292541 1087498 finetune.py:68] layer 13_o @ epoch 3 new loss 0.00014524158905260265 old loss 0.0001502626109868288 BETTER
I0325 20:13:38.383702 1086587 finetune.py:68] layer 12_o @ epoch 4 new loss 0.00010948062117677182 old loss 0.00011221475870115682 BETTER
I0325 20:13:57.069616 1087498 finetune.py:68] layer 13_o @ epoch 4 new loss 0.00014116162492427975 old loss 0.00014524158905260265 BETTER
I0325 20:14:00.184697 1086587 finetune.py:45] layer 12_up initial loss 0.00019161275122314692
I0325 20:14:18.633196 1087498 finetune.py:45] layer 13_up initial loss 0.00023700197925791144
I0325 20:14:32.374732 1086587 finetune.py:68] layer 12_up @ epoch 0 new loss 0.00018558358715381473 old loss 0.00019161275122314692 BETTER
I0325 20:14:49.575427 1087498 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00022926995006855577 old loss 0.00023700197925791144 BETTER
I0325 20:15:05.911600 1086587 finetune.py:68] layer 12_up @ epoch 1 new loss 0.00018180618644692004 old loss 0.00018558358715381473 BETTER
I0325 20:15:21.686326 1087498 finetune.py:68] layer 13_up @ epoch 1 new loss 0.00022447580704465508 old loss 0.00022926995006855577 BETTER
I0325 20:15:39.726200 1086587 finetune.py:68] layer 12_up @ epoch 2 new loss 0.00017869997827801853 old loss 0.00018180618644692004 BETTER
I0325 20:15:53.786619 1087498 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0002205114287789911 old loss 0.00022447580704465508 BETTER
I0325 20:16:13.736691 1086587 finetune.py:68] layer 12_up @ epoch 3 new loss 0.00017601392755750567 old loss 0.00017869997827801853 BETTER
I0325 20:16:25.967771 1087498 finetune.py:68] layer 13_up @ epoch 3 new loss 0.00021708493295591325 old loss 0.0002205114287789911 BETTER
I0325 20:16:47.558476 1086587 finetune.py:68] layer 12_up @ epoch 4 new loss 0.00017360503261443228 old loss 0.00017601392755750567 BETTER
I0325 20:16:58.166881 1087498 finetune.py:68] layer 13_up @ epoch 4 new loss 0.00021403735445346683 old loss 0.00021708493295591325 BETTER
I0325 20:17:09.781122 1086587 finetune.py:45] layer 12_gate initial loss 0.00020898948423564434
I0325 20:17:19.939274 1087498 finetune.py:45] layer 13_gate initial loss 0.00025419576559215784
I0325 20:17:40.027574 1086587 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.00020624266471713781 old loss 0.00020898948423564434 BETTER
I0325 20:17:48.687072 1087498 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.00025082117645069957 old loss 0.00025419576559215784 BETTER
I0325 20:18:11.272511 1086587 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.00020409243006724864 old loss 0.00020624266471713781 BETTER
I0325 20:18:18.336563 1087498 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0002481576520949602 old loss 0.00025082117645069957 BETTER
I0325 20:18:42.687670 1086587 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.00020219579164404422 old loss 0.00020409243006724864 BETTER
I0325 20:18:48.016283 1087498 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.00024579689488746226 old loss 0.0002481576520949602 BETTER
I0325 20:19:13.991007 1086587 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.00020045865676365793 old loss 0.00020219579164404422 BETTER
I0325 20:19:17.867472 1087498 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.0002436423528706655 old loss 0.00024579689488746226 BETTER
I0325 20:19:45.393294 1086587 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.00019886434893123806 old loss 0.00020045865676365793 BETTER
I0325 20:19:47.802494 1087498 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.00024167609808500856 old loss 0.0002436423528706655 BETTER
I0325 20:20:08.439610 1086587 finetune.py:45] layer 12_down initial loss 0.0003016219416167587
I0325 20:20:10.496378 1087498 finetune.py:45] layer 13_down initial loss 0.00036438691313378513
I0325 20:20:36.267618 1086587 finetune.py:68] layer 12_down @ epoch 0 new loss 0.00030150834936648607 old loss 0.0003016219416167587 BETTER
I0325 20:20:36.909803 1087498 finetune.py:68] layer 13_down @ epoch 0 new loss 0.00036424456629902124 old loss 0.00036438691313378513 BETTER
I0325 20:21:04.537101 1087498 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0003641156363300979 old loss 0.00036424456629902124 BETTER
I0325 20:21:04.777526 1086587 finetune.py:68] layer 12_down @ epoch 1 new loss 0.0003014138783328235 old loss 0.00030150834936648607 BETTER
I0325 20:21:32.052633 1087498 finetune.py:68] layer 13_down @ epoch 2 new loss 0.0003639898495748639 old loss 0.0003641156363300979 BETTER
I0325 20:21:33.798381 1086587 finetune.py:68] layer 12_down @ epoch 2 new loss 0.00030131355742923915 old loss 0.0003014138783328235 BETTER
I0325 20:21:59.846922 1087498 finetune.py:68] layer 13_down @ epoch 3 new loss 0.0003638826310634613 old loss 0.0003639898495748639 BETTER
I0325 20:22:03.052499 1086587 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0003012308443430811 old loss 0.00030131355742923915 BETTER
I0325 20:22:27.778031 1087498 finetune.py:68] layer 13_down @ epoch 4 new loss 0.0003637778863776475 old loss 0.0003638826310634613 BETTER
13_v proxy err 0.056453973054885864 tr(WHW.T) 280.153564453125
bpp_loss 2.2467914819717407
13_q proxy err 0.0036169711966067553 tr(WHW.T) 20912.23828125
bpp_loss 2.9894498586654663
13_k proxy err 0.0012436134275048971 tr(WHW.T) 17839.908203125
bpp_loss 3.8994380235671997
13_o proxy err 0.10335276275873184 tr(WHW.T) 691.0519409179688
bpp_loss 2.3603811264038086
13_up proxy err 0.030218536034226418 tr(WHW.T) 10030.8271484375
bpp_loss 2.487630162920271
13_gate proxy err 0.008302019909024239 tr(WHW.T) 39065.61328125
bpp_loss 2.681422914777483
13_down proxy err 0.046039190143346786 tr(WHW.T) 6687.49853515625
bpp_loss 2.4809188842773438
I0325 20:22:32.190913 1086587 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0003011506632901728 old loss 0.0003012308443430811 BETTER
12_v proxy err 0.04661518335342407 tr(WHW.T) 363.6233825683594
bpp_loss 2.3052966594696045
12_q proxy err 0.0024443280417472124 tr(WHW.T) 34153.7734375
bpp_loss 3.017366051673889
12_k proxy err 0.00104459747672081 tr(WHW.T) 23145.99609375
bpp_loss 3.8876867294311523
12_o proxy err 0.09214373677968979 tr(WHW.T) 794.1841430664062
bpp_loss 2.377172350883484
12_up proxy err 0.03015110455453396 tr(WHW.T) 10032.9443359375
bpp_loss 2.4839745930262973
12_gate proxy err 0.00859588012099266 tr(WHW.T) 37415.03125
bpp_loss 2.6766439165387834
12_down proxy err 0.044534292072057724 tr(WHW.T) 6941.32421875
bpp_loss 2.4808134010859897
I0325 20:23:39.872258 975333 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 62.80493783950806s
I0325 20:23:43.054715 1104711 config.py:54] PyTorch version 2.6.0 available.
W0325 20:23:43.338232 1104711 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 20:23:44.215755 1104711 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 20:23:44.219719 975333 quantize_finetune_llama.py:209] layer 15 gpu 1
I0325 20:23:44.232769 1104711 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 20:23:51.451155 1104711 finetune.py:45] layer 14_v initial loss 6.569765537278727e-05
W0325 20:23:51.451365 1104711 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 20:24:27.012372 1104711 finetune.py:68] layer 14_v @ epoch 0 new loss 4.804741183761507e-05 old loss 6.569765537278727e-05 BETTER
I0325 20:24:44.325501 975333 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 59.71700954437256s
I0325 20:24:47.768200 1105617 config.py:54] PyTorch version 2.6.0 available.
W0325 20:24:48.072358 1105617 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 20:24:49.037631 1105617 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 20:24:49.041445 975333 quantize_finetune_llama.py:209] layer 16 gpu 0
I0325 20:24:49.054188 1105617 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 20:24:56.059881 1105617 finetune.py:45] layer 15_v initial loss 7.239482511067763e-05
W0325 20:24:56.060074 1105617 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 20:25:03.834201 1104711 finetune.py:68] layer 14_v @ epoch 1 new loss 4.455005910131149e-05 old loss 4.804741183761507e-05 BETTER
I0325 20:25:29.540351 1105617 finetune.py:68] layer 15_v @ epoch 0 new loss 5.131576835992746e-05 old loss 7.239482511067763e-05 BETTER
I0325 20:25:40.823301 1104711 finetune.py:68] layer 14_v @ epoch 2 new loss 4.2552477680146694e-05 old loss 4.455005910131149e-05 BETTER
I0325 20:26:04.193093 1105617 finetune.py:68] layer 15_v @ epoch 1 new loss 4.7648107283748686e-05 old loss 5.131576835992746e-05 BETTER
I0325 20:26:18.052994 1104711 finetune.py:68] layer 14_v @ epoch 3 new loss 4.113708564545959e-05 old loss 4.2552477680146694e-05 BETTER
I0325 20:26:39.045940 1105617 finetune.py:68] layer 15_v @ epoch 2 new loss 4.5574302930617705e-05 old loss 4.7648107283748686e-05 BETTER
I0325 20:26:55.245541 1104711 finetune.py:68] layer 14_v @ epoch 4 new loss 4.005639493698254e-05 old loss 4.113708564545959e-05 BETTER
I0325 20:27:05.286576 1104711 finetune.py:45] layer 14_q initial loss 5.312752182362601e-05
I0325 20:27:14.173538 1105617 finetune.py:68] layer 15_v @ epoch 3 new loss 4.411114059621468e-05 old loss 4.5574302930617705e-05 BETTER
I0325 20:27:41.404747 1104711 finetune.py:68] layer 14_q @ epoch 0 new loss 5.018331648898311e-05 old loss 5.312752182362601e-05 BETTER
I0325 20:27:49.362588 1105617 finetune.py:68] layer 15_v @ epoch 4 new loss 4.3009393266402185e-05 old loss 4.411114059621468e-05 BETTER
I0325 20:27:59.037275 1105617 finetune.py:45] layer 15_q initial loss 5.103177682030946e-05
I0325 20:28:18.418545 1104711 finetune.py:68] layer 14_q @ epoch 1 new loss 4.8950834752758965e-05 old loss 5.018331648898311e-05 BETTER
I0325 20:28:32.827519 1105617 finetune.py:68] layer 15_q @ epoch 0 new loss 4.886530223302543e-05 old loss 5.103177682030946e-05 BETTER
I0325 20:28:55.295893 1104711 finetune.py:68] layer 14_q @ epoch 2 new loss 4.802638432011008e-05 old loss 4.8950834752758965e-05 BETTER
I0325 20:29:07.406230 1105617 finetune.py:68] layer 15_q @ epoch 1 new loss 4.780633753398433e-05 old loss 4.886530223302543e-05 BETTER
I0325 20:29:32.193624 1104711 finetune.py:68] layer 14_q @ epoch 3 new loss 4.727565828943625e-05 old loss 4.802638432011008e-05 BETTER
I0325 20:29:41.916111 1105617 finetune.py:68] layer 15_q @ epoch 2 new loss 4.698587872553617e-05 old loss 4.780633753398433e-05 BETTER
I0325 20:30:09.128951 1104711 finetune.py:68] layer 14_q @ epoch 4 new loss 4.663982326746918e-05 old loss 4.727565828943625e-05 BETTER
I0325 20:30:16.649354 1105617 finetune.py:68] layer 15_q @ epoch 3 new loss 4.632025593309663e-05 old loss 4.698587872553617e-05 BETTER
I0325 20:30:17.357720 1104711 finetune.py:45] layer 14_k initial loss 5.0610153266461566e-05
I0325 20:30:51.446877 1105617 finetune.py:68] layer 15_q @ epoch 4 new loss 4.575070852297358e-05 old loss 4.632025593309663e-05 BETTER
I0325 20:30:53.291687 1104711 finetune.py:68] layer 14_k @ epoch 0 new loss 4.950864604325034e-05 old loss 5.0610153266461566e-05 BETTER
I0325 20:30:59.454526 1105617 finetune.py:45] layer 15_k initial loss 4.8656384024070576e-05
I0325 20:31:30.106587 1104711 finetune.py:68] layer 14_k @ epoch 1 new loss 4.895945676253177e-05 old loss 4.950864604325034e-05 BETTER
I0325 20:31:33.229941 1105617 finetune.py:68] layer 15_k @ epoch 0 new loss 4.7945461119525135e-05 old loss 4.8656384024070576e-05 BETTER
I0325 20:32:06.948756 1104711 finetune.py:68] layer 14_k @ epoch 2 new loss 4.849319520872086e-05 old loss 4.895945676253177e-05 BETTER
I0325 20:32:07.885751 1105617 finetune.py:68] layer 15_k @ epoch 1 new loss 4.74836997454986e-05 old loss 4.7945461119525135e-05 BETTER
I0325 20:32:42.575884 1105617 finetune.py:68] layer 15_k @ epoch 2 new loss 4.709756831289269e-05 old loss 4.74836997454986e-05 BETTER
I0325 20:32:43.915955 1104711 finetune.py:68] layer 14_k @ epoch 3 new loss 4.8092140787048265e-05 old loss 4.849319520872086e-05 BETTER
I0325 20:33:17.281908 1105617 finetune.py:68] layer 15_k @ epoch 3 new loss 4.674108640756458e-05 old loss 4.709756831289269e-05 BETTER
I0325 20:33:20.948274 1104711 finetune.py:68] layer 14_k @ epoch 4 new loss 4.7728928620927036e-05 old loss 4.8092140787048265e-05 BETTER
I0325 20:33:31.294285 1104711 finetune.py:45] layer 14_o initial loss 0.00019480634364299476
I0325 20:33:51.948074 1105617 finetune.py:68] layer 15_k @ epoch 4 new loss 4.64436125184875e-05 old loss 4.674108640756458e-05 BETTER
I0325 20:34:01.655010 1105617 finetune.py:45] layer 15_o initial loss 0.00017065840074792504
I0325 20:34:06.826686 1104711 finetune.py:68] layer 14_o @ epoch 0 new loss 0.0001762606989359483 old loss 0.00019480634364299476 BETTER
I0325 20:34:34.954865 1105617 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0001547662541270256 old loss 0.00017065840074792504 BETTER
I0325 20:34:42.941480 1104711 finetune.py:68] layer 14_o @ epoch 1 new loss 0.00016716295795049518 old loss 0.0001762606989359483 BETTER
I0325 20:35:08.843518 1105617 finetune.py:68] layer 15_o @ epoch 1 new loss 0.00014750414993613958 old loss 0.0001547662541270256 BETTER
I0325 20:35:18.937247 1104711 finetune.py:68] layer 14_o @ epoch 2 new loss 0.00016068499826360494 old loss 0.00016716295795049518 BETTER
I0325 20:35:42.819177 1105617 finetune.py:68] layer 15_o @ epoch 2 new loss 0.00014230114175006747 old loss 0.00014750414993613958 BETTER
I0325 20:35:54.901449 1104711 finetune.py:68] layer 14_o @ epoch 3 new loss 0.00015565866488032043 old loss 0.00016068499826360494 BETTER
I0325 20:36:16.608788 1105617 finetune.py:68] layer 15_o @ epoch 3 new loss 0.0001382799819111824 old loss 0.00014230114175006747 BETTER
I0325 20:36:31.149441 1104711 finetune.py:68] layer 14_o @ epoch 4 new loss 0.0001515678159194067 old loss 0.00015565866488032043 BETTER
I0325 20:36:50.574759 1105617 finetune.py:68] layer 15_o @ epoch 4 new loss 0.00013496440078597516 old loss 0.0001382799819111824 BETTER
I0325 20:36:53.253784 1104711 finetune.py:45] layer 14_up initial loss 0.00026708244695328176
I0325 20:37:12.463066 1105617 finetune.py:45] layer 15_up initial loss 0.0002740460040513426
I0325 20:37:25.767463 1104711 finetune.py:68] layer 14_up @ epoch 0 new loss 0.00025793808163143694 old loss 0.00026708244695328176 BETTER
I0325 20:37:43.206867 1105617 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0002634515112731606 old loss 0.0002740460040513426 BETTER
I0325 20:37:59.254063 1104711 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0002522790164221078 old loss 0.00025793808163143694 BETTER
I0325 20:38:15.289382 1105617 finetune.py:68] layer 15_up @ epoch 1 new loss 0.00025724549777805805 old loss 0.0002634515112731606 BETTER
I0325 20:38:33.109593 1104711 finetune.py:68] layer 14_up @ epoch 2 new loss 0.00024768870207481086 old loss 0.0002522790164221078 BETTER
I0325 20:38:47.438323 1105617 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0002522478753235191 old loss 0.00025724549777805805 BETTER
I0325 20:39:07.048440 1104711 finetune.py:68] layer 14_up @ epoch 3 new loss 0.00024375077919103205 old loss 0.00024768870207481086 BETTER
I0325 20:39:19.627312 1105617 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0002479783142916858 old loss 0.0002522478753235191 BETTER
I0325 20:39:41.081864 1104711 finetune.py:68] layer 14_up @ epoch 4 new loss 0.00024023232981562614 old loss 0.00024375077919103205 BETTER
I0325 20:39:51.849762 1105617 finetune.py:68] layer 15_up @ epoch 4 new loss 0.0002441788965370506 old loss 0.0002479783142916858 BETTER
I0325 20:40:03.212347 1104711 finetune.py:45] layer 14_gate initial loss 0.0002833919716067612
I0325 20:40:13.946783 1105617 finetune.py:45] layer 15_gate initial loss 0.0002930104383267462
I0325 20:40:33.747542 1104711 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.00027952250093221664 old loss 0.0002833919716067612 BETTER
I0325 20:40:42.778302 1105617 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.00028882717015221715 old loss 0.0002930104383267462 BETTER
I0325 20:41:05.128288 1104711 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0002764569653663784 old loss 0.00027952250093221664 BETTER
I0325 20:41:12.319051 1105617 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.00028540752828121185 old loss 0.00028882717015221715 BETTER
I0325 20:41:36.578352 1104711 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.00027374777710065246 old loss 0.0002764569653663784 BETTER
I0325 20:41:42.026136 1105617 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.00028238349477760494 old loss 0.00028540752828121185 BETTER
I0325 20:42:08.194166 1104711 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.00027126300847157836 old loss 0.00027374777710065246 BETTER
I0325 20:42:11.856511 1105617 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0002796280023176223 old loss 0.00028238349477760494 BETTER
I0325 20:42:39.927098 1104711 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0002689810062292963 old loss 0.00027126300847157836 BETTER
I0325 20:42:41.703399 1105617 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0002771074359770864 old loss 0.0002796280023176223 BETTER
I0325 20:43:03.044032 1104711 finetune.py:45] layer 14_down initial loss 0.0004133698530495167
I0325 20:43:04.394852 1105617 finetune.py:45] layer 15_down initial loss 0.00046036296407692134
I0325 20:43:30.914895 1104711 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0004131933965254575 old loss 0.0004133698530495167 BETTER
I0325 20:43:31.147810 1105617 finetune.py:68] layer 15_down @ epoch 0 new loss 0.00046020562876947224 old loss 0.00046036296407692134 BETTER
I0325 20:43:58.864111 1105617 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00046006793854758143 old loss 0.00046020562876947224 BETTER
I0325 20:44:00.125508 1104711 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0004130264569539577 old loss 0.0004131933965254575 BETTER
I0325 20:44:26.624461 1105617 finetune.py:68] layer 15_down @ epoch 2 new loss 0.000459936069091782 old loss 0.00046006793854758143 BETTER
I0325 20:44:29.468517 1104711 finetune.py:68] layer 14_down @ epoch 2 new loss 0.0004128805303480476 old loss 0.0004130264569539577 BETTER
I0325 20:44:54.341824 1105617 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0004598127561621368 old loss 0.000459936069091782 BETTER
I0325 20:44:59.028232 1104711 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0004127314896322787 old loss 0.0004128805303480476 BETTER
I0325 20:45:22.047269 1105617 finetune.py:68] layer 15_down @ epoch 4 new loss 0.00045970178325660527 old loss 0.0004598127561621368 BETTER
15_v proxy err 0.057602886110544205 tr(WHW.T) 284.0271301269531
bpp_loss 2.3007348775863647
15_q proxy err 0.002806036500260234 tr(WHW.T) 28102.138671875
bpp_loss 3.0839407444000244
15_k proxy err 0.0012011121725663543 tr(WHW.T) 18919.7421875
bpp_loss 3.8970730304718018
15_o proxy err 0.09338708221912384 tr(WHW.T) 838.4528198242188
bpp_loss 2.3807982206344604
15_up proxy err 0.03362263739109039 tr(WHW.T) 9003.9921875
bpp_loss 2.475524766104562
15_gate proxy err 0.007198262494057417 tr(WHW.T) 45968.95703125
bpp_loss 2.746112414768764
15_down proxy err 0.047591835260391235 tr(WHW.T) 6529.00341796875
bpp_loss 2.471566472734724
I0325 20:45:28.479692 1104711 finetune.py:68] layer 14_down @ epoch 4 new loss 0.000412603811128065 old loss 0.0004127314896322787 BETTER
14_v proxy err 0.05424364656209946 tr(WHW.T) 281.3382873535156
bpp_loss 2.2389087677001953
14_q proxy err 0.003553260350599885 tr(WHW.T) 20906.17578125
bpp_loss 2.961079478263855
14_k proxy err 0.0011619885917752981 tr(WHW.T) 18674.810546875
bpp_loss 3.8502085208892822
14_o proxy err 0.10595621168613434 tr(WHW.T) 704.8590087890625
bpp_loss 2.3516077995300293
14_up proxy err 0.03276130557060242 tr(WHW.T) 9182.171875
bpp_loss 2.482362747192383
14_gate proxy err 0.007747517433017492 tr(WHW.T) 41940.68359375
bpp_loss 2.7098864146641324
14_down proxy err 0.04713796079158783 tr(WHW.T) 6510.36572265625
bpp_loss 2.476783071245466
I0325 20:46:36.176578 975333 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 62.81927704811096s
I0325 20:46:39.439673 1121353 config.py:54] PyTorch version 2.6.0 available.
W0325 20:46:39.723038 1121353 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 20:46:40.602524 1121353 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 20:46:40.606525 975333 quantize_finetune_llama.py:209] layer 17 gpu 1
I0325 20:46:40.625278 1121353 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 20:46:47.917015 1121353 finetune.py:45] layer 16_v initial loss 8.035101200221106e-05
W0325 20:46:47.917233 1121353 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 20:47:23.587554 1121353 finetune.py:68] layer 16_v @ epoch 0 new loss 5.688067903975025e-05 old loss 8.035101200221106e-05 BETTER
I0325 20:47:41.566159 975333 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 60.547457695007324s
I0325 20:47:45.051277 1122196 config.py:54] PyTorch version 2.6.0 available.
W0325 20:47:45.332244 1122196 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 20:47:46.238027 1122196 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 20:47:46.241938 975333 quantize_finetune_llama.py:209] layer 18 gpu 0
I0325 20:47:46.256195 1122196 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 20:47:53.257476 1122196 finetune.py:45] layer 17_v initial loss 7.468448166036978e-05
W0325 20:47:53.257692 1122196 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 20:48:00.468552 1121353 finetune.py:68] layer 16_v @ epoch 1 new loss 5.245173451839946e-05 old loss 5.688067903975025e-05 BETTER
I0325 20:48:26.835329 1122196 finetune.py:68] layer 17_v @ epoch 0 new loss 5.0223690777784213e-05 old loss 7.468448166036978e-05 BETTER
I0325 20:48:37.465655 1121353 finetune.py:68] layer 16_v @ epoch 2 new loss 4.984426050214097e-05 old loss 5.245173451839946e-05 BETTER
I0325 20:49:01.367707 1122196 finetune.py:68] layer 17_v @ epoch 1 new loss 4.630458352039568e-05 old loss 5.0223690777784213e-05 BETTER
I0325 20:49:14.492588 1121353 finetune.py:68] layer 16_v @ epoch 3 new loss 4.7999510570662096e-05 old loss 4.984426050214097e-05 BETTER
I0325 20:49:36.287622 1122196 finetune.py:68] layer 17_v @ epoch 2 new loss 4.409439497976564e-05 old loss 4.630458352039568e-05 BETTER
I0325 20:49:51.577646 1121353 finetune.py:68] layer 16_v @ epoch 4 new loss 4.65889424958732e-05 old loss 4.7999510570662096e-05 BETTER
I0325 20:50:01.405322 1121353 finetune.py:45] layer 16_q initial loss 5.6736811529845e-05
I0325 20:50:11.296622 1122196 finetune.py:68] layer 17_v @ epoch 3 new loss 4.261845606379211e-05 old loss 4.409439497976564e-05 BETTER
I0325 20:50:37.548721 1121353 finetune.py:68] layer 16_q @ epoch 0 new loss 5.347264595911838e-05 old loss 5.6736811529845e-05 BETTER
I0325 20:50:46.475351 1122196 finetune.py:68] layer 17_v @ epoch 4 new loss 4.1464194509899244e-05 old loss 4.261845606379211e-05 BETTER
I0325 20:50:55.937011 1122196 finetune.py:45] layer 17_q initial loss 4.9427704652771354e-05
I0325 20:51:14.383525 1121353 finetune.py:68] layer 16_q @ epoch 1 new loss 5.205985871725716e-05 old loss 5.347264595911838e-05 BETTER
I0325 20:51:29.638953 1122196 finetune.py:68] layer 17_q @ epoch 0 new loss 4.742237433674745e-05 old loss 4.9427704652771354e-05 BETTER
I0325 20:51:51.138535 1121353 finetune.py:68] layer 16_q @ epoch 2 new loss 5.0972754252143204e-05 old loss 5.205985871725716e-05 BETTER
I0325 20:52:04.082648 1122196 finetune.py:68] layer 17_q @ epoch 1 new loss 4.633546632248908e-05 old loss 4.742237433674745e-05 BETTER
I0325 20:52:27.946931 1121353 finetune.py:68] layer 16_q @ epoch 3 new loss 5.005615821573883e-05 old loss 5.0972754252143204e-05 BETTER
I0325 20:52:38.667330 1122196 finetune.py:68] layer 17_q @ epoch 2 new loss 4.548098149825819e-05 old loss 4.633546632248908e-05 BETTER
I0325 20:53:04.838766 1121353 finetune.py:68] layer 16_q @ epoch 4 new loss 4.929244823870249e-05 old loss 5.005615821573883e-05 BETTER
I0325 20:53:12.838555 1121353 finetune.py:45] layer 16_k initial loss 5.2828891057288274e-05
I0325 20:53:13.331767 1122196 finetune.py:68] layer 17_q @ epoch 3 new loss 4.477418406167999e-05 old loss 4.548098149825819e-05 BETTER
I0325 20:53:48.116046 1122196 finetune.py:68] layer 17_q @ epoch 4 new loss 4.4191107008373365e-05 old loss 4.477418406167999e-05 BETTER
I0325 20:53:49.128010 1121353 finetune.py:68] layer 16_k @ epoch 0 new loss 5.149551361682825e-05 old loss 5.2828891057288274e-05 BETTER
I0325 20:53:55.977618 1122196 finetune.py:45] layer 17_k initial loss 4.714840542874299e-05
I0325 20:54:26.051440 1121353 finetune.py:68] layer 16_k @ epoch 1 new loss 5.084971780888736e-05 old loss 5.149551361682825e-05 BETTER
I0325 20:54:29.859548 1122196 finetune.py:68] layer 17_k @ epoch 0 new loss 4.6235116315074265e-05 old loss 4.714840542874299e-05 BETTER
I0325 20:55:02.978260 1121353 finetune.py:68] layer 16_k @ epoch 2 new loss 5.029978638049215e-05 old loss 5.084971780888736e-05 BETTER
I0325 20:55:04.425423 1122196 finetune.py:68] layer 17_k @ epoch 1 new loss 4.5755459723295644e-05 old loss 4.6235116315074265e-05 BETTER
I0325 20:55:38.923444 1122196 finetune.py:68] layer 17_k @ epoch 2 new loss 4.5339733333094046e-05 old loss 4.5755459723295644e-05 BETTER
I0325 20:55:39.693151 1121353 finetune.py:68] layer 16_k @ epoch 3 new loss 4.9829894123831764e-05 old loss 5.029978638049215e-05 BETTER
I0325 20:56:13.546494 1122196 finetune.py:68] layer 17_k @ epoch 3 new loss 4.4979686208534986e-05 old loss 4.5339733333094046e-05 BETTER
I0325 20:56:16.618679 1121353 finetune.py:68] layer 16_k @ epoch 4 new loss 4.940098006045446e-05 old loss 4.9829894123831764e-05 BETTER
I0325 20:56:26.513772 1121353 finetune.py:45] layer 16_o initial loss 0.0001707501069176942
I0325 20:56:48.062548 1122196 finetune.py:68] layer 17_k @ epoch 4 new loss 4.465872552827932e-05 old loss 4.4979686208534986e-05 BETTER
I0325 20:56:57.567801 1122196 finetune.py:45] layer 17_o initial loss 0.00014331485726870596
I0325 20:57:01.906753 1121353 finetune.py:68] layer 16_o @ epoch 0 new loss 0.00015539793821517378 old loss 0.0001707501069176942 BETTER
I0325 20:57:30.893774 1122196 finetune.py:68] layer 17_o @ epoch 0 new loss 0.00013103305536787957 old loss 0.00014331485726870596 BETTER
I0325 20:57:37.970668 1121353 finetune.py:68] layer 16_o @ epoch 1 new loss 0.00014824981917627156 old loss 0.00015539793821517378 BETTER
I0325 20:58:04.932230 1122196 finetune.py:68] layer 17_o @ epoch 1 new loss 0.0001254266535397619 old loss 0.00013103305536787957 BETTER
I0325 20:58:13.938226 1121353 finetune.py:68] layer 16_o @ epoch 2 new loss 0.00014326728705782443 old loss 0.00014824981917627156 BETTER
I0325 20:58:39.057826 1122196 finetune.py:68] layer 17_o @ epoch 2 new loss 0.00012157895253039896 old loss 0.0001254266535397619 BETTER
I0325 20:58:50.401473 1121353 finetune.py:68] layer 16_o @ epoch 3 new loss 0.0001393647980876267 old loss 0.00014326728705782443 BETTER
I0325 20:59:13.207951 1122196 finetune.py:68] layer 17_o @ epoch 3 new loss 0.00011862160317832604 old loss 0.00012157895253039896 BETTER
I0325 20:59:26.367622 1121353 finetune.py:68] layer 16_o @ epoch 4 new loss 0.00013619268429465592 old loss 0.0001393647980876267 BETTER
I0325 20:59:47.248034 1122196 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0001161911859526299 old loss 0.00011862160317832604 BETTER
I0325 20:59:48.308691 1121353 finetune.py:45] layer 16_up initial loss 0.0002833819598890841
I0325 21:00:08.918851 1122196 finetune.py:45] layer 17_up initial loss 0.0002858242078218609
I0325 21:00:20.540476 1121353 finetune.py:68] layer 16_up @ epoch 0 new loss 0.0002731313288677484 old loss 0.0002833819598890841 BETTER
I0325 21:00:39.730441 1122196 finetune.py:68] layer 17_up @ epoch 0 new loss 0.000274402933428064 old loss 0.0002858242078218609 BETTER
I0325 21:00:53.989493 1121353 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0002672154805622995 old loss 0.0002731313288677484 BETTER
I0325 21:01:11.723808 1122196 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0002680261095520109 old loss 0.000274402933428064 BETTER
I0325 21:01:27.568571 1121353 finetune.py:68] layer 16_up @ epoch 2 new loss 0.00026240936131216586 old loss 0.0002672154805622995 BETTER
I0325 21:01:43.853959 1122196 finetune.py:68] layer 17_up @ epoch 2 new loss 0.00026301006437279284 old loss 0.0002680261095520109 BETTER
I0325 21:02:01.222329 1121353 finetune.py:68] layer 16_up @ epoch 3 new loss 0.00025829655351117253 old loss 0.00026240936131216586 BETTER
I0325 21:02:16.004276 1122196 finetune.py:68] layer 17_up @ epoch 3 new loss 0.00025876195286400616 old loss 0.00026301006437279284 BETTER
I0325 21:02:34.927042 1121353 finetune.py:68] layer 16_up @ epoch 4 new loss 0.00025465694488957524 old loss 0.00025829655351117253 BETTER
I0325 21:02:48.118978 1122196 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0002550571516621858 old loss 0.00025876195286400616 BETTER
I0325 21:02:56.939505 1121353 finetune.py:45] layer 16_gate initial loss 0.000305706518702209
I0325 21:03:10.151226 1122196 finetune.py:45] layer 17_gate initial loss 0.00031523985671810806
I0325 21:03:27.276987 1121353 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.00030171548132784665 old loss 0.000305706518702209 BETTER
I0325 21:03:38.978444 1122196 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.000310923729557544 old loss 0.00031523985671810806 BETTER
I0325 21:03:58.681522 1121353 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.00029848527628928423 old loss 0.00030171548132784665 BETTER
I0325 21:04:08.646240 1122196 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0003075393324252218 old loss 0.000310923729557544 BETTER
I0325 21:04:30.324828 1121353 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.0002955809177365154 old loss 0.00029848527628928423 BETTER
I0325 21:04:38.384183 1122196 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.0003045168414246291 old loss 0.0003075393324252218 BETTER
I0325 21:05:01.925376 1121353 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0002929446054622531 old loss 0.0002955809177365154 BETTER
I0325 21:05:08.157610 1122196 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.00030174385756254196 old loss 0.0003045168414246291 BETTER
I0325 21:05:33.557490 1121353 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.0002905346336774528 old loss 0.0002929446054622531 BETTER
I0325 21:05:38.008765 1122196 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.0002992466324940324 old loss 0.00030174385756254196 BETTER
I0325 21:05:56.664746 1121353 finetune.py:45] layer 16_down initial loss 0.00048495782539248466
I0325 21:06:00.751364 1122196 finetune.py:45] layer 17_down initial loss 0.00053420226322487
I0325 21:06:24.809076 1121353 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0004847990348935127 old loss 0.00048495782539248466 BETTER
I0325 21:06:27.388743 1122196 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0005340175121091306 old loss 0.00053420226322487 BETTER
I0325 21:06:53.688173 1121353 finetune.py:68] layer 16_down @ epoch 1 new loss 0.00048465252621099353 old loss 0.0004847990348935127 BETTER
I0325 21:06:55.071334 1122196 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0005338535411283374 old loss 0.0005340175121091306 BETTER
I0325 21:07:22.778391 1122196 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0005337009206414223 old loss 0.0005338535411283374 BETTER
I0325 21:07:22.817624 1121353 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0004845248186029494 old loss 0.00048465252621099353 BETTER
I0325 21:07:50.416235 1122196 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0005335600581020117 old loss 0.0005337009206414223 BETTER
I0325 21:07:52.007316 1121353 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0004844022623728961 old loss 0.0004845248186029494 BETTER
I0325 21:08:18.310122 1122196 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0005334330489858985 old loss 0.0005335600581020117 BETTER
17_v proxy err 0.058072544634342194 tr(WHW.T) 283.9730224609375
bpp_loss 2.3277868032455444
17_q proxy err 0.0028629067819565535 tr(WHW.T) 27590.234375
bpp_loss 3.0773006677627563
17_k proxy err 0.0013120495714247227 tr(WHW.T) 17467.6328125
bpp_loss 3.9162360429763794
17_o proxy err 0.0713680312037468 tr(WHW.T) 1118.0145263671875
bpp_loss 2.3904343843460083
17_up proxy err 0.03707573190331459 tr(WHW.T) 8457.88671875
bpp_loss 2.461451530456543
17_gate proxy err 0.00834305863827467 tr(WHW.T) 41514.84375
bpp_loss 2.7912278856549944
17_down proxy err 0.04854639247059822 tr(WHW.T) 6330.61669921875
bpp_loss 2.454011474336897
I0325 21:08:21.473616 1121353 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0004842876805923879 old loss 0.0004844022623728961 BETTER
16_v proxy err 0.05506754294037819 tr(WHW.T) 274.28167724609375
bpp_loss 2.26007878780365
16_q proxy err 0.003016347996890545 tr(WHW.T) 24498.01953125
bpp_loss 3.0631872415542603
16_k proxy err 0.0010908684926107526 tr(WHW.T) 19542.404296875
bpp_loss 3.88871967792511
16_o proxy err 0.07777155190706253 tr(WHW.T) 983.4280395507812
bpp_loss 2.3629177808761597
16_up proxy err 0.037185780704021454 tr(WHW.T) 8334.9228515625
bpp_loss 2.4635037013462613
16_gate proxy err 0.008291180245578289 tr(WHW.T) 41065.91796875
bpp_loss 2.7781244005475725
16_down proxy err 0.0476854033768177 tr(WHW.T) 6406.92578125
bpp_loss 2.457817418234689
I0325 21:09:29.589848 975333 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 63.12130784988403s
I0325 21:09:32.833482 1137841 config.py:54] PyTorch version 2.6.0 available.
W0325 21:09:33.116140 1137841 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 21:09:33.993250 1137841 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 21:09:33.997280 975333 quantize_finetune_llama.py:209] layer 19 gpu 1
I0325 21:09:34.010782 1137841 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 21:09:41.321705 1137841 finetune.py:45] layer 18_v initial loss 6.559997564181685e-05
W0325 21:09:41.321970 1137841 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 21:10:17.251137 1137841 finetune.py:68] layer 18_v @ epoch 0 new loss 4.0001101297093555e-05 old loss 6.559997564181685e-05 BETTER
I0325 21:10:34.459907 975333 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 60.027934312820435s
I0325 21:10:38.045637 1138684 config.py:54] PyTorch version 2.6.0 available.
W0325 21:10:38.349495 1138684 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 21:10:39.286310 1138684 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 21:10:39.290460 975333 quantize_finetune_llama.py:209] layer 20 gpu 0
I0325 21:10:39.305193 1138684 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 21:10:46.198161 1138684 finetune.py:45] layer 19_v initial loss 7.592589827254415e-05
W0325 21:10:46.198443 1138684 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 21:10:54.291888 1137841 finetune.py:68] layer 18_v @ epoch 1 new loss 3.7088480894453824e-05 old loss 4.0001101297093555e-05 BETTER
I0325 21:11:19.713560 1138684 finetune.py:68] layer 19_v @ epoch 0 new loss 4.177527443971485e-05 old loss 7.592589827254415e-05 BETTER
I0325 21:11:31.252730 1137841 finetune.py:68] layer 18_v @ epoch 2 new loss 3.544554056134075e-05 old loss 3.7088480894453824e-05 BETTER
I0325 21:11:54.238486 1138684 finetune.py:68] layer 19_v @ epoch 1 new loss 3.842740261461586e-05 old loss 4.177527443971485e-05 BETTER
I0325 21:12:08.211056 1137841 finetune.py:68] layer 18_v @ epoch 3 new loss 3.430901779211126e-05 old loss 3.544554056134075e-05 BETTER
I0325 21:12:29.037685 1138684 finetune.py:68] layer 19_v @ epoch 2 new loss 3.673993342090398e-05 old loss 3.842740261461586e-05 BETTER
I0325 21:12:45.584429 1137841 finetune.py:68] layer 18_v @ epoch 4 new loss 3.34532669512555e-05 old loss 3.430901779211126e-05 BETTER
I0325 21:12:55.473266 1137841 finetune.py:45] layer 18_q initial loss 4.332023672759533e-05
I0325 21:13:04.144188 1138684 finetune.py:68] layer 19_v @ epoch 3 new loss 3.560348704922944e-05 old loss 3.673993342090398e-05 BETTER
I0325 21:13:31.691963 1137841 finetune.py:68] layer 18_q @ epoch 0 new loss 3.977357118856162e-05 old loss 4.332023672759533e-05 BETTER
I0325 21:13:39.355366 1138684 finetune.py:68] layer 19_v @ epoch 4 new loss 3.478097642073408e-05 old loss 3.560348704922944e-05 BETTER
I0325 21:13:48.883093 1138684 finetune.py:45] layer 19_q initial loss 4.378735684440471e-05
I0325 21:14:08.591885 1137841 finetune.py:68] layer 18_q @ epoch 1 new loss 3.88310436392203e-05 old loss 3.977357118856162e-05 BETTER
I0325 21:14:22.807599 1138684 finetune.py:68] layer 19_q @ epoch 0 new loss 4.012165663880296e-05 old loss 4.378735684440471e-05 BETTER
I0325 21:14:45.569682 1137841 finetune.py:68] layer 18_q @ epoch 2 new loss 3.812844806816429e-05 old loss 3.88310436392203e-05 BETTER
I0325 21:14:57.542031 1138684 finetune.py:68] layer 19_q @ epoch 1 new loss 3.92230213037692e-05 old loss 4.012165663880296e-05 BETTER
I0325 21:15:22.540767 1137841 finetune.py:68] layer 18_q @ epoch 3 new loss 3.755889702006243e-05 old loss 3.812844806816429e-05 BETTER
I0325 21:15:32.240188 1138684 finetune.py:68] layer 19_q @ epoch 2 new loss 3.8543214031960815e-05 old loss 3.92230213037692e-05 BETTER
I0325 21:15:59.541828 1137841 finetune.py:68] layer 18_q @ epoch 4 new loss 3.7082172639202327e-05 old loss 3.755889702006243e-05 BETTER
I0325 21:16:06.913467 1138684 finetune.py:68] layer 19_q @ epoch 3 new loss 3.799445039476268e-05 old loss 3.8543214031960815e-05 BETTER
I0325 21:16:07.652844 1137841 finetune.py:45] layer 18_k initial loss 4.0190752770286053e-05
I0325 21:16:41.713108 1138684 finetune.py:68] layer 19_q @ epoch 4 new loss 3.7528719985857606e-05 old loss 3.799445039476268e-05 BETTER
I0325 21:16:43.592596 1137841 finetune.py:68] layer 18_k @ epoch 0 new loss 3.8913829484954476e-05 old loss 4.0190752770286053e-05 BETTER
I0325 21:16:49.562154 1138684 finetune.py:45] layer 19_k initial loss 4.0912120311986655e-05
I0325 21:17:20.519097 1137841 finetune.py:68] layer 18_k @ epoch 1 new loss 3.850794746540487e-05 old loss 3.8913829484954476e-05 BETTER
I0325 21:17:23.356693 1138684 finetune.py:68] layer 19_k @ epoch 0 new loss 3.953044142690487e-05 old loss 4.0912120311986655e-05 BETTER
I0325 21:17:57.489227 1137841 finetune.py:68] layer 18_k @ epoch 2 new loss 3.815995660261251e-05 old loss 3.850794746540487e-05 BETTER
I0325 21:17:57.912403 1138684 finetune.py:68] layer 19_k @ epoch 1 new loss 3.9158832805696875e-05 old loss 3.953044142690487e-05 BETTER
I0325 21:18:32.496096 1138684 finetune.py:68] layer 19_k @ epoch 2 new loss 3.885622209054418e-05 old loss 3.9158832805696875e-05 BETTER
I0325 21:18:34.403041 1137841 finetune.py:68] layer 18_k @ epoch 3 new loss 3.785962690017186e-05 old loss 3.815995660261251e-05 BETTER
I0325 21:19:07.102072 1138684 finetune.py:68] layer 19_k @ epoch 3 new loss 3.855853356071748e-05 old loss 3.885622209054418e-05 BETTER
I0325 21:19:11.118392 1137841 finetune.py:68] layer 18_k @ epoch 4 new loss 3.7586738471873105e-05 old loss 3.785962690017186e-05 BETTER
I0325 21:19:20.910295 1137841 finetune.py:45] layer 18_o initial loss 0.00010790211672428995
I0325 21:19:41.745631 1138684 finetune.py:68] layer 19_k @ epoch 4 new loss 3.832612492260523e-05 old loss 3.855853356071748e-05 BETTER
I0325 21:19:51.261219 1138684 finetune.py:45] layer 19_o initial loss 0.00010272688086843118
I0325 21:19:56.067561 1137841 finetune.py:68] layer 18_o @ epoch 0 new loss 9.959191811503842e-05 old loss 0.00010790211672428995 BETTER
I0325 21:20:24.366572 1138684 finetune.py:68] layer 19_o @ epoch 0 new loss 9.420308197150007e-05 old loss 0.00010272688086843118 BETTER
I0325 21:20:32.000435 1137841 finetune.py:68] layer 18_o @ epoch 1 new loss 9.631546708988026e-05 old loss 9.959191811503842e-05 BETTER
I0325 21:20:58.191983 1138684 finetune.py:68] layer 19_o @ epoch 1 new loss 9.124025382334366e-05 old loss 9.420308197150007e-05 BETTER
I0325 21:21:08.077674 1137841 finetune.py:68] layer 18_o @ epoch 2 new loss 9.406900790054351e-05 old loss 9.631546708988026e-05 BETTER
I0325 21:21:31.994375 1138684 finetune.py:68] layer 19_o @ epoch 2 new loss 8.925003203330562e-05 old loss 9.124025382334366e-05 BETTER
I0325 21:21:43.811849 1137841 finetune.py:68] layer 18_o @ epoch 3 new loss 9.232632874045521e-05 old loss 9.406900790054351e-05 BETTER
I0325 21:22:05.939163 1138684 finetune.py:68] layer 19_o @ epoch 3 new loss 8.77046913956292e-05 old loss 8.925003203330562e-05 BETTER
I0325 21:22:19.829181 1137841 finetune.py:68] layer 18_o @ epoch 4 new loss 9.096498979488388e-05 old loss 9.232632874045521e-05 BETTER
I0325 21:22:40.022289 1138684 finetune.py:68] layer 19_o @ epoch 4 new loss 8.64738249219954e-05 old loss 8.77046913956292e-05 BETTER
I0325 21:22:42.391629 1137841 finetune.py:45] layer 18_up initial loss 0.0002629494993016124
I0325 21:23:02.188701 1138684 finetune.py:45] layer 19_up initial loss 0.00027019690605811775
I0325 21:23:15.087743 1137841 finetune.py:68] layer 18_up @ epoch 0 new loss 0.00025324831949546933 old loss 0.0002629494993016124 BETTER
I0325 21:23:33.093719 1138684 finetune.py:68] layer 19_up @ epoch 0 new loss 0.00026049514417536557 old loss 0.00027019690605811775 BETTER
I0325 21:23:48.506044 1137841 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0002478132664691657 old loss 0.00025324831949546933 BETTER
I0325 21:24:05.176723 1138684 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0002550774661358446 old loss 0.00026049514417536557 BETTER
I0325 21:24:22.261738 1137841 finetune.py:68] layer 18_up @ epoch 2 new loss 0.00024355748610105366 old loss 0.0002478132664691657 BETTER
I0325 21:24:37.367660 1138684 finetune.py:68] layer 19_up @ epoch 2 new loss 0.000250790937570855 old loss 0.0002550774661358446 BETTER
I0325 21:24:56.008157 1137841 finetune.py:68] layer 18_up @ epoch 3 new loss 0.00023991904163267463 old loss 0.00024355748610105366 BETTER
I0325 21:25:09.505945 1138684 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0002472305786795914 old loss 0.000250790937570855 BETTER
I0325 21:25:29.901425 1137841 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0002367981505813077 old loss 0.00023991904163267463 BETTER
I0325 21:25:41.672740 1138684 finetune.py:68] layer 19_up @ epoch 4 new loss 0.00024411265621893108 old loss 0.0002472305786795914 BETTER
I0325 21:25:51.914806 1137841 finetune.py:45] layer 18_gate initial loss 0.0002996056282427162
I0325 21:26:03.500019 1138684 finetune.py:45] layer 19_gate initial loss 0.00031167612178251147
I0325 21:26:22.139671 1137841 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0002959872654173523 old loss 0.0002996056282427162 BETTER
I0325 21:26:32.220533 1138684 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.00030814495403319597 old loss 0.00031167612178251147 BETTER
I0325 21:26:53.464434 1137841 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.00029312833794392645 old loss 0.0002959872654173523 BETTER
I0325 21:27:01.778878 1138684 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.00030531486845575273 old loss 0.00030814495403319597 BETTER
I0325 21:27:24.986019 1137841 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0002905812580138445 old loss 0.00029312833794392645 BETTER
I0325 21:27:31.547003 1138684 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0003027445636689663 old loss 0.00030531486845575273 BETTER
I0325 21:27:56.715080 1137841 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.00028828135691583157 old loss 0.0002905812580138445 BETTER
I0325 21:28:01.374641 1138684 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.00030046390020288527 old loss 0.0003027445636689663 BETTER
I0325 21:28:28.426475 1137841 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.0002861536340788007 old loss 0.00028828135691583157 BETTER
I0325 21:28:31.242377 1138684 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.00029839063063263893 old loss 0.00030046390020288527 BETTER
I0325 21:28:51.605145 1137841 finetune.py:45] layer 18_down initial loss 0.0005152940284460783
I0325 21:28:53.868049 1138684 finetune.py:45] layer 19_down initial loss 0.0005348972044885159
I0325 21:29:19.619668 1137841 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0005151664954610169 old loss 0.0005152940284460783 BETTER
I0325 21:29:20.592599 1138684 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0005347568076103926 old loss 0.0005348972044885159 BETTER
I0325 21:29:48.171130 1138684 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0005346362595446408 old loss 0.0005347568076103926 BETTER
I0325 21:29:48.549898 1137841 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0005150550859980285 old loss 0.0005151664954610169 BETTER
I0325 21:30:15.873074 1138684 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0005345326499082148 old loss 0.0005346362595446408 BETTER
I0325 21:30:17.854406 1137841 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0005149537464603782 old loss 0.0005150550859980285 BETTER
I0325 21:30:43.577437 1138684 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0005344375385902822 old loss 0.0005345326499082148 BETTER
I0325 21:30:47.053112 1137841 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0005148594500496984 old loss 0.0005149537464603782 BETTER
I0325 21:31:11.370141 1138684 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0005343506927601993 old loss 0.0005344375385902822 BETTER
19_v proxy err 0.050671737641096115 tr(WHW.T) 341.0596618652344
bpp_loss 2.2992842197418213
19_q proxy err 0.0034412560053169727 tr(WHW.T) 24077.755859375
bpp_loss 3.081114172935486
19_k proxy err 0.00151574588380754 tr(WHW.T) 15600.5361328125
bpp_loss 3.9018049240112305
19_o proxy err 0.06489390879869461 tr(WHW.T) 1177.058349609375
bpp_loss 2.3842713832855225
19_up proxy err 0.042032092809677124 tr(WHW.T) 7649.8701171875
bpp_loss 2.455279895237514
19_gate proxy err 0.010835882276296616 tr(WHW.T) 32612.373046875
bpp_loss 2.8103203092302596
19_down proxy err 0.047800082713365555 tr(WHW.T) 6290.37890625
bpp_loss 2.4521373340061734
I0325 21:31:16.420984 1137841 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0005147733027115464 old loss 0.0005148594500496984 BETTER
18_v proxy err 0.057826556265354156 tr(WHW.T) 287.61376953125
bpp_loss 2.254668951034546
18_q proxy err 0.0036139520816504955 tr(WHW.T) 22438.546875
bpp_loss 3.0781320333480835
18_k proxy err 0.0013459633337333798 tr(WHW.T) 17460.44140625
bpp_loss 3.9878108501434326
18_o proxy err 0.06343480944633484 tr(WHW.T) 1212.7999267578125
bpp_loss 2.3711220026016235
18_up proxy err 0.04010963439941406 tr(WHW.T) 7980.74853515625
bpp_loss 2.458251953125
18_gate proxy err 0.010081405751407146 tr(WHW.T) 34947.2265625
bpp_loss 2.799555097307478
18_down proxy err 0.04800768196582794 tr(WHW.T) 6337.32177734375
bpp_loss 2.4538467270987376
I0325 21:32:16.495188 975333 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 55.18049597740173s
I0325 21:32:19.830369 1154345 config.py:54] PyTorch version 2.6.0 available.
W0325 21:32:20.113975 1154345 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 21:32:20.992218 1154345 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 21:32:20.996104 975333 quantize_finetune_llama.py:209] layer 21 gpu 1
I0325 21:32:21.009459 1154345 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 21:32:28.346422 1154345 finetune.py:45] layer 20_v initial loss 7.153145270422101e-05
W0325 21:32:28.346620 1154345 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 21:33:04.618128 1154345 finetune.py:68] layer 20_v @ epoch 0 new loss 4.3879968870896846e-05 old loss 7.153145270422101e-05 BETTER
I0325 21:33:21.585480 975333 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 60.16664266586304s
I0325 21:33:25.053928 1155181 config.py:54] PyTorch version 2.6.0 available.
W0325 21:33:25.358187 1155181 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 21:33:26.309934 1155181 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 21:33:26.313975 975333 quantize_finetune_llama.py:209] layer 22 gpu 0
I0325 21:33:26.327329 1155181 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 21:33:33.263092 1155181 finetune.py:45] layer 21_v initial loss 8.185086335288361e-05
W0325 21:33:33.263512 1155181 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 21:33:41.978529 1154345 finetune.py:68] layer 20_v @ epoch 1 new loss 4.0794926462695e-05 old loss 4.3879968870896846e-05 BETTER
I0325 21:34:06.940122 1155181 finetune.py:68] layer 21_v @ epoch 0 new loss 5.355793109629303e-05 old loss 8.185086335288361e-05 BETTER
I0325 21:34:19.590334 1154345 finetune.py:68] layer 20_v @ epoch 2 new loss 3.907720747520216e-05 old loss 4.0794926462695e-05 BETTER
I0325 21:34:41.528552 1155181 finetune.py:68] layer 21_v @ epoch 1 new loss 4.9431891966378316e-05 old loss 5.355793109629303e-05 BETTER
I0325 21:34:56.869604 1154345 finetune.py:68] layer 20_v @ epoch 3 new loss 3.786537126870826e-05 old loss 3.907720747520216e-05 BETTER
I0325 21:35:16.499960 1155181 finetune.py:68] layer 21_v @ epoch 2 new loss 4.704081220552325e-05 old loss 4.9431891966378316e-05 BETTER
I0325 21:35:34.491486 1154345 finetune.py:68] layer 20_v @ epoch 4 new loss 3.697351712617092e-05 old loss 3.786537126870826e-05 BETTER
I0325 21:35:44.348886 1154345 finetune.py:45] layer 20_q initial loss 4.7540481318719685e-05
I0325 21:35:51.655990 1155181 finetune.py:68] layer 21_v @ epoch 3 new loss 4.541432281257585e-05 old loss 4.704081220552325e-05 BETTER
I0325 21:36:20.443352 1154345 finetune.py:68] layer 20_q @ epoch 0 new loss 4.313815225032158e-05 old loss 4.7540481318719685e-05 BETTER
I0325 21:36:26.850016 1155181 finetune.py:68] layer 21_v @ epoch 4 new loss 4.415780495037325e-05 old loss 4.541432281257585e-05 BETTER
I0325 21:36:36.390151 1155181 finetune.py:45] layer 21_q initial loss 5.8094821724807844e-05
I0325 21:36:57.429647 1154345 finetune.py:68] layer 20_q @ epoch 1 new loss 4.210841143503785e-05 old loss 4.313815225032158e-05 BETTER
I0325 21:37:10.383064 1155181 finetune.py:68] layer 21_q @ epoch 0 new loss 5.3960240620654076e-05 old loss 5.8094821724807844e-05 BETTER
I0325 21:37:34.562424 1154345 finetune.py:68] layer 20_q @ epoch 2 new loss 4.1344170313095674e-05 old loss 4.210841143503785e-05 BETTER
I0325 21:37:45.006978 1155181 finetune.py:68] layer 21_q @ epoch 1 new loss 5.2401606808416545e-05 old loss 5.3960240620654076e-05 BETTER
I0325 21:38:11.642580 1154345 finetune.py:68] layer 20_q @ epoch 3 new loss 4.072196315973997e-05 old loss 4.1344170313095674e-05 BETTER
I0325 21:38:19.746636 1155181 finetune.py:68] layer 21_q @ epoch 2 new loss 5.125904135638848e-05 old loss 5.2401606808416545e-05 BETTER
I0325 21:38:48.725182 1154345 finetune.py:68] layer 20_q @ epoch 4 new loss 4.021077256766148e-05 old loss 4.072196315973997e-05 BETTER
I0325 21:38:54.524241 1155181 finetune.py:68] layer 21_q @ epoch 3 new loss 5.0311340601183474e-05 old loss 5.125904135638848e-05 BETTER
I0325 21:38:56.841515 1154345 finetune.py:45] layer 20_k initial loss 4.3318690586602315e-05
I0325 21:39:29.274620 1155181 finetune.py:68] layer 21_q @ epoch 4 new loss 4.953532697982155e-05 old loss 5.0311340601183474e-05 BETTER
I0325 21:39:32.905709 1154345 finetune.py:68] layer 20_k @ epoch 0 new loss 4.220556729706004e-05 old loss 4.3318690586602315e-05 BETTER
I0325 21:39:37.220645 1155181 finetune.py:45] layer 21_k initial loss 5.389210855355486e-05
I0325 21:40:09.765763 1154345 finetune.py:68] layer 20_k @ epoch 1 new loss 4.1784034692682326e-05 old loss 4.220556729706004e-05 BETTER
I0325 21:40:11.053154 1155181 finetune.py:68] layer 21_k @ epoch 0 new loss 5.271965346764773e-05 old loss 5.389210855355486e-05 BETTER
I0325 21:40:45.478730 1155181 finetune.py:68] layer 21_k @ epoch 1 new loss 5.210063682170585e-05 old loss 5.271965346764773e-05 BETTER
I0325 21:40:46.649366 1154345 finetune.py:68] layer 20_k @ epoch 2 new loss 4.1424074879614636e-05 old loss 4.1784034692682326e-05 BETTER
I0325 21:41:20.030903 1155181 finetune.py:68] layer 21_k @ epoch 2 new loss 5.156690167495981e-05 old loss 5.210063682170585e-05 BETTER
I0325 21:41:23.475543 1154345 finetune.py:68] layer 20_k @ epoch 3 new loss 4.112657916266471e-05 old loss 4.1424074879614636e-05 BETTER
I0325 21:41:54.593351 1155181 finetune.py:68] layer 21_k @ epoch 3 new loss 5.1119859563186765e-05 old loss 5.156690167495981e-05 BETTER
I0325 21:42:00.325194 1154345 finetune.py:68] layer 20_k @ epoch 4 new loss 4.0836417610989884e-05 old loss 4.112657916266471e-05 BETTER
I0325 21:42:10.200423 1154345 finetune.py:45] layer 20_o initial loss 0.00010800888412632048
I0325 21:42:29.125903 1155181 finetune.py:68] layer 21_k @ epoch 4 new loss 5.07194890815299e-05 old loss 5.1119859563186765e-05 BETTER
I0325 21:42:38.698823 1155181 finetune.py:45] layer 21_o initial loss 0.0001549767330288887
I0325 21:42:45.353801 1154345 finetune.py:68] layer 20_o @ epoch 0 new loss 9.997349116019905e-05 old loss 0.00010800888412632048 BETTER
I0325 21:43:11.845118 1155181 finetune.py:68] layer 21_o @ epoch 0 new loss 0.00014026377175468951 old loss 0.0001549767330288887 BETTER
I0325 21:43:21.277468 1154345 finetune.py:68] layer 20_o @ epoch 1 new loss 9.692031744634733e-05 old loss 9.997349116019905e-05 BETTER
I0325 21:43:46.050199 1155181 finetune.py:68] layer 21_o @ epoch 1 new loss 0.00013408942322712392 old loss 0.00014026377175468951 BETTER
I0325 21:43:57.150247 1154345 finetune.py:68] layer 20_o @ epoch 2 new loss 9.484904876444489e-05 old loss 9.692031744634733e-05 BETTER
I0325 21:44:20.304445 1155181 finetune.py:68] layer 21_o @ epoch 2 new loss 0.00012984038039576262 old loss 0.00013408942322712392 BETTER
I0325 21:44:33.139890 1154345 finetune.py:68] layer 20_o @ epoch 3 new loss 9.320706158177927e-05 old loss 9.484904876444489e-05 BETTER
I0325 21:44:54.032884 1155181 finetune.py:68] layer 21_o @ epoch 3 new loss 0.00012660902575589716 old loss 0.00012984038039576262 BETTER
I0325 21:45:09.041637 1154345 finetune.py:68] layer 20_o @ epoch 4 new loss 9.190183482132852e-05 old loss 9.320706158177927e-05 BETTER
I0325 21:45:28.271223 1155181 finetune.py:68] layer 21_o @ epoch 4 new loss 0.00012402549327816814 old loss 0.00012660902575589716 BETTER
I0325 21:45:31.629166 1154345 finetune.py:45] layer 20_up initial loss 0.0002886284200940281
I0325 21:45:50.039984 1155181 finetune.py:45] layer 21_up initial loss 0.00035331150866113603
I0325 21:46:03.809140 1154345 finetune.py:68] layer 20_up @ epoch 0 new loss 0.00027812470216304064 old loss 0.0002886284200940281 BETTER
I0325 21:46:20.751884 1155181 finetune.py:68] layer 21_up @ epoch 0 new loss 0.00033991504460573196 old loss 0.00035331150866113603 BETTER
I0325 21:46:37.044231 1154345 finetune.py:68] layer 20_up @ epoch 1 new loss 0.00027245288947597146 old loss 0.00027812470216304064 BETTER
I0325 21:46:52.495684 1155181 finetune.py:68] layer 21_up @ epoch 1 new loss 0.0003327132435515523 old loss 0.00033991504460573196 BETTER
I0325 21:47:10.461465 1154345 finetune.py:68] layer 20_up @ epoch 2 new loss 0.0002680316974874586 old loss 0.00027245288947597146 BETTER
I0325 21:47:24.625800 1155181 finetune.py:68] layer 21_up @ epoch 2 new loss 0.00032708741491660476 old loss 0.0003327132435515523 BETTER
I0325 21:47:44.182937 1154345 finetune.py:68] layer 20_up @ epoch 3 new loss 0.00026429485296830535 old loss 0.0002680316974874586 BETTER
I0325 21:47:56.766157 1155181 finetune.py:68] layer 21_up @ epoch 3 new loss 0.0003223325766157359 old loss 0.00032708741491660476 BETTER
I0325 21:48:17.971637 1154345 finetune.py:68] layer 20_up @ epoch 4 new loss 0.00026112364139407873 old loss 0.00026429485296830535 BETTER
I0325 21:48:29.012099 1155181 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0003182580694556236 old loss 0.0003223325766157359 BETTER
I0325 21:48:40.473465 1154345 finetune.py:45] layer 20_gate initial loss 0.00033686484675854445
I0325 21:48:51.437572 1155181 finetune.py:45] layer 21_gate initial loss 0.00040643088868819177
I0325 21:49:11.015225 1154345 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.0003330919425934553 old loss 0.00033686484675854445 BETTER
I0325 21:49:20.315160 1155181 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.000401523633627221 old loss 0.00040643088868819177 BETTER
I0325 21:49:42.490008 1154345 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0003301407559774816 old loss 0.0003330919425934553 BETTER
I0325 21:49:50.157922 1155181 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.00039774563629180193 old loss 0.000401523633627221 BETTER
I0325 21:50:13.980296 1154345 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.00032750656828284264 old loss 0.0003301407559774816 BETTER
I0325 21:50:19.761013 1155181 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0003944153431802988 old loss 0.00039774563629180193 BETTER
I0325 21:50:45.416568 1154345 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0003251340240240097 old loss 0.00032750656828284264 BETTER
I0325 21:50:49.609316 1155181 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.00039139538421295583 old loss 0.0003944153431802988 BETTER
I0325 21:51:16.846075 1154345 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.00032298133010044694 old loss 0.0003251340240240097 BETTER
I0325 21:51:19.414956 1155181 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.0003886401536874473 old loss 0.00039139538421295583 BETTER
I0325 21:51:39.728907 1154345 finetune.py:45] layer 20_down initial loss 0.0005739176995120943
I0325 21:51:42.065068 1155181 finetune.py:45] layer 21_down initial loss 0.0006877837004140019
I0325 21:52:07.533894 1154345 finetune.py:68] layer 20_down @ epoch 0 new loss 0.0005737898754887283 old loss 0.0005739176995120943 BETTER
I0325 21:52:08.679997 1155181 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0006876221741549671 old loss 0.0006877837004140019 BETTER
I0325 21:52:36.320651 1155181 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0006874750251881778 old loss 0.0006876221741549671 BETTER
I0325 21:52:36.667710 1154345 finetune.py:68] layer 20_down @ epoch 1 new loss 0.00057367974659428 old loss 0.0005737898754887283 BETTER
I0325 21:53:04.005687 1155181 finetune.py:68] layer 21_down @ epoch 2 new loss 0.0006873452803120017 old loss 0.0006874750251881778 BETTER
I0325 21:53:05.947381 1154345 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0005735860904678702 old loss 0.00057367974659428 BETTER
I0325 21:53:31.737543 1155181 finetune.py:68] layer 21_down @ epoch 3 new loss 0.0006872224039398134 old loss 0.0006873452803120017 BETTER
I0325 21:53:35.387079 1154345 finetune.py:68] layer 20_down @ epoch 3 new loss 0.0005734963342547417 old loss 0.0005735860904678702 BETTER
I0325 21:53:59.527639 1155181 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0006871108198538423 old loss 0.0006872224039398134 BETTER
21_v proxy err 0.04714026302099228 tr(WHW.T) 362.87310791015625
bpp_loss 2.3577475547790527
21_q proxy err 0.0031128516420722008 tr(WHW.T) 25871.021484375
bpp_loss 3.0547889471054077
21_k proxy err 0.0013654805952683091 tr(WHW.T) 16846.404296875
bpp_loss 3.89887535572052
21_o proxy err 0.054763708263635635 tr(WHW.T) 1279.5426025390625
bpp_loss 2.3862987756729126
21_up proxy err 0.04145781695842743 tr(WHW.T) 7783.92333984375
bpp_loss 2.4619809559413364
21_gate proxy err 0.011295866221189499 tr(WHW.T) 31411.03515625
bpp_loss 2.82346248626709
21_down proxy err 0.04501601308584213 tr(WHW.T) 6477.79833984375
bpp_loss 2.4568347930908203
I0325 21:54:04.883333 1154345 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0005734211299568415 old loss 0.0005734963342547417 BETTER
20_v proxy err 0.050789717584848404 tr(WHW.T) 330.192138671875
bpp_loss 2.332668662071228
20_q proxy err 0.0037908933591097593 tr(WHW.T) 20777.865234375
bpp_loss 3.054174304008484
20_k proxy err 0.0014556370442733169 tr(WHW.T) 15453.8095703125
bpp_loss 3.8614779710769653
20_o proxy err 0.06504251062870026 tr(WHW.T) 1212.967529296875
bpp_loss 2.371019124984741
20_up proxy err 0.042488958686590195 tr(WHW.T) 7605.8369140625
bpp_loss 2.4592368262154713
20_gate proxy err 0.011605005711317062 tr(WHW.T) 30541.625
bpp_loss 2.8128230231148854
20_down proxy err 0.04670067876577377 tr(WHW.T) 6414.55810546875
bpp_loss 2.456508125577654
I0325 21:55:12.612664 975333 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 62.80806612968445s
I0325 21:55:15.885070 1170894 config.py:54] PyTorch version 2.6.0 available.
W0325 21:55:16.167231 1170894 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 21:55:17.056562 1170894 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 21:55:17.060714 975333 quantize_finetune_llama.py:209] layer 23 gpu 1
I0325 21:55:17.073778 1170894 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 21:55:24.355990 1170894 finetune.py:45] layer 22_v initial loss 7.17490038368851e-05
W0325 21:55:24.356268 1170894 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 21:56:00.116123 1170894 finetune.py:68] layer 22_v @ epoch 0 new loss 4.290806100470945e-05 old loss 7.17490038368851e-05 BETTER
I0325 21:56:18.224074 975333 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 60.738808393478394s
I0325 21:56:21.705437 1171740 config.py:54] PyTorch version 2.6.0 available.
W0325 21:56:21.997081 1171740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 21:56:22.914346 1171740 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 21:56:22.918204 975333 quantize_finetune_llama.py:209] layer 24 gpu 0
I0325 21:56:22.933778 1171740 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 21:56:29.835309 1171740 finetune.py:45] layer 23_v initial loss 7.723961607553065e-05
W0325 21:56:29.835630 1171740 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 21:56:37.139217 1170894 finetune.py:68] layer 22_v @ epoch 1 new loss 3.9673803257755935e-05 old loss 4.290806100470945e-05 BETTER
I0325 21:57:03.546705 1171740 finetune.py:68] layer 23_v @ epoch 0 new loss 4.3102783820359036e-05 old loss 7.723961607553065e-05 BETTER
I0325 21:57:14.190265 1170894 finetune.py:68] layer 22_v @ epoch 2 new loss 3.798293982981704e-05 old loss 3.9673803257755935e-05 BETTER
I0325 21:57:38.215279 1171740 finetune.py:68] layer 23_v @ epoch 1 new loss 3.9771173760527745e-05 old loss 4.3102783820359036e-05 BETTER
I0325 21:57:51.291332 1170894 finetune.py:68] layer 22_v @ epoch 3 new loss 3.684121838887222e-05 old loss 3.798293982981704e-05 BETTER
I0325 21:58:13.264240 1171740 finetune.py:68] layer 23_v @ epoch 2 new loss 3.811980059253983e-05 old loss 3.9771173760527745e-05 BETTER
I0325 21:58:28.380061 1170894 finetune.py:68] layer 22_v @ epoch 4 new loss 3.599351839511655e-05 old loss 3.684121838887222e-05 BETTER
I0325 21:58:38.233787 1170894 finetune.py:45] layer 22_q initial loss 4.879912012256682e-05
I0325 21:58:48.279907 1171740 finetune.py:68] layer 23_v @ epoch 3 new loss 3.703699621837586e-05 old loss 3.811980059253983e-05 BETTER
I0325 21:59:14.442280 1170894 finetune.py:68] layer 22_q @ epoch 0 new loss 4.495259781833738e-05 old loss 4.879912012256682e-05 BETTER
I0325 21:59:23.387054 1171740 finetune.py:68] layer 23_v @ epoch 4 new loss 3.623360680649057e-05 old loss 3.703699621837586e-05 BETTER
I0325 21:59:32.986549 1171740 finetune.py:45] layer 23_q initial loss 4.74245025543496e-05
I0325 21:59:51.392549 1170894 finetune.py:68] layer 22_q @ epoch 1 new loss 4.391242691781372e-05 old loss 4.495259781833738e-05 BETTER
I0325 22:00:07.054125 1171740 finetune.py:68] layer 23_q @ epoch 0 new loss 4.431799607118592e-05 old loss 4.74245025543496e-05 BETTER
I0325 22:00:28.397438 1170894 finetune.py:68] layer 22_q @ epoch 2 new loss 4.314185935072601e-05 old loss 4.391242691781372e-05 BETTER
I0325 22:00:41.719985 1171740 finetune.py:68] layer 23_q @ epoch 1 new loss 4.335666017141193e-05 old loss 4.431799607118592e-05 BETTER
I0325 22:01:05.452033 1170894 finetune.py:68] layer 22_q @ epoch 3 new loss 4.2534236854407936e-05 old loss 4.314185935072601e-05 BETTER
I0325 22:01:16.517973 1171740 finetune.py:68] layer 23_q @ epoch 2 new loss 4.262139555066824e-05 old loss 4.335666017141193e-05 BETTER
I0325 22:01:42.426579 1170894 finetune.py:68] layer 22_q @ epoch 4 new loss 4.201932461000979e-05 old loss 4.2534236854407936e-05 BETTER
I0325 22:01:50.393139 1170894 finetune.py:45] layer 22_k initial loss 4.5966597099322826e-05
I0325 22:01:51.193900 1171740 finetune.py:68] layer 23_q @ epoch 3 new loss 4.205090590403415e-05 old loss 4.262139555066824e-05 BETTER
I0325 22:02:25.992592 1171740 finetune.py:68] layer 23_q @ epoch 4 new loss 4.157724106335081e-05 old loss 4.205090590403415e-05 BETTER
I0325 22:02:26.703605 1170894 finetune.py:68] layer 22_k @ epoch 0 new loss 4.5004118874203414e-05 old loss 4.5966597099322826e-05 BETTER
I0325 22:02:33.742268 1171740 finetune.py:45] layer 23_k initial loss 4.619659375748597e-05
I0325 22:03:03.671612 1170894 finetune.py:68] layer 22_k @ epoch 1 new loss 4.46034136984963e-05 old loss 4.5004118874203414e-05 BETTER
I0325 22:03:07.674004 1171740 finetune.py:68] layer 23_k @ epoch 0 new loss 4.522671224549413e-05 old loss 4.619659375748597e-05 BETTER
I0325 22:03:40.694585 1170894 finetune.py:68] layer 22_k @ epoch 2 new loss 4.4256103137740865e-05 old loss 4.46034136984963e-05 BETTER
I0325 22:03:42.280955 1171740 finetune.py:68] layer 23_k @ epoch 1 new loss 4.483551674638875e-05 old loss 4.522671224549413e-05 BETTER
I0325 22:04:16.778191 1171740 finetune.py:68] layer 23_k @ epoch 2 new loss 4.4504806282930076e-05 old loss 4.483551674638875e-05 BETTER
I0325 22:04:17.734074 1170894 finetune.py:68] layer 22_k @ epoch 3 new loss 4.396533404360525e-05 old loss 4.4256103137740865e-05 BETTER
I0325 22:04:51.409098 1171740 finetune.py:68] layer 23_k @ epoch 3 new loss 4.422882557264529e-05 old loss 4.4504806282930076e-05 BETTER
I0325 22:04:54.715915 1170894 finetune.py:68] layer 22_k @ epoch 4 new loss 4.370894384919666e-05 old loss 4.396533404360525e-05 BETTER
I0325 22:05:04.510048 1170894 finetune.py:45] layer 22_o initial loss 0.00013013099669478834
I0325 22:05:26.025518 1171740 finetune.py:68] layer 23_k @ epoch 4 new loss 4.397818702273071e-05 old loss 4.422882557264529e-05 BETTER
I0325 22:05:35.484843 1171740 finetune.py:45] layer 23_o initial loss 0.00011827160778921098
I0325 22:05:39.764108 1170894 finetune.py:68] layer 22_o @ epoch 0 new loss 0.00012067025090800598 old loss 0.00013013099669478834 BETTER
I0325 22:06:08.720149 1171740 finetune.py:68] layer 23_o @ epoch 0 new loss 0.00011027077562175691 old loss 0.00011827160778921098 BETTER
I0325 22:06:15.833620 1170894 finetune.py:68] layer 22_o @ epoch 1 new loss 0.00011716179142240435 old loss 0.00012067025090800598 BETTER
I0325 22:06:42.631128 1171740 finetune.py:68] layer 23_o @ epoch 1 new loss 0.00010755332914413884 old loss 0.00011027077562175691 BETTER
I0325 22:06:51.714530 1170894 finetune.py:68] layer 22_o @ epoch 2 new loss 0.00011469600576674566 old loss 0.00011716179142240435 BETTER
I0325 22:07:16.480562 1171740 finetune.py:68] layer 23_o @ epoch 2 new loss 0.00010567601566435769 old loss 0.00010755332914413884 BETTER
I0325 22:07:27.606965 1170894 finetune.py:68] layer 22_o @ epoch 3 new loss 0.00011283870844636112 old loss 0.00011469600576674566 BETTER
I0325 22:07:50.401318 1171740 finetune.py:68] layer 23_o @ epoch 3 new loss 0.00010425286018289626 old loss 0.00010567601566435769 BETTER
I0325 22:08:03.637675 1170894 finetune.py:68] layer 22_o @ epoch 4 new loss 0.00011130498751299456 old loss 0.00011283870844636112 BETTER
I0325 22:08:24.270231 1171740 finetune.py:68] layer 23_o @ epoch 4 new loss 0.00010314684186596423 old loss 0.00010425286018289626 BETTER
I0325 22:08:25.577347 1170894 finetune.py:45] layer 22_up initial loss 0.00035025071701966226
I0325 22:08:46.008170 1171740 finetune.py:45] layer 23_up initial loss 0.0003620708012022078
I0325 22:08:57.840406 1170894 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0003376785898581147 old loss 0.00035025071701966226 BETTER
I0325 22:09:16.707501 1171740 finetune.py:68] layer 23_up @ epoch 0 new loss 0.000350151996826753 old loss 0.0003620708012022078 BETTER
I0325 22:09:31.202782 1170894 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0003311598557047546 old loss 0.0003376785898581147 BETTER
I0325 22:09:48.673955 1171740 finetune.py:68] layer 23_up @ epoch 1 new loss 0.00034381361911073327 old loss 0.000350151996826753 BETTER
I0325 22:10:04.850177 1170894 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0003260294033680111 old loss 0.0003311598557047546 BETTER
I0325 22:10:20.897732 1171740 finetune.py:68] layer 23_up @ epoch 2 new loss 0.00033890691702254117 old loss 0.00034381361911073327 BETTER
I0325 22:10:38.683535 1170894 finetune.py:68] layer 22_up @ epoch 3 new loss 0.00032176560489460826 old loss 0.0003260294033680111 BETTER
I0325 22:10:52.996748 1171740 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0003348408208694309 old loss 0.00033890691702254117 BETTER
I0325 22:11:12.539406 1170894 finetune.py:68] layer 22_up @ epoch 4 new loss 0.000318078207783401 old loss 0.00032176560489460826 BETTER
I0325 22:11:25.128548 1171740 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0003313583438284695 old loss 0.0003348408208694309 BETTER
I0325 22:11:34.581516 1170894 finetune.py:45] layer 22_gate initial loss 0.0004124388506170362
I0325 22:11:46.972156 1171740 finetune.py:45] layer 23_gate initial loss 0.00043641417869366705
I0325 22:12:04.882679 1170894 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0004080084909219295 old loss 0.0004124388506170362 BETTER
I0325 22:12:15.505289 1171740 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0004323856846895069 old loss 0.00043641417869366705 BETTER
I0325 22:12:36.051866 1170894 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.00040458005969412625 old loss 0.0004080084909219295 BETTER
I0325 22:12:45.117377 1171740 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.00042924462468363345 old loss 0.0004323856846895069 BETTER
I0325 22:13:07.481786 1170894 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.00040155809256248176 old loss 0.00040458005969412625 BETTER
I0325 22:13:14.869490 1171740 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0004264414310455322 old loss 0.00042924462468363345 BETTER
I0325 22:13:39.146732 1170894 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.00039881651173345745 old loss 0.00040155809256248176 BETTER
I0325 22:13:44.660307 1171740 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.00042391213355585933 old loss 0.0004264414310455322 BETTER
I0325 22:14:11.128785 1170894 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0003963409399148077 old loss 0.00039881651173345745 BETTER
I0325 22:14:14.833668 1171740 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0004216301895212382 old loss 0.00042391213355585933 BETTER
I0325 22:14:34.799900 1170894 finetune.py:45] layer 22_down initial loss 0.0007081127259880304
I0325 22:14:37.920852 1171740 finetune.py:45] layer 23_down initial loss 0.0007442087517119944
I0325 22:15:02.770636 1170894 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0007079452043399215 old loss 0.0007081127259880304 BETTER
I0325 22:15:04.723467 1171740 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0007440600893460214 old loss 0.0007442087517119944 BETTER
I0325 22:15:32.033479 1170894 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0007077938644215465 old loss 0.0007079452043399215 BETTER
I0325 22:15:32.365302 1171740 finetune.py:68] layer 23_down @ epoch 1 new loss 0.000743925804272294 old loss 0.0007440600893460214 BETTER
I0325 22:16:00.068428 1171740 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0007438090979121625 old loss 0.000743925804272294 BETTER
I0325 22:16:01.430395 1170894 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0007076648180373013 old loss 0.0007077938644215465 BETTER
I0325 22:16:27.794319 1171740 finetune.py:68] layer 23_down @ epoch 3 new loss 0.000743703858461231 old loss 0.0007438090979121625 BETTER
I0325 22:16:30.957370 1170894 finetune.py:68] layer 22_down @ epoch 3 new loss 0.000707545957993716 old loss 0.0007076648180373013 BETTER
I0325 22:16:55.478505 1171740 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0007436044397763908 old loss 0.000743703858461231 BETTER
23_v proxy err 0.04450780525803566 tr(WHW.T) 397.90704345703125
bpp_loss 2.4568783044815063
23_q proxy err 0.003547361819073558 tr(WHW.T) 22649.484375
bpp_loss 3.0296661853790283
23_k proxy err 0.0015320025850087404 tr(WHW.T) 14898.54296875
bpp_loss 3.84549880027771
23_o proxy err 0.046698231250047684 tr(WHW.T) 1753.703857421875
bpp_loss 2.4413039684295654
23_up proxy err 0.043664563447237015 tr(WHW.T) 7428.759765625
bpp_loss 2.470231464930943
23_gate proxy err 0.013061294332146645 tr(WHW.T) 27097.90625
bpp_loss 2.8305489676339284
23_down proxy err 0.043680716305971146 tr(WHW.T) 6779.3271484375
bpp_loss 2.4682815074920654
I0325 22:17:00.289168 1170894 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0007074375753290951 old loss 0.000707545957993716 BETTER
22_v proxy err 0.04778490960597992 tr(WHW.T) 346.0789489746094
bpp_loss 2.4068453311920166
22_q proxy err 0.003711257129907608 tr(WHW.T) 20364.79296875
bpp_loss 3.0183005332946777
22_k proxy err 0.0014622346498072147 tr(WHW.T) 14760.837890625
bpp_loss 3.842568278312683
22_o proxy err 0.06574903428554535 tr(WHW.T) 1226.2769775390625
bpp_loss 2.4182357788085938
22_up proxy err 0.04292821139097214 tr(WHW.T) 7552.70556640625
bpp_loss 2.466308729989188
22_gate proxy err 0.012102631852030754 tr(WHW.T) 29353.080078125
bpp_loss 2.8283848081316267
22_down proxy err 0.044685591012239456 tr(WHW.T) 6656.13232421875
bpp_loss 2.4630163056509837
I0325 22:18:07.656989 975333 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 62.52779293060303s
I0325 22:18:11.038755 1187435 config.py:54] PyTorch version 2.6.0 available.
W0325 22:18:11.320154 1187435 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 22:18:12.195794 1187435 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 22:18:12.199885 975333 quantize_finetune_llama.py:209] layer 25 gpu 1
I0325 22:18:12.218701 1187435 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 22:18:19.629390 1187435 finetune.py:45] layer 24_v initial loss 7.517120684497058e-05
W0325 22:18:19.629569 1187435 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 22:18:55.514556 1187435 finetune.py:68] layer 24_v @ epoch 0 new loss 4.323343455325812e-05 old loss 7.517120684497058e-05 BETTER
I0325 22:19:12.987018 975333 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 60.382619857788086s
I0325 22:19:16.430315 1188278 config.py:54] PyTorch version 2.6.0 available.
W0325 22:19:16.722105 1188278 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 22:19:17.664889 1188278 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 22:19:17.668615 975333 quantize_finetune_llama.py:209] layer 26 gpu 0
I0325 22:19:17.681395 1188278 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 22:19:24.617637 1188278 finetune.py:45] layer 25_v initial loss 8.921434346120805e-05
W0325 22:19:24.617897 1188278 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 22:19:32.494215 1187435 finetune.py:68] layer 24_v @ epoch 1 new loss 4.0265553252538666e-05 old loss 4.323343455325812e-05 BETTER
I0325 22:19:58.197841 1188278 finetune.py:68] layer 25_v @ epoch 0 new loss 5.208159927860834e-05 old loss 8.921434346120805e-05 BETTER
I0325 22:20:09.687842 1187435 finetune.py:68] layer 24_v @ epoch 2 new loss 3.87508662242908e-05 old loss 4.0265553252538666e-05 BETTER
I0325 22:20:32.635301 1188278 finetune.py:68] layer 25_v @ epoch 1 new loss 4.843312854063697e-05 old loss 5.208159927860834e-05 BETTER
I0325 22:20:46.980503 1187435 finetune.py:68] layer 24_v @ epoch 3 new loss 3.781032137339935e-05 old loss 3.87508662242908e-05 BETTER
I0325 22:21:07.410328 1188278 finetune.py:68] layer 25_v @ epoch 2 new loss 4.649930269806646e-05 old loss 4.843312854063697e-05 BETTER
I0325 22:21:24.154591 1187435 finetune.py:68] layer 24_v @ epoch 4 new loss 3.706939969561063e-05 old loss 3.781032137339935e-05 BETTER
I0325 22:21:34.033136 1187435 finetune.py:45] layer 24_q initial loss 4.912206713925116e-05
I0325 22:21:42.493840 1188278 finetune.py:68] layer 25_v @ epoch 3 new loss 4.530447404249571e-05 old loss 4.649930269806646e-05 BETTER
I0325 22:22:10.290641 1187435 finetune.py:68] layer 24_q @ epoch 0 new loss 4.702001388068311e-05 old loss 4.912206713925116e-05 BETTER
I0325 22:22:17.755670 1188278 finetune.py:68] layer 25_v @ epoch 4 new loss 4.432671266840771e-05 old loss 4.530447404249571e-05 BETTER
I0325 22:22:27.345515 1188278 finetune.py:45] layer 25_q initial loss 6.71783127472736e-05
I0325 22:22:47.199512 1187435 finetune.py:68] layer 24_q @ epoch 1 new loss 4.60653864138294e-05 old loss 4.702001388068311e-05 BETTER
I0325 22:23:01.081876 1188278 finetune.py:68] layer 25_q @ epoch 0 new loss 6.201165524544194e-05 old loss 6.71783127472736e-05 BETTER
I0325 22:23:24.024520 1187435 finetune.py:68] layer 24_q @ epoch 2 new loss 4.536297274171375e-05 old loss 4.60653864138294e-05 BETTER
I0325 22:23:35.674879 1188278 finetune.py:68] layer 25_q @ epoch 1 new loss 6.0140391724416986e-05 old loss 6.201165524544194e-05 BETTER
I0325 22:24:00.816346 1187435 finetune.py:68] layer 24_q @ epoch 3 new loss 4.4831169361714274e-05 old loss 4.536297274171375e-05 BETTER
I0325 22:24:10.265456 1188278 finetune.py:68] layer 25_q @ epoch 2 new loss 5.8850051573244855e-05 old loss 6.0140391724416986e-05 BETTER
I0325 22:24:37.728896 1187435 finetune.py:68] layer 24_q @ epoch 4 new loss 4.4365027861204e-05 old loss 4.4831169361714274e-05 BETTER
I0325 22:24:44.816682 1188278 finetune.py:68] layer 25_q @ epoch 3 new loss 5.781738582300022e-05 old loss 5.8850051573244855e-05 BETTER
I0325 22:24:45.841113 1187435 finetune.py:45] layer 24_k initial loss 4.975717456545681e-05
I0325 22:25:19.608930 1188278 finetune.py:68] layer 25_q @ epoch 4 new loss 5.69733529118821e-05 old loss 5.781738582300022e-05 BETTER
I0325 22:25:22.035510 1187435 finetune.py:68] layer 24_k @ epoch 0 new loss 4.9148726247949526e-05 old loss 4.975717456545681e-05 BETTER
I0325 22:25:27.408519 1188278 finetune.py:45] layer 25_k initial loss 6.630759162362665e-05
I0325 22:25:58.840285 1187435 finetune.py:68] layer 24_k @ epoch 1 new loss 4.8808557039592415e-05 old loss 4.9148726247949526e-05 BETTER
I0325 22:26:01.140579 1188278 finetune.py:68] layer 25_k @ epoch 0 new loss 6.494323315564543e-05 old loss 6.630759162362665e-05 BETTER
I0325 22:26:35.688754 1187435 finetune.py:68] layer 24_k @ epoch 2 new loss 4.853793507209048e-05 old loss 4.8808557039592415e-05 BETTER
I0325 22:26:35.698576 1188278 finetune.py:68] layer 25_k @ epoch 1 new loss 6.430439680116251e-05 old loss 6.494323315564543e-05 BETTER
I0325 22:27:10.154489 1188278 finetune.py:68] layer 25_k @ epoch 2 new loss 6.379064870998263e-05 old loss 6.430439680116251e-05 BETTER
I0325 22:27:12.381478 1187435 finetune.py:68] layer 24_k @ epoch 3 new loss 4.829734461964108e-05 old loss 4.853793507209048e-05 BETTER
I0325 22:27:44.561665 1188278 finetune.py:68] layer 25_k @ epoch 3 new loss 6.33416129858233e-05 old loss 6.379064870998263e-05 BETTER
I0325 22:27:49.093053 1187435 finetune.py:68] layer 24_k @ epoch 4 new loss 4.8070276534417644e-05 old loss 4.829734461964108e-05 BETTER
I0325 22:27:58.877923 1187435 finetune.py:45] layer 24_o initial loss 0.00013408256927505136
I0325 22:28:19.314115 1188278 finetune.py:68] layer 25_k @ epoch 4 new loss 6.296228093560785e-05 old loss 6.33416129858233e-05 BETTER
I0325 22:28:28.717304 1188278 finetune.py:45] layer 25_o initial loss 0.00015749287558719516
I0325 22:28:34.643467 1187435 finetune.py:68] layer 24_o @ epoch 0 new loss 0.0001261284778593108 old loss 0.00013408256927505136 BETTER
I0325 22:29:01.853075 1188278 finetune.py:68] layer 25_o @ epoch 0 new loss 0.00014688647934235632 old loss 0.00015749287558719516 BETTER
I0325 22:29:10.610302 1187435 finetune.py:68] layer 24_o @ epoch 1 new loss 0.00012346553558018059 old loss 0.0001261284778593108 BETTER
I0325 22:29:35.881500 1188278 finetune.py:68] layer 25_o @ epoch 1 new loss 0.00014332994760479778 old loss 0.00014688647934235632 BETTER
I0325 22:29:46.608338 1187435 finetune.py:68] layer 24_o @ epoch 2 new loss 0.0001216606906382367 old loss 0.00012346553558018059 BETTER
I0325 22:30:09.811330 1188278 finetune.py:68] layer 25_o @ epoch 2 new loss 0.00014094314246904105 old loss 0.00014332994760479778 BETTER
I0325 22:30:22.727656 1187435 finetune.py:68] layer 24_o @ epoch 3 new loss 0.00012026671174680814 old loss 0.0001216606906382367 BETTER
I0325 22:30:43.746915 1188278 finetune.py:68] layer 25_o @ epoch 3 new loss 0.0001391526311635971 old loss 0.00014094314246904105 BETTER
I0325 22:30:58.831669 1187435 finetune.py:68] layer 24_o @ epoch 4 new loss 0.00011911526962649077 old loss 0.00012026671174680814 BETTER
I0325 22:31:17.764532 1188278 finetune.py:68] layer 25_o @ epoch 4 new loss 0.00013773047248832881 old loss 0.0001391526311635971 BETTER
I0325 22:31:21.247123 1187435 finetune.py:45] layer 24_up initial loss 0.0003953399136662483
I0325 22:31:39.401538 1188278 finetune.py:45] layer 25_up initial loss 0.0004456051392480731
I0325 22:31:53.583083 1187435 finetune.py:68] layer 24_up @ epoch 0 new loss 0.000383406993933022 old loss 0.0003953399136662483 BETTER
I0325 22:32:10.148667 1188278 finetune.py:68] layer 25_up @ epoch 0 new loss 0.00043137051397934556 old loss 0.0004456051392480731 BETTER
I0325 22:32:26.910711 1187435 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0003772324707824737 old loss 0.000383406993933022 BETTER
I0325 22:32:42.012140 1188278 finetune.py:68] layer 25_up @ epoch 1 new loss 0.000424444442614913 old loss 0.00043137051397934556 BETTER
I0325 22:33:00.544378 1187435 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0003725050191860646 old loss 0.0003772324707824737 BETTER
I0325 22:33:14.138047 1188278 finetune.py:68] layer 25_up @ epoch 2 new loss 0.000419204356148839 old loss 0.000424444442614913 BETTER
I0325 22:33:34.292274 1187435 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0003685421834234148 old loss 0.0003725050191860646 BETTER
I0325 22:33:46.208847 1188278 finetune.py:68] layer 25_up @ epoch 3 new loss 0.00041488537681289017 old loss 0.000419204356148839 BETTER
I0325 22:34:08.117779 1187435 finetune.py:68] layer 24_up @ epoch 4 new loss 0.00036516759428195655 old loss 0.0003685421834234148 BETTER
I0325 22:34:18.386401 1188278 finetune.py:68] layer 25_up @ epoch 4 new loss 0.00041118485387414694 old loss 0.00041488537681289017 BETTER
I0325 22:34:30.099434 1187435 finetune.py:45] layer 24_gate initial loss 0.0004821468610316515
I0325 22:34:40.345069 1188278 finetune.py:45] layer 25_gate initial loss 0.0005450440803542733
I0325 22:35:00.605008 1187435 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0004782674659509212 old loss 0.0004821468610316515 BETTER
I0325 22:35:09.268202 1188278 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.000540721055585891 old loss 0.0005450440803542733 BETTER
I0325 22:35:32.239262 1187435 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0004751737287733704 old loss 0.0004782674659509212 BETTER
I0325 22:35:38.923310 1188278 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.0005373206222429872 old loss 0.000540721055585891 BETTER
I0325 22:36:03.872482 1187435 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.00047250528587028384 old loss 0.0004751737287733704 BETTER
I0325 22:36:08.785124 1188278 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.000534399354364723 old loss 0.0005373206222429872 BETTER
I0325 22:36:35.604507 1187435 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0004700311692431569 old loss 0.00047250528587028384 BETTER
I0325 22:36:38.701774 1188278 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.0005317573086358607 old loss 0.000534399354364723 BETTER
I0325 22:37:07.269330 1187435 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.00046784206642769277 old loss 0.0004700311692431569 BETTER
I0325 22:37:08.651236 1188278 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0005293752765282989 old loss 0.0005317573086358607 BETTER
I0325 22:37:30.061670 1187435 finetune.py:45] layer 24_down initial loss 0.0007995173218660057
I0325 22:37:31.266963 1188278 finetune.py:45] layer 25_down initial loss 0.0008856302592903376
I0325 22:37:58.202952 1188278 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0008855128544382751 old loss 0.0008856302592903376 BETTER
I0325 22:37:58.249339 1187435 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0007993871113285422 old loss 0.0007995173218660057 BETTER
I0325 22:38:25.857706 1188278 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0008854010957293212 old loss 0.0008855128544382751 BETTER
I0325 22:38:27.367344 1187435 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0007992746541276574 old loss 0.0007993871113285422 BETTER
I0325 22:38:53.646932 1188278 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0008853011531755328 old loss 0.0008854010957293212 BETTER
I0325 22:38:56.824601 1187435 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0007991746533662081 old loss 0.0007992746541276574 BETTER
I0325 22:39:21.625783 1188278 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0008852009777911007 old loss 0.0008853011531755328 BETTER
I0325 22:39:26.080970 1187435 finetune.py:68] layer 24_down @ epoch 3 new loss 0.000799082568846643 old loss 0.0007991746533662081 BETTER
I0325 22:39:49.647993 1188278 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0008851161692291498 old loss 0.0008852009777911007 BETTER
25_v proxy err 0.03242329880595207 tr(WHW.T) 557.8086547851562
bpp_loss 2.560833692550659
25_q proxy err 0.003062412841245532 tr(WHW.T) 26178.201171875
bpp_loss 2.981853723526001
25_k proxy err 0.0015598476165905595 tr(WHW.T) 14456.4072265625
bpp_loss 3.6739931106567383
25_o proxy err 0.038939476013183594 tr(WHW.T) 1996.8663330078125
bpp_loss 2.4780505895614624
25_up proxy err 0.04390835389494896 tr(WHW.T) 7389.798828125
bpp_loss 2.483543804713658
25_gate proxy err 0.013494598679244518 tr(WHW.T) 26150.478515625
bpp_loss 2.8441929136003767
25_down proxy err 0.041805777698755264 tr(WHW.T) 6741.3427734375
bpp_loss 2.4820755209241594
I0325 22:39:55.431089 1187435 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0007989987498149276 old loss 0.000799082568846643 BETTER
24_v proxy err 0.0377056710422039 tr(WHW.T) 467.2783508300781
bpp_loss 2.5491700172424316
24_q proxy err 0.003490801900625229 tr(WHW.T) 22492.978515625
bpp_loss 2.9993138313293457
24_k proxy err 0.001546488027088344 tr(WHW.T) 14225.8056640625
bpp_loss 3.6938228607177734
24_o proxy err 0.04873351752758026 tr(WHW.T) 1600.1904296875
bpp_loss 2.480334997177124
24_up proxy err 0.04448739439249039 tr(WHW.T) 7314.12353515625
bpp_loss 2.4745826721191406
24_gate proxy err 0.013766277581453323 tr(WHW.T) 25749.48828125
bpp_loss 2.835929734366281
24_down proxy err 0.042359307408332825 tr(WHW.T) 6865.36669921875
bpp_loss 2.4735172476087297
I0325 22:41:03.092728 975333 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 62.888338565826416s
I0325 22:41:06.278940 1203976 config.py:54] PyTorch version 2.6.0 available.
W0325 22:41:06.563241 1203976 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 22:41:07.443993 1203976 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 22:41:07.447874 975333 quantize_finetune_llama.py:209] layer 27 gpu 1
I0325 22:41:07.460938 1203976 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 22:41:14.949467 1203976 finetune.py:45] layer 26_v initial loss 0.0001196568482555449
W0325 22:41:14.949709 1203976 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 22:41:51.093273 1203976 finetune.py:68] layer 26_v @ epoch 0 new loss 7.696689863223583e-05 old loss 0.0001196568482555449 BETTER
I0325 22:42:08.448535 975333 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 60.58663296699524s
I0325 22:42:11.796190 1204827 config.py:54] PyTorch version 2.6.0 available.
W0325 22:42:12.090315 1204827 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 22:42:13.053334 1204827 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 22:42:13.057358 975333 quantize_finetune_llama.py:209] layer 28 gpu 0
I0325 22:42:13.070612 1204827 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 22:42:19.927392 1204827 finetune.py:45] layer 27_v initial loss 0.00011551145144039765
W0325 22:42:19.927652 1204827 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 22:42:28.092239 1203976 finetune.py:68] layer 26_v @ epoch 1 new loss 7.166772411437705e-05 old loss 7.696689863223583e-05 BETTER
I0325 22:42:53.332613 1204827 finetune.py:68] layer 27_v @ epoch 0 new loss 7.060788630042225e-05 old loss 0.00011551145144039765 BETTER
I0325 22:43:05.070892 1203976 finetune.py:68] layer 26_v @ epoch 2 new loss 6.862486043246463e-05 old loss 7.166772411437705e-05 BETTER
I0325 22:43:27.723343 1204827 finetune.py:68] layer 27_v @ epoch 1 new loss 6.634539022343233e-05 old loss 7.060788630042225e-05 BETTER
I0325 22:43:42.271900 1203976 finetune.py:68] layer 26_v @ epoch 3 new loss 6.653670425293967e-05 old loss 6.862486043246463e-05 BETTER
I0325 22:44:02.427341 1204827 finetune.py:68] layer 27_v @ epoch 2 new loss 6.404856685549021e-05 old loss 6.634539022343233e-05 BETTER
I0325 22:44:19.173184 1203976 finetune.py:68] layer 26_v @ epoch 4 new loss 6.494847912108526e-05 old loss 6.653670425293967e-05 BETTER
I0325 22:44:28.917542 1203976 finetune.py:45] layer 26_q initial loss 8.335078018717468e-05
I0325 22:44:37.320882 1204827 finetune.py:68] layer 27_v @ epoch 3 new loss 6.250423029996455e-05 old loss 6.404856685549021e-05 BETTER
I0325 22:45:04.771535 1203976 finetune.py:68] layer 26_q @ epoch 0 new loss 7.935386383906007e-05 old loss 8.335078018717468e-05 BETTER
I0325 22:45:12.373541 1204827 finetune.py:68] layer 27_v @ epoch 4 new loss 6.115227733971551e-05 old loss 6.250423029996455e-05 BETTER
I0325 22:45:21.834459 1204827 finetune.py:45] layer 27_q initial loss 9.32803814066574e-05
I0325 22:45:41.653064 1203976 finetune.py:68] layer 26_q @ epoch 1 new loss 7.744538015685976e-05 old loss 7.935386383906007e-05 BETTER
I0325 22:45:55.622284 1204827 finetune.py:68] layer 27_q @ epoch 0 new loss 8.523064025212079e-05 old loss 9.32803814066574e-05 BETTER
I0325 22:46:18.657836 1203976 finetune.py:68] layer 26_q @ epoch 2 new loss 7.604963320773095e-05 old loss 7.744538015685976e-05 BETTER
I0325 22:46:30.054089 1204827 finetune.py:68] layer 27_q @ epoch 1 new loss 8.324190275743604e-05 old loss 8.523064025212079e-05 BETTER
I0325 22:46:55.593750 1203976 finetune.py:68] layer 26_q @ epoch 3 new loss 7.489002746297047e-05 old loss 7.604963320773095e-05 BETTER
I0325 22:47:04.738852 1204827 finetune.py:68] layer 27_q @ epoch 2 new loss 8.186241757357493e-05 old loss 8.324190275743604e-05 BETTER
I0325 22:47:32.599623 1203976 finetune.py:68] layer 26_q @ epoch 4 new loss 7.397105218842626e-05 old loss 7.489002746297047e-05 BETTER
I0325 22:47:39.364319 1204827 finetune.py:68] layer 27_q @ epoch 3 new loss 8.084584260359406e-05 old loss 8.186241757357493e-05 BETTER
I0325 22:47:40.704620 1203976 finetune.py:45] layer 26_k initial loss 8.091200288617983e-05
I0325 22:48:14.045830 1204827 finetune.py:68] layer 27_q @ epoch 4 new loss 7.993748295120895e-05 old loss 8.084584260359406e-05 BETTER
I0325 22:48:16.630458 1203976 finetune.py:68] layer 26_k @ epoch 0 new loss 7.96670065028593e-05 old loss 8.091200288617983e-05 BETTER
I0325 22:48:21.919988 1204827 finetune.py:45] layer 27_k initial loss 9.219743515131995e-05
I0325 22:48:53.137726 1203976 finetune.py:68] layer 26_k @ epoch 1 new loss 7.89603654993698e-05 old loss 7.96670065028593e-05 BETTER
I0325 22:48:55.627947 1204827 finetune.py:68] layer 27_k @ epoch 0 new loss 9.05323977349326e-05 old loss 9.219743515131995e-05 BETTER
I0325 22:49:29.870007 1203976 finetune.py:68] layer 26_k @ epoch 2 new loss 7.834135612938553e-05 old loss 7.89603654993698e-05 BETTER
I0325 22:49:29.996073 1204827 finetune.py:68] layer 27_k @ epoch 1 new loss 8.972798241302371e-05 old loss 9.05323977349326e-05 BETTER
I0325 22:50:04.462995 1204827 finetune.py:68] layer 27_k @ epoch 2 new loss 8.915961370803416e-05 old loss 8.972798241302371e-05 BETTER
I0325 22:50:06.533247 1203976 finetune.py:68] layer 26_k @ epoch 3 new loss 7.78534886194393e-05 old loss 7.834135612938553e-05 BETTER
I0325 22:50:39.035884 1204827 finetune.py:68] layer 27_k @ epoch 3 new loss 8.870047167874873e-05 old loss 8.915961370803416e-05 BETTER
I0325 22:50:43.431309 1203976 finetune.py:68] layer 26_k @ epoch 4 new loss 7.739999273326248e-05 old loss 7.78534886194393e-05 BETTER
I0325 22:50:53.201755 1203976 finetune.py:45] layer 26_o initial loss 0.00022729174816049635
I0325 22:51:13.518293 1204827 finetune.py:68] layer 27_k @ epoch 4 new loss 8.828187128528953e-05 old loss 8.870047167874873e-05 BETTER
I0325 22:51:22.999420 1204827 finetune.py:45] layer 27_o initial loss 0.00026471135788597167
I0325 22:51:28.370960 1203976 finetune.py:68] layer 26_o @ epoch 0 new loss 0.00021116057178005576 old loss 0.00022729174816049635 BETTER
I0325 22:51:56.317364 1204827 finetune.py:68] layer 27_o @ epoch 0 new loss 0.0002462481497786939 old loss 0.00026471135788597167 BETTER
I0325 22:52:04.450286 1203976 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0002057139208773151 old loss 0.00021116057178005576 BETTER
I0325 22:52:30.156030 1204827 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0002403687685728073 old loss 0.0002462481497786939 BETTER
I0325 22:52:40.566816 1203976 finetune.py:68] layer 26_o @ epoch 2 new loss 0.00020194327225908637 old loss 0.0002057139208773151 BETTER
I0325 22:53:04.029580 1204827 finetune.py:68] layer 27_o @ epoch 2 new loss 0.00023630440409760922 old loss 0.0002403687685728073 BETTER
I0325 22:53:16.399992 1203976 finetune.py:68] layer 26_o @ epoch 3 new loss 0.00019899576727766544 old loss 0.00020194327225908637 BETTER
I0325 22:53:37.998970 1204827 finetune.py:68] layer 27_o @ epoch 3 new loss 0.00023308649542741477 old loss 0.00023630440409760922 BETTER
I0325 22:53:52.140554 1203976 finetune.py:68] layer 26_o @ epoch 4 new loss 0.0001965697156265378 old loss 0.00019899576727766544 BETTER
I0325 22:54:11.785127 1204827 finetune.py:68] layer 27_o @ epoch 4 new loss 0.00023063752450980246 old loss 0.00023308649542741477 BETTER
I0325 22:54:14.000154 1203976 finetune.py:45] layer 26_up initial loss 0.0005554366507567465
I0325 22:54:33.360192 1204827 finetune.py:45] layer 27_up initial loss 0.0006495580892078578
I0325 22:54:46.230771 1203976 finetune.py:68] layer 26_up @ epoch 0 new loss 0.0005383945535868406 old loss 0.0005554366507567465 BETTER
I0325 22:55:04.210566 1204827 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0006264643161557615 old loss 0.0006495580892078578 BETTER
I0325 22:55:19.599947 1203976 finetune.py:68] layer 26_up @ epoch 1 new loss 0.000530070683453232 old loss 0.0005383945535868406 BETTER
I0325 22:55:36.344945 1204827 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0006162699428386986 old loss 0.0006264643161557615 BETTER
I0325 22:55:53.295526 1203976 finetune.py:68] layer 26_up @ epoch 2 new loss 0.0005237300065346062 old loss 0.000530070683453232 BETTER
I0325 22:56:08.496917 1204827 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0006087442743591964 old loss 0.0006162699428386986 BETTER
I0325 22:56:27.071974 1203976 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0005184129113331437 old loss 0.0005237300065346062 BETTER
I0325 22:56:40.545729 1204827 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0006025342736393213 old loss 0.0006087442743591964 BETTER
I0325 22:57:00.781840 1203976 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0005138414562679827 old loss 0.0005184129113331437 BETTER
I0325 22:57:12.635937 1204827 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0005971165955998003 old loss 0.0006025342736393213 BETTER
I0325 22:57:22.779415 1203976 finetune.py:45] layer 26_gate initial loss 0.0006714415503665805
I0325 22:57:34.321326 1204827 finetune.py:45] layer 27_gate initial loss 0.0007916775648482144
I0325 22:57:53.074338 1203976 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0006662358064204454 old loss 0.0006714415503665805 BETTER
I0325 22:58:03.021896 1204827 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0007838425226509571 old loss 0.0007916775648482144 BETTER
I0325 22:58:24.294084 1203976 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.0006621928187087178 old loss 0.0006662358064204454 BETTER
I0325 22:58:32.526471 1204827 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.000778823799919337 old loss 0.0007838425226509571 BETTER
I0325 22:58:55.511645 1203976 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0006586689851246774 old loss 0.0006621928187087178 BETTER
I0325 22:59:02.250149 1204827 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.000774563173763454 old loss 0.000778823799919337 BETTER
I0325 22:59:26.973078 1203976 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0006554524297825992 old loss 0.0006586689851246774 BETTER
I0325 22:59:32.046464 1204827 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0007707803160883486 old loss 0.000774563173763454 BETTER
I0325 22:59:58.664479 1203976 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0006526137585751712 old loss 0.0006554524297825992 BETTER
I0325 23:00:02.001106 1204827 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.0007674010121263564 old loss 0.0007707803160883486 BETTER
I0325 23:00:21.730901 1203976 finetune.py:45] layer 26_down initial loss 0.001067283097654581
I0325 23:00:24.430941 1204827 finetune.py:45] layer 27_down initial loss 0.0012761319521814585
I0325 23:00:50.133915 1203976 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0010671402560546994 old loss 0.001067283097654581 BETTER
I0325 23:00:51.343632 1204827 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0012759309029206634 old loss 0.0012761319521814585 BETTER
I0325 23:01:19.007284 1204827 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0012757456861436367 old loss 0.0012759309029206634 BETTER
I0325 23:01:19.390030 1203976 finetune.py:68] layer 26_down @ epoch 1 new loss 0.00106701604090631 old loss 0.0010671402560546994 BETTER
I0325 23:01:46.758937 1204827 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0012755899224430323 old loss 0.0012757456861436367 BETTER
I0325 23:01:48.798092 1203976 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0010669014882296324 old loss 0.00106701604090631 BETTER
I0325 23:02:14.523552 1204827 finetune.py:68] layer 27_down @ epoch 3 new loss 0.001275450922548771 old loss 0.0012755899224430323 BETTER
I0325 23:02:18.390979 1203976 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0010667952010408044 old loss 0.0010669014882296324 BETTER
I0325 23:02:42.515725 1204827 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0012753208866342902 old loss 0.001275450922548771 BETTER
27_v proxy err 0.02738705277442932 tr(WHW.T) 677.69384765625
bpp_loss 2.693069577217102
27_q proxy err 0.0037718252278864384 tr(WHW.T) 21332.44140625
bpp_loss 2.9620773792266846
27_k proxy err 0.0016322677256539464 tr(WHW.T) 14065.0400390625
bpp_loss 3.7065515518188477
27_o proxy err 0.037033163011074066 tr(WHW.T) 2165.4326171875
bpp_loss 2.5350942611694336
27_up proxy err 0.03868689388036728 tr(WHW.T) 8478.3583984375
bpp_loss 2.5073536464146207
27_gate proxy err 0.01091949176043272 tr(WHW.T) 32680.58984375
bpp_loss 2.8579909460885182
27_down proxy err 0.03651474416255951 tr(WHW.T) 6707.0205078125
bpp_loss 2.4997407027653287
I0325 23:02:48.027059 1203976 finetune.py:68] layer 26_down @ epoch 4 new loss 0.0010666949674487114 old loss 0.0010667952010408044 BETTER
26_v proxy err 0.03974016755819321 tr(WHW.T) 434.54583740234375
bpp_loss 2.606510877609253
26_q proxy err 0.0035558040253818035 tr(WHW.T) 21425.0859375
bpp_loss 2.9906351566314697
26_k proxy err 0.0014059159439057112 tr(WHW.T) 15470.7431640625
bpp_loss 3.7486437559127808
26_o proxy err 0.03245045617222786 tr(WHW.T) 2391.002685546875
bpp_loss 2.4947623014450073
26_up proxy err 0.04240575060248375 tr(WHW.T) 7657.51025390625
bpp_loss 2.492487362452916
26_gate proxy err 0.012271314859390259 tr(WHW.T) 28802.591796875
bpp_loss 2.848677090236119
26_down proxy err 0.042263295501470566 tr(WHW.T) 6732.38720703125
bpp_loss 2.4894533157348633
I0325 23:03:56.331683 975333 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 63.37276244163513s
I0325 23:03:59.574014 1220501 config.py:54] PyTorch version 2.6.0 available.
W0325 23:03:59.859294 1220501 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 23:04:00.744817 1220501 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 23:04:00.748859 975333 quantize_finetune_llama.py:209] layer 29 gpu 1
I0325 23:04:00.768208 1220501 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 23:04:08.185112 1220501 finetune.py:45] layer 28_v initial loss 0.00018576451111584902
W0325 23:04:08.185315 1220501 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 23:04:44.550575 1220501 finetune.py:68] layer 28_v @ epoch 0 new loss 0.00010302125883754343 old loss 0.00018576451111584902 BETTER
I0325 23:05:01.225585 975333 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 60.07337021827698s
I0325 23:05:04.618545 1221325 config.py:54] PyTorch version 2.6.0 available.
W0325 23:05:04.912669 1221325 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 23:05:05.811071 1221325 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 23:05:05.815662 975333 quantize_finetune_llama.py:209] layer 30 gpu 0
I0325 23:05:05.829807 1221325 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 23:05:12.754573 1221325 finetune.py:45] layer 29_v initial loss 0.00018263769743498415
W0325 23:05:12.754806 1221325 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 23:05:22.063467 1220501 finetune.py:68] layer 28_v @ epoch 1 new loss 9.645658428780735e-05 old loss 0.00010302125883754343 BETTER
I0325 23:05:46.229497 1221325 finetune.py:68] layer 29_v @ epoch 0 new loss 0.00011315396113786846 old loss 0.00018263769743498415 BETTER
I0325 23:05:59.481794 1220501 finetune.py:68] layer 28_v @ epoch 2 new loss 9.277969365939498e-05 old loss 9.645658428780735e-05 BETTER
I0325 23:06:20.776696 1221325 finetune.py:68] layer 29_v @ epoch 1 new loss 0.00010715088137658313 old loss 0.00011315396113786846 BETTER
I0325 23:06:36.800869 1220501 finetune.py:68] layer 28_v @ epoch 3 new loss 9.016053809318691e-05 old loss 9.277969365939498e-05 BETTER
I0325 23:06:55.574443 1221325 finetune.py:68] layer 29_v @ epoch 2 new loss 0.00010367251525167376 old loss 0.00010715088137658313 BETTER
I0325 23:07:14.356748 1220501 finetune.py:68] layer 28_v @ epoch 4 new loss 8.844651893014088e-05 old loss 9.016053809318691e-05 BETTER
I0325 23:07:24.149382 1220501 finetune.py:45] layer 28_q initial loss 0.00012366128794383258
I0325 23:07:30.801214 1221325 finetune.py:68] layer 29_v @ epoch 3 new loss 0.00010184964048676193 old loss 0.00010367251525167376 BETTER
I0325 23:08:00.225188 1220501 finetune.py:68] layer 28_q @ epoch 0 new loss 0.00011437475041020662 old loss 0.00012366128794383258 BETTER
I0325 23:08:05.828291 1221325 finetune.py:68] layer 29_v @ epoch 4 new loss 0.00010006674710894004 old loss 0.00010184964048676193 BETTER
I0325 23:08:15.364765 1221325 finetune.py:45] layer 29_q initial loss 0.00019712721405085176
I0325 23:08:37.091007 1220501 finetune.py:68] layer 28_q @ epoch 1 new loss 0.00011158018605783582 old loss 0.00011437475041020662 BETTER
I0325 23:08:49.042483 1221325 finetune.py:68] layer 29_q @ epoch 0 new loss 0.00016787106869742274 old loss 0.00019712721405085176 BETTER
I0325 23:09:13.808486 1220501 finetune.py:68] layer 28_q @ epoch 2 new loss 0.00010950497380690649 old loss 0.00011158018605783582 BETTER
I0325 23:09:23.475528 1221325 finetune.py:68] layer 29_q @ epoch 1 new loss 0.00016120605869218707 old loss 0.00016787106869742274 BETTER
I0325 23:09:50.566913 1220501 finetune.py:68] layer 28_q @ epoch 3 new loss 0.00010786577331600711 old loss 0.00010950497380690649 BETTER
I0325 23:09:57.813512 1221325 finetune.py:68] layer 29_q @ epoch 2 new loss 0.00015679259377066046 old loss 0.00016120605869218707 BETTER
I0325 23:10:27.372263 1220501 finetune.py:68] layer 28_q @ epoch 4 new loss 0.00010649242904037237 old loss 0.00010786577331600711 BETTER
I0325 23:10:32.204585 1221325 finetune.py:68] layer 29_q @ epoch 3 new loss 0.00015329652524087578 old loss 0.00015679259377066046 BETTER
I0325 23:10:35.571167 1220501 finetune.py:45] layer 28_k initial loss 0.00011980195995420218
I0325 23:11:06.673898 1221325 finetune.py:68] layer 29_q @ epoch 4 new loss 0.0001506769476691261 old loss 0.00015329652524087578 BETTER
I0325 23:11:11.172042 1220501 finetune.py:68] layer 28_k @ epoch 0 new loss 0.00011746611562557518 old loss 0.00011980195995420218 BETTER
I0325 23:11:14.610745 1221325 finetune.py:45] layer 29_k initial loss 0.00017268701049033552
I0325 23:11:47.688349 1220501 finetune.py:68] layer 28_k @ epoch 1 new loss 0.00011641686432994902 old loss 0.00011746611562557518 BETTER
I0325 23:11:48.207576 1221325 finetune.py:68] layer 29_k @ epoch 0 new loss 0.0001679080887697637 old loss 0.00017268701049033552 BETTER
I0325 23:12:22.483002 1221325 finetune.py:68] layer 29_k @ epoch 1 new loss 0.00016582533135078847 old loss 0.0001679080887697637 BETTER
I0325 23:12:24.178282 1220501 finetune.py:68] layer 28_k @ epoch 2 new loss 0.00011562490544747561 old loss 0.00011641686432994902 BETTER
I0325 23:12:56.960414 1221325 finetune.py:68] layer 29_k @ epoch 2 new loss 0.00016429850074928254 old loss 0.00016582533135078847 BETTER
I0325 23:13:00.793270 1220501 finetune.py:68] layer 28_k @ epoch 3 new loss 0.00011482773697935045 old loss 0.00011562490544747561 BETTER
I0325 23:13:31.637421 1221325 finetune.py:68] layer 29_k @ epoch 3 new loss 0.00016299766139127314 old loss 0.00016429850074928254 BETTER
I0325 23:13:37.587864 1220501 finetune.py:68] layer 28_k @ epoch 4 new loss 0.00011427636491134763 old loss 0.00011482773697935045 BETTER
I0325 23:13:47.403198 1220501 finetune.py:45] layer 28_o initial loss 0.00034587044501677155
I0325 23:14:06.119131 1221325 finetune.py:68] layer 29_k @ epoch 4 new loss 0.00016195604985114187 old loss 0.00016299766139127314 BETTER
I0325 23:14:15.532744 1221325 finetune.py:45] layer 29_o initial loss 0.00041029779822565615
I0325 23:14:22.854372 1220501 finetune.py:68] layer 28_o @ epoch 0 new loss 0.00031770390341989696 old loss 0.00034587044501677155 BETTER
I0325 23:14:48.729250 1221325 finetune.py:68] layer 29_o @ epoch 0 new loss 0.00037762350984849036 old loss 0.00041029779822565615 BETTER
I0325 23:14:58.698037 1220501 finetune.py:68] layer 28_o @ epoch 1 new loss 0.00030837368103675544 old loss 0.00031770390341989696 BETTER
I0325 23:15:22.787904 1221325 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0003676731139421463 old loss 0.00037762350984849036 BETTER
I0325 23:15:34.654172 1220501 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0003014664980582893 old loss 0.00030837368103675544 BETTER
I0325 23:15:56.773056 1221325 finetune.py:68] layer 29_o @ epoch 2 new loss 0.0003613425069488585 old loss 0.0003676731139421463 BETTER
I0325 23:16:10.598509 1220501 finetune.py:68] layer 28_o @ epoch 3 new loss 0.000296199694275856 old loss 0.0003014664980582893 BETTER
I0325 23:16:30.672006 1221325 finetune.py:68] layer 29_o @ epoch 3 new loss 0.0003559183678589761 old loss 0.0003613425069488585 BETTER
I0325 23:16:46.510324 1220501 finetune.py:68] layer 28_o @ epoch 4 new loss 0.00029185437597334385 old loss 0.000296199694275856 BETTER
I0325 23:17:04.545849 1221325 finetune.py:68] layer 29_o @ epoch 4 new loss 0.00035171012859791517 old loss 0.0003559183678589761 BETTER
I0325 23:17:08.985382 1220501 finetune.py:45] layer 28_up initial loss 0.0008229211089201272
I0325 23:17:26.199223 1221325 finetune.py:45] layer 29_up initial loss 0.0011085143778473139
I0325 23:17:41.530162 1220501 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0007873148424550891 old loss 0.0008229211089201272 BETTER
I0325 23:17:57.053862 1221325 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0010495927417650819 old loss 0.0011085143778473139 BETTER
I0325 23:18:14.917569 1220501 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0007730005891062319 old loss 0.0007873148424550891 BETTER
I0325 23:18:29.076587 1221325 finetune.py:68] layer 29_up @ epoch 1 new loss 0.001027253339998424 old loss 0.0010495927417650819 BETTER
I0325 23:18:48.612237 1220501 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0007622925913892686 old loss 0.0007730005891062319 BETTER
I0325 23:19:01.193204 1221325 finetune.py:68] layer 29_up @ epoch 2 new loss 0.00101115508005023 old loss 0.001027253339998424 BETTER
I0325 23:19:22.402288 1220501 finetune.py:68] layer 28_up @ epoch 3 new loss 0.0007536251214332879 old loss 0.0007622925913892686 BETTER
I0325 23:19:33.303745 1221325 finetune.py:68] layer 29_up @ epoch 3 new loss 0.000997892813757062 old loss 0.00101115508005023 BETTER
I0325 23:19:56.338749 1220501 finetune.py:68] layer 28_up @ epoch 4 new loss 0.000746109988540411 old loss 0.0007536251214332879 BETTER
I0325 23:20:05.426531 1221325 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0009868824854493141 old loss 0.000997892813757062 BETTER
I0325 23:20:18.260600 1220501 finetune.py:45] layer 28_gate initial loss 0.0010032739955931902
I0325 23:20:27.335925 1221325 finetune.py:45] layer 29_gate initial loss 0.001341744908131659
I0325 23:20:48.622528 1220501 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0009916307171806693 old loss 0.0010032739955931902 BETTER
I0325 23:20:55.997355 1221325 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0013232267228886485 old loss 0.001341744908131659 BETTER
I0325 23:21:19.959018 1220501 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0009843568550422788 old loss 0.0009916307171806693 BETTER
I0325 23:21:25.602372 1221325 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.0013125173281878233 old loss 0.0013232267228886485 BETTER
I0325 23:21:51.278115 1220501 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.0009780239779502153 old loss 0.0009843568550422788 BETTER
I0325 23:21:55.389207 1221325 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0013037753524258733 old loss 0.0013125173281878233 BETTER
I0325 23:22:22.919700 1220501 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0009727111901156604 old loss 0.0009780239779502153 BETTER
I0325 23:22:25.146049 1221325 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0012958983425050974 old loss 0.0013037753524258733 BETTER
I0325 23:22:54.689077 1220501 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.0009677846101112664 old loss 0.0009727111901156604 BETTER
I0325 23:22:54.963130 1221325 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0012891382211819291 old loss 0.0012958983425050974 BETTER
I0325 23:23:17.424818 1221325 finetune.py:45] layer 29_down initial loss 0.0022597224451601505
I0325 23:23:17.603961 1220501 finetune.py:45] layer 28_down initial loss 0.0016503303777426481
I0325 23:23:44.365974 1221325 finetune.py:68] layer 29_down @ epoch 0 new loss 0.0022590605076402426 old loss 0.0022597224451601505 BETTER
I0325 23:23:46.238142 1220501 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0016500558704137802 old loss 0.0016503303777426481 BETTER
I0325 23:24:12.077262 1221325 finetune.py:68] layer 29_down @ epoch 1 new loss 0.002258479595184326 old loss 0.0022590605076402426 BETTER
I0325 23:24:15.647233 1220501 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0016498132608830929 old loss 0.0016500558704137802 BETTER
I0325 23:24:39.899571 1221325 finetune.py:68] layer 29_down @ epoch 2 new loss 0.002257970627397299 old loss 0.002258479595184326 BETTER
I0325 23:24:45.085888 1220501 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0016495853196829557 old loss 0.0016498132608830929 BETTER
I0325 23:25:07.912813 1221325 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0022575007751584053 old loss 0.002257970627397299 BETTER
I0325 23:25:14.616482 1220501 finetune.py:68] layer 28_down @ epoch 3 new loss 0.0016493859002366662 old loss 0.0016495853196829557 BETTER
I0325 23:25:35.960355 1221325 finetune.py:68] layer 29_down @ epoch 4 new loss 0.002257079351693392 old loss 0.0022575007751584053 BETTER
29_v proxy err 0.024191314354538918 tr(WHW.T) 850.4290161132812
bpp_loss 2.8044910430908203
29_q proxy err 0.004441418685019016 tr(WHW.T) 20669.4453125
bpp_loss 2.964335799217224
29_k proxy err 0.0015895467950031161 tr(WHW.T) 16462.392578125
bpp_loss 3.7488839626312256
29_o proxy err 0.022187594324350357 tr(WHW.T) 3112.523193359375
bpp_loss 2.595513701438904
29_up proxy err 0.025806086137890816 tr(WHW.T) 12855.7041015625
bpp_loss 2.5602239881243025
29_gate proxy err 0.0093216048553586 tr(WHW.T) 38147.59765625
bpp_loss 2.8351364135742188
29_down proxy err 0.03172048181295395 tr(WHW.T) 7641.96728515625
bpp_loss 2.5269740309034074
I0325 23:25:44.338157 1220501 finetune.py:68] layer 28_down @ epoch 4 new loss 0.001649200450628996 old loss 0.0016493859002366662 BETTER
28_v proxy err 0.030978430062532425 tr(WHW.T) 601.4844360351562
bpp_loss 2.7394168376922607
28_q proxy err 0.003534297225996852 tr(WHW.T) 23196.9453125
bpp_loss 2.9727929830551147
28_k proxy err 0.001519892131909728 tr(WHW.T) 15068.578125
bpp_loss 3.674659490585327
28_o proxy err 0.032258693128824234 tr(WHW.T) 2525.586181640625
bpp_loss 2.563110589981079
28_up proxy err 0.03187992796301842 tr(WHW.T) 10244.7470703125
bpp_loss 2.5281930650983537
28_gate proxy err 0.009880967438220978 tr(WHW.T) 35764.015625
bpp_loss 2.843986919948033
28_down proxy err 0.03628622740507126 tr(WHW.T) 7370.9970703125
bpp_loss 2.5138136999947682
I0325 23:26:52.067468 975333 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 62.82428526878357s
I0325 23:26:55.508251 1237042 config.py:54] PyTorch version 2.6.0 available.
W0325 23:26:55.805487 1237042 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 23:26:56.734440 1237042 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 23:26:56.738553 975333 quantize_finetune_llama.py:209] layer 31 gpu 1
I0325 23:26:56.760198 1237042 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 23:27:04.284608 1237042 finetune.py:45] layer 30_v initial loss 0.00033520147553645074
W0325 23:27:04.284819 1237042 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 23:27:40.218733 1237042 finetune.py:68] layer 30_v @ epoch 0 new loss 0.00021191111591178924 old loss 0.00033520147553645074 BETTER
I0325 23:27:58.663913 975333 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 61.47942805290222s
I0325 23:28:02.089875 1237874 config.py:54] PyTorch version 2.6.0 available.
W0325 23:28:02.388820 1237874 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 23:28:03.330923 1237874 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 23:28:03.348260 1237874 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 23:28:10.226846 1237874 finetune.py:45] layer 31_v initial loss 0.000834841572213918
W0325 23:28:10.227136 1237874 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 23:28:17.428229 1237042 finetune.py:68] layer 30_v @ epoch 1 new loss 0.00019838157459162176 old loss 0.00021191111591178924 BETTER
I0325 23:28:43.782686 1237874 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0003727848525159061 old loss 0.000834841572213918 BETTER
I0325 23:28:54.847437 1237042 finetune.py:68] layer 30_v @ epoch 2 new loss 0.0001907623664010316 old loss 0.00019838157459162176 BETTER
I0325 23:29:18.490522 1237874 finetune.py:68] layer 31_v @ epoch 1 new loss 0.000343636202160269 old loss 0.0003727848525159061 BETTER
I0325 23:29:32.306626 1237042 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0001859692856669426 old loss 0.0001907623664010316 BETTER
I0325 23:29:53.482654 1237874 finetune.py:68] layer 31_v @ epoch 2 new loss 0.0003326202277094126 old loss 0.000343636202160269 BETTER
I0325 23:30:09.731428 1237042 finetune.py:68] layer 30_v @ epoch 4 new loss 0.00018166482914239168 old loss 0.0001859692856669426 BETTER
I0325 23:30:19.835900 1237042 finetune.py:45] layer 30_q initial loss 0.0002599262516014278
I0325 23:30:28.676525 1237874 finetune.py:68] layer 31_v @ epoch 3 new loss 0.00031317435787059367 old loss 0.0003326202277094126 BETTER
I0325 23:30:55.948423 1237042 finetune.py:68] layer 30_q @ epoch 0 new loss 0.00023372078430838883 old loss 0.0002599262516014278 BETTER
I0325 23:31:03.876810 1237874 finetune.py:76] layer 31_v @ epoch 4 new loss 0.00031498787575401366 old loss 0.00031317435787059367 WORSE
I0325 23:31:12.957571 1237874 finetune.py:45] layer 31_q initial loss 0.0006405984750017524
I0325 23:31:32.845984 1237042 finetune.py:68] layer 30_q @ epoch 1 new loss 0.00022813075338490307 old loss 0.00023372078430838883 BETTER
I0325 23:31:46.938600 1237874 finetune.py:68] layer 31_q @ epoch 0 new loss 0.00044519518269225955 old loss 0.0006405984750017524 BETTER
I0325 23:32:09.622158 1237042 finetune.py:68] layer 30_q @ epoch 2 new loss 0.00022433082631323487 old loss 0.00022813075338490307 BETTER
I0325 23:32:21.643525 1237874 finetune.py:68] layer 31_q @ epoch 1 new loss 0.00040099851321429014 old loss 0.00044519518269225955 BETTER
I0325 23:32:46.489343 1237042 finetune.py:68] layer 30_q @ epoch 3 new loss 0.00022110794088803232 old loss 0.00022433082631323487 BETTER
I0325 23:32:56.387239 1237874 finetune.py:68] layer 31_q @ epoch 2 new loss 0.0003787116438616067 old loss 0.00040099851321429014 BETTER
I0325 23:33:23.492406 1237042 finetune.py:68] layer 30_q @ epoch 4 new loss 0.000218881294131279 old loss 0.00022110794088803232 BETTER
I0325 23:33:31.097995 1237874 finetune.py:68] layer 31_q @ epoch 3 new loss 0.00036595386336557567 old loss 0.0003787116438616067 BETTER
I0325 23:33:31.339989 1237042 finetune.py:45] layer 30_k initial loss 0.0002569591742940247
I0325 23:34:05.868726 1237874 finetune.py:68] layer 31_q @ epoch 4 new loss 0.00035540267708711326 old loss 0.00036595386336557567 BETTER
I0325 23:34:07.331201 1237042 finetune.py:68] layer 30_k @ epoch 0 new loss 0.0002487003512214869 old loss 0.0002569591742940247 BETTER
I0325 23:34:13.641045 1237874 finetune.py:45] layer 31_k initial loss 0.0005038172239437699
I0325 23:34:44.045769 1237042 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0002468024904374033 old loss 0.0002487003512214869 BETTER
I0325 23:34:47.410765 1237874 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0004364372289273888 old loss 0.0005038172239437699 BETTER
I0325 23:35:20.793603 1237042 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0002459096722304821 old loss 0.0002468024904374033 BETTER
I0325 23:35:21.869714 1237874 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0004221198323648423 old loss 0.0004364372289273888 BETTER
I0325 23:35:56.454251 1237874 finetune.py:68] layer 31_k @ epoch 2 new loss 0.00041248169145546854 old loss 0.0004221198323648423 BETTER
I0325 23:35:57.432126 1237042 finetune.py:68] layer 30_k @ epoch 3 new loss 0.00024275174655485898 old loss 0.0002459096722304821 BETTER
I0325 23:36:31.066009 1237874 finetune.py:68] layer 31_k @ epoch 3 new loss 0.00040662719402462244 old loss 0.00041248169145546854 BETTER
I0325 23:36:34.190609 1237042 finetune.py:68] layer 30_k @ epoch 4 new loss 0.0002417891228105873 old loss 0.00024275174655485898 BETTER
I0325 23:36:44.022930 1237042 finetune.py:45] layer 30_o initial loss 0.000698687566909939
I0325 23:37:05.670996 1237874 finetune.py:68] layer 31_k @ epoch 4 new loss 0.00040283415000885725 old loss 0.00040662719402462244 BETTER
I0325 23:37:15.172692 1237874 finetune.py:45] layer 31_o initial loss 0.0018010953208431602
I0325 23:37:19.260140 1237042 finetune.py:68] layer 30_o @ epoch 0 new loss 0.0006308307638391852 old loss 0.000698687566909939 BETTER
I0325 23:37:48.352162 1237874 finetune.py:68] layer 31_o @ epoch 0 new loss 0.001166065689176321 old loss 0.0018010953208431602 BETTER
I0325 23:37:55.312971 1237042 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0006141247577033937 old loss 0.0006308307638391852 BETTER
I0325 23:38:22.366140 1237874 finetune.py:68] layer 31_o @ epoch 1 new loss 0.001080897869542241 old loss 0.001166065689176321 BETTER
I0325 23:38:31.145537 1237042 finetune.py:68] layer 30_o @ epoch 2 new loss 0.000602405983954668 old loss 0.0006141247577033937 BETTER
I0325 23:38:56.329719 1237874 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0010273640509694815 old loss 0.001080897869542241 BETTER
I0325 23:39:07.217334 1237042 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0005933975335210562 old loss 0.000602405983954668 BETTER
I0325 23:39:30.256356 1237874 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0009888920467346907 old loss 0.0010273640509694815 BETTER
I0325 23:39:43.187482 1237042 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0005857022479176521 old loss 0.0005933975335210562 BETTER
I0325 23:40:04.213972 1237874 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0009592839633114636 old loss 0.0009888920467346907 BETTER
I0325 23:40:04.972972 1237042 finetune.py:45] layer 30_up initial loss 0.0021731997840106487
I0325 23:40:25.665473 1237874 finetune.py:45] layer 31_up initial loss 0.007613515947014093
I0325 23:40:37.239674 1237042 finetune.py:68] layer 30_up @ epoch 0 new loss 0.001980224857106805 old loss 0.0021731997840106487 BETTER
I0325 23:40:56.554604 1237874 finetune.py:68] layer 31_up @ epoch 0 new loss 0.006009495817124844 old loss 0.007613515947014093 BETTER
I0325 23:41:10.618710 1237042 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0019168172730132937 old loss 0.001980224857106805 BETTER
I0325 23:41:28.570147 1237874 finetune.py:68] layer 31_up @ epoch 1 new loss 0.005591975525021553 old loss 0.006009495817124844 BETTER
I0325 23:41:44.139528 1237042 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0018717254279181361 old loss 0.0019168172730132937 BETTER
I0325 23:42:00.713061 1237874 finetune.py:68] layer 31_up @ epoch 2 new loss 0.005319703835994005 old loss 0.005591975525021553 BETTER
I0325 23:42:17.953615 1237042 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0018358256202191114 old loss 0.0018717254279181361 BETTER
I0325 23:42:32.880101 1237874 finetune.py:68] layer 31_up @ epoch 3 new loss 0.005110377445816994 old loss 0.005319703835994005 BETTER
I0325 23:42:51.778561 1237042 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0018066783668473363 old loss 0.0018358256202191114 BETTER
I0325 23:43:05.059035 1237874 finetune.py:68] layer 31_up @ epoch 4 new loss 0.00493881618604064 old loss 0.005110377445816994 BETTER
I0325 23:43:13.582675 1237042 finetune.py:45] layer 30_gate initial loss 0.00238023535348475
I0325 23:43:26.816787 1237874 finetune.py:45] layer 31_gate initial loss 0.006543941330164671
I0325 23:43:44.113599 1237042 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0023087230511009693 old loss 0.00238023535348475 BETTER
I0325 23:43:55.573225 1237874 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.006078036036342382 old loss 0.006543941330164671 BETTER
I0325 23:44:15.495112 1237042 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.0022813603281974792 old loss 0.0023087230511009693 BETTER
I0325 23:44:25.142541 1237874 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.0058836485259234905 old loss 0.006078036036342382 BETTER
I0325 23:44:47.039973 1237042 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0022585697006434202 old loss 0.0022813603281974792 BETTER
I0325 23:44:54.809898 1237874 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.005744636990129948 old loss 0.0058836485259234905 BETTER
I0325 23:45:18.560929 1237042 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.0022381972521543503 old loss 0.0022585697006434202 BETTER
I0325 23:45:24.666385 1237874 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.005632074549794197 old loss 0.005744636990129948 BETTER
I0325 23:45:50.162065 1237042 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0022210150491446257 old loss 0.0022381972521543503 BETTER
I0325 23:45:54.612078 1237874 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.005537356715649366 old loss 0.005632074549794197 BETTER
I0325 23:46:12.995502 1237042 finetune.py:45] layer 30_down initial loss 0.004148499108850956
I0325 23:46:17.138599 1237874 finetune.py:45] layer 31_down initial loss 0.01499785203486681
I0325 23:46:41.308930 1237042 finetune.py:68] layer 30_down @ epoch 0 new loss 0.004145870450884104 old loss 0.004148499108850956 BETTER
I0325 23:46:44.071658 1237874 finetune.py:68] layer 31_down @ epoch 0 new loss 0.014969506300985813 old loss 0.01499785203486681 BETTER
I0325 23:47:10.411056 1237042 finetune.py:68] layer 30_down @ epoch 1 new loss 0.004143444821238518 old loss 0.004145870450884104 BETTER
I0325 23:47:11.740450 1237874 finetune.py:68] layer 31_down @ epoch 1 new loss 0.014945636503398418 old loss 0.014969506300985813 BETTER
I0325 23:47:39.468367 1237874 finetune.py:68] layer 31_down @ epoch 2 new loss 0.014926197938621044 old loss 0.014945636503398418 BETTER
I0325 23:47:39.796809 1237042 finetune.py:68] layer 30_down @ epoch 2 new loss 0.00414128415286541 old loss 0.004143444821238518 BETTER
I0325 23:48:07.198040 1237874 finetune.py:68] layer 31_down @ epoch 3 new loss 0.014909601770341396 old loss 0.014926197938621044 BETTER
I0325 23:48:09.429846 1237042 finetune.py:68] layer 30_down @ epoch 3 new loss 0.004139244090765715 old loss 0.00414128415286541 BETTER
I0325 23:48:35.042673 1237874 finetune.py:68] layer 31_down @ epoch 4 new loss 0.014896032400429249 old loss 0.014909601770341396 BETTER
31_v proxy err 0.012877728790044785 tr(WHW.T) 1808.723876953125
bpp_loss 2.8981378078460693
31_q proxy err 0.0024857409298419952 tr(WHW.T) 46220.24609375
bpp_loss 3.0102750062942505
31_k proxy err 0.001567147788591683 tr(WHW.T) 20612.078125
bpp_loss 3.6563825607299805
31_o proxy err 0.02004687860608101 tr(WHW.T) 2255.168701171875
bpp_loss 2.631263017654419
31_up proxy err 0.006005703471601009 tr(WHW.T) 68838.75
bpp_loss 2.7694808415004184
31_gate proxy err 0.0031041111797094345 tr(WHW.T) 144101.984375
bpp_loss 3.073749133518764
31_down proxy err 0.013734414242208004 tr(WHW.T) 10415.1025390625
bpp_loss 2.546602317265102
I0325 23:48:38.909132 1237042 finetune.py:68] layer 30_down @ epoch 4 new loss 0.004137428477406502 old loss 0.004139244090765715 BETTER
30_v proxy err 0.02269272319972515 tr(WHW.T) 863.060791015625
bpp_loss 3.0591189861297607
30_q proxy err 0.0034344776067882776 tr(WHW.T) 24080.2109375
bpp_loss 2.8731144666671753
30_k proxy err 0.0016489996341988444 tr(WHW.T) 14121.572265625
bpp_loss 3.454119324684143
30_o proxy err 0.017446881160140038 tr(WHW.T) 4889.0498046875
bpp_loss 2.6759594678878784
30_up proxy err 0.016121765598654747 tr(WHW.T) 21678.583984375
bpp_loss 2.5904150009155273
30_gate proxy err 0.007208869326859713 tr(WHW.T) 51498.97265625
bpp_loss 2.8799586977277483
30_down proxy err 0.02229374088346958 tr(WHW.T) 9048.7001953125
bpp_loss 2.5258778503962924
I0325 23:48:57.798812 1253469 config.py:54] PyTorch version 2.6.0 available.
W0325 23:48:58.106273 1253469 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 23:48:58.346215 1253469 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.30it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  9.16it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  9.48it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  9.95it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 10.18it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.90it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  9.76it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00, 10.06it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  9.99it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.89it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 10.00it/s]
I0325 23:49:01.607027 1253469 hfize_llama.py:153] loaded layer 0
I0325 23:49:02.537338 1253469 hfize_llama.py:153] loaded layer 1
I0325 23:49:03.435670 1253469 hfize_llama.py:153] loaded layer 2
I0325 23:49:04.268407 1253469 hfize_llama.py:153] loaded layer 3
I0325 23:49:05.170322 1253469 hfize_llama.py:153] loaded layer 4
I0325 23:49:06.029994 1253469 hfize_llama.py:153] loaded layer 5
I0325 23:49:06.861458 1253469 hfize_llama.py:153] loaded layer 6
I0325 23:49:07.695869 1253469 hfize_llama.py:153] loaded layer 7
I0325 23:49:08.548261 1253469 hfize_llama.py:153] loaded layer 8
I0325 23:49:09.393607 1253469 hfize_llama.py:153] loaded layer 9
I0325 23:49:10.242891 1253469 hfize_llama.py:153] loaded layer 10
I0325 23:49:11.064120 1253469 hfize_llama.py:153] loaded layer 11
I0325 23:49:11.923542 1253469 hfize_llama.py:153] loaded layer 12
I0325 23:49:12.759680 1253469 hfize_llama.py:153] loaded layer 13
I0325 23:49:13.611791 1253469 hfize_llama.py:153] loaded layer 14
I0325 23:49:14.438992 1253469 hfize_llama.py:153] loaded layer 15
I0325 23:49:15.216651 1253469 hfize_llama.py:153] loaded layer 16
I0325 23:49:15.969916 1253469 hfize_llama.py:153] loaded layer 17
I0325 23:49:16.778585 1253469 hfize_llama.py:153] loaded layer 18
I0325 23:49:17.570577 1253469 hfize_llama.py:153] loaded layer 19
I0325 23:49:18.366731 1253469 hfize_llama.py:153] loaded layer 20
I0325 23:49:19.485355 1253469 hfize_llama.py:153] loaded layer 21
I0325 23:49:20.248795 1253469 hfize_llama.py:153] loaded layer 22
I0325 23:49:20.978688 1253469 hfize_llama.py:153] loaded layer 23
I0325 23:49:21.829539 1253469 hfize_llama.py:153] loaded layer 24
I0325 23:49:22.651427 1253469 hfize_llama.py:153] loaded layer 25
I0325 23:49:23.393222 1253469 hfize_llama.py:153] loaded layer 26
I0325 23:49:24.159429 1253469 hfize_llama.py:153] loaded layer 27
I0325 23:49:24.912688 1253469 hfize_llama.py:153] loaded layer 28
I0325 23:49:25.739436 1253469 hfize_llama.py:153] loaded layer 29
I0325 23:49:26.536985 1253469 hfize_llama.py:153] loaded layer 30
I0325 23:49:27.255551 1253469 hfize_llama.py:153] loaded layer 31
I0325 23:49:27.255690 1253469 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:05,  1.12it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:01<00:03,  1.28it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:02<00:03,  1.15it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.13s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.27s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.11s/it]
I0325 23:50:04.737993 1253469 hfize_llama.py:167] successfully loaded hfized model
W0325 23:50:08.734661 1254449 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0325 23:50:09.286558 1254449 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:04,  1.26it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:01<00:04,  1.24it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:02<00:03,  1.03it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.25s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.36s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.46s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.19s/it]
I0325 23:50:18.055194 1254449 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.866773009300232:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.866773009300232:   1%|          | 1/141 [00:01<04:31,  1.94s/it]avg_loss = 2.133648455142975:   1%|          | 1/141 [00:03<04:31,  1.94s/it]avg_loss = 2.133648455142975:   1%|▏         | 2/141 [00:03<04:00,  1.73s/it]avg_loss = 2.2663135131200156:   1%|▏         | 2/141 [00:05<04:00,  1.73s/it]avg_loss = 2.2663135131200156:   2%|▏         | 3/141 [00:05<03:49,  1.66s/it]avg_loss = 2.21607968211174:   2%|▏         | 3/141 [00:06<03:49,  1.66s/it]  avg_loss = 2.21607968211174:   3%|▎         | 4/141 [00:06<03:44,  1.64s/it]avg_loss = 2.1746157884597777:   3%|▎         | 4/141 [00:08<03:44,  1.64s/it]avg_loss = 2.1746157884597777:   4%|▎         | 5/141 [00:08<03:40,  1.62s/it]avg_loss = 2.0818150838216147:   4%|▎         | 5/141 [00:09<03:40,  1.62s/it]avg_loss = 2.0818150838216147:   4%|▍         | 6/141 [00:09<03:38,  1.62s/it]avg_loss = 2.0227720737457275:   4%|▍         | 6/141 [00:11<03:38,  1.62s/it]avg_loss = 2.0227720737457275:   5%|▍         | 7/141 [00:11<03:36,  1.62s/it]avg_loss = 2.0187376886606216:   5%|▍         | 7/141 [00:13<03:36,  1.62s/it]avg_loss = 2.0187376886606216:   6%|▌         | 8/141 [00:13<03:35,  1.62s/it]avg_loss = 2.054288453525967:   6%|▌         | 8/141 [00:14<03:35,  1.62s/it] avg_loss = 2.054288453525967:   6%|▋         | 9/141 [00:14<03:33,  1.62s/it]avg_loss = 2.0472025513648986:   6%|▋         | 9/141 [00:16<03:33,  1.62s/it]avg_loss = 2.0472025513648986:   7%|▋         | 10/141 [00:16<03:32,  1.62s/it]avg_loss = 2.0384982607581397:   7%|▋         | 10/141 [00:18<03:32,  1.62s/it]avg_loss = 2.0384982607581397:   8%|▊         | 11/141 [00:18<03:31,  1.63s/it]avg_loss = 2.0595476726690927:   8%|▊         | 11/141 [00:19<03:31,  1.63s/it]avg_loss = 2.0595476726690927:   9%|▊         | 12/141 [00:19<03:30,  1.63s/it]avg_loss = 2.070721983909607:   9%|▊         | 12/141 [00:21<03:30,  1.63s/it] avg_loss = 2.070721983909607:   9%|▉         | 13/141 [00:21<03:28,  1.63s/it]avg_loss = 2.0866396682603017:   9%|▉         | 13/141 [00:22<03:28,  1.63s/it]avg_loss = 2.0866396682603017:  10%|▉         | 14/141 [00:22<03:27,  1.64s/it]avg_loss = 2.0956581036249795:  10%|▉         | 14/141 [00:24<03:27,  1.64s/it]avg_loss = 2.0956581036249795:  11%|█         | 15/141 [00:24<03:26,  1.64s/it]avg_loss = 2.116250418126583:  11%|█         | 15/141 [00:26<03:26,  1.64s/it] avg_loss = 2.116250418126583:  11%|█▏        | 16/141 [00:26<03:25,  1.64s/it]avg_loss = 2.119876335648929:  11%|█▏        | 16/141 [00:27<03:25,  1.64s/it]avg_loss = 2.119876335648929:  12%|█▏        | 17/141 [00:27<03:23,  1.64s/it]avg_loss = 2.12260240316391:  12%|█▏        | 17/141 [00:29<03:23,  1.64s/it] avg_loss = 2.12260240316391:  13%|█▎        | 18/141 [00:29<03:22,  1.65s/it]avg_loss = 2.1137265029706453:  13%|█▎        | 18/141 [00:31<03:22,  1.65s/it]avg_loss = 2.1137265029706453:  13%|█▎        | 19/141 [00:31<03:21,  1.65s/it]avg_loss = 2.1131789088249207:  13%|█▎        | 19/141 [00:32<03:21,  1.65s/it]avg_loss = 2.1131789088249207:  14%|█▍        | 20/141 [00:32<03:20,  1.65s/it]avg_loss = 2.118323257991246:  14%|█▍        | 20/141 [00:34<03:20,  1.65s/it] avg_loss = 2.118323257991246:  15%|█▍        | 21/141 [00:34<03:19,  1.66s/it]avg_loss = 2.120051849972118:  15%|█▍        | 21/141 [00:36<03:19,  1.66s/it]avg_loss = 2.120051849972118:  16%|█▌        | 22/141 [00:36<03:17,  1.66s/it]avg_loss = 2.121755309726881:  16%|█▌        | 22/141 [00:37<03:17,  1.66s/it]avg_loss = 2.121755309726881:  16%|█▋        | 23/141 [00:37<03:16,  1.66s/it]avg_loss = 2.1269830564657846:  16%|█▋        | 23/141 [00:39<03:16,  1.66s/it]avg_loss = 2.1269830564657846:  17%|█▋        | 24/141 [00:39<03:14,  1.67s/it]avg_loss = 2.1327203845977785:  17%|█▋        | 24/141 [00:41<03:14,  1.67s/it]avg_loss = 2.1327203845977785:  18%|█▊        | 25/141 [00:41<03:13,  1.67s/it]avg_loss = 2.144251447457534:  18%|█▊        | 25/141 [00:42<03:13,  1.67s/it] avg_loss = 2.144251447457534:  18%|█▊        | 26/141 [00:42<03:12,  1.67s/it]avg_loss = 2.1560837780987776:  18%|█▊        | 26/141 [00:44<03:12,  1.67s/it]avg_loss = 2.1560837780987776:  19%|█▉        | 27/141 [00:44<03:10,  1.68s/it]avg_loss = 2.16094377211162:  19%|█▉        | 27/141 [00:46<03:10,  1.68s/it]  avg_loss = 2.16094377211162:  20%|█▉        | 28/141 [00:46<03:09,  1.68s/it]avg_loss = 2.1580497889683166:  20%|█▉        | 28/141 [00:47<03:09,  1.68s/it]avg_loss = 2.1580497889683166:  21%|██        | 29/141 [00:47<03:08,  1.68s/it]avg_loss = 2.1494449019432067:  21%|██        | 29/141 [00:49<03:08,  1.68s/it]avg_loss = 2.1494449019432067:  21%|██▏       | 30/141 [00:49<03:06,  1.68s/it]avg_loss = 2.1383604849538496:  21%|██▏       | 30/141 [00:51<03:06,  1.68s/it]avg_loss = 2.1383604849538496:  22%|██▏       | 31/141 [00:51<03:05,  1.68s/it]avg_loss = 2.127995353192091:  22%|██▏       | 31/141 [00:53<03:05,  1.68s/it] avg_loss = 2.127995353192091:  23%|██▎       | 32/141 [00:53<03:03,  1.69s/it]avg_loss = 2.125457362695174:  23%|██▎       | 32/141 [00:54<03:03,  1.69s/it]avg_loss = 2.125457362695174:  23%|██▎       | 33/141 [00:54<03:02,  1.69s/it]avg_loss = 2.123537936631371:  23%|██▎       | 33/141 [00:56<03:02,  1.69s/it]avg_loss = 2.123537936631371:  24%|██▍       | 34/141 [00:56<03:00,  1.69s/it]avg_loss = 2.125255056789943:  24%|██▍       | 34/141 [00:58<03:00,  1.69s/it]avg_loss = 2.125255056789943:  25%|██▍       | 35/141 [00:58<02:59,  1.69s/it]avg_loss = 2.108447276883655:  25%|██▍       | 35/141 [00:59<02:59,  1.69s/it]avg_loss = 2.108447276883655:  26%|██▌       | 36/141 [00:59<02:57,  1.69s/it]avg_loss = 2.0931899644233085:  26%|██▌       | 36/141 [01:01<02:57,  1.69s/it]avg_loss = 2.0931899644233085:  26%|██▌       | 37/141 [01:01<02:56,  1.69s/it]avg_loss = 2.077157726413325:  26%|██▌       | 37/141 [01:03<02:56,  1.69s/it] avg_loss = 2.077157726413325:  27%|██▋       | 38/141 [01:03<02:54,  1.70s/it]avg_loss = 2.062580704689026:  27%|██▋       | 38/141 [01:04<02:54,  1.70s/it]avg_loss = 2.062580704689026:  28%|██▊       | 39/141 [01:04<02:53,  1.70s/it]avg_loss = 2.052887734770775:  28%|██▊       | 39/141 [01:06<02:53,  1.70s/it]avg_loss = 2.052887734770775:  28%|██▊       | 40/141 [01:06<02:51,  1.70s/it]avg_loss = 2.058712665627642:  28%|██▊       | 40/141 [01:08<02:51,  1.70s/it]avg_loss = 2.058712665627642:  29%|██▉       | 41/141 [01:08<02:49,  1.70s/it]avg_loss = 2.0738854834011624:  29%|██▉       | 41/141 [01:10<02:49,  1.70s/it]avg_loss = 2.0738854834011624:  30%|██▉       | 42/141 [01:10<02:48,  1.70s/it]avg_loss = 2.0889926428018613:  30%|██▉       | 42/141 [01:11<02:48,  1.70s/it]avg_loss = 2.0889926428018613:  30%|███       | 43/141 [01:11<02:46,  1.70s/it]avg_loss = 2.0948617431250485:  30%|███       | 43/141 [01:13<02:46,  1.70s/it]avg_loss = 2.0948617431250485:  31%|███       | 44/141 [01:13<02:44,  1.70s/it]avg_loss = 2.100402919451396:  31%|███       | 44/141 [01:15<02:44,  1.70s/it] avg_loss = 2.100402919451396:  32%|███▏      | 45/141 [01:15<02:43,  1.70s/it]avg_loss = 2.1042970963146375:  32%|███▏      | 45/141 [01:16<02:43,  1.70s/it]avg_loss = 2.1042970963146375:  33%|███▎      | 46/141 [01:16<02:41,  1.70s/it]avg_loss = 2.109756309935387:  33%|███▎      | 46/141 [01:18<02:41,  1.70s/it] avg_loss = 2.109756309935387:  33%|███▎      | 47/141 [01:18<02:39,  1.70s/it]avg_loss = 2.1115478004018464:  33%|███▎      | 47/141 [01:20<02:39,  1.70s/it]avg_loss = 2.1115478004018464:  34%|███▍      | 48/141 [01:20<02:38,  1.70s/it]avg_loss = 2.1117253571140524:  34%|███▍      | 48/141 [01:21<02:38,  1.70s/it]avg_loss = 2.1117253571140524:  35%|███▍      | 49/141 [01:21<02:36,  1.70s/it]avg_loss = 2.1118237805366515:  35%|███▍      | 49/141 [01:23<02:36,  1.70s/it]avg_loss = 2.1118237805366515:  35%|███▌      | 50/141 [01:23<02:35,  1.70s/it]avg_loss = 2.106880022030251:  35%|███▌      | 50/141 [01:25<02:35,  1.70s/it] avg_loss = 2.106880022030251:  36%|███▌      | 51/141 [01:25<02:33,  1.70s/it]avg_loss = 2.1024693915477166:  36%|███▌      | 51/141 [01:27<02:33,  1.70s/it]avg_loss = 2.1024693915477166:  37%|███▋      | 52/141 [01:27<02:31,  1.71s/it]avg_loss = 2.0957412044957:  37%|███▋      | 52/141 [01:28<02:31,  1.71s/it]   avg_loss = 2.0957412044957:  38%|███▊      | 53/141 [01:28<02:30,  1.71s/it]avg_loss = 2.0923306390091225:  38%|███▊      | 53/141 [01:30<02:30,  1.71s/it]avg_loss = 2.0923306390091225:  38%|███▊      | 54/141 [01:30<02:28,  1.71s/it]avg_loss = 2.084125436436046:  38%|███▊      | 54/141 [01:32<02:28,  1.71s/it] avg_loss = 2.084125436436046:  39%|███▉      | 55/141 [01:32<02:26,  1.71s/it]avg_loss = 2.0757990990366255:  39%|███▉      | 55/141 [01:33<02:26,  1.71s/it]avg_loss = 2.0757990990366255:  40%|███▉      | 56/141 [01:33<02:25,  1.71s/it]avg_loss = 2.0736350992269683:  40%|███▉      | 56/141 [01:35<02:25,  1.71s/it]avg_loss = 2.0736350992269683:  40%|████      | 57/141 [01:35<02:23,  1.71s/it]avg_loss = 2.0701737609402886:  40%|████      | 57/141 [01:37<02:23,  1.71s/it]avg_loss = 2.0701737609402886:  41%|████      | 58/141 [01:37<02:21,  1.71s/it]avg_loss = 2.0721295607292047:  41%|████      | 58/141 [01:38<02:21,  1.71s/it]avg_loss = 2.0721295607292047:  42%|████▏     | 59/141 [01:38<02:20,  1.71s/it]avg_loss = 2.0769468943277993:  42%|████▏     | 59/141 [01:40<02:20,  1.71s/it]avg_loss = 2.0769468943277993:  43%|████▎     | 60/141 [01:40<02:18,  1.71s/it]avg_loss = 2.0814391237790466:  43%|████▎     | 60/141 [01:42<02:18,  1.71s/it]avg_loss = 2.0814391237790466:  43%|████▎     | 61/141 [01:42<02:16,  1.71s/it]avg_loss = 2.088596893895057:  43%|████▎     | 61/141 [01:44<02:16,  1.71s/it] avg_loss = 2.088596893895057:  44%|████▍     | 62/141 [01:44<02:15,  1.71s/it]avg_loss = 2.0809429959645347:  44%|████▍     | 62/141 [01:45<02:15,  1.71s/it]avg_loss = 2.0809429959645347:  45%|████▍     | 63/141 [01:45<02:13,  1.71s/it]avg_loss = 2.078714059665799:  45%|████▍     | 63/141 [01:47<02:13,  1.71s/it] avg_loss = 2.078714059665799:  45%|████▌     | 64/141 [01:47<02:11,  1.71s/it]avg_loss = 2.0767686550433817:  45%|████▌     | 64/141 [01:49<02:11,  1.71s/it]avg_loss = 2.0767686550433817:  46%|████▌     | 65/141 [01:49<02:10,  1.71s/it]avg_loss = 2.070475043672504:  46%|████▌     | 65/141 [01:50<02:10,  1.71s/it] avg_loss = 2.070475043672504:  47%|████▋     | 66/141 [01:50<02:08,  1.71s/it]avg_loss = 2.0674634424608147:  47%|████▋     | 66/141 [01:52<02:08,  1.71s/it]avg_loss = 2.0674634424608147:  48%|████▊     | 67/141 [01:52<02:06,  1.71s/it]avg_loss = 2.065291620352689:  48%|████▊     | 67/141 [01:54<02:06,  1.71s/it] avg_loss = 2.065291620352689:  48%|████▊     | 68/141 [01:54<02:05,  1.71s/it]avg_loss = 2.063520025515902:  48%|████▊     | 68/141 [01:56<02:05,  1.71s/it]avg_loss = 2.063520025515902:  49%|████▉     | 69/141 [01:56<02:03,  1.71s/it]avg_loss = 2.0646188855171204:  49%|████▉     | 69/141 [01:57<02:03,  1.71s/it]avg_loss = 2.0646188855171204:  50%|████▉     | 70/141 [01:57<02:01,  1.71s/it]avg_loss = 2.0680922001180515:  50%|████▉     | 70/141 [01:59<02:01,  1.71s/it]avg_loss = 2.0680922001180515:  50%|█████     | 71/141 [01:59<01:59,  1.71s/it]avg_loss = 2.0708380854792066:  50%|█████     | 71/141 [02:01<01:59,  1.71s/it]avg_loss = 2.0708380854792066:  51%|█████     | 72/141 [02:01<01:58,  1.72s/it]avg_loss = 2.0688272926905382:  51%|█████     | 72/141 [02:02<01:58,  1.72s/it]avg_loss = 2.0688272926905382:  52%|█████▏    | 73/141 [02:02<01:56,  1.72s/it]avg_loss = 2.0700971758043445:  52%|█████▏    | 73/141 [02:04<01:56,  1.72s/it]avg_loss = 2.0700971758043445:  52%|█████▏    | 74/141 [02:04<01:55,  1.72s/it]avg_loss = 2.069960536956787:  52%|█████▏    | 74/141 [02:06<01:55,  1.72s/it] avg_loss = 2.069960536956787:  53%|█████▎    | 75/141 [02:06<01:53,  1.72s/it]avg_loss = 2.0687646708990397:  53%|█████▎    | 75/141 [02:08<01:53,  1.72s/it]avg_loss = 2.0687646708990397:  54%|█████▍    | 76/141 [02:08<01:51,  1.72s/it]avg_loss = 2.06953865521914:  54%|█████▍    | 76/141 [02:09<01:51,  1.72s/it]  avg_loss = 2.06953865521914:  55%|█████▍    | 77/141 [02:09<01:49,  1.72s/it]avg_loss = 2.071610239835886:  55%|█████▍    | 77/141 [02:11<01:49,  1.72s/it]avg_loss = 2.071610239835886:  55%|█████▌    | 78/141 [02:11<01:48,  1.72s/it]avg_loss = 2.075119094003605:  55%|█████▌    | 78/141 [02:13<01:48,  1.72s/it]avg_loss = 2.075119094003605:  56%|█████▌    | 79/141 [02:13<01:46,  1.72s/it]avg_loss = 2.0707864969968797:  56%|█████▌    | 79/141 [02:15<01:46,  1.72s/it]avg_loss = 2.0707864969968797:  57%|█████▋    | 80/141 [02:15<01:44,  1.72s/it]avg_loss = 2.069028713085033:  57%|█████▋    | 80/141 [02:16<01:44,  1.72s/it] avg_loss = 2.069028713085033:  57%|█████▋    | 81/141 [02:16<01:43,  1.72s/it]avg_loss = 2.0677543893093016:  57%|█████▋    | 81/141 [02:18<01:43,  1.72s/it]avg_loss = 2.0677543893093016:  58%|█████▊    | 82/141 [02:18<01:41,  1.72s/it]avg_loss = 2.0655507237078194:  58%|█████▊    | 82/141 [02:20<01:41,  1.72s/it]avg_loss = 2.0655507237078194:  59%|█████▉    | 83/141 [02:20<01:39,  1.72s/it]avg_loss = 2.063190024523508:  59%|█████▉    | 83/141 [02:21<01:39,  1.72s/it] avg_loss = 2.063190024523508:  60%|█████▉    | 84/141 [02:21<01:38,  1.72s/it]avg_loss = 2.060661017193514:  60%|█████▉    | 84/141 [02:23<01:38,  1.72s/it]avg_loss = 2.060661017193514:  60%|██████    | 85/141 [02:23<01:36,  1.72s/it]avg_loss = 2.0620246779086977:  60%|██████    | 85/141 [02:25<01:36,  1.72s/it]avg_loss = 2.0620246779086977:  61%|██████    | 86/141 [02:25<01:34,  1.72s/it]avg_loss = 2.06384333933907:  61%|██████    | 86/141 [02:27<01:34,  1.72s/it]  avg_loss = 2.06384333933907:  62%|██████▏   | 87/141 [02:27<01:33,  1.72s/it]avg_loss = 2.065285486253825:  62%|██████▏   | 87/141 [02:28<01:33,  1.72s/it]avg_loss = 2.065285486253825:  62%|██████▏   | 88/141 [02:28<01:31,  1.72s/it]avg_loss = 2.0737015517909874:  62%|██████▏   | 88/141 [02:30<01:31,  1.72s/it]avg_loss = 2.0737015517909874:  63%|██████▎   | 89/141 [02:30<01:29,  1.72s/it]avg_loss = 2.0809786253505282:  63%|██████▎   | 89/141 [02:32<01:29,  1.72s/it]avg_loss = 2.0809786253505282:  64%|██████▍   | 90/141 [02:32<01:27,  1.72s/it]avg_loss = 2.084227893378708:  64%|██████▍   | 90/141 [02:33<01:27,  1.72s/it] avg_loss = 2.084227893378708:  65%|██████▍   | 91/141 [02:33<01:26,  1.72s/it]avg_loss = 2.0891262513139974:  65%|██████▍   | 91/141 [02:35<01:26,  1.72s/it]avg_loss = 2.0891262513139974:  65%|██████▌   | 92/141 [02:35<01:24,  1.72s/it]avg_loss = 2.0940761373889063:  65%|██████▌   | 92/141 [02:37<01:24,  1.72s/it]avg_loss = 2.0940761373889063:  66%|██████▌   | 93/141 [02:37<01:22,  1.72s/it]avg_loss = 2.094547115741892:  66%|██████▌   | 93/141 [02:39<01:22,  1.72s/it] avg_loss = 2.094547115741892:  67%|██████▋   | 94/141 [02:39<01:21,  1.72s/it]avg_loss = 2.0983457452372503:  67%|██████▋   | 94/141 [02:40<01:21,  1.72s/it]avg_loss = 2.0983457452372503:  67%|██████▋   | 95/141 [02:40<01:19,  1.72s/it]avg_loss = 2.0986095033586025:  67%|██████▋   | 95/141 [02:42<01:19,  1.72s/it]avg_loss = 2.0986095033586025:  68%|██████▊   | 96/141 [02:42<01:17,  1.72s/it]avg_loss = 2.10028694216738:  68%|██████▊   | 96/141 [02:44<01:17,  1.72s/it]  avg_loss = 2.10028694216738:  69%|██████▉   | 97/141 [02:44<01:15,  1.72s/it]avg_loss = 2.09856837258047:  69%|██████▉   | 97/141 [02:46<01:15,  1.72s/it]avg_loss = 2.09856837258047:  70%|██████▉   | 98/141 [02:46<01:14,  1.72s/it]avg_loss = 2.099680954759771:  70%|██████▉   | 98/141 [02:47<01:14,  1.72s/it]avg_loss = 2.099680954759771:  70%|███████   | 99/141 [02:47<01:12,  1.72s/it]avg_loss = 2.1020507967472075:  70%|███████   | 99/141 [02:49<01:12,  1.72s/it]avg_loss = 2.1020507967472075:  71%|███████   | 100/141 [02:49<01:10,  1.72s/it]avg_loss = 2.1019797266119777:  71%|███████   | 100/141 [02:51<01:10,  1.72s/it]avg_loss = 2.1019797266119777:  72%|███████▏  | 101/141 [02:51<01:08,  1.72s/it]avg_loss = 2.102874256816565:  72%|███████▏  | 101/141 [02:52<01:08,  1.72s/it] avg_loss = 2.102874256816565:  72%|███████▏  | 102/141 [02:52<01:07,  1.72s/it]avg_loss = 2.1026735062738067:  72%|███████▏  | 102/141 [02:54<01:07,  1.72s/it]avg_loss = 2.1026735062738067:  73%|███████▎  | 103/141 [02:54<01:05,  1.72s/it]avg_loss = 2.1060484017317114:  73%|███████▎  | 103/141 [02:56<01:05,  1.72s/it]avg_loss = 2.1060484017317114:  74%|███████▍  | 104/141 [02:56<01:03,  1.72s/it]avg_loss = 2.105549745332627:  74%|███████▍  | 104/141 [02:58<01:03,  1.72s/it] avg_loss = 2.105549745332627:  74%|███████▍  | 105/141 [02:58<01:02,  1.73s/it]avg_loss = 2.10529514641132:  74%|███████▍  | 105/141 [02:59<01:02,  1.73s/it] avg_loss = 2.10529514641132:  75%|███████▌  | 106/141 [02:59<01:00,  1.72s/it]avg_loss = 2.1036906064113725:  75%|███████▌  | 106/141 [03:01<01:00,  1.72s/it]avg_loss = 2.1036906064113725:  76%|███████▌  | 107/141 [03:01<00:58,  1.72s/it]avg_loss = 2.1018082234594555:  76%|███████▌  | 107/141 [03:03<00:58,  1.72s/it]avg_loss = 2.1018082234594555:  77%|███████▋  | 108/141 [03:03<00:56,  1.72s/it]avg_loss = 2.1000673836524335:  77%|███████▋  | 108/141 [03:04<00:56,  1.72s/it]avg_loss = 2.1000673836524335:  77%|███████▋  | 109/141 [03:04<00:55,  1.72s/it]avg_loss = 2.0976389158855784:  77%|███████▋  | 109/141 [03:06<00:55,  1.72s/it]avg_loss = 2.0976389158855784:  78%|███████▊  | 110/141 [03:06<00:53,  1.72s/it]avg_loss = 2.1000597552136258:  78%|███████▊  | 110/141 [03:08<00:53,  1.72s/it]avg_loss = 2.1000597552136258:  79%|███████▊  | 111/141 [03:08<00:51,  1.72s/it]avg_loss = 2.0995430297085216:  79%|███████▊  | 111/141 [03:10<00:51,  1.72s/it]avg_loss = 2.0995430297085216:  79%|███████▉  | 112/141 [03:10<00:49,  1.72s/it]avg_loss = 2.100626391647136:  79%|███████▉  | 112/141 [03:11<00:49,  1.72s/it] avg_loss = 2.100626391647136:  80%|████████  | 113/141 [03:11<00:48,  1.72s/it]avg_loss = 2.10171590876161:  80%|████████  | 113/141 [03:13<00:48,  1.72s/it] avg_loss = 2.10171590876161:  81%|████████  | 114/141 [03:13<00:46,  1.72s/it]avg_loss = 2.1008590833000516:  81%|████████  | 114/141 [03:15<00:46,  1.72s/it]avg_loss = 2.1008590833000516:  82%|████████▏ | 115/141 [03:15<00:44,  1.72s/it]avg_loss = 2.0998301711575738:  82%|████████▏ | 115/141 [03:17<00:44,  1.72s/it]avg_loss = 2.0998301711575738:  82%|████████▏ | 116/141 [03:17<00:43,  1.72s/it]avg_loss = 2.101993041160779:  82%|████████▏ | 116/141 [03:18<00:43,  1.72s/it] avg_loss = 2.101993041160779:  83%|████████▎ | 117/141 [03:18<00:41,  1.72s/it]avg_loss = 2.1010879520642556:  83%|████████▎ | 117/141 [03:20<00:41,  1.72s/it]avg_loss = 2.1010879520642556:  84%|████████▎ | 118/141 [03:20<00:39,  1.72s/it]avg_loss = 2.0994385330616927:  84%|████████▎ | 118/141 [03:22<00:39,  1.72s/it]avg_loss = 2.0994385330616927:  84%|████████▍ | 119/141 [03:22<00:37,  1.72s/it]avg_loss = 2.0976171960433323:  84%|████████▍ | 119/141 [03:23<00:37,  1.72s/it]avg_loss = 2.0976171960433323:  85%|████████▌ | 120/141 [03:23<00:36,  1.73s/it]avg_loss = 2.0976008728516002:  85%|████████▌ | 120/141 [03:25<00:36,  1.73s/it]avg_loss = 2.0976008728516002:  86%|████████▌ | 121/141 [03:25<00:34,  1.72s/it]avg_loss = 2.0983310224579985:  86%|████████▌ | 121/141 [03:27<00:34,  1.72s/it]avg_loss = 2.0983310224579985:  87%|████████▋ | 122/141 [03:27<00:32,  1.72s/it]avg_loss = 2.097618212544821:  87%|████████▋ | 122/141 [03:29<00:32,  1.72s/it] avg_loss = 2.097618212544821:  87%|████████▋ | 123/141 [03:29<00:31,  1.72s/it]avg_loss = 2.0977206182095314:  87%|████████▋ | 123/141 [03:30<00:31,  1.72s/it]avg_loss = 2.0977206182095314:  88%|████████▊ | 124/141 [03:30<00:29,  1.72s/it]avg_loss = 2.096213562965393:  88%|████████▊ | 124/141 [03:32<00:29,  1.72s/it] avg_loss = 2.096213562965393:  89%|████████▊ | 125/141 [03:32<00:27,  1.73s/it]avg_loss = 2.096396916442447:  89%|████████▊ | 125/141 [03:34<00:27,  1.73s/it]avg_loss = 2.096396916442447:  89%|████████▉ | 126/141 [03:34<00:25,  1.73s/it]avg_loss = 2.0961352117418306:  89%|████████▉ | 126/141 [03:36<00:25,  1.73s/it]avg_loss = 2.0961352117418306:  90%|█████████ | 127/141 [03:36<00:24,  1.73s/it]avg_loss = 2.095002643764019:  90%|█████████ | 127/141 [03:37<00:24,  1.73s/it] avg_loss = 2.095002643764019:  91%|█████████ | 128/141 [03:37<00:22,  1.73s/it]avg_loss = 2.0948813368183696:  91%|█████████ | 128/141 [03:39<00:22,  1.73s/it]avg_loss = 2.0948813368183696:  91%|█████████▏| 129/141 [03:39<00:20,  1.73s/it]avg_loss = 2.0961986505068264:  91%|█████████▏| 129/141 [03:41<00:20,  1.73s/it]avg_loss = 2.0961986505068264:  92%|█████████▏| 130/141 [03:41<00:18,  1.73s/it]avg_loss = 2.0971061418984682:  92%|█████████▏| 130/141 [03:42<00:18,  1.73s/it]avg_loss = 2.0971061418984682:  93%|█████████▎| 131/141 [03:42<00:17,  1.73s/it]avg_loss = 2.097627497080601:  93%|█████████▎| 131/141 [03:44<00:17,  1.73s/it] avg_loss = 2.097627497080601:  94%|█████████▎| 132/141 [03:44<00:15,  1.72s/it]avg_loss = 2.0944946412753342:  94%|█████████▎| 132/141 [03:46<00:15,  1.72s/it]avg_loss = 2.0944946412753342:  94%|█████████▍| 133/141 [03:46<00:13,  1.72s/it]avg_loss = 2.0895878218892796:  94%|█████████▍| 133/141 [03:48<00:13,  1.72s/it]avg_loss = 2.0895878218892796:  95%|█████████▌| 134/141 [03:48<00:12,  1.72s/it]avg_loss = 2.0916913827260335:  95%|█████████▌| 134/141 [03:49<00:12,  1.72s/it]avg_loss = 2.0916913827260335:  96%|█████████▌| 135/141 [03:49<00:10,  1.72s/it]avg_loss = 2.095099547330071:  96%|█████████▌| 135/141 [03:51<00:10,  1.72s/it] avg_loss = 2.095099547330071:  96%|█████████▋| 136/141 [03:51<00:08,  1.72s/it]avg_loss = 2.0965581497136694:  96%|█████████▋| 136/141 [03:53<00:08,  1.72s/it]avg_loss = 2.0965581497136694:  97%|█████████▋| 137/141 [03:53<00:06,  1.72s/it]avg_loss = 2.0956538449163022:  97%|█████████▋| 137/141 [03:55<00:06,  1.72s/it]avg_loss = 2.0956538449163022:  98%|█████████▊| 138/141 [03:55<00:05,  1.72s/it]avg_loss = 2.0962197677694636:  98%|█████████▊| 138/141 [03:56<00:05,  1.72s/it]avg_loss = 2.0962197677694636:  99%|█████████▊| 139/141 [03:56<00:03,  1.72s/it]avg_loss = 2.0972846610205513:  99%|█████████▊| 139/141 [03:58<00:03,  1.72s/it]avg_loss = 2.0972846610205513:  99%|█████████▉| 140/141 [03:58<00:01,  1.72s/it]avg_loss = 2.0984702905019126:  99%|█████████▉| 140/141 [04:00<00:01,  1.72s/it]avg_loss = 2.0984702905019126: 100%|██████████| 141/141 [04:00<00:00,  1.72s/it]avg_loss = 2.0984702905019126: 100%|██████████| 141/141 [04:00<00:00,  1.70s/it]
I0325 23:54:43.460964 1254449 eval_ppl.py:107] wikitext2 perplexity: 8.1536865234375
wikitext2 perplexity: 8.154
