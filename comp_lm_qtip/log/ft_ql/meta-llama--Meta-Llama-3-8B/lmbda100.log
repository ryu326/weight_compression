I0325 23:54:50.884041 1258494 config.py:54] PyTorch version 2.6.0 available.
W0325 23:54:51.165197 1258494 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 23:54:52.063841 1258494 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.68it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.63it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.59it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.86it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.02it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.11it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.27it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.03it/s]
I0325 23:54:53.516271 1258494 quantize_finetune_llama.py:150] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:23,  1.33it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:18,  1.64it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:01<00:16,  1.76it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:15,  1.83it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:02<00:14,  1.86it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:13,  1.87it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:03<00:13,  1.89it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.91it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:04<00:12,  1.91it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.90it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:05<00:10,  1.92it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.92it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:06<00:09,  1.93it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:09,  1.92it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  1.92it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:08,  1.91it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.93it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:07,  1.94it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:10<00:06,  1.95it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:06,  1.94it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  1.95it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:05,  1.94it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  1.94it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:04,  1.95it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.96it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:03,  1.94it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  1.95it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:02,  1.94it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  1.95it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:01,  1.95it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:16<00:00,  1.97it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.92it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  1.91it/s]
I0325 23:55:15.987225 1258494 quantize_finetune_llama.py:185] loaded compression model
I0325 23:55:34.320821 1258494 quantize_finetune_llama.py:189] loaded dataset and devset
I0325 23:55:37.299721 1258494 quantize_finetune_llama.py:209] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 23:56:32.031911 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 0 in 54.59167766571045s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0325 23:56:56.721032 1260134 config.py:54] PyTorch version 2.6.0 available.
W0325 23:56:57.000954 1260134 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 23:56:57.880215 1260134 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 23:56:57.884376 1258494 quantize_finetune_llama.py:209] layer 1 gpu 1
I0325 23:56:57.898029 1260134 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 23:57:05.122788 1260134 finetune.py:45] layer 0_v initial loss 2.8650890726567013e-06
W0325 23:57:05.123139 1260134 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 23:57:40.651498 1260134 finetune.py:68] layer 0_v @ epoch 0 new loss 1.8607526044434053e-06 old loss 2.8650890726567013e-06 BETTER
I0325 23:57:54.090328 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 1 in 55.99205160140991s
I0325 23:58:02.561149 1260961 config.py:54] PyTorch version 2.6.0 available.
W0325 23:58:02.845398 1260961 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0325 23:58:03.901385 1260961 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0325 23:58:03.905973 1258494 quantize_finetune_llama.py:209] layer 2 gpu 0
I0325 23:58:03.919444 1260961 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0325 23:58:10.867635 1260961 finetune.py:45] layer 1_v initial loss 6.372138159349561e-06
W0325 23:58:10.868016 1260961 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0325 23:58:17.515927 1260134 finetune.py:68] layer 0_v @ epoch 1 new loss 1.63201139002922e-06 old loss 1.8607526044434053e-06 BETTER
I0325 23:58:44.119884 1260961 finetune.py:68] layer 1_v @ epoch 0 new loss 2.961499831144465e-06 old loss 6.372138159349561e-06 BETTER
I0325 23:58:54.427903 1260134 finetune.py:68] layer 0_v @ epoch 2 new loss 1.5330298310800572e-06 old loss 1.63201139002922e-06 BETTER
I0325 23:59:18.433066 1260961 finetune.py:68] layer 1_v @ epoch 1 new loss 2.3392913135467097e-06 old loss 2.961499831144465e-06 BETTER
I0325 23:59:31.448977 1260134 finetune.py:68] layer 0_v @ epoch 3 new loss 1.4741760878678178e-06 old loss 1.5330298310800572e-06 BETTER
I0325 23:59:53.051994 1260961 finetune.py:68] layer 1_v @ epoch 2 new loss 2.141186541848583e-06 old loss 2.3392913135467097e-06 BETTER
I0326 00:00:08.380913 1260134 finetune.py:68] layer 0_v @ epoch 4 new loss 1.43242880312755e-06 old loss 1.4741760878678178e-06 BETTER
I0326 00:00:18.108486 1260134 finetune.py:45] layer 0_q initial loss 1.4360391560330754e-06
I0326 00:00:27.892685 1260961 finetune.py:68] layer 1_v @ epoch 3 new loss 2.0331531231931876e-06 old loss 2.141186541848583e-06 BETTER
I0326 00:00:53.887613 1260134 finetune.py:68] layer 0_q @ epoch 0 new loss 1.3988191085445578e-06 old loss 1.4360391560330754e-06 BETTER
I0326 00:01:03.062731 1260961 finetune.py:68] layer 1_v @ epoch 4 new loss 1.9549486296455143e-06 old loss 2.0331531231931876e-06 BETTER
I0326 00:01:12.523887 1260961 finetune.py:45] layer 1_q initial loss 2.01656689569063e-06
I0326 00:01:30.415599 1260134 finetune.py:68] layer 0_q @ epoch 1 new loss 1.3692505262952182e-06 old loss 1.3988191085445578e-06 BETTER
I0326 00:01:46.346761 1260961 finetune.py:68] layer 1_q @ epoch 0 new loss 1.930987764353631e-06 old loss 2.01656689569063e-06 BETTER
I0326 00:02:07.004932 1260134 finetune.py:68] layer 0_q @ epoch 2 new loss 1.3442406725516776e-06 old loss 1.3692505262952182e-06 BETTER
I0326 00:02:20.923546 1260961 finetune.py:68] layer 1_q @ epoch 1 new loss 1.8724086885413271e-06 old loss 1.930987764353631e-06 BETTER
I0326 00:02:43.744297 1260134 finetune.py:68] layer 0_q @ epoch 3 new loss 1.3225258044258226e-06 old loss 1.3442406725516776e-06 BETTER
I0326 00:02:55.481777 1260961 finetune.py:68] layer 1_q @ epoch 2 new loss 1.8229890201837406e-06 old loss 1.8724086885413271e-06 BETTER
I0326 00:03:20.608125 1260134 finetune.py:68] layer 0_q @ epoch 4 new loss 1.3028227385802893e-06 old loss 1.3225258044258226e-06 BETTER
I0326 00:03:28.636564 1260134 finetune.py:45] layer 0_k initial loss 1.3077740277367411e-06
I0326 00:03:30.209892 1260961 finetune.py:68] layer 1_q @ epoch 3 new loss 1.7806806908993167e-06 old loss 1.8229890201837406e-06 BETTER
I0326 00:04:04.789490 1260134 finetune.py:68] layer 0_k @ epoch 0 new loss 1.2894935252916184e-06 old loss 1.3077740277367411e-06 BETTER
I0326 00:04:04.945731 1260961 finetune.py:68] layer 1_q @ epoch 4 new loss 1.7442215494156699e-06 old loss 1.7806806908993167e-06 BETTER
I0326 00:04:12.684000 1260961 finetune.py:45] layer 1_k initial loss 1.7806688674681936e-06
I0326 00:04:41.609135 1260134 finetune.py:68] layer 0_k @ epoch 1 new loss 1.2733974017464789e-06 old loss 1.2894935252916184e-06 BETTER
I0326 00:04:46.397235 1260961 finetune.py:68] layer 1_k @ epoch 0 new loss 1.7422876226191875e-06 old loss 1.7806688674681936e-06 BETTER
I0326 00:05:18.620675 1260134 finetune.py:68] layer 0_k @ epoch 2 new loss 1.2587340734171448e-06 old loss 1.2733974017464789e-06 BETTER
I0326 00:05:20.786998 1260961 finetune.py:68] layer 1_k @ epoch 1 new loss 1.7113565036197542e-06 old loss 1.7422876226191875e-06 BETTER
I0326 00:05:55.802863 1260961 finetune.py:68] layer 1_k @ epoch 2 new loss 1.6839155705383746e-06 old loss 1.7113565036197542e-06 BETTER
I0326 00:05:55.901396 1260134 finetune.py:68] layer 0_k @ epoch 3 new loss 1.2452489954739576e-06 old loss 1.2587340734171448e-06 BETTER
I0326 00:06:30.428496 1260961 finetune.py:68] layer 1_k @ epoch 3 new loss 1.6592226756984019e-06 old loss 1.6839155705383746e-06 BETTER
I0326 00:06:32.619640 1260134 finetune.py:68] layer 0_k @ epoch 4 new loss 1.2328913499004557e-06 old loss 1.2452489954739576e-06 BETTER
I0326 00:06:42.342570 1260134 finetune.py:45] layer 0_o initial loss 2.1206963083386654e-06
I0326 00:07:05.018547 1260961 finetune.py:68] layer 1_k @ epoch 4 new loss 1.63603067449003e-06 old loss 1.6592226756984019e-06 BETTER
I0326 00:07:14.538417 1260961 finetune.py:45] layer 1_o initial loss 4.056753368786303e-06
I0326 00:07:17.528470 1260134 finetune.py:68] layer 0_o @ epoch 0 new loss 2.0916488665534416e-06 old loss 2.1206963083386654e-06 BETTER
I0326 00:07:47.578053 1260961 finetune.py:68] layer 1_o @ epoch 0 new loss 3.9553460737806745e-06 old loss 4.056753368786303e-06 BETTER
I0326 00:07:53.724898 1260134 finetune.py:68] layer 0_o @ epoch 1 new loss 2.0678260170825524e-06 old loss 2.0916488665534416e-06 BETTER
I0326 00:08:21.310252 1260961 finetune.py:68] layer 1_o @ epoch 1 new loss 3.879658379446482e-06 old loss 3.9553460737806745e-06 BETTER
I0326 00:08:29.475386 1260134 finetune.py:68] layer 0_o @ epoch 2 new loss 2.0474653865676373e-06 old loss 2.0678260170825524e-06 BETTER
I0326 00:08:55.045413 1260961 finetune.py:68] layer 1_o @ epoch 2 new loss 3.8201665120141115e-06 old loss 3.879658379446482e-06 BETTER
I0326 00:09:05.586018 1260134 finetune.py:68] layer 0_o @ epoch 3 new loss 2.0298602976254188e-06 old loss 2.0474653865676373e-06 BETTER
I0326 00:09:28.915453 1260961 finetune.py:68] layer 1_o @ epoch 3 new loss 3.7722479646618012e-06 old loss 3.8201665120141115e-06 BETTER
I0326 00:09:41.741831 1260134 finetune.py:68] layer 0_o @ epoch 4 new loss 2.0145139387750532e-06 old loss 2.0298602976254188e-06 BETTER
I0326 00:10:02.773711 1260961 finetune.py:68] layer 1_o @ epoch 4 new loss 3.732888217200525e-06 old loss 3.7722479646618012e-06 BETTER
I0326 00:10:03.392482 1260134 finetune.py:45] layer 0_up initial loss 2.687428150238702e-06
I0326 00:10:24.246829 1260961 finetune.py:45] layer 1_up initial loss 5.268305812933249e-06
I0326 00:10:36.038527 1260134 finetune.py:68] layer 0_up @ epoch 0 new loss 2.663830628080177e-06 old loss 2.687428150238702e-06 BETTER
I0326 00:10:54.978081 1260961 finetune.py:68] layer 1_up @ epoch 0 new loss 5.1960932978545316e-06 old loss 5.268305812933249e-06 BETTER
I0326 00:11:09.458815 1260134 finetune.py:68] layer 0_up @ epoch 1 new loss 2.646120037752553e-06 old loss 2.663830628080177e-06 BETTER
I0326 00:11:26.869465 1260961 finetune.py:68] layer 1_up @ epoch 1 new loss 5.165611128177261e-06 old loss 5.1960932978545316e-06 BETTER
I0326 00:11:42.906114 1260134 finetune.py:68] layer 0_up @ epoch 2 new loss 2.6320612960262224e-06 old loss 2.646120037752553e-06 BETTER
I0326 00:11:58.614953 1260961 finetune.py:68] layer 1_up @ epoch 2 new loss 5.139957465871703e-06 old loss 5.165611128177261e-06 BETTER
I0326 00:12:16.443354 1260134 finetune.py:68] layer 0_up @ epoch 3 new loss 2.6203456400253344e-06 old loss 2.6320612960262224e-06 BETTER
I0326 00:12:30.730921 1260961 finetune.py:68] layer 1_up @ epoch 3 new loss 5.117843102198094e-06 old loss 5.139957465871703e-06 BETTER
I0326 00:12:50.114475 1260134 finetune.py:68] layer 0_up @ epoch 4 new loss 2.610349838505499e-06 old loss 2.6203456400253344e-06 BETTER
I0326 00:13:02.821054 1260961 finetune.py:68] layer 1_up @ epoch 4 new loss 5.098274868942099e-06 old loss 5.117843102198094e-06 BETTER
I0326 00:13:11.798866 1260134 finetune.py:45] layer 0_gate initial loss 3.074150299653411e-06
I0326 00:13:24.449412 1260961 finetune.py:45] layer 1_gate initial loss 6.14399050391512e-06
I0326 00:13:41.981622 1260134 finetune.py:68] layer 0_gate @ epoch 0 new loss 3.055150273212348e-06 old loss 3.074150299653411e-06 BETTER
I0326 00:13:52.987268 1260961 finetune.py:68] layer 1_gate @ epoch 0 new loss 6.122661488916492e-06 old loss 6.14399050391512e-06 BETTER
I0326 00:14:13.144703 1260134 finetune.py:68] layer 0_gate @ epoch 1 new loss 3.0397634418477537e-06 old loss 3.055150273212348e-06 BETTER
I0326 00:14:22.502913 1260961 finetune.py:68] layer 1_gate @ epoch 1 new loss 6.104765361669706e-06 old loss 6.122661488916492e-06 BETTER
I0326 00:14:44.633512 1260134 finetune.py:68] layer 0_gate @ epoch 2 new loss 3.0270996376202675e-06 old loss 3.0397634418477537e-06 BETTER
I0326 00:14:52.185717 1260961 finetune.py:68] layer 1_gate @ epoch 2 new loss 6.089340331527637e-06 old loss 6.104765361669706e-06 BETTER
I0326 00:15:16.275221 1260134 finetune.py:68] layer 0_gate @ epoch 3 new loss 3.0164846975822e-06 old loss 3.0270996376202675e-06 BETTER
I0326 00:15:21.951151 1260961 finetune.py:68] layer 1_gate @ epoch 3 new loss 6.075281817174982e-06 old loss 6.089340331527637e-06 BETTER
I0326 00:15:47.823189 1260134 finetune.py:68] layer 0_gate @ epoch 4 new loss 3.007551413247711e-06 old loss 3.0164846975822e-06 BETTER
I0326 00:15:51.741937 1260961 finetune.py:68] layer 1_gate @ epoch 4 new loss 6.06270714342827e-06 old loss 6.075281817174982e-06 BETTER
I0326 00:16:10.757477 1260134 finetune.py:45] layer 0_down initial loss 4.461761818674859e-06
I0326 00:16:14.224230 1260961 finetune.py:45] layer 1_down initial loss 1.5387722669402137e-05
I0326 00:16:38.989468 1260134 finetune.py:68] layer 0_down @ epoch 0 new loss 4.45525256509427e-06 old loss 4.461761818674859e-06 BETTER
I0326 00:16:41.063879 1260961 finetune.py:68] layer 1_down @ epoch 0 new loss 1.5376041119452566e-05 old loss 1.5387722669402137e-05 BETTER
I0326 00:17:07.965839 1260134 finetune.py:68] layer 0_down @ epoch 1 new loss 4.450685082701966e-06 old loss 4.45525256509427e-06 BETTER
I0326 00:17:08.757781 1260961 finetune.py:68] layer 1_down @ epoch 1 new loss 1.5372937923530117e-05 old loss 1.5376041119452566e-05 BETTER
I0326 00:17:36.367114 1260961 finetune.py:68] layer 1_down @ epoch 2 new loss 1.53705186676234e-05 old loss 1.5372937923530117e-05 BETTER
I0326 00:17:37.174916 1260134 finetune.py:68] layer 0_down @ epoch 2 new loss 4.447219907888211e-06 old loss 4.450685082701966e-06 BETTER
I0326 00:18:04.068633 1260961 finetune.py:68] layer 1_down @ epoch 3 new loss 1.5368470485555008e-05 old loss 1.53705186676234e-05 BETTER
I0326 00:18:06.432323 1260134 finetune.py:68] layer 0_down @ epoch 3 new loss 4.444472779141506e-06 old loss 4.447219907888211e-06 BETTER
I0326 00:18:31.749602 1260961 finetune.py:68] layer 1_down @ epoch 4 new loss 1.5366746083600447e-05 old loss 1.5368470485555008e-05 BETTER
1_v proxy err 0.046562448143959045 tr(WHW.T) 109.07096099853516
bpp_loss 2.686340093612671
1_q proxy err 0.0001833484711823985 tr(WHW.T) 144833.140625
bpp_loss 3.6093554496765137
1_k proxy err 9.84450161922723e-05 tr(WHW.T) 75525.8828125
bpp_loss 4.237854242324829
1_o proxy err 0.0180953498929739 tr(WHW.T) 1993.1068115234375
bpp_loss 2.752925157546997
1_up proxy err 0.019811946898698807 tr(WHW.T) 8231.591796875
bpp_loss 2.9739142826625278
1_gate proxy err 0.011994988657534122 tr(WHW.T) 13943.9033203125
bpp_loss 3.0766214643205916
1_down proxy err 0.0011299669276922941 tr(WHW.T) 13979.87890625
bpp_loss 2.9683865138462613
I0326 00:18:35.829066 1260134 finetune.py:68] layer 0_down @ epoch 4 new loss 4.442075351107633e-06 old loss 4.444472779141506e-06 BETTER
0_v proxy err 0.07475443184375763 tr(WHW.T) 60.88684844970703
bpp_loss 2.5797919034957886
0_q proxy err 0.00010935857426375151 tr(WHW.T) 288060.25
bpp_loss 3.365899920463562
0_k proxy err 9.431681974092498e-05 tr(WHW.T) 100112.7578125
bpp_loss 3.8800989389419556
0_o proxy err 0.009015558287501335 tr(WHW.T) 3162.73388671875
bpp_loss 2.6759133338928223
0_up proxy err 0.018032172694802284 tr(WHW.T) 8920.314453125
bpp_loss 2.9597602571759904
0_gate proxy err 0.01045056339353323 tr(WHW.T) 15774.9384765625
bpp_loss 3.0657746451241628
0_down proxy err 0.01265264954417944 tr(WHW.T) 10863.1552734375
bpp_loss 2.953433241162981
I0326 00:19:44.642009 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 2 in 63.11340594291687s
I0326 00:19:47.958806 1270432 config.py:54] PyTorch version 2.6.0 available.
W0326 00:19:48.240022 1270432 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 00:19:49.136423 1270432 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 00:19:49.140324 1258494 quantize_finetune_llama.py:209] layer 3 gpu 1
I0326 00:19:49.153294 1270432 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 00:19:56.422645 1270432 finetune.py:45] layer 2_v initial loss 1.2515096386778168e-05
W0326 00:19:56.422843 1270432 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 00:20:32.782461 1270432 finetune.py:68] layer 2_v @ epoch 0 new loss 4.125465238757897e-06 old loss 1.2515096386778168e-05 BETTER
I0326 00:20:49.890847 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 3 in 60.350900650024414s
I0326 00:20:53.320683 1270802 config.py:54] PyTorch version 2.6.0 available.
W0326 00:20:53.616267 1270802 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 00:20:54.573207 1270802 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 00:20:54.577285 1258494 quantize_finetune_llama.py:209] layer 4 gpu 0
I0326 00:20:54.590498 1270802 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 00:21:01.477616 1270802 finetune.py:45] layer 3_v initial loss 1.4925681171007454e-05
W0326 00:21:01.477920 1270802 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 00:21:10.252206 1270432 finetune.py:68] layer 2_v @ epoch 1 new loss 3.0865880944475066e-06 old loss 4.125465238757897e-06 BETTER
I0326 00:21:34.958930 1270802 finetune.py:68] layer 3_v @ epoch 0 new loss 5.513958967640065e-06 old loss 1.4925681171007454e-05 BETTER
I0326 00:21:47.730093 1270432 finetune.py:68] layer 2_v @ epoch 2 new loss 2.832095333360485e-06 old loss 3.0865880944475066e-06 BETTER
I0326 00:22:09.427565 1270802 finetune.py:68] layer 3_v @ epoch 1 new loss 4.285943305148976e-06 old loss 5.513958967640065e-06 BETTER
I0326 00:22:24.676382 1270432 finetune.py:68] layer 2_v @ epoch 3 new loss 2.7110982045996934e-06 old loss 2.832095333360485e-06 BETTER
I0326 00:22:44.301704 1270802 finetune.py:68] layer 3_v @ epoch 2 new loss 3.90235209124512e-06 old loss 4.285943305148976e-06 BETTER
I0326 00:23:01.775292 1270432 finetune.py:68] layer 2_v @ epoch 4 new loss 2.629378059282317e-06 old loss 2.7110982045996934e-06 BETTER
I0326 00:23:11.458374 1270432 finetune.py:45] layer 2_q initial loss 3.0843366403132677e-06
I0326 00:23:19.462476 1270802 finetune.py:68] layer 3_v @ epoch 3 new loss 3.7013205655966885e-06 old loss 3.90235209124512e-06 BETTER
I0326 00:23:47.403886 1270432 finetune.py:68] layer 2_q @ epoch 0 new loss 2.9367881779762683e-06 old loss 3.0843366403132677e-06 BETTER
I0326 00:23:54.445076 1270802 finetune.py:68] layer 3_v @ epoch 4 new loss 3.5675363960763207e-06 old loss 3.7013205655966885e-06 BETTER
I0326 00:24:03.892296 1270802 finetune.py:45] layer 3_q initial loss 4.337801328802016e-06
I0326 00:24:24.342641 1270432 finetune.py:68] layer 2_q @ epoch 1 new loss 2.8635131457122043e-06 old loss 2.9367881779762683e-06 BETTER
I0326 00:24:37.738082 1270802 finetune.py:68] layer 3_q @ epoch 0 new loss 4.133384663873585e-06 old loss 4.337801328802016e-06 BETTER
I0326 00:25:01.159597 1270432 finetune.py:68] layer 2_q @ epoch 2 new loss 2.8068400297343032e-06 old loss 2.8635131457122043e-06 BETTER
I0326 00:25:12.321895 1270802 finetune.py:68] layer 3_q @ epoch 1 new loss 4.0247668948723e-06 old loss 4.133384663873585e-06 BETTER
I0326 00:25:37.988835 1270432 finetune.py:68] layer 2_q @ epoch 3 new loss 2.759841208899161e-06 old loss 2.8068400297343032e-06 BETTER
I0326 00:25:46.904734 1270802 finetune.py:68] layer 3_q @ epoch 2 new loss 3.943388946936466e-06 old loss 4.0247668948723e-06 BETTER
I0326 00:26:14.789867 1270432 finetune.py:68] layer 2_q @ epoch 4 new loss 2.7197929739486426e-06 old loss 2.759841208899161e-06 BETTER
I0326 00:26:21.612466 1270802 finetune.py:68] layer 3_q @ epoch 3 new loss 3.8747948565287516e-06 old loss 3.943388946936466e-06 BETTER
I0326 00:26:22.698598 1270432 finetune.py:45] layer 2_k initial loss 2.8787853807443753e-06
I0326 00:26:56.257652 1270802 finetune.py:68] layer 3_q @ epoch 4 new loss 3.81956033379538e-06 old loss 3.8747948565287516e-06 BETTER
I0326 00:26:58.978015 1270432 finetune.py:68] layer 2_k @ epoch 0 new loss 2.8255735742277466e-06 old loss 2.8787853807443753e-06 BETTER
I0326 00:27:03.991668 1270802 finetune.py:45] layer 3_k initial loss 4.131995865463978e-06
I0326 00:27:35.797216 1270432 finetune.py:68] layer 2_k @ epoch 1 new loss 2.7932674129260704e-06 old loss 2.8255735742277466e-06 BETTER
I0326 00:27:37.678851 1270802 finetune.py:68] layer 3_k @ epoch 0 new loss 4.059873390360735e-06 old loss 4.131995865463978e-06 BETTER
I0326 00:28:12.065190 1270802 finetune.py:68] layer 3_k @ epoch 1 new loss 4.013141278846888e-06 old loss 4.059873390360735e-06 BETTER
I0326 00:28:12.787190 1270432 finetune.py:68] layer 2_k @ epoch 2 new loss 2.7647995466395514e-06 old loss 2.7932674129260704e-06 BETTER
I0326 00:28:46.418171 1270802 finetune.py:68] layer 3_k @ epoch 2 new loss 3.972712420363678e-06 old loss 4.013141278846888e-06 BETTER
I0326 00:28:49.418569 1270432 finetune.py:68] layer 2_k @ epoch 3 new loss 2.7394839889893774e-06 old loss 2.7647995466395514e-06 BETTER
I0326 00:29:20.967517 1270802 finetune.py:68] layer 3_k @ epoch 3 new loss 3.9366332202916965e-06 old loss 3.972712420363678e-06 BETTER
I0326 00:29:26.335149 1270432 finetune.py:68] layer 2_k @ epoch 4 new loss 2.7158521334058605e-06 old loss 2.7394839889893774e-06 BETTER
I0326 00:29:36.105319 1270432 finetune.py:45] layer 2_o initial loss 6.211891104612732e-06
I0326 00:29:55.636482 1270802 finetune.py:68] layer 3_k @ epoch 4 new loss 3.903627657564357e-06 old loss 3.9366332202916965e-06 BETTER
I0326 00:30:04.912497 1270802 finetune.py:45] layer 3_o initial loss 1.0432380804559216e-05
I0326 00:30:11.781391 1270432 finetune.py:68] layer 2_o @ epoch 0 new loss 5.984014933346771e-06 old loss 6.211891104612732e-06 BETTER
I0326 00:30:38.204643 1270802 finetune.py:68] layer 3_o @ epoch 0 new loss 9.904710168484598e-06 old loss 1.0432380804559216e-05 BETTER
I0326 00:30:48.120660 1270432 finetune.py:68] layer 2_o @ epoch 1 new loss 5.840943231305573e-06 old loss 5.984014933346771e-06 BETTER
I0326 00:31:12.165205 1270802 finetune.py:68] layer 3_o @ epoch 1 new loss 9.6746862254804e-06 old loss 9.904710168484598e-06 BETTER
I0326 00:31:24.420748 1270432 finetune.py:68] layer 2_o @ epoch 2 new loss 5.743086603615666e-06 old loss 5.840943231305573e-06 BETTER
I0326 00:31:46.190795 1270802 finetune.py:68] layer 3_o @ epoch 2 new loss 9.52585742197698e-06 old loss 9.6746862254804e-06 BETTER
I0326 00:32:00.758220 1270432 finetune.py:68] layer 2_o @ epoch 3 new loss 5.670994141837582e-06 old loss 5.743086603615666e-06 BETTER
I0326 00:32:20.332813 1270802 finetune.py:68] layer 3_o @ epoch 3 new loss 9.408612640982028e-06 old loss 9.52585742197698e-06 BETTER
I0326 00:32:37.105860 1270432 finetune.py:68] layer 2_o @ epoch 4 new loss 5.61426168133039e-06 old loss 5.670994141837582e-06 BETTER
I0326 00:32:54.608355 1270802 finetune.py:68] layer 3_o @ epoch 4 new loss 9.309775123256259e-06 old loss 9.408612640982028e-06 BETTER
I0326 00:32:58.865149 1270432 finetune.py:45] layer 2_up initial loss 9.131375009019393e-06
I0326 00:33:16.182293 1270802 finetune.py:45] layer 3_up initial loss 1.737139064061921e-05
I0326 00:33:31.367979 1270432 finetune.py:68] layer 2_up @ epoch 0 new loss 9.061180207936559e-06 old loss 9.131375009019393e-06 BETTER
I0326 00:33:47.096160 1270802 finetune.py:68] layer 3_up @ epoch 0 new loss 1.720262662274763e-05 old loss 1.737139064061921e-05 BETTER
I0326 00:34:05.055352 1270432 finetune.py:68] layer 2_up @ epoch 1 new loss 9.007061635202263e-06 old loss 9.061180207936559e-06 BETTER
I0326 00:34:19.124559 1270802 finetune.py:68] layer 3_up @ epoch 1 new loss 1.7082318663597107e-05 old loss 1.720262662274763e-05 BETTER
I0326 00:34:38.767664 1270432 finetune.py:68] layer 2_up @ epoch 2 new loss 8.9616296463646e-06 old loss 9.007061635202263e-06 BETTER
I0326 00:34:51.253532 1270802 finetune.py:68] layer 3_up @ epoch 2 new loss 1.698153391771484e-05 old loss 1.7082318663597107e-05 BETTER
I0326 00:35:12.477965 1270432 finetune.py:68] layer 2_up @ epoch 3 new loss 8.922431334212888e-06 old loss 8.9616296463646e-06 BETTER
I0326 00:35:23.430077 1270802 finetune.py:68] layer 3_up @ epoch 3 new loss 1.6890755432541482e-05 old loss 1.698153391771484e-05 BETTER
I0326 00:35:46.296922 1270432 finetune.py:68] layer 2_up @ epoch 4 new loss 8.887229341780767e-06 old loss 8.922431334212888e-06 BETTER
I0326 00:35:55.576505 1270802 finetune.py:68] layer 3_up @ epoch 4 new loss 1.6808271539048292e-05 old loss 1.6890755432541482e-05 BETTER
I0326 00:36:08.145185 1270432 finetune.py:45] layer 2_gate initial loss 1.1143568372062873e-05
I0326 00:36:17.285025 1270802 finetune.py:45] layer 3_gate initial loss 2.0738320017699152e-05
I0326 00:36:38.585870 1270432 finetune.py:68] layer 2_gate @ epoch 0 new loss 1.1100207302661147e-05 old loss 1.1143568372062873e-05 BETTER
I0326 00:36:45.906836 1270802 finetune.py:68] layer 3_gate @ epoch 0 new loss 2.063294050458353e-05 old loss 2.0738320017699152e-05 BETTER
I0326 00:37:09.971419 1270432 finetune.py:68] layer 2_gate @ epoch 1 new loss 1.1063105375797022e-05 old loss 1.1100207302661147e-05 BETTER
I0326 00:37:15.470714 1270802 finetune.py:68] layer 3_gate @ epoch 1 new loss 2.054760989267379e-05 old loss 2.063294050458353e-05 BETTER
I0326 00:37:41.402512 1270432 finetune.py:68] layer 2_gate @ epoch 2 new loss 1.1030107089027297e-05 old loss 1.1063105375797022e-05 BETTER
I0326 00:37:45.041031 1270802 finetune.py:68] layer 3_gate @ epoch 2 new loss 2.0471468815230764e-05 old loss 2.054760989267379e-05 BETTER
I0326 00:38:12.819926 1270432 finetune.py:68] layer 2_gate @ epoch 3 new loss 1.1000195627275389e-05 old loss 1.1030107089027297e-05 BETTER
I0326 00:38:14.771385 1270802 finetune.py:68] layer 3_gate @ epoch 3 new loss 2.04022326215636e-05 old loss 2.0471468815230764e-05 BETTER
I0326 00:38:44.415292 1270802 finetune.py:68] layer 3_gate @ epoch 4 new loss 2.033763303188607e-05 old loss 2.04022326215636e-05 BETTER
I0326 00:38:44.445155 1270432 finetune.py:68] layer 2_gate @ epoch 4 new loss 1.09728589450242e-05 old loss 1.1000195627275389e-05 BETTER
I0326 00:39:06.772727 1270802 finetune.py:45] layer 3_down initial loss 3.114644277957268e-05
I0326 00:39:07.134249 1270432 finetune.py:45] layer 2_down initial loss 1.6339858120772988e-05
I0326 00:39:33.512425 1270802 finetune.py:68] layer 3_down @ epoch 0 new loss 3.113677303190343e-05 old loss 3.114644277957268e-05 BETTER
I0326 00:39:35.188891 1270432 finetune.py:68] layer 2_down @ epoch 0 new loss 1.6332378436345607e-05 old loss 1.6339858120772988e-05 BETTER
I0326 00:40:01.115380 1270802 finetune.py:68] layer 3_down @ epoch 1 new loss 3.1129162380238995e-05 old loss 3.113677303190343e-05 BETTER
I0326 00:40:04.077913 1270432 finetune.py:68] layer 2_down @ epoch 1 new loss 1.632655221328605e-05 old loss 1.6332378436345607e-05 BETTER
I0326 00:40:28.780094 1270802 finetune.py:68] layer 3_down @ epoch 2 new loss 3.1122934160521254e-05 old loss 3.1129162380238995e-05 BETTER
I0326 00:40:32.965895 1270432 finetune.py:68] layer 2_down @ epoch 2 new loss 1.6321962903020903e-05 old loss 1.632655221328605e-05 BETTER
I0326 00:40:56.411271 1270802 finetune.py:68] layer 3_down @ epoch 3 new loss 3.111778642050922e-05 old loss 3.1122934160521254e-05 BETTER
I0326 00:41:02.318519 1270432 finetune.py:68] layer 2_down @ epoch 3 new loss 1.6318337657139637e-05 old loss 1.6321962903020903e-05 BETTER
I0326 00:41:24.059724 1270802 finetune.py:68] layer 3_down @ epoch 4 new loss 3.1113613658817485e-05 old loss 3.111778642050922e-05 BETTER
3_v proxy err 0.03063531219959259 tr(WHW.T) 289.3331604003906
bpp_loss 2.695397138595581
3_q proxy err 0.00091141666052863 tr(WHW.T) 47582.62890625
bpp_loss 3.6043070554733276
3_k proxy err 0.0004636633675545454 tr(WHW.T) 26197.5390625
bpp_loss 4.420673370361328
3_o proxy err 0.021130187436938286 tr(WHW.T) 1860.9708251953125
bpp_loss 2.808919072151184
3_up proxy err 0.02179114893078804 tr(WHW.T) 7537.185546875
bpp_loss 2.948828969682966
3_gate proxy err 0.008311998099088669 tr(WHW.T) 20857.162109375
bpp_loss 3.178770065307617
3_down proxy err 0.02191818319261074 tr(WHW.T) 7054.28466796875
bpp_loss 2.949080671582903
I0326 00:41:31.737527 1270432 finetune.py:68] layer 2_down @ epoch 4 new loss 1.631545637792442e-05 old loss 1.6318337657139637e-05 BETTER
2_v proxy err 0.04595709592103958 tr(WHW.T) 155.95950317382812
bpp_loss 2.5975788831710815
2_q proxy err 0.0008589719654992223 tr(WHW.T) 41460.23046875
bpp_loss 3.563246011734009
2_k proxy err 0.0004335051926318556 tr(WHW.T) 22634.474609375
bpp_loss 4.346205711364746
2_o proxy err 0.016277719289064407 tr(WHW.T) 1973.630859375
bpp_loss 2.7111432552337646
2_up proxy err 0.021912984549999237 tr(WHW.T) 7602.2255859375
bpp_loss 2.96612126486642
2_gate proxy err 0.011386873200535774 tr(WHW.T) 15115.4287109375
bpp_loss 3.1080850873674666
2_down proxy err 0.019166134297847748 tr(WHW.T) 7768.0537109375
bpp_loss 2.9709469250270297
I0326 00:42:40.939750 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 4 in 63.56494450569153s
I0326 00:42:44.210202 1282255 config.py:54] PyTorch version 2.6.0 available.
W0326 00:42:44.492636 1282255 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 00:42:45.374166 1282255 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 00:42:45.378168 1258494 quantize_finetune_llama.py:209] layer 5 gpu 1
I0326 00:42:45.398754 1282255 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 00:42:52.756575 1282255 finetune.py:45] layer 4_v initial loss 1.616010194993578e-05
W0326 00:42:52.756848 1282255 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 00:43:29.056954 1282255 finetune.py:68] layer 4_v @ epoch 0 new loss 6.627823495364282e-06 old loss 1.616010194993578e-05 BETTER
I0326 00:43:46.006390 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 5 in 60.20921230316162s
I0326 00:43:49.404898 1283101 config.py:54] PyTorch version 2.6.0 available.
W0326 00:43:49.713449 1283101 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 00:43:50.620616 1283101 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 00:43:50.626028 1258494 quantize_finetune_llama.py:209] layer 6 gpu 0
I0326 00:43:50.640260 1283101 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 00:43:57.472573 1283101 finetune.py:45] layer 5_v initial loss 1.7452517568017356e-05
W0326 00:43:57.472852 1283101 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 00:44:06.276348 1282255 finetune.py:68] layer 4_v @ epoch 1 new loss 5.549185516429134e-06 old loss 6.627823495364282e-06 BETTER
I0326 00:44:30.952406 1283101 finetune.py:68] layer 5_v @ epoch 0 new loss 8.870383680914529e-06 old loss 1.7452517568017356e-05 BETTER
I0326 00:44:43.305480 1282255 finetune.py:68] layer 4_v @ epoch 2 new loss 5.129421879246365e-06 old loss 5.549185516429134e-06 BETTER
I0326 00:45:05.464290 1283101 finetune.py:68] layer 5_v @ epoch 1 new loss 7.892374924267642e-06 old loss 8.870383680914529e-06 BETTER
I0326 00:45:20.338945 1282255 finetune.py:68] layer 4_v @ epoch 3 new loss 4.887148861598689e-06 old loss 5.129421879246365e-06 BETTER
I0326 00:45:40.176477 1283101 finetune.py:68] layer 5_v @ epoch 2 new loss 7.456295861629769e-06 old loss 7.892374924267642e-06 BETTER
I0326 00:45:57.914626 1282255 finetune.py:68] layer 4_v @ epoch 4 new loss 4.721005097962916e-06 old loss 4.887148861598689e-06 BETTER
I0326 00:46:07.608609 1282255 finetune.py:45] layer 4_q initial loss 5.854437858943129e-06
I0326 00:46:15.022031 1283101 finetune.py:68] layer 5_v @ epoch 3 new loss 7.183179150160868e-06 old loss 7.456295861629769e-06 BETTER
I0326 00:46:43.752389 1282255 finetune.py:68] layer 4_q @ epoch 0 new loss 5.522922947420739e-06 old loss 5.854437858943129e-06 BETTER
I0326 00:46:50.030097 1283101 finetune.py:68] layer 5_v @ epoch 4 new loss 6.99131669534836e-06 old loss 7.183179150160868e-06 BETTER
I0326 00:46:59.472199 1283101 finetune.py:45] layer 5_q initial loss 8.764743142819498e-06
I0326 00:47:20.717524 1282255 finetune.py:68] layer 4_q @ epoch 1 new loss 5.367772700992646e-06 old loss 5.522922947420739e-06 BETTER
I0326 00:47:33.289578 1283101 finetune.py:68] layer 5_q @ epoch 0 new loss 8.312845238833688e-06 old loss 8.764743142819498e-06 BETTER
I0326 00:47:57.659535 1282255 finetune.py:68] layer 4_q @ epoch 2 new loss 5.248964043857995e-06 old loss 5.367772700992646e-06 BETTER
I0326 00:48:07.887004 1283101 finetune.py:68] layer 5_q @ epoch 1 new loss 8.114073352771811e-06 old loss 8.312845238833688e-06 BETTER
I0326 00:48:34.546469 1282255 finetune.py:68] layer 4_q @ epoch 3 new loss 5.15222654939862e-06 old loss 5.248964043857995e-06 BETTER
I0326 00:48:42.476945 1283101 finetune.py:68] layer 5_q @ epoch 2 new loss 7.963692951307166e-06 old loss 8.114073352771811e-06 BETTER
I0326 00:49:11.936223 1282255 finetune.py:68] layer 4_q @ epoch 4 new loss 5.0712314987322316e-06 old loss 5.15222654939862e-06 BETTER
I0326 00:49:17.269806 1283101 finetune.py:68] layer 5_q @ epoch 3 new loss 7.841316801204812e-06 old loss 7.963692951307166e-06 BETTER
I0326 00:49:20.051055 1282255 finetune.py:45] layer 4_k initial loss 5.460071406560019e-06
I0326 00:49:51.975102 1283101 finetune.py:68] layer 5_q @ epoch 4 new loss 7.736863153695595e-06 old loss 7.841316801204812e-06 BETTER
I0326 00:49:55.794640 1282255 finetune.py:68] layer 4_k @ epoch 0 new loss 5.350036190066021e-06 old loss 5.460071406560019e-06 BETTER
I0326 00:49:59.837842 1283101 finetune.py:45] layer 5_k initial loss 8.236027497332543e-06
I0326 00:50:32.434065 1282255 finetune.py:68] layer 4_k @ epoch 1 new loss 5.282734036882175e-06 old loss 5.350036190066021e-06 BETTER
I0326 00:50:33.568163 1283101 finetune.py:68] layer 5_k @ epoch 0 new loss 8.063185305218212e-06 old loss 8.236027497332543e-06 BETTER
I0326 00:51:08.246378 1283101 finetune.py:68] layer 5_k @ epoch 1 new loss 7.97809934738325e-06 old loss 8.063185305218212e-06 BETTER
I0326 00:51:09.091487 1282255 finetune.py:68] layer 4_k @ epoch 2 new loss 5.2254199545132e-06 old loss 5.282734036882175e-06 BETTER
I0326 00:51:42.930845 1283101 finetune.py:68] layer 5_k @ epoch 2 new loss 7.904463927843608e-06 old loss 7.97809934738325e-06 BETTER
I0326 00:51:45.774468 1282255 finetune.py:68] layer 4_k @ epoch 3 new loss 5.1756705943262205e-06 old loss 5.2254199545132e-06 BETTER
I0326 00:52:17.625857 1283101 finetune.py:68] layer 5_k @ epoch 3 new loss 7.838509191060439e-06 old loss 7.904463927843608e-06 BETTER
I0326 00:52:22.596170 1282255 finetune.py:68] layer 4_k @ epoch 4 new loss 5.129166765982518e-06 old loss 5.1756705943262205e-06 BETTER
I0326 00:52:32.320583 1282255 finetune.py:45] layer 4_o initial loss 1.486441760789603e-05
I0326 00:52:52.301180 1283101 finetune.py:68] layer 5_k @ epoch 4 new loss 7.77983404987026e-06 old loss 7.838509191060439e-06 BETTER
I0326 00:53:01.629808 1283101 finetune.py:45] layer 5_o initial loss 2.115176903316751e-05
I0326 00:53:07.569760 1282255 finetune.py:68] layer 4_o @ epoch 0 new loss 1.3886763554182835e-05 old loss 1.486441760789603e-05 BETTER
I0326 00:53:34.752546 1283101 finetune.py:68] layer 5_o @ epoch 0 new loss 1.9770610379055142e-05 old loss 2.115176903316751e-05 BETTER
I0326 00:53:43.461166 1282255 finetune.py:68] layer 4_o @ epoch 1 new loss 1.3486771422321908e-05 old loss 1.3886763554182835e-05 BETTER
I0326 00:54:08.701535 1283101 finetune.py:68] layer 5_o @ epoch 1 new loss 1.917000554385595e-05 old loss 1.9770610379055142e-05 BETTER
I0326 00:54:19.429823 1282255 finetune.py:68] layer 4_o @ epoch 2 new loss 1.3196939107729122e-05 old loss 1.3486771422321908e-05 BETTER
I0326 00:54:42.524322 1283101 finetune.py:68] layer 5_o @ epoch 2 new loss 1.8743929103948176e-05 old loss 1.917000554385595e-05 BETTER
I0326 00:54:55.341103 1282255 finetune.py:68] layer 4_o @ epoch 3 new loss 1.2968127521162387e-05 old loss 1.3196939107729122e-05 BETTER
I0326 00:55:16.385883 1283101 finetune.py:68] layer 5_o @ epoch 3 new loss 1.841302218963392e-05 old loss 1.8743929103948176e-05 BETTER
I0326 00:55:31.224630 1282255 finetune.py:68] layer 4_o @ epoch 4 new loss 1.2774478818755597e-05 old loss 1.2968127521162387e-05 BETTER
I0326 00:55:50.342818 1283101 finetune.py:68] layer 5_o @ epoch 4 new loss 1.813923336158041e-05 old loss 1.841302218963392e-05 BETTER
I0326 00:55:52.825116 1282255 finetune.py:45] layer 4_up initial loss 2.7579038942349143e-05
I0326 00:56:11.814269 1283101 finetune.py:45] layer 5_up initial loss 3.9480313716921955e-05
I0326 00:56:24.835602 1282255 finetune.py:68] layer 4_up @ epoch 0 new loss 2.7095573386759497e-05 old loss 2.7579038942349143e-05 BETTER
I0326 00:56:42.432994 1283101 finetune.py:68] layer 5_up @ epoch 0 new loss 3.86221036023926e-05 old loss 3.9480313716921955e-05 BETTER
I0326 00:56:58.029852 1282255 finetune.py:68] layer 4_up @ epoch 1 new loss 2.678231794561725e-05 old loss 2.7095573386759497e-05 BETTER
I0326 00:57:14.312018 1283101 finetune.py:68] layer 5_up @ epoch 1 new loss 3.8089237932581455e-05 old loss 3.86221036023926e-05 BETTER
I0326 00:57:31.541166 1282255 finetune.py:68] layer 4_up @ epoch 2 new loss 2.652344664966222e-05 old loss 2.678231794561725e-05 BETTER
I0326 00:57:46.389281 1283101 finetune.py:68] layer 5_up @ epoch 2 new loss 3.764790380955674e-05 old loss 3.8089237932581455e-05 BETTER
I0326 00:58:05.314867 1282255 finetune.py:68] layer 4_up @ epoch 3 new loss 2.6290126697858796e-05 old loss 2.652344664966222e-05 BETTER
I0326 00:58:18.463616 1283101 finetune.py:68] layer 5_up @ epoch 3 new loss 3.726255818037316e-05 old loss 3.764790380955674e-05 BETTER
I0326 00:58:38.988374 1282255 finetune.py:68] layer 4_up @ epoch 4 new loss 2.607942406029906e-05 old loss 2.6290126697858796e-05 BETTER
I0326 00:58:50.510992 1283101 finetune.py:68] layer 5_up @ epoch 4 new loss 3.691627352964133e-05 old loss 3.726255818037316e-05 BETTER
I0326 00:59:00.713300 1282255 finetune.py:45] layer 4_gate initial loss 3.158375693601556e-05
I0326 00:59:12.200681 1283101 finetune.py:45] layer 5_gate initial loss 4.470974818104878e-05
I0326 00:59:31.108637 1282255 finetune.py:68] layer 4_gate @ epoch 0 new loss 3.133763311780058e-05 old loss 3.158375693601556e-05 BETTER
I0326 00:59:40.752930 1283101 finetune.py:68] layer 5_gate @ epoch 0 new loss 4.4304400944383815e-05 old loss 4.470974818104878e-05 BETTER
I0326 01:00:02.421612 1282255 finetune.py:68] layer 4_gate @ epoch 1 new loss 3.11355761368759e-05 old loss 3.133763311780058e-05 BETTER
I0326 01:00:10.212380 1283101 finetune.py:68] layer 5_gate @ epoch 1 new loss 4.397850716486573e-05 old loss 4.4304400944383815e-05 BETTER
I0326 01:00:33.757016 1282255 finetune.py:68] layer 4_gate @ epoch 2 new loss 3.09525421471335e-05 old loss 3.11355761368759e-05 BETTER
I0326 01:00:39.837787 1283101 finetune.py:68] layer 5_gate @ epoch 2 new loss 4.368333611637354e-05 old loss 4.397850716486573e-05 BETTER
I0326 01:01:05.204638 1282255 finetune.py:68] layer 4_gate @ epoch 3 new loss 3.0784427508478984e-05 old loss 3.09525421471335e-05 BETTER
I0326 01:01:09.485409 1283101 finetune.py:68] layer 5_gate @ epoch 3 new loss 4.3419626308605075e-05 old loss 4.368333611637354e-05 BETTER
I0326 01:01:36.673037 1282255 finetune.py:68] layer 4_gate @ epoch 4 new loss 3.062742325710133e-05 old loss 3.0784427508478984e-05 BETTER
I0326 01:01:39.110614 1283101 finetune.py:68] layer 5_gate @ epoch 4 new loss 4.3168332922505215e-05 old loss 4.3419626308605075e-05 BETTER
I0326 01:01:59.555976 1282255 finetune.py:45] layer 4_down initial loss 5.023360427003354e-05
I0326 01:02:01.474917 1283101 finetune.py:45] layer 5_down initial loss 7.065788668114692e-05
I0326 01:02:27.745555 1282255 finetune.py:68] layer 4_down @ epoch 0 new loss 5.021876859245822e-05 old loss 5.023360427003354e-05 BETTER
I0326 01:02:28.455199 1283101 finetune.py:68] layer 5_down @ epoch 0 new loss 7.064032251946628e-05 old loss 7.065788668114692e-05 BETTER
I0326 01:02:56.161170 1283101 finetune.py:68] layer 5_down @ epoch 1 new loss 7.063013617880642e-05 old loss 7.064032251946628e-05 BETTER
I0326 01:02:57.051056 1282255 finetune.py:68] layer 4_down @ epoch 1 new loss 5.020873140892945e-05 old loss 5.021876859245822e-05 BETTER
I0326 01:03:23.872325 1283101 finetune.py:68] layer 5_down @ epoch 2 new loss 7.062170334393159e-05 old loss 7.063013617880642e-05 BETTER
I0326 01:03:26.348039 1282255 finetune.py:68] layer 4_down @ epoch 2 new loss 5.0200873374706134e-05 old loss 5.020873140892945e-05 BETTER
I0326 01:03:51.623250 1283101 finetune.py:68] layer 5_down @ epoch 3 new loss 7.061476935632527e-05 old loss 7.062170334393159e-05 BETTER
I0326 01:03:55.643250 1282255 finetune.py:68] layer 4_down @ epoch 3 new loss 5.019473610445857e-05 old loss 5.0200873374706134e-05 BETTER
I0326 01:04:19.359171 1283101 finetune.py:68] layer 5_down @ epoch 4 new loss 7.060910866130143e-05 old loss 7.061476935632527e-05 BETTER
5_v proxy err 0.038701556622982025 tr(WHW.T) 208.81988525390625
bpp_loss 2.6287732124328613
5_q proxy err 0.00109406269621104 tr(WHW.T) 36005.64453125
bpp_loss 3.545473337173462
5_k proxy err 0.00047688986524008214 tr(WHW.T) 23032.130859375
bpp_loss 4.3780388832092285
5_o proxy err 0.032091304659843445 tr(WHW.T) 1067.384765625
bpp_loss 2.766375184059143
5_up proxy err 0.02074560336768627 tr(WHW.T) 7657.859375
bpp_loss 2.927220344543457
5_gate proxy err 0.005629274994134903 tr(WHW.T) 30393.546875
bpp_loss 3.2494044985089983
5_down proxy err 0.024235591292381287 tr(WHW.T) 6462.16748046875
bpp_loss 2.9302937303270613
I0326 01:04:24.851311 1282255 finetune.py:68] layer 4_down @ epoch 4 new loss 5.0189588364446536e-05 old loss 5.019473610445857e-05 BETTER
4_v proxy err 0.028486058115959167 tr(WHW.T) 285.30712890625
bpp_loss 2.7395126819610596
4_q proxy err 0.0007801157189533114 tr(WHW.T) 50132.984375
bpp_loss 3.56978976726532
4_k proxy err 0.0003754198260139674 tr(WHW.T) 29348.28515625
bpp_loss 4.403228044509888
4_o proxy err 0.027689214795827866 tr(WHW.T) 1306.1678466796875
bpp_loss 2.8174424171447754
4_up proxy err 0.02164674922823906 tr(WHW.T) 7381.78955078125
bpp_loss 2.9219991139003207
4_gate proxy err 0.005926461424678564 tr(WHW.T) 29097.611328125
bpp_loss 3.2465381622314453
4_down proxy err 0.02465338632464409 tr(WHW.T) 6445.46435546875
bpp_loss 2.9253656523568288
I0326 01:05:33.369514 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 6 in 63.57040882110596s
I0326 01:05:36.610226 1298892 config.py:54] PyTorch version 2.6.0 available.
W0326 01:05:36.890624 1298892 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 01:05:37.790070 1298892 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 01:05:37.794022 1258494 quantize_finetune_llama.py:209] layer 7 gpu 1
I0326 01:05:37.808682 1298892 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 01:05:45.007958 1298892 finetune.py:45] layer 6_v initial loss 1.843012432800606e-05
W0326 01:05:45.008224 1298892 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 01:06:20.832146 1298892 finetune.py:68] layer 6_v @ epoch 0 new loss 1.0542238669586368e-05 old loss 1.843012432800606e-05 BETTER
I0326 01:06:38.189057 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 7 in 59.9728000164032s
I0326 01:06:41.555904 1299728 config.py:54] PyTorch version 2.6.0 available.
W0326 01:06:41.848602 1299728 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 01:06:42.734778 1299728 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 01:06:42.739353 1258494 quantize_finetune_llama.py:209] layer 8 gpu 0
I0326 01:06:42.753388 1299728 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 01:06:49.612774 1299728 finetune.py:45] layer 7_v initial loss 2.0098812456126325e-05
W0326 01:06:49.613024 1299728 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 01:06:57.746546 1298892 finetune.py:68] layer 6_v @ epoch 1 new loss 9.55878113018116e-06 old loss 1.0542238669586368e-05 BETTER
I0326 01:07:23.169256 1299728 finetune.py:68] layer 7_v @ epoch 0 new loss 1.2901175068691373e-05 old loss 2.0098812456126325e-05 BETTER
I0326 01:07:35.014772 1298892 finetune.py:68] layer 6_v @ epoch 2 new loss 9.077511094801594e-06 old loss 9.55878113018116e-06 BETTER
I0326 01:07:57.617887 1299728 finetune.py:68] layer 7_v @ epoch 1 new loss 1.187718407891225e-05 old loss 1.2901175068691373e-05 BETTER
I0326 01:08:11.985096 1298892 finetune.py:68] layer 6_v @ epoch 3 new loss 8.76649392012041e-06 old loss 9.077511094801594e-06 BETTER
I0326 01:08:32.444640 1299728 finetune.py:68] layer 7_v @ epoch 2 new loss 1.134007288783323e-05 old loss 1.187718407891225e-05 BETTER
I0326 01:08:48.888084 1298892 finetune.py:68] layer 6_v @ epoch 4 new loss 8.537121175322682e-06 old loss 8.76649392012041e-06 BETTER
I0326 01:08:58.586200 1298892 finetune.py:45] layer 6_q initial loss 1.0762729289126582e-05
I0326 01:09:07.470326 1299728 finetune.py:68] layer 7_v @ epoch 3 new loss 1.0980243132507894e-05 old loss 1.134007288783323e-05 BETTER
I0326 01:09:34.528059 1298892 finetune.py:68] layer 6_q @ epoch 0 new loss 1.0138476682186592e-05 old loss 1.0762729289126582e-05 BETTER
I0326 01:09:42.438906 1299728 finetune.py:68] layer 7_v @ epoch 4 new loss 1.0706399734772276e-05 old loss 1.0980243132507894e-05 BETTER
I0326 01:09:51.914499 1299728 finetune.py:45] layer 7_q initial loss 1.3877150195185095e-05
I0326 01:10:11.220650 1298892 finetune.py:68] layer 6_q @ epoch 1 new loss 9.896423762256745e-06 old loss 1.0138476682186592e-05 BETTER
I0326 01:10:25.894225 1299728 finetune.py:68] layer 7_q @ epoch 0 new loss 1.3146324818080757e-05 old loss 1.3877150195185095e-05 BETTER
I0326 01:10:47.756620 1298892 finetune.py:68] layer 6_q @ epoch 2 new loss 9.71446388575714e-06 old loss 9.896423762256745e-06 BETTER
I0326 01:11:00.454366 1299728 finetune.py:68] layer 7_q @ epoch 1 new loss 1.2838658221880905e-05 old loss 1.3146324818080757e-05 BETTER
I0326 01:11:24.356060 1298892 finetune.py:68] layer 6_q @ epoch 3 new loss 9.559436875861138e-06 old loss 9.71446388575714e-06 BETTER
I0326 01:11:34.908434 1299728 finetune.py:68] layer 7_q @ epoch 2 new loss 1.2604755283973645e-05 old loss 1.2838658221880905e-05 BETTER
I0326 01:12:01.085002 1298892 finetune.py:68] layer 6_q @ epoch 4 new loss 9.430752470507286e-06 old loss 9.559436875861138e-06 BETTER
I0326 01:12:08.950193 1298892 finetune.py:45] layer 6_k initial loss 1.0180529898207169e-05
I0326 01:12:09.494614 1299728 finetune.py:68] layer 7_q @ epoch 3 new loss 1.2419181985023897e-05 old loss 1.2604755283973645e-05 BETTER
I0326 01:12:44.158639 1299728 finetune.py:68] layer 7_q @ epoch 4 new loss 1.2254994544491637e-05 old loss 1.2419181985023897e-05 BETTER
I0326 01:12:45.010832 1298892 finetune.py:68] layer 6_k @ epoch 0 new loss 9.916252565744799e-06 old loss 1.0180529898207169e-05 BETTER
I0326 01:12:51.773545 1299728 finetune.py:45] layer 7_k initial loss 1.3084452803013846e-05
I0326 01:13:21.799260 1298892 finetune.py:68] layer 6_k @ epoch 1 new loss 9.8036007329938e-06 old loss 9.916252565744799e-06 BETTER
I0326 01:13:25.629866 1299728 finetune.py:68] layer 7_k @ epoch 0 new loss 1.285197231482016e-05 old loss 1.3084452803013846e-05 BETTER
I0326 01:13:58.629782 1298892 finetune.py:68] layer 6_k @ epoch 2 new loss 9.708909601613414e-06 old loss 9.8036007329938e-06 BETTER
I0326 01:14:00.089516 1299728 finetune.py:68] layer 7_k @ epoch 1 new loss 1.2713921023532748e-05 old loss 1.285197231482016e-05 BETTER
I0326 01:14:34.585787 1299728 finetune.py:68] layer 7_k @ epoch 2 new loss 1.2602249626070261e-05 old loss 1.2713921023532748e-05 BETTER
I0326 01:14:35.527344 1298892 finetune.py:68] layer 6_k @ epoch 3 new loss 9.623469850339461e-06 old loss 9.708909601613414e-06 BETTER
I0326 01:15:09.173713 1299728 finetune.py:68] layer 7_k @ epoch 3 new loss 1.2501758646976668e-05 old loss 1.2602249626070261e-05 BETTER
I0326 01:15:12.315155 1298892 finetune.py:68] layer 6_k @ epoch 4 new loss 9.549118658469524e-06 old loss 9.623469850339461e-06 BETTER
I0326 01:15:21.987589 1298892 finetune.py:45] layer 6_o initial loss 2.943470281024929e-05
I0326 01:15:43.670541 1299728 finetune.py:68] layer 7_k @ epoch 4 new loss 1.2408239854266867e-05 old loss 1.2501758646976668e-05 BETTER
I0326 01:15:53.040506 1299728 finetune.py:45] layer 7_o initial loss 4.16090406361036e-05
I0326 01:15:57.059674 1298892 finetune.py:68] layer 6_o @ epoch 0 new loss 2.7257543479208834e-05 old loss 2.943470281024929e-05 BETTER
I0326 01:16:26.089458 1299728 finetune.py:68] layer 7_o @ epoch 0 new loss 3.7954501749482006e-05 old loss 4.16090406361036e-05 BETTER
I0326 01:16:33.079720 1298892 finetune.py:68] layer 6_o @ epoch 1 new loss 2.6300032914150506e-05 old loss 2.7257543479208834e-05 BETTER
I0326 01:17:00.104148 1299728 finetune.py:68] layer 7_o @ epoch 1 new loss 3.625637327786535e-05 old loss 3.7954501749482006e-05 BETTER
I0326 01:17:08.874265 1298892 finetune.py:68] layer 6_o @ epoch 2 new loss 2.5628831281210296e-05 old loss 2.6300032914150506e-05 BETTER
I0326 01:17:34.081212 1299728 finetune.py:68] layer 7_o @ epoch 2 new loss 3.509754969854839e-05 old loss 3.625637327786535e-05 BETTER
I0326 01:17:44.885787 1298892 finetune.py:68] layer 6_o @ epoch 3 new loss 2.510466583771631e-05 old loss 2.5628831281210296e-05 BETTER
I0326 01:18:07.975965 1299728 finetune.py:68] layer 7_o @ epoch 3 new loss 3.4208405850222334e-05 old loss 3.509754969854839e-05 BETTER
I0326 01:18:20.845350 1298892 finetune.py:68] layer 6_o @ epoch 4 new loss 2.4669938284205273e-05 old loss 2.510466583771631e-05 BETTER
I0326 01:18:42.044250 1299728 finetune.py:68] layer 7_o @ epoch 4 new loss 3.349447069922462e-05 old loss 3.4208405850222334e-05 BETTER
I0326 01:18:42.641271 1298892 finetune.py:45] layer 6_up initial loss 5.199166844249703e-05
I0326 01:19:03.293643 1299728 finetune.py:45] layer 7_up initial loss 6.321801629383117e-05
I0326 01:19:15.034457 1298892 finetune.py:68] layer 6_up @ epoch 0 new loss 5.063179924036376e-05 old loss 5.199166844249703e-05 BETTER
I0326 01:19:34.089919 1299728 finetune.py:68] layer 7_up @ epoch 0 new loss 6.147977546788752e-05 old loss 6.321801629383117e-05 BETTER
I0326 01:19:48.433944 1298892 finetune.py:68] layer 6_up @ epoch 1 new loss 4.978726428817026e-05 old loss 5.063179924036376e-05 BETTER
I0326 01:20:05.890663 1299728 finetune.py:68] layer 7_up @ epoch 1 new loss 6.039821892045438e-05 old loss 6.147977546788752e-05 BETTER
I0326 01:20:22.225378 1298892 finetune.py:68] layer 6_up @ epoch 2 new loss 4.909493509330787e-05 old loss 4.978726428817026e-05 BETTER
I0326 01:20:37.761552 1299728 finetune.py:68] layer 7_up @ epoch 2 new loss 5.950555714662187e-05 old loss 6.039821892045438e-05 BETTER
I0326 01:20:55.948131 1298892 finetune.py:68] layer 6_up @ epoch 3 new loss 4.849553442909382e-05 old loss 4.909493509330787e-05 BETTER
I0326 01:21:09.900672 1299728 finetune.py:68] layer 7_up @ epoch 3 new loss 5.873349800822325e-05 old loss 5.950555714662187e-05 BETTER
I0326 01:21:29.432436 1298892 finetune.py:68] layer 6_up @ epoch 4 new loss 4.795605855179019e-05 old loss 4.849553442909382e-05 BETTER
I0326 01:21:42.048660 1299728 finetune.py:68] layer 7_up @ epoch 4 new loss 5.804304237244651e-05 old loss 5.873349800822325e-05 BETTER
I0326 01:21:51.203174 1298892 finetune.py:45] layer 6_gate initial loss 5.728190080844797e-05
I0326 01:22:03.657106 1299728 finetune.py:45] layer 7_gate initial loss 6.94149493938312e-05
I0326 01:22:21.388707 1298892 finetune.py:68] layer 6_gate @ epoch 0 new loss 5.666560537065379e-05 old loss 5.728190080844797e-05 BETTER
I0326 01:22:32.176927 1299728 finetune.py:68] layer 7_gate @ epoch 0 new loss 6.86355633661151e-05 old loss 6.94149493938312e-05 BETTER
I0326 01:22:52.483690 1298892 finetune.py:68] layer 6_gate @ epoch 1 new loss 5.617827264359221e-05 old loss 5.666560537065379e-05 BETTER
I0326 01:23:01.580647 1299728 finetune.py:68] layer 7_gate @ epoch 1 new loss 6.803036376368254e-05 old loss 6.86355633661151e-05 BETTER
I0326 01:23:23.849675 1298892 finetune.py:68] layer 6_gate @ epoch 2 new loss 5.5746055295458063e-05 old loss 5.617827264359221e-05 BETTER
I0326 01:23:31.262153 1299728 finetune.py:68] layer 7_gate @ epoch 2 new loss 6.749441672582179e-05 old loss 6.803036376368254e-05 BETTER
I0326 01:23:55.201025 1298892 finetune.py:68] layer 6_gate @ epoch 3 new loss 5.534846422960982e-05 old loss 5.5746055295458063e-05 BETTER
I0326 01:24:00.927438 1299728 finetune.py:68] layer 7_gate @ epoch 3 new loss 6.700392259517685e-05 old loss 6.749441672582179e-05 BETTER
I0326 01:24:26.551806 1298892 finetune.py:68] layer 6_gate @ epoch 4 new loss 5.498210884979926e-05 old loss 5.534846422960982e-05 BETTER
I0326 01:24:30.572412 1299728 finetune.py:68] layer 7_gate @ epoch 4 new loss 6.65531333652325e-05 old loss 6.700392259517685e-05 BETTER
I0326 01:24:49.477011 1298892 finetune.py:45] layer 6_down initial loss 8.959278056863695e-05
I0326 01:24:53.005093 1299728 finetune.py:45] layer 7_down initial loss 0.0001046152610797435
I0326 01:25:17.675467 1298892 finetune.py:68] layer 6_down @ epoch 0 new loss 8.957126556197181e-05 old loss 8.959278056863695e-05 BETTER
I0326 01:25:19.860126 1299728 finetune.py:68] layer 7_down @ epoch 0 new loss 0.00010459366603754461 old loss 0.0001046152610797435 BETTER
I0326 01:25:46.510251 1298892 finetune.py:68] layer 6_down @ epoch 1 new loss 8.955737575888634e-05 old loss 8.957126556197181e-05 BETTER
I0326 01:25:47.463150 1299728 finetune.py:68] layer 7_down @ epoch 1 new loss 0.00010457766620675102 old loss 0.00010459366603754461 BETTER
I0326 01:26:15.086363 1299728 finetune.py:68] layer 7_down @ epoch 2 new loss 0.00010456427116878331 old loss 0.00010457766620675102 BETTER
I0326 01:26:15.675676 1298892 finetune.py:68] layer 6_down @ epoch 2 new loss 8.954549412010238e-05 old loss 8.955737575888634e-05 BETTER
I0326 01:26:42.748202 1299728 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00010455233859829605 old loss 0.00010456427116878331 BETTER
I0326 01:26:44.967712 1298892 finetune.py:68] layer 6_down @ epoch 3 new loss 8.95358098205179e-05 old loss 8.954549412010238e-05 BETTER
I0326 01:27:10.491195 1299728 finetune.py:68] layer 7_down @ epoch 4 new loss 0.00010454258153913543 old loss 0.00010455233859829605 BETTER
7_v proxy err 0.026929883286356926 tr(WHW.T) 309.4270935058594
bpp_loss 2.6720385551452637
7_q proxy err 0.0011718852911144495 tr(WHW.T) 35165.53515625
bpp_loss 3.5189669132232666
7_k proxy err 0.0004413604910951108 tr(WHW.T) 26917.767578125
bpp_loss 4.466965436935425
7_o proxy err 0.037439245730638504 tr(WHW.T) 971.2352294921875
bpp_loss 2.8096357583999634
7_up proxy err 0.01785941608250141 tr(WHW.T) 8628.771484375
bpp_loss 2.9387521743774414
7_gate proxy err 0.0047258855774998665 tr(WHW.T) 34912.7265625
bpp_loss 3.2261581420898438
7_down proxy err 0.024213485419750214 tr(WHW.T) 6591.88623046875
bpp_loss 2.944946527481079
I0326 01:27:14.235053 1298892 finetune.py:68] layer 6_down @ epoch 4 new loss 8.9527246018406e-05 old loss 8.95358098205179e-05 BETTER
6_v proxy err 0.03219399228692055 tr(WHW.T) 253.63377380371094
bpp_loss 2.6738226413726807
6_q proxy err 0.0011250650277361274 tr(WHW.T) 35664.37109375
bpp_loss 3.590239405632019
6_k proxy err 0.0004358861187938601 tr(WHW.T) 26213.390625
bpp_loss 4.44566535949707
6_o proxy err 0.03672691434621811 tr(WHW.T) 1022.5650634765625
bpp_loss 2.7989741563796997
6_up proxy err 0.0196684617549181 tr(WHW.T) 7920.25830078125
bpp_loss 2.926279749189104
6_gate proxy err 0.0047076549381017685 tr(WHW.T) 35719.16015625
bpp_loss 3.253603662763323
6_down proxy err 0.02405906468629837 tr(WHW.T) 6540.03271484375
bpp_loss 2.930805512837001
I0326 01:28:22.768478 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 8 in 63.651798486709595s
I0326 01:28:25.906538 1315366 config.py:54] PyTorch version 2.6.0 available.
W0326 01:28:26.185847 1315366 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 01:28:27.073860 1315366 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 01:28:27.077779 1258494 quantize_finetune_llama.py:209] layer 9 gpu 1
I0326 01:28:27.090955 1315366 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 01:28:34.371341 1315366 finetune.py:45] layer 8_v initial loss 2.1916650439379737e-05
W0326 01:28:34.371605 1315366 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 01:29:10.351175 1315366 finetune.py:68] layer 8_v @ epoch 0 new loss 1.4583880329155363e-05 old loss 2.1916650439379737e-05 BETTER
I0326 01:29:26.921980 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 9 in 59.416852951049805s
I0326 01:29:30.323832 1316199 config.py:54] PyTorch version 2.6.0 available.
W0326 01:29:30.606917 1316199 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 01:29:31.561690 1316199 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 01:29:31.565491 1258494 quantize_finetune_llama.py:209] layer 10 gpu 0
I0326 01:29:31.578480 1316199 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 01:29:38.405101 1316199 finetune.py:45] layer 9_v initial loss 2.5025210561580025e-05
W0326 01:29:38.405371 1316199 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 01:29:47.287820 1315366 finetune.py:68] layer 8_v @ epoch 1 new loss 1.346777116850717e-05 old loss 1.4583880329155363e-05 BETTER
I0326 01:30:11.865970 1316199 finetune.py:68] layer 9_v @ epoch 0 new loss 1.5227506082737818e-05 old loss 2.5025210561580025e-05 BETTER
I0326 01:30:24.215295 1315366 finetune.py:68] layer 8_v @ epoch 2 new loss 1.2875518223154359e-05 old loss 1.346777116850717e-05 BETTER
I0326 01:30:46.329215 1316199 finetune.py:68] layer 9_v @ epoch 1 new loss 1.389633325743489e-05 old loss 1.5227506082737818e-05 BETTER
I0326 01:31:01.678092 1315366 finetune.py:68] layer 8_v @ epoch 3 new loss 1.2471069567254744e-05 old loss 1.2875518223154359e-05 BETTER
I0326 01:31:21.106734 1316199 finetune.py:68] layer 9_v @ epoch 2 new loss 1.321718536928529e-05 old loss 1.389633325743489e-05 BETTER
I0326 01:31:38.848858 1315366 finetune.py:68] layer 8_v @ epoch 4 new loss 1.2167311069788411e-05 old loss 1.2471069567254744e-05 BETTER
I0326 01:31:48.565153 1315366 finetune.py:45] layer 8_q initial loss 1.5507152056670748e-05
I0326 01:31:56.031097 1316199 finetune.py:68] layer 9_v @ epoch 3 new loss 1.2761064681399148e-05 old loss 1.321718536928529e-05 BETTER
I0326 01:32:24.513866 1315366 finetune.py:68] layer 8_q @ epoch 0 new loss 1.4812325389357284e-05 old loss 1.5507152056670748e-05 BETTER
I0326 01:32:31.075165 1316199 finetune.py:68] layer 9_v @ epoch 4 new loss 1.2436395081749652e-05 old loss 1.2761064681399148e-05 BETTER
I0326 01:32:40.442341 1316199 finetune.py:45] layer 9_q initial loss 1.6127161870826967e-05
I0326 01:33:01.220021 1315366 finetune.py:68] layer 8_q @ epoch 1 new loss 1.448081366106635e-05 old loss 1.4812325389357284e-05 BETTER
I0326 01:33:14.162215 1316199 finetune.py:68] layer 9_q @ epoch 0 new loss 1.5360148609033786e-05 old loss 1.6127161870826967e-05 BETTER
I0326 01:33:37.981744 1315366 finetune.py:68] layer 8_q @ epoch 2 new loss 1.4231135537556838e-05 old loss 1.448081366106635e-05 BETTER
I0326 01:33:48.683268 1316199 finetune.py:68] layer 9_q @ epoch 1 new loss 1.5012153198767919e-05 old loss 1.5360148609033786e-05 BETTER
I0326 01:34:14.850767 1315366 finetune.py:68] layer 8_q @ epoch 3 new loss 1.402141606376972e-05 old loss 1.4231135537556838e-05 BETTER
I0326 01:34:23.301371 1316199 finetune.py:68] layer 9_q @ epoch 2 new loss 1.4755228221474681e-05 old loss 1.5012153198767919e-05 BETTER
I0326 01:34:52.189363 1315366 finetune.py:68] layer 8_q @ epoch 4 new loss 1.3845392459188588e-05 old loss 1.402141606376972e-05 BETTER
I0326 01:34:58.176753 1316199 finetune.py:68] layer 9_q @ epoch 3 new loss 1.4543339602823835e-05 old loss 1.4755228221474681e-05 BETTER
I0326 01:35:00.323219 1315366 finetune.py:45] layer 8_k initial loss 1.4675504644401371e-05
I0326 01:35:32.790131 1316199 finetune.py:68] layer 9_q @ epoch 4 new loss 1.4360547538672108e-05 old loss 1.4543339602823835e-05 BETTER
I0326 01:35:36.518639 1315366 finetune.py:68] layer 8_k @ epoch 0 new loss 1.4427312635234557e-05 old loss 1.4675504644401371e-05 BETTER
I0326 01:35:40.595540 1316199 finetune.py:45] layer 9_k initial loss 1.530878580524586e-05
I0326 01:36:13.490747 1315366 finetune.py:68] layer 8_k @ epoch 1 new loss 1.4286411897046492e-05 old loss 1.4427312635234557e-05 BETTER
I0326 01:36:14.465960 1316199 finetune.py:68] layer 9_k @ epoch 0 new loss 1.5073625945660751e-05 old loss 1.530878580524586e-05 BETTER
I0326 01:36:48.932016 1316199 finetune.py:68] layer 9_k @ epoch 1 new loss 1.492137471359456e-05 old loss 1.5073625945660751e-05 BETTER
I0326 01:36:50.360762 1315366 finetune.py:68] layer 8_k @ epoch 2 new loss 1.416418581356993e-05 old loss 1.4286411897046492e-05 BETTER
I0326 01:37:23.407076 1316199 finetune.py:68] layer 9_k @ epoch 2 new loss 1.4790827663091477e-05 old loss 1.492137471359456e-05 BETTER
I0326 01:37:27.027449 1315366 finetune.py:68] layer 8_k @ epoch 3 new loss 1.4054027815291192e-05 old loss 1.416418581356993e-05 BETTER
I0326 01:37:58.128411 1316199 finetune.py:68] layer 9_k @ epoch 3 new loss 1.4684987036162056e-05 old loss 1.4790827663091477e-05 BETTER
I0326 01:38:03.572139 1315366 finetune.py:68] layer 8_k @ epoch 4 new loss 1.3958162526250817e-05 old loss 1.4054027815291192e-05 BETTER
I0326 01:38:13.335257 1315366 finetune.py:45] layer 8_o initial loss 5.221170431468636e-05
I0326 01:38:32.493509 1316199 finetune.py:68] layer 9_k @ epoch 4 new loss 1.4576780813513324e-05 old loss 1.4684987036162056e-05 BETTER
I0326 01:38:41.821741 1316199 finetune.py:45] layer 9_o initial loss 5.66877170058433e-05
I0326 01:38:48.436276 1315366 finetune.py:68] layer 8_o @ epoch 0 new loss 4.764904952025972e-05 old loss 5.221170431468636e-05 BETTER
I0326 01:39:14.727282 1316199 finetune.py:68] layer 9_o @ epoch 0 new loss 5.150546712684445e-05 old loss 5.66877170058433e-05 BETTER
I0326 01:39:24.090247 1315366 finetune.py:68] layer 8_o @ epoch 1 new loss 4.550413723336533e-05 old loss 4.764904952025972e-05 BETTER
I0326 01:39:48.324215 1316199 finetune.py:68] layer 9_o @ epoch 1 new loss 4.907879701931961e-05 old loss 5.150546712684445e-05 BETTER
I0326 01:40:00.240931 1315366 finetune.py:68] layer 8_o @ epoch 2 new loss 4.4006072130287066e-05 old loss 4.550413723336533e-05 BETTER
I0326 01:40:22.320339 1316199 finetune.py:68] layer 9_o @ epoch 2 new loss 4.7378140152432024e-05 old loss 4.907879701931961e-05 BETTER
I0326 01:40:36.251160 1315366 finetune.py:68] layer 8_o @ epoch 3 new loss 4.285158502170816e-05 old loss 4.4006072130287066e-05 BETTER
I0326 01:40:56.328313 1316199 finetune.py:68] layer 9_o @ epoch 3 new loss 4.608942617778666e-05 old loss 4.7378140152432024e-05 BETTER
I0326 01:41:12.200222 1315366 finetune.py:68] layer 8_o @ epoch 4 new loss 4.190875915810466e-05 old loss 4.285158502170816e-05 BETTER
I0326 01:41:30.425417 1316199 finetune.py:68] layer 9_o @ epoch 4 new loss 4.5037864765617996e-05 old loss 4.608942617778666e-05 BETTER
I0326 01:41:33.766425 1315366 finetune.py:45] layer 8_up initial loss 7.431029371218756e-05
I0326 01:41:51.748535 1316199 finetune.py:45] layer 9_up initial loss 8.070277544902638e-05
I0326 01:42:06.126556 1315366 finetune.py:68] layer 8_up @ epoch 0 new loss 7.236124656628817e-05 old loss 7.431029371218756e-05 BETTER
I0326 01:42:22.409744 1316199 finetune.py:68] layer 9_up @ epoch 0 new loss 7.840642501832917e-05 old loss 8.070277544902638e-05 BETTER
I0326 01:42:39.565177 1315366 finetune.py:68] layer 8_up @ epoch 1 new loss 7.109620491974056e-05 old loss 7.236124656628817e-05 BETTER
I0326 01:42:54.172203 1316199 finetune.py:68] layer 9_up @ epoch 1 new loss 7.695659587625414e-05 old loss 7.840642501832917e-05 BETTER
I0326 01:43:12.954712 1315366 finetune.py:68] layer 8_up @ epoch 2 new loss 7.004888175288215e-05 old loss 7.109620491974056e-05 BETTER
I0326 01:43:26.215938 1316199 finetune.py:68] layer 9_up @ epoch 2 new loss 7.57640809752047e-05 old loss 7.695659587625414e-05 BETTER
I0326 01:43:46.685359 1315366 finetune.py:68] layer 8_up @ epoch 3 new loss 6.914704135851935e-05 old loss 7.004888175288215e-05 BETTER
I0326 01:43:58.194459 1316199 finetune.py:68] layer 9_up @ epoch 3 new loss 7.47239901102148e-05 old loss 7.57640809752047e-05 BETTER
I0326 01:44:20.573804 1315366 finetune.py:68] layer 8_up @ epoch 4 new loss 6.832860526628792e-05 old loss 6.914704135851935e-05 BETTER
I0326 01:44:30.289776 1316199 finetune.py:68] layer 9_up @ epoch 4 new loss 7.381191244348884e-05 old loss 7.47239901102148e-05 BETTER
I0326 01:44:42.340942 1315366 finetune.py:45] layer 8_gate initial loss 8.047276787692681e-05
I0326 01:44:51.871922 1316199 finetune.py:45] layer 9_gate initial loss 8.696778968442231e-05
I0326 01:45:12.543543 1315366 finetune.py:68] layer 8_gate @ epoch 0 new loss 7.959442882565781e-05 old loss 8.047276787692681e-05 BETTER
I0326 01:45:20.528299 1316199 finetune.py:68] layer 9_gate @ epoch 0 new loss 8.596099360147491e-05 old loss 8.696778968442231e-05 BETTER
I0326 01:45:43.839248 1315366 finetune.py:68] layer 8_gate @ epoch 1 new loss 7.888718391768634e-05 old loss 7.959442882565781e-05 BETTER
I0326 01:45:49.968619 1316199 finetune.py:68] layer 9_gate @ epoch 1 new loss 8.517181413481012e-05 old loss 8.596099360147491e-05 BETTER
I0326 01:46:15.231243 1315366 finetune.py:68] layer 8_gate @ epoch 2 new loss 7.825404463801533e-05 old loss 7.888718391768634e-05 BETTER
I0326 01:46:19.696927 1316199 finetune.py:68] layer 9_gate @ epoch 2 new loss 8.446396532235667e-05 old loss 8.517181413481012e-05 BETTER
I0326 01:46:46.887802 1315366 finetune.py:68] layer 8_gate @ epoch 3 new loss 7.768012437736616e-05 old loss 7.825404463801533e-05 BETTER
I0326 01:46:49.452341 1316199 finetune.py:68] layer 9_gate @ epoch 3 new loss 8.381266525248066e-05 old loss 8.446396532235667e-05 BETTER
I0326 01:47:18.449626 1315366 finetune.py:68] layer 8_gate @ epoch 4 new loss 7.714294042671099e-05 old loss 7.768012437736616e-05 BETTER
I0326 01:47:19.179760 1316199 finetune.py:68] layer 9_gate @ epoch 4 new loss 8.32217774586752e-05 old loss 8.381266525248066e-05 BETTER
I0326 01:47:41.092582 1315366 finetune.py:45] layer 8_down initial loss 0.00011848919530166313
I0326 01:47:41.521491 1316199 finetune.py:45] layer 9_down initial loss 0.00012927564966958016
I0326 01:48:08.274258 1316199 finetune.py:68] layer 9_down @ epoch 0 new loss 0.00012924718612339348 old loss 0.00012927564966958016 BETTER
I0326 01:48:08.925938 1315366 finetune.py:68] layer 8_down @ epoch 0 new loss 0.00011846187408082187 old loss 0.00011848919530166313 BETTER
I0326 01:48:35.915360 1316199 finetune.py:68] layer 9_down @ epoch 1 new loss 0.00012922311725560576 old loss 0.00012924718612339348 BETTER
I0326 01:48:37.817421 1315366 finetune.py:68] layer 8_down @ epoch 1 new loss 0.00011843990796478465 old loss 0.00011846187408082187 BETTER
I0326 01:49:03.586113 1316199 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00012920402514282614 old loss 0.00012922311725560576 BETTER
I0326 01:49:06.798238 1315366 finetune.py:68] layer 8_down @ epoch 2 new loss 0.00011842227104352787 old loss 0.00011843990796478465 BETTER
I0326 01:49:31.280277 1316199 finetune.py:68] layer 9_down @ epoch 3 new loss 0.00012918573338538408 old loss 0.00012920402514282614 BETTER
I0326 01:49:35.808629 1315366 finetune.py:68] layer 8_down @ epoch 3 new loss 0.00011840731895063072 old loss 0.00011842227104352787 BETTER
I0326 01:49:58.882541 1316199 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00012917020649183542 old loss 0.00012918573338538408 BETTER
9_v proxy err 0.022929061204195023 tr(WHW.T) 351.4288024902344
bpp_loss 2.78976309299469
9_q proxy err 0.0014685873175039887 tr(WHW.T) 25665.158203125
bpp_loss 3.517716407775879
9_k proxy err 0.0005148737109266222 tr(WHW.T) 20978.525390625
bpp_loss 4.403251647949219
9_o proxy err 0.046694111078977585 tr(WHW.T) 786.8587036132812
bpp_loss 2.873822569847107
9_up proxy err 0.01717926748096943 tr(WHW.T) 8972.4140625
bpp_loss 2.94366523197719
9_gate proxy err 0.004197461996227503 tr(WHW.T) 39474.1796875
bpp_loss 3.2431231907435825
9_down proxy err 0.025054289028048515 tr(WHW.T) 6331.73583984375
bpp_loss 2.9446070875440324
I0326 01:50:04.978192 1315366 finetune.py:68] layer 8_down @ epoch 4 new loss 0.00011839155195048079 old loss 0.00011840731895063072 BETTER
8_v proxy err 0.030793126672506332 tr(WHW.T) 257.7052307128906
bpp_loss 2.6926302909851074
8_q proxy err 0.0014212469104677439 tr(WHW.T) 26613.21875
bpp_loss 3.5057283639907837
8_k proxy err 0.0004813117266166955 tr(WHW.T) 22553.19140625
bpp_loss 4.383333444595337
8_o proxy err 0.04917140305042267 tr(WHW.T) 755.37109375
bpp_loss 2.8200957775115967
8_up proxy err 0.018055615946650505 tr(WHW.T) 8498.5302734375
bpp_loss 2.9346847534179688
8_gate proxy err 0.0044081928208470345 tr(WHW.T) 37317.6171875
bpp_loss 3.2303338732038225
8_down proxy err 0.024727223441004753 tr(WHW.T) 6542.9052734375
bpp_loss 2.94330256325858
I0326 01:51:12.729676 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 10 in 62.79403042793274s
I0326 01:51:16.076084 1327063 config.py:54] PyTorch version 2.6.0 available.
W0326 01:51:16.355788 1327063 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 01:51:17.224377 1327063 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 01:51:17.228261 1258494 quantize_finetune_llama.py:209] layer 11 gpu 1
I0326 01:51:17.241218 1327063 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 01:51:24.545699 1327063 finetune.py:45] layer 10_v initial loss 2.8997887056902982e-05
W0326 01:51:24.545900 1327063 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 01:52:00.676993 1327063 finetune.py:68] layer 10_v @ epoch 0 new loss 2.0057061192346737e-05 old loss 2.8997887056902982e-05 BETTER
I0326 01:52:17.201049 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 11 in 59.57663583755493s
I0326 01:52:20.604084 1327175 config.py:54] PyTorch version 2.6.0 available.
W0326 01:52:20.883243 1327175 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 01:52:21.787563 1327175 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 01:52:21.791261 1258494 quantize_finetune_llama.py:209] layer 12 gpu 0
I0326 01:52:21.803781 1327175 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 01:52:28.607434 1327175 finetune.py:45] layer 11_v initial loss 2.5657116566435434e-05
W0326 01:52:28.607689 1327175 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 01:52:37.572112 1327063 finetune.py:68] layer 10_v @ epoch 1 new loss 1.8537728465162218e-05 old loss 2.0057061192346737e-05 BETTER
I0326 01:53:01.860939 1327175 finetune.py:68] layer 11_v @ epoch 0 new loss 1.7617012417758815e-05 old loss 2.5657116566435434e-05 BETTER
I0326 01:53:14.475457 1327063 finetune.py:68] layer 10_v @ epoch 2 new loss 1.772093128238339e-05 old loss 1.8537728465162218e-05 BETTER
I0326 01:53:36.353611 1327175 finetune.py:68] layer 11_v @ epoch 1 new loss 1.6322481315000914e-05 old loss 1.7617012417758815e-05 BETTER
I0326 01:53:51.297654 1327063 finetune.py:68] layer 10_v @ epoch 3 new loss 1.7155722161987796e-05 old loss 1.772093128238339e-05 BETTER
I0326 01:54:10.987128 1327175 finetune.py:68] layer 11_v @ epoch 2 new loss 1.5621091733919457e-05 old loss 1.6322481315000914e-05 BETTER
I0326 01:54:28.285010 1327063 finetune.py:68] layer 10_v @ epoch 4 new loss 1.6727455658838153e-05 old loss 1.7155722161987796e-05 BETTER
I0326 01:54:37.953548 1327063 finetune.py:45] layer 10_q initial loss 2.1193154680076987e-05
I0326 01:54:45.971603 1327175 finetune.py:68] layer 11_v @ epoch 3 new loss 1.5137893569772132e-05 old loss 1.5621091733919457e-05 BETTER
I0326 01:55:13.790351 1327063 finetune.py:68] layer 10_q @ epoch 0 new loss 2.0095330910407938e-05 old loss 2.1193154680076987e-05 BETTER
I0326 01:55:21.006709 1327175 finetune.py:68] layer 11_v @ epoch 4 new loss 1.4777038813917898e-05 old loss 1.5137893569772132e-05 BETTER
I0326 01:55:30.343953 1327175 finetune.py:45] layer 11_q initial loss 1.9721906937775202e-05
I0326 01:55:50.661485 1327063 finetune.py:68] layer 10_q @ epoch 1 new loss 1.9637505829450674e-05 old loss 2.0095330910407938e-05 BETTER
I0326 01:56:04.231687 1327175 finetune.py:68] layer 11_q @ epoch 0 new loss 1.875887392088771e-05 old loss 1.9721906937775202e-05 BETTER
I0326 01:56:27.344987 1327063 finetune.py:68] layer 10_q @ epoch 2 new loss 1.9278799300082028e-05 old loss 1.9637505829450674e-05 BETTER
I0326 01:56:38.861378 1327175 finetune.py:68] layer 11_q @ epoch 1 new loss 1.8359458408667706e-05 old loss 1.875887392088771e-05 BETTER
I0326 01:57:04.263279 1327063 finetune.py:68] layer 10_q @ epoch 3 new loss 1.899167000374291e-05 old loss 1.9278799300082028e-05 BETTER
I0326 01:57:13.371732 1327175 finetune.py:68] layer 11_q @ epoch 2 new loss 1.8041751900454983e-05 old loss 1.8359458408667706e-05 BETTER
I0326 01:57:41.128558 1327063 finetune.py:68] layer 10_q @ epoch 4 new loss 1.8750946765067056e-05 old loss 1.899167000374291e-05 BETTER
I0326 01:57:47.969509 1327175 finetune.py:68] layer 11_q @ epoch 3 new loss 1.7810256395023316e-05 old loss 1.8041751900454983e-05 BETTER
I0326 01:57:49.021948 1327063 finetune.py:45] layer 10_k initial loss 1.966458876268007e-05
I0326 01:58:22.752543 1327175 finetune.py:68] layer 11_q @ epoch 4 new loss 1.7600308638066053e-05 old loss 1.7810256395023316e-05 BETTER
I0326 01:58:24.901412 1327063 finetune.py:68] layer 10_k @ epoch 0 new loss 1.9388706277823076e-05 old loss 1.966458876268007e-05 BETTER
I0326 01:58:30.383066 1327175 finetune.py:45] layer 11_k initial loss 1.8640770576894283e-05
I0326 01:59:01.599192 1327063 finetune.py:68] layer 10_k @ epoch 1 new loss 1.9198218069504946e-05 old loss 1.9388706277823076e-05 BETTER
I0326 01:59:03.999708 1327175 finetune.py:68] layer 11_k @ epoch 0 new loss 1.8351618564338423e-05 old loss 1.8640770576894283e-05 BETTER
I0326 01:59:38.169452 1327063 finetune.py:68] layer 10_k @ epoch 2 new loss 1.903742850117851e-05 old loss 1.9198218069504946e-05 BETTER
I0326 01:59:38.334041 1327175 finetune.py:68] layer 11_k @ epoch 1 new loss 1.8139775420422666e-05 old loss 1.8351618564338423e-05 BETTER
I0326 02:00:12.781291 1327175 finetune.py:68] layer 11_k @ epoch 2 new loss 1.8005064703174867e-05 old loss 1.8139775420422666e-05 BETTER
I0326 02:00:14.771681 1327063 finetune.py:68] layer 10_k @ epoch 3 new loss 1.888512633740902e-05 old loss 1.903742850117851e-05 BETTER
I0326 02:00:47.248860 1327175 finetune.py:68] layer 11_k @ epoch 3 new loss 1.7848444258561358e-05 old loss 1.8005064703174867e-05 BETTER
I0326 02:00:51.411183 1327063 finetune.py:68] layer 10_k @ epoch 4 new loss 1.8755075871013105e-05 old loss 1.888512633740902e-05 BETTER
I0326 02:01:01.106333 1327063 finetune.py:45] layer 10_o initial loss 6.536708679050207e-05
I0326 02:01:21.793783 1327175 finetune.py:68] layer 11_k @ epoch 4 new loss 1.7729349565343e-05 old loss 1.7848444258561358e-05 BETTER
I0326 02:01:31.120761 1327175 finetune.py:45] layer 11_o initial loss 7.255269883899018e-05
I0326 02:01:36.451867 1327063 finetune.py:68] layer 10_o @ epoch 0 new loss 5.943903306615539e-05 old loss 6.536708679050207e-05 BETTER
I0326 02:02:04.249884 1327175 finetune.py:68] layer 11_o @ epoch 0 new loss 6.500709423562512e-05 old loss 7.255269883899018e-05 BETTER
I0326 02:02:12.482022 1327063 finetune.py:68] layer 10_o @ epoch 1 new loss 5.656015855493024e-05 old loss 5.943903306615539e-05 BETTER
I0326 02:02:38.039381 1327175 finetune.py:68] layer 11_o @ epoch 1 new loss 6.145903171272948e-05 old loss 6.500709423562512e-05 BETTER
I0326 02:02:48.368873 1327063 finetune.py:68] layer 10_o @ epoch 2 new loss 5.458869782160036e-05 old loss 5.656015855493024e-05 BETTER
I0326 02:03:11.783692 1327175 finetune.py:68] layer 11_o @ epoch 2 new loss 5.907362719881348e-05 old loss 6.145903171272948e-05 BETTER
I0326 02:03:24.206513 1327063 finetune.py:68] layer 10_o @ epoch 3 new loss 5.307398896547966e-05 old loss 5.458869782160036e-05 BETTER
I0326 02:03:45.503964 1327175 finetune.py:68] layer 11_o @ epoch 3 new loss 5.7283508795080706e-05 old loss 5.907362719881348e-05 BETTER
I0326 02:04:00.242402 1327063 finetune.py:68] layer 10_o @ epoch 4 new loss 5.18452470714692e-05 old loss 5.307398896547966e-05 BETTER
I0326 02:04:19.183890 1327175 finetune.py:68] layer 11_o @ epoch 4 new loss 5.58322062715888e-05 old loss 5.7283508795080706e-05 BETTER
I0326 02:04:21.852673 1327063 finetune.py:45] layer 10_up initial loss 8.84365726960823e-05
I0326 02:04:40.504072 1327175 finetune.py:45] layer 11_up initial loss 9.489400690654293e-05
I0326 02:04:54.060766 1327063 finetune.py:68] layer 10_up @ epoch 0 new loss 8.595504914410412e-05 old loss 8.84365726960823e-05 BETTER
I0326 02:05:11.160589 1327175 finetune.py:68] layer 11_up @ epoch 0 new loss 9.204744128510356e-05 old loss 9.489400690654293e-05 BETTER
I0326 02:05:27.487617 1327063 finetune.py:68] layer 10_up @ epoch 1 new loss 8.437003998551518e-05 old loss 8.595504914410412e-05 BETTER
I0326 02:05:43.070714 1327175 finetune.py:68] layer 11_up @ epoch 1 new loss 9.023822349263355e-05 old loss 9.204744128510356e-05 BETTER
I0326 02:06:01.132325 1327063 finetune.py:68] layer 10_up @ epoch 2 new loss 8.30868084449321e-05 old loss 8.437003998551518e-05 BETTER
I0326 02:06:15.147756 1327175 finetune.py:68] layer 11_up @ epoch 2 new loss 8.876383799361065e-05 old loss 9.023822349263355e-05 BETTER
I0326 02:06:34.992450 1327063 finetune.py:68] layer 10_up @ epoch 3 new loss 8.195861300919205e-05 old loss 8.30868084449321e-05 BETTER
I0326 02:06:47.244623 1327175 finetune.py:68] layer 11_up @ epoch 3 new loss 8.747427636990324e-05 old loss 8.876383799361065e-05 BETTER
I0326 02:07:08.802022 1327063 finetune.py:68] layer 10_up @ epoch 4 new loss 8.096399687929079e-05 old loss 8.195861300919205e-05 BETTER
I0326 02:07:19.352254 1327175 finetune.py:68] layer 11_up @ epoch 4 new loss 8.63384993863292e-05 old loss 8.747427636990324e-05 BETTER
I0326 02:07:30.662459 1327063 finetune.py:45] layer 10_gate initial loss 9.538222366245463e-05
I0326 02:07:40.771866 1327175 finetune.py:45] layer 11_gate initial loss 0.00010186423605773598
I0326 02:08:01.122091 1327063 finetune.py:68] layer 10_gate @ epoch 0 new loss 9.428567136637866e-05 old loss 9.538222366245463e-05 BETTER
I0326 02:08:09.582404 1327175 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.00010062754154205322 old loss 0.00010186423605773598 BETTER
I0326 02:08:32.385077 1327063 finetune.py:68] layer 10_gate @ epoch 1 new loss 9.342575503978878e-05 old loss 9.428567136637866e-05 BETTER
I0326 02:08:39.076652 1327175 finetune.py:68] layer 11_gate @ epoch 1 new loss 9.96584931272082e-05 old loss 0.00010062754154205322 BETTER
I0326 02:09:03.726292 1327063 finetune.py:68] layer 10_gate @ epoch 2 new loss 9.265844710171223e-05 old loss 9.342575503978878e-05 BETTER
I0326 02:09:08.676904 1327175 finetune.py:68] layer 11_gate @ epoch 2 new loss 9.881432197289541e-05 old loss 9.96584931272082e-05 BETTER
I0326 02:09:35.119220 1327063 finetune.py:68] layer 10_gate @ epoch 3 new loss 9.196797327604145e-05 old loss 9.265844710171223e-05 BETTER
I0326 02:09:38.408858 1327175 finetune.py:68] layer 11_gate @ epoch 3 new loss 9.803449211176485e-05 old loss 9.881432197289541e-05 BETTER
I0326 02:10:06.620290 1327063 finetune.py:68] layer 10_gate @ epoch 4 new loss 9.132169361691922e-05 old loss 9.196797327604145e-05 BETTER
I0326 02:10:08.206240 1327175 finetune.py:68] layer 11_gate @ epoch 4 new loss 9.732621401781216e-05 old loss 9.803449211176485e-05 BETTER
I0326 02:10:29.491686 1327063 finetune.py:45] layer 10_down initial loss 0.00013859198952559382
I0326 02:10:30.427640 1327175 finetune.py:45] layer 11_down initial loss 0.00014672870747745037
I0326 02:10:57.421972 1327175 finetune.py:68] layer 11_down @ epoch 0 new loss 0.00014669535448774695 old loss 0.00014672870747745037 BETTER
I0326 02:10:57.601565 1327063 finetune.py:68] layer 10_down @ epoch 0 new loss 0.00013855646830052137 old loss 0.00013859198952559382 BETTER
I0326 02:11:25.018484 1327175 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0001466658868594095 old loss 0.00014669535448774695 BETTER
I0326 02:11:26.786679 1327063 finetune.py:68] layer 10_down @ epoch 1 new loss 0.00013852994015906006 old loss 0.00013855646830052137 BETTER
I0326 02:11:52.731457 1327175 finetune.py:68] layer 11_down @ epoch 2 new loss 0.00014663778711110353 old loss 0.0001466658868594095 BETTER
I0326 02:11:56.081668 1327063 finetune.py:68] layer 10_down @ epoch 2 new loss 0.00013850540562998503 old loss 0.00013852994015906006 BETTER
I0326 02:12:20.437186 1327175 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0001466153480578214 old loss 0.00014663778711110353 BETTER
I0326 02:12:25.516731 1327063 finetune.py:68] layer 10_down @ epoch 3 new loss 0.00013848402886651456 old loss 0.00013850540562998503 BETTER
I0326 02:12:48.047284 1327175 finetune.py:68] layer 11_down @ epoch 4 new loss 0.00014659340376965702 old loss 0.0001466153480578214 BETTER
11_v proxy err 0.024582654237747192 tr(WHW.T) 319.5691223144531
bpp_loss 2.68950617313385
11_q proxy err 0.0016719565028324723 tr(WHW.T) 22205.85546875
bpp_loss 3.4612538814544678
11_k proxy err 0.0005945854936726391 tr(WHW.T) 18039.21484375
bpp_loss 4.402628421783447
11_o proxy err 0.059280458837747574 tr(WHW.T) 574.9369506835938
bpp_loss 2.8317453861236572
11_up proxy err 0.0166657492518425 tr(WHW.T) 9206.4033203125
bpp_loss 2.9645998818533763
11_gate proxy err 0.0043859477154910564 tr(WHW.T) 37018.76953125
bpp_loss 3.1943088259015764
11_down proxy err 0.023341244086623192 tr(WHW.T) 6721.96875
bpp_loss 2.9678946222577776
I0326 02:12:54.918296 1327063 finetune.py:68] layer 10_down @ epoch 4 new loss 0.00013846607180312276 old loss 0.00013848402886651456 BETTER
10_v proxy err 0.03004014678299427 tr(WHW.T) 251.83889770507812
bpp_loss 2.682924509048462
10_q proxy err 0.0015573455020785332 tr(WHW.T) 23347.04296875
bpp_loss 3.5154407024383545
10_k proxy err 0.0005226594512350857 tr(WHW.T) 19775.0546875
bpp_loss 4.401828289031982
10_o proxy err 0.05199577286839485 tr(WHW.T) 690.8175048828125
bpp_loss 2.812895894050598
10_up proxy err 0.016938352957367897 tr(WHW.T) 9205.7685546875
bpp_loss 2.9604159763881137
10_gate proxy err 0.004425015300512314 tr(WHW.T) 37493.41796875
bpp_loss 3.214888027736119
10_down proxy err 0.02419321797788143 tr(WHW.T) 6593.4365234375
bpp_loss 2.961066518511091
I0326 02:14:02.981718 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 12 in 63.23876643180847s
I0326 02:14:06.224304 1328535 config.py:54] PyTorch version 2.6.0 available.
W0326 02:14:06.505113 1328535 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 02:14:07.386140 1328535 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 02:14:07.390049 1258494 quantize_finetune_llama.py:209] layer 13 gpu 1
I0326 02:14:07.403690 1328535 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 02:14:14.726037 1328535 finetune.py:45] layer 12_v initial loss 2.944380321423523e-05
W0326 02:14:14.726309 1328535 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 02:14:50.941145 1328535 finetune.py:68] layer 12_v @ epoch 0 new loss 1.9844968846882693e-05 old loss 2.944380321423523e-05 BETTER
I0326 02:15:07.552095 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 13 in 59.782668352127075s
I0326 02:15:11.068969 1328656 config.py:54] PyTorch version 2.6.0 available.
W0326 02:15:11.348050 1328656 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 02:15:12.246167 1328656 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 02:15:12.249946 1258494 quantize_finetune_llama.py:209] layer 14 gpu 0
I0326 02:15:12.262680 1328656 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 02:15:19.116314 1328656 finetune.py:45] layer 13_v initial loss 3.171640128130093e-05
W0326 02:15:19.116528 1328656 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 02:15:28.007506 1328535 finetune.py:68] layer 12_v @ epoch 1 new loss 1.828352833399549e-05 old loss 1.9844968846882693e-05 BETTER
I0326 02:15:52.579599 1328656 finetune.py:68] layer 13_v @ epoch 0 new loss 2.1740135707659647e-05 old loss 3.171640128130093e-05 BETTER
I0326 02:16:05.316741 1328535 finetune.py:68] layer 12_v @ epoch 2 new loss 1.7385667888447642e-05 old loss 1.828352833399549e-05 BETTER
I0326 02:16:27.163001 1328656 finetune.py:68] layer 13_v @ epoch 1 new loss 2.002074324991554e-05 old loss 2.1740135707659647e-05 BETTER
I0326 02:16:42.270379 1328535 finetune.py:68] layer 12_v @ epoch 3 new loss 1.6748410416767e-05 old loss 1.7385667888447642e-05 BETTER
I0326 02:17:02.021118 1328656 finetune.py:68] layer 13_v @ epoch 2 new loss 1.9090499336016364e-05 old loss 2.002074324991554e-05 BETTER
I0326 02:17:19.294869 1328535 finetune.py:68] layer 12_v @ epoch 4 new loss 1.627368328627199e-05 old loss 1.6748410416767e-05 BETTER
I0326 02:17:29.087345 1328535 finetune.py:45] layer 12_q initial loss 2.0193370801280253e-05
I0326 02:17:37.032304 1328656 finetune.py:68] layer 13_v @ epoch 3 new loss 1.846529085014481e-05 old loss 1.9090499336016364e-05 BETTER
I0326 02:18:05.293073 1328535 finetune.py:68] layer 12_q @ epoch 0 new loss 1.9277798855910078e-05 old loss 2.0193370801280253e-05 BETTER
I0326 02:18:12.181600 1328656 finetune.py:68] layer 13_v @ epoch 4 new loss 1.80087245098548e-05 old loss 1.846529085014481e-05 BETTER
I0326 02:18:21.566771 1328656 finetune.py:45] layer 13_q initial loss 2.273093741678167e-05
I0326 02:18:42.106139 1328535 finetune.py:68] layer 12_q @ epoch 1 new loss 1.8804645151249133e-05 old loss 1.9277798855910078e-05 BETTER
I0326 02:18:55.387351 1328656 finetune.py:68] layer 13_q @ epoch 0 new loss 2.175917870772537e-05 old loss 2.273093741678167e-05 BETTER
I0326 02:19:19.020342 1328535 finetune.py:68] layer 12_q @ epoch 2 new loss 1.8425722373649478e-05 old loss 1.8804645151249133e-05 BETTER
I0326 02:19:29.953690 1328656 finetune.py:68] layer 13_q @ epoch 1 new loss 2.132133158738725e-05 old loss 2.175917870772537e-05 BETTER
I0326 02:19:55.800425 1328535 finetune.py:68] layer 12_q @ epoch 3 new loss 1.8116439605364576e-05 old loss 1.8425722373649478e-05 BETTER
I0326 02:20:04.438507 1328656 finetune.py:68] layer 13_q @ epoch 2 new loss 2.098567165376153e-05 old loss 2.132133158738725e-05 BETTER
I0326 02:20:32.732659 1328535 finetune.py:68] layer 12_q @ epoch 4 new loss 1.783578954928089e-05 old loss 1.8116439605364576e-05 BETTER
I0326 02:20:39.060545 1328656 finetune.py:68] layer 13_q @ epoch 3 new loss 2.070237314910628e-05 old loss 2.098567165376153e-05 BETTER
I0326 02:20:40.552984 1328535 finetune.py:45] layer 12_k initial loss 1.932063423737418e-05
I0326 02:21:14.078464 1328656 finetune.py:68] layer 13_q @ epoch 4 new loss 2.0457011487451382e-05 old loss 2.070237314910628e-05 BETTER
I0326 02:21:16.626788 1328535 finetune.py:68] layer 12_k @ epoch 0 new loss 1.8872349755838513e-05 old loss 1.932063423737418e-05 BETTER
I0326 02:21:21.795577 1328656 finetune.py:45] layer 13_k initial loss 2.1551335521508008e-05
I0326 02:21:53.273512 1328535 finetune.py:68] layer 12_k @ epoch 1 new loss 1.862794852058869e-05 old loss 1.8872349755838513e-05 BETTER
I0326 02:21:55.576565 1328656 finetune.py:68] layer 13_k @ epoch 0 new loss 2.1263827875372954e-05 old loss 2.1551335521508008e-05 BETTER
I0326 02:22:30.064011 1328656 finetune.py:68] layer 13_k @ epoch 1 new loss 2.1059577193227597e-05 old loss 2.1263827875372954e-05 BETTER
I0326 02:22:30.107928 1328535 finetune.py:68] layer 12_k @ epoch 2 new loss 1.8438104234519415e-05 old loss 1.862794852058869e-05 BETTER
I0326 02:23:04.418915 1328656 finetune.py:68] layer 13_k @ epoch 2 new loss 2.08992187253898e-05 old loss 2.1059577193227597e-05 BETTER
I0326 02:23:06.762584 1328535 finetune.py:68] layer 12_k @ epoch 3 new loss 1.8257111150887795e-05 old loss 1.8438104234519415e-05 BETTER
I0326 02:23:38.781112 1328656 finetune.py:68] layer 13_k @ epoch 3 new loss 2.0747438611579128e-05 old loss 2.08992187253898e-05 BETTER
I0326 02:23:43.412274 1328535 finetune.py:68] layer 12_k @ epoch 4 new loss 1.8112094039679505e-05 old loss 1.8257111150887795e-05 BETTER
I0326 02:23:53.415546 1328535 finetune.py:45] layer 12_o initial loss 6.987633969401941e-05
I0326 02:24:13.384765 1328656 finetune.py:68] layer 13_k @ epoch 4 new loss 2.0624143871827982e-05 old loss 2.0747438611579128e-05 BETTER
I0326 02:24:22.652507 1328656 finetune.py:45] layer 13_o initial loss 9.378419781569391e-05
I0326 02:24:28.374700 1328535 finetune.py:68] layer 12_o @ epoch 0 new loss 6.330567703116685e-05 old loss 6.987633969401941e-05 BETTER
I0326 02:24:55.606340 1328656 finetune.py:68] layer 13_o @ epoch 0 new loss 8.342898945556954e-05 old loss 9.378419781569391e-05 BETTER
I0326 02:25:04.133059 1328535 finetune.py:68] layer 12_o @ epoch 1 new loss 6.0209280491108075e-05 old loss 6.330567703116685e-05 BETTER
I0326 02:25:29.205753 1328656 finetune.py:68] layer 13_o @ epoch 1 new loss 7.850726251490414e-05 old loss 8.342898945556954e-05 BETTER
I0326 02:25:39.899469 1328535 finetune.py:68] layer 12_o @ epoch 2 new loss 5.801439328934066e-05 old loss 6.0209280491108075e-05 BETTER
I0326 02:26:02.917985 1328656 finetune.py:68] layer 13_o @ epoch 2 new loss 7.51136030885391e-05 old loss 7.850726251490414e-05 BETTER
I0326 02:26:15.826582 1328535 finetune.py:68] layer 12_o @ epoch 3 new loss 5.6306569604203105e-05 old loss 5.801439328934066e-05 BETTER
I0326 02:26:36.733359 1328656 finetune.py:68] layer 13_o @ epoch 3 new loss 7.254283991642296e-05 old loss 7.51136030885391e-05 BETTER
I0326 02:26:51.811763 1328535 finetune.py:68] layer 12_o @ epoch 4 new loss 5.490664625540376e-05 old loss 5.6306569604203105e-05 BETTER
I0326 02:27:10.612540 1328656 finetune.py:68] layer 13_o @ epoch 4 new loss 7.048615225357935e-05 old loss 7.254283991642296e-05 BETTER
I0326 02:27:13.484407 1328535 finetune.py:45] layer 12_up initial loss 9.55959185375832e-05
I0326 02:27:31.903973 1328656 finetune.py:45] layer 13_up initial loss 0.00011799079948104918
I0326 02:27:45.732371 1328535 finetune.py:68] layer 12_up @ epoch 0 new loss 9.240800136467442e-05 old loss 9.55959185375832e-05 BETTER
I0326 02:28:02.570858 1328656 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00011391165753593668 old loss 0.00011799079948104918 BETTER
I0326 02:28:19.291144 1328535 finetune.py:68] layer 12_up @ epoch 1 new loss 9.04388798517175e-05 old loss 9.240800136467442e-05 BETTER
I0326 02:28:34.356491 1328656 finetune.py:68] layer 13_up @ epoch 1 new loss 0.00011141835420858115 old loss 0.00011391165753593668 BETTER
I0326 02:28:52.923295 1328535 finetune.py:68] layer 12_up @ epoch 2 new loss 8.883448754204437e-05 old loss 9.04388798517175e-05 BETTER
I0326 02:29:06.224932 1328656 finetune.py:68] layer 13_up @ epoch 2 new loss 0.00010937676415778697 old loss 0.00011141835420858115 BETTER
I0326 02:29:26.491332 1328535 finetune.py:68] layer 12_up @ epoch 3 new loss 8.745698141865432e-05 old loss 8.883448754204437e-05 BETTER
I0326 02:29:38.315081 1328656 finetune.py:68] layer 13_up @ epoch 3 new loss 0.00010762346937553957 old loss 0.00010937676415778697 BETTER
I0326 02:30:00.188584 1328535 finetune.py:68] layer 12_up @ epoch 4 new loss 8.623195753898472e-05 old loss 8.745698141865432e-05 BETTER
I0326 02:30:10.382634 1328656 finetune.py:68] layer 13_up @ epoch 4 new loss 0.00010608373122522607 old loss 0.00010762346937553957 BETTER
I0326 02:30:21.795050 1328535 finetune.py:45] layer 12_gate initial loss 0.00010379876039223745
I0326 02:30:32.044683 1328656 finetune.py:45] layer 13_gate initial loss 0.00012593623250722885
I0326 02:30:51.979626 1328535 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.0001024101220536977 old loss 0.00010379876039223745 BETTER
I0326 02:31:00.483454 1328656 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.00012421135033946484 old loss 0.00012593623250722885 BETTER
I0326 02:31:23.067030 1328535 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.00010132388706551865 old loss 0.0001024101220536977 BETTER
I0326 02:31:29.739349 1328656 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.00012286579294595867 old loss 0.00012421135033946484 BETTER
I0326 02:31:54.277681 1328535 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.00010037645552074537 old loss 0.00010132388706551865 BETTER
I0326 02:31:59.359271 1328656 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.00012168323883088306 old loss 0.00012286579294595867 BETTER
I0326 02:32:25.796628 1328535 finetune.py:68] layer 12_gate @ epoch 3 new loss 9.951085667125881e-05 old loss 0.00010037645552074537 BETTER
I0326 02:32:29.164450 1328656 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.00012060724111506715 old loss 0.00012168323883088306 BETTER
I0326 02:32:57.283342 1328535 finetune.py:68] layer 12_gate @ epoch 4 new loss 9.872427472146228e-05 old loss 9.951085667125881e-05 BETTER
I0326 02:32:58.939004 1328656 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.00011963608267251402 old loss 0.00012060724111506715 BETTER
I0326 02:33:19.904617 1328535 finetune.py:45] layer 12_down initial loss 0.00015170506958384067
I0326 02:33:21.292807 1328656 finetune.py:45] layer 13_down initial loss 0.00018313023610971868
I0326 02:33:47.969416 1328535 finetune.py:68] layer 12_down @ epoch 0 new loss 0.00015167001402005553 old loss 0.00015170506958384067 BETTER
I0326 02:33:48.179471 1328656 finetune.py:68] layer 13_down @ epoch 0 new loss 0.00018308463040739298 old loss 0.00018313023610971868 BETTER
I0326 02:34:15.770680 1328656 finetune.py:68] layer 13_down @ epoch 1 new loss 0.00018304557306692004 old loss 0.00018308463040739298 BETTER
I0326 02:34:17.147437 1328535 finetune.py:68] layer 12_down @ epoch 1 new loss 0.00015164271462708712 old loss 0.00015167001402005553 BETTER
I0326 02:34:43.501551 1328656 finetune.py:68] layer 13_down @ epoch 2 new loss 0.0001830121036618948 old loss 0.00018304557306692004 BETTER
I0326 02:34:46.402229 1328535 finetune.py:68] layer 12_down @ epoch 2 new loss 0.00015161826740950346 old loss 0.00015164271462708712 BETTER
I0326 02:35:11.194044 1328656 finetune.py:68] layer 13_down @ epoch 3 new loss 0.00018298282520845532 old loss 0.0001830121036618948 BETTER
I0326 02:35:15.693478 1328535 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0001515938638476655 old loss 0.00015161826740950346 BETTER
I0326 02:35:38.883910 1328656 finetune.py:68] layer 13_down @ epoch 4 new loss 0.00018295610789209604 old loss 0.00018298282520845532 BETTER
13_v proxy err 0.02812921442091465 tr(WHW.T) 280.153564453125
bpp_loss 2.746765375137329
13_q proxy err 0.0017703664489090443 tr(WHW.T) 20904.228515625
bpp_loss 3.4917027950286865
13_k proxy err 0.0006005314644426107 tr(WHW.T) 17820.357421875
bpp_loss 4.415666580200195
13_o proxy err 0.052507128566503525 tr(WHW.T) 683.4244995117188
bpp_loss 2.859803795814514
13_up proxy err 0.015034698881208897 tr(WHW.T) 10026.2109375
bpp_loss 2.9862237657819475
13_gate proxy err 0.004065902903676033 tr(WHW.T) 39059.1328125
bpp_loss 3.17966856275286
13_down proxy err 0.02324315719306469 tr(WHW.T) 6626.79931640625
bpp_loss 2.9792719227927074
I0326 02:35:44.991226 1328535 finetune.py:68] layer 12_down @ epoch 4 new loss 0.000151574844494462 old loss 0.0001515938638476655 BETTER
12_v proxy err 0.023128394037485123 tr(WHW.T) 363.6233825683594
bpp_loss 2.804708957672119
12_q proxy err 0.0011762856738641858 tr(WHW.T) 34128.84375
bpp_loss 3.518293023109436
12_k proxy err 0.0004949661670252681 tr(WHW.T) 23103.806640625
bpp_loss 4.402306795120239
12_o proxy err 0.046779949218034744 tr(WHW.T) 788.8425903320312
bpp_loss 2.8764692544937134
12_up proxy err 0.01498906034976244 tr(WHW.T) 10032.0771484375
bpp_loss 2.9826059341430664
12_gate proxy err 0.004229698330163956 tr(WHW.T) 37385.61328125
bpp_loss 3.1751629965645924
12_down proxy err 0.02246231399476528 tr(WHW.T) 6884.68994140625
bpp_loss 2.9792873859405518
I0326 02:36:53.093916 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 14 in 63.25790071487427s
I0326 02:36:56.254506 1330049 config.py:54] PyTorch version 2.6.0 available.
W0326 02:36:56.534972 1330049 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 02:36:57.409608 1330049 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 02:36:57.413629 1258494 quantize_finetune_llama.py:209] layer 15 gpu 1
I0326 02:36:57.427003 1330049 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 02:37:04.745870 1330049 finetune.py:45] layer 14_v initial loss 3.234134055674076e-05
W0326 02:37:04.746130 1330049 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 02:37:41.000665 1330049 finetune.py:68] layer 14_v @ epoch 0 new loss 2.3418955606757663e-05 old loss 3.234134055674076e-05 BETTER
I0326 02:37:57.731616 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 15 in 59.9143602848053s
I0326 02:38:01.158344 1330167 config.py:54] PyTorch version 2.6.0 available.
W0326 02:38:01.439518 1330167 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 02:38:02.359533 1330167 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 02:38:02.363337 1258494 quantize_finetune_llama.py:209] layer 16 gpu 0
I0326 02:38:02.376478 1330167 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 02:38:09.169833 1330167 finetune.py:45] layer 15_v initial loss 3.601687785703689e-05
W0326 02:38:09.170041 1330167 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 02:38:18.317280 1330049 finetune.py:68] layer 14_v @ epoch 1 new loss 2.1735757400165312e-05 old loss 2.3418955606757663e-05 BETTER
I0326 02:38:42.559003 1330167 finetune.py:68] layer 15_v @ epoch 0 new loss 2.508363286324311e-05 old loss 3.601687785703689e-05 BETTER
I0326 02:38:55.368170 1330049 finetune.py:68] layer 14_v @ epoch 2 new loss 2.07824105018517e-05 old loss 2.1735757400165312e-05 BETTER
I0326 02:39:16.971096 1330167 finetune.py:68] layer 15_v @ epoch 1 new loss 2.33043720072601e-05 old loss 2.508363286324311e-05 BETTER
I0326 02:39:32.355036 1330049 finetune.py:68] layer 14_v @ epoch 3 new loss 2.01141901925439e-05 old loss 2.07824105018517e-05 BETTER
I0326 02:39:51.623748 1330167 finetune.py:68] layer 15_v @ epoch 2 new loss 2.229705933132209e-05 old loss 2.33043720072601e-05 BETTER
I0326 02:40:09.576768 1330049 finetune.py:68] layer 14_v @ epoch 4 new loss 1.9605136913014576e-05 old loss 2.01141901925439e-05 BETTER
I0326 02:40:19.300953 1330049 finetune.py:45] layer 14_q initial loss 2.591661541373469e-05
I0326 02:40:26.477290 1330167 finetune.py:68] layer 15_v @ epoch 3 new loss 2.159979885618668e-05 old loss 2.229705933132209e-05 BETTER
I0326 02:40:55.222377 1330049 finetune.py:68] layer 14_q @ epoch 0 new loss 2.4650082195876166e-05 old loss 2.591661541373469e-05 BETTER
I0326 02:41:01.528274 1330167 finetune.py:68] layer 15_v @ epoch 4 new loss 2.107618456648197e-05 old loss 2.159979885618668e-05 BETTER
I0326 02:41:10.847869 1330167 finetune.py:45] layer 15_q initial loss 2.4871016648830846e-05
I0326 02:41:32.141822 1330049 finetune.py:68] layer 14_q @ epoch 1 new loss 2.4075385226751678e-05 old loss 2.4650082195876166e-05 BETTER
I0326 02:41:44.663159 1330167 finetune.py:68] layer 15_q @ epoch 0 new loss 2.3986431187950075e-05 old loss 2.4871016648830846e-05 BETTER
I0326 02:42:08.958718 1330049 finetune.py:68] layer 14_q @ epoch 2 new loss 2.3654409233131446e-05 old loss 2.4075385226751678e-05 BETTER
I0326 02:42:19.219004 1330167 finetune.py:68] layer 15_q @ epoch 1 new loss 2.3508166123065166e-05 old loss 2.3986431187950075e-05 BETTER
I0326 02:42:45.767576 1330049 finetune.py:68] layer 14_q @ epoch 3 new loss 2.33052960538771e-05 old loss 2.3654409233131446e-05 BETTER
I0326 02:42:53.874988 1330167 finetune.py:68] layer 15_q @ epoch 2 new loss 2.3135695300879888e-05 old loss 2.3508166123065166e-05 BETTER
I0326 02:43:22.597109 1330049 finetune.py:68] layer 14_q @ epoch 4 new loss 2.301411768712569e-05 old loss 2.33052960538771e-05 BETTER
I0326 02:43:28.569660 1330167 finetune.py:68] layer 15_q @ epoch 3 new loss 2.283158573845867e-05 old loss 2.3135695300879888e-05 BETTER
I0326 02:43:30.620876 1330049 finetune.py:45] layer 14_k initial loss 2.4915678295656107e-05
I0326 02:44:03.187992 1330167 finetune.py:68] layer 15_q @ epoch 4 new loss 2.2580274162464775e-05 old loss 2.283158573845867e-05 BETTER
I0326 02:44:06.822181 1330049 finetune.py:68] layer 14_k @ epoch 0 new loss 2.4470120479236357e-05 old loss 2.4915678295656107e-05 BETTER
I0326 02:44:10.974705 1330167 finetune.py:45] layer 15_k initial loss 2.4053919332800433e-05
I0326 02:44:43.600465 1330049 finetune.py:68] layer 14_k @ epoch 1 new loss 2.4219139959313907e-05 old loss 2.4470120479236357e-05 BETTER
I0326 02:44:44.772315 1330167 finetune.py:68] layer 15_k @ epoch 0 new loss 2.3683223844273016e-05 old loss 2.4053919332800433e-05 BETTER
I0326 02:45:19.147881 1330167 finetune.py:68] layer 15_k @ epoch 1 new loss 2.3466991478926502e-05 old loss 2.3683223844273016e-05 BETTER
I0326 02:45:20.423274 1330049 finetune.py:68] layer 14_k @ epoch 2 new loss 2.4007418687688187e-05 old loss 2.4219139959313907e-05 BETTER
I0326 02:45:53.715116 1330167 finetune.py:68] layer 15_k @ epoch 2 new loss 2.3279879314941354e-05 old loss 2.3466991478926502e-05 BETTER
I0326 02:45:57.293680 1330049 finetune.py:68] layer 14_k @ epoch 3 new loss 2.3829557903809473e-05 old loss 2.4007418687688187e-05 BETTER
I0326 02:46:28.282921 1330167 finetune.py:68] layer 15_k @ epoch 3 new loss 2.3126558517105877e-05 old loss 2.3279879314941354e-05 BETTER
I0326 02:46:34.140629 1330049 finetune.py:68] layer 14_k @ epoch 4 new loss 2.3672835595789365e-05 old loss 2.3829557903809473e-05 BETTER
I0326 02:46:43.787978 1330049 finetune.py:45] layer 14_o initial loss 9.900051372824237e-05
I0326 02:47:02.667970 1330167 finetune.py:68] layer 15_k @ epoch 4 new loss 2.2985756004345603e-05 old loss 2.3126558517105877e-05 BETTER
I0326 02:47:11.849755 1330167 finetune.py:45] layer 15_o initial loss 8.57538398122415e-05
I0326 02:47:18.869306 1330049 finetune.py:68] layer 14_o @ epoch 0 new loss 8.891699690138921e-05 old loss 9.900051372824237e-05 BETTER
I0326 02:47:45.009000 1330167 finetune.py:68] layer 15_o @ epoch 0 new loss 7.74723885115236e-05 old loss 8.57538398122415e-05 BETTER
I0326 02:47:54.852393 1330049 finetune.py:68] layer 14_o @ epoch 1 new loss 8.406844426644966e-05 old loss 8.891699690138921e-05 BETTER
I0326 02:48:18.996594 1330167 finetune.py:68] layer 15_o @ epoch 1 new loss 7.360918971244246e-05 old loss 7.74723885115236e-05 BETTER
I0326 02:48:30.642674 1330049 finetune.py:68] layer 14_o @ epoch 2 new loss 8.068084571277723e-05 old loss 8.406844426644966e-05 BETTER
I0326 02:48:53.071959 1330167 finetune.py:68] layer 15_o @ epoch 2 new loss 7.088367419783026e-05 old loss 7.360918971244246e-05 BETTER
I0326 02:49:06.564027 1330049 finetune.py:68] layer 14_o @ epoch 3 new loss 7.809105591150001e-05 old loss 8.068084571277723e-05 BETTER
I0326 02:49:27.168623 1330167 finetune.py:68] layer 15_o @ epoch 3 new loss 6.880287401145324e-05 old loss 7.088367419783026e-05 BETTER
I0326 02:49:42.333521 1330049 finetune.py:68] layer 14_o @ epoch 4 new loss 7.601706602144986e-05 old loss 7.809105591150001e-05 BETTER
I0326 02:50:01.172768 1330167 finetune.py:68] layer 15_o @ epoch 4 new loss 6.710279558319598e-05 old loss 6.880287401145324e-05 BETTER
I0326 02:50:04.056340 1330049 finetune.py:45] layer 14_up initial loss 0.00013324656174518168
I0326 02:50:22.403225 1330167 finetune.py:45] layer 15_up initial loss 0.00013597318320535123
I0326 02:50:36.405733 1330049 finetune.py:68] layer 14_up @ epoch 0 new loss 0.00012831695494242013 old loss 0.00013324656174518168 BETTER
I0326 02:50:53.080662 1330167 finetune.py:68] layer 15_up @ epoch 0 new loss 0.00013017808669246733 old loss 0.00013597318320535123 BETTER
I0326 02:51:09.525050 1330049 finetune.py:68] layer 14_up @ epoch 1 new loss 0.00012537065776996315 old loss 0.00012831695494242013 BETTER
I0326 02:51:24.666402 1330167 finetune.py:68] layer 15_up @ epoch 1 new loss 0.0001268635387532413 old loss 0.00013017808669246733 BETTER
I0326 02:51:42.909853 1330049 finetune.py:68] layer 14_up @ epoch 2 new loss 0.00012299316585995257 old loss 0.00012537065776996315 BETTER
I0326 02:51:56.482139 1330167 finetune.py:68] layer 15_up @ epoch 2 new loss 0.00012422843428794295 old loss 0.0001268635387532413 BETTER
I0326 02:52:16.367105 1330049 finetune.py:68] layer 14_up @ epoch 3 new loss 0.00012097277794964612 old loss 0.00012299316585995257 BETTER
I0326 02:52:28.458427 1330167 finetune.py:68] layer 15_up @ epoch 3 new loss 0.00012199987395433709 old loss 0.00012422843428794295 BETTER
I0326 02:52:49.956144 1330049 finetune.py:68] layer 14_up @ epoch 4 new loss 0.00011919222015421838 old loss 0.00012097277794964612 BETTER
I0326 02:53:00.535750 1330167 finetune.py:68] layer 15_up @ epoch 4 new loss 0.00012003933807136491 old loss 0.00012199987395433709 BETTER
I0326 02:53:11.670541 1330049 finetune.py:45] layer 14_gate initial loss 0.00014053798804525286
I0326 02:53:22.077080 1330167 finetune.py:45] layer 15_gate initial loss 0.00014380466018337756
I0326 02:53:41.974487 1330049 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.00013855814177077264 old loss 0.00014053798804525286 BETTER
I0326 02:53:50.902795 1330167 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.00014159534475766122 old loss 0.00014380466018337756 BETTER
I0326 02:54:13.265516 1330049 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.00013701397983822972 old loss 0.00013855814177077264 BETTER
I0326 02:54:20.274837 1330167 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.00013982216478325427 old loss 0.00014159534475766122 BETTER
I0326 02:54:44.646506 1330049 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.0001356641441816464 old loss 0.00013701397983822972 BETTER
I0326 02:54:49.784827 1330167 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.00013827468501403928 old loss 0.00013982216478325427 BETTER
I0326 02:55:16.095998 1330049 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.00013443856732919812 old loss 0.0001356641441816464 BETTER
I0326 02:55:19.414210 1330167 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0001368861412629485 old loss 0.00013827468501403928 BETTER
I0326 02:55:47.576163 1330049 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.00013332351227290928 old loss 0.00013443856732919812 BETTER
I0326 02:55:49.082007 1330167 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.00013563183892983943 old loss 0.0001368861412629485 BETTER
I0326 02:56:10.243957 1330049 finetune.py:45] layer 14_down initial loss 0.00020762615895364434
I0326 02:56:11.416913 1330167 finetune.py:45] layer 15_down initial loss 0.00022992944286670536
I0326 02:56:38.090211 1330167 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0002298759645782411 old loss 0.00022992944286670536 BETTER
I0326 02:56:38.098689 1330049 finetune.py:68] layer 14_down @ epoch 0 new loss 0.00020757057063747197 old loss 0.00020762615895364434 BETTER
I0326 02:57:05.710324 1330167 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00022983283270150423 old loss 0.0002298759645782411 BETTER
I0326 02:57:07.036508 1330049 finetune.py:68] layer 14_down @ epoch 1 new loss 0.00020752291311509907 old loss 0.00020757057063747197 BETTER
I0326 02:57:33.433345 1330167 finetune.py:68] layer 15_down @ epoch 2 new loss 0.00022979319328442216 old loss 0.00022983283270150423 BETTER
I0326 02:57:36.109274 1330049 finetune.py:68] layer 14_down @ epoch 2 new loss 0.0002074799849651754 old loss 0.00020752291311509907 BETTER
I0326 02:58:01.069535 1330167 finetune.py:68] layer 15_down @ epoch 3 new loss 0.00022975953470449895 old loss 0.00022979319328442216 BETTER
I0326 02:58:05.436877 1330049 finetune.py:68] layer 14_down @ epoch 3 new loss 0.000207439879886806 old loss 0.0002074799849651754 BETTER
I0326 02:58:28.797794 1330167 finetune.py:68] layer 15_down @ epoch 4 new loss 0.00022973002342041582 old loss 0.00022975953470449895 BETTER
15_v proxy err 0.02873329445719719 tr(WHW.T) 284.0271301269531
bpp_loss 2.800526976585388
15_q proxy err 0.0013734992826357484 tr(WHW.T) 28094.58984375
bpp_loss 3.587757706642151
15_k proxy err 0.0005795155302621424 tr(WHW.T) 18903.2734375
bpp_loss 4.411444187164307
15_o proxy err 0.04718218371272087 tr(WHW.T) 834.0298461914062
bpp_loss 2.8803213834762573
15_up proxy err 0.01674531027674675 tr(WHW.T) 9001.705078125
bpp_loss 2.974156379699707
15_gate proxy err 0.0035101051907986403 tr(WHW.T) 46077.453125
bpp_loss 3.2442309515816823
15_down proxy err 0.02402540296316147 tr(WHW.T) 6471.36181640625
bpp_loss 2.969730888094221
I0326 02:58:34.816342 1330049 finetune.py:68] layer 14_down @ epoch 4 new loss 0.00020740510080941021 old loss 0.000207439879886806 BETTER
14_v proxy err 0.027054717764258385 tr(WHW.T) 281.3382873535156
bpp_loss 2.7389358282089233
14_q proxy err 0.0017208014614880085 tr(WHW.T) 20896.78515625
bpp_loss 3.462535262107849
14_k proxy err 0.0005542908329516649 tr(WHW.T) 18649.552734375
bpp_loss 4.365667104721069
14_o proxy err 0.05413845553994179 tr(WHW.T) 696.212646484375
bpp_loss 2.851096749305725
14_up proxy err 0.016323016956448555 tr(WHW.T) 9173.220703125
bpp_loss 2.9809597560337613
14_gate proxy err 0.003801406593993306 tr(WHW.T) 41867.45703125
bpp_loss 3.2082131249564037
14_down proxy err 0.02377677895128727 tr(WHW.T) 6454.353515625
bpp_loss 2.975114039012364
I0326 02:59:43.365963 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 16 in 63.68353295326233s
I0326 02:59:46.594656 1331542 config.py:54] PyTorch version 2.6.0 available.
W0326 02:59:46.872751 1331542 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 02:59:47.743687 1331542 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 02:59:47.747621 1258494 quantize_finetune_llama.py:209] layer 17 gpu 1
I0326 02:59:47.760570 1331542 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 02:59:55.045314 1331542 finetune.py:45] layer 16_v initial loss 3.9798331272322685e-05
W0326 02:59:55.045522 1331542 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 03:00:31.318885 1331542 finetune.py:68] layer 16_v @ epoch 0 new loss 2.7778227376984432e-05 old loss 3.9798331272322685e-05 BETTER
I0326 03:00:47.028506 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 17 in 58.86404037475586s
I0326 03:00:50.333073 1331657 config.py:54] PyTorch version 2.6.0 available.
W0326 03:00:50.615367 1331657 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 03:00:51.493542 1331657 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 03:00:51.497454 1258494 quantize_finetune_llama.py:209] layer 18 gpu 0
I0326 03:00:51.510834 1331657 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 03:00:58.338911 1331657 finetune.py:45] layer 17_v initial loss 3.6676057789009064e-05
W0326 03:00:58.339234 1331657 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 03:01:08.257249 1331542 finetune.py:68] layer 16_v @ epoch 1 new loss 2.5595851184334606e-05 old loss 2.7778227376984432e-05 BETTER
I0326 03:01:31.817868 1331657 finetune.py:68] layer 17_v @ epoch 0 new loss 2.4457782274112105e-05 old loss 3.6676057789009064e-05 BETTER
I0326 03:01:45.240582 1331542 finetune.py:68] layer 16_v @ epoch 2 new loss 2.4318500436493196e-05 old loss 2.5595851184334606e-05 BETTER
I0326 03:02:06.352620 1331657 finetune.py:68] layer 17_v @ epoch 1 new loss 2.2594349502469413e-05 old loss 2.4457782274112105e-05 BETTER
I0326 03:02:22.220422 1331542 finetune.py:68] layer 16_v @ epoch 3 new loss 2.3423697712132707e-05 old loss 2.4318500436493196e-05 BETTER
I0326 03:02:41.257278 1331657 finetune.py:68] layer 17_v @ epoch 2 new loss 2.153334025933873e-05 old loss 2.2594349502469413e-05 BETTER
I0326 03:02:59.297359 1331542 finetune.py:68] layer 16_v @ epoch 4 new loss 2.2748292394680902e-05 old loss 2.3423697712132707e-05 BETTER
I0326 03:03:08.941681 1331542 finetune.py:45] layer 16_q initial loss 2.7664482331601903e-05
I0326 03:03:16.176627 1331657 finetune.py:68] layer 17_v @ epoch 3 new loss 2.083454637613613e-05 old loss 2.153334025933873e-05 BETTER
I0326 03:03:45.181612 1331542 finetune.py:68] layer 16_q @ epoch 0 new loss 2.618254984554369e-05 old loss 2.7664482331601903e-05 BETTER
I0326 03:03:51.207141 1331657 finetune.py:68] layer 17_v @ epoch 4 new loss 2.0307203158154152e-05 old loss 2.083454637613613e-05 BETTER
I0326 03:04:00.571381 1331657 finetune.py:45] layer 17_q initial loss 2.4200318875955418e-05
I0326 03:04:22.090594 1331542 finetune.py:68] layer 16_q @ epoch 1 new loss 2.5513696527923457e-05 old loss 2.618254984554369e-05 BETTER
I0326 03:04:34.349261 1331657 finetune.py:68] layer 17_q @ epoch 0 new loss 2.3299839085666463e-05 old loss 2.4200318875955418e-05 BETTER
I0326 03:04:58.986099 1331542 finetune.py:68] layer 16_q @ epoch 2 new loss 2.5010014724102803e-05 old loss 2.5513696527923457e-05 BETTER
I0326 03:05:08.874132 1331657 finetune.py:68] layer 17_q @ epoch 1 new loss 2.2785992769058794e-05 old loss 2.3299839085666463e-05 BETTER
I0326 03:05:35.847364 1331542 finetune.py:68] layer 16_q @ epoch 3 new loss 2.4575199859100394e-05 old loss 2.5010014724102803e-05 BETTER
I0326 03:05:43.296761 1331657 finetune.py:68] layer 17_q @ epoch 2 new loss 2.239860077679623e-05 old loss 2.2785992769058794e-05 BETTER
I0326 03:06:12.748224 1331542 finetune.py:68] layer 16_q @ epoch 4 new loss 2.4227076210081577e-05 old loss 2.4575199859100394e-05 BETTER
I0326 03:06:17.914592 1331657 finetune.py:68] layer 17_q @ epoch 3 new loss 2.2059921320760623e-05 old loss 2.239860077679623e-05 BETTER
I0326 03:06:20.829629 1331542 finetune.py:45] layer 16_k initial loss 2.573708479758352e-05
I0326 03:06:53.241038 1331657 finetune.py:68] layer 17_q @ epoch 4 new loss 2.1790627215523273e-05 old loss 2.2059921320760623e-05 BETTER
I0326 03:06:57.543946 1331542 finetune.py:68] layer 16_k @ epoch 0 new loss 2.522940303606447e-05 old loss 2.573708479758352e-05 BETTER
I0326 03:07:01.006815 1331657 finetune.py:45] layer 17_k initial loss 2.3153274014475755e-05
I0326 03:07:34.509219 1331542 finetune.py:68] layer 16_k @ epoch 1 new loss 2.4942912205005996e-05 old loss 2.522940303606447e-05 BETTER
I0326 03:07:34.776144 1331657 finetune.py:68] layer 17_k @ epoch 0 new loss 2.2752657969249412e-05 old loss 2.3153274014475755e-05 BETTER
I0326 03:08:09.179189 1331657 finetune.py:68] layer 17_k @ epoch 1 new loss 2.2521655409946106e-05 old loss 2.2752657969249412e-05 BETTER
I0326 03:08:11.507294 1331542 finetune.py:68] layer 16_k @ epoch 2 new loss 2.4702216251171194e-05 old loss 2.4942912205005996e-05 BETTER
I0326 03:08:43.713139 1331657 finetune.py:68] layer 17_k @ epoch 2 new loss 2.2334381355904043e-05 old loss 2.2521655409946106e-05 BETTER
I0326 03:08:48.414697 1331542 finetune.py:68] layer 16_k @ epoch 3 new loss 2.4501421648892574e-05 old loss 2.4702216251171194e-05 BETTER
I0326 03:09:18.279472 1331657 finetune.py:68] layer 17_k @ epoch 3 new loss 2.21728696487844e-05 old loss 2.2334381355904043e-05 BETTER
I0326 03:09:25.348112 1331542 finetune.py:68] layer 16_k @ epoch 4 new loss 2.431023858662229e-05 old loss 2.4501421648892574e-05 BETTER
I0326 03:09:35.011189 1331542 finetune.py:45] layer 16_o initial loss 8.607577910879627e-05
I0326 03:09:52.676139 1331657 finetune.py:68] layer 17_k @ epoch 4 new loss 2.2037838789401576e-05 old loss 2.21728696487844e-05 BETTER
I0326 03:10:01.987262 1331657 finetune.py:45] layer 17_o initial loss 7.18944938853383e-05
I0326 03:10:10.456186 1331542 finetune.py:68] layer 16_o @ epoch 0 new loss 7.780332816764712e-05 old loss 8.607577910879627e-05 BETTER
I0326 03:10:34.996964 1331657 finetune.py:68] layer 17_o @ epoch 0 new loss 6.544149800902233e-05 old loss 7.18944938853383e-05 BETTER
I0326 03:10:46.594268 1331542 finetune.py:68] layer 16_o @ epoch 1 new loss 7.39894894650206e-05 old loss 7.780332816764712e-05 BETTER
I0326 03:11:09.072104 1331657 finetune.py:68] layer 17_o @ epoch 1 new loss 6.244211544981226e-05 old loss 6.544149800902233e-05 BETTER
I0326 03:11:23.179209 1331542 finetune.py:68] layer 16_o @ epoch 2 new loss 7.138271030271426e-05 old loss 7.39894894650206e-05 BETTER
I0326 03:11:43.153384 1331657 finetune.py:68] layer 17_o @ epoch 2 new loss 6.0428774304455146e-05 old loss 6.244211544981226e-05 BETTER
I0326 03:11:59.332834 1331542 finetune.py:68] layer 16_o @ epoch 3 new loss 6.935879355296493e-05 old loss 7.138271030271426e-05 BETTER
I0326 03:12:17.041884 1331657 finetune.py:68] layer 17_o @ epoch 3 new loss 5.889810563530773e-05 old loss 6.0428774304455146e-05 BETTER
I0326 03:12:35.576943 1331542 finetune.py:68] layer 16_o @ epoch 4 new loss 6.773777568014339e-05 old loss 6.935879355296493e-05 BETTER
I0326 03:12:50.776381 1331657 finetune.py:68] layer 17_o @ epoch 4 new loss 5.76550119149033e-05 old loss 5.889810563530773e-05 BETTER
I0326 03:12:57.174086 1331542 finetune.py:45] layer 16_up initial loss 0.00014072128396946937
I0326 03:13:12.205823 1331657 finetune.py:45] layer 17_up initial loss 0.0001416182640241459
I0326 03:13:29.817569 1331542 finetune.py:68] layer 16_up @ epoch 0 new loss 0.0001351558166788891 old loss 0.00014072128396946937 BETTER
I0326 03:13:43.089423 1331657 finetune.py:68] layer 17_up @ epoch 0 new loss 0.00013536443293560296 old loss 0.0001416182640241459 BETTER
I0326 03:14:03.410363 1331542 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0001319939474342391 old loss 0.0001351558166788891 BETTER
I0326 03:14:15.108368 1331657 finetune.py:68] layer 17_up @ epoch 1 new loss 0.00013192668848205358 old loss 0.00013536443293560296 BETTER
I0326 03:14:37.027397 1331542 finetune.py:68] layer 16_up @ epoch 2 new loss 0.0001294561370741576 old loss 0.0001319939474342391 BETTER
I0326 03:14:47.126203 1331657 finetune.py:68] layer 17_up @ epoch 2 new loss 0.00012926575436722487 old loss 0.00013192668848205358 BETTER
I0326 03:15:10.833372 1331542 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0001273050147574395 old loss 0.0001294561370741576 BETTER
I0326 03:15:19.184146 1331657 finetune.py:68] layer 17_up @ epoch 3 new loss 0.00012703145330306143 old loss 0.00012926575436722487 BETTER
I0326 03:15:44.590645 1331542 finetune.py:68] layer 16_up @ epoch 4 new loss 0.00012542145850602537 old loss 0.0001273050147574395 BETTER
I0326 03:15:51.283119 1331657 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0001251120847882703 old loss 0.00012703145330306143 BETTER
I0326 03:16:06.423025 1331542 finetune.py:45] layer 16_gate initial loss 0.00015032902592793107
I0326 03:16:12.902147 1331657 finetune.py:45] layer 17_gate initial loss 0.00015427955077029765
I0326 03:16:36.841509 1331542 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.00014823120727669448 old loss 0.00015032902592793107 BETTER
I0326 03:16:41.558870 1331657 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.00015197777247522026 old loss 0.00015427955077029765 BETTER
I0326 03:17:08.035157 1331542 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.00014656186976935714 old loss 0.00014823120727669448 BETTER
I0326 03:17:10.911267 1331657 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0001502058730693534 old loss 0.00015197777247522026 BETTER
I0326 03:17:39.484344 1331542 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.00014508677122648805 old loss 0.00014656186976935714 BETTER
I0326 03:17:40.310821 1331657 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.00014865795674268156 old loss 0.0001502058730693534 BETTER
I0326 03:18:09.996817 1331657 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.0001472547883167863 old loss 0.00014865795674268156 BETTER
I0326 03:18:10.936864 1331542 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0001437654864275828 old loss 0.00014508677122648805 BETTER
I0326 03:18:39.860342 1331657 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.00014601662405766547 old loss 0.0001472547883167863 BETTER
I0326 03:18:42.397770 1331542 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.00014257121074479073 old loss 0.0001437654864275828 BETTER
I0326 03:19:02.490237 1331657 finetune.py:45] layer 17_down initial loss 0.0002666826476342976
I0326 03:19:05.108516 1331542 finetune.py:45] layer 16_down initial loss 0.00024248013505712152
I0326 03:19:29.393354 1331657 finetune.py:68] layer 17_down @ epoch 0 new loss 0.00026662013260647655 old loss 0.0002666826476342976 BETTER
I0326 03:19:33.130636 1331542 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0002424308768240735 old loss 0.00024248013505712152 BETTER
I0326 03:19:57.057806 1331657 finetune.py:68] layer 17_down @ epoch 1 new loss 0.00026656955014914274 old loss 0.00026662013260647655 BETTER
I0326 03:20:02.243921 1331542 finetune.py:68] layer 16_down @ epoch 1 new loss 0.00024239211052190512 old loss 0.0002424308768240735 BETTER
I0326 03:20:24.646623 1331657 finetune.py:68] layer 17_down @ epoch 2 new loss 0.00026652554515749216 old loss 0.00026656955014914274 BETTER
I0326 03:20:31.422792 1331542 finetune.py:68] layer 16_down @ epoch 2 new loss 0.00024235706951003522 old loss 0.00024239211052190512 BETTER
I0326 03:20:52.259327 1331657 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0002664870989974588 old loss 0.00026652554515749216 BETTER
I0326 03:21:00.679546 1331542 finetune.py:68] layer 16_down @ epoch 3 new loss 0.00024232815485447645 old loss 0.00024235706951003522 BETTER
I0326 03:21:19.937775 1331657 finetune.py:68] layer 17_down @ epoch 4 new loss 0.00026645520119927824 old loss 0.0002664870989974588 BETTER
17_v proxy err 0.029003437608480453 tr(WHW.T) 283.9730224609375
bpp_loss 2.8272005319595337
17_q proxy err 0.001402609283104539 tr(WHW.T) 27582.369140625
bpp_loss 3.580423593521118
17_k proxy err 0.000633443531114608 tr(WHW.T) 17452.791015625
bpp_loss 4.4304304122924805
17_o proxy err 0.03601250797510147 tr(WHW.T) 1112.5902099609375
bpp_loss 2.8897757530212402
17_up proxy err 0.018465939909219742 tr(WHW.T) 8455.3310546875
bpp_loss 2.9601474489484514
17_gate proxy err 0.004071922041475773 tr(WHW.T) 41628.93359375
bpp_loss 3.2895515986851285
17_down proxy err 0.02447536028921604 tr(WHW.T) 6276.0673828125
bpp_loss 2.9521712916237965
I0326 03:21:29.923946 1331542 finetune.py:68] layer 16_down @ epoch 4 new loss 0.00024230207782238722 old loss 0.00024232815485447645 BETTER
16_v proxy err 0.027525760233402252 tr(WHW.T) 274.28167724609375
bpp_loss 2.76007878780365
16_q proxy err 0.001468996750190854 tr(WHW.T) 24491.1171875
bpp_loss 3.56585431098938
16_k proxy err 0.0005257379962131381 tr(WHW.T) 19520.01171875
bpp_loss 4.4032979011535645
16_o proxy err 0.039377495646476746 tr(WHW.T) 976.8883666992188
bpp_loss 2.8622243404388428
16_up proxy err 0.01853206939995289 tr(WHW.T) 8333.2392578125
bpp_loss 2.962196486336844
16_gate proxy err 0.004059604834765196 tr(WHW.T) 41104.80859375
bpp_loss 3.276402609688895
16_down proxy err 0.02405206859111786 tr(WHW.T) 6351.60888671875
bpp_loss 2.956111805779593
I0326 03:22:38.017661 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 18 in 63.20360064506531s
I0326 03:22:41.288037 1335910 config.py:54] PyTorch version 2.6.0 available.
W0326 03:22:41.566294 1335910 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 03:22:42.443807 1335910 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 03:22:42.447762 1258494 quantize_finetune_llama.py:209] layer 19 gpu 1
I0326 03:22:42.461086 1335910 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 03:22:49.874379 1335910 finetune.py:45] layer 18_v initial loss 3.282484976807609e-05
W0326 03:22:49.874565 1335910 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 03:23:26.071911 1335910 finetune.py:68] layer 18_v @ epoch 0 new loss 1.963132672244683e-05 old loss 3.282484976807609e-05 BETTER
I0326 03:23:42.751645 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 19 in 59.8927948474884s
I0326 03:23:46.151116 1336562 config.py:54] PyTorch version 2.6.0 available.
W0326 03:23:46.452201 1336562 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 03:23:47.374979 1336562 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 03:23:47.379205 1258494 quantize_finetune_llama.py:209] layer 20 gpu 0
I0326 03:23:47.393682 1336562 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 03:23:54.213216 1336562 finetune.py:45] layer 19_v initial loss 3.7770318158436567e-05
W0326 03:23:54.213490 1336562 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 03:24:02.998016 1335910 finetune.py:68] layer 18_v @ epoch 1 new loss 1.822538979467936e-05 old loss 1.963132672244683e-05 BETTER
I0326 03:24:27.706109 1336562 finetune.py:68] layer 19_v @ epoch 0 new loss 2.0578156181727536e-05 old loss 3.7770318158436567e-05 BETTER
I0326 03:24:39.976642 1335910 finetune.py:68] layer 18_v @ epoch 2 new loss 1.7428155842935666e-05 old loss 1.822538979467936e-05 BETTER
I0326 03:25:02.174539 1336562 finetune.py:68] layer 19_v @ epoch 1 new loss 1.8941038433695212e-05 old loss 2.0578156181727536e-05 BETTER
I0326 03:25:17.297786 1335910 finetune.py:68] layer 18_v @ epoch 3 new loss 1.6886160665308125e-05 old loss 1.7428155842935666e-05 BETTER
I0326 03:25:36.999999 1336562 finetune.py:68] layer 19_v @ epoch 2 new loss 1.811857509892434e-05 old loss 1.8941038433695212e-05 BETTER
I0326 03:25:54.786034 1335910 finetune.py:68] layer 18_v @ epoch 4 new loss 1.6482674254802987e-05 old loss 1.6886160665308125e-05 BETTER
I0326 03:26:04.542847 1335910 finetune.py:45] layer 18_q initial loss 2.1030598873039708e-05
I0326 03:26:11.939406 1336562 finetune.py:68] layer 19_v @ epoch 3 new loss 1.756865094648674e-05 old loss 1.811857509892434e-05 BETTER
I0326 03:26:40.314571 1335910 finetune.py:68] layer 18_q @ epoch 0 new loss 1.957817039510701e-05 old loss 2.1030598873039708e-05 BETTER
I0326 03:26:46.927295 1336562 finetune.py:68] layer 19_v @ epoch 4 new loss 1.717542727419641e-05 old loss 1.756865094648674e-05 BETTER
I0326 03:26:56.380924 1336562 finetune.py:45] layer 19_q initial loss 2.07393186428817e-05
I0326 03:27:16.956832 1335910 finetune.py:68] layer 18_q @ epoch 1 new loss 1.9137634808430448e-05 old loss 1.957817039510701e-05 BETTER
I0326 03:27:30.073220 1336562 finetune.py:68] layer 19_q @ epoch 0 new loss 1.9720908312592655e-05 old loss 2.07393186428817e-05 BETTER
I0326 03:27:53.516056 1335910 finetune.py:68] layer 18_q @ epoch 2 new loss 1.881235402834136e-05 old loss 1.9137634808430448e-05 BETTER
I0326 03:28:04.524346 1336562 finetune.py:68] layer 19_q @ epoch 1 new loss 1.9315719328005798e-05 old loss 1.9720908312592655e-05 BETTER
I0326 03:28:30.437300 1335910 finetune.py:68] layer 18_q @ epoch 3 new loss 1.8553260815679096e-05 old loss 1.881235402834136e-05 BETTER
I0326 03:28:39.118546 1336562 finetune.py:68] layer 19_q @ epoch 2 new loss 1.8995058781001717e-05 old loss 1.9315719328005798e-05 BETTER
I0326 03:29:07.326578 1335910 finetune.py:68] layer 18_q @ epoch 4 new loss 1.833356509450823e-05 old loss 1.8553260815679096e-05 BETTER
I0326 03:29:13.718225 1336562 finetune.py:68] layer 19_q @ epoch 3 new loss 1.8770064343698323e-05 old loss 1.8995058781001717e-05 BETTER
I0326 03:29:15.494334 1335910 finetune.py:45] layer 18_k initial loss 1.943326242326293e-05
I0326 03:29:48.388033 1336562 finetune.py:68] layer 19_q @ epoch 4 new loss 1.853502544690855e-05 old loss 1.8770064343698323e-05 BETTER
I0326 03:29:51.607890 1335910 finetune.py:68] layer 18_k @ epoch 0 new loss 1.911890103656333e-05 old loss 1.943326242326293e-05 BETTER
I0326 03:29:56.104678 1336562 finetune.py:45] layer 19_k initial loss 1.9846780560328625e-05
I0326 03:30:28.300599 1335910 finetune.py:68] layer 18_k @ epoch 1 new loss 1.8935395928565413e-05 old loss 1.911890103656333e-05 BETTER
I0326 03:30:29.970188 1336562 finetune.py:68] layer 19_k @ epoch 0 new loss 1.9459555915091187e-05 old loss 1.9846780560328625e-05 BETTER
I0326 03:31:04.358884 1336562 finetune.py:68] layer 19_k @ epoch 1 new loss 1.9292139768367633e-05 old loss 1.9459555915091187e-05 BETTER
I0326 03:31:04.835922 1335910 finetune.py:68] layer 18_k @ epoch 2 new loss 1.8783222913043573e-05 old loss 1.8935395928565413e-05 BETTER
I0326 03:31:38.874980 1336562 finetune.py:68] layer 19_k @ epoch 2 new loss 1.9158023860654794e-05 old loss 1.9292139768367633e-05 BETTER
I0326 03:31:41.222089 1335910 finetune.py:68] layer 18_k @ epoch 3 new loss 1.865305057435762e-05 old loss 1.8783222913043573e-05 BETTER
I0326 03:32:13.333019 1336562 finetune.py:68] layer 19_k @ epoch 3 new loss 1.9027544112759642e-05 old loss 1.9158023860654794e-05 BETTER
I0326 03:32:17.884268 1335910 finetune.py:68] layer 18_k @ epoch 4 new loss 1.85310000233585e-05 old loss 1.865305057435762e-05 BETTER
I0326 03:32:27.699413 1335910 finetune.py:45] layer 18_o initial loss 5.3805943025508896e-05
I0326 03:32:47.955535 1336562 finetune.py:68] layer 19_k @ epoch 4 new loss 1.892536056402605e-05 old loss 1.9027544112759642e-05 BETTER
I0326 03:32:57.295084 1336562 finetune.py:45] layer 19_o initial loss 5.1129340135958046e-05
I0326 03:33:02.966652 1335910 finetune.py:68] layer 18_o @ epoch 0 new loss 4.963590617990121e-05 old loss 5.3805943025508896e-05 BETTER
I0326 03:33:30.514299 1336562 finetune.py:68] layer 19_o @ epoch 0 new loss 4.705617902800441e-05 old loss 5.1129340135958046e-05 BETTER
I0326 03:33:39.127978 1335910 finetune.py:68] layer 18_o @ epoch 1 new loss 4.795327186002396e-05 old loss 4.963590617990121e-05 BETTER
I0326 03:34:04.411338 1336562 finetune.py:68] layer 19_o @ epoch 1 new loss 4.553834514808841e-05 old loss 4.705617902800441e-05 BETTER
I0326 03:34:15.166813 1335910 finetune.py:68] layer 18_o @ epoch 2 new loss 4.6814056986477226e-05 old loss 4.795327186002396e-05 BETTER
I0326 03:34:38.302030 1336562 finetune.py:68] layer 19_o @ epoch 2 new loss 4.4531778257805854e-05 old loss 4.553834514808841e-05 BETTER
I0326 03:34:51.329764 1335910 finetune.py:68] layer 18_o @ epoch 3 new loss 4.5942317228764296e-05 old loss 4.6814056986477226e-05 BETTER
I0326 03:35:12.253900 1336562 finetune.py:68] layer 19_o @ epoch 3 new loss 4.375532080302946e-05 old loss 4.4531778257805854e-05 BETTER
I0326 03:35:27.305822 1335910 finetune.py:68] layer 18_o @ epoch 4 new loss 4.5266129745868966e-05 old loss 4.5942317228764296e-05 BETTER
I0326 03:35:46.154986 1336562 finetune.py:68] layer 19_o @ epoch 4 new loss 4.314598481869325e-05 old loss 4.375532080302946e-05 BETTER
I0326 03:35:49.019028 1335910 finetune.py:45] layer 18_up initial loss 0.0001304720644839108
I0326 03:36:07.565345 1336562 finetune.py:45] layer 19_up initial loss 0.00013435151777230203
I0326 03:36:21.533658 1335910 finetune.py:68] layer 18_up @ epoch 0 new loss 0.00012516547576524317 old loss 0.0001304720644839108 BETTER
I0326 03:36:38.462576 1336562 finetune.py:68] layer 19_up @ epoch 0 new loss 0.000128982646856457 old loss 0.00013435151777230203 BETTER
I0326 03:36:54.914431 1335910 finetune.py:68] layer 18_up @ epoch 1 new loss 0.00012223122757859528 old loss 0.00012516547576524317 BETTER
I0326 03:37:10.273356 1336562 finetune.py:68] layer 19_up @ epoch 1 new loss 0.00012603899813257158 old loss 0.000128982646856457 BETTER
I0326 03:37:28.561005 1335910 finetune.py:68] layer 18_up @ epoch 2 new loss 0.00011997644469374791 old loss 0.00012223122757859528 BETTER
I0326 03:37:42.399636 1336562 finetune.py:68] layer 19_up @ epoch 2 new loss 0.0001237410178873688 old loss 0.00012603899813257158 BETTER
I0326 03:38:02.322637 1335910 finetune.py:68] layer 18_up @ epoch 3 new loss 0.00011806079419329762 old loss 0.00011997644469374791 BETTER
I0326 03:38:14.601890 1336562 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0001218547549797222 old loss 0.0001237410178873688 BETTER
I0326 03:38:36.072322 1335910 finetune.py:68] layer 18_up @ epoch 4 new loss 0.00011644070036709309 old loss 0.00011806079419329762 BETTER
I0326 03:38:46.654119 1336562 finetune.py:68] layer 19_up @ epoch 4 new loss 0.00012022232112940401 old loss 0.0001218547549797222 BETTER
I0326 03:38:57.971421 1335910 finetune.py:45] layer 18_gate initial loss 0.00014708474918734282
I0326 03:39:08.209295 1336562 finetune.py:45] layer 19_gate initial loss 0.00015332800103351474
I0326 03:39:28.418703 1335910 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.00014518311945721507 old loss 0.00014708474918734282 BETTER
I0326 03:39:36.842244 1336562 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.00015144380449783057 old loss 0.00015332800103351474 BETTER
I0326 03:39:59.759972 1335910 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.00014369009295478463 old loss 0.00014518311945721507 BETTER
I0326 03:40:06.296291 1336562 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.00014996362733654678 old loss 0.00015144380449783057 BETTER
I0326 03:40:31.122019 1335910 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.00014238305448088795 old loss 0.00014369009295478463 BETTER
I0326 03:40:35.978783 1336562 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0001486446417402476 old loss 0.00014996362733654678 BETTER
I0326 03:41:02.712217 1335910 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0001412242854712531 old loss 0.00014238305448088795 BETTER
I0326 03:41:05.745444 1336562 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0001474886666983366 old loss 0.0001486446417402476 BETTER
I0326 03:41:34.217159 1335910 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.00014016056957188994 old loss 0.0001412242854712531 BETTER
I0326 03:41:35.485890 1336562 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.00014645636838395149 old loss 0.0001474886666983366 BETTER
I0326 03:41:56.917141 1335910 finetune.py:45] layer 18_down initial loss 0.00025796538102440536
I0326 03:41:57.797885 1336562 finetune.py:45] layer 19_down initial loss 0.0002679873432498425
I0326 03:42:24.518883 1336562 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0002679444442037493 old loss 0.0002679873432498425 BETTER
I0326 03:42:24.610600 1335910 finetune.py:68] layer 18_down @ epoch 0 new loss 0.00025792515953071415 old loss 0.00025796538102440536 BETTER
I0326 03:42:52.062920 1336562 finetune.py:68] layer 19_down @ epoch 1 new loss 0.00026791071286424994 old loss 0.0002679444442037493 BETTER
I0326 03:42:53.574283 1335910 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0002578909043222666 old loss 0.00025792515953071415 BETTER
I0326 03:43:19.755333 1336562 finetune.py:68] layer 19_down @ epoch 2 new loss 0.00026788347167894244 old loss 0.00026791071286424994 BETTER
I0326 03:43:22.682137 1335910 finetune.py:68] layer 18_down @ epoch 2 new loss 0.00025786174228414893 old loss 0.0002578909043222666 BETTER
I0326 03:43:47.395353 1336562 finetune.py:68] layer 19_down @ epoch 3 new loss 0.00026785992668010294 old loss 0.00026788347167894244 BETTER
I0326 03:43:51.911366 1335910 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0002578366838861257 old loss 0.00025786174228414893 BETTER
I0326 03:44:15.004602 1336562 finetune.py:68] layer 19_down @ epoch 4 new loss 0.00026783981593325734 old loss 0.00026785992668010294 BETTER
19_v proxy err 0.025285042822360992 tr(WHW.T) 341.0596618652344
bpp_loss 2.7990424633026123
19_q proxy err 0.001683523878455162 tr(WHW.T) 24062.958984375
bpp_loss 3.583001494407654
19_k proxy err 0.0007322914898395538 tr(WHW.T) 15584.12109375
bpp_loss 4.415575742721558
19_o proxy err 0.03261850029230118 tr(WHW.T) 1174.8516845703125
bpp_loss 2.8835697174072266
19_up proxy err 0.020966853946447372 tr(WHW.T) 7650.33935546875
bpp_loss 2.954024451119559
19_gate proxy err 0.005323607474565506 tr(WHW.T) 32706.18359375
bpp_loss 3.3087376185825894
19_down proxy err 0.024057399481534958 tr(WHW.T) 6242.73095703125
bpp_loss 2.950387580054147
I0326 03:44:21.182221 1335910 finetune.py:68] layer 18_down @ epoch 4 new loss 0.00025781456497497857 old loss 0.0002578366838861257 BETTER
18_v proxy err 0.028874032199382782 tr(WHW.T) 287.61376953125
bpp_loss 2.7546801567077637
18_q proxy err 0.0017713971901685 tr(WHW.T) 22425.5859375
bpp_loss 3.580264449119568
18_k proxy err 0.0006472232053056359 tr(WHW.T) 17431.814453125
bpp_loss 4.503715991973877
18_o proxy err 0.03189392760396004 tr(WHW.T) 1209.439453125
bpp_loss 2.870406150817871
18_up proxy err 0.01999683491885662 tr(WHW.T) 7981.99462890625
bpp_loss 2.9570011411394392
18_gate proxy err 0.004933212883770466 tr(WHW.T) 35087.015625
bpp_loss 3.2979948861258372
18_down proxy err 0.024202777072787285 tr(WHW.T) 6287.1806640625
bpp_loss 2.952068771634783
I0326 03:45:28.949105 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 20 in 62.906408071517944s
I0326 03:45:32.134459 1347994 config.py:54] PyTorch version 2.6.0 available.
W0326 03:45:32.416838 1347994 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 03:45:33.301502 1347994 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 03:45:33.305654 1258494 quantize_finetune_llama.py:209] layer 21 gpu 1
I0326 03:45:33.321560 1347994 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 03:45:40.652394 1347994 finetune.py:45] layer 20_v initial loss 3.4771273931255564e-05
W0326 03:45:40.652646 1347994 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 03:46:16.787916 1347994 finetune.py:68] layer 20_v @ epoch 0 new loss 2.166399463021662e-05 old loss 3.4771273931255564e-05 BETTER
I0326 03:46:25.946973 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 21 in 52.24210786819458s
I0326 03:46:29.309129 1348554 config.py:54] PyTorch version 2.6.0 available.
W0326 03:46:29.602612 1348554 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 03:46:30.499180 1348554 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 03:46:30.503677 1258494 quantize_finetune_llama.py:209] layer 22 gpu 0
I0326 03:46:30.518903 1348554 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 03:46:37.319458 1348554 finetune.py:45] layer 21_v initial loss 4.024485679110512e-05
W0326 03:46:37.319679 1348554 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 03:46:53.764130 1347994 finetune.py:68] layer 20_v @ epoch 1 new loss 2.0148145267739892e-05 old loss 2.166399463021662e-05 BETTER
I0326 03:47:10.740286 1348554 finetune.py:68] layer 21_v @ epoch 0 new loss 2.603306529636029e-05 old loss 4.024485679110512e-05 BETTER
I0326 03:47:30.744454 1347994 finetune.py:68] layer 20_v @ epoch 2 new loss 1.9318056729389355e-05 old loss 2.0148145267739892e-05 BETTER
I0326 03:47:45.189607 1348554 finetune.py:68] layer 21_v @ epoch 1 new loss 2.405888335488271e-05 old loss 2.603306529636029e-05 BETTER
I0326 03:48:07.635185 1347994 finetune.py:68] layer 20_v @ epoch 3 new loss 1.8732356693362817e-05 old loss 1.9318056729389355e-05 BETTER
I0326 03:48:19.878942 1348554 finetune.py:68] layer 21_v @ epoch 2 new loss 2.290863631060347e-05 old loss 2.405888335488271e-05 BETTER
I0326 03:48:44.754772 1347994 finetune.py:68] layer 20_v @ epoch 4 new loss 1.8305605408386327e-05 old loss 1.8732356693362817e-05 BETTER
I0326 03:48:54.416580 1347994 finetune.py:45] layer 20_q initial loss 2.2895614165463485e-05
I0326 03:48:54.772016 1348554 finetune.py:68] layer 21_v @ epoch 3 new loss 2.2126339899841696e-05 old loss 2.290863631060347e-05 BETTER
I0326 03:49:29.740761 1348554 finetune.py:68] layer 21_v @ epoch 4 new loss 2.1523257601074874e-05 old loss 2.2126339899841696e-05 BETTER
I0326 03:49:30.393938 1347994 finetune.py:68] layer 20_q @ epoch 0 new loss 2.126798608514946e-05 old loss 2.2895614165463485e-05 BETTER
I0326 03:49:39.147315 1348554 finetune.py:45] layer 21_q initial loss 2.8095160814700648e-05
I0326 03:50:07.297367 1347994 finetune.py:68] layer 20_q @ epoch 1 new loss 2.079343539662659e-05 old loss 2.126798608514946e-05 BETTER
I0326 03:50:12.867451 1348554 finetune.py:68] layer 21_q @ epoch 0 new loss 2.634332122397609e-05 old loss 2.8095160814700648e-05 BETTER
I0326 03:50:44.102896 1347994 finetune.py:68] layer 20_q @ epoch 2 new loss 2.0441848391783424e-05 old loss 2.079343539662659e-05 BETTER
I0326 03:50:47.508721 1348554 finetune.py:68] layer 21_q @ epoch 1 new loss 2.5625364287407137e-05 old loss 2.634332122397609e-05 BETTER
I0326 03:51:20.980426 1347994 finetune.py:68] layer 20_q @ epoch 3 new loss 2.015367499552667e-05 old loss 2.0441848391783424e-05 BETTER
I0326 03:51:21.996824 1348554 finetune.py:68] layer 21_q @ epoch 2 new loss 2.508922443666961e-05 old loss 2.5625364287407137e-05 BETTER
I0326 03:51:56.626672 1348554 finetune.py:68] layer 21_q @ epoch 3 new loss 2.4650000341353007e-05 old loss 2.508922443666961e-05 BETTER
I0326 03:51:57.862279 1347994 finetune.py:68] layer 20_q @ epoch 4 new loss 1.9918563339160755e-05 old loss 2.015367499552667e-05 BETTER
I0326 03:52:05.775962 1347994 finetune.py:45] layer 20_k initial loss 2.132042391167488e-05
I0326 03:52:31.211438 1348554 finetune.py:68] layer 21_q @ epoch 4 new loss 2.42896221607225e-05 old loss 2.4650000341353007e-05 BETTER
I0326 03:52:39.237282 1348554 finetune.py:45] layer 21_k initial loss 2.6244828404742293e-05
I0326 03:52:41.980875 1347994 finetune.py:68] layer 20_k @ epoch 0 new loss 2.086317545035854e-05 old loss 2.132042391167488e-05 BETTER
I0326 03:53:12.845336 1348554 finetune.py:68] layer 21_k @ epoch 0 new loss 2.578054954938125e-05 old loss 2.6244828404742293e-05 BETTER
I0326 03:53:18.654058 1347994 finetune.py:68] layer 20_k @ epoch 1 new loss 2.0670946469181217e-05 old loss 2.086317545035854e-05 BETTER
I0326 03:53:47.185764 1348554 finetune.py:68] layer 21_k @ epoch 1 new loss 2.5519782866467722e-05 old loss 2.578054954938125e-05 BETTER
I0326 03:53:55.044495 1347994 finetune.py:68] layer 20_k @ epoch 2 new loss 2.0506055079749785e-05 old loss 2.0670946469181217e-05 BETTER
I0326 03:54:21.548807 1348554 finetune.py:68] layer 21_k @ epoch 2 new loss 2.528347795305308e-05 old loss 2.5519782866467722e-05 BETTER
I0326 03:54:31.734736 1347994 finetune.py:68] layer 20_k @ epoch 3 new loss 2.0368568584672175e-05 old loss 2.0506055079749785e-05 BETTER
I0326 03:54:55.915281 1348554 finetune.py:68] layer 21_k @ epoch 3 new loss 2.5090525014093146e-05 old loss 2.528347795305308e-05 BETTER
I0326 03:55:08.639672 1347994 finetune.py:68] layer 20_k @ epoch 4 new loss 2.024256536969915e-05 old loss 2.0368568584672175e-05 BETTER
I0326 03:55:18.299489 1347994 finetune.py:45] layer 20_o initial loss 5.408563811215572e-05
I0326 03:55:30.326125 1348554 finetune.py:68] layer 21_k @ epoch 4 new loss 2.4920987925725058e-05 old loss 2.5090525014093146e-05 BETTER
I0326 03:55:39.679300 1348554 finetune.py:45] layer 21_o initial loss 7.793169061187655e-05
I0326 03:55:53.717224 1347994 finetune.py:68] layer 20_o @ epoch 0 new loss 5.010662425775081e-05 old loss 5.408563811215572e-05 BETTER
I0326 03:56:12.800995 1348554 finetune.py:68] layer 21_o @ epoch 0 new loss 7.003913924563676e-05 old loss 7.793169061187655e-05 BETTER
I0326 03:56:29.666954 1347994 finetune.py:68] layer 20_o @ epoch 1 new loss 4.8525216698180884e-05 old loss 5.010662425775081e-05 BETTER
I0326 03:56:46.662679 1348554 finetune.py:68] layer 21_o @ epoch 1 new loss 6.673739699181169e-05 old loss 7.003913924563676e-05 BETTER
I0326 03:57:05.615052 1347994 finetune.py:68] layer 20_o @ epoch 2 new loss 4.7466066462220624e-05 old loss 4.8525216698180884e-05 BETTER
I0326 03:57:20.855496 1348554 finetune.py:68] layer 21_o @ epoch 2 new loss 6.451745866797864e-05 old loss 6.673739699181169e-05 BETTER
I0326 03:57:42.003565 1347994 finetune.py:68] layer 20_o @ epoch 3 new loss 4.663461368181743e-05 old loss 4.7466066462220624e-05 BETTER
I0326 03:57:54.829874 1348554 finetune.py:68] layer 21_o @ epoch 3 new loss 6.286612915573642e-05 old loss 6.451745866797864e-05 BETTER
I0326 03:58:18.024067 1347994 finetune.py:68] layer 20_o @ epoch 4 new loss 4.598662781063467e-05 old loss 4.663461368181743e-05 BETTER
I0326 03:58:28.696710 1348554 finetune.py:68] layer 21_o @ epoch 4 new loss 6.156138988444582e-05 old loss 6.286612915573642e-05 BETTER
I0326 03:58:39.723588 1347994 finetune.py:45] layer 20_up initial loss 0.00014362811634782702
I0326 03:58:50.144627 1348554 finetune.py:45] layer 21_up initial loss 0.00017528676835354418
I0326 03:59:12.008977 1347994 finetune.py:68] layer 20_up @ epoch 0 new loss 0.00013794387632515281 old loss 0.00014362811634782702 BETTER
I0326 03:59:21.057629 1348554 finetune.py:68] layer 21_up @ epoch 0 new loss 0.00016791970119811594 old loss 0.00017528676835354418 BETTER
I0326 03:59:45.252812 1347994 finetune.py:68] layer 20_up @ epoch 1 new loss 0.0001348943478660658 old loss 0.00013794387632515281 BETTER
I0326 03:59:52.879839 1348554 finetune.py:68] layer 21_up @ epoch 1 new loss 0.00016402167966589332 old loss 0.00016791970119811594 BETTER
I0326 04:00:18.909437 1347994 finetune.py:68] layer 20_up @ epoch 2 new loss 0.00013254539226181805 old loss 0.0001348943478660658 BETTER
I0326 04:00:25.003379 1348554 finetune.py:68] layer 21_up @ epoch 2 new loss 0.00016101986693684012 old loss 0.00016402167966589332 BETTER
I0326 04:00:52.680027 1347994 finetune.py:68] layer 20_up @ epoch 3 new loss 0.00013057257456239313 old loss 0.00013254539226181805 BETTER
I0326 04:00:57.109693 1348554 finetune.py:68] layer 21_up @ epoch 3 new loss 0.00015851327043492347 old loss 0.00016101986693684012 BETTER
I0326 04:01:26.669066 1347994 finetune.py:68] layer 20_up @ epoch 4 new loss 0.00012892075756099075 old loss 0.00013057257456239313 BETTER
I0326 04:01:29.150515 1348554 finetune.py:68] layer 21_up @ epoch 4 new loss 0.00015638854529242963 old loss 0.00015851327043492347 BETTER
I0326 04:01:48.501136 1347994 finetune.py:45] layer 20_gate initial loss 0.00016627233708277345
I0326 04:01:50.807719 1348554 finetune.py:45] layer 21_gate initial loss 0.0001996024657273665
I0326 04:02:18.911633 1347994 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.00016423477791249752 old loss 0.00016627233708277345 BETTER
I0326 04:02:19.499598 1348554 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.00019697991956491023 old loss 0.0001996024657273665 BETTER
I0326 04:02:49.083944 1348554 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.00019500793132465333 old loss 0.00019697991956491023 BETTER
I0326 04:02:50.202283 1347994 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0001626951270736754 old loss 0.00016423477791249752 BETTER
I0326 04:03:18.880141 1348554 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.00019329623319208622 old loss 0.00019500793132465333 BETTER
I0326 04:03:21.561514 1347994 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.00016134577163029462 old loss 0.0001626951270736754 BETTER
I0326 04:03:48.652483 1348554 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.00019177983631379902 old loss 0.00019329623319208622 BETTER
I0326 04:03:52.911313 1347994 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.00016015199071262032 old loss 0.00016134577163029462 BETTER
I0326 04:04:18.450764 1348554 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.00019041479390580207 old loss 0.00019177983631379902 BETTER
I0326 04:04:24.359132 1347994 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.00015908708155620843 old loss 0.00016015199071262032 BETTER
I0326 04:04:40.937065 1348554 finetune.py:45] layer 21_down initial loss 0.00034390395740047097
I0326 04:04:47.013535 1347994 finetune.py:45] layer 20_down initial loss 0.00028821240994147956
I0326 04:05:07.633786 1348554 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0003438493295107037 old loss 0.00034390395740047097 BETTER
I0326 04:05:15.046888 1347994 finetune.py:68] layer 20_down @ epoch 0 new loss 0.00028817710699513555 old loss 0.00028821240994147956 BETTER
I0326 04:05:35.185847 1348554 finetune.py:68] layer 21_down @ epoch 1 new loss 0.00034380238503217697 old loss 0.0003438493295107037 BETTER
I0326 04:05:43.655295 1347994 finetune.py:68] layer 20_down @ epoch 1 new loss 0.00028814905090257525 old loss 0.00028817710699513555 BETTER
I0326 04:06:02.859962 1348554 finetune.py:68] layer 21_down @ epoch 2 new loss 0.00034376478288322687 old loss 0.00034380238503217697 BETTER
I0326 04:06:12.759023 1347994 finetune.py:68] layer 20_down @ epoch 2 new loss 0.00028812672826461494 old loss 0.00028814905090257525 BETTER
I0326 04:06:30.524681 1348554 finetune.py:68] layer 21_down @ epoch 3 new loss 0.00034373157541267574 old loss 0.00034376478288322687 BETTER
I0326 04:06:41.831346 1347994 finetune.py:68] layer 20_down @ epoch 3 new loss 0.00028810856747440994 old loss 0.00028812672826461494 BETTER
I0326 04:06:58.179651 1348554 finetune.py:68] layer 21_down @ epoch 4 new loss 0.00034370311186648905 old loss 0.00034373157541267574 BETTER
21_v proxy err 0.023490045219659805 tr(WHW.T) 362.87310791015625
bpp_loss 2.8569209575653076
21_q proxy err 0.0015279864892363548 tr(WHW.T) 25857.7890625
bpp_loss 3.5561561584472656
21_k proxy err 0.0006605273811146617 tr(WHW.T) 16820.234375
bpp_loss 4.412306547164917
21_o proxy err 0.027617698535323143 tr(WHW.T) 1273.6905517578125
bpp_loss 2.8852521181106567
21_up proxy err 0.020694883540272713 tr(WHW.T) 7778.2314453125
bpp_loss 2.9606005804879323
21_gate proxy err 0.005546979606151581 tr(WHW.T) 31505.654296875
bpp_loss 3.321847370692662
21_down proxy err 0.022682400420308113 tr(WHW.T) 6422.58984375
bpp_loss 2.9549878665379117
I0326 04:07:10.974581 1347994 finetune.py:68] layer 20_down @ epoch 4 new loss 0.00028809349169023335 old loss 0.00028810856747440994 BETTER
20_v proxy err 0.025412311777472496 tr(WHW.T) 330.192138671875
bpp_loss 2.832133650779724
20_q proxy err 0.0018671322613954544 tr(WHW.T) 20759.708984375
bpp_loss 3.556238293647766
20_k proxy err 0.0007072627195157111 tr(WHW.T) 15431.419921875
bpp_loss 4.374249696731567
20_o proxy err 0.03270551189780235 tr(WHW.T) 1210.0560302734375
bpp_loss 2.870189070701599
20_up proxy err 0.021212812513113022 tr(WHW.T) 7602.67431640625
bpp_loss 2.957859992980957
20_gate proxy err 0.005709727760404348 tr(WHW.T) 30628.51953125
bpp_loss 3.3111155373709544
20_down proxy err 0.023531844839453697 tr(WHW.T) 6361.484375
bpp_loss 2.9546473026275635
I0326 04:08:19.344305 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 22 in 63.453755140304565s
I0326 04:08:22.697061 1359270 config.py:54] PyTorch version 2.6.0 available.
W0326 04:08:22.978607 1359270 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 04:08:23.882503 1359270 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 04:08:23.886514 1258494 quantize_finetune_llama.py:209] layer 23 gpu 1
I0326 04:08:23.900009 1359270 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 04:08:31.343502 1359270 finetune.py:45] layer 22_v initial loss 3.628104241215624e-05
W0326 04:08:31.343778 1359270 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 04:09:07.837683 1359270 finetune.py:68] layer 22_v @ epoch 0 new loss 2.1142379409866408e-05 old loss 3.628104241215624e-05 BETTER
I0326 04:09:24.445304 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 23 in 60.167810678482056s
I0326 04:09:27.802892 1359860 config.py:54] PyTorch version 2.6.0 available.
W0326 04:09:28.102049 1359860 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 04:09:28.996555 1359860 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 04:09:29.000895 1258494 quantize_finetune_llama.py:209] layer 24 gpu 0
I0326 04:09:29.014433 1359860 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 04:09:35.797224 1359860 finetune.py:45] layer 23_v initial loss 3.8515481719514355e-05
W0326 04:09:35.797476 1359860 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 04:09:45.123069 1359270 finetune.py:68] layer 22_v @ epoch 1 new loss 1.955029983946588e-05 old loss 2.1142379409866408e-05 BETTER
I0326 04:10:09.391960 1359860 finetune.py:68] layer 23_v @ epoch 0 new loss 2.1053820091765374e-05 old loss 3.8515481719514355e-05 BETTER
I0326 04:10:22.422780 1359270 finetune.py:68] layer 22_v @ epoch 2 new loss 1.871879976533819e-05 old loss 1.955029983946588e-05 BETTER
I0326 04:10:43.825275 1359860 finetune.py:68] layer 23_v @ epoch 1 new loss 1.947614691744093e-05 old loss 2.1053820091765374e-05 BETTER
I0326 04:10:59.849869 1359270 finetune.py:68] layer 22_v @ epoch 3 new loss 1.816297481127549e-05 old loss 1.871879976533819e-05 BETTER
I0326 04:11:18.498646 1359860 finetune.py:68] layer 23_v @ epoch 2 new loss 1.8691178411245346e-05 old loss 1.947614691744093e-05 BETTER
I0326 04:11:37.358774 1359270 finetune.py:68] layer 22_v @ epoch 4 new loss 1.776091448846273e-05 old loss 1.816297481127549e-05 BETTER
I0326 04:11:47.055891 1359270 finetune.py:45] layer 22_q initial loss 2.3486998543376103e-05
I0326 04:11:53.483094 1359860 finetune.py:68] layer 23_v @ epoch 3 new loss 1.817104930523783e-05 old loss 1.8691178411245346e-05 BETTER
I0326 04:12:23.297091 1359270 finetune.py:68] layer 22_q @ epoch 0 new loss 2.211221362813376e-05 old loss 2.3486998543376103e-05 BETTER
I0326 04:12:28.628094 1359860 finetune.py:68] layer 23_v @ epoch 4 new loss 1.7797863620216958e-05 old loss 1.817104930523783e-05 BETTER
I0326 04:12:38.010604 1359860 finetune.py:45] layer 23_q initial loss 2.2987958800513297e-05
I0326 04:13:00.227651 1359270 finetune.py:68] layer 22_q @ epoch 1 new loss 2.1623296561301686e-05 old loss 2.211221362813376e-05 BETTER
I0326 04:13:11.967352 1359860 finetune.py:68] layer 23_q @ epoch 0 new loss 2.1764906705357134e-05 old loss 2.2987958800513297e-05 BETTER
I0326 04:13:37.145385 1359270 finetune.py:68] layer 22_q @ epoch 2 new loss 2.1270661818562075e-05 old loss 2.1623296561301686e-05 BETTER
I0326 04:13:46.493879 1359860 finetune.py:68] layer 23_q @ epoch 1 new loss 2.1323947294149548e-05 old loss 2.1764906705357134e-05 BETTER
I0326 04:14:13.856016 1359270 finetune.py:68] layer 22_q @ epoch 3 new loss 2.099116682074964e-05 old loss 2.1270661818562075e-05 BETTER
I0326 04:14:20.942252 1359860 finetune.py:68] layer 23_q @ epoch 2 new loss 2.098074764944613e-05 old loss 2.1323947294149548e-05 BETTER
I0326 04:14:50.676479 1359270 finetune.py:68] layer 22_q @ epoch 4 new loss 2.0753388525918126e-05 old loss 2.099116682074964e-05 BETTER
I0326 04:14:55.661734 1359860 finetune.py:68] layer 23_q @ epoch 3 new loss 2.0719093299703673e-05 old loss 2.098074764944613e-05 BETTER
I0326 04:14:58.642409 1359270 finetune.py:45] layer 22_k initial loss 2.267517447762657e-05
I0326 04:15:30.397347 1359860 finetune.py:68] layer 23_q @ epoch 4 new loss 2.0502828192547895e-05 old loss 2.0719093299703673e-05 BETTER
I0326 04:15:34.366008 1359270 finetune.py:68] layer 22_k @ epoch 0 new loss 2.2235350115806796e-05 old loss 2.267517447762657e-05 BETTER
I0326 04:15:38.183431 1359860 finetune.py:45] layer 23_k initial loss 2.257523374282755e-05
I0326 04:16:10.834228 1359270 finetune.py:68] layer 22_k @ epoch 1 new loss 2.2043272110749967e-05 old loss 2.2235350115806796e-05 BETTER
I0326 04:16:11.847375 1359860 finetune.py:68] layer 23_k @ epoch 0 new loss 2.2196352801984176e-05 old loss 2.257523374282755e-05 BETTER
I0326 04:16:46.273305 1359860 finetune.py:68] layer 23_k @ epoch 1 new loss 2.200741982960608e-05 old loss 2.2196352801984176e-05 BETTER
I0326 04:16:47.756773 1359270 finetune.py:68] layer 22_k @ epoch 2 new loss 2.1881680368096568e-05 old loss 2.2043272110749967e-05 BETTER
I0326 04:17:20.766344 1359860 finetune.py:68] layer 23_k @ epoch 2 new loss 2.1856609237147495e-05 old loss 2.200741982960608e-05 BETTER
I0326 04:17:24.607311 1359270 finetune.py:68] layer 22_k @ epoch 3 new loss 2.1749103325419128e-05 old loss 2.1881680368096568e-05 BETTER
I0326 04:17:55.426688 1359860 finetune.py:68] layer 23_k @ epoch 3 new loss 2.17305696423864e-05 old loss 2.1856609237147495e-05 BETTER
I0326 04:18:01.536584 1359270 finetune.py:68] layer 22_k @ epoch 4 new loss 2.1633484720950946e-05 old loss 2.1749103325419128e-05 BETTER
I0326 04:18:11.171501 1359270 finetune.py:45] layer 22_o initial loss 6.529856182169169e-05
I0326 04:18:29.970803 1359860 finetune.py:68] layer 23_k @ epoch 4 new loss 2.1619716790155508e-05 old loss 2.17305696423864e-05 BETTER
I0326 04:18:39.335115 1359860 finetune.py:45] layer 23_o initial loss 5.877043804503046e-05
I0326 04:18:46.772932 1359270 finetune.py:68] layer 22_o @ epoch 0 new loss 6.058983490220271e-05 old loss 6.529856182169169e-05 BETTER
I0326 04:19:12.522677 1359860 finetune.py:68] layer 23_o @ epoch 0 new loss 5.497339952853508e-05 old loss 5.877043804503046e-05 BETTER
I0326 04:19:22.706427 1359270 finetune.py:68] layer 22_o @ epoch 1 new loss 5.8740435633808374e-05 old loss 6.058983490220271e-05 BETTER
I0326 04:19:46.399931 1359860 finetune.py:68] layer 23_o @ epoch 1 new loss 5.359306305763312e-05 old loss 5.497339952853508e-05 BETTER
I0326 04:19:58.554289 1359270 finetune.py:68] layer 22_o @ epoch 2 new loss 5.746796523453668e-05 old loss 5.8740435633808374e-05 BETTER
I0326 04:20:20.230275 1359860 finetune.py:68] layer 23_o @ epoch 2 new loss 5.2645871619461104e-05 old loss 5.359306305763312e-05 BETTER
I0326 04:20:34.583317 1359270 finetune.py:68] layer 22_o @ epoch 3 new loss 5.6521599617553875e-05 old loss 5.746796523453668e-05 BETTER
I0326 04:20:54.175108 1359860 finetune.py:68] layer 23_o @ epoch 3 new loss 5.1933569920947775e-05 old loss 5.2645871619461104e-05 BETTER
I0326 04:21:10.551179 1359270 finetune.py:68] layer 22_o @ epoch 4 new loss 5.575294198933989e-05 old loss 5.6521599617553875e-05 BETTER
I0326 04:21:28.081863 1359860 finetune.py:68] layer 23_o @ epoch 4 new loss 5.139374115969986e-05 old loss 5.1933569920947775e-05 BETTER
I0326 04:21:32.307376 1359270 finetune.py:45] layer 22_up initial loss 0.00017409645079169422
I0326 04:21:49.595434 1359860 finetune.py:45] layer 23_up initial loss 0.00017997210670728236
I0326 04:22:04.551059 1359270 finetune.py:68] layer 22_up @ epoch 0 new loss 0.00016722772852517664 old loss 0.00017409645079169422 BETTER
I0326 04:22:20.328520 1359860 finetune.py:68] layer 23_up @ epoch 0 new loss 0.00017334088624920696 old loss 0.00017997210670728236 BETTER
I0326 04:22:38.217954 1359270 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0001637176173971966 old loss 0.00016722772852517664 BETTER
I0326 04:22:52.183698 1359860 finetune.py:68] layer 23_up @ epoch 1 new loss 0.00016988148854579777 old loss 0.00017334088624920696 BETTER
I0326 04:23:12.084525 1359270 finetune.py:68] layer 22_up @ epoch 2 new loss 0.00016098130436148494 old loss 0.0001637176173971966 BETTER
I0326 04:23:24.274288 1359860 finetune.py:68] layer 23_up @ epoch 2 new loss 0.00016725659952498972 old loss 0.00016988148854579777 BETTER
I0326 04:23:46.074295 1359270 finetune.py:68] layer 22_up @ epoch 3 new loss 0.00015873897064011544 old loss 0.00016098130436148494 BETTER
I0326 04:23:56.420978 1359860 finetune.py:68] layer 23_up @ epoch 3 new loss 0.00016510355635546148 old loss 0.00016725659952498972 BETTER
I0326 04:24:19.996243 1359270 finetune.py:68] layer 22_up @ epoch 4 new loss 0.00015682213415857404 old loss 0.00015873897064011544 BETTER
I0326 04:24:28.501101 1359860 finetune.py:68] layer 23_up @ epoch 4 new loss 0.00016328154015354812 old loss 0.00016510355635546148 BETTER
I0326 04:24:41.831015 1359270 finetune.py:45] layer 22_gate initial loss 0.00020326535741332918
I0326 04:24:50.202549 1359860 finetune.py:45] layer 23_gate initial loss 0.00021515680418815464
I0326 04:25:12.386237 1359270 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.00020090144244022667 old loss 0.00020326535741332918 BETTER
I0326 04:25:19.034540 1359860 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0002130027423845604 old loss 0.00021515680418815464 BETTER
I0326 04:25:43.723436 1359270 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.00019910956325475127 old loss 0.00020090144244022667 BETTER
I0326 04:25:48.507261 1359860 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.0002113521477440372 old loss 0.0002130027423845604 BETTER
I0326 04:26:15.231557 1359270 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.00019756222900468856 old loss 0.00019910956325475127 BETTER
I0326 04:26:18.000257 1359860 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0002099097619066015 old loss 0.0002113521477440372 BETTER
I0326 04:26:46.566503 1359270 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.00019618745136540383 old loss 0.00019756222900468856 BETTER
I0326 04:26:47.703454 1359860 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.000208636571187526 old loss 0.0002099097619066015 BETTER
I0326 04:27:17.573811 1359860 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.00020750725525431335 old loss 0.000208636571187526 BETTER
I0326 04:27:17.932756 1359270 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0001949668803717941 old loss 0.00019618745136540383 BETTER
I0326 04:27:39.866277 1359860 finetune.py:45] layer 23_down initial loss 0.0003733363701030612
I0326 04:27:40.557350 1359270 finetune.py:45] layer 22_down initial loss 0.0003551585541572422
I0326 04:28:06.734459 1359860 finetune.py:68] layer 23_down @ epoch 0 new loss 0.00037328345933929086 old loss 0.0003733363701030612 BETTER
I0326 04:28:08.528587 1359270 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0003551003464963287 old loss 0.0003551585541572422 BETTER
I0326 04:28:34.355491 1359860 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0003732403856702149 old loss 0.00037328345933929086 BETTER
I0326 04:28:37.456354 1359270 finetune.py:68] layer 22_down @ epoch 1 new loss 0.000355056079570204 old loss 0.0003551003464963287 BETTER
I0326 04:29:02.036082 1359860 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0003732055483851582 old loss 0.0003732403856702149 BETTER
I0326 04:29:06.682747 1359270 finetune.py:68] layer 22_down @ epoch 2 new loss 0.0003550195542629808 old loss 0.000355056079570204 BETTER
I0326 04:29:29.597939 1359860 finetune.py:68] layer 23_down @ epoch 3 new loss 0.00037317530950531363 old loss 0.0003732055483851582 BETTER
I0326 04:29:35.987101 1359270 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0003549866669345647 old loss 0.0003550195542629808 BETTER
I0326 04:29:57.298979 1359860 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0003731494944076985 old loss 0.00037317530950531363 BETTER
23_v proxy err 0.022270919755101204 tr(WHW.T) 397.90704345703125
bpp_loss 2.9554076194763184
23_q proxy err 0.0017566874157637358 tr(WHW.T) 22630.611328125
bpp_loss 3.5304630994796753
23_k proxy err 0.000750469509512186 tr(WHW.T) 14885.4990234375
bpp_loss 4.357204437255859
23_o proxy err 0.023431267589330673 tr(WHW.T) 1750.22900390625
bpp_loss 2.9400354623794556
23_up proxy err 0.02182110585272312 tr(WHW.T) 7425.4462890625
bpp_loss 2.9688562665666853
23_gate proxy err 0.0064475033432245255 tr(WHW.T) 27188.064453125
bpp_loss 3.3289726802280972
23_down proxy err 0.02199522592127323 tr(WHW.T) 6728.775390625
bpp_loss 2.9664937428065707
I0326 04:30:05.313579 1359270 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0003549580287653953 old loss 0.0003549866669345647 BETTER
22_v proxy err 0.023956462740898132 tr(WHW.T) 346.0789489746094
bpp_loss 2.90558397769928
22_q proxy err 0.001834478578530252 tr(WHW.T) 20344.017578125
bpp_loss 3.519228219985962
22_k proxy err 0.0007139351801015437 tr(WHW.T) 14750.470703125
bpp_loss 4.354429006576538
22_o proxy err 0.033100295811891556 tr(WHW.T) 1223.3681640625
bpp_loss 2.917126178741455
22_up proxy err 0.0214283037930727 tr(WHW.T) 7551.109375
bpp_loss 2.9649434770856584
22_gate proxy err 0.005957318004220724 tr(WHW.T) 29465.796875
bpp_loss 3.3267886298043385
22_down proxy err 0.02252899669110775 tr(WHW.T) 6600.29345703125
bpp_loss 2.961158445903233
I0326 04:31:12.987146 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 24 in 62.84011745452881s
I0326 04:31:16.305366 1370522 config.py:54] PyTorch version 2.6.0 available.
W0326 04:31:16.584068 1370522 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 04:31:17.463851 1370522 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 04:31:17.467783 1258494 quantize_finetune_llama.py:209] layer 25 gpu 1
I0326 04:31:17.481195 1370522 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 04:31:24.760885 1370522 finetune.py:45] layer 24_v initial loss 3.706590723595582e-05
W0326 04:31:24.761165 1370522 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 04:32:01.071872 1370522 finetune.py:68] layer 24_v @ epoch 0 new loss 2.1269102944643237e-05 old loss 3.706590723595582e-05 BETTER
I0326 04:32:18.177693 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 25 in 60.29526233673096s
I0326 04:32:21.655949 1371104 config.py:54] PyTorch version 2.6.0 available.
W0326 04:32:21.951572 1371104 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 04:32:22.854739 1371104 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 04:32:22.858487 1258494 quantize_finetune_llama.py:209] layer 26 gpu 0
I0326 04:32:22.870902 1371104 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 04:32:29.614379 1371104 finetune.py:45] layer 25_v initial loss 4.397788143251091e-05
W0326 04:32:29.614667 1371104 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 04:32:38.282565 1370522 finetune.py:68] layer 24_v @ epoch 1 new loss 1.9874481949955225e-05 old loss 2.1269102944643237e-05 BETTER
I0326 04:33:03.195019 1371104 finetune.py:68] layer 25_v @ epoch 0 new loss 2.5336365069961175e-05 old loss 4.397788143251091e-05 BETTER
I0326 04:33:15.763114 1370522 finetune.py:68] layer 24_v @ epoch 2 new loss 1.915763641591184e-05 old loss 1.9874481949955225e-05 BETTER
I0326 04:33:37.740215 1371104 finetune.py:68] layer 25_v @ epoch 1 new loss 2.365673572057858e-05 old loss 2.5336365069961175e-05 BETTER
I0326 04:33:52.774627 1370522 finetune.py:68] layer 24_v @ epoch 3 new loss 1.8723496395978145e-05 old loss 1.915763641591184e-05 BETTER
I0326 04:34:12.469564 1371104 finetune.py:68] layer 25_v @ epoch 2 new loss 2.275611950608436e-05 old loss 2.365673572057858e-05 BETTER
I0326 04:34:29.782207 1370522 finetune.py:68] layer 24_v @ epoch 4 new loss 1.8371594705968164e-05 old loss 1.8723496395978145e-05 BETTER
I0326 04:34:39.550671 1370522 finetune.py:45] layer 24_q initial loss 2.4284734536195174e-05
I0326 04:34:47.502524 1371104 finetune.py:68] layer 25_v @ epoch 3 new loss 2.233364648418501e-05 old loss 2.275611950608436e-05 BETTER
I0326 04:35:15.740356 1370522 finetune.py:68] layer 24_q @ epoch 0 new loss 2.3251250240718946e-05 old loss 2.4284734536195174e-05 BETTER
I0326 04:35:22.568614 1371104 finetune.py:68] layer 25_v @ epoch 4 new loss 2.1815945729031228e-05 old loss 2.233364648418501e-05 BETTER
I0326 04:35:31.941876 1371104 finetune.py:45] layer 25_q initial loss 3.364087751833722e-05
I0326 04:35:52.518304 1370522 finetune.py:68] layer 24_q @ epoch 1 new loss 2.279851833009161e-05 old loss 2.3251250240718946e-05 BETTER
I0326 04:36:05.711163 1371104 finetune.py:68] layer 25_q @ epoch 0 new loss 3.0624800274381414e-05 old loss 3.364087751833722e-05 BETTER
I0326 04:36:29.335584 1370522 finetune.py:68] layer 24_q @ epoch 2 new loss 2.2474883735412732e-05 old loss 2.279851833009161e-05 BETTER
I0326 04:36:40.224467 1371104 finetune.py:68] layer 25_q @ epoch 1 new loss 2.9636439649038948e-05 old loss 3.0624800274381414e-05 BETTER
I0326 04:37:06.273114 1370522 finetune.py:68] layer 24_q @ epoch 3 new loss 2.223987212346401e-05 old loss 2.2474883735412732e-05 BETTER
I0326 04:37:14.774942 1371104 finetune.py:68] layer 25_q @ epoch 2 new loss 2.900332947319839e-05 old loss 2.9636439649038948e-05 BETTER
I0326 04:37:43.113443 1370522 finetune.py:68] layer 24_q @ epoch 4 new loss 2.2024181816959754e-05 old loss 2.223987212346401e-05 BETTER
I0326 04:37:49.343319 1371104 finetune.py:68] layer 25_q @ epoch 3 new loss 2.8506232411018573e-05 old loss 2.900332947319839e-05 BETTER
I0326 04:37:51.128177 1370522 finetune.py:45] layer 24_k initial loss 2.4580542230978608e-05
I0326 04:38:24.208455 1371104 finetune.py:68] layer 25_q @ epoch 4 new loss 2.8093121727579273e-05 old loss 2.8506232411018573e-05 BETTER
I0326 04:38:27.311895 1370522 finetune.py:68] layer 24_k @ epoch 0 new loss 2.433935515000485e-05 old loss 2.4580542230978608e-05 BETTER
I0326 04:38:32.394438 1371104 finetune.py:45] layer 25_k initial loss 3.278132498962805e-05
I0326 04:39:04.398617 1370522 finetune.py:68] layer 24_k @ epoch 1 new loss 2.417711220914498e-05 old loss 2.433935515000485e-05 BETTER
I0326 04:39:06.455755 1371104 finetune.py:68] layer 25_k @ epoch 0 new loss 3.217077028239146e-05 old loss 3.278132498962805e-05 BETTER
I0326 04:39:41.124178 1371104 finetune.py:68] layer 25_k @ epoch 1 new loss 3.18515049002599e-05 old loss 3.217077028239146e-05 BETTER
I0326 04:39:41.219195 1370522 finetune.py:68] layer 24_k @ epoch 2 new loss 2.4058686904027127e-05 old loss 2.417711220914498e-05 BETTER
I0326 04:40:15.856855 1371104 finetune.py:68] layer 25_k @ epoch 2 new loss 3.160726555506699e-05 old loss 3.18515049002599e-05 BETTER
I0326 04:40:18.051002 1370522 finetune.py:68] layer 24_k @ epoch 3 new loss 2.395204865024425e-05 old loss 2.4058686904027127e-05 BETTER
I0326 04:40:50.530628 1371104 finetune.py:68] layer 25_k @ epoch 3 new loss 3.1400395528180525e-05 old loss 3.160726555506699e-05 BETTER
I0326 04:40:54.714139 1370522 finetune.py:68] layer 24_k @ epoch 4 new loss 2.3847163902246393e-05 old loss 2.395204865024425e-05 BETTER
I0326 04:41:04.382864 1370522 finetune.py:45] layer 24_o initial loss 6.721456156810746e-05
I0326 04:41:25.120625 1371104 finetune.py:68] layer 25_k @ epoch 4 new loss 3.123120768577792e-05 old loss 3.1400395528180525e-05 BETTER
I0326 04:41:34.315853 1371104 finetune.py:45] layer 25_o initial loss 7.886677485657856e-05
I0326 04:41:39.805101 1370522 finetune.py:68] layer 24_o @ epoch 0 new loss 6.336181832011789e-05 old loss 6.721456156810746e-05 BETTER
I0326 04:42:07.748771 1371104 finetune.py:68] layer 25_o @ epoch 0 new loss 7.354703120654449e-05 old loss 7.886677485657856e-05 BETTER
I0326 04:42:16.092453 1370522 finetune.py:68] layer 24_o @ epoch 1 new loss 6.197571201482788e-05 old loss 6.336181832011789e-05 BETTER
I0326 04:42:41.620682 1371104 finetune.py:68] layer 25_o @ epoch 1 new loss 7.168670708779246e-05 old loss 7.354703120654449e-05 BETTER
I0326 04:42:52.189864 1370522 finetune.py:68] layer 24_o @ epoch 2 new loss 6.10422357567586e-05 old loss 6.197571201482788e-05 BETTER
I0326 04:43:15.565400 1371104 finetune.py:68] layer 25_o @ epoch 2 new loss 7.046195969451219e-05 old loss 7.168670708779246e-05 BETTER
I0326 04:43:28.320923 1370522 finetune.py:68] layer 24_o @ epoch 3 new loss 6.0333957662805915e-05 old loss 6.10422357567586e-05 BETTER
I0326 04:43:49.658772 1371104 finetune.py:68] layer 25_o @ epoch 3 new loss 6.956618744879961e-05 old loss 7.046195969451219e-05 BETTER
I0326 04:44:04.604154 1370522 finetune.py:68] layer 24_o @ epoch 4 new loss 5.975730164209381e-05 old loss 6.0333957662805915e-05 BETTER
I0326 04:44:23.842774 1371104 finetune.py:68] layer 25_o @ epoch 4 new loss 6.888034840812907e-05 old loss 6.956618744879961e-05 BETTER
I0326 04:44:26.197304 1370522 finetune.py:45] layer 24_up initial loss 0.00019688189786393195
I0326 04:44:45.172897 1371104 finetune.py:45] layer 25_up initial loss 0.00022171817545313388
I0326 04:44:58.543807 1370522 finetune.py:68] layer 24_up @ epoch 0 new loss 0.00019023117783945054 old loss 0.00019688189786393195 BETTER
I0326 04:45:16.177091 1371104 finetune.py:68] layer 25_up @ epoch 0 new loss 0.00021398796525318176 old loss 0.00022171817545313388 BETTER
I0326 04:45:31.968076 1370522 finetune.py:68] layer 24_up @ epoch 1 new loss 0.00018688563432078809 old loss 0.00019023117783945054 BETTER
I0326 04:45:48.294851 1371104 finetune.py:68] layer 25_up @ epoch 1 new loss 0.00021028421178925782 old loss 0.00021398796525318176 BETTER
I0326 04:46:05.687929 1370522 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0001843658392317593 old loss 0.00018688563432078809 BETTER
I0326 04:46:20.388384 1371104 finetune.py:68] layer 25_up @ epoch 2 new loss 0.00020750406838487834 old loss 0.00021028421178925782 BETTER
I0326 04:46:39.461357 1370522 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0001822657068260014 old loss 0.0001843658392317593 BETTER
I0326 04:46:52.522905 1371104 finetune.py:68] layer 25_up @ epoch 3 new loss 0.00020523404236882925 old loss 0.00020750406838487834 BETTER
I0326 04:47:13.333090 1370522 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0001805070205591619 old loss 0.0001822657068260014 BETTER
I0326 04:47:24.666048 1371104 finetune.py:68] layer 25_up @ epoch 4 new loss 0.00020331608538981527 old loss 0.00020523404236882925 BETTER
I0326 04:47:35.101077 1370522 finetune.py:45] layer 24_gate initial loss 0.00023867333948146552
I0326 04:47:46.195698 1371104 finetune.py:45] layer 25_gate initial loss 0.00027009681798517704
I0326 04:48:05.498582 1370522 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0002365967520745471 old loss 0.00023867333948146552 BETTER
I0326 04:48:14.858124 1371104 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.0002678162418305874 old loss 0.00027009681798517704 BETTER
I0326 04:48:36.956532 1370522 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0002349875430809334 old loss 0.0002365967520745471 BETTER
I0326 04:48:44.449266 1371104 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.0002660585450939834 old loss 0.0002678162418305874 BETTER
I0326 04:49:08.506279 1370522 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.00023362197680398822 old loss 0.0002349875430809334 BETTER
I0326 04:49:14.173256 1371104 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.00026457521016709507 old loss 0.0002660585450939834 BETTER
I0326 04:49:40.219398 1370522 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.00023238341964315623 old loss 0.00023362197680398822 BETTER
I0326 04:49:43.975981 1371104 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.0002632584946695715 old loss 0.00026457521016709507 BETTER
I0326 04:50:11.937539 1370522 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.00023130585032049567 old loss 0.00023238341964315623 BETTER
I0326 04:50:13.813523 1371104 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.00026208776398561895 old loss 0.0002632584946695715 BETTER
I0326 04:50:34.672734 1370522 finetune.py:45] layer 24_down initial loss 0.00040226441342383623
I0326 04:50:36.133534 1371104 finetune.py:45] layer 25_down initial loss 0.0004456455062609166
I0326 04:51:03.108964 1370522 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0004022190114483237 old loss 0.00040226441342383623 BETTER
I0326 04:51:03.122192 1371104 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0004456036549527198 old loss 0.0004456455062609166 BETTER
I0326 04:51:30.691686 1371104 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0004455663438420743 old loss 0.0004456036549527198 BETTER
I0326 04:51:32.247624 1370522 finetune.py:68] layer 24_down @ epoch 1 new loss 0.00040218321373686194 old loss 0.0004022190114483237 BETTER
I0326 04:51:58.395654 1371104 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0004455352609511465 old loss 0.0004455663438420743 BETTER
I0326 04:52:01.500819 1370522 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0004021529748570174 old loss 0.00040218321373686194 BETTER
I0326 04:52:26.143574 1371104 finetune.py:68] layer 25_down @ epoch 3 new loss 0.00044550671009346843 old loss 0.0004455352609511465 BETTER
I0326 04:52:30.983655 1370522 finetune.py:68] layer 24_down @ epoch 3 new loss 0.00040212812018580735 old loss 0.0004021529748570174 BETTER
I0326 04:52:53.927608 1371104 finetune.py:68] layer 25_down @ epoch 4 new loss 0.00044548336882144213 old loss 0.00044550671009346843 BETTER
25_v proxy err 0.016167225316166878 tr(WHW.T) 557.8086547851562
bpp_loss 3.0590734481811523
25_q proxy err 0.001516291406005621 tr(WHW.T) 26151.703125
bpp_loss 3.4819886684417725
25_k proxy err 0.0007606510771438479 tr(WHW.T) 14442.9150390625
bpp_loss 4.1845152378082275
25_o proxy err 0.0195381548255682 tr(WHW.T) 1993.7313232421875
bpp_loss 2.9766368865966797
25_up proxy err 0.021950535476207733 tr(WHW.T) 7386.37451171875
bpp_loss 2.9820841380528043
25_gate proxy err 0.006675733719021082 tr(WHW.T) 26238.14453125
bpp_loss 3.342624936785017
25_down proxy err 0.02106585167348385 tr(WHW.T) 6685.84130859375
bpp_loss 2.9803041390010288
I0326 04:53:00.443436 1370522 finetune.py:68] layer 24_down @ epoch 4 new loss 0.00040210637962445617 old loss 0.00040212812018580735 BETTER
24_v proxy err 0.018847495317459106 tr(WHW.T) 467.2783508300781
bpp_loss 3.0477782487869263
24_q proxy err 0.0017270472599193454 tr(WHW.T) 22470.029296875
bpp_loss 3.5000298023223877
24_k proxy err 0.0007552954484708607 tr(WHW.T) 14210.67578125
bpp_loss 4.2036967277526855
24_o proxy err 0.02451815828680992 tr(WHW.T) 1596.3975830078125
bpp_loss 2.979040741920471
24_up proxy err 0.02223249338567257 tr(WHW.T) 7312.716796875
bpp_loss 2.973172460283552
24_gate proxy err 0.006808477453887463 tr(WHW.T) 25816.44921875
bpp_loss 3.334437506539481
24_down proxy err 0.02135240100324154 tr(WHW.T) 6811.05029296875
bpp_loss 2.9717189243861606
I0326 04:54:08.042366 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 26 in 62.721168994903564s
I0326 04:54:11.217580 1381863 config.py:54] PyTorch version 2.6.0 available.
W0326 04:54:11.499669 1381863 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 04:54:12.377474 1381863 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 04:54:12.381418 1258494 quantize_finetune_llama.py:209] layer 27 gpu 1
I0326 04:54:12.395016 1381863 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 04:54:19.740291 1381863 finetune.py:45] layer 26_v initial loss 5.917993621551432e-05
W0326 04:54:19.740539 1381863 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 04:54:56.116732 1381863 finetune.py:68] layer 26_v @ epoch 0 new loss 3.781284249271266e-05 old loss 5.917993621551432e-05 BETTER
I0326 04:55:12.607933 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 27 in 59.80666637420654s
I0326 04:55:15.903846 1382443 config.py:54] PyTorch version 2.6.0 available.
W0326 04:55:16.195485 1382443 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 04:55:17.086265 1382443 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 04:55:17.090281 1258494 quantize_finetune_llama.py:209] layer 28 gpu 0
I0326 04:55:17.104076 1382443 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 04:55:23.924799 1382443 finetune.py:45] layer 27_v initial loss 5.7861889217747375e-05
W0326 04:55:23.925117 1382443 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 04:55:33.123203 1381863 finetune.py:68] layer 26_v @ epoch 1 new loss 3.520498285070062e-05 old loss 3.781284249271266e-05 BETTER
I0326 04:55:57.324540 1382443 finetune.py:68] layer 27_v @ epoch 0 new loss 3.474129334790632e-05 old loss 5.7861889217747375e-05 BETTER
I0326 04:56:10.180744 1381863 finetune.py:68] layer 26_v @ epoch 2 new loss 3.372271385160275e-05 old loss 3.520498285070062e-05 BETTER
I0326 04:56:31.933606 1382443 finetune.py:68] layer 27_v @ epoch 1 new loss 3.264487531851046e-05 old loss 3.474129334790632e-05 BETTER
I0326 04:56:47.123361 1381863 finetune.py:68] layer 26_v @ epoch 3 new loss 3.270697925472632e-05 old loss 3.372271385160275e-05 BETTER
I0326 04:57:06.966487 1382443 finetune.py:68] layer 27_v @ epoch 2 new loss 3.1564475648337975e-05 old loss 3.264487531851046e-05 BETTER
I0326 04:57:24.204433 1381863 finetune.py:68] layer 26_v @ epoch 4 new loss 3.196370744262822e-05 old loss 3.270697925472632e-05 BETTER
I0326 04:57:33.939689 1381863 finetune.py:45] layer 26_q initial loss 4.090331640327349e-05
I0326 04:57:42.088037 1382443 finetune.py:68] layer 27_v @ epoch 3 new loss 3.0835064535494894e-05 old loss 3.1564475648337975e-05 BETTER
I0326 04:58:10.082138 1381863 finetune.py:68] layer 26_q @ epoch 0 new loss 3.9121830923249945e-05 old loss 4.090331640327349e-05 BETTER
I0326 04:58:17.063617 1382443 finetune.py:68] layer 27_v @ epoch 4 new loss 3.0191245969035663e-05 old loss 3.0835064535494894e-05 BETTER
I0326 04:58:26.402267 1382443 finetune.py:45] layer 27_q initial loss 4.474385059438646e-05
I0326 04:58:47.069661 1381863 finetune.py:68] layer 26_q @ epoch 1 new loss 3.821205973508768e-05 old loss 3.9121830923249945e-05 BETTER
I0326 04:59:00.438250 1382443 finetune.py:68] layer 27_q @ epoch 0 new loss 4.184998033451848e-05 old loss 4.474385059438646e-05 BETTER
I0326 04:59:23.953624 1381863 finetune.py:68] layer 26_q @ epoch 2 new loss 3.756622027140111e-05 old loss 3.821205973508768e-05 BETTER
I0326 04:59:34.962868 1382443 finetune.py:68] layer 27_q @ epoch 1 new loss 4.093328243470751e-05 old loss 4.184998033451848e-05 BETTER
I0326 05:00:00.796927 1381863 finetune.py:68] layer 26_q @ epoch 3 new loss 3.701079185702838e-05 old loss 3.756622027140111e-05 BETTER
I0326 05:00:09.538686 1382443 finetune.py:68] layer 27_q @ epoch 2 new loss 4.0333114156965166e-05 old loss 4.093328243470751e-05 BETTER
I0326 05:00:37.631961 1381863 finetune.py:68] layer 26_q @ epoch 4 new loss 3.660549919004552e-05 old loss 3.701079185702838e-05 BETTER
I0326 05:00:44.113582 1382443 finetune.py:68] layer 27_q @ epoch 3 new loss 3.987924355897121e-05 old loss 4.0333114156965166e-05 BETTER
I0326 05:00:45.604606 1381863 finetune.py:45] layer 26_k initial loss 3.97565308958292e-05
I0326 05:01:18.747794 1382443 finetune.py:68] layer 27_q @ epoch 4 new loss 3.947857840103097e-05 old loss 3.987924355897121e-05 BETTER
I0326 05:01:21.504407 1381863 finetune.py:68] layer 26_k @ epoch 0 new loss 3.91853682231158e-05 old loss 3.97565308958292e-05 BETTER
I0326 05:01:26.415223 1382443 finetune.py:45] layer 27_k initial loss 4.5503791625378653e-05
I0326 05:01:57.930139 1381863 finetune.py:68] layer 26_k @ epoch 1 new loss 3.889441359206103e-05 old loss 3.91853682231158e-05 BETTER
I0326 05:02:00.080263 1382443 finetune.py:68] layer 27_k @ epoch 0 new loss 4.462791184778325e-05 old loss 4.5503791625378653e-05 BETTER
I0326 05:02:34.584182 1382443 finetune.py:68] layer 27_k @ epoch 1 new loss 4.419919059728272e-05 old loss 4.462791184778325e-05 BETTER
I0326 05:02:34.600727 1381863 finetune.py:68] layer 26_k @ epoch 2 new loss 3.861756340484135e-05 old loss 3.889441359206103e-05 BETTER
I0326 05:03:09.125479 1382443 finetune.py:68] layer 27_k @ epoch 2 new loss 4.393848212203011e-05 old loss 4.419919059728272e-05 BETTER
I0326 05:03:11.337064 1381863 finetune.py:68] layer 26_k @ epoch 3 new loss 3.8416434108512476e-05 old loss 3.861756340484135e-05 BETTER
I0326 05:03:43.864835 1382443 finetune.py:68] layer 27_k @ epoch 3 new loss 4.374142736196518e-05 old loss 4.393848212203011e-05 BETTER
I0326 05:03:48.124873 1381863 finetune.py:68] layer 26_k @ epoch 4 new loss 3.8218178815441206e-05 old loss 3.8416434108512476e-05 BETTER
I0326 05:03:57.822672 1381863 finetune.py:45] layer 26_o initial loss 0.0001138640072895214
I0326 05:04:18.391113 1382443 finetune.py:68] layer 27_k @ epoch 4 new loss 4.356962381280027e-05 old loss 4.374142736196518e-05 BETTER
I0326 05:04:27.752551 1382443 finetune.py:45] layer 27_o initial loss 0.000132826273329556
I0326 05:04:33.034324 1381863 finetune.py:68] layer 26_o @ epoch 0 new loss 0.00010554093023529276 old loss 0.0001138640072895214 BETTER
I0326 05:05:00.966509 1382443 finetune.py:68] layer 27_o @ epoch 0 new loss 0.00012338673695921898 old loss 0.000132826273329556 BETTER
I0326 05:05:09.011092 1381863 finetune.py:68] layer 26_o @ epoch 1 new loss 0.00010263289004797116 old loss 0.00010554093023529276 BETTER
I0326 05:05:35.140938 1382443 finetune.py:68] layer 27_o @ epoch 1 new loss 0.00012024942407151684 old loss 0.00012338673695921898 BETTER
I0326 05:05:45.267370 1381863 finetune.py:68] layer 26_o @ epoch 2 new loss 0.00010065985406981781 old loss 0.00010263289004797116 BETTER
I0326 05:06:09.298089 1382443 finetune.py:68] layer 27_o @ epoch 2 new loss 0.00011812189768534154 old loss 0.00012024942407151684 BETTER
I0326 05:06:21.579191 1381863 finetune.py:68] layer 26_o @ epoch 3 new loss 9.914193651638925e-05 old loss 0.00010065985406981781 BETTER
I0326 05:06:43.384875 1382443 finetune.py:68] layer 27_o @ epoch 3 new loss 0.00011649171938188374 old loss 0.00011812189768534154 BETTER
I0326 05:06:57.873961 1381863 finetune.py:68] layer 26_o @ epoch 4 new loss 9.791131014935672e-05 old loss 9.914193651638925e-05 BETTER
I0326 05:07:17.447607 1382443 finetune.py:68] layer 27_o @ epoch 4 new loss 0.00011526218440849334 old loss 0.00011649171938188374 BETTER
I0326 05:07:19.400196 1381863 finetune.py:45] layer 26_up initial loss 0.00027594223502092063
I0326 05:07:38.786129 1382443 finetune.py:45] layer 27_up initial loss 0.0003226604894734919
I0326 05:07:51.638968 1381863 finetune.py:68] layer 26_up @ epoch 0 new loss 0.0002667300868779421 old loss 0.00027594223502092063 BETTER
I0326 05:08:09.505490 1382443 finetune.py:68] layer 27_up @ epoch 0 new loss 0.00031052459962666035 old loss 0.0003226604894734919 BETTER
I0326 05:08:24.813458 1381863 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0002622377942316234 old loss 0.0002667300868779421 BETTER
I0326 05:08:41.293312 1382443 finetune.py:68] layer 27_up @ epoch 1 new loss 0.00030510107171721756 old loss 0.00031052459962666035 BETTER
I0326 05:08:58.525243 1381863 finetune.py:68] layer 26_up @ epoch 2 new loss 0.00025886044022627175 old loss 0.0002622377942316234 BETTER
I0326 05:09:13.435922 1382443 finetune.py:68] layer 27_up @ epoch 2 new loss 0.00030110508669167757 old loss 0.00030510107171721756 BETTER
I0326 05:09:32.242196 1381863 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0002560539578553289 old loss 0.00025886044022627175 BETTER
I0326 05:09:45.517380 1382443 finetune.py:68] layer 27_up @ epoch 3 new loss 0.00029784927028231323 old loss 0.00030110508669167757 BETTER
I0326 05:10:06.098834 1381863 finetune.py:68] layer 26_up @ epoch 4 new loss 0.00025367308990098536 old loss 0.0002560539578553289 BETTER
I0326 05:10:17.587887 1382443 finetune.py:68] layer 27_up @ epoch 4 new loss 0.00029505888232961297 old loss 0.00029784927028231323 BETTER
I0326 05:10:27.736650 1381863 finetune.py:45] layer 26_gate initial loss 0.0003323837008792907
I0326 05:10:39.230532 1382443 finetune.py:45] layer 27_gate initial loss 0.00039215717697516084
I0326 05:10:57.971836 1381863 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0003295870264992118 old loss 0.0003323837008792907 BETTER
I0326 05:11:07.923980 1382443 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0003883899771608412 old loss 0.00039215717697516084 BETTER
I0326 05:11:29.179529 1381863 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.0003274767950642854 old loss 0.0003295870264992118 BETTER
I0326 05:11:37.278011 1382443 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.0003857869596686214 old loss 0.0003883899771608412 BETTER
I0326 05:12:00.631609 1381863 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.00032567698508501053 old loss 0.0003274767950642854 BETTER
I0326 05:12:06.874403 1382443 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.00038362201303243637 old loss 0.0003857869596686214 BETTER
I0326 05:12:32.216371 1381863 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0003240691148675978 old loss 0.00032567698508501053 BETTER
I0326 05:12:36.621415 1382443 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0003817327960859984 old loss 0.00038362201303243637 BETTER
I0326 05:13:03.789167 1381863 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0003226661356166005 old loss 0.0003240691148675978 BETTER
I0326 05:13:06.428231 1382443 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.0003800739941652864 old loss 0.0003817327960859984 BETTER
I0326 05:13:26.531229 1381863 finetune.py:45] layer 26_down initial loss 0.0005363244563341141
I0326 05:13:28.637964 1382443 finetune.py:45] layer 27_down initial loss 0.0006420836434699595
I0326 05:13:54.446308 1381863 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0005362753290683031 old loss 0.0005363244563341141 BETTER
I0326 05:13:55.329808 1382443 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0006420117570087314 old loss 0.0006420836434699595 BETTER
I0326 05:14:22.981621 1382443 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0006419530254788697 old loss 0.0006420117570087314 BETTER
I0326 05:14:23.788603 1381863 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0005362325464375317 old loss 0.0005362753290683031 BETTER
I0326 05:14:50.645087 1382443 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0006419025012291968 old loss 0.0006419530254788697 BETTER
I0326 05:14:53.015214 1381863 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0005361945368349552 old loss 0.0005362325464375317 BETTER
I0326 05:15:18.343086 1382443 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0006418597185984254 old loss 0.0006419025012291968 BETTER
I0326 05:15:22.369779 1381863 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0005361612420529127 old loss 0.0005361945368349552 BETTER
I0326 05:15:46.058359 1382443 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0006418190314434469 old loss 0.0006418597185984254 BETTER
27_v proxy err 0.013694045133888721 tr(WHW.T) 677.69384765625
bpp_loss 3.191537857055664
27_q proxy err 0.0018615680746734142 tr(WHW.T) 21320.017578125
bpp_loss 3.462545156478882
27_k proxy err 0.0007922175573185086 tr(WHW.T) 14044.283203125
bpp_loss 4.216889381408691
27_o proxy err 0.018631037324666977 tr(WHW.T) 2162.01123046875
bpp_loss 3.0336549282073975
27_up proxy err 0.019293170422315598 tr(WHW.T) 8479.1240234375
bpp_loss 3.0058788572038924
27_gate proxy err 0.00539393862709403 tr(WHW.T) 32761.208984375
bpp_loss 3.356621742248535
27_down proxy err 0.018470527604222298 tr(WHW.T) 6627.17626953125
bpp_loss 2.9980625084468295
I0326 05:15:51.684295 1381863 finetune.py:68] layer 26_down @ epoch 4 new loss 0.0005361315561458468 old loss 0.0005361612420529127 BETTER
26_v proxy err 0.01988927274942398 tr(WHW.T) 434.54583740234375
bpp_loss 3.1048452854156494
26_q proxy err 0.0017607727786526084 tr(WHW.T) 21414.78515625
bpp_loss 3.4909974336624146
26_k proxy err 0.0006860221619717777 tr(WHW.T) 15456.8486328125
bpp_loss 4.259861469268799
26_o proxy err 0.01628684252500534 tr(WHW.T) 2392.3818359375
bpp_loss 2.9934065341949463
26_up proxy err 0.021189739927649498 tr(WHW.T) 7652.41259765625
bpp_loss 2.990966251918248
26_gate proxy err 0.00607317266985774 tr(WHW.T) 28857.29296875
bpp_loss 3.3472016198294505
26_down proxy err 0.021287329494953156 tr(WHW.T) 6681.07421875
bpp_loss 2.9876749515533447
I0326 05:16:59.994404 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 28 in 63.437443017959595s
I0326 05:17:03.215656 1393090 config.py:54] PyTorch version 2.6.0 available.
W0326 05:17:03.496733 1393090 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 05:17:04.371004 1393090 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 05:17:04.374916 1258494 quantize_finetune_llama.py:209] layer 29 gpu 1
I0326 05:17:04.387922 1393090 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 05:17:11.712229 1393090 finetune.py:45] layer 28_v initial loss 9.127727389568463e-05
W0326 05:17:11.712435 1393090 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 05:17:47.749167 1393090 finetune.py:68] layer 28_v @ epoch 0 new loss 5.034704736317508e-05 old loss 9.127727389568463e-05 BETTER
I0326 05:18:04.752592 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 29 in 59.964786529541016s
I0326 05:18:08.225840 1393672 config.py:54] PyTorch version 2.6.0 available.
W0326 05:18:08.499783 1393672 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 05:18:09.387991 1393672 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 05:18:09.391896 1258494 quantize_finetune_llama.py:209] layer 30 gpu 0
I0326 05:18:09.405237 1393672 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 05:18:16.230648 1393672 finetune.py:45] layer 29_v initial loss 9.133299317909405e-05
W0326 05:18:16.230929 1393672 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 05:18:24.600803 1393090 finetune.py:68] layer 28_v @ epoch 1 new loss 4.711808651336469e-05 old loss 5.034704736317508e-05 BETTER
I0326 05:18:49.700994 1393672 finetune.py:68] layer 29_v @ epoch 0 new loss 5.548568879021332e-05 old loss 9.133299317909405e-05 BETTER
I0326 05:19:01.621361 1393090 finetune.py:68] layer 28_v @ epoch 2 new loss 4.549077493720688e-05 old loss 4.711808651336469e-05 BETTER
I0326 05:19:24.171438 1393672 finetune.py:68] layer 29_v @ epoch 1 new loss 5.2560673793777823e-05 old loss 5.548568879021332e-05 BETTER
I0326 05:19:38.654508 1393090 finetune.py:68] layer 28_v @ epoch 3 new loss 4.416620868141763e-05 old loss 4.549077493720688e-05 BETTER
I0326 05:19:58.861398 1393672 finetune.py:68] layer 29_v @ epoch 2 new loss 5.096934546600096e-05 old loss 5.2560673793777823e-05 BETTER
I0326 05:20:15.605220 1393090 finetune.py:68] layer 28_v @ epoch 4 new loss 4.367763904156163e-05 old loss 4.416620868141763e-05 BETTER
I0326 05:20:25.223369 1393090 finetune.py:45] layer 28_q initial loss 6.080911771277897e-05
I0326 05:20:33.870859 1393672 finetune.py:68] layer 29_v @ epoch 3 new loss 5.066148878540844e-05 old loss 5.096934546600096e-05 BETTER
I0326 05:21:01.528337 1393090 finetune.py:68] layer 28_q @ epoch 0 new loss 5.6385051721008494e-05 old loss 6.080911771277897e-05 BETTER
I0326 05:21:08.996286 1393672 finetune.py:68] layer 29_v @ epoch 4 new loss 5.011069879401475e-05 old loss 5.066148878540844e-05 BETTER
I0326 05:21:18.324007 1393672 finetune.py:45] layer 29_q initial loss 9.460491128265858e-05
I0326 05:21:38.475013 1393090 finetune.py:68] layer 28_q @ epoch 1 new loss 5.5033440730767325e-05 old loss 5.6385051721008494e-05 BETTER
I0326 05:21:52.164603 1393672 finetune.py:68] layer 29_q @ epoch 0 new loss 8.286393131129444e-05 old loss 9.460491128265858e-05 BETTER
I0326 05:22:15.337660 1393090 finetune.py:68] layer 28_q @ epoch 2 new loss 5.403931572800502e-05 old loss 5.5033440730767325e-05 BETTER
I0326 05:22:26.888188 1393672 finetune.py:68] layer 29_q @ epoch 1 new loss 7.951161387609318e-05 old loss 8.286393131129444e-05 BETTER
I0326 05:22:52.241399 1393090 finetune.py:68] layer 28_q @ epoch 3 new loss 5.325486199581064e-05 old loss 5.403931572800502e-05 BETTER
I0326 05:23:01.809989 1393672 finetune.py:68] layer 29_q @ epoch 2 new loss 7.734246901236475e-05 old loss 7.951161387609318e-05 BETTER
I0326 05:23:29.099774 1393090 finetune.py:68] layer 28_q @ epoch 4 new loss 5.264686114969663e-05 old loss 5.325486199581064e-05 BETTER
I0326 05:23:36.690230 1393672 finetune.py:68] layer 29_q @ epoch 3 new loss 7.557664503110573e-05 old loss 7.734246901236475e-05 BETTER
I0326 05:23:36.866569 1393090 finetune.py:45] layer 28_k initial loss 5.887201405130327e-05
I0326 05:24:11.532499 1393672 finetune.py:68] layer 29_q @ epoch 4 new loss 7.434980216203257e-05 old loss 7.557664503110573e-05 BETTER
I0326 05:24:13.156227 1393090 finetune.py:68] layer 28_k @ epoch 0 new loss 5.7956582168117166e-05 old loss 5.887201405130327e-05 BETTER
I0326 05:24:19.291681 1393672 finetune.py:45] layer 29_k initial loss 8.358771447092295e-05
I0326 05:24:50.135205 1393090 finetune.py:68] layer 28_k @ epoch 1 new loss 5.7457724324194714e-05 old loss 5.7956582168117166e-05 BETTER
I0326 05:24:53.668313 1393672 finetune.py:68] layer 29_k @ epoch 0 new loss 8.180913573596627e-05 old loss 8.358771447092295e-05 BETTER
I0326 05:25:26.794843 1393090 finetune.py:68] layer 28_k @ epoch 2 new loss 5.71207856410183e-05 old loss 5.7457724324194714e-05 BETTER
I0326 05:25:28.352258 1393672 finetune.py:68] layer 29_k @ epoch 1 new loss 8.087327296379954e-05 old loss 8.180913573596627e-05 BETTER
I0326 05:26:02.849978 1393672 finetune.py:68] layer 29_k @ epoch 2 new loss 8.025964052649215e-05 old loss 8.087327296379954e-05 BETTER
I0326 05:26:03.645746 1393090 finetune.py:68] layer 28_k @ epoch 3 new loss 5.674288695445284e-05 old loss 5.71207856410183e-05 BETTER
I0326 05:26:37.444514 1393672 finetune.py:68] layer 29_k @ epoch 3 new loss 7.96253007138148e-05 old loss 8.025964052649215e-05 BETTER
I0326 05:26:40.556909 1393090 finetune.py:68] layer 28_k @ epoch 4 new loss 5.654849883285351e-05 old loss 5.674288695445284e-05 BETTER
I0326 05:26:50.187345 1393090 finetune.py:45] layer 28_o initial loss 0.00017330468108411878
I0326 05:27:12.071671 1393672 finetune.py:68] layer 29_k @ epoch 4 new loss 7.931108848424628e-05 old loss 7.96253007138148e-05 BETTER
I0326 05:27:21.798132 1393672 finetune.py:45] layer 29_o initial loss 0.00020464652334339917
I0326 05:27:25.489520 1393090 finetune.py:68] layer 28_o @ epoch 0 new loss 0.00015886107576079667 old loss 0.00017330468108411878 BETTER
I0326 05:27:54.859930 1393672 finetune.py:68] layer 29_o @ epoch 0 new loss 0.00018741999519988894 old loss 0.00020464652334339917 BETTER
I0326 05:28:01.614754 1393090 finetune.py:68] layer 28_o @ epoch 1 new loss 0.00015377748059108853 old loss 0.00015886107576079667 BETTER
I0326 05:28:28.644796 1393672 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0001820104371290654 old loss 0.00018741999519988894 BETTER
I0326 05:28:37.518057 1393090 finetune.py:68] layer 28_o @ epoch 2 new loss 0.00015015136159490794 old loss 0.00015377748059108853 BETTER
I0326 05:29:02.452054 1393672 finetune.py:68] layer 29_o @ epoch 2 new loss 0.00017868196300696582 old loss 0.0001820104371290654 BETTER
I0326 05:29:13.516397 1393090 finetune.py:68] layer 28_o @ epoch 3 new loss 0.0001474105956731364 old loss 0.00015015136159490794 BETTER
I0326 05:29:36.212263 1393672 finetune.py:68] layer 29_o @ epoch 3 new loss 0.00017598693375475705 old loss 0.00017868196300696582 BETTER
I0326 05:29:49.447855 1393090 finetune.py:68] layer 28_o @ epoch 4 new loss 0.00014520279364660382 old loss 0.0001474105956731364 BETTER
I0326 05:30:09.938193 1393672 finetune.py:68] layer 29_o @ epoch 4 new loss 0.00017377373296767473 old loss 0.00017598693375475705 BETTER
I0326 05:30:10.982342 1393090 finetune.py:45] layer 28_up initial loss 0.0004078115161973983
I0326 05:30:31.152342 1393672 finetune.py:45] layer 29_up initial loss 0.0005487682647071779
I0326 05:30:43.114055 1393090 finetune.py:68] layer 28_up @ epoch 0 new loss 0.0003893445827998221 old loss 0.0004078115161973983 BETTER
I0326 05:31:01.862726 1393672 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0005170376389287412 old loss 0.0005487682647071779 BETTER
I0326 05:31:16.608948 1393090 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0003816948155872524 old loss 0.0003893445827998221 BETTER
I0326 05:31:33.826307 1393672 finetune.py:68] layer 29_up @ epoch 1 new loss 0.0005052833002991974 old loss 0.0005170376389287412 BETTER
I0326 05:31:50.544744 1393090 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0003761223633773625 old loss 0.0003816948155872524 BETTER
I0326 05:32:05.929219 1393672 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0004968504654243588 old loss 0.0005052833002991974 BETTER
I0326 05:32:24.605876 1393090 finetune.py:68] layer 28_up @ epoch 3 new loss 0.00037162465741857886 old loss 0.0003761223633773625 BETTER
I0326 05:32:38.071392 1393672 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0004899342893622816 old loss 0.0004968504654243588 BETTER
I0326 05:32:58.463157 1393090 finetune.py:68] layer 28_up @ epoch 4 new loss 0.00036776650813408196 old loss 0.00037162465741857886 BETTER
I0326 05:33:10.186868 1393672 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0004843675415031612 old loss 0.0004899342893622816 BETTER
I0326 05:33:20.326383 1393090 finetune.py:45] layer 28_gate initial loss 0.0004957822384312749
I0326 05:33:31.570100 1393672 finetune.py:45] layer 29_gate initial loss 0.0006606538663618267
I0326 05:33:50.689109 1393090 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0004900682251900434 old loss 0.0004957822384312749 BETTER
I0326 05:34:00.391594 1393672 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0006513239932246506 old loss 0.0006606538663618267 BETTER
I0326 05:34:22.104266 1393090 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0004863925860263407 old loss 0.0004900682251900434 BETTER
I0326 05:34:29.953466 1393672 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.0006457637064158916 old loss 0.0006513239932246506 BETTER
I0326 05:34:53.525520 1393090 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.00048320755013264716 old loss 0.0004863925860263407 BETTER
I0326 05:34:59.720967 1393672 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.000641411985270679 old loss 0.0006457637064158916 BETTER
I0326 05:35:24.948297 1393090 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0004806037759408355 old loss 0.00048320755013264716 BETTER
I0326 05:35:29.485841 1393672 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0006375475786626339 old loss 0.000641411985270679 BETTER
I0326 05:35:56.424046 1393090 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.00047820768668316305 old loss 0.0004806037759408355 BETTER
I0326 05:35:59.188656 1393672 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.000634306576102972 old loss 0.0006375475786626339 BETTER
I0326 05:36:19.085396 1393090 finetune.py:45] layer 28_down initial loss 0.000829414464533329
I0326 05:36:21.343273 1393672 finetune.py:45] layer 29_down initial loss 0.001130912103690207
I0326 05:36:47.380666 1393090 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0008293191203847528 old loss 0.000829414464533329 BETTER
I0326 05:36:48.152175 1393672 finetune.py:68] layer 29_down @ epoch 0 new loss 0.001130647142417729 old loss 0.001130912103690207 BETTER
I0326 05:37:15.738420 1393672 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0011304327053949237 old loss 0.001130647142417729 BETTER
I0326 05:37:16.512969 1393090 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0008292316342703998 old loss 0.0008293191203847528 BETTER
I0326 05:37:43.378306 1393672 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0011302619241178036 old loss 0.0011304327053949237 BETTER
I0326 05:37:45.711416 1393090 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0008291543927043676 old loss 0.0008292316342703998 BETTER
I0326 05:38:11.046751 1393672 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0011301117483526468 old loss 0.0011302619241178036 BETTER
I0326 05:38:14.955325 1393090 finetune.py:68] layer 28_down @ epoch 3 new loss 0.0008290846017189324 old loss 0.0008291543927043676 BETTER
I0326 05:38:38.812116 1393672 finetune.py:68] layer 29_down @ epoch 4 new loss 0.0011299708858132362 old loss 0.0011301117483526468 BETTER
29_v proxy err 0.012052749283611774 tr(WHW.T) 850.4290161132812
bpp_loss 3.303333878517151
29_q proxy err 0.002128331921994686 tr(WHW.T) 20659.447265625
bpp_loss 3.464329719543457
29_k proxy err 0.0007535289041697979 tr(WHW.T) 16413.349609375
bpp_loss 4.259103536605835
29_o proxy err 0.011118665337562561 tr(WHW.T) 3110.4033203125
bpp_loss 3.0940483808517456
29_up proxy err 0.01278185285627842 tr(WHW.T) 12858.0390625
bpp_loss 3.0586471557617188
29_gate proxy err 0.004573336336761713 tr(WHW.T) 38262.15625
bpp_loss 3.3337278366088867
29_down proxy err 0.01593877002596855 tr(WHW.T) 7569.83251953125
bpp_loss 3.02508476802281
I0326 05:38:44.365360 1393090 finetune.py:68] layer 28_down @ epoch 4 new loss 0.000829018244985491 old loss 0.0008290846017189324 BETTER
28_v proxy err 0.01549232006072998 tr(WHW.T) 601.4844360351562
bpp_loss 3.2381471395492554
28_q proxy err 0.0017294017598032951 tr(WHW.T) 23182.099609375
bpp_loss 3.4729233980178833
28_k proxy err 0.0007407473167404532 tr(WHW.T) 15032.2666015625
bpp_loss 4.182328462600708
28_o proxy err 0.016269654035568237 tr(WHW.T) 2520.239990234375
bpp_loss 3.061489701271057
28_up proxy err 0.0158582404255867 tr(WHW.T) 10249.9443359375
bpp_loss 3.0267814908708846
28_gate proxy err 0.004864963702857494 tr(WHW.T) 35859.140625
bpp_loss 3.3426480974469865
28_down proxy err 0.01833811029791832 tr(WHW.T) 7296.0439453125
bpp_loss 3.0120692934308733
I0326 05:39:52.779891 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 30 in 63.52716636657715s
I0326 05:39:55.995074 1404318 config.py:54] PyTorch version 2.6.0 available.
W0326 05:39:56.275761 1404318 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 05:39:57.154014 1404318 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 05:39:57.158058 1258494 quantize_finetune_llama.py:209] layer 31 gpu 1
I0326 05:39:57.172825 1404318 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 05:40:04.486388 1404318 finetune.py:45] layer 30_v initial loss 0.0001642741699470207
W0326 05:40:04.486656 1404318 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 05:40:40.537892 1404318 finetune.py:68] layer 30_v @ epoch 0 new loss 0.00010395493154646829 old loss 0.0001642741699470207 BETTER
I0326 05:40:57.657101 1258494 quantize_finetune_llama.py:240] computed original embedding for layer 31 in 60.08300542831421s
I0326 05:41:01.094366 1404897 config.py:54] PyTorch version 2.6.0 available.
W0326 05:41:01.380356 1404897 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0326 05:41:02.296084 1404897 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0326 05:41:02.313217 1404897 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0326 05:41:09.154508 1404897 finetune.py:45] layer 31_v initial loss 0.0003513468836899847
W0326 05:41:09.154785 1404897 warnings.py:109] /workspace/Weight_compression/comp_lm_qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0326 05:41:17.632887 1404318 finetune.py:68] layer 30_v @ epoch 1 new loss 9.708157449495047e-05 old loss 0.00010395493154646829 BETTER
I0326 05:41:42.694456 1404897 finetune.py:68] layer 31_v @ epoch 0 new loss 0.00018022608128376305 old loss 0.0003513468836899847 BETTER
I0326 05:41:54.778874 1404318 finetune.py:68] layer 30_v @ epoch 2 new loss 9.366049926029518e-05 old loss 9.708157449495047e-05 BETTER
I0326 05:42:17.269700 1404897 finetune.py:76] layer 31_v @ epoch 1 new loss 0.0002143776509910822 old loss 0.00018022608128376305 WORSE
I0326 05:42:31.729207 1404318 finetune.py:68] layer 30_v @ epoch 3 new loss 9.219833737006411e-05 old loss 9.366049926029518e-05 BETTER
I0326 05:42:51.517716 1404897 finetune.py:68] layer 31_v @ epoch 2 new loss 0.00017505350115243345 old loss 0.00018022608128376305 BETTER
I0326 05:43:08.708329 1404318 finetune.py:68] layer 30_v @ epoch 4 new loss 8.974374213721603e-05 old loss 9.219833737006411e-05 BETTER
I0326 05:43:18.308706 1404318 finetune.py:45] layer 30_q initial loss 0.00012468185741454363
I0326 05:43:26.565055 1404897 finetune.py:68] layer 31_v @ epoch 3 new loss 0.00016167009016498923 old loss 0.00017505350115243345 BETTER
I0326 05:43:54.384648 1404318 finetune.py:68] layer 30_q @ epoch 0 new loss 0.00011424379772506654 old loss 0.00012468185741454363 BETTER
I0326 05:44:01.657197 1404897 finetune.py:68] layer 31_v @ epoch 4 new loss 0.0001544160331832245 old loss 0.00016167009016498923 BETTER
I0326 05:44:11.010736 1404897 finetune.py:45] layer 31_q initial loss 0.00026758696185424924
I0326 05:44:31.317464 1404318 finetune.py:68] layer 30_q @ epoch 1 new loss 0.0001116563071263954 old loss 0.00011424379772506654 BETTER
I0326 05:44:44.801214 1404897 finetune.py:68] layer 31_q @ epoch 0 new loss 0.0002178874856326729 old loss 0.00026758696185424924 BETTER
I0326 05:45:08.007378 1404318 finetune.py:68] layer 30_q @ epoch 2 new loss 0.00010992924944730476 old loss 0.0001116563071263954 BETTER
I0326 05:45:19.370225 1404897 finetune.py:68] layer 31_q @ epoch 1 new loss 0.00019909338152501732 old loss 0.0002178874856326729 BETTER
I0326 05:45:44.675101 1404318 finetune.py:68] layer 30_q @ epoch 3 new loss 0.00010845594806596637 old loss 0.00010992924944730476 BETTER
I0326 05:45:53.781330 1404897 finetune.py:68] layer 31_q @ epoch 2 new loss 0.00018422673747409135 old loss 0.00019909338152501732 BETTER
I0326 05:46:21.481620 1404318 finetune.py:68] layer 30_q @ epoch 4 new loss 0.00010761822341009974 old loss 0.00010845594806596637 BETTER
I0326 05:46:28.271013 1404897 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0001781372120603919 old loss 0.00018422673747409135 BETTER
I0326 05:46:29.423193 1404318 finetune.py:45] layer 30_k initial loss 0.00012462316954042763
I0326 05:47:02.735176 1404897 finetune.py:68] layer 31_q @ epoch 4 new loss 0.0001746647321851924 old loss 0.0001781372120603919 BETTER
I0326 05:47:05.249349 1404318 finetune.py:68] layer 30_k @ epoch 0 new loss 0.00012117460573790595 old loss 0.00012462316954042763 BETTER
I0326 05:47:10.338740 1404897 finetune.py:45] layer 31_k initial loss 0.00024210664560087025
I0326 05:47:41.767865 1404318 finetune.py:68] layer 30_k @ epoch 1 new loss 0.00012097063881810755 old loss 0.00012117460573790595 BETTER
I0326 05:47:44.165051 1404897 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0002086689928546548 old loss 0.00024210664560087025 BETTER
I0326 05:48:18.483678 1404318 finetune.py:76] layer 30_k @ epoch 2 new loss 0.00012284916010685265 old loss 0.00012097063881810755 WORSE
I0326 05:48:18.685189 1404897 finetune.py:68] layer 31_k @ epoch 1 new loss 0.00020253934781067073 old loss 0.0002086689928546548 BETTER
I0326 05:48:53.116378 1404897 finetune.py:68] layer 31_k @ epoch 2 new loss 0.00019995338516309857 old loss 0.00020253934781067073 BETTER
I0326 05:48:54.744272 1404318 finetune.py:68] layer 30_k @ epoch 3 new loss 0.00011894700583070517 old loss 0.00012097063881810755 BETTER
I0326 05:49:27.744560 1404897 finetune.py:68] layer 31_k @ epoch 3 new loss 0.00019616575445979834 old loss 0.00019995338516309857 BETTER
I0326 05:49:31.686995 1404318 finetune.py:76] layer 30_k @ epoch 4 new loss 0.00011936304508708417 old loss 0.00011894700583070517 WORSE
I0326 05:49:40.886933 1404318 finetune.py:45] layer 30_o initial loss 0.0003475823614280671
I0326 05:50:02.290509 1404897 finetune.py:76] layer 31_k @ epoch 4 new loss 0.00019714517111424357 old loss 0.00019616575445979834 WORSE
I0326 05:50:11.164474 1404897 finetune.py:45] layer 31_o initial loss 0.0008992103976197541
I0326 05:50:15.892472 1404318 finetune.py:68] layer 30_o @ epoch 0 new loss 0.00031302147544920444 old loss 0.0003475823614280671 BETTER
I0326 05:50:44.418755 1404897 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0005688836681656539 old loss 0.0008992103976197541 BETTER
I0326 05:50:51.672902 1404318 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0003041406162083149 old loss 0.00031302147544920444 BETTER
I0326 05:51:18.637691 1404897 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0005249091773293912 old loss 0.0005688836681656539 BETTER
I0326 05:51:27.499392 1404318 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0002979389682877809 old loss 0.0003041406162083149 BETTER
I0326 05:51:52.662630 1404897 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0004979507648386061 old loss 0.0005249091773293912 BETTER
I0326 05:52:03.690560 1404318 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0002933406503871083 old loss 0.0002979389682877809 BETTER
I0326 05:52:26.458369 1404897 finetune.py:68] layer 31_o @ epoch 3 new loss 0.00047845582594163716 old loss 0.0004979507648386061 BETTER
I0326 05:52:39.923146 1404318 finetune.py:68] layer 30_o @ epoch 4 new loss 0.00028928936808370054 old loss 0.0002933406503871083 BETTER
I0326 05:53:00.367159 1404897 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0004643256834242493 old loss 0.00047845582594163716 BETTER
I0326 05:53:01.459906 1404318 finetune.py:45] layer 30_up initial loss 0.0010698022088035941
I0326 05:53:21.457834 1404897 finetune.py:45] layer 31_up initial loss 0.003638804890215397
I0326 05:53:34.052231 1404318 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0009721568203531206 old loss 0.0010698022088035941 BETTER
I0326 05:53:52.525937 1404897 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0028488195966929197 old loss 0.003638804890215397 BETTER
I0326 05:54:07.551435 1404318 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0009380393894389272 old loss 0.0009721568203531206 BETTER
I0326 05:54:24.562548 1404897 finetune.py:68] layer 31_up @ epoch 1 new loss 0.0026445596013218164 old loss 0.0028488195966929197 BETTER
I0326 05:54:41.469207 1404318 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0009147036471404135 old loss 0.0009380393894389272 BETTER
I0326 05:54:56.697059 1404897 finetune.py:68] layer 31_up @ epoch 2 new loss 0.002507931785658002 old loss 0.0026445596013218164 BETTER
I0326 05:55:15.285403 1404318 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0008960881968960166 old loss 0.0009147036471404135 BETTER
I0326 05:55:28.819123 1404897 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0024023023433983326 old loss 0.002507931785658002 BETTER
I0326 05:55:49.186374 1404318 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0008809060673229396 old loss 0.0008960881968960166 BETTER
I0326 05:56:00.910188 1404897 finetune.py:68] layer 31_up @ epoch 4 new loss 0.002315387362614274 old loss 0.0024023023433983326 BETTER
I0326 05:56:10.840009 1404318 finetune.py:45] layer 30_gate initial loss 0.0011589433997869492
I0326 05:56:22.322272 1404897 finetune.py:45] layer 31_gate initial loss 0.0030178488232195377
I0326 05:56:41.197129 1404318 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.001126458402723074 old loss 0.0011589433997869492 BETTER
I0326 05:56:51.073520 1404897 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.002799293724820018 old loss 0.0030178488232195377 BETTER
I0326 05:57:12.532076 1404318 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.0011122080031782389 old loss 0.001126458402723074 BETTER
I0326 05:57:20.860217 1404897 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.002718116622418165 old loss 0.002799293724820018 BETTER
I0326 05:57:44.046418 1404318 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0011009335285052657 old loss 0.0011122080031782389 BETTER
I0326 05:57:50.736326 1404897 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.002659558318555355 old loss 0.002718116622418165 BETTER
I0326 05:58:15.558741 1404318 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.001090974430553615 old loss 0.0011009335285052657 BETTER
I0326 05:58:20.698265 1404897 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.002610393799841404 old loss 0.002659558318555355 BETTER
I0326 05:58:47.165958 1404318 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.001082706730812788 old loss 0.001090974430553615 BETTER
I0326 05:58:50.512862 1404897 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.0025679070968180895 old loss 0.002610393799841404 BETTER
I0326 05:59:09.977751 1404318 finetune.py:45] layer 30_down initial loss 0.0020640073344111443
I0326 05:59:12.734623 1404897 finetune.py:45] layer 31_down initial loss 0.0072979251854121685
I0326 05:59:38.165350 1404318 finetune.py:68] layer 30_down @ epoch 0 new loss 0.00206277659162879 old loss 0.0020640073344111443 BETTER
I0326 05:59:39.677033 1404897 finetune.py:68] layer 31_down @ epoch 0 new loss 0.007287353742867708 old loss 0.0072979251854121685 BETTER
I0326 06:00:07.263804 1404318 finetune.py:68] layer 30_down @ epoch 1 new loss 0.0020617134869098663 old loss 0.00206277659162879 BETTER
I0326 06:00:07.314428 1404897 finetune.py:68] layer 31_down @ epoch 1 new loss 0.007280129939317703 old loss 0.007287353742867708 BETTER
I0326 06:00:34.968374 1404897 finetune.py:68] layer 31_down @ epoch 2 new loss 0.007275312207639217 old loss 0.007280129939317703 BETTER
I0326 06:00:36.551983 1404318 finetune.py:68] layer 30_down @ epoch 2 new loss 0.002060814993456006 old loss 0.0020617134869098663 BETTER
I0326 06:01:02.755146 1404897 finetune.py:68] layer 31_down @ epoch 3 new loss 0.00727164326235652 old loss 0.007275312207639217 BETTER
I0326 06:01:05.865587 1404318 finetune.py:68] layer 30_down @ epoch 3 new loss 0.0020600503776222467 old loss 0.002060814993456006 BETTER
I0326 06:01:30.658626 1404897 finetune.py:68] layer 31_down @ epoch 4 new loss 0.007268762681633234 old loss 0.00727164326235652 BETTER
31_v proxy err 0.006244642194360495 tr(WHW.T) 1808.723876953125
bpp_loss 3.397847890853882
31_q proxy err 0.0011193036334589124 tr(WHW.T) 46202.19921875
bpp_loss 3.5113250017166138
31_k proxy err 0.0007064933306537569 tr(WHW.T) 20551.330078125
bpp_loss 4.163848876953125
31_o proxy err 0.010172375477850437 tr(WHW.T) 2232.72509765625
bpp_loss 3.1298352479934692
31_up proxy err 0.0027900750283151865 tr(WHW.T) 69027.0234375
bpp_loss 3.268442153930664
31_gate proxy err 0.0014410061994567513 tr(WHW.T) 144227.8125
bpp_loss 3.574042728969029
31_down proxy err 0.0068992446176707745 tr(WHW.T) 10203.3251953125
bpp_loss 3.044839688709804
I0326 06:01:35.150139 1404318 finetune.py:68] layer 30_down @ epoch 4 new loss 0.002059377497062087 old loss 0.0020600503776222467 BETTER
30_v proxy err 0.011348276399075985 tr(WHW.T) 863.060791015625
bpp_loss 3.560143828392029
30_q proxy err 0.0016639853129163384 tr(WHW.T) 24067.41796875
bpp_loss 3.373095989227295
30_k proxy err 0.0007837163284420967 tr(WHW.T) 14080.142578125
bpp_loss 3.9581743478775024
30_o proxy err 0.008734168484807014 tr(WHW.T) 4880.0654296875
bpp_loss 3.1747266054153442
30_up proxy err 0.00786198303103447 tr(WHW.T) 21686.6328125
bpp_loss 3.088944980076381
30_gate proxy err 0.003504542401060462 tr(WHW.T) 51682.4453125
bpp_loss 3.3788062504359653
30_down proxy err 0.011215315200388432 tr(WHW.T) 8927.966796875
bpp_loss 3.024054629462106
I0326 06:01:54.003495 1415116 config.py:54] PyTorch version 2.6.0 available.
W0326 06:01:54.307238 1415116 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0326 06:01:54.546892 1415116 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.50it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  9.86it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00, 10.11it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 10.43it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 10.20it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00, 10.42it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00, 10.21it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00, 10.30it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00, 10.34it/s]
I0326 06:01:57.992369 1415116 hfize_llama.py:153] loaded layer 0
I0326 06:01:59.721167 1415116 hfize_llama.py:153] loaded layer 1
I0326 06:02:01.470719 1415116 hfize_llama.py:153] loaded layer 2
I0326 06:02:03.077251 1415116 hfize_llama.py:153] loaded layer 3
I0326 06:02:04.914783 1415116 hfize_llama.py:153] loaded layer 4
I0326 06:02:06.763906 1415116 hfize_llama.py:153] loaded layer 5
I0326 06:02:08.628713 1415116 hfize_llama.py:153] loaded layer 6
I0326 06:02:10.487784 1415116 hfize_llama.py:153] loaded layer 7
I0326 06:02:12.225461 1415116 hfize_llama.py:153] loaded layer 8
I0326 06:02:14.092294 1415116 hfize_llama.py:153] loaded layer 9
I0326 06:02:15.754642 1415116 hfize_llama.py:153] loaded layer 10
I0326 06:02:17.312698 1415116 hfize_llama.py:153] loaded layer 11
I0326 06:02:19.041678 1415116 hfize_llama.py:153] loaded layer 12
I0326 06:02:20.618749 1415116 hfize_llama.py:153] loaded layer 13
I0326 06:02:22.246968 1415116 hfize_llama.py:153] loaded layer 14
I0326 06:02:23.694549 1415116 hfize_llama.py:153] loaded layer 15
I0326 06:02:25.055085 1415116 hfize_llama.py:153] loaded layer 16
I0326 06:02:26.277438 1415116 hfize_llama.py:153] loaded layer 17
I0326 06:02:27.974467 1415116 hfize_llama.py:153] loaded layer 18
I0326 06:02:29.568563 1415116 hfize_llama.py:153] loaded layer 19
I0326 06:02:30.922652 1415116 hfize_llama.py:153] loaded layer 20
I0326 06:02:32.172241 1415116 hfize_llama.py:153] loaded layer 21
I0326 06:02:33.736431 1415116 hfize_llama.py:153] loaded layer 22
I0326 06:02:34.954647 1415116 hfize_llama.py:153] loaded layer 23
I0326 06:02:36.215151 1415116 hfize_llama.py:153] loaded layer 24
I0326 06:02:37.413478 1415116 hfize_llama.py:153] loaded layer 25
I0326 06:02:38.838274 1415116 hfize_llama.py:153] loaded layer 26
I0326 06:02:40.193100 1415116 hfize_llama.py:153] loaded layer 27
I0326 06:02:41.576702 1415116 hfize_llama.py:153] loaded layer 28
I0326 06:02:42.944773 1415116 hfize_llama.py:153] loaded layer 29
I0326 06:02:44.422289 1415116 hfize_llama.py:153] loaded layer 30
I0326 06:02:45.573083 1415116 hfize_llama.py:153] loaded layer 31
I0326 06:02:45.573265 1415116 hfize_llama.py:157] saving model...
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.04s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:01<00:04,  1.17it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:02<00:03,  1.23it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:03<00:02,  1.16it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:04<00:02,  1.07s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.01it/s]
I0326 06:03:21.925525 1415116 hfize_llama.py:167] successfully loaded hfized model
W0326 06:03:25.879873 1415945 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0326 06:03:26.411521 1415945 modeling.py:990] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:04,  1.22it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:01<00:04,  1.22it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:02<00:03,  1.20it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:03<00:02,  1.11it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.15s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.05s/it]
I0326 06:03:34.184298 1415945 config.py:54] PyTorch version 2.6.0 available.
  0%|          | 0/141 [00:00<?, ?it/s]avg_loss = 1.7388981580734253:   0%|          | 0/141 [00:01<?, ?it/s]avg_loss = 1.7388981580734253:   1%|          | 1/141 [00:01<04:28,  1.92s/it]avg_loss = 2.0074896216392517:   1%|          | 1/141 [00:03<04:28,  1.92s/it]avg_loss = 2.0074896216392517:   1%|▏         | 2/141 [00:03<03:57,  1.71s/it]avg_loss = 2.1248052517573037:   1%|▏         | 2/141 [00:05<03:57,  1.71s/it]avg_loss = 2.1248052517573037:   2%|▏         | 3/141 [00:05<03:47,  1.65s/it]avg_loss = 2.0728924572467804:   2%|▏         | 3/141 [00:06<03:47,  1.65s/it]avg_loss = 2.0728924572467804:   3%|▎         | 4/141 [00:06<03:42,  1.62s/it]avg_loss = 2.0253432273864744:   3%|▎         | 4/141 [00:08<03:42,  1.62s/it]avg_loss = 2.0253432273864744:   4%|▎         | 5/141 [00:08<03:38,  1.61s/it]avg_loss = 1.9267250498135884:   4%|▎         | 5/141 [00:09<03:38,  1.61s/it]avg_loss = 1.9267250498135884:   4%|▍         | 6/141 [00:09<03:36,  1.60s/it]avg_loss = 1.8648228304726737:   4%|▍         | 6/141 [00:11<03:36,  1.60s/it]avg_loss = 1.8648228304726737:   5%|▍         | 7/141 [00:11<03:34,  1.60s/it]avg_loss = 1.8609348237514496:   5%|▍         | 7/141 [00:13<03:34,  1.60s/it]avg_loss = 1.8609348237514496:   6%|▌         | 8/141 [00:13<03:32,  1.60s/it]avg_loss = 1.8941906558142767:   6%|▌         | 8/141 [00:14<03:32,  1.60s/it]avg_loss = 1.8941906558142767:   6%|▋         | 9/141 [00:14<03:31,  1.60s/it]avg_loss = 1.8923024654388427:   6%|▋         | 9/141 [00:16<03:31,  1.60s/it]avg_loss = 1.8923024654388427:   7%|▋         | 10/141 [00:16<03:30,  1.61s/it]avg_loss = 1.8860940824855457:   7%|▋         | 10/141 [00:17<03:30,  1.61s/it]avg_loss = 1.8860940824855457:   8%|▊         | 11/141 [00:17<03:29,  1.61s/it]avg_loss = 1.9081371128559113:   8%|▊         | 11/141 [00:19<03:29,  1.61s/it]avg_loss = 1.9081371128559113:   9%|▊         | 12/141 [00:19<03:27,  1.61s/it]avg_loss = 1.9210736659856944:   9%|▊         | 12/141 [00:21<03:27,  1.61s/it]avg_loss = 1.9210736659856944:   9%|▉         | 13/141 [00:21<03:26,  1.61s/it]avg_loss = 1.9379174964768546:   9%|▉         | 13/141 [00:22<03:26,  1.61s/it]avg_loss = 1.9379174964768546:  10%|▉         | 14/141 [00:22<03:25,  1.62s/it]avg_loss = 1.947532550493876:  10%|▉         | 14/141 [00:24<03:25,  1.62s/it] avg_loss = 1.947532550493876:  11%|█         | 15/141 [00:24<03:24,  1.62s/it]avg_loss = 1.9700855389237404:  11%|█         | 15/141 [00:25<03:24,  1.62s/it]avg_loss = 1.9700855389237404:  11%|█▏        | 16/141 [00:25<03:23,  1.63s/it]avg_loss = 1.9738387009676766:  11%|█▏        | 16/141 [00:27<03:23,  1.63s/it]avg_loss = 1.9738387009676766:  12%|█▏        | 17/141 [00:27<03:22,  1.63s/it]avg_loss = 1.9767053458425734:  12%|█▏        | 17/141 [00:29<03:22,  1.63s/it]avg_loss = 1.9767053458425734:  13%|█▎        | 18/141 [00:29<03:20,  1.63s/it]avg_loss = 1.9659027174899453:  13%|█▎        | 18/141 [00:30<03:20,  1.63s/it]avg_loss = 1.9659027174899453:  13%|█▎        | 19/141 [00:30<03:19,  1.64s/it]avg_loss = 1.965579879283905:  13%|█▎        | 19/141 [00:32<03:19,  1.64s/it] avg_loss = 1.965579879283905:  14%|█▍        | 20/141 [00:32<03:18,  1.64s/it]avg_loss = 1.9704598245166598:  14%|█▍        | 20/141 [00:34<03:18,  1.64s/it]avg_loss = 1.9704598245166598:  15%|█▍        | 21/141 [00:34<03:16,  1.64s/it]avg_loss = 1.9724486632780596:  15%|█▍        | 21/141 [00:35<03:16,  1.64s/it]avg_loss = 1.9724486632780596:  16%|█▌        | 22/141 [00:35<03:15,  1.64s/it]avg_loss = 1.9742930142775825:  16%|█▌        | 22/141 [00:37<03:15,  1.64s/it]avg_loss = 1.9742930142775825:  16%|█▋        | 23/141 [00:37<03:14,  1.65s/it]avg_loss = 1.979857196410497:  16%|█▋        | 23/141 [00:39<03:14,  1.65s/it] avg_loss = 1.979857196410497:  17%|█▋        | 24/141 [00:39<03:13,  1.65s/it]avg_loss = 1.984782886505127:  17%|█▋        | 24/141 [00:40<03:13,  1.65s/it]avg_loss = 1.984782886505127:  18%|█▊        | 25/141 [00:40<03:11,  1.65s/it]avg_loss = 1.9961627996884859:  18%|█▊        | 25/141 [00:42<03:11,  1.65s/it]avg_loss = 1.9961627996884859:  18%|█▊        | 26/141 [00:42<03:10,  1.66s/it]avg_loss = 2.0085217069696495:  18%|█▊        | 26/141 [00:44<03:10,  1.66s/it]avg_loss = 2.0085217069696495:  19%|█▉        | 27/141 [00:44<03:09,  1.66s/it]avg_loss = 2.0137540698051453:  19%|█▉        | 27/141 [00:45<03:09,  1.66s/it]avg_loss = 2.0137540698051453:  20%|█▉        | 28/141 [00:45<03:07,  1.66s/it]avg_loss = 2.0106545810041756:  20%|█▉        | 28/141 [00:47<03:07,  1.66s/it]avg_loss = 2.0106545810041756:  21%|██        | 29/141 [00:47<03:06,  1.66s/it]avg_loss = 2.0014370640118915:  21%|██        | 29/141 [00:49<03:06,  1.66s/it]avg_loss = 2.0014370640118915:  21%|██▏       | 30/141 [00:49<03:04,  1.66s/it]avg_loss = 1.9887274042252572:  21%|██▏       | 30/141 [00:50<03:04,  1.66s/it]avg_loss = 1.9887274042252572:  22%|██▏       | 31/141 [00:50<03:03,  1.67s/it]avg_loss = 1.9773573316633701:  22%|██▏       | 31/141 [00:52<03:03,  1.67s/it]avg_loss = 1.9773573316633701:  23%|██▎       | 32/141 [00:52<03:01,  1.67s/it]avg_loss = 1.9758100726387717:  23%|██▎       | 32/141 [00:54<03:01,  1.67s/it]avg_loss = 1.9758100726387717:  23%|██▎       | 33/141 [00:54<03:00,  1.67s/it]avg_loss = 1.9741701027926277:  23%|██▎       | 33/141 [00:55<03:00,  1.67s/it]avg_loss = 1.9741701027926277:  24%|██▍       | 34/141 [00:55<02:58,  1.67s/it]avg_loss = 1.9761200087411064:  24%|██▍       | 34/141 [00:57<02:58,  1.67s/it]avg_loss = 1.9761200087411064:  25%|██▍       | 35/141 [00:57<02:57,  1.67s/it]avg_loss = 1.9595226281219058:  25%|██▍       | 35/141 [00:59<02:57,  1.67s/it]avg_loss = 1.9595226281219058:  26%|██▌       | 36/141 [00:59<02:55,  1.68s/it]avg_loss = 1.9437081169437718:  26%|██▌       | 36/141 [01:00<02:55,  1.68s/it]avg_loss = 1.9437081169437718:  26%|██▌       | 37/141 [01:00<02:54,  1.68s/it]avg_loss = 1.927973355117597:  26%|██▌       | 37/141 [01:02<02:54,  1.68s/it] avg_loss = 1.927973355117597:  27%|██▋       | 38/141 [01:02<02:52,  1.68s/it]avg_loss = 1.9134407929885082:  27%|██▋       | 38/141 [01:04<02:52,  1.68s/it]avg_loss = 1.9134407929885082:  28%|██▊       | 39/141 [01:04<02:51,  1.68s/it]avg_loss = 1.9041453033685685:  28%|██▊       | 39/141 [01:05<02:51,  1.68s/it]avg_loss = 1.9041453033685685:  28%|██▊       | 40/141 [01:05<02:50,  1.68s/it]avg_loss = 1.9099885632352132:  28%|██▊       | 40/141 [01:07<02:50,  1.68s/it]avg_loss = 1.9099885632352132:  29%|██▉       | 41/141 [01:07<02:48,  1.69s/it]avg_loss = 1.926209032535553:  29%|██▉       | 41/141 [01:09<02:48,  1.69s/it] avg_loss = 1.926209032535553:  30%|██▉       | 42/141 [01:09<02:47,  1.69s/it]avg_loss = 1.9417138681855313:  30%|██▉       | 42/141 [01:11<02:47,  1.69s/it]avg_loss = 1.9417138681855313:  30%|███       | 43/141 [01:11<02:45,  1.69s/it]avg_loss = 1.946529965509068:  30%|███       | 43/141 [01:12<02:45,  1.69s/it] avg_loss = 1.946529965509068:  31%|███       | 44/141 [01:12<02:43,  1.69s/it]avg_loss = 1.9510750585132175:  31%|███       | 44/141 [01:14<02:43,  1.69s/it]avg_loss = 1.9510750585132175:  32%|███▏      | 45/141 [01:14<02:42,  1.69s/it]avg_loss = 1.9555487399515898:  32%|███▏      | 45/141 [01:16<02:42,  1.69s/it]avg_loss = 1.9555487399515898:  33%|███▎      | 46/141 [01:16<02:40,  1.69s/it]avg_loss = 1.96172101193286:  33%|███▎      | 46/141 [01:17<02:40,  1.69s/it]  avg_loss = 1.96172101193286:  33%|███▎      | 47/141 [01:17<02:39,  1.69s/it]avg_loss = 1.9644397621353467:  33%|███▎      | 47/141 [01:19<02:39,  1.69s/it]avg_loss = 1.9644397621353467:  34%|███▍      | 48/141 [01:19<02:37,  1.69s/it]avg_loss = 1.9638585411772436:  34%|███▍      | 48/141 [01:21<02:37,  1.69s/it]avg_loss = 1.9638585411772436:  35%|███▍      | 49/141 [01:21<02:35,  1.69s/it]avg_loss = 1.9639508438110351:  35%|███▍      | 49/141 [01:22<02:35,  1.69s/it]avg_loss = 1.9639508438110351:  35%|███▌      | 50/141 [01:22<02:34,  1.70s/it]avg_loss = 1.9583118078755397:  35%|███▌      | 50/141 [01:24<02:34,  1.70s/it]avg_loss = 1.9583118078755397:  36%|███▌      | 51/141 [01:24<02:32,  1.70s/it]avg_loss = 1.9543693776314075:  36%|███▌      | 51/141 [01:26<02:32,  1.70s/it]avg_loss = 1.9543693776314075:  37%|███▋      | 52/141 [01:26<02:31,  1.70s/it]avg_loss = 1.9477250283619143:  37%|███▋      | 52/141 [01:27<02:31,  1.70s/it]avg_loss = 1.9477250283619143:  38%|███▊      | 53/141 [01:27<02:29,  1.70s/it]avg_loss = 1.9443093869421217:  38%|███▊      | 53/141 [01:29<02:29,  1.70s/it]avg_loss = 1.9443093869421217:  38%|███▊      | 54/141 [01:29<02:27,  1.70s/it]avg_loss = 1.9363965619694103:  38%|███▊      | 54/141 [01:31<02:27,  1.70s/it]avg_loss = 1.9363965619694103:  39%|███▉      | 55/141 [01:31<02:26,  1.70s/it]avg_loss = 1.9284999306712831:  39%|███▉      | 55/141 [01:33<02:26,  1.70s/it]avg_loss = 1.9284999306712831:  40%|███▉      | 56/141 [01:33<02:24,  1.70s/it]avg_loss = 1.925128842654981:  40%|███▉      | 56/141 [01:34<02:24,  1.70s/it] avg_loss = 1.925128842654981:  40%|████      | 57/141 [01:34<02:22,  1.70s/it]avg_loss = 1.9220646496476799:  40%|████      | 57/141 [01:36<02:22,  1.70s/it]avg_loss = 1.9220646496476799:  41%|████      | 58/141 [01:36<02:21,  1.70s/it]avg_loss = 1.9240688388630496:  41%|████      | 58/141 [01:38<02:21,  1.70s/it]avg_loss = 1.9240688388630496:  42%|████▏     | 59/141 [01:38<02:19,  1.70s/it]avg_loss = 1.9291650931040445:  42%|████▏     | 59/141 [01:39<02:19,  1.70s/it]avg_loss = 1.9291650931040445:  43%|████▎     | 60/141 [01:39<02:17,  1.70s/it]avg_loss = 1.934363091578249:  43%|████▎     | 60/141 [01:41<02:17,  1.70s/it] avg_loss = 1.934363091578249:  43%|████▎     | 61/141 [01:41<02:16,  1.70s/it]avg_loss = 1.9412429601915422:  43%|████▎     | 61/141 [01:43<02:16,  1.70s/it]avg_loss = 1.9412429601915422:  44%|████▍     | 62/141 [01:43<02:14,  1.70s/it]avg_loss = 1.9327199591530695:  44%|████▍     | 62/141 [01:44<02:14,  1.70s/it]avg_loss = 1.9327199591530695:  45%|████▍     | 63/141 [01:44<02:12,  1.70s/it]avg_loss = 1.930416326969862:  45%|████▍     | 63/141 [01:46<02:12,  1.70s/it] avg_loss = 1.930416326969862:  45%|████▌     | 64/141 [01:46<02:11,  1.70s/it]avg_loss = 1.9279857048621545:  45%|████▌     | 64/141 [01:48<02:11,  1.70s/it]avg_loss = 1.9279857048621545:  46%|████▌     | 65/141 [01:48<02:09,  1.70s/it]avg_loss = 1.9219887653986614:  46%|████▌     | 65/141 [01:50<02:09,  1.70s/it]avg_loss = 1.9219887653986614:  47%|████▋     | 66/141 [01:50<02:07,  1.70s/it]avg_loss = 1.9190265260525603:  47%|████▋     | 66/141 [01:51<02:07,  1.70s/it]avg_loss = 1.9190265260525603:  48%|████▊     | 67/141 [01:51<02:06,  1.70s/it]avg_loss = 1.9161288527881397:  48%|████▊     | 67/141 [01:53<02:06,  1.70s/it]avg_loss = 1.9161288527881397:  48%|████▊     | 68/141 [01:53<02:04,  1.70s/it]avg_loss = 1.9139411933180215:  48%|████▊     | 68/141 [01:55<02:04,  1.70s/it]avg_loss = 1.9139411933180215:  49%|████▉     | 69/141 [01:55<02:02,  1.70s/it]avg_loss = 1.9148227538381304:  49%|████▉     | 69/141 [01:56<02:02,  1.70s/it]avg_loss = 1.9148227538381304:  50%|████▉     | 70/141 [01:56<02:01,  1.71s/it]avg_loss = 1.9182038995581614:  50%|████▉     | 70/141 [01:58<02:01,  1.71s/it]avg_loss = 1.9182038995581614:  50%|█████     | 71/141 [01:58<01:59,  1.71s/it]avg_loss = 1.9208673553334341:  50%|█████     | 71/141 [02:00<01:59,  1.71s/it]avg_loss = 1.9208673553334341:  51%|█████     | 72/141 [02:00<01:57,  1.71s/it]avg_loss = 1.9193204135110933:  51%|█████     | 72/141 [02:02<01:57,  1.71s/it]avg_loss = 1.9193204135110933:  52%|█████▏    | 73/141 [02:02<01:56,  1.71s/it]avg_loss = 1.920702421987379:  52%|█████▏    | 73/141 [02:03<01:56,  1.71s/it] avg_loss = 1.920702421987379:  52%|█████▏    | 74/141 [02:03<01:54,  1.71s/it]avg_loss = 1.9206816101074218:  52%|█████▏    | 74/141 [02:05<01:54,  1.71s/it]avg_loss = 1.9206816101074218:  53%|█████▎    | 75/141 [02:05<01:52,  1.71s/it]avg_loss = 1.9196979403495789:  53%|█████▎    | 75/141 [02:07<01:52,  1.71s/it]avg_loss = 1.9196979403495789:  54%|█████▍    | 76/141 [02:07<01:51,  1.71s/it]avg_loss = 1.920824651594286:  54%|█████▍    | 76/141 [02:08<01:51,  1.71s/it] avg_loss = 1.920824651594286:  55%|█████▍    | 77/141 [02:08<01:49,  1.71s/it]avg_loss = 1.9231177323903792:  55%|█████▍    | 77/141 [02:10<01:49,  1.71s/it]avg_loss = 1.9231177323903792:  55%|█████▌    | 78/141 [02:10<01:47,  1.71s/it]avg_loss = 1.9268942271606833:  55%|█████▌    | 78/141 [02:12<01:47,  1.71s/it]avg_loss = 1.9268942271606833:  56%|█████▌    | 79/141 [02:12<01:45,  1.71s/it]avg_loss = 1.9234724566340446:  56%|█████▌    | 79/141 [02:13<01:45,  1.71s/it]avg_loss = 1.9234724566340446:  57%|█████▋    | 80/141 [02:13<01:44,  1.71s/it]avg_loss = 1.9222241345747018:  57%|█████▋    | 80/141 [02:15<01:44,  1.71s/it]avg_loss = 1.9222241345747018:  57%|█████▋    | 81/141 [02:15<01:42,  1.71s/it]avg_loss = 1.921160679037978:  57%|█████▋    | 81/141 [02:17<01:42,  1.71s/it] avg_loss = 1.921160679037978:  58%|█████▊    | 82/141 [02:17<01:40,  1.71s/it]avg_loss = 1.9190982264208507:  58%|█████▊    | 82/141 [02:19<01:40,  1.71s/it]avg_loss = 1.9190982264208507:  59%|█████▉    | 83/141 [02:19<01:39,  1.71s/it]avg_loss = 1.9168301551114946:  59%|█████▉    | 83/141 [02:20<01:39,  1.71s/it]avg_loss = 1.9168301551114946:  60%|█████▉    | 84/141 [02:20<01:37,  1.71s/it]avg_loss = 1.9143059716505162:  60%|█████▉    | 84/141 [02:22<01:37,  1.71s/it]avg_loss = 1.9143059716505162:  60%|██████    | 85/141 [02:22<01:35,  1.71s/it]avg_loss = 1.9158924194269402:  60%|██████    | 85/141 [02:24<01:35,  1.71s/it]avg_loss = 1.9158924194269402:  61%|██████    | 86/141 [02:24<01:34,  1.71s/it]avg_loss = 1.917828517398615:  61%|██████    | 86/141 [02:25<01:34,  1.71s/it] avg_loss = 1.917828517398615:  62%|██████▏   | 87/141 [02:25<01:32,  1.71s/it]avg_loss = 1.9187575009736149:  62%|██████▏   | 87/141 [02:27<01:32,  1.71s/it]avg_loss = 1.9187575009736149:  62%|██████▏   | 88/141 [02:27<01:30,  1.71s/it]avg_loss = 1.9273797662070629:  62%|██████▏   | 88/141 [02:29<01:30,  1.71s/it]avg_loss = 1.9273797662070629:  63%|██████▎   | 89/141 [02:29<01:29,  1.71s/it]avg_loss = 1.9346892277399699:  63%|██████▎   | 89/141 [02:31<01:29,  1.71s/it]avg_loss = 1.9346892277399699:  64%|██████▍   | 90/141 [02:31<01:27,  1.71s/it]avg_loss = 1.9379279168097527:  64%|██████▍   | 90/141 [02:32<01:27,  1.71s/it]avg_loss = 1.9379279168097527:  65%|██████▍   | 91/141 [02:32<01:25,  1.71s/it]avg_loss = 1.9426851713139077:  65%|██████▍   | 91/141 [02:34<01:25,  1.71s/it]avg_loss = 1.9426851713139077:  65%|██████▌   | 92/141 [02:34<01:23,  1.71s/it]avg_loss = 1.9476954834435576:  65%|██████▌   | 92/141 [02:36<01:23,  1.71s/it]avg_loss = 1.9476954834435576:  66%|██████▌   | 93/141 [02:36<01:22,  1.71s/it]avg_loss = 1.9485492148297898:  66%|██████▌   | 93/141 [02:37<01:22,  1.71s/it]avg_loss = 1.9485492148297898:  67%|██████▋   | 94/141 [02:37<01:20,  1.71s/it]avg_loss = 1.9523706862801:  67%|██████▋   | 94/141 [02:39<01:20,  1.71s/it]   avg_loss = 1.9523706862801:  67%|██████▋   | 95/141 [02:39<01:18,  1.72s/it]avg_loss = 1.9529917264978092:  67%|██████▋   | 95/141 [02:41<01:18,  1.72s/it]avg_loss = 1.9529917264978092:  68%|██████▊   | 96/141 [02:41<01:17,  1.71s/it]avg_loss = 1.9547659834635627:  68%|██████▊   | 96/141 [02:43<01:17,  1.71s/it]avg_loss = 1.9547659834635627:  69%|██████▉   | 97/141 [02:43<01:15,  1.71s/it]avg_loss = 1.952110336751354:  69%|██████▉   | 97/141 [02:44<01:15,  1.71s/it] avg_loss = 1.952110336751354:  70%|██████▉   | 98/141 [02:44<01:13,  1.71s/it]avg_loss = 1.9531731653695155:  70%|██████▉   | 98/141 [02:46<01:13,  1.71s/it]avg_loss = 1.9531731653695155:  70%|███████   | 99/141 [02:46<01:11,  1.71s/it]avg_loss = 1.9552791380882264:  70%|███████   | 99/141 [02:48<01:11,  1.71s/it]avg_loss = 1.9552791380882264:  71%|███████   | 100/141 [02:48<01:10,  1.71s/it]avg_loss = 1.9545523164295915:  71%|███████   | 100/141 [02:49<01:10,  1.71s/it]avg_loss = 1.9545523164295915:  72%|███████▏  | 101/141 [02:49<01:08,  1.71s/it]avg_loss = 1.95512268706864:  72%|███████▏  | 101/141 [02:51<01:08,  1.71s/it]  avg_loss = 1.95512268706864:  72%|███████▏  | 102/141 [02:51<01:06,  1.71s/it]avg_loss = 1.9541599727371364:  72%|███████▏  | 102/141 [02:53<01:06,  1.71s/it]avg_loss = 1.9541599727371364:  73%|███████▎  | 103/141 [02:53<01:05,  1.71s/it]avg_loss = 1.9571062188882093:  73%|███████▎  | 103/141 [02:55<01:05,  1.71s/it]avg_loss = 1.9571062188882093:  74%|███████▍  | 104/141 [02:55<01:03,  1.71s/it]avg_loss = 1.9563386406217302:  74%|███████▍  | 104/141 [02:56<01:03,  1.71s/it]avg_loss = 1.9563386406217302:  74%|███████▍  | 105/141 [02:56<01:01,  1.71s/it]avg_loss = 1.955751825053737:  74%|███████▍  | 105/141 [02:58<01:01,  1.71s/it] avg_loss = 1.955751825053737:  75%|███████▌  | 106/141 [02:58<00:59,  1.71s/it]avg_loss = 1.9537388097460024:  75%|███████▌  | 106/141 [03:00<00:59,  1.71s/it]avg_loss = 1.9537388097460024:  76%|███████▌  | 107/141 [03:00<00:58,  1.71s/it]avg_loss = 1.9516231340390664:  76%|███████▌  | 107/141 [03:01<00:58,  1.71s/it]avg_loss = 1.9516231340390664:  77%|███████▋  | 108/141 [03:01<00:56,  1.71s/it]avg_loss = 1.9494121938670448:  77%|███████▋  | 108/141 [03:03<00:56,  1.71s/it]avg_loss = 1.9494121938670448:  77%|███████▋  | 109/141 [03:03<00:54,  1.71s/it]avg_loss = 1.9468668233264577:  77%|███████▋  | 109/141 [03:05<00:54,  1.71s/it]avg_loss = 1.9468668233264577:  78%|███████▊  | 110/141 [03:05<00:53,  1.71s/it]avg_loss = 1.949225875708434:  78%|███████▊  | 110/141 [03:07<00:53,  1.71s/it] avg_loss = 1.949225875708434:  79%|███████▊  | 111/141 [03:07<00:51,  1.71s/it]avg_loss = 1.948780708014965:  79%|███████▊  | 111/141 [03:08<00:51,  1.71s/it]avg_loss = 1.948780708014965:  79%|███████▉  | 112/141 [03:08<00:49,  1.71s/it]avg_loss = 1.9498013637762155:  79%|███████▉  | 112/141 [03:10<00:49,  1.71s/it]avg_loss = 1.9498013637762155:  80%|████████  | 113/141 [03:10<00:47,  1.71s/it]avg_loss = 1.9507753713089122:  80%|████████  | 113/141 [03:12<00:47,  1.71s/it]avg_loss = 1.9507753713089122:  81%|████████  | 114/141 [03:12<00:46,  1.71s/it]avg_loss = 1.9498418393342392:  81%|████████  | 114/141 [03:13<00:46,  1.71s/it]avg_loss = 1.9498418393342392:  82%|████████▏ | 115/141 [03:13<00:44,  1.71s/it]avg_loss = 1.9485762191229854:  82%|████████▏ | 115/141 [03:15<00:44,  1.71s/it]avg_loss = 1.9485762191229854:  82%|████████▏ | 116/141 [03:15<00:42,  1.71s/it]avg_loss = 1.9507920161271706:  82%|████████▏ | 116/141 [03:17<00:42,  1.71s/it]avg_loss = 1.9507920161271706:  83%|████████▎ | 117/141 [03:17<00:41,  1.71s/it]avg_loss = 1.9502201686471194:  83%|████████▎ | 117/141 [03:19<00:41,  1.71s/it]avg_loss = 1.9502201686471194:  84%|████████▎ | 118/141 [03:19<00:39,  1.71s/it]avg_loss = 1.9487162818427848:  84%|████████▎ | 118/141 [03:20<00:39,  1.71s/it]avg_loss = 1.9487162818427848:  84%|████████▍ | 119/141 [03:20<00:37,  1.72s/it]avg_loss = 1.946944863597552:  84%|████████▍ | 119/141 [03:22<00:37,  1.72s/it] avg_loss = 1.946944863597552:  85%|████████▌ | 120/141 [03:22<00:36,  1.72s/it]avg_loss = 1.9467106968903345:  85%|████████▌ | 120/141 [03:24<00:36,  1.72s/it]avg_loss = 1.9467106968903345:  86%|████████▌ | 121/141 [03:24<00:34,  1.72s/it]avg_loss = 1.947237425163144:  86%|████████▌ | 121/141 [03:25<00:34,  1.72s/it] avg_loss = 1.947237425163144:  87%|████████▋ | 122/141 [03:25<00:32,  1.72s/it]avg_loss = 1.9468768689690568:  87%|████████▋ | 122/141 [03:27<00:32,  1.72s/it]avg_loss = 1.9468768689690568:  87%|████████▋ | 123/141 [03:27<00:30,  1.72s/it]avg_loss = 1.9469775259494781:  87%|████████▋ | 123/141 [03:29<00:30,  1.72s/it]avg_loss = 1.9469775259494781:  88%|████████▊ | 124/141 [03:29<00:29,  1.72s/it]avg_loss = 1.945689043045044:  88%|████████▊ | 124/141 [03:31<00:29,  1.72s/it] avg_loss = 1.945689043045044:  89%|████████▊ | 125/141 [03:31<00:27,  1.72s/it]avg_loss = 1.945877524595412:  89%|████████▊ | 125/141 [03:32<00:27,  1.72s/it]avg_loss = 1.945877524595412:  89%|████████▉ | 126/141 [03:32<00:25,  1.72s/it]avg_loss = 1.945567701745221:  89%|████████▉ | 126/141 [03:34<00:25,  1.72s/it]avg_loss = 1.945567701745221:  90%|█████████ | 127/141 [03:34<00:24,  1.72s/it]avg_loss = 1.9444048050791025:  90%|█████████ | 127/141 [03:36<00:24,  1.72s/it]avg_loss = 1.9444048050791025:  91%|█████████ | 128/141 [03:36<00:22,  1.72s/it]avg_loss = 1.94451455367628:  91%|█████████ | 128/141 [03:37<00:22,  1.72s/it]  avg_loss = 1.94451455367628:  91%|█████████▏| 129/141 [03:37<00:20,  1.72s/it]avg_loss = 1.9455506985004132:  91%|█████████▏| 129/141 [03:39<00:20,  1.72s/it]avg_loss = 1.9455506985004132:  92%|█████████▏| 130/141 [03:39<00:18,  1.72s/it]avg_loss = 1.9465418899332294:  92%|█████████▏| 130/141 [03:41<00:18,  1.72s/it]avg_loss = 1.9465418899332294:  93%|█████████▎| 131/141 [03:41<00:17,  1.72s/it]avg_loss = 1.947192006038897:  93%|█████████▎| 131/141 [03:43<00:17,  1.72s/it] avg_loss = 1.947192006038897:  94%|█████████▎| 132/141 [03:43<00:15,  1.72s/it]avg_loss = 1.9442292299485744:  94%|█████████▎| 132/141 [03:44<00:15,  1.72s/it]avg_loss = 1.9442292299485744:  94%|█████████▍| 133/141 [03:44<00:13,  1.72s/it]avg_loss = 1.9395642680908316:  94%|█████████▍| 133/141 [03:46<00:13,  1.72s/it]avg_loss = 1.9395642680908316:  95%|█████████▌| 134/141 [03:46<00:12,  1.72s/it]avg_loss = 1.9418372798849035:  95%|█████████▌| 134/141 [03:48<00:12,  1.72s/it]avg_loss = 1.9418372798849035:  96%|█████████▌| 135/141 [03:48<00:10,  1.72s/it]avg_loss = 1.9452277237878126:  96%|█████████▌| 135/141 [03:50<00:10,  1.72s/it]avg_loss = 1.9452277237878126:  96%|█████████▋| 136/141 [03:50<00:08,  1.72s/it]avg_loss = 1.9464643793384524:  96%|█████████▋| 136/141 [03:51<00:08,  1.72s/it]avg_loss = 1.9464643793384524:  97%|█████████▋| 137/141 [03:51<00:06,  1.72s/it]avg_loss = 1.9453373413155044:  97%|█████████▋| 137/141 [03:53<00:06,  1.72s/it]avg_loss = 1.9453373413155044:  98%|█████████▊| 138/141 [03:53<00:05,  1.72s/it]avg_loss = 1.945803687726851:  98%|█████████▊| 138/141 [03:55<00:05,  1.72s/it] avg_loss = 1.945803687726851:  99%|█████████▊| 139/141 [03:55<00:03,  1.72s/it]avg_loss = 1.9465230865137917:  99%|█████████▊| 139/141 [03:56<00:03,  1.72s/it]avg_loss = 1.9465230865137917:  99%|█████████▉| 140/141 [03:56<00:01,  1.72s/it]avg_loss = 1.9477305742020303:  99%|█████████▉| 140/141 [03:58<00:01,  1.72s/it]avg_loss = 1.9477305742020303: 100%|██████████| 141/141 [03:58<00:00,  1.72s/it]avg_loss = 1.9477305742020303: 100%|██████████| 141/141 [03:58<00:00,  1.69s/it]
I0326 06:07:55.298763 1415945 eval_ppl.py:107] wikitext2 perplexity: 7.012754440307617
wikitext2 perplexity: 7.013
