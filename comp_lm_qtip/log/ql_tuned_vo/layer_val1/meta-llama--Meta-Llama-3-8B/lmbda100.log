I0409 18:09:01.947573 1581507 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 18:09:01.947675 1581507 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 18:09:01.947714 1581507 utils.py:162] NumExpr defaulting to 16 threads.
I0409 18:09:02.270537 1581507 config.py:54] PyTorch version 2.6.0 available.
W0409 18:09:02.458954 1581507 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 18:09:03.081429 1581507 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.12it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.53it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.77it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.90it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.80it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.71it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.93it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.80it/s]
I0409 18:09:04.556277 1581507 quantize_finetune_llama.py:163] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:20,  1.50it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:20,  1.48it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:19,  1.50it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:17,  1.63it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:15,  1.72it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:14,  1.79it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:13,  1.83it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:04<00:12,  1.86it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:05<00:12,  1.88it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:05<00:11,  1.89it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:11,  1.90it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:06<00:10,  1.90it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:09,  1.96it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:07<00:08,  2.03it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:08<00:08,  2.07it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:08<00:07,  2.13it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:08<00:06,  2.17it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:09<00:06,  2.19it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:09<00:05,  2.20it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:10<00:05,  2.21it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:10<00:04,  2.21it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:11<00:04,  2.21it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:11<00:04,  2.21it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:12<00:03,  2.21it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:12<00:03,  2.15it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:13<00:02,  2.14it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:13<00:02,  2.14it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:14<00:01,  2.15it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:14<00:01,  2.08it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:15<00:00,  2.07it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:15<00:00,  2.04it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  2.03it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:16<00:00,  2.00it/s]
I0409 18:09:26.110955 1581507 quantize_finetune_llama.py:201] loaded compression model
I0409 18:09:45.144186 1581507 quantize_finetune_llama.py:205] loaded dataset and devset
I0409 18:09:47.200772 1581507 quantize_finetune_llama.py:225] layer 0 gpu 0
I0409 18:09:49.081490 1581507 quantize_finetune_llama.py:256] computed original embedding for layer 0 in 1.7313733100891113s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0409 18:10:02.555149 1582487 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 18:10:02.555317 1582487 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 18:10:02.555401 1582487 utils.py:162] NumExpr defaulting to 16 threads.
I0409 18:10:02.945452 1582487 config.py:54] PyTorch version 2.6.0 available.
W0409 18:10:03.162466 1582487 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 18:10:03.773041 1582487 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 18:10:03.777070 1581507 quantize_finetune_llama.py:225] layer 1 gpu 0
I0409 18:10:03.790607 1582487 data_utils.py:336] using 256 training seqs, 128 validation seqs
0_v proxy err 0.012663488276302814 err 0.7710399031639099 tr(WHW.T) 60.88684844970703
bpp_loss 4.403017997741699
0_q proxy err 0.00015234753664117306 err 43.89051055908203 tr(WHW.T) 288094.65625
bpp_loss 3.383499503135681
0_k proxy err 0.00014897323853801936 err 14.924247741699219 tr(WHW.T) 100180.734375
bpp_loss 3.9246712923049927
0_o proxy err 0.0070782992988824844 err 22.10706901550293 tr(WHW.T) 3123.217529296875
bpp_loss 2.8600224256515503
0_up proxy err 0.018107548356056213 err 161.5999298095703 tr(WHW.T) 8924.451171875
bpp_loss 2.9570471899850026
0_gate proxy err 0.01046028546988964 err 165.04934692382812 tr(WHW.T) 15778.666015625
bpp_loss 3.066056524004255
0_down proxy err 0.012809163890779018 err 138.57215881347656 tr(WHW.T) 10818.205078125
bpp_loss 2.9505288941519603
I0409 18:10:45.095808 1581507 quantize_finetune_llama.py:256] computed original embedding for layer 1 in 0.71470046043396s
I0409 18:10:48.772734 1583364 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 18:10:48.772854 1583364 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 18:10:48.772896 1583364 utils.py:162] NumExpr defaulting to 16 threads.
I0409 18:10:49.160902 1583364 config.py:54] PyTorch version 2.6.0 available.
W0409 18:10:49.379388 1583364 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 18:10:49.998381 1583364 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 18:10:50.002392 1581507 quantize_finetune_llama.py:225] layer 2 gpu 0
I0409 18:10:50.016117 1583364 data_utils.py:336] using 256 training seqs, 128 validation seqs
1_v proxy err 0.007458393462002277 err 0.8134941458702087 tr(WHW.T) 109.07096099853516
bpp_loss 5.229259729385376
1_q proxy err 0.00020187514019198716 err 29.236865997314453 tr(WHW.T) 144826.484375
bpp_loss 3.6344006061553955
1_k proxy err 0.00012200995115563273 err 9.21651554107666 tr(WHW.T) 75539.046875
bpp_loss 4.297932863235474
1_o proxy err 0.012640335597097874 err 25.076303482055664 tr(WHW.T) 1983.8321533203125
bpp_loss 2.985153913497925
1_up proxy err 0.019899675622582436 err 163.7906494140625 tr(WHW.T) 8230.8203125
bpp_loss 2.9713310514177596
1_gate proxy err 0.011976547539234161 err 167.10116577148438 tr(WHW.T) 13952.365234375
bpp_loss 3.077267646789551
1_down proxy err 0.0011802114313468337 err 16.497032165527344 tr(WHW.T) 13978.03125
bpp_loss 2.966089589255197
I0409 18:11:26.761708 1581507 quantize_finetune_llama.py:256] computed original embedding for layer 2 in 0.7799184322357178s
