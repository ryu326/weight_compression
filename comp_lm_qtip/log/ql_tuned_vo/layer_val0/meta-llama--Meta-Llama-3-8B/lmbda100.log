I0409 18:11:39.424216 1584333 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 18:11:39.424334 1584333 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 18:11:39.424376 1584333 utils.py:162] NumExpr defaulting to 16 threads.
I0409 18:11:39.818656 1584333 config.py:54] PyTorch version 2.6.0 available.
W0409 18:11:40.034268 1584333 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 18:11:40.706144 1584333 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  5.58it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  5.68it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  5.74it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  5.77it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  5.78it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  5.63it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.67it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  5.69it/s]
I0409 18:11:42.584153 1584333 quantize_finetune_llama.py:163] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:22,  1.35it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:21,  1.38it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:21,  1.38it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:18,  1.50it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:17,  1.56it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:03<00:16,  1.60it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:15,  1.64it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:14,  1.66it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:05<00:13,  1.68it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:06<00:13,  1.69it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:06<00:12,  1.70it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:07<00:11,  1.73it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:07<00:10,  1.79it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:08<00:10,  1.78it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:09<00:09,  1.79it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:09<00:08,  1.86it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:09<00:07,  1.94it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:10<00:07,  1.97it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:11<00:06,  1.93it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:11<00:06,  1.98it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:11<00:05,  2.01it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:12<00:04,  2.03it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:12<00:04,  2.00it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:13<00:04,  1.99it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:13<00:03,  1.99it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:14<00:03,  1.99it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:14<00:02,  2.00it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:15<00:01,  2.01it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:15<00:01,  2.01it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:16<00:00,  2.02it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:17<00:00,  1.97it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:17<00:00,  1.98it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:17<00:00,  1.83it/s]
I0409 18:12:08.520950 1584333 quantize_finetune_llama.py:201] loaded compression model
I0409 18:12:28.476860 1584333 quantize_finetune_llama.py:205] loaded dataset and devset
I0409 18:12:30.676696 1584333 quantize_finetune_llama.py:225] layer 0 gpu 0
I0409 18:12:33.069717 1584333 quantize_finetune_llama.py:256] computed original embedding for layer 0 in 2.1675527095794678s
tensor(-4.7143e-06) tensor(0.0125)
tensor(0.0125) tensor(-4.7143e-06)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0409 18:12:44.152718 1585909 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 18:12:44.152821 1585909 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 18:12:44.152862 1585909 utils.py:162] NumExpr defaulting to 16 threads.
I0409 18:12:44.511779 1585909 config.py:54] PyTorch version 2.6.0 available.
W0409 18:12:44.723792 1585909 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

W0409 18:12:45.324892 1585909 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @amp.autocast(enabled=False)

I0409 18:12:45.329203 1584333 quantize_finetune_llama.py:225] layer 1 gpu 0
I0409 18:12:45.343183 1585909 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0409 18:13:13.471370 1586519 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0409 18:13:13.471620 1586519 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0409 18:13:13.471663 1586519 utils.py:162] NumExpr defaulting to 16 threads.
I0409 18:13:13.793046 1586519 config.py:54] PyTorch version 2.6.0 available.
W0409 18:13:14.007649 1586519 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

I0409 18:13:14.120718 1586519 hfize_llama.py:27] LlamaConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {},
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.47.1",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.97it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.58it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.90it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.89it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  9.05it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.11it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.37it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.07it/s]
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.71it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.45it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.74it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.73it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.96it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  9.06it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  9.10it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.93it/s]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 194, in <module>
    main(args)
  File "/workspace/Weight_compression/comp_lm_qtip/quantize_llama/hfize_llama.py", line 140, in main
    saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 1425, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 751, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/opt/conda/lib/python3.10/site-packages/torch/serialization.py", line 732, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '../hf_model_comp/comp_qtip/ckpt/ql_tuned_vo/layer_val0/meta-llama--Meta-Llama-3-8B/lmbda100/0_gate.pt'
