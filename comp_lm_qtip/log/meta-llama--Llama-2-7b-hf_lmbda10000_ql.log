I0312 07:25:47.300651 1886474 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.79it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 10.16it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.55it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.30it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.08it/s]
I0312 07:25:49.162904 1886474 quantize_finetune_llama.py:134] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:20,  1.52it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:20,  1.48it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:19,  1.46it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:19,  1.46it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:18,  1.45it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:17,  1.45it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:04<00:17,  1.44it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:05<00:16,  1.44it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:15,  1.46it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:06<00:14,  1.50it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:07<00:13,  1.53it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:12,  1.54it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:08<00:12,  1.55it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:09<00:11,  1.56it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:09<00:10,  1.55it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:10<00:10,  1.55it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:11<00:09,  1.55it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:11<00:09,  1.55it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:12<00:08,  1.54it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:13<00:07,  1.54it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:13<00:07,  1.53it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:14<00:06,  1.52it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:15<00:05,  1.52it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:15<00:05,  1.52it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:16<00:04,  1.52it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:17<00:03,  1.53it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:17<00:03,  1.55it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:18<00:02,  1.54it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:19<00:01,  1.51it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:19<00:01,  1.50it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:20<00:00,  1.48it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:21<00:00,  1.45it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:21<00:00,  1.51it/s]
I0312 07:26:20.004212 1886474 quantize_finetune_llama.py:159] loaded compression model
I0312 07:26:34.055106 1886474 quantize_finetune_llama.py:163] loaded dataset and devset
I0312 07:26:39.130907 1886474 quantize_finetune_llama.py:183] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 07:27:45.514676 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 0 in 66.26824808120728s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0312 07:28:05.791168 1886633 config.py:54] PyTorch version 2.1.1 available.
I0312 07:28:06.719062 1886474 quantize_finetune_llama.py:183] layer 1 gpu 1
I0312 07:28:06.784626 1886633 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 07:28:14.654124 1886633 finetune.py:45] layer 0_v initial loss 2.3137631899317057e-07
I0312 07:28:46.712654 1886633 finetune.py:68] layer 0_v @ epoch 0 new loss 6.360730964161121e-08 old loss 2.3137631899317057e-07 BETTER
I0312 07:29:16.536704 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 1 in 69.6807873249054s
I0312 07:29:22.382184 1886633 finetune.py:68] layer 0_v @ epoch 1 new loss 2.6459233026798756e-08 old loss 6.360730964161121e-08 BETTER
I0312 07:29:24.711562 1886781 config.py:54] PyTorch version 2.1.1 available.
I0312 07:29:25.689921 1886474 quantize_finetune_llama.py:183] layer 2 gpu 2
I0312 07:29:25.773793 1886781 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 07:29:33.548216 1886781 finetune.py:45] layer 1_v initial loss 2.8330293844192056e-06
I0312 07:29:56.093901 1886633 finetune.py:68] layer 0_v @ epoch 2 new loss 1.545436134620104e-08 old loss 2.6459233026798756e-08 BETTER
I0312 07:30:04.297801 1886781 finetune.py:68] layer 1_v @ epoch 0 new loss 1.8803051489157951e-06 old loss 2.8330293844192056e-06 BETTER
I0312 07:30:30.070673 1886633 finetune.py:68] layer 0_v @ epoch 3 new loss 1.1597588667200398e-08 old loss 1.545436134620104e-08 BETTER
I0312 07:30:34.534244 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 2 in 68.72664999961853s
I0312 07:30:38.265908 1886781 finetune.py:68] layer 1_v @ epoch 1 new loss 1.5083013522598776e-06 old loss 1.8803051489157951e-06 BETTER
I0312 07:30:42.576455 1886914 config.py:54] PyTorch version 2.1.1 available.
I0312 07:30:43.618563 1886474 quantize_finetune_llama.py:183] layer 3 gpu 3
I0312 07:30:43.685658 1886914 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 07:30:51.771987 1886914 finetune.py:45] layer 2_v initial loss 4.698117379575706e-07
I0312 07:31:04.256076 1886633 finetune.py:68] layer 0_v @ epoch 4 new loss 9.885047447255602e-09 old loss 1.1597588667200398e-08 BETTER
I0312 07:31:10.464512 1886781 finetune.py:76] layer 1_v @ epoch 2 new loss 4.804196123586735e-06 old loss 1.5083013522598776e-06 WORSE
I0312 07:31:13.452117 1886633 finetune.py:45] layer 0_q initial loss 1.540702676550154e-08
I0312 07:31:22.820040 1886914 finetune.py:68] layer 2_v @ epoch 0 new loss 3.3337360605401045e-07 old loss 4.698117379575706e-07 BETTER
I0312 07:31:42.203257 1886781 finetune.py:68] layer 1_v @ epoch 3 new loss 7.041713843136677e-07 old loss 1.5083013522598776e-06 BETTER
I0312 07:31:46.181757 1886633 finetune.py:68] layer 0_q @ epoch 0 new loss 9.829205893652215e-09 old loss 1.540702676550154e-08 BETTER
I0312 07:31:54.753461 1886914 finetune.py:68] layer 2_v @ epoch 1 new loss 2.662547160525719e-07 old loss 3.3337360605401045e-07 BETTER
I0312 07:31:55.429672 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 3 in 71.66151714324951s
I0312 07:32:04.174163 1887047 config.py:54] PyTorch version 2.1.1 available.
I0312 07:32:05.240681 1886474 quantize_finetune_llama.py:183] layer 4 gpu 0
I0312 07:32:05.310848 1887047 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 07:32:13.352725 1887047 finetune.py:45] layer 3_v initial loss 6.959159577490936e-07
I0312 07:32:15.093427 1886781 finetune.py:76] layer 1_v @ epoch 4 new loss 1.4580921288143145e-06 old loss 7.041713843136677e-07 WORSE
I0312 07:32:20.104299 1886633 finetune.py:68] layer 0_q @ epoch 1 new loss 8.77570016655227e-09 old loss 9.829205893652215e-09 BETTER
I0312 07:32:23.919215 1886781 finetune.py:45] layer 1_q initial loss 7.999535682756687e-07
I0312 07:32:27.004423 1886914 finetune.py:68] layer 2_v @ epoch 2 new loss 2.3066201038091094e-07 old loss 2.662547160525719e-07 BETTER
I0312 07:32:43.728059 1887047 finetune.py:68] layer 3_v @ epoch 0 new loss 3.9710712940177473e-07 old loss 6.959159577490936e-07 BETTER
I0312 07:32:54.307146 1886633 finetune.py:68] layer 0_q @ epoch 2 new loss 8.065772050258602e-09 old loss 8.77570016655227e-09 BETTER
I0312 07:32:55.542137 1886781 finetune.py:76] layer 1_q @ epoch 0 new loss 9.466930919188599e-07 old loss 7.999535682756687e-07 WORSE
I0312 07:32:59.897071 1886914 finetune.py:68] layer 2_v @ epoch 3 new loss 2.0953046941940556e-07 old loss 2.3066201038091094e-07 BETTER
I0312 07:33:15.576797 1887047 finetune.py:68] layer 3_v @ epoch 1 new loss 3.332401661282347e-07 old loss 3.9710712940177473e-07 BETTER
I0312 07:33:27.353153 1886781 finetune.py:68] layer 1_q @ epoch 1 new loss 5.747762656937994e-07 old loss 7.999535682756687e-07 BETTER
I0312 07:33:28.338817 1886633 finetune.py:68] layer 0_q @ epoch 3 new loss 7.531742340916026e-09 old loss 8.065772050258602e-09 BETTER
I0312 07:33:32.642272 1886914 finetune.py:68] layer 2_v @ epoch 4 new loss 1.9561414887903084e-07 old loss 2.0953046941940556e-07 BETTER
I0312 07:33:41.601765 1886914 finetune.py:45] layer 2_q initial loss 2.642252638906939e-07
I0312 07:33:47.511077 1887047 finetune.py:68] layer 3_v @ epoch 2 new loss 3.051665942166437e-07 old loss 3.332401661282347e-07 BETTER
I0312 07:33:59.623090 1886781 finetune.py:76] layer 1_q @ epoch 2 new loss 1.055536927196954e-06 old loss 5.747762656937994e-07 WORSE
I0312 07:34:02.546345 1886633 finetune.py:68] layer 0_q @ epoch 4 new loss 7.086839559633518e-09 old loss 7.531742340916026e-09 BETTER
I0312 07:34:11.675445 1886633 finetune.py:45] layer 0_k initial loss 6.584479450566505e-08
I0312 07:34:13.033635 1886914 finetune.py:68] layer 2_q @ epoch 0 new loss 2.0883254592263256e-07 old loss 2.642252638906939e-07 BETTER
I0312 07:34:19.626331 1887047 finetune.py:68] layer 3_v @ epoch 3 new loss 2.8680094033006753e-07 old loss 3.051665942166437e-07 BETTER
I0312 07:34:31.353804 1886781 finetune.py:76] layer 1_q @ epoch 3 new loss 1.3057172054686816e-06 old loss 5.747762656937994e-07 WORSE
I0312 07:34:44.496418 1886633 finetune.py:68] layer 0_k @ epoch 0 new loss 1.2305600982642773e-08 old loss 6.584479450566505e-08 BETTER
I0312 07:34:45.464615 1886914 finetune.py:68] layer 2_q @ epoch 1 new loss 1.9733700185042835e-07 old loss 2.0883254592263256e-07 BETTER
I0312 07:34:51.857845 1887047 finetune.py:68] layer 3_v @ epoch 4 new loss 2.734510928803502e-07 old loss 2.8680094033006753e-07 BETTER
I0312 07:35:00.851966 1887047 finetune.py:45] layer 3_q initial loss 4.603104457601148e-07
I0312 07:35:03.245776 1886781 finetune.py:76] layer 1_q @ epoch 4 new loss 4.855275619775057e-06 old loss 5.747762656937994e-07 WORSE
I0312 07:35:11.780728 1886781 finetune.py:45] layer 1_k initial loss 6.503169629468175e-07
I0312 07:35:17.855182 1886914 finetune.py:68] layer 2_q @ epoch 2 new loss 1.8886255759298365e-07 old loss 1.9733700185042835e-07 BETTER
I0312 07:35:18.088283 1886633 finetune.py:68] layer 0_k @ epoch 1 new loss 9.850657178844813e-09 old loss 1.2305600982642773e-08 BETTER
I0312 07:35:32.158550 1887047 finetune.py:68] layer 3_q @ epoch 0 new loss 3.3775950214476325e-07 old loss 4.603104457601148e-07 BETTER
I0312 07:35:43.192265 1886781 finetune.py:68] layer 1_k @ epoch 0 new loss 4.990803859072912e-07 old loss 6.503169629468175e-07 BETTER
I0312 07:35:50.185752 1886914 finetune.py:68] layer 2_q @ epoch 3 new loss 1.8203709828412684e-07 old loss 1.8886255759298365e-07 BETTER
I0312 07:35:51.853651 1886633 finetune.py:68] layer 0_k @ epoch 2 new loss 8.89790463531881e-09 old loss 9.850657178844813e-09 BETTER
I0312 07:36:04.073527 1887047 finetune.py:68] layer 3_q @ epoch 1 new loss 3.1727483928989386e-07 old loss 3.3775950214476325e-07 BETTER
I0312 07:36:15.262470 1886781 finetune.py:76] layer 1_k @ epoch 1 new loss 7.711970511081745e-07 old loss 4.990803859072912e-07 WORSE
I0312 07:36:22.961466 1886914 finetune.py:68] layer 2_q @ epoch 4 new loss 1.7631390392125468e-07 old loss 1.8203709828412684e-07 BETTER
I0312 07:36:25.700170 1886633 finetune.py:68] layer 0_k @ epoch 3 new loss 8.321035416258837e-09 old loss 8.89790463531881e-09 BETTER
I0312 07:36:32.090207 1886914 finetune.py:45] layer 2_k initial loss 2.457909999975527e-07
I0312 07:36:36.083457 1887047 finetune.py:68] layer 3_q @ epoch 2 new loss 3.0372243031706603e-07 old loss 3.1727483928989386e-07 BETTER
I0312 07:36:46.672606 1886781 finetune.py:68] layer 1_k @ epoch 2 new loss 4.2859593918365135e-07 old loss 4.990803859072912e-07 BETTER
I0312 07:36:59.397176 1886633 finetune.py:68] layer 0_k @ epoch 4 new loss 7.898496967584379e-09 old loss 8.321035416258837e-09 BETTER
I0312 07:37:03.564105 1886914 finetune.py:68] layer 2_k @ epoch 0 new loss 2.3333880960763054e-07 old loss 2.457909999975527e-07 BETTER
I0312 07:37:08.412945 1887047 finetune.py:68] layer 3_q @ epoch 3 new loss 2.9353611807891866e-07 old loss 3.0372243031706603e-07 BETTER
I0312 07:37:09.324006 1886633 finetune.py:45] layer 0_o initial loss 4.014696131093842e-08
I0312 07:37:18.863945 1886781 finetune.py:76] layer 1_k @ epoch 3 new loss 4.5247358571032237e-07 old loss 4.2859593918365135e-07 WORSE
I0312 07:37:35.693050 1886914 finetune.py:68] layer 2_k @ epoch 1 new loss 2.258246638575656e-07 old loss 2.3333880960763054e-07 BETTER
I0312 07:37:40.467285 1887047 finetune.py:68] layer 3_q @ epoch 4 new loss 2.8538858032334247e-07 old loss 2.9353611807891866e-07 BETTER
I0312 07:37:41.670633 1886633 finetune.py:68] layer 0_o @ epoch 0 new loss 3.504932521991577e-08 old loss 4.014696131093842e-08 BETTER
I0312 07:37:49.591296 1887047 finetune.py:45] layer 3_k initial loss 4.78895799460588e-07
I0312 07:37:50.292507 1886781 finetune.py:76] layer 1_k @ epoch 4 new loss 1.092107027034217e-06 old loss 4.2859593918365135e-07 WORSE
I0312 07:37:58.905025 1886781 finetune.py:45] layer 1_o initial loss 6.102056886447826e-07
I0312 07:38:07.744083 1886914 finetune.py:68] layer 2_k @ epoch 2 new loss 2.1988846299336728e-07 old loss 2.258246638575656e-07 BETTER
I0312 07:38:14.661582 1886633 finetune.py:68] layer 0_o @ epoch 1 new loss 3.17221093837361e-08 old loss 3.504932521991577e-08 BETTER
I0312 07:38:20.355231 1887047 finetune.py:68] layer 3_k @ epoch 0 new loss 4.3530809534786385e-07 old loss 4.78895799460588e-07 BETTER
I0312 07:38:29.680490 1886781 finetune.py:68] layer 1_o @ epoch 0 new loss 5.640462177325389e-07 old loss 6.102056886447826e-07 BETTER
I0312 07:38:39.870544 1886914 finetune.py:68] layer 2_k @ epoch 3 new loss 2.1486276580162667e-07 old loss 2.1988846299336728e-07 BETTER
I0312 07:38:47.974141 1886633 finetune.py:68] layer 0_o @ epoch 2 new loss 2.9213808261374652e-08 old loss 3.17221093837361e-08 BETTER
I0312 07:38:52.006689 1887047 finetune.py:68] layer 3_k @ epoch 1 new loss 4.212187150187674e-07 old loss 4.3530809534786385e-07 BETTER
I0312 07:39:01.212191 1886781 finetune.py:68] layer 1_o @ epoch 1 new loss 5.117882437843946e-07 old loss 5.640462177325389e-07 BETTER
I0312 07:39:11.924348 1886914 finetune.py:68] layer 2_k @ epoch 4 new loss 2.1048452936156536e-07 old loss 2.1486276580162667e-07 BETTER
I0312 07:39:21.580615 1886914 finetune.py:45] layer 2_o initial loss 4.4626088424593036e-07
I0312 07:39:21.589018 1886633 finetune.py:68] layer 0_o @ epoch 3 new loss 2.7271520153249185e-08 old loss 2.9213808261374652e-08 BETTER
I0312 07:39:24.176382 1887047 finetune.py:68] layer 3_k @ epoch 2 new loss 4.11016685575305e-07 old loss 4.212187150187674e-07 BETTER
I0312 07:39:32.853624 1886781 finetune.py:68] layer 1_o @ epoch 2 new loss 4.88519731334236e-07 old loss 5.117882437843946e-07 BETTER
I0312 07:39:52.989181 1886914 finetune.py:68] layer 2_o @ epoch 0 new loss 4.4062400661459833e-07 old loss 4.4626088424593036e-07 BETTER
I0312 07:39:55.036942 1886633 finetune.py:68] layer 0_o @ epoch 4 new loss 2.5764718358800565e-08 old loss 2.7271520153249185e-08 BETTER
I0312 07:39:55.967921 1887047 finetune.py:68] layer 3_k @ epoch 3 new loss 4.028619287055335e-07 old loss 4.11016685575305e-07 BETTER
I0312 07:40:04.519315 1886781 finetune.py:68] layer 1_o @ epoch 3 new loss 4.6276105081233254e-07 old loss 4.88519731334236e-07 BETTER
I0312 07:40:10.513954 1886633 finetune.py:45] layer 0_up initial loss 3.088402422690706e-08
I0312 07:40:24.873569 1886914 finetune.py:68] layer 2_o @ epoch 1 new loss 4.360175580586656e-07 old loss 4.4062400661459833e-07 BETTER
I0312 07:40:27.797048 1887047 finetune.py:68] layer 3_k @ epoch 4 new loss 3.9598606349500187e-07 old loss 4.028619287055335e-07 BETTER
I0312 07:40:36.183575 1886781 finetune.py:68] layer 1_o @ epoch 4 new loss 4.4535741494655667e-07 old loss 4.6276105081233254e-07 BETTER
I0312 07:40:36.754670 1887047 finetune.py:45] layer 3_o initial loss 8.591148912273638e-07
I0312 07:40:41.163298 1886633 finetune.py:68] layer 0_up @ epoch 0 new loss 2.95300495167794e-08 old loss 3.088402422690706e-08 BETTER
I0312 07:40:51.394405 1886781 finetune.py:45] layer 1_up initial loss 8.885125453161891e-07
I0312 07:40:56.533434 1886914 finetune.py:68] layer 2_o @ epoch 2 new loss 4.3212452283114544e-07 old loss 4.360175580586656e-07 BETTER
I0312 07:41:07.043598 1887047 finetune.py:68] layer 3_o @ epoch 0 new loss 8.303803724629688e-07 old loss 8.591148912273638e-07 BETTER
I0312 07:41:12.651726 1886633 finetune.py:68] layer 0_up @ epoch 1 new loss 2.851197677955497e-08 old loss 2.95300495167794e-08 BETTER
I0312 07:41:20.227339 1886781 finetune.py:68] layer 1_up @ epoch 0 new loss 5.104265596855839e-07 old loss 8.885125453161891e-07 BETTER
I0312 07:41:28.241847 1886914 finetune.py:68] layer 2_o @ epoch 3 new loss 4.287496722099604e-07 old loss 4.3212452283114544e-07 BETTER
I0312 07:41:38.013169 1887047 finetune.py:68] layer 3_o @ epoch 1 new loss 8.129463253681024e-07 old loss 8.303803724629688e-07 BETTER
I0312 07:41:44.217334 1886633 finetune.py:68] layer 0_up @ epoch 2 new loss 2.7681117842348613e-08 old loss 2.851197677955497e-08 BETTER
I0312 07:41:49.978836 1886781 finetune.py:68] layer 1_up @ epoch 1 new loss 4.883255542154075e-07 old loss 5.104265596855839e-07 BETTER
I0312 07:41:59.892847 1886914 finetune.py:68] layer 2_o @ epoch 4 new loss 4.257062471424433e-07 old loss 4.287496722099604e-07 BETTER
I0312 07:42:08.947402 1887047 finetune.py:68] layer 3_o @ epoch 2 new loss 8.007438623280905e-07 old loss 8.129463253681024e-07 BETTER
I0312 07:42:15.025092 1886914 finetune.py:45] layer 2_up initial loss 6.015742428644444e-07
I0312 07:42:15.763118 1886633 finetune.py:68] layer 0_up @ epoch 3 new loss 2.7013637549089253e-08 old loss 2.7681117842348613e-08 BETTER
I0312 07:42:19.574865 1886781 finetune.py:68] layer 1_up @ epoch 2 new loss 4.799979933522991e-07 old loss 4.883255542154075e-07 BETTER
I0312 07:42:39.877054 1887047 finetune.py:68] layer 3_o @ epoch 3 new loss 7.909865757937951e-07 old loss 8.007438623280905e-07 BETTER
I0312 07:42:44.017278 1886914 finetune.py:68] layer 2_up @ epoch 0 new loss 5.984832114336314e-07 old loss 6.015742428644444e-07 BETTER
I0312 07:42:47.339360 1886633 finetune.py:68] layer 0_up @ epoch 4 new loss 2.646200591982506e-08 old loss 2.7013637549089253e-08 BETTER
I0312 07:42:49.393981 1886781 finetune.py:76] layer 1_up @ epoch 3 new loss 4.807684490515385e-07 old loss 4.799979933522991e-07 WORSE
I0312 07:43:02.635217 1886633 finetune.py:45] layer 0_gate initial loss 3.136659287861221e-08
I0312 07:43:10.882036 1887047 finetune.py:68] layer 3_o @ epoch 4 new loss 7.82858194270375e-07 old loss 7.909865757937951e-07 BETTER
I0312 07:43:13.889563 1886914 finetune.py:68] layer 2_up @ epoch 1 new loss 5.957044209026208e-07 old loss 5.984832114336314e-07 BETTER
I0312 07:43:18.493562 1886781 finetune.py:68] layer 1_up @ epoch 4 new loss 4.785492819792125e-07 old loss 4.799979933522991e-07 BETTER
I0312 07:43:25.795846 1887047 finetune.py:45] layer 3_up initial loss 1.179343826152035e-06
I0312 07:43:31.671798 1886633 finetune.py:68] layer 0_gate @ epoch 0 new loss 3.0799863992569954e-08 old loss 3.136659287861221e-08 BETTER
I0312 07:43:33.523201 1886781 finetune.py:45] layer 1_gate initial loss 6.370780738507165e-07
I0312 07:43:43.701455 1886914 finetune.py:68] layer 2_up @ epoch 2 new loss 5.931893838351243e-07 old loss 5.957044209026208e-07 BETTER
I0312 07:43:54.412561 1887047 finetune.py:68] layer 3_up @ epoch 0 new loss 1.1698946309479652e-06 old loss 1.179343826152035e-06 BETTER
I0312 07:44:01.157619 1886781 finetune.py:68] layer 1_gate @ epoch 0 new loss 5.318544253896107e-07 old loss 6.370780738507165e-07 BETTER
I0312 07:44:01.483119 1886633 finetune.py:68] layer 0_gate @ epoch 1 new loss 3.036007001355756e-08 old loss 3.0799863992569954e-08 BETTER
I0312 07:44:13.592975 1886914 finetune.py:68] layer 2_up @ epoch 3 new loss 5.907757554268755e-07 old loss 5.931893838351243e-07 BETTER
I0312 07:44:23.923792 1887047 finetune.py:68] layer 3_up @ epoch 1 new loss 1.1620417126323446e-06 old loss 1.1698946309479652e-06 BETTER
I0312 07:44:29.350111 1886781 finetune.py:68] layer 1_gate @ epoch 1 new loss 5.088890588922368e-07 old loss 5.318544253896107e-07 BETTER
I0312 07:44:31.415033 1886633 finetune.py:68] layer 0_gate @ epoch 2 new loss 2.9995309347441435e-08 old loss 3.036007001355756e-08 BETTER
I0312 07:44:43.503459 1886914 finetune.py:68] layer 2_up @ epoch 4 new loss 5.885283940187946e-07 old loss 5.907757554268755e-07 BETTER
I0312 07:44:53.439061 1887047 finetune.py:68] layer 3_up @ epoch 2 new loss 1.155146833298204e-06 old loss 1.1620417126323446e-06 BETTER
I0312 07:44:57.774666 1886781 finetune.py:68] layer 1_gate @ epoch 2 new loss 5.083356882096268e-07 old loss 5.088890588922368e-07 BETTER
I0312 07:44:58.767176 1886914 finetune.py:45] layer 2_gate initial loss 7.310874252652866e-07
I0312 07:45:01.665560 1886633 finetune.py:68] layer 0_gate @ epoch 3 new loss 2.96730497950648e-08 old loss 2.9995309347441435e-08 BETTER
I0312 07:45:23.133527 1887047 finetune.py:68] layer 3_up @ epoch 3 new loss 1.1488535847092862e-06 old loss 1.155146833298204e-06 BETTER
I0312 07:45:25.911244 1886781 finetune.py:68] layer 1_gate @ epoch 3 new loss 5.047555760029354e-07 old loss 5.083356882096268e-07 BETTER
I0312 07:45:26.349162 1886914 finetune.py:68] layer 2_gate @ epoch 0 new loss 7.287069934136525e-07 old loss 7.310874252652866e-07 BETTER
I0312 07:45:31.957986 1886633 finetune.py:68] layer 0_gate @ epoch 4 new loss 2.939809817803507e-08 old loss 2.96730497950648e-08 BETTER
I0312 07:45:47.669627 1886633 finetune.py:45] layer 0_down initial loss 6.291893583920682e-08
I0312 07:45:52.932223 1887047 finetune.py:68] layer 3_up @ epoch 4 new loss 1.1431066013756208e-06 old loss 1.1488535847092862e-06 BETTER
I0312 07:45:54.326726 1886781 finetune.py:68] layer 1_gate @ epoch 4 new loss 5.011406756239012e-07 old loss 5.047555760029354e-07 BETTER
I0312 07:45:54.621590 1886914 finetune.py:68] layer 2_gate @ epoch 1 new loss 7.267631190188695e-07 old loss 7.287069934136525e-07 BETTER
I0312 07:46:07.846921 1887047 finetune.py:45] layer 3_gate initial loss 1.456667860111338e-06
I0312 07:46:09.917210 1886781 finetune.py:45] layer 1_down initial loss 0.000250248092925176
I0312 07:46:15.286663 1886633 finetune.py:68] layer 0_down @ epoch 0 new loss 6.282299125359714e-08 old loss 6.291893583920682e-08 BETTER
I0312 07:46:22.949306 1886914 finetune.py:68] layer 2_gate @ epoch 2 new loss 7.249189479807683e-07 old loss 7.267631190188695e-07 BETTER
I0312 07:46:35.083873 1887047 finetune.py:68] layer 3_gate @ epoch 0 new loss 1.4505534409181564e-06 old loss 1.456667860111338e-06 BETTER
I0312 07:46:35.911934 1886781 finetune.py:68] layer 1_down @ epoch 0 new loss 0.00019045986118726432 old loss 0.000250248092925176 BETTER
I0312 07:46:43.773979 1886633 finetune.py:68] layer 0_down @ epoch 1 new loss 6.278927600078532e-08 old loss 6.282299125359714e-08 BETTER
I0312 07:46:51.245066 1886914 finetune.py:68] layer 2_gate @ epoch 3 new loss 7.231894869619282e-07 old loss 7.249189479807683e-07 BETTER
I0312 07:47:02.704806 1886781 finetune.py:68] layer 1_down @ epoch 1 new loss 0.00017297726299148053 old loss 0.00019045986118726432 BETTER
I0312 07:47:03.416589 1887047 finetune.py:68] layer 3_gate @ epoch 1 new loss 1.4455299606197514e-06 old loss 1.4505534409181564e-06 BETTER
I0312 07:47:12.273872 1886633 finetune.py:68] layer 0_down @ epoch 2 new loss 6.27679952458493e-08 old loss 6.278927600078532e-08 BETTER
I0312 07:47:19.487502 1886914 finetune.py:68] layer 2_gate @ epoch 4 new loss 7.215086270662141e-07 old loss 7.231894869619282e-07 BETTER
I0312 07:47:29.435537 1886781 finetune.py:68] layer 1_down @ epoch 2 new loss 0.00017033246695064008 old loss 0.00017297726299148053 BETTER
I0312 07:47:31.627861 1887047 finetune.py:68] layer 3_gate @ epoch 2 new loss 1.4408901733986568e-06 old loss 1.4455299606197514e-06 BETTER
I0312 07:47:35.268092 1886914 finetune.py:45] layer 2_down initial loss 1.0620356079016346e-06
I0312 07:47:40.759296 1886633 finetune.py:68] layer 0_down @ epoch 3 new loss 6.274547814655307e-08 old loss 6.27679952458493e-08 BETTER
I0312 07:47:56.507827 1886781 finetune.py:68] layer 1_down @ epoch 3 new loss 0.000170062281540595 old loss 0.00017033246695064008 BETTER
I0312 07:47:59.895113 1887047 finetune.py:68] layer 3_gate @ epoch 3 new loss 1.436590650882863e-06 old loss 1.4408901733986568e-06 BETTER
I0312 07:48:01.500561 1886914 finetune.py:68] layer 2_down @ epoch 0 new loss 1.0615898418109282e-06 old loss 1.0620356079016346e-06 BETTER
I0312 07:48:09.393964 1886633 finetune.py:68] layer 0_down @ epoch 4 new loss 6.273159414149632e-08 old loss 6.274547814655307e-08 BETTER
0_v proxy err 0.001562951598316431 tr(WHW.T) 4.225186347961426
0_q proxy err 0.00010263844160363078 tr(WHW.T) 2710.2373046875
0_k proxy err 9.33235787670128e-05 tr(WHW.T) 1698.9102783203125
0_o proxy err 0.00013993863831274211 tr(WHW.T) 0.9690864086151123
0_up proxy err 0.00031692057382315397 tr(WHW.T) 43.270538330078125
0_gate proxy err 0.00023835798492655158 tr(WHW.T) 63.47172546386719
0_down proxy err 0.000217798791709356 tr(WHW.T) 0.6568756699562073
I0312 07:48:23.679236 1886781 finetune.py:68] layer 1_down @ epoch 4 new loss 0.00017000774096231908 old loss 0.000170062281540595 BETTER
1_v proxy err 0.002451320644468069 tr(WHW.T) 16.465883255004883
1_q proxy err 7.957599882502109e-05 tr(WHW.T) 4778.42041015625
1_k proxy err 0.00010098243365064263 tr(WHW.T) 4995.7216796875
1_o proxy err 0.0005479453247971833 tr(WHW.T) 1.1127949953079224
1_up proxy err 0.00037235987838357687 tr(WHW.T) 109.70446014404297
1_gate proxy err 0.000224110612180084 tr(WHW.T) 221.37985229492188
1_down proxy err 0.0005056320806033909 tr(WHW.T) 2041.3953857421875
I0312 07:48:28.319380 1887047 finetune.py:68] layer 3_gate @ epoch 4 new loss 1.432514181942679e-06 old loss 1.436590650882863e-06 BETTER
I0312 07:48:28.906804 1886914 finetune.py:68] layer 2_down @ epoch 1 new loss 1.0614567145239562e-06 old loss 1.0615898418109282e-06 BETTER
I0312 07:48:43.880004 1887047 finetune.py:45] layer 3_down initial loss 2.1270138859108556e-06
I0312 07:48:55.932416 1886914 finetune.py:68] layer 2_down @ epoch 2 new loss 1.0613504173306865e-06 old loss 1.0614567145239562e-06 BETTER
I0312 07:49:09.657287 1887047 finetune.py:68] layer 3_down @ epoch 0 new loss 2.12643135455437e-06 old loss 2.1270138859108556e-06 BETTER
I0312 07:49:22.978700 1886914 finetune.py:68] layer 2_down @ epoch 3 new loss 1.0612711776047945e-06 old loss 1.0613504173306865e-06 BETTER
I0312 07:49:35.889719 1887047 finetune.py:68] layer 3_down @ epoch 1 new loss 2.126194885931909e-06 old loss 2.12643135455437e-06 BETTER
I0312 07:49:42.709482 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 4 in 73.09787201881409s
I0312 07:49:46.041306 1887433 config.py:54] PyTorch version 2.1.1 available.
I0312 07:49:47.141400 1886474 quantize_finetune_llama.py:183] layer 5 gpu 1
I0312 07:49:47.232074 1887433 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 07:49:49.949107 1886914 finetune.py:68] layer 2_down @ epoch 4 new loss 1.0612221785777365e-06 old loss 1.0612711776047945e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
2_v proxy err 0.0007037645555101335 tr(WHW.T) 136.67332458496094
2_q proxy err 5.385595068219118e-05 tr(WHW.T) 7753.4990234375
2_k proxy err 4.931496368953958e-05 tr(WHW.T) 10205.76953125
2_o proxy err 0.0005116495885886252 tr(WHW.T) 1.460561990737915
2_up proxy err 0.00042975714313797653 tr(WHW.T) 193.43820190429688
2_gate proxy err 0.0003048313083127141 tr(WHW.T) 306.671630859375
2_down proxy err 0.00046336877858266234 tr(WHW.T) 3.011180877685547
I0312 07:49:55.765898 1887433 finetune.py:45] layer 4_v initial loss 1.2181573083580588e-06
I0312 07:50:02.142865 1887047 finetune.py:68] layer 3_down @ epoch 2 new loss 2.1260289031488355e-06 old loss 2.126194885931909e-06 BETTER
I0312 07:50:28.574374 1887047 finetune.py:68] layer 3_down @ epoch 3 new loss 2.1258986180328066e-06 old loss 2.1260289031488355e-06 BETTER
I0312 07:50:28.879456 1887433 finetune.py:68] layer 4_v @ epoch 0 new loss 5.963316311863309e-07 old loss 1.2181573083580588e-06 BETTER
I0312 07:50:54.946475 1887047 finetune.py:68] layer 3_down @ epoch 4 new loss 2.125766968674725e-06 old loss 2.1258986180328066e-06 BETTER
3_v proxy err 0.0007481459761038423 tr(WHW.T) 284.77557373046875
3_q proxy err 7.863191422075033e-05 tr(WHW.T) 7218.32861328125
3_k proxy err 6.756374932592735e-05 tr(WHW.T) 10075.5966796875
3_o proxy err 0.00047714117681607604 tr(WHW.T) 3.355159044265747
3_up proxy err 0.00047481252113357186 tr(WHW.T) 284.7733459472656
3_gate proxy err 0.00031237336224876344 tr(WHW.T) 478.1452941894531
3_down proxy err 0.0004702758160419762 tr(WHW.T) 6.132971286773682
I0312 07:51:03.235194 1887433 finetune.py:68] layer 4_v @ epoch 1 new loss 5.007834715797799e-07 old loss 5.963316311863309e-07 BETTER
I0312 07:51:04.219210 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 5 in 69.4908516407013s
I0312 07:51:07.381842 1887564 config.py:54] PyTorch version 2.1.1 available.
I0312 07:51:08.350902 1886474 quantize_finetune_llama.py:183] layer 6 gpu 2
I0312 07:51:08.417179 1887564 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 07:51:16.349431 1887564 finetune.py:45] layer 5_v initial loss 1.7726962369124522e-06
I0312 07:51:38.027679 1887433 finetune.py:68] layer 4_v @ epoch 2 new loss 4.5800283032804145e-07 old loss 5.007834715797799e-07 BETTER
I0312 07:51:47.493105 1887564 finetune.py:68] layer 5_v @ epoch 0 new loss 9.037004247147706e-07 old loss 1.7726962369124522e-06 BETTER
I0312 07:52:13.002766 1887433 finetune.py:68] layer 4_v @ epoch 3 new loss 4.291911750442523e-07 old loss 4.5800283032804145e-07 BETTER
I0312 07:52:16.882596 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 6 in 68.15614438056946s
I0312 07:52:19.543003 1887564 finetune.py:68] layer 5_v @ epoch 1 new loss 7.800367143317999e-07 old loss 9.037004247147706e-07 BETTER
I0312 07:52:19.992522 1887710 config.py:54] PyTorch version 2.1.1 available.
I0312 07:52:21.013228 1886474 quantize_finetune_llama.py:183] layer 7 gpu 3
I0312 07:52:21.089341 1887710 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 07:52:29.127995 1887710 finetune.py:45] layer 6_v initial loss 2.259617986055673e-06
I0312 07:52:47.934416 1887433 finetune.py:68] layer 4_v @ epoch 4 new loss 4.123313885884272e-07 old loss 4.291911750442523e-07 BETTER
I0312 07:52:51.850957 1887564 finetune.py:68] layer 5_v @ epoch 2 new loss 7.195939133453066e-07 old loss 7.800367143317999e-07 BETTER
I0312 07:52:57.186897 1887433 finetune.py:45] layer 4_q initial loss 6.432279064938484e-07
I0312 07:53:00.460258 1887710 finetune.py:68] layer 6_v @ epoch 0 new loss 1.1035052693841862e-06 old loss 2.259617986055673e-06 BETTER
I0312 07:53:24.145170 1887564 finetune.py:68] layer 5_v @ epoch 3 new loss 6.817231223976705e-07 old loss 7.195939133453066e-07 BETTER
I0312 07:53:29.595365 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 7 in 68.17009449005127s
I0312 07:53:30.732401 1887433 finetune.py:68] layer 4_q @ epoch 0 new loss 4.913470093015349e-07 old loss 6.432279064938484e-07 BETTER
I0312 07:53:32.778681 1887710 finetune.py:68] layer 6_v @ epoch 1 new loss 9.64190121521824e-07 old loss 1.1035052693841862e-06 BETTER
I0312 07:53:32.892649 1887841 config.py:54] PyTorch version 2.1.1 available.
I0312 07:53:33.946189 1886474 quantize_finetune_llama.py:183] layer 8 gpu 0
I0312 07:53:34.012573 1887841 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 07:53:41.761684 1887841 finetune.py:45] layer 7_v initial loss 2.945769438156276e-06
I0312 07:53:56.754997 1887564 finetune.py:68] layer 5_v @ epoch 4 new loss 6.571580115632969e-07 old loss 6.817231223976705e-07 BETTER
I0312 07:54:05.183799 1887433 finetune.py:68] layer 4_q @ epoch 1 new loss 4.627914620414231e-07 old loss 4.913470093015349e-07 BETTER
I0312 07:54:05.605813 1887710 finetune.py:68] layer 6_v @ epoch 2 new loss 8.953597330219054e-07 old loss 9.64190121521824e-07 BETTER
I0312 07:54:05.910435 1887564 finetune.py:45] layer 5_q initial loss 9.714957514006528e-07
I0312 07:54:12.564167 1887841 finetune.py:68] layer 7_v @ epoch 0 new loss 1.457034613849828e-06 old loss 2.945769438156276e-06 BETTER
I0312 07:54:37.435540 1887564 finetune.py:68] layer 5_q @ epoch 0 new loss 7.649740041415498e-07 old loss 9.714957514006528e-07 BETTER
I0312 07:54:38.558403 1887710 finetune.py:68] layer 6_v @ epoch 3 new loss 8.717188961782085e-07 old loss 8.953597330219054e-07 BETTER
I0312 07:54:39.745183 1887433 finetune.py:68] layer 4_q @ epoch 2 new loss 4.43337825117851e-07 old loss 4.627914620414231e-07 BETTER
I0312 07:54:44.427349 1887841 finetune.py:68] layer 7_v @ epoch 1 new loss 1.2872433217125945e-06 old loss 1.457034613849828e-06 BETTER
I0312 07:55:09.872843 1887564 finetune.py:68] layer 5_q @ epoch 1 new loss 7.272502102750877e-07 old loss 7.649740041415498e-07 BETTER
I0312 07:55:11.321415 1887710 finetune.py:68] layer 6_v @ epoch 4 new loss 8.40542440982972e-07 old loss 8.717188961782085e-07 BETTER
I0312 07:55:14.332463 1887433 finetune.py:68] layer 4_q @ epoch 3 new loss 4.28329002488681e-07 old loss 4.43337825117851e-07 BETTER
I0312 07:55:16.579089 1887841 finetune.py:68] layer 7_v @ epoch 2 new loss 1.2035098961860058e-06 old loss 1.2872433217125945e-06 BETTER
I0312 07:55:20.823193 1887710 finetune.py:45] layer 6_q initial loss 1.4712810525452369e-06
I0312 07:55:42.503748 1887564 finetune.py:68] layer 5_q @ epoch 2 new loss 7.029311746009625e-07 old loss 7.272502102750877e-07 BETTER
I0312 07:55:48.746155 1887433 finetune.py:68] layer 4_q @ epoch 4 new loss 4.162641005223122e-07 old loss 4.28329002488681e-07 BETTER
I0312 07:55:48.792402 1887841 finetune.py:68] layer 7_v @ epoch 3 new loss 1.1474189705040772e-06 old loss 1.2035098961860058e-06 BETTER
I0312 07:55:52.557057 1887710 finetune.py:68] layer 6_q @ epoch 0 new loss 1.0983187621604884e-06 old loss 1.4712810525452369e-06 BETTER
I0312 07:55:58.298946 1887433 finetune.py:45] layer 4_k initial loss 6.219651709216123e-07
I0312 07:56:14.833233 1887564 finetune.py:68] layer 5_q @ epoch 3 new loss 6.843478104201495e-07 old loss 7.029311746009625e-07 BETTER
I0312 07:56:21.024451 1887841 finetune.py:68] layer 7_v @ epoch 4 new loss 1.1238396382395877e-06 old loss 1.1474189705040772e-06 BETTER
I0312 07:56:24.797364 1887710 finetune.py:68] layer 6_q @ epoch 1 new loss 1.024702896756935e-06 old loss 1.0983187621604884e-06 BETTER
I0312 07:56:30.279170 1887841 finetune.py:45] layer 7_q initial loss 2.0383645278343465e-06
I0312 07:56:31.490064 1887433 finetune.py:68] layer 4_k @ epoch 0 new loss 5.743204383179545e-07 old loss 6.219651709216123e-07 BETTER
I0312 07:56:47.284388 1887564 finetune.py:68] layer 5_q @ epoch 4 new loss 6.688595135528885e-07 old loss 6.843478104201495e-07 BETTER
I0312 07:56:56.209081 1887564 finetune.py:45] layer 5_k initial loss 9.401731517755252e-07
I0312 07:56:57.379511 1887710 finetune.py:68] layer 6_q @ epoch 2 new loss 9.863982768365531e-07 old loss 1.024702896756935e-06 BETTER
I0312 07:57:01.654962 1887841 finetune.py:68] layer 7_q @ epoch 0 new loss 1.4598608686355874e-06 old loss 2.0383645278343465e-06 BETTER
I0312 07:57:05.479187 1887433 finetune.py:68] layer 4_k @ epoch 1 new loss 5.582252811109356e-07 old loss 5.743204383179545e-07 BETTER
I0312 07:57:27.614852 1887564 finetune.py:68] layer 5_k @ epoch 0 new loss 8.548527716811805e-07 old loss 9.401731517755252e-07 BETTER
I0312 07:57:29.740228 1887710 finetune.py:68] layer 6_q @ epoch 3 new loss 9.568624363964773e-07 old loss 9.863982768365531e-07 BETTER
I0312 07:57:34.277394 1887841 finetune.py:68] layer 7_q @ epoch 1 new loss 1.3735505035583628e-06 old loss 1.4598608686355874e-06 BETTER
I0312 07:57:39.631330 1887433 finetune.py:68] layer 4_k @ epoch 2 new loss 5.460065608531295e-07 old loss 5.582252811109356e-07 BETTER
I0312 07:57:59.919663 1887564 finetune.py:68] layer 5_k @ epoch 1 new loss 8.321429731950047e-07 old loss 8.548527716811805e-07 BETTER
I0312 07:58:02.498680 1887710 finetune.py:68] layer 6_q @ epoch 4 new loss 9.343311830889434e-07 old loss 9.568624363964773e-07 BETTER
I0312 07:58:06.395371 1887841 finetune.py:68] layer 7_q @ epoch 2 new loss 1.3241027545518591e-06 old loss 1.3735505035583628e-06 BETTER
I0312 07:58:11.834763 1887710 finetune.py:45] layer 6_k initial loss 1.4807649222348118e-06
I0312 07:58:13.808984 1887433 finetune.py:68] layer 4_k @ epoch 3 new loss 5.360616341931745e-07 old loss 5.460065608531295e-07 BETTER
I0312 07:58:31.991927 1887564 finetune.py:68] layer 5_k @ epoch 2 new loss 8.161579216903192e-07 old loss 8.321429731950047e-07 BETTER
I0312 07:58:38.425371 1887841 finetune.py:68] layer 7_q @ epoch 3 new loss 1.2898345858047833e-06 old loss 1.3241027545518591e-06 BETTER
I0312 07:58:43.282575 1887710 finetune.py:68] layer 6_k @ epoch 0 new loss 1.3112734222886502e-06 old loss 1.4807649222348118e-06 BETTER
I0312 07:58:47.644160 1887433 finetune.py:68] layer 4_k @ epoch 4 new loss 5.274718546388613e-07 old loss 5.360616341931745e-07 BETTER
I0312 07:58:56.922903 1887433 finetune.py:45] layer 4_o initial loss 1.2324034059929545e-06
I0312 07:59:03.962403 1887564 finetune.py:68] layer 5_k @ epoch 3 new loss 8.032502591959201e-07 old loss 8.161579216903192e-07 BETTER
I0312 07:59:10.535258 1887841 finetune.py:68] layer 7_q @ epoch 4 new loss 1.2620164397958433e-06 old loss 1.2898345858047833e-06 BETTER
I0312 07:59:15.396383 1887710 finetune.py:68] layer 6_k @ epoch 1 new loss 1.2792138477379922e-06 old loss 1.3112734222886502e-06 BETTER
I0312 07:59:19.690985 1887841 finetune.py:45] layer 7_k initial loss 2.091928081426886e-06
I0312 07:59:29.652201 1887433 finetune.py:68] layer 4_o @ epoch 0 new loss 1.1827551134047098e-06 old loss 1.2324034059929545e-06 BETTER
I0312 07:59:36.391668 1887564 finetune.py:68] layer 5_k @ epoch 4 new loss 7.929248226901109e-07 old loss 8.032502591959201e-07 BETTER
I0312 07:59:45.607382 1887564 finetune.py:45] layer 5_o initial loss 2.063775809801882e-06
I0312 07:59:47.625160 1887710 finetune.py:68] layer 6_k @ epoch 2 new loss 1.2580677548612584e-06 old loss 1.2792138477379922e-06 BETTER
I0312 07:59:50.659054 1887841 finetune.py:68] layer 7_k @ epoch 0 new loss 1.8483528947399464e-06 old loss 2.091928081426886e-06 BETTER
I0312 08:00:03.476063 1887433 finetune.py:68] layer 4_o @ epoch 1 new loss 1.1606567795752198e-06 old loss 1.1827551134047098e-06 BETTER
I0312 08:00:16.378147 1887564 finetune.py:68] layer 5_o @ epoch 0 new loss 1.9414831058384152e-06 old loss 2.063775809801882e-06 BETTER
I0312 08:00:19.893602 1887710 finetune.py:68] layer 6_k @ epoch 3 new loss 1.2434099971869728e-06 old loss 1.2580677548612584e-06 BETTER
I0312 08:00:22.438236 1887841 finetune.py:68] layer 7_k @ epoch 1 new loss 1.7928567785929772e-06 old loss 1.8483528947399464e-06 BETTER
I0312 08:00:36.773948 1887433 finetune.py:68] layer 4_o @ epoch 2 new loss 1.1443114544817945e-06 old loss 1.1606567795752198e-06 BETTER
I0312 08:00:47.967798 1887564 finetune.py:68] layer 5_o @ epoch 1 new loss 1.8805975514624151e-06 old loss 1.9414831058384152e-06 BETTER
I0312 08:00:52.126271 1887710 finetune.py:68] layer 6_k @ epoch 4 new loss 1.2295164424358518e-06 old loss 1.2434099971869728e-06 BETTER
I0312 08:00:54.123293 1887841 finetune.py:68] layer 7_k @ epoch 2 new loss 1.7555049680595403e-06 old loss 1.7928567785929772e-06 BETTER
I0312 08:01:01.481167 1887710 finetune.py:45] layer 6_o initial loss 2.916882976933266e-06
I0312 08:01:10.326067 1887433 finetune.py:68] layer 4_o @ epoch 3 new loss 1.13100986709469e-06 old loss 1.1443114544817945e-06 BETTER
I0312 08:01:19.440954 1887564 finetune.py:68] layer 5_o @ epoch 2 new loss 1.8362422906648135e-06 old loss 1.8805975514624151e-06 BETTER
I0312 08:01:25.933681 1887841 finetune.py:68] layer 7_k @ epoch 3 new loss 1.7280168549405062e-06 old loss 1.7555049680595403e-06 BETTER
I0312 08:01:32.250265 1887710 finetune.py:68] layer 6_o @ epoch 0 new loss 2.7256000976194628e-06 old loss 2.916882976933266e-06 BETTER
I0312 08:01:43.974263 1887433 finetune.py:68] layer 4_o @ epoch 4 new loss 1.1194105127287912e-06 old loss 1.13100986709469e-06 BETTER
I0312 08:01:50.970262 1887564 finetune.py:68] layer 5_o @ epoch 3 new loss 1.8009578752753441e-06 old loss 1.8362422906648135e-06 BETTER
I0312 08:01:57.624365 1887841 finetune.py:68] layer 7_k @ epoch 4 new loss 1.7047378833012772e-06 old loss 1.7280168549405062e-06 BETTER
I0312 08:01:59.724519 1887433 finetune.py:45] layer 4_up initial loss 1.8708290099311853e-06
I0312 08:02:03.765088 1887710 finetune.py:68] layer 6_o @ epoch 1 new loss 2.6467744191904785e-06 old loss 2.7256000976194628e-06 BETTER
I0312 08:02:07.118094 1887841 finetune.py:45] layer 7_o initial loss 4.069319402333349e-06
I0312 08:02:22.481011 1887564 finetune.py:68] layer 5_o @ epoch 4 new loss 1.7717964055918856e-06 old loss 1.8009578752753441e-06 BETTER
I0312 08:02:30.450694 1887433 finetune.py:68] layer 4_up @ epoch 0 new loss 1.8454015844326932e-06 old loss 1.8708290099311853e-06 BETTER
I0312 08:02:35.692618 1887710 finetune.py:68] layer 6_o @ epoch 2 new loss 2.5922820441337535e-06 old loss 2.6467744191904785e-06 BETTER
I0312 08:02:37.391690 1887841 finetune.py:68] layer 7_o @ epoch 0 new loss 3.730886419361923e-06 old loss 4.069319402333349e-06 BETTER
I0312 08:02:37.532459 1887564 finetune.py:45] layer 5_up initial loss 2.9819461815350223e-06
I0312 08:03:02.134046 1887433 finetune.py:68] layer 4_up @ epoch 1 new loss 1.8289787249159417e-06 old loss 1.8454015844326932e-06 BETTER
I0312 08:03:06.760949 1887564 finetune.py:68] layer 5_up @ epoch 0 new loss 2.9269015158206457e-06 old loss 2.9819461815350223e-06 BETTER
I0312 08:03:08.216344 1887710 finetune.py:68] layer 6_o @ epoch 3 new loss 2.5501788059045793e-06 old loss 2.5922820441337535e-06 BETTER
I0312 08:03:09.122616 1887841 finetune.py:68] layer 7_o @ epoch 1 new loss 3.5993766687170137e-06 old loss 3.730886419361923e-06 BETTER
I0312 08:03:34.080279 1887433 finetune.py:68] layer 4_up @ epoch 2 new loss 1.8151405356547912e-06 old loss 1.8289787249159417e-06 BETTER
I0312 08:03:37.170182 1887564 finetune.py:68] layer 5_up @ epoch 1 new loss 2.8905899398523616e-06 old loss 2.9269015158206457e-06 BETTER
I0312 08:03:40.372087 1887841 finetune.py:68] layer 7_o @ epoch 2 new loss 3.5128141462337226e-06 old loss 3.5993766687170137e-06 BETTER
I0312 08:03:40.533825 1887710 finetune.py:68] layer 6_o @ epoch 4 new loss 2.5160156837955583e-06 old loss 2.5501788059045793e-06 BETTER
I0312 08:03:55.585478 1887710 finetune.py:45] layer 6_up initial loss 4.371969680505572e-06
I0312 08:04:05.970216 1887433 finetune.py:68] layer 4_up @ epoch 3 new loss 1.8027416217591963e-06 old loss 1.8151405356547912e-06 BETTER
I0312 08:04:07.026416 1887564 finetune.py:68] layer 5_up @ epoch 2 new loss 2.859939513655263e-06 old loss 2.8905899398523616e-06 BETTER
I0312 08:04:11.403753 1887841 finetune.py:68] layer 7_o @ epoch 3 new loss 3.4478050565667218e-06 old loss 3.5128141462337226e-06 BETTER
I0312 08:04:24.639101 1887710 finetune.py:68] layer 6_up @ epoch 0 new loss 4.265692041371949e-06 old loss 4.371969680505572e-06 BETTER
I0312 08:04:36.954145 1887564 finetune.py:68] layer 5_up @ epoch 3 new loss 2.832965037669055e-06 old loss 2.859939513655263e-06 BETTER
I0312 08:04:37.753938 1887433 finetune.py:68] layer 4_up @ epoch 4 new loss 1.791354407032486e-06 old loss 1.8027416217591963e-06 BETTER
I0312 08:04:42.393492 1887841 finetune.py:68] layer 7_o @ epoch 4 new loss 3.3954897844523657e-06 old loss 3.4478050565667218e-06 BETTER
I0312 08:04:53.062438 1887433 finetune.py:45] layer 4_gate initial loss 2.348069301660871e-06
I0312 08:04:54.570661 1887710 finetune.py:68] layer 6_up @ epoch 1 new loss 4.205396635370562e-06 old loss 4.265692041371949e-06 BETTER
I0312 08:04:57.644008 1887841 finetune.py:45] layer 7_up initial loss 5.940595656284131e-06
I0312 08:05:07.137514 1887564 finetune.py:68] layer 5_up @ epoch 4 new loss 2.8091671993024647e-06 old loss 2.832965037669055e-06 BETTER
I0312 08:05:22.140551 1887433 finetune.py:68] layer 4_gate @ epoch 0 new loss 2.331445784875541e-06 old loss 2.348069301660871e-06 BETTER
I0312 08:05:22.717254 1887564 finetune.py:45] layer 5_gate initial loss 3.6671892758022295e-06
I0312 08:05:24.720825 1887710 finetune.py:68] layer 6_up @ epoch 2 new loss 4.157064722676296e-06 old loss 4.205396635370562e-06 BETTER
I0312 08:05:26.477578 1887841 finetune.py:68] layer 7_up @ epoch 0 new loss 5.7498768910591025e-06 old loss 5.940595656284131e-06 BETTER
I0312 08:05:50.427467 1887564 finetune.py:68] layer 5_gate @ epoch 0 new loss 3.632332891356782e-06 old loss 3.6671892758022295e-06 BETTER
I0312 08:05:52.267637 1887433 finetune.py:68] layer 4_gate @ epoch 1 new loss 2.3199268071039114e-06 old loss 2.331445784875541e-06 BETTER
I0312 08:05:54.852052 1887710 finetune.py:68] layer 6_up @ epoch 3 new loss 4.115805040783016e-06 old loss 4.157064722676296e-06 BETTER
I0312 08:05:56.229066 1887841 finetune.py:68] layer 7_up @ epoch 1 new loss 5.652459094562801e-06 old loss 5.7498768910591025e-06 BETTER
I0312 08:06:18.884774 1887564 finetune.py:68] layer 5_gate @ epoch 1 new loss 3.6083122267882572e-06 old loss 3.632332891356782e-06 BETTER
I0312 08:06:22.391665 1887433 finetune.py:68] layer 4_gate @ epoch 2 new loss 2.3095694814401213e-06 old loss 2.3199268071039114e-06 BETTER
I0312 08:06:25.114382 1887710 finetune.py:68] layer 6_up @ epoch 4 new loss 4.080095095559955e-06 old loss 4.115805040783016e-06 BETTER
I0312 08:06:25.957689 1887841 finetune.py:68] layer 7_up @ epoch 2 new loss 5.576367584581021e-06 old loss 5.652459094562801e-06 BETTER
I0312 08:06:40.574594 1887710 finetune.py:45] layer 6_gate initial loss 5.323416189639829e-06
I0312 08:06:47.338288 1887564 finetune.py:68] layer 5_gate @ epoch 2 new loss 3.587301080187899e-06 old loss 3.6083122267882572e-06 BETTER
I0312 08:06:52.406557 1887433 finetune.py:68] layer 4_gate @ epoch 3 new loss 2.3000643523118924e-06 old loss 2.3095694814401213e-06 BETTER
I0312 08:06:55.587073 1887841 finetune.py:68] layer 7_up @ epoch 3 new loss 5.513536962098442e-06 old loss 5.576367584581021e-06 BETTER
I0312 08:07:08.107103 1887710 finetune.py:68] layer 6_gate @ epoch 0 new loss 5.257248631096445e-06 old loss 5.323416189639829e-06 BETTER
I0312 08:07:15.593083 1887564 finetune.py:68] layer 5_gate @ epoch 3 new loss 3.5679399843502324e-06 old loss 3.587301080187899e-06 BETTER
I0312 08:07:22.348360 1887433 finetune.py:68] layer 4_gate @ epoch 4 new loss 2.291074451932218e-06 old loss 2.3000643523118924e-06 BETTER
I0312 08:07:25.141386 1887841 finetune.py:68] layer 7_up @ epoch 4 new loss 5.459838575916365e-06 old loss 5.513536962098442e-06 BETTER
I0312 08:07:36.536662 1887710 finetune.py:68] layer 6_gate @ epoch 1 new loss 5.216820682107937e-06 old loss 5.257248631096445e-06 BETTER
I0312 08:07:38.698900 1887433 finetune.py:45] layer 4_down initial loss 3.598934199544601e-06
I0312 08:07:40.750637 1887841 finetune.py:45] layer 7_gate initial loss 7.151283170969691e-06
I0312 08:07:43.898434 1887564 finetune.py:68] layer 5_gate @ epoch 4 new loss 3.550454948708648e-06 old loss 3.5679399843502324e-06 BETTER
I0312 08:07:59.850739 1887564 finetune.py:45] layer 5_down initial loss 5.440908807941014e-06
I0312 08:08:04.732248 1887710 finetune.py:68] layer 6_gate @ epoch 2 new loss 5.183268967812182e-06 old loss 5.216820682107937e-06 BETTER
I0312 08:08:05.923714 1887433 finetune.py:68] layer 4_down @ epoch 0 new loss 3.5981377095595235e-06 old loss 3.598934199544601e-06 BETTER
I0312 08:08:07.669974 1887841 finetune.py:68] layer 7_gate @ epoch 0 new loss 7.037414889055071e-06 old loss 7.151283170969691e-06 BETTER
I0312 08:08:25.619757 1887564 finetune.py:68] layer 5_down @ epoch 0 new loss 5.439786491479026e-06 old loss 5.440908807941014e-06 BETTER
I0312 08:08:33.037583 1887710 finetune.py:68] layer 6_gate @ epoch 3 new loss 5.153116035216954e-06 old loss 5.183268967812182e-06 BETTER
I0312 08:08:34.219907 1887433 finetune.py:68] layer 4_down @ epoch 1 new loss 3.5979355743620545e-06 old loss 3.5981377095595235e-06 BETTER
I0312 08:08:35.501242 1887841 finetune.py:68] layer 7_gate @ epoch 1 new loss 6.9743573476444e-06 old loss 7.037414889055071e-06 BETTER
I0312 08:08:53.250406 1887564 finetune.py:68] layer 5_down @ epoch 1 new loss 5.439642791316146e-06 old loss 5.439786491479026e-06 BETTER
I0312 08:09:01.936829 1887710 finetune.py:68] layer 6_gate @ epoch 4 new loss 5.1265133151900955e-06 old loss 5.153116035216954e-06 BETTER
I0312 08:09:03.425572 1887433 finetune.py:68] layer 4_down @ epoch 2 new loss 3.59789737558458e-06 old loss 3.5979355743620545e-06 BETTER
I0312 08:09:04.189342 1887841 finetune.py:68] layer 7_gate @ epoch 2 new loss 6.922434749867534e-06 old loss 6.9743573476444e-06 BETTER
I0312 08:09:18.775779 1887710 finetune.py:45] layer 6_down initial loss 7.96089898358332e-06
I0312 08:09:20.585068 1887564 finetune.py:68] layer 5_down @ epoch 2 new loss 5.439382221084088e-06 old loss 5.439642791316146e-06 BETTER
I0312 08:09:31.985355 1887433 finetune.py:68] layer 4_down @ epoch 3 new loss 3.5977154766442254e-06 old loss 3.59789737558458e-06 BETTER
I0312 08:09:32.471276 1887841 finetune.py:68] layer 7_gate @ epoch 3 new loss 6.87810006638756e-06 old loss 6.922434749867534e-06 BETTER
I0312 08:09:44.577209 1887710 finetune.py:68] layer 6_down @ epoch 0 new loss 7.958916285133455e-06 old loss 7.96089898358332e-06 BETTER
I0312 08:09:47.424090 1887564 finetune.py:68] layer 5_down @ epoch 3 new loss 5.439343112811912e-06 old loss 5.439382221084088e-06 BETTER
I0312 08:10:00.575956 1887433 finetune.py:68] layer 4_down @ epoch 4 new loss 3.597632257879013e-06 old loss 3.5977154766442254e-06 BETTER
I0312 08:10:00.946603 1887841 finetune.py:68] layer 7_gate @ epoch 4 new loss 6.839300112915225e-06 old loss 6.87810006638756e-06 BETTER
4_v proxy err 0.00072060851380229 tr(WHW.T) 274.6131286621094
4_q proxy err 7.489226118195802e-05 tr(WHW.T) 6916.35693359375
4_k proxy err 6.352918717311695e-05 tr(WHW.T) 10416.6884765625
4_o proxy err 0.0005095399101264775 tr(WHW.T) 5.13873815536499
4_up proxy err 0.00046325684525072575 tr(WHW.T) 397.67999267578125
4_gate proxy err 0.0002619852893985808 tr(WHW.T) 821.0234985351562
4_down proxy err 0.00047396065201610327 tr(WHW.T) 11.565248489379883
I0312 08:10:11.513038 1887710 finetune.py:68] layer 6_down @ epoch 1 new loss 7.958470632729586e-06 old loss 7.958916285133455e-06 BETTER
I0312 08:10:14.272722 1887564 finetune.py:68] layer 5_down @ epoch 4 new loss 5.4391921366914175e-06 old loss 5.439343112811912e-06 BETTER
5_v proxy err 0.0007212361670099199 tr(WHW.T) 298.47540283203125
5_q proxy err 7.774305413477123e-05 tr(WHW.T) 6771.6435546875
5_k proxy err 6.683908577542752e-05 tr(WHW.T) 10843.248046875
5_o proxy err 0.0006413983646780252 tr(WHW.T) 7.953219890594482
5_up proxy err 0.0004559607768896967 tr(WHW.T) 506.6575927734375
5_gate proxy err 0.00024523294996470213 tr(WHW.T) 1104.526123046875
5_down proxy err 0.000502798066008836 tr(WHW.T) 15.655978202819824
I0312 08:10:17.042855 1887841 finetune.py:45] layer 7_down initial loss 1.0641351764206775e-05
I0312 08:10:38.223631 1887710 finetune.py:68] layer 6_down @ epoch 2 new loss 7.95817504695151e-06 old loss 7.958470632729586e-06 BETTER
I0312 08:10:42.230058 1887841 finetune.py:68] layer 7_down @ epoch 0 new loss 1.0638722415023949e-05 old loss 1.0641351764206775e-05 BETTER
I0312 08:11:05.064248 1887710 finetune.py:68] layer 6_down @ epoch 3 new loss 7.957935849844944e-06 old loss 7.95817504695151e-06 BETTER
I0312 08:11:08.375323 1887841 finetune.py:68] layer 7_down @ epoch 1 new loss 1.063811760104727e-05 old loss 1.0638722415023949e-05 BETTER
I0312 08:11:32.015235 1887710 finetune.py:68] layer 6_down @ epoch 4 new loss 7.957712114148308e-06 old loss 7.957935849844944e-06 BETTER
I0312 08:11:32.632732 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 8 in 74.0755033493042s
6_v proxy err 0.000723537290468812 tr(WHW.T) 443.5464782714844
6_q proxy err 9.098614100366831e-05 tr(WHW.T) 7576.9169921875
6_k proxy err 7.327218190766871e-05 tr(WHW.T) 10410.7080078125
6_o proxy err 0.0006435983232222497 tr(WHW.T) 11.57405948638916
6_up proxy err 0.0004546719719655812 tr(WHW.T) 617.2670288085938
6_gate proxy err 0.00022117109620012343 tr(WHW.T) 1554.782470703125
6_down proxy err 0.0005148847121745348 tr(WHW.T) 22.99199867248535
I0312 08:11:34.861963 1887841 finetune.py:68] layer 7_down @ epoch 2 new loss 1.0637974810379092e-05 old loss 1.063811760104727e-05 BETTER
I0312 08:11:36.090589 1888227 config.py:54] PyTorch version 2.1.1 available.
I0312 08:11:37.215526 1886474 quantize_finetune_llama.py:183] layer 9 gpu 1
I0312 08:11:37.297151 1888227 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:11:46.124827 1888227 finetune.py:45] layer 8_v initial loss 4.538330358627718e-06
I0312 08:12:01.098709 1887841 finetune.py:68] layer 7_down @ epoch 3 new loss 1.0637500963639468e-05 old loss 1.0637974810379092e-05 BETTER
I0312 08:12:19.054865 1888227 finetune.py:68] layer 8_v @ epoch 0 new loss 2.0794300326087978e-06 old loss 4.538330358627718e-06 BETTER
I0312 08:12:27.332150 1887841 finetune.py:68] layer 7_down @ epoch 4 new loss 1.0637312698236201e-05 old loss 1.0637500963639468e-05 BETTER
7_v proxy err 0.0007137919310480356 tr(WHW.T) 489.9357604980469
7_q proxy err 9.566263179294765e-05 tr(WHW.T) 7672.78466796875
7_k proxy err 7.86924283602275e-05 tr(WHW.T) 10197.0439453125
7_o proxy err 0.0007226658053696156 tr(WHW.T) 15.114840507507324
7_up proxy err 0.0004461864591576159 tr(WHW.T) 735.7846069335938
7_gate proxy err 0.00021485440083779395 tr(WHW.T) 1875.052490234375
7_down proxy err 0.0005206767818890512 tr(WHW.T) 30.59491729736328
I0312 08:12:48.857637 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 9 in 71.16958951950073s
I0312 08:12:52.121926 1888358 config.py:54] PyTorch version 2.1.1 available.
I0312 08:12:53.326744 1886474 quantize_finetune_llama.py:183] layer 10 gpu 2
I0312 08:12:53.393547 1888358 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 08:12:53.485321 1888227 finetune.py:68] layer 8_v @ epoch 1 new loss 1.822980721044587e-06 old loss 2.0794300326087978e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:13:02.029802 1888358 finetune.py:45] layer 9_v initial loss 4.9425966608396266e-06
I0312 08:13:28.178150 1888227 finetune.py:68] layer 8_v @ epoch 2 new loss 1.7009207340379362e-06 old loss 1.822980721044587e-06 BETTER
I0312 08:13:33.343263 1888358 finetune.py:68] layer 9_v @ epoch 0 new loss 2.4726541596464813e-06 old loss 4.9425966608396266e-06 BETTER
I0312 08:14:02.945968 1888227 finetune.py:68] layer 8_v @ epoch 3 new loss 1.63682204856741e-06 old loss 1.7009207340379362e-06 BETTER
I0312 08:14:04.516540 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 10 in 70.76597785949707s
I0312 08:14:05.537935 1888358 finetune.py:68] layer 9_v @ epoch 1 new loss 2.2062129119149176e-06 old loss 2.4726541596464813e-06 BETTER
I0312 08:14:07.811897 1888489 config.py:54] PyTorch version 2.1.1 available.
I0312 08:14:08.834755 1886474 quantize_finetune_llama.py:183] layer 11 gpu 3
I0312 08:14:08.904038 1888489 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:14:16.904836 1888489 finetune.py:45] layer 10_v initial loss 7.077059763105353e-06
I0312 08:14:37.794490 1888227 finetune.py:68] layer 8_v @ epoch 4 new loss 1.6330287735399907e-06 old loss 1.63682204856741e-06 BETTER
I0312 08:14:38.052585 1888358 finetune.py:68] layer 9_v @ epoch 2 new loss 2.0561556084430777e-06 old loss 2.2062129119149176e-06 BETTER
I0312 08:14:47.132114 1888227 finetune.py:45] layer 8_q initial loss 2.7021455935027916e-06
I0312 08:14:48.379875 1888489 finetune.py:68] layer 10_v @ epoch 0 new loss 3.4593863347254228e-06 old loss 7.077059763105353e-06 BETTER
I0312 08:15:10.862216 1888358 finetune.py:68] layer 9_v @ epoch 3 new loss 1.9736635294975713e-06 old loss 2.0561556084430777e-06 BETTER
I0312 08:15:19.567607 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 11 in 70.40492010116577s
I0312 08:15:20.835406 1888489 finetune.py:68] layer 10_v @ epoch 1 new loss 3.062765017602942e-06 old loss 3.4593863347254228e-06 BETTER
I0312 08:15:20.844041 1888227 finetune.py:68] layer 8_q @ epoch 0 new loss 2.0139191292400938e-06 old loss 2.7021455935027916e-06 BETTER
I0312 08:15:22.926996 1888635 config.py:54] PyTorch version 2.1.1 available.
I0312 08:15:23.986453 1886474 quantize_finetune_llama.py:183] layer 12 gpu 0
I0312 08:15:24.056677 1888635 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:15:32.048357 1888635 finetune.py:45] layer 11_v initial loss 7.528367859777063e-06
I0312 08:15:43.612271 1888358 finetune.py:68] layer 9_v @ epoch 4 new loss 1.9181909465260105e-06 old loss 1.9736635294975713e-06 BETTER
I0312 08:15:52.689500 1888358 finetune.py:45] layer 9_q initial loss 2.8684157769021112e-06
I0312 08:15:53.287290 1888489 finetune.py:68] layer 10_v @ epoch 2 new loss 2.8712995572277578e-06 old loss 3.062765017602942e-06 BETTER
I0312 08:15:55.071416 1888227 finetune.py:68] layer 8_q @ epoch 1 new loss 1.895112291094847e-06 old loss 2.0139191292400938e-06 BETTER
I0312 08:16:02.881443 1888635 finetune.py:68] layer 11_v @ epoch 0 new loss 3.611131432990078e-06 old loss 7.528367859777063e-06 BETTER
I0312 08:16:24.378871 1888358 finetune.py:68] layer 9_q @ epoch 0 new loss 2.405177838227246e-06 old loss 2.8684157769021112e-06 BETTER
I0312 08:16:26.177470 1888489 finetune.py:68] layer 10_v @ epoch 3 new loss 2.804856194416061e-06 old loss 2.8712995572277578e-06 BETTER
I0312 08:16:29.514410 1888227 finetune.py:68] layer 8_q @ epoch 2 new loss 1.8331929823034443e-06 old loss 1.895112291094847e-06 BETTER
I0312 08:16:34.867408 1888635 finetune.py:68] layer 11_v @ epoch 1 new loss 3.202989773853915e-06 old loss 3.611131432990078e-06 BETTER
I0312 08:16:56.610231 1888358 finetune.py:68] layer 9_q @ epoch 1 new loss 2.2973363229539245e-06 old loss 2.405177838227246e-06 BETTER
I0312 08:16:59.439408 1888489 finetune.py:68] layer 10_v @ epoch 4 new loss 2.751589363469975e-06 old loss 2.804856194416061e-06 BETTER
I0312 08:17:04.195902 1888227 finetune.py:68] layer 8_q @ epoch 3 new loss 1.7757780597094097e-06 old loss 1.8331929823034443e-06 BETTER
I0312 08:17:07.084601 1888635 finetune.py:68] layer 11_v @ epoch 2 new loss 3.012897195731057e-06 old loss 3.202989773853915e-06 BETTER
I0312 08:17:08.983755 1888489 finetune.py:45] layer 10_q initial loss 4.186926162219606e-06
I0312 08:17:29.105663 1888358 finetune.py:68] layer 9_q @ epoch 2 new loss 2.2488043214252684e-06 old loss 2.2973363229539245e-06 BETTER
I0312 08:17:39.101919 1888227 finetune.py:68] layer 8_q @ epoch 4 new loss 1.7450670384278055e-06 old loss 1.7757780597094097e-06 BETTER
I0312 08:17:39.352279 1888635 finetune.py:68] layer 11_v @ epoch 3 new loss 2.9275943234097213e-06 old loss 3.012897195731057e-06 BETTER
I0312 08:17:40.754989 1888489 finetune.py:68] layer 10_q @ epoch 0 new loss 3.316150468890555e-06 old loss 4.186926162219606e-06 BETTER
I0312 08:17:48.707478 1888227 finetune.py:45] layer 8_k initial loss 2.623740328999702e-06
I0312 08:18:01.290952 1888358 finetune.py:68] layer 9_q @ epoch 3 new loss 2.2180020096129738e-06 old loss 2.2488043214252684e-06 BETTER
I0312 08:18:11.776858 1888635 finetune.py:76] layer 11_v @ epoch 4 new loss 2.9568996069428977e-06 old loss 2.9275943234097213e-06 WORSE
I0312 08:18:13.285662 1888489 finetune.py:68] layer 10_q @ epoch 1 new loss 3.1471208785660565e-06 old loss 3.316150468890555e-06 BETTER
I0312 08:18:20.342994 1888635 finetune.py:45] layer 11_q initial loss 4.40506664745044e-06
I0312 08:18:21.878346 1888227 finetune.py:68] layer 8_k @ epoch 0 new loss 2.3603170120622963e-06 old loss 2.623740328999702e-06 BETTER
I0312 08:18:33.822342 1888358 finetune.py:68] layer 9_q @ epoch 4 new loss 2.194627313656383e-06 old loss 2.2180020096129738e-06 BETTER
I0312 08:18:42.996121 1888358 finetune.py:45] layer 9_k initial loss 3.2112106964632403e-06
I0312 08:18:45.975312 1888489 finetune.py:68] layer 10_q @ epoch 2 new loss 3.0279763905127766e-06 old loss 3.1471208785660565e-06 BETTER
I0312 08:18:51.731915 1888635 finetune.py:68] layer 11_q @ epoch 0 new loss 3.6346532397146802e-06 old loss 4.40506664745044e-06 BETTER
I0312 08:18:55.696100 1888227 finetune.py:68] layer 8_k @ epoch 1 new loss 2.30475188800483e-06 old loss 2.3603170120622963e-06 BETTER
I0312 08:19:14.488382 1888358 finetune.py:68] layer 9_k @ epoch 0 new loss 2.820322833940736e-06 old loss 3.2112106964632403e-06 BETTER
I0312 08:19:18.565191 1888489 finetune.py:68] layer 10_q @ epoch 3 new loss 2.9406480734905927e-06 old loss 3.0279763905127766e-06 BETTER
I0312 08:19:23.822870 1888635 finetune.py:68] layer 11_q @ epoch 1 new loss 3.4300553579669213e-06 old loss 3.6346532397146802e-06 BETTER
I0312 08:19:29.493293 1888227 finetune.py:68] layer 8_k @ epoch 2 new loss 2.2666542918159394e-06 old loss 2.30475188800483e-06 BETTER
I0312 08:19:46.499300 1888358 finetune.py:68] layer 9_k @ epoch 1 new loss 2.7702501483872766e-06 old loss 2.820322833940736e-06 BETTER
I0312 08:19:51.235670 1888489 finetune.py:68] layer 10_q @ epoch 4 new loss 2.8719371130137006e-06 old loss 2.9406480734905927e-06 BETTER
I0312 08:19:55.752203 1888635 finetune.py:68] layer 11_q @ epoch 2 new loss 3.3143699056381593e-06 old loss 3.4300553579669213e-06 BETTER
I0312 08:20:00.941219 1888489 finetune.py:45] layer 10_k initial loss 4.121928213862702e-06
I0312 08:20:04.390479 1888227 finetune.py:68] layer 8_k @ epoch 3 new loss 2.234588691862882e-06 old loss 2.2666542918159394e-06 BETTER
I0312 08:20:19.326743 1888358 finetune.py:68] layer 9_k @ epoch 2 new loss 2.724445039348211e-06 old loss 2.7702501483872766e-06 BETTER
I0312 08:20:27.878314 1888635 finetune.py:68] layer 11_q @ epoch 3 new loss 3.217737230443163e-06 old loss 3.3143699056381593e-06 BETTER
I0312 08:20:32.563079 1888489 finetune.py:68] layer 10_k @ epoch 0 new loss 3.7358925055741565e-06 old loss 4.121928213862702e-06 BETTER
I0312 08:20:38.311396 1888227 finetune.py:68] layer 8_k @ epoch 4 new loss 2.2184169665706577e-06 old loss 2.234588691862882e-06 BETTER
I0312 08:20:47.813355 1888227 finetune.py:45] layer 8_o initial loss 5.73043826079811e-06
I0312 08:20:51.676306 1888358 finetune.py:68] layer 9_k @ epoch 3 new loss 2.689287612156477e-06 old loss 2.724445039348211e-06 BETTER
I0312 08:20:59.972553 1888635 finetune.py:68] layer 11_q @ epoch 4 new loss 3.1754436804476427e-06 old loss 3.217737230443163e-06 BETTER
I0312 08:21:04.673166 1888489 finetune.py:68] layer 10_k @ epoch 1 new loss 3.659052936200169e-06 old loss 3.7358925055741565e-06 BETTER
I0312 08:21:09.287969 1888635 finetune.py:45] layer 11_k initial loss 4.678548975789454e-06
I0312 08:21:20.373990 1888227 finetune.py:68] layer 8_o @ epoch 0 new loss 5.194401182961883e-06 old loss 5.73043826079811e-06 BETTER
I0312 08:21:23.887491 1888358 finetune.py:68] layer 9_k @ epoch 4 new loss 2.670142976057832e-06 old loss 2.689287612156477e-06 BETTER
I0312 08:21:33.164880 1888358 finetune.py:45] layer 9_o initial loss 7.17523425919353e-06
I0312 08:21:36.820615 1888489 finetune.py:68] layer 10_k @ epoch 2 new loss 3.6034157346875872e-06 old loss 3.659052936200169e-06 BETTER
I0312 08:21:40.039200 1888635 finetune.py:68] layer 11_k @ epoch 0 new loss 4.195363544567954e-06 old loss 4.678548975789454e-06 BETTER
I0312 08:21:53.679892 1888227 finetune.py:68] layer 8_o @ epoch 1 new loss 4.985759460396366e-06 old loss 5.194401182961883e-06 BETTER
I0312 08:22:03.835018 1888358 finetune.py:68] layer 9_o @ epoch 0 new loss 6.473566827480681e-06 old loss 7.17523425919353e-06 BETTER
I0312 08:22:09.168276 1888489 finetune.py:76] layer 10_k @ epoch 3 new loss 3.6411308883543825e-06 old loss 3.6034157346875872e-06 WORSE
I0312 08:22:11.634516 1888635 finetune.py:68] layer 11_k @ epoch 1 new loss 4.141639237786876e-06 old loss 4.195363544567954e-06 BETTER
I0312 08:22:26.913009 1888227 finetune.py:68] layer 8_o @ epoch 2 new loss 4.849278866458917e-06 old loss 4.985759460396366e-06 BETTER
I0312 08:22:35.360572 1888358 finetune.py:68] layer 9_o @ epoch 1 new loss 6.1921077758597676e-06 old loss 6.473566827480681e-06 BETTER
I0312 08:22:40.724953 1888489 finetune.py:76] layer 10_k @ epoch 4 new loss 3.6162775813863846e-06 old loss 3.6034157346875872e-06 WORSE
I0312 08:22:43.337765 1888635 finetune.py:68] layer 11_k @ epoch 2 new loss 4.059216280438704e-06 old loss 4.141639237786876e-06 BETTER
I0312 08:22:49.518406 1888489 finetune.py:45] layer 10_o initial loss 9.921975106408354e-06
I0312 08:23:00.405622 1888227 finetune.py:68] layer 8_o @ epoch 3 new loss 4.7489720600424334e-06 old loss 4.849278866458917e-06 BETTER
I0312 08:23:06.837156 1888358 finetune.py:68] layer 9_o @ epoch 2 new loss 6.017541181790875e-06 old loss 6.1921077758597676e-06 BETTER
I0312 08:23:14.925436 1888635 finetune.py:68] layer 11_k @ epoch 3 new loss 4.029566753160907e-06 old loss 4.059216280438704e-06 BETTER
I0312 08:23:20.311211 1888489 finetune.py:68] layer 10_o @ epoch 0 new loss 8.939019608078524e-06 old loss 9.921975106408354e-06 BETTER
I0312 08:23:33.991112 1888227 finetune.py:68] layer 8_o @ epoch 4 new loss 4.670265752793057e-06 old loss 4.7489720600424334e-06 BETTER
I0312 08:23:38.625596 1888358 finetune.py:68] layer 9_o @ epoch 3 new loss 5.88818784308387e-06 old loss 6.017541181790875e-06 BETTER
I0312 08:23:46.882181 1888635 finetune.py:68] layer 11_k @ epoch 4 new loss 4.028071543871192e-06 old loss 4.029566753160907e-06 BETTER
I0312 08:23:49.547125 1888227 finetune.py:45] layer 8_up initial loss 7.741416993667372e-06
I0312 08:23:51.944760 1888489 finetune.py:68] layer 10_o @ epoch 1 new loss 8.508877726853825e-06 old loss 8.939019608078524e-06 BETTER
I0312 08:23:56.529286 1888635 finetune.py:45] layer 11_o initial loss 1.0546493285801262e-05
I0312 08:24:10.236730 1888358 finetune.py:68] layer 9_o @ epoch 4 new loss 5.787635927845258e-06 old loss 5.88818784308387e-06 BETTER
I0312 08:24:19.915788 1888227 finetune.py:68] layer 8_up @ epoch 0 new loss 7.468199328286573e-06 old loss 7.741416993667372e-06 BETTER
I0312 08:24:23.578899 1888489 finetune.py:68] layer 10_o @ epoch 2 new loss 8.239569069701247e-06 old loss 8.508877726853825e-06 BETTER
I0312 08:24:25.401593 1888358 finetune.py:45] layer 9_up initial loss 9.371962732984684e-06
I0312 08:24:26.791126 1888635 finetune.py:68] layer 11_o @ epoch 0 new loss 9.446670446777716e-06 old loss 1.0546493285801262e-05 BETTER
I0312 08:24:51.435617 1888227 finetune.py:68] layer 8_up @ epoch 1 new loss 7.329865184146911e-06 old loss 7.468199328286573e-06 BETTER
I0312 08:24:54.271186 1888358 finetune.py:68] layer 9_up @ epoch 0 new loss 9.024111022881698e-06 old loss 9.371962732984684e-06 BETTER
I0312 08:24:55.415685 1888489 finetune.py:68] layer 10_o @ epoch 3 new loss 8.040220563998446e-06 old loss 8.239569069701247e-06 BETTER
I0312 08:24:57.612760 1888635 finetune.py:68] layer 11_o @ epoch 1 new loss 9.027504347614013e-06 old loss 9.446670446777716e-06 BETTER
I0312 08:25:23.058783 1888227 finetune.py:68] layer 8_up @ epoch 2 new loss 7.224293767649215e-06 old loss 7.329865184146911e-06 BETTER
I0312 08:25:24.073669 1888358 finetune.py:68] layer 9_up @ epoch 1 new loss 8.852992323227227e-06 old loss 9.024111022881698e-06 BETTER
I0312 08:25:27.092226 1888489 finetune.py:68] layer 10_o @ epoch 4 new loss 7.88722809375031e-06 old loss 8.040220563998446e-06 BETTER
I0312 08:25:28.410926 1888635 finetune.py:68] layer 11_o @ epoch 2 new loss 8.771368811721914e-06 old loss 9.027504347614013e-06 BETTER
I0312 08:25:43.177465 1888489 finetune.py:45] layer 10_up initial loss 1.2001825780316722e-05
I0312 08:25:54.942335 1888358 finetune.py:68] layer 9_up @ epoch 2 new loss 8.722512575332075e-06 old loss 8.852992323227227e-06 BETTER
I0312 08:25:55.470242 1888227 finetune.py:68] layer 8_up @ epoch 3 new loss 7.13862209522631e-06 old loss 7.224293767649215e-06 BETTER
I0312 08:25:59.384734 1888635 finetune.py:68] layer 11_o @ epoch 3 new loss 8.584406714362558e-06 old loss 8.771368811721914e-06 BETTER
I0312 08:26:12.197519 1888489 finetune.py:68] layer 10_up @ epoch 0 new loss 1.1539234947122168e-05 old loss 1.2001825780316722e-05 BETTER
I0312 08:26:24.973908 1888358 finetune.py:68] layer 9_up @ epoch 3 new loss 8.617604180471972e-06 old loss 8.722512575332075e-06 BETTER
I0312 08:26:27.278012 1888227 finetune.py:68] layer 8_up @ epoch 4 new loss 7.0667415457137395e-06 old loss 7.13862209522631e-06 BETTER
I0312 08:26:30.476217 1888635 finetune.py:68] layer 11_o @ epoch 4 new loss 8.442619218840264e-06 old loss 8.584406714362558e-06 BETTER
I0312 08:26:42.267456 1888489 finetune.py:68] layer 10_up @ epoch 1 new loss 1.1311883099551778e-05 old loss 1.1539234947122168e-05 BETTER
I0312 08:26:42.698477 1888227 finetune.py:45] layer 8_gate initial loss 9.237640369974542e-06
I0312 08:26:45.682376 1888635 finetune.py:45] layer 11_up initial loss 1.3055492672719993e-05
I0312 08:26:54.826992 1888358 finetune.py:68] layer 9_up @ epoch 4 new loss 8.529963452019729e-06 old loss 8.617604180471972e-06 BETTER
I0312 08:27:10.134975 1888358 finetune.py:45] layer 9_gate initial loss 1.1144301424792502e-05
I0312 08:27:12.018725 1888227 finetune.py:68] layer 8_gate @ epoch 0 new loss 9.075271918845829e-06 old loss 9.237640369974542e-06 BETTER
I0312 08:27:12.698853 1888489 finetune.py:68] layer 10_up @ epoch 2 new loss 1.1140756214444991e-05 old loss 1.1311883099551778e-05 BETTER
I0312 08:27:14.598657 1888635 finetune.py:68] layer 11_up @ epoch 0 new loss 1.2574481843330432e-05 old loss 1.3055492672719993e-05 BETTER
I0312 08:27:37.750413 1888358 finetune.py:68] layer 9_gate @ epoch 0 new loss 1.0945082976832055e-05 old loss 1.1144301424792502e-05 BETTER
I0312 08:27:42.257675 1888227 finetune.py:68] layer 8_gate @ epoch 1 new loss 8.987599358079024e-06 old loss 9.075271918845829e-06 BETTER
I0312 08:27:43.270942 1888489 finetune.py:68] layer 10_up @ epoch 3 new loss 1.1003749932569917e-05 old loss 1.1140756214444991e-05 BETTER
I0312 08:27:44.508394 1888635 finetune.py:68] layer 11_up @ epoch 1 new loss 1.2335292922216468e-05 old loss 1.2574481843330432e-05 BETTER
I0312 08:28:06.175331 1888358 finetune.py:68] layer 9_gate @ epoch 1 new loss 1.0836661203939002e-05 old loss 1.0945082976832055e-05 BETTER
I0312 08:28:12.283735 1888227 finetune.py:68] layer 8_gate @ epoch 2 new loss 8.918435014493298e-06 old loss 8.987599358079024e-06 BETTER
I0312 08:28:13.700671 1888489 finetune.py:68] layer 10_up @ epoch 4 new loss 1.0888196811720263e-05 old loss 1.1003749932569917e-05 BETTER
I0312 08:28:14.250244 1888635 finetune.py:68] layer 11_up @ epoch 2 new loss 1.2159630387031939e-05 old loss 1.2335292922216468e-05 BETTER
I0312 08:28:28.793935 1888489 finetune.py:45] layer 10_gate initial loss 1.4027770703251008e-05
I0312 08:28:34.414150 1888358 finetune.py:68] layer 9_gate @ epoch 2 new loss 1.0751167792477645e-05 old loss 1.0836661203939002e-05 BETTER
I0312 08:28:42.420453 1888227 finetune.py:68] layer 8_gate @ epoch 3 new loss 8.859470653987955e-06 old loss 8.918435014493298e-06 BETTER
I0312 08:28:43.770160 1888635 finetune.py:68] layer 11_up @ epoch 3 new loss 1.2019834684906527e-05 old loss 1.2159630387031939e-05 BETTER
I0312 08:28:56.151368 1888489 finetune.py:68] layer 10_gate @ epoch 0 new loss 1.375346073473338e-05 old loss 1.4027770703251008e-05 BETTER
I0312 08:29:02.424452 1888358 finetune.py:68] layer 9_gate @ epoch 3 new loss 1.0679421393433586e-05 old loss 1.0751167792477645e-05 BETTER
I0312 08:29:12.528155 1888227 finetune.py:68] layer 8_gate @ epoch 4 new loss 8.808748134470079e-06 old loss 8.859470653987955e-06 BETTER
I0312 08:29:13.220307 1888635 finetune.py:68] layer 11_up @ epoch 4 new loss 1.1902832738996949e-05 old loss 1.2019834684906527e-05 BETTER
I0312 08:29:24.360077 1888489 finetune.py:68] layer 10_gate @ epoch 1 new loss 1.3612750990432687e-05 old loss 1.375346073473338e-05 BETTER
I0312 08:29:28.812339 1888635 finetune.py:45] layer 11_gate initial loss 1.5436717148986645e-05
I0312 08:29:28.978436 1888227 finetune.py:45] layer 8_down initial loss 1.3404864148469642e-05
I0312 08:29:30.815702 1888358 finetune.py:68] layer 9_gate @ epoch 4 new loss 1.0618240594340023e-05 old loss 1.0679421393433586e-05 BETTER
I0312 08:29:46.625272 1888358 finetune.py:45] layer 9_down initial loss 1.594846798980143e-05
I0312 08:29:52.831881 1888489 finetune.py:68] layer 10_gate @ epoch 2 new loss 1.350350612483453e-05 old loss 1.3612750990432687e-05 BETTER
I0312 08:29:56.091482 1888635 finetune.py:68] layer 11_gate @ epoch 0 new loss 1.5148638340178877e-05 old loss 1.5436717148986645e-05 BETTER
I0312 08:29:56.373887 1888227 finetune.py:68] layer 8_down @ epoch 0 new loss 1.3401748219621368e-05 old loss 1.3404864148469642e-05 BETTER
I0312 08:30:12.582878 1888358 finetune.py:68] layer 9_down @ epoch 0 new loss 1.59447081387043e-05 old loss 1.594846798980143e-05 BETTER
I0312 08:30:21.300391 1888489 finetune.py:68] layer 10_gate @ epoch 3 new loss 1.3412246516963933e-05 old loss 1.350350612483453e-05 BETTER
I0312 08:30:24.121743 1888635 finetune.py:68] layer 11_gate @ epoch 1 new loss 1.5001217434473801e-05 old loss 1.5148638340178877e-05 BETTER
I0312 08:30:24.982908 1888227 finetune.py:68] layer 8_down @ epoch 1 new loss 1.3400999705481809e-05 old loss 1.3401748219621368e-05 BETTER
I0312 08:30:39.791105 1888358 finetune.py:68] layer 9_down @ epoch 1 new loss 1.594397508597467e-05 old loss 1.59447081387043e-05 BETTER
I0312 08:30:49.706043 1888489 finetune.py:68] layer 10_gate @ epoch 4 new loss 1.333459931629477e-05 old loss 1.3412246516963933e-05 BETTER
I0312 08:30:52.055250 1888635 finetune.py:68] layer 11_gate @ epoch 2 new loss 1.4886997632856946e-05 old loss 1.5001217434473801e-05 BETTER
I0312 08:30:53.917530 1888227 finetune.py:68] layer 8_down @ epoch 2 new loss 1.3400720490608364e-05 old loss 1.3400999705481809e-05 BETTER
I0312 08:31:06.082966 1888489 finetune.py:45] layer 10_down initial loss 1.956045161932707e-05
I0312 08:31:06.849225 1888358 finetune.py:68] layer 9_down @ epoch 2 new loss 1.5943694961606525e-05 old loss 1.594397508597467e-05 BETTER
I0312 08:31:20.750307 1888635 finetune.py:68] layer 11_gate @ epoch 3 new loss 1.4792995898460504e-05 old loss 1.4886997632856946e-05 BETTER
I0312 08:31:22.505690 1888227 finetune.py:68] layer 8_down @ epoch 3 new loss 1.3400519492279273e-05 old loss 1.3400720490608364e-05 BETTER
I0312 08:31:32.416051 1888489 finetune.py:68] layer 10_down @ epoch 0 new loss 1.95558004634222e-05 old loss 1.956045161932707e-05 BETTER
I0312 08:31:35.056476 1888358 finetune.py:76] layer 9_down @ epoch 3 new loss 1.5943754988256842e-05 old loss 1.5943694961606525e-05 WORSE
I0312 08:31:50.450268 1888635 finetune.py:68] layer 11_gate @ epoch 4 new loss 1.4713236851093825e-05 old loss 1.4792995898460504e-05 BETTER
I0312 08:31:52.138029 1888227 finetune.py:68] layer 8_down @ epoch 4 new loss 1.3400285752140917e-05 old loss 1.3400519492279273e-05 BETTER
8_v proxy err 0.0006727410363964736 tr(WHW.T) 530.9967041015625
8_q proxy err 0.00010024220682680607 tr(WHW.T) 7228.4287109375
8_k proxy err 7.989769073901698e-05 tr(WHW.T) 10639.4189453125
8_o proxy err 0.0008133541559800506 tr(WHW.T) 20.09137535095215
8_up proxy err 0.00041388283716514707 tr(WHW.T) 866.2940063476562
8_gate proxy err 0.00021575592109002173 tr(WHW.T) 1970.1685791015625
8_down proxy err 0.0005198524449951947 tr(WHW.T) 37.18357849121094
I0312 08:32:00.122122 1888489 finetune.py:68] layer 10_down @ epoch 1 new loss 1.955480001925025e-05 old loss 1.95558004634222e-05 BETTER
I0312 08:32:01.972366 1888358 finetune.py:68] layer 9_down @ epoch 4 new loss 1.5943385733407922e-05 old loss 1.5943694961606525e-05 BETTER
9_v proxy err 0.0006448369822464883 tr(WHW.T) 565.0663452148438
9_q proxy err 9.984225471271202e-05 tr(WHW.T) 6970.81494140625
9_k proxy err 7.89287150837481e-05 tr(WHW.T) 10988.8017578125
9_o proxy err 0.0008193891262635589 tr(WHW.T) 25.61722183227539
9_up proxy err 0.000402135745389387 tr(WHW.T) 970.790771484375
9_gate proxy err 0.0002147005870938301 tr(WHW.T) 2131.854248046875
9_down proxy err 0.000523024529684335 tr(WHW.T) 43.00539779663086
I0312 08:32:07.966010 1888635 finetune.py:45] layer 11_down initial loss 2.1551111785811372e-05
I0312 08:32:27.181990 1888489 finetune.py:68] layer 10_down @ epoch 2 new loss 1.955429615918547e-05 old loss 1.955480001925025e-05 BETTER
I0312 08:32:33.031978 1888635 finetune.py:68] layer 11_down @ epoch 0 new loss 2.1546147763729095e-05 old loss 2.1551111785811372e-05 BETTER
I0312 08:32:53.942962 1888489 finetune.py:68] layer 10_down @ epoch 3 new loss 1.9554048776626587e-05 old loss 1.955429615918547e-05 BETTER
I0312 08:32:59.085272 1888635 finetune.py:68] layer 11_down @ epoch 1 new loss 2.154484718630556e-05 old loss 2.1546147763729095e-05 BETTER
I0312 08:33:18.388651 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 12 in 72.40367889404297s
I0312 08:33:20.665851 1888489 finetune.py:68] layer 10_down @ epoch 4 new loss 1.95538505067816e-05 old loss 1.9554048776626587e-05 BETTER
I0312 08:33:21.649328 1889021 config.py:54] PyTorch version 2.1.1 available.
10_v proxy err 0.0006510076927952468 tr(WHW.T) 578.807373046875
10_q proxy err 0.00010250777995679528 tr(WHW.T) 6916.26708984375
10_k proxy err 8.021843677852303e-05 tr(WHW.T) 10996.3798828125
10_o proxy err 0.0008474269998259842 tr(WHW.T) 35.1975212097168
10_up proxy err 0.0003825717431027442 tr(WHW.T) 1080.052490234375
10_gate proxy err 0.00021151678811293095 tr(WHW.T) 2259.966552734375
10_down proxy err 0.0005018476513214409 tr(WHW.T) 52.3614387512207
I0312 08:33:22.668049 1886474 quantize_finetune_llama.py:183] layer 13 gpu 1
I0312 08:33:22.744460 1889021 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 08:33:25.338458 1888635 finetune.py:68] layer 11_down @ epoch 2 new loss 2.1544312403420918e-05 old loss 2.154484718630556e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:33:31.325710 1889021 finetune.py:45] layer 12_v initial loss 7.5383077273727395e-06
I0312 08:33:51.795243 1888635 finetune.py:68] layer 11_down @ epoch 3 new loss 2.1544023184105754e-05 old loss 2.1544312403420918e-05 BETTER
I0312 08:34:04.448763 1889021 finetune.py:68] layer 12_v @ epoch 0 new loss 3.815406216745032e-06 old loss 7.5383077273727395e-06 BETTER
I0312 08:34:18.227453 1888635 finetune.py:68] layer 11_down @ epoch 4 new loss 2.1543766706599854e-05 old loss 2.1544023184105754e-05 BETTER
11_v proxy err 0.0006330765318125486 tr(WHW.T) 723.1956176757812
11_q proxy err 0.00011354141315678135 tr(WHW.T) 7027.3154296875
11_k proxy err 8.808319398667663e-05 tr(WHW.T) 10511.396484375
11_o proxy err 0.000856875441968441 tr(WHW.T) 36.68510055541992
11_up proxy err 0.00038836183375678957 tr(WHW.T) 1139.749755859375
11_gate proxy err 0.000214412430068478 tr(WHW.T) 2392.728759765625
11_down proxy err 0.0005125190364196897 tr(WHW.T) 56.141849517822266
I0312 08:34:35.641267 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 13 in 71.40024375915527s
I0312 08:34:38.706382 1889152 config.py:54] PyTorch version 2.1.1 available.
I0312 08:34:38.900420 1889021 finetune.py:68] layer 12_v @ epoch 1 new loss 3.405111556276097e-06 old loss 3.815406216745032e-06 BETTER
I0312 08:34:39.737750 1886474 quantize_finetune_llama.py:183] layer 14 gpu 2
I0312 08:34:39.820888 1889152 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:34:48.475960 1889152 finetune.py:45] layer 13_v initial loss 7.81895960244583e-06
I0312 08:35:13.677196 1889021 finetune.py:68] layer 12_v @ epoch 2 new loss 3.2100574571813922e-06 old loss 3.405111556276097e-06 BETTER
I0312 08:35:19.840243 1889152 finetune.py:68] layer 13_v @ epoch 0 new loss 4.032486231153598e-06 old loss 7.81895960244583e-06 BETTER
I0312 08:35:48.576007 1889021 finetune.py:68] layer 12_v @ epoch 3 new loss 3.1071729154064087e-06 old loss 3.2100574571813922e-06 BETTER
I0312 08:35:51.649998 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 14 in 71.47458577156067s
I0312 08:35:52.205478 1889152 finetune.py:68] layer 13_v @ epoch 1 new loss 3.641984676505672e-06 old loss 4.032486231153598e-06 BETTER
I0312 08:35:54.952112 1889283 config.py:54] PyTorch version 2.1.1 available.
I0312 08:35:55.964795 1886474 quantize_finetune_llama.py:183] layer 15 gpu 3
I0312 08:35:56.029804 1889283 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:36:04.242923 1889283 finetune.py:45] layer 14_v initial loss 9.929502084560227e-06
I0312 08:36:23.471777 1889021 finetune.py:68] layer 12_v @ epoch 4 new loss 3.0898343084118096e-06 old loss 3.1071729154064087e-06 BETTER
I0312 08:36:24.682573 1889152 finetune.py:68] layer 13_v @ epoch 2 new loss 3.502947492961539e-06 old loss 3.641984676505672e-06 BETTER
I0312 08:36:32.743967 1889021 finetune.py:45] layer 12_q initial loss 4.929334863845725e-06
I0312 08:36:35.709056 1889283 finetune.py:68] layer 14_v @ epoch 0 new loss 5.193095148570137e-06 old loss 9.929502084560227e-06 BETTER
I0312 08:36:57.390562 1889152 finetune.py:68] layer 13_v @ epoch 3 new loss 3.4354043236817233e-06 old loss 3.502947492961539e-06 BETTER
I0312 08:37:06.091357 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 15 in 69.72393083572388s
I0312 08:37:06.324714 1889021 finetune.py:68] layer 12_q @ epoch 0 new loss 3.818374807451619e-06 old loss 4.929334863845725e-06 BETTER
I0312 08:37:08.026455 1889283 finetune.py:68] layer 14_v @ epoch 1 new loss 4.622305368684465e-06 old loss 5.193095148570137e-06 BETTER
I0312 08:37:09.465627 1889414 config.py:54] PyTorch version 2.1.1 available.
I0312 08:37:10.541384 1886474 quantize_finetune_llama.py:183] layer 16 gpu 0
I0312 08:37:10.609666 1889414 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:37:21.440680 1889414 finetune.py:45] layer 15_v initial loss 1.003618490358349e-05
I0312 08:37:30.640375 1889152 finetune.py:76] layer 13_v @ epoch 4 new loss 3.5059933907177765e-06 old loss 3.4354043236817233e-06 WORSE
I0312 08:37:40.988119 1889152 finetune.py:45] layer 13_q initial loss 4.899097348243231e-06
I0312 08:37:41.378214 1889021 finetune.py:68] layer 12_q @ epoch 1 new loss 3.6079111396247754e-06 old loss 3.818374807451619e-06 BETTER
I0312 08:37:41.396410 1889283 finetune.py:68] layer 14_v @ epoch 2 new loss 4.339295173849678e-06 old loss 4.622305368684465e-06 BETTER
I0312 08:37:53.220840 1889414 finetune.py:68] layer 15_v @ epoch 0 new loss 5.160940872883657e-06 old loss 1.003618490358349e-05 BETTER
I0312 08:38:12.626918 1889152 finetune.py:68] layer 13_q @ epoch 0 new loss 4.110695044801105e-06 old loss 4.899097348243231e-06 BETTER
I0312 08:38:14.347227 1889283 finetune.py:68] layer 14_v @ epoch 3 new loss 4.1850348679872695e-06 old loss 4.339295173849678e-06 BETTER
I0312 08:38:16.057732 1889021 finetune.py:68] layer 12_q @ epoch 2 new loss 3.494520115054911e-06 old loss 3.6079111396247754e-06 BETTER
I0312 08:38:25.618983 1889414 finetune.py:68] layer 15_v @ epoch 1 new loss 4.597722181642894e-06 old loss 5.160940872883657e-06 BETTER
I0312 08:38:44.977860 1889152 finetune.py:68] layer 13_q @ epoch 1 new loss 3.856909643218387e-06 old loss 4.110695044801105e-06 BETTER
I0312 08:38:47.217131 1889283 finetune.py:68] layer 14_v @ epoch 4 new loss 4.151709163124906e-06 old loss 4.1850348679872695e-06 BETTER
I0312 08:38:50.638406 1889021 finetune.py:68] layer 12_q @ epoch 3 new loss 3.3987716960837133e-06 old loss 3.494520115054911e-06 BETTER
I0312 08:38:56.345144 1889283 finetune.py:45] layer 14_q initial loss 6.4624027800164185e-06
I0312 08:38:58.197533 1889414 finetune.py:68] layer 15_v @ epoch 2 new loss 4.308806637709495e-06 old loss 4.597722181642894e-06 BETTER
I0312 08:39:17.229546 1889152 finetune.py:68] layer 13_q @ epoch 2 new loss 3.7450006402650615e-06 old loss 3.856909643218387e-06 BETTER
I0312 08:39:25.390576 1889021 finetune.py:68] layer 12_q @ epoch 4 new loss 3.3363951388309943e-06 old loss 3.3987716960837133e-06 BETTER
I0312 08:39:28.083999 1889283 finetune.py:68] layer 14_q @ epoch 0 new loss 5.076090928923804e-06 old loss 6.4624027800164185e-06 BETTER
I0312 08:39:30.701474 1889414 finetune.py:68] layer 15_v @ epoch 3 new loss 4.16454622609308e-06 old loss 4.308806637709495e-06 BETTER
I0312 08:39:34.999648 1889021 finetune.py:45] layer 12_k initial loss 4.781297320732847e-06
I0312 08:39:49.438411 1889152 finetune.py:68] layer 13_q @ epoch 3 new loss 3.638114321802277e-06 old loss 3.7450006402650615e-06 BETTER
I0312 08:40:00.793774 1889283 finetune.py:68] layer 14_q @ epoch 1 new loss 4.8108777264133096e-06 old loss 5.076090928923804e-06 BETTER
I0312 08:40:03.587807 1889414 finetune.py:76] layer 15_v @ epoch 4 new loss 4.175206868239911e-06 old loss 4.16454622609308e-06 WORSE
I0312 08:40:08.034642 1889021 finetune.py:68] layer 12_k @ epoch 0 new loss 4.373711362859467e-06 old loss 4.781297320732847e-06 BETTER
I0312 08:40:12.689914 1889414 finetune.py:45] layer 15_q initial loss 6.3371403484779876e-06
I0312 08:40:22.019544 1889152 finetune.py:68] layer 13_q @ epoch 4 new loss 3.5938173823524266e-06 old loss 3.638114321802277e-06 BETTER
I0312 08:40:31.413282 1889152 finetune.py:45] layer 13_k initial loss 4.797963356395485e-06
I0312 08:40:33.440163 1889283 finetune.py:68] layer 14_q @ epoch 2 new loss 4.6434233809122816e-06 old loss 4.8108777264133096e-06 BETTER
I0312 08:40:42.250116 1889021 finetune.py:68] layer 12_k @ epoch 1 new loss 4.313440967962379e-06 old loss 4.373711362859467e-06 BETTER
I0312 08:40:44.664722 1889414 finetune.py:68] layer 15_q @ epoch 0 new loss 5.083237283542985e-06 old loss 6.3371403484779876e-06 BETTER
I0312 08:41:02.982895 1889152 finetune.py:68] layer 13_k @ epoch 0 new loss 4.528808858594857e-06 old loss 4.797963356395485e-06 BETTER
I0312 08:41:06.433799 1889283 finetune.py:68] layer 14_q @ epoch 3 new loss 4.5202564251667354e-06 old loss 4.6434233809122816e-06 BETTER
I0312 08:41:16.672658 1889021 finetune.py:68] layer 12_k @ epoch 2 new loss 4.2785513869603164e-06 old loss 4.313440967962379e-06 BETTER
I0312 08:41:17.898499 1889414 finetune.py:68] layer 15_q @ epoch 1 new loss 4.82164068671409e-06 old loss 5.083237283542985e-06 BETTER
I0312 08:41:35.395221 1889152 finetune.py:68] layer 13_k @ epoch 1 new loss 4.452613666217076e-06 old loss 4.528808858594857e-06 BETTER
I0312 08:41:39.121001 1889283 finetune.py:68] layer 14_q @ epoch 4 new loss 4.421878657012712e-06 old loss 4.5202564251667354e-06 BETTER
I0312 08:41:49.548558 1889283 finetune.py:45] layer 14_k initial loss 6.502182259282563e-06
I0312 08:41:51.647811 1889414 finetune.py:68] layer 15_q @ epoch 2 new loss 4.65260018245317e-06 old loss 4.82164068671409e-06 BETTER
I0312 08:41:51.842289 1889021 finetune.py:68] layer 12_k @ epoch 3 new loss 4.20146625401685e-06 old loss 4.2785513869603164e-06 BETTER
I0312 08:42:07.633482 1889152 finetune.py:68] layer 13_k @ epoch 2 new loss 4.395980340632377e-06 old loss 4.452613666217076e-06 BETTER
I0312 08:42:21.031675 1889283 finetune.py:68] layer 14_k @ epoch 0 new loss 5.875648639630526e-06 old loss 6.502182259282563e-06 BETTER
I0312 08:42:24.456501 1889414 finetune.py:68] layer 15_q @ epoch 3 new loss 4.564260052575264e-06 old loss 4.65260018245317e-06 BETTER
I0312 08:42:26.230869 1889021 finetune.py:68] layer 12_k @ epoch 4 new loss 4.179332790954504e-06 old loss 4.20146625401685e-06 BETTER
I0312 08:42:37.724468 1889021 finetune.py:45] layer 12_o initial loss 1.1266208275628742e-05
I0312 08:42:41.160756 1889152 finetune.py:68] layer 13_k @ epoch 3 new loss 4.351965799287427e-06 old loss 4.395980340632377e-06 BETTER
I0312 08:42:54.674240 1889283 finetune.py:68] layer 14_k @ epoch 1 new loss 5.7691022448125295e-06 old loss 5.875648639630526e-06 BETTER
I0312 08:42:58.775664 1889414 finetune.py:68] layer 15_q @ epoch 4 new loss 4.451268978300504e-06 old loss 4.564260052575264e-06 BETTER
I0312 08:43:11.377408 1889021 finetune.py:68] layer 12_o @ epoch 0 new loss 1.0134148396900855e-05 old loss 1.1266208275628742e-05 BETTER
I0312 08:43:11.613748 1889414 finetune.py:45] layer 15_k initial loss 6.454594768001698e-06
I0312 08:43:14.587140 1889152 finetune.py:68] layer 13_k @ epoch 4 new loss 4.320710104366299e-06 old loss 4.351965799287427e-06 BETTER
I0312 08:43:23.825355 1889152 finetune.py:45] layer 13_o initial loss 1.1753792023228016e-05
I0312 08:43:27.107732 1889283 finetune.py:68] layer 14_k @ epoch 2 new loss 5.694434094039025e-06 old loss 5.7691022448125295e-06 BETTER
I0312 08:43:43.141350 1889414 finetune.py:68] layer 15_k @ epoch 0 new loss 5.971281552774599e-06 old loss 6.454594768001698e-06 BETTER
I0312 08:43:44.614122 1889021 finetune.py:68] layer 12_o @ epoch 1 new loss 9.691817467682995e-06 old loss 1.0134148396900855e-05 BETTER
I0312 08:43:54.337374 1889152 finetune.py:68] layer 13_o @ epoch 0 new loss 1.0437053788336925e-05 old loss 1.1753792023228016e-05 BETTER
I0312 08:43:59.215373 1889283 finetune.py:76] layer 14_k @ epoch 3 new loss 5.699635494238464e-06 old loss 5.694434094039025e-06 WORSE
I0312 08:44:15.300727 1889414 finetune.py:68] layer 15_k @ epoch 1 new loss 5.847320608154405e-06 old loss 5.971281552774599e-06 BETTER
I0312 08:44:17.654554 1889021 finetune.py:68] layer 12_o @ epoch 2 new loss 9.416755347047001e-06 old loss 9.691817467682995e-06 BETTER
I0312 08:44:25.629496 1889152 finetune.py:68] layer 13_o @ epoch 1 new loss 9.952771506505087e-06 old loss 1.0437053788336925e-05 BETTER
I0312 08:44:30.687556 1889283 finetune.py:68] layer 14_k @ epoch 4 new loss 5.6110347941285e-06 old loss 5.694434094039025e-06 BETTER
I0312 08:44:39.702068 1889283 finetune.py:45] layer 14_o initial loss 1.4945620023354422e-05
I0312 08:44:47.630259 1889414 finetune.py:68] layer 15_k @ epoch 2 new loss 5.716987288906239e-06 old loss 5.847320608154405e-06 BETTER
I0312 08:44:51.263978 1889021 finetune.py:68] layer 12_o @ epoch 3 new loss 9.216710168402642e-06 old loss 9.416755347047001e-06 BETTER
I0312 08:44:57.019029 1889152 finetune.py:68] layer 13_o @ epoch 2 new loss 9.6562189355609e-06 old loss 9.952771506505087e-06 BETTER
I0312 08:45:10.655637 1889283 finetune.py:68] layer 14_o @ epoch 0 new loss 1.3466625205182936e-05 old loss 1.4945620023354422e-05 BETTER
I0312 08:45:19.942337 1889414 finetune.py:68] layer 15_k @ epoch 3 new loss 5.6483822845621035e-06 old loss 5.716987288906239e-06 BETTER
I0312 08:45:24.864809 1889021 finetune.py:68] layer 12_o @ epoch 4 new loss 9.064436198968906e-06 old loss 9.216710168402642e-06 BETTER
I0312 08:45:28.426343 1889152 finetune.py:68] layer 13_o @ epoch 3 new loss 9.44889325182885e-06 old loss 9.6562189355609e-06 BETTER
I0312 08:45:40.518349 1889021 finetune.py:45] layer 12_up initial loss 1.4235781236493494e-05
I0312 08:45:42.265144 1889283 finetune.py:68] layer 14_o @ epoch 1 new loss 1.2880251233582385e-05 old loss 1.3466625205182936e-05 BETTER
I0312 08:45:52.296952 1889414 finetune.py:68] layer 15_k @ epoch 4 new loss 5.622490334644681e-06 old loss 5.6483822845621035e-06 BETTER
I0312 08:45:59.959224 1889152 finetune.py:68] layer 13_o @ epoch 4 new loss 9.289457921113353e-06 old loss 9.44889325182885e-06 BETTER
I0312 08:46:02.538465 1889414 finetune.py:45] layer 15_o initial loss 1.5075260307639837e-05
I0312 08:46:11.517032 1889021 finetune.py:68] layer 12_up @ epoch 0 new loss 1.3693672372028232e-05 old loss 1.4235781236493494e-05 BETTER
I0312 08:46:14.565929 1889283 finetune.py:68] layer 14_o @ epoch 2 new loss 1.2521985809144098e-05 old loss 1.2880251233582385e-05 BETTER
I0312 08:46:16.657295 1889152 finetune.py:45] layer 13_up initial loss 1.5480054571526125e-05
I0312 08:46:34.261429 1889414 finetune.py:68] layer 15_o @ epoch 0 new loss 1.3285503882798366e-05 old loss 1.5075260307639837e-05 BETTER
I0312 08:46:43.778475 1889021 finetune.py:68] layer 12_up @ epoch 1 new loss 1.342995892628096e-05 old loss 1.3693672372028232e-05 BETTER
I0312 08:46:46.541265 1889152 finetune.py:68] layer 13_up @ epoch 0 new loss 1.4758527868252713e-05 old loss 1.5480054571526125e-05 BETTER
I0312 08:46:47.012300 1889283 finetune.py:68] layer 14_o @ epoch 3 new loss 1.2264699762454256e-05 old loss 1.2521985809144098e-05 BETTER
I0312 08:47:06.406857 1889414 finetune.py:68] layer 15_o @ epoch 1 new loss 1.2693683856923599e-05 old loss 1.3285503882798366e-05 BETTER
I0312 08:47:17.300330 1889021 finetune.py:68] layer 12_up @ epoch 2 new loss 1.3235720871307421e-05 old loss 1.342995892628096e-05 BETTER
I0312 08:47:18.222760 1889152 finetune.py:68] layer 13_up @ epoch 1 new loss 1.4430763258133084e-05 old loss 1.4758527868252713e-05 BETTER
I0312 08:47:19.808267 1889283 finetune.py:68] layer 14_o @ epoch 4 new loss 1.2068756404914893e-05 old loss 1.2264699762454256e-05 BETTER
I0312 08:47:38.185688 1889283 finetune.py:45] layer 14_up initial loss 1.892225191113539e-05
I0312 08:47:39.283636 1889414 finetune.py:68] layer 15_o @ epoch 2 new loss 1.233584862347925e-05 old loss 1.2693683856923599e-05 BETTER
I0312 08:47:48.754704 1889152 finetune.py:68] layer 13_up @ epoch 2 new loss 1.4197565178619698e-05 old loss 1.4430763258133084e-05 BETTER
I0312 08:47:49.529285 1889021 finetune.py:68] layer 12_up @ epoch 3 new loss 1.3081747056276072e-05 old loss 1.3235720871307421e-05 BETTER
I0312 08:48:08.339607 1889283 finetune.py:68] layer 14_up @ epoch 0 new loss 1.8166138033848256e-05 old loss 1.892225191113539e-05 BETTER
I0312 08:48:11.816415 1889414 finetune.py:68] layer 15_o @ epoch 3 new loss 1.2088480616512243e-05 old loss 1.233584862347925e-05 BETTER
I0312 08:48:19.967024 1889152 finetune.py:68] layer 13_up @ epoch 3 new loss 1.4019391528563574e-05 old loss 1.4197565178619698e-05 BETTER
I0312 08:48:22.220162 1889021 finetune.py:68] layer 12_up @ epoch 4 new loss 1.2955928468727507e-05 old loss 1.3081747056276072e-05 BETTER
I0312 08:48:40.413008 1889283 finetune.py:68] layer 14_up @ epoch 1 new loss 1.7812883015722036e-05 old loss 1.8166138033848256e-05 BETTER
I0312 08:48:40.714126 1889021 finetune.py:45] layer 12_gate initial loss 1.7023292457452044e-05
I0312 08:48:44.921701 1889414 finetune.py:68] layer 15_o @ epoch 4 new loss 1.1902970072696917e-05 old loss 1.2088480616512243e-05 BETTER
I0312 08:48:49.547026 1889152 finetune.py:68] layer 13_up @ epoch 4 new loss 1.3875925105821807e-05 old loss 1.4019391528563574e-05 BETTER
I0312 08:49:01.188991 1889414 finetune.py:45] layer 15_up initial loss 2.0286295693949796e-05
I0312 08:49:04.960764 1889152 finetune.py:45] layer 13_gate initial loss 1.885079291241709e-05
I0312 08:49:09.649029 1889021 finetune.py:68] layer 12_gate @ epoch 0 new loss 1.6690861230017617e-05 old loss 1.7023292457452044e-05 BETTER
I0312 08:49:10.777576 1889283 finetune.py:68] layer 14_up @ epoch 2 new loss 1.7561282220412977e-05 old loss 1.7812883015722036e-05 BETTER
I0312 08:49:30.431507 1889414 finetune.py:68] layer 15_up @ epoch 0 new loss 1.9281089407741092e-05 old loss 2.0286295693949796e-05 BETTER
I0312 08:49:32.336824 1889152 finetune.py:68] layer 13_gate @ epoch 0 new loss 1.839937613112852e-05 old loss 1.885079291241709e-05 BETTER
I0312 08:49:39.532989 1889021 finetune.py:68] layer 12_gate @ epoch 1 new loss 1.652524952078238e-05 old loss 1.6690861230017617e-05 BETTER
I0312 08:49:40.689831 1889283 finetune.py:68] layer 14_up @ epoch 3 new loss 1.736600461299531e-05 old loss 1.7561282220412977e-05 BETTER
I0312 08:50:00.702271 1889152 finetune.py:68] layer 13_gate @ epoch 1 new loss 1.818941382225603e-05 old loss 1.839937613112852e-05 BETTER
I0312 08:50:00.730044 1889414 finetune.py:68] layer 15_up @ epoch 1 new loss 1.88567992154276e-05 old loss 1.9281089407741092e-05 BETTER
I0312 08:50:09.421575 1889021 finetune.py:68] layer 12_gate @ epoch 2 new loss 1.6398949810536578e-05 old loss 1.652524952078238e-05 BETTER
I0312 08:50:10.712528 1889283 finetune.py:68] layer 14_up @ epoch 4 new loss 1.7208338249474764e-05 old loss 1.736600461299531e-05 BETTER
I0312 08:50:26.267799 1889283 finetune.py:45] layer 14_gate initial loss 2.295151170983445e-05
I0312 08:50:29.266811 1889152 finetune.py:68] layer 13_gate @ epoch 2 new loss 1.803604027372785e-05 old loss 1.818941382225603e-05 BETTER
I0312 08:50:30.810018 1889414 finetune.py:68] layer 15_up @ epoch 2 new loss 1.8566450307844207e-05 old loss 1.88567992154276e-05 BETTER
I0312 08:50:39.408455 1889021 finetune.py:68] layer 12_gate @ epoch 3 new loss 1.629599864827469e-05 old loss 1.6398949810536578e-05 BETTER
I0312 08:50:53.924541 1889283 finetune.py:68] layer 14_gate @ epoch 0 new loss 2.2455131329479627e-05 old loss 2.295151170983445e-05 BETTER
I0312 08:50:57.715492 1889152 finetune.py:68] layer 13_gate @ epoch 3 new loss 1.7911377653945237e-05 old loss 1.803604027372785e-05 BETTER
I0312 08:51:01.002849 1889414 finetune.py:68] layer 15_up @ epoch 3 new loss 1.8344426280236803e-05 old loss 1.8566450307844207e-05 BETTER
I0312 08:51:09.317418 1889021 finetune.py:68] layer 12_gate @ epoch 4 new loss 1.620922557776794e-05 old loss 1.629599864827469e-05 BETTER
I0312 08:51:22.454825 1889283 finetune.py:68] layer 14_gate @ epoch 1 new loss 2.2224863641895354e-05 old loss 2.2455131329479627e-05 BETTER
I0312 08:51:25.703987 1889021 finetune.py:45] layer 12_down initial loss 2.4012528228922747e-05
I0312 08:51:26.261145 1889152 finetune.py:68] layer 13_gate @ epoch 4 new loss 1.7808917618822306e-05 old loss 1.7911377653945237e-05 BETTER
I0312 08:51:30.777106 1889414 finetune.py:68] layer 15_up @ epoch 4 new loss 1.8173272110288963e-05 old loss 1.8344426280236803e-05 BETTER
I0312 08:51:42.482855 1889152 finetune.py:45] layer 13_down initial loss 2.74207086476963e-05
I0312 08:51:46.927665 1889414 finetune.py:45] layer 15_gate initial loss 2.5282966817030683e-05
I0312 08:51:50.961587 1889283 finetune.py:68] layer 14_gate @ epoch 2 new loss 2.2056710804463364e-05 old loss 2.2224863641895354e-05 BETTER
I0312 08:51:52.992664 1889021 finetune.py:68] layer 12_down @ epoch 0 new loss 2.4006792955333367e-05 old loss 2.4012528228922747e-05 BETTER
I0312 08:52:08.319988 1889152 finetune.py:68] layer 13_down @ epoch 0 new loss 2.7413590942160226e-05 old loss 2.74207086476963e-05 BETTER
I0312 08:52:14.370188 1889414 finetune.py:68] layer 15_gate @ epoch 0 new loss 2.4623663193779066e-05 old loss 2.5282966817030683e-05 BETTER
I0312 08:52:19.125681 1889283 finetune.py:68] layer 14_gate @ epoch 3 new loss 2.192287502111867e-05 old loss 2.2056710804463364e-05 BETTER
I0312 08:52:21.510344 1889021 finetune.py:68] layer 12_down @ epoch 1 new loss 2.4005988962017e-05 old loss 2.4006792955333367e-05 BETTER
I0312 08:52:34.995181 1889152 finetune.py:68] layer 13_down @ epoch 1 new loss 2.741210482781753e-05 old loss 2.7413590942160226e-05 BETTER
I0312 08:52:42.354228 1889414 finetune.py:68] layer 15_gate @ epoch 1 new loss 2.4341434254893102e-05 old loss 2.4623663193779066e-05 BETTER
I0312 08:52:47.311470 1889283 finetune.py:68] layer 14_gate @ epoch 4 new loss 2.1809810277773067e-05 old loss 2.192287502111867e-05 BETTER
I0312 08:52:49.969513 1889021 finetune.py:68] layer 12_down @ epoch 2 new loss 2.4005503291846253e-05 old loss 2.4005988962017e-05 BETTER
I0312 08:53:01.786021 1889152 finetune.py:68] layer 13_down @ epoch 2 new loss 2.7411300834501162e-05 old loss 2.741210482781753e-05 BETTER
I0312 08:53:03.423216 1889283 finetune.py:45] layer 14_down initial loss 3.293738336651586e-05
I0312 08:53:10.495033 1889414 finetune.py:68] layer 15_gate @ epoch 2 new loss 2.414061782474164e-05 old loss 2.4341434254893102e-05 BETTER
I0312 08:53:18.487240 1889021 finetune.py:68] layer 12_down @ epoch 3 new loss 2.400528683210723e-05 old loss 2.4005503291846253e-05 BETTER
I0312 08:53:28.513282 1889152 finetune.py:68] layer 13_down @ epoch 3 new loss 2.7410804250393994e-05 old loss 2.7411300834501162e-05 BETTER
I0312 08:53:29.450711 1889283 finetune.py:68] layer 14_down @ epoch 0 new loss 3.292913970653899e-05 old loss 3.293738336651586e-05 BETTER
I0312 08:53:38.484764 1889414 finetune.py:68] layer 15_gate @ epoch 3 new loss 2.398240758338943e-05 old loss 2.414061782474164e-05 BETTER
I0312 08:53:46.889361 1889021 finetune.py:68] layer 12_down @ epoch 4 new loss 2.400494304310996e-05 old loss 2.400528683210723e-05 BETTER
12_v proxy err 0.0006483556353487074 tr(WHW.T) 703.318603515625
12_q proxy err 0.00011522725253598765 tr(WHW.T) 7045.72607421875
12_k proxy err 9.196514292852953e-05 tr(WHW.T) 10891.943359375
12_o proxy err 0.0008695555734448135 tr(WHW.T) 39.30789566040039
12_up proxy err 0.0003856080293189734 tr(WHW.T) 1228.234619140625
12_gate proxy err 0.00022468248789664358 tr(WHW.T) 2381.41162109375
12_down proxy err 0.0005104127340018749 tr(WHW.T) 64.18862915039062
I0312 08:53:57.984580 1889152 finetune.py:68] layer 13_down @ epoch 4 new loss 2.7410524126025848e-05 old loss 2.7410804250393994e-05 BETTER
I0312 08:53:58.776926 1889283 finetune.py:68] layer 14_down @ epoch 1 new loss 3.292734254500829e-05 old loss 3.292913970653899e-05 BETTER
13_v proxy err 0.0006445269100368023 tr(WHW.T) 714.5677490234375
13_q proxy err 0.00011098043614765629 tr(WHW.T) 6956.40869140625
13_k proxy err 8.614012767793611e-05 tr(WHW.T) 10426.962890625
13_o proxy err 0.0007770521333441138 tr(WHW.T) 45.856422424316406
13_up proxy err 0.0003729879099410027 tr(WHW.T) 1367.49072265625
13_gate proxy err 0.00022267179156187922 tr(WHW.T) 2600.974609375
13_down proxy err 0.0005086813471280038 tr(WHW.T) 79.37957763671875
I0312 08:54:07.248407 1889414 finetune.py:68] layer 15_gate @ epoch 4 new loss 2.3854239771026187e-05 old loss 2.398240758338943e-05 BETTER
I0312 08:54:23.506306 1889414 finetune.py:45] layer 15_down initial loss 3.7861835153307766e-05
I0312 08:54:25.960131 1889283 finetune.py:68] layer 14_down @ epoch 2 new loss 3.2926520361797884e-05 old loss 3.292734254500829e-05 BETTER
I0312 08:54:49.654422 1889414 finetune.py:68] layer 15_down @ epoch 0 new loss 3.785136868827976e-05 old loss 3.7861835153307766e-05 BETTER
I0312 08:54:52.833347 1889283 finetune.py:68] layer 14_down @ epoch 3 new loss 3.292601104476489e-05 old loss 3.2926520361797884e-05 BETTER
I0312 08:55:15.501640 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 16 in 72.94459843635559s
I0312 08:55:16.798964 1889414 finetune.py:68] layer 15_down @ epoch 1 new loss 3.784952423302457e-05 old loss 3.785136868827976e-05 BETTER
I0312 08:55:18.824135 1889695 config.py:54] PyTorch version 2.1.1 available.
I0312 08:55:19.763509 1889283 finetune.py:68] layer 14_down @ epoch 4 new loss 3.29255526594352e-05 old loss 3.292601104476489e-05 BETTER
I0312 08:55:19.864549 1886474 quantize_finetune_llama.py:183] layer 17 gpu 1
I0312 08:55:19.935316 1889695 data_utils.py:336] using 256 training seqs, 128 validation seqs
14_v proxy err 0.0006811996572650969 tr(WHW.T) 706.1612548828125
14_q proxy err 0.00011697450099745765 tr(WHW.T) 7077.31005859375
14_k proxy err 8.985818567452952e-05 tr(WHW.T) 11296.0380859375
14_o proxy err 0.0008591485675424337 tr(WHW.T) 50.97962188720703
14_up proxy err 0.00037776053068228066 tr(WHW.T) 1464.196044921875
14_gate proxy err 0.00023189403873402625 tr(WHW.T) 2680.746337890625
14_down proxy err 0.0005153288948349655 tr(WHW.T) 90.29544067382812
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:55:28.382254 1889695 finetune.py:45] layer 16_v initial loss 1.1861179700645152e-05
I0312 08:55:43.813658 1889414 finetune.py:68] layer 15_down @ epoch 2 new loss 3.784840737353079e-05 old loss 3.784952423302457e-05 BETTER
I0312 08:56:01.368777 1889695 finetune.py:68] layer 16_v @ epoch 0 new loss 6.64880371914478e-06 old loss 1.1861179700645152e-05 BETTER
I0312 08:56:10.828238 1889414 finetune.py:68] layer 15_down @ epoch 3 new loss 3.7847639760002494e-05 old loss 3.784840737353079e-05 BETTER
I0312 08:56:35.298586 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 17 in 71.93697261810303s
I0312 08:56:35.608498 1889695 finetune.py:68] layer 16_v @ epoch 1 new loss 5.948090802121442e-06 old loss 6.64880371914478e-06 BETTER
I0312 08:56:37.801684 1889414 finetune.py:68] layer 15_down @ epoch 4 new loss 3.7847352359676734e-05 old loss 3.7847639760002494e-05 BETTER
I0312 08:56:38.586585 1889811 config.py:54] PyTorch version 2.1.1 available.
15_v proxy err 0.0006252502789720893 tr(WHW.T) 762.7275390625
15_q proxy err 0.00011079143587267026 tr(WHW.T) 7252.595703125
15_k proxy err 8.650669042253867e-05 tr(WHW.T) 11072.9326171875
15_o proxy err 0.0007287778425961733 tr(WHW.T) 59.65945053100586
15_up proxy err 0.00036882804124616086 tr(WHW.T) 1640.8192138671875
15_gate proxy err 0.00023409852292388678 tr(WHW.T) 2904.291259765625
15_down proxy err 0.0005119295092299581 tr(WHW.T) 114.110107421875
I0312 08:56:39.864470 1886474 quantize_finetune_llama.py:183] layer 18 gpu 2
I0312 08:56:39.932627 1889811 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:56:48.157567 1889811 finetune.py:45] layer 17_v initial loss 9.9108156064176e-06
I0312 08:57:10.239966 1889695 finetune.py:68] layer 16_v @ epoch 2 new loss 5.644255452352809e-06 old loss 5.948090802121442e-06 BETTER
I0312 08:57:19.497123 1889811 finetune.py:68] layer 17_v @ epoch 0 new loss 5.606153536064085e-06 old loss 9.9108156064176e-06 BETTER
I0312 08:57:44.938840 1889695 finetune.py:68] layer 16_v @ epoch 3 new loss 5.605955266219098e-06 old loss 5.644255452352809e-06 BETTER
I0312 08:57:51.591012 1889811 finetune.py:68] layer 17_v @ epoch 1 new loss 5.084638814878417e-06 old loss 5.606153536064085e-06 BETTER
I0312 08:57:52.100429 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 18 in 70.09884643554688s
I0312 08:57:55.258440 1889927 config.py:54] PyTorch version 2.1.1 available.
I0312 08:57:56.280916 1886474 quantize_finetune_llama.py:183] layer 19 gpu 3
I0312 08:57:56.368635 1889927 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:58:06.224450 1889927 finetune.py:45] layer 18_v initial loss 9.636312825023197e-06
I0312 08:58:19.948443 1889695 finetune.py:76] layer 16_v @ epoch 4 new loss 5.673859504895518e-06 old loss 5.605955266219098e-06 WORSE
I0312 08:58:24.302772 1889811 finetune.py:68] layer 17_v @ epoch 2 new loss 4.946411081618862e-06 old loss 5.084638814878417e-06 BETTER
I0312 08:58:30.124640 1889695 finetune.py:45] layer 16_q initial loss 7.951996849442367e-06
I0312 08:58:38.359463 1889927 finetune.py:68] layer 18_v @ epoch 0 new loss 5.625753146887291e-06 old loss 9.636312825023197e-06 BETTER
I0312 08:58:57.513255 1889811 finetune.py:76] layer 17_v @ epoch 3 new loss 5.011645043850876e-06 old loss 4.946411081618862e-06 WORSE
I0312 08:59:03.699984 1889695 finetune.py:68] layer 16_q @ epoch 0 new loss 6.656187906628475e-06 old loss 7.951996849442367e-06 BETTER
I0312 08:59:11.763943 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 19 in 75.0833170413971s
I0312 08:59:11.847156 1889927 finetune.py:68] layer 18_v @ epoch 1 new loss 5.3535222832579166e-06 old loss 5.625753146887291e-06 BETTER
I0312 08:59:15.228436 1890043 config.py:54] PyTorch version 2.1.1 available.
I0312 08:59:16.342959 1886474 quantize_finetune_llama.py:183] layer 20 gpu 0
I0312 08:59:16.418099 1890043 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 08:59:27.793537 1890043 finetune.py:45] layer 19_v initial loss 9.464220966037828e-06
I0312 08:59:31.781857 1889811 finetune.py:76] layer 17_v @ epoch 4 new loss 4.95883978146594e-06 old loss 4.946411081618862e-06 WORSE
I0312 08:59:39.366242 1889695 finetune.py:68] layer 16_q @ epoch 1 new loss 6.2629901549371425e-06 old loss 6.656187906628475e-06 BETTER
I0312 08:59:43.336034 1889811 finetune.py:45] layer 17_q initial loss 7.181509317888413e-06
I0312 08:59:45.437043 1889927 finetune.py:68] layer 18_v @ epoch 2 new loss 5.322522611095337e-06 old loss 5.3535222832579166e-06 BETTER
I0312 08:59:59.547545 1890043 finetune.py:68] layer 19_v @ epoch 0 new loss 5.55479573449702e-06 old loss 9.464220966037828e-06 BETTER
I0312 09:00:14.043084 1889695 finetune.py:68] layer 16_q @ epoch 2 new loss 6.069137725717155e-06 old loss 6.2629901549371425e-06 BETTER
I0312 09:00:14.843507 1889811 finetune.py:68] layer 17_q @ epoch 0 new loss 5.951221737632295e-06 old loss 7.181509317888413e-06 BETTER
I0312 09:00:18.609104 1889927 finetune.py:68] layer 18_v @ epoch 3 new loss 4.997040377929807e-06 old loss 5.322522611095337e-06 BETTER
I0312 09:00:32.014758 1890043 finetune.py:68] layer 19_v @ epoch 1 new loss 5.137381322128931e-06 old loss 5.55479573449702e-06 BETTER
I0312 09:00:47.342000 1889811 finetune.py:68] layer 17_q @ epoch 1 new loss 5.610439075098839e-06 old loss 5.951221737632295e-06 BETTER
I0312 09:00:48.621737 1889695 finetune.py:68] layer 16_q @ epoch 3 new loss 5.917521775700152e-06 old loss 6.069137725717155e-06 BETTER
I0312 09:00:52.359362 1889927 finetune.py:68] layer 18_v @ epoch 4 new loss 4.840868768951623e-06 old loss 4.997040377929807e-06 BETTER
I0312 09:01:02.112449 1889927 finetune.py:45] layer 18_q initial loss 7.651760824956e-06
I0312 09:01:04.797782 1890043 finetune.py:68] layer 19_v @ epoch 2 new loss 4.92372555527254e-06 old loss 5.137381322128931e-06 BETTER
I0312 09:01:19.959146 1889811 finetune.py:68] layer 17_q @ epoch 2 new loss 5.411321126302937e-06 old loss 5.610439075098839e-06 BETTER
I0312 09:01:23.254544 1889695 finetune.py:68] layer 16_q @ epoch 4 new loss 5.81198855797993e-06 old loss 5.917521775700152e-06 BETTER
I0312 09:01:33.041749 1889695 finetune.py:45] layer 16_k initial loss 7.765329428366385e-06
I0312 09:01:34.490198 1889927 finetune.py:68] layer 18_q @ epoch 0 new loss 6.048534942237893e-06 old loss 7.651760824956e-06 BETTER
I0312 09:01:37.791689 1890043 finetune.py:68] layer 19_v @ epoch 3 new loss 4.876851107837865e-06 old loss 4.92372555527254e-06 BETTER
I0312 09:01:52.459919 1889811 finetune.py:68] layer 17_q @ epoch 3 new loss 5.281383891997393e-06 old loss 5.411321126302937e-06 BETTER
I0312 09:02:05.939448 1889695 finetune.py:68] layer 16_k @ epoch 0 new loss 7.401050424959976e-06 old loss 7.765329428366385e-06 BETTER
I0312 09:02:07.650583 1889927 finetune.py:68] layer 18_q @ epoch 1 new loss 5.789361694041872e-06 old loss 6.048534942237893e-06 BETTER
I0312 09:02:10.607984 1890043 finetune.py:68] layer 19_v @ epoch 4 new loss 4.854494363826234e-06 old loss 4.876851107837865e-06 BETTER
I0312 09:02:20.268098 1890043 finetune.py:45] layer 19_q initial loss 7.220049610623391e-06
I0312 09:02:24.729146 1889811 finetune.py:68] layer 17_q @ epoch 4 new loss 5.231645900494186e-06 old loss 5.281383891997393e-06 BETTER
I0312 09:02:33.984179 1889811 finetune.py:45] layer 17_k initial loss 7.3961105044872966e-06
I0312 09:02:40.152393 1889695 finetune.py:68] layer 16_k @ epoch 1 new loss 7.2937950790219475e-06 old loss 7.401050424959976e-06 BETTER
I0312 09:02:40.677730 1889927 finetune.py:68] layer 18_q @ epoch 2 new loss 5.5936457101779524e-06 old loss 5.789361694041872e-06 BETTER
I0312 09:02:52.003752 1890043 finetune.py:68] layer 19_q @ epoch 0 new loss 5.938774393143831e-06 old loss 7.220049610623391e-06 BETTER
I0312 09:03:05.555774 1889811 finetune.py:68] layer 17_k @ epoch 0 new loss 6.81691062709433e-06 old loss 7.3961105044872966e-06 BETTER
I0312 09:03:13.837949 1889695 finetune.py:68] layer 16_k @ epoch 2 new loss 7.226240086311009e-06 old loss 7.2937950790219475e-06 BETTER
I0312 09:03:14.150291 1889927 finetune.py:68] layer 18_q @ epoch 3 new loss 5.547759883484105e-06 old loss 5.5936457101779524e-06 BETTER
I0312 09:03:24.311160 1890043 finetune.py:68] layer 19_q @ epoch 1 new loss 5.6406342991977e-06 old loss 5.938774393143831e-06 BETTER
I0312 09:03:37.646370 1889811 finetune.py:68] layer 17_k @ epoch 1 new loss 6.700172434648266e-06 old loss 6.81691062709433e-06 BETTER
I0312 09:03:47.524087 1889927 finetune.py:68] layer 18_q @ epoch 4 new loss 5.478434559336165e-06 old loss 5.547759883484105e-06 BETTER
I0312 09:03:47.647426 1889695 finetune.py:68] layer 16_k @ epoch 3 new loss 7.1953400038182735e-06 old loss 7.226240086311009e-06 BETTER
I0312 09:03:56.964012 1890043 finetune.py:68] layer 19_q @ epoch 2 new loss 5.473396868183045e-06 old loss 5.6406342991977e-06 BETTER
I0312 09:03:57.537552 1889927 finetune.py:45] layer 18_k initial loss 7.813364391040523e-06
I0312 09:04:09.714030 1889811 finetune.py:68] layer 17_k @ epoch 2 new loss 6.607108389289351e-06 old loss 6.700172434648266e-06 BETTER
I0312 09:04:21.330484 1889695 finetune.py:68] layer 16_k @ epoch 4 new loss 7.142150025174487e-06 old loss 7.1953400038182735e-06 BETTER
I0312 09:04:30.211370 1889927 finetune.py:68] layer 18_k @ epoch 0 new loss 7.305689905479085e-06 old loss 7.813364391040523e-06 BETTER
I0312 09:04:30.684237 1890043 finetune.py:68] layer 19_q @ epoch 3 new loss 5.356608198781032e-06 old loss 5.473396868183045e-06 BETTER
I0312 09:04:31.304099 1889695 finetune.py:45] layer 16_o initial loss 1.9052318748435937e-05
I0312 09:04:41.894318 1889811 finetune.py:68] layer 17_k @ epoch 3 new loss 6.564861905644648e-06 old loss 6.607108389289351e-06 BETTER
I0312 09:05:03.521328 1889695 finetune.py:68] layer 16_o @ epoch 0 new loss 1.695979699434247e-05 old loss 1.9052318748435937e-05 BETTER
I0312 09:05:03.586272 1889927 finetune.py:68] layer 18_k @ epoch 1 new loss 7.168421234382549e-06 old loss 7.305689905479085e-06 BETTER
I0312 09:05:04.023268 1890043 finetune.py:68] layer 19_q @ epoch 4 new loss 5.272495400276966e-06 old loss 5.356608198781032e-06 BETTER
I0312 09:05:13.977811 1890043 finetune.py:45] layer 19_k initial loss 7.354654371738434e-06
I0312 09:05:14.212981 1889811 finetune.py:68] layer 17_k @ epoch 4 new loss 6.491717158496613e-06 old loss 6.564861905644648e-06 BETTER
I0312 09:05:23.551393 1889811 finetune.py:45] layer 17_o initial loss 1.5244844689732417e-05
I0312 09:05:36.212537 1889927 finetune.py:68] layer 18_k @ epoch 2 new loss 7.084619028319139e-06 old loss 7.168421234382549e-06 BETTER
I0312 09:05:36.499546 1889695 finetune.py:68] layer 16_o @ epoch 1 new loss 1.6199754099943675e-05 old loss 1.695979699434247e-05 BETTER
I0312 09:05:45.470881 1890043 finetune.py:68] layer 19_k @ epoch 0 new loss 6.947249403310707e-06 old loss 7.354654371738434e-06 BETTER
I0312 09:05:54.116883 1889811 finetune.py:68] layer 17_o @ epoch 0 new loss 1.3824829693476204e-05 old loss 1.5244844689732417e-05 BETTER
I0312 09:06:08.716340 1889927 finetune.py:68] layer 18_k @ epoch 3 new loss 7.029798325675074e-06 old loss 7.084619028319139e-06 BETTER
I0312 09:06:09.899029 1889695 finetune.py:68] layer 16_o @ epoch 2 new loss 1.5738483853056096e-05 old loss 1.6199754099943675e-05 BETTER
I0312 09:06:17.673335 1890043 finetune.py:68] layer 19_k @ epoch 1 new loss 6.862616373837227e-06 old loss 6.947249403310707e-06 BETTER
I0312 09:06:25.395075 1889811 finetune.py:68] layer 17_o @ epoch 1 new loss 1.3328934983292129e-05 old loss 1.3824829693476204e-05 BETTER
I0312 09:06:41.328330 1889927 finetune.py:68] layer 18_k @ epoch 4 new loss 7.0021756073401775e-06 old loss 7.029798325675074e-06 BETTER
I0312 09:06:43.424141 1889695 finetune.py:68] layer 16_o @ epoch 3 new loss 1.5408148101414554e-05 old loss 1.5738483853056096e-05 BETTER
I0312 09:06:50.461379 1890043 finetune.py:68] layer 19_k @ epoch 2 new loss 6.820134785812115e-06 old loss 6.862616373837227e-06 BETTER
I0312 09:06:51.742645 1889927 finetune.py:45] layer 18_o initial loss 1.639238871575799e-05
I0312 09:06:56.757453 1889811 finetune.py:68] layer 17_o @ epoch 2 new loss 1.3038153156230692e-05 old loss 1.3328934983292129e-05 BETTER
I0312 09:07:16.491895 1889695 finetune.py:68] layer 16_o @ epoch 4 new loss 1.516611700935755e-05 old loss 1.5408148101414554e-05 BETTER
I0312 09:07:22.910027 1890043 finetune.py:68] layer 19_k @ epoch 3 new loss 6.786447102058446e-06 old loss 6.820134785812115e-06 BETTER
I0312 09:07:23.346873 1889927 finetune.py:68] layer 18_o @ epoch 0 new loss 1.4916780855855905e-05 old loss 1.639238871575799e-05 BETTER
I0312 09:07:28.116533 1889811 finetune.py:68] layer 17_o @ epoch 3 new loss 1.2829856132157147e-05 old loss 1.3038153156230692e-05 BETTER
I0312 09:07:32.293354 1889695 finetune.py:45] layer 16_up initial loss 2.5997995180659927e-05
I0312 09:07:55.358904 1890043 finetune.py:68] layer 19_k @ epoch 4 new loss 6.715896233799867e-06 old loss 6.786447102058446e-06 BETTER
I0312 09:07:55.926588 1889927 finetune.py:68] layer 18_o @ epoch 1 new loss 1.4424706932913978e-05 old loss 1.4916780855855905e-05 BETTER
I0312 09:07:59.811281 1889811 finetune.py:68] layer 17_o @ epoch 4 new loss 1.2690427865891252e-05 old loss 1.2829856132157147e-05 BETTER
I0312 09:08:03.054135 1889695 finetune.py:68] layer 16_up @ epoch 0 new loss 2.4727685740799643e-05 old loss 2.5997995180659927e-05 BETTER
I0312 09:08:05.448080 1890043 finetune.py:45] layer 19_o initial loss 1.550592787680216e-05
I0312 09:08:15.205437 1889811 finetune.py:45] layer 17_up initial loss 2.4666014724061824e-05
I0312 09:08:27.894972 1889927 finetune.py:68] layer 18_o @ epoch 2 new loss 1.4138856386125553e-05 old loss 1.4424706932913978e-05 BETTER
I0312 09:08:34.699048 1889695 finetune.py:68] layer 16_up @ epoch 1 new loss 2.4182947527151555e-05 old loss 2.4727685740799643e-05 BETTER
I0312 09:08:36.079628 1890043 finetune.py:68] layer 19_o @ epoch 0 new loss 1.4228657164494507e-05 old loss 1.550592787680216e-05 BETTER
I0312 09:08:44.110301 1889811 finetune.py:68] layer 17_up @ epoch 0 new loss 2.350910108361859e-05 old loss 2.4666014724061824e-05 BETTER
I0312 09:08:59.946122 1889927 finetune.py:68] layer 18_o @ epoch 3 new loss 1.3953010238765273e-05 old loss 1.4138856386125553e-05 BETTER
I0312 09:09:06.287764 1889695 finetune.py:68] layer 16_up @ epoch 2 new loss 2.3820002752472647e-05 old loss 2.4182947527151555e-05 BETTER
I0312 09:09:07.667933 1890043 finetune.py:68] layer 19_o @ epoch 1 new loss 1.3858889360562898e-05 old loss 1.4228657164494507e-05 BETTER
I0312 09:09:13.740465 1889811 finetune.py:68] layer 17_up @ epoch 1 new loss 2.2999776774668135e-05 old loss 2.350910108361859e-05 BETTER
I0312 09:09:33.741055 1889927 finetune.py:68] layer 18_o @ epoch 4 new loss 1.3821419088344555e-05 old loss 1.3953010238765273e-05 BETTER
I0312 09:09:38.746057 1889695 finetune.py:68] layer 16_up @ epoch 3 new loss 2.3549579054815695e-05 old loss 2.3820002752472647e-05 BETTER
I0312 09:09:40.719702 1890043 finetune.py:68] layer 19_o @ epoch 2 new loss 1.3614494491775986e-05 old loss 1.3858889360562898e-05 BETTER
I0312 09:09:43.829570 1889811 finetune.py:68] layer 17_up @ epoch 2 new loss 2.26732408918906e-05 old loss 2.2999776774668135e-05 BETTER
I0312 09:09:51.261703 1889927 finetune.py:45] layer 18_up initial loss 2.8074440706404857e-05
I0312 09:10:10.720350 1889695 finetune.py:68] layer 16_up @ epoch 4 new loss 2.3342510758084245e-05 old loss 2.3549579054815695e-05 BETTER
I0312 09:10:12.462723 1890043 finetune.py:68] layer 19_o @ epoch 3 new loss 1.3480286725098267e-05 old loss 1.3614494491775986e-05 BETTER
I0312 09:10:13.661737 1889811 finetune.py:68] layer 17_up @ epoch 3 new loss 2.2431086108554155e-05 old loss 2.26732408918906e-05 BETTER
I0312 09:10:20.606146 1889927 finetune.py:68] layer 18_up @ epoch 0 new loss 2.6754420105135068e-05 old loss 2.8074440706404857e-05 BETTER
I0312 09:10:26.079880 1889695 finetune.py:45] layer 16_gate initial loss 3.282799298176542e-05
I0312 09:10:43.333725 1889811 finetune.py:68] layer 17_up @ epoch 4 new loss 2.224747368018143e-05 old loss 2.2431086108554155e-05 BETTER
I0312 09:10:43.952620 1890043 finetune.py:68] layer 19_o @ epoch 4 new loss 1.3378501535044052e-05 old loss 1.3480286725098267e-05 BETTER
I0312 09:10:50.751847 1889927 finetune.py:68] layer 18_up @ epoch 1 new loss 2.6199249987257645e-05 old loss 2.6754420105135068e-05 BETTER
I0312 09:10:55.074544 1889695 finetune.py:68] layer 16_gate @ epoch 0 new loss 3.187738548149355e-05 old loss 3.282799298176542e-05 BETTER
I0312 09:10:58.443969 1889811 finetune.py:45] layer 17_gate initial loss 3.282618126831949e-05
I0312 09:10:59.414022 1890043 finetune.py:45] layer 19_up initial loss 2.943803156085778e-05
I0312 09:11:21.121947 1889927 finetune.py:68] layer 18_up @ epoch 2 new loss 2.5843308321782388e-05 old loss 2.6199249987257645e-05 BETTER
I0312 09:11:24.952019 1889695 finetune.py:68] layer 16_gate @ epoch 1 new loss 3.149612894048914e-05 old loss 3.187738548149355e-05 BETTER
I0312 09:11:26.010104 1889811 finetune.py:68] layer 17_gate @ epoch 0 new loss 3.1960509659256786e-05 old loss 3.282618126831949e-05 BETTER
I0312 09:11:28.422203 1890043 finetune.py:68] layer 19_up @ epoch 0 new loss 2.8032060072291642e-05 old loss 2.943803156085778e-05 BETTER
I0312 09:11:51.549920 1889927 finetune.py:68] layer 18_up @ epoch 3 new loss 2.5583665774320252e-05 old loss 2.5843308321782388e-05 BETTER
I0312 09:11:54.209626 1889811 finetune.py:68] layer 17_gate @ epoch 1 new loss 3.1586678232997656e-05 old loss 3.1960509659256786e-05 BETTER
I0312 09:11:54.969492 1889695 finetune.py:68] layer 16_gate @ epoch 2 new loss 3.123068017885089e-05 old loss 3.149612894048914e-05 BETTER
I0312 09:11:58.197329 1890043 finetune.py:68] layer 19_up @ epoch 1 new loss 2.7472160581965e-05 old loss 2.8032060072291642e-05 BETTER
I0312 09:12:22.079583 1889927 finetune.py:68] layer 18_up @ epoch 4 new loss 2.5393377654836513e-05 old loss 2.5583665774320252e-05 BETTER
I0312 09:12:22.562989 1889811 finetune.py:68] layer 17_gate @ epoch 2 new loss 3.1335734092863277e-05 old loss 3.1586678232997656e-05 BETTER
I0312 09:12:24.931392 1889695 finetune.py:68] layer 16_gate @ epoch 3 new loss 3.1033207051223144e-05 old loss 3.123068017885089e-05 BETTER
I0312 09:12:27.965863 1890043 finetune.py:68] layer 19_up @ epoch 2 new loss 2.711834713409189e-05 old loss 2.7472160581965e-05 BETTER
I0312 09:12:37.495593 1889927 finetune.py:45] layer 18_gate initial loss 3.762758569791913e-05
I0312 09:12:51.053553 1889811 finetune.py:68] layer 17_gate @ epoch 3 new loss 3.1145824323175475e-05 old loss 3.1335734092863277e-05 BETTER
I0312 09:12:55.100140 1889695 finetune.py:68] layer 16_gate @ epoch 4 new loss 3.0874991352902725e-05 old loss 3.1033207051223144e-05 BETTER
I0312 09:12:57.998875 1890043 finetune.py:68] layer 19_up @ epoch 3 new loss 2.6875490220845677e-05 old loss 2.711834713409189e-05 BETTER
I0312 09:13:05.294159 1889927 finetune.py:68] layer 18_gate @ epoch 0 new loss 3.678984649013728e-05 old loss 3.762758569791913e-05 BETTER
I0312 09:13:11.247338 1889695 finetune.py:45] layer 16_down initial loss 4.990653178538196e-05
I0312 09:13:19.403786 1889811 finetune.py:68] layer 17_gate @ epoch 4 new loss 3.099552486673929e-05 old loss 3.1145824323175475e-05 BETTER
I0312 09:13:27.687842 1890043 finetune.py:68] layer 19_up @ epoch 4 new loss 2.670064895937685e-05 old loss 2.6875490220845677e-05 BETTER
I0312 09:13:33.753559 1889927 finetune.py:68] layer 18_gate @ epoch 1 new loss 3.6404861020855606e-05 old loss 3.678984649013728e-05 BETTER
I0312 09:13:35.082795 1889811 finetune.py:45] layer 17_down initial loss 5.176105332793668e-05
I0312 09:13:38.570747 1889695 finetune.py:68] layer 16_down @ epoch 0 new loss 4.989163426216692e-05 old loss 4.990653178538196e-05 BETTER
I0312 09:13:43.404809 1890043 finetune.py:45] layer 19_gate initial loss 4.087442357558757e-05
I0312 09:14:00.954057 1889811 finetune.py:68] layer 17_down @ epoch 0 new loss 5.174364196136594e-05 old loss 5.176105332793668e-05 BETTER
I0312 09:14:02.310152 1889927 finetune.py:68] layer 18_gate @ epoch 2 new loss 3.614058732637204e-05 old loss 3.6404861020855606e-05 BETTER
I0312 09:14:07.032920 1889695 finetune.py:68] layer 16_down @ epoch 1 new loss 4.9888662033481523e-05 old loss 4.989163426216692e-05 BETTER
I0312 09:14:10.877344 1890043 finetune.py:68] layer 19_gate @ epoch 0 new loss 3.9964248571777716e-05 old loss 4.087442357558757e-05 BETTER
I0312 09:14:27.663364 1889811 finetune.py:68] layer 17_down @ epoch 1 new loss 5.174034959054552e-05 old loss 5.174364196136594e-05 BETTER
I0312 09:14:31.099682 1889927 finetune.py:68] layer 18_gate @ epoch 3 new loss 3.594868758227676e-05 old loss 3.614058732637204e-05 BETTER
I0312 09:14:35.418945 1889695 finetune.py:68] layer 16_down @ epoch 2 new loss 4.9886846682056785e-05 old loss 4.9888662033481523e-05 BETTER
I0312 09:14:39.277666 1890043 finetune.py:68] layer 19_gate @ epoch 1 new loss 3.9551065128762275e-05 old loss 3.9964248571777716e-05 BETTER
I0312 09:14:56.894136 1889811 finetune.py:68] layer 17_down @ epoch 2 new loss 5.1738315960392356e-05 old loss 5.174034959054552e-05 BETTER
I0312 09:15:02.177251 1889927 finetune.py:68] layer 18_gate @ epoch 4 new loss 3.580152406357229e-05 old loss 3.594868758227676e-05 BETTER
I0312 09:15:05.527297 1889695 finetune.py:68] layer 16_down @ epoch 3 new loss 4.988575528841466e-05 old loss 4.9886846682056785e-05 BETTER
I0312 09:15:08.754843 1890043 finetune.py:68] layer 19_gate @ epoch 2 new loss 3.927967190975323e-05 old loss 3.9551065128762275e-05 BETTER
I0312 09:15:22.317427 1889927 finetune.py:45] layer 18_down initial loss 6.071142706787214e-05
I0312 09:15:24.105469 1889811 finetune.py:68] layer 17_down @ epoch 3 new loss 5.1737340982072055e-05 old loss 5.1738315960392356e-05 BETTER
I0312 09:15:34.310706 1889695 finetune.py:68] layer 16_down @ epoch 4 new loss 4.9885205953614786e-05 old loss 4.988575528841466e-05 BETTER
16_v proxy err 0.0006383373402059078 tr(WHW.T) 780.7407836914062
16_q proxy err 0.00011586981418076903 tr(WHW.T) 7193.61376953125
16_k proxy err 8.810973668005317e-05 tr(WHW.T) 11629.4931640625
16_o proxy err 0.0006032573292031884 tr(WHW.T) 88.25988006591797
16_up proxy err 0.00036551684024743736 tr(WHW.T) 1891.0439453125
16_gate proxy err 0.0002351749426452443 tr(WHW.T) 3370.3974609375
16_down proxy err 0.0005195910343900323 tr(WHW.T) 152.04962158203125
I0312 09:15:37.270186 1890043 finetune.py:68] layer 19_gate @ epoch 3 new loss 3.908212966052815e-05 old loss 3.927967190975323e-05 BETTER
I0312 09:15:48.832454 1889927 finetune.py:68] layer 18_down @ epoch 0 new loss 6.0693728300975636e-05 old loss 6.071142706787214e-05 BETTER
I0312 09:15:51.250339 1889811 finetune.py:68] layer 17_down @ epoch 4 new loss 5.1736304158112034e-05 old loss 5.1737340982072055e-05 BETTER
17_v proxy err 0.0006147792446427047 tr(WHW.T) 845.7654418945312
17_q proxy err 0.00011584549065446481 tr(WHW.T) 7163.6494140625
17_k proxy err 9.188919648295268e-05 tr(WHW.T) 10697.55078125
17_o proxy err 0.0006550610414706171 tr(WHW.T) 58.188907623291016
17_up proxy err 0.0003973451675847173 tr(WHW.T) 1921.271728515625
17_gate proxy err 0.0002474625362083316 tr(WHW.T) 3572.0302734375
17_down proxy err 0.0005203672917559743 tr(WHW.T) 165.4643096923828
I0312 09:16:06.059837 1890043 finetune.py:68] layer 19_gate @ epoch 4 new loss 3.893630128004588e-05 old loss 3.908212966052815e-05 BETTER
I0312 09:16:15.911555 1889927 finetune.py:68] layer 18_down @ epoch 1 new loss 6.069004302844405e-05 old loss 6.0693728300975636e-05 BETTER
I0312 09:16:21.972103 1890043 finetune.py:45] layer 19_down initial loss 6.61752128507942e-05
I0312 09:16:43.001428 1889927 finetune.py:68] layer 18_down @ epoch 2 new loss 6.068783113732934e-05 old loss 6.069004302844405e-05 BETTER
I0312 09:16:48.080335 1890043 finetune.py:68] layer 19_down @ epoch 0 new loss 6.615408346988261e-05 old loss 6.61752128507942e-05 BETTER
I0312 09:17:08.529392 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 20 in 73.45193386077881s
I0312 09:17:10.216633 1889927 finetune.py:68] layer 18_down @ epoch 3 new loss 6.068639413570054e-05 old loss 6.068783113732934e-05 BETTER
I0312 09:17:11.775799 1890159 config.py:54] PyTorch version 2.1.1 available.
I0312 09:17:12.849209 1886474 quantize_finetune_llama.py:183] layer 21 gpu 1
I0312 09:17:12.920483 1890159 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 09:17:15.085672 1890043 finetune.py:68] layer 19_down @ epoch 1 new loss 6.615072925342247e-05 old loss 6.615408346988261e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 09:17:21.535541 1890159 finetune.py:45] layer 20_v initial loss 1.1246018402744085e-05
I0312 09:17:37.381381 1889927 finetune.py:68] layer 18_down @ epoch 4 new loss 6.068508810130879e-05 old loss 6.068639413570054e-05 BETTER
18_v proxy err 0.0005744260852225125 tr(WHW.T) 1003.7705078125
18_q proxy err 0.00011802693188656121 tr(WHW.T) 7510.5751953125
18_k proxy err 9.505648631602526e-05 tr(WHW.T) 10462.4267578125
18_o proxy err 0.0005719690816476941 tr(WHW.T) 69.97306060791016
18_up proxy err 0.00041984772542491555 tr(WHW.T) 2023.3056640625
18_gate proxy err 0.0002590329968370497 tr(WHW.T) 3782.73876953125
18_down proxy err 0.0005180619773454964 tr(WHW.T) 198.56732177734375
I0312 09:17:41.782743 1890043 finetune.py:68] layer 19_down @ epoch 2 new loss 6.614921585423872e-05 old loss 6.615072925342247e-05 BETTER
I0312 09:17:54.477477 1890159 finetune.py:68] layer 20_v @ epoch 0 new loss 6.606032911804505e-06 old loss 1.1246018402744085e-05 BETTER
I0312 09:18:08.578767 1890043 finetune.py:68] layer 19_down @ epoch 3 new loss 6.614809535676613e-05 old loss 6.614921585423872e-05 BETTER
I0312 09:18:28.810841 1890159 finetune.py:68] layer 20_v @ epoch 1 new loss 6.3484067140962e-06 old loss 6.606032911804505e-06 BETTER
I0312 09:18:35.357565 1890043 finetune.py:68] layer 19_down @ epoch 4 new loss 6.614738231291994e-05 old loss 6.614809535676613e-05 BETTER
19_v proxy err 0.0005674234707839787 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.000120970253192354 tr(WHW.T) 6944.6259765625
19_k proxy err 9.341542317997664e-05 tr(WHW.T) 10549.0380859375
19_o proxy err 0.0005735689192079008 tr(WHW.T) 62.31464767456055
19_up proxy err 0.0004219438415020704 tr(WHW.T) 2149.588623046875
19_gate proxy err 0.0002798664791043848 tr(WHW.T) 3688.7509765625
19_down proxy err 0.0005052440683357418 tr(WHW.T) 222.96279907226562
I0312 09:18:52.006487 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 21 in 70.77431178092957s
I0312 09:18:55.108409 1890275 config.py:54] PyTorch version 2.1.1 available.
I0312 09:18:56.126161 1886474 quantize_finetune_llama.py:183] layer 22 gpu 2
I0312 09:18:56.204774 1890275 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 09:19:03.603345 1890159 finetune.py:76] layer 20_v @ epoch 2 new loss 6.484437108156271e-06 old loss 6.3484067140962e-06 WORSE
I0312 09:19:04.642785 1890275 finetune.py:45] layer 21_v initial loss 9.801408850762527e-06
I0312 09:19:35.937168 1890275 finetune.py:68] layer 21_v @ epoch 0 new loss 5.9889903241128195e-06 old loss 9.801408850762527e-06 BETTER
I0312 09:19:37.861474 1890159 finetune.py:68] layer 20_v @ epoch 3 new loss 6.233920430531725e-06 old loss 6.3484067140962e-06 BETTER
I0312 09:20:05.292835 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 22 in 68.79237341880798s
I0312 09:20:08.234234 1890275 finetune.py:68] layer 21_v @ epoch 1 new loss 5.722483820136404e-06 old loss 5.9889903241128195e-06 BETTER
I0312 09:20:08.558978 1890391 config.py:54] PyTorch version 2.1.1 available.
I0312 09:20:09.613044 1886474 quantize_finetune_llama.py:183] layer 23 gpu 3
I0312 09:20:09.678253 1890391 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0312 09:20:12.777288 1890159 finetune.py:68] layer 20_v @ epoch 4 new loss 5.910402705922024e-06 old loss 6.233920430531725e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 09:20:17.659192 1890391 finetune.py:45] layer 22_v initial loss 1.1895228453795426e-05
I0312 09:20:22.055327 1890159 finetune.py:45] layer 20_q initial loss 8.94884578883648e-06
I0312 09:20:41.056395 1890275 finetune.py:68] layer 21_v @ epoch 2 new loss 5.70405018152087e-06 old loss 5.722483820136404e-06 BETTER
I0312 09:20:49.763210 1890391 finetune.py:68] layer 22_v @ epoch 0 new loss 7.3764449552982114e-06 old loss 1.1895228453795426e-05 BETTER
I0312 09:20:55.769360 1890159 finetune.py:68] layer 20_q @ epoch 0 new loss 7.350784017035039e-06 old loss 8.94884578883648e-06 BETTER
I0312 09:21:13.695979 1890275 finetune.py:76] layer 21_v @ epoch 3 new loss 5.747391242039157e-06 old loss 5.70405018152087e-06 WORSE
I0312 09:21:21.284378 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 23 in 71.20922827720642s
I0312 09:21:22.121751 1890391 finetune.py:68] layer 22_v @ epoch 1 new loss 7.272017228387995e-06 old loss 7.3764449552982114e-06 BETTER
I0312 09:21:24.678575 1890507 config.py:54] PyTorch version 2.1.1 available.
I0312 09:21:25.751192 1886474 quantize_finetune_llama.py:183] layer 24 gpu 0
I0312 09:21:25.837912 1890507 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 09:21:30.058915 1890159 finetune.py:68] layer 20_q @ epoch 1 new loss 6.990626843617065e-06 old loss 7.350784017035039e-06 BETTER
I0312 09:21:34.028769 1890507 finetune.py:45] layer 23_v initial loss 1.1796150829468388e-05
I0312 09:21:45.978570 1890275 finetune.py:68] layer 21_v @ epoch 4 new loss 5.516500550584169e-06 old loss 5.70405018152087e-06 BETTER
I0312 09:21:56.154681 1890391 finetune.py:76] layer 22_v @ epoch 2 new loss 7.456345429091016e-06 old loss 7.272017228387995e-06 WORSE
I0312 09:21:57.352821 1890275 finetune.py:45] layer 21_q initial loss 7.976244887686335e-06
I0312 09:22:05.455787 1890159 finetune.py:68] layer 20_q @ epoch 2 new loss 6.837467026343802e-06 old loss 6.990626843617065e-06 BETTER
I0312 09:22:06.384202 1890507 finetune.py:68] layer 23_v @ epoch 0 new loss 7.358632501563989e-06 old loss 1.1796150829468388e-05 BETTER
I0312 09:22:28.411576 1890391 finetune.py:68] layer 22_v @ epoch 3 new loss 7.062758868414676e-06 old loss 7.272017228387995e-06 BETTER
I0312 09:22:28.911010 1890275 finetune.py:68] layer 21_q @ epoch 0 new loss 6.726671017531771e-06 old loss 7.976244887686335e-06 BETTER
I0312 09:22:38.407359 1890507 finetune.py:68] layer 23_v @ epoch 1 new loss 7.013028152869083e-06 old loss 7.358632501563989e-06 BETTER
I0312 09:22:40.116166 1890159 finetune.py:68] layer 20_q @ epoch 3 new loss 6.733103873557411e-06 old loss 6.837467026343802e-06 BETTER
I0312 09:23:01.904618 1890275 finetune.py:68] layer 21_q @ epoch 1 new loss 6.410205969586968e-06 old loss 6.726671017531771e-06 BETTER
I0312 09:23:02.275242 1890391 finetune.py:68] layer 22_v @ epoch 4 new loss 6.779275281587616e-06 old loss 7.062758868414676e-06 BETTER
I0312 09:23:10.890582 1890507 finetune.py:76] layer 23_v @ epoch 2 new loss 7.055301466607489e-06 old loss 7.013028152869083e-06 WORSE
I0312 09:23:11.964566 1890391 finetune.py:45] layer 22_q initial loss 1.1232853466935921e-05
I0312 09:23:14.791745 1890159 finetune.py:68] layer 20_q @ epoch 4 new loss 6.63227001496125e-06 old loss 6.733103873557411e-06 BETTER
I0312 09:23:24.761589 1890159 finetune.py:45] layer 20_k initial loss 9.663009223004337e-06
I0312 09:23:34.657379 1890275 finetune.py:68] layer 21_q @ epoch 2 new loss 6.251088052522391e-06 old loss 6.410205969586968e-06 BETTER
I0312 09:23:44.072697 1890507 finetune.py:76] layer 23_v @ epoch 3 new loss 7.031247605482349e-06 old loss 7.013028152869083e-06 WORSE
I0312 09:23:44.881185 1890391 finetune.py:68] layer 22_q @ epoch 0 new loss 8.665057976031676e-06 old loss 1.1232853466935921e-05 BETTER
I0312 09:23:57.956679 1890159 finetune.py:68] layer 20_k @ epoch 0 new loss 8.944890396378469e-06 old loss 9.663009223004337e-06 BETTER
I0312 09:24:07.201620 1890275 finetune.py:68] layer 21_q @ epoch 3 new loss 6.1409800764522515e-06 old loss 6.251088052522391e-06 BETTER
I0312 09:24:15.704747 1890507 finetune.py:68] layer 23_v @ epoch 4 new loss 6.778913757443661e-06 old loss 7.013028152869083e-06 BETTER
I0312 09:24:17.636621 1890391 finetune.py:68] layer 22_q @ epoch 1 new loss 8.254235581262037e-06 old loss 8.665057976031676e-06 BETTER
I0312 09:24:26.632247 1890507 finetune.py:45] layer 23_q initial loss 1.0027867574535776e-05
I0312 09:24:32.313749 1890159 finetune.py:68] layer 20_k @ epoch 1 new loss 8.759715456108097e-06 old loss 8.944890396378469e-06 BETTER
I0312 09:24:39.508791 1890275 finetune.py:68] layer 21_q @ epoch 4 new loss 6.073688837204827e-06 old loss 6.1409800764522515e-06 BETTER
I0312 09:24:49.946141 1890275 finetune.py:45] layer 21_k initial loss 8.280067959276494e-06
I0312 09:24:51.302692 1890391 finetune.py:68] layer 22_q @ epoch 2 new loss 7.989304322109092e-06 old loss 8.254235581262037e-06 BETTER
I0312 09:24:58.203531 1890507 finetune.py:68] layer 23_q @ epoch 0 new loss 8.22812580736354e-06 old loss 1.0027867574535776e-05 BETTER
I0312 09:25:06.084970 1890159 finetune.py:68] layer 20_k @ epoch 2 new loss 8.68462575454032e-06 old loss 8.759715456108097e-06 BETTER
I0312 09:25:22.748697 1890275 finetune.py:68] layer 21_k @ epoch 0 new loss 7.953407475724816e-06 old loss 8.280067959276494e-06 BETTER
I0312 09:25:24.899712 1890391 finetune.py:68] layer 22_q @ epoch 3 new loss 7.867580279707909e-06 old loss 7.989304322109092e-06 BETTER
I0312 09:25:30.624820 1890507 finetune.py:68] layer 23_q @ epoch 1 new loss 7.844177162041888e-06 old loss 8.22812580736354e-06 BETTER
I0312 09:25:39.715695 1890159 finetune.py:68] layer 20_k @ epoch 3 new loss 8.655956662551034e-06 old loss 8.68462575454032e-06 BETTER
I0312 09:25:55.182398 1890275 finetune.py:68] layer 21_k @ epoch 1 new loss 7.862217898946255e-06 old loss 7.953407475724816e-06 BETTER
I0312 09:25:57.455939 1890391 finetune.py:68] layer 22_q @ epoch 4 new loss 7.751920747978147e-06 old loss 7.867580279707909e-06 BETTER
I0312 09:26:05.208147 1890507 finetune.py:68] layer 23_q @ epoch 2 new loss 7.670420927752275e-06 old loss 7.844177162041888e-06 BETTER
I0312 09:26:12.673849 1890391 finetune.py:45] layer 22_k initial loss 1.1506226655910723e-05
I0312 09:26:14.390192 1890159 finetune.py:68] layer 20_k @ epoch 4 new loss 8.640235137136187e-06 old loss 8.655956662551034e-06 BETTER
I0312 09:26:25.379199 1890159 finetune.py:45] layer 20_o initial loss 1.9949577108491212e-05
I0312 09:26:27.703848 1890275 finetune.py:76] layer 21_k @ epoch 2 new loss 7.877265488787089e-06 old loss 7.862217898946255e-06 WORSE
I0312 09:26:37.583490 1890507 finetune.py:68] layer 23_q @ epoch 3 new loss 7.552757779194508e-06 old loss 7.670420927752275e-06 BETTER
I0312 09:26:44.112726 1890391 finetune.py:68] layer 22_k @ epoch 0 new loss 1.0819932867889293e-05 old loss 1.1506226655910723e-05 BETTER
I0312 09:26:59.401579 1890159 finetune.py:68] layer 20_o @ epoch 0 new loss 1.7954795112018473e-05 old loss 1.9949577108491212e-05 BETTER
I0312 09:27:00.759418 1890275 finetune.py:76] layer 21_k @ epoch 3 new loss 7.87786666478496e-06 old loss 7.862217898946255e-06 WORSE
I0312 09:27:10.663119 1890507 finetune.py:68] layer 23_q @ epoch 4 new loss 7.465050657629035e-06 old loss 7.552757779194508e-06 BETTER
I0312 09:27:16.712921 1890391 finetune.py:68] layer 22_k @ epoch 1 new loss 1.0637116247380618e-05 old loss 1.0819932867889293e-05 BETTER
I0312 09:27:20.784827 1890507 finetune.py:45] layer 23_k initial loss 1.0290831596648786e-05
I0312 09:27:32.311171 1890275 finetune.py:68] layer 21_k @ epoch 4 new loss 7.810791430529207e-06 old loss 7.862217898946255e-06 BETTER
I0312 09:27:32.889135 1890159 finetune.py:68] layer 20_o @ epoch 1 new loss 1.7345031665172428e-05 old loss 1.7954795112018473e-05 BETTER
I0312 09:27:42.770603 1890275 finetune.py:45] layer 21_o initial loss 1.69222621480003e-05
I0312 09:27:50.084261 1890391 finetune.py:68] layer 22_k @ epoch 2 new loss 1.0511521395528689e-05 old loss 1.0637116247380618e-05 BETTER
I0312 09:27:52.755718 1890507 finetune.py:68] layer 23_k @ epoch 0 new loss 9.869218047242612e-06 old loss 1.0290831596648786e-05 BETTER
I0312 09:28:06.305411 1890159 finetune.py:68] layer 20_o @ epoch 2 new loss 1.7032694813678972e-05 old loss 1.7345031665172428e-05 BETTER
I0312 09:28:13.482343 1890275 finetune.py:68] layer 21_o @ epoch 0 new loss 1.572721521370113e-05 old loss 1.69222621480003e-05 BETTER
I0312 09:28:22.440176 1890391 finetune.py:68] layer 22_k @ epoch 3 new loss 1.0412362826173194e-05 old loss 1.0511521395528689e-05 BETTER
I0312 09:28:24.499801 1890507 finetune.py:68] layer 23_k @ epoch 1 new loss 9.741090252646245e-06 old loss 9.869218047242612e-06 BETTER
I0312 09:28:39.830907 1890159 finetune.py:68] layer 20_o @ epoch 3 new loss 1.6818896256154403e-05 old loss 1.7032694813678972e-05 BETTER
I0312 09:28:44.967806 1890275 finetune.py:68] layer 21_o @ epoch 1 new loss 1.5437804904649965e-05 old loss 1.572721521370113e-05 BETTER
I0312 09:28:55.201076 1890391 finetune.py:68] layer 22_k @ epoch 4 new loss 1.0371915777795948e-05 old loss 1.0412362826173194e-05 BETTER
I0312 09:28:56.968012 1890507 finetune.py:68] layer 23_k @ epoch 2 new loss 9.650819265516475e-06 old loss 9.741090252646245e-06 BETTER
I0312 09:29:05.608128 1890391 finetune.py:45] layer 22_o initial loss 2.4155620849342085e-05
I0312 09:29:13.472103 1890159 finetune.py:68] layer 20_o @ epoch 4 new loss 1.6695157682988793e-05 old loss 1.6818896256154403e-05 BETTER
I0312 09:29:16.518372 1890275 finetune.py:68] layer 21_o @ epoch 2 new loss 1.5283734683180228e-05 old loss 1.5437804904649965e-05 BETTER
I0312 09:29:28.947329 1890507 finetune.py:68] layer 23_k @ epoch 3 new loss 9.61879050009884e-06 old loss 9.650819265516475e-06 BETTER
I0312 09:29:29.370962 1890159 finetune.py:45] layer 20_up initial loss 3.571946217562072e-05
I0312 09:29:36.323070 1890391 finetune.py:68] layer 22_o @ epoch 0 new loss 2.1092742827022448e-05 old loss 2.4155620849342085e-05 BETTER
I0312 09:29:48.043954 1890275 finetune.py:68] layer 21_o @ epoch 3 new loss 1.5194046682154294e-05 old loss 1.5283734683180228e-05 BETTER
I0312 09:29:59.756430 1890159 finetune.py:68] layer 20_up @ epoch 0 new loss 3.41186678269878e-05 old loss 3.571946217562072e-05 BETTER
I0312 09:30:00.683980 1890507 finetune.py:68] layer 23_k @ epoch 4 new loss 9.59543922363082e-06 old loss 9.61879050009884e-06 BETTER
I0312 09:30:07.805200 1890391 finetune.py:68] layer 22_o @ epoch 1 new loss 2.0513241906883195e-05 old loss 2.1092742827022448e-05 BETTER
I0312 09:30:10.128808 1890507 finetune.py:45] layer 23_o initial loss 2.0050047169206664e-05
I0312 09:30:19.385036 1890275 finetune.py:68] layer 21_o @ epoch 4 new loss 1.5156234439928085e-05 old loss 1.5194046682154294e-05 BETTER
I0312 09:30:31.337775 1890159 finetune.py:68] layer 20_up @ epoch 1 new loss 3.3450633054599166e-05 old loss 3.41186678269878e-05 BETTER
I0312 09:30:34.698293 1890275 finetune.py:45] layer 21_up initial loss 3.586192906368524e-05
I0312 09:30:40.150300 1890391 finetune.py:68] layer 22_o @ epoch 2 new loss 2.018041050177999e-05 old loss 2.0513241906883195e-05 BETTER
I0312 09:30:41.270096 1890507 finetune.py:68] layer 23_o @ epoch 0 new loss 1.8784663552651182e-05 old loss 2.0050047169206664e-05 BETTER
I0312 09:31:02.884500 1890159 finetune.py:68] layer 20_up @ epoch 2 new loss 3.304644633317366e-05 old loss 3.3450633054599166e-05 BETTER
I0312 09:31:03.750996 1890275 finetune.py:68] layer 21_up @ epoch 0 new loss 3.4395277907606214e-05 old loss 3.586192906368524e-05 BETTER
I0312 09:31:12.090406 1890391 finetune.py:68] layer 22_o @ epoch 3 new loss 2.0007151761092246e-05 old loss 2.018041050177999e-05 BETTER
I0312 09:31:12.662413 1890507 finetune.py:68] layer 23_o @ epoch 1 new loss 1.8469489077688195e-05 old loss 1.8784663552651182e-05 BETTER
I0312 09:31:33.364103 1890275 finetune.py:68] layer 21_up @ epoch 1 new loss 3.3808079024311155e-05 old loss 3.4395277907606214e-05 BETTER
I0312 09:31:34.855464 1890159 finetune.py:68] layer 20_up @ epoch 3 new loss 3.2755335269030184e-05 old loss 3.304644633317366e-05 BETTER
I0312 09:31:43.857984 1890507 finetune.py:68] layer 23_o @ epoch 2 new loss 1.8307302525499836e-05 old loss 1.8469489077688195e-05 BETTER
I0312 09:31:43.884052 1890391 finetune.py:68] layer 22_o @ epoch 4 new loss 1.993401201616507e-05 old loss 2.0007151761092246e-05 BETTER
I0312 09:31:59.554218 1890391 finetune.py:45] layer 22_up initial loss 4.359245576779358e-05
I0312 09:32:03.177106 1890275 finetune.py:68] layer 21_up @ epoch 2 new loss 3.3449909096816555e-05 old loss 3.3808079024311155e-05 BETTER
I0312 09:32:06.533882 1890159 finetune.py:68] layer 20_up @ epoch 4 new loss 3.255210322095081e-05 old loss 3.2755335269030184e-05 BETTER
I0312 09:32:14.773741 1890507 finetune.py:68] layer 23_o @ epoch 3 new loss 1.81948889803607e-05 old loss 1.8307302525499836e-05 BETTER
I0312 09:32:22.169896 1890159 finetune.py:45] layer 20_gate initial loss 4.949997310177423e-05
I0312 09:32:28.408234 1890391 finetune.py:68] layer 22_up @ epoch 0 new loss 4.194170469418168e-05 old loss 4.359245576779358e-05 BETTER
I0312 09:32:33.112466 1890275 finetune.py:68] layer 21_up @ epoch 3 new loss 3.320894393255003e-05 old loss 3.3449909096816555e-05 BETTER
I0312 09:32:46.187260 1890507 finetune.py:68] layer 23_o @ epoch 4 new loss 1.8180089682573453e-05 old loss 1.81948889803607e-05 BETTER
I0312 09:32:51.171511 1890159 finetune.py:68] layer 20_gate @ epoch 0 new loss 4.838405948248692e-05 old loss 4.949997310177423e-05 BETTER
I0312 09:32:58.459040 1890391 finetune.py:68] layer 22_up @ epoch 1 new loss 4.126532076043077e-05 old loss 4.194170469418168e-05 BETTER
I0312 09:33:03.120026 1890275 finetune.py:68] layer 21_up @ epoch 4 new loss 3.303361518192105e-05 old loss 3.320894393255003e-05 BETTER
I0312 09:33:04.225936 1890507 finetune.py:45] layer 23_up initial loss 4.408064705785364e-05
I0312 09:33:18.898328 1890275 finetune.py:45] layer 21_gate initial loss 5.133241575094871e-05
I0312 09:33:21.055094 1890159 finetune.py:68] layer 20_gate @ epoch 1 new loss 4.78869114886038e-05 old loss 4.838405948248692e-05 BETTER
I0312 09:33:28.448862 1890391 finetune.py:68] layer 22_up @ epoch 2 new loss 4.085718683199957e-05 old loss 4.126532076043077e-05 BETTER
I0312 09:33:32.777059 1890507 finetune.py:68] layer 23_up @ epoch 0 new loss 4.252471626386978e-05 old loss 4.408064705785364e-05 BETTER
I0312 09:33:46.460203 1890275 finetune.py:68] layer 21_gate @ epoch 0 new loss 5.0373510021017864e-05 old loss 5.133241575094871e-05 BETTER
I0312 09:33:50.870221 1890159 finetune.py:68] layer 20_gate @ epoch 2 new loss 4.755889676744118e-05 old loss 4.78869114886038e-05 BETTER
I0312 09:33:58.399005 1890391 finetune.py:68] layer 22_up @ epoch 3 new loss 4.0578554035164416e-05 old loss 4.085718683199957e-05 BETTER
I0312 09:34:02.253566 1890507 finetune.py:68] layer 23_up @ epoch 1 new loss 4.186494697933085e-05 old loss 4.252471626386978e-05 BETTER
I0312 09:34:14.769449 1890275 finetune.py:68] layer 21_gate @ epoch 1 new loss 4.993856055079959e-05 old loss 5.0373510021017864e-05 BETTER
I0312 09:34:20.819288 1890159 finetune.py:68] layer 20_gate @ epoch 3 new loss 4.733025707537308e-05 old loss 4.755889676744118e-05 BETTER
I0312 09:34:28.704441 1890391 finetune.py:68] layer 22_up @ epoch 4 new loss 4.038835322717205e-05 old loss 4.0578554035164416e-05 BETTER
I0312 09:34:31.915494 1890507 finetune.py:68] layer 23_up @ epoch 2 new loss 4.148982770857401e-05 old loss 4.186494697933085e-05 BETTER
I0312 09:34:43.055084 1890275 finetune.py:68] layer 21_gate @ epoch 2 new loss 4.964633262716234e-05 old loss 4.993856055079959e-05 BETTER
I0312 09:34:44.582763 1890391 finetune.py:45] layer 22_gate initial loss 6.145161751192063e-05
I0312 09:34:50.899403 1890159 finetune.py:68] layer 20_gate @ epoch 4 new loss 4.715660907095298e-05 old loss 4.733025707537308e-05 BETTER
I0312 09:35:01.451012 1890507 finetune.py:68] layer 23_up @ epoch 3 new loss 4.1231964132748544e-05 old loss 4.148982770857401e-05 BETTER
I0312 09:35:07.197532 1890159 finetune.py:45] layer 20_down initial loss 8.079527469817549e-05
I0312 09:35:11.472185 1890275 finetune.py:68] layer 21_gate @ epoch 3 new loss 4.944931060890667e-05 old loss 4.964633262716234e-05 BETTER
I0312 09:35:12.196986 1890391 finetune.py:68] layer 22_gate @ epoch 0 new loss 6.0390411817934364e-05 old loss 6.145161751192063e-05 BETTER
I0312 09:35:31.193024 1890507 finetune.py:68] layer 23_up @ epoch 4 new loss 4.1061404772335663e-05 old loss 4.1231964132748544e-05 BETTER
I0312 09:35:34.595444 1890159 finetune.py:68] layer 20_down @ epoch 0 new loss 8.076889207586646e-05 old loss 8.079527469817549e-05 BETTER
I0312 09:35:39.803191 1890275 finetune.py:68] layer 21_gate @ epoch 4 new loss 4.9305377615382895e-05 old loss 4.944931060890667e-05 BETTER
I0312 09:35:40.658744 1890391 finetune.py:68] layer 22_gate @ epoch 1 new loss 5.9884485381189734e-05 old loss 6.0390411817934364e-05 BETTER
I0312 09:35:46.968532 1890507 finetune.py:45] layer 23_gate initial loss 6.422685692086816e-05
I0312 09:35:55.657336 1890275 finetune.py:45] layer 21_down initial loss 8.355156023753807e-05
I0312 09:36:03.130383 1890159 finetune.py:68] layer 20_down @ epoch 1 new loss 8.076200901996344e-05 old loss 8.076889207586646e-05 BETTER
I0312 09:36:09.611176 1890391 finetune.py:68] layer 22_gate @ epoch 2 new loss 5.9561152738751844e-05 old loss 5.9884485381189734e-05 BETTER
I0312 09:36:14.765034 1890507 finetune.py:68] layer 23_gate @ epoch 0 new loss 6.328034942271188e-05 old loss 6.422685692086816e-05 BETTER
I0312 09:36:22.313280 1890275 finetune.py:68] layer 21_down @ epoch 0 new loss 8.3530081610661e-05 old loss 8.355156023753807e-05 BETTER
I0312 09:36:31.961299 1890159 finetune.py:68] layer 20_down @ epoch 2 new loss 8.075802907114848e-05 old loss 8.076200901996344e-05 BETTER
I0312 09:36:38.285437 1890391 finetune.py:68] layer 22_gate @ epoch 3 new loss 5.932644853601232e-05 old loss 5.9561152738751844e-05 BETTER
I0312 09:36:42.837167 1890507 finetune.py:68] layer 23_gate @ epoch 1 new loss 6.279213994275779e-05 old loss 6.328034942271188e-05 BETTER
I0312 09:36:49.303072 1890275 finetune.py:68] layer 21_down @ epoch 1 new loss 8.352640725206584e-05 old loss 8.3530081610661e-05 BETTER
I0312 09:37:00.505590 1890159 finetune.py:68] layer 20_down @ epoch 3 new loss 8.075503137661144e-05 old loss 8.075802907114848e-05 BETTER
I0312 09:37:06.342683 1890391 finetune.py:68] layer 22_gate @ epoch 4 new loss 5.918428360018879e-05 old loss 5.932644853601232e-05 BETTER
I0312 09:37:10.988696 1890507 finetune.py:68] layer 23_gate @ epoch 2 new loss 6.248665158636868e-05 old loss 6.279213994275779e-05 BETTER
I0312 09:37:16.135533 1890275 finetune.py:68] layer 21_down @ epoch 2 new loss 8.352429722435772e-05 old loss 8.352640725206584e-05 BETTER
I0312 09:37:23.192980 1890391 finetune.py:45] layer 22_down initial loss 9.79401302174665e-05
I0312 09:37:28.958157 1890159 finetune.py:68] layer 20_down @ epoch 4 new loss 8.07529577286914e-05 old loss 8.075503137661144e-05 BETTER
20_v proxy err 0.0005872751935385168 tr(WHW.T) 990.5983276367188
20_q proxy err 0.00012535938003566116 tr(WHW.T) 7151.03125
20_k proxy err 0.00010011395352194086 tr(WHW.T) 10386.4228515625
20_o proxy err 0.0004477175825741142 tr(WHW.T) 100.3355941772461
20_up proxy err 0.00041677290573716164 tr(WHW.T) 2341.335693359375
20_gate proxy err 0.0002797514898702502 tr(WHW.T) 4025.99365234375
20_down proxy err 0.0005015340866521001 tr(WHW.T) 274.9573059082031
I0312 09:37:39.739447 1890507 finetune.py:68] layer 23_gate @ epoch 3 new loss 6.227186531759799e-05 old loss 6.248665158636868e-05 BETTER
I0312 09:37:43.266949 1890275 finetune.py:68] layer 21_down @ epoch 3 new loss 8.352299482794479e-05 old loss 8.352429722435772e-05 BETTER
I0312 09:37:49.685309 1890391 finetune.py:68] layer 22_down @ epoch 0 new loss 9.79150427156128e-05 old loss 9.79401302174665e-05 BETTER
I0312 09:38:08.221581 1890507 finetune.py:68] layer 23_gate @ epoch 4 new loss 6.213744927663356e-05 old loss 6.227186531759799e-05 BETTER
I0312 09:38:10.279585 1890275 finetune.py:68] layer 21_down @ epoch 4 new loss 8.35216196719557e-05 old loss 8.352299482794479e-05 BETTER
21_v proxy err 0.0005710764671675861 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.00013262283755466342 tr(WHW.T) 7064.83642578125
21_k proxy err 0.00010218072566203773 tr(WHW.T) 9975.33203125
21_o proxy err 0.000482338888105005 tr(WHW.T) 75.52051544189453
21_up proxy err 0.0004363150510471314 tr(WHW.T) 2361.4228515625
21_gate proxy err 0.0002969278139062226 tr(WHW.T) 4003.285400390625
21_down proxy err 0.0005050917388871312 tr(WHW.T) 276.6049499511719
I0312 09:38:16.340555 1890391 finetune.py:68] layer 22_down @ epoch 1 new loss 9.79062679107301e-05 old loss 9.79150427156128e-05 BETTER
I0312 09:38:24.828115 1890507 finetune.py:45] layer 23_down initial loss 0.00010211343033006415
I0312 09:38:43.233793 1890391 finetune.py:68] layer 22_down @ epoch 2 new loss 9.790518379304558e-05 old loss 9.79062679107301e-05 BETTER
I0312 09:38:50.056661 1890507 finetune.py:68] layer 23_down @ epoch 0 new loss 0.00010208717139903456 old loss 0.00010211343033006415 BETTER
I0312 09:39:10.063238 1890391 finetune.py:68] layer 22_down @ epoch 3 new loss 9.79009346337989e-05 old loss 9.790518379304558e-05 BETTER
I0312 09:39:16.249102 1890507 finetune.py:68] layer 23_down @ epoch 1 new loss 0.00010208199819317088 old loss 0.00010208717139903456 BETTER
I0312 09:39:26.445182 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 24 in 71.99416589736938s
I0312 09:39:29.660977 1890623 config.py:54] PyTorch version 2.1.1 available.
I0312 09:39:30.661891 1886474 quantize_finetune_llama.py:183] layer 25 gpu 1
I0312 09:39:30.736922 1890623 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 09:39:36.854911 1890391 finetune.py:68] layer 22_down @ epoch 4 new loss 9.789905743673444e-05 old loss 9.79009346337989e-05 BETTER
22_v proxy err 0.000546377501450479 tr(WHW.T) 1243.2529296875
22_q proxy err 0.00012818809773307294 tr(WHW.T) 7746.96875
22_k proxy err 0.0001056827386491932 tr(WHW.T) 10601.61328125
22_o proxy err 0.0004740665899589658 tr(WHW.T) 114.30491638183594
22_up proxy err 0.0004400229372549802 tr(WHW.T) 2474.465576171875
22_gate proxy err 0.00030408549355342984 tr(WHW.T) 4156.1650390625
22_down proxy err 0.0005037628579884768 tr(WHW.T) 311.87286376953125
I0312 09:39:39.125197 1890623 finetune.py:45] layer 24_v initial loss 1.3832072909281123e-05
I0312 09:39:42.382088 1890507 finetune.py:68] layer 23_down @ epoch 2 new loss 0.00010207980085397139 old loss 0.00010208199819317088 BETTER
I0312 09:40:08.695769 1890507 finetune.py:68] layer 23_down @ epoch 3 new loss 0.00010207849845755845 old loss 0.00010207980085397139 BETTER
I0312 09:40:12.022756 1890623 finetune.py:68] layer 24_v @ epoch 0 new loss 8.71263000590261e-06 old loss 1.3832072909281123e-05 BETTER
I0312 09:40:34.943878 1890507 finetune.py:68] layer 23_down @ epoch 4 new loss 0.00010207743616774678 old loss 0.00010207849845755845 BETTER
23_v proxy err 0.0005096119130030274 tr(WHW.T) 1486.037353515625
23_q proxy err 0.00013680499978363514 tr(WHW.T) 7346.67333984375
23_k proxy err 0.00010862524504773319 tr(WHW.T) 9981.921875
23_o proxy err 0.00047978825750760734 tr(WHW.T) 85.12646484375
23_up proxy err 0.0004533530445769429 tr(WHW.T) 2533.587890625
23_gate proxy err 0.00031870781094767153 tr(WHW.T) 4096.98583984375
23_down proxy err 0.0005047699087299407 tr(WHW.T) 321.3697509765625
I0312 09:40:46.254337 1890623 finetune.py:68] layer 24_v @ epoch 1 new loss 8.2598080552998e-06 old loss 8.71263000590261e-06 BETTER
I0312 09:40:50.287738 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 25 in 69.60099506378174s
I0312 09:40:53.302301 1890739 config.py:54] PyTorch version 2.1.1 available.
I0312 09:40:54.318666 1886474 quantize_finetune_llama.py:183] layer 26 gpu 2
I0312 09:40:54.384646 1890739 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 09:41:02.451794 1890739 finetune.py:45] layer 25_v initial loss 1.3240928637969773e-05
I0312 09:41:20.669482 1890623 finetune.py:76] layer 24_v @ epoch 2 new loss 8.432935828750487e-06 old loss 8.2598080552998e-06 WORSE
I0312 09:41:33.528199 1890739 finetune.py:68] layer 25_v @ epoch 0 new loss 7.499725143134128e-06 old loss 1.3240928637969773e-05 BETTER
I0312 09:41:54.635953 1890623 finetune.py:68] layer 24_v @ epoch 3 new loss 8.04579394753091e-06 old loss 8.2598080552998e-06 BETTER
I0312 09:42:03.919009 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 26 in 69.18924951553345s
I0312 09:42:05.524330 1890739 finetune.py:76] layer 25_v @ epoch 1 new loss 7.5289190135663375e-06 old loss 7.499725143134128e-06 WORSE
I0312 09:42:07.196881 1890855 config.py:54] PyTorch version 2.1.1 available.
I0312 09:42:08.220971 1886474 quantize_finetune_llama.py:183] layer 27 gpu 3
I0312 09:42:08.292596 1890855 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 09:42:16.355216 1890855 finetune.py:45] layer 26_v initial loss 1.9202187104383484e-05
I0312 09:42:29.391157 1890623 finetune.py:68] layer 24_v @ epoch 4 new loss 7.824472049833275e-06 old loss 8.04579394753091e-06 BETTER
I0312 09:42:37.176815 1890739 finetune.py:76] layer 25_v @ epoch 2 new loss 7.535906661360059e-06 old loss 7.499725143134128e-06 WORSE
I0312 09:42:38.542202 1890623 finetune.py:45] layer 24_q initial loss 1.1669255400192924e-05
I0312 09:42:47.805979 1890855 finetune.py:68] layer 26_v @ epoch 0 new loss 1.1551272109500133e-05 old loss 1.9202187104383484e-05 BETTER
I0312 09:43:09.223461 1890739 finetune.py:68] layer 25_v @ epoch 3 new loss 7.289975201274501e-06 old loss 7.499725143134128e-06 BETTER
I0312 09:43:11.940790 1890623 finetune.py:68] layer 24_q @ epoch 0 new loss 9.889138709695544e-06 old loss 1.1669255400192924e-05 BETTER
I0312 09:43:17.239649 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 27 in 68.67511081695557s
I0312 09:43:20.219945 1890855 finetune.py:68] layer 26_v @ epoch 1 new loss 1.1065051694458816e-05 old loss 1.1551272109500133e-05 BETTER
I0312 09:43:20.563954 1890971 config.py:54] PyTorch version 2.1.1 available.
I0312 09:43:21.582203 1886474 quantize_finetune_llama.py:183] layer 28 gpu 0
I0312 09:43:21.652566 1890971 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 09:43:29.784638 1890971 finetune.py:45] layer 27_v initial loss 1.5299907317967154e-05
I0312 09:43:41.839603 1890739 finetune.py:68] layer 25_v @ epoch 4 new loss 6.989832399995066e-06 old loss 7.289975201274501e-06 BETTER
I0312 09:43:46.285943 1890623 finetune.py:68] layer 24_q @ epoch 1 new loss 9.530688657832798e-06 old loss 9.889138709695544e-06 BETTER
I0312 09:43:51.174768 1890739 finetune.py:45] layer 25_q initial loss 1.0558042959019076e-05
I0312 09:43:52.579308 1890855 finetune.py:68] layer 26_v @ epoch 2 new loss 1.097803487937199e-05 old loss 1.1065051694458816e-05 BETTER
I0312 09:44:00.529714 1890971 finetune.py:68] layer 27_v @ epoch 0 new loss 9.951710126188118e-06 old loss 1.5299907317967154e-05 BETTER
I0312 09:44:20.758539 1890623 finetune.py:68] layer 24_q @ epoch 2 new loss 9.294074516219553e-06 old loss 9.530688657832798e-06 BETTER
I0312 09:44:23.416248 1890739 finetune.py:68] layer 25_q @ epoch 0 new loss 8.920368600229267e-06 old loss 1.0558042959019076e-05 BETTER
I0312 09:44:26.309555 1890855 finetune.py:68] layer 26_v @ epoch 3 new loss 1.0919249689322896e-05 old loss 1.097803487937199e-05 BETTER
I0312 09:44:32.489058 1890971 finetune.py:68] layer 27_v @ epoch 1 new loss 9.832318937696982e-06 old loss 9.951710126188118e-06 BETTER
I0312 09:44:55.616911 1890623 finetune.py:68] layer 24_q @ epoch 3 new loss 9.15252894628793e-06 old loss 9.294074516219553e-06 BETTER
I0312 09:44:56.019123 1890739 finetune.py:68] layer 25_q @ epoch 1 new loss 8.584215720475186e-06 old loss 8.920368600229267e-06 BETTER
I0312 09:44:59.259727 1890855 finetune.py:68] layer 26_v @ epoch 4 new loss 1.079056255548494e-05 old loss 1.0919249689322896e-05 BETTER
I0312 09:45:04.523662 1890971 finetune.py:76] layer 27_v @ epoch 2 new loss 1.011430322250817e-05 old loss 9.832318937696982e-06 WORSE
I0312 09:45:08.696803 1890855 finetune.py:45] layer 26_q initial loss 1.6477963072247803e-05
I0312 09:45:29.615785 1890739 finetune.py:68] layer 25_q @ epoch 2 new loss 8.295921361423098e-06 old loss 8.584215720475186e-06 BETTER
I0312 09:45:31.350312 1890623 finetune.py:68] layer 24_q @ epoch 4 new loss 9.078703442355618e-06 old loss 9.15252894628793e-06 BETTER
I0312 09:45:36.829968 1890971 finetune.py:76] layer 27_v @ epoch 3 new loss 1.0094750905409455e-05 old loss 9.832318937696982e-06 WORSE
I0312 09:45:41.146186 1890855 finetune.py:68] layer 26_q @ epoch 0 new loss 1.321245053986786e-05 old loss 1.6477963072247803e-05 BETTER
I0312 09:45:43.067871 1890623 finetune.py:45] layer 24_k initial loss 1.2498166142904665e-05
I0312 09:46:02.059170 1890739 finetune.py:68] layer 25_q @ epoch 3 new loss 8.236052053689491e-06 old loss 8.295921361423098e-06 BETTER
I0312 09:46:08.389812 1890971 finetune.py:68] layer 27_v @ epoch 4 new loss 9.542167390463874e-06 old loss 9.832318937696982e-06 BETTER
I0312 09:46:13.659482 1890855 finetune.py:68] layer 26_q @ epoch 1 new loss 1.2679891369771212e-05 old loss 1.321245053986786e-05 BETTER
I0312 09:46:15.816660 1890623 finetune.py:68] layer 24_k @ epoch 0 new loss 1.2025351679767482e-05 old loss 1.2498166142904665e-05 BETTER
I0312 09:46:17.967147 1890971 finetune.py:45] layer 27_q initial loss 1.4034248124517035e-05
I0312 09:46:34.249624 1890739 finetune.py:76] layer 25_q @ epoch 4 new loss 8.368259841518011e-06 old loss 8.236052053689491e-06 WORSE
I0312 09:46:42.886337 1890739 finetune.py:45] layer 25_k initial loss 1.1450792044342961e-05
I0312 09:46:46.206600 1890855 finetune.py:68] layer 26_q @ epoch 2 new loss 1.2321328540565446e-05 old loss 1.2679891369771212e-05 BETTER
I0312 09:46:49.377988 1890971 finetune.py:68] layer 27_q @ epoch 0 new loss 1.1683085176628083e-05 old loss 1.4034248124517035e-05 BETTER
I0312 09:46:49.473864 1890623 finetune.py:68] layer 24_k @ epoch 1 new loss 1.187404450320173e-05 old loss 1.2025351679767482e-05 BETTER
I0312 09:47:14.297085 1890739 finetune.py:68] layer 25_k @ epoch 0 new loss 1.1034871931769885e-05 old loss 1.1450792044342961e-05 BETTER
I0312 09:47:18.799867 1890855 finetune.py:68] layer 26_q @ epoch 3 new loss 1.2077643077645916e-05 old loss 1.2321328540565446e-05 BETTER
I0312 09:47:21.426601 1890971 finetune.py:68] layer 27_q @ epoch 1 new loss 1.1252891454205383e-05 old loss 1.1683085176628083e-05 BETTER
I0312 09:47:23.301529 1890623 finetune.py:68] layer 24_k @ epoch 2 new loss 1.17631716420874e-05 old loss 1.187404450320173e-05 BETTER
I0312 09:47:46.428666 1890739 finetune.py:68] layer 25_k @ epoch 1 new loss 1.0944580935756676e-05 old loss 1.1034871931769885e-05 BETTER
I0312 09:47:51.341172 1890855 finetune.py:68] layer 26_q @ epoch 4 new loss 1.1945973710680846e-05 old loss 1.2077643077645916e-05 BETTER
I0312 09:47:53.560336 1890971 finetune.py:68] layer 27_q @ epoch 2 new loss 1.102685109799495e-05 old loss 1.1252891454205383e-05 BETTER
I0312 09:47:57.020558 1890623 finetune.py:68] layer 24_k @ epoch 3 new loss 1.1698182788677514e-05 old loss 1.17631716420874e-05 BETTER
I0312 09:48:01.006152 1890855 finetune.py:45] layer 26_k initial loss 1.8681184883462265e-05
I0312 09:48:18.478357 1890739 finetune.py:68] layer 25_k @ epoch 2 new loss 1.0926736649707891e-05 old loss 1.0944580935756676e-05 BETTER
I0312 09:48:25.704181 1890971 finetune.py:68] layer 27_q @ epoch 3 new loss 1.0621874025673606e-05 old loss 1.102685109799495e-05 BETTER
I0312 09:48:30.787455 1890623 finetune.py:68] layer 24_k @ epoch 4 new loss 1.1669973901007324e-05 old loss 1.1698182788677514e-05 BETTER
I0312 09:48:32.437664 1890855 finetune.py:68] layer 26_k @ epoch 0 new loss 1.7367590771755204e-05 old loss 1.8681184883462265e-05 BETTER
I0312 09:48:40.195731 1890623 finetune.py:45] layer 24_o initial loss 2.4761046006460674e-05
I0312 09:48:50.449636 1890739 finetune.py:68] layer 25_k @ epoch 3 new loss 1.078645345842233e-05 old loss 1.0926736649707891e-05 BETTER
I0312 09:48:57.821679 1890971 finetune.py:68] layer 27_q @ epoch 4 new loss 1.0475950148247648e-05 old loss 1.0621874025673606e-05 BETTER
I0312 09:49:04.496611 1890855 finetune.py:68] layer 26_k @ epoch 1 new loss 1.7027177818818018e-05 old loss 1.7367590771755204e-05 BETTER
I0312 09:49:07.057483 1890971 finetune.py:45] layer 27_k initial loss 1.4912743608874734e-05
I0312 09:49:12.726634 1890623 finetune.py:68] layer 24_o @ epoch 0 new loss 2.296509592270013e-05 old loss 2.4761046006460674e-05 BETTER
I0312 09:49:22.438868 1890739 finetune.py:68] layer 25_k @ epoch 4 new loss 1.077475280908402e-05 old loss 1.078645345842233e-05 BETTER
I0312 09:49:31.633932 1890739 finetune.py:45] layer 25_o initial loss 2.067673995043151e-05
I0312 09:49:36.618537 1890855 finetune.py:68] layer 26_k @ epoch 2 new loss 1.6866762962308712e-05 old loss 1.7027177818818018e-05 BETTER
I0312 09:49:37.846865 1890971 finetune.py:68] layer 27_k @ epoch 0 new loss 1.4244908925320487e-05 old loss 1.4912743608874734e-05 BETTER
I0312 09:49:45.638185 1890623 finetune.py:68] layer 24_o @ epoch 1 new loss 2.2642587282462046e-05 old loss 2.296509592270013e-05 BETTER
I0312 09:50:02.167182 1890739 finetune.py:68] layer 25_o @ epoch 0 new loss 1.9842143956338987e-05 old loss 2.067673995043151e-05 BETTER
I0312 09:50:08.809784 1890855 finetune.py:68] layer 26_k @ epoch 3 new loss 1.6722204236430116e-05 old loss 1.6866762962308712e-05 BETTER
I0312 09:50:09.351049 1890971 finetune.py:68] layer 27_k @ epoch 1 new loss 1.3975828551338054e-05 old loss 1.4244908925320487e-05 BETTER
I0312 09:50:18.790050 1890623 finetune.py:68] layer 24_o @ epoch 2 new loss 2.2531268768943846e-05 old loss 2.2642587282462046e-05 BETTER
I0312 09:50:33.536038 1890739 finetune.py:68] layer 25_o @ epoch 1 new loss 1.959495057235472e-05 old loss 1.9842143956338987e-05 BETTER
I0312 09:50:40.840425 1890855 finetune.py:68] layer 26_k @ epoch 4 new loss 1.66644331329735e-05 old loss 1.6722204236430116e-05 BETTER
I0312 09:50:41.068696 1890971 finetune.py:68] layer 27_k @ epoch 2 new loss 1.3899613804824185e-05 old loss 1.3975828551338054e-05 BETTER
I0312 09:50:49.976522 1890855 finetune.py:45] layer 26_o initial loss 3.404731614864431e-05
I0312 09:50:52.119058 1890623 finetune.py:68] layer 24_o @ epoch 3 new loss 2.246049916720949e-05 old loss 2.2531268768943846e-05 BETTER
I0312 09:51:04.969478 1890739 finetune.py:68] layer 25_o @ epoch 2 new loss 1.948019962583203e-05 old loss 1.959495057235472e-05 BETTER
I0312 09:51:12.702579 1890971 finetune.py:68] layer 27_k @ epoch 3 new loss 1.3840084648109041e-05 old loss 1.3899613804824185e-05 BETTER
I0312 09:51:20.807637 1890855 finetune.py:68] layer 26_o @ epoch 0 new loss 3.0458115361398086e-05 old loss 3.404731614864431e-05 BETTER
I0312 09:51:25.462321 1890623 finetune.py:68] layer 24_o @ epoch 4 new loss 2.2383290342986584e-05 old loss 2.246049916720949e-05 BETTER
I0312 09:51:36.579808 1890739 finetune.py:68] layer 25_o @ epoch 3 new loss 1.9442091797827743e-05 old loss 1.948019962583203e-05 BETTER
I0312 09:51:40.745149 1890623 finetune.py:45] layer 24_up initial loss 5.0331316742813215e-05
I0312 09:51:44.492751 1890971 finetune.py:68] layer 27_k @ epoch 4 new loss 1.3682903045264538e-05 old loss 1.3840084648109041e-05 BETTER
I0312 09:51:52.558539 1890855 finetune.py:68] layer 26_o @ epoch 1 new loss 2.9928651201771572e-05 old loss 3.0458115361398086e-05 BETTER
I0312 09:51:53.425139 1890971 finetune.py:45] layer 27_o initial loss 2.7695919925463386e-05
I0312 09:52:08.191077 1890739 finetune.py:68] layer 25_o @ epoch 4 new loss 1.9419849195401184e-05 old loss 1.9442091797827743e-05 BETTER
I0312 09:52:11.440094 1890623 finetune.py:68] layer 24_up @ epoch 0 new loss 4.880479536950588e-05 old loss 5.0331316742813215e-05 BETTER
I0312 09:52:26.149187 1890739 finetune.py:45] layer 25_up initial loss 5.0842409109463915e-05
I0312 09:52:26.968935 1890971 finetune.py:68] layer 27_o @ epoch 0 new loss 2.5588737116777338e-05 old loss 2.7695919925463386e-05 BETTER
I0312 09:52:27.097565 1890855 finetune.py:68] layer 26_o @ epoch 2 new loss 2.9715607524849474e-05 old loss 2.9928651201771572e-05 BETTER
I0312 09:52:43.848595 1890623 finetune.py:68] layer 24_up @ epoch 1 new loss 4.819923924515024e-05 old loss 4.880479536950588e-05 BETTER
I0312 09:52:55.386300 1890739 finetune.py:68] layer 25_up @ epoch 0 new loss 4.904327943222597e-05 old loss 5.0842409109463915e-05 BETTER
I0312 09:52:58.386316 1890971 finetune.py:68] layer 27_o @ epoch 1 new loss 2.5121467842836864e-05 old loss 2.5588737116777338e-05 BETTER
I0312 09:52:59.098391 1890855 finetune.py:68] layer 26_o @ epoch 3 new loss 2.956628486572299e-05 old loss 2.9715607524849474e-05 BETTER
I0312 09:53:15.835229 1890623 finetune.py:68] layer 24_up @ epoch 2 new loss 4.7837718739174306e-05 old loss 4.819923924515024e-05 BETTER
I0312 09:53:25.366377 1890739 finetune.py:68] layer 25_up @ epoch 1 new loss 4.840228575631045e-05 old loss 4.904327943222597e-05 BETTER
I0312 09:53:29.506185 1890971 finetune.py:68] layer 27_o @ epoch 2 new loss 2.4974449843284674e-05 old loss 2.5121467842836864e-05 BETTER
I0312 09:53:30.908368 1890855 finetune.py:68] layer 26_o @ epoch 4 new loss 2.9414917662506923e-05 old loss 2.956628486572299e-05 BETTER
I0312 09:53:46.140704 1890855 finetune.py:45] layer 26_up initial loss 6.382195715559646e-05
I0312 09:53:47.784183 1890623 finetune.py:68] layer 24_up @ epoch 3 new loss 4.7602399718016386e-05 old loss 4.7837718739174306e-05 BETTER
I0312 09:53:55.336632 1890739 finetune.py:68] layer 25_up @ epoch 2 new loss 4.800611350219697e-05 old loss 4.840228575631045e-05 BETTER
I0312 09:54:00.531368 1890971 finetune.py:68] layer 27_o @ epoch 3 new loss 2.488764039298985e-05 old loss 2.4974449843284674e-05 BETTER
I0312 09:54:15.176836 1890855 finetune.py:68] layer 26_up @ epoch 0 new loss 6.18927733739838e-05 old loss 6.382195715559646e-05 BETTER
I0312 09:54:19.754588 1890623 finetune.py:68] layer 24_up @ epoch 4 new loss 4.7444798838114366e-05 old loss 4.7602399718016386e-05 BETTER
I0312 09:54:25.087732 1890739 finetune.py:68] layer 25_up @ epoch 3 new loss 4.77627690997906e-05 old loss 4.800611350219697e-05 BETTER
I0312 09:54:31.560997 1890971 finetune.py:68] layer 27_o @ epoch 4 new loss 2.4833174393279478e-05 old loss 2.488764039298985e-05 BETTER
I0312 09:54:35.326813 1890623 finetune.py:45] layer 24_gate initial loss 7.313303649425507e-05
I0312 09:54:45.161969 1890855 finetune.py:68] layer 26_up @ epoch 1 new loss 6.115069845691323e-05 old loss 6.18927733739838e-05 BETTER
I0312 09:54:46.892796 1890971 finetune.py:45] layer 27_up initial loss 6.45189720671624e-05
I0312 09:54:54.968096 1890739 finetune.py:68] layer 25_up @ epoch 4 new loss 4.759309013024904e-05 old loss 4.77627690997906e-05 BETTER
I0312 09:55:04.480580 1890623 finetune.py:68] layer 24_gate @ epoch 0 new loss 7.222248677862808e-05 old loss 7.313303649425507e-05 BETTER
I0312 09:55:10.289052 1890739 finetune.py:45] layer 25_gate initial loss 7.623811688972637e-05
I0312 09:55:15.137329 1890855 finetune.py:68] layer 26_up @ epoch 2 new loss 6.071644020266831e-05 old loss 6.115069845691323e-05 BETTER
I0312 09:55:15.494437 1890971 finetune.py:68] layer 27_up @ epoch 0 new loss 6.180158379720524e-05 old loss 6.45189720671624e-05 BETTER
I0312 09:55:34.474008 1890623 finetune.py:68] layer 24_gate @ epoch 1 new loss 7.172957703005522e-05 old loss 7.222248677862808e-05 BETTER
I0312 09:55:37.943901 1890739 finetune.py:68] layer 25_gate @ epoch 0 new loss 7.523177919210866e-05 old loss 7.623811688972637e-05 BETTER
I0312 09:55:45.198362 1890971 finetune.py:68] layer 27_up @ epoch 1 new loss 6.0773112636525184e-05 old loss 6.180158379720524e-05 BETTER
I0312 09:55:45.213776 1890855 finetune.py:68] layer 26_up @ epoch 3 new loss 6.043318353476934e-05 old loss 6.071644020266831e-05 BETTER
I0312 09:56:04.504470 1890623 finetune.py:68] layer 24_gate @ epoch 2 new loss 7.142548565752804e-05 old loss 7.172957703005522e-05 BETTER
I0312 09:56:06.136434 1890739 finetune.py:68] layer 25_gate @ epoch 1 new loss 7.473222649423406e-05 old loss 7.523177919210866e-05 BETTER
I0312 09:56:14.769221 1890971 finetune.py:68] layer 27_up @ epoch 2 new loss 6.022082015988417e-05 old loss 6.0773112636525184e-05 BETTER
I0312 09:56:15.378047 1890855 finetune.py:68] layer 26_up @ epoch 4 new loss 6.024254616932012e-05 old loss 6.043318353476934e-05 BETTER
I0312 09:56:30.602911 1890855 finetune.py:45] layer 26_gate initial loss 9.304064587922767e-05
I0312 09:56:34.266760 1890739 finetune.py:68] layer 25_gate @ epoch 2 new loss 7.441708294209093e-05 old loss 7.473222649423406e-05 BETTER
I0312 09:56:34.675202 1890623 finetune.py:68] layer 24_gate @ epoch 3 new loss 7.123703107936308e-05 old loss 7.142548565752804e-05 BETTER
I0312 09:56:44.340464 1890971 finetune.py:68] layer 27_up @ epoch 3 new loss 5.986191172269173e-05 old loss 6.022082015988417e-05 BETTER
I0312 09:56:58.163680 1890855 finetune.py:68] layer 26_gate @ epoch 0 new loss 9.180730558000505e-05 old loss 9.304064587922767e-05 BETTER
I0312 09:57:02.223811 1890739 finetune.py:68] layer 25_gate @ epoch 3 new loss 7.421003101626411e-05 old loss 7.441708294209093e-05 BETTER
I0312 09:57:04.686585 1890623 finetune.py:68] layer 24_gate @ epoch 4 new loss 7.108615682227537e-05 old loss 7.123703107936308e-05 BETTER
I0312 09:57:13.813759 1890971 finetune.py:68] layer 27_up @ epoch 4 new loss 5.963706280454062e-05 old loss 5.986191172269173e-05 BETTER
I0312 09:57:20.614318 1890623 finetune.py:45] layer 24_down initial loss 0.00011353782610967755
I0312 09:57:26.316861 1890855 finetune.py:68] layer 26_gate @ epoch 1 new loss 9.119590686168522e-05 old loss 9.180730558000505e-05 BETTER
I0312 09:57:28.799977 1890971 finetune.py:45] layer 27_gate initial loss 9.764888818608597e-05
I0312 09:57:30.371883 1890739 finetune.py:68] layer 25_gate @ epoch 4 new loss 7.406745862681419e-05 old loss 7.421003101626411e-05 BETTER
I0312 09:57:45.903180 1890739 finetune.py:45] layer 25_down initial loss 0.00011937046656385064
I0312 09:57:48.111550 1890623 finetune.py:68] layer 24_down @ epoch 0 new loss 0.00011351037392159924 old loss 0.00011353782610967755 BETTER
I0312 09:57:54.426028 1890855 finetune.py:68] layer 26_gate @ epoch 2 new loss 9.08264410099946e-05 old loss 9.119590686168522e-05 BETTER
I0312 09:57:56.062410 1890971 finetune.py:68] layer 27_gate @ epoch 0 new loss 9.597185271559283e-05 old loss 9.764888818608597e-05 BETTER
I0312 09:58:13.036757 1890739 finetune.py:68] layer 25_down @ epoch 0 new loss 0.00011933283531107008 old loss 0.00011937046656385064 BETTER
I0312 09:58:16.840668 1890623 finetune.py:68] layer 24_down @ epoch 1 new loss 0.00011350403656251729 old loss 0.00011351037392159924 BETTER
I0312 09:58:22.841550 1890855 finetune.py:68] layer 26_gate @ epoch 3 new loss 9.057005809154361e-05 old loss 9.08264410099946e-05 BETTER
I0312 09:58:24.308660 1890971 finetune.py:68] layer 27_gate @ epoch 1 new loss 9.522026812192053e-05 old loss 9.597185271559283e-05 BETTER
I0312 09:58:40.089414 1890739 finetune.py:68] layer 25_down @ epoch 1 new loss 0.000119324991828762 old loss 0.00011933283531107008 BETTER
I0312 09:58:45.455139 1890623 finetune.py:68] layer 24_down @ epoch 2 new loss 0.00011350122804287821 old loss 0.00011350403656251729 BETTER
I0312 09:58:51.049477 1890855 finetune.py:68] layer 26_gate @ epoch 4 new loss 9.040684381034225e-05 old loss 9.057005809154361e-05 BETTER
I0312 09:58:52.716604 1890971 finetune.py:68] layer 27_gate @ epoch 2 new loss 9.47662556427531e-05 old loss 9.522026812192053e-05 BETTER
I0312 09:59:07.074477 1890739 finetune.py:68] layer 25_down @ epoch 2 new loss 0.00011932091001654044 old loss 0.000119324991828762 BETTER
I0312 09:59:07.183677 1890855 finetune.py:45] layer 26_down initial loss 0.00014010144514031708
I0312 09:59:14.094488 1890623 finetune.py:68] layer 24_down @ epoch 3 new loss 0.00011349931446602568 old loss 0.00011350122804287821 BETTER
I0312 09:59:20.971076 1890971 finetune.py:68] layer 27_gate @ epoch 3 new loss 9.444401803193614e-05 old loss 9.47662556427531e-05 BETTER
I0312 09:59:33.631047 1890855 finetune.py:68] layer 26_down @ epoch 0 new loss 0.00014005799312144518 old loss 0.00014010144514031708 BETTER
I0312 09:59:34.211780 1890739 finetune.py:68] layer 25_down @ epoch 3 new loss 0.00011931808694498613 old loss 0.00011932091001654044 BETTER
I0312 09:59:42.559176 1890623 finetune.py:68] layer 24_down @ epoch 4 new loss 0.00011349826672812924 old loss 0.00011349931446602568 BETTER
24_v proxy err 0.0005341796204447746 tr(WHW.T) 1394.900634765625
24_q proxy err 0.00014343061775434762 tr(WHW.T) 7020.349609375
24_k proxy err 0.00010900959750870243 tr(WHW.T) 10324.560546875
24_o proxy err 0.0003829298948403448 tr(WHW.T) 134.0005645751953
24_up proxy err 0.00045931831118650734 tr(WHW.T) 2621.885986328125
24_gate proxy err 0.00032187593751586974 tr(WHW.T) 4262.68603515625
24_down proxy err 0.0005038054659962654 tr(WHW.T) 340.2633361816406
I0312 09:59:49.556678 1890971 finetune.py:68] layer 27_gate @ epoch 4 new loss 9.425038297194988e-05 old loss 9.444401803193614e-05 BETTER
I0312 10:00:00.510093 1890855 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0001400438486598432 old loss 0.00014005799312144518 BETTER
I0312 10:00:01.345102 1890739 finetune.py:68] layer 25_down @ epoch 4 new loss 0.00011931536573683843 old loss 0.00011931808694498613 BETTER
25_v proxy err 0.0004981181118637323 tr(WHW.T) 1707.664794921875
25_q proxy err 0.00015185947995632887 tr(WHW.T) 7161.939453125
25_k proxy err 0.00012257687922101468 tr(WHW.T) 9612.09765625
25_o proxy err 0.00045775421313010156 tr(WHW.T) 83.54535675048828
25_up proxy err 0.00045566351036541164 tr(WHW.T) 2805.825439453125
25_gate proxy err 0.000312014453811571 tr(WHW.T) 4666.3173828125
25_down proxy err 0.000492237857542932 tr(WHW.T) 373.4819641113281
I0312 10:00:05.281109 1890971 finetune.py:45] layer 27_down initial loss 0.00015187871758826077
I0312 10:00:27.572187 1890855 finetune.py:68] layer 26_down @ epoch 2 new loss 0.00014003941032569855 old loss 0.0001400438486598432 BETTER
I0312 10:00:30.679545 1890971 finetune.py:68] layer 27_down @ epoch 0 new loss 0.00015183469804469496 old loss 0.00015187871758826077 BETTER
I0312 10:00:54.353257 1890855 finetune.py:68] layer 26_down @ epoch 3 new loss 0.00014003516116645187 old loss 0.00014003941032569855 BETTER
I0312 10:00:56.899563 1890971 finetune.py:68] layer 27_down @ epoch 1 new loss 0.00015182046627160162 old loss 0.00015183469804469496 BETTER
I0312 10:01:18.327488 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 28 in 72.73000693321228s
I0312 10:01:21.304809 1890855 finetune.py:68] layer 26_down @ epoch 4 new loss 0.000140030198963359 old loss 0.00014003516116645187 BETTER
I0312 10:01:21.783226 1891087 config.py:54] PyTorch version 2.1.1 available.
I0312 10:01:22.869991 1886474 quantize_finetune_llama.py:183] layer 29 gpu 1
I0312 10:01:22.940406 1891087 data_utils.py:336] using 256 training seqs, 128 validation seqs
26_v proxy err 0.0004922305233776569 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.00015033542877063155 tr(WHW.T) 7469.763671875
26_k proxy err 0.00012140082981204614 tr(WHW.T) 10488.087890625
26_o proxy err 0.0003359144611749798 tr(WHW.T) 202.86956787109375
26_up proxy err 0.0004305242619011551 tr(WHW.T) 3154.89794921875
26_gate proxy err 0.0002955187519546598 tr(WHW.T) 5304.38134765625
26_down proxy err 0.0005024491110816598 tr(WHW.T) 401.3248596191406
I0312 10:01:22.989292 1890971 finetune.py:68] layer 27_down @ epoch 2 new loss 0.00015181377239059657 old loss 0.00015182046627160162 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:01:31.586808 1891087 finetune.py:45] layer 28_v initial loss 2.1236477550701238e-05
I0312 10:01:49.385042 1890971 finetune.py:68] layer 27_down @ epoch 3 new loss 0.00015180930495262146 old loss 0.00015181377239059657 BETTER
I0312 10:02:04.409832 1891087 finetune.py:68] layer 28_v @ epoch 0 new loss 1.3183619557821658e-05 old loss 2.1236477550701238e-05 BETTER
I0312 10:02:15.640267 1890971 finetune.py:68] layer 27_down @ epoch 4 new loss 0.00015180571062956005 old loss 0.00015180930495262146 BETTER
27_v proxy err 0.00046605643001385033 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.0001452983997296542 tr(WHW.T) 7691.28759765625
27_k proxy err 0.00011365388490958139 tr(WHW.T) 10619.1123046875
27_o proxy err 0.00041730841621756554 tr(WHW.T) 126.13755798339844
27_up proxy err 0.0003977129817940295 tr(WHW.T) 3692.0126953125
27_gate proxy err 0.00028041930636391044 tr(WHW.T) 5991.720703125
27_down proxy err 0.0005024021957069635 tr(WHW.T) 467.1007080078125
I0312 10:02:35.421225 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 29 in 70.61978578567505s
I0312 10:02:38.698083 1891087 finetune.py:68] layer 28_v @ epoch 1 new loss 1.2698492355411872e-05 old loss 1.3183619557821658e-05 BETTER
I0312 10:02:38.724064 1891203 config.py:54] PyTorch version 2.1.1 available.
I0312 10:02:39.727446 1886474 quantize_finetune_llama.py:183] layer 30 gpu 2
I0312 10:02:39.792438 1891203 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:02:48.211755 1891203 finetune.py:45] layer 29_v initial loss 2.1284942704369314e-05
I0312 10:03:13.209921 1891087 finetune.py:68] layer 28_v @ epoch 2 new loss 1.2641844477911945e-05 old loss 1.2698492355411872e-05 BETTER
I0312 10:03:19.396373 1891203 finetune.py:68] layer 29_v @ epoch 0 new loss 1.4621694390370976e-05 old loss 2.1284942704369314e-05 BETTER
I0312 10:03:47.915214 1891087 finetune.py:68] layer 28_v @ epoch 3 new loss 1.2491444067563862e-05 old loss 1.2641844477911945e-05 BETTER
I0312 10:03:49.320581 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 30 in 69.19093942642212s
I0312 10:03:51.452889 1891203 finetune.py:68] layer 29_v @ epoch 1 new loss 1.4316356100607663e-05 old loss 1.4621694390370976e-05 BETTER
I0312 10:03:52.621864 1891319 config.py:54] PyTorch version 2.1.1 available.
I0312 10:03:53.625885 1886474 quantize_finetune_llama.py:183] layer 31 gpu 3
I0312 10:03:53.689585 1891319 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:04:01.743675 1891319 finetune.py:45] layer 30_v initial loss 2.2434662241721526e-05
I0312 10:04:22.646605 1891087 finetune.py:68] layer 28_v @ epoch 4 new loss 1.177416470454773e-05 old loss 1.2491444067563862e-05 BETTER
I0312 10:04:23.701617 1891203 finetune.py:76] layer 29_v @ epoch 2 new loss 1.4829439351160545e-05 old loss 1.4316356100607663e-05 WORSE
I0312 10:04:32.046601 1891087 finetune.py:45] layer 28_q initial loss 1.8194106814917177e-05
I0312 10:04:33.302086 1891319 finetune.py:68] layer 30_v @ epoch 0 new loss 1.5198757864709478e-05 old loss 2.2434662241721526e-05 BETTER
I0312 10:04:55.733618 1891203 finetune.py:76] layer 29_v @ epoch 3 new loss 1.5436973626492545e-05 old loss 1.4316356100607663e-05 WORSE
I0312 10:05:04.457439 1886474 quantize_finetune_llama.py:210] computed original embedding for layer 31 in 70.41857624053955s
I0312 10:05:05.522458 1891087 finetune.py:68] layer 28_q @ epoch 0 new loss 1.5286073903553188e-05 old loss 1.8194106814917177e-05 BETTER
I0312 10:05:05.732488 1891319 finetune.py:68] layer 30_v @ epoch 1 new loss 1.4991492207627743e-05 old loss 1.5198757864709478e-05 BETTER
I0312 10:05:07.794587 1891435 config.py:54] PyTorch version 2.1.1 available.
I0312 10:05:08.933451 1891435 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0312 10:05:17.003243 1891435 finetune.py:45] layer 31_v initial loss 4.52036147180479e-05
I0312 10:05:28.144735 1891203 finetune.py:76] layer 29_v @ epoch 4 new loss 1.4693388948217034e-05 old loss 1.4316356100607663e-05 WORSE
I0312 10:05:36.967427 1891203 finetune.py:45] layer 29_q initial loss 2.029867573583033e-05
I0312 10:05:38.396424 1891319 finetune.py:76] layer 30_v @ epoch 2 new loss 1.7400383512722328e-05 old loss 1.4991492207627743e-05 WORSE
I0312 10:05:40.087310 1891087 finetune.py:68] layer 28_q @ epoch 1 new loss 1.4528483916365076e-05 old loss 1.5286073903553188e-05 BETTER
I0312 10:05:48.046744 1891435 finetune.py:68] layer 31_v @ epoch 0 new loss 3.1286399462260306e-05 old loss 4.52036147180479e-05 BETTER
I0312 10:06:08.555022 1891203 finetune.py:68] layer 29_q @ epoch 0 new loss 1.7503833078080788e-05 old loss 2.029867573583033e-05 BETTER
I0312 10:06:10.664904 1891319 finetune.py:68] layer 30_v @ epoch 3 new loss 1.4893058505549561e-05 old loss 1.4991492207627743e-05 BETTER
I0312 10:06:14.716306 1891087 finetune.py:68] layer 28_q @ epoch 2 new loss 1.423909816367086e-05 old loss 1.4528483916365076e-05 BETTER
I0312 10:06:20.067885 1891435 finetune.py:76] layer 31_v @ epoch 1 new loss 5.198490180191584e-05 old loss 3.1286399462260306e-05 WORSE
I0312 10:06:41.175693 1891203 finetune.py:68] layer 29_q @ epoch 1 new loss 1.6445208530058153e-05 old loss 1.7503833078080788e-05 BETTER
I0312 10:06:44.145076 1891319 finetune.py:76] layer 30_v @ epoch 4 new loss 1.58903312694747e-05 old loss 1.4893058505549561e-05 WORSE
I0312 10:06:49.487702 1891087 finetune.py:68] layer 28_q @ epoch 3 new loss 1.405297462042654e-05 old loss 1.423909816367086e-05 BETTER
I0312 10:06:52.011449 1891435 finetune.py:76] layer 31_v @ epoch 2 new loss 4.856856321566738e-05 old loss 3.1286399462260306e-05 WORSE
I0312 10:06:53.261026 1891319 finetune.py:45] layer 30_q initial loss 2.1615176592604257e-05
I0312 10:07:13.805384 1891203 finetune.py:68] layer 29_q @ epoch 2 new loss 1.5999406969058327e-05 old loss 1.6445208530058153e-05 BETTER
I0312 10:07:24.062376 1891435 finetune.py:76] layer 31_v @ epoch 3 new loss 5.073397915111855e-05 old loss 3.1286399462260306e-05 WORSE
I0312 10:07:24.522979 1891087 finetune.py:68] layer 28_q @ epoch 4 new loss 1.3742552255280316e-05 old loss 1.405297462042654e-05 BETTER
I0312 10:07:25.332975 1891319 finetune.py:68] layer 30_q @ epoch 0 new loss 1.7678530639386736e-05 old loss 2.1615176592604257e-05 BETTER
I0312 10:07:33.939197 1891087 finetune.py:45] layer 28_k initial loss 1.9297925973660313e-05
I0312 10:07:46.291119 1891203 finetune.py:68] layer 29_q @ epoch 3 new loss 1.578368573973421e-05 old loss 1.5999406969058327e-05 BETTER
I0312 10:07:56.128351 1891435 finetune.py:76] layer 31_v @ epoch 4 new loss 4.2442061385372654e-05 old loss 3.1286399462260306e-05 WORSE
I0312 10:07:57.642954 1891319 finetune.py:68] layer 30_q @ epoch 1 new loss 1.6658786989864893e-05 old loss 1.7678530639386736e-05 BETTER
I0312 10:08:05.011484 1891435 finetune.py:45] layer 31_q initial loss 6.230895814951509e-05
I0312 10:08:07.095184 1891087 finetune.py:68] layer 28_k @ epoch 0 new loss 1.8525199266150594e-05 old loss 1.9297925973660313e-05 BETTER
I0312 10:08:18.551074 1891203 finetune.py:68] layer 29_q @ epoch 4 new loss 1.5580175386276096e-05 old loss 1.578368573973421e-05 BETTER
I0312 10:08:27.668742 1891203 finetune.py:45] layer 29_k initial loss 2.0999474145355634e-05
I0312 10:08:30.123083 1891319 finetune.py:68] layer 30_q @ epoch 2 new loss 1.6076235624495894e-05 old loss 1.6658786989864893e-05 BETTER
I0312 10:08:36.339411 1891435 finetune.py:68] layer 31_q @ epoch 0 new loss 4.6883920731488615e-05 old loss 6.230895814951509e-05 BETTER
I0312 10:08:40.634852 1891087 finetune.py:68] layer 28_k @ epoch 1 new loss 1.8359565729042515e-05 old loss 1.8525199266150594e-05 BETTER
I0312 10:08:59.115377 1891203 finetune.py:68] layer 29_k @ epoch 0 new loss 2.0280689568608068e-05 old loss 2.0999474145355634e-05 BETTER
I0312 10:09:02.663379 1891319 finetune.py:76] layer 30_q @ epoch 3 new loss 1.616674126125872e-05 old loss 1.6076235624495894e-05 WORSE
I0312 10:09:08.459603 1891435 finetune.py:76] layer 31_q @ epoch 1 new loss 4.764418918057345e-05 old loss 4.6883920731488615e-05 WORSE
I0312 10:09:14.474833 1891087 finetune.py:68] layer 28_k @ epoch 2 new loss 1.8183423890150152e-05 old loss 1.8359565729042515e-05 BETTER
I0312 10:09:32.154299 1891203 finetune.py:68] layer 29_k @ epoch 1 new loss 1.9915958546334878e-05 old loss 2.0280689568608068e-05 BETTER
I0312 10:09:35.054160 1891319 finetune.py:68] layer 30_q @ epoch 4 new loss 1.5894323951215483e-05 old loss 1.6076235624495894e-05 BETTER
I0312 10:09:39.858371 1891435 finetune.py:68] layer 31_q @ epoch 2 new loss 4.393835479277186e-05 old loss 4.6883920731488615e-05 BETTER
I0312 10:09:44.656614 1891319 finetune.py:45] layer 30_k initial loss 2.1450414351420477e-05
I0312 10:09:48.771959 1891087 finetune.py:68] layer 28_k @ epoch 3 new loss 1.8182036001235247e-05 old loss 1.8183423890150152e-05 BETTER
I0312 10:10:04.496168 1891203 finetune.py:68] layer 29_k @ epoch 2 new loss 1.981736386369448e-05 old loss 1.9915958546334878e-05 BETTER
I0312 10:10:12.069239 1891435 finetune.py:68] layer 31_q @ epoch 3 new loss 4.215900116832927e-05 old loss 4.393835479277186e-05 BETTER
I0312 10:10:16.280684 1891319 finetune.py:68] layer 30_k @ epoch 0 new loss 2.0697932995972224e-05 old loss 2.1450414351420477e-05 BETTER
I0312 10:10:22.413640 1891087 finetune.py:68] layer 28_k @ epoch 4 new loss 1.816053372749593e-05 old loss 1.8182036001235247e-05 BETTER
I0312 10:10:31.857076 1891087 finetune.py:45] layer 28_o initial loss 3.579695112421177e-05
I0312 10:10:36.529286 1891203 finetune.py:68] layer 29_k @ epoch 3 new loss 1.9729586711036973e-05 old loss 1.981736386369448e-05 BETTER
I0312 10:10:44.145397 1891435 finetune.py:76] layer 31_q @ epoch 4 new loss 4.555379200610332e-05 old loss 4.215900116832927e-05 WORSE
I0312 10:10:48.250626 1891319 finetune.py:68] layer 30_k @ epoch 1 new loss 2.0223957108100876e-05 old loss 2.0697932995972224e-05 BETTER
I0312 10:10:52.715290 1891435 finetune.py:45] layer 31_k initial loss 6.672612653346732e-05
I0312 10:11:04.291220 1891087 finetune.py:68] layer 28_o @ epoch 0 new loss 3.328186721773818e-05 old loss 3.579695112421177e-05 BETTER
I0312 10:11:08.580030 1891203 finetune.py:76] layer 29_k @ epoch 4 new loss 1.986946153920144e-05 old loss 1.9729586711036973e-05 WORSE
I0312 10:11:17.329187 1891203 finetune.py:45] layer 29_o initial loss 3.8034071621950716e-05
I0312 10:11:20.322668 1891319 finetune.py:76] layer 30_k @ epoch 2 new loss 2.0637435227399692e-05 old loss 2.0223957108100876e-05 WORSE
I0312 10:11:23.629508 1891435 finetune.py:68] layer 31_k @ epoch 0 new loss 5.232425974099897e-05 old loss 6.672612653346732e-05 BETTER
I0312 10:11:37.697452 1891087 finetune.py:68] layer 28_o @ epoch 1 new loss 3.249192741350271e-05 old loss 3.328186721773818e-05 BETTER
I0312 10:11:47.887913 1891203 finetune.py:68] layer 29_o @ epoch 0 new loss 3.4672950278036296e-05 old loss 3.8034071621950716e-05 BETTER
I0312 10:11:51.792180 1891319 finetune.py:76] layer 30_k @ epoch 3 new loss 2.036931982729584e-05 old loss 2.0223957108100876e-05 WORSE
I0312 10:11:55.264658 1891435 finetune.py:68] layer 31_k @ epoch 1 new loss 4.635370714822784e-05 old loss 5.232425974099897e-05 BETTER
I0312 10:12:11.165832 1891087 finetune.py:68] layer 28_o @ epoch 2 new loss 3.217576158931479e-05 old loss 3.249192741350271e-05 BETTER
I0312 10:12:19.258874 1891203 finetune.py:68] layer 29_o @ epoch 1 new loss 3.4053693525493145e-05 old loss 3.4672950278036296e-05 BETTER
I0312 10:12:23.222348 1891319 finetune.py:76] layer 30_k @ epoch 4 new loss 2.031960138992872e-05 old loss 2.0223957108100876e-05 WORSE
I0312 10:12:26.894681 1891435 finetune.py:68] layer 31_k @ epoch 2 new loss 4.494887252803892e-05 old loss 4.635370714822784e-05 BETTER
I0312 10:12:31.958593 1891319 finetune.py:45] layer 30_o initial loss 4.259617344359867e-05
I0312 10:12:44.774145 1891087 finetune.py:68] layer 28_o @ epoch 3 new loss 3.1986517569748685e-05 old loss 3.217576158931479e-05 BETTER
I0312 10:12:50.668827 1891203 finetune.py:68] layer 29_o @ epoch 2 new loss 3.386126991244964e-05 old loss 3.4053693525493145e-05 BETTER
I0312 10:12:58.465650 1891435 finetune.py:76] layer 31_k @ epoch 3 new loss 4.5672961277887225e-05 old loss 4.494887252803892e-05 WORSE
I0312 10:13:02.769253 1891319 finetune.py:68] layer 30_o @ epoch 0 new loss 3.708982330863364e-05 old loss 4.259617344359867e-05 BETTER
I0312 10:13:18.367277 1891087 finetune.py:68] layer 28_o @ epoch 4 new loss 3.189189010299742e-05 old loss 3.1986517569748685e-05 BETTER
I0312 10:13:22.117877 1891203 finetune.py:68] layer 29_o @ epoch 3 new loss 3.365537850186229e-05 old loss 3.386126991244964e-05 BETTER
I0312 10:13:29.588933 1891435 finetune.py:76] layer 31_k @ epoch 4 new loss 4.785374039784074e-05 old loss 4.494887252803892e-05 WORSE
I0312 10:13:33.859868 1891087 finetune.py:45] layer 28_up initial loss 7.921539508970454e-05
I0312 10:13:34.360657 1891319 finetune.py:68] layer 30_o @ epoch 1 new loss 3.619517883635126e-05 old loss 3.708982330863364e-05 BETTER
I0312 10:13:38.471437 1891435 finetune.py:45] layer 31_o initial loss 9.57266820478253e-05
I0312 10:13:53.629179 1891203 finetune.py:76] layer 29_o @ epoch 4 new loss 3.3892953069880605e-05 old loss 3.365537850186229e-05 WORSE
I0312 10:14:04.580846 1891087 finetune.py:68] layer 28_up @ epoch 0 new loss 7.525205001002178e-05 old loss 7.921539508970454e-05 BETTER
I0312 10:14:06.150981 1891319 finetune.py:68] layer 30_o @ epoch 2 new loss 3.594395820982754e-05 old loss 3.619517883635126e-05 BETTER
I0312 10:14:08.611702 1891203 finetune.py:45] layer 29_up initial loss 9.200482600135729e-05
I0312 10:14:08.876433 1891435 finetune.py:68] layer 31_o @ epoch 0 new loss 6.855673564132303e-05 old loss 9.57266820478253e-05 BETTER
I0312 10:14:36.409780 1891087 finetune.py:68] layer 28_up @ epoch 1 new loss 7.403412746498361e-05 old loss 7.525205001002178e-05 BETTER
I0312 10:14:37.747665 1891203 finetune.py:68] layer 29_up @ epoch 0 new loss 8.513490320183337e-05 old loss 9.200482600135729e-05 BETTER
I0312 10:14:38.100932 1891319 finetune.py:68] layer 30_o @ epoch 3 new loss 3.545818981365301e-05 old loss 3.594395820982754e-05 BETTER
I0312 10:14:39.971881 1891435 finetune.py:68] layer 31_o @ epoch 1 new loss 6.604796362807974e-05 old loss 6.855673564132303e-05 BETTER
I0312 10:15:07.600422 1891203 finetune.py:68] layer 29_up @ epoch 1 new loss 8.342970249941573e-05 old loss 8.513490320183337e-05 BETTER
I0312 10:15:08.515880 1891087 finetune.py:68] layer 28_up @ epoch 2 new loss 7.33121923985891e-05 old loss 7.403412746498361e-05 BETTER
I0312 10:15:10.049445 1891319 finetune.py:76] layer 30_o @ epoch 4 new loss 3.575053051463328e-05 old loss 3.545818981365301e-05 WORSE
I0312 10:15:11.188274 1891435 finetune.py:68] layer 31_o @ epoch 2 new loss 6.5878193709068e-05 old loss 6.604796362807974e-05 BETTER
I0312 10:15:24.962244 1891319 finetune.py:45] layer 30_up initial loss 0.00014037055370863527
I0312 10:15:37.415060 1891203 finetune.py:68] layer 29_up @ epoch 2 new loss 8.249318489106372e-05 old loss 8.342970249941573e-05 BETTER
I0312 10:15:40.425403 1891087 finetune.py:68] layer 28_up @ epoch 3 new loss 7.286364416358992e-05 old loss 7.33121923985891e-05 BETTER
I0312 10:15:42.313099 1891435 finetune.py:68] layer 31_o @ epoch 3 new loss 6.437578849727288e-05 old loss 6.5878193709068e-05 BETTER
I0312 10:15:53.995065 1891319 finetune.py:68] layer 30_up @ epoch 0 new loss 0.00011493235069792718 old loss 0.00014037055370863527 BETTER
I0312 10:16:07.334829 1891203 finetune.py:68] layer 29_up @ epoch 3 new loss 8.189013897208497e-05 old loss 8.249318489106372e-05 BETTER
I0312 10:16:12.403009 1891087 finetune.py:68] layer 28_up @ epoch 4 new loss 7.257005199790001e-05 old loss 7.286364416358992e-05 BETTER
I0312 10:16:13.647412 1891435 finetune.py:76] layer 31_o @ epoch 4 new loss 6.925148045411333e-05 old loss 6.437578849727288e-05 WORSE
I0312 10:16:23.991195 1891319 finetune.py:68] layer 30_up @ epoch 1 new loss 0.00010999722871929407 old loss 0.00011493235069792718 BETTER
I0312 10:16:28.291877 1891087 finetune.py:45] layer 28_gate initial loss 0.00011784780508605763
I0312 10:16:28.852384 1891435 finetune.py:45] layer 31_up initial loss 0.00037443474866449833
I0312 10:16:37.069047 1891203 finetune.py:68] layer 29_up @ epoch 4 new loss 8.147306652972475e-05 old loss 8.189013897208497e-05 BETTER
I0312 10:16:52.773493 1891203 finetune.py:45] layer 29_gate initial loss 0.00013586881686933339
I0312 10:16:54.115131 1891319 finetune.py:68] layer 30_up @ epoch 2 new loss 0.00010725431639002636 old loss 0.00010999722871929407 BETTER
I0312 10:16:57.657039 1891087 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.00011552926298463717 old loss 0.00011784780508605763 BETTER
I0312 10:16:58.032500 1891435 finetune.py:68] layer 31_up @ epoch 0 new loss 0.00022748659830540419 old loss 0.00037443474866449833 BETTER
I0312 10:17:20.386707 1891203 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.00013242960267234594 old loss 0.00013586881686933339 BETTER
I0312 10:17:24.176486 1891319 finetune.py:68] layer 30_up @ epoch 3 new loss 0.00010541443043621257 old loss 0.00010725431639002636 BETTER
I0312 10:17:27.743000 1891087 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.00011456813081167638 old loss 0.00011552926298463717 BETTER
I0312 10:17:27.858657 1891435 finetune.py:68] layer 31_up @ epoch 1 new loss 0.00020884656987618655 old loss 0.00022748659830540419 BETTER
I0312 10:17:48.774820 1891203 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.0001312041567871347 old loss 0.00013242960267234594 BETTER
I0312 10:17:54.520816 1891319 finetune.py:68] layer 30_up @ epoch 4 new loss 0.00010403338819742203 old loss 0.00010541443043621257 BETTER
I0312 10:17:57.611797 1891435 finetune.py:68] layer 31_up @ epoch 2 new loss 0.0001969856966752559 old loss 0.00020884656987618655 BETTER
I0312 10:17:57.835870 1891087 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.00011397773050703108 old loss 0.00011456813081167638 BETTER
I0312 10:18:10.230708 1891319 finetune.py:45] layer 30_gate initial loss 0.00017976081289816648
I0312 10:18:17.125543 1891203 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0001304260949837044 old loss 0.0001312041567871347 BETTER
I0312 10:18:27.424105 1891435 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0001886053360067308 old loss 0.0001969856966752559 BETTER
I0312 10:18:28.060614 1891087 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.00011359719064785168 old loss 0.00011397773050703108 BETTER
I0312 10:18:37.525554 1891319 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0001690595381660387 old loss 0.00017976081289816648 BETTER
I0312 10:18:45.156947 1891203 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0001299345021834597 old loss 0.0001304260949837044 BETTER
I0312 10:18:57.213117 1891435 finetune.py:68] layer 31_up @ epoch 4 new loss 0.00018202848150394857 old loss 0.0001886053360067308 BETTER
I0312 10:18:58.255537 1891087 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.00011329937115078792 old loss 0.00011359719064785168 BETTER
I0312 10:19:05.732321 1891319 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.00016574746405240148 old loss 0.0001690595381660387 BETTER
I0312 10:19:12.981326 1891435 finetune.py:45] layer 31_gate initial loss 0.0003392345679458231
I0312 10:19:13.879303 1891203 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0001295913680223748 old loss 0.0001299345021834597 BETTER
I0312 10:19:15.029373 1891087 finetune.py:45] layer 28_down initial loss 0.00018553320842329413
I0312 10:19:29.774424 1891203 finetune.py:45] layer 29_down initial loss 0.00022273283684626222
I0312 10:19:34.083896 1891319 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.00016408160445280373 old loss 0.00016574746405240148 BETTER
I0312 10:19:40.050224 1891435 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.0002832968602888286 old loss 0.0003392345679458231 BETTER
I0312 10:19:42.502620 1891087 finetune.py:68] layer 28_down @ epoch 0 new loss 0.00018547104264143854 old loss 0.00018553320842329413 BETTER
I0312 10:19:55.453681 1891203 finetune.py:68] layer 29_down @ epoch 0 new loss 0.00022264015569817275 old loss 0.00022273283684626222 BETTER
I0312 10:20:02.280505 1891319 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.00016295519890263677 old loss 0.00016408160445280373 BETTER
I0312 10:20:08.470235 1891435 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.000274994526989758 old loss 0.0002832968602888286 BETTER
I0312 10:20:11.296535 1891087 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0001854476722655818 old loss 0.00018547104264143854 BETTER
I0312 10:20:23.023145 1891203 finetune.py:68] layer 29_down @ epoch 1 new loss 0.00022259668912738562 old loss 0.00022264015569817275 BETTER
I0312 10:20:31.169056 1891319 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0001621209376025945 old loss 0.00016295519890263677 BETTER
I0312 10:20:37.080988 1891435 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.0002697452437132597 old loss 0.000274994526989758 BETTER
I0312 10:20:39.973408 1891087 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0001854332658695057 old loss 0.0001854476722655818 BETTER
I0312 10:20:48.069117 1891319 finetune.py:45] layer 30_down initial loss 0.00047507003182545304
I0312 10:20:50.224744 1891203 finetune.py:68] layer 29_down @ epoch 2 new loss 0.00022256920055951923 old loss 0.00022259668912738562 BETTER
I0312 10:21:05.575998 1891435 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.0002657621225807816 old loss 0.0002697452437132597 BETTER
I0312 10:21:08.673037 1891087 finetune.py:68] layer 28_down @ epoch 3 new loss 0.00018542353063821793 old loss 0.0001854332658695057 BETTER
I0312 10:21:14.385896 1891319 finetune.py:68] layer 30_down @ epoch 0 new loss 0.0004677580145653337 old loss 0.00047507003182545304 BETTER
I0312 10:21:17.126089 1891203 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0002225481002824381 old loss 0.00022256920055951923 BETTER
I0312 10:21:33.862534 1891435 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.00026264501502737403 old loss 0.0002657621225807816 BETTER
I0312 10:21:37.187401 1891087 finetune.py:68] layer 28_down @ epoch 4 new loss 0.00018541578901931643 old loss 0.00018542353063821793 BETTER
28_v proxy err 0.00043755219667218626 tr(WHW.T) 2018.944091796875
28_q proxy err 0.00015287150745280087 tr(WHW.T) 7650.97216796875
28_k proxy err 0.00011971397179877385 tr(WHW.T) 10544.2177734375
28_o proxy err 0.00035020324867218733 tr(WHW.T) 194.83900451660156
28_up proxy err 0.00033864242141135037 tr(WHW.T) 4660.22021484375
28_gate proxy err 0.00026820774655789137 tr(WHW.T) 6545.62060546875
28_down proxy err 0.0004925935063511133 tr(WHW.T) 603.8658447265625
I0312 10:21:41.484694 1891319 finetune.py:68] layer 30_down @ epoch 1 new loss 0.00046225794358178973 old loss 0.0004677580145653337 BETTER
I0312 10:21:44.020190 1891203 finetune.py:68] layer 29_down @ epoch 4 new loss 0.00022252970666158944 old loss 0.0002225481002824381 BETTER
29_v proxy err 0.00045841868268325925 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.00015938376600388438 tr(WHW.T) 7227.17724609375
29_k proxy err 0.00011970131163252518 tr(WHW.T) 10557.5947265625
29_o proxy err 0.0003294700582046062 tr(WHW.T) 207.88978576660156
29_up proxy err 0.0002795863838400692 tr(WHW.T) 6070.56787109375
29_gate proxy err 0.0002498118847142905 tr(WHW.T) 7368.84228515625
29_down proxy err 0.000488469609990716 tr(WHW.T) 782.677001953125
I0312 10:21:49.876585 1891435 finetune.py:45] layer 31_down initial loss 0.001044365344569087
I0312 10:22:08.698120 1891319 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0004578562220558524 old loss 0.00046225794358178973 BETTER
I0312 10:22:15.242243 1891435 finetune.py:68] layer 31_down @ epoch 0 new loss 0.001039070193655789 old loss 0.001044365344569087 BETTER
I0312 10:22:35.520964 1891319 finetune.py:68] layer 30_down @ epoch 3 new loss 0.00045448605669662356 old loss 0.0004578562220558524 BETTER
I0312 10:22:41.462311 1891435 finetune.py:68] layer 31_down @ epoch 1 new loss 0.0010368562070652843 old loss 0.001039070193655789 BETTER
I0312 10:23:02.301372 1891319 finetune.py:68] layer 30_down @ epoch 4 new loss 0.00045184738701209426 old loss 0.00045448605669662356 BETTER
30_v proxy err 0.0003937416186090559 tr(WHW.T) 2261.489501953125
30_q proxy err 0.00015193600847851485 tr(WHW.T) 7815.921875
30_k proxy err 0.0001226357271661982 tr(WHW.T) 10520.8486328125
30_o proxy err 0.0003235440526623279 tr(WHW.T) 251.89483642578125
30_up proxy err 0.00018982402980327606 tr(WHW.T) 10016.0703125
30_gate proxy err 0.00018619147886056453 tr(WHW.T) 11000.98828125
30_down proxy err 0.0003542828490026295 tr(WHW.T) 3584.29248046875
I0312 10:23:07.752689 1891435 finetune.py:68] layer 31_down @ epoch 2 new loss 0.0010355027625337243 old loss 0.0010368562070652843 BETTER
I0312 10:23:34.006744 1891435 finetune.py:68] layer 31_down @ epoch 3 new loss 0.0010346067138016224 old loss 0.0010355027625337243 BETTER
I0312 10:24:00.299096 1891435 finetune.py:68] layer 31_down @ epoch 4 new loss 0.0010339220752939582 old loss 0.0010346067138016224 BETTER
31_v proxy err 0.00046041831956245005 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.00013921651407144964 tr(WHW.T) 6858.77294921875
31_k proxy err 0.00010609799210214987 tr(WHW.T) 10234.798828125
31_o proxy err 0.00024923234013840556 tr(WHW.T) 457.8403015136719
31_up proxy err 0.0001365718198940158 tr(WHW.T) 14559.2724609375
31_gate proxy err 0.00014542795543093234 tr(WHW.T) 14827.4609375
31_down proxy err 0.00018209706468041986 tr(WHW.T) 17885.5703125
