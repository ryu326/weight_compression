I0313 01:57:45.558749 1985945 config.py:54] PyTorch version 2.1.1 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.91it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00, 10.12it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.96it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.84it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 10.06it/s]
I0313 01:57:47.345823 1985945 quantize_finetune_llama.py:135] loaded model
calculating model weight mean & std:   0%|          | 0/32 [00:00<?, ?it/s]calculating model weight mean & std:   3%|▎         | 1/32 [00:00<00:21,  1.44it/s]calculating model weight mean & std:   6%|▋         | 2/32 [00:01<00:20,  1.44it/s]calculating model weight mean & std:   9%|▉         | 3/32 [00:02<00:20,  1.44it/s]calculating model weight mean & std:  12%|█▎        | 4/32 [00:02<00:19,  1.44it/s]calculating model weight mean & std:  16%|█▌        | 5/32 [00:03<00:18,  1.44it/s]calculating model weight mean & std:  19%|█▉        | 6/32 [00:04<00:19,  1.36it/s]calculating model weight mean & std:  22%|██▏       | 7/32 [00:05<00:19,  1.28it/s]calculating model weight mean & std:  25%|██▌       | 8/32 [00:06<00:19,  1.23it/s]calculating model weight mean & std:  28%|██▊       | 9/32 [00:06<00:17,  1.29it/s]calculating model weight mean & std:  31%|███▏      | 10/32 [00:07<00:16,  1.33it/s]calculating model weight mean & std:  34%|███▍      | 11/32 [00:08<00:15,  1.34it/s]calculating model weight mean & std:  38%|███▊      | 12/32 [00:08<00:14,  1.34it/s]calculating model weight mean & std:  41%|████      | 13/32 [00:09<00:13,  1.36it/s]calculating model weight mean & std:  44%|████▍     | 14/32 [00:10<00:13,  1.38it/s]calculating model weight mean & std:  47%|████▋     | 15/32 [00:11<00:12,  1.39it/s]calculating model weight mean & std:  50%|█████     | 16/32 [00:11<00:11,  1.40it/s]calculating model weight mean & std:  53%|█████▎    | 17/32 [00:12<00:10,  1.41it/s]calculating model weight mean & std:  56%|█████▋    | 18/32 [00:13<00:09,  1.42it/s]calculating model weight mean & std:  59%|█████▉    | 19/32 [00:13<00:09,  1.42it/s]calculating model weight mean & std:  62%|██████▎   | 20/32 [00:14<00:08,  1.42it/s]calculating model weight mean & std:  66%|██████▌   | 21/32 [00:15<00:07,  1.42it/s]calculating model weight mean & std:  69%|██████▉   | 22/32 [00:15<00:07,  1.43it/s]calculating model weight mean & std:  72%|███████▏  | 23/32 [00:16<00:06,  1.43it/s]calculating model weight mean & std:  75%|███████▌  | 24/32 [00:17<00:05,  1.46it/s]calculating model weight mean & std:  78%|███████▊  | 25/32 [00:17<00:04,  1.48it/s]calculating model weight mean & std:  81%|████████▏ | 26/32 [00:18<00:04,  1.49it/s]calculating model weight mean & std:  84%|████████▍ | 27/32 [00:19<00:03,  1.50it/s]calculating model weight mean & std:  88%|████████▊ | 28/32 [00:19<00:02,  1.51it/s]calculating model weight mean & std:  91%|█████████ | 29/32 [00:20<00:01,  1.52it/s]calculating model weight mean & std:  94%|█████████▍| 30/32 [00:21<00:01,  1.52it/s]calculating model weight mean & std:  97%|█████████▋| 31/32 [00:21<00:00,  1.53it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.53it/s]calculating model weight mean & std: 100%|██████████| 32/32 [00:22<00:00,  1.42it/s]
I0313 01:58:19.495519 1985945 quantize_finetune_llama.py:160] loaded compression model
I0313 01:58:33.056693 1985945 quantize_finetune_llama.py:164] loaded dataset and devset
I0313 01:58:38.405621 1985945 quantize_finetune_llama.py:184] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 01:59:59.223777 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 0 in 80.70205402374268s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0313 02:00:21.994711 1987950 config.py:54] PyTorch version 2.1.1 available.
I0313 02:00:23.040404 1985945 quantize_finetune_llama.py:184] layer 1 gpu 1
I0313 02:00:23.108590 1987950 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:00:40.143256 1987950 finetune.py:45] layer 0_v initial loss 2.870400237497961e-07
I0313 02:01:12.013486 1987950 finetune.py:68] layer 0_v @ epoch 0 new loss 1.3196306269946945e-07 old loss 2.870400237497961e-07 BETTER
I0313 02:01:32.992608 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 1 in 69.77637219429016s
I0313 02:01:43.745651 1989071 config.py:54] PyTorch version 2.1.1 available.
I0313 02:01:44.756682 1985945 quantize_finetune_llama.py:184] layer 2 gpu 2
I0313 02:01:44.825353 1989071 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 02:01:45.405247 1987950 finetune.py:68] layer 0_v @ epoch 1 new loss 8.358924930007561e-08 old loss 1.3196306269946945e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:02:01.817012 1989071 finetune.py:45] layer 1_v initial loss 8.611550583736971e-05
I0313 02:02:19.017459 1987950 finetune.py:68] layer 0_v @ epoch 2 new loss 6.684212650043264e-08 old loss 8.358924930007561e-08 BETTER
I0313 02:02:32.712050 1989071 finetune.py:68] layer 1_v @ epoch 0 new loss 1.0108044079970568e-05 old loss 8.611550583736971e-05 BETTER
I0313 02:02:53.148319 1987950 finetune.py:68] layer 0_v @ epoch 3 new loss 5.9419619446998695e-08 old loss 6.684212650043264e-08 BETTER
I0313 02:02:54.475377 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 2 in 69.5282027721405s
I0313 02:03:02.922785 1990119 config.py:54] PyTorch version 2.1.1 available.
I0313 02:03:03.962701 1985945 quantize_finetune_llama.py:184] layer 3 gpu 3
I0313 02:03:04.035905 1990119 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 02:03:04.606645 1989071 finetune.py:68] layer 1_v @ epoch 1 new loss 7.943514901853632e-06 old loss 1.0108044079970568e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:03:20.989781 1990119 finetune.py:45] layer 2_v initial loss 2.284164474986028e-05
I0313 02:03:27.357550 1987950 finetune.py:68] layer 0_v @ epoch 4 new loss 5.5497707762697246e-08 old loss 5.9419619446998695e-08 BETTER
I0313 02:03:36.821504 1989071 finetune.py:68] layer 1_v @ epoch 2 new loss 6.59077932141372e-06 old loss 7.943514901853632e-06 BETTER
I0313 02:03:44.950283 1987950 finetune.py:45] layer 0_q initial loss 5.649275180985569e-08
I0313 02:03:52.265470 1990119 finetune.py:68] layer 2_v @ epoch 0 new loss 1.196040648210328e-05 old loss 2.284164474986028e-05 BETTER
I0313 02:04:10.982986 1989071 finetune.py:76] layer 1_v @ epoch 3 new loss 7.2351872404397e-06 old loss 6.59077932141372e-06 WORSE
I0313 02:04:19.014713 1987950 finetune.py:68] layer 0_q @ epoch 0 new loss 5.3077801709378036e-08 old loss 5.649275180985569e-08 BETTER
I0313 02:04:23.130696 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 3 in 78.97543406486511s
I0313 02:04:29.068089 1990119 finetune.py:68] layer 2_v @ epoch 1 new loss 8.122487997752614e-06 old loss 1.196040648210328e-05 BETTER
I0313 02:04:34.375869 1991347 config.py:54] PyTorch version 2.1.1 available.
I0313 02:04:35.476868 1985945 quantize_finetune_llama.py:184] layer 4 gpu 0
I0313 02:04:35.548932 1991347 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 02:04:44.281671 1989071 finetune.py:76] layer 1_v @ epoch 4 new loss 7.2412613008054905e-06 old loss 6.59077932141372e-06 WORSE
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:04:52.531621 1991347 finetune.py:45] layer 3_v initial loss 5.040111864218488e-05
I0313 02:04:53.521741 1987950 finetune.py:68] layer 0_q @ epoch 1 new loss 5.1001144640849816e-08 old loss 5.3077801709378036e-08 BETTER
I0313 02:05:01.286953 1990119 finetune.py:68] layer 2_v @ epoch 2 new loss 6.6339589466224425e-06 old loss 8.122487997752614e-06 BETTER
I0313 02:05:01.567558 1989071 finetune.py:45] layer 1_q initial loss 1.9923783838748932e-05
I0313 02:05:23.450748 1991347 finetune.py:68] layer 3_v @ epoch 0 new loss 2.19557732634712e-05 old loss 5.040111864218488e-05 BETTER
I0313 02:05:27.409272 1987950 finetune.py:68] layer 0_q @ epoch 2 new loss 4.9396454926409206e-08 old loss 5.1001144640849816e-08 BETTER
I0313 02:05:32.926323 1989071 finetune.py:68] layer 1_q @ epoch 0 new loss 7.811092473275494e-06 old loss 1.9923783838748932e-05 BETTER
I0313 02:05:34.152560 1990119 finetune.py:68] layer 2_v @ epoch 3 new loss 6.01308056502603e-06 old loss 6.6339589466224425e-06 BETTER
I0313 02:05:55.443055 1991347 finetune.py:68] layer 3_v @ epoch 1 new loss 1.5062315469549503e-05 old loss 2.19557732634712e-05 BETTER
I0313 02:06:01.461208 1987950 finetune.py:68] layer 0_q @ epoch 3 new loss 4.815217380382819e-08 old loss 4.9396454926409206e-08 BETTER
I0313 02:06:04.997711 1989071 finetune.py:68] layer 1_q @ epoch 1 new loss 7.131964594009332e-06 old loss 7.811092473275494e-06 BETTER
I0313 02:06:07.187254 1990119 finetune.py:68] layer 2_v @ epoch 4 new loss 5.733924353990005e-06 old loss 6.01308056502603e-06 BETTER
I0313 02:06:24.741328 1990119 finetune.py:45] layer 2_q initial loss 6.200195457495283e-06
I0313 02:06:27.591607 1991347 finetune.py:68] layer 3_v @ epoch 2 new loss 1.3069366104900837e-05 old loss 1.5062315469549503e-05 BETTER
I0313 02:06:35.414099 1987950 finetune.py:68] layer 0_q @ epoch 4 new loss 4.7119137036588654e-08 old loss 4.815217380382819e-08 BETTER
I0313 02:06:37.108405 1989071 finetune.py:76] layer 1_q @ epoch 2 new loss 8.49470143293729e-06 old loss 7.131964594009332e-06 WORSE
I0313 02:06:52.930450 1987950 finetune.py:45] layer 0_k initial loss 4.9119439182732094e-08
I0313 02:06:56.190752 1990119 finetune.py:68] layer 2_q @ epoch 0 new loss 5.9760691328847315e-06 old loss 6.200195457495283e-06 BETTER
I0313 02:06:59.898924 1991347 finetune.py:68] layer 3_v @ epoch 3 new loss 1.2351952136668842e-05 old loss 1.3069366104900837e-05 BETTER
I0313 02:07:08.538408 1989071 finetune.py:76] layer 1_q @ epoch 3 new loss 7.3689866439963225e-06 old loss 7.131964594009332e-06 WORSE
I0313 02:07:25.194878 1987950 finetune.py:68] layer 0_k @ epoch 0 new loss 4.7304666850322974e-08 old loss 4.9119439182732094e-08 BETTER
I0313 02:07:28.601544 1990119 finetune.py:68] layer 2_q @ epoch 1 new loss 5.885160135221668e-06 old loss 5.9760691328847315e-06 BETTER
I0313 02:07:32.256485 1991347 finetune.py:68] layer 3_v @ epoch 4 new loss 1.2016689652227797e-05 old loss 1.2351952136668842e-05 BETTER
I0313 02:07:39.948156 1989071 finetune.py:76] layer 1_q @ epoch 4 new loss 1.2389079529384617e-05 old loss 7.131964594009332e-06 WORSE
I0313 02:07:49.684093 1991347 finetune.py:45] layer 3_q initial loss 1.4211566849553492e-05
I0313 02:07:56.797729 1989071 finetune.py:45] layer 1_k initial loss 1.064565822161967e-05
I0313 02:07:58.474510 1987950 finetune.py:68] layer 0_k @ epoch 1 new loss 4.638850015226126e-08 old loss 4.7304666850322974e-08 BETTER
I0313 02:08:00.851914 1990119 finetune.py:68] layer 2_q @ epoch 2 new loss 5.823193077958422e-06 old loss 5.885160135221668e-06 BETTER
I0313 02:08:20.818847 1991347 finetune.py:68] layer 3_q @ epoch 0 new loss 1.3755236977885943e-05 old loss 1.4211566849553492e-05 BETTER
I0313 02:08:27.752135 1989071 finetune.py:68] layer 1_k @ epoch 0 new loss 8.381002771784551e-06 old loss 1.064565822161967e-05 BETTER
I0313 02:08:31.938819 1987950 finetune.py:68] layer 0_k @ epoch 2 new loss 4.56444162466596e-08 old loss 4.638850015226126e-08 BETTER
I0313 02:08:33.162289 1990119 finetune.py:68] layer 2_q @ epoch 3 new loss 5.77297032577917e-06 old loss 5.823193077958422e-06 BETTER
I0313 02:08:52.801031 1991347 finetune.py:68] layer 3_q @ epoch 1 new loss 1.3549441064242274e-05 old loss 1.3755236977885943e-05 BETTER
I0313 02:08:59.702709 1989071 finetune.py:68] layer 1_k @ epoch 1 new loss 8.181635166693013e-06 old loss 8.381002771784551e-06 BETTER
I0313 02:09:05.449819 1987950 finetune.py:68] layer 0_k @ epoch 3 new loss 4.498183869827699e-08 old loss 4.56444162466596e-08 BETTER
I0313 02:09:05.512406 1990119 finetune.py:68] layer 2_q @ epoch 4 new loss 5.729993063141592e-06 old loss 5.77297032577917e-06 BETTER
I0313 02:09:23.105470 1990119 finetune.py:45] layer 2_k initial loss 6.171823770273477e-06
I0313 02:09:24.762297 1991347 finetune.py:68] layer 3_q @ epoch 2 new loss 1.3393018889473751e-05 old loss 1.3549441064242274e-05 BETTER
I0313 02:09:31.713515 1989071 finetune.py:76] layer 1_k @ epoch 2 new loss 8.24256676423829e-06 old loss 8.181635166693013e-06 WORSE
I0313 02:09:38.831441 1987950 finetune.py:68] layer 0_k @ epoch 4 new loss 4.4534608889534866e-08 old loss 4.498183869827699e-08 BETTER
I0313 02:09:54.563044 1990119 finetune.py:68] layer 2_k @ epoch 0 new loss 6.125070285634138e-06 old loss 6.171823770273477e-06 BETTER
I0313 02:09:56.771356 1991347 finetune.py:68] layer 3_q @ epoch 3 new loss 1.3268239854369313e-05 old loss 1.3393018889473751e-05 BETTER
I0313 02:09:56.835487 1987950 finetune.py:45] layer 0_o initial loss 3.2731452392908977e-07
I0313 02:10:03.344029 1989071 finetune.py:68] layer 1_k @ epoch 3 new loss 7.792303222231567e-06 old loss 8.181635166693013e-06 BETTER
I0313 02:10:26.787332 1990119 finetune.py:68] layer 2_k @ epoch 1 new loss 6.087988822400803e-06 old loss 6.125070285634138e-06 BETTER
I0313 02:10:28.633868 1987950 finetune.py:68] layer 0_o @ epoch 0 new loss 3.1027212799017434e-07 old loss 3.2731452392908977e-07 BETTER
I0313 02:10:28.857748 1991347 finetune.py:68] layer 3_q @ epoch 4 new loss 1.3161083188606426e-05 old loss 1.3268239854369313e-05 BETTER
I0313 02:10:35.334190 1989071 finetune.py:68] layer 1_k @ epoch 4 new loss 7.069069397402927e-06 old loss 7.792303222231567e-06 BETTER
I0313 02:10:46.419595 1991347 finetune.py:45] layer 3_k initial loss 1.4899984307703562e-05
I0313 02:10:53.134608 1989071 finetune.py:45] layer 1_o initial loss 2.5830862796283327e-05
I0313 02:10:58.937622 1990119 finetune.py:68] layer 2_k @ epoch 2 new loss 6.0548895817191806e-06 old loss 6.087988822400803e-06 BETTER
I0313 02:11:01.314861 1987950 finetune.py:68] layer 0_o @ epoch 1 new loss 2.9786758659611223e-07 old loss 3.1027212799017434e-07 BETTER
I0313 02:11:17.206939 1991347 finetune.py:68] layer 3_k @ epoch 0 new loss 1.4733383977727499e-05 old loss 1.4899984307703562e-05 BETTER
I0313 02:11:23.397682 1989071 finetune.py:68] layer 1_o @ epoch 0 new loss 1.542878817417659e-05 old loss 2.5830862796283327e-05 BETTER
I0313 02:11:31.045567 1990119 finetune.py:68] layer 2_k @ epoch 3 new loss 6.025093625794398e-06 old loss 6.0548895817191806e-06 BETTER
I0313 02:11:34.255392 1987950 finetune.py:68] layer 0_o @ epoch 2 new loss 2.8810742946916434e-07 old loss 2.9786758659611223e-07 BETTER
I0313 02:11:48.854850 1991347 finetune.py:68] layer 3_k @ epoch 1 new loss 1.4637497770308983e-05 old loss 1.4733383977727499e-05 BETTER
I0313 02:11:54.568913 1989071 finetune.py:68] layer 1_o @ epoch 1 new loss 1.5289100701920688e-05 old loss 1.542878817417659e-05 BETTER
I0313 02:12:03.318215 1990119 finetune.py:68] layer 2_k @ epoch 4 new loss 5.997553216730012e-06 old loss 6.025093625794398e-06 BETTER
I0313 02:12:07.580686 1987950 finetune.py:68] layer 0_o @ epoch 3 new loss 2.8035071863996563e-07 old loss 2.8810742946916434e-07 BETTER
I0313 02:12:20.435711 1991347 finetune.py:68] layer 3_k @ epoch 2 new loss 1.4558633665728848e-05 old loss 1.4637497770308983e-05 BETTER
I0313 02:12:21.447233 1990119 finetune.py:45] layer 2_o initial loss 2.0742403648910113e-05
I0313 02:12:25.899365 1989071 finetune.py:68] layer 1_o @ epoch 2 new loss 1.5099964002729394e-05 old loss 1.5289100701920688e-05 BETTER
I0313 02:12:40.768470 1987950 finetune.py:68] layer 0_o @ epoch 4 new loss 2.7412170311436057e-07 old loss 2.8035071863996563e-07 BETTER
I0313 02:12:52.098316 1990119 finetune.py:68] layer 2_o @ epoch 0 new loss 2.030650466622319e-05 old loss 2.0742403648910113e-05 BETTER
I0313 02:12:52.099500 1991347 finetune.py:68] layer 3_k @ epoch 3 new loss 1.4488340639218222e-05 old loss 1.4558633665728848e-05 BETTER
I0313 02:12:57.270165 1989071 finetune.py:68] layer 1_o @ epoch 3 new loss 1.5088251529959962e-05 old loss 1.5099964002729394e-05 BETTER
I0313 02:13:04.193561 1987950 finetune.py:45] layer 0_up initial loss 5.001584781894053e-07
I0313 02:13:23.570382 1990119 finetune.py:68] layer 2_o @ epoch 1 new loss 2.000252243306022e-05 old loss 2.030650466622319e-05 BETTER
I0313 02:13:23.783293 1991347 finetune.py:68] layer 3_k @ epoch 4 new loss 1.4425834706344176e-05 old loss 1.4488340639218222e-05 BETTER
I0313 02:13:28.861769 1989071 finetune.py:68] layer 1_o @ epoch 4 new loss 1.5035886463010684e-05 old loss 1.5088251529959962e-05 BETTER
I0313 02:13:34.353762 1987950 finetune.py:68] layer 0_up @ epoch 0 new loss 4.717265937870252e-07 old loss 5.001584781894053e-07 BETTER
I0313 02:13:41.620306 1991347 finetune.py:45] layer 3_o initial loss 4.1107061406364664e-05
I0313 02:13:51.703790 1989071 finetune.py:45] layer 1_up initial loss 2.3369966584141366e-05
I0313 02:13:55.060949 1990119 finetune.py:68] layer 2_o @ epoch 2 new loss 1.9771880033658817e-05 old loss 2.000252243306022e-05 BETTER
I0313 02:14:05.562755 1987950 finetune.py:68] layer 0_up @ epoch 1 new loss 4.5764048195451323e-07 old loss 4.717265937870252e-07 BETTER
I0313 02:14:11.548808 1991347 finetune.py:68] layer 3_o @ epoch 0 new loss 3.993789869127795e-05 old loss 4.1107061406364664e-05 BETTER
I0313 02:14:20.425540 1989071 finetune.py:68] layer 1_up @ epoch 0 new loss 1.76539397216402e-05 old loss 2.3369966584141366e-05 BETTER
I0313 02:14:26.516397 1990119 finetune.py:68] layer 2_o @ epoch 3 new loss 1.9587534552556463e-05 old loss 1.9771880033658817e-05 BETTER
I0313 02:14:37.040610 1987950 finetune.py:68] layer 0_up @ epoch 2 new loss 4.4950192545911705e-07 old loss 4.5764048195451323e-07 BETTER
I0313 02:14:42.503571 1991347 finetune.py:68] layer 3_o @ epoch 1 new loss 3.920986637240276e-05 old loss 3.993789869127795e-05 BETTER
I0313 02:14:50.176283 1989071 finetune.py:68] layer 1_up @ epoch 1 new loss 1.7574207959114574e-05 old loss 1.76539397216402e-05 BETTER
I0313 02:14:58.315958 1990119 finetune.py:68] layer 2_o @ epoch 4 new loss 1.9436276488704607e-05 old loss 1.9587534552556463e-05 BETTER
I0313 02:15:08.569281 1987950 finetune.py:68] layer 0_up @ epoch 3 new loss 4.4415739353098616e-07 old loss 4.4950192545911705e-07 BETTER
I0313 02:15:13.431157 1991347 finetune.py:68] layer 3_o @ epoch 2 new loss 3.8728474464733154e-05 old loss 3.920986637240276e-05 BETTER
I0313 02:15:19.925595 1989071 finetune.py:68] layer 1_up @ epoch 2 new loss 1.742262793413829e-05 old loss 1.7574207959114574e-05 BETTER
I0313 02:15:21.728676 1990119 finetune.py:45] layer 2_up initial loss 2.8494612706708722e-05
I0313 02:15:40.162401 1987950 finetune.py:68] layer 0_up @ epoch 4 new loss 4.4030795720573224e-07 old loss 4.4415739353098616e-07 BETTER
I0313 02:15:44.504293 1991347 finetune.py:68] layer 3_o @ epoch 3 new loss 3.838254633592442e-05 old loss 3.8728474464733154e-05 BETTER
I0313 02:15:49.659240 1989071 finetune.py:68] layer 1_up @ epoch 3 new loss 1.7198019122588448e-05 old loss 1.742262793413829e-05 BETTER
I0313 02:15:50.605763 1990119 finetune.py:68] layer 2_up @ epoch 0 new loss 2.8309241315582767e-05 old loss 2.8494612706708722e-05 BETTER
I0313 02:16:03.263232 1987950 finetune.py:45] layer 0_gate initial loss 6.873640359117417e-07
I0313 02:16:15.426740 1991347 finetune.py:68] layer 3_o @ epoch 4 new loss 3.8120597309898585e-05 old loss 3.838254633592442e-05 BETTER
I0313 02:16:19.418191 1989071 finetune.py:68] layer 1_up @ epoch 4 new loss 1.7114703950937837e-05 old loss 1.7198019122588448e-05 BETTER
I0313 02:16:20.258156 1990119 finetune.py:68] layer 2_up @ epoch 1 new loss 2.817761378537398e-05 old loss 2.8309241315582767e-05 BETTER
I0313 02:16:31.916348 1987950 finetune.py:68] layer 0_gate @ epoch 0 new loss 6.552828040184977e-07 old loss 6.873640359117417e-07 BETTER
I0313 02:16:38.754780 1991347 finetune.py:45] layer 3_up initial loss 6.096020297263749e-05
I0313 02:16:42.295743 1989071 finetune.py:45] layer 1_gate initial loss 2.2358412024914287e-05
I0313 02:16:49.928673 1990119 finetune.py:68] layer 2_up @ epoch 2 new loss 2.8073367502656765e-05 old loss 2.817761378537398e-05 BETTER
I0313 02:17:01.649028 1987950 finetune.py:68] layer 0_gate @ epoch 1 new loss 6.383205004567571e-07 old loss 6.552828040184977e-07 BETTER
I0313 02:17:07.237502 1991347 finetune.py:68] layer 3_up @ epoch 0 new loss 6.0567752370843664e-05 old loss 6.096020297263749e-05 BETTER
I0313 02:17:09.640202 1989071 finetune.py:68] layer 1_gate @ epoch 0 new loss 1.945046824403107e-05 old loss 2.2358412024914287e-05 BETTER
I0313 02:17:19.759585 1990119 finetune.py:68] layer 2_up @ epoch 3 new loss 2.7984850021312013e-05 old loss 2.8073367502656765e-05 BETTER
I0313 02:17:31.360359 1987950 finetune.py:68] layer 0_gate @ epoch 2 new loss 6.279216790971986e-07 old loss 6.383205004567571e-07 BETTER
I0313 02:17:36.650720 1991347 finetune.py:68] layer 3_up @ epoch 1 new loss 6.0299909819150344e-05 old loss 6.0567752370843664e-05 BETTER
I0313 02:17:37.842984 1989071 finetune.py:68] layer 1_gate @ epoch 1 new loss 1.9390541638131253e-05 old loss 1.945046824403107e-05 BETTER
I0313 02:17:49.650306 1990119 finetune.py:68] layer 2_up @ epoch 4 new loss 2.790805410768371e-05 old loss 2.7984850021312013e-05 BETTER
I0313 02:18:01.116232 1987950 finetune.py:68] layer 0_gate @ epoch 3 new loss 6.207827141224698e-07 old loss 6.279216790971986e-07 BETTER
I0313 02:18:05.991680 1989071 finetune.py:68] layer 1_gate @ epoch 2 new loss 1.91925519175129e-05 old loss 1.9390541638131253e-05 BETTER
I0313 02:18:06.122704 1991347 finetune.py:68] layer 3_up @ epoch 2 new loss 6.009888238622807e-05 old loss 6.0299909819150344e-05 BETTER
I0313 02:18:12.762881 1990119 finetune.py:45] layer 2_gate initial loss 3.4497676097089425e-05
I0313 02:18:31.046442 1987950 finetune.py:68] layer 0_gate @ epoch 4 new loss 6.156051881589519e-07 old loss 6.207827141224698e-07 BETTER
I0313 02:18:34.144819 1989071 finetune.py:68] layer 1_gate @ epoch 3 new loss 1.9119232092634775e-05 old loss 1.91925519175129e-05 BETTER
I0313 02:18:35.586516 1991347 finetune.py:68] layer 3_up @ epoch 3 new loss 5.99289451201912e-05 old loss 6.009888238622807e-05 BETTER
I0313 02:18:40.272793 1990119 finetune.py:68] layer 2_gate @ epoch 0 new loss 3.4353182854829356e-05 old loss 3.4497676097089425e-05 BETTER
I0313 02:19:02.197278 1989071 finetune.py:68] layer 1_gate @ epoch 4 new loss 1.907765181385912e-05 old loss 1.9119232092634775e-05 BETTER
I0313 02:19:05.076889 1991347 finetune.py:68] layer 3_up @ epoch 4 new loss 5.977924229227938e-05 old loss 5.99289451201912e-05 BETTER
I0313 02:19:08.286090 1990119 finetune.py:68] layer 2_gate @ epoch 1 new loss 3.425385875743814e-05 old loss 3.4353182854829356e-05 BETTER
I0313 02:19:11.881022 1987950 finetune.py:45] layer 0_down initial loss 1.5318396435759496e-06
I0313 02:19:27.649086 1991347 finetune.py:45] layer 3_gate initial loss 7.654225191799924e-05
I0313 02:19:36.396912 1990119 finetune.py:68] layer 2_gate @ epoch 2 new loss 3.417626430746168e-05 old loss 3.425385875743814e-05 BETTER
I0313 02:19:38.237768 1987950 finetune.py:68] layer 0_down @ epoch 0 new loss 1.5263494788086973e-06 old loss 1.5318396435759496e-06 BETTER
I0313 02:19:42.746957 1989071 finetune.py:45] layer 1_down initial loss 3.05112171190558e-05
I0313 02:19:54.471151 1991347 finetune.py:68] layer 3_gate @ epoch 0 new loss 7.625539728906006e-05 old loss 7.654225191799924e-05 BETTER
I0313 02:20:04.786036 1990119 finetune.py:68] layer 2_gate @ epoch 3 new loss 3.4111446439055726e-05 old loss 3.417626430746168e-05 BETTER
I0313 02:20:06.150729 1987950 finetune.py:68] layer 0_down @ epoch 1 new loss 1.5232383248076076e-06 old loss 1.5263494788086973e-06 BETTER
I0313 02:20:08.268006 1989071 finetune.py:68] layer 1_down @ epoch 0 new loss 3.0325700208777562e-05 old loss 3.05112171190558e-05 BETTER
I0313 02:20:22.285489 1991347 finetune.py:68] layer 3_gate @ epoch 1 new loss 7.603743870276958e-05 old loss 7.625539728906006e-05 BETTER
I0313 02:20:32.979371 1990119 finetune.py:68] layer 2_gate @ epoch 4 new loss 3.4055821743095294e-05 old loss 3.4111446439055726e-05 BETTER
I0313 02:20:34.339068 1987950 finetune.py:68] layer 0_down @ epoch 2 new loss 1.5211221580102574e-06 old loss 1.5232383248076076e-06 BETTER
I0313 02:20:34.606589 1989071 finetune.py:68] layer 1_down @ epoch 1 new loss 3.0219809559639543e-05 old loss 3.0325700208777562e-05 BETTER
I0313 02:20:50.170753 1991347 finetune.py:68] layer 3_gate @ epoch 2 new loss 7.586355059174821e-05 old loss 7.603743870276958e-05 BETTER
I0313 02:21:01.099499 1989071 finetune.py:68] layer 1_down @ epoch 2 new loss 3.0178283850545995e-05 old loss 3.0219809559639543e-05 BETTER
I0313 02:21:02.719498 1987950 finetune.py:68] layer 0_down @ epoch 3 new loss 1.519620695944468e-06 old loss 1.5211221580102574e-06 BETTER
I0313 02:21:14.913109 1990119 finetune.py:45] layer 2_down initial loss 5.365052857086994e-05
I0313 02:21:18.282530 1991347 finetune.py:68] layer 3_gate @ epoch 3 new loss 7.57201632950455e-05 old loss 7.586355059174821e-05 BETTER
I0313 02:21:27.873745 1989071 finetune.py:76] layer 1_down @ epoch 3 new loss 3.02068601740757e-05 old loss 3.0178283850545995e-05 WORSE
I0313 02:21:30.965234 1987950 finetune.py:68] layer 0_down @ epoch 4 new loss 1.5184612038865453e-06 old loss 1.519620695944468e-06 BETTER
0_v proxy err 0.00980015005916357 tr(WHW.T) 4.225186347961426
0_q proxy err 2.210937236668542e-05 tr(WHW.T) 2710.45751953125
0_k proxy err 3.169892806909047e-05 tr(WHW.T) 1698.7501220703125
0_o proxy err 0.001019088551402092 tr(WHW.T) 0.9687241911888123
0_up proxy err 0.01019021775573492 tr(WHW.T) 43.27389907836914
0_gate proxy err 0.006972184404730797 tr(WHW.T) 63.442996978759766
0_down proxy err 0.00610636780038476 tr(WHW.T) 0.6576115489006042
I0313 02:21:41.371716 1990119 finetune.py:68] layer 2_down @ epoch 0 new loss 5.362524461816065e-05 old loss 5.365052857086994e-05 BETTER
I0313 02:21:46.870419 1991347 finetune.py:68] layer 3_gate @ epoch 4 new loss 7.559471850981936e-05 old loss 7.57201632950455e-05 BETTER
I0313 02:21:54.783385 1989071 finetune.py:68] layer 1_down @ epoch 4 new loss 3.0126586352707818e-05 old loss 3.0178283850545995e-05 BETTER
1_v proxy err 0.02642023004591465 tr(WHW.T) 16.465883255004883
1_q proxy err 0.00010706015746109188 tr(WHW.T) 4778.5966796875
1_k proxy err 0.00010488845146028325 tr(WHW.T) 4996.0341796875
1_o proxy err 0.018868230283260345 tr(WHW.T) 1.1122004985809326
1_up proxy err 0.01442593801766634 tr(WHW.T) 109.73991394042969
1_gate proxy err 0.007256765384227037 tr(WHW.T) 221.26754760742188
1_down proxy err 0.00018243800150230527 tr(WHW.T) 2042.3294677734375
I0313 02:22:08.188987 1990119 finetune.py:68] layer 2_down @ epoch 1 new loss 5.360946670407429e-05 old loss 5.362524461816065e-05 BETTER
I0313 02:22:27.816806 1991347 finetune.py:45] layer 3_down initial loss 0.0001165738285635598
I0313 02:22:34.840156 1990119 finetune.py:68] layer 2_down @ epoch 2 new loss 5.3598567319568247e-05 old loss 5.360946670407429e-05 BETTER
I0313 02:22:52.640534 1991347 finetune.py:68] layer 3_down @ epoch 0 new loss 0.0001165227877208963 old loss 0.0001165738285635598 BETTER
I0313 02:23:01.611896 1990119 finetune.py:68] layer 2_down @ epoch 3 new loss 5.3590738389175385e-05 old loss 5.3598567319568247e-05 BETTER
I0313 02:23:13.464422 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 4 in 73.61947512626648s
I0313 02:23:16.935437 1991729 config.py:54] PyTorch version 2.1.1 available.
I0313 02:23:18.059111 1985945 quantize_finetune_llama.py:184] layer 5 gpu 1
I0313 02:23:18.129933 1991729 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 02:23:18.542246 1991347 finetune.py:68] layer 3_down @ epoch 1 new loss 0.00011648962390609086 old loss 0.0001165227877208963 BETTER
I0313 02:23:28.539345 1990119 finetune.py:68] layer 2_down @ epoch 4 new loss 5.358489943319e-05 old loss 5.3590738389175385e-05 BETTER
2_v proxy err 0.017010115087032318 tr(WHW.T) 136.67332458496094
2_q proxy err 0.0003417187253944576 tr(WHW.T) 7749.8310546875
2_k proxy err 0.0002618407888803631 tr(WHW.T) 10205.7177734375
2_o proxy err 0.02852557972073555 tr(WHW.T) 1.46206796169281
2_up proxy err 0.020702166482806206 tr(WHW.T) 193.44711303710938
2_gate proxy err 0.013298201374709606 tr(WHW.T) 306.4130554199219
2_down proxy err 0.027394097298383713 tr(WHW.T) 3.018242597579956
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:23:35.538039 1991729 finetune.py:45] layer 4_v initial loss 6.71806265017949e-05
I0313 02:23:44.516040 1991347 finetune.py:68] layer 3_down @ epoch 2 new loss 0.00011646539496723562 old loss 0.00011648962390609086 BETTER
I0313 02:24:08.369227 1991729 finetune.py:68] layer 4_v @ epoch 0 new loss 2.958037111966405e-05 old loss 6.71806265017949e-05 BETTER
I0313 02:24:10.765048 1991347 finetune.py:68] layer 3_down @ epoch 3 new loss 0.00011644676851574332 old loss 0.00011646539496723562 BETTER
I0313 02:24:37.158278 1991347 finetune.py:68] layer 3_down @ epoch 4 new loss 0.0001164325003628619 old loss 0.00011644676851574332 BETTER
3_v proxy err 0.028541048988699913 tr(WHW.T) 284.77557373046875
3_q proxy err 0.0012847298057749867 tr(WHW.T) 7217.0810546875
3_k proxy err 0.0009284138795919716 tr(WHW.T) 10076.7587890625
3_o proxy err 0.024605954065918922 tr(WHW.T) 3.362360715866089
3_up proxy err 0.0258343368768692 tr(WHW.T) 284.74493408203125
3_gate proxy err 0.015686748549342155 tr(WHW.T) 477.6507568359375
3_down proxy err 0.02858733758330345 tr(WHW.T) 6.156436443328857
I0313 02:24:42.624949 1991729 finetune.py:68] layer 4_v @ epoch 1 new loss 2.236938053101767e-05 old loss 2.958037111966405e-05 BETTER
I0313 02:24:43.467099 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 5 in 69.97060823440552s
I0313 02:24:46.678882 1991845 config.py:54] PyTorch version 2.1.1 available.
I0313 02:24:47.627409 1985945 quantize_finetune_llama.py:184] layer 6 gpu 2
I0313 02:24:47.693411 1991845 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:25:03.928736 1991845 finetune.py:45] layer 5_v initial loss 9.650945867178962e-05
I0313 02:25:17.285322 1991729 finetune.py:68] layer 4_v @ epoch 2 new loss 2.041852167167235e-05 old loss 2.236938053101767e-05 BETTER
I0313 02:25:35.150918 1991845 finetune.py:68] layer 5_v @ epoch 0 new loss 4.895278834737837e-05 old loss 9.650945867178962e-05 BETTER
I0313 02:25:51.981177 1991729 finetune.py:68] layer 4_v @ epoch 3 new loss 1.9606393834692426e-05 old loss 2.041852167167235e-05 BETTER
I0313 02:25:56.935215 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 6 in 68.93873405456543s
I0313 02:26:00.027880 1991961 config.py:54] PyTorch version 2.1.1 available.
I0313 02:26:01.022265 1985945 quantize_finetune_llama.py:184] layer 7 gpu 3
I0313 02:26:01.088574 1991961 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 02:26:07.281180 1991845 finetune.py:68] layer 5_v @ epoch 1 new loss 4.116813579457812e-05 old loss 4.895278834737837e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:26:18.017100 1991961 finetune.py:45] layer 6_v initial loss 0.00014721152547281235
I0313 02:26:26.950189 1991729 finetune.py:68] layer 4_v @ epoch 4 new loss 1.913897540362086e-05 old loss 1.9606393834692426e-05 BETTER
I0313 02:26:39.452901 1991845 finetune.py:68] layer 5_v @ epoch 2 new loss 3.8852136640343815e-05 old loss 4.116813579457812e-05 BETTER
I0313 02:26:45.101293 1991729 finetune.py:45] layer 4_q initial loss 2.2416188585339114e-05
I0313 02:26:49.389963 1991961 finetune.py:68] layer 6_v @ epoch 0 new loss 6.496364949271083e-05 old loss 0.00014721152547281235 BETTER
I0313 02:27:11.558027 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 7 in 70.01697516441345s
I0313 02:27:12.363131 1991845 finetune.py:68] layer 5_v @ epoch 3 new loss 3.767881207750179e-05 old loss 3.8852136640343815e-05 BETTER
I0313 02:27:14.901190 1992077 config.py:54] PyTorch version 2.1.1 available.
I0313 02:27:15.930122 1985945 quantize_finetune_llama.py:184] layer 8 gpu 0
I0313 02:27:15.995325 1992077 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 02:27:18.172627 1991729 finetune.py:68] layer 4_q @ epoch 0 new loss 2.1761972675449215e-05 old loss 2.2416188585339114e-05 BETTER
I0313 02:27:21.748576 1991961 finetune.py:68] layer 6_v @ epoch 1 new loss 5.4540010751225054e-05 old loss 6.496364949271083e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:27:32.953146 1992077 finetune.py:45] layer 7_v initial loss 0.0001720751606626436
I0313 02:27:45.333249 1991845 finetune.py:68] layer 5_v @ epoch 4 new loss 3.692358222906478e-05 old loss 3.767881207750179e-05 BETTER
I0313 02:27:52.270840 1991729 finetune.py:68] layer 4_q @ epoch 1 new loss 2.1411369743873365e-05 old loss 2.1761972675449215e-05 BETTER
I0313 02:27:54.301367 1991961 finetune.py:68] layer 6_v @ epoch 2 new loss 5.134840102982707e-05 old loss 5.4540010751225054e-05 BETTER
I0313 02:28:02.712774 1991845 finetune.py:45] layer 5_q initial loss 4.1894640162354335e-05
I0313 02:28:03.804519 1992077 finetune.py:68] layer 7_v @ epoch 0 new loss 8.71798547450453e-05 old loss 0.0001720751606626436 BETTER
I0313 02:28:26.523195 1991729 finetune.py:68] layer 4_q @ epoch 2 new loss 2.1143065168871544e-05 old loss 2.1411369743873365e-05 BETTER
I0313 02:28:27.051688 1991961 finetune.py:68] layer 6_v @ epoch 3 new loss 4.970836016582325e-05 old loss 5.134840102982707e-05 BETTER
I0313 02:28:34.177464 1991845 finetune.py:68] layer 5_q @ epoch 0 new loss 4.083984094904736e-05 old loss 4.1894640162354335e-05 BETTER
I0313 02:28:35.721678 1992077 finetune.py:68] layer 7_v @ epoch 1 new loss 7.678399560973048e-05 old loss 8.71798547450453e-05 BETTER
I0313 02:29:00.041747 1991961 finetune.py:68] layer 6_v @ epoch 4 new loss 4.863298454438336e-05 old loss 4.970836016582325e-05 BETTER
I0313 02:29:00.866363 1991729 finetune.py:68] layer 4_q @ epoch 3 new loss 2.0920462702633813e-05 old loss 2.1143065168871544e-05 BETTER
I0313 02:29:06.266682 1991845 finetune.py:68] layer 5_q @ epoch 1 new loss 4.0213155443780124e-05 old loss 4.083984094904736e-05 BETTER
I0313 02:29:07.814705 1992077 finetune.py:68] layer 7_v @ epoch 2 new loss 7.331695087486878e-05 old loss 7.678399560973048e-05 BETTER
I0313 02:29:17.555248 1991961 finetune.py:45] layer 6_q initial loss 5.941800191067159e-05
I0313 02:29:35.134001 1991729 finetune.py:68] layer 4_q @ epoch 4 new loss 2.0733406927320175e-05 old loss 2.0920462702633813e-05 BETTER
I0313 02:29:38.418347 1991845 finetune.py:68] layer 5_q @ epoch 2 new loss 3.9732269215164706e-05 old loss 4.0213155443780124e-05 BETTER
I0313 02:29:39.923730 1992077 finetune.py:68] layer 7_v @ epoch 3 new loss 7.136658678064123e-05 old loss 7.331695087486878e-05 BETTER
I0313 02:29:48.985357 1991961 finetune.py:68] layer 6_q @ epoch 0 new loss 5.761057400377467e-05 old loss 5.941800191067159e-05 BETTER
I0313 02:29:53.145830 1991729 finetune.py:45] layer 4_k initial loss 2.3349301045527682e-05
I0313 02:30:10.951287 1991845 finetune.py:68] layer 5_q @ epoch 3 new loss 3.9317284972639754e-05 old loss 3.9732269215164706e-05 BETTER
I0313 02:30:11.953283 1992077 finetune.py:68] layer 7_v @ epoch 4 new loss 7.006199302850291e-05 old loss 7.136658678064123e-05 BETTER
I0313 02:30:21.176589 1991961 finetune.py:68] layer 6_q @ epoch 1 new loss 5.6669941841391847e-05 old loss 5.761057400377467e-05 BETTER
I0313 02:30:25.558934 1991729 finetune.py:68] layer 4_k @ epoch 0 new loss 2.3054401026456617e-05 old loss 2.3349301045527682e-05 BETTER
I0313 02:30:29.361739 1992077 finetune.py:45] layer 7_q initial loss 8.582149894209579e-05
I0313 02:30:43.778619 1991845 finetune.py:68] layer 5_q @ epoch 4 new loss 3.8952272007009014e-05 old loss 3.9317284972639754e-05 BETTER
I0313 02:30:53.647010 1991961 finetune.py:68] layer 6_q @ epoch 2 new loss 5.5940119636943564e-05 old loss 5.6669941841391847e-05 BETTER
I0313 02:30:59.371072 1991729 finetune.py:68] layer 4_k @ epoch 1 new loss 2.2882204575580545e-05 old loss 2.3054401026456617e-05 BETTER
I0313 02:31:00.585892 1992077 finetune.py:68] layer 7_q @ epoch 0 new loss 8.317722677020356e-05 old loss 8.582149894209579e-05 BETTER
I0313 02:31:01.530032 1991845 finetune.py:45] layer 5_k initial loss 4.199973773211241e-05
I0313 02:31:26.016717 1991961 finetune.py:68] layer 6_q @ epoch 3 new loss 5.53334757569246e-05 old loss 5.5940119636943564e-05 BETTER
I0313 02:31:32.653384 1991845 finetune.py:68] layer 5_k @ epoch 0 new loss 4.157091461820528e-05 old loss 4.199973773211241e-05 BETTER
I0313 02:31:32.793150 1992077 finetune.py:68] layer 7_q @ epoch 1 new loss 8.186618651961908e-05 old loss 8.317722677020356e-05 BETTER
I0313 02:31:32.965406 1991729 finetune.py:68] layer 4_k @ epoch 2 new loss 2.2739386622561142e-05 old loss 2.2882204575580545e-05 BETTER
I0313 02:31:58.506485 1991961 finetune.py:68] layer 6_q @ epoch 4 new loss 5.4819134675199166e-05 old loss 5.53334757569246e-05 BETTER
I0313 02:32:04.592499 1991845 finetune.py:68] layer 5_k @ epoch 1 new loss 4.1257582779508084e-05 old loss 4.157091461820528e-05 BETTER
I0313 02:32:04.843695 1992077 finetune.py:68] layer 7_q @ epoch 2 new loss 8.081956912064925e-05 old loss 8.186618651961908e-05 BETTER
I0313 02:32:06.873958 1991729 finetune.py:68] layer 4_k @ epoch 3 new loss 2.261108602397144e-05 old loss 2.2739386622561142e-05 BETTER
I0313 02:32:15.894363 1991961 finetune.py:45] layer 6_k initial loss 6.19438142166473e-05
I0313 02:32:36.581520 1991845 finetune.py:68] layer 5_k @ epoch 2 new loss 4.0988023101817816e-05 old loss 4.1257582779508084e-05 BETTER
I0313 02:32:36.873748 1992077 finetune.py:68] layer 7_q @ epoch 3 new loss 7.994029874680564e-05 old loss 8.081956912064925e-05 BETTER
I0313 02:32:40.775209 1991729 finetune.py:68] layer 4_k @ epoch 4 new loss 2.2496882593259215e-05 old loss 2.261108602397144e-05 BETTER
I0313 02:32:47.291970 1991961 finetune.py:68] layer 6_k @ epoch 0 new loss 6.112910341471434e-05 old loss 6.19438142166473e-05 BETTER
I0313 02:32:58.568706 1991729 finetune.py:45] layer 4_o initial loss 6.443876918638125e-05
I0313 02:33:08.552708 1991845 finetune.py:68] layer 5_k @ epoch 3 new loss 4.074499156558886e-05 old loss 4.0988023101817816e-05 BETTER
I0313 02:33:08.864706 1992077 finetune.py:68] layer 7_q @ epoch 4 new loss 7.919631025288254e-05 old loss 7.994029874680564e-05 BETTER
I0313 02:33:19.341226 1991961 finetune.py:68] layer 6_k @ epoch 1 new loss 6.068669608794153e-05 old loss 6.112910341471434e-05 BETTER
I0313 02:33:26.295629 1992077 finetune.py:45] layer 7_k initial loss 9.069406951311976e-05
I0313 02:33:30.360020 1991729 finetune.py:68] layer 4_o @ epoch 0 new loss 6.190511339809746e-05 old loss 6.443876918638125e-05 BETTER
I0313 02:33:40.596177 1991845 finetune.py:68] layer 5_k @ epoch 4 new loss 4.053115480928682e-05 old loss 4.074499156558886e-05 BETTER
I0313 02:33:51.352115 1991961 finetune.py:68] layer 6_k @ epoch 2 new loss 6.0302496422082186e-05 old loss 6.068669608794153e-05 BETTER
I0313 02:33:56.888985 1992077 finetune.py:68] layer 7_k @ epoch 0 new loss 8.904658170649782e-05 old loss 9.069406951311976e-05 BETTER
I0313 02:33:58.653961 1991845 finetune.py:45] layer 5_o initial loss 0.00010450083937030286
I0313 02:34:03.553994 1991729 finetune.py:68] layer 4_o @ epoch 1 new loss 6.0919817769899964e-05 old loss 6.190511339809746e-05 BETTER
I0313 02:34:23.403384 1991961 finetune.py:68] layer 6_k @ epoch 3 new loss 5.997709740768187e-05 old loss 6.0302496422082186e-05 BETTER
I0313 02:34:28.379714 1992077 finetune.py:68] layer 7_k @ epoch 1 new loss 8.83281827555038e-05 old loss 8.904658170649782e-05 BETTER
I0313 02:34:29.007083 1991845 finetune.py:68] layer 5_o @ epoch 0 new loss 0.00010044515511253849 old loss 0.00010450083937030286 BETTER
I0313 02:34:36.818043 1991729 finetune.py:68] layer 4_o @ epoch 2 new loss 6.036816193955019e-05 old loss 6.0919817769899964e-05 BETTER
I0313 02:34:55.392139 1991961 finetune.py:68] layer 6_k @ epoch 4 new loss 5.969262929284014e-05 old loss 5.997709740768187e-05 BETTER
I0313 02:34:59.961938 1992077 finetune.py:68] layer 7_k @ epoch 2 new loss 8.772654109634459e-05 old loss 8.83281827555038e-05 BETTER
I0313 02:35:00.178256 1991845 finetune.py:68] layer 5_o @ epoch 1 new loss 9.898095595417544e-05 old loss 0.00010044515511253849 BETTER
I0313 02:35:10.341369 1991729 finetune.py:68] layer 4_o @ epoch 3 new loss 5.997333937557414e-05 old loss 6.036816193955019e-05 BETTER
I0313 02:35:13.341410 1991961 finetune.py:45] layer 6_o initial loss 0.0001553534675622359
I0313 02:35:31.464585 1991845 finetune.py:68] layer 5_o @ epoch 2 new loss 9.806311572901905e-05 old loss 9.898095595417544e-05 BETTER
I0313 02:35:31.496172 1992077 finetune.py:68] layer 7_k @ epoch 3 new loss 8.722425991436467e-05 old loss 8.772654109634459e-05 BETTER
I0313 02:35:44.067613 1991961 finetune.py:68] layer 6_o @ epoch 0 new loss 0.0001480448991060257 old loss 0.0001553534675622359 BETTER
I0313 02:35:44.073202 1991729 finetune.py:68] layer 4_o @ epoch 4 new loss 5.964942465652712e-05 old loss 5.997333937557414e-05 BETTER
I0313 02:36:03.028197 1991845 finetune.py:68] layer 5_o @ epoch 3 new loss 9.733373735798523e-05 old loss 9.806311572901905e-05 BETTER
I0313 02:36:03.250589 1992077 finetune.py:68] layer 7_k @ epoch 4 new loss 8.677323057781905e-05 old loss 8.722425991436467e-05 BETTER
I0313 02:36:07.029285 1991729 finetune.py:45] layer 4_up initial loss 0.00010209362517343834
I0313 02:36:15.467291 1991961 finetune.py:68] layer 6_o @ epoch 1 new loss 0.00014601345174014568 old loss 0.0001480448991060257 BETTER
I0313 02:36:20.658807 1992077 finetune.py:45] layer 7_o initial loss 0.00021245052630547434
I0313 02:36:34.288913 1991845 finetune.py:68] layer 5_o @ epoch 4 new loss 9.672228043200448e-05 old loss 9.733373735798523e-05 BETTER
I0313 02:36:37.542109 1991729 finetune.py:68] layer 4_up @ epoch 0 new loss 0.00010135983757209033 old loss 0.00010209362517343834 BETTER
I0313 02:36:46.857064 1991961 finetune.py:68] layer 6_o @ epoch 2 new loss 0.00014476245269179344 old loss 0.00014601345174014568 BETTER
I0313 02:36:50.779144 1992077 finetune.py:68] layer 7_o @ epoch 0 new loss 0.00020383342052809894 old loss 0.00021245052630547434 BETTER
I0313 02:36:58.441808 1991845 finetune.py:45] layer 5_up initial loss 0.0001641747949179262
I0313 02:37:09.450465 1991729 finetune.py:68] layer 4_up @ epoch 1 new loss 0.00010088203271152452 old loss 0.00010135983757209033 BETTER
I0313 02:37:19.077826 1991961 finetune.py:68] layer 6_o @ epoch 3 new loss 0.00014377431944012642 old loss 0.00014476245269179344 BETTER
I0313 02:37:22.774787 1992077 finetune.py:68] layer 7_o @ epoch 1 new loss 0.00020113274513278157 old loss 0.00020383342052809894 BETTER
I0313 02:37:27.493115 1991845 finetune.py:68] layer 5_up @ epoch 0 new loss 0.00016299457638524473 old loss 0.0001641747949179262 BETTER
I0313 02:37:41.123569 1991729 finetune.py:68] layer 4_up @ epoch 2 new loss 0.000100510056654457 old loss 0.00010088203271152452 BETTER
I0313 02:37:51.316916 1991961 finetune.py:68] layer 6_o @ epoch 4 new loss 0.00014293928688857704 old loss 0.00014377431944012642 BETTER
I0313 02:37:54.766568 1992077 finetune.py:68] layer 7_o @ epoch 2 new loss 0.00019927590619772673 old loss 0.00020113274513278157 BETTER
I0313 02:37:57.223608 1991845 finetune.py:68] layer 5_up @ epoch 1 new loss 0.00016219499229919165 old loss 0.00016299457638524473 BETTER
I0313 02:38:13.939454 1991729 finetune.py:68] layer 4_up @ epoch 3 new loss 0.00010018681496148929 old loss 0.000100510056654457 BETTER
I0313 02:38:16.985009 1991961 finetune.py:45] layer 6_up initial loss 0.0002481331175658852
I0313 02:38:25.944699 1992077 finetune.py:68] layer 7_o @ epoch 3 new loss 0.00019777953275479376 old loss 0.00019927590619772673 BETTER
I0313 02:38:27.252313 1991845 finetune.py:68] layer 5_up @ epoch 2 new loss 0.0001615188957657665 old loss 0.00016219499229919165 BETTER
I0313 02:38:45.729452 1991729 finetune.py:68] layer 4_up @ epoch 4 new loss 9.989343379857019e-05 old loss 0.00010018681496148929 BETTER
I0313 02:38:45.955484 1991961 finetune.py:68] layer 6_up @ epoch 0 new loss 0.00024614715948700905 old loss 0.0002481331175658852 BETTER
I0313 02:38:56.903928 1992077 finetune.py:68] layer 7_o @ epoch 4 new loss 0.00019648519810289145 old loss 0.00019777953275479376 BETTER
I0313 02:38:56.986315 1991845 finetune.py:68] layer 5_up @ epoch 3 new loss 0.00016090748249553144 old loss 0.0001615188957657665 BETTER
I0313 02:39:08.492074 1991729 finetune.py:45] layer 4_gate initial loss 0.00012784465798176825
I0313 02:39:15.612427 1991961 finetune.py:68] layer 6_up @ epoch 1 new loss 0.00024486175971105695 old loss 0.00024614715948700905 BETTER
I0313 02:39:19.741159 1992077 finetune.py:45] layer 7_up initial loss 0.000339565915055573
I0313 02:39:26.715192 1991845 finetune.py:68] layer 5_up @ epoch 4 new loss 0.0001603542477823794 old loss 0.00016090748249553144 BETTER
I0313 02:39:37.309125 1991729 finetune.py:68] layer 4_gate @ epoch 0 new loss 0.0001273115340154618 old loss 0.00012784465798176825 BETTER
I0313 02:39:45.442673 1991961 finetune.py:68] layer 6_up @ epoch 2 new loss 0.0002437777875456959 old loss 0.00024486175971105695 BETTER
I0313 02:39:48.174457 1992077 finetune.py:68] layer 7_up @ epoch 0 new loss 0.00033639511093497276 old loss 0.000339565915055573 BETTER
I0313 02:39:49.756282 1991845 finetune.py:45] layer 5_gate initial loss 0.00020233937539160252
I0313 02:40:07.000440 1991729 finetune.py:68] layer 4_gate @ epoch 1 new loss 0.0001269082131329924 old loss 0.0001273115340154618 BETTER
I0313 02:40:15.392325 1991961 finetune.py:68] layer 6_up @ epoch 3 new loss 0.00024279749777633697 old loss 0.0002437777875456959 BETTER
I0313 02:40:17.218376 1991845 finetune.py:68] layer 5_gate @ epoch 0 new loss 0.00020145582675468177 old loss 0.00020233937539160252 BETTER
I0313 02:40:17.603637 1992077 finetune.py:68] layer 7_up @ epoch 1 new loss 0.0003344104334246367 old loss 0.00033639511093497276 BETTER
I0313 02:40:36.740442 1991729 finetune.py:68] layer 4_gate @ epoch 2 new loss 0.0001265704195247963 old loss 0.0001269082131329924 BETTER
I0313 02:40:45.426407 1991961 finetune.py:68] layer 6_up @ epoch 4 new loss 0.00024190379190258682 old loss 0.00024279749777633697 BETTER
I0313 02:40:45.570782 1991845 finetune.py:68] layer 5_gate @ epoch 1 new loss 0.0002007996226893738 old loss 0.00020145582675468177 BETTER
I0313 02:40:47.220240 1992077 finetune.py:68] layer 7_up @ epoch 2 new loss 0.0003327084123156965 old loss 0.0003344104334246367 BETTER
I0313 02:41:06.694181 1991729 finetune.py:68] layer 4_gate @ epoch 3 new loss 0.00012627318210434169 old loss 0.0001265704195247963 BETTER
I0313 02:41:08.534712 1991961 finetune.py:45] layer 6_gate initial loss 0.0003018666466232389
I0313 02:41:13.723283 1991845 finetune.py:68] layer 5_gate @ epoch 2 new loss 0.00020023553224746138 old loss 0.0002007996226893738 BETTER
I0313 02:41:16.954467 1992077 finetune.py:68] layer 7_up @ epoch 3 new loss 0.00033117938437499106 old loss 0.0003327084123156965 BETTER
I0313 02:41:36.126694 1991961 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.00030047239852137864 old loss 0.0003018666466232389 BETTER
I0313 02:41:36.611897 1991729 finetune.py:68] layer 4_gate @ epoch 4 new loss 0.00012600913760252297 old loss 0.00012627318210434169 BETTER
I0313 02:41:41.915579 1991845 finetune.py:68] layer 5_gate @ epoch 3 new loss 0.00019972093286924064 old loss 0.00020023553224746138 BETTER
I0313 02:41:46.344262 1992077 finetune.py:68] layer 7_up @ epoch 4 new loss 0.00032977489172481 old loss 0.00033117938437499106 BETTER
I0313 02:42:04.140805 1991961 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.0002994419483002275 old loss 0.00030047239852137864 BETTER
I0313 02:42:08.671961 1992077 finetune.py:45] layer 7_gate initial loss 0.0004100592923350632
I0313 02:42:09.946419 1991845 finetune.py:68] layer 5_gate @ epoch 4 new loss 0.00019925717788282782 old loss 0.00019972093286924064 BETTER
I0313 02:42:17.291115 1991729 finetune.py:45] layer 4_down initial loss 0.00020200492872390896
I0313 02:42:32.191060 1991961 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.00029854095191694796 old loss 0.0002994419483002275 BETTER
I0313 02:42:35.515991 1992077 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.0004079887585248798 old loss 0.0004100592923350632 BETTER
I0313 02:42:43.735301 1991729 finetune.py:68] layer 4_down @ epoch 0 new loss 0.00020192487863823771 old loss 0.00020200492872390896 BETTER
I0313 02:42:50.522904 1991845 finetune.py:45] layer 5_down initial loss 0.00031299341935664415
I0313 02:43:00.302728 1991961 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.0002977327967528254 old loss 0.00029854095191694796 BETTER
I0313 02:43:03.238565 1992077 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.0004064812674187124 old loss 0.0004079887585248798 BETTER
I0313 02:43:11.713989 1991729 finetune.py:68] layer 4_down @ epoch 1 new loss 0.00020187071640975773 old loss 0.00020192487863823771 BETTER
I0313 02:43:15.815409 1991845 finetune.py:68] layer 5_down @ epoch 0 new loss 0.00031290898914448917 old loss 0.00031299341935664415 BETTER
I0313 02:43:28.553419 1991961 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.0002969919005408883 old loss 0.0002977327967528254 BETTER
I0313 02:43:31.208607 1992077 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.0004051467985846102 old loss 0.0004064812674187124 BETTER
I0313 02:43:40.053003 1991729 finetune.py:68] layer 4_down @ epoch 2 new loss 0.00020183311426080763 old loss 0.00020187071640975773 BETTER
I0313 02:43:42.236322 1991845 finetune.py:68] layer 5_down @ epoch 1 new loss 0.0003128504322376102 old loss 0.00031290898914448917 BETTER
I0313 02:43:59.221849 1992077 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.0004039504856336862 old loss 0.0004051467985846102 BETTER
I0313 02:44:08.455171 1991729 finetune.py:68] layer 4_down @ epoch 3 new loss 0.0002018065279116854 old loss 0.00020183311426080763 BETTER
I0313 02:44:08.928048 1991845 finetune.py:68] layer 5_down @ epoch 2 new loss 0.0003128076787106693 old loss 0.0003128504322376102 BETTER
I0313 02:44:09.988211 1991961 finetune.py:45] layer 6_down initial loss 0.00046494934940710664
I0313 02:44:27.329114 1992077 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.0004028618277516216 old loss 0.0004039504856336862 BETTER
I0313 02:44:35.656191 1991961 finetune.py:68] layer 6_down @ epoch 0 new loss 0.0004648268222808838 old loss 0.00046494934940710664 BETTER
I0313 02:44:35.760576 1991845 finetune.py:68] layer 5_down @ epoch 3 new loss 0.0003127781965304166 old loss 0.0003128076787106693 BETTER
I0313 02:44:36.999781 1991729 finetune.py:68] layer 4_down @ epoch 4 new loss 0.00020178740669507533 old loss 0.0002018065279116854 BETTER
4_v proxy err 0.02736581489443779 tr(WHW.T) 274.6131286621094
4_q proxy err 0.0012448383495211601 tr(WHW.T) 6916.568359375
4_k proxy err 0.0008297593449242413 tr(WHW.T) 10416.1494140625
4_o proxy err 0.028047757223248482 tr(WHW.T) 5.149250507354736
4_up proxy err 0.02532152459025383 tr(WHW.T) 397.3300476074219
4_gate proxy err 0.012592779472470284 tr(WHW.T) 819.22900390625
4_down proxy err 0.02828744240105152 tr(WHW.T) 11.651936531066895
I0313 02:45:03.212809 1991845 finetune.py:68] layer 5_down @ epoch 4 new loss 0.0003127558738924563 old loss 0.0003127781965304166 BETTER
I0313 02:45:03.260196 1991961 finetune.py:68] layer 6_down @ epoch 1 new loss 0.0004647405876312405 old loss 0.0004648268222808838 BETTER
5_v proxy err 0.029404155910015106 tr(WHW.T) 298.47540283203125
5_q proxy err 0.0014840896474197507 tr(WHW.T) 6773.1259765625
5_k proxy err 0.0009346416918560863 tr(WHW.T) 10841.765625
5_o proxy err 0.030401384457945824 tr(WHW.T) 7.954762935638428
5_up proxy err 0.024955112487077713 tr(WHW.T) 506.19720458984375
5_gate proxy err 0.011785765178501606 tr(WHW.T) 1100.60498046875
5_down proxy err 0.030977489426732063 tr(WHW.T) 15.777250289916992
I0313 02:45:08.701733 1992077 finetune.py:45] layer 7_down initial loss 0.00062580406665802
I0313 02:45:30.055183 1991961 finetune.py:68] layer 6_down @ epoch 2 new loss 0.00046467676293104887 old loss 0.0004647405876312405 BETTER
I0313 02:45:33.589760 1992077 finetune.py:68] layer 7_down @ epoch 0 new loss 0.0006256516207940876 old loss 0.00062580406665802 BETTER
I0313 02:45:56.888297 1991961 finetune.py:68] layer 6_down @ epoch 3 new loss 0.00046462766476906836 old loss 0.00046467676293104887 BETTER
I0313 02:45:59.499687 1992077 finetune.py:68] layer 7_down @ epoch 1 new loss 0.0006255409098230302 old loss 0.0006256516207940876 BETTER
I0313 02:46:19.286561 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 8 in 71.65739965438843s
I0313 02:46:22.703259 1997401 config.py:54] PyTorch version 2.1.1 available.
I0313 02:46:23.938939 1991961 finetune.py:68] layer 6_down @ epoch 4 new loss 0.00046459221630357206 old loss 0.00046462766476906836 BETTER
I0313 02:46:23.954610 1985945 quantize_finetune_llama.py:184] layer 9 gpu 1
I0313 02:46:24.025608 1997401 data_utils.py:336] using 256 training seqs, 128 validation seqs
6_v proxy err 0.03221985325217247 tr(WHW.T) 443.5464782714844
6_q proxy err 0.0021446789614856243 tr(WHW.T) 7578.072265625
6_k proxy err 0.0015658583724871278 tr(WHW.T) 10408.9072265625
6_o proxy err 0.03409627825021744 tr(WHW.T) 11.625321388244629
6_up proxy err 0.025366127490997314 tr(WHW.T) 616.672119140625
6_gate proxy err 0.010400526225566864 tr(WHW.T) 1552.008056640625
6_down proxy err 0.03130096569657326 tr(WHW.T) 23.177581787109375
I0313 02:46:25.799027 1992077 finetune.py:68] layer 7_down @ epoch 2 new loss 0.0006254580803215504 old loss 0.0006255409098230302 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:46:41.058913 1997401 finetune.py:45] layer 8_v initial loss 0.000226721735089086
I0313 02:46:51.890734 1992077 finetune.py:68] layer 7_down @ epoch 3 new loss 0.0006253978936001658 old loss 0.0006254580803215504 BETTER
I0313 02:47:13.842823 1997401 finetune.py:68] layer 8_v @ epoch 0 new loss 0.00012452961527742445 old loss 0.000226721735089086 BETTER
I0313 02:47:18.042736 1992077 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0006253530154936016 old loss 0.0006253978936001658 BETTER
7_v proxy err 0.032211508601903915 tr(WHW.T) 489.9357604980469
7_q proxy err 0.0023295702412724495 tr(WHW.T) 7675.6640625
7_k proxy err 0.0017545135924592614 tr(WHW.T) 10202.4775390625
7_o proxy err 0.03687133267521858 tr(WHW.T) 15.200474739074707
7_up proxy err 0.024620067328214645 tr(WHW.T) 735.4943237304688
7_gate proxy err 0.009961046278476715 tr(WHW.T) 1873.366943359375
7_down proxy err 0.03128955885767937 tr(WHW.T) 30.822547912597656
I0313 02:47:36.704164 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 9 in 68.93582034111023s
I0313 02:47:39.979910 1997628 config.py:54] PyTorch version 2.1.1 available.
I0313 02:47:40.989627 1985945 quantize_finetune_llama.py:184] layer 10 gpu 2
I0313 02:47:41.057536 1997628 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 02:47:48.093712 1997401 finetune.py:68] layer 8_v @ epoch 1 new loss 0.00011117468966403976 old loss 0.00012452961527742445 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:47:59.624985 1997628 finetune.py:45] layer 9_v initial loss 0.00019808307115454227
I0313 02:48:23.748123 1997401 finetune.py:68] layer 8_v @ epoch 2 new loss 0.00010641891276463866 old loss 0.00011117468966403976 BETTER
I0313 02:48:31.917500 1997628 finetune.py:68] layer 9_v @ epoch 0 new loss 0.00014621031004935503 old loss 0.00019808307115454227 BETTER
I0313 02:48:56.954974 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 10 in 75.5723991394043s
I0313 02:48:59.168673 1997401 finetune.py:68] layer 8_v @ epoch 3 new loss 0.00010370145901106298 old loss 0.00010641891276463866 BETTER
I0313 02:49:00.341881 1997861 config.py:54] PyTorch version 2.1.1 available.
I0313 02:49:01.495750 1985945 quantize_finetune_llama.py:184] layer 11 gpu 3
I0313 02:49:01.589605 1997861 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 02:49:05.683875 1997628 finetune.py:68] layer 9_v @ epoch 1 new loss 0.00013864052016288042 old loss 0.00014621031004935503 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:49:22.495011 1997861 finetune.py:45] layer 10_v initial loss 0.0002604794572107494
I0313 02:49:34.286519 1997401 finetune.py:68] layer 8_v @ epoch 4 new loss 0.00010177552758250386 old loss 0.00010370145901106298 BETTER
I0313 02:49:38.596145 1997628 finetune.py:68] layer 9_v @ epoch 2 new loss 0.00013494768063537776 old loss 0.00013864052016288042 BETTER
I0313 02:49:52.053597 1997401 finetune.py:45] layer 8_q initial loss 0.00012180790508864447
I0313 02:49:54.588974 1997861 finetune.py:68] layer 10_v @ epoch 0 new loss 0.00020984496222808957 old loss 0.0002604794572107494 BETTER
I0313 02:50:11.502856 1997628 finetune.py:68] layer 9_v @ epoch 3 new loss 0.00013242772547528148 old loss 0.00013494768063537776 BETTER
I0313 02:50:14.846613 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 11 in 72.90894317626953s
I0313 02:50:18.069004 1998091 config.py:54] PyTorch version 2.1.1 available.
I0313 02:50:19.098826 1985945 quantize_finetune_llama.py:184] layer 12 gpu 0
I0313 02:50:19.172483 1998091 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 02:50:25.121428 1997401 finetune.py:68] layer 8_q @ epoch 0 new loss 0.00011854207696160302 old loss 0.00012180790508864447 BETTER
I0313 02:50:27.423164 1997861 finetune.py:68] layer 10_v @ epoch 1 new loss 0.00020055663480889052 old loss 0.00020984496222808957 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 02:50:36.163898 1998091 finetune.py:45] layer 11_v initial loss 0.00027759381919167936
I0313 02:50:44.723889 1997628 finetune.py:68] layer 9_v @ epoch 4 new loss 0.00013047238462604582 old loss 0.00013242772547528148 BETTER
I0313 02:50:59.476679 1997401 finetune.py:68] layer 8_q @ epoch 1 new loss 0.00011666095088003203 old loss 0.00011854207696160302 BETTER
I0313 02:51:00.988890 1997861 finetune.py:68] layer 10_v @ epoch 2 new loss 0.00019510145648382604 old loss 0.00020055663480889052 BETTER
I0313 02:51:02.994713 1997628 finetune.py:45] layer 9_q initial loss 0.0001555517374072224
I0313 02:51:07.206217 1998091 finetune.py:68] layer 11_v @ epoch 0 new loss 0.00021465768804773688 old loss 0.00027759381919167936 BETTER
I0313 02:51:33.707531 1997401 finetune.py:68] layer 8_q @ epoch 2 new loss 0.0001151910619228147 old loss 0.00011666095088003203 BETTER
I0313 02:51:34.443186 1997861 finetune.py:68] layer 10_v @ epoch 3 new loss 0.00019111635629087687 old loss 0.00019510145648382604 BETTER
I0313 02:51:34.846331 1997628 finetune.py:68] layer 9_q @ epoch 0 new loss 0.000151842410559766 old loss 0.0001555517374072224 BETTER
I0313 02:51:39.177526 1998091 finetune.py:68] layer 11_v @ epoch 1 new loss 0.00020429361029528081 old loss 0.00021465768804773688 BETTER
I0313 02:52:07.836129 1997628 finetune.py:68] layer 9_q @ epoch 1 new loss 0.000149717612657696 old loss 0.000151842410559766 BETTER
I0313 02:52:08.331404 1997401 finetune.py:68] layer 8_q @ epoch 3 new loss 0.00011391890438972041 old loss 0.0001151910619228147 BETTER
I0313 02:52:08.473455 1997861 finetune.py:68] layer 10_v @ epoch 4 new loss 0.00018795511277858168 old loss 0.00019111635629087687 BETTER
I0313 02:52:11.291231 1998091 finetune.py:68] layer 11_v @ epoch 2 new loss 0.00019863923080265522 old loss 0.00020429361029528081 BETTER
I0313 02:52:26.901776 1997861 finetune.py:45] layer 10_q initial loss 0.00021753128385171294
I0313 02:52:40.922006 1997628 finetune.py:68] layer 9_q @ epoch 2 new loss 0.0001479665661463514 old loss 0.000149717612657696 BETTER
I0313 02:52:42.675761 1997401 finetune.py:68] layer 8_q @ epoch 4 new loss 0.00011283335334155709 old loss 0.00011391890438972041 BETTER
I0313 02:52:43.339874 1998091 finetune.py:68] layer 11_v @ epoch 3 new loss 0.00019472675921861082 old loss 0.00019863923080265522 BETTER
I0313 02:52:58.647394 1997861 finetune.py:68] layer 10_q @ epoch 0 new loss 0.0002121522993547842 old loss 0.00021753128385171294 BETTER
I0313 02:53:00.316787 1997401 finetune.py:45] layer 8_k initial loss 0.0001272240187972784
I0313 02:53:13.511503 1997628 finetune.py:68] layer 9_q @ epoch 3 new loss 0.00014648877549916506 old loss 0.0001479665661463514 BETTER
I0313 02:53:15.830474 1998091 finetune.py:68] layer 11_v @ epoch 4 new loss 0.00019172097381670028 old loss 0.00019472675921861082 BETTER
I0313 02:53:31.480218 1997861 finetune.py:68] layer 10_q @ epoch 1 new loss 0.00020899585797451437 old loss 0.0002121522993547842 BETTER
I0313 02:53:32.868370 1997401 finetune.py:68] layer 8_k @ epoch 0 new loss 0.00012568295642267913 old loss 0.0001272240187972784 BETTER
I0313 02:53:33.723159 1998091 finetune.py:45] layer 11_q initial loss 0.00022532120055984706
I0313 02:53:46.246713 1997628 finetune.py:68] layer 9_q @ epoch 4 new loss 0.00014514838403556496 old loss 0.00014648877549916506 BETTER
I0313 02:54:04.179487 1997628 finetune.py:45] layer 9_k initial loss 0.000163778560818173
I0313 02:54:04.922207 1997861 finetune.py:68] layer 10_q @ epoch 2 new loss 0.00020633287203963846 old loss 0.00020899585797451437 BETTER
I0313 02:54:05.394530 1998091 finetune.py:68] layer 11_q @ epoch 0 new loss 0.00022022258781362325 old loss 0.00022532120055984706 BETTER
I0313 02:54:06.422771 1997401 finetune.py:68] layer 8_k @ epoch 1 new loss 0.00012471017544157803 old loss 0.00012568295642267913 BETTER
I0313 02:54:35.681531 1997628 finetune.py:68] layer 9_k @ epoch 0 new loss 0.00016141821106430143 old loss 0.000163778560818173 BETTER
I0313 02:54:37.605251 1998091 finetune.py:68] layer 11_q @ epoch 1 new loss 0.00021721856319345534 old loss 0.00022022258781362325 BETTER
I0313 02:54:37.807388 1997861 finetune.py:68] layer 10_q @ epoch 3 new loss 0.00020401809888426214 old loss 0.00020633287203963846 BETTER
I0313 02:54:39.835743 1997401 finetune.py:68] layer 8_k @ epoch 2 new loss 0.00012388720642775297 old loss 0.00012471017544157803 BETTER
I0313 02:55:08.016809 1997628 finetune.py:68] layer 9_k @ epoch 1 new loss 0.00016021874034777284 old loss 0.00016141821106430143 BETTER
I0313 02:55:09.733097 1998091 finetune.py:68] layer 11_q @ epoch 2 new loss 0.00021473025844898075 old loss 0.00021721856319345534 BETTER
I0313 02:55:10.770691 1997861 finetune.py:68] layer 10_q @ epoch 4 new loss 0.00020197198318783194 old loss 0.00020401809888426214 BETTER
I0313 02:55:13.275459 1997401 finetune.py:68] layer 8_k @ epoch 3 new loss 0.00012316428183112293 old loss 0.00012388720642775297 BETTER
I0313 02:55:29.234256 1997861 finetune.py:45] layer 10_k initial loss 0.0002242798509541899
I0313 02:55:40.400492 1997628 finetune.py:68] layer 9_k @ epoch 2 new loss 0.00015918610733933747 old loss 0.00016021874034777284 BETTER
I0313 02:55:41.968332 1998091 finetune.py:68] layer 11_q @ epoch 3 new loss 0.00021258604829199612 old loss 0.00021473025844898075 BETTER
I0313 02:55:47.015085 1997401 finetune.py:68] layer 8_k @ epoch 4 new loss 0.00012253245222382247 old loss 0.00012316428183112293 BETTER
I0313 02:56:01.077452 1997861 finetune.py:68] layer 10_k @ epoch 0 new loss 0.00022096994507592171 old loss 0.0002242798509541899 BETTER
I0313 02:56:05.557056 1997401 finetune.py:45] layer 8_o initial loss 0.00029993747011758387
I0313 02:56:12.857118 1997628 finetune.py:68] layer 9_k @ epoch 3 new loss 0.0001582697150297463 old loss 0.00015918610733933747 BETTER
I0313 02:56:14.195706 1998091 finetune.py:68] layer 11_q @ epoch 4 new loss 0.0002107294712914154 old loss 0.00021258604829199612 BETTER
I0313 02:56:33.098660 1998091 finetune.py:45] layer 11_k initial loss 0.00023669740767218173
I0313 02:56:34.243514 1997861 finetune.py:68] layer 10_k @ epoch 1 new loss 0.00021901357104070485 old loss 0.00022096994507592171 BETTER
I0313 02:56:37.676110 1997401 finetune.py:68] layer 8_o @ epoch 0 new loss 0.0002885290014091879 old loss 0.00029993747011758387 BETTER
I0313 02:56:45.492002 1997628 finetune.py:68] layer 9_k @ epoch 4 new loss 0.0001574504713062197 old loss 0.0001582697150297463 BETTER
I0313 02:57:04.725184 1997628 finetune.py:45] layer 9_o initial loss 0.0003800195117946714
I0313 02:57:04.782734 1998091 finetune.py:68] layer 11_k @ epoch 0 new loss 0.00023336148296948522 old loss 0.00023669740767218173 BETTER
I0313 02:57:07.629450 1997861 finetune.py:68] layer 10_k @ epoch 2 new loss 0.00021735651534982026 old loss 0.00021901357104070485 BETTER
I0313 02:57:11.002985 1997401 finetune.py:68] layer 8_o @ epoch 1 new loss 0.00028479454340413213 old loss 0.0002885290014091879 BETTER
I0313 02:57:35.478810 1997628 finetune.py:68] layer 9_o @ epoch 0 new loss 0.00037120768683962524 old loss 0.0003800195117946714 BETTER
I0313 02:57:36.465999 1998091 finetune.py:68] layer 11_k @ epoch 1 new loss 0.00023159285774454474 old loss 0.00023336148296948522 BETTER
I0313 02:57:40.229960 1997861 finetune.py:68] layer 10_k @ epoch 3 new loss 0.00021588272647932172 old loss 0.00021735651534982026 BETTER
I0313 02:57:44.150457 1997401 finetune.py:68] layer 8_o @ epoch 2 new loss 0.00028217118233442307 old loss 0.00028479454340413213 BETTER
I0313 02:58:07.466203 1997628 finetune.py:68] layer 9_o @ epoch 1 new loss 0.0003674072795547545 old loss 0.00037120768683962524 BETTER
I0313 02:58:08.300724 1998091 finetune.py:68] layer 11_k @ epoch 2 new loss 0.00023010549193713814 old loss 0.00023159285774454474 BETTER
I0313 02:58:12.898827 1997861 finetune.py:68] layer 10_k @ epoch 4 new loss 0.00021452613873407245 old loss 0.00021588272647932172 BETTER
I0313 02:58:17.527065 1997401 finetune.py:68] layer 8_o @ epoch 3 new loss 0.0002800200891215354 old loss 0.00028217118233442307 BETTER
I0313 02:58:31.809636 1997861 finetune.py:45] layer 10_o initial loss 0.0005121484282426536
I0313 02:58:39.658480 1997628 finetune.py:68] layer 9_o @ epoch 2 new loss 0.00036442207056097686 old loss 0.0003674072795547545 BETTER
I0313 02:58:40.340857 1998091 finetune.py:68] layer 11_k @ epoch 3 new loss 0.00022882876510266215 old loss 0.00023010549193713814 BETTER
I0313 02:58:50.951709 1997401 finetune.py:68] layer 8_o @ epoch 4 new loss 0.0002781626826617867 old loss 0.0002800200891215354 BETTER
I0313 02:59:05.260100 1997861 finetune.py:68] layer 10_o @ epoch 0 new loss 0.000501192465890199 old loss 0.0005121484282426536 BETTER
I0313 02:59:14.113456 1997628 finetune.py:68] layer 9_o @ epoch 3 new loss 0.00036184105556458235 old loss 0.00036442207056097686 BETTER
I0313 02:59:14.689964 1998091 finetune.py:68] layer 11_k @ epoch 4 new loss 0.00022763079323340207 old loss 0.00022882876510266215 BETTER
I0313 02:59:20.745358 1997401 finetune.py:45] layer 8_up initial loss 0.0004478447081055492
I0313 02:59:37.061187 1998091 finetune.py:45] layer 11_o initial loss 0.0005343523225747049
I0313 02:59:39.631687 1997861 finetune.py:68] layer 10_o @ epoch 1 new loss 0.0004957085475325584 old loss 0.000501192465890199 BETTER
I0313 02:59:47.764256 1997628 finetune.py:68] layer 9_o @ epoch 4 new loss 0.00035955567727796733 old loss 0.00036184105556458235 BETTER
I0313 02:59:52.168631 1997401 finetune.py:68] layer 8_up @ epoch 0 new loss 0.0004436108283698559 old loss 0.0004478447081055492 BETTER
I0313 03:00:08.736146 1998091 finetune.py:68] layer 11_o @ epoch 0 new loss 0.0005234198761172593 old loss 0.0005343523225747049 BETTER
I0313 03:00:14.844018 1997861 finetune.py:68] layer 10_o @ epoch 2 new loss 0.0004912878503091633 old loss 0.0004957085475325584 BETTER
I0313 03:00:17.791432 1997628 finetune.py:45] layer 9_up initial loss 0.0005561035359278321
I0313 03:00:25.085520 1997401 finetune.py:68] layer 8_up @ epoch 1 new loss 0.0004409245157148689 old loss 0.0004436108283698559 BETTER
I0313 03:00:41.643733 1998091 finetune.py:68] layer 11_o @ epoch 1 new loss 0.0005182353779673576 old loss 0.0005234198761172593 BETTER
I0313 03:00:47.263581 1997628 finetune.py:68] layer 9_up @ epoch 0 new loss 0.0005513005307875574 old loss 0.0005561035359278321 BETTER
I0313 03:00:47.909871 1997861 finetune.py:68] layer 10_o @ epoch 3 new loss 0.0004874761216342449 old loss 0.0004912878503091633 BETTER
I0313 03:00:56.865707 1997401 finetune.py:68] layer 8_up @ epoch 2 new loss 0.00043860540608875453 old loss 0.0004409245157148689 BETTER
I0313 03:01:12.563937 1998091 finetune.py:68] layer 11_o @ epoch 2 new loss 0.0005140911671333015 old loss 0.0005182353779673576 BETTER
I0313 03:01:17.226279 1997628 finetune.py:68] layer 9_up @ epoch 1 new loss 0.0005481342668645084 old loss 0.0005513005307875574 BETTER
I0313 03:01:20.119480 1997861 finetune.py:68] layer 10_o @ epoch 4 new loss 0.00048412304022349417 old loss 0.0004874761216342449 BETTER
I0313 03:01:28.461063 1997401 finetune.py:68] layer 8_up @ epoch 3 new loss 0.00043653883039951324 old loss 0.00043860540608875453 BETTER
I0313 03:01:43.474489 1998091 finetune.py:68] layer 11_o @ epoch 3 new loss 0.0005105096497572958 old loss 0.0005140911671333015 BETTER
I0313 03:01:44.008490 1997861 finetune.py:45] layer 10_up initial loss 0.0007071387954056263
I0313 03:01:47.420136 1997628 finetune.py:68] layer 9_up @ epoch 2 new loss 0.00054531468776986 old loss 0.0005481342668645084 BETTER
I0313 03:02:00.074302 1997401 finetune.py:68] layer 8_up @ epoch 4 new loss 0.000434654182754457 old loss 0.00043653883039951324 BETTER
I0313 03:02:13.431056 1997861 finetune.py:68] layer 10_up @ epoch 0 new loss 0.0007008904940448701 old loss 0.0007071387954056263 BETTER
I0313 03:02:14.644116 1998091 finetune.py:68] layer 11_o @ epoch 4 new loss 0.0005074107903055847 old loss 0.0005105096497572958 BETTER
I0313 03:02:17.311323 1997628 finetune.py:68] layer 9_up @ epoch 3 new loss 0.0005427725263871253 old loss 0.00054531468776986 BETTER
I0313 03:02:23.374881 1997401 finetune.py:45] layer 8_gate initial loss 0.0005386186530813575
I0313 03:02:37.616716 1998091 finetune.py:45] layer 11_up initial loss 0.000765314616728574
I0313 03:02:43.942879 1997861 finetune.py:68] layer 10_up @ epoch 1 new loss 0.0006966865621507168 old loss 0.0007008904940448701 BETTER
I0313 03:02:47.551096 1997628 finetune.py:68] layer 9_up @ epoch 4 new loss 0.0005404638941399753 old loss 0.0005427725263871253 BETTER
I0313 03:02:52.234700 1997401 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.0005358037888072431 old loss 0.0005386186530813575 BETTER
I0313 03:03:06.501878 1998091 finetune.py:68] layer 11_up @ epoch 0 new loss 0.0007587250438518822 old loss 0.000765314616728574 BETTER
I0313 03:03:12.140818 1997628 finetune.py:45] layer 9_gate initial loss 0.0006654313765466213
I0313 03:03:15.045488 1997861 finetune.py:68] layer 10_up @ epoch 2 new loss 0.0006929597584530711 old loss 0.0006966865621507168 BETTER
I0313 03:03:22.080866 1997401 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.0005338090704753995 old loss 0.0005358037888072431 BETTER
I0313 03:03:36.139321 1998091 finetune.py:68] layer 11_up @ epoch 1 new loss 0.0007542833336628973 old loss 0.0007587250438518822 BETTER
I0313 03:03:39.760954 1997628 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0006623394438065588 old loss 0.0006654313765466213 BETTER
I0313 03:03:45.749155 1997861 finetune.py:68] layer 10_up @ epoch 3 new loss 0.0006896405830048025 old loss 0.0006929597584530711 BETTER
I0313 03:03:51.905028 1997401 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.0005321115022525191 old loss 0.0005338090704753995 BETTER
I0313 03:04:05.752171 1998091 finetune.py:68] layer 11_up @ epoch 2 new loss 0.0007504888344556093 old loss 0.0007542833336628973 BETTER
I0313 03:04:07.994935 1997628 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.0006600393098779023 old loss 0.0006623394438065588 BETTER
I0313 03:04:16.531244 1997861 finetune.py:68] layer 10_up @ epoch 4 new loss 0.000686561455950141 old loss 0.0006896405830048025 BETTER
I0313 03:04:21.834031 1997401 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.0005305694648995996 old loss 0.0005321115022525191 BETTER
I0313 03:04:35.700883 1998091 finetune.py:68] layer 11_up @ epoch 3 new loss 0.0007470653508789837 old loss 0.0007504888344556093 BETTER
I0313 03:04:36.650351 1997628 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.0006579893524758518 old loss 0.0006600393098779023 BETTER
I0313 03:04:41.503631 1997861 finetune.py:45] layer 10_gate initial loss 0.000835468468721956
I0313 03:04:51.976351 1997401 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.0005291611887514591 old loss 0.0005305694648995996 BETTER
I0313 03:05:05.484706 1997628 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.0006561358459293842 old loss 0.0006579893524758518 BETTER
I0313 03:05:05.531149 1998091 finetune.py:68] layer 11_up @ epoch 4 new loss 0.0007439014734700322 old loss 0.0007470653508789837 BETTER
I0313 03:05:09.624174 1997861 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.0008315354352816939 old loss 0.000835468468721956 BETTER
I0313 03:05:30.093021 1998091 finetune.py:45] layer 11_gate initial loss 0.0009170119883492589
I0313 03:05:36.307276 1997628 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0006544312927871943 old loss 0.0006561358459293842 BETTER
I0313 03:05:36.398190 1997401 finetune.py:45] layer 8_down initial loss 0.000796327949501574
I0313 03:05:40.208900 1997861 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.000828598509542644 old loss 0.0008315354352816939 BETTER
I0313 03:05:57.069455 1998091 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.0009127022931352258 old loss 0.0009170119883492589 BETTER
I0313 03:06:02.872662 1997401 finetune.py:68] layer 8_down @ epoch 0 new loss 0.0007961566443555057 old loss 0.000796327949501574 BETTER
I0313 03:06:09.345913 1997861 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.000826001341920346 old loss 0.000828598509542644 BETTER
I0313 03:06:20.705347 1997628 finetune.py:45] layer 9_down initial loss 0.0009663283126428723
I0313 03:06:25.629938 1998091 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.0009095586137846112 old loss 0.0009127022931352258 BETTER
I0313 03:06:30.921169 1997401 finetune.py:68] layer 8_down @ epoch 1 new loss 0.0007960280054248869 old loss 0.0007961566443555057 BETTER
I0313 03:06:38.343974 1997861 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.0008235993445850909 old loss 0.000826001341920346 BETTER
I0313 03:06:47.646447 1997628 finetune.py:68] layer 9_down @ epoch 0 new loss 0.0009661441436037421 old loss 0.0009663283126428723 BETTER
I0313 03:06:54.513910 1998091 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.0009067589417099953 old loss 0.0009095586137846112 BETTER
I0313 03:06:59.408006 1997401 finetune.py:68] layer 8_down @ epoch 2 new loss 0.0007959333597682416 old loss 0.0007960280054248869 BETTER
I0313 03:07:07.432909 1997861 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.0008213917026296258 old loss 0.0008235993445850909 BETTER
I0313 03:07:14.556463 1997628 finetune.py:68] layer 9_down @ epoch 1 new loss 0.000966005667578429 old loss 0.0009661441436037421 BETTER
I0313 03:07:23.104297 1998091 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.0009042049641720951 old loss 0.0009067589417099953 BETTER
I0313 03:07:28.052049 1997401 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0007958606583997607 old loss 0.0007959333597682416 BETTER
I0313 03:07:41.697814 1997628 finetune.py:68] layer 9_down @ epoch 2 new loss 0.0009659016504883766 old loss 0.000966005667578429 BETTER
I0313 03:07:53.340197 1997861 finetune.py:45] layer 10_down initial loss 0.0011827745474874973
I0313 03:07:53.394335 1998091 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.0009018699056468904 old loss 0.0009042049641720951 BETTER
I0313 03:07:57.726575 1997401 finetune.py:68] layer 8_down @ epoch 4 new loss 0.0007958051282912493 old loss 0.0007958606583997607 BETTER
8_v proxy err 0.0291436817497015 tr(WHW.T) 530.9967041015625
8_q proxy err 0.002418937860056758 tr(WHW.T) 7233.814453125
8_k proxy err 0.001647352590225637 tr(WHW.T) 10643.5751953125
8_o proxy err 0.03991295397281647 tr(WHW.T) 20.24227523803711
8_up proxy err 0.022515783086419106 tr(WHW.T) 865.6057739257812
8_gate proxy err 0.010141278617084026 tr(WHW.T) 1966.5599365234375
8_down proxy err 0.030927952378988266 tr(WHW.T) 37.46827697753906
I0313 03:08:09.521251 1997628 finetune.py:68] layer 9_down @ epoch 3 new loss 0.0009658260969445109 old loss 0.0009659016504883766 BETTER
I0313 03:08:20.251121 1997861 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0011825587134808302 old loss 0.0011827745474874973 BETTER
I0313 03:08:36.641250 1997628 finetune.py:68] layer 9_down @ epoch 4 new loss 0.0009657697519287467 old loss 0.0009658260969445109 BETTER
I0313 03:08:36.885642 1998091 finetune.py:45] layer 11_down initial loss 0.0013014448340982199
9_v proxy err 0.02931654267013073 tr(WHW.T) 565.0663452148438
9_q proxy err 0.0026895811315625906 tr(WHW.T) 6976.43212890625
9_k proxy err 0.001714437734335661 tr(WHW.T) 10994.84765625
9_o proxy err 0.0407833456993103 tr(WHW.T) 25.833396911621094
9_up proxy err 0.02177773229777813 tr(WHW.T) 970.078857421875
9_gate proxy err 0.010118141770362854 tr(WHW.T) 2127.489990234375
9_down proxy err 0.03135199472308159 tr(WHW.T) 43.2625617980957
I0313 03:08:47.180465 1997861 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0011823972454294562 old loss 0.0011825587134808302 BETTER
I0313 03:09:01.740054 1998091 finetune.py:68] layer 11_down @ epoch 0 new loss 0.0013011660194024444 old loss 0.0013014448340982199 BETTER
I0313 03:09:14.567060 1997861 finetune.py:68] layer 10_down @ epoch 2 new loss 0.0011822773376479745 old loss 0.0011823972454294562 BETTER
I0313 03:09:27.639281 1998091 finetune.py:68] layer 11_down @ epoch 1 new loss 0.001300952979363501 old loss 0.0013011660194024444 BETTER
I0313 03:09:41.649907 1997861 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0011821876978501678 old loss 0.0011822773376479745 BETTER
I0313 03:09:52.311030 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 12 in 71.37417459487915s
I0313 03:09:54.335291 1998091 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0013007880188524723 old loss 0.001300952979363501 BETTER
I0313 03:09:56.011286 1999920 config.py:54] PyTorch version 2.1.1 available.
I0313 03:09:57.219948 1985945 quantize_finetune_llama.py:184] layer 13 gpu 1
I0313 03:09:57.310519 1999920 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:10:10.191639 1997861 finetune.py:68] layer 10_down @ epoch 4 new loss 0.001182116218842566 old loss 0.0011821876978501678 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
10_v proxy err 0.02971767634153366 tr(WHW.T) 578.807373046875
10_q proxy err 0.002811650512740016 tr(WHW.T) 6923.0966796875
10_k proxy err 0.001778262434527278 tr(WHW.T) 11005.7626953125
10_o proxy err 0.04090607538819313 tr(WHW.T) 35.45693588256836
10_up proxy err 0.020526893436908722 tr(WHW.T) 1078.5260009765625
10_gate proxy err 0.00995270162820816 tr(WHW.T) 2258.01220703125
10_down proxy err 0.029771603643894196 tr(WHW.T) 52.64961242675781
I0313 03:10:16.572027 1999920 finetune.py:45] layer 12_v initial loss 0.00028980596107430756
I0313 03:10:21.228443 1998091 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0013006598455831409 old loss 0.0013007880188524723 BETTER
I0313 03:10:49.336392 1998091 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0013005625223740935 old loss 0.0013006598455831409 BETTER
I0313 03:10:51.168412 1999920 finetune.py:68] layer 12_v @ epoch 0 new loss 0.00023097798111848533 old loss 0.00028980596107430756 BETTER
11_v proxy err 0.02977731078863144 tr(WHW.T) 723.1956176757812
11_q proxy err 0.0033704163506627083 tr(WHW.T) 7033.9111328125
11_k proxy err 0.0022506893146783113 tr(WHW.T) 10524.884765625
11_o proxy err 0.041376903653144836 tr(WHW.T) 36.94636917114258
11_up proxy err 0.021427644416689873 tr(WHW.T) 1139.1678466796875
11_gate proxy err 0.010322877205908298 tr(WHW.T) 2394.41943359375
11_down proxy err 0.030647432431578636 tr(WHW.T) 56.506839752197266
I0313 03:11:25.712961 1999920 finetune.py:68] layer 12_v @ epoch 1 new loss 0.00022106528922449797 old loss 0.00023097798111848533 BETTER
I0313 03:11:28.620954 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 13 in 73.310537815094s
I0313 03:11:31.876432 2000177 config.py:54] PyTorch version 2.1.1 available.
I0313 03:11:32.921241 1985945 quantize_finetune_llama.py:184] layer 14 gpu 2
I0313 03:11:33.003574 2000177 data_utils.py:336] using 256 training seqs, 128 validation seqs
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:11:50.753845 2000177 finetune.py:45] layer 13_v initial loss 0.0003211298317182809
I0313 03:12:00.616324 1999920 finetune.py:68] layer 12_v @ epoch 2 new loss 0.00021545948402490467 old loss 0.00022106528922449797 BETTER
I0313 03:12:21.857522 2000177 finetune.py:68] layer 13_v @ epoch 0 new loss 0.0002539886918384582 old loss 0.0003211298317182809 BETTER
I0313 03:12:35.560680 1999920 finetune.py:68] layer 12_v @ epoch 3 new loss 0.0002114937815349549 old loss 0.00021545948402490467 BETTER
I0313 03:12:42.965577 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 14 in 69.60438895225525s
I0313 03:12:46.157297 2000401 config.py:54] PyTorch version 2.1.1 available.
I0313 03:12:47.151071 1985945 quantize_finetune_llama.py:184] layer 15 gpu 3
I0313 03:12:47.220674 2000401 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:12:53.944620 2000177 finetune.py:68] layer 13_v @ epoch 1 new loss 0.00024341343669220805 old loss 0.0002539886918384582 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:13:04.010251 2000401 finetune.py:45] layer 14_v initial loss 0.00039000410470180213
I0313 03:13:10.682767 1999920 finetune.py:68] layer 12_v @ epoch 4 new loss 0.00020835285249631852 old loss 0.0002114937815349549 BETTER
I0313 03:13:26.140349 2000177 finetune.py:68] layer 13_v @ epoch 2 new loss 0.00023741944460198283 old loss 0.00024341343669220805 BETTER
I0313 03:13:29.858563 1999920 finetune.py:45] layer 12_q initial loss 0.0002486002922523767
I0313 03:13:35.313956 2000401 finetune.py:68] layer 14_v @ epoch 0 new loss 0.0003245958941988647 old loss 0.00039000410470180213 BETTER
I0313 03:13:55.447365 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 15 in 67.91934061050415s
I0313 03:13:58.774608 2000177 finetune.py:68] layer 13_v @ epoch 3 new loss 0.00023306134971790016 old loss 0.00023741944460198283 BETTER
I0313 03:13:58.813780 2000625 config.py:54] PyTorch version 2.1.1 available.
I0313 03:13:59.965029 1985945 quantize_finetune_llama.py:184] layer 16 gpu 0
I0313 03:14:00.056983 2000625 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:14:03.429710 1999920 finetune.py:68] layer 12_q @ epoch 0 new loss 0.0002428900043014437 old loss 0.0002486002922523767 BETTER
I0313 03:14:07.681960 2000401 finetune.py:68] layer 14_v @ epoch 1 new loss 0.0003114974533673376 old loss 0.0003245958941988647 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:14:20.124144 2000625 finetune.py:45] layer 15_v initial loss 0.000406938255764544
I0313 03:14:33.167176 2000177 finetune.py:68] layer 13_v @ epoch 4 new loss 0.00022964368690736592 old loss 0.00023306134971790016 BETTER
I0313 03:14:38.980192 1999920 finetune.py:68] layer 12_q @ epoch 1 new loss 0.00023946714645717293 old loss 0.0002428900043014437 BETTER
I0313 03:14:40.953053 2000401 finetune.py:68] layer 14_v @ epoch 2 new loss 0.00030342763056978583 old loss 0.0003114974533673376 BETTER
I0313 03:14:52.682878 2000625 finetune.py:68] layer 15_v @ epoch 0 new loss 0.0003242775273974985 old loss 0.000406938255764544 BETTER
I0313 03:14:54.022657 2000177 finetune.py:45] layer 13_q initial loss 0.00027085529291071
I0313 03:15:15.092421 1999920 finetune.py:68] layer 12_q @ epoch 2 new loss 0.00023666935157962143 old loss 0.00023946714645717293 BETTER
I0313 03:15:15.236565 2000401 finetune.py:68] layer 14_v @ epoch 3 new loss 0.00029742674087174237 old loss 0.00030342763056978583 BETTER
I0313 03:15:25.242753 2000625 finetune.py:68] layer 15_v @ epoch 1 new loss 0.0003111094410996884 old loss 0.0003242775273974985 BETTER
I0313 03:15:25.704636 2000177 finetune.py:68] layer 13_q @ epoch 0 new loss 0.000264863163465634 old loss 0.00027085529291071 BETTER
I0313 03:15:48.955653 2000401 finetune.py:68] layer 14_v @ epoch 4 new loss 0.0002925200969912112 old loss 0.00029742674087174237 BETTER
I0313 03:15:50.826115 1999920 finetune.py:68] layer 12_q @ epoch 3 new loss 0.00023432733723893762 old loss 0.00023666935157962143 BETTER
I0313 03:15:58.346532 2000625 finetune.py:68] layer 15_v @ epoch 2 new loss 0.0003031712840311229 old loss 0.0003111094410996884 BETTER
I0313 03:15:58.867590 2000177 finetune.py:68] layer 13_q @ epoch 1 new loss 0.00026106444420292974 old loss 0.000264863163465634 BETTER
I0313 03:16:09.381955 2000401 finetune.py:45] layer 14_q initial loss 0.0003409419732633978
I0313 03:16:27.706156 1999920 finetune.py:68] layer 12_q @ epoch 4 new loss 0.00023217765556182712 old loss 0.00023432733723893762 BETTER
I0313 03:16:32.090687 2000625 finetune.py:68] layer 15_v @ epoch 3 new loss 0.0002970751957036555 old loss 0.0003031712840311229 BETTER
I0313 03:16:32.456675 2000177 finetune.py:68] layer 13_q @ epoch 2 new loss 0.00025792786618694663 old loss 0.00026106444420292974 BETTER
I0313 03:16:40.879716 2000401 finetune.py:68] layer 14_q @ epoch 0 new loss 0.0003328082384541631 old loss 0.0003409419732633978 BETTER
I0313 03:16:49.416836 1999920 finetune.py:45] layer 12_k initial loss 0.00026082174736075103
I0313 03:17:06.835273 2000177 finetune.py:68] layer 13_q @ epoch 3 new loss 0.0002552210644353181 old loss 0.00025792786618694663 BETTER
I0313 03:17:07.005989 2000625 finetune.py:68] layer 15_v @ epoch 4 new loss 0.00029222018201835454 old loss 0.0002970751957036555 BETTER
I0313 03:17:13.426271 2000401 finetune.py:68] layer 14_q @ epoch 1 new loss 0.0003277233918197453 old loss 0.0003328082384541631 BETTER
I0313 03:17:22.484256 1999920 finetune.py:68] layer 12_k @ epoch 0 new loss 0.0002572041703388095 old loss 0.00026082174736075103 BETTER
I0313 03:17:25.763124 2000625 finetune.py:45] layer 15_q initial loss 0.0003486046043690294
I0313 03:17:40.353061 2000177 finetune.py:68] layer 13_q @ epoch 4 new loss 0.00025287241442129016 old loss 0.0002552210644353181 BETTER
I0313 03:17:47.079878 2000401 finetune.py:68] layer 14_q @ epoch 2 new loss 0.0003233570314478129 old loss 0.0003277233918197453 BETTER
I0313 03:17:58.804619 1999920 finetune.py:68] layer 12_k @ epoch 1 new loss 0.00025523011572659016 old loss 0.0002572041703388095 BETTER
I0313 03:17:59.436840 2000625 finetune.py:68] layer 15_q @ epoch 0 new loss 0.00033895240630954504 old loss 0.0003486046043690294 BETTER
I0313 03:18:02.183159 2000177 finetune.py:45] layer 13_k initial loss 0.00028395181288942695
I0313 03:18:19.766985 2000401 finetune.py:68] layer 14_q @ epoch 3 new loss 0.0003196390753146261 old loss 0.0003233570314478129 BETTER
I0313 03:18:33.697065 2000625 finetune.py:68] layer 15_q @ epoch 1 new loss 0.0003332790802232921 old loss 0.00033895240630954504 BETTER
I0313 03:18:34.946677 1999920 finetune.py:68] layer 12_k @ epoch 2 new loss 0.00025357387494295835 old loss 0.00025523011572659016 BETTER
I0313 03:18:35.071562 2000177 finetune.py:68] layer 13_k @ epoch 0 new loss 0.00028014989220537245 old loss 0.00028395181288942695 BETTER
I0313 03:18:53.068430 2000401 finetune.py:68] layer 14_q @ epoch 4 new loss 0.00031632842728868127 old loss 0.0003196390753146261 BETTER
I0313 03:19:07.807454 2000625 finetune.py:68] layer 15_q @ epoch 2 new loss 0.0003286090213805437 old loss 0.0003332790802232921 BETTER
I0313 03:19:08.737876 2000177 finetune.py:68] layer 13_k @ epoch 1 new loss 0.0002779289789032191 old loss 0.00028014989220537245 BETTER
I0313 03:19:10.979439 1999920 finetune.py:68] layer 12_k @ epoch 3 new loss 0.0002520929847378284 old loss 0.00025357387494295835 BETTER
I0313 03:19:14.004133 2000401 finetune.py:45] layer 14_k initial loss 0.0003525507927406579
I0313 03:19:42.368173 2000625 finetune.py:68] layer 15_q @ epoch 3 new loss 0.00032459734939038754 old loss 0.0003286090213805437 BETTER
I0313 03:19:43.559157 2000177 finetune.py:68] layer 13_k @ epoch 2 new loss 0.00027603100170381367 old loss 0.0002779289789032191 BETTER
I0313 03:19:47.382568 1999920 finetune.py:68] layer 12_k @ epoch 4 new loss 0.00025077888858504593 old loss 0.0002520929847378284 BETTER
I0313 03:19:47.451735 2000401 finetune.py:68] layer 14_k @ epoch 0 new loss 0.00034801597939804196 old loss 0.0003525507927406579 BETTER
I0313 03:20:07.913796 1999920 finetune.py:45] layer 12_o initial loss 0.0005964100710116327
I0313 03:20:17.502086 2000625 finetune.py:68] layer 15_q @ epoch 4 new loss 0.0003210375434719026 old loss 0.00032459734939038754 BETTER
I0313 03:20:18.355767 2000177 finetune.py:68] layer 13_k @ epoch 3 new loss 0.00027434376534074545 old loss 0.00027603100170381367 BETTER
I0313 03:20:21.111994 2000401 finetune.py:68] layer 14_k @ epoch 1 new loss 0.0003450032963883132 old loss 0.00034801597939804196 BETTER
I0313 03:20:40.653018 2000625 finetune.py:45] layer 15_k initial loss 0.00036220147740095854
I0313 03:20:43.011850 1999920 finetune.py:68] layer 12_o @ epoch 0 new loss 0.0005844828556291759 old loss 0.0005964100710116327 BETTER
I0313 03:20:50.898603 2000177 finetune.py:68] layer 13_k @ epoch 4 new loss 0.00027285623946227133 old loss 0.00027434376534074545 BETTER
I0313 03:20:53.737685 2000401 finetune.py:68] layer 14_k @ epoch 2 new loss 0.00034246407449245453 old loss 0.0003450032963883132 BETTER
I0313 03:21:13.935594 2000177 finetune.py:45] layer 13_o initial loss 0.0006430247449316084
I0313 03:21:14.138568 2000625 finetune.py:68] layer 15_k @ epoch 0 new loss 0.00035696590202860534 old loss 0.00036220147740095854 BETTER
I0313 03:21:18.852077 1999920 finetune.py:68] layer 12_o @ epoch 1 new loss 0.0005787277477793396 old loss 0.0005844828556291759 BETTER
I0313 03:21:27.626416 2000401 finetune.py:68] layer 14_k @ epoch 3 new loss 0.00034017825964838266 old loss 0.00034246407449245453 BETTER
I0313 03:21:45.043251 2000177 finetune.py:68] layer 13_o @ epoch 0 new loss 0.0006281491951085627 old loss 0.0006430247449316084 BETTER
I0313 03:21:46.585549 2000625 finetune.py:68] layer 15_k @ epoch 1 new loss 0.0003537895972840488 old loss 0.00035696590202860534 BETTER
I0313 03:21:55.709553 1999920 finetune.py:68] layer 12_o @ epoch 2 new loss 0.0005740760825574398 old loss 0.0005787277477793396 BETTER
I0313 03:22:01.908135 2000401 finetune.py:68] layer 14_k @ epoch 4 new loss 0.00033806447754614055 old loss 0.00034017825964838266 BETTER
I0313 03:22:18.578843 2000177 finetune.py:68] layer 13_o @ epoch 1 new loss 0.0006212693406268954 old loss 0.0006281491951085627 BETTER
I0313 03:22:21.072874 2000625 finetune.py:68] layer 15_k @ epoch 2 new loss 0.00035106984432786703 old loss 0.0003537895972840488 BETTER
I0313 03:22:23.518864 2000401 finetune.py:45] layer 14_o initial loss 0.0007952168234623969
I0313 03:22:30.744876 1999920 finetune.py:68] layer 12_o @ epoch 3 new loss 0.0005700599285773933 old loss 0.0005740760825574398 BETTER
I0313 03:22:52.031736 2000177 finetune.py:68] layer 13_o @ epoch 2 new loss 0.0006156981107778847 old loss 0.0006212693406268954 BETTER
I0313 03:22:54.277852 2000625 finetune.py:68] layer 15_k @ epoch 3 new loss 0.0003486787318252027 old loss 0.00035106984432786703 BETTER
I0313 03:22:54.662284 2000401 finetune.py:68] layer 14_o @ epoch 0 new loss 0.0007795991841703653 old loss 0.0007952168234623969 BETTER
I0313 03:23:04.809220 1999920 finetune.py:68] layer 12_o @ epoch 4 new loss 0.0005665381322614849 old loss 0.0005700599285773933 BETTER
I0313 03:23:23.783447 2000177 finetune.py:68] layer 13_o @ epoch 3 new loss 0.0006110218237154186 old loss 0.0006156981107778847 BETTER
I0313 03:23:26.422619 2000401 finetune.py:68] layer 14_o @ epoch 1 new loss 0.0007715370738878846 old loss 0.0007795991841703653 BETTER
I0313 03:23:26.721309 2000625 finetune.py:68] layer 15_k @ epoch 4 new loss 0.00034653072361834347 old loss 0.0003486787318252027 BETTER
I0313 03:23:29.414355 1999920 finetune.py:45] layer 12_up initial loss 0.000857456645462662
I0313 03:23:44.489648 2000625 finetune.py:45] layer 15_o initial loss 0.0008156478288583457
I0313 03:23:55.410443 2000177 finetune.py:68] layer 13_o @ epoch 4 new loss 0.0006068695802241564 old loss 0.0006110218237154186 BETTER
I0313 03:23:58.044090 2000401 finetune.py:68] layer 14_o @ epoch 2 new loss 0.000765118922572583 old loss 0.0007715370738878846 BETTER
I0313 03:24:00.236791 1999920 finetune.py:68] layer 12_up @ epoch 0 new loss 0.0008499586838297546 old loss 0.000857456645462662 BETTER
I0313 03:24:14.782131 2000625 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0007981621893122792 old loss 0.0008156478288583457 BETTER
I0313 03:24:19.408537 2000177 finetune.py:45] layer 13_up initial loss 0.0009502873872406781
I0313 03:24:29.694626 2000401 finetune.py:68] layer 14_o @ epoch 3 new loss 0.0007596475770696998 old loss 0.000765118922572583 BETTER
I0313 03:24:32.016976 1999920 finetune.py:68] layer 12_up @ epoch 1 new loss 0.0008448821608908474 old loss 0.0008499586838297546 BETTER
I0313 03:24:45.908447 2000625 finetune.py:68] layer 15_o @ epoch 1 new loss 0.0007890675333328545 old loss 0.0007981621893122792 BETTER
I0313 03:24:48.151839 2000177 finetune.py:68] layer 13_up @ epoch 0 new loss 0.0009406398166902363 old loss 0.0009502873872406781 BETTER
I0313 03:25:01.365844 2000401 finetune.py:68] layer 14_o @ epoch 4 new loss 0.000754802895244211 old loss 0.0007596475770696998 BETTER
I0313 03:25:03.988761 1999920 finetune.py:68] layer 12_up @ epoch 2 new loss 0.0008404721156693995 old loss 0.0008448821608908474 BETTER
I0313 03:25:17.098532 2000625 finetune.py:68] layer 15_o @ epoch 2 new loss 0.0007817355799488723 old loss 0.0007890675333328545 BETTER
I0313 03:25:17.704478 2000177 finetune.py:68] layer 13_up @ epoch 1 new loss 0.000934102397877723 old loss 0.0009406398166902363 BETTER
I0313 03:25:24.711221 2000401 finetune.py:45] layer 14_up initial loss 0.0011459204833954573
I0313 03:25:36.191979 1999920 finetune.py:68] layer 12_up @ epoch 3 new loss 0.0008365134126506746 old loss 0.0008404721156693995 BETTER
I0313 03:25:47.405603 2000177 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0009285459527745843 old loss 0.000934102397877723 BETTER
I0313 03:25:48.531680 2000625 finetune.py:68] layer 15_o @ epoch 3 new loss 0.000775493448600173 old loss 0.0007817355799488723 BETTER
I0313 03:25:53.567122 2000401 finetune.py:68] layer 14_up @ epoch 0 new loss 0.0011345098027959466 old loss 0.0011459204833954573 BETTER
I0313 03:26:08.433458 1999920 finetune.py:68] layer 12_up @ epoch 4 new loss 0.0008329072734341025 old loss 0.0008365134126506746 BETTER
I0313 03:26:17.245076 2000177 finetune.py:68] layer 13_up @ epoch 3 new loss 0.0009236079058609903 old loss 0.0009285459527745843 BETTER
I0313 03:26:19.739948 2000625 finetune.py:68] layer 15_o @ epoch 4 new loss 0.0007700489368289709 old loss 0.000775493448600173 BETTER
I0313 03:26:23.419524 2000401 finetune.py:68] layer 14_up @ epoch 1 new loss 0.0011270532850176096 old loss 0.0011345098027959466 BETTER
I0313 03:26:32.465121 1999920 finetune.py:45] layer 12_gate initial loss 0.0010368935763835907
I0313 03:26:42.755145 2000625 finetune.py:45] layer 15_up initial loss 0.0012392069911584258
I0313 03:26:47.197527 2000177 finetune.py:68] layer 13_up @ epoch 4 new loss 0.0009191521094180644 old loss 0.0009236079058609903 BETTER
I0313 03:26:53.110422 2000401 finetune.py:68] layer 14_up @ epoch 2 new loss 0.0011207708157598972 old loss 0.0011270532850176096 BETTER
I0313 03:27:01.618223 1999920 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.0010319555876776576 old loss 0.0010368935763835907 BETTER
I0313 03:27:10.984496 2000177 finetune.py:45] layer 13_gate initial loss 0.0011646461207419634
I0313 03:27:11.481659 2000625 finetune.py:68] layer 15_up @ epoch 0 new loss 0.001224904670380056 old loss 0.0012392069911584258 BETTER
I0313 03:27:23.001535 2000401 finetune.py:68] layer 14_up @ epoch 3 new loss 0.0011152043007314205 old loss 0.0011207708157598972 BETTER
I0313 03:27:32.337886 1999920 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0010282807052135468 old loss 0.0010319555876776576 BETTER
I0313 03:27:38.436483 2000177 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.0011585094034671783 old loss 0.0011646461207419634 BETTER
I0313 03:27:41.272827 2000625 finetune.py:68] layer 15_up @ epoch 1 new loss 0.0012157007586210966 old loss 0.001224904670380056 BETTER
I0313 03:27:52.772362 2000401 finetune.py:68] layer 14_up @ epoch 4 new loss 0.0011101685231551528 old loss 0.0011152043007314205 BETTER
I0313 03:28:02.611311 1999920 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.0010250068735331297 old loss 0.0010282807052135468 BETTER
I0313 03:28:06.572065 2000177 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0011538614053279161 old loss 0.0011585094034671783 BETTER
I0313 03:28:10.884938 2000625 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0012080165324732661 old loss 0.0012157007586210966 BETTER
I0313 03:28:16.098371 2000401 finetune.py:45] layer 14_gate initial loss 0.0014047391014173627
I0313 03:28:32.954705 1999920 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.0010220353724434972 old loss 0.0010250068735331297 BETTER
I0313 03:28:35.003703 2000177 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0011497511295601726 old loss 0.0011538614053279161 BETTER
I0313 03:28:40.502952 2000625 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0012011703802272677 old loss 0.0012080165324732661 BETTER
I0313 03:28:43.654884 2000401 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0013976739719510078 old loss 0.0014047391014173627 BETTER
I0313 03:29:03.511411 1999920 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.0010193096240982413 old loss 0.0010220353724434972 BETTER
I0313 03:29:03.527624 2000177 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.0011459813686087728 old loss 0.0011497511295601726 BETTER
I0313 03:29:10.393452 2000625 finetune.py:68] layer 15_up @ epoch 4 new loss 0.0011951071210205555 old loss 0.0012011703802272677 BETTER
I0313 03:29:11.687243 2000401 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.001392255537211895 old loss 0.0013976739719510078 BETTER
I0313 03:29:31.985048 2000177 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0011425777338445187 old loss 0.0011459813686087728 BETTER
I0313 03:29:34.187600 2000625 finetune.py:45] layer 15_gate initial loss 0.0015537503641098738
I0313 03:29:39.959979 2000401 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.0013874447904527187 old loss 0.001392255537211895 BETTER
I0313 03:29:46.772620 1999920 finetune.py:45] layer 12_down initial loss 0.0014761809725314379
I0313 03:30:01.591398 2000625 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.0015450961655005813 old loss 0.0015537503641098738 BETTER
I0313 03:30:09.500162 2000401 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.0013830889947712421 old loss 0.0013874447904527187 BETTER
I0313 03:30:15.428527 1999920 finetune.py:68] layer 12_down @ epoch 0 new loss 0.0014758141478523612 old loss 0.0014761809725314379 BETTER
I0313 03:30:17.986021 2000177 finetune.py:45] layer 13_down initial loss 0.0016974739264696836
I0313 03:30:30.138702 2000625 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.0015384299913421273 old loss 0.0015450961655005813 BETTER
I0313 03:30:38.431694 2000401 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0013790710363537073 old loss 0.0013830889947712421 BETTER
I0313 03:30:44.103677 2000177 finetune.py:68] layer 13_down @ epoch 0 new loss 0.001696973922662437 old loss 0.0016974739264696836 BETTER
I0313 03:30:44.732926 1999920 finetune.py:68] layer 12_down @ epoch 1 new loss 0.0014755259035155177 old loss 0.0014758141478523612 BETTER
I0313 03:30:59.471911 2000625 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.0015325662679970264 old loss 0.0015384299913421273 BETTER
I0313 03:31:11.621959 2000177 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0016965792747214437 old loss 0.001696973922662437 BETTER
I0313 03:31:14.061504 1999920 finetune.py:68] layer 12_down @ epoch 2 new loss 0.001475300989113748 old loss 0.0014755259035155177 BETTER
I0313 03:31:24.429702 2000401 finetune.py:45] layer 14_down initial loss 0.0020311670377850533
I0313 03:31:29.522003 2000625 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.001527200685814023 old loss 0.0015325662679970264 BETTER
I0313 03:31:40.213368 2000177 finetune.py:68] layer 13_down @ epoch 2 new loss 0.001696261577308178 old loss 0.0016965792747214437 BETTER
I0313 03:31:44.619017 1999920 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0014751205453649163 old loss 0.001475300989113748 BETTER
I0313 03:31:50.774798 2000401 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0020304308272898197 old loss 0.0020311670377850533 BETTER
I0313 03:31:58.071040 2000625 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0015223246300593019 old loss 0.001527200685814023 BETTER
I0313 03:32:08.562966 2000177 finetune.py:68] layer 13_down @ epoch 3 new loss 0.0016960061620920897 old loss 0.001696261577308178 BETTER
I0313 03:32:15.893372 1999920 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0014749814290553331 old loss 0.0014751205453649163 BETTER
12_v proxy err 0.031217370182275772 tr(WHW.T) 703.318603515625
12_q proxy err 0.0034793103113770485 tr(WHW.T) 7054.04296875
12_k proxy err 0.002264173701405525 tr(WHW.T) 10902.78515625
12_o proxy err 0.04336750507354736 tr(WHW.T) 39.62916946411133
12_up proxy err 0.021443041041493416 tr(WHW.T) 1227.1007080078125
12_gate proxy err 0.011145684868097305 tr(WHW.T) 2380.809814453125
12_down proxy err 0.030633430927991867 tr(WHW.T) 64.65869903564453
I0313 03:32:18.664247 2000401 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0020298389717936516 old loss 0.0020304308272898197 BETTER
I0313 03:32:37.980932 2000177 finetune.py:68] layer 13_down @ epoch 4 new loss 0.0016958012711256742 old loss 0.0016960061620920897 BETTER
13_v proxy err 0.03255677968263626 tr(WHW.T) 714.5677490234375
13_q proxy err 0.003697783686220646 tr(WHW.T) 6963.4462890625
13_k proxy err 0.0024761336389929056 tr(WHW.T) 10434.9990234375
13_o proxy err 0.039430759847164154 tr(WHW.T) 46.215938568115234
13_up proxy err 0.020407801494002342 tr(WHW.T) 1365.40283203125
13_gate proxy err 0.010795689187943935 tr(WHW.T) 2595.322265625
13_down proxy err 0.03005538135766983 tr(WHW.T) 79.95247650146484
I0313 03:32:44.054018 2000625 finetune.py:45] layer 15_down initial loss 0.002331552328541875
I0313 03:32:46.162546 2000401 finetune.py:68] layer 14_down @ epoch 2 new loss 0.002029352355748415 old loss 0.0020298389717936516 BETTER
I0313 03:33:10.658875 2000625 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0023307022638618946 old loss 0.002331552328541875 BETTER
I0313 03:33:14.255871 2000401 finetune.py:68] layer 14_down @ epoch 3 new loss 0.002028967719525099 old loss 0.002029352355748415 BETTER
I0313 03:33:37.726233 2000625 finetune.py:68] layer 15_down @ epoch 1 new loss 0.0023300095926970243 old loss 0.0023307022638618946 BETTER
I0313 03:33:41.811985 2000401 finetune.py:68] layer 14_down @ epoch 4 new loss 0.002028647344559431 old loss 0.002028967719525099 BETTER
14_v proxy err 0.03384723514318466 tr(WHW.T) 706.1612548828125
14_q proxy err 0.003745779162272811 tr(WHW.T) 7084.23486328125
14_k proxy err 0.002353215590119362 tr(WHW.T) 11307.134765625
14_o proxy err 0.04329776391386986 tr(WHW.T) 51.36770248413086
14_up proxy err 0.021092375740408897 tr(WHW.T) 1463.9356689453125
14_gate proxy err 0.011567005887627602 tr(WHW.T) 2682.058837890625
14_down proxy err 0.030921103432774544 tr(WHW.T) 90.98294830322266
I0313 03:33:57.691342 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 16 in 74.53448677062988s
I0313 03:34:00.716701 2002490 config.py:54] PyTorch version 2.1.1 available.
I0313 03:34:01.737619 1985945 quantize_finetune_llama.py:184] layer 17 gpu 1
I0313 03:34:01.805992 2002490 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:34:04.419006 2000625 finetune.py:68] layer 15_down @ epoch 2 new loss 0.002329440088942647 old loss 0.0023300095926970243 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:34:18.396635 2002490 finetune.py:45] layer 16_v initial loss 0.0005273909773677588
I0313 03:34:30.989586 2000625 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0023289797827601433 old loss 0.002329440088942647 BETTER
I0313 03:34:51.017203 2002490 finetune.py:68] layer 16_v @ epoch 0 new loss 0.0004120238299947232 old loss 0.0005273909773677588 BETTER
I0313 03:34:57.477745 2000625 finetune.py:68] layer 15_down @ epoch 4 new loss 0.002328602597117424 old loss 0.0023289797827601433 BETTER
15_v proxy err 0.030737174674868584 tr(WHW.T) 762.7275390625
15_q proxy err 0.0035491432063281536 tr(WHW.T) 7259.4697265625
15_k proxy err 0.002342240884900093 tr(WHW.T) 11081.388671875
15_o proxy err 0.03691847622394562 tr(WHW.T) 60.09682846069336
15_up proxy err 0.02021750807762146 tr(WHW.T) 1640.3314208984375
15_gate proxy err 0.011475266888737679 tr(WHW.T) 2902.875732421875
15_down proxy err 0.030275903642177582 tr(WHW.T) 115.03499603271484
I0313 03:35:11.180563 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 17 in 68.99322080612183s
I0313 03:35:14.154300 2004053 config.py:54] PyTorch version 2.1.1 available.
I0313 03:35:15.140658 1985945 quantize_finetune_llama.py:184] layer 18 gpu 2
I0313 03:35:15.208218 2004053 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:35:24.974001 2002490 finetune.py:68] layer 16_v @ epoch 1 new loss 0.0003952654078602791 old loss 0.0004120238299947232 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:35:31.860376 2004053 finetune.py:45] layer 17_v initial loss 0.0004544318653643131
I0313 03:35:59.405670 2002490 finetune.py:68] layer 16_v @ epoch 2 new loss 0.00038504559779539704 old loss 0.0003952654078602791 BETTER
I0313 03:36:02.980180 2004053 finetune.py:68] layer 17_v @ epoch 0 new loss 0.0003382315335329622 old loss 0.0004544318653643131 BETTER
I0313 03:36:23.576714 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 18 in 68.03985714912415s
I0313 03:36:26.642481 2004918 config.py:54] PyTorch version 2.1.1 available.
I0313 03:36:27.660831 1985945 quantize_finetune_llama.py:184] layer 19 gpu 3
I0313 03:36:27.728899 2004918 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:36:34.012764 2002490 finetune.py:68] layer 16_v @ epoch 3 new loss 0.00037742324639111757 old loss 0.00038504559779539704 BETTER
I0313 03:36:35.009541 2004053 finetune.py:68] layer 17_v @ epoch 1 new loss 0.0003237798810005188 old loss 0.0003382315335329622 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:36:44.450916 2004918 finetune.py:45] layer 18_v initial loss 0.0005132879596203566
I0313 03:37:07.363608 2004053 finetune.py:68] layer 17_v @ epoch 2 new loss 0.0003152893914375454 old loss 0.0003237798810005188 BETTER
I0313 03:37:08.641314 2002490 finetune.py:68] layer 16_v @ epoch 4 new loss 0.00037126868846826255 old loss 0.00037742324639111757 BETTER
I0313 03:37:15.814611 2004918 finetune.py:68] layer 18_v @ epoch 0 new loss 0.0003513034898787737 old loss 0.0005132879596203566 BETTER
I0313 03:37:26.212980 2002490 finetune.py:45] layer 16_q initial loss 0.00043714488856494427
I0313 03:37:35.651208 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 19 in 67.59530401229858s
I0313 03:37:38.942027 2005656 config.py:54] PyTorch version 2.1.1 available.
I0313 03:37:39.975431 1985945 quantize_finetune_llama.py:184] layer 20 gpu 0
I0313 03:37:40.035242 2004053 finetune.py:68] layer 17_v @ epoch 3 new loss 0.0003089697565883398 old loss 0.0003152893914375454 BETTER
I0313 03:37:40.059311 2005656 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:37:48.018758 2004918 finetune.py:68] layer 18_v @ epoch 1 new loss 0.0003358157991897315 old loss 0.0003513034898787737 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:37:57.629222 2005656 finetune.py:45] layer 19_v initial loss 0.0005193580291233957
I0313 03:37:59.736959 2002490 finetune.py:68] layer 16_q @ epoch 0 new loss 0.0004251736681908369 old loss 0.00043714488856494427 BETTER
I0313 03:38:12.784082 2004053 finetune.py:68] layer 17_v @ epoch 4 new loss 0.00030393103952519596 old loss 0.0003089697565883398 BETTER
I0313 03:38:20.383685 2004918 finetune.py:68] layer 18_v @ epoch 2 new loss 0.0003273454785812646 old loss 0.0003358157991897315 BETTER
I0313 03:38:29.033845 2005656 finetune.py:68] layer 19_v @ epoch 0 new loss 0.00034764231531880796 old loss 0.0005193580291233957 BETTER
I0313 03:38:31.250783 2004053 finetune.py:45] layer 17_q initial loss 0.00037036664434708655
I0313 03:38:33.922889 2002490 finetune.py:68] layer 16_q @ epoch 1 new loss 0.0004182406119070947 old loss 0.0004251736681908369 BETTER
I0313 03:38:53.500348 2004918 finetune.py:68] layer 18_v @ epoch 3 new loss 0.00032143722637556493 old loss 0.0003273454785812646 BETTER
I0313 03:39:01.389966 2005656 finetune.py:68] layer 19_v @ epoch 1 new loss 0.00033150133094750345 old loss 0.00034764231531880796 BETTER
I0313 03:39:02.866856 2004053 finetune.py:68] layer 17_q @ epoch 0 new loss 0.00035715612466447055 old loss 0.00037036664434708655 BETTER
I0313 03:39:08.231791 2002490 finetune.py:68] layer 16_q @ epoch 2 new loss 0.0004124678089283407 old loss 0.0004182406119070947 BETTER
I0313 03:39:26.598837 2004918 finetune.py:68] layer 18_v @ epoch 4 new loss 0.0003167576214764267 old loss 0.00032143722637556493 BETTER
I0313 03:39:33.677907 2005656 finetune.py:68] layer 19_v @ epoch 2 new loss 0.00032329687383025885 old loss 0.00033150133094750345 BETTER
I0313 03:39:34.992665 2004053 finetune.py:68] layer 17_q @ epoch 1 new loss 0.00035081550595350564 old loss 0.00035715612466447055 BETTER
I0313 03:39:42.515758 2002490 finetune.py:68] layer 16_q @ epoch 3 new loss 0.0004075189062859863 old loss 0.0004124678089283407 BETTER
I0313 03:39:45.087754 2004918 finetune.py:45] layer 18_q initial loss 0.000404082442400977
I0313 03:40:06.422805 2005656 finetune.py:68] layer 19_v @ epoch 3 new loss 0.00031763542210683227 old loss 0.00032329687383025885 BETTER
I0313 03:40:07.368815 2004053 finetune.py:68] layer 17_q @ epoch 2 new loss 0.0003455610712990165 old loss 0.00035081550595350564 BETTER
I0313 03:40:16.898794 2004918 finetune.py:68] layer 18_q @ epoch 0 new loss 0.0003900973533745855 old loss 0.000404082442400977 BETTER
I0313 03:40:17.235432 2002490 finetune.py:68] layer 16_q @ epoch 4 new loss 0.0004032056895084679 old loss 0.0004075189062859863 BETTER
I0313 03:40:35.940652 2002490 finetune.py:45] layer 16_k initial loss 0.0004514434840530157
I0313 03:40:39.479701 2005656 finetune.py:68] layer 19_v @ epoch 4 new loss 0.00031318815308623016 old loss 0.00031763542210683227 BETTER
I0313 03:40:40.049732 2004053 finetune.py:68] layer 17_q @ epoch 3 new loss 0.0003411463985685259 old loss 0.0003455610712990165 BETTER
I0313 03:40:49.412893 2004918 finetune.py:68] layer 18_q @ epoch 1 new loss 0.00038323746412061155 old loss 0.0003900973533745855 BETTER
I0313 03:40:58.149026 2005656 finetune.py:45] layer 19_q initial loss 0.00039452227065339684
I0313 03:41:09.028588 2002490 finetune.py:68] layer 16_k @ epoch 0 new loss 0.00044570124009624124 old loss 0.0004514434840530157 BETTER
I0313 03:41:12.799650 2004053 finetune.py:68] layer 17_q @ epoch 4 new loss 0.00033733833697624505 old loss 0.0003411463985685259 BETTER
I0313 03:41:21.643927 2004918 finetune.py:68] layer 18_q @ epoch 2 new loss 0.00037783183506689966 old loss 0.00038323746412061155 BETTER
I0313 03:41:30.458619 2005656 finetune.py:68] layer 19_q @ epoch 0 new loss 0.0003792421775870025 old loss 0.00039452227065339684 BETTER
I0313 03:41:32.010314 2004053 finetune.py:45] layer 17_k initial loss 0.00038630637573078275
I0313 03:41:42.733486 2002490 finetune.py:68] layer 16_k @ epoch 1 new loss 0.0004418303433340043 old loss 0.00044570124009624124 BETTER
I0313 03:41:54.178619 2004918 finetune.py:68] layer 18_q @ epoch 3 new loss 0.0003732714394573122 old loss 0.00037783183506689966 BETTER
I0313 03:42:02.963078 2005656 finetune.py:68] layer 19_q @ epoch 1 new loss 0.0003731798497028649 old loss 0.0003792421775870025 BETTER
I0313 03:42:03.232500 2004053 finetune.py:68] layer 17_k @ epoch 0 new loss 0.00038039032369852066 old loss 0.00038630637573078275 BETTER
I0313 03:42:16.450348 2002490 finetune.py:68] layer 16_k @ epoch 2 new loss 0.0004385171050671488 old loss 0.0004418303433340043 BETTER
I0313 03:42:26.704335 2004918 finetune.py:68] layer 18_q @ epoch 4 new loss 0.00036937385448254645 old loss 0.0003732714394573122 BETTER
I0313 03:42:35.186721 2005656 finetune.py:68] layer 19_q @ epoch 2 new loss 0.0003683993127197027 old loss 0.0003731798497028649 BETTER
I0313 03:42:35.498970 2004053 finetune.py:68] layer 17_k @ epoch 1 new loss 0.0003768271126318723 old loss 0.00038039032369852066 BETTER
I0313 03:42:45.747545 2004918 finetune.py:45] layer 18_k initial loss 0.00043476352584548295
I0313 03:42:50.857567 2002490 finetune.py:68] layer 16_k @ epoch 3 new loss 0.0004356407735031098 old loss 0.0004385171050671488 BETTER
I0313 03:43:09.998782 2004053 finetune.py:68] layer 17_k @ epoch 2 new loss 0.0003738044179044664 old loss 0.0003768271126318723 BETTER
I0313 03:43:10.061848 2005656 finetune.py:68] layer 19_q @ epoch 3 new loss 0.0003644242824520916 old loss 0.0003683993127197027 BETTER
I0313 03:43:18.385183 2004918 finetune.py:68] layer 18_k @ epoch 0 new loss 0.00042898868559859693 old loss 0.00043476352584548295 BETTER
I0313 03:43:25.208172 2002490 finetune.py:68] layer 16_k @ epoch 4 new loss 0.00043302972335368395 old loss 0.0004356407735031098 BETTER
I0313 03:43:44.148746 2004053 finetune.py:68] layer 17_k @ epoch 3 new loss 0.0003711732570081949 old loss 0.0003738044179044664 BETTER
I0313 03:43:44.477816 2005656 finetune.py:68] layer 19_q @ epoch 4 new loss 0.00036098272539675236 old loss 0.0003644242824520916 BETTER
I0313 03:43:46.910067 2002490 finetune.py:45] layer 16_o initial loss 0.001082726987078786
I0313 03:43:51.769515 2004918 finetune.py:68] layer 18_k @ epoch 1 new loss 0.0004254204104654491 old loss 0.00042898868559859693 BETTER
I0313 03:44:06.602419 2005656 finetune.py:45] layer 19_k initial loss 0.00042271686834283173
I0313 03:44:18.423794 2004053 finetune.py:68] layer 17_k @ epoch 4 new loss 0.0003688637516461313 old loss 0.0003711732570081949 BETTER
I0313 03:44:20.937445 2002490 finetune.py:68] layer 16_o @ epoch 0 new loss 0.001055992441251874 old loss 0.001082726987078786 BETTER
I0313 03:44:25.252953 2004918 finetune.py:68] layer 18_k @ epoch 2 new loss 0.00042247952660545707 old loss 0.0004254204104654491 BETTER
I0313 03:44:38.856386 2005656 finetune.py:68] layer 19_k @ epoch 0 new loss 0.00041674202657304704 old loss 0.00042271686834283173 BETTER
I0313 03:44:40.852069 2004053 finetune.py:45] layer 17_o initial loss 0.0008631209493614733
I0313 03:44:54.882292 2002490 finetune.py:68] layer 16_o @ epoch 1 new loss 0.0010431504342705011 old loss 0.001055992441251874 BETTER
I0313 03:44:57.755430 2004918 finetune.py:68] layer 18_k @ epoch 3 new loss 0.00041994143975898623 old loss 0.00042247952660545707 BETTER
I0313 03:45:10.839462 2005656 finetune.py:68] layer 19_k @ epoch 1 new loss 0.00041308708023279905 old loss 0.00041674202657304704 BETTER
I0313 03:45:11.346441 2004053 finetune.py:68] layer 17_o @ epoch 0 new loss 0.0008445707499049604 old loss 0.0008631209493614733 BETTER
I0313 03:45:28.181779 2002490 finetune.py:68] layer 16_o @ epoch 2 new loss 0.0010329901706427336 old loss 0.0010431504342705011 BETTER
I0313 03:45:29.927912 2004918 finetune.py:68] layer 18_k @ epoch 4 new loss 0.00041776345460675657 old loss 0.00041994143975898623 BETTER
I0313 03:45:42.658688 2004053 finetune.py:68] layer 17_o @ epoch 1 new loss 0.000835121376439929 old loss 0.0008445707499049604 BETTER
I0313 03:45:42.796608 2005656 finetune.py:68] layer 19_k @ epoch 2 new loss 0.0004101455269847065 old loss 0.00041308708023279905 BETTER
I0313 03:45:47.815598 2004918 finetune.py:45] layer 18_o initial loss 0.0009785037254914641
I0313 03:46:01.678234 2002490 finetune.py:68] layer 16_o @ epoch 3 new loss 0.0010242885909974575 old loss 0.0010329901706427336 BETTER
I0313 03:46:14.015683 2004053 finetune.py:68] layer 17_o @ epoch 2 new loss 0.0008276141015812755 old loss 0.000835121376439929 BETTER
I0313 03:46:14.715411 2005656 finetune.py:68] layer 19_k @ epoch 3 new loss 0.0004075814504176378 old loss 0.0004101455269847065 BETTER
I0313 03:46:18.531212 2004918 finetune.py:68] layer 18_o @ epoch 0 new loss 0.0009555323631502688 old loss 0.0009785037254914641 BETTER
I0313 03:46:35.226664 2002490 finetune.py:68] layer 16_o @ epoch 4 new loss 0.0010166403371840715 old loss 0.0010242885909974575 BETTER
I0313 03:46:45.496672 2004053 finetune.py:68] layer 17_o @ epoch 3 new loss 0.0008211647509597242 old loss 0.0008276141015812755 BETTER
I0313 03:46:46.549047 2005656 finetune.py:68] layer 19_k @ epoch 4 new loss 0.00040547980461269617 old loss 0.0004075814504176378 BETTER
I0313 03:46:50.162261 2004918 finetune.py:68] layer 18_o @ epoch 1 new loss 0.000945214822422713 old loss 0.0009555323631502688 BETTER
I0313 03:46:58.335376 2002490 finetune.py:45] layer 16_up initial loss 0.001617344794794917
I0313 03:47:04.593238 2005656 finetune.py:45] layer 19_o initial loss 0.0009377510286867619
I0313 03:47:17.057589 2004053 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0008155829855240881 old loss 0.0008211647509597242 BETTER
I0313 03:47:21.872454 2004918 finetune.py:68] layer 18_o @ epoch 2 new loss 0.0009369086474180222 old loss 0.000945214822422713 BETTER
I0313 03:47:28.789139 2002490 finetune.py:68] layer 16_up @ epoch 0 new loss 0.001598750939592719 old loss 0.001617344794794917 BETTER
I0313 03:47:34.751314 2005656 finetune.py:68] layer 19_o @ epoch 0 new loss 0.00091660360340029 old loss 0.0009377510286867619 BETTER
I0313 03:47:40.319776 2004053 finetune.py:45] layer 17_up initial loss 0.0015098399017006159
I0313 03:47:53.662787 2004918 finetune.py:68] layer 18_o @ epoch 3 new loss 0.0009299483499489725 old loss 0.0009369086474180222 BETTER
I0313 03:48:00.619720 2002490 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0015867870533838868 old loss 0.001598750939592719 BETTER
I0313 03:48:05.860701 2005656 finetune.py:68] layer 19_o @ epoch 1 new loss 0.0009074874105863273 old loss 0.00091660360340029 BETTER
I0313 03:48:09.199275 2004053 finetune.py:68] layer 17_up @ epoch 0 new loss 0.0014925742289051414 old loss 0.0015098399017006159 BETTER
I0313 03:48:25.333046 2004918 finetune.py:68] layer 18_o @ epoch 4 new loss 0.0009239245555363595 old loss 0.0009299483499489725 BETTER
I0313 03:48:32.190780 2002490 finetune.py:68] layer 16_up @ epoch 2 new loss 0.0015769981546327472 old loss 0.0015867870533838868 BETTER
I0313 03:48:37.035934 2005656 finetune.py:68] layer 19_o @ epoch 2 new loss 0.0009002705337479711 old loss 0.0009074874105863273 BETTER
I0313 03:48:38.671873 2004053 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0014816251350566745 old loss 0.0014925742289051414 BETTER
I0313 03:48:48.714782 2004918 finetune.py:45] layer 18_up initial loss 0.0017631397349759936
I0313 03:49:03.867207 2002490 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0015683964593335986 old loss 0.0015769981546327472 BETTER
I0313 03:49:08.306273 2005656 finetune.py:68] layer 19_o @ epoch 3 new loss 0.0008944637957029045 old loss 0.0009002705337479711 BETTER
I0313 03:49:08.462509 2004053 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0014727979432791471 old loss 0.0014816251350566745 BETTER
I0313 03:49:17.733660 2004918 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0017426599515601993 old loss 0.0017631397349759936 BETTER
I0313 03:49:35.729714 2002490 finetune.py:68] layer 16_up @ epoch 4 new loss 0.0015607844106853008 old loss 0.0015683964593335986 BETTER
I0313 03:49:38.371864 2004053 finetune.py:68] layer 17_up @ epoch 3 new loss 0.001464980305172503 old loss 0.0014727979432791471 BETTER
I0313 03:49:39.491626 2005656 finetune.py:68] layer 19_o @ epoch 4 new loss 0.0008894296479411423 old loss 0.0008944637957029045 BETTER
I0313 03:49:47.488324 2004918 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0017301132902503014 old loss 0.0017426599515601993 BETTER
I0313 03:49:58.837319 2002490 finetune.py:45] layer 16_gate initial loss 0.0020269593223929405
I0313 03:50:02.435346 2005656 finetune.py:45] layer 19_up initial loss 0.0018419341649860144
I0313 03:50:08.108891 2004053 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0014580499846488237 old loss 0.001464980305172503 BETTER
I0313 03:50:17.258144 2004918 finetune.py:68] layer 18_up @ epoch 2 new loss 0.001719961641356349 old loss 0.0017301132902503014 BETTER
I0313 03:50:27.574455 2002490 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.0020160183776170015 old loss 0.0020269593223929405 BETTER
I0313 03:50:31.228250 2005656 finetune.py:68] layer 19_up @ epoch 0 new loss 0.001820315490476787 old loss 0.0018419341649860144 BETTER
I0313 03:50:31.492050 2004053 finetune.py:45] layer 17_gate initial loss 0.0019960065837949514
I0313 03:50:47.168548 2004918 finetune.py:68] layer 18_up @ epoch 3 new loss 0.00171107507776469 old loss 0.001719961641356349 BETTER
I0313 03:50:57.596427 2002490 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.002007289556786418 old loss 0.0020160183776170015 BETTER
I0313 03:50:58.727454 2004053 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0019855669233947992 old loss 0.0019960065837949514 BETTER
I0313 03:51:00.825902 2005656 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0018075527623295784 old loss 0.001820315490476787 BETTER
I0313 03:51:17.161453 2004918 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0017032595351338387 old loss 0.00171107507776469 BETTER
I0313 03:51:27.052140 2004053 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0019770502112805843 old loss 0.0019855669233947992 BETTER
I0313 03:51:27.556295 2002490 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.00199950416572392 old loss 0.002007289556786418 BETTER
I0313 03:51:30.316036 2005656 finetune.py:68] layer 19_up @ epoch 2 new loss 0.0017973113572224975 old loss 0.0018075527623295784 BETTER
I0313 03:51:40.645236 2004918 finetune.py:45] layer 18_gate initial loss 0.0023413440212607384
I0313 03:51:55.258916 2004053 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.001969548175111413 old loss 0.0019770502112805843 BETTER
I0313 03:51:57.486910 2002490 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0019925220403820276 old loss 0.00199950416572392 BETTER
I0313 03:51:59.931384 2005656 finetune.py:68] layer 19_up @ epoch 3 new loss 0.00178859184961766 old loss 0.0017973113572224975 BETTER
I0313 03:52:08.073030 2004918 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0023301062174141407 old loss 0.0023413440212607384 BETTER
I0313 03:52:23.673588 2004053 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.0019627842120826244 old loss 0.001969548175111413 BETTER
I0313 03:52:27.537648 2002490 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.0019861378241330385 old loss 0.0019925220403820276 BETTER
I0313 03:52:29.523781 2005656 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0017810388235375285 old loss 0.00178859184961766 BETTER
I0313 03:52:36.153952 2004918 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.002320671221241355 old loss 0.0023301062174141407 BETTER
I0313 03:52:52.032587 2004053 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.001956636318936944 old loss 0.0019627842120826244 BETTER
I0313 03:52:52.897802 2005656 finetune.py:45] layer 19_gate initial loss 0.0025352099910378456
I0313 03:53:04.425488 2004918 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0023123135324567556 old loss 0.002320671221241355 BETTER
I0313 03:53:08.557653 2002490 finetune.py:45] layer 16_down initial loss 0.003096448490396142
I0313 03:53:19.838398 2005656 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.0025232001207768917 old loss 0.0025352099910378456 BETTER
I0313 03:53:32.715500 2004918 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0023048240691423416 old loss 0.0023123135324567556 BETTER
I0313 03:53:33.150163 2004053 finetune.py:45] layer 17_down initial loss 0.0032150133047252893
I0313 03:53:35.071966 2002490 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0030951155349612236 old loss 0.003096448490396142 BETTER
I0313 03:53:47.975771 2005656 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0025135204195976257 old loss 0.0025232001207768917 BETTER
I0313 03:53:58.543563 2004053 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0032135334331542253 old loss 0.0032150133047252893 BETTER
I0313 03:54:01.299655 2004918 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.002298019826412201 old loss 0.0023048240691423416 BETTER
I0313 03:54:02.929463 2002490 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0030939688440412283 old loss 0.0030951155349612236 BETTER
I0313 03:54:18.133568 2005656 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0025050342082977295 old loss 0.0025135204195976257 BETTER
I0313 03:54:25.977612 2004053 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0032122924458235502 old loss 0.0032135334331542253 BETTER
I0313 03:54:31.947643 2002490 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0030929846689105034 old loss 0.0030939688440412283 BETTER
I0313 03:54:45.397399 2004918 finetune.py:45] layer 18_down initial loss 0.0037938698660582304
I0313 03:54:48.121327 2005656 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0024974073749035597 old loss 0.0025050342082977295 BETTER
I0313 03:54:53.826986 2004053 finetune.py:68] layer 17_down @ epoch 2 new loss 0.00321121234446764 old loss 0.0032122924458235502 BETTER
I0313 03:55:01.726089 2002490 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0030921325087547302 old loss 0.0030929846689105034 BETTER
I0313 03:55:12.769932 2004918 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0037921611219644547 old loss 0.0037938698660582304 BETTER
I0313 03:55:17.739127 2005656 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.002490570768713951 old loss 0.0024974073749035597 BETTER
I0313 03:55:21.956700 2004053 finetune.py:68] layer 17_down @ epoch 3 new loss 0.003210272639989853 old loss 0.00321121234446764 BETTER
I0313 03:55:31.159988 2002490 finetune.py:68] layer 16_down @ epoch 4 new loss 0.003091405611485243 old loss 0.0030921325087547302 BETTER
16_v proxy err 0.03197178989648819 tr(WHW.T) 780.7407836914062
16_q proxy err 0.00376629875972867 tr(WHW.T) 7197.7060546875
16_k proxy err 0.002340378239750862 tr(WHW.T) 11639.8271484375
16_o proxy err 0.03310520946979523 tr(WHW.T) 88.81657409667969
16_up proxy err 0.01985488273203373 tr(WHW.T) 1886.02197265625
16_gate proxy err 0.011234588921070099 tr(WHW.T) 3355.513916015625
16_down proxy err 0.03089439682662487 tr(WHW.T) 153.4259796142578
I0313 03:55:41.087683 2004918 finetune.py:68] layer 18_down @ epoch 1 new loss 0.00379064935259521 old loss 0.0037921611219644547 BETTER
I0313 03:55:49.775646 2004053 finetune.py:68] layer 17_down @ epoch 4 new loss 0.003209466580301523 old loss 0.003210272639989853 BETTER
17_v proxy err 0.03167465329170227 tr(WHW.T) 845.7654418945312
17_q proxy err 0.004059385973960161 tr(WHW.T) 7172.0888671875
17_k proxy err 0.002728487132117152 tr(WHW.T) 10708.7109375
17_o proxy err 0.03752640262246132 tr(WHW.T) 58.57147979736328
17_up proxy err 0.02251841500401497 tr(WHW.T) 1920.9190673828125
17_gate proxy err 0.012257790192961693 tr(WHW.T) 3565.1162109375
17_down proxy err 0.03225254639983177 tr(WHW.T) 166.90359497070312
I0313 03:56:02.032878 2005656 finetune.py:45] layer 19_down initial loss 0.004143974278122187
I0313 03:56:07.727665 2004918 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0037893226835876703 old loss 0.00379064935259521 BETTER
I0313 03:56:27.050533 2005656 finetune.py:68] layer 19_down @ epoch 0 new loss 0.004142069723457098 old loss 0.004143974278122187 BETTER
I0313 03:56:34.620363 2004918 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0037881622556596994 old loss 0.0037893226835876703 BETTER
I0313 03:56:53.199414 2005656 finetune.py:68] layer 19_down @ epoch 1 new loss 0.004140431992709637 old loss 0.004142069723457098 BETTER
I0313 03:57:01.431421 2004918 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0037871217355132103 old loss 0.0037881622556596994 BETTER
18_v proxy err 0.03090747259557247 tr(WHW.T) 1003.7705078125
18_q proxy err 0.0044178348034620285 tr(WHW.T) 7516.67138671875
18_k proxy err 0.003184911096468568 tr(WHW.T) 10469.28125
18_o proxy err 0.03410695120692253 tr(WHW.T) 70.45160675048828
18_up proxy err 0.02411889098584652 tr(WHW.T) 2022.3756103515625
18_gate proxy err 0.013116012327373028 tr(WHW.T) 3769.485107421875
18_down proxy err 0.031817179173231125 tr(WHW.T) 200.43109130859375
I0313 03:57:06.099225 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 20 in 71.81247067451477s
I0313 03:57:09.147368 2015949 config.py:54] PyTorch version 2.1.1 available.
I0313 03:57:10.153630 1985945 quantize_finetune_llama.py:184] layer 21 gpu 1
I0313 03:57:10.218944 2015949 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:57:19.488447 2005656 finetune.py:68] layer 19_down @ epoch 2 new loss 0.004138991702347994 old loss 0.004140431992709637 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:57:27.016129 2015949 finetune.py:45] layer 20_v initial loss 0.0005937751848250628
I0313 03:57:45.505758 2005656 finetune.py:68] layer 19_down @ epoch 3 new loss 0.004137720447033644 old loss 0.004138991702347994 BETTER
I0313 03:57:59.830269 2015949 finetune.py:68] layer 20_v @ epoch 0 new loss 0.0004090582369826734 old loss 0.0005937751848250628 BETTER
I0313 03:58:11.717529 2005656 finetune.py:68] layer 19_down @ epoch 4 new loss 0.004136624280363321 old loss 0.004137720447033644 BETTER
19_v proxy err 0.0303298681974411 tr(WHW.T) 1019.1412353515625
19_q proxy err 0.004728717729449272 tr(WHW.T) 6954.72900390625
19_k proxy err 0.0031228044535964727 tr(WHW.T) 10561.9013671875
19_o proxy err 0.03498639911413193 tr(WHW.T) 62.75870132446289
19_up proxy err 0.024497827515006065 tr(WHW.T) 2149.203369140625
19_gate proxy err 0.01452038623392582 tr(WHW.T) 3679.78076171875
19_down proxy err 0.03141994774341583 tr(WHW.T) 224.8908233642578
I0313 03:58:19.179887 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 21 in 68.62756896018982s
I0313 03:58:22.193277 2016691 config.py:54] PyTorch version 2.1.1 available.
I0313 03:58:23.207499 1985945 quantize_finetune_llama.py:184] layer 22 gpu 2
I0313 03:58:23.277549 2016691 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:58:33.761630 2015949 finetune.py:68] layer 20_v @ epoch 1 new loss 0.0003884149482473731 old loss 0.0004090582369826734 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:58:40.007624 2016691 finetune.py:45] layer 21_v initial loss 0.0005792487063445151
I0313 03:59:08.170567 2015949 finetune.py:68] layer 20_v @ epoch 2 new loss 0.00037817872362211347 old loss 0.0003884149482473731 BETTER
I0313 03:59:11.123534 2016691 finetune.py:68] layer 21_v @ epoch 0 new loss 0.00037365630851127207 old loss 0.0005792487063445151 BETTER
I0313 03:59:31.693831 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 22 in 68.0881278514862s
I0313 03:59:35.036409 2017433 config.py:54] PyTorch version 2.1.1 available.
I0313 03:59:36.028983 1985945 quantize_finetune_llama.py:184] layer 23 gpu 3
I0313 03:59:36.108756 2017433 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 03:59:42.903303 2015949 finetune.py:68] layer 20_v @ epoch 3 new loss 0.0003711664758156985 old loss 0.00037817872362211347 BETTER
I0313 03:59:43.058523 2016691 finetune.py:68] layer 21_v @ epoch 1 new loss 0.0003540989418979734 old loss 0.00037365630851127207 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 03:59:52.763013 2017433 finetune.py:45] layer 22_v initial loss 0.0007015328737907112
I0313 04:00:15.371267 2016691 finetune.py:68] layer 21_v @ epoch 2 new loss 0.0003454074903856963 old loss 0.0003540989418979734 BETTER
I0313 04:00:17.568177 2015949 finetune.py:68] layer 20_v @ epoch 4 new loss 0.00036607717629522085 old loss 0.0003711664758156985 BETTER
I0313 04:00:24.102491 2017433 finetune.py:68] layer 22_v @ epoch 0 new loss 0.0004699584096670151 old loss 0.0007015328737907112 BETTER
I0313 04:00:35.078141 2015949 finetune.py:45] layer 20_q initial loss 0.000459641101770103
I0313 04:00:44.848078 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 23 in 68.37908387184143s
I0313 04:00:47.730728 2016691 finetune.py:68] layer 21_v @ epoch 3 new loss 0.00033994417754001915 old loss 0.0003454074903856963 BETTER
I0313 04:00:48.229950 2018182 config.py:54] PyTorch version 2.1.1 available.
I0313 04:00:49.301456 1985945 quantize_finetune_llama.py:184] layer 24 gpu 0
I0313 04:00:49.371982 2018182 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 04:00:56.540192 2017433 finetune.py:68] layer 22_v @ epoch 1 new loss 0.0004449461121112108 old loss 0.0004699584096670151 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 04:01:06.473485 2018182 finetune.py:45] layer 23_v initial loss 0.000770850048866123
I0313 04:01:08.094085 2015949 finetune.py:68] layer 20_q @ epoch 0 new loss 0.000444548117229715 old loss 0.000459641101770103 BETTER
I0313 04:01:20.432980 2016691 finetune.py:68] layer 21_v @ epoch 4 new loss 0.00033589047961868346 old loss 0.00033994417754001915 BETTER
I0313 04:01:28.845909 2017433 finetune.py:68] layer 22_v @ epoch 2 new loss 0.00043366840691305697 old loss 0.0004449461121112108 BETTER
I0313 04:01:37.538719 2018182 finetune.py:68] layer 23_v @ epoch 0 new loss 0.0004919663188047707 old loss 0.000770850048866123 BETTER
I0313 04:01:38.102617 2016691 finetune.py:45] layer 21_q initial loss 0.0004195690562482923
I0313 04:01:42.038220 2015949 finetune.py:68] layer 20_q @ epoch 1 new loss 0.0004373473930172622 old loss 0.000444548117229715 BETTER
I0313 04:02:01.915756 2017433 finetune.py:68] layer 22_v @ epoch 3 new loss 0.0004263763257768005 old loss 0.00043366840691305697 BETTER
I0313 04:02:09.548797 2016691 finetune.py:68] layer 21_q @ epoch 0 new loss 0.00040484045166522264 old loss 0.0004195690562482923 BETTER
I0313 04:02:09.672226 2018182 finetune.py:68] layer 23_v @ epoch 1 new loss 0.0004600395914167166 old loss 0.0004919663188047707 BETTER
I0313 04:02:16.356716 2015949 finetune.py:68] layer 20_q @ epoch 2 new loss 0.0004317009006626904 old loss 0.0004373473930172622 BETTER
I0313 04:02:35.086274 2017433 finetune.py:68] layer 22_v @ epoch 4 new loss 0.00042088417103514075 old loss 0.0004263763257768005 BETTER
I0313 04:02:41.686373 2016691 finetune.py:68] layer 21_q @ epoch 1 new loss 0.0003995258011855185 old loss 0.00040484045166522264 BETTER
I0313 04:02:41.730215 2018182 finetune.py:68] layer 23_v @ epoch 2 new loss 0.00044814631110057235 old loss 0.0004600395914167166 BETTER
I0313 04:02:50.856276 2015949 finetune.py:68] layer 20_q @ epoch 3 new loss 0.0004269863129593432 old loss 0.0004317009006626904 BETTER
I0313 04:02:52.956349 2017433 finetune.py:45] layer 22_q initial loss 0.0005632835091091692
I0313 04:03:13.870025 2016691 finetune.py:68] layer 21_q @ epoch 2 new loss 0.00039540883153676987 old loss 0.0003995258011855185 BETTER
I0313 04:03:14.137266 2018182 finetune.py:68] layer 23_v @ epoch 3 new loss 0.000440935546066612 old loss 0.00044814631110057235 BETTER
I0313 04:03:24.644603 2017433 finetune.py:68] layer 22_q @ epoch 0 new loss 0.0005413335748016834 old loss 0.0005632835091091692 BETTER
I0313 04:03:25.324795 2015949 finetune.py:68] layer 20_q @ epoch 4 new loss 0.0004230345948599279 old loss 0.0004269863129593432 BETTER
I0313 04:03:43.313564 2015949 finetune.py:45] layer 20_k initial loss 0.0004910429706797004
I0313 04:03:46.526839 2016691 finetune.py:68] layer 21_q @ epoch 3 new loss 0.0003920038871001452 old loss 0.00039540883153676987 BETTER
I0313 04:03:46.707312 2018182 finetune.py:68] layer 23_v @ epoch 4 new loss 0.00043565870146267116 old loss 0.000440935546066612 BETTER
I0313 04:03:57.041707 2017433 finetune.py:68] layer 22_q @ epoch 1 new loss 0.000532333564478904 old loss 0.0005413335748016834 BETTER
I0313 04:04:05.221273 2018182 finetune.py:45] layer 23_q initial loss 0.0005542223225347698
I0313 04:04:16.154016 2015949 finetune.py:68] layer 20_k @ epoch 0 new loss 0.00048501678975299 old loss 0.0004910429706797004 BETTER
I0313 04:04:19.047307 2016691 finetune.py:68] layer 21_q @ epoch 4 new loss 0.0003891145170200616 old loss 0.0003920038871001452 BETTER
I0313 04:04:29.711473 2017433 finetune.py:68] layer 22_q @ epoch 2 new loss 0.0005252789705991745 old loss 0.000532333564478904 BETTER
I0313 04:04:36.394578 2018182 finetune.py:68] layer 23_q @ epoch 0 new loss 0.0005333096487447619 old loss 0.0005542223225347698 BETTER
I0313 04:04:36.799978 2016691 finetune.py:45] layer 21_k initial loss 0.0004663866711780429
I0313 04:04:49.934240 2015949 finetune.py:68] layer 20_k @ epoch 1 new loss 0.0004810133541468531 old loss 0.00048501678975299 BETTER
I0313 04:05:02.382020 2017433 finetune.py:68] layer 22_q @ epoch 3 new loss 0.0005193764227442443 old loss 0.0005252789705991745 BETTER
I0313 04:05:08.040847 2016691 finetune.py:68] layer 21_k @ epoch 0 new loss 0.0004603699198924005 old loss 0.0004663866711780429 BETTER
I0313 04:05:08.506289 2018182 finetune.py:68] layer 23_q @ epoch 1 new loss 0.0005258052260614932 old loss 0.0005333096487447619 BETTER
I0313 04:05:25.170083 2015949 finetune.py:68] layer 20_k @ epoch 2 new loss 0.0004777518333867192 old loss 0.0004810133541468531 BETTER
I0313 04:05:35.877826 2017433 finetune.py:68] layer 22_q @ epoch 4 new loss 0.0005143347079865634 old loss 0.0005193764227442443 BETTER
I0313 04:05:41.440410 2016691 finetune.py:68] layer 21_k @ epoch 1 new loss 0.00045702068018727005 old loss 0.0004603699198924005 BETTER
I0313 04:05:41.896764 2018182 finetune.py:68] layer 23_q @ epoch 2 new loss 0.0005203048349358141 old loss 0.0005258052260614932 BETTER
I0313 04:05:57.748734 2017433 finetune.py:45] layer 22_k initial loss 0.0006279430817812681
I0313 04:06:00.073625 2015949 finetune.py:68] layer 20_k @ epoch 3 new loss 0.0004749779764097184 old loss 0.0004777518333867192 BETTER
I0313 04:06:15.147687 2016691 finetune.py:68] layer 21_k @ epoch 2 new loss 0.00045426381984725595 old loss 0.00045702068018727005 BETTER
I0313 04:06:15.682674 2018182 finetune.py:68] layer 23_q @ epoch 3 new loss 0.0005157190025784075 old loss 0.0005203048349358141 BETTER
I0313 04:06:30.091910 2017433 finetune.py:68] layer 22_k @ epoch 0 new loss 0.0006205614190548658 old loss 0.0006279430817812681 BETTER
I0313 04:06:35.045445 2015949 finetune.py:68] layer 20_k @ epoch 4 new loss 0.0004725730395875871 old loss 0.0004749779764097184 BETTER
I0313 04:06:48.873979 2016691 finetune.py:68] layer 21_k @ epoch 3 new loss 0.0004518810019362718 old loss 0.00045426381984725595 BETTER
I0313 04:06:49.417660 2018182 finetune.py:68] layer 23_q @ epoch 4 new loss 0.0005118502303957939 old loss 0.0005157190025784075 BETTER
I0313 04:06:55.284813 2015949 finetune.py:45] layer 20_o initial loss 0.0011001506354659796
I0313 04:07:03.016738 2017433 finetune.py:68] layer 22_k @ epoch 1 new loss 0.0006156805902719498 old loss 0.0006205614190548658 BETTER
I0313 04:07:09.275887 2018182 finetune.py:45] layer 23_k initial loss 0.0006224457756616175
I0313 04:07:21.221240 2016691 finetune.py:68] layer 21_k @ epoch 4 new loss 0.0004498740308918059 old loss 0.0004518810019362718 BETTER
I0313 04:07:27.362894 2015949 finetune.py:68] layer 20_o @ epoch 0 new loss 0.0010758904973044991 old loss 0.0011001506354659796 BETTER
I0313 04:07:35.373859 2017433 finetune.py:68] layer 22_k @ epoch 2 new loss 0.0006116747390478849 old loss 0.0006156805902719498 BETTER
I0313 04:07:39.149658 2016691 finetune.py:45] layer 21_o initial loss 0.0010281139984726906
I0313 04:07:39.719396 2018182 finetune.py:68] layer 23_k @ epoch 0 new loss 0.000616158009506762 old loss 0.0006224457756616175 BETTER
I0313 04:08:00.377215 2015949 finetune.py:68] layer 20_o @ epoch 1 new loss 0.0010651429183781147 old loss 0.0010758904973044991 BETTER
I0313 04:08:07.557054 2017433 finetune.py:68] layer 22_k @ epoch 3 new loss 0.0006081286701373756 old loss 0.0006116747390478849 BETTER
I0313 04:08:09.678936 2016691 finetune.py:68] layer 21_o @ epoch 0 new loss 0.0010080027859658003 old loss 0.0010281139984726906 BETTER
I0313 04:08:11.448137 2018182 finetune.py:68] layer 23_k @ epoch 1 new loss 0.0006119716563262045 old loss 0.000616158009506762 BETTER
I0313 04:08:33.717786 2015949 finetune.py:68] layer 20_o @ epoch 2 new loss 0.0010567785939201713 old loss 0.0010651429183781147 BETTER
I0313 04:08:39.836882 2017433 finetune.py:68] layer 22_k @ epoch 4 new loss 0.0006050991942174733 old loss 0.0006081286701373756 BETTER
I0313 04:08:40.914573 2016691 finetune.py:68] layer 21_o @ epoch 1 new loss 0.0010005637304857373 old loss 0.0010080027859658003 BETTER
I0313 04:08:43.041986 2018182 finetune.py:68] layer 23_k @ epoch 2 new loss 0.0006084982887841761 old loss 0.0006119716563262045 BETTER
I0313 04:08:57.724474 2017433 finetune.py:45] layer 22_o initial loss 0.0013044432271271944
I0313 04:09:07.475289 2015949 finetune.py:68] layer 20_o @ epoch 3 new loss 0.0010500041535124183 old loss 0.0010567785939201713 BETTER
I0313 04:09:12.386238 2016691 finetune.py:68] layer 21_o @ epoch 2 new loss 0.0009947656653821468 old loss 0.0010005637304857373 BETTER
I0313 04:09:14.912329 2018182 finetune.py:68] layer 23_k @ epoch 3 new loss 0.0006056549609638751 old loss 0.0006084982887841761 BETTER
I0313 04:09:28.257782 2017433 finetune.py:68] layer 22_o @ epoch 0 new loss 0.0012795923976227641 old loss 0.0013044432271271944 BETTER
I0313 04:09:41.108539 2015949 finetune.py:68] layer 20_o @ epoch 4 new loss 0.0010440843179821968 old loss 0.0010500041535124183 BETTER
I0313 04:09:43.888755 2016691 finetune.py:68] layer 21_o @ epoch 3 new loss 0.0009901714511215687 old loss 0.0009947656653821468 BETTER
I0313 04:09:46.582022 2018182 finetune.py:68] layer 23_k @ epoch 4 new loss 0.0006030983058735728 old loss 0.0006056549609638751 BETTER
I0313 04:09:59.807802 2017433 finetune.py:68] layer 22_o @ epoch 1 new loss 0.0012690880103036761 old loss 0.0012795923976227641 BETTER
I0313 04:10:04.461102 2015949 finetune.py:45] layer 20_up initial loss 0.0021637659519910812
I0313 04:10:04.790652 2018182 finetune.py:45] layer 23_o initial loss 0.0012608739780262113
I0313 04:10:15.486314 2016691 finetune.py:68] layer 21_o @ epoch 4 new loss 0.0009864080930128694 old loss 0.0009901714511215687 BETTER
I0313 04:10:32.760318 2017433 finetune.py:68] layer 22_o @ epoch 2 new loss 0.0012606411473825574 old loss 0.0012690880103036761 BETTER
I0313 04:10:35.362604 2015949 finetune.py:68] layer 20_up @ epoch 0 new loss 0.0021399473771452904 old loss 0.0021637659519910812 BETTER
I0313 04:10:35.544470 2018182 finetune.py:68] layer 23_o @ epoch 0 new loss 0.0012371110497042537 old loss 0.0012608739780262113 BETTER
I0313 04:10:39.543086 2016691 finetune.py:45] layer 21_up initial loss 0.0022296004462987185
I0313 04:11:04.557007 2017433 finetune.py:68] layer 22_o @ epoch 3 new loss 0.0012536318972706795 old loss 0.0012606411473825574 BETTER
I0313 04:11:06.611171 2018182 finetune.py:68] layer 23_o @ epoch 1 new loss 0.0012280659284442663 old loss 0.0012371110497042537 BETTER
I0313 04:11:06.995082 2015949 finetune.py:68] layer 20_up @ epoch 1 new loss 0.002125584054738283 old loss 0.0021399473771452904 BETTER
I0313 04:11:08.625120 2016691 finetune.py:68] layer 21_up @ epoch 0 new loss 0.002208918798714876 old loss 0.0022296004462987185 BETTER
I0313 04:11:36.857119 2017433 finetune.py:68] layer 22_o @ epoch 4 new loss 0.0012478932039812207 old loss 0.0012536318972706795 BETTER
I0313 04:11:38.250262 2018182 finetune.py:68] layer 23_o @ epoch 2 new loss 0.001221325364895165 old loss 0.0012280659284442663 BETTER
I0313 04:11:38.998149 2016691 finetune.py:68] layer 21_up @ epoch 1 new loss 0.002196599030867219 old loss 0.002208918798714876 BETTER
I0313 04:11:39.078182 2015949 finetune.py:68] layer 20_up @ epoch 2 new loss 0.0021142077166587114 old loss 0.002125584054738283 BETTER
I0313 04:12:00.559336 2017433 finetune.py:45] layer 22_up initial loss 0.0026715355925261974
I0313 04:12:09.357186 2016691 finetune.py:68] layer 21_up @ epoch 2 new loss 0.002186679746955633 old loss 0.002196599030867219 BETTER
I0313 04:12:09.869552 2018182 finetune.py:68] layer 23_o @ epoch 3 new loss 0.0012155664153397083 old loss 0.001221325364895165 BETTER
I0313 04:12:11.171995 2015949 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0021043559536337852 old loss 0.0021142077166587114 BETTER
I0313 04:12:29.517567 2017433 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0026495091151446104 old loss 0.0026715355925261974 BETTER
I0313 04:12:39.219391 2016691 finetune.py:68] layer 21_up @ epoch 3 new loss 0.0021783867850899696 old loss 0.002186679746955633 BETTER
I0313 04:12:41.061218 2018182 finetune.py:68] layer 23_o @ epoch 4 new loss 0.0012110898969694972 old loss 0.0012155664153397083 BETTER
I0313 04:12:42.865309 2015949 finetune.py:68] layer 20_up @ epoch 4 new loss 0.0020958820823580027 old loss 0.0021043559536337852 BETTER
I0313 04:12:59.502720 2017433 finetune.py:68] layer 22_up @ epoch 1 new loss 0.002635971875861287 old loss 0.0026495091151446104 BETTER
I0313 04:13:05.385985 2018182 finetune.py:45] layer 23_up initial loss 0.002806437900289893
I0313 04:13:06.745572 2015949 finetune.py:45] layer 20_gate initial loss 0.002977808704599738
I0313 04:13:09.271497 2016691 finetune.py:68] layer 21_up @ epoch 4 new loss 0.002171012805774808 old loss 0.0021783867850899696 BETTER
I0313 04:13:29.463852 2017433 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0026252567768096924 old loss 0.002635971875861287 BETTER
I0313 04:13:33.120102 2016691 finetune.py:45] layer 21_gate initial loss 0.0031440157908946276
I0313 04:13:34.186534 2018182 finetune.py:68] layer 23_up @ epoch 0 new loss 0.002785653807222843 old loss 0.002806437900289893 BETTER
I0313 04:13:35.799521 2015949 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.002965053776279092 old loss 0.002977808704599738 BETTER
I0313 04:13:59.675786 2017433 finetune.py:68] layer 22_up @ epoch 3 new loss 0.002616015961393714 old loss 0.0026252567768096924 BETTER
I0313 04:14:00.493621 2016691 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0031325609888881445 old loss 0.0031440157908946276 BETTER
I0313 04:14:04.143688 2018182 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0027721207588911057 old loss 0.002785653807222843 BETTER
I0313 04:14:05.935886 2015949 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0029543519485741854 old loss 0.002965053776279092 BETTER
I0313 04:14:29.116266 2016691 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.003123363945633173 old loss 0.0031325609888881445 BETTER
I0313 04:14:29.824956 2017433 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0026081977412104607 old loss 0.002616015961393714 BETTER
I0313 04:14:33.750253 2018182 finetune.py:68] layer 23_up @ epoch 2 new loss 0.0027617861051112413 old loss 0.0027721207588911057 BETTER
I0313 04:14:35.847004 2015949 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.0029447227716445923 old loss 0.0029543519485741854 BETTER
I0313 04:14:53.332571 2017433 finetune.py:45] layer 22_gate initial loss 0.0037186769768595695
I0313 04:14:57.477957 2016691 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0031149997375905514 old loss 0.003123363945633173 BETTER
I0313 04:15:03.423860 2018182 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0027527157217264175 old loss 0.0027617861051112413 BETTER
I0313 04:15:05.830839 2015949 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0029362442437559366 old loss 0.0029447227716445923 BETTER
I0313 04:15:20.906943 2017433 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0037061942275613546 old loss 0.0037186769768595695 BETTER
I0313 04:15:25.729970 2016691 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.0031077475287020206 old loss 0.0031149997375905514 BETTER
I0313 04:15:33.027081 2018182 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0027449706103652716 old loss 0.0027527157217264175 BETTER
I0313 04:15:35.905742 2015949 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.002928356872871518 old loss 0.0029362442437559366 BETTER
I0313 04:15:49.040776 2017433 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.0036963520105928183 old loss 0.0037061942275613546 BETTER
I0313 04:15:53.939994 2016691 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.0031011172104626894 old loss 0.0031077475287020206 BETTER
I0313 04:15:56.364035 2018182 finetune.py:45] layer 23_gate initial loss 0.004018858075141907
I0313 04:16:16.067062 2015949 finetune.py:45] layer 20_down initial loss 0.004961122293025255
I0313 04:16:17.251931 2017433 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0036875978112220764 old loss 0.0036963520105928183 BETTER
I0313 04:16:23.218348 2018182 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.004007335752248764 old loss 0.004018858075141907 BETTER
I0313 04:16:34.242999 2016691 finetune.py:45] layer 21_down initial loss 0.005239429883658886
I0313 04:16:42.620768 2015949 finetune.py:68] layer 20_down @ epoch 0 new loss 0.004958591889590025 old loss 0.004961122293025255 BETTER
I0313 04:16:45.349508 2017433 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.003679681336507201 old loss 0.0036875978112220764 BETTER
I0313 04:16:54.564935 2018182 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.00399771286174655 old loss 0.004007335752248764 BETTER
I0313 04:17:01.449213 2016691 finetune.py:68] layer 21_down @ epoch 0 new loss 0.005237252917140722 old loss 0.005239429883658886 BETTER
I0313 04:17:12.824874 2015949 finetune.py:68] layer 20_down @ epoch 1 new loss 0.004956332501024008 old loss 0.004958591889590025 BETTER
I0313 04:17:15.560926 2017433 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0036729711573570967 old loss 0.003679681336507201 BETTER
I0313 04:17:24.122015 2018182 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.003989189863204956 old loss 0.00399771286174655 BETTER
I0313 04:17:29.040262 2016691 finetune.py:68] layer 21_down @ epoch 1 new loss 0.005235300865024328 old loss 0.005237252917140722 BETTER
I0313 04:17:41.899008 2015949 finetune.py:68] layer 20_down @ epoch 2 new loss 0.00495433434844017 old loss 0.004956332501024008 BETTER
I0313 04:17:53.939084 2018182 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.003981412388384342 old loss 0.003989189863204956 BETTER
I0313 04:17:56.950364 2016691 finetune.py:68] layer 21_down @ epoch 2 new loss 0.005233556963503361 old loss 0.005235300865024328 BETTER
I0313 04:18:02.545762 2017433 finetune.py:45] layer 22_down initial loss 0.006107935216277838
I0313 04:18:12.309828 2015949 finetune.py:68] layer 20_down @ epoch 3 new loss 0.004952531773597002 old loss 0.00495433434844017 BETTER
I0313 04:18:24.310212 2018182 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.00397469149902463 old loss 0.003981412388384342 BETTER
I0313 04:18:26.044851 2016691 finetune.py:68] layer 21_down @ epoch 3 new loss 0.005232017021626234 old loss 0.005233556963503361 BETTER
I0313 04:18:29.509445 2017433 finetune.py:68] layer 22_down @ epoch 0 new loss 0.006105663254857063 old loss 0.006107935216277838 BETTER
I0313 04:18:41.046177 2015949 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0049509587697684765 old loss 0.004952531773597002 BETTER
20_v proxy err 0.03200520575046539 tr(WHW.T) 990.5983276367188
20_q proxy err 0.004693660885095596 tr(WHW.T) 7157.40087890625
20_k proxy err 0.0032408570405095816 tr(WHW.T) 10399.7333984375
20_o proxy err 0.02474266104400158 tr(WHW.T) 100.75094604492188
20_up proxy err 0.023931676521897316 tr(WHW.T) 2339.1103515625
20_gate proxy err 0.014163577929139137 tr(WHW.T) 4015.728515625
20_down proxy err 0.030960969626903534 tr(WHW.T) 277.1379089355469
I0313 04:18:53.704364 2016691 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0052306437864899635 old loss 0.005232017021626234 BETTER
21_v proxy err 0.031407907605171204 tr(WHW.T) 1144.5655517578125
21_q proxy err 0.0053160120733082294 tr(WHW.T) 7075.255859375
21_k proxy err 0.0037697069346904755 tr(WHW.T) 9990.951171875
21_o proxy err 0.030418403446674347 tr(WHW.T) 76.0047378540039
21_up proxy err 0.02563614957034588 tr(WHW.T) 2361.353759765625
21_gate proxy err 0.015428869053721428 tr(WHW.T) 3997.309814453125
21_down proxy err 0.032349057495594025 tr(WHW.T) 278.70111083984375
I0313 04:18:57.079749 2017433 finetune.py:68] layer 22_down @ epoch 1 new loss 0.006103644613176584 old loss 0.006105663254857063 BETTER
I0313 04:19:10.340860 2018182 finetune.py:45] layer 23_down initial loss 0.006515273824334145
I0313 04:19:24.131873 2017433 finetune.py:68] layer 22_down @ epoch 2 new loss 0.006101821083575487 old loss 0.006103644613176584 BETTER
I0313 04:19:35.196362 2018182 finetune.py:68] layer 23_down @ epoch 0 new loss 0.006513086147606373 old loss 0.006515273824334145 BETTER
I0313 04:19:50.964477 2017433 finetune.py:68] layer 22_down @ epoch 3 new loss 0.006100188475102186 old loss 0.006101821083575487 BETTER
I0313 04:20:00.989565 2018182 finetune.py:68] layer 23_down @ epoch 1 new loss 0.00651116669178009 old loss 0.006513086147606373 BETTER
I0313 04:20:10.780720 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 24 in 72.81170845031738s
I0313 04:20:14.015872 2028426 config.py:54] PyTorch version 2.1.1 available.
I0313 04:20:15.058307 1985945 quantize_finetune_llama.py:184] layer 25 gpu 1
I0313 04:20:15.126502 2028426 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 04:20:17.617109 2017433 finetune.py:68] layer 22_down @ epoch 4 new loss 0.006098732352256775 old loss 0.006100188475102186 BETTER
22_v proxy err 0.030074380338191986 tr(WHW.T) 1243.2529296875
22_q proxy err 0.005072969477623701 tr(WHW.T) 7755.1064453125
22_k proxy err 0.003715934930369258 tr(WHW.T) 10614.029296875
22_o proxy err 0.024149250239133835 tr(WHW.T) 114.89376831054688
22_up proxy err 0.026012927293777466 tr(WHW.T) 2475.06787109375
22_gate proxy err 0.015827136114239693 tr(WHW.T) 4150.052734375
22_down proxy err 0.032446399331092834 tr(WHW.T) 314.0501708984375
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 04:20:27.136626 2018182 finetune.py:68] layer 23_down @ epoch 2 new loss 0.00650942325592041 old loss 0.00651116669178009 BETTER
I0313 04:20:32.049298 2028426 finetune.py:45] layer 24_v initial loss 0.0008106370805762708
I0313 04:20:53.195236 2018182 finetune.py:68] layer 23_down @ epoch 3 new loss 0.006507911719381809 old loss 0.00650942325592041 BETTER
I0313 04:21:04.919780 2028426 finetune.py:68] layer 24_v @ epoch 0 new loss 0.0005515596712939441 old loss 0.0008106370805762708 BETTER
I0313 04:21:19.428270 2018182 finetune.py:68] layer 23_down @ epoch 4 new loss 0.006506532896310091 old loss 0.006507911719381809 BETTER
23_v proxy err 0.028853483498096466 tr(WHW.T) 1486.037353515625
23_q proxy err 0.006067998707294464 tr(WHW.T) 7353.6357421875
23_k proxy err 0.0044720894657075405 tr(WHW.T) 9992.7626953125
23_o proxy err 0.030315741896629333 tr(WHW.T) 85.69275665283203
23_up proxy err 0.027384117245674133 tr(WHW.T) 2534.067626953125
23_gate proxy err 0.017275167629122734 tr(WHW.T) 4093.377197265625
23_down proxy err 0.03304321691393852 tr(WHW.T) 323.6351013183594
I0313 04:21:30.953953 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 25 in 69.57485246658325s
I0313 04:21:34.019785 2029233 config.py:54] PyTorch version 2.1.1 available.
I0313 04:21:35.053923 1985945 quantize_finetune_llama.py:184] layer 26 gpu 2
I0313 04:21:35.129367 2029233 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 04:21:39.069270 2028426 finetune.py:68] layer 24_v @ epoch 1 new loss 0.0005195473204366863 old loss 0.0005515596712939441 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 04:21:51.742269 2029233 finetune.py:45] layer 25_v initial loss 0.0008762638899497688
I0313 04:22:13.552793 2028426 finetune.py:68] layer 24_v @ epoch 2 new loss 0.00050633359933272 old loss 0.0005195473204366863 BETTER
I0313 04:22:22.895705 2029233 finetune.py:68] layer 25_v @ epoch 0 new loss 0.0005165464244782925 old loss 0.0008762638899497688 BETTER
I0313 04:22:44.118531 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 26 in 68.64261364936829s
I0313 04:22:47.234113 2029978 config.py:54] PyTorch version 2.1.1 available.
I0313 04:22:48.296575 2028426 finetune.py:68] layer 24_v @ epoch 3 new loss 0.0004981154925189912 old loss 0.00050633359933272 BETTER
I0313 04:22:48.316342 1985945 quantize_finetune_llama.py:184] layer 27 gpu 3
I0313 04:22:48.384113 2029978 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 04:22:55.148197 2029233 finetune.py:68] layer 25_v @ epoch 1 new loss 0.0004771911189891398 old loss 0.0005165464244782925 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 04:23:05.233627 2029978 finetune.py:45] layer 26_v initial loss 0.001110496697947383
I0313 04:23:23.024185 2028426 finetune.py:68] layer 24_v @ epoch 4 new loss 0.000492578255943954 old loss 0.0004981154925189912 BETTER
I0313 04:23:27.438545 2029233 finetune.py:68] layer 25_v @ epoch 2 new loss 0.0004642516141757369 old loss 0.0004771911189891398 BETTER
I0313 04:23:36.631373 2029978 finetune.py:68] layer 26_v @ epoch 0 new loss 0.000762134906835854 old loss 0.001110496697947383 BETTER
I0313 04:23:40.987544 2028426 finetune.py:45] layer 24_q initial loss 0.0006310620228759944
I0313 04:23:57.124385 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 27 in 68.40765690803528s
I0313 04:24:00.109586 2029233 finetune.py:68] layer 25_v @ epoch 3 new loss 0.0004569278098642826 old loss 0.0004642516141757369 BETTER
I0313 04:24:00.400710 2030729 config.py:54] PyTorch version 2.1.1 available.
I0313 04:24:01.423422 1985945 quantize_finetune_llama.py:184] layer 28 gpu 0
I0313 04:24:01.493046 2030729 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 04:24:08.802596 2029978 finetune.py:68] layer 26_v @ epoch 1 new loss 0.0007316799601539969 old loss 0.000762134906835854 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 04:24:13.781249 2028426 finetune.py:68] layer 24_q @ epoch 0 new loss 0.0006118853343650699 old loss 0.0006310620228759944 BETTER
I0313 04:24:18.461362 2030729 finetune.py:45] layer 27_v initial loss 0.001056025386787951
I0313 04:24:32.879370 2029233 finetune.py:68] layer 25_v @ epoch 4 new loss 0.0004520502989180386 old loss 0.0004569278098642826 BETTER
I0313 04:24:41.534081 2029978 finetune.py:68] layer 26_v @ epoch 2 new loss 0.0007171639008447528 old loss 0.0007316799601539969 BETTER
I0313 04:24:48.212795 2028426 finetune.py:68] layer 24_q @ epoch 1 new loss 0.0006041598389856517 old loss 0.0006118853343650699 BETTER
I0313 04:24:49.951608 2030729 finetune.py:68] layer 27_v @ epoch 0 new loss 0.0006840737187303603 old loss 0.001056025386787951 BETTER
I0313 04:24:50.903809 2029233 finetune.py:45] layer 25_q initial loss 0.0005902741104364395
I0313 04:25:14.760584 2029978 finetune.py:68] layer 26_v @ epoch 3 new loss 0.000707880244590342 old loss 0.0007171639008447528 BETTER
I0313 04:25:22.304371 2030729 finetune.py:68] layer 27_v @ epoch 1 new loss 0.0006625850801356137 old loss 0.0006840737187303603 BETTER
I0313 04:25:22.383288 2028426 finetune.py:68] layer 24_q @ epoch 2 new loss 0.0005984030431136489 old loss 0.0006041598389856517 BETTER
I0313 04:25:22.437919 2029233 finetune.py:68] layer 25_q @ epoch 0 new loss 0.0005659267771989107 old loss 0.0005902741104364395 BETTER
I0313 04:25:48.015045 2029978 finetune.py:68] layer 26_v @ epoch 4 new loss 0.0007006754167377949 old loss 0.000707880244590342 BETTER
I0313 04:25:54.401304 2030729 finetune.py:68] layer 27_v @ epoch 2 new loss 0.0006525112548843026 old loss 0.0006625850801356137 BETTER
I0313 04:25:54.553741 2029233 finetune.py:68] layer 25_q @ epoch 1 new loss 0.0005585909821093082 old loss 0.0005659267771989107 BETTER
I0313 04:25:56.838416 2028426 finetune.py:68] layer 24_q @ epoch 3 new loss 0.0005937120877206326 old loss 0.0005984030431136489 BETTER
I0313 04:26:05.919543 2029978 finetune.py:45] layer 26_q initial loss 0.0008964221342466772
I0313 04:26:26.684150 2029233 finetune.py:68] layer 25_q @ epoch 2 new loss 0.0005533312214538455 old loss 0.0005585909821093082 BETTER
I0313 04:26:26.742373 2030729 finetune.py:68] layer 27_v @ epoch 3 new loss 0.0006454105023294687 old loss 0.0006525112548843026 BETTER
I0313 04:26:31.406459 2028426 finetune.py:68] layer 24_q @ epoch 4 new loss 0.0005899060051888227 old loss 0.0005937120877206326 BETTER
I0313 04:26:37.378481 2029978 finetune.py:68] layer 26_q @ epoch 0 new loss 0.0008725335355848074 old loss 0.0008964221342466772 BETTER
I0313 04:26:49.232248 2028426 finetune.py:45] layer 24_k initial loss 0.0007169285672716796
I0313 04:26:59.112325 2029233 finetune.py:68] layer 25_q @ epoch 3 new loss 0.0005492793279699981 old loss 0.0005533312214538455 BETTER
I0313 04:26:59.362905 2030729 finetune.py:68] layer 27_v @ epoch 4 new loss 0.0006400238489732146 old loss 0.0006454105023294687 BETTER
I0313 04:27:10.150184 2029978 finetune.py:68] layer 26_q @ epoch 1 new loss 0.0008620494045317173 old loss 0.0008725335355848074 BETTER
I0313 04:27:16.895637 2030729 finetune.py:45] layer 27_q initial loss 0.0008538224501535296
I0313 04:27:21.873311 2028426 finetune.py:68] layer 24_k @ epoch 0 new loss 0.000710075197275728 old loss 0.0007169285672716796 BETTER
I0313 04:27:31.607248 2029233 finetune.py:68] layer 25_q @ epoch 4 new loss 0.0005457078805193305 old loss 0.0005492793279699981 BETTER
I0313 04:27:42.739933 2029978 finetune.py:68] layer 26_q @ epoch 2 new loss 0.0008543338626623154 old loss 0.0008620494045317173 BETTER
I0313 04:27:48.166754 2030729 finetune.py:68] layer 27_q @ epoch 0 new loss 0.0008198505383916199 old loss 0.0008538224501535296 BETTER
I0313 04:27:49.435673 2029233 finetune.py:45] layer 25_k initial loss 0.0006846931064501405
I0313 04:27:57.898941 2028426 finetune.py:68] layer 24_k @ epoch 1 new loss 0.0007056762697175145 old loss 0.000710075197275728 BETTER
I0313 04:28:16.296292 2029978 finetune.py:68] layer 26_q @ epoch 3 new loss 0.0008474349160678685 old loss 0.0008543338626623154 BETTER
I0313 04:28:22.065566 2030729 finetune.py:68] layer 27_q @ epoch 1 new loss 0.0008086905581876636 old loss 0.0008198505383916199 BETTER
I0313 04:28:22.401793 2029233 finetune.py:68] layer 25_k @ epoch 0 new loss 0.0006771640037186444 old loss 0.0006846931064501405 BETTER
I0313 04:28:32.196582 2028426 finetune.py:68] layer 24_k @ epoch 2 new loss 0.0007020791526883841 old loss 0.0007056762697175145 BETTER
I0313 04:28:49.732721 2029978 finetune.py:68] layer 26_q @ epoch 4 new loss 0.0008418988436460495 old loss 0.0008474349160678685 BETTER
I0313 04:28:55.511783 2030729 finetune.py:68] layer 27_q @ epoch 2 new loss 0.0008000959060154855 old loss 0.0008086905581876636 BETTER
I0313 04:28:55.825549 2029233 finetune.py:68] layer 25_k @ epoch 1 new loss 0.0006722771213389933 old loss 0.0006771640037186444 BETTER
I0313 04:29:06.676972 2028426 finetune.py:68] layer 24_k @ epoch 3 new loss 0.0006990926922298968 old loss 0.0007020791526883841 BETTER
I0313 04:29:11.262819 2029978 finetune.py:45] layer 26_k initial loss 0.001017923466861248
I0313 04:29:29.385955 2030729 finetune.py:68] layer 27_q @ epoch 3 new loss 0.0007933721644803882 old loss 0.0008000959060154855 BETTER
I0313 04:29:29.720247 2029233 finetune.py:68] layer 25_k @ epoch 2 new loss 0.0006683831452392042 old loss 0.0006722771213389933 BETTER
I0313 04:29:41.992372 2028426 finetune.py:68] layer 24_k @ epoch 4 new loss 0.0006964161293581128 old loss 0.0006990926922298968 BETTER
I0313 04:29:43.560477 2029978 finetune.py:68] layer 26_k @ epoch 0 new loss 0.0010078171035274863 old loss 0.001017923466861248 BETTER
I0313 04:29:59.774885 2028426 finetune.py:45] layer 24_o initial loss 0.0014756686287000775
I0313 04:30:01.688238 2030729 finetune.py:68] layer 27_q @ epoch 4 new loss 0.0007874780567362905 old loss 0.0007933721644803882 BETTER
I0313 04:30:02.159368 2029233 finetune.py:68] layer 25_k @ epoch 3 new loss 0.0006651139701716602 old loss 0.0006683831452392042 BETTER
I0313 04:30:15.686619 2029978 finetune.py:68] layer 26_k @ epoch 1 new loss 0.001002380857244134 old loss 0.0010078171035274863 BETTER
I0313 04:30:19.179852 2030729 finetune.py:45] layer 27_k initial loss 0.0009634674061089754
I0313 04:30:31.608158 2028426 finetune.py:68] layer 24_o @ epoch 0 new loss 0.0014545853482559323 old loss 0.0014756686287000775 BETTER
I0313 04:30:34.250280 2029233 finetune.py:68] layer 25_k @ epoch 4 new loss 0.000662374310195446 old loss 0.0006651139701716602 BETTER
I0313 04:30:47.863269 2029978 finetune.py:68] layer 26_k @ epoch 2 new loss 0.0009977188892662525 old loss 0.001002380857244134 BETTER
I0313 04:30:49.792289 2030729 finetune.py:68] layer 27_k @ epoch 0 new loss 0.0009552383562549949 old loss 0.0009634674061089754 BETTER
I0313 04:30:52.356868 2029233 finetune.py:45] layer 25_o initial loss 0.0013219316024333239
I0313 04:31:04.699430 2028426 finetune.py:68] layer 24_o @ epoch 1 new loss 0.0014464522246271372 old loss 0.0014545853482559323 BETTER
I0313 04:31:20.119805 2029978 finetune.py:68] layer 26_k @ epoch 3 new loss 0.0009937549475580454 old loss 0.0009977188892662525 BETTER
I0313 04:31:21.352620 2030729 finetune.py:68] layer 27_k @ epoch 1 new loss 0.0009498224244453013 old loss 0.0009552383562549949 BETTER
I0313 04:31:22.751994 2029233 finetune.py:68] layer 25_o @ epoch 0 new loss 0.0012964963680133224 old loss 0.0013219316024333239 BETTER
I0313 04:31:38.029466 2028426 finetune.py:68] layer 24_o @ epoch 2 new loss 0.0014402802335098386 old loss 0.0014464522246271372 BETTER
I0313 04:31:52.265676 2029978 finetune.py:68] layer 26_k @ epoch 4 new loss 0.0009902969468384981 old loss 0.0009937549475580454 BETTER
I0313 04:31:52.944081 2030729 finetune.py:68] layer 27_k @ epoch 2 new loss 0.0009454133687540889 old loss 0.0009498224244453013 BETTER
I0313 04:31:53.963458 2029233 finetune.py:68] layer 25_o @ epoch 1 new loss 0.0012885795440524817 old loss 0.0012964963680133224 BETTER
I0313 04:32:10.440773 2029978 finetune.py:45] layer 26_o initial loss 0.0019357443088665605
I0313 04:32:11.398751 2028426 finetune.py:68] layer 24_o @ epoch 3 new loss 0.0014353831065818667 old loss 0.0014402802335098386 BETTER
I0313 04:32:24.603960 2030729 finetune.py:68] layer 27_k @ epoch 3 new loss 0.0009417302790097892 old loss 0.0009454133687540889 BETTER
I0313 04:32:25.374998 2029233 finetune.py:68] layer 25_o @ epoch 2 new loss 0.0012827826431021094 old loss 0.0012885795440524817 BETTER
I0313 04:32:41.028831 2029978 finetune.py:68] layer 26_o @ epoch 0 new loss 0.0019056538585573435 old loss 0.0019357443088665605 BETTER
I0313 04:32:44.788976 2028426 finetune.py:68] layer 24_o @ epoch 4 new loss 0.0014314199797809124 old loss 0.0014353831065818667 BETTER
I0313 04:32:56.337212 2030729 finetune.py:68] layer 27_k @ epoch 4 new loss 0.0009384482400491834 old loss 0.0009417302790097892 BETTER
I0313 04:32:56.744965 2029233 finetune.py:68] layer 25_o @ epoch 3 new loss 0.001278091687709093 old loss 0.0012827826431021094 BETTER
I0313 04:33:07.965224 2028426 finetune.py:45] layer 24_up initial loss 0.0031798765994608402
I0313 04:33:12.671062 2029978 finetune.py:68] layer 26_o @ epoch 1 new loss 0.0018938904395326972 old loss 0.0019056538585573435 BETTER
I0313 04:33:14.590062 2030729 finetune.py:45] layer 27_o initial loss 0.001767475507222116
I0313 04:33:28.323498 2029233 finetune.py:68] layer 25_o @ epoch 4 new loss 0.0012741837417706847 old loss 0.001278091687709093 BETTER
I0313 04:33:38.389577 2028426 finetune.py:68] layer 24_up @ epoch 0 new loss 0.003159250132739544 old loss 0.0031798765994608402 BETTER
I0313 04:33:45.063212 2029978 finetune.py:68] layer 26_o @ epoch 2 new loss 0.001884588971734047 old loss 0.0018938904395326972 BETTER
I0313 04:33:45.278704 2030729 finetune.py:68] layer 27_o @ epoch 0 new loss 0.0017347370740026236 old loss 0.001767475507222116 BETTER
I0313 04:33:52.122362 2029233 finetune.py:45] layer 25_up initial loss 0.003251550253480673
I0313 04:34:09.822435 2028426 finetune.py:68] layer 24_up @ epoch 1 new loss 0.003146675182506442 old loss 0.003159250132739544 BETTER
I0313 04:34:16.528674 2030729 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0017213119426742196 old loss 0.0017347370740026236 BETTER
I0313 04:34:17.240988 2029978 finetune.py:68] layer 26_o @ epoch 3 new loss 0.0018771042814478278 old loss 0.001884588971734047 BETTER
I0313 04:34:21.727792 2029233 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0032288834918290377 old loss 0.003251550253480673 BETTER
I0313 04:34:41.500667 2028426 finetune.py:68] layer 24_up @ epoch 2 new loss 0.003136679530143738 old loss 0.003146675182506442 BETTER
I0313 04:34:47.448464 2030729 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0017109309555962682 old loss 0.0017213119426742196 BETTER
I0313 04:34:49.408503 2029978 finetune.py:68] layer 26_o @ epoch 4 new loss 0.001870919717475772 old loss 0.0018771042814478278 BETTER
I0313 04:34:51.589036 2029233 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0032152999192476273 old loss 0.0032288834918290377 BETTER
I0313 04:35:14.304524 2028426 finetune.py:68] layer 24_up @ epoch 3 new loss 0.0031283756252378225 old loss 0.003136679530143738 BETTER
I0313 04:35:15.587386 2029978 finetune.py:45] layer 26_up initial loss 0.004039456602185965
I0313 04:35:18.746104 2030729 finetune.py:68] layer 27_o @ epoch 3 new loss 0.0017026133136823773 old loss 0.0017109309555962682 BETTER
I0313 04:35:21.440636 2029233 finetune.py:68] layer 25_up @ epoch 2 new loss 0.003204345703125 old loss 0.0032152999192476273 BETTER
I0313 04:35:44.657391 2029978 finetune.py:68] layer 26_up @ epoch 0 new loss 0.004014085978269577 old loss 0.004039456602185965 BETTER
I0313 04:35:46.502717 2028426 finetune.py:68] layer 24_up @ epoch 4 new loss 0.003121135989204049 old loss 0.0031283756252378225 BETTER
I0313 04:35:50.421847 2030729 finetune.py:68] layer 27_o @ epoch 4 new loss 0.0016956399194896221 old loss 0.0017026133136823773 BETTER
I0313 04:35:51.822068 2029233 finetune.py:68] layer 25_up @ epoch 3 new loss 0.00319500919431448 old loss 0.003204345703125 BETTER
I0313 04:36:13.829727 2028426 finetune.py:45] layer 24_gate initial loss 0.00454994710162282
I0313 04:36:16.924987 2029978 finetune.py:68] layer 26_up @ epoch 1 new loss 0.003998781554400921 old loss 0.004014085978269577 BETTER
I0313 04:36:17.540444 2030729 finetune.py:45] layer 27_up initial loss 0.00414135679602623
I0313 04:36:22.260408 2029233 finetune.py:68] layer 25_up @ epoch 4 new loss 0.00318685220554471 old loss 0.00319500919431448 BETTER
I0313 04:36:43.207977 2028426 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0045389956794679165 old loss 0.00454994710162282 BETTER
I0313 04:36:47.797652 2030729 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0041110944002866745 old loss 0.00414135679602623 BETTER
I0313 04:36:48.159102 2029233 finetune.py:45] layer 25_gate initial loss 0.004794931039214134
I0313 04:36:48.749868 2029978 finetune.py:68] layer 26_up @ epoch 2 new loss 0.003986372612416744 old loss 0.003998781554400921 BETTER
I0313 04:37:14.831307 2028426 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.004529525525867939 old loss 0.0045389956794679165 BETTER
I0313 04:37:17.915556 2029233 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.004783175885677338 old loss 0.004794931039214134 BETTER
I0313 04:37:19.166695 2030729 finetune.py:68] layer 27_up @ epoch 1 new loss 0.004092616029083729 old loss 0.0041110944002866745 BETTER
I0313 04:37:20.119337 2029978 finetune.py:68] layer 26_up @ epoch 3 new loss 0.003975895699113607 old loss 0.003986372612416744 BETTER
I0313 04:37:46.682957 2028426 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.004521183203905821 old loss 0.004529525525867939 BETTER
I0313 04:37:48.665511 2029233 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.004773497581481934 old loss 0.004783175885677338 BETTER
I0313 04:37:50.536902 2030729 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0040778969414532185 old loss 0.004092616029083729 BETTER
I0313 04:37:51.772292 2029978 finetune.py:68] layer 26_up @ epoch 4 new loss 0.00396658293902874 old loss 0.003975895699113607 BETTER
I0313 04:38:18.822432 2029978 finetune.py:45] layer 26_gate initial loss 0.005777623038738966
I0313 04:38:19.484714 2028426 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.004513973835855722 old loss 0.004521183203905821 BETTER
I0313 04:38:19.912811 2029233 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.00476487260311842 old loss 0.004773497581481934 BETTER
I0313 04:38:22.513067 2030729 finetune.py:68] layer 27_up @ epoch 3 new loss 0.00406534131616354 old loss 0.0040778969414532185 BETTER
I0313 04:38:47.400916 2029978 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0057643111795187 old loss 0.005777623038738966 BETTER
I0313 04:38:50.001745 2029233 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.00475725531578064 old loss 0.00476487260311842 BETTER
I0313 04:38:50.789614 2028426 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.004507211036980152 old loss 0.004513973835855722 BETTER
I0313 04:38:53.067123 2030729 finetune.py:68] layer 27_up @ epoch 4 new loss 0.004054637160152197 old loss 0.00406534131616354 BETTER
I0313 04:39:16.970964 2029978 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.00575356325134635 old loss 0.0057643111795187 BETTER
I0313 04:39:19.157778 2030729 finetune.py:45] layer 27_gate initial loss 0.00613274984061718
I0313 04:39:19.590641 2029233 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.004750329069793224 old loss 0.00475725531578064 BETTER
I0313 04:39:35.792605 2028426 finetune.py:45] layer 24_down initial loss 0.007202476263046265
I0313 04:39:47.055198 2029978 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.005743979010730982 old loss 0.00575356325134635 BETTER
I0313 04:39:47.335492 2030729 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0061165764927864075 old loss 0.00613274984061718 BETTER
I0313 04:40:03.787367 2028426 finetune.py:68] layer 24_down @ epoch 0 new loss 0.007200193591415882 old loss 0.007202476263046265 BETTER
I0313 04:40:04.987712 2029233 finetune.py:45] layer 25_down initial loss 0.007577943615615368
I0313 04:40:17.749185 2030729 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.006104126572608948 old loss 0.0061165764927864075 BETTER
I0313 04:40:17.928060 2029978 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.005735349841415882 old loss 0.005743979010730982 BETTER
I0313 04:40:31.909256 2029233 finetune.py:68] layer 25_down @ epoch 0 new loss 0.007575654424726963 old loss 0.007577943615615368 BETTER
I0313 04:40:33.623591 2028426 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0071981498040258884 old loss 0.007200193591415882 BETTER
I0313 04:40:47.265484 2030729 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.006093278527259827 old loss 0.006104126572608948 BETTER
I0313 04:40:47.806823 2029978 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.005727757699787617 old loss 0.005735349841415882 BETTER
I0313 04:40:58.715785 2029233 finetune.py:68] layer 25_down @ epoch 1 new loss 0.007573608309030533 old loss 0.007575654424726963 BETTER
I0313 04:41:01.992971 2028426 finetune.py:68] layer 24_down @ epoch 2 new loss 0.007196311838924885 old loss 0.0071981498040258884 BETTER
I0313 04:41:15.363162 2030729 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.006083392072468996 old loss 0.006093278527259827 BETTER
I0313 04:41:25.638015 2029233 finetune.py:68] layer 25_down @ epoch 2 new loss 0.007571800611913204 old loss 0.007573608309030533 BETTER
I0313 04:41:28.666597 2029978 finetune.py:45] layer 26_down initial loss 0.008802824653685093
I0313 04:41:30.512262 2028426 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0071946922689676285 old loss 0.007196311838924885 BETTER
I0313 04:41:43.412535 2030729 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.006074678618460894 old loss 0.006083392072468996 BETTER
I0313 04:41:52.559632 2029233 finetune.py:68] layer 25_down @ epoch 3 new loss 0.007570160552859306 old loss 0.007571800611913204 BETTER
I0313 04:41:54.225348 2029978 finetune.py:68] layer 26_down @ epoch 0 new loss 0.008800443261861801 old loss 0.008802824653685093 BETTER
I0313 04:41:59.016635 2028426 finetune.py:68] layer 24_down @ epoch 4 new loss 0.007193225435912609 old loss 0.0071946922689676285 BETTER
24_v proxy err 0.029909178614616394 tr(WHW.T) 1394.900634765625
24_q proxy err 0.006115851923823357 tr(WHW.T) 7028.580078125
24_k proxy err 0.004160208161920309 tr(WHW.T) 10334.55078125
24_o proxy err 0.02278079465031624 tr(WHW.T) 134.63221740722656
24_up proxy err 0.02805832028388977 tr(WHW.T) 2622.879150390625
24_gate proxy err 0.017609382048249245 tr(WHW.T) 4258.77294921875
24_down proxy err 0.03304572030901909 tr(WHW.T) 342.52764892578125
I0313 04:42:20.230384 2029233 finetune.py:68] layer 25_down @ epoch 4 new loss 0.007568701636046171 old loss 0.007570160552859306 BETTER
I0313 04:42:21.629934 2029978 finetune.py:68] layer 26_down @ epoch 1 new loss 0.00879839900881052 old loss 0.008800443261861801 BETTER
25_v proxy err 0.02896587736904621 tr(WHW.T) 1707.664794921875
25_q proxy err 0.007082145195454359 tr(WHW.T) 7169.00927734375
25_k proxy err 0.0052821217104792595 tr(WHW.T) 9620.4560546875
25_o proxy err 0.029909254983067513 tr(WHW.T) 84.19975280761719
25_up proxy err 0.02805032767355442 tr(WHW.T) 2807.2412109375
25_gate proxy err 0.01720447652041912 tr(WHW.T) 4660.46533203125
25_down proxy err 0.03172127157449722 tr(WHW.T) 376.1217041015625
I0313 04:42:25.953969 2030729 finetune.py:45] layer 27_down initial loss 0.009485756047070026
I0313 04:42:48.502318 2029978 finetune.py:68] layer 26_down @ epoch 2 new loss 0.008796557784080505 old loss 0.00879839900881052 BETTER
I0313 04:42:50.891264 2030729 finetune.py:68] layer 27_down @ epoch 0 new loss 0.009483424946665764 old loss 0.009485756047070026 BETTER
I0313 04:43:15.513582 2029978 finetune.py:68] layer 26_down @ epoch 3 new loss 0.008794899098575115 old loss 0.008796557784080505 BETTER
I0313 04:43:16.845632 2030729 finetune.py:68] layer 27_down @ epoch 1 new loss 0.009481381624937057 old loss 0.009483424946665764 BETTER
I0313 04:43:36.717110 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 28 in 71.73224878311157s
I0313 04:43:39.998333 2041089 config.py:54] PyTorch version 2.1.1 available.
I0313 04:43:41.130886 1985945 quantize_finetune_llama.py:184] layer 29 gpu 1
I0313 04:43:41.211787 2041089 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 04:43:42.325197 2029978 finetune.py:68] layer 26_down @ epoch 4 new loss 0.008793448098003864 old loss 0.008794899098575115 BETTER
I0313 04:43:42.852203 2030729 finetune.py:68] layer 27_down @ epoch 2 new loss 0.009479520842432976 old loss 0.009481381624937057 BETTER
26_v proxy err 0.028413062915205956 tr(WHW.T) 1668.8843994140625
26_q proxy err 0.006452167872339487 tr(WHW.T) 7479.4521484375
26_k proxy err 0.004601928871124983 tr(WHW.T) 10501.0712890625
26_o proxy err 0.01798025332391262 tr(WHW.T) 203.91177368164062
26_up proxy err 0.02630757726728916 tr(WHW.T) 3156.910888671875
26_gate proxy err 0.01595885120332241 tr(WHW.T) 5298.45947265625
26_down proxy err 0.03218100592494011 tr(WHW.T) 404.1075439453125
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 04:43:58.321914 2041089 finetune.py:45] layer 28_v initial loss 0.0015367580344900489
I0313 04:44:09.050012 2030729 finetune.py:68] layer 27_down @ epoch 3 new loss 0.009477897547185421 old loss 0.009479520842432976 BETTER
I0313 04:44:30.950067 2041089 finetune.py:68] layer 28_v @ epoch 0 new loss 0.0008935551159083843 old loss 0.0015367580344900489 BETTER
I0313 04:44:35.096346 2030729 finetune.py:68] layer 27_down @ epoch 4 new loss 0.009476417675614357 old loss 0.009477897547185421 BETTER
27_v proxy err 0.027433285489678383 tr(WHW.T) 1799.3350830078125
27_q proxy err 0.006610726937651634 tr(WHW.T) 7696.2060546875
27_k proxy err 0.004800710361450911 tr(WHW.T) 10626.662109375
27_o proxy err 0.024659961462020874 tr(WHW.T) 127.12421417236328
27_up proxy err 0.023794585838913918 tr(WHW.T) 3689.086669921875
27_gate proxy err 0.014933728612959385 tr(WHW.T) 5975.892578125
27_down proxy err 0.030847562476992607 tr(WHW.T) 470.0771789550781
I0313 04:44:55.609912 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 29 in 69.64482426643372s
I0313 04:44:58.761209 2041895 config.py:54] PyTorch version 2.1.1 available.
I0313 04:44:59.739670 1985945 quantize_finetune_llama.py:184] layer 30 gpu 2
I0313 04:44:59.812407 2041895 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 04:45:05.100640 2041089 finetune.py:68] layer 28_v @ epoch 1 new loss 0.0008596231346018612 old loss 0.0008935551159083843 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 04:45:16.219310 2041895 finetune.py:45] layer 29_v initial loss 0.0013374206610023975
I0313 04:45:39.666371 2041089 finetune.py:68] layer 28_v @ epoch 2 new loss 0.0008440703386440873 old loss 0.0008596231346018612 BETTER
I0313 04:45:47.359381 2041895 finetune.py:68] layer 29_v @ epoch 0 new loss 0.0009438477572984993 old loss 0.0013374206610023975 BETTER
I0313 04:46:08.744081 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 30 in 68.62295484542847s
I0313 04:46:12.021587 2042642 config.py:54] PyTorch version 2.1.1 available.
I0313 04:46:13.078096 1985945 quantize_finetune_llama.py:184] layer 31 gpu 3
I0313 04:46:13.148432 2042642 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 04:46:14.283097 2041089 finetune.py:68] layer 28_v @ epoch 3 new loss 0.0008332900470122695 old loss 0.0008440703386440873 BETTER
I0313 04:46:19.387613 2041895 finetune.py:68] layer 29_v @ epoch 1 new loss 0.0009211694123223424 old loss 0.0009438477572984993 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 04:46:30.288312 2042642 finetune.py:45] layer 30_v initial loss 0.0015129573876038194
I0313 04:46:49.260687 2041089 finetune.py:68] layer 28_v @ epoch 4 new loss 0.0008253575069829822 old loss 0.0008332900470122695 BETTER
I0313 04:46:51.986128 2041895 finetune.py:68] layer 29_v @ epoch 2 new loss 0.000907620822545141 old loss 0.0009211694123223424 BETTER
I0313 04:47:01.877285 2042642 finetune.py:68] layer 30_v @ epoch 0 new loss 0.0009226711699739099 old loss 0.0015129573876038194 BETTER
I0313 04:47:07.595790 2041089 finetune.py:45] layer 28_q initial loss 0.0010723130544647574
I0313 04:47:24.614383 2041895 finetune.py:68] layer 29_v @ epoch 3 new loss 0.0008986843167804182 old loss 0.000907620822545141 BETTER
I0313 04:47:25.263430 1985945 quantize_finetune_llama.py:211] computed original embedding for layer 31 in 71.71100687980652s
I0313 04:47:28.459028 2043428 config.py:54] PyTorch version 2.1.1 available.
I0313 04:47:29.658216 2043428 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0313 04:47:34.387706 2042642 finetune.py:68] layer 30_v @ epoch 1 new loss 0.000898404570762068 old loss 0.0009226711699739099 BETTER
I0313 04:47:40.670738 2041089 finetune.py:68] layer 28_q @ epoch 0 new loss 0.0010351453674957156 old loss 0.0010723130544647574 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0313 04:47:46.790951 2043428 finetune.py:45] layer 31_v initial loss 0.0018340464448556304
I0313 04:47:57.408061 2041895 finetune.py:68] layer 29_v @ epoch 4 new loss 0.000890730821993202 old loss 0.0008986843167804182 BETTER
I0313 04:48:06.962006 2042642 finetune.py:68] layer 30_v @ epoch 2 new loss 0.0008843152900226414 old loss 0.000898404570762068 BETTER
I0313 04:48:14.939778 2041089 finetune.py:68] layer 28_q @ epoch 1 new loss 0.0010226360755041242 old loss 0.0010351453674957156 BETTER
I0313 04:48:15.758831 2041895 finetune.py:45] layer 29_q initial loss 0.001116845989599824
I0313 04:48:17.892853 2043428 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0013608387671411037 old loss 0.0018340464448556304 BETTER
I0313 04:48:39.632513 2042642 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0008745523518882692 old loss 0.0008843152900226414 BETTER
I0313 04:48:47.210327 2041895 finetune.py:68] layer 29_q @ epoch 0 new loss 0.0010842891642823815 old loss 0.001116845989599824 BETTER
I0313 04:48:49.565146 2041089 finetune.py:68] layer 28_q @ epoch 2 new loss 0.0010128718568012118 old loss 0.0010226360755041242 BETTER
I0313 04:48:50.043745 2043428 finetune.py:68] layer 31_v @ epoch 1 new loss 0.0013074638554826379 old loss 0.0013608387671411037 BETTER
I0313 04:49:12.682917 2042642 finetune.py:68] layer 30_v @ epoch 4 new loss 0.0008666271460242569 old loss 0.0008745523518882692 BETTER
I0313 04:49:19.814958 2041895 finetune.py:68] layer 29_q @ epoch 1 new loss 0.0010731735965237021 old loss 0.0010842891642823815 BETTER
I0313 04:49:22.553573 2043428 finetune.py:68] layer 31_v @ epoch 2 new loss 0.0012812863569706678 old loss 0.0013074638554826379 BETTER
I0313 04:49:24.143117 2041089 finetune.py:68] layer 28_q @ epoch 3 new loss 0.0010045997332781553 old loss 0.0010128718568012118 BETTER
I0313 04:49:30.375943 2042642 finetune.py:45] layer 30_q initial loss 0.0011750113917514682
I0313 04:49:52.113092 2041895 finetune.py:68] layer 29_q @ epoch 2 new loss 0.0010650336043909192 old loss 0.0010731735965237021 BETTER
I0313 04:49:54.984054 2043428 finetune.py:68] layer 31_v @ epoch 3 new loss 0.0012790932087227702 old loss 0.0012812863569706678 BETTER
I0313 04:49:58.642112 2041089 finetune.py:68] layer 28_q @ epoch 4 new loss 0.0009978503221645951 old loss 0.0010045997332781553 BETTER
I0313 04:50:01.945003 2042642 finetune.py:68] layer 30_q @ epoch 0 new loss 0.001126372255384922 old loss 0.0011750113917514682 BETTER
I0313 04:50:18.603689 2041089 finetune.py:45] layer 28_k initial loss 0.0012312957551330328
I0313 04:50:25.427164 2041895 finetune.py:68] layer 29_q @ epoch 3 new loss 0.0010581179521977901 old loss 0.0010650336043909192 BETTER
I0313 04:50:28.546970 2043428 finetune.py:68] layer 31_v @ epoch 4 new loss 0.0012530668172985315 old loss 0.0012790932087227702 BETTER
I0313 04:50:34.832445 2042642 finetune.py:68] layer 30_q @ epoch 1 new loss 0.0011108252219855785 old loss 0.001126372255384922 BETTER
I0313 04:50:48.902389 2043428 finetune.py:45] layer 31_q initial loss 0.002373688155785203
I0313 04:50:52.000876 2041089 finetune.py:68] layer 28_k @ epoch 0 new loss 0.001219463418237865 old loss 0.0012312957551330328 BETTER
I0313 04:50:59.097349 2041895 finetune.py:68] layer 29_q @ epoch 4 new loss 0.0010524580720812082 old loss 0.0010581179521977901 BETTER
I0313 04:51:08.294396 2042642 finetune.py:68] layer 30_q @ epoch 2 new loss 0.0010996217606589198 old loss 0.0011108252219855785 BETTER
I0313 04:51:19.398916 2041895 finetune.py:45] layer 29_k initial loss 0.0012715490302070975
I0313 04:51:21.345277 2043428 finetune.py:68] layer 31_q @ epoch 0 new loss 0.002203715732321143 old loss 0.002373688155785203 BETTER
I0313 04:51:26.694986 2041089 finetune.py:68] layer 28_k @ epoch 1 new loss 0.0012123167980462313 old loss 0.001219463418237865 BETTER
I0313 04:51:41.874776 2042642 finetune.py:68] layer 30_q @ epoch 3 new loss 0.001090164529159665 old loss 0.0010996217606589198 BETTER
I0313 04:51:51.796528 2041895 finetune.py:68] layer 29_k @ epoch 0 new loss 0.001260972348973155 old loss 0.0012715490302070975 BETTER
I0313 04:51:54.703678 2043428 finetune.py:68] layer 31_q @ epoch 1 new loss 0.002140796510502696 old loss 0.002203715732321143 BETTER
I0313 04:52:00.765829 2041089 finetune.py:68] layer 28_k @ epoch 2 new loss 0.0012069829972460866 old loss 0.0012123167980462313 BETTER
I0313 04:52:14.567786 2042642 finetune.py:68] layer 30_q @ epoch 4 new loss 0.0010819281451404095 old loss 0.001090164529159665 BETTER
I0313 04:52:24.263550 2041895 finetune.py:68] layer 29_k @ epoch 1 new loss 0.0012544081546366215 old loss 0.001260972348973155 BETTER
I0313 04:52:27.055025 2043428 finetune.py:68] layer 31_q @ epoch 2 new loss 0.0021023734007030725 old loss 0.002140796510502696 BETTER
I0313 04:52:32.278143 2042642 finetune.py:45] layer 30_k initial loss 0.0013483407674357295
I0313 04:52:34.513525 2041089 finetune.py:68] layer 28_k @ epoch 3 new loss 0.0012021181173622608 old loss 0.0012069829972460866 BETTER
I0313 04:52:56.335216 2041895 finetune.py:68] layer 29_k @ epoch 2 new loss 0.0012492791283875704 old loss 0.0012544081546366215 BETTER
I0313 04:52:59.069922 2043428 finetune.py:68] layer 31_q @ epoch 3 new loss 0.002066149143502116 old loss 0.0021023734007030725 BETTER
I0313 04:53:03.673047 2042642 finetune.py:68] layer 30_k @ epoch 0 new loss 0.0013345619663596153 old loss 0.0013483407674357295 BETTER
I0313 04:53:08.340870 2041089 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0011981906136497855 old loss 0.0012021181173622608 BETTER
I0313 04:53:26.301487 2041089 finetune.py:45] layer 28_o initial loss 0.0022256982047110796
I0313 04:53:28.486652 2041895 finetune.py:68] layer 29_k @ epoch 3 new loss 0.0012446262408047915 old loss 0.0012492791283875704 BETTER
I0313 04:53:31.198160 2043428 finetune.py:68] layer 31_q @ epoch 4 new loss 0.00203985208645463 old loss 0.002066149143502116 BETTER
I0313 04:53:35.783467 2042642 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0013272358337417245 old loss 0.0013345619663596153 BETTER
I0313 04:53:48.993526 2043428 finetune.py:45] layer 31_k initial loss 0.00236507854424417
I0313 04:53:58.264522 2041089 finetune.py:68] layer 28_o @ epoch 0 new loss 0.002178968396037817 old loss 0.0022256982047110796 BETTER
I0313 04:54:00.616168 2041895 finetune.py:68] layer 29_k @ epoch 4 new loss 0.0012410234194248915 old loss 0.0012446262408047915 BETTER
I0313 04:54:07.793635 2042642 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0013206824660301208 old loss 0.0013272358337417245 BETTER
I0313 04:54:19.022071 2041895 finetune.py:45] layer 29_o initial loss 0.0022549480199813843
I0313 04:54:20.069511 2043428 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0023041514214128256 old loss 0.00236507854424417 BETTER
I0313 04:54:31.553238 2041089 finetune.py:68] layer 28_o @ epoch 1 new loss 0.002163429046049714 old loss 0.002178968396037817 BETTER
I0313 04:54:39.764012 2042642 finetune.py:68] layer 30_k @ epoch 3 new loss 0.0013153491308912635 old loss 0.0013206824660301208 BETTER
I0313 04:54:49.537362 2041895 finetune.py:68] layer 29_o @ epoch 0 new loss 0.002220850670710206 old loss 0.0022549480199813843 BETTER
I0313 04:54:51.729157 2043428 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0022745560854673386 old loss 0.0023041514214128256 BETTER
I0313 04:55:05.168061 2041089 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0021511311642825603 old loss 0.002163429046049714 BETTER
I0313 04:55:11.833566 2042642 finetune.py:68] layer 30_k @ epoch 4 new loss 0.0013107294216752052 old loss 0.0013153491308912635 BETTER
I0313 04:55:20.869867 2041895 finetune.py:68] layer 29_o @ epoch 1 new loss 0.002207227284088731 old loss 0.002220850670710206 BETTER
I0313 04:55:23.278203 2043428 finetune.py:68] layer 31_k @ epoch 2 new loss 0.002255574334412813 old loss 0.0022745560854673386 BETTER
I0313 04:55:29.579168 2042642 finetune.py:45] layer 30_o initial loss 0.0025112791918218136
I0313 04:55:38.756705 2041089 finetune.py:68] layer 28_o @ epoch 3 new loss 0.002141390461474657 old loss 0.0021511311642825603 BETTER
I0313 04:55:52.520725 2041895 finetune.py:68] layer 29_o @ epoch 2 new loss 0.0021972197573632 old loss 0.002207227284088731 BETTER
I0313 04:55:55.293761 2043428 finetune.py:68] layer 31_k @ epoch 3 new loss 0.002239272464066744 old loss 0.002255574334412813 BETTER
I0313 04:56:00.358373 2042642 finetune.py:68] layer 30_o @ epoch 0 new loss 0.0024437899701297283 old loss 0.0025112791918218136 BETTER
I0313 04:56:12.194555 2041089 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0021330760791897774 old loss 0.002141390461474657 BETTER
I0313 04:56:23.972762 2041895 finetune.py:68] layer 29_o @ epoch 3 new loss 0.0021889975760132074 old loss 0.0021972197573632 BETTER
I0313 04:56:27.319490 2043428 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0022275473456829786 old loss 0.002239272464066744 BETTER
I0313 04:56:31.809126 2042642 finetune.py:68] layer 30_o @ epoch 1 new loss 0.0024152228143066168 old loss 0.0024437899701297283 BETTER
I0313 04:56:35.623703 2041089 finetune.py:45] layer 28_up initial loss 0.004968303255736828
I0313 04:56:45.572362 2043428 finetune.py:45] layer 31_o initial loss 0.0037619369104504585
I0313 04:56:55.700525 2041895 finetune.py:68] layer 29_o @ epoch 4 new loss 0.002182555850595236 old loss 0.0021889975760132074 BETTER
I0313 04:57:03.493203 2042642 finetune.py:68] layer 30_o @ epoch 2 new loss 0.002395178424194455 old loss 0.0024152228143066168 BETTER
I0313 04:57:05.975553 2041089 finetune.py:68] layer 28_up @ epoch 0 new loss 0.00492993276566267 old loss 0.004968303255736828 BETTER
I0313 04:57:15.788731 2043428 finetune.py:68] layer 31_o @ epoch 0 new loss 0.003555624047294259 old loss 0.0037619369104504585 BETTER
I0313 04:57:20.549991 2041895 finetune.py:45] layer 29_up initial loss 0.00549166789278388
I0313 04:57:35.188584 2042642 finetune.py:68] layer 30_o @ epoch 3 new loss 0.0023789494298398495 old loss 0.002395178424194455 BETTER
I0313 04:57:37.268634 2041089 finetune.py:68] layer 28_up @ epoch 1 new loss 0.004906738176941872 old loss 0.00492993276566267 BETTER
I0313 04:57:46.818627 2043428 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0034944454673677683 old loss 0.003555624047294259 BETTER
I0313 04:57:49.889612 2041895 finetune.py:68] layer 29_up @ epoch 0 new loss 0.005442735273391008 old loss 0.00549166789278388 BETTER
I0313 04:58:08.740423 2042642 finetune.py:68] layer 30_o @ epoch 4 new loss 0.002365997526794672 old loss 0.0023789494298398495 BETTER
I0313 04:58:10.338469 2041089 finetune.py:68] layer 28_up @ epoch 2 new loss 0.004888259805738926 old loss 0.004906738176941872 BETTER
I0313 04:58:18.631408 2043428 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0034539427142590284 old loss 0.0034944454673677683 BETTER
I0313 04:58:20.135328 2041895 finetune.py:68] layer 29_up @ epoch 1 new loss 0.005415088497102261 old loss 0.005442735273391008 BETTER
I0313 04:58:35.622920 2042642 finetune.py:45] layer 30_up initial loss 0.00739256851375103
I0313 04:58:42.840694 2041089 finetune.py:68] layer 28_up @ epoch 3 new loss 0.004872305318713188 old loss 0.004888259805738926 BETTER
I0313 04:58:50.592995 2043428 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0034225706476718187 old loss 0.0034539427142590284 BETTER
I0313 04:58:50.783081 2041895 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0053929914720356464 old loss 0.005415088497102261 BETTER
I0313 04:59:05.266449 2042642 finetune.py:68] layer 30_up @ epoch 0 new loss 0.007290429901331663 old loss 0.00739256851375103 BETTER
I0313 04:59:15.326830 2041089 finetune.py:68] layer 28_up @ epoch 4 new loss 0.004858322907239199 old loss 0.004872305318713188 BETTER
I0313 04:59:21.708055 2041895 finetune.py:68] layer 29_up @ epoch 3 new loss 0.005374090746045113 old loss 0.0053929914720356464 BETTER
I0313 04:59:22.490386 2043428 finetune.py:68] layer 31_o @ epoch 4 new loss 0.0033985115587711334 old loss 0.0034225706476718187 BETTER
I0313 04:59:35.563355 2042642 finetune.py:68] layer 30_up @ epoch 1 new loss 0.007224648259580135 old loss 0.007290429901331663 BETTER
I0313 04:59:42.571046 2041089 finetune.py:45] layer 28_gate initial loss 0.0073141660541296005
I0313 04:59:50.060782 2043428 finetune.py:45] layer 31_up initial loss 0.015095408074557781
I0313 04:59:52.782376 2041895 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0053573474287986755 old loss 0.005374090746045113 BETTER
I0313 05:00:06.157419 2042642 finetune.py:68] layer 30_up @ epoch 2 new loss 0.007170966826379299 old loss 0.007224648259580135 BETTER
I0313 05:00:12.342633 2041089 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.00729458499699831 old loss 0.0073141660541296005 BETTER
I0313 05:00:19.760928 2043428 finetune.py:68] layer 31_up @ epoch 0 new loss 0.014574534259736538 old loss 0.015095408074557781 BETTER
I0313 05:00:20.324700 2041895 finetune.py:45] layer 29_gate initial loss 0.008246753364801407
I0313 05:00:37.285476 2042642 finetune.py:68] layer 30_up @ epoch 3 new loss 0.007125522941350937 old loss 0.007170966826379299 BETTER
I0313 05:00:42.934651 2041089 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.007279374171048403 old loss 0.00729458499699831 BETTER
I0313 05:00:49.215499 2041895 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.008222625590860844 old loss 0.008246753364801407 BETTER
I0313 05:00:50.813438 2043428 finetune.py:68] layer 31_up @ epoch 1 new loss 0.014283287338912487 old loss 0.014574534259736538 BETTER
I0313 05:01:08.366303 2042642 finetune.py:68] layer 30_up @ epoch 4 new loss 0.007084182929247618 old loss 0.007125522941350937 BETTER
I0313 05:01:13.572382 2041089 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.0072658793069422245 old loss 0.007279374171048403 BETTER
I0313 05:01:18.683956 2041895 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.008203740231692791 old loss 0.008222625590860844 BETTER
I0313 05:01:21.681825 2043428 finetune.py:68] layer 31_up @ epoch 2 new loss 0.014040903188288212 old loss 0.014283287338912487 BETTER
I0313 05:01:35.241425 2042642 finetune.py:45] layer 30_gate initial loss 0.010511425323784351
I0313 05:01:43.823622 2041089 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.007254335097968578 old loss 0.0072658793069422245 BETTER
I0313 05:01:49.009519 2041895 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.00818820483982563 old loss 0.008203740231692791 BETTER
I0313 05:01:52.936780 2043428 finetune.py:68] layer 31_up @ epoch 3 new loss 0.01382391806691885 old loss 0.014040903188288212 BETTER
I0313 05:02:04.270320 2042642 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.01045992225408554 old loss 0.010511425323784351 BETTER
I0313 05:02:15.263175 2041089 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.007243250496685505 old loss 0.007254335097968578 BETTER
I0313 05:02:18.809880 2041895 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.008174021728336811 old loss 0.00818820483982563 BETTER
I0313 05:02:23.373694 2043428 finetune.py:68] layer 31_up @ epoch 4 new loss 0.013629929162561893 old loss 0.01382391806691885 BETTER
I0313 05:02:32.913810 2042642 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.010426451452076435 old loss 0.01045992225408554 BETTER
I0313 05:02:48.522286 2041895 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.008161589503288269 old loss 0.008174021728336811 BETTER
I0313 05:02:49.821178 2043428 finetune.py:45] layer 31_gate initial loss 0.01896115019917488
I0313 05:02:59.419267 2041089 finetune.py:45] layer 28_down initial loss 0.011224430985748768
I0313 05:03:01.380733 2042642 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.010394757613539696 old loss 0.010426451452076435 BETTER
I0313 05:03:16.466883 2043428 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.01873801089823246 old loss 0.01896115019917488 BETTER
I0313 05:03:25.763514 2041089 finetune.py:68] layer 28_down @ epoch 0 new loss 0.011221769265830517 old loss 0.011224430985748768 BETTER
I0313 05:03:29.579946 2042642 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.01036839373409748 old loss 0.010394757613539696 BETTER
I0313 05:03:31.645308 2041895 finetune.py:45] layer 29_down initial loss 0.012820933945477009
I0313 05:03:44.251895 2043428 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.018609248101711273 old loss 0.01873801089823246 BETTER
I0313 05:03:53.761575 2041089 finetune.py:68] layer 28_down @ epoch 1 new loss 0.011219515465199947 old loss 0.011221769265830517 BETTER
I0313 05:03:57.073394 2041895 finetune.py:68] layer 29_down @ epoch 0 new loss 0.012818334624171257 old loss 0.012820933945477009 BETTER
I0313 05:03:58.064284 2042642 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.010343026369810104 old loss 0.01036839373409748 BETTER
I0313 05:04:12.139925 2043428 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.01850128173828125 old loss 0.018609248101711273 BETTER
I0313 05:04:22.184178 2041089 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0112175103276968 old loss 0.011219515465199947 BETTER
I0313 05:04:23.399113 2041895 finetune.py:68] layer 29_down @ epoch 1 new loss 0.012816081754863262 old loss 0.012818334624171257 BETTER
I0313 05:04:38.957621 2042642 finetune.py:45] layer 30_down initial loss 0.016726238653063774
I0313 05:04:40.252770 2043428 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.018412157893180847 old loss 0.01850128173828125 BETTER
I0313 05:04:49.932150 2041895 finetune.py:68] layer 29_down @ epoch 2 new loss 0.012814137153327465 old loss 0.012816081754863262 BETTER
I0313 05:04:50.660608 2041089 finetune.py:68] layer 28_down @ epoch 3 new loss 0.011215700767934322 old loss 0.0112175103276968 BETTER
I0313 05:05:04.606623 2042642 finetune.py:68] layer 30_down @ epoch 0 new loss 0.016723614186048508 old loss 0.016726238653063774 BETTER
I0313 05:05:08.364625 2043428 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.018327418714761734 old loss 0.018412157893180847 BETTER
I0313 05:05:16.802729 2041895 finetune.py:68] layer 29_down @ epoch 3 new loss 0.012812393717467785 old loss 0.012814137153327465 BETTER
I0313 05:05:19.106184 2041089 finetune.py:68] layer 28_down @ epoch 4 new loss 0.011214068159461021 old loss 0.011215700767934322 BETTER
28_v proxy err 0.025289738550782204 tr(WHW.T) 2018.944091796875
28_q proxy err 0.006757637020200491 tr(WHW.T) 7654.51708984375
28_k proxy err 0.004918845370411873 tr(WHW.T) 10555.326171875
28_o proxy err 0.020090071484446526 tr(WHW.T) 196.07972717285156
28_up proxy err 0.019648641347885132 tr(WHW.T) 4662.52734375
28_gate proxy err 0.014203503727912903 tr(WHW.T) 6533.95947265625
28_down proxy err 0.028081616386771202 tr(WHW.T) 608.3831787109375
I0313 05:05:31.241800 2042642 finetune.py:68] layer 30_down @ epoch 1 new loss 0.016721472144126892 old loss 0.016723614186048508 BETTER
I0313 05:05:43.755935 2041895 finetune.py:68] layer 29_down @ epoch 4 new loss 0.012810827232897282 old loss 0.012812393717467785 BETTER
29_v proxy err 0.026282675564289093 tr(WHW.T) 1801.7730712890625
29_q proxy err 0.006558127235621214 tr(WHW.T) 7237.310546875
29_k proxy err 0.004497426562011242 tr(WHW.T) 10574.5166015625
29_o proxy err 0.017929822206497192 tr(WHW.T) 208.97425842285156
29_up proxy err 0.015461701899766922 tr(WHW.T) 6072.61962890625
29_gate proxy err 0.012895622290670872 tr(WHW.T) 7362.14404296875
29_down proxy err 0.025346651673316956 tr(WHW.T) 787.494873046875
I0313 05:05:48.718856 2043428 finetune.py:45] layer 31_down initial loss 0.029421379789710045
I0313 05:05:57.932780 2042642 finetune.py:68] layer 30_down @ epoch 2 new loss 0.016719698905944824 old loss 0.016721472144126892 BETTER
I0313 05:06:13.462031 2043428 finetune.py:68] layer 31_down @ epoch 0 new loss 0.029414044693112373 old loss 0.029421379789710045 BETTER
I0313 05:06:24.705160 2042642 finetune.py:68] layer 30_down @ epoch 3 new loss 0.016718095168471336 old loss 0.016719698905944824 BETTER
I0313 05:06:39.294672 2043428 finetune.py:68] layer 31_down @ epoch 1 new loss 0.029408657923340797 old loss 0.029414044693112373 BETTER
I0313 05:06:51.377655 2042642 finetune.py:68] layer 30_down @ epoch 4 new loss 0.01671661250293255 old loss 0.016718095168471336 BETTER
30_v proxy err 0.02227151393890381 tr(WHW.T) 2261.489501953125
30_q proxy err 0.006435857620090246 tr(WHW.T) 7822.94140625
30_k proxy err 0.004798666574060917 tr(WHW.T) 10536.45703125
30_o proxy err 0.01707499846816063 tr(WHW.T) 253.34495544433594
30_up proxy err 0.009159745648503304 tr(WHW.T) 10017.4189453125
30_gate proxy err 0.00846562348306179 tr(WHW.T) 10979.091796875
30_down proxy err 0.007757877465337515 tr(WHW.T) 3595.0048828125
I0313 05:07:05.381603 2043428 finetune.py:68] layer 31_down @ epoch 2 new loss 0.029404155910015106 old loss 0.029408657923340797 BETTER
I0313 05:07:31.659724 2043428 finetune.py:68] layer 31_down @ epoch 3 new loss 0.029400238767266273 old loss 0.029404155910015106 BETTER
I0313 05:07:57.870041 2043428 finetune.py:68] layer 31_down @ epoch 4 new loss 0.029396532103419304 old loss 0.029400238767266273 BETTER
31_v proxy err 0.025131819769740105 tr(WHW.T) 1268.2034912109375
31_q proxy err 0.004768230952322483 tr(WHW.T) 6862.46337890625
31_k proxy err 0.003217102261260152 tr(WHW.T) 10257.408203125
31_o proxy err 0.01027156226336956 tr(WHW.T) 459.1577453613281
31_up proxy err 0.004850502125918865 tr(WHW.T) 14570.71875
31_gate proxy err 0.004843729082494974 tr(WHW.T) 14808.74609375
31_down proxy err 0.0027886766474694014 tr(WHW.T) 17941.876953125
