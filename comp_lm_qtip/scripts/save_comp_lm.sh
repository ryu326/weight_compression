# comp_model_base="../NWC/checkpoint/nwc/block_seq_row_16"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random__llama-3-8b-hf/block_seq_ql_random_col_16"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_4096_RHT.pt"
# comp_model_base="../NWC/checkpoint/nwc_tr_with_hyp/block_seq_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt"
# comp_model_base="../NWC/checkpoint/nwc_tr/block_seq_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt"
# comp_model_base="../NWC/checkpoint/nwc/block_seq_scaler_meta-llama--Meta-Llama-3-8B__scaled3_RHT_sig1e-06_col_1024.pt"
# comp_model_base="../NWC/checkpoint/nwc/block_seq_scaler_meta-llama--Llama-2-7b-hf__scaled3_RHT_sig0.0001_col_4096.pt"
# comp_model_base="../NWC/checkpoint/nwc/block_seq_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt"
# comp_model_base="../NWC/checkpoint/nwc_ql_cdt/block_seq_ql_random_lstats_scaler_meta-llama--Meta-Llama-3-8B__col_1024_layerwise_stats.pt"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/M32"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/M16"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Llama-2-7b-hf__col_1024.pt/llama8b_c1024_7b_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter20000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__shuffled_col_1024.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Llama-2-7b-hf__shuffled_col_1024.pt/8b_shuffle_7b_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter20000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Llama-2-7b-hf__col_1024.pt/8b_shuffle_7b_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter20000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Llama-2-7b-hf__droplast_col_1024.pt/8b_7b_droplast_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter20000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_gaussian__llama8b_col_1024.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Llama-2-7b-hf__droplast_col_1024.pt/gaussian_7b_droplast_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter20000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/no_lnorm_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/gaussian_8b_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter20000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Llama-2-7b-hf__droplast_col_1024.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_openai--clip-vit-large-patch14__vision_text_col_256.pt/clip_llama8b_col1024_pretrained_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size4096_total_iter100000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_llama_8b_7b__droplast_col_1024.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_synthetic__8b_kde.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_llama8b+7b__droplast_modelwise_norm2_col_1024.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql_pe/block_seq_ql_random_pos_scaler_meta-llama--Meta-Llama-3-8B__col_1024_idx_ltype_stats.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql2/block_vec_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/learnable_scale_no_norm_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/use_hyper_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql_pe/block_seq_ql_random_pos_scaler_meta-llama--Meta-Llama-3-8B__col_1024_idx_ltype_stats.pt/use_hyper_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__rnormed_col_1024.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql_ste/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="/workspace/Weight_compression/NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/rdloss_ql_size16_encdim512_M16_Q2_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="/workspace/Weight_compression/NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_gaussian__llama8b_col_1024.pt/rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__scaleH_sig0.0001_rnormed_row_1024.pt/rdloss_ql_size128_encdim1024_M256_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_pos_scaler_meta-llama--Meta-Llama-3-8B__scaleHinv_sig0.0001_std_rnormed_lidx_row_1024.pt/rdloss_ql_size128_encdim1024_M256_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_qmap/block_seq_qmap_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/rdloss_qmap2_size16_encdim512_M17_Q0_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/M32__rdloss_ql_size16_encdim512_M32_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"
# comp_model_base="../NWC/checkpoint/nwc_ql_pe/block_seq_ql_random_pos_scaler_meta-llama--Meta-Llama-3-8B__scaleH_sig0.0001_rnormed_lidx_row_1024.pt/rdloss_ql_size128_encdim1024_M256_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"



# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/M16"
# comp_model_base="../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/M32__rdloss_ql_size16_encdim512_M32_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100"

# model_name="meta-llama--Llama-2-7b-hf"
# HESS="../Wparam_dataset/quip_hess/Hessians-Llama-2-7b-6144"

model_name="meta-llama--Meta-Llama-3-8B"
HESS="../Wparam_dataset/quip_hess/llama3_8b_6144"

# model_name="meta-llama--Llama-2-13b-hf"
# HESS="../Wparam_dataset/quip_hess/Hessians-Llama-2-13b-6144"

# model_name="meta-llama--Llama-3.2-3B"
# HESS="../Wparam_dataset/quip_hess/meta-llama--Llama-3.2-3B-256"

# model_name="lmsys--vicuna-7b-v1.5"
# HESS="/workspace/Weight_compression/Wparam_dataset/quip_hess/lmsys--vicuna-7b-v1.5_256"

# ql="../Wparam_dataset/hessian/$model_name/quip_hess_n6144_top3_qlevel3.pt"
# ql="../Wparam_dataset/hessian/$model_name/pileval_n_samples128_seqlen512_top[ 0.1  1.  10. ]_qlevel[3, 2, 1].pt"
# ql='../Wparam_dataset/hessian/meta-llama--Llama-2-7b-hf/quip_hess_n6144_all_layers_top[ 0.1  1.  10. ]_qlevel[3, 2, 1].pt'
############################################

lm_model_path="../Wparam_dataset/hf_model/$model_name"

CKPT="../hf_model_comp/comp_qtip/ckpt"
HF="../hf_model_comp/comp_qtip/hf"
LOG="./log"
RES="../hf_model_comp_results"

mkdir -p $CKPT
mkdir -p $HF
mkdir -p $LOG
mkdir -p $RES
export CUDA_VISIBLE_DEVICES=0,1,2,3
export WANDB_SILENT=true

# lmbda_values=(30 50 100 300 1000 10000)
lmbda_values=(50 100 300 1000)
ql_value=(0 1)
for qlv in "${ql_value[@]}"; do
    for lmbda in "${lmbda_values[@]}"; do
        echo "################## Running compression lmbda=${lmbda} ##################"
        ## ========= Change this =========
        # SAVE_NAME=ft_ql_tuned_ldlq/${model_name}/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ft_comp_tr_dec_laywise_norm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/8b_use_codes_test3/lmbda${lmbda}
        # SAVE_NAME=${model_name}/layerwise_norm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/gaussian_7b_droplast/lmbda${lmbda}
        # SAVE_NAME=${model_name}/gaussian_8b/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_7b_droplast/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_gaussian_row_norm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_no_norm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_8b/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_8b_7b_droplast/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_8b_clip/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_8b_row_col_norm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_d_8b+7b_droplast_modelnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_d_8b_naive_code_edit/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_d_8b_naive_code_edit100_nonomean2/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_code_optim200_rnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_d_8b_ldlq256_rnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_d_8b_ldlq256_rnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_d_8b_ldlq256_rnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_ldlq128_rnorm_ft/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_qs_optim100_lnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_qs_optim100_lnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_8b_pe/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_8b_learnable_scale_no_norm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_lnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_qmap22_optim/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_qmap22_rc_mse_test/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_rnorm_trained_scale_std/lmbda100_scale${lmbda}
        # SAVE_NAME=${model_name}/ql_rnorm_optim_rnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/scaleH2_ldlq128/size128_encdim1024_M256__ql${qlv}/lmbda${lmbda}
        SAVE_NAME=${model_name}/scaleHinv_ldlq128/size128_encdim1024_M256__ql${qlv}/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_d8b_ste_/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_d8b_Q2_trained_r3/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_gaussian_rnorm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_gaussian/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_rnorm_one_batch_use_codes/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_rnorm_use_code/lmbda${lmbda}
        # SAVE_NAME=${model_name}/test/ql_rnorm_optim_no_norm/lmbda${lmbda}
        # SAVE_NAME=${model_name}/ql_ldlq128_rnorm/lmbda${lmbda}
        # ## ========= Change this =========

        comp_model=$comp_model_base/lmbda${lmbda}_*/best_loss*.pth.tar
        # comp_model=$comp_model_base/lmbda100_*/best_loss*.pth.tar
        # comp_model=$(ls -t $comp_model_base/lmbda${lmbda}_*/best_loss*.pth.tar | head -n 1)
        mkdir -p $(dirname "$LOG/$SAVE_NAME.log")
        
        taskset -c 0-31 \
        python -m quantize_llama.quantize_finetune_llama --save_path $CKPT/$SAVE_NAME \
            --base_model $lm_model_path \
            --comp_model_path $comp_model \
            --in_hess_path $HESS --devset_size 384 --ft_valid_size 128 --batch_size 8 \
            --ft_epochs 0 \
            --direction row --scaleHinv --row_normalize  --ql_search_value ${qlv} \
            --ldlq --comp_batch_size 128 \
            2>&1 | tee $LOG/$SAVE_NAME.log

            # --ql_search --ql_search_value $qlv \
            # --direction row --scaleH --row_normalize --comp_batch_size 2048 \
            # --use_codes \
            # --direction row --scaleH --row_normalize --comp_batch_size 4096 --ql_search_value 1 \
            # --row_normalize --rnorm_optim --code_optim_lmbda $lmbda --qmap_optim_iter 5 \
            # --qmap_optim  --code_optim_lmbda $lmbda --qmap_optim_iter 5 \
            # --code_optim --code_optim_it 100 --loss rdloss_ql --code_optim_lmbda $lmbda --code_optim_lr 5e-3 --code_optim_model nwc_ql_sga_vbr --optim_qs \
            # --code_optim --code_optim_it 200 --loss rdloss_ql --code_optim_lmbda $lmbda --code_optim_lr 5e-3 \
            # --code_optim_test \
            # --code_optim --code_optim_it 100 --loss rdloss_ql --code_optim_lmbda $lmbda --code_optim_lr 5e-3 \
            # --incoh_mode had  --rescale_WH_2  --sigma_reg 1e-4 --use_train_scale \
            # --ldlq --comp_batch_size 128 \
            # --ft_comp_model2 --ft_comp_lmbda $lmbda --ft_comp_ep 100 --direction row \
            # --ft_comp_model2 --ft_comp_lmbda $lmbda --ft_comp_ep 200 \
            # --ft_comp_model2 --ft_comp_lmbda $lmbda --ft_comp_steps 400 --direction row --ft_train_dec \
            # --layerwise_scale \
            # --row_normalize \
            # --col_normalize \
            # --ql_tuned \
            # --ql --Q 4 \

        echo "################## Running hfize lmbda=${lmbda} ##################"
        python -m quantize_llama.hfize_llama --quantized_path $CKPT/${SAVE_NAME} \
                --hf_output_path $HF/$SAVE_NAME 2>&1 | tee -a $LOG/$SAVE_NAME.log

        echo "################## Running PPL evaluation lmbda=${lmbda} ##################"
        pretrain_path=$HF/$SAVE_NAME
        mkdir -p "$log_dir"
        echo "Running evaluation for directory: $pretrain_path"
        python -m eval.eval_ppl_hf \
            --hf_path $pretrain_path \
            --seqlen 2048 \
            --output_path $RES/$SAVE_NAME \
            --no_use_cuda_graph 2>&1 | tee -a $LOG/$SAVE_NAME.log

        # echo "################## Running benchmark evaluation lmbda=${lmbda} ##################"
        # pretrain_path=$HF/$SAVE_NAME
        # python -m eval.eval_zeroshot_hf \
        #     --tasks arc_challenge,arc_easy,boolq,piqa,winogrande \
        #     --batch_size 8  \
        #     --hf_path $pretrain_path \
        #     --output_path $RES/$SAVE_NAME

        if [ "$pretrain_path" != "$HF" ]; then
            rm -r "$pretrain_path"
            rm -r "$CKPT/$SAVE_NAME"
        fi
    done
done

    # output_path=$(echo "$pretrain_path" | sed 's|model_reconstructed|model_eval|')_harness_results
    # lm_eval --model hf \
    #     --model_args "pretrained=$pretrain_path,parallelize=True" \
    #     --tasks arc_easy,arc_challenge,winogrande,piqa,boolq \
    #     --batch_size 4 \
    #     --output_path $output_path \
    #     --trust_remote_code \
    #     2>&1 | tee -a $LOG/$SAVE_NAME.log
