{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c219641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# path = '/home/jgryu/workspace/weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_ldlq128_rnorm_codes_test/lmbda50/0_up.pt'\n",
    "\n",
    "# s = torch.load(path, weights_only=False, map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c369faf",
   "metadata": {},
   "source": [
    "### NWC state_dict 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d16c544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "path = '/home/jgryu/workspace/weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_ldlq128_rnorm_codes_test/lmbda50'\n",
    "\n",
    "codes_dict = {}\n",
    "metadata_dict = {}\n",
    "\n",
    "for i in range(32):\n",
    "    for m in ['q', 'k', 'v', 'o', 'up', 'down', 'gate']:\n",
    "        pt = f'{path}/{i}_{m}.pt'\n",
    "        s = torch.load(pt, weights_only=False, map_location='cpu')\n",
    "\n",
    "        codes = s['codes']\n",
    "        tensor_code_list = []\n",
    "        for c in codes:\n",
    "            # print(len(c['strings'][0][0]))\n",
    "            byte_stream = c['strings'][0][0]\n",
    "            tensor_view = torch.frombuffer(byte_stream, dtype=torch.uint8)\n",
    "            tensor_code_list.append(tensor_view)\n",
    "            # print(tensor_view.shape)\n",
    "        \n",
    "        padded_code_tensor = pad_sequence(tensor_code_list, batch_first=True, padding_value=0)\n",
    "        \n",
    "        key = f'{i}_{m}'\n",
    "        \n",
    "        codes_dict[key] = padded_code_tensor\n",
    "        \n",
    "        metadata_dict[key] = {\n",
    "            'row_std': s['metadata']['row_std'],\n",
    "            'qlevel': s['metadata']['qlevel'],\n",
    "            'shape': (128, int(c['shape'][0] / 128 * 16))\n",
    "        }\n",
    "\n",
    "# 4. 두 딕셔너리를 별도의 파일로 저장\n",
    "torch.save(codes_dict, './8b_compressed_codes.pth')\n",
    "torch.save(metadata_dict, './8b_compressed_metadata.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a40bb4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:04<00:00,  1.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "path = '/home/jgryu/workspace/weight_compression/Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B'\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='cpu')\n",
    "\n",
    "torch.save(model.state_dict(), './8b_sd_original.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747096f",
   "metadata": {},
   "source": [
    "### 시간측정 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "82680733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "def measure_load_time(model_path, device):\n",
    "    \"\"\"\n",
    "    .pth 파일을 로드하고 GPU로 전송하는 시간을 측정합니다.\n",
    "    이 함수는 캐시 상태(hot/cold)를 알지 못합니다.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # --- 1. Disk -> CPU RAM 시간 측정 (CPU 작업) ---\n",
    "    # (Cold Load 시: Disk I/O + Pickle 역직렬화)\n",
    "    # (Hot Load 시: OS Cache I/O + Pickle 역직렬화)\n",
    "    \n",
    "    torch.cuda.synchronize(device) # 이전 GPU 작업 완료 보장\n",
    "    start_cpu_time = time.perf_counter()\n",
    "    \n",
    "    # torch.load는 I/O와 역직렬화(CPU 작업)를 수행합니다.\n",
    "    state_dict_cpu = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        \n",
    "    end_cpu_time = time.perf_counter()\n",
    "    torch.cuda.synchronize(device)\n",
    "    \n",
    "    load_time_ms = (end_cpu_time - start_cpu_time) * 1000\n",
    "    print(f\"  [1] Disk/Cache -> CPU RAM 시간: {load_time_ms:.2f} ms\")\n",
    "\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"오류: {model_path} 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    if device.type == 'cpu':\n",
    "        print(\"오류: CUDA 디바이스가 필요합니다.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. CPU RAM -> GPU VRAM 시간 측정 (GPU 작업) ---\n",
    "    # GPU는 비동기로 작동하므로 정확한 측정을 위해 cuda.Event 사용\n",
    "    start_gpu_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_gpu_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start_gpu_event.record()\n",
    "    \n",
    "    # state_dict의 모든 텐서를 GPU로 이동시킵니다.\n",
    "    state_dict_gpu = {k: v.to(device) for k, v in state_dict_cpu.items()}\n",
    "    \n",
    "    end_gpu_event.record()\n",
    "    \n",
    "    # GPU 작업이 끝날 때까지 CPU가 기다리도록 강제\n",
    "    torch.cuda.synchronize(device)\n",
    "    \n",
    "    transfer_time_ms = start_gpu_event.elapsed_time(end_gpu_event)\n",
    "    print(f\"  [2] CPU RAM -> GPU VRAM 시간: {transfer_time_ms:.2f} ms\")\n",
    "    print(f\"  -------------------------------------\")\n",
    "    print(f\"  총합: {load_time_ms + transfer_time_ms:.2f} ms\")\n",
    "    \n",
    "    # # 측정 후 메모리 정리\n",
    "    del state_dict_cpu, state_dict_gpu\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b3863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8b_sd_original.pth\n",
      "  [1] Disk/Cache -> CPU RAM 시간: 5166.80 ms\n",
      "  [2] CPU RAM -> GPU VRAM 시간: 1769.98 ms\n",
      "  -------------------------------------\n",
      "  총합: 6936.78 ms\n",
      "8b_compressed_codes.pth\n",
      "  [1] Disk/Cache -> CPU RAM 시간: 315.34 ms\n",
      "  [2] CPU RAM -> GPU VRAM 시간: 286.43 ms\n",
      "  -------------------------------------\n",
      "  총합: 601.78 ms\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "paths = [\n",
    "    '8b_sd_original.pth',\n",
    "    '8b_compressed_codes.pth'\n",
    "]\n",
    "for p in paths:\n",
    "    print(p)\n",
    "    measure_load_time(p, torch.device('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26d34c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
