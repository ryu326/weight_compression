{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1977f888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0929 02:28:14.369672 3408649 warnings.py:109] /opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "I0929 02:28:14.955851 3408649 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "I0929 02:28:14.957226 3408649 utils.py:149] Note: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "I0929 02:28:14.957905 3408649 utils.py:162] NumExpr defaulting to 16 threads.\n",
      "W0929 02:28:17.958549 3408649 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @amp.autocast(enabled=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import glog, json\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_attn_mask_utils import \\\n",
    "    _prepare_4d_causal_attention_mask\n",
    "\n",
    "from lib import utils\n",
    "from lib.algo import finetune\n",
    "from lib.codebook import bitshift\n",
    "from operator import attrgetter\n",
    "\n",
    "import sys\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "\n",
    "from NWC.models import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e8224a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 65534, 65535, 65536],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "comp_model_path = '/workspace/Weight_compression/NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/M16/lmbda50_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_3.87239_bpp_4.65884_MSE_0.0162_total_iter_95000.pth.tar'\n",
    "# comp_model_path = '/workspace/Weight_compression/NWC/checkpoint/nwc_scale_cond/block_seq_scale_cond_scaler_meta-llama--Meta-Llama-3-8B__scaleH_sig0.0001_std_rnormed_with_col_std_lidx_row_1024.pt/rdloss_size128_encdim1024_M256_Q0_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100/lmbda50_/best_loss_model_loss_3.94749_bpp_3.26997_MSE_4.91093_total_iter_192500.pth.tar'\n",
    "config = os.path.join(os.path.dirname(comp_model_path), 'config.json')\n",
    "with open(config, 'r', encoding='utf-8') as file:\n",
    "    config = json.load(file)\n",
    "config = Config(**config)\n",
    "\n",
    "shift, scale = None, None\n",
    "if config.architecture == 'nwc_ql' and not hasattr(config, \"Q\"):\n",
    "    config.Q = 4\n",
    "if not hasattr(config, \"no_layernorm\"):\n",
    "    config.no_layernorm = False\n",
    "\n",
    "\n",
    "comp_model = get_model(config.architecture, config, scale=scale, shift=shift)\n",
    "comp_model.config = config\n",
    "ckpt = torch.load(comp_model_path, weights_only=False)\n",
    "scale, shift  = torch.zeros(1), torch.zeros(1)\n",
    "\n",
    "comp_model.load_state_dict(ckpt[\"state_dict\"], strict = False)\n",
    "comp_model.scale = scale\n",
    "comp_model.shift = shift\n",
    "comp_model.eval()\n",
    "comp_model.update()\n",
    "\n",
    "comp_model.update(force=True)              # CompressAI: CDF 고정 및 버퍼 등록\n",
    "comp_model.entropy_bottleneck._quantized_cdf  # 캐시되어 이후 재계산 안 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b30ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 12.359 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:4')\n",
    "\n",
    "tt = []\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        T  = torch.zeros(256, 256)\n",
    "        T = T.reshape(1, -1, 16).to(device)\n",
    "        # T = T.reshape(1, -1, 128).to(device)\n",
    "        data = {}\n",
    "        data['weight_block'] = T\n",
    "        data['q_level'] = torch.zeros(1, T.shape[1]).to(torch.int).to(device)\n",
    "        # data['scale_cond'] = torch.zeros_like(T).to(device)\n",
    "\n",
    "        comp_model.to(device)\n",
    "        out_enc = comp_model.compress(data)\n",
    "\n",
    "        # torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        out_dec = comp_model.decompress(out_enc)\n",
    "        # out_dec  = comp_model.decompress(out_enc)\n",
    "        # torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        elapsed_ms = (end - start) * 1000\n",
    "        tt.append(elapsed_ms)\n",
    "        # print(f\"Decompress time: {elapsed_ms:.3f} ms\")\n",
    "        # for step, duration in timings.items():\n",
    "        #     print(f\"{step:<25}: {duration:.4f} ms\")\n",
    "\n",
    "\n",
    "avg = 0\n",
    "for t in tt:\n",
    "    avg += t\n",
    "avg /= len(tt)\n",
    "print(f\"Average: {avg:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7b696ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing warm-up run...\n",
      "Warm-up finished. Starting latency measurement.\n",
      "\n",
      "--- Latency Measurement Results ---\n",
      "Average Encoding Latency: 12.509 ms\n",
      "Average Decoding Latency: 13.667 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# --- 모델 및 장치 설정 (사용자 환경에 맞게 정의 필요) ---\n",
    "# comp_model = YourCompressionModel() \n",
    "# device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "# comp_model.to(device)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "encoding_times = []\n",
    "decoding_times = []\n",
    "num_iterations = 100  # 측정 반복 횟수\n",
    "\n",
    "# --- Warm-up (첫 실행 시 발생하는 오버헤드 제외) ---\n",
    "# 정확한 측정을 위해 실제 측정 전에 모델을 한두 번 실행해주는 것이 좋습니다.\n",
    "with torch.no_grad():\n",
    "    print(\"Performing warm-up run...\")\n",
    "    temp_tensor = torch.zeros(256, 256).reshape(1, -1, 16).to(device)\n",
    "    temp_data = {\n",
    "        'weight_block': temp_tensor,\n",
    "        'q_level': torch.zeros(1, temp_tensor.shape[1], dtype=torch.int).to(device)\n",
    "    }\n",
    "    out_enc_warmup = comp_model.compress(temp_data)\n",
    "    _ = comp_model.decompress(out_enc_warmup)\n",
    "    torch.cuda.synchronize(device=device) # Warm-up이 끝날 때까지 대기\n",
    "    print(\"Warm-up finished. Starting latency measurement.\")\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_iterations):\n",
    "        # 테스트 데이터 생성\n",
    "        # T = torch.zeros(256, 256)\n",
    "        T = torch.zeros(256, 256)\n",
    "        T = T.reshape(1, -1, 16).to(device)\n",
    "        data = {\n",
    "            'weight_block': T,\n",
    "            'q_level': torch.zeros(1, T.shape[1], dtype=torch.int).to(device)\n",
    "        }\n",
    "\n",
    "        # --- 인코딩(압축) 시간 측정 ---\n",
    "        torch.cuda.synchronize(device=device)  # 이전 GPU 작업이 모두 끝날 때까지 대기\n",
    "        start_time = time.time()\n",
    "\n",
    "        out_enc = comp_model.compress(data)\n",
    "\n",
    "        torch.cuda.synchronize(device=device)  # compress 작업이 끝날 때까지 대기\n",
    "        end_time = time.time()\n",
    "        encoding_times.append((end_time - start_time) * 1000) # ms 단위로 변환하여 저장\n",
    "\n",
    "        # --- 디코딩(복원) 시간 측정 ---\n",
    "        torch.cuda.synchronize(device=device)  # 이전 GPU 작업이 모두 끝날 때까지 대기\n",
    "        start_time = time.time()\n",
    "        \n",
    "        out_dec = comp_model.decompress(out_enc)\n",
    "        \n",
    "        torch.cuda.synchronize(device=device)  # decompress 작업이 끝날 때까지 대기\n",
    "        end_time = time.time()\n",
    "        decoding_times.append((end_time - start_time) * 1000) # ms 단위로 변환하여 저장\n",
    "\n",
    "\n",
    "# 결과 계산 및 출력\n",
    "# 첫 번째 측정값은 여전히 불안정할 수 있으므로 제외하고 계산하는 것이 일반적입니다.\n",
    "avg_encoding_ms = sum(encoding_times[1:]) / len(encoding_times[1:]) if len(encoding_times) > 1 else sum(encoding_times) / len(encoding_times)\n",
    "avg_decoding_ms = sum(decoding_times[1:]) / len(decoding_times[1:]) if len(decoding_times) > 1 else sum(decoding_times) / len(decoding_times)\n",
    "\n",
    "print(\"\\n--- Latency Measurement Results ---\")\n",
    "print(f\"Average Encoding Latency: {avg_encoding_ms:.3f} ms\")\n",
    "print(f\"Average Decoding Latency: {avg_decoding_ms:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e994bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 각 단계별 평균 소요 시간 (10회 실행) ---\n",
      "total_decompress_ms      : 14.6150 ms\n",
      "entropy_decompress_ms    : 13.8498 ms\n",
      "synthesis_g_s_ms         : 0.6292 ms\n",
      "quality_embedding_ms     : 0.0484 ms\n",
      "rescale_shift_ms         : 0.0363 ms\n",
      "permute_ms               : 0.0344 ms\n",
      "parse_input_ms           : 0.0017 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# 이전에 정의된 comp_model이 있다고 가정합니다.\n",
    "# device = torch.device('cuda:5')\n",
    "\n",
    "# 각 스텝별 시간을 저장할 딕셔너리\n",
    "all_timings = defaultdict(list)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        # --- 데이터 준비 (기존 코드와 동일) ---\n",
    "        T = torch.zeros(256, 256)\n",
    "        T = T.reshape(1, -1, 16).to(device)\n",
    "        data = {}\n",
    "        data['weight_block'] = T\n",
    "        data['q_level'] = torch.zeros(1, T.shape[1], dtype=torch.int).to(device)\n",
    "\n",
    "        comp_model.to(device)\n",
    "        out_enc = comp_model.compress(data)\n",
    "\n",
    "        # Decompress 실행 및 시간 측정\n",
    "        # fast_decompress는 (결과, 시간_딕셔너리)를 반환한다고 가정\n",
    "        out_dec, timings = comp_model.fast_decompress(out_enc)\n",
    "\n",
    "        # 각 단계의 측정 시간을 리스트에 추가\n",
    "        for step, duration in timings.items():\n",
    "            all_timings[step].append(duration)\n",
    "\n",
    "# --- 각 단계별 평균 시간 계산 및 출력 ---\n",
    "print(\"\\n--- 각 단계별 평균 소요 시간 (10회 실행) ---\")\n",
    "avg_timings = {}\n",
    "for step, duration_list in all_timings.items():\n",
    "    # 저장된 시간 리스트의 평균을 계산\n",
    "    average_duration = sum(duration_list) / len(duration_list)\n",
    "    avg_timings[step] = average_duration\n",
    "\n",
    "# 보기 좋게 정렬하여 출력 (총 소요시간이 긴 순서대로)\n",
    "sorted_avg_timings = sorted(avg_timings.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "for step, avg_duration in sorted_avg_timings:\n",
    "    print(f\"{step:<25}: {avg_duration:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caac08ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 50.876 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:4')\n",
    "\n",
    "tt = []\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        T  = torch.zeros(256, 256)\n",
    "        T = T.reshape(1, -1, 16).to(device)\n",
    "        # T = T.reshape(1, -1, 128).to(device)\n",
    "        data = {}\n",
    "        data['weight_block'] = T\n",
    "        data['q_level'] = torch.zeros(1, T.shape[1]).to(torch.int).to(device)\n",
    "        # data['scale_cond'] = torch.zeros_like(T).to(device)\n",
    "\n",
    "        comp_model.to(device)\n",
    "        start = time.time()\n",
    "        out_enc = comp_model(data)\n",
    "        end = time.time()\n",
    "\n",
    "        elapsed_ms = (end - start) * 1000\n",
    "        tt.append(elapsed_ms)\n",
    "        # print(f\"Decompress time: {elapsed_ms:.3f} ms\")\n",
    "        # for step, duration in timings.items():\n",
    "        #     print(f\"{step:<25}: {duration:.4f} ms\")\n",
    "\n",
    "\n",
    "avg = 0\n",
    "for t in tt:\n",
    "    avg += t\n",
    "avg /= len(tt)\n",
    "print(f\"Average: {avg:.3f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
