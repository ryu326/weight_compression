{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575d12c-2319-4417-aeba-f1a33d1ce740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a367b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeed46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "path = \"/home/jgryu/Weight_compression/Wparam_dataset/path_json\"\n",
    "list = os.listdir(path)\n",
    "\n",
    "for l in list :\n",
    "    file_path = os.path.join(path, l)\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i].replace(\"/home/jgryu/Weight_compression\", \"\")\n",
    "        \n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4adaa9",
   "metadata": {},
   "source": [
    "# V3\n",
    "각 tensor의 path를 train val split해서 json 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import json\n",
    "def contains_all_substrings(string, substrings):\n",
    "    return all(substring in string for substring in substrings)\n",
    "\n",
    "save_path = '/home/jgryu/Weight_compression/model_param_dataset/'\n",
    "dataset_path = '/home/jgryu/Weight_compression/model_param_tensor/'\n",
    "\n",
    "tenosr_path_list = glob.glob(f'{dataset_path}/**/*.npy', recursive=True)\n",
    "\n",
    "print(f'all : {len(tenosr_path_list)}')\n",
    "\n",
    "data = {'attn':[], 'mlp':[], 'embed':[], 'layernorm':[]}   \n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "for weight_type in ['attn', 'mlp', 'embed', 'layernorm']: \n",
    "    for path in tenosr_path_list:\n",
    "        if contains_all_substrings(path.lower(), [weight_type]):\n",
    "            data[weight_type].append(path)\n",
    "    print(f'{weight_type} : {len(data[weight_type])}')\n",
    "\n",
    "\n",
    "    random.shuffle(data[weight_type])\n",
    "\n",
    "    split_index = int(0.8 * len(data[weight_type]))\n",
    "    train_data = data[weight_type][:split_index]\n",
    "    validation_data = data[weight_type][split_index:]\n",
    "\n",
    "    file_path_train = os.path.join(save_path, f'{weight_type}_tensor_path_train.json')\n",
    "    file_path_validation = file_path_train.replace('train', 'val')\n",
    "\n",
    "    with open(file_path_train, 'w') as json_file:\n",
    "        json.dump(train_data, json_file, indent=4)\n",
    "    with open(file_path_validation, 'w') as json_file:\n",
    "        json.dump(validation_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e99bd3",
   "metadata": {},
   "source": [
    "# V2\n",
    "state_dict tensor별로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814e8449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4\n",
      "models--meta-llama--Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--meta-llama--Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--meta-llama--Meta-Llama-3.1-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--meta-llama--Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.54s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "def get_ckpt_path(path, branch = 'main'):\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "def check_contains_any_substrings(string, substrings):\n",
    "    return any(substring in string for substring in substrings)\n",
    "\n",
    "model_zoo_path = '/home/jgryu/Weight_compression/Wparam_dataset/model_zoo/huggingface'\n",
    "# model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b', '13b', 'mini', 'small']\n",
    "# model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b','mini']\n",
    "# model_filter = ['-13b', '-small']\n",
    "model_filter = ['meta-llama-3-8b', 'meta-llama-3.1-8b']\n",
    "\n",
    "model_list = os.listdir(model_zoo_path)\n",
    "print()\n",
    "ckpt_path_list = []\n",
    "for ck in model_list:\n",
    "    if check_contains_any_substrings(ck.lower(), model_filter):\n",
    "        ckpt_path = get_ckpt_path(os.path.join(model_zoo_path, ck))\n",
    "        if ckpt_path is not None:\n",
    "            ckpt_path_list.append(ckpt_path)\n",
    "\n",
    "print(len(ckpt_path_list))\n",
    "\n",
    "save_dir = '/home/jgryu/Weight_compression/Wparam_dataset/model_param_tensor'\n",
    "\n",
    "for ckpt_path in tqdm.tqdm(ckpt_path_list):\n",
    "    model_name = ckpt_path.split('/')[-3]\n",
    "    print(model_name)\n",
    "    model_name = model_name.split('--')\n",
    "    \n",
    "    save_path = os.path.join(save_dir, model_name[1], model_name[2])\n",
    "\n",
    "    if os.path.isdir(save_path) and bool(os.listdir(save_path)):\n",
    "        print('### Skip ###')\n",
    "        continue\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True, trust_remote_code=True)\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "        for k, v in state_dict.items():\n",
    "            np.save(os.path.join(save_path, k.replace(\".\", \"-\")), v)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Fail load model from {ckpt_path}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf83332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ckpt_path(path, branch = 'main'):\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "    return os.path.join(path, 'snapshots', revision)\n",
    "\n",
    "def check_contains_any_substrings(string, substrings):\n",
    "    return any(substring in string for substring in substrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3adfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zoo_path = '/home/jgryu/Weight_compression/model_zoo/huggingface'\n",
    "# model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b', '13b', 'mini', 'small']\n",
    "model_filter = ['0.5b','1.5b','2b','3b', '7b', '8b', '9b','mini']\n",
    "model_list = os.listdir(model_zoo_path)\n",
    "ckpt_path_list = []\n",
    "for ck in model_list:\n",
    "    if check_contains_any_substrings(ck.lower(), model_filter):\n",
    "        ckpt_path = get_ckpt_path(os.path.join(model_zoo_path, ck))\n",
    "        if ckpt_path is not None:\n",
    "            ckpt_path_list.append(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79728f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/home/jgryu/Weight_compression/model_param_tensor'\n",
    "\n",
    "for ckpt_path in ckpt_path_list:\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Fail load model from {ckpt_path}\")\n",
    "        continue\n",
    "    \n",
    "    model_name = ckpt_path.split('/')[-3]\n",
    "    print(model_name)\n",
    "    model_name = model_name.split('--')\n",
    "    \n",
    "    save_path = os.path.join(save_dir, model_name[1], model_name[2])\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    if os.path.isdir(save_path) and not os.listdir(save_path):\n",
    "        continue\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    for k, v in state_dict.items():\n",
    "        np.save(os.path.join(save_path, k.replace(\".\", \"-\")), v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
