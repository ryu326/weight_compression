{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c219641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_665459/2865850411.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  s = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# path = '/home/jgryu/workspace/weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_ldlq128_rnorm_codes_test/lmbda50/0_up.pt'\n",
    "\n",
    "# s = torch.load(path, weights_only=False, map_location='cpu')\n",
    "\n",
    "import torch\n",
    "# path = '/home/jgryu/workspace/weight_compression/complexity_test/8b_qtip_ft1_4bit.pth'\n",
    "path = '/home/jgryu/workspace/weight_compression/complexity_test/8b-w4-g128-awq-v2.pt'\n",
    "s = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4678f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096]) torch.float16\n",
      "model.layers.0.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.0.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.0.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.0.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.0.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.0.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.0.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.0.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.0.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.0.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.0.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.0.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.0.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.0.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.0.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.0.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.0.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.0.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.0.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.0.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.0.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.1.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.1.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.1.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.1.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.1.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.1.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.1.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.1.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.1.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.1.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.1.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.1.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.1.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.1.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.1.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.1.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.1.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.1.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.1.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.1.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.1.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.2.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.2.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.2.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.2.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.2.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.2.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.2.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.2.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.2.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.2.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.2.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.2.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.2.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.2.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.2.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.2.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.2.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.2.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.2.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.2.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.2.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.3.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.3.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.3.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.3.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.3.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.3.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.3.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.3.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.3.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.3.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.3.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.3.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.3.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.3.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.3.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.3.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.3.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.3.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.3.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.3.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.3.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.4.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.4.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.4.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.4.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.4.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.4.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.4.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.4.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.4.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.4.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.4.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.4.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.4.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.4.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.4.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.4.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.4.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.4.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.4.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.4.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.4.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.5.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.5.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.5.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.5.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.5.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.5.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.5.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.5.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.5.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.5.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.5.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.5.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.5.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.5.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.5.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.5.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.5.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.5.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.5.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.5.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.5.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.6.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.6.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.6.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.6.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.6.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.6.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.6.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.6.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.6.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.6.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.6.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.6.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.6.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.6.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.6.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.6.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.6.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.6.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.6.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.6.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.6.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.7.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.7.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.7.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.7.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.7.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.7.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.7.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.7.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.7.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.7.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.7.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.7.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.7.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.7.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.7.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.7.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.7.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.7.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.7.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.7.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.7.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.8.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.8.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.8.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.8.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.8.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.8.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.8.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.8.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.8.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.8.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.8.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.8.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.8.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.8.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.8.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.8.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.8.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.8.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.8.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.8.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.8.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.9.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.9.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.9.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.9.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.9.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.9.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.9.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.9.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.9.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.9.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.9.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.9.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.9.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.9.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.9.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.9.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.9.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.9.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.9.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.9.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.9.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.10.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.10.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.10.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.10.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.10.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.10.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.10.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.10.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.10.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.10.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.10.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.10.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.10.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.10.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.10.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.10.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.10.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.10.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.10.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.10.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.10.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.11.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.11.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.11.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.11.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.11.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.11.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.11.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.11.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.11.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.11.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.11.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.11.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.11.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.11.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.11.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.11.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.11.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.11.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.11.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.11.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.11.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.12.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.12.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.12.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.12.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.12.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.12.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.12.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.12.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.12.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.12.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.12.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.12.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.12.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.12.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.12.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.12.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.12.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.12.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.12.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.12.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.12.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.13.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.13.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.13.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.13.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.13.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.13.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.13.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.13.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.13.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.13.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.13.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.13.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.13.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.13.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.13.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.13.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.13.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.13.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.13.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.13.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.13.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.14.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.14.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.14.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.14.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.14.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.14.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.14.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.14.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.14.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.14.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.14.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.14.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.14.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.14.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.14.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.14.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.14.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.14.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.14.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.14.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.14.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.15.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.15.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.15.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.15.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.15.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.15.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.15.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.15.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.15.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.15.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.15.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.15.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.15.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.15.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.15.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.15.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.15.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.15.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.15.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.15.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.15.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.16.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.16.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.16.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.16.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.16.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.16.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.16.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.16.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.16.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.16.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.16.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.16.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.16.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.16.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.16.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.16.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.16.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.16.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.16.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.16.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.16.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.17.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.17.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.17.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.17.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.17.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.17.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.17.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.17.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.17.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.17.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.17.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.17.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.17.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.17.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.17.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.17.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.17.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.17.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.17.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.17.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.17.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.18.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.18.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.18.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.18.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.18.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.18.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.18.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.18.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.18.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.18.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.18.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.18.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.18.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.18.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.18.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.18.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.18.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.18.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.18.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.18.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.18.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.19.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.19.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.19.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.19.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.19.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.19.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.19.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.19.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.19.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.19.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.19.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.19.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.19.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.19.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.19.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.19.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.19.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.19.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.19.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.19.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.19.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.20.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.20.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.20.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.20.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.20.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.20.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.20.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.20.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.20.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.20.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.20.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.20.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.20.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.20.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.20.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.20.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.20.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.20.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.20.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.20.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.20.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.21.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.21.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.21.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.21.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.21.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.21.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.21.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.21.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.21.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.21.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.21.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.21.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.21.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.21.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.21.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.21.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.21.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.21.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.21.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.21.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.21.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.22.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.22.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.22.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.22.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.22.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.22.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.22.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.22.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.22.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.22.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.22.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.22.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.22.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.22.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.22.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.22.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.22.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.22.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.22.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.22.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.22.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.23.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.23.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.23.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.23.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.23.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.23.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.23.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.23.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.23.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.23.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.23.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.23.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.23.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.23.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.23.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.23.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.23.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.23.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.23.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.23.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.23.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.24.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.24.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.24.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.24.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.24.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.24.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.24.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.24.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.24.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.24.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.24.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.24.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.24.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.24.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.24.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.24.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.24.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.24.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.24.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.24.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.24.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.25.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.25.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.25.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.25.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.25.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.25.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.25.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.25.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.25.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.25.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.25.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.25.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.25.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.25.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.25.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.25.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.25.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.25.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.25.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.25.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.25.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.26.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.26.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.26.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.26.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.26.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.26.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.26.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.26.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.26.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.26.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.26.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.26.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.26.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.26.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.26.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.26.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.26.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.26.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.26.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.26.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.26.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.27.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.27.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.27.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.27.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.27.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.27.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.27.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.27.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.27.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.27.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.27.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.27.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.27.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.27.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.27.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.27.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.27.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.27.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.27.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.27.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.27.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.28.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.28.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.28.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.28.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.28.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.28.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.28.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.28.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.28.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.28.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.28.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.28.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.28.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.28.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.28.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.28.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.28.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.28.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.28.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.28.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.28.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.29.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.29.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.29.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.29.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.29.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.29.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.29.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.29.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.29.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.29.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.29.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.29.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.29.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.29.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.29.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.29.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.29.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.29.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.29.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.29.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.29.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.30.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.30.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.30.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.30.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.30.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.30.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.30.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.30.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.30.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.30.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.30.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.30.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.30.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.30.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.30.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.30.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.30.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.30.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.30.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.30.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.30.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.31.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.31.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.31.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.31.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.31.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.31.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.31.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.31.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.31.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.31.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.31.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.31.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.31.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.31.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.31.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.31.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.31.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.31.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.31.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.31.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.31.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.norm.weight torch.Size([4096]) torch.float16\n",
      "lm_head.weight torch.Size([128256, 4096]) torch.float16\n"
     ]
    }
   ],
   "source": [
    "for k, v in s.items():\n",
    "    print(k, v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfb2f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c369faf",
   "metadata": {},
   "source": [
    "### NWC state_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c544a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.10.13)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "path = '/home/jgryu/workspace/weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_ldlq128_rnorm_codes_test/lmbda300'\n",
    "codes_dict = {}\n",
    "metadata_dict = {}\n",
    "\n",
    "n_parm = 0\n",
    "n_bit = 0\n",
    "\n",
    "for i in range(32):\n",
    "    for m in ['q', 'k', 'v', 'o', 'up', 'down', 'gate']:\n",
    "        pt = f'{path}/{i}_{m}.pt'\n",
    "        s = torch.load(pt, weights_only=False, map_location='cpu')\n",
    "\n",
    "        codes = s['codes']\n",
    "        tensor_code_list = []\n",
    "        for c in codes:\n",
    "            # print(len(c['strings'][0][0]))\n",
    "            byte_stream = c['strings'][0][0]\n",
    "            tensor_view = torch.frombuffer(byte_stream, dtype=torch.uint8)\n",
    "            tensor_code_list.append(tensor_view)\n",
    "            # print(tensor_view.shape)\n",
    "        \n",
    "        padded_code_tensor = pad_sequence(tensor_code_list, batch_first=True, padding_value=0)\n",
    "        n_bit += padded_code_tensor.numel() * 8\n",
    "        \n",
    "        \n",
    "        key = f'{i}_{m}'\n",
    "        \n",
    "        codes_dict[key] = padded_code_tensor\n",
    "        codes_dict[key + '_row_std'] = s['metadata']['row_std']\n",
    "        codes_dict[key + '_qlevel'] = s['metadata']['qlevel'].to(dtype = torch.uint8)\n",
    "        \n",
    "        metadata_dict[key] = {\n",
    "            'row_std': s['metadata']['row_std'],\n",
    "            'qlevel': s['metadata']['qlevel'].to(dtype = torch.uint8),\n",
    "            'shape': (128, int(c['shape'][0] / 128 * 16))\n",
    "        }\n",
    "\n",
    "path = '/home/jgryu/workspace/weight_compression/Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B'\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='cpu')\n",
    "# torch.save(model.state_dict(), './8b_sd_original.pth')\n",
    "\n",
    "original_sd = model.state_dict()\n",
    "keys_to_keep = []\n",
    "for k in original_sd.keys():\n",
    "    if (\n",
    "        \"layernorm\" in k.lower()\n",
    "        or 'embed' in k.lower()\n",
    "        or k == \"model.norm.weight\"\n",
    "        or k == \"lm_head.weight\"\n",
    "    ):\n",
    "        keys_to_keep.append(k)\n",
    "    else:\n",
    "        n_parm += original_sd[k].numel()\n",
    "for k in list(original_sd.keys()):\n",
    "    if k not in keys_to_keep:\n",
    "        del original_sd[k]\n",
    "\n",
    "codes_dict.update(original_sd)\n",
    "\n",
    "# 4.     \n",
    "# torch.save(codes_dict, './8b_compressed_ld300_codes.pth')\n",
    "# torch.save(metadata_dict, './8b_compressed_ld300_metadata.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6daa76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.056882564838116"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_bit/n_parm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747096f",
   "metadata": {},
   "source": [
    "###   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82680733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np #    \n",
    "\n",
    "def measure_load_time(model_path, device):\n",
    "    \"\"\"\n",
    "    .pth   GPU    ms  .\n",
    "       (hot/cold)  .\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 0.   ---\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"  : {model_path}    .\")\n",
    "        return None, None #   None \n",
    "\n",
    "    if device.type == 'cpu':\n",
    "        print(\"  : CUDA  .\")\n",
    "        return None, None #   None \n",
    "        \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"  : CUDA   .\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 1. Disk -> CPU RAM   (CPU ) ---\n",
    "    torch.cuda.synchronize(device) #  GPU   \n",
    "    start_cpu_time = time.perf_counter()\n",
    "    \n",
    "    # torch.load I/O (CPU ) .\n",
    "    state_dict_cpu = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "        \n",
    "    end_cpu_time = time.perf_counter()\n",
    "    torch.cuda.synchronize(device)\n",
    "    \n",
    "    load_time_ms = (end_cpu_time - start_cpu_time) * 1000\n",
    "    # print(f\"  [1] Disk/Cache -> CPU RAM : {load_time_ms:.2f} ms\") # <--   \n",
    "\n",
    "    # --- 2. CPU RAM -> GPU VRAM   (GPU ) ---\n",
    "    start_gpu_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_gpu_event = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start_gpu_event.record()\n",
    "    \n",
    "    # state_dict   GPU .\n",
    "    state_dict_gpu = {k: v.to(device) for k, v in state_dict_cpu.items()}\n",
    "    \n",
    "    end_gpu_event.record()\n",
    "    \n",
    "    # GPU    CPU  \n",
    "    torch.cuda.synchronize(device)\n",
    "    \n",
    "    transfer_time_ms = start_gpu_event.elapsed_time(end_gpu_event)\n",
    "    # print(f\"  [2] CPU RAM -> GPU VRAM : {transfer_time_ms:.2f} ms\") # <--   \n",
    "    \n",
    "    #    \n",
    "    del state_dict_cpu, state_dict_gpu\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return load_time_ms, transfer_time_ms\n",
    "\n",
    "# ---    ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046b3863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ : 8b_sd_original.pth]\n",
      "  --- 8b_sd_original.pth   (5 ) ---\n",
      "  [1] Disk/Cache -> CPU RAM  (): 5.015 s\n",
      "  [2] CPU RAM -> GPU VRAM  (): 1.564 s\n",
      "  -------------------------------------\n",
      "   (): 6.579 s\n",
      "\n",
      "[ : 8b_compressed_ld300_codes.pth]\n",
      "  --- 8b_compressed_ld300_codes.pth   (5 ) ---\n",
      "  [1] Disk/Cache -> CPU RAM  (): 1.379 s\n",
      "  [2] CPU RAM -> GPU VRAM  (): 0.813 s\n",
      "  -------------------------------------\n",
      "   (): 2.192 s\n",
      "\n",
      "[ : 8b_qtip_noft_4bit.pth]\n",
      "  --- 8b_qtip_noft_4bit.pth   (5 ) ---\n",
      "  [1] Disk/Cache -> CPU RAM  (): 1.592 s\n",
      "  [2] CPU RAM -> GPU VRAM  (): 0.886 s\n",
      "  -------------------------------------\n",
      "   (): 2.478 s\n",
      "\n",
      "[ : 8b-w4-g128-awq-v2.pt]\n",
      "  --- 8b-w4-g128-awq-v2.pt   (5 ) ---\n",
      "  [1] Disk/Cache -> CPU RAM  (): 1.145 s\n",
      "  [2] CPU RAM -> GPU VRAM  (): 1.195 s\n",
      "  -------------------------------------\n",
      "   (): 2.341 s\n",
      "\n",
      "---    ---\n"
     ]
    }
   ],
   "source": [
    "paths = [\n",
    "    '8b_sd_original.pth',\n",
    "    '8b_compressed_ld300_codes.pth',\n",
    "    '8b_qtip_noft_4bit.pth',\n",
    "    '8b-w4-g128-awq-v2.pt',\n",
    "]\n",
    "N_RUNS = 5 # <---    \n",
    "DEVICE = torch.device('cuda:2')\n",
    "\n",
    "for p in paths:\n",
    "    print(f\"\\n[ : {p}]\")\n",
    "    \n",
    "    load_times_ms = []\n",
    "    transfer_times_ms = []\n",
    "    \n",
    "    for i in range(N_RUNS):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        load_ms, transfer_ms = measure_load_time(p, DEVICE)\n",
    "        \n",
    "        if load_ms is not None and transfer_ms is not None:\n",
    "            # print(f\"    Run {i+1}/{N_RUNS}: Load={load_ms:.2f}ms, Transfer={transfer_ms:.2f}ms, Total={load_ms+transfer_ms:.2f}ms\")\n",
    "            load_times_ms.append(load_ms)\n",
    "            transfer_times_ms.append(transfer_ms)\n",
    "        else:\n",
    "            print(f\"    Run {i+1}/{N_RUNS}:  \")\n",
    "\n",
    "    if load_times_ms: #     \n",
    "        avg_load = np.mean(load_times_ms)\n",
    "        avg_transfer = np.mean(transfer_times_ms)\n",
    "        avg_total = avg_load + avg_transfer\n",
    "        \n",
    "        print(f\"  --- {p}   ({len(load_times_ms)} ) ---\")\n",
    "        print(f\"  [1] Disk/Cache -> CPU RAM  (): {avg_load/1000:.3f} s\")\n",
    "        print(f\"  [2] CPU RAM -> GPU VRAM  (): {avg_transfer/1000:.3f} s\")\n",
    "        print(f\"  -------------------------------------\")\n",
    "        print(f\"   (): {avg_total/1000:.3f} s\")\n",
    "    else:\n",
    "        print(f\"  --- {p}     ---\")\n",
    "\n",
    "print(\"\\n---    ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f26d34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1290775/135903826.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  q = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096]) torch.float16\n",
      "model.layers.0.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.0.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.0.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.0.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.0.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.0.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.0.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.0.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.0.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.0.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.0.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.0.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.0.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.0.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.0.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.0.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.0.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.0.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.0.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.0.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.0.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.1.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.1.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.1.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.1.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.1.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.1.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.1.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.1.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.1.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.1.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.1.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.1.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.1.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.1.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.1.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.1.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.1.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.1.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.1.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.1.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.1.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.2.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.2.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.2.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.2.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.2.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.2.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.2.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.2.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.2.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.2.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.2.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.2.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.2.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.2.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.2.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.2.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.2.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.2.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.2.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.2.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.2.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.3.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.3.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.3.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.3.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.3.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.3.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.3.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.3.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.3.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.3.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.3.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.3.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.3.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.3.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.3.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.3.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.3.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.3.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.3.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.3.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.3.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.4.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.4.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.4.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.4.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.4.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.4.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.4.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.4.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.4.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.4.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.4.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.4.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.4.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.4.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.4.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.4.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.4.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.4.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.4.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.4.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.4.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.5.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.5.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.5.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.5.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.5.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.5.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.5.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.5.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.5.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.5.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.5.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.5.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.5.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.5.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.5.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.5.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.5.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.5.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.5.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.5.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.5.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.6.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.6.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.6.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.6.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.6.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.6.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.6.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.6.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.6.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.6.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.6.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.6.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.6.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.6.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.6.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.6.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.6.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.6.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.6.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.6.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.6.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.7.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.7.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.7.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.7.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.7.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.7.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.7.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.7.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.7.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.7.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.7.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.7.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.7.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.7.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.7.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.7.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.7.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.7.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.7.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.7.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.7.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.8.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.8.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.8.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.8.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.8.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.8.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.8.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.8.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.8.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.8.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.8.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.8.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.8.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.8.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.8.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.8.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.8.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.8.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.8.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.8.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.8.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.9.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.9.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.9.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.9.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.9.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.9.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.9.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.9.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.9.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.9.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.9.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.9.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.9.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.9.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.9.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.9.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.9.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.9.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.9.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.9.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.9.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.10.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.10.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.10.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.10.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.10.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.10.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.10.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.10.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.10.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.10.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.10.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.10.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.10.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.10.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.10.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.10.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.10.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.10.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.10.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.10.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.10.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.11.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.11.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.11.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.11.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.11.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.11.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.11.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.11.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.11.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.11.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.11.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.11.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.11.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.11.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.11.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.11.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.11.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.11.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.11.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.11.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.11.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.12.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.12.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.12.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.12.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.12.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.12.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.12.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.12.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.12.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.12.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.12.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.12.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.12.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.12.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.12.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.12.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.12.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.12.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.12.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.12.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.12.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.13.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.13.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.13.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.13.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.13.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.13.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.13.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.13.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.13.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.13.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.13.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.13.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.13.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.13.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.13.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.13.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.13.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.13.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.13.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.13.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.13.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.14.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.14.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.14.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.14.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.14.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.14.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.14.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.14.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.14.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.14.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.14.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.14.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.14.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.14.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.14.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.14.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.14.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.14.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.14.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.14.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.14.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.15.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.15.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.15.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.15.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.15.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.15.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.15.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.15.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.15.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.15.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.15.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.15.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.15.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.15.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.15.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.15.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.15.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.15.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.15.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.15.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.15.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.16.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.16.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.16.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.16.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.16.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.16.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.16.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.16.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.16.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.16.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.16.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.16.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.16.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.16.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.16.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.16.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.16.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.16.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.16.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.16.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.16.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.17.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.17.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.17.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.17.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.17.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.17.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.17.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.17.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.17.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.17.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.17.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.17.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.17.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.17.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.17.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.17.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.17.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.17.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.17.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.17.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.17.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.18.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.18.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.18.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.18.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.18.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.18.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.18.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.18.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.18.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.18.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.18.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.18.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.18.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.18.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.18.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.18.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.18.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.18.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.18.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.18.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.18.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.19.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.19.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.19.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.19.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.19.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.19.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.19.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.19.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.19.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.19.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.19.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.19.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.19.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.19.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.19.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.19.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.19.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.19.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.19.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.19.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.19.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.20.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.20.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.20.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.20.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.20.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.20.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.20.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.20.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.20.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.20.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.20.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.20.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.20.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.20.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.20.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.20.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.20.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.20.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.20.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.20.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.20.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.21.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.21.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.21.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.21.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.21.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.21.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.21.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.21.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.21.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.21.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.21.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.21.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.21.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.21.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.21.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.21.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.21.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.21.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.21.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.21.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.21.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.22.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.22.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.22.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.22.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.22.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.22.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.22.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.22.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.22.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.22.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.22.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.22.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.22.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.22.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.22.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.22.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.22.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.22.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.22.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.22.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.22.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.23.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.23.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.23.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.23.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.23.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.23.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.23.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.23.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.23.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.23.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.23.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.23.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.23.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.23.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.23.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.23.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.23.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.23.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.23.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.23.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.23.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.24.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.24.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.24.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.24.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.24.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.24.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.24.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.24.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.24.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.24.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.24.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.24.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.24.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.24.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.24.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.24.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.24.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.24.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.24.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.24.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.24.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.25.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.25.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.25.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.25.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.25.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.25.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.25.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.25.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.25.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.25.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.25.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.25.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.25.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.25.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.25.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.25.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.25.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.25.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.25.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.25.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.25.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.26.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.26.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.26.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.26.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.26.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.26.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.26.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.26.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.26.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.26.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.26.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.26.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.26.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.26.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.26.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.26.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.26.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.26.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.26.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.26.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.26.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.27.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.27.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.27.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.27.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.27.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.27.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.27.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.27.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.27.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.27.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.27.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.27.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.27.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.27.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.27.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.27.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.27.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.27.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.27.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.27.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.27.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.28.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.28.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.28.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.28.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.28.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.28.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.28.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.28.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.28.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.28.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.28.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.28.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.28.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.28.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.28.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.28.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.28.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.28.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.28.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.28.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.28.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.29.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.29.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.29.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.29.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.29.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.29.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.29.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.29.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.29.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.29.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.29.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.29.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.29.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.29.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.29.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.29.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.29.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.29.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.29.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.29.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.29.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.30.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.30.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.30.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.30.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.30.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.30.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.30.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.30.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.30.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.30.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.30.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.30.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.30.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.30.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.30.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.30.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.30.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.30.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.30.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.30.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.30.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.31.self_attn.q_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.31.self_attn.q_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.31.self_attn.q_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.31.self_attn.k_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.31.self_attn.k_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.31.self_attn.k_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.31.self_attn.v_proj.qweight torch.Size([256, 4096]) torch.int16\n",
      "model.layers.31.self_attn.v_proj.scales torch.Size([32, 1024]) torch.float16\n",
      "model.layers.31.self_attn.v_proj.scaled_zeros torch.Size([32, 1024]) torch.float16\n",
      "model.layers.31.self_attn.o_proj.qweight torch.Size([1024, 4096]) torch.int16\n",
      "model.layers.31.self_attn.o_proj.scales torch.Size([32, 4096]) torch.float16\n",
      "model.layers.31.self_attn.o_proj.scaled_zeros torch.Size([32, 4096]) torch.float16\n",
      "model.layers.31.mlp.gate_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.31.mlp.gate_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.31.mlp.gate_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.31.mlp.up_proj.qweight torch.Size([3584, 4096]) torch.int16\n",
      "model.layers.31.mlp.up_proj.scales torch.Size([32, 14336]) torch.float16\n",
      "model.layers.31.mlp.up_proj.scaled_zeros torch.Size([32, 14336]) torch.float16\n",
      "model.layers.31.mlp.down_proj.qweight torch.Size([1024, 14336]) torch.int16\n",
      "model.layers.31.mlp.down_proj.scales torch.Size([112, 4096]) torch.float16\n",
      "model.layers.31.mlp.down_proj.scaled_zeros torch.Size([112, 4096]) torch.float16\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.norm.weight torch.Size([4096]) torch.float16\n",
      "lm_head.weight torch.Size([128256, 4096]) torch.float16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# path = '/home/jgryu/workspace/weight_compression/complexity_test/8b_qtip_noft_4bit.pth'\n",
    "path = '/home/jgryu/workspace/weight_compression/complexity_test/8b-w4-g128-awq-v2.pt'\n",
    "q = torch.load(path)\n",
    "\n",
    "for k,v in q.items():\n",
    "    print(k, v.shape, v.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
