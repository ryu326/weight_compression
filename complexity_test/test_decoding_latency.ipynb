{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1977f888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1106 11:55:30.958710 1763486 warnings.py:109] /opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "W1106 11:55:32.261043 1763486 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "\n",
      "I1106 11:55:32.514288 1763486 utils.py:148] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "I1106 11:55:32.515771 1763486 utils.py:151] Note: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "I1106 11:55:32.516744 1763486 utils.py:164] NumExpr defaulting to 16 threads.\n",
      "W1106 11:55:33.120088 1763486 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @amp.autocast(enabled=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import glog, json\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_attn_mask_utils import \\\n",
    "    _prepare_4d_causal_attention_mask\n",
    "\n",
    "\n",
    "from operator import attrgetter\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "notebook_dir = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from NWC.models import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e8224a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 65534, 65535, 65536],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "comp_model_path = '../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/M16/lmbda50_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_3.87239_bpp_4.65884_MSE_0.0162_total_iter_95000.pth.tar'\n",
    "# comp_model_path = '/workspace/Weight_compression/NWC/checkpoint/nwc_scale_cond/block_seq_scale_cond_scaler_meta-llama--Meta-Llama-3-8B__scaleH_sig0.0001_std_rnormed_with_col_std_lidx_row_1024.pt/rdloss_size128_encdim1024_M256_Q0_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100/lmbda50_/best_loss_model_loss_3.94749_bpp_3.26997_MSE_4.91093_total_iter_192500.pth.tar'\n",
    "config = os.path.join(os.path.dirname(comp_model_path), 'config.json')\n",
    "with open(config, 'r', encoding='utf-8') as file:\n",
    "    config = json.load(file)\n",
    "config = Config(**config)\n",
    "\n",
    "shift, scale = None, None\n",
    "if config.architecture == 'nwc_ql' and not hasattr(config, \"Q\"):\n",
    "    config.Q = 4\n",
    "if not hasattr(config, \"no_layernorm\"):\n",
    "    config.no_layernorm = False\n",
    "\n",
    "\n",
    "comp_model = get_model(config.architecture, config, scale=scale, shift=shift)\n",
    "comp_model.config = config\n",
    "ckpt = torch.load(comp_model_path, weights_only=False)\n",
    "scale, shift  = torch.zeros(1), torch.zeros(1)\n",
    "\n",
    "comp_model.load_state_dict(ckpt[\"state_dict\"], strict = False)\n",
    "comp_model.scale = scale\n",
    "comp_model.shift = shift\n",
    "comp_model.eval()\n",
    "comp_model.update()\n",
    "\n",
    "comp_model.update(force=True)              # CompressAI: CDF ê³ ì • ë° ë²„í¼ ë“±ë¡\n",
    "comp_model.entropy_bottleneck._quantized_cdf  # ìºì‹œë˜ì–´ ì´í›„ ì¬ê³„ì‚° ì•ˆ ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b30ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 11.283 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:4')\n",
    "\n",
    "tt = []\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        T  = torch.zeros(256, 256)\n",
    "        T = T.reshape(1, -1, 16).to(device)\n",
    "        # T = T.reshape(1, -1, 128).to(device)\n",
    "        data = {}\n",
    "        data['weight_block'] = T\n",
    "        data['q_level'] = torch.zeros(1, T.shape[1]).to(torch.int).to(device)\n",
    "        # data['scale_cond'] = torch.zeros_like(T).to(device)\n",
    "\n",
    "        comp_model.to(device)\n",
    "        out_enc = comp_model.compress(data)\n",
    "\n",
    "        # torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        out_dec = comp_model.decompress(out_enc)\n",
    "        # out_dec  = comp_model.decompress(out_enc)\n",
    "        # torch.cuda.synchronize()\n",
    "        end = time.time()\n",
    "        elapsed_ms = (end - start) * 1000\n",
    "        tt.append(elapsed_ms)\n",
    "        # print(f\"Decompress time: {elapsed_ms:.3f} ms\")\n",
    "        # for step, duration in timings.items():\n",
    "        #     print(f\"{step:<25}: {duration:.4f} ms\")\n",
    "\n",
    "\n",
    "avg = 0\n",
    "for t in tt:\n",
    "    avg += t\n",
    "avg /= len(tt)\n",
    "print(f\"Average: {avg:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b696ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing warm-up run...\n",
      "Warm-up finished. Starting latency measurement.\n",
      "\n",
      "--- Latency Measurement Results ---\n",
      "Average Encoding Latency: 7.550 ms\n",
      "Average Decoding Latency: 11.658 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# --- ëª¨ë¸ ë° ì¥ì¹˜ ì„¤ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ì •ì˜ í•„ìš”) ---\n",
    "# comp_model = YourCompressionModel() \n",
    "# device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "# comp_model.to(device)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "encoding_times = []\n",
    "decoding_times = []\n",
    "num_iterations = 100  # ì¸¡ì • ë°˜ë³µ íšŸìˆ˜\n",
    "\n",
    "# --- Warm-up (ì²« ì‹¤í–‰ ì‹œ ë°œìƒí•˜ëŠ” ì˜¤ë²„í—¤ë“œ ì œì™¸) ---\n",
    "# ì •í™•í•œ ì¸¡ì •ì„ ìœ„í•´ ì‹¤ì œ ì¸¡ì • ì „ì— ëª¨ë¸ì„ í•œë‘ ë²ˆ ì‹¤í–‰í•´ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
    "with torch.no_grad():\n",
    "    print(\"Performing warm-up run...\")\n",
    "    temp_tensor = torch.zeros(256, 256).reshape(1, -1, 16).to(device)\n",
    "    temp_data = {\n",
    "        'weight_block': temp_tensor,\n",
    "        'q_level': torch.zeros(1, temp_tensor.shape[1], dtype=torch.int).to(device)\n",
    "    }\n",
    "    out_enc_warmup = comp_model.compress(temp_data)\n",
    "    _ = comp_model.decompress(out_enc_warmup)\n",
    "    torch.cuda.synchronize(device=device) # Warm-upì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°\n",
    "    print(\"Warm-up finished. Starting latency measurement.\")\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_iterations):\n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
    "        # T = torch.zeros(256, 256)\n",
    "        T = torch.zeros(256, 256)\n",
    "        T = T.reshape(1, -1, 16).to(device)\n",
    "        data = {\n",
    "            'weight_block': T,\n",
    "            'q_level': torch.zeros(1, T.shape[1], dtype=torch.int).to(device)\n",
    "        }\n",
    "\n",
    "        # --- ì¸ì½”ë”©(ì••ì¶•) ì‹œê°„ ì¸¡ì • ---\n",
    "        torch.cuda.synchronize(device=device)  # ì´ì „ GPU ì‘ì—…ì´ ëª¨ë‘ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°\n",
    "        start_time = time.time()\n",
    "\n",
    "        out_enc = comp_model.compress(data)\n",
    "\n",
    "        torch.cuda.synchronize(device=device)  # compress ì‘ì—…ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°\n",
    "        end_time = time.time()\n",
    "        encoding_times.append((end_time - start_time) * 1000) # ms ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥\n",
    "\n",
    "        # --- ë””ì½”ë”©(ë³µì›) ì‹œê°„ ì¸¡ì • ---\n",
    "        torch.cuda.synchronize(device=device)  # ì´ì „ GPU ì‘ì—…ì´ ëª¨ë‘ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°\n",
    "        start_time = time.time()\n",
    "        \n",
    "        out_dec = comp_model.decompress(out_enc)\n",
    "        \n",
    "        torch.cuda.synchronize(device=device)  # decompress ì‘ì—…ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°\n",
    "        end_time = time.time()\n",
    "        decoding_times.append((end_time - start_time) * 1000) # ms ë‹¨ìœ„ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ê³„ì‚° ë° ì¶œë ¥\n",
    "# ì²« ë²ˆì§¸ ì¸¡ì •ê°’ì€ ì—¬ì „íˆ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œì™¸í•˜ê³  ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.\n",
    "avg_encoding_ms = sum(encoding_times[1:]) / len(encoding_times[1:]) if len(encoding_times) > 1 else sum(encoding_times) / len(encoding_times)\n",
    "avg_decoding_ms = sum(decoding_times[1:]) / len(decoding_times[1:]) if len(decoding_times) > 1 else sum(decoding_times) / len(decoding_times)\n",
    "\n",
    "print(\"\\n--- Latency Measurement Results ---\")\n",
    "print(f\"Average Encoding Latency: {avg_encoding_ms:.3f} ms\")\n",
    "print(f\"Average Decoding Latency: {avg_decoding_ms:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e994bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "\n",
      "--- ê° ë‹¨ê³„ë³„ í‰ê·  ì†Œìš” ì‹œê°„ (10íšŒ ì‹¤í–‰) ---\n",
      "setup_ms                 : 0.0667 ms\n",
      "transfer:indexes         : 0.3058 ms\n",
      "transfer:cdf_length,offeset: 0.0513 ms\n",
      "transfer:cdf             : 0.0624 ms\n",
      "transfer:entropy_dec_loop_indexes_to_list_ms: 1.4341 ms\n",
      "entropy_dec_loop_decode_call_ms: 4.0333 ms\n",
      "transfer:entropy_dec_loop_values_to_tensor_ms: 4.4711 ms\n",
      "entropy_dec_dequantize_ms: 0.0301 ms\n",
      "synthesis_g_s_ms         : 0.5955 ms\n",
      "total_decompress_ms      : 11.0603 ms\n",
      "\n",
      "--- ğŸš€ GPU ê°€ì† ì‹œë‚˜ë¦¬ì˜¤ ê³„ì‚° (32x ë³‘ë ¬í™”) ---\n",
      "ì›ë³¸ ì´ ì‹œê°„: 11.0603 ms\n",
      "\n",
      "[ì ˆê° ë‚´ì—­]\n",
      "  ë°ì´í„° ì „ì†¡ ì˜¤ë²„í—¤ë“œ ì œê±°: 6.3246 ms\n",
      "  CPU ë””ì½”ë”© ì‹œê°„ (ì›ë³¸): 4.0333 ms\n",
      "  GPU ë””ì½”ë”© ì‹œê°„ (32x ë³‘ë ¬): 0.1260 ms\n",
      "  -> ë””ì½”ë”© ë³‘ë ¬í™”ë¡œ ì¸í•œ ì ˆê°: 3.9073 ms\n",
      "-------------------------------------------------\n",
      "ì´ ì ˆê° ì‹œê°„ (ì˜¤ë²„í—¤ë“œ + ë³‘ë ¬í™”): 10.2319 ms\n",
      "âœ… ìµœì¢… ì˜ˆìƒ ì‹œê°„: 0.8284 ms\n",
      "âœ… ì‹œê°„ ì ˆê°ë¥ : 92.51 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# ì´ì „ì— ì •ì˜ëœ comp_modelì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "# ê° ìŠ¤í…ë³„ ì‹œê°„ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬\n",
    "all_timings = defaultdict(list)\n",
    "num_iterations = 10  # ì¸¡ì • ë°˜ë³µ íšŸìˆ˜\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_iterations):\n",
    "        # --- ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---\n",
    "        T = torch.zeros(256, 256)\n",
    "        T = T.reshape(1, -1, 16).to(device)\n",
    "        # T = torch.zeros(4096, 4096)\n",
    "        # T = T.reshape(32, 32768, 16).to(device)\n",
    "        data = {}\n",
    "        data['weight_block'] = T\n",
    "        data['q_level'] = torch.zeros(1, T.shape[1], dtype=torch.int).to(device)\n",
    "\n",
    "        comp_model.to(device)\n",
    "        out_enc = comp_model.compress(data)\n",
    "\n",
    "        # Decompress ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •\n",
    "        # fast_decompressëŠ” (ê²°ê³¼, ì‹œê°„_ë”•ì…”ë„ˆë¦¬)ë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì •\n",
    "        out_dec, timings = comp_model.fast_decompress_v2(out_enc)\n",
    "        out_dec2 = comp_model.decompress(out_enc)\n",
    "        print(torch.equal(out_dec['x_hat'], out_dec2['x_hat']))\n",
    "\n",
    "        # ê° ë‹¨ê³„ì˜ ì¸¡ì • ì‹œê°„ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "        for step, duration in timings.items():\n",
    "            all_timings[step].append(duration)\n",
    "\n",
    "# --- ê° ë‹¨ê³„ë³„ í‰ê·  ì‹œê°„ ê³„ì‚° ë° ì¶œë ¥ ---\n",
    "print(f\"\\n--- ê° ë‹¨ê³„ë³„ í‰ê·  ì†Œìš” ì‹œê°„ ({num_iterations}íšŒ ì‹¤í–‰) ---\")\n",
    "avg_timings = {}\n",
    "for step, duration_list in all_timings.items():\n",
    "    # ì €ì¥ëœ ì‹œê°„ ë¦¬ìŠ¤íŠ¸ì˜ í‰ê· ì„ ê³„ì‚°\n",
    "    average_duration = sum(duration_list) / len(duration_list)\n",
    "    avg_timings[step] = average_duration\n",
    "\n",
    "# ë³´ê¸° ì¢‹ê²Œ ì •ë ¬í•˜ì—¬ ì¶œë ¥ (ì´ ì†Œìš”ì‹œê°„ì´ ê¸´ ìˆœì„œëŒ€ë¡œ)\n",
    "# sorted_avg_timings = sorted(avg_timings.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_avg_timings = avg_timings.items()\n",
    "\n",
    "for step, avg_duration in sorted_avg_timings:\n",
    "    print(f\"{step:<25}: {avg_duration:.4f} ms\")\n",
    "    \n",
    "    \n",
    "print(\"\\n--- ğŸš€ GPU ê°€ì† ì‹œë‚˜ë¦¬ì˜¤ ê³„ì‚° (32x ë³‘ë ¬í™”) ---\")\n",
    "\n",
    "try:\n",
    "    # 1. ì›ë³¸ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n",
    "    original_total_time = avg_timings['total_decompress_ms']\n",
    "    \n",
    "    # 2. ì ˆê° 1: ë°ì´í„° ì „ì†¡ ì˜¤ë²„í—¤ë“œ ì œê±° (CPU <-> GPU)\n",
    "    # .tolist() ë° list -> tensor() ë³€í™˜ì— ì†Œìš”ë˜ëŠ” ì‹œê°„\n",
    "    # 'transfer:'ë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“  íƒ€ì´ë° ê°’ì„ ë”í•´ì„œ ì˜¤ë²„í—¤ë“œ ì‹œê°„ìœ¼ë¡œ ì§‘ê³„\n",
    "    overhead_elimination_time = sum(\n",
    "        value for key, value in avg_timings.items() if 'transfer:' in key\n",
    "    )\n",
    "    \n",
    "    # 3. ì ˆê° 2: ë””ì½”ë”© ë³‘ë ¬í™”\n",
    "    original_decode_time = avg_timings.get('entropy_dec_loop_decode_call_ms', 0)\n",
    "    parallelization_factor = 32\n",
    "    # GPU ì»¤ë„ ì‹¤í–‰ ì‹œê°„ (ë³‘ë ¬í™” ê°€ì •)\n",
    "    new_decode_time = original_decode_time / parallelization_factor\n",
    "    \n",
    "    # ë³‘ë ¬í™”ë¡œ ì¸í•œ ìˆœìˆ˜ ì ˆê° ì‹œê°„\n",
    "    time_saved_from_parallelization = original_decode_time - new_decode_time\n",
    "    \n",
    "    # 4. ì´ ì ˆê° ì‹œê°„ ê³„ì‚°\n",
    "    total_time_saved = overhead_elimination_time + time_saved_from_parallelization\n",
    "    \n",
    "    # 5. ìƒˆë¡œìš´ ì´ ì‹œê°„ ê³„ì‚°\n",
    "    new_total_decompress_time = original_total_time - total_time_saved\n",
    "    \n",
    "    # 6. ì ˆê°ë¥  ê³„ì‚°\n",
    "    percentage_saved = (total_time_saved / original_total_time) * 100\n",
    "\n",
    "    # 7. ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"ì›ë³¸ ì´ ì‹œê°„: {original_total_time:.4f} ms\")\n",
    "    print(\"\\n[ì ˆê° ë‚´ì—­]\")\n",
    "    print(f\"  ë°ì´í„° ì „ì†¡ ì˜¤ë²„í—¤ë“œ ì œê±°: {overhead_elimination_time:.4f} ms\")\n",
    "    print(f\"  CPU ë””ì½”ë”© ì‹œê°„ (ì›ë³¸): {original_decode_time:.4f} ms\")\n",
    "    print(f\"  GPU ë””ì½”ë”© ì‹œê°„ (32x ë³‘ë ¬): {new_decode_time:.4f} ms\")\n",
    "    print(f\"  -> ë””ì½”ë”© ë³‘ë ¬í™”ë¡œ ì¸í•œ ì ˆê°: {time_saved_from_parallelization:.4f} ms\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"ì´ ì ˆê° ì‹œê°„ (ì˜¤ë²„í—¤ë“œ + ë³‘ë ¬í™”): {total_time_saved:.4f} ms\")\n",
    "    print(f\"âœ… ìµœì¢… ì˜ˆìƒ ì‹œê°„: {new_total_decompress_time:.4f} ms\")\n",
    "    print(f\"âœ… ì‹œê°„ ì ˆê°ë¥ : {percentage_saved:.2f} %\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"\\n[ì˜¤ë¥˜] ê³„ì‚°ì— í•„ìš”í•œ íƒ€ì´ë° í‚¤({e})ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ì˜¤ë¥˜] ê³„ì‚° ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95cd6d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6267370869270278"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6.4673 / 10.3190"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caac08ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 6.100 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:4')\n",
    "\n",
    "tt = []\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        T  = torch.zeros(256, 256)\n",
    "        T = T.reshape(1, -1, 16).to(device)\n",
    "        # T = T.reshape(1, -1, 128).to(device)\n",
    "        data = {}\n",
    "        data['weight_block'] = T\n",
    "        data['q_level'] = torch.zeros(1, T.shape[1]).to(torch.int).to(device)\n",
    "        # data['scale_cond'] = torch.zeros_like(T).to(device)\n",
    "\n",
    "        comp_model.to(device)\n",
    "        start = time.time()\n",
    "        out_enc = comp_model(data)\n",
    "        end = time.time()\n",
    "\n",
    "        elapsed_ms = (end - start) * 1000\n",
    "        tt.append(elapsed_ms)\n",
    "        # print(f\"Decompress time: {elapsed_ms:.3f} ms\")\n",
    "        # for step, duration in timings.items():\n",
    "        #     print(f\"{step:<25}: {duration:.4f} ms\")\n",
    "\n",
    "\n",
    "avg = 0\n",
    "for t in tt:\n",
    "    avg += t\n",
    "avg /= len(tt)\n",
    "print(f\"Average: {avg:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ae59f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
