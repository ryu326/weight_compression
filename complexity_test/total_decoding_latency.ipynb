{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1977f888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1106 12:30:26.501163 1801269 warnings.py:109] /opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "W1106 12:30:27.951740 1801269 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "\n",
      "I1106 12:30:28.310747 1801269 utils.py:148] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "I1106 12:30:28.311720 1801269 utils.py:151] Note: NumExpr detected 128 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\n",
      "I1106 12:30:28.312429 1801269 utils.py:164] NumExpr defaulting to 16 threads.\n",
      "W1106 12:30:28.768769 1801269 warnings.py:109] /opt/conda/lib/python3.10/site-packages/compressai/models/video/google.py:353: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @amp.autocast(enabled=False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import glog, json\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_attn_mask_utils import \\\n",
    "    _prepare_4d_causal_attention_mask\n",
    "\n",
    "\n",
    "from operator import attrgetter\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "notebook_dir = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from NWC.models import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e8224a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ...,     0,     0,     0],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        [    0,     1,     2,  ..., 65535, 65536,     0],\n",
       "        ...,\n",
       "        [    0,     1,     2,  ..., 65535, 65536,     0],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0],\n",
       "        [    0,     1,     2,  ...,     0,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "# comp_model_path = '../NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/M16/lmbda50_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_3.87239_bpp_4.65884_MSE_0.0162_total_iter_95000.pth.tar'\n",
    "# comp_model_path = '/workspace/Weight_compression/NWC/checkpoint/nwc_scale_cond/block_seq_scale_cond_scaler_meta-llama--Meta-Llama-3-8B__scaleH_sig0.0001_std_rnormed_with_col_std_lidx_row_1024.pt/rdloss_size128_encdim1024_M256_Q0_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100/lmbda50_/best_loss_model_loss_3.94749_bpp_3.26997_MSE_4.91093_total_iter_192500.pth.tar'\n",
    "comp_model_path = '/home/jgryu/workspace/weight_compression/NWC/checkpoint/nwc_ql/block_seq_ql_random_scaler_meta-llama--Meta-Llama-3-8B__col_1024_gaussian_padding.pt/M16/lmbda300_rdloss_ql_size16_encdim512_M16_Q4_R0_m0_batch_size2048_total_iter200000_lr0.0001_seed100/best_loss_model_loss_5.32101_bpp_5.72603_MSE_0.00289_total_iter_95000.pth.tar'\n",
    "config = os.path.join(os.path.dirname(comp_model_path), 'config.json')\n",
    "with open(config, 'r', encoding='utf-8') as file:\n",
    "    config = json.load(file)\n",
    "config = Config(**config)\n",
    "\n",
    "shift, scale = None, None\n",
    "if config.architecture == 'nwc_ql' and not hasattr(config, \"Q\"):\n",
    "    config.Q = 4\n",
    "if not hasattr(config, \"no_layernorm\"):\n",
    "    config.no_layernorm = False\n",
    "\n",
    "\n",
    "comp_model = get_model(config.architecture, config, scale=scale, shift=shift)\n",
    "comp_model.config = config\n",
    "ckpt = torch.load(comp_model_path, weights_only=False)\n",
    "scale, shift  = torch.zeros(1), torch.zeros(1)\n",
    "\n",
    "comp_model.load_state_dict(ckpt[\"state_dict\"], strict = False)\n",
    "comp_model.scale = scale\n",
    "comp_model.shift = shift\n",
    "comp_model.eval()\n",
    "comp_model.update()\n",
    "\n",
    "comp_model.update(force=True)              # CompressAI: CDF κ³ μ • λ° λ²„νΌ λ“±λ΅\n",
    "comp_model.entropy_bottleneck._quantized_cdf  # μΊμ‹λμ–΄ μ΄ν›„ μ¬κ³„μ‚° μ• λ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b30ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup_ms                 : 1053.9334 ms\n",
      "transfer:indexes         : 22798.9820 ms\n",
      "transfer:cdf_length,offeset: 620.0715 ms\n",
      "transfer:cdf             : 949.6154 ms\n",
      "transfer:entropy_dec_loop_indexes_to_list_ms: 155777.9585 ms\n",
      "entropy_dec_loop_decode_call_ms: 449002.3644 ms\n",
      "transfer:entropy_dec_loop_values_to_tensor_ms: 527616.1310 ms\n",
      "entropy_dec_dequantize_ms: 540.8108 ms\n",
      "synthesis_g_s_ms         : 9605.5118 ms\n",
      "total_decompress_ms      : 1168116.0237 ms\n",
      "\n",
      "--- π€ GPU κ°€μ† μ‹λ‚λ¦¬μ¤ κ³„μ‚° (32x λ³‘λ ¬ν™”) ---\n",
      "μ›λ³Έ μ΄ μ‹κ°„: 1168116.0237 ms\n",
      "\n",
      "[μ κ° λ‚΄μ—­]\n",
      "  λ°μ΄ν„° μ „μ†΅ μ¤λ²„ν—¤λ“ μ κ±°: 707762.7584 ms\n",
      "  CPU λ””μ½”λ”© μ‹κ°„ (μ›λ³Έ): 449002.3644 ms\n",
      "  GPU λ””μ½”λ”© μ‹κ°„ (32x λ³‘λ ¬): 14031.3239 ms\n",
      "  -> λ””μ½”λ”© λ³‘λ ¬ν™”λ΅ μΈν• μ κ°: 434971.0405 ms\n",
      "-------------------------------------------------\n",
      "μ΄ μ κ° μ‹κ°„ (μ¤λ²„ν—¤λ“ + λ³‘λ ¬ν™”): 1142733.7989 ms\n",
      "β… μµμΆ… μμƒ μ‹κ°„: 25382.2247 ms\n",
      "β… μ‹κ°„ μ κ°λ¥ : 97.83 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "path = '/home/jgryu/workspace/weight_compression/hf_model_comp/comp_qtip/ckpt/meta-llama--Meta-Llama-3-8B/ql_ldlq128_rnorm_codes_test/lmbda300'\n",
    "codes_dict = {}\n",
    "metadata_dict = {}\n",
    "\n",
    "# n_parm = 0\n",
    "# n_bit = 0\n",
    "\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "all_timings = defaultdict(list)\n",
    "\n",
    "comp_model.to(device)\n",
    "\n",
    "\n",
    "for i in range(32):\n",
    "    for m in ['q', 'k', 'v', 'o', 'up', 'down', 'gate']:\n",
    "        pt = f'{path}/{i}_{m}.pt'\n",
    "        s = torch.load(pt, weights_only=False, map_location='cpu')\n",
    "\n",
    "        codes = s['codes']\n",
    "        tensor_code_list = []\n",
    "        for c in codes:\n",
    "            c['q_level'] = c['q_level'].to(device)\n",
    "            \n",
    "            out_dec, timings = comp_model.fast_decompress_v2(c)\n",
    "\n",
    "            for step, duration in timings.items():\n",
    "                all_timings[step].append(duration)\n",
    "            \n",
    "            \n",
    "total_timings = {}\n",
    "for step, duration_list in all_timings.items():\n",
    "    average_duration = sum(duration_list)\n",
    "    total_timings[step] = average_duration\n",
    "\n",
    "sorted_total_timings = total_timings.items()\n",
    "\n",
    "for step, avg_duration in sorted_total_timings:\n",
    "    print(f\"{step:<25}: {avg_duration:.4f} ms\")\n",
    "    \n",
    "print(\"\\n--- π€ GPU κ°€μ† μ‹λ‚λ¦¬μ¤ κ³„μ‚° (32x λ³‘λ ¬ν™”) ---\")\n",
    "try:\n",
    "    original_total_time = total_timings['total_decompress_ms']\n",
    "    overhead_elimination_time = sum(\n",
    "        value for key, value in total_timings.items() if 'transfer:' in key\n",
    "    )\n",
    "    \n",
    "    original_decode_time = total_timings.get('entropy_dec_loop_decode_call_ms', 0)\n",
    "    parallelization_factor = 32\n",
    "    new_decode_time = original_decode_time / parallelization_factor\n",
    "    time_saved_from_parallelization = original_decode_time - new_decode_time\n",
    "    total_time_saved = overhead_elimination_time + time_saved_from_parallelization\n",
    "    new_total_decompress_time = original_total_time - total_time_saved\n",
    "    percentage_saved = (total_time_saved / original_total_time) * 100\n",
    "    print(f\"μ›λ³Έ μ΄ μ‹κ°„: {original_total_time:.4f} ms\")\n",
    "    print(\"\\n[μ κ° λ‚΄μ—­]\")\n",
    "    print(f\"  λ°μ΄ν„° μ „μ†΅ μ¤λ²„ν—¤λ“ μ κ±°: {overhead_elimination_time:.4f} ms\")\n",
    "    print(f\"  CPU λ””μ½”λ”© μ‹κ°„ (μ›λ³Έ): {original_decode_time:.4f} ms\")\n",
    "    print(f\"  GPU λ””μ½”λ”© μ‹κ°„ (32x λ³‘λ ¬): {new_decode_time:.4f} ms\")\n",
    "    print(f\"  -> λ””μ½”λ”© λ³‘λ ¬ν™”λ΅ μΈν• μ κ°: {time_saved_from_parallelization:.4f} ms\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\"μ΄ μ κ° μ‹κ°„ (μ¤λ²„ν—¤λ“ + λ³‘λ ¬ν™”): {total_time_saved:.4f} ms\")\n",
    "    print(f\"β… μµμΆ… μμƒ μ‹κ°„: {new_total_decompress_time:.4f} ms\")\n",
    "    print(f\"β… μ‹κ°„ μ κ°λ¥ : {percentage_saved:.2f} %\")\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"\\n[μ¤λ¥] κ³„μ‚°μ— ν•„μ”ν• νƒ€μ΄λ° ν‚¤({e})λ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[μ¤λ¥] κ³„μ‚° μ¤‘ μμ™Έ λ°μƒ: {e}\")      \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9ae59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.3822247"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25382.2247 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2035f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
