{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook visualizes a trained 2-d LTC model on the Banana source. Prior to running the notebook, the LTC model needs to be trained. Use\n",
    "```\n",
    "python train_model.py --n 1 --d 2 --dy 2 --n_train_samples 1000000 --num_eval_samples 1000000 --model_name \"LTCDither\" --data_name \"Banana\" --transform_name \"MLP2\" --eb_name \"FactorizedPrior\" --lattice_name \"HexagonalUnitVol\" --lam_sweep 32 --epochs 1 --save\n",
    "```\n",
    "which will save the trained model in ``trained/Banana``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LTC.layers import LTC\n",
    "import LTC.data as data\n",
    "import torch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from colorhash import ColorHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "C_colors_fixed = [[0.8352941176470589, 0.8676470588235294, 0.38235294117647056], [0.6441176470588236, 0.47352941176470587, 0.38235294117647056], [0.6029411764705882, 0.44411764705882356, 0.42058823529411765], [0.38235294117647056, 0.7382352941176471, 0.8676470588235294], [0.6264705882352941, 0.8117647058823529, 0.43823529411764706], [0.6058823529411764, 0.7147058823529412, 0.8676470588235294], [0.6911764705882353, 0.6470588235294118, 0.8294117647058824], [0.6911764705882353, 0.6470588235294118, 0.8294117647058824], [0.6911764705882353, 0.6470588235294118, 0.8294117647058824], [0.6441176470588236, 0.5352941176470588, 0.38235294117647056], [0.6441176470588236, 0.5352941176470588, 0.38235294117647056], [0.6441176470588236, 0.5352941176470588, 0.38235294117647056], [0.8117647058823529, 0.44411764705882356, 0.43823529411764706], [0.6470588235294118, 0.65, 0.8294117647058824], [0.6323529411764706, 0.3411764705882353, 0.6823529411764706], [0.7558823529411764, 0.7264705882352941, 0.49411764705882355], [0.7558823529411764, 0.7264705882352941, 0.49411764705882355], [0.7558823529411764, 0.7264705882352941, 0.49411764705882355], [0.43823529411764706, 0.8117647058823529, 0.7176470588235294], [0.8117647058823529, 0.5264705882352941, 0.43823529411764706], [0.6470588235294118, 0.8294117647058824, 0.7029411764705882], [0.6029411764705882, 0.42058823529411765, 0.5205882352941176], [0.6029411764705882, 0.42058823529411765, 0.5205882352941176], [0.6029411764705882, 0.42058823529411765, 0.5205882352941176], [0.8676470588235294, 0.38235294117647056, 0.5029411764705882], [0.8676470588235294, 0.38235294117647056, 0.5029411764705882], [0.8676470588235294, 0.38235294117647056, 0.5029411764705882], [0.6441176470588236, 0.4294117647058824, 0.38235294117647056], [0.6441176470588236, 0.4294117647058824, 0.38235294117647056], [0.6441176470588236, 0.4294117647058824, 0.38235294117647056], [0.6441176470588236, 0.38235294117647056, 0.3911764705882353], [0.6029411764705882, 0.42058823529411765, 0.5411764705882354], [0.8205882352941176, 0.6470588235294118, 0.8294117647058824], [0.6029411764705882, 0.6441176470588236, 0.38235294117647056], [0.43823529411764706, 0.7, 0.8117647058823529], [0.43823529411764706, 0.7, 0.8117647058823529], [0.43823529411764706, 0.7, 0.8117647058823529], [0.6029411764705882, 0.5735294117647058, 0.42058823529411765], [0.9088235294117647, 0.5676470588235294, 0.7147058823529412], [0.45, 0.6029411764705882, 0.42058823529411765], [0.45, 0.6029411764705882, 0.42058823529411765], [0.45, 0.6029411764705882, 0.42058823529411765], [0.6294117647058823, 0.38235294117647056, 0.6441176470588236], [0.7676470588235293, 0.6058823529411764, 0.8676470588235294], [0.5941176470588235, 0.43823529411764706, 0.8117647058823529], [0.7558823529411764, 0.49411764705882355, 0.6264705882352941], [0.4470588235294118, 0.6441176470588236, 0.38235294117647056], [0.7558823529411764, 0.49411764705882355, 0.6647058823529413]]\n",
    "def plot_quantizer2(x, x_hat, y, y_hat, y_unif=None, lik=None):\n",
    "    C2 = torch.unique(torch.cat((x_hat, y_hat), dim=1), dim=0)\n",
    "    C, C_latent = C2[:,:2], C2[:,2:]\n",
    "\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    colors = prop_cycle.by_key()['color']\n",
    "    colors = C_colors_fixed\n",
    "    color_iter = itertools.cycle(colors)\n",
    "    C_colors = []\n",
    "    Ms = []\n",
    "    latent_dist = []\n",
    "    plt.figure(1, figsize=(4.5, 4.5), constrained_layout=True)\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    for i in range(C_latent.shape[0]):\n",
    "        M = np.all(y_hat.numpy() == C_latent[i, :].numpy(), axis=1)\n",
    "        Ms.append(M)\n",
    "        # print(np.sum(M) / len(M))\n",
    "        latent_dist.append(np.sum(M) / len(M)) # append frequency\n",
    "        # color_i = np.random.rand(3) * 0.5 + 0.5\n",
    "        color_i = list((np.array(ColorHash(C_latent[i,:]).rgb, dtype=float) / 255) * 0.7 + 0.3)\n",
    "        # color_i = next(color_iter)\n",
    "        C_colors.append(color_i)\n",
    "        plt.scatter(y[M,0], y[M,1], s=1, alpha=0.75, color=color_i)\n",
    "    plt.scatter(C_latent[:, 0], C_latent[:, 1], marker='o', c='black', edgecolors='black', s=20, label=r'$\\hat{Y}$')\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    plt.xlim([-2.2, 2.2])\n",
    "    plt.ylim([-2.2, 2.2])\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    plt.figure(2, figsize=(4.5, 4.5), constrained_layout=True)\n",
    "    for i in range(C.shape[0]):\n",
    "        M = Ms[i]\n",
    "        plt.scatter(x[M,0], x[M,1], s=2, alpha=0.75, color=C_colors[i])\n",
    "    # plt.scatter(C[:, 0], C[:, 1], marker='o', c=np.array(C_colors), edgecolors='salmon', s=50, label=r'$\\hat{X}$')\n",
    "    plt.scatter(C[:, 0], C[:, 1], marker='o', c='black', edgecolors='black', s=40, label=r'$\\hat{X}$')\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    plt.xlim([-2.5, 4])\n",
    "    plt.ylim([-1.25, 3])\n",
    "    plt.axis('off')\n",
    "    print(C_colors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Companders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hexagonal Lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'Banana'\n",
    "eb_name = 'FactorizedPrior'\n",
    "lattice = 'HexagonalUnitVol'\n",
    "tname = 'MLP2'\n",
    "model_name = 'LTCDither'\n",
    "n=1\n",
    "lam = 32.0\n",
    "model = LTC(d=2, dy=2, d_hidden=100, lattice=lattice, tname=tname, eb_name=eb_name)\n",
    "saved = torch.load(f'trained/{source}/{model_name}_{lattice}_{tname}_{eb_name}_n{n}_d2_dy2_Nint2048_lam{lam}.pt', map_location='cpu')\n",
    "model.load_state_dict(saved)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate reconstructions\n",
    "# import data\n",
    "loader = data.Banana(30000)\n",
    "# loader = data.Gaussian(n=2, batch_size=50000)\n",
    "it = iter(loader)\n",
    "x = next(it)[0]\n",
    "# x = torch.rand(150000, 2)\n",
    "# x[:,0] = x[:,0]*8 - 3\n",
    "# x[:,1] = x[:,1]*7 -2\n",
    "x = x.to(device) \n",
    "model = model.to(device)\n",
    "# xq, lik = model.forward(x)\n",
    "xq = []\n",
    "lik = []\n",
    "y = []\n",
    "y_hat = []\n",
    "bsize = 200\n",
    "for i in tqdm.trange(len(x) // bsize):\n",
    "    xq1, lik1, y1, y_hat1 = model.eval(x[i*bsize:i*bsize+bsize], return_y=True)\n",
    "    xq.append(xq1.detach().cpu())\n",
    "    lik.append(lik1.detach().cpu())\n",
    "    y.append(y1.detach().cpu())\n",
    "    y_hat.append(y_hat1.detach().cpu())\n",
    "xq = torch.cat(xq)\n",
    "lik = torch.cat(lik)\n",
    "y = torch.cat(y)\n",
    "y_hat = torch.cat(y_hat)\n",
    "x = x.cpu()\n",
    "print(f'rate={torch.sum(-torch.log2(lik)) / (lik.shape[0]*n)}, distortion={torch.mean((x-xq)**2)}')\n",
    "# xq = xq.detach().cpu()\n",
    "# lik = lik.detach().cpu()\n",
    "# y = y.detach().cpu()\n",
    "# y_hat = y_hat.detach().cpu()\n",
    "# xq, idx = Q(x, retidx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quantizer2(x, xq, y, y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
