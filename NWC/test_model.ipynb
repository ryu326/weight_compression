{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    CLIPVisionModelWithProjection,\n",
    "    ViTForImageClassification,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1454676/843036172.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleVAECompressionModel(\n",
       "  (g_a): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=512, bias=True)\n",
       "    (1): ResidualStack(\n",
       "      (stack): ModuleList(\n",
       "        (0-3): 4 x Linear_ResBlock(\n",
       "          (lin_1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Linear(in_features=512, out_features=16, bias=True)\n",
       "  )\n",
       "  (g_s): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=512, bias=True)\n",
       "    (1): ResidualStack(\n",
       "      (stack): ModuleList(\n",
       "        (0-3): 4 x Linear_ResBlock(\n",
       "          (lin_1): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Linear(in_features=512, out_features=16, bias=True)\n",
       "  )\n",
       "  (entropy_bottleneck): EntropyBottleneck(\n",
       "    (likelihood_lower_bound): LowerBound()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(f\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = '/home/jgryu/Weight_compression/VQVAE/checkpoint/nwc_ql/block_ql_col_16/lmbda100_rdloss_ql_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100/best_loss_model_loss_12.00164_bpp_6.0_MSE_0.02411_total_iter_450000.pth.tar'\n",
    "model_path = '/home/jgryu/Weight_compression/VQVAE/checkpoint/nwc_ql_random/block_ql_random_col_128/lmbda100_rdloss_ql_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100/best_loss_model_loss_4.38201_bpp_5.515_MSE_0.00803_total_iter_1330000.pth.tar'\n",
    "# model_path = '/home/jgryu/Weight_compression/VQVAE/checkpoint/nwc_ql_random/block_ql_random_col_128/lmbda1000_rdloss_ql_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100/best_loss_model_loss_6.22785_bpp_7.30125_MSE_0.00081_total_iter_1330000.pth.tar'\n",
    "model_path = '/home/jgryu/Weight_compression/VQVAE/checkpoint/nwc/block_row_16/lmbda100_rdloss_encdim512_batch_size4096_total_iter1500000_lr0.0001_seed100/best_mse_model_bpp_6.394_MSE_0.00727_total_iter_1390000.pth.tar'\n",
    "\n",
    "config = os.path.join(os.path.dirname(model_path), 'config.json')\n",
    "with open(config, 'r', encoding='utf-8') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "comp_model = models.SimpleVAECompressionModel(\n",
    "    input_size=config['input_size'],\n",
    "    dim_encoder=config['dim_encoder'],\n",
    "    n_resblock=config['n_resblock'],\n",
    "    scale=torch.zeros(config['input_size']),\n",
    "    shift=torch.zeros(config['input_size'])\n",
    ")\n",
    "\n",
    "ckpt = torch.load(model_path)\n",
    "comp_model.load_state_dict(ckpt[\"state_dict\"])\n",
    "comp_model.to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_model.quality_embedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, perplexity=2, random_state=42)  # perplexity는 1 이상이어야 함\n",
    "# vectors_embedded = tsne.fit_transform(comp_model.quality_embedding.weight[0:4].detach().numpy())\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for i, (x, y) in enumerate(vectors_embedded):\n",
    "#     plt.scatter(x, y, label=f\"Vector {i+1}\")\n",
    "#     # plt.text(x + 0.02, y + 0.02, f\"V{i+1}\", fontsize=9)  # 벡터 번호 추가\n",
    "\n",
    "# plt.title(\"t-SNE Visualization of 5 Vectors\")\n",
    "# plt.xlabel(\"t-SNE Component 1\")\n",
    "# plt.ylabel(\"t-SNE Component 2\")\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# empty_model = models.NWC_ql(\n",
    "#     input_size=config['input_size'],\n",
    "#     dim_encoder=config['dim_encoder'],\n",
    "#     n_resblock=config['n_resblock'],\n",
    "#     scale=torch.zeros(config['input_size']),\n",
    "#     shift=torch.zeros(config['input_size'])\n",
    "# )\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# tsne = TSNE(n_components=2, perplexity=2, random_state=42)  # perplexity는 1 이상이어야 함\n",
    "# vectors_embedded = tsne.fit_transform(empty_model.quality_embedding.weight[0:4].detach().numpy())\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for i, (x, y) in enumerate(vectors_embedded):\n",
    "#     plt.scatter(x, y, label=f\"Vector {i+1}\")\n",
    "#     # plt.text(x + 0.02, y + 0.02, f\"V{i+1}\", fontsize=9)  # 벡터 번호 추가\n",
    "\n",
    "# plt.title(\"t-SNE Visualization of 5 Vectors\")\n",
    "# plt.xlabel(\"t-SNE Component 1\")\n",
    "# plt.ylabel(\"t-SNE Component 2\")\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/Weight_compression/VQVAE/datasets_weight_block.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(dataset_folder_path)\n"
     ]
    }
   ],
   "source": [
    "from datasets_weight_block_qlevel import get_dataset_block_qlevel, LayerInputs\n",
    "from datasets_weight_block_qlevel_random import get_dataset_block_random_qlevel\n",
    "from datasets_weight_block_qlevel_random import get_dataset_block_random_qlevel\n",
    "from datasets_weight_block import get_datasets_block\n",
    "\n",
    "# train_dataset, test_dataset, train_std, test_std = get_dataset_block_random_qlevel('../Wparam_dataset/dataset_block/meta-llama/Meta-Llama-3-8B/mlp_attn_128_col_dataset.pt')\n",
    "# train_dataset, test_dataset, train_std, test_std = get_dataset_block_qlevel('col', 16)\n",
    "train_dataset, test_dataset, train_std, test_std = get_datasets_block('../Wparam_dataset/dataset_block/meta-llama/Meta-Llama-3-8B/mlp_attn_16_row_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_dataset, model, criterion):\n",
    "    mean_MSE = 0\n",
    "    avg_bpp = 0\n",
    "    mean_loss = 0\n",
    "    mean_recon_loss = 0\n",
    "    mean_bpp_loss = 0\n",
    "    device = next(model.parameters()).device\n",
    "    mse_func = nn.MSELoss()\n",
    "\n",
    "    batch_size = 100  # 원하는 배치 크기\n",
    "    num_samples = len(test_dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch = {key: [] for key in test_dataset[0].keys()}\n",
    "\n",
    "            # 배치 데이터를 수집\n",
    "            for idx in range(i, min(i + batch_size, num_samples)):\n",
    "                data = test_dataset[idx]\n",
    "                for key, value in data.items():\n",
    "                    batch[key].append(value)\n",
    "\n",
    "            # 리스트를 텐서로 변환 후 GPU로 이동\n",
    "            batch = {key: torch.stack(value).to(device) for key, value in batch.items()}\n",
    "\n",
    "            # Forward pass through the model\n",
    "            out_net = model(batch)\n",
    "            out_loss = criterion(data=batch, output=out_net)\n",
    "\n",
    "            # Accumulate losses\n",
    "            mean_loss += out_loss['loss'].item() * len(batch['weight_block'])\n",
    "            mean_recon_loss += out_loss['recon_loss'].item() * len(batch['weight_block'])\n",
    "            mean_bpp_loss += out_loss['bpp_loss'].item() * len(batch['weight_block'])\n",
    "\n",
    "            # Compression and decompression\n",
    "            \n",
    "            batch['weight_block'] =  batch['weight_block'].permute(0, 2, 1).contiguous()\n",
    "            \n",
    "            out_enc = model.compress(batch)\n",
    "            out_dec = model.decompress(out_enc[\"strings\"], torch.size(16))\n",
    "\n",
    "            # Calculate BPP and MSE for the batch\n",
    "            num_pixels = batch['weight_block'].numel() // batch['weight_block'].size(0)  # Per image\n",
    "\n",
    "            batch_bpp = 0\n",
    "            for s in out_enc[\"strings\"]:\n",
    "                batch_bpp += sum(len(string) for string in s) * 8.0 / num_pixels\n",
    "\n",
    "            x_hat = out_dec[\"x_hat\"].clone().detach()\n",
    "            mean_MSE += mse_func(batch['weight_block'], x_hat).item() * len(batch['weight_block'])\n",
    "            avg_bpp += batch_bpp * len(batch['weight_block'])\n",
    "\n",
    "    # Normalize accumulated metrics by the total number of samples\n",
    "    mean_loss /= num_samples\n",
    "    mean_recon_loss /= num_samples\n",
    "    mean_bpp_loss /= num_samples\n",
    "    mean_MSE /= num_samples\n",
    "    avg_bpp /= num_samples\n",
    "\n",
    "    return {\n",
    "        'TEST MSE': mean_MSE,\n",
    "        'TEST BPP': avg_bpp,\n",
    "        'TEST loss': mean_loss,\n",
    "        'TEST recon_loss': mean_recon_loss,\n",
    "        'TEST bpp_loss': mean_bpp_loss\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import *\n",
    "\n",
    "criterion = RateDistortionLoss(train_std, lmbda=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decode_with_indexes(): incompatible function arguments. The following argument types are supported:\n    1. (self: compressai.ans.RansDecoder, arg0: str, arg1: list[int], arg2: list[list[int]], arg3: list[int], arg4: list[int]) -> list[int]\n\nInvoked with: <compressai.ans.RansDecoder object at 0x7fe231bc7670>, [b'\\x8b\\xbaths*\\x04\\x00\\xebE\\x13W', b'\\xb0uj\\xd1\\xb5k\\xf7\\x01W=\\xa4\\xf9', b'\\xd1\\xc5\\xe5\\x13\\xb7\\xc6\\xca\\x00Z\\xb6,\\xfb', b'-\\xc6?rq\\xa6\\x104\\xdf>s\\x9e', b'\\xb5\\xbc\\x1f\\x99\\xdb\\x0b\\x19\\x11a\\x83\\xc4\\x07', b' \\xc9 Q\\xd7\\xcc\\x18\\x002\\xddo\\x06', b'\\xf7Mx\\xe7\\x00\\x00\\x00\\x00P\\xc8 V\\x04\\x03y\\x12', b'\\xc6Ug+%\\xf4\\xd3\\x07?\\xc3\\xfe\\xe6', b'B\\xec\\xe5\\xaf\\x05\\x00\\x00\\x00\\xcc\\x7f\\x88\\xe5^\\x19\\xf2\\xe1', b'd[V\\x87\\xe3M\\x02\\x00\\xdd7\\xe2X', b'~\\x92\\x02\\xb0N\\x88<\\x00LT\\x05\\xc9', b'l\\xde\\x9c\\x86\\x10\\x00\\x00\\x00\\xcfN\\x15:\\x0b\\x1b\\xf2<', b'\\x074ror\\xca\\t\\x00\\x8eHuU', b'\\xd6L!\\xef\\xc6j\\x1a\\x00\\x86\\x8a\\x16g', b\"v@\\xc2\\x12\\xcb\\x867\\x01'\\xc0W\\xfc\", b\"\\xd1/\\xce\\x96\\xd7\\xcb'p\\xfdyE\\xed\", b'\\x0e\\xce\\x7f\\x0c\\x85tr\\x00\\x81V\\xaf\\xbb', b'\\xc8\\x92\\xe1C/W+\\x00^\\xbb[]', b'\\xd3\\x9cJ\\x1a\\xb1^y\\x00s\\xdc|\\x1d', b'Q\\xc8\\xd6I7\\x00\\x00\\x00A\"\\x94\\x94\\x18x~\\xcc', b'\\xb0\\xbd89\\xf8\\x8a7\\x02J\\xb5\\xab\\xd0', b'\\xa1\\xacW\\xe3\\xa2\\xeb\\r\\x00\\xfe#\\x8b\\x03', b'\\x94\\x1df\\xc1$\"d\\x00\\x1a\\xec\"\\xee', b'\\x9b\\x1d\\xcb\\x05\\xd27\\xca\\x01\\xa4|UZ', b'\\xd5\\x17\\xc7\\x91\\x01\\x00\\x00\\x00U\\xc9^\\x03\\xa3\\x9f\\xc8\\xfa', b'\\xceO\\xf7\\x98k\\x1d]\\x00:\\xe4\\xc0Q', b'\\x82Q\\xd7)\\xecI\\x12\\x01\\xbc]\\xd3\\xd1', b'f+k\\xf6\\x03\\x89\\xa4\\x01\\x98\\x9b@\\xea', b'\\x15\\x88\\xad2\\xac6\\xd5\\x11\\xf1\\xee\\x98\\x0b', b'\\xe7\\xd2\\x96\\xdf/\\x00\\x00\\x008F\\x0bgL\\xb8N\\xe3', b'\\xec\\x13\\xd9X\\x183\\xe16Q\\xfa\\xc8v', b'\\xa9\\x184=\\xdd6\\xbb\\x01j\\x91|N', b'y\\x9d2\\xe8\\xb1\\x0f\\xa0\\x1d\\xb1\\xf9}\\xce', b'_=8\\x96\\x88\\x11\\xdc1\\xa2\\r=x', b'\\xc7\\xdb\\xce\\xcb\\xd5\\x08O\\x02xJ\\xf0\\xa8', b\"\\xc4\\xaa\\x13\\t\\xa5\\x00'\\rB\\xee\\xf7\\xc7\", b\"\\x86\\xb4\\x1e'DG\\xed\\x01\\x00=)S\", b'\\x06\\x1f\\x90%\\xd4\\xa8\\xef\\x11\\xbc\\x16\\x07\\x01', b'!w\\x18\\xfb\\xb0s\\xb4\\x0b\\xa4\\xc7;\\xc8', b'\\xe3\\xcc\\xc21\\xa9\\xaaq\\x00\\xdf\\x8b\\xcb\\xa5', b'\\x02\\xb7zL\\x1c\\xda\\xe66\\x8f`\\xaa\\x7f', b'6\\x951\\x1d;m\\x1d\\x00\\xdd\\xa4\\xa4\\x92', b'\\x10\\xa0\\x8d\\xaf\\x0b3\\x03\\x00\\xc6#\\x96R', b'\\xfc\\xfau\\xdf\\xf68\\x01\\x00P\\x95\\x9e\\xee\\xd2\\xbf\\x8f\\xae', b'\\xff-\\xc8w\\xbakm\\x01q\\xf3\\xdea', b'}\\xb6\\xd1a/\\xb0R\\x00{\\x81k\\\\', b'\\xb4\\x1d\\xf3\\xec\\x951\\x19\\x00uQ\\xb5\\x97', b'\\x96\\xc4\\xdc:\\x9e\\xce\\xaeI9\\xe61\\xd0', b'I\\xf4\\x8aimt\\x88\\x00\\xdbZE\\xe7', b'.\\xe2\\xaa\\xd6\\xe4\\x99\\xda!\\xef*\\xf3\\xce', b'o\\x05@\\x85\\xcc\\x02\\x00\\x00\\xdf|/\\x14aH\\xa9\\xd1', b'\\xd3\\xd1t\\x1c%\\x00\\x00\\x00\\x86fXS\\xe2\\xb6\\xf9 ', b\"\\t\\xb1.-\\xa23\\x00\\x00\\x9f\\x01'{\\xd6\\x07{;\", b'\\x907\\x02\\xcd\\r\\x00\\x00\\x00\\xa2\\x01\\x0c|\\xea\\xb1\\xc8\\xd1', b'\\x04\\t\\xc7LT\\xc3\\x99\\tm\\r\\x990', b'z9\\xbf\\xb1_a\\xb7\\x01\\x1a\\xb8\\x91t', b'|\\xcc <\\xf2\\xc5 K\\xf0\\x04\\x85\\x1b', b'\\xad\\x1b\\xe0i\\xa4\\xa0w\\x03;\\xf4~L', b'i\\xb0\\x17K\\x0f\\x00\\x00\\x00\\x8d_\\xf17x\\xe1\\x1e\\xe5', b'\\x99\\xb3\\x0f\\xc6\\x8f\\xe3}\\x00\\x06\\xccC\\xc9', b't\\xf1\\xae\\xee\\x00\\x00\\x00\\x00\\xb4n\\xbe\\x88\\xa2\\x05H\\x0c', b'\\xa7\\xd9\\xd6\\xa9\\xc5$K\\x07\\x07\\xb9G2', b'S?\\x11^\\xeb\\xa7\\xc5\\x02\\xd8\\xa3N%', b\"\\xd07\\x964=\\xb4`\\x05\\xa1+\\x1a'\", b'\\xfa\\x1d\\xf3\\xc3H\\xc6\\xa4\\x00{\\x97Y#', b'@\\x02\\x1bU\\xd2\\t\\x01\\x00VT\\xb4%\\xd6\\xfa\\xeb\\x08', b'\\xc2\\xcd\\x1bN\\xdf\\xf1\\x06\\x00Mu\\x80\\xc2', b'\\x8ar\\xaf\\x9b\\r\\x00\\x00\\x00\\tKd:5x\\x0f\\xcc', b'\\x19\\x92\\x16W \\xbcs\\x01|qW\\x84', b'\\xea)#\\xde5\\x00\\x00\\x00\\xc6d\\xbf\\xfd\\xbf\\xc7\\xc9\\t', b'E>\\xaa[uKP\\x03\\xef\\x03\\xb5\\xf3', b'p\\xbf\\xac\\xcd\\x02\\x00\\x00\\x00\\xec\\x16?\\x8dz\\xcb\\xf6\\n', b'\\xf1C\\x13n\\xe3\\x96\\xc7\\x0e\\x01\\xcc\\x06\\xbc', b'\\xde\\xcc\\xf0+\\x1d\\xbf\\xf6\\x0c\\xae\\x17m,', b\"\\xa1I`n\\xdb\\x81z\\x00\\xbe'\\xca\\xa4\", b'\\xa5x\\xe7v\\x16\\x81\\xa5\"W\\x7f_\\x0b', b'\\xe5I\\x13\\x90[\\xe1\\x90\\x00\\xa3\\x12\\x19B', b'r\\x1a\\xd2\\x96GT\\x89\\x01^\\xd6;\\xda', b'WPc.\\xc5\\xa9c\\x04\\xc5\\xe2\\xffi', b'.\\xe5\\xe6i\\xb1Yo\\x03\\xdd\\xbf\\xf0\\x90', b':\\xb9\\xf3\\xc6jP\\xeb]\\xee\\x98\\x00%', b'\\xa3_\\xf8x*\\x08A\\x06\\x00\\xc7l\\xa0', b')*<g\\x08c\\xea\\x00\\xce9]I', b\"[(\\xb8\\xb0Bg'7\\x14L\\xd5d\", b'}z\\to,B\\x16].\\x8cB\\x03', b'\\xe8\\xab\\xe5\\xba_\\xa6?\\x00\\x84O\\xbd\\x10', b' 4\\xb6R\\x01\\x00\\x00\\x00C\\x05\\xe9\\xc7G\\xcf3\\xbc', b'\\xdb~\\xacS5\\xd4\\x0c\\x00`1\\x0f\\xc9', b'\\xe6\\xe6\\xdd\\xb5\\x90+\\xb5\\x08\\xddg\\xdb\\x95', b'\\xef\\x006\\xb8\\x00\\x00\\x00\\x00\\xcc\\x81+\\xe1\\x07\\x83dq', b'\\x97\\x19\\x9d5\\x01\\x00\\x00\\x00:\\xdb\\xd0\\xb7yb\\xe4\\xc7', b'\\xa3\\xd4\\x8f\\xc3\\xdf\\x0bK\\x00\\xc2\\xe0\\x08\\x0f', b'O*G\\xd5\\xc0a8\\x01p\\xac\\xaf\\xa0', b'/\\x93\\x14\\x1ceN\\xfe\\x00\\xef\\xc1\\xe6\\x81', b'\\x01\\xf5\\x1f}f+E\\x1d]?IR', b\"'R\\xe7\\xfc\\xea\\xf8\\x14\\x00\\xfcN)L\", b'\\x95\\xa1\\xee\\xdeYk9\\x01\\xa3?\\xdfr', b'n\\x84n\\x7f59b\\x01C\\x89\\x04\\x8e', b'\\x9d\\x97f5\\x12\\x00\\x00\\x00\"\\xd4\\x84a_\\xfb%\\x95', b'\\x18\\xe1E\\xcb\\xabg\\x94\\x08 \\xbc\\xca\\xec'], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 46, 83, 148, 266, 485, 894, 1649, 2994, 5252, 8766, 13812, 20475, 28460, 36971, 44953, 51610, 56656, 60179, 62449, 63803, 64564, 64977, 65200, 65322, 65390, 65429, 65452, 65453, 65454, 65455, 65456, 65457, 65458, 65459, 65460, 65461, 65462, 65463, 65464, 65465, 65466, 65467, 65468, 65469, 65470, 65471, 65472, 65473, 65474, 65475, 65476, 65477, 65478, 65479, 65480, 65481, 65482, 65483, 65484, 65485, 65486, 65487, 65488, 65489, 65490, 65491, 65492, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 64, 103, 171, 293, 517, 931, 1689, 3033, 5284, 8793, 13843, 20515, 28502, 37014, 45009, 51678, 56723, 60238, 62500, 63848, 64605, 65016, 65238, 65360, 65429, 65469, 65493, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 52, 80, 126, 204, 338, 573, 992, 1739, 3049, 5248, 8700, 13714, 20413, 28498, 37101, 45124, 51789, 56818, 60299, 62522, 63847, 64597, 65011, 65238, 65364, 65435, 65476, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 48, 73, 115, 188, 316, 545, 960, 1707, 3015, 5201, 8633, 13641, 20346, 28423, 37003, 45006, 51667, 56717, 60236, 62494, 63839, 64597, 65012, 65238, 65362, 65432, 65472, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 70, 113, 187, 317, 549, 968, 1721, 3039, 5241, 8694, 13723, 20438, 28508, 37082, 45095, 51762, 56798, 60296, 62539, 63874, 64623, 65030, 65249, 65368, 65434, 65472, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 62, 105, 179, 308, 537, 951, 1698, 3009, 5201, 8631, 13615, 20288, 28371, 37017, 45103, 51798, 56811, 60261, 62465, 63786, 64541, 64960, 65190, 65318, 65390, 65432, 65456, 65457, 65458, 65459, 65460, 65461, 65462, 65463, 65464, 65465, 65466, 65467, 65468, 65469, 65470, 65471, 65472, 65473, 65474, 65475, 65476, 65477, 65478, 65479, 65480, 65481, 65482, 65483, 65484, 65485, 65486, 65487, 65488, 65489, 65490, 65491, 65492, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 65, 104, 171, 291, 511, 920, 1674, 3013, 5258, 8760, 13810, 20500, 28527, 37083, 45101, 51771, 56805, 60302, 62542, 63872, 64618, 65023, 65242, 65362, 65430, 65469, 65493, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 97, 164, 285, 507, 920, 1678, 3022, 5277, 8797, 13861, 20534, 28503, 36988, 44958, 51611, 56651, 60171, 62448, 63815, 64588, 65008, 65235, 65359, 65429, 65469, 65493, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 62, 104, 175, 301, 530, 949, 1707, 3037, 5253, 8718, 13749, 20462, 28554, 37190, 45258, 51916, 56887, 60311, 62502, 63813, 64559, 64973, 65204, 65335, 65411, 65456, 65483, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 65, 109, 184, 315, 547, 962, 1704, 3002, 5178, 8603, 13601, 20294, 28378, 36996, 45033, 51691, 56712, 60209, 62462, 63808, 64568, 64985, 65214, 65342, 65415, 65458, 65484, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 62, 101, 169, 291, 515, 931, 1697, 3057, 5330, 8854, 13908, 20582, 28573, 37072, 45038, 51691, 56735, 60247, 62503, 63849, 64607, 65020, 65243, 65365, 65433, 65472, 65495, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 63, 106, 179, 307, 536, 950, 1695, 2998, 5172, 8576, 13541, 20218, 28336, 37027, 45124, 51788, 56770, 60215, 62427, 63754, 64511, 64932, 65165, 65296, 65372, 65417, 65444, 65445, 65446, 65447, 65448, 65449, 65450, 65451, 65452, 65453, 65454, 65455, 65456, 65457, 65458, 65459, 65460, 65461, 65462, 65463, 65464, 65465, 65466, 65467, 65468, 65469, 65470, 65471, 65472, 65473, 65474, 65475, 65476, 65477, 65478, 65479, 65480, 65481, 65482, 65483, 65484, 65485, 65486, 65487, 65488, 65489, 65490, 65536], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 66, 107, 177, 301, 526, 941, 1702, 3052, 5313, 8827, 13869, 20529, 28519, 37043, 45038, 51698, 56736, 60247, 62505, 63851, 64607, 65018, 65240, 65361, 65429, 65468, 65492, 65493, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 49, 76, 121, 196, 327, 559, 976, 1724, 3033, 5223, 8666, 13691, 20412, 28499, 37089, 45107, 51770, 56800, 60291, 62526, 63857, 64606, 65015, 65237, 65359, 65428, 65468, 65492, 65493, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 46, 71, 113, 186, 315, 547, 970, 1732, 3065, 5284, 8752, 13793, 20523, 28613, 37201, 45209, 51859, 56879, 60361, 62584, 63900, 64635, 65034, 65249, 65367, 65433, 65471, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 42, 66, 106, 176, 299, 522, 931, 1681, 3020, 5275, 8784, 13812, 20456, 28440, 36958, 44957, 51647, 56720, 60248, 62510, 63858, 64616, 65028, 65249, 65369, 65436, 65475, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [109, 113, 100, 108, 109, 101, 113, 102, 103, 101, 107, 121, 116, 116, 111, 102], [-53, -55, -49, -53, -53, -49, -55, -49, -50, -49, -52, -59, -57, -57, -54, -49]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m comp_model\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m comp_model\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 37\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(test_dataset, model, criterion)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Compression and decompression\u001b[39;00m\n\u001b[1;32m     36\u001b[0m out_enc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompress(batch)\n\u001b[0;32m---> 37\u001b[0m out_dec \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Calculate BPP and MSE for the batch\u001b[39;00m\n\u001b[1;32m     40\u001b[0m num_pixels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_block\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_block\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Per image\u001b[39;00m\n",
      "File \u001b[0;32m~/Weight_compression/VQVAE/models/nwc.py:150\u001b[0m, in \u001b[0;36mSimpleVAECompressionModel.decompress\u001b[0;34m(self, strings, shape, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecompress\u001b[39m(\u001b[38;5;28mself\u001b[39m, strings: List[List[\u001b[38;5;28mbytes\u001b[39m]], shape, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 150\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentropy_bottleneck\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# x_hat = self.g_s(y_hat).clamp_(0, 1)\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_s(y_hat)\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/entropy_models/entropy_models.py:573\u001b[0m, in \u001b[0;36mEntropyBottleneck.decompress\u001b[0;34m(self, strings, size)\u001b[0m\n\u001b[1;32m    571\u001b[0m medians \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_ndims(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_medians()\u001b[38;5;241m.\u001b[39mdetach(), \u001b[38;5;28mlen\u001b[39m(size))\n\u001b[1;32m    572\u001b[0m medians \u001b[38;5;241m=\u001b[39m medians\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mlen\u001b[39m(strings), \u001b[38;5;241m*\u001b[39m([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m--> 573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmedians\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmedians\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/entropy_models/entropy_models.py:314\u001b[0m, in \u001b[0;36mEntropyModel.decompress\u001b[0;34m(self, strings, indexes, dtype, means)\u001b[0m\n\u001b[1;32m    311\u001b[0m outputs \u001b[38;5;241m=\u001b[39m cdf\u001b[38;5;241m.\u001b[39mnew_empty(indexes\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(strings):\n\u001b[0;32m--> 314\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentropy_coder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_with_indexes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cdf_length\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     outputs[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    322\u001b[0m         values, device\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    323\u001b[0m     )\u001b[38;5;241m.\u001b[39mreshape(outputs[i]\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m    324\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequantize(outputs, means, dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/nic/lib/python3.10/site-packages/compressai/entropy_models/entropy_models.py:80\u001b[0m, in \u001b[0;36m_EntropyCoder.decode_with_indexes\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_with_indexes\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_with_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: decode_with_indexes(): incompatible function arguments. The following argument types are supported:\n    1. (self: compressai.ans.RansDecoder, arg0: str, arg1: list[int], arg2: list[list[int]], arg3: list[int], arg4: list[int]) -> list[int]\n\nInvoked with: <compressai.ans.RansDecoder object at 0x7fe231bc7670>, [b'\\x8b\\xbaths*\\x04\\x00\\xebE\\x13W', b'\\xb0uj\\xd1\\xb5k\\xf7\\x01W=\\xa4\\xf9', b'\\xd1\\xc5\\xe5\\x13\\xb7\\xc6\\xca\\x00Z\\xb6,\\xfb', b'-\\xc6?rq\\xa6\\x104\\xdf>s\\x9e', b'\\xb5\\xbc\\x1f\\x99\\xdb\\x0b\\x19\\x11a\\x83\\xc4\\x07', b' \\xc9 Q\\xd7\\xcc\\x18\\x002\\xddo\\x06', b'\\xf7Mx\\xe7\\x00\\x00\\x00\\x00P\\xc8 V\\x04\\x03y\\x12', b'\\xc6Ug+%\\xf4\\xd3\\x07?\\xc3\\xfe\\xe6', b'B\\xec\\xe5\\xaf\\x05\\x00\\x00\\x00\\xcc\\x7f\\x88\\xe5^\\x19\\xf2\\xe1', b'd[V\\x87\\xe3M\\x02\\x00\\xdd7\\xe2X', b'~\\x92\\x02\\xb0N\\x88<\\x00LT\\x05\\xc9', b'l\\xde\\x9c\\x86\\x10\\x00\\x00\\x00\\xcfN\\x15:\\x0b\\x1b\\xf2<', b'\\x074ror\\xca\\t\\x00\\x8eHuU', b'\\xd6L!\\xef\\xc6j\\x1a\\x00\\x86\\x8a\\x16g', b\"v@\\xc2\\x12\\xcb\\x867\\x01'\\xc0W\\xfc\", b\"\\xd1/\\xce\\x96\\xd7\\xcb'p\\xfdyE\\xed\", b'\\x0e\\xce\\x7f\\x0c\\x85tr\\x00\\x81V\\xaf\\xbb', b'\\xc8\\x92\\xe1C/W+\\x00^\\xbb[]', b'\\xd3\\x9cJ\\x1a\\xb1^y\\x00s\\xdc|\\x1d', b'Q\\xc8\\xd6I7\\x00\\x00\\x00A\"\\x94\\x94\\x18x~\\xcc', b'\\xb0\\xbd89\\xf8\\x8a7\\x02J\\xb5\\xab\\xd0', b'\\xa1\\xacW\\xe3\\xa2\\xeb\\r\\x00\\xfe#\\x8b\\x03', b'\\x94\\x1df\\xc1$\"d\\x00\\x1a\\xec\"\\xee', b'\\x9b\\x1d\\xcb\\x05\\xd27\\xca\\x01\\xa4|UZ', b'\\xd5\\x17\\xc7\\x91\\x01\\x00\\x00\\x00U\\xc9^\\x03\\xa3\\x9f\\xc8\\xfa', b'\\xceO\\xf7\\x98k\\x1d]\\x00:\\xe4\\xc0Q', b'\\x82Q\\xd7)\\xecI\\x12\\x01\\xbc]\\xd3\\xd1', b'f+k\\xf6\\x03\\x89\\xa4\\x01\\x98\\x9b@\\xea', b'\\x15\\x88\\xad2\\xac6\\xd5\\x11\\xf1\\xee\\x98\\x0b', b'\\xe7\\xd2\\x96\\xdf/\\x00\\x00\\x008F\\x0bgL\\xb8N\\xe3', b'\\xec\\x13\\xd9X\\x183\\xe16Q\\xfa\\xc8v', b'\\xa9\\x184=\\xdd6\\xbb\\x01j\\x91|N', b'y\\x9d2\\xe8\\xb1\\x0f\\xa0\\x1d\\xb1\\xf9}\\xce', b'_=8\\x96\\x88\\x11\\xdc1\\xa2\\r=x', b'\\xc7\\xdb\\xce\\xcb\\xd5\\x08O\\x02xJ\\xf0\\xa8', b\"\\xc4\\xaa\\x13\\t\\xa5\\x00'\\rB\\xee\\xf7\\xc7\", b\"\\x86\\xb4\\x1e'DG\\xed\\x01\\x00=)S\", b'\\x06\\x1f\\x90%\\xd4\\xa8\\xef\\x11\\xbc\\x16\\x07\\x01', b'!w\\x18\\xfb\\xb0s\\xb4\\x0b\\xa4\\xc7;\\xc8', b'\\xe3\\xcc\\xc21\\xa9\\xaaq\\x00\\xdf\\x8b\\xcb\\xa5', b'\\x02\\xb7zL\\x1c\\xda\\xe66\\x8f`\\xaa\\x7f', b'6\\x951\\x1d;m\\x1d\\x00\\xdd\\xa4\\xa4\\x92', b'\\x10\\xa0\\x8d\\xaf\\x0b3\\x03\\x00\\xc6#\\x96R', b'\\xfc\\xfau\\xdf\\xf68\\x01\\x00P\\x95\\x9e\\xee\\xd2\\xbf\\x8f\\xae', b'\\xff-\\xc8w\\xbakm\\x01q\\xf3\\xdea', b'}\\xb6\\xd1a/\\xb0R\\x00{\\x81k\\\\', b'\\xb4\\x1d\\xf3\\xec\\x951\\x19\\x00uQ\\xb5\\x97', b'\\x96\\xc4\\xdc:\\x9e\\xce\\xaeI9\\xe61\\xd0', b'I\\xf4\\x8aimt\\x88\\x00\\xdbZE\\xe7', b'.\\xe2\\xaa\\xd6\\xe4\\x99\\xda!\\xef*\\xf3\\xce', b'o\\x05@\\x85\\xcc\\x02\\x00\\x00\\xdf|/\\x14aH\\xa9\\xd1', b'\\xd3\\xd1t\\x1c%\\x00\\x00\\x00\\x86fXS\\xe2\\xb6\\xf9 ', b\"\\t\\xb1.-\\xa23\\x00\\x00\\x9f\\x01'{\\xd6\\x07{;\", b'\\x907\\x02\\xcd\\r\\x00\\x00\\x00\\xa2\\x01\\x0c|\\xea\\xb1\\xc8\\xd1', b'\\x04\\t\\xc7LT\\xc3\\x99\\tm\\r\\x990', b'z9\\xbf\\xb1_a\\xb7\\x01\\x1a\\xb8\\x91t', b'|\\xcc <\\xf2\\xc5 K\\xf0\\x04\\x85\\x1b', b'\\xad\\x1b\\xe0i\\xa4\\xa0w\\x03;\\xf4~L', b'i\\xb0\\x17K\\x0f\\x00\\x00\\x00\\x8d_\\xf17x\\xe1\\x1e\\xe5', b'\\x99\\xb3\\x0f\\xc6\\x8f\\xe3}\\x00\\x06\\xccC\\xc9', b't\\xf1\\xae\\xee\\x00\\x00\\x00\\x00\\xb4n\\xbe\\x88\\xa2\\x05H\\x0c', b'\\xa7\\xd9\\xd6\\xa9\\xc5$K\\x07\\x07\\xb9G2', b'S?\\x11^\\xeb\\xa7\\xc5\\x02\\xd8\\xa3N%', b\"\\xd07\\x964=\\xb4`\\x05\\xa1+\\x1a'\", b'\\xfa\\x1d\\xf3\\xc3H\\xc6\\xa4\\x00{\\x97Y#', b'@\\x02\\x1bU\\xd2\\t\\x01\\x00VT\\xb4%\\xd6\\xfa\\xeb\\x08', b'\\xc2\\xcd\\x1bN\\xdf\\xf1\\x06\\x00Mu\\x80\\xc2', b'\\x8ar\\xaf\\x9b\\r\\x00\\x00\\x00\\tKd:5x\\x0f\\xcc', b'\\x19\\x92\\x16W \\xbcs\\x01|qW\\x84', b'\\xea)#\\xde5\\x00\\x00\\x00\\xc6d\\xbf\\xfd\\xbf\\xc7\\xc9\\t', b'E>\\xaa[uKP\\x03\\xef\\x03\\xb5\\xf3', b'p\\xbf\\xac\\xcd\\x02\\x00\\x00\\x00\\xec\\x16?\\x8dz\\xcb\\xf6\\n', b'\\xf1C\\x13n\\xe3\\x96\\xc7\\x0e\\x01\\xcc\\x06\\xbc', b'\\xde\\xcc\\xf0+\\x1d\\xbf\\xf6\\x0c\\xae\\x17m,', b\"\\xa1I`n\\xdb\\x81z\\x00\\xbe'\\xca\\xa4\", b'\\xa5x\\xe7v\\x16\\x81\\xa5\"W\\x7f_\\x0b', b'\\xe5I\\x13\\x90[\\xe1\\x90\\x00\\xa3\\x12\\x19B', b'r\\x1a\\xd2\\x96GT\\x89\\x01^\\xd6;\\xda', b'WPc.\\xc5\\xa9c\\x04\\xc5\\xe2\\xffi', b'.\\xe5\\xe6i\\xb1Yo\\x03\\xdd\\xbf\\xf0\\x90', b':\\xb9\\xf3\\xc6jP\\xeb]\\xee\\x98\\x00%', b'\\xa3_\\xf8x*\\x08A\\x06\\x00\\xc7l\\xa0', b')*<g\\x08c\\xea\\x00\\xce9]I', b\"[(\\xb8\\xb0Bg'7\\x14L\\xd5d\", b'}z\\to,B\\x16].\\x8cB\\x03', b'\\xe8\\xab\\xe5\\xba_\\xa6?\\x00\\x84O\\xbd\\x10', b' 4\\xb6R\\x01\\x00\\x00\\x00C\\x05\\xe9\\xc7G\\xcf3\\xbc', b'\\xdb~\\xacS5\\xd4\\x0c\\x00`1\\x0f\\xc9', b'\\xe6\\xe6\\xdd\\xb5\\x90+\\xb5\\x08\\xddg\\xdb\\x95', b'\\xef\\x006\\xb8\\x00\\x00\\x00\\x00\\xcc\\x81+\\xe1\\x07\\x83dq', b'\\x97\\x19\\x9d5\\x01\\x00\\x00\\x00:\\xdb\\xd0\\xb7yb\\xe4\\xc7', b'\\xa3\\xd4\\x8f\\xc3\\xdf\\x0bK\\x00\\xc2\\xe0\\x08\\x0f', b'O*G\\xd5\\xc0a8\\x01p\\xac\\xaf\\xa0', b'/\\x93\\x14\\x1ceN\\xfe\\x00\\xef\\xc1\\xe6\\x81', b'\\x01\\xf5\\x1f}f+E\\x1d]?IR', b\"'R\\xe7\\xfc\\xea\\xf8\\x14\\x00\\xfcN)L\", b'\\x95\\xa1\\xee\\xdeYk9\\x01\\xa3?\\xdfr', b'n\\x84n\\x7f59b\\x01C\\x89\\x04\\x8e', b'\\x9d\\x97f5\\x12\\x00\\x00\\x00\"\\xd4\\x84a_\\xfb%\\x95', b'\\x18\\xe1E\\xcb\\xabg\\x94\\x08 \\xbc\\xca\\xec'], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 46, 83, 148, 266, 485, 894, 1649, 2994, 5252, 8766, 13812, 20475, 28460, 36971, 44953, 51610, 56656, 60179, 62449, 63803, 64564, 64977, 65200, 65322, 65390, 65429, 65452, 65453, 65454, 65455, 65456, 65457, 65458, 65459, 65460, 65461, 65462, 65463, 65464, 65465, 65466, 65467, 65468, 65469, 65470, 65471, 65472, 65473, 65474, 65475, 65476, 65477, 65478, 65479, 65480, 65481, 65482, 65483, 65484, 65485, 65486, 65487, 65488, 65489, 65490, 65491, 65492, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 64, 103, 171, 293, 517, 931, 1689, 3033, 5284, 8793, 13843, 20515, 28502, 37014, 45009, 51678, 56723, 60238, 62500, 63848, 64605, 65016, 65238, 65360, 65429, 65469, 65493, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 52, 80, 126, 204, 338, 573, 992, 1739, 3049, 5248, 8700, 13714, 20413, 28498, 37101, 45124, 51789, 56818, 60299, 62522, 63847, 64597, 65011, 65238, 65364, 65435, 65476, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 48, 73, 115, 188, 316, 545, 960, 1707, 3015, 5201, 8633, 13641, 20346, 28423, 37003, 45006, 51667, 56717, 60236, 62494, 63839, 64597, 65012, 65238, 65362, 65432, 65472, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 44, 70, 113, 187, 317, 549, 968, 1721, 3039, 5241, 8694, 13723, 20438, 28508, 37082, 45095, 51762, 56798, 60296, 62539, 63874, 64623, 65030, 65249, 65368, 65434, 65472, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 62, 105, 179, 308, 537, 951, 1698, 3009, 5201, 8631, 13615, 20288, 28371, 37017, 45103, 51798, 56811, 60261, 62465, 63786, 64541, 64960, 65190, 65318, 65390, 65432, 65456, 65457, 65458, 65459, 65460, 65461, 65462, 65463, 65464, 65465, 65466, 65467, 65468, 65469, 65470, 65471, 65472, 65473, 65474, 65475, 65476, 65477, 65478, 65479, 65480, 65481, 65482, 65483, 65484, 65485, 65486, 65487, 65488, 65489, 65490, 65491, 65492, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 65, 104, 171, 291, 511, 920, 1674, 3013, 5258, 8760, 13810, 20500, 28527, 37083, 45101, 51771, 56805, 60302, 62542, 63872, 64618, 65023, 65242, 65362, 65430, 65469, 65493, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 59, 97, 164, 285, 507, 920, 1678, 3022, 5277, 8797, 13861, 20534, 28503, 36988, 44958, 51611, 56651, 60171, 62448, 63815, 64588, 65008, 65235, 65359, 65429, 65469, 65493, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 62, 104, 175, 301, 530, 949, 1707, 3037, 5253, 8718, 13749, 20462, 28554, 37190, 45258, 51916, 56887, 60311, 62502, 63813, 64559, 64973, 65204, 65335, 65411, 65456, 65483, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 38, 65, 109, 184, 315, 547, 962, 1704, 3002, 5178, 8603, 13601, 20294, 28378, 36996, 45033, 51691, 56712, 60209, 62462, 63808, 64568, 64985, 65214, 65342, 65415, 65458, 65484, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 62, 101, 169, 291, 515, 931, 1697, 3057, 5330, 8854, 13908, 20582, 28573, 37072, 45038, 51691, 56735, 60247, 62503, 63849, 64607, 65020, 65243, 65365, 65433, 65472, 65495, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 63, 106, 179, 307, 536, 950, 1695, 2998, 5172, 8576, 13541, 20218, 28336, 37027, 45124, 51788, 56770, 60215, 62427, 63754, 64511, 64932, 65165, 65296, 65372, 65417, 65444, 65445, 65446, 65447, 65448, 65449, 65450, 65451, 65452, 65453, 65454, 65455, 65456, 65457, 65458, 65459, 65460, 65461, 65462, 65463, 65464, 65465, 65466, 65467, 65468, 65469, 65470, 65471, 65472, 65473, 65474, 65475, 65476, 65477, 65478, 65479, 65480, 65481, 65482, 65483, 65484, 65485, 65486, 65487, 65488, 65489, 65490, 65536], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 66, 107, 177, 301, 526, 941, 1702, 3052, 5313, 8827, 13869, 20529, 28519, 37043, 45038, 51698, 56736, 60247, 62505, 63851, 64607, 65018, 65240, 65361, 65429, 65468, 65492, 65493, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 49, 76, 121, 196, 327, 559, 976, 1724, 3033, 5223, 8666, 13691, 20412, 28499, 37089, 45107, 51770, 56800, 60291, 62526, 63857, 64606, 65015, 65237, 65359, 65428, 65468, 65492, 65493, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 46, 71, 113, 186, 315, 547, 970, 1732, 3065, 5284, 8752, 13793, 20523, 28613, 37201, 45209, 51859, 56879, 60361, 62584, 63900, 64635, 65034, 65249, 65367, 65433, 65471, 65494, 65495, 65496, 65497, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 42, 66, 106, 176, 299, 522, 931, 1681, 3020, 5275, 8784, 13812, 20456, 28440, 36958, 44957, 51647, 56720, 60248, 62510, 63858, 64616, 65028, 65249, 65369, 65436, 65475, 65498, 65499, 65500, 65501, 65502, 65503, 65504, 65505, 65506, 65507, 65508, 65509, 65510, 65511, 65512, 65513, 65514, 65515, 65516, 65517, 65518, 65519, 65520, 65521, 65522, 65523, 65524, 65525, 65526, 65527, 65528, 65529, 65530, 65531, 65532, 65533, 65534, 65535, 65536, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], [109, 113, 100, 108, 109, 101, 113, 102, 103, 101, 107, 121, 116, 116, 111, 102], [-53, -55, -49, -53, -53, -49, -55, -49, -50, -49, -52, -59, -57, -57, -54, -49]"
     ]
    }
   ],
   "source": [
    "comp_model = comp_model.eval().to(device)\n",
    "comp_model.requires_grad_(False)\n",
    "comp_model.update()\n",
    "\n",
    "result = test(test_dataset, comp_model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randperm(len(train_dataset))  # 데이터셋 길이만큼 무작위 인덱스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = {}\n",
    "for ql in range(4):\n",
    "    data_ = []\n",
    "    n = 0\n",
    "    for i in indices[:1000000]:\n",
    "        data = train_dataset[i]\n",
    "        if data['q_level'] == ql:\n",
    "            data_.append(data)\n",
    "            n += 1\n",
    "            if n == 500: break\n",
    "    print(n)\n",
    "    datas[ql] = data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = {}\n",
    "for ql in range(4):\n",
    "    out_ = []\n",
    "    \n",
    "    for data in datas[ql]:\n",
    "        data = {key: tensor.unsqueeze(0).to(device) for key, tensor in data.items()}\n",
    "\n",
    "        out_net = comp_model(data)\n",
    "        out_.append(out_net)\n",
    "    outs[ql] = out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = {}\n",
    "std = 0.012528747320175171\n",
    "mse_fn = nn.MSELoss()\n",
    "for ql in range(4):\n",
    "    mse_ = []\n",
    "    for out in outs[ql]:\n",
    "        mse_.append(mse_fn(out['x'], out['x_hat']).item())\n",
    "    \n",
    "    mses[ql] = np.array(mse_).mean() / std ** 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ql in range(4):\n",
    "    print(ql, mses[ql])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 0.02060998404537972\n",
    "# 1 0.010955656458541706\n",
    "# 2 0.0018706787592914606\n",
    "# 3 0.0010826996227265911"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 0.02421522901695616\n",
    "1 0.01249026141716976\n",
    "2 0.003785866645109464\n",
    "3 0.00438829171409325"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 0.03074731674599537\n",
    "1 0.020813048131938707\n",
    "2 0.012604427266820322\n",
    "3 0.05009694816633827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = []\n",
    "y_hat_list = []\n",
    "\n",
    "for idx, data in enumerate(test_dataset):\n",
    "    data = {key: tensor.unsqueeze(0).to(device) for key, tensor in data.items()}\n",
    "    \n",
    "    out_net = comp_model(data)\n",
    "    # print(out_net)\n",
    "    y_list.append(out_net[\"y\"].detach().cpu())\n",
    "    y_hat_list.append(out_net[\"y_hat\"].detach().cpu())\n",
    "    \n",
    "y_list = torch.cat(y_list, dim = 0)\n",
    "y_hat_list = torch.cat(y_hat_list, dim = 0)\n",
    "print(y_list.shape)\n",
    "print(y_hat_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE(n_components=2, perplexity=100, random_state=42)  # perplexity는 1 이상이어야 함\n",
    "# vectors_embedded = tsne.fit_transform(y_list)\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "\n",
    "# for ql in range(4):\n",
    "#     plt.scatter(vectors_embedded[test_dataset['q_level'] == ql, 0], vectors_embedded[test_dataset['q_level'] == ql, 1], label=f'ql {ql}')\n",
    "\n",
    "# plt.title(\"t-SNE Visualization of 5 Vectors\")\n",
    "# plt.xlabel(\"t-SNE Component 1\")\n",
    "# plt.ylabel(\"t-SNE Component 2\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ql_list == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
