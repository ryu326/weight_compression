{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93a367b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/RD/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "def contains_all_substrings(string, substrings):\n",
    "    return all(substring in string for substring in substrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7680f",
   "metadata": {},
   "source": [
    "## Weight Param 이미지 size로 만들기\n",
    "JPEG, image pretrained nic model에 사용\n",
    "\n",
    "path 파일로 split해서 사용하자 그냥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f466af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### /home/jgryu/Weight_compression/Wparam_dataset/path_json/llama-2-7b_mlp_train.json ###\n",
      "## 64 ##\n",
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:54<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836608, 64, 64)\n",
      "(64, 64) (64, 64) -4.6755717e-06 0.018473955\n",
      "## 256 ##\n",
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:13<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52288, 256, 256)\n",
      "(256, 256) (256, 256) -4.6755717e-06 0.018473955\n",
      "## 512 ##\n",
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [00:09<00:00,  7.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13072, 512, 512)\n",
      "(512, 512) (512, 512) -4.6755717e-06 0.018473955\n",
      "## 1024 ##\n",
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [01:08<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3268, 1024, 1024)\n",
      "(1024, 1024) (1024, 1024) -4.6755717e-06 0.018473955\n",
      "### /home/jgryu/Weight_compression/Wparam_dataset/path_json/llama-2-7b_attn_train.json ###\n",
      "## 64 ##\n",
      "102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:22<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417792, 64, 64)\n",
      "(64, 64) (64, 64) 3.2762267e-07 0.02040687\n",
      "## 256 ##\n",
      "102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:04<00:00, 21.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26112, 256, 256)\n",
      "(256, 256) (256, 256) 3.2762267e-07 0.02040687\n",
      "## 512 ##\n",
      "102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:04<00:00, 23.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6528, 512, 512)\n",
      "(512, 512) (512, 512) 3.2762267e-07 0.02040687\n",
      "## 1024 ##\n",
      "102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:04<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1632, 1024, 1024)\n",
      "(1024, 1024) (1024, 1024) 3.2762267e-07 0.02040687\n"
     ]
    }
   ],
   "source": [
    "# json_path1 = '/home/jgryu/Weight_compression/Wparam_dataset/path_json/meta-llama-3-8b_mlp_train.json'\n",
    "# json_path2 = '/home/jgryu/Weight_compression/Wparam_dataset/path_json/meta-llama-3-8b_attn_train.json'\n",
    "json_path1 = '/home/jgryu/Weight_compression/Wparam_dataset/path_json/llama-2-7b_mlp_train.json'\n",
    "json_path2 = '/home/jgryu/Weight_compression/Wparam_dataset/path_json/llama-2-7b_attn_train.json'\n",
    "save_dir_path = '/home/jgryu/Weight_compression/Wparam_dataset/image_shape_wp/'\n",
    "\n",
    "for json_path in [json_path1, json_path2]:\n",
    "    print(f'### {json_path} ###')\n",
    "    for d in [64, 256, 512, 1024]:\n",
    "    # for d in [256]:\n",
    "        print(f'## {d} ##')\n",
    "        dim = (d, d)\n",
    "        file_name = os.path.splitext(os.path.basename(json_path))[0]\n",
    "        file_name = file_name  + \"_json\"\n",
    "\n",
    "        save_path = os.path.join(save_dir_path, file_name, f'{dim[0]}_{dim[1]}')\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        with open(json_path, 'r') as f:\n",
    "            w_path = json.load(f)\n",
    "\n",
    "        print(len(w_path))\n",
    "\n",
    "        w_list = []\n",
    "        for wp in tqdm(w_path):\n",
    "            w  = np.load(wp)\n",
    "            w = w.reshape(-1, *dim)\n",
    "            \n",
    "            # \"/home/jgryu/Weight_compression/Wparam_dataset/model_param_tensor/meta-llama/Meta-Llama-3-8B/model-layers-3-mlp-down_proj-weight.npy\"\n",
    "            # 디렉토리의 마지막 부분과 파일명 추출\n",
    "            dir_name = os.path.basename(os.path.dirname(wp))  # \"Meta-Llama-3-8B\"\n",
    "            file_name = os.path.splitext(os.path.basename(wp))[0]  # \"model-layers-3-mlp-down_proj-weight\"    \n",
    "            path = os.path.join(save_path, f\"{dir_name}/{file_name}_npy\")\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            for i, wi in enumerate(w):\n",
    "                np.save(f'{path}/{i}', wi)\n",
    "            w_list.append(w)\n",
    "            \n",
    "\n",
    "        w_list = np.concatenate(w_list, axis = 0)\n",
    "        print(w_list.shape)\n",
    "\n",
    "\n",
    "        mean_vector = w_list.mean(axis=0)\n",
    "        mean_value = w_list.mean()\n",
    "        std_vector = w_list.std(axis=0)\n",
    "        std_value = w_list.std()\n",
    "        print(mean_vector.shape, std_vector.shape, mean_value, std_value)\n",
    "\n",
    "        filename = save_path\n",
    "        np.save(filename + '/mean_vector.npy', mean_vector)\n",
    "        np.save(filename + '/mean_value.npy', mean_value)\n",
    "        np.save(filename + '/std_vector.npy', std_vector)\n",
    "        np.save(filename + '/std_value.npy', std_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
