{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, socket, lpips, shutil, operator\n",
    "\n",
    "# 시간 측정해보기\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets_Imagenet_best_worst import Imagenet_best_worst\n",
    "from datasets_ImageNet import ImageNet_dataset\n",
    "from datasets_WeightParam import WParam_dataset\n",
    "# from datasets_openimages_v6 import Openimages_v6_dataset\n",
    "\n",
    "from pytorch_msssim import ms_ssim as ms_ssim_func\n",
    "\n",
    "from models.TCM import TCM\n",
    "from models.FTIC import FrequencyAwareTransFormer\n",
    "from models.ELIC import ELIC, model_config\n",
    "\n",
    "from utils.optimizers import *\n",
    "from utils.util import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, p):\n",
    "    h, w = x.size(2), x.size(3)\n",
    "    new_h = (h + p - 1) // p * p\n",
    "    new_w = (w + p - 1) // p * p\n",
    "    padding_left = (new_w - w) // 2\n",
    "    padding_right = new_w - w - padding_left\n",
    "    padding_top = (new_h - h) // 2\n",
    "    padding_bottom = new_h - h - padding_top\n",
    "    x_padded = F.pad(\n",
    "        x,\n",
    "        (padding_left, padding_right, padding_top, padding_bottom),\n",
    "        mode=\"constant\",\n",
    "        value=0,\n",
    "    )\n",
    "    return x_padded, (padding_left, padding_right, padding_top, padding_bottom)\n",
    "\n",
    "def crop(x, padding):\n",
    "    return F.pad(\n",
    "        x,\n",
    "        (-padding[0], -padding[1], -padding[2], -padding[3]),\n",
    "    )\n",
    "    \n",
    "def make_image_format(W, wp_mean, wp_std, normalize):\n",
    "    if normalize:\n",
    "        W = (W - wp_mean) / wp_std\n",
    "    W = W.unsqueeze(1).repeat(1, 3, 1, 1)\n",
    "    return W\n",
    "\n",
    "def reverse_image_format(W, wp_mean, wp_std, normalize):\n",
    "    # 이미지를 채널 축에서 3 -> 1로 줄이기\n",
    "    # W = W[:, 0, :, :]  # 첫 번째 채널만 유지\n",
    "    W = W.mean(1)  # 첫 번째 채널만 유지\n",
    "    # Normalize를 반대로 적용\n",
    "    if normalize:\n",
    "        W = W * wp_std + wp_mean\n",
    "    return W\n",
    "\n",
    "def reconstruct_model(state_dict, model, save_path, logger, size, weight_condition, mean, std, batch=4, normalize = True):\n",
    "    avg_bpp = 0.0\n",
    "    mean_MSE = 0\n",
    "    count = 0\n",
    "    mse_func = nn.MSELoss()\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    recon_state_dict = {}\n",
    "    \n",
    "    for k, W in state_dict.items():\n",
    "        if not weight_condition in k: continue\n",
    "        print(f'### Reconstructing {k} ####')\n",
    "        \n",
    "        W_reshaped = W.reshape(-1, size, size) # ( -1, -1) --> (-1, size, size)\n",
    "        W_reshaped = W_reshaped.to(device)\n",
    "        W_reshaped = make_image_format(W_reshaped, mean, std, normalize)  # (-1, size, size) --> (-1, 3, size, size)\n",
    "        \n",
    "        # try : \n",
    "        #     W_reshaped = W_reshaped.reshape(-1, batch, 3, size, size)  # (-1, 3, size, size) --> (-1, batch, 3, size, size)\n",
    "        # except:\n",
    "        #     W_reshaped = W_reshaped.reshape(-1, 1, 3, size, size)  # (-1, 3, size, size) --> (-1, 1, 3, size, size)\n",
    "            \n",
    "        W_reshaped = W_reshaped.reshape(-1, 1, 3, size, size)  # (-1, 3, size, size) --> (-1, 1, 3, size, size)\n",
    "        W_recon = torch.zeros(W_reshaped.shape, dtype=W_reshaped.dtype, device=W_reshaped.device)\n",
    "        \n",
    "        for idx, W_slice in tqdm(enumerate(W_reshaped)): # (bath, 3, size, size) in (-1, bath, 3, size, size)\n",
    "            # print(W_slice.shape)\n",
    "            count += 1\n",
    "            x = W_slice.to(device)  # (bach3, size, size) --> (1, 3, size, size)\n",
    "\n",
    "            try:\n",
    "                x_paddeimg, padding = pad(x, p = 128)\n",
    "                out_enc = model.compress(x_paddeimg.to(device))\n",
    "            except:\n",
    "                x_paddeimg, padding = pad(x, p = 256)\n",
    "                out_enc = model.compress(x_paddeimg.to(device))\n",
    "            \n",
    "            out_dec = model.decompress(out_enc[\"strings\"], out_enc[\"shape\"])\n",
    "            \n",
    "            num_pixels = x.size(0) * x.size(2) * x.size(3)\n",
    "            bpp = 0\n",
    "            for s in out_enc[\"strings\"]:\n",
    "                if s != [0]: #  \n",
    "                    bpp += len(s[0]) * 8.0 / num_pixels \n",
    "\n",
    "            x_hat = crop(out_dec[\"x_hat\"], padding).clone().detach() # (1, 3, size, size)\n",
    "            mse = mse_func(x, x_hat).item()\n",
    "            avg_bpp += bpp\n",
    "            mean_MSE += mse\n",
    "            \n",
    "            W_recon_slice = x_hat\n",
    "            W_recon[idx] = W_recon_slice\n",
    "            # logger.info(f\"File name: {idx}, MSE: {mse}, BPP: {bpp}\")\n",
    "\n",
    "        W_recon = W_recon.reshape(-1, 3, size, size).to('cpu')  # (-1, batch, 3, size, size) --> (-1, 3, size, size)\n",
    "        W_recon = reverse_image_format(W_recon, mean, std, normalize)  #  (-1, 3, size, size) --> (-1, size, size)\n",
    "        recon_state_dict[k] = W_recon\n",
    "        \n",
    "        \n",
    "    avg_bpp /= count\n",
    "    mean_MSE /= count  \n",
    "    # logger.info(f'Average_MSE: {mean_MSE}, Average_Bit-rate: {avg_bpp} bpp')\n",
    "\n",
    "    return recon_state_dict, avg_bpp, mean_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n",
      "/tmp/ipykernel_2232299/2922996308.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ck_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.025]\n",
      "##### lambda: 0.025 #####\n",
      "Checkpoint for 0.025 loaded successfully.\n",
      "Model state_dict loaded successfully for 0.025.\n",
      "### Reconstructing model.layers.0.mlp.gate_proj.weight ####\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 672.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 211.31 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.74 GiB is allocated by PyTorch, and 389.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     model\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m     model\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m---> 50\u001b[0m     recon_state_dict, avg_bpp, mean_MSE \u001b[38;5;241m=\u001b[39m \u001b[43mreconstruct_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_condition\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_condition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(avg_bpp, mean_MSE)\n\u001b[1;32m     55\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(recon_state_dict, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreconstruncted_state_dict/meta-llama--Meta-Llama-3-8B_mlp_d256_256.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m, in \u001b[0;36mreconstruct_model\u001b[0;34m(state_dict, model, save_path, logger, size, weight_condition, mean, std, batch, normalize)\u001b[0m\n\u001b[1;32m     52\u001b[0m W_reshaped \u001b[38;5;241m=\u001b[39m W\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, size, size) \u001b[38;5;66;03m# ( -1, -1) --> (-1, size, size)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m W_reshaped \u001b[38;5;241m=\u001b[39m W_reshaped\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 54\u001b[0m W_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mmake_image_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (-1, size, size) --> (-1, 3, size, size)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# try : \u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#     W_reshaped = W_reshaped.reshape(-1, batch, 3, size, size)  # (-1, 3, size, size) --> (-1, batch, 3, size, size)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# except:\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#     W_reshaped = W_reshaped.reshape(-1, 1, 3, size, size)  # (-1, 3, size, size) --> (-1, 1, 3, size, size)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m W_reshaped \u001b[38;5;241m=\u001b[39m W_reshaped\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, size, size)  \u001b[38;5;66;03m# (-1, 3, size, size) --> (-1, 1, 3, size, size)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mmake_image_format\u001b[0;34m(W, wp_mean, wp_std, normalize)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[1;32m     25\u001b[0m     W \u001b[38;5;241m=\u001b[39m (W \u001b[38;5;241m-\u001b[39m wp_mean) \u001b[38;5;241m/\u001b[39m wp_std\n\u001b[0;32m---> 26\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m W\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 672.00 MiB. GPU 0 has a total capacity of 23.58 GiB of which 211.31 MiB is free. Including non-PyTorch memory, this process has 23.36 GiB memory in use. Of the allocated memory 22.74 GiB is allocated by PyTorch, and 389.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "ckpt_path = '/home/jgryu/Weight_compression/llm-awq/model_cache/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920'\n",
    "net = AutoModelForCausalLM.from_pretrained(ckpt_path, local_files_only=True)\n",
    "\n",
    "mean = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_mean.npy')\n",
    "std = np.load(f'/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama--Meta-Llama-3-8B/mlp/d16/mlp_d16_train_std.npy')\n",
    "mean = torch.from_numpy(mean)\n",
    "std = torch.from_numpy(std)\n",
    "\n",
    "size = 256\n",
    "weight_condition = 'mlp'\n",
    "\n",
    "path = 'checkpoints_image_pretrained'\n",
    "pt_list = os.listdir(path)\n",
    "lmbdas = []\n",
    "for pt in pt_list:\n",
    "    lm = pt.replace('.pth', '')\n",
    "    lmbdas.append(float(lm))\n",
    "lmbdas = sorted(lmbdas)[-2:-1]\n",
    "print(lmbdas)\n",
    "\n",
    "for lm in lmbdas:\n",
    "    print(f'##### lambda: {lm} #####')\n",
    "    pt = f'{lm}.pth'\n",
    "    ck_path = f'checkpoints_image_pretrained/{lm}.pth'\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(ck_path, map_location=device)\n",
    "        assert isinstance(checkpoint, dict), \"Checkpoint is not a dictionary\"\n",
    "        assert \"state_dict\" in checkpoint, \"Missing 'state_dict' in checkpoint\"\n",
    "        print(f\"Checkpoint for {lm} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load checkpoint for {lm}: {e}\")\n",
    "\n",
    "\n",
    "    model = TCM(N=64)\n",
    "    try:\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        print(f\"Model state_dict loaded successfully for {lm}.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to load model state_dict for {lm}: {e}\")\n",
    "        \n",
    "    model = model.eval().to(device)\n",
    "    model.requires_grad_(False)\n",
    "    model.update()\n",
    "        \n",
    "    recon_state_dict, avg_bpp, mean_MSE = reconstruct_model(\n",
    "        net.state_dict(), model, save_path = None, logger= None, size = size, \n",
    "        weight_condition = weight_condition, mean = mean, std = std)\n",
    "\n",
    "print(avg_bpp, mean_MSE)\n",
    "torch.save(recon_state_dict, \"reconstruncted_state_dict/meta-llama--Meta-Llama-3-8B_mlp_d256_256.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nicc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
