{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '0'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/jgryu/Weight_compression/Wparam_dataset/Wparam_npy/llama_7b_self_attn_d=2048_val.npy'\n",
    "os.path.isfile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gparams = np.load('/home/jgryu/Weight_compression/RD-sandwich/data/gaussian_params-dim=1000.npz')\n",
    "loc = gparams['loc']\n",
    "scale = gparams['scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"/home/jgryu/Weight_compression/Wparam_dataset/Wparam_npy/llama_7b_self_attn_d=2048_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2936012, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_mean = 8.708306e-07\n",
    "wp_std = 0.023440132\n",
    "nor_d = (d - wp_mean) / wp_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.453125 -1.4765625\n",
      "61.993004 -62.99296\n"
     ]
    }
   ],
   "source": [
    "print(d.max(), d.min())\n",
    "print(nor_d.max(), nor_d.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = d.mean(axis=0)\n",
    "# s = d.var(axis=0)\n",
    "m = d.mean()\n",
    "s = d.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = d.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023440132"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_d = (d-m)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0009615, 1.0072731, 1.1371735, ..., 0.9774631, 0.9887671,\n",
       "       0.987939 ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nor_d.var(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023440132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.708306e-07"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005519013\n",
      "0.00055592053\n",
      "0.0006244048\n",
      "0.0005502901\n",
      "0.00054507813\n",
      "0.000547752\n",
      "0.00054456946\n",
      "0.0005488622\n",
      "0.00055248634\n",
      "0.0005476372\n",
      "0.000549594\n",
      "0.0005429699\n",
      "0.0005469753\n",
      "0.0005620375\n",
      "0.0005419004\n",
      "0.0005519321\n",
      "0.00054698763\n",
      "0.00054371735\n",
      "0.000549649\n",
      "0.0005512757\n",
      "0.0005441512\n",
      "0.0005516877\n",
      "0.0006109052\n",
      "0.0005499016\n",
      "0.0005367981\n",
      "0.00054645847\n",
      "0.00056516216\n",
      "0.0005391629\n",
      "0.0005643185\n",
      "0.000583725\n",
      "0.00056811195\n",
      "0.0005474951\n",
      "0.0005441823\n",
      "0.00054927083\n",
      "0.00054851687\n",
      "0.000545287\n",
      "0.00054395455\n",
      "0.00054915145\n",
      "0.0005391971\n",
      "0.00054115546\n",
      "0.0005386838\n",
      "0.0005464965\n",
      "0.0005479124\n",
      "0.0005638776\n",
      "0.00055515656\n",
      "0.0005476286\n",
      "0.0005493475\n",
      "0.0005439585\n",
      "0.0005352645\n",
      "0.0005484602\n",
      "0.0005593901\n",
      "0.0005435107\n",
      "0.00054504856\n",
      "0.0005528241\n",
      "0.00054054696\n",
      "0.0005467469\n",
      "0.00054280367\n",
      "0.00055291894\n",
      "0.0005438386\n",
      "0.0005444267\n",
      "0.0005405132\n",
      "0.0005475916\n",
      "0.0005508495\n",
      "0.0005472913\n",
      "0.00053791975\n",
      "0.00054547255\n",
      "0.00055312295\n",
      "0.0005538455\n",
      "0.00054435764\n",
      "0.00054462935\n",
      "0.00054843625\n",
      "0.00054445997\n",
      "0.0005436076\n",
      "0.0005515073\n",
      "0.0005502792\n",
      "0.0005487697\n",
      "0.0005412088\n",
      "0.00054916216\n",
      "0.00054357346\n",
      "0.00054680556\n",
      "0.0005485215\n",
      "0.00055849\n",
      "0.0005979184\n",
      "0.00054924586\n",
      "0.00054546737\n",
      "0.0005483934\n",
      "0.0005515984\n",
      "0.000565907\n",
      "0.0005696226\n",
      "0.00054679945\n",
      "0.00054908515\n",
      "0.00055430946\n",
      "0.00055438513\n",
      "0.0005433284\n",
      "0.0006592242\n",
      "0.0005481704\n",
      "0.0005512947\n",
      "0.00054892746\n",
      "0.00054078095\n",
      "0.0006518882\n",
      "0.0005511107\n",
      "0.0005412502\n",
      "0.00057102187\n",
      "0.00054320676\n",
      "0.00055108\n",
      "0.00055363704\n",
      "0.00054412655\n",
      "0.0005546547\n",
      "0.00054122985\n",
      "0.0005550651\n",
      "0.00067940075\n",
      "0.00054819224\n",
      "0.000551816\n",
      "0.00055604515\n",
      "0.0005476375\n",
      "0.00054489187\n",
      "0.00054342067\n",
      "0.00054792437\n",
      "0.00055108685\n",
      "0.00055271335\n",
      "0.0005733612\n",
      "0.000555322\n",
      "0.0005486346\n",
      "0.0005416474\n",
      "0.00055832585\n",
      "0.0005717245\n",
      "0.0005445365\n",
      "0.0005548472\n",
      "0.0005435558\n",
      "0.00055514544\n",
      "0.00054992066\n",
      "0.00053420913\n",
      "0.0005359713\n",
      "0.00054946647\n",
      "0.0005382828\n",
      "0.0005346547\n",
      "0.0005435742\n",
      "0.0005419839\n",
      "0.000538577\n",
      "0.0005473035\n",
      "0.00054626964\n",
      "0.00055592053\n",
      "0.0005610532\n",
      "0.00053906353\n",
      "0.0005464742\n",
      "0.0005478726\n",
      "0.0005457358\n",
      "0.0005416949\n",
      "0.00056854327\n",
      "0.00053916464\n",
      "0.0005432612\n",
      "0.0005424266\n",
      "0.0005541041\n",
      "0.0005440243\n",
      "0.00054376933\n",
      "0.0005555724\n",
      "0.00054022996\n",
      "0.00054239854\n",
      "0.0005443779\n",
      "0.00054518785\n",
      "0.00054103776\n",
      "0.0005511023\n",
      "0.00054603774\n",
      "0.0005455751\n",
      "0.00055115565\n",
      "0.00054464256\n",
      "0.000565954\n",
      "0.000551945\n",
      "0.00053745415\n",
      "0.0005431884\n",
      "0.0005480429\n",
      "0.00055071147\n",
      "0.000546574\n",
      "0.00054144394\n",
      "0.000539613\n",
      "0.0005445402\n",
      "0.0005338884\n",
      "0.0005456375\n",
      "0.0005419872\n",
      "0.0005418899\n",
      "0.000545542\n",
      "0.0005441746\n",
      "0.0005497119\n",
      "0.00054247334\n",
      "0.00056756276\n",
      "0.0005423981\n",
      "0.0005415966\n",
      "0.00069024705\n",
      "0.00054721424\n",
      "0.00053955783\n",
      "0.0005433452\n",
      "0.0005493827\n",
      "0.0005570674\n",
      "0.0005439361\n",
      "0.0005488272\n",
      "0.0005440717\n",
      "0.0005488732\n",
      "0.0005423179\n",
      "0.00053895445\n",
      "0.0005608268\n",
      "0.0005436385\n",
      "0.0005506174\n",
      "0.00055227755\n",
      "0.00054405024\n",
      "0.0005449366\n",
      "0.0005504647\n",
      "0.0005458557\n",
      "0.000566675\n",
      "0.0005457013\n",
      "0.0005384934\n",
      "0.0005707396\n",
      "0.0005449624\n",
      "0.00055014994\n",
      "0.00054717896\n",
      "0.0005434663\n",
      "0.00054173265\n",
      "0.00055804953\n",
      "0.00054014544\n",
      "0.00054358086\n",
      "0.00055022305\n",
      "0.0005402252\n",
      "0.0005419573\n",
      "0.00054476096\n",
      "0.0005471737\n",
      "0.00054274907\n",
      "0.0005470958\n",
      "0.0005388353\n",
      "0.000547072\n",
      "0.0005427312\n",
      "0.00054204464\n",
      "0.0005501535\n",
      "0.0005486738\n",
      "0.0005411606\n",
      "0.0005412173\n",
      "0.0005463975\n",
      "0.00054280687\n",
      "0.000542993\n",
      "0.00054012815\n",
      "0.0005509781\n",
      "0.0005563784\n",
      "0.0005543708\n",
      "0.0005432278\n",
      "0.0005433246\n",
      "0.00053786766\n",
      "0.0005519128\n",
      "0.0005465386\n",
      "0.00054033834\n",
      "0.00054014416\n",
      "0.00054052315\n",
      "0.0005608756\n",
      "0.00063837325\n",
      "0.00053876813\n",
      "0.00054078276\n",
      "0.0005399547\n",
      "0.00054739235\n",
      "0.00054673053\n",
      "0.0005419804\n",
      "0.0006603859\n",
      "0.0005401483\n",
      "0.0005489226\n",
      "0.00054310967\n",
      "0.0005493876\n",
      "0.0005569993\n",
      "0.0005485991\n",
      "0.0005385232\n",
      "0.00053219585\n",
      "0.00057324336\n",
      "0.0005366119\n",
      "0.0005524317\n",
      "0.0005819659\n",
      "0.0005435362\n",
      "0.00053864176\n",
      "0.0005478021\n",
      "0.000539413\n",
      "0.00053831306\n",
      "0.0005420136\n",
      "0.0005440257\n",
      "0.0005350721\n",
      "0.0005457712\n",
      "0.0005409147\n",
      "0.00054139644\n",
      "0.00054157217\n",
      "0.00054478884\n",
      "0.00055063213\n",
      "0.0005349822\n",
      "0.00054031005\n",
      "0.00054647203\n",
      "0.0005489815\n",
      "0.00057944725\n",
      "0.0005698545\n",
      "0.000534024\n",
      "0.00054579746\n",
      "0.00053835934\n",
      "0.00054545014\n",
      "0.0005443305\n",
      "0.0005559369\n",
      "0.0005467386\n",
      "0.00054972136\n",
      "0.0005477037\n",
      "0.0005345481\n",
      "0.00053696794\n",
      "0.00053727574\n",
      "0.0005793039\n",
      "0.00053293747\n",
      "0.00054827385\n",
      "0.0005379574\n",
      "0.00053516513\n",
      "0.00054110016\n",
      "0.0005468463\n",
      "0.00054997095\n",
      "0.0006649615\n",
      "0.0005496187\n",
      "0.00054250297\n",
      "0.0005402597\n",
      "0.00054069265\n",
      "0.00054367067\n",
      "0.0005385996\n",
      "0.0005625415\n",
      "0.00053661846\n",
      "0.0005561862\n",
      "0.00055575394\n",
      "0.0005480967\n",
      "0.0005366211\n",
      "0.0005345148\n",
      "0.0005539589\n",
      "0.0005392789\n",
      "0.0005396846\n",
      "0.0005476876\n",
      "0.000551511\n",
      "0.00055105396\n",
      "0.00054403767\n",
      "0.0005437808\n",
      "0.0005393363\n",
      "0.00054127397\n",
      "0.0005336224\n",
      "0.0005386199\n",
      "0.0005507699\n",
      "0.00054111116\n",
      "0.00054650544\n",
      "0.00059722475\n",
      "0.00054624164\n",
      "0.0005443432\n",
      "0.0005442937\n",
      "0.0005349785\n",
      "0.00053497247\n",
      "0.00067751115\n",
      "0.00056365633\n",
      "0.0005330127\n",
      "0.0005455213\n",
      "0.0005465921\n",
      "0.00053541764\n",
      "0.00054293725\n",
      "0.00053505896\n",
      "0.0005383347\n",
      "0.0005483638\n",
      "0.0005935965\n",
      "0.0005418189\n",
      "0.00055147376\n",
      "0.00053589453\n",
      "0.0005403774\n",
      "0.00054662256\n",
      "0.00053705164\n",
      "0.00054314395\n",
      "0.00064451294\n",
      "0.0005392112\n",
      "0.00053743325\n",
      "0.00053724385\n",
      "0.0005418051\n",
      "0.0005392168\n",
      "0.0005478891\n",
      "0.0005353293\n",
      "0.0005372595\n",
      "0.00054197805\n",
      "0.000548902\n",
      "0.0005291924\n",
      "0.0005833903\n",
      "0.00053909875\n",
      "0.0005397476\n",
      "0.00054213783\n",
      "0.00053677417\n",
      "0.00054158445\n",
      "0.0005465187\n",
      "0.00053501606\n",
      "0.0005415537\n",
      "0.00055672834\n",
      "0.00054576277\n",
      "0.0005511756\n",
      "0.0005498617\n",
      "0.0005416927\n",
      "0.0005465189\n",
      "0.0005486028\n",
      "0.0005629047\n",
      "0.00054266484\n",
      "0.0005473621\n",
      "0.00055619923\n",
      "0.0005428821\n",
      "0.00054598437\n",
      "0.00054526824\n",
      "0.000549971\n",
      "0.0005437013\n",
      "0.0005451508\n",
      "0.00054165226\n",
      "0.00054962083\n",
      "0.0005443917\n",
      "0.0005488883\n",
      "0.00055312953\n",
      "0.000556336\n",
      "0.0005424565\n",
      "0.00055862387\n",
      "0.00055166864\n",
      "0.00054801436\n",
      "0.00058511033\n",
      "0.0005504497\n",
      "0.0005511821\n",
      "0.0005396566\n",
      "0.0005473805\n",
      "0.00054724806\n",
      "0.00054689083\n",
      "0.00053791667\n",
      "0.00054835004\n",
      "0.0005462243\n",
      "0.00061693386\n",
      "0.0005468636\n",
      "0.000556935\n",
      "0.00056588365\n",
      "0.00054295594\n",
      "0.00054273306\n",
      "0.0005474381\n",
      "0.00053852075\n",
      "0.00055202574\n",
      "0.00055251486\n",
      "0.0005515227\n",
      "0.00055339973\n",
      "0.0005425003\n",
      "0.000548374\n",
      "0.00054108875\n",
      "0.0005508373\n",
      "0.00054593105\n",
      "0.0005441638\n",
      "0.00055228244\n",
      "0.00055744057\n",
      "0.00054480263\n",
      "0.00054108584\n",
      "0.00054792664\n",
      "0.000558101\n",
      "0.0005457196\n",
      "0.0005423177\n",
      "0.0005726983\n",
      "0.00055161276\n",
      "0.0005483382\n",
      "0.00054329046\n",
      "0.00053786335\n",
      "0.0005506228\n",
      "0.0005445029\n",
      "0.00054016686\n",
      "0.0005544955\n",
      "0.00054473954\n",
      "0.0005471012\n",
      "0.0005392531\n",
      "0.0005503585\n",
      "0.0005523769\n",
      "0.0005555771\n",
      "0.00062650436\n",
      "0.0005451782\n",
      "0.00054388994\n",
      "0.00054897205\n",
      "0.00053593784\n",
      "0.00055551744\n",
      "0.00054665306\n",
      "0.00055099145\n",
      "0.00054737745\n",
      "0.00054893835\n",
      "0.00055478315\n",
      "0.0005845402\n",
      "0.0005813295\n",
      "0.0005413535\n",
      "0.0005476347\n",
      "0.00055013935\n",
      "0.0005592799\n",
      "0.00054103095\n",
      "0.0005611275\n",
      "0.00054476206\n",
      "0.00054433633\n",
      "0.0005445688\n",
      "0.0005522327\n",
      "0.0016615904\n",
      "0.0005443687\n",
      "0.00054933695\n",
      "0.00053891086\n",
      "0.00054368057\n",
      "0.0006597125\n",
      "0.00055088644\n",
      "0.00054959545\n",
      "0.0005506872\n",
      "0.00055117276\n",
      "0.00064972753\n",
      "0.00057860895\n",
      "0.000556243\n",
      "0.0005480613\n",
      "0.00055303273\n",
      "0.0005503357\n",
      "0.00054443884\n",
      "0.0005427365\n",
      "0.0005449588\n",
      "0.00054802716\n",
      "0.00055652694\n",
      "0.0005445351\n",
      "0.00055604626\n",
      "0.00054668856\n",
      "0.0005421283\n",
      "0.00055408233\n",
      "0.0005506491\n",
      "0.0005511081\n",
      "0.0005431089\n",
      "0.0005409451\n",
      "0.00055040815\n",
      "0.0005453447\n",
      "0.0005541893\n",
      "0.0005409102\n",
      "0.0005459432\n",
      "0.000536431\n",
      "0.00054570875\n",
      "0.0005394606\n",
      "0.00054925255\n",
      "0.00054466963\n",
      "0.000538108\n",
      "0.0005509556\n",
      "0.0005364616\n",
      "0.000552134\n",
      "0.0005365286\n",
      "0.00054305076\n",
      "0.0005392457\n",
      "0.00055316766\n",
      "0.0005412339\n",
      "0.0005524769\n",
      "0.0005384051\n",
      "0.00055770244\n",
      "0.00054331706\n",
      "0.00055178767\n",
      "0.0005485497\n",
      "0.0005554648\n",
      "0.00053917686\n",
      "0.000542989\n",
      "0.00053799554\n",
      "0.0005400289\n",
      "0.0005615234\n",
      "0.0005381879\n",
      "0.0005519019\n",
      "0.00054334133\n",
      "0.00054077804\n",
      "0.00054669875\n",
      "0.00053563615\n",
      "0.00055654574\n",
      "0.00055083574\n",
      "0.000541109\n",
      "0.0005467986\n",
      "0.00053698797\n",
      "0.0005448265\n",
      "0.00054390234\n",
      "0.0005525665\n",
      "0.00069859135\n",
      "0.00054995547\n",
      "0.0005418513\n",
      "0.0005497266\n",
      "0.00054517115\n",
      "0.00054764154\n",
      "0.00054152345\n",
      "0.0005559967\n",
      "0.00055778574\n",
      "0.0005447909\n",
      "0.0005370001\n",
      "0.0005375221\n",
      "0.0005346139\n",
      "0.00053905364\n",
      "0.0005820607\n",
      "0.00060031575\n",
      "0.00054639083\n",
      "0.0005734359\n",
      "0.000553995\n",
      "0.00054572785\n",
      "0.00060088147\n",
      "0.0005436641\n",
      "0.0005348598\n",
      "0.00054682663\n",
      "0.000548649\n",
      "0.0005540734\n",
      "0.0005397334\n",
      "0.0005499275\n",
      "0.00064328837\n",
      "0.0005477531\n",
      "0.00053633505\n",
      "0.0005399548\n",
      "0.0005341278\n",
      "0.0005423449\n",
      "0.000546479\n",
      "0.0005413355\n",
      "0.0005477013\n",
      "0.00057274505\n",
      "0.0005448847\n",
      "0.0005468884\n",
      "0.0005356328\n",
      "0.0005574473\n",
      "0.0005397083\n",
      "0.0005535612\n",
      "0.0005448351\n",
      "0.0005411461\n",
      "0.0005474977\n",
      "0.0005443069\n",
      "0.00054974505\n",
      "0.0005485258\n",
      "0.0005469733\n",
      "0.0005515325\n",
      "0.00053792424\n",
      "0.0005562488\n",
      "0.0005453696\n",
      "0.000537371\n",
      "0.0005456543\n",
      "0.00054505235\n",
      "0.0005436341\n",
      "0.00053915783\n",
      "0.0005560524\n",
      "0.0005396527\n",
      "0.00053485733\n",
      "0.0005348299\n",
      "0.0005387965\n",
      "0.0005492096\n",
      "0.0005471313\n",
      "0.00054748816\n",
      "0.00054919935\n",
      "0.0005499126\n",
      "0.00054309657\n",
      "0.00054066424\n",
      "0.0005388019\n",
      "0.0005449061\n",
      "0.00054669566\n",
      "0.00053944165\n",
      "0.0005392401\n",
      "0.00054844597\n",
      "0.00053937297\n",
      "0.0005397438\n",
      "0.0005521201\n",
      "0.0005479509\n",
      "0.00055308803\n",
      "0.0005539667\n",
      "0.0005505758\n",
      "0.00054586184\n",
      "0.00054872734\n",
      "0.0005453262\n",
      "0.00053987274\n",
      "0.0005512578\n",
      "0.00054574054\n",
      "0.00053871516\n",
      "0.0005496014\n",
      "0.00053671957\n",
      "0.0005483387\n",
      "0.000549329\n",
      "0.00058019086\n",
      "0.00054459705\n",
      "0.00054521684\n",
      "0.0005435576\n",
      "0.00054665253\n",
      "0.00054049166\n",
      "0.0005518504\n",
      "0.0005414382\n",
      "0.0005514824\n",
      "0.0005483255\n",
      "0.0005473531\n",
      "0.0005398934\n",
      "0.0005514318\n",
      "0.00054465054\n",
      "0.0005458858\n",
      "0.00054927834\n",
      "0.00055048673\n",
      "0.00053593115\n",
      "0.0005385737\n",
      "0.0005408268\n",
      "0.00054486806\n",
      "0.000556381\n",
      "0.0005452205\n",
      "0.0005549307\n",
      "0.0005423918\n",
      "0.00053133344\n",
      "0.0005359017\n",
      "0.0005447541\n",
      "0.00054034323\n",
      "0.0005539722\n",
      "0.00054227293\n",
      "0.00054058986\n",
      "0.0005447029\n",
      "0.0005429368\n",
      "0.00054309645\n",
      "0.0005423163\n",
      "0.0005461737\n",
      "0.000550977\n",
      "0.00054226466\n",
      "0.000540209\n",
      "0.00055346184\n",
      "0.00054569147\n",
      "0.0005401517\n",
      "0.0005412258\n",
      "0.0005395775\n",
      "0.00054673717\n",
      "0.0005524871\n",
      "0.0005366952\n",
      "0.0005412453\n",
      "0.0005404523\n",
      "0.00053621194\n",
      "0.0005404922\n",
      "0.00053992966\n",
      "0.0005411599\n",
      "0.00053794164\n",
      "0.0005464249\n",
      "0.0005505952\n",
      "0.00055629626\n",
      "0.00055184716\n",
      "0.0005517896\n",
      "0.0005420424\n",
      "0.0005491741\n",
      "0.000545846\n",
      "0.00054110604\n",
      "0.000538256\n",
      "0.0005400529\n",
      "0.0005386458\n",
      "0.00054772734\n",
      "0.00054527924\n",
      "0.000552917\n",
      "0.00054795895\n",
      "0.0005405293\n",
      "0.0005380288\n",
      "0.00055407383\n",
      "0.0005753043\n",
      "0.000542729\n",
      "0.0005429725\n",
      "0.0005466919\n",
      "0.000552064\n",
      "0.0005415948\n",
      "0.000550139\n",
      "0.0005407592\n",
      "0.00054327014\n",
      "0.00054782757\n",
      "0.00054246525\n",
      "0.0006509991\n",
      "0.0005383084\n",
      "0.0005432908\n",
      "0.0005497703\n",
      "0.0005533805\n",
      "0.00054676796\n",
      "0.00054288574\n",
      "0.00054216484\n",
      "0.0005539083\n",
      "0.0005452388\n",
      "0.00054292387\n",
      "0.0005457926\n",
      "0.0005505937\n",
      "0.0005422008\n",
      "0.00053621427\n",
      "0.00054445036\n",
      "0.0005456289\n",
      "0.0005446432\n",
      "0.00056556886\n",
      "0.00056829577\n",
      "0.00054490793\n",
      "0.00053406757\n",
      "0.00054903916\n",
      "0.0005381828\n",
      "0.0005383145\n",
      "0.0005364998\n",
      "0.00053668953\n",
      "0.00054620777\n",
      "0.00055124296\n",
      "0.0005532149\n",
      "0.00053670886\n",
      "0.0005510405\n",
      "0.0005508177\n",
      "0.0005436698\n",
      "0.0005368525\n",
      "0.00053897436\n",
      "0.0005578981\n",
      "0.0005512271\n",
      "0.0005493481\n",
      "0.00056227756\n",
      "0.0005370717\n",
      "0.0005538559\n",
      "0.00054210634\n",
      "0.00055018987\n",
      "0.0005449278\n",
      "0.00054362614\n",
      "0.00054730097\n",
      "0.00057253643\n",
      "0.0005462591\n",
      "0.0005469289\n",
      "0.0005436666\n",
      "0.00055419235\n",
      "0.00054948713\n",
      "0.0005531458\n",
      "0.00054773485\n",
      "0.0005709316\n",
      "0.00054263376\n",
      "0.00054141745\n",
      "0.0005449876\n",
      "0.0005616809\n",
      "0.0005502736\n",
      "0.0005581774\n",
      "0.00054804544\n",
      "0.000536775\n",
      "0.0005763956\n",
      "0.0005501729\n",
      "0.0005522178\n",
      "0.0005542189\n",
      "0.00055888086\n",
      "0.0005370238\n",
      "0.00054338644\n",
      "0.0005588462\n",
      "0.00053839094\n",
      "0.0005440496\n",
      "0.0005817568\n",
      "0.00054734905\n",
      "0.00055193494\n",
      "0.0005811533\n",
      "0.00053668\n",
      "0.0005453283\n",
      "0.0005407314\n",
      "0.0005487627\n",
      "0.00055635156\n",
      "0.0005495436\n",
      "0.0005451146\n",
      "0.00054460386\n",
      "0.00055011397\n",
      "0.00055307045\n",
      "0.0005485184\n",
      "0.00054373307\n",
      "0.0005439094\n",
      "0.0005581877\n",
      "0.0005437728\n",
      "0.00054035836\n",
      "0.0005496779\n",
      "0.0005569394\n",
      "0.00053989707\n",
      "0.0005437385\n",
      "0.000537486\n",
      "0.0005478883\n",
      "0.0005448802\n",
      "0.0005551778\n",
      "0.00054389593\n",
      "0.0005425255\n",
      "0.00054606184\n",
      "0.0005424106\n",
      "0.0005414644\n",
      "0.00054324686\n",
      "0.0005454308\n",
      "0.0005469304\n",
      "0.00054649153\n",
      "0.0005397766\n",
      "0.00055000116\n",
      "0.000550555\n",
      "0.000540189\n",
      "0.0005530829\n",
      "0.00053714315\n",
      "0.0005351031\n",
      "0.000548101\n",
      "0.00054689235\n",
      "0.000547055\n",
      "0.0005451078\n",
      "0.0005462038\n",
      "0.0005517844\n",
      "0.0005479569\n",
      "0.00059282104\n",
      "0.00054544775\n",
      "0.00054582604\n",
      "0.00054799387\n",
      "0.00054317544\n",
      "0.00054215593\n",
      "0.00055435987\n",
      "0.00054513325\n",
      "0.0005379588\n",
      "0.0005487546\n",
      "0.0005370145\n",
      "0.0005510761\n",
      "0.0005503215\n",
      "0.000664139\n",
      "0.0005483639\n",
      "0.0005432997\n",
      "0.00054520625\n",
      "0.00054878026\n",
      "0.0005468356\n",
      "0.0005481759\n",
      "0.0005615544\n",
      "0.0005506624\n",
      "0.0005682525\n",
      "0.0005428888\n",
      "0.00054658373\n",
      "0.00054911384\n",
      "0.00055330165\n",
      "0.00053630065\n",
      "0.00054899737\n",
      "0.00055394444\n",
      "0.00063464465\n",
      "0.0005460392\n",
      "0.0005492939\n",
      "0.0005626812\n",
      "0.00054674293\n",
      "0.0005460311\n",
      "0.0005460257\n",
      "0.0005389155\n",
      "0.0005442388\n",
      "0.0005590581\n",
      "0.00054636446\n",
      "0.0005506551\n",
      "0.0005446864\n",
      "0.0005462887\n",
      "0.00057948974\n",
      "0.0005460241\n",
      "0.0005482574\n",
      "0.0005648052\n",
      "0.0005667295\n",
      "0.000544955\n",
      "0.00054472947\n",
      "0.0005443987\n",
      "0.0005399684\n",
      "0.00055517285\n",
      "0.000549533\n",
      "0.0005488356\n",
      "0.000543665\n",
      "0.00055591005\n",
      "0.0005478822\n",
      "0.0005395455\n",
      "0.0005444546\n",
      "0.0005434164\n",
      "0.0005449813\n",
      "0.00054362335\n",
      "0.00054538785\n",
      "0.0005464874\n",
      "0.0005419215\n",
      "0.000542499\n",
      "0.0005483571\n",
      "0.0005516283\n",
      "0.000550642\n",
      "0.0005415726\n",
      "0.00055666774\n",
      "0.00053977844\n",
      "0.0005478274\n",
      "0.0005474218\n",
      "0.00054585125\n",
      "0.0005425834\n",
      "0.0005607603\n",
      "0.0005610241\n",
      "0.0005599659\n",
      "0.0005599529\n",
      "0.0005562394\n",
      "0.000549985\n",
      "0.00055621815\n",
      "0.00054303155\n",
      "0.00055573677\n",
      "0.000544619\n",
      "0.0005430003\n",
      "0.0005421107\n",
      "0.00055192714\n",
      "0.00055023504\n",
      "0.00054361933\n",
      "0.00058691244\n",
      "0.0005420075\n",
      "0.0005463527\n",
      "0.00054041814\n",
      "0.0005467911\n",
      "0.00054566615\n",
      "0.0005440003\n",
      "0.0005521357\n",
      "0.00054517225\n",
      "0.00054535794\n",
      "0.0005606265\n",
      "0.00053787685\n",
      "0.000548221\n",
      "0.0005721795\n",
      "0.00054659706\n",
      "0.0005581758\n",
      "0.00055503985\n",
      "0.0005502273\n",
      "0.00054950657\n",
      "0.0005428659\n",
      "0.0005507097\n",
      "0.00055392546\n",
      "0.0005534597\n",
      "0.00055910554\n",
      "0.00055411225\n",
      "0.0005492469\n",
      "0.0005512636\n",
      "0.00053723465\n",
      "0.00054431014\n",
      "0.00054356636\n",
      "0.00055190455\n",
      "0.00053782255\n",
      "0.0005518465\n",
      "0.0005429246\n",
      "0.0005493143\n",
      "0.00054896274\n",
      "0.00054992695\n",
      "0.0005422421\n",
      "0.0005781961\n",
      "0.0005473955\n",
      "0.0005513834\n",
      "0.0005414551\n",
      "0.0005442553\n",
      "0.00055298547\n",
      "0.0005488761\n",
      "0.0005463921\n",
      "0.0005464635\n",
      "0.00054715626\n",
      "0.00054974767\n",
      "0.0005463451\n",
      "0.00054617703\n",
      "0.0005497664\n",
      "0.00055242213\n",
      "0.0005499224\n",
      "0.00054802065\n",
      "0.00055150676\n",
      "0.00054399314\n",
      "0.0005380653\n",
      "0.00054836826\n",
      "0.000554632\n",
      "0.00055208657\n",
      "0.00054392114\n",
      "0.0005445374\n",
      "0.000538643\n",
      "0.00054851576\n",
      "0.00054155977\n",
      "0.0005488598\n",
      "0.0005401033\n",
      "0.0005418306\n",
      "0.00054668786\n",
      "0.00054374273\n",
      "0.0005872723\n",
      "0.0005445272\n",
      "0.00054554665\n",
      "0.00054015935\n",
      "0.0005460395\n",
      "0.00054164097\n",
      "0.00055994384\n",
      "0.0005375687\n",
      "0.00054679695\n",
      "0.00055995554\n",
      "0.00054366305\n",
      "0.00055144314\n",
      "0.0005520251\n",
      "0.0005401819\n",
      "0.000546208\n",
      "0.0005544582\n",
      "0.00054036157\n",
      "0.00055773574\n",
      "0.00054550887\n",
      "0.00056075625\n",
      "0.0005460038\n",
      "0.00054215477\n",
      "0.00053786166\n",
      "0.0005368466\n",
      "0.0005581298\n",
      "0.00056023826\n",
      "0.00054888363\n",
      "0.0005539489\n",
      "0.00054882467\n",
      "0.0005657847\n",
      "0.0005502299\n",
      "0.0005465509\n",
      "0.00055006845\n",
      "0.0005408794\n",
      "0.0005422149\n",
      "0.0005473671\n",
      "0.00055632414\n",
      "0.0005536602\n",
      "0.00056167576\n",
      "0.0005575172\n",
      "0.00055004994\n",
      "0.00054361\n",
      "0.00054155645\n",
      "0.00054963323\n",
      "0.00055673067\n",
      "0.0005453392\n",
      "0.0006406646\n",
      "0.00054143346\n",
      "0.0005519009\n",
      "0.0005459175\n",
      "0.00054539513\n",
      "0.0005581848\n",
      "0.0005417246\n",
      "0.0005398503\n",
      "0.0005467981\n",
      "0.0005422656\n",
      "0.00055063545\n",
      "0.00061352877\n",
      "0.0005510863\n",
      "0.00054900127\n",
      "0.00055125554\n",
      "0.0005439592\n",
      "0.0005448191\n",
      "0.00055159937\n",
      "0.00054127845\n",
      "0.00054645515\n",
      "0.0005422747\n",
      "0.0005461984\n",
      "0.0005498674\n",
      "0.0005376059\n",
      "0.00055362\n",
      "0.0005556685\n",
      "0.000545365\n",
      "0.000554316\n",
      "0.0005426064\n",
      "0.0005409396\n",
      "0.00054131396\n",
      "0.0005466982\n",
      "0.0005422801\n",
      "0.0005480281\n",
      "0.0005873259\n",
      "0.00054841756\n",
      "0.00055107224\n",
      "0.00054295134\n",
      "0.0005464883\n",
      "0.00054307666\n",
      "0.0005920042\n",
      "0.00054162287\n",
      "0.0005593426\n",
      "0.00054398493\n",
      "0.0005667891\n",
      "0.0005459601\n",
      "0.0005389028\n",
      "0.00055529753\n",
      "0.0005418466\n",
      "0.0005469114\n",
      "0.00054650236\n",
      "0.0005559727\n",
      "0.0005425213\n",
      "0.0005464598\n",
      "0.00057387707\n",
      "0.0005345026\n",
      "0.0005422292\n",
      "0.0005362063\n",
      "0.0005425538\n",
      "0.00054600067\n",
      "0.0005440234\n",
      "0.0005359785\n",
      "0.00054666435\n",
      "0.0005456358\n",
      "0.00054300675\n",
      "0.00055556395\n",
      "0.0005550811\n",
      "0.0005442283\n",
      "0.00053853274\n",
      "0.00054053165\n",
      "0.0005410838\n",
      "0.0005418469\n",
      "0.0005412333\n",
      "0.000544492\n",
      "0.00054276554\n",
      "0.0005473265\n",
      "0.0005468584\n",
      "0.0005392541\n",
      "0.000553151\n",
      "0.0005447731\n",
      "0.0005427675\n",
      "0.00054417213\n",
      "0.0005412754\n",
      "0.000563885\n",
      "0.0005432379\n",
      "0.00066974066\n",
      "0.00054247695\n",
      "0.0005539821\n",
      "0.0005539853\n",
      "0.0005498236\n",
      "0.00053933315\n",
      "0.00055834156\n",
      "0.0005450142\n",
      "0.00054728024\n",
      "0.00055389095\n",
      "0.0005411942\n",
      "0.0005356024\n",
      "0.00055374485\n",
      "0.00057464716\n",
      "0.0005381621\n",
      "0.000548698\n",
      "0.00054496125\n",
      "0.00054518844\n",
      "0.00054319605\n",
      "0.00053495116\n",
      "0.00055094046\n",
      "0.00055368914\n",
      "0.0005565257\n",
      "0.0005523173\n",
      "0.00054974953\n",
      "0.0005389969\n",
      "0.00055526895\n",
      "0.0005514255\n",
      "0.000536801\n",
      "0.0005867926\n",
      "0.0005427153\n",
      "0.0005497667\n",
      "0.00053808605\n",
      "0.0005456526\n",
      "0.0005444552\n",
      "0.00054176606\n",
      "0.0005458201\n",
      "0.00054448965\n",
      "0.00053811865\n",
      "0.00054449926\n",
      "0.00054122927\n",
      "0.0005363971\n",
      "0.00054428086\n",
      "0.00053717627\n",
      "0.00054752745\n",
      "0.0005527864\n",
      "0.00054907444\n",
      "0.00054258393\n",
      "0.000542542\n",
      "0.0005565715\n",
      "0.0005438122\n",
      "0.0005604162\n",
      "0.00055258855\n",
      "0.00054332777\n",
      "0.00054738537\n",
      "0.0005439953\n",
      "0.00055079156\n",
      "0.00068400323\n",
      "0.00053295866\n",
      "0.00055984233\n",
      "0.0005459788\n",
      "0.0005430985\n",
      "0.0005392408\n",
      "0.00054649456\n",
      "0.0005430142\n",
      "0.00054244045\n",
      "0.0005430081\n",
      "0.00054402446\n",
      "0.00054568506\n",
      "0.00054390036\n",
      "0.00055313244\n",
      "0.000548526\n",
      "0.0005429363\n",
      "0.000544531\n",
      "0.00054889754\n",
      "0.00055301347\n",
      "0.0005437377\n",
      "0.0005438741\n",
      "0.0005460372\n",
      "0.00053780887\n",
      "0.0005427072\n",
      "0.000543844\n",
      "0.00054349296\n",
      "0.0005479636\n",
      "0.0005409986\n",
      "0.00054684497\n",
      "0.00055729016\n",
      "0.00053547806\n",
      "0.00055457593\n",
      "0.00053846295\n",
      "0.0005467713\n",
      "0.0005502567\n",
      "0.00054063654\n",
      "0.0005524086\n",
      "0.0005634341\n",
      "0.0005408593\n",
      "0.0005463002\n",
      "0.0005672434\n",
      "0.00055492943\n",
      "0.00054202345\n",
      "0.00057908375\n",
      "0.00055788347\n",
      "0.0005571455\n",
      "0.0005566931\n",
      "0.000551364\n",
      "0.00055080565\n",
      "0.0005449258\n",
      "0.00054889667\n",
      "0.00055490545\n",
      "0.0005598905\n",
      "0.0005429109\n",
      "0.00056060834\n",
      "0.00054477877\n",
      "0.00054995215\n",
      "0.0005419597\n",
      "0.00055919203\n",
      "0.00053959736\n",
      "0.00054038916\n",
      "0.00055783853\n",
      "0.0005357755\n",
      "0.00053723407\n",
      "0.00054092007\n",
      "0.0005512736\n",
      "0.0005512573\n",
      "0.0005388905\n",
      "0.00054213835\n",
      "0.0005380973\n",
      "0.0005342669\n",
      "0.00054150843\n",
      "0.0005422576\n",
      "0.00054685544\n",
      "0.0005409618\n",
      "0.00054097275\n",
      "0.0005495684\n",
      "0.00054773776\n",
      "0.00054203573\n",
      "0.0005367355\n",
      "0.00054291\n",
      "0.00054616155\n",
      "0.0005475548\n",
      "0.00053489994\n",
      "0.0005495023\n",
      "0.00055483234\n",
      "0.00054846314\n",
      "0.0005412934\n",
      "0.0005337954\n",
      "0.0005698937\n",
      "0.0005390248\n",
      "0.0005282131\n",
      "0.0005423004\n",
      "0.00055953336\n",
      "0.00053349003\n",
      "0.0005504905\n",
      "0.00055133866\n",
      "0.00055397704\n",
      "0.00054457\n",
      "0.00053886446\n",
      "0.0005361352\n",
      "0.000543619\n",
      "0.0005399649\n",
      "0.00054656295\n",
      "0.00054784276\n",
      "0.00055067765\n",
      "0.0005439038\n",
      "0.0005373956\n",
      "0.0005459203\n",
      "0.00053358846\n",
      "0.00054233667\n",
      "0.0005475003\n",
      "0.0005482788\n",
      "0.0005423066\n",
      "0.00054452574\n",
      "0.00054691336\n",
      "0.0005439164\n",
      "0.00053293695\n",
      "0.0005401416\n",
      "0.0005435851\n",
      "0.00055196363\n",
      "0.00054574973\n",
      "0.000547272\n",
      "0.00052958203\n",
      "0.00054593495\n",
      "0.00055435614\n",
      "0.00054245937\n",
      "0.0005510524\n",
      "0.0005442494\n",
      "0.00054876157\n",
      "0.0005467529\n",
      "0.0005405225\n",
      "0.0005443061\n",
      "0.00054133125\n",
      "0.00053512404\n",
      "0.0005469888\n",
      "0.00054071285\n",
      "0.0005342057\n",
      "0.0005451586\n",
      "0.0005450506\n",
      "0.00054842076\n",
      "0.0005502757\n",
      "0.0005451726\n",
      "0.0005396164\n",
      "0.0005448276\n",
      "0.0005457326\n",
      "0.0005333686\n",
      "0.0005418194\n",
      "0.0005410143\n",
      "0.00054217986\n",
      "0.0005446134\n",
      "0.0005450751\n",
      "0.0005415695\n",
      "0.0005555953\n",
      "0.00054018083\n",
      "0.0005414149\n",
      "0.000551671\n",
      "0.0005546193\n",
      "0.0005426148\n",
      "0.0005387141\n",
      "0.0005419176\n",
      "0.0005460059\n",
      "0.0005428796\n",
      "0.0005374463\n",
      "0.00054528145\n",
      "0.0006724596\n",
      "0.00054373464\n",
      "0.00054758653\n",
      "0.00054435304\n",
      "0.00054735556\n",
      "0.000549542\n",
      "0.00053715374\n",
      "0.0005388318\n",
      "0.00055364857\n",
      "0.000550175\n",
      "0.00056466454\n",
      "0.0005630422\n",
      "0.0006149836\n",
      "0.0005403754\n",
      "0.0005404114\n",
      "0.00053297024\n",
      "0.0005531134\n",
      "0.0005519031\n",
      "0.000532917\n",
      "0.00054906675\n",
      "0.0005367661\n",
      "0.0006940402\n",
      "0.00054882653\n",
      "0.00054774835\n",
      "0.0005499777\n",
      "0.0005386555\n",
      "0.0005476035\n",
      "0.00053525995\n",
      "0.0006432039\n",
      "0.000540328\n",
      "0.00054616685\n",
      "0.0005414803\n",
      "0.00043930986\n",
      "0.0005514416\n",
      "0.0005407152\n",
      "0.0005474859\n",
      "0.0005440092\n",
      "0.000552805\n",
      "0.00053854473\n",
      "0.0005448789\n",
      "0.0005628949\n",
      "0.0005510438\n",
      "0.0005387233\n",
      "0.0005395437\n",
      "0.00055002794\n",
      "0.0005399936\n",
      "0.000542283\n",
      "0.000538568\n",
      "0.0005409596\n",
      "0.0005441838\n",
      "0.0005551848\n",
      "0.00054559123\n",
      "0.0005487609\n",
      "0.0005518146\n",
      "0.00054651935\n",
      "0.00055947993\n",
      "0.00054356735\n",
      "0.0005532082\n",
      "0.00054441974\n",
      "0.0005572315\n",
      "0.000556733\n",
      "0.00055415713\n",
      "0.00054605736\n",
      "0.00054116134\n",
      "0.000541867\n",
      "0.00055297365\n",
      "0.0005473551\n",
      "0.000544173\n",
      "0.0005453933\n",
      "0.00054971693\n",
      "0.0005426216\n",
      "0.00054175017\n",
      "0.0005492388\n",
      "0.00056169485\n",
      "0.0005402259\n",
      "0.00054723694\n",
      "0.00054586085\n",
      "0.0005496379\n",
      "0.0005424625\n",
      "0.00054769905\n",
      "0.0005425724\n",
      "0.00055008964\n",
      "0.000550079\n",
      "0.0005449493\n",
      "0.00055266364\n",
      "0.0005469024\n",
      "0.0005416104\n",
      "0.000543066\n",
      "0.0005361875\n",
      "0.0005325193\n",
      "0.00054435123\n",
      "0.0005457546\n",
      "0.0005404928\n",
      "0.0005379304\n",
      "0.00054203876\n",
      "0.00055066735\n",
      "0.0005442146\n",
      "0.000542987\n",
      "0.00053332886\n",
      "0.00053515285\n",
      "0.00054918666\n",
      "0.0005454006\n",
      "0.00054506154\n",
      "0.0005444505\n",
      "0.0005575712\n",
      "0.0005500986\n",
      "0.0005460478\n",
      "0.00054255076\n",
      "0.00054278807\n",
      "0.0005474772\n",
      "0.00054437824\n",
      "0.0005377724\n",
      "0.0005431072\n",
      "0.0005381619\n",
      "0.00056492025\n",
      "0.0005528619\n",
      "0.0005460912\n",
      "0.0005433547\n",
      "0.0005532913\n",
      "0.00057471765\n",
      "0.0005477179\n",
      "0.00054781284\n",
      "0.00053913466\n",
      "0.00054103526\n",
      "0.00055450614\n",
      "0.0005433973\n",
      "0.00054829015\n",
      "0.0005407249\n",
      "0.00054409716\n",
      "0.00055449706\n",
      "0.0005404046\n",
      "0.000540632\n",
      "0.0005404243\n",
      "0.000546363\n",
      "0.0005348396\n",
      "0.000539381\n",
      "0.000550559\n",
      "0.00053907564\n",
      "0.0005450393\n",
      "0.0005475655\n",
      "0.000537041\n",
      "0.00057313807\n",
      "0.0005416496\n",
      "0.0005414282\n",
      "0.0005573568\n",
      "0.00054492423\n",
      "0.0005353774\n",
      "0.00054452196\n",
      "0.0005420158\n",
      "0.0005423206\n",
      "0.00054271735\n",
      "0.0005418287\n",
      "0.0005469774\n",
      "0.0005445397\n",
      "0.0005466552\n",
      "0.00054705417\n",
      "0.00053828815\n",
      "0.0005433187\n",
      "0.0005427878\n",
      "0.000565773\n",
      "0.0005438428\n",
      "0.00057068287\n",
      "0.00053409726\n",
      "0.0005411689\n",
      "0.0005386456\n",
      "0.0005428346\n",
      "0.00054634083\n",
      "0.00055786414\n",
      "0.0005442526\n",
      "0.0005416938\n",
      "0.00057284493\n",
      "0.0005409687\n",
      "0.00054349797\n",
      "0.0005408581\n",
      "0.0005481258\n",
      "0.0005441435\n",
      "0.00053987815\n",
      "0.00054134347\n",
      "0.00054915494\n",
      "0.00054977974\n",
      "0.0005442168\n",
      "0.0005602539\n",
      "0.00055261166\n",
      "0.00054114696\n",
      "0.00054061506\n",
      "0.0005568103\n",
      "0.00054096634\n",
      "0.0005499897\n",
      "0.0006416262\n",
      "0.000556648\n",
      "0.00054997567\n",
      "0.0005328904\n",
      "0.0005434723\n",
      "0.0005376292\n",
      "0.0005393671\n",
      "0.00054888014\n",
      "0.0005416132\n",
      "0.00054693245\n",
      "0.00055934157\n",
      "0.0005469718\n",
      "0.00055666093\n",
      "0.0005500717\n",
      "0.00054459204\n",
      "0.00054373575\n",
      "0.00054350105\n",
      "0.0005387624\n",
      "0.0005372766\n",
      "0.00054225855\n",
      "0.0005619396\n",
      "0.00053578574\n",
      "0.0005395359\n",
      "0.0005457968\n",
      "0.0005395631\n",
      "0.00053813815\n",
      "0.00054246077\n",
      "0.0005454129\n",
      "0.00053766684\n",
      "0.0005429065\n",
      "0.00054323307\n",
      "0.00053941086\n",
      "0.00053855835\n",
      "0.0005480846\n",
      "0.00053972314\n",
      "0.00054480176\n",
      "0.0005399602\n",
      "0.00061951095\n",
      "0.0005444077\n",
      "0.00053509587\n",
      "0.0005412924\n",
      "0.00054745254\n",
      "0.0005398531\n",
      "0.0005427782\n",
      "0.0005520792\n",
      "0.00054744084\n",
      "0.00053813745\n",
      "0.00054147077\n",
      "0.00053742895\n",
      "0.0005587334\n",
      "0.00054811727\n",
      "0.00060109264\n",
      "0.00054378406\n",
      "0.0005402127\n",
      "0.00053659757\n",
      "0.0005703713\n",
      "0.00054514786\n",
      "0.0005491832\n",
      "0.00054128584\n",
      "0.0005452379\n",
      "0.0005401137\n",
      "0.0005357259\n",
      "0.00053065276\n",
      "0.0005474352\n",
      "0.0005369643\n",
      "0.0005535777\n",
      "0.0005427063\n",
      "0.00054470194\n",
      "0.0005482268\n",
      "0.0005348211\n",
      "0.00054707704\n",
      "0.00054329593\n",
      "0.00054229953\n",
      "0.0005534012\n",
      "0.00054281746\n",
      "0.0005421776\n",
      "0.000550066\n",
      "0.0005630074\n",
      "0.000542412\n",
      "0.0005422994\n",
      "0.0005559444\n",
      "0.0005390159\n",
      "0.0005552448\n",
      "0.00054725574\n",
      "0.0005387313\n",
      "0.00053683476\n",
      "0.00054844643\n",
      "0.0005464892\n",
      "0.0005520675\n",
      "0.00054372713\n",
      "0.0005375352\n",
      "0.0005428454\n",
      "0.0005411102\n",
      "0.0005363423\n",
      "0.0005451579\n",
      "0.00054071314\n",
      "0.0005448054\n",
      "0.0005499183\n",
      "0.00054601335\n",
      "0.00053687446\n",
      "0.0005366554\n",
      "0.0005356402\n",
      "0.0005393182\n",
      "0.00054420345\n",
      "0.00054959464\n",
      "0.00055492576\n",
      "0.00054494035\n",
      "0.00055357127\n",
      "0.0005418632\n",
      "0.0005413279\n",
      "0.00057008595\n",
      "0.0005407124\n",
      "0.0005463882\n",
      "0.00054579636\n",
      "0.00054003083\n",
      "0.00054411776\n",
      "0.0005426005\n",
      "0.0005456686\n",
      "0.000537848\n",
      "0.0005351101\n",
      "0.0005427303\n",
      "0.00054490595\n",
      "0.0005363134\n",
      "0.0005465944\n",
      "0.0005475276\n",
      "0.0005458991\n",
      "0.0005410088\n",
      "0.000530906\n",
      "0.0005413323\n",
      "0.00053675316\n",
      "0.00053667283\n",
      "0.00054046256\n",
      "0.00053971476\n",
      "0.00055661716\n",
      "0.00054343\n",
      "0.0005399695\n",
      "0.0005446929\n",
      "0.0005444318\n",
      "0.0005473992\n",
      "0.0005668694\n",
      "0.0005344857\n",
      "0.0005522312\n",
      "0.00054314686\n",
      "0.00055359874\n",
      "0.00053853734\n",
      "0.00054210564\n",
      "0.0005545991\n",
      "0.00055167155\n",
      "0.0005463668\n",
      "0.0005519978\n",
      "0.00054379855\n",
      "0.00053634134\n",
      "0.0005509754\n",
      "0.0005503622\n",
      "0.0005491423\n",
      "0.0005404716\n",
      "0.00055356545\n",
      "0.00054647465\n",
      "0.00054086547\n",
      "0.0005368016\n",
      "0.0005546159\n",
      "0.0005428072\n",
      "0.00054361735\n",
      "0.00054553594\n",
      "0.00054580555\n",
      "0.0005433713\n",
      "0.0005436042\n",
      "0.00055101374\n",
      "0.0005478652\n",
      "0.0005458111\n",
      "0.0005584481\n",
      "0.0005411121\n",
      "0.00054249616\n",
      "0.0006451758\n",
      "0.0005445333\n",
      "0.0005417986\n",
      "0.00055208284\n",
      "0.00053952925\n",
      "0.0005798482\n",
      "0.0005397504\n",
      "0.00054216833\n",
      "0.00053838716\n",
      "0.0005413525\n",
      "0.00054061017\n",
      "0.0005441447\n",
      "0.00054046453\n",
      "0.00054844393\n",
      "0.00053607055\n",
      "0.00059900095\n",
      "0.0005470049\n",
      "0.0005390588\n",
      "0.0005384953\n",
      "0.0005431889\n",
      "0.00055102294\n",
      "0.00054485985\n",
      "0.0005509029\n",
      "0.0005457236\n",
      "0.00053871184\n",
      "0.0005393949\n",
      "0.00053358334\n",
      "0.0005407897\n",
      "0.0005584509\n",
      "0.0005335\n",
      "0.0005405661\n",
      "0.00053914817\n",
      "0.0005376494\n",
      "0.0005448173\n",
      "0.00054129056\n",
      "0.00054172624\n",
      "0.00053741224\n",
      "0.00055043714\n",
      "0.0005495644\n",
      "0.0005384411\n",
      "0.0005487986\n",
      "0.00053585577\n",
      "0.00054986426\n",
      "0.00054199673\n",
      "0.0005762671\n",
      "0.00054047274\n",
      "0.0005450595\n",
      "0.0005569506\n",
      "0.000555516\n",
      "0.00061716\n",
      "0.0005515308\n",
      "0.0005441841\n",
      "0.0005312305\n",
      "0.00054161437\n",
      "0.00054859027\n",
      "0.00053608126\n",
      "0.0005508714\n",
      "0.00054200983\n",
      "0.00054152816\n",
      "0.00054365344\n",
      "0.00057927746\n",
      "0.0005488311\n",
      "0.0005679306\n",
      "0.00054103835\n",
      "0.0005599656\n",
      "0.00054901786\n",
      "0.00053646346\n",
      "0.0005383032\n",
      "0.0005451758\n",
      "0.00059276604\n",
      "0.00055334513\n",
      "0.00055700185\n",
      "0.0005415448\n",
      "0.0005481392\n",
      "0.00054496183\n",
      "0.00054015126\n",
      "0.00054543087\n",
      "0.00055077486\n",
      "0.0005475413\n",
      "0.0005445207\n",
      "0.00055799\n",
      "0.000533474\n",
      "0.0005452422\n",
      "0.00054005586\n",
      "0.0005449271\n",
      "0.0005851318\n",
      "0.0005525345\n",
      "0.00054190145\n",
      "0.00054343673\n",
      "0.0005382974\n",
      "0.0005412955\n",
      "0.0005432179\n",
      "0.00053799315\n",
      "0.00053936365\n",
      "0.0005549208\n",
      "0.00054341747\n",
      "0.00055097195\n",
      "0.0005402778\n",
      "0.0005395046\n",
      "0.0005372872\n",
      "0.0005396075\n",
      "0.0006503688\n",
      "0.0005418972\n",
      "0.00054273015\n",
      "0.00054568145\n",
      "0.0005495966\n",
      "0.0005546021\n",
      "0.00054539926\n",
      "0.0005449885\n",
      "0.00054986536\n",
      "0.00054476364\n",
      "0.00054072856\n",
      "0.0005483147\n",
      "0.00054125587\n",
      "0.000549383\n",
      "0.00054278073\n",
      "0.0005389702\n",
      "0.00053670077\n",
      "0.0005463865\n",
      "0.00054938736\n",
      "0.0005464475\n",
      "0.00054413517\n",
      "0.000545346\n",
      "0.0005825301\n",
      "0.00054871273\n",
      "0.0005431926\n",
      "0.0005432698\n",
      "0.0005428384\n",
      "0.0005785178\n",
      "0.0005483924\n",
      "0.00054500846\n",
      "0.00055449776\n",
      "0.0005400076\n",
      "0.0005401533\n",
      "0.0005469079\n",
      "0.0005411253\n",
      "0.0005376456\n",
      "0.0005495189\n",
      "0.00053750724\n",
      "0.00054031715\n",
      "0.0005446414\n",
      "0.0005764493\n",
      "0.00055290083\n",
      "0.0005584519\n",
      "0.00054653455\n",
      "0.00054700737\n",
      "0.00055830576\n",
      "0.0005596476\n",
      "0.00053951435\n",
      "0.0005360701\n",
      "0.00053720205\n",
      "0.000540231\n",
      "0.00055312575\n",
      "0.00054943375\n",
      "0.0005785037\n",
      "0.00054037047\n",
      "0.00053276366\n",
      "0.0005389892\n",
      "0.0005504081\n",
      "0.0005418335\n",
      "0.0005511203\n",
      "0.00055705645\n",
      "0.00054441724\n",
      "0.0005367306\n",
      "0.00054979144\n",
      "0.00053778687\n",
      "0.0005343468\n",
      "0.0005327942\n",
      "0.0005452599\n",
      "0.00054223055\n",
      "0.0005429646\n",
      "0.00054570654\n",
      "0.00065264426\n",
      "0.00054759154\n",
      "0.00054168515\n",
      "0.00054983626\n",
      "0.0005407038\n",
      "0.0005435461\n",
      "0.0005448294\n",
      "0.0005504934\n",
      "0.0005447892\n",
      "0.0005461627\n",
      "0.0005442988\n",
      "0.0005468565\n",
      "0.0005456753\n",
      "0.0005599811\n",
      "0.0005389784\n",
      "0.0005455084\n",
      "0.0005358066\n",
      "0.0005491044\n",
      "0.00054237\n",
      "0.00054567005\n",
      "0.0005612149\n",
      "0.0005408713\n",
      "0.0005505537\n",
      "0.0005451732\n",
      "0.0005557935\n",
      "0.00054514786\n",
      "0.0005453024\n",
      "0.00054080377\n",
      "0.00054488826\n",
      "0.00055040506\n",
      "0.0005787141\n",
      "0.0005390123\n",
      "0.00054872583\n",
      "0.0005576169\n",
      "0.0005463676\n",
      "0.0005652609\n",
      "0.0005494681\n",
      "0.0005432746\n",
      "0.00054580957\n",
      "0.0005454809\n",
      "0.0005609887\n",
      "0.00054218556\n",
      "0.0005418183\n",
      "0.0005485822\n",
      "0.0005386015\n",
      "0.00055409595\n",
      "0.0005518511\n",
      "0.00055289216\n",
      "0.0005464208\n",
      "0.0005488109\n",
      "0.0005555435\n",
      "0.0005508765\n",
      "0.00055518857\n",
      "0.0005424861\n",
      "0.00054052804\n",
      "0.0005424173\n",
      "0.00054812664\n",
      "0.00055297976\n",
      "0.00054606487\n",
      "0.00054294814\n",
      "0.0005393486\n",
      "0.0005736697\n",
      "0.00054237584\n",
      "0.0005489452\n",
      "0.0005413758\n",
      "0.0005466225\n",
      "0.00061712775\n",
      "0.00056161365\n",
      "0.00054660294\n",
      "0.00053633895\n",
      "0.00055569626\n",
      "0.0005419966\n",
      "0.0005557002\n",
      "0.00054056564\n",
      "0.00053622987\n",
      "0.0005592196\n",
      "0.00055096776\n",
      "0.0005480393\n",
      "0.0005467126\n",
      "0.0005795945\n",
      "0.0005746832\n",
      "0.00054833817\n",
      "0.00053818646\n",
      "0.0005404852\n",
      "0.00055315404\n",
      "0.0005377845\n",
      "0.00054587715\n",
      "0.0006111484\n",
      "0.0005481538\n",
      "0.0005366819\n",
      "0.00053828093\n",
      "0.00053693727\n",
      "0.0005573991\n",
      "0.00054844207\n",
      "0.0005412453\n",
      "0.00054577936\n",
      "0.00054729084\n",
      "0.0005382123\n",
      "0.00055036525\n",
      "0.00056988467\n",
      "0.00055445824\n",
      "0.00053822243\n",
      "0.0005574448\n",
      "0.00055056467\n",
      "0.0005364059\n",
      "0.0005449893\n",
      "0.0005611624\n",
      "0.00067216787\n",
      "0.0005376657\n",
      "0.0005589261\n",
      "0.00054441585\n",
      "0.0005498653\n",
      "0.0005800665\n",
      "0.0005497745\n",
      "0.00055404264\n",
      "0.0005464429\n",
      "0.0005393285\n",
      "0.00054162944\n",
      "0.00056288525\n",
      "0.0005359735\n",
      "0.0005404096\n",
      "0.0005401592\n",
      "0.00054527225\n",
      "0.00055732456\n",
      "0.000545641\n",
      "0.000536215\n",
      "0.000540414\n",
      "0.00054467714\n",
      "0.0005383304\n",
      "0.0005381535\n",
      "0.00054465537\n",
      "0.0005432368\n"
     ]
    }
   ],
   "source": [
    "scale_s = s\n",
    "for i in scale_s:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgryu/miniconda3/envs/effl_project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.229, 0.224, 0.225)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGENET_DEFAULT_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.004078466"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.048813503"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.004078466, 0.08444769, -0.49945402, 0.49980858)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc.mean(), loc.var(), loc.min(), loc.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(1,1, 1)\n",
    "\n",
    "ax.hist(loc, bins=100, density=True)\n",
    "# plt.title(p)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.018677436"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_version_path(cache_dir, model_name, branch = 'main'):\n",
    "    model_name_dir =  \"models--\" + model_name.replace('/', '--')\n",
    "    path = os.path.join(cache_dir, model_name_dir)\n",
    "\n",
    "    if not os.path.isdir(os.path.join(path, 'snapshots')):\n",
    "        return None\n",
    "    \n",
    "    branch_file =  os.path.join(path, 'refs', branch)\n",
    "\n",
    "    with open(branch_file, 'r', encoding='utf-8') as file:\n",
    "        revision = file.read()\n",
    "\n",
    "    return os.path.join(path, 'snapshots', revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "torch.Size([256000, 3072])\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.0.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.1.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.2.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.3.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.4.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.5.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.6.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.7.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.8.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.9.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.10.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.11.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.12.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.13.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.14.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.15.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.16.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.17.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.18.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.19.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.20.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.21.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.22.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.23.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.24.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.25.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.26.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "torch.Size([4096, 3072])\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "torch.Size([3072, 4096])\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "torch.Size([24576, 3072])\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "torch.Size([3072, 24576])\n",
      "model.layers.27.input_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "torch.Size([3072])\n",
      "model.norm.weight\n",
      "torch.Size([3072])\n",
      "lm_head.weight\n",
      "torch.Size([256000, 3072])\n"
     ]
    }
   ],
   "source": [
    "cache_path = \"/home/jgryu/Weight_compression/model_zoo/huggingface/\"\n",
    "# model_name =  \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "model_name =  \"google--codegemma-1.1-7b-it\"\n",
    "\n",
    "\n",
    "filepath = latest_version_path(cache_path, model_name)\n",
    "# print(filepath)\n",
    "if filepath is not None:\n",
    "    model = AutoModelForCausalLM.from_pretrained(filepath, local_files_only=True)\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        print(k)\n",
    "        print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "torch.Size([32000, 5120])\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.0.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.1.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.2.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.3.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.4.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.5.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.6.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.7.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.8.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.9.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.10.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.11.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.12.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.13.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.14.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.15.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.16.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.17.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.18.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.19.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.20.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.21.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.22.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.23.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.24.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.25.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.26.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.27.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.28.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.28.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.28.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.28.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.29.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.29.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.29.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.29.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.30.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.30.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.30.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.30.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.31.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.31.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.31.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.31.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.32.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.32.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.32.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.32.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.33.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.33.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.33.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.33.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.34.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.34.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.34.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.34.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.35.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.35.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.35.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.35.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.36.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.36.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.36.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.36.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.37.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.37.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.37.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.37.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.38.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.38.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.38.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.38.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.39.self_attn.q_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.k_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.v_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.o_proj.weight\n",
      "torch.Size([5120, 5120])\n",
      "model.layers.39.mlp.gate_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.up_proj.weight\n",
      "torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.down_proj.weight\n",
      "torch.Size([5120, 13824])\n",
      "model.layers.39.input_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.layers.39.post_attention_layernorm.weight\n",
      "torch.Size([5120])\n",
      "model.norm.weight\n",
      "torch.Size([5120])\n",
      "lm_head.weight\n",
      "torch.Size([32000, 5120])\n"
     ]
    }
   ],
   "source": [
    "cache_path = \"/home/jgryu/Weight_compression/model_zoo/huggingface/\"\n",
    "model_name =  \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# model_name =  \"google--codegemma-1.1-7b-it\"\n",
    "\n",
    "\n",
    "filepath = latest_version_path(cache_path, model_name)\n",
    "# print(filepath)\n",
    "if filepath is not None:\n",
    "    model = AutoModelForCausalLM.from_pretrained(filepath, local_files_only=True)\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        print(k)\n",
    "        print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFrecord dataset mean std계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 차원별 평균 값: [ 1.0062791e-05  4.0517407e-05 -1.0374568e-04 ...  1.5147983e-06\n",
      " -1.7613287e-04  4.0792006e-06]\n",
      "각 차원별 표준 편차: [0.01182688 0.01179839 0.01185048 ... 0.01183916 0.01147975 0.01184377]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# TFRecord 파일 경로\n",
    "tfrecord_file = '/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama/Meta-Llama-3-8B/mlp_d1024_val.tfrecord'\n",
    "\n",
    "# Feature 파싱 함수 정의\n",
    "def parse_tfrecord_fn(example_proto):\n",
    "    # TFRecord에 있는 데이터의 feature 정의 (1024차원 feature)\n",
    "    feature_description = {\n",
    "        'slice': tf.io.FixedLenFeature([1024], tf.float32),  # 1024차원 feature로 설정\n",
    "    }\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return parsed_example['slice']  # 사용할 feature 선택\n",
    "\n",
    "# TFRecord 데이터셋 로드\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "dataset = dataset.map(parse_tfrecord_fn)\n",
    "\n",
    "# 평균 및 표준 편차 계산 함수\n",
    "def calculate_mean_and_std(dataset, feature_dim=1024):\n",
    "    # 평균 계산\n",
    "    total_sum = tf.zeros([feature_dim], dtype=tf.float32)\n",
    "    total_count = 0\n",
    "    for value in dataset:\n",
    "        total_sum += value\n",
    "        total_count += 1\n",
    "    mean_vector = total_sum / total_count if total_count > 0 else tf.zeros([feature_dim], dtype=tf.float32)\n",
    "    \n",
    "    # 분산 계산\n",
    "    variance_sum = tf.zeros([feature_dim], dtype=tf.float32)\n",
    "    for value in dataset:\n",
    "        variance_sum += (value - mean_vector) ** 2\n",
    "    std_vector = tf.sqrt(variance_sum / total_count) if total_count > 0 else tf.zeros([feature_dim], dtype=tf.float32)\n",
    "    \n",
    "    return mean_vector, std_vector\n",
    "\n",
    "# 평균 및 표준 편차 계산\n",
    "mean_vector, std_vector = calculate_mean_and_std(dataset)\n",
    "print(f'각 차원별 평균 값: {mean_vector.numpy()}')\n",
    "print(f'각 차원별 표준 편차: {std_vector.numpy()}')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "path = '/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama/Meta-Llama-3-8B/mlp_d1024_val_mean_channel.npy'\n",
    "np.save(path, mean_vector.numpy())\n",
    "path = '/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama/Meta-Llama-3-8B/mlp_d1024_val_std_channel.npy'\n",
    "np.save(path, std_vector.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 평균 값: -5.42295056421355e-06\n",
      "전체 데이터 표준 편차: 0.011819059083636133\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# TFRecord 파일 경로\n",
    "tfrecord_file = '/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama/Meta-Llama-3-8B/mlp_d1024_val.tfrecord'\n",
    "\n",
    "# Feature 파싱 함수 정의\n",
    "def parse_tfrecord_fn(example_proto):\n",
    "    # TFRecord에 있는 데이터의 feature 정의 (1024차원 feature)\n",
    "    feature_description = {\n",
    "        'slice': tf.io.FixedLenFeature([1024], tf.float32),  # 1024차원 feature로 설정\n",
    "    }\n",
    "    parsed_example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return parsed_example['slice']  # 사용할 feature 선택\n",
    "\n",
    "# TFRecord 데이터셋 로드\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "dataset = dataset.map(parse_tfrecord_fn)\n",
    "\n",
    "# 전체 평균 및 표준 편차 계산 함수\n",
    "def calculate_overall_mean_and_std(dataset):\n",
    "    total_sum = 0.0\n",
    "    total_count = 0\n",
    "\n",
    "    # 모든 샘플의 합과 개수 계산\n",
    "    for value in dataset:\n",
    "        total_sum += tf.reduce_sum(value).numpy()  # 전체 feature의 합을 누적\n",
    "        total_count += value.shape[0]  # 전체 feature의 개수 누적 (1024씩 증가)\n",
    "\n",
    "    # 전체 평균\n",
    "    mean_value = total_sum / total_count if total_count > 0 else 0\n",
    "\n",
    "    # 분산 계산\n",
    "    variance_sum = 0.0\n",
    "    for value in dataset:\n",
    "        variance_sum += tf.reduce_sum((value - mean_value) ** 2).numpy()\n",
    "    std_value = (variance_sum / total_count) ** 0.5 if total_count > 0 else 0\n",
    "    \n",
    "    return mean_value, std_value\n",
    "\n",
    "# 전체 평균 및 표준 편차 계산\n",
    "mean_value, std_value = calculate_overall_mean_and_std(dataset)\n",
    "print(f'전체 데이터 평균 값: {mean_value}')\n",
    "print(f'전체 데이터 표준 편차: {std_value}')\n",
    "\n",
    "import numpy as np\n",
    "path = '/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama/Meta-Llama-3-8B/mlp_d1024_val_mean.npy'\n",
    "np.save(path, mean_vector.numpy())\n",
    "path = '/home/jgryu/Weight_compression/Wparam_dataset/TFRecord/meta-llama/Meta-Llama-3-8B/mlp_d1024_val_std.npy'\n",
    "np.save(path, std_vector.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 차원별 평균 값: [ 1.00627913e-05  4.05174069e-05 -1.03745682e-04  4.08225096e-05\n",
      "  8.00399866e-05 -1.20562154e-05 -2.35987063e-05 -2.57367574e-05\n",
      "  7.61593474e-05  5.54684520e-05 -1.53281144e-05  7.49116516e-05\n",
      " -2.25087315e-05 -3.83869847e-05 -1.02051898e-04 -4.26129351e-04\n",
      " -8.75328333e-05 -2.69055145e-05  1.21108875e-04 -6.66051201e-05\n",
      "  8.03597686e-06 -3.19661667e-05  1.24425942e-05 -1.93563164e-06\n",
      " -1.08507244e-04 -4.64593722e-05 -6.73827817e-05 -1.02629510e-05\n",
      "  1.95690245e-05 -3.20448089e-05 -8.45169052e-05  1.02131962e-05\n",
      "  3.89791021e-05 -4.59037219e-05 -3.86079773e-04 -3.76285170e-04\n",
      "  1.41589517e-05  5.69974873e-05 -6.35780380e-05 -9.97388634e-05\n",
      "  5.63533213e-05 -1.15002549e-04 -1.10231158e-04  1.29960347e-06\n",
      "  5.04176132e-05 -3.25167406e-07  7.75887474e-06  1.55667640e-05\n",
      "  1.11139678e-04 -4.07637235e-05  2.59978915e-05  7.74624641e-05\n",
      " -6.37498888e-05  7.44852805e-05 -1.34185420e-05 -7.11185976e-06\n",
      " -4.63900033e-05 -2.24426876e-05  6.21258596e-06  1.61783115e-04\n",
      " -8.30200224e-05 -6.26365072e-05 -3.08706913e-05  2.21793220e-04\n",
      "  5.67662973e-05  1.72305750e-04 -3.47865644e-05 -1.44414123e-04\n",
      "  3.25031666e-04  5.03794108e-05 -5.90772652e-05 -1.35837536e-05\n",
      " -1.19585020e-05  4.57548158e-05 -4.43767276e-05 -1.79119615e-05\n",
      " -8.25609895e-05  4.61315358e-06  1.08071017e-05  2.22253748e-05\n",
      " -1.08163713e-05 -1.19595548e-04 -5.41809641e-06  5.34673491e-05\n",
      "  4.52088316e-05 -6.54741089e-05 -1.00093268e-04 -1.42326826e-04\n",
      "  4.01554717e-05  6.17789992e-05 -5.24629868e-05  1.42920544e-04\n",
      "  8.31662692e-05 -3.34012184e-05 -6.72725510e-05  3.13409728e-05\n",
      "  1.23608203e-04 -5.40347457e-07  3.47623100e-05  1.26367813e-04\n",
      "  3.12063634e-07  3.68893125e-05 -1.04437740e-05  1.28631473e-05\n",
      "  2.78675725e-05  1.48215358e-05  6.52738572e-06 -7.16600844e-05\n",
      "  4.82766300e-05 -5.86918468e-05 -3.65278429e-05 -3.85667663e-04\n",
      " -5.38324493e-05 -2.97912466e-05  8.34371531e-05  9.74247450e-05\n",
      "  5.31204432e-06 -2.63228721e-05 -4.66835336e-05 -7.95093001e-05\n",
      " -6.81779202e-05 -1.54116169e-05  3.49503025e-05  6.42941177e-06\n",
      " -9.91237903e-05  7.77169189e-05 -7.05031880e-06  1.17538300e-04\n",
      "  1.04787832e-04  1.99046326e-06 -1.21128920e-04  2.56854946e-05\n",
      "  8.53300116e-06 -3.46318906e-04 -1.04560109e-04  4.97820620e-05\n",
      " -1.51893299e-04  7.27341103e-05  4.99084359e-04 -1.56035414e-04\n",
      "  2.24521373e-05 -1.10668543e-05 -6.78159558e-05  2.84778730e-06\n",
      " -9.25649583e-05 -1.19089316e-04  7.55821020e-05  3.97904005e-05\n",
      " -9.05024499e-05  1.55618527e-05 -5.80158776e-05 -7.18291340e-05\n",
      " -4.83366275e-05 -9.51366928e-07  9.85603547e-05 -4.00228746e-05\n",
      " -3.45830376e-05  1.95266730e-05  4.62674798e-05  3.35507182e-04\n",
      " -1.91159470e-05 -3.72481067e-04  1.84685632e-04  1.73317094e-05\n",
      " -1.23367281e-04  4.59660194e-04 -1.16296156e-04 -1.64171928e-04\n",
      "  1.47112705e-05 -1.65059129e-04 -3.29427239e-05 -3.02730768e-04\n",
      "  5.34278915e-05  9.21691390e-05  1.93221695e-05  1.03089887e-05\n",
      "  4.07875705e-05 -3.84087834e-05 -7.16913064e-06 -4.74625601e-08\n",
      " -1.03436396e-04 -1.06084641e-04 -8.44595997e-05 -5.90441268e-05\n",
      " -1.14260183e-05  3.84199157e-05  4.35832699e-05  1.69368068e-04\n",
      "  1.17016316e-04  1.30607645e-04 -1.81947198e-05  1.34416667e-04\n",
      " -3.03534466e-06  5.72509052e-05  5.38148219e-04 -2.26797383e-05\n",
      "  3.75950804e-05 -1.94410077e-05  2.87432413e-05 -1.17296935e-04\n",
      " -6.12350123e-05  5.45893963e-05  2.19235244e-05  4.42545570e-05\n",
      " -2.66090297e-04  6.70143781e-05  2.72414923e-06 -3.44803630e-05\n",
      "  2.48056131e-05 -5.24390853e-05  6.63676401e-05 -4.29230022e-05\n",
      " -9.81810808e-05 -3.18763487e-04 -3.12115816e-07 -5.28844721e-05\n",
      " -4.60224765e-05 -4.16403345e-04  8.03472067e-05  3.55444463e-05\n",
      "  9.03626133e-05  1.42624267e-05 -1.40640201e-04 -4.00953068e-05\n",
      " -6.12677832e-05  3.34589604e-05 -7.91425919e-06  8.80044627e-06\n",
      " -1.68378010e-05 -2.97358565e-06 -1.07263841e-05  1.21092300e-04\n",
      " -2.75805396e-05 -2.71360441e-05 -1.21450401e-04 -8.24657491e-06\n",
      "  9.33871124e-05 -1.02170430e-04  6.42836585e-06 -1.61489770e-05\n",
      " -1.37883944e-05  1.73913741e-05 -2.19756876e-05  2.96036087e-05\n",
      " -1.32530922e-05 -6.67408458e-05 -2.72932357e-05 -1.20059718e-04\n",
      "  4.27456871e-05 -5.70800657e-05 -5.01631948e-05  1.52499313e-04\n",
      "  4.49214094e-05 -5.62546711e-06 -5.98690440e-06  3.27059475e-04\n",
      "  4.74629123e-05  7.71443956e-05  2.09222726e-05 -4.56789076e-06\n",
      " -4.15862305e-05 -6.69458750e-05 -1.35730792e-04 -1.53998790e-05\n",
      "  7.89439873e-05 -7.81388371e-06  1.96385986e-06  7.86380042e-05\n",
      " -4.54432702e-05  5.78676736e-05  2.65017552e-05  1.74741635e-05\n",
      " -7.21888719e-05  8.01348215e-05 -4.57170419e-04 -1.18975935e-04\n",
      "  4.94976484e-05 -1.60385258e-04  2.59653134e-05  1.23140519e-04\n",
      "  4.47484745e-05  4.65675002e-05  2.41189045e-05  3.16537044e-05\n",
      "  2.27342807e-05  3.30373732e-05 -1.71971842e-04 -5.27256598e-05\n",
      "  1.63463064e-05 -1.89811879e-04 -2.97940249e-04  4.02462378e-04\n",
      " -3.33057216e-07 -3.35928853e-05 -5.03757437e-05 -5.62043861e-05\n",
      "  7.54074017e-06 -6.26006222e-05 -9.69587491e-05 -4.40698132e-05\n",
      "  2.09722043e-06  1.26864052e-05  1.73678882e-06  1.49993218e-06\n",
      "  3.17896192e-05  5.45176081e-05  4.26536826e-06  1.54618119e-05\n",
      " -9.04941407e-06 -2.90604250e-04  1.13907030e-04  9.04539957e-06\n",
      "  5.61621055e-05 -3.54279873e-05  1.84779681e-04 -5.63997928e-05\n",
      " -6.56858174e-05 -6.01369575e-05 -2.61305149e-06 -2.20639122e-05\n",
      "  5.98146617e-05 -7.92189021e-05 -1.27992840e-04 -5.20130488e-05\n",
      " -7.28452278e-06  1.71033709e-04 -8.04989249e-05  6.43574822e-05\n",
      "  7.78955837e-06  1.04418177e-05  1.67387873e-06 -1.08403088e-04\n",
      "  1.00542375e-05 -1.24336031e-04  1.91541694e-04 -1.14372997e-05\n",
      "  3.72082468e-05 -4.33322293e-06 -9.04958870e-05 -3.23593777e-05\n",
      " -6.34242169e-05 -2.89263844e-05  7.23277044e-05  1.06228254e-04\n",
      "  1.42568228e-04 -1.77680488e-06 -1.54972440e-04 -1.20515062e-04\n",
      "  3.31530609e-05  2.75600614e-05  3.48698813e-05  1.74144283e-04\n",
      "  5.79546540e-05 -8.91392301e-06  1.04189885e-05 -2.14889606e-05\n",
      " -2.56782751e-05 -1.73756227e-04  5.09668535e-05  1.87553323e-05\n",
      "  1.90528153e-04  2.54865190e-05  4.27131126e-05 -8.19378474e-05\n",
      " -6.18486083e-05  3.70508205e-05 -1.38216683e-05 -1.24924954e-05\n",
      " -5.36593725e-05  8.18807712e-06 -6.87354768e-05  3.43521824e-05\n",
      "  3.15569596e-05 -2.94003141e-04 -2.43473241e-05 -8.24159815e-06\n",
      "  9.06569039e-05  3.58290890e-05  7.81958079e-05 -2.21838172e-06\n",
      "  7.93667914e-06  5.73043872e-05  2.67947198e-05  2.53106464e-05\n",
      " -4.79149967e-05 -3.16195365e-05  1.97065601e-05 -2.88910323e-05\n",
      "  1.05248910e-05  1.06780797e-04 -8.32048390e-05 -1.74727538e-05\n",
      " -1.21653917e-04  5.20446738e-05  2.66697934e-05 -8.18620902e-05\n",
      " -4.43236604e-05  4.60281648e-04  5.86129354e-05 -4.56239577e-05\n",
      " -2.85986371e-05  5.83760720e-06 -3.09374038e-04 -4.86550271e-05\n",
      "  5.05374519e-05  6.14442342e-06  3.30309886e-05  2.84309444e-05\n",
      " -1.59707328e-04  4.01692232e-05 -1.78653150e-04  7.71679552e-05\n",
      "  4.32767811e-05 -4.13649432e-05 -7.04733684e-05 -2.68216609e-05\n",
      " -1.70290383e-04 -4.24246427e-05 -1.14027869e-04 -3.13174496e-05\n",
      " -1.11785730e-05 -5.21563488e-05  2.97274928e-05 -1.00331621e-04\n",
      "  2.50630168e-04  4.28176500e-05  1.27899359e-04 -1.48772669e-05\n",
      " -3.14002900e-05  3.97377589e-05 -5.88994917e-05  8.25049428e-05\n",
      "  2.05076776e-05 -2.61757596e-05  5.78623039e-05  3.66284949e-05\n",
      "  1.37478317e-04 -3.69126588e-04  2.27946017e-04  1.16103147e-04\n",
      "  2.10810540e-05  7.14616981e-05  1.05479914e-04  3.90772620e-05\n",
      " -3.25655419e-04 -6.55177937e-05 -2.84228317e-05 -8.39035492e-05\n",
      "  6.36155892e-05 -5.17977169e-05  7.58309252e-05  1.75102068e-05\n",
      "  7.11089233e-05 -2.56386338e-05  1.42493645e-05 -2.44653129e-05\n",
      "  5.77411884e-05  2.38365174e-05 -4.56010748e-05  2.62701578e-05\n",
      " -1.47685278e-05  8.30680074e-05 -4.84638804e-06 -1.31205481e-04\n",
      " -1.10062516e-04 -5.34105056e-05  3.60642916e-05  9.20484454e-05\n",
      " -4.34794711e-05  1.46612045e-04 -1.44228092e-04 -1.37429728e-04\n",
      "  1.07964559e-04  1.08102067e-04  1.23311174e-05 -3.43057254e-05\n",
      "  2.90370317e-05 -1.72574655e-04 -3.17962849e-06 -3.01627937e-04\n",
      "  1.33668305e-04  2.59183662e-06  2.67932446e-05 -4.27293562e-05\n",
      " -3.01657874e-05 -1.31582265e-05 -4.21319055e-05 -6.75027404e-05\n",
      "  4.37842891e-06  6.59776633e-05 -3.70408670e-05  1.38801643e-05\n",
      "  9.59186727e-05  7.07658546e-06 -6.87768443e-06  1.11156442e-04\n",
      " -5.78213112e-05  6.25896209e-05 -9.16624049e-05  4.40198310e-05\n",
      " -8.56665793e-06  6.59998113e-05  1.21367939e-05  5.77240207e-05\n",
      " -2.82545981e-04 -2.83769327e-06 -3.29449758e-05  7.77498656e-07\n",
      "  9.18723981e-06  1.53516594e-05 -3.37194469e-05  7.79896654e-05\n",
      " -3.07986804e-04 -5.32881531e-04 -6.92389440e-05  9.43578125e-05\n",
      " -5.49029428e-05 -3.45161534e-05  4.71063795e-05 -8.18706303e-06\n",
      " -4.52539534e-05  1.39480971e-05  1.68220913e-06 -1.57509217e-04\n",
      " -3.24944704e-05  8.11680002e-05 -4.54177680e-05  3.50029695e-05\n",
      " -1.02946316e-04  6.36046680e-05  7.18341289e-06  1.31309003e-04\n",
      " -1.96890571e-04  6.61158901e-06 -1.08674303e-05  4.71429412e-05\n",
      "  1.04765670e-04  1.66229856e-05  1.32427318e-04  4.91042229e-05\n",
      " -7.99177506e-05  3.31401650e-04 -6.59296638e-05  6.82942118e-05\n",
      "  4.59291368e-05 -1.68226288e-05  1.04659603e-06  3.01957189e-05\n",
      " -5.94997146e-05  2.16353747e-05  7.21926199e-07 -4.19753160e-05\n",
      " -1.60605559e-05  2.62432559e-06  5.87954819e-05  3.22718893e-06\n",
      "  2.97711886e-05  1.29994645e-04 -7.33872730e-05  2.19791225e-04\n",
      "  4.75130873e-05  1.73578213e-04  5.15836291e-05 -1.19704353e-04\n",
      "  5.72690951e-05  3.28470545e-04 -2.04789603e-06 -6.92601752e-05\n",
      "  3.21678875e-04 -3.32010018e-06 -1.71281776e-04 -1.00691823e-05\n",
      " -7.77262758e-05 -1.21078501e-05  5.33603816e-05  9.57142984e-05\n",
      "  5.85362686e-05  3.85399544e-05  1.17550109e-04  5.07892855e-06\n",
      " -8.42862719e-05  4.52998327e-04 -1.86468969e-05 -1.39796990e-04\n",
      "  3.45149710e-05 -4.62962525e-05  1.04367253e-04 -1.27536565e-04\n",
      "  1.14038494e-05  2.60673569e-05 -1.26991814e-04 -1.08672975e-04\n",
      "  1.96873734e-05 -8.94633922e-05 -1.91249219e-05 -5.91439821e-05\n",
      " -4.65923076e-05 -5.83397596e-05  4.96578519e-04  2.20950333e-05\n",
      " -1.20931691e-04 -1.58591429e-05 -4.59527073e-05 -9.45199281e-05\n",
      "  8.04901865e-05  7.46586957e-05 -7.65565928e-05 -4.11337387e-04\n",
      " -6.56194061e-06 -4.00330573e-05  4.13519701e-05 -9.50680842e-05\n",
      "  9.50199828e-05  8.16991742e-05 -1.35104696e-04  9.29020825e-05\n",
      "  2.90013140e-05  2.33009614e-05 -2.53545168e-05 -2.54798579e-05\n",
      " -1.31269262e-05  1.02706081e-05 -3.46361958e-05  1.77041402e-05\n",
      "  5.74599399e-05 -1.08925560e-05 -7.03108453e-05 -6.31268413e-05\n",
      " -2.34516142e-06  8.30766585e-05 -2.95882437e-05  1.02068443e-04\n",
      "  2.75345519e-04  2.95782265e-05 -4.36825474e-04 -2.60763081e-05\n",
      "  1.31691049e-04  6.60359728e-05 -1.99269562e-05  1.22968777e-04\n",
      " -3.62916471e-05  6.30269496e-05 -3.00696538e-05 -1.78629794e-04\n",
      "  3.12316086e-04  8.37559783e-05 -1.54112113e-05  7.12928450e-05\n",
      "  3.96353571e-05  4.35923284e-05 -1.00447767e-04 -4.55492955e-05\n",
      " -6.87144420e-05 -4.39924297e-05 -7.70549320e-07  3.17552585e-05\n",
      " -5.85178395e-06 -1.86699399e-04 -1.10841993e-05  2.78147604e-06\n",
      "  7.93978325e-05 -1.37762900e-05 -1.19421984e-05  2.25813001e-05\n",
      "  1.95822577e-05  9.94202564e-05 -4.41270386e-05 -1.08261356e-05\n",
      " -1.61899196e-04 -7.01636891e-05  2.83220161e-05 -4.97748806e-05\n",
      "  3.41608800e-04 -7.08236112e-05 -8.26436371e-05 -9.15494093e-05\n",
      "  1.04038780e-04 -1.84836663e-05  1.97855654e-04 -4.96435532e-05\n",
      " -2.02821648e-05  1.78886658e-05  3.02785978e-04 -2.52151112e-05\n",
      " -5.60444023e-05 -2.87648982e-05  7.57000598e-05  3.72089817e-05\n",
      " -5.94112607e-05 -7.83848227e-05 -3.47004534e-05 -8.88626455e-05\n",
      "  1.55645637e-06  1.22885149e-05  5.66263509e-04  2.71153652e-07\n",
      "  5.75188369e-06 -6.85121704e-05  9.62598278e-06 -1.23040314e-04\n",
      " -6.53458847e-05 -1.08525412e-04 -1.42887337e-04  1.00893652e-04\n",
      " -7.12969340e-05  1.73211884e-05 -3.19008213e-05  2.81693588e-04\n",
      " -4.92120453e-05  4.61968681e-04  1.20636139e-06  6.56849006e-05\n",
      "  1.78241084e-04 -2.59568551e-05  1.23621692e-04 -4.66697893e-05\n",
      " -4.58464856e-05 -3.31661831e-05  5.42683410e-05 -6.89770895e-05\n",
      " -7.80474948e-05 -1.21226567e-05  3.31588162e-05 -2.14192805e-05\n",
      "  5.54280814e-05 -1.05251624e-04 -2.71571679e-07 -5.87699242e-06\n",
      " -2.67559608e-05 -2.05184770e-05  7.29841940e-06  1.69589857e-05\n",
      " -2.71018598e-05 -2.39882502e-05 -7.51987973e-05 -2.82523033e-05\n",
      " -4.60140927e-06 -2.74216640e-04 -5.76245748e-06 -1.06241758e-04\n",
      "  1.88139922e-04 -6.03708031e-05 -2.42221213e-05  6.51162409e-05\n",
      " -8.69325086e-05  9.35519784e-05 -2.88378524e-05 -2.78357347e-05\n",
      " -7.04037666e-05  4.02220467e-05 -3.99672354e-05  1.15598341e-04\n",
      "  1.11849413e-05 -5.80337792e-06  6.98394524e-06  4.83340737e-05\n",
      " -2.89439813e-05 -6.99820666e-05 -8.80586413e-06  1.00612153e-04\n",
      "  6.23229698e-06 -6.99449040e-04  1.62364067e-05 -4.50881089e-05\n",
      "  3.91984795e-05 -1.25789797e-04 -7.76495654e-05  4.55517693e-06\n",
      "  4.45643818e-05 -2.80812692e-05  4.62936441e-05 -7.06328647e-05\n",
      " -2.76679748e-05  6.71755988e-05  5.32373888e-05  9.96279632e-05\n",
      "  3.85650055e-04 -1.68378610e-05 -5.14465682e-05 -3.66873428e-05\n",
      "  5.98724582e-05  3.05067195e-04 -5.39452449e-05 -2.91490505e-05\n",
      " -3.95051711e-06  1.08538752e-05 -1.02156027e-05 -4.48830251e-05\n",
      " -5.92131750e-04 -2.62330614e-05 -8.75432597e-05 -4.22749552e-04\n",
      "  1.75678342e-05 -4.83381846e-05 -1.69163905e-04  5.30166617e-05\n",
      " -1.45471437e-04  9.64295468e-05  2.76622977e-05 -1.08153341e-04\n",
      "  3.97700705e-06  6.31539137e-07 -6.12192016e-05 -6.29313872e-05\n",
      "  3.41087457e-06 -5.37259111e-05 -1.27579347e-04 -4.50455300e-05\n",
      " -7.27109436e-05 -3.54257179e-04 -2.07031117e-05  3.39248683e-04\n",
      " -1.20157478e-04 -1.02868158e-04 -5.06042961e-05 -6.33954187e-05\n",
      " -3.90759924e-05 -6.79296209e-05  1.24303806e-05  1.67462968e-05\n",
      " -1.20235651e-04 -2.96964317e-05 -1.44705555e-04 -7.81856070e-05\n",
      "  2.20326856e-05  8.97130158e-05 -1.48981553e-05  2.52810587e-05\n",
      "  1.10193150e-05  5.42335802e-05 -5.77805440e-05 -5.14495187e-05\n",
      "  1.87729805e-04 -4.99280250e-05 -1.52530512e-04  7.76886591e-05\n",
      "  8.79363324e-06 -2.68103060e-04 -5.56332338e-07  3.69954387e-06\n",
      " -2.53877166e-04  6.21345753e-05  5.79765365e-05  3.94552299e-05\n",
      "  2.38907141e-05 -8.87476999e-05 -2.30240003e-05 -2.68449330e-05\n",
      "  8.64446702e-05  7.22504192e-05 -6.64867139e-06 -9.13430704e-05\n",
      " -7.13708505e-05 -4.27830892e-05 -2.20705333e-05  7.45241778e-05\n",
      " -5.38016309e-07  3.31150368e-05 -5.19148398e-06 -1.37194918e-04\n",
      "  1.07918291e-04 -6.92214890e-05 -2.66505667e-05 -8.02380309e-05\n",
      " -9.75662842e-05  2.82340352e-05  2.41982329e-04  6.58842982e-05\n",
      " -4.21568329e-05  1.83687735e-05  7.76947636e-05 -6.32468873e-05\n",
      "  2.13697949e-05 -4.17756877e-04 -2.38963130e-05 -1.80145391e-04\n",
      "  1.26711793e-05  1.89803413e-05  3.66545537e-05  5.14515050e-05\n",
      " -8.57482446e-05 -5.58823303e-05  2.73156456e-05 -4.82528303e-05\n",
      "  4.61453310e-04  2.58568452e-05  7.71397845e-06  1.02169804e-04\n",
      "  2.95256123e-05 -5.56585692e-05 -3.97486656e-05 -3.94736599e-05\n",
      "  2.38956145e-05 -1.64489815e-04 -2.99159528e-05  4.14000126e-04\n",
      " -8.80560310e-06 -1.86712077e-05 -1.90434821e-06 -1.88906415e-04\n",
      "  1.83677930e-05  7.70294428e-05  2.45964347e-05 -9.43744235e-05\n",
      "  1.13453869e-04  1.86759316e-05 -7.86446617e-05 -4.24841437e-06\n",
      "  1.27703956e-04  6.95886192e-05 -6.71363232e-05  1.82520496e-04\n",
      "  4.37825394e-04 -6.49277863e-05  2.72971283e-05 -7.97713656e-05\n",
      "  3.61183156e-05  3.67569464e-06  2.34084418e-05 -4.42037599e-05\n",
      "  9.37063451e-05  8.19796041e-05 -6.01939610e-05  3.30934381e-05\n",
      "  1.90149294e-04  1.17523196e-04  3.66300956e-05  2.16949848e-04\n",
      "  3.96639334e-05  4.12125955e-04  2.00517432e-04  3.20632680e-05\n",
      "  2.19383510e-05  4.07036641e-05  1.97162390e-05 -5.59887048e-05\n",
      "  7.66040030e-05  7.79089387e-05  1.46372977e-05 -9.37737786e-05\n",
      " -5.17883564e-05  1.12257931e-05 -1.06254620e-05 -1.03254322e-04\n",
      " -5.90291747e-04  7.42390766e-05  1.36306582e-04 -1.01524936e-04\n",
      " -6.05175883e-05 -4.34208814e-05  7.15323986e-05  4.11402434e-05\n",
      "  7.38909512e-05  3.59590081e-06  1.12235961e-04 -7.67225138e-05\n",
      "  3.98550073e-05  2.90682110e-05 -6.37171033e-05  3.60966369e-05\n",
      " -1.11473957e-04 -2.18651003e-05 -1.03575508e-06  4.05888859e-05\n",
      "  1.50791366e-05 -1.48657555e-05 -5.20515059e-05  1.01243146e-04\n",
      " -1.99077622e-05  1.68107954e-06 -1.54625759e-05 -1.11357753e-04\n",
      "  1.55071713e-04 -1.18408119e-04  8.22225702e-05  2.53275357e-05\n",
      "  5.44683171e-05  3.17644881e-05 -8.40891589e-05 -1.32812827e-04\n",
      "  1.01159130e-05 -1.11763213e-04 -3.25157853e-05 -4.69227438e-04\n",
      " -5.49059332e-05  4.67985083e-05 -7.26306025e-05 -8.15149906e-05\n",
      "  2.23410243e-05 -2.21383507e-05  1.45688318e-04 -2.35234620e-04\n",
      "  5.13004052e-05 -1.01623657e-04 -5.49184078e-05 -4.35090369e-05\n",
      "  2.04878452e-05 -9.20264429e-05 -5.34650680e-05  4.21311815e-05\n",
      " -5.15806096e-05  4.29836327e-05  4.83400290e-05  8.44739043e-05\n",
      "  1.04031133e-04 -7.06095016e-05  4.72384454e-05 -4.69341285e-05\n",
      "  7.11383909e-05 -3.82441067e-05  1.35071678e-05 -8.00846974e-05\n",
      "  9.31532559e-05 -3.32469899e-05 -9.24549531e-05 -8.29959572e-06\n",
      " -3.18674603e-04 -1.01735219e-04  1.00942263e-04 -4.89755985e-05\n",
      "  2.03379634e-04  1.51479833e-06 -1.76132875e-04  4.07920061e-06]\n",
      "각 차원별 표준 편차: [0.01182688 0.01179839 0.01185048 0.01183327 0.01170557 0.01177592\n",
      " 0.01184144 0.0118383  0.01182236 0.01187069 0.01173935 0.01182129\n",
      " 0.01183375 0.01184653 0.01186696 0.01144271 0.01179016 0.01183821\n",
      " 0.01188873 0.01187345 0.01182468 0.01174073 0.01181839 0.01181111\n",
      " 0.01171062 0.01180714 0.01180017 0.01171929 0.01189421 0.01175948\n",
      " 0.01183307 0.01191654 0.01183062 0.01187848 0.01197393 0.01161283\n",
      " 0.01182454 0.01180568 0.0118112  0.01184011 0.01175279 0.01185971\n",
      " 0.01178423 0.01181236 0.01185698 0.01179656 0.01199004 0.0118916\n",
      " 0.01180494 0.01186294 0.01176253 0.01175495 0.01176971 0.01180556\n",
      " 0.01184209 0.01183686 0.01185908 0.01179921 0.01181254 0.01174299\n",
      " 0.01184366 0.01181789 0.01183737 0.01171497 0.01183178 0.01189492\n",
      " 0.01186527 0.01191151 0.01159876 0.01184104 0.01181387 0.01182404\n",
      " 0.01183959 0.01173317 0.01181061 0.01188448 0.01186727 0.01195282\n",
      " 0.01184054 0.01186464 0.01173517 0.01184468 0.01178827 0.01182806\n",
      " 0.01181239 0.01173897 0.01180898 0.01169096 0.01180141 0.01184847\n",
      " 0.01186797 0.01183453 0.01176894 0.01185387 0.01185437 0.01181197\n",
      " 0.01182397 0.01182695 0.01176566 0.01173541 0.01188896 0.01180014\n",
      " 0.01176116 0.01165781 0.01184293 0.01183086 0.0118338  0.01182893\n",
      " 0.01185165 0.01180532 0.01178331 0.01171758 0.01182439 0.01187287\n",
      " 0.0117907  0.01180984 0.01179327 0.01184588 0.01186396 0.01179676\n",
      " 0.01185164 0.01184701 0.01187481 0.01185694 0.01176947 0.01171891\n",
      " 0.0118077  0.01180956 0.01182964 0.01185672 0.0118215  0.01184231\n",
      " 0.01182364 0.01151527 0.01187955 0.01176057 0.01178655 0.01174321\n",
      " 0.0115914  0.01173234 0.01182247 0.01182254 0.01188894 0.01180378\n",
      " 0.01179931 0.01183274 0.01182387 0.01183335 0.01185738 0.01179958\n",
      " 0.01184277 0.01181517 0.01179148 0.01186383 0.01185133 0.0118323\n",
      " 0.01188859 0.01182026 0.0117619  0.01153716 0.01183093 0.01192478\n",
      " 0.01178796 0.01186467 0.01198993 0.01158478 0.01178994 0.01182641\n",
      " 0.01179675 0.01182738 0.01186389 0.0118642  0.01187933 0.01182099\n",
      " 0.01177559 0.01166883 0.01183991 0.01184233 0.01180572 0.01183035\n",
      " 0.01178423 0.0118157  0.01189194 0.01181079 0.01185835 0.01182763\n",
      " 0.01178549 0.01166717 0.01183686 0.01177999 0.01179144 0.01188276\n",
      " 0.01187848 0.01183025 0.01148868 0.01187297 0.01183523 0.01181044\n",
      " 0.01183964 0.01176019 0.01178865 0.01185242 0.01184977 0.01183454\n",
      " 0.01192469 0.01171941 0.01185788 0.0118627  0.01184264 0.01176083\n",
      " 0.01181805 0.01185123 0.01165979 0.01175242 0.01190156 0.01183577\n",
      " 0.01182617 0.01164516 0.01182302 0.01180705 0.01185801 0.01183591\n",
      " 0.01182998 0.0118494  0.01184849 0.01180382 0.0117926  0.01183371\n",
      " 0.0118458  0.01188027 0.01183368 0.01184164 0.01180282 0.01183753\n",
      " 0.01181821 0.01178389 0.01173121 0.01182148 0.01180107 0.01186469\n",
      " 0.01185455 0.01185134 0.01184628 0.01184064 0.01179032 0.01183589\n",
      " 0.01182008 0.01184253 0.01182577 0.01184895 0.01186846 0.01181715\n",
      " 0.01176095 0.01186914 0.01178104 0.0114524  0.0118897  0.01182575\n",
      " 0.01185837 0.01180332 0.01192751 0.01183422 0.0118655  0.01177682\n",
      " 0.01177377 0.01183184 0.01177243 0.01183908 0.01185587 0.01183284\n",
      " 0.01180436 0.01184505 0.01172907 0.01183179 0.01158381 0.01177187\n",
      " 0.01185728 0.0118746  0.01183472 0.01182165 0.01182907 0.01163193\n",
      " 0.01171402 0.01184191 0.01183762 0.01181697 0.01175253 0.01182066\n",
      " 0.01184022 0.0118395  0.01136439 0.01139573 0.01189751 0.01184579\n",
      " 0.01181787 0.01182203 0.01181117 0.01180932 0.01172116 0.01179614\n",
      " 0.01184383 0.01174751 0.01182413 0.01183013 0.01155939 0.01184912\n",
      " 0.01180612 0.01184447 0.01183772 0.01186512 0.01179181 0.01185937\n",
      " 0.01182064 0.01184518 0.01158589 0.01181707 0.01180409 0.01184487\n",
      " 0.01185698 0.01182753 0.01187869 0.01188234 0.01182412 0.01183739\n",
      " 0.01181603 0.01187142 0.01181452 0.01184423 0.01182727 0.0118513\n",
      " 0.01181982 0.01185782 0.01184644 0.01180708 0.01180719 0.01188205\n",
      " 0.01187991 0.01183429 0.01188536 0.01184296 0.01181128 0.01178873\n",
      " 0.01183186 0.0118197  0.01181392 0.01185957 0.0117906  0.0118126\n",
      " 0.01184599 0.01186617 0.01182611 0.01184928 0.01182998 0.01179585\n",
      " 0.01188368 0.01181125 0.01187434 0.01183335 0.01189212 0.0118645\n",
      " 0.01189857 0.01181863 0.0118346  0.0118342  0.01183671 0.01178336\n",
      " 0.01186333 0.01181794 0.01185159 0.01184484 0.01179309 0.01180058\n",
      " 0.01180364 0.01168883 0.01173467 0.01181419 0.01176782 0.01183112\n",
      " 0.01186074 0.0118361  0.01181693 0.01183056 0.01184698 0.0118387\n",
      " 0.01181696 0.0118479  0.01180268 0.01186695 0.01181295 0.01180141\n",
      " 0.01181842 0.01182918 0.01182764 0.01180859 0.01180435 0.01189322\n",
      " 0.01186224 0.01178638 0.01181831 0.01178556 0.01180285 0.01182183\n",
      " 0.011968   0.01178391 0.01180453 0.0118074  0.01186441 0.011867\n",
      " 0.01177156 0.01182212 0.01173187 0.01179668 0.01182572 0.01177874\n",
      " 0.01179388 0.01182401 0.01171459 0.01183761 0.01183422 0.01179415\n",
      " 0.01182413 0.01180976 0.0118172  0.01188141 0.01184735 0.01182066\n",
      " 0.011829   0.01187014 0.01184634 0.0118419  0.01187196 0.01178874\n",
      " 0.01182738 0.01185489 0.01187371 0.01183137 0.01175278 0.01159267\n",
      " 0.01176569 0.01182722 0.01182634 0.01185076 0.01179305 0.0118207\n",
      " 0.01150909 0.01180054 0.01181038 0.01179012 0.01175036 0.01180661\n",
      " 0.01176737 0.01185667 0.01181394 0.01189622 0.01183877 0.01183245\n",
      " 0.01181802 0.01180615 0.01183007 0.01192769 0.01186539 0.01182803\n",
      " 0.0118391  0.01180241 0.01184939 0.01181801 0.01186832 0.01186007\n",
      " 0.01176921 0.01183085 0.01176639 0.01186431 0.01183372 0.01183357\n",
      " 0.01182637 0.01184932 0.0118178  0.01179401 0.01184263 0.01181126\n",
      " 0.01168905 0.01174375 0.01177987 0.01180489 0.01186483 0.01185249\n",
      " 0.01185115 0.01188331 0.01186405 0.01185991 0.01185229 0.01175769\n",
      " 0.01182667 0.01184039 0.01181055 0.01183445 0.01180438 0.01183085\n",
      " 0.01184427 0.01182825 0.01183987 0.01183798 0.01182906 0.01185979\n",
      " 0.01200763 0.01187867 0.01185664 0.01179969 0.01184114 0.01178704\n",
      " 0.01184419 0.01181347 0.01176916 0.01160992 0.01187662 0.01186462\n",
      " 0.01183019 0.01182668 0.0118652  0.01181427 0.01189658 0.01185224\n",
      " 0.01179778 0.01175627 0.01171383 0.0118561  0.01183012 0.01183906\n",
      " 0.01181096 0.0117692  0.01182665 0.01173351 0.01179596 0.01181686\n",
      " 0.01183989 0.01180686 0.01182991 0.01178647 0.01178103 0.0118131\n",
      " 0.01185359 0.01160828 0.01184835 0.01181394 0.0118046  0.01186319\n",
      " 0.01182092 0.01178621 0.01172687 0.0118414  0.01181508 0.01184987\n",
      " 0.01181797 0.01185706 0.01184086 0.01182405 0.01188293 0.0117047\n",
      " 0.01177077 0.0117855  0.01172918 0.01170663 0.0118331  0.01184799\n",
      " 0.01177685 0.01204668 0.01186648 0.0117946  0.0118554  0.0118449\n",
      " 0.01176536 0.01183949 0.01182548 0.01185529 0.01185326 0.01181704\n",
      " 0.0118432  0.01183212 0.01172004 0.01182583 0.01184828 0.01174046\n",
      " 0.01184507 0.01183879 0.01186674 0.01183046 0.01189428 0.01182361\n",
      " 0.01182052 0.01177671 0.01184987 0.01179187 0.01183969 0.01184479\n",
      " 0.01186097 0.0118522  0.0118015  0.01184248 0.01161779 0.01178349\n",
      " 0.01176767 0.01184657 0.01186925 0.01187393 0.01181191 0.01184182\n",
      " 0.01181833 0.01180154 0.01182404 0.01183468 0.01186132 0.01193299\n",
      " 0.01174192 0.01180679 0.0118505  0.01184904 0.01186108 0.01183993\n",
      " 0.01191062 0.01182237 0.01181702 0.01185078 0.01187591 0.01173339\n",
      " 0.01183332 0.01183497 0.01176449 0.01184339 0.01183587 0.01183146\n",
      " 0.01172792 0.01184105 0.01169257 0.01187155 0.0119776  0.01182771\n",
      " 0.01175003 0.01186509 0.01182519 0.0118194  0.01183507 0.01184078\n",
      " 0.01188767 0.01176308 0.01157589 0.01176921 0.01185236 0.01176088\n",
      " 0.01184369 0.01184951 0.01183364 0.01177222 0.01181838 0.01186639\n",
      " 0.01184964 0.01181649 0.01180304 0.01184474 0.01186901 0.01183624\n",
      " 0.01185095 0.01180585 0.01171222 0.01177608 0.01190767 0.01185569\n",
      " 0.01182333 0.0118162  0.01183562 0.01184033 0.01179963 0.01183754\n",
      " 0.01165877 0.01177971 0.01184183 0.01182087 0.01184252 0.01179935\n",
      " 0.01174045 0.01175644 0.01183644 0.01186209 0.01170161 0.0118045\n",
      " 0.01184432 0.0118694  0.01179394 0.01178986 0.0118269  0.01174746\n",
      " 0.01184981 0.0118827  0.01182467 0.01177971 0.01204445 0.0118364\n",
      " 0.01188176 0.01183693 0.01186942 0.01180145 0.01178704 0.01194946\n",
      " 0.01190102 0.01180697 0.01185637 0.01186704 0.01183216 0.01168044\n",
      " 0.01185068 0.01163844 0.01183583 0.01181759 0.0118532  0.01183021\n",
      " 0.01171588 0.01177063 0.01163197 0.01176789 0.0118576  0.01181384\n",
      " 0.01183443 0.01188037 0.01181608 0.01193904 0.01183993 0.01171386\n",
      " 0.01181385 0.0118165  0.01182237 0.01185721 0.01185098 0.01182352\n",
      " 0.01175493 0.01181384 0.01178689 0.01182527 0.01179524 0.01184084\n",
      " 0.01177456 0.01186864 0.01182824 0.01176946 0.01178647 0.01180077\n",
      " 0.01181261 0.01182714 0.01183198 0.01182463 0.01183925 0.01170172\n",
      " 0.0117612  0.01180157 0.01180916 0.01179881 0.01184038 0.01180965\n",
      " 0.01179994 0.01178816 0.01182211 0.01186484 0.01180683 0.01205054\n",
      " 0.01181173 0.01179837 0.01182223 0.01168369 0.0117241  0.01182487\n",
      " 0.0118565  0.01187415 0.01181883 0.01183193 0.01187261 0.01183173\n",
      " 0.01178258 0.01177456 0.01165151 0.01181883 0.01186984 0.01183038\n",
      " 0.01184623 0.01186598 0.01190227 0.01182976 0.01192223 0.01178543\n",
      " 0.01186076 0.01184907 0.01165568 0.01182241 0.01184457 0.01154262\n",
      " 0.01181565 0.01180158 0.01184729 0.01182455 0.01181231 0.01165808\n",
      " 0.01181332 0.01179046 0.01184214 0.01177717 0.01190233 0.01185472\n",
      " 0.01185978 0.01183703 0.01180915 0.01182077 0.01179625 0.01155608\n",
      " 0.01191276 0.01167526 0.01185813 0.01181965 0.01185302 0.01182103\n",
      " 0.01179784 0.01179784 0.01182464 0.01185431 0.01183463 0.01187584\n",
      " 0.01174424 0.01185307 0.01181468 0.0118429  0.011836   0.01180585\n",
      " 0.01185481 0.01183658 0.01177912 0.01177055 0.01175239 0.01185294\n",
      " 0.01183408 0.01180834 0.01182984 0.01170065 0.01186081 0.0118382\n",
      " 0.01174519 0.01184618 0.0118187  0.01186056 0.0117948  0.01167415\n",
      " 0.01185387 0.01184398 0.01184362 0.01181522 0.01182398 0.01185058\n",
      " 0.01185066 0.01188486 0.01180259 0.01180216 0.0118687  0.01180246\n",
      " 0.01182374 0.01182035 0.01168615 0.01185015 0.01183693 0.0118712\n",
      " 0.01187075 0.01183996 0.01167064 0.01186564 0.01184305 0.01183677\n",
      " 0.01181824 0.0118258  0.01180075 0.01162163 0.01187617 0.01176347\n",
      " 0.0118606  0.01180627 0.01176786 0.01185049 0.01180556 0.01191592\n",
      " 0.01186283 0.01180763 0.01158252 0.01184059 0.01183206 0.01182374\n",
      " 0.01181997 0.01189559 0.01190238 0.01176546 0.01181634 0.01197273\n",
      " 0.01183051 0.01161369 0.01185441 0.01183343 0.01183208 0.01185641\n",
      " 0.01185298 0.01178979 0.01184613 0.01181348 0.01186577 0.01183882\n",
      " 0.01184549 0.01182553 0.01169404 0.01185442 0.0119103  0.0117103\n",
      " 0.01155693 0.01186929 0.011822   0.01187761 0.01184526 0.01184683\n",
      " 0.01183298 0.01184153 0.01178042 0.01179538 0.01180919 0.01183678\n",
      " 0.01184554 0.01176041 0.01178442 0.01187563 0.0118361  0.01158514\n",
      " 0.01174623 0.01175073 0.01179892 0.01184278 0.01182316 0.01184594\n",
      " 0.01182516 0.01184225 0.01182759 0.01185447 0.01186766 0.01189136\n",
      " 0.01181296 0.01182169 0.01179374 0.01180388 0.01182787 0.01204101\n",
      " 0.01182626 0.01183362 0.01173421 0.01187787 0.01186847 0.01188183\n",
      " 0.01181612 0.01183496 0.01172896 0.01185372 0.01178077 0.0118234\n",
      " 0.01167031 0.01173794 0.0118269  0.01181865 0.01186608 0.01184514\n",
      " 0.0118181  0.01187313 0.01189802 0.01182861 0.01181375 0.01178153\n",
      " 0.01179234 0.01185746 0.01174473 0.01180775 0.01173287 0.01187948\n",
      " 0.01192966 0.01177784 0.01185188 0.01159065 0.0118705  0.01130862\n",
      " 0.01184933 0.01181203 0.0117153  0.01184212 0.01176073 0.01182954\n",
      " 0.01191188 0.01167578 0.01184746 0.01185015 0.01187204 0.01183154\n",
      " 0.0118414  0.01183358 0.01184453 0.01171422 0.01183516 0.01181566\n",
      " 0.01180012 0.01183455 0.01185667 0.01183544 0.01185487 0.01179366\n",
      " 0.01183436 0.01181665 0.01184393 0.01183994 0.01184009 0.01181578\n",
      " 0.01192464 0.01180241 0.01169563 0.01189108 0.01177873 0.01173806\n",
      " 0.01191392 0.01183916 0.01147975 0.01184377]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print(f'각 차원별 평균 값: {mean_vector.numpy()}')\n",
    "print(f'각 차원별 표준 편차: {std_vector.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
