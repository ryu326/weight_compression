{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec63ff3-4227-410e-8d13-0917d44947e9",
   "metadata": {},
   "source": [
    "# Estimating sandwich bounds on the rate-distortion function of a Gaussian source\n",
    "\n",
    "This notebook demonstrates the method proposed in \n",
    "\n",
    ">  \"Towards Empirical Sandwich Bounds on the Rate-Distortion Function\"\n",
    ">\n",
    ">  Yibo Yang, Stephan Mandt\n",
    ">\n",
    ">  [https://arxiv.org/abs/2111.12166](https://arxiv.org/abs/2111.12166)\n",
    "\n",
    "on a randomly generated 2-dimensional Gaussian distribution.\n",
    "Also see [blog post](https://yiboyang.com/posts/estimating-the-rate-distortion-function-of-real-world-data-part-1/) for an introduction to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ae9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575d12c-2319-4417-aeba-f1a33d1ce740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "dtype = np.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524a723-c6e3-4e9f-b1da-870bcd79389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71ee68-a77f-47fa-bce1-fba23e7997cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "# module_name = \"RD-sandwich.utils\"\n",
    "# utils = importlib.import_module(module_name)\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a367b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection, ViTForImageClassification, AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy\n",
    "\n",
    "from huggingface_hub import try_to_load_from_cache, _CACHED_NO_EXIST\n",
    "from huggingface_hub import scan_cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa3bf2-be81-4c42-afd4-e2f97379031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "seed = 0\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364a400-ccbb-47ef-a909-0acd747a95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We keep lambda fixed throughout the notebook; rerunning with different lamb traces out the full UB/LB\n",
    "lamb = 3.0\n",
    "batchsize = 256\n",
    "data_dim = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccdf6f-9724-4c1f-9750-d705a72ea35e",
   "metadata": {},
   "source": [
    "## Prepare Custom dataset from .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_np_datasets(np_file, batchsize, append_channel_dim=False, get_validation_data=True):\n",
    "    assert np_file.endswith('.npy') or np_file.endswith('.npz')\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "\n",
    "    def get_dataset(ar_path, repeat):\n",
    "        X = np.load(ar_path).astype('float32')\n",
    "        if append_channel_dim:  # convolutional models often require data to have a channel dim\n",
    "            X = X[..., np.newaxis]\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "        dataset = dataset.shuffle(len(X), reshuffle_each_iteration=True)\n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()\n",
    "        dataset = dataset.batch(batchsize)\n",
    "        return dataset\n",
    "\n",
    "    train_dataset = get_dataset(np_file, repeat=False)\n",
    "\n",
    "    if not get_validation_data:\n",
    "        return train_dataset\n",
    "    else:\n",
    "        validation_dataset = None\n",
    "        if 'train' in np_file:  # dataset named as such comes with a validation set\n",
    "            val_dataset = None\n",
    "            if os.path.isfile(np_file.replace('train', 'val')):\n",
    "                val_dataset = np_file.replace('train', 'val')\n",
    "            elif os.path.isfile(np_file.replace('train', 'test')):\n",
    "                val_dataset = np_file.replace('train', 'test')\n",
    "            if val_dataset:\n",
    "                validation_dataset = get_dataset(val_dataset, repeat=False)\n",
    "                print(f'Validating on {val_dataset}')\n",
    "\n",
    "        if validation_dataset is None:\n",
    "            print(f\"Couldn't find validation data for {np_file}; validating on a subset of train data\")\n",
    "            validation_dataset = train_dataset\n",
    "        return train_dataset, validation_dataset\n",
    "\n",
    "path_dataset = \"/home/jgryu/Weight_compression/model_parm_dataset/gemma_2b_attn_d=128.npy\"\n",
    "dataset = get_np_datasets(path_dataset, batchsize=batchsize, get_validation_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1b580-c531-4e27-90a3-b8255f22b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.random.seed(seed)\n",
    "# loc = np.random.uniform(-0.5, 0.5, data_dim).astype(dtype)\n",
    "# var = np.random.uniform(0, 2, data_dim).astype(dtype)\n",
    "# scale = var ** 0.5\n",
    "# source = tfd.Normal(loc=loc, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7613fe9-92eb-42ed-b226-bace194ac794",
   "metadata": {},
   "source": [
    "## Estimate a point on the upper bound of the R-D curve, by training a VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f8d7f-4151-4589-a248-18ac76412917",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"encoder_units\": [],\n",
    "    \"encoder_activation\": None,\n",
    "    \"decoder_units\": [\n",
    "        10\n",
    "    ],\n",
    "    \"decoder_activation\": \"leaky_relu\",\n",
    "    \"posterior_type\": \"gaussian\",\n",
    "    \"prior_type\": \"gmm_1\",  # factorized Gaussian prior with learned mean and scale\n",
    "    \"maf_stacks\": 0,\n",
    "    \"ar_activation\": None,\n",
    "\n",
    "    \"batchsize\": batchsize,\n",
    "    \"data_dim\": data_dim,\n",
    "    \"latent_dim\": data_dim,\n",
    "\n",
    "    \"lmbda\": lamb,\n",
    "    \"lr\": 5e-4,\n",
    "\n",
    "    \"epochs\": 20,\n",
    "    \"seed\": 0,\n",
    "    \"steps_per_epoch\": 10000,\n",
    "    \"max_validation_steps\": 1000,\n",
    "    \"nats\": True,\n",
    "    \"rpd\": False  # do not scale lambda by the source dimension\n",
    "}\n",
    "args = utils.get_args_as_obj(args)\n",
    "\n",
    "# module_name = \"RD-sandwich.rdub_mlp\"\n",
    "# rdub_mlp = importlib.import_module(module_name)\n",
    "import rdub_mlp\n",
    "ub_model = rdub_mlp.Model.create_model(args)\n",
    "ub_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=args.lr),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4699f7db-8536-4a99-b920-d287dffe4566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43200/43200 [==============================] - 138s 3ms/step - loss: 0.0058 - rate: 0.0053 - mse: 1.7515e-04 - lr: 5.0000e-04\n",
      "Epoch 3/20\n",
      "43200/43200 [==============================] - 148s 3ms/step - loss: 0.0058 - rate: 0.0053 - mse: 1.7515e-04 - lr: 5.0000e-04\n",
      "Epoch 4/20\n",
      "43200/43200 [==============================] - 143s 3ms/step - loss: 0.0058 - rate: 0.0052 - mse: 1.7515e-04 - lr: 5.0000e-04\n",
      "Epoch 5/20\n",
      "43200/43200 [==============================] - 137s 3ms/step - loss: 0.0058 - rate: 0.0052 - mse: 1.7515e-04 - lr: 5.0000e-04\n",
      "Epoch 6/20\n",
      "43200/43200 [==============================] - 147s 3ms/step - loss: 0.0057 - rate: 0.0052 - mse: 1.7515e-04 - lr: 5.0000e-04\n",
      "Epoch 7/20\n",
      "43200/43200 [==============================] - 145s 3ms/step - loss: 0.0057 - rate: 0.0052 - mse: 1.7515e-04 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "43200/43200 [==============================] - 145s 3ms/step - loss: 0.0057 - rate: 0.0052 - mse: 1.7515e-04 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "43200/43200 [==============================] - 146s 3ms/step - loss: 0.0057 - rate: 0.0052 - mse: 1.7515e-04 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "43200/43200 [==============================] - 137s 3ms/step - loss: 0.0057 - rate: 0.0052 - mse: 1.7515e-04 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "43200/43200 [==============================] - 154s 3ms/step - loss: 0.0026 - rate: 0.0021 - mse: 1.7498e-04 - lr: 1.0000e-04\n",
      "Epoch 12/20\n",
      "43200/43200 [==============================] - 146s 3ms/step - loss: 0.0018 - rate: 0.0013 - mse: 1.7498e-04 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "43200/43200 [==============================] - 147s 3ms/step - loss: 0.0016 - rate: 0.0011 - mse: 1.7498e-04 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "43200/43200 [==============================] - 135s 3ms/step - loss: 0.0016 - rate: 0.0011 - mse: 1.7498e-04 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "43200/43200 [==============================] - 148s 3ms/step - loss: 0.0016 - rate: 0.0010 - mse: 1.7498e-04 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "43200/43200 [==============================] - 152s 3ms/step - loss: 0.0011 - rate: 5.6085e-04 - mse: 1.7494e-04 - lr: 2.0000e-05\n",
      "Epoch 17/20\n",
      "43200/43200 [==============================] - 143s 3ms/step - loss: 9.8640e-04 - rate: 4.6158e-04 - mse: 1.7494e-04 - lr: 2.0000e-05\n",
      "Epoch 18/20\n",
      "43200/43200 [==============================] - 147s 3ms/step - loss: 9.3839e-04 - rate: 4.1357e-04 - mse: 1.7494e-04 - lr: 2.0000e-05\n",
      "Epoch 19/20\n",
      "43200/43200 [==============================] - 150s 3ms/step - loss: 8.3165e-04 - rate: 3.0685e-04 - mse: 1.7493e-04 - lr: 4.0000e-06\n",
      "Epoch 20/20\n",
      "43200/43200 [==============================] - 154s 3ms/step - loss: 8.0379e-04 - rate: 2.7899e-04 - mse: 1.7493e-04 - lr: 4.0000e-06\n"
     ]
    }
   ],
   "source": [
    "tmp_save_dir = './tmp/rdvae'\n",
    "lr_scheduler = rdub_mlp.get_lr_scheduler(args.lr, args.epochs, decay_factor=0.2)\n",
    "hist = ub_model.fit(\n",
    "    dataset.prefetch(tf.data.AUTOTUNE),\n",
    "    epochs=args.epochs,\n",
    "    # steps_per_epoch=args.steps_per_epoch,\n",
    "    validation_data=None,\n",
    "    validation_freq=1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.TerminateOnNaN(),\n",
    "        tf.keras.callbacks.experimental.BackupAndRestore(tmp_save_dir),\n",
    "        tf.keras.callbacks.LearningRateScheduler(lr_scheduler),\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "ub_records = hist.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49413a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ub_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece7fbf-8bab-4ef4-bf9f-f71e757a681c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Estimate a point on the R-D curve by running Blahut-Arimoto on the discretized source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623abb87-6eb0-44c8-86d9-dd8ad20584d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 100000\n",
    "# samples = source.sample(N).numpy()\n",
    "# bins = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889d399-aa11-4000-b908-9bbd38e34148",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the discretized source\n",
    "# fig, ax = plt.subplots(figsize=(5, 4))\n",
    "# h = ax.hist2d(samples[:, 0], samples[:, 1], bins=bins, cmap='viridis', weights=np.ones(N) / N) # plot freq: https://stackoverflow.com/a/16399202\n",
    "# fig.colorbar(h[3], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6687d441-7a46-4d2f-be96-a3833dc08cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the PMF of the discretized source, and set up the discretized alphabets\n",
    "# from ba import bin_edges_to_grid_pts, vectorized_mse, blahut_arimoto\n",
    "\n",
    "# n = samples.shape[1]\n",
    "# hist_res = np.histogramdd(samples, bins=bins)  # , range=(xlim, ylim))\n",
    "# counts = hist_res[0]  # joint counts\n",
    "# bin_edges = hist_res[1]  # length-n list of arrays, each array has length bins+1\n",
    "\n",
    "# grid_axes = [bin_edges_to_grid_pts(edges) for edges in bin_edges]\n",
    "\n",
    "# # Enumerate grid points corresponding to the histogram (using the center of each bin).\n",
    "# meshgrid = np.meshgrid(*grid_axes, indexing='ij')  # length-n list, one 'mesh' for each data dimension\n",
    "# grid_pts = np.dstack(meshgrid)  # each grid point (n-tuple) resides in the inner-most dimension\n",
    "# grid_pts_flat = np.reshape(grid_pts, [-1, n])  # preserve the inner-most dim while flatten the rest\n",
    "# counts_flat = counts.ravel()\n",
    "\n",
    "\n",
    "# good_pts_ind = (counts_flat != 0)\n",
    "# src_alphabet = grid_pts_flat[good_pts_ind]  # remove bins with zero samples from the source alphabet\n",
    "# src_dist = counts_flat[good_pts_ind]\n",
    "# src_dist /= src_dist.sum()\n",
    "\n",
    "# rep_alphabet = grid_pts_flat  # use all bins from the histogram for the reproduction alphabet\n",
    "# Rho = vectorized_mse(src_alphabet, rep_alphabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160105b1-d993-40be-8e70-2b3f2ac1c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ba_records, log_Q, log_q_y = blahut_arimoto(Rho=Rho, p_x=src_dist, steps=100, lamb=lamb, verbose=True, tol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819646d-5704-4057-9f83-cccd2e22d1f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Estimate a linear lower bound of the R-D curve, by training a neural network \"-log u\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114f1ef-18a3-480c-9232-f16a0d9ab4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_models import make_mlp\n",
    "lb_model = make_mlp(units=[20, 10, 1], activation='selu', name='mlp', input_shape=[data_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad77cc-42a9-428b-8c21-cf1cee2bc4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lb(log_u_model, nn_optimizer, train_dataset_iter, args):\n",
    "    # Simplified version of rdlb.train\n",
    "    records = []\n",
    "    from rdlb import optimize_y, compute_Ckobj\n",
    "    M = args.num_Ck_samples\n",
    "    log_M = np.log(float(M))\n",
    "\n",
    "    for step in range(-1, args.last_step):\n",
    "        # Run M many global optimization runs to draw M samples of C_k, in order to estimate the training objective\n",
    "        opt_ys = []\n",
    "        log_Ck_samples = []\n",
    "        x_M_batch = []  # a batch containing M minibatches of data samples, each minibatch consisting of k samples\n",
    "        for j in range(M):  # embarrassingly parallelizable\n",
    "            x = next(train_dataset_iter)\n",
    "            x_M_batch.append(x)\n",
    "\n",
    "            res = optimize_y(log_u_fun=log_u_model, x=x, lamb=args.lamb, num_steps=args.y_steps,\n",
    "                             lr=args.y_lr, tol=args.y_tol, init=args.y_init,\n",
    "                             quick_topn=args.y_quick_topn, verbose=False, chunksize=args.chunksize)\n",
    "            opt_y = res['opt_y']\n",
    "            opt_ys.append(opt_y)\n",
    "            log_Ck_samples.append(res['opt_log_supobj'])\n",
    "\n",
    "        if step == -1:  # the initial run is just to set the log expansion point alpha\n",
    "            # 'log_avg_Ck' here is just the avg of the M samples of Ck in log domain, for numerical reasons\n",
    "            # (and as a R.V., its expected value underestimates the true log E[C_k], just like in IWAE)\n",
    "            log_avg_Ck = tf.reduce_logsumexp(log_Ck_samples) - log_M\n",
    "            log_alpha = prev_log_avg_Ck = log_avg_Ck  # for next iter\n",
    "            continue\n",
    "\n",
    "        # Update log_alpha for next iter\n",
    "        beta = args.beta  # should be in [0, 1)\n",
    "        if abs(prev_log_avg_Ck - log_alpha) <= 10.0:  # heuristic, skips update if the new value would be too extreme\n",
    "            if beta == 0:\n",
    "                log_alpha = prev_log_avg_Ck\n",
    "            else:  # retain beta fraction of its current value, and update alpha with (1-beta) fraction of prev_log_Ck\n",
    "                # alpha = beta * alpha + (1-beta) * prev E[C_k]\n",
    "                log_alpha = tf.reduce_logsumexp(\n",
    "                    [log_alpha + tf.math.log(beta), prev_log_avg_Ck + tf.math.log(1 - beta)])\n",
    "                \n",
    "        # Estimate the RD LB objective and do gradient update\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_Ck_samples = []\n",
    "            log_us = []\n",
    "            for j in tf.range(M):\n",
    "                x = x_M_batch[j]\n",
    "                opt_y = opt_ys[j]\n",
    "                log_Ck, log_u = compute_Ckobj(log_u_fun=log_u_model, x=x, y=opt_y, lamb=lamb)\n",
    "                log_Ck_samples.append(log_Ck)\n",
    "                log_us.append(log_u)  # each is a length k tensor\n",
    "\n",
    "            log_avg_Ck = tf.reduce_logsumexp(log_Ck_samples) - log_M\n",
    "            log_us = tf.concat(log_us, axis=0)\n",
    "            E_log_u = tf.reduce_mean(log_us)\n",
    "            log_E_Ck_est = tf.math.exp(\n",
    "                log_avg_Ck - log_alpha) + log_alpha - 1  # overestimator of log(E[C_k]) by linearization\n",
    "            loss = E_log_u + log_E_Ck_est\n",
    "\n",
    "        prev_log_avg_Ck = log_avg_Ck  # for next iter\n",
    "        trainable_vars = log_u_model.trainable_variables\n",
    "        grads = tape.gradient(loss, trainable_vars)\n",
    "        nn_optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "        step_rcd = dict(log_alpha=log_alpha, log_avg_Ck=log_avg_Ck, E_log_u=E_log_u,\n",
    "                        log_E_Ck_est=log_E_Ck_est, loss=loss)\n",
    "\n",
    "        print_to_console = (10 * step) % args.last_step == 0\n",
    "        if print_to_console:\n",
    "            str_to_print = f\"step {step}:\\t\\tloss = {loss:.4g}, log_alpha = {log_alpha:.4g}, log_avg_Ck = {log_avg_Ck:.4g}, log_E_Ck_est = {log_E_Ck_est:.4g}, \"\n",
    "            str_to_print += f\"E_log_u = {E_log_u:.4}\"\n",
    "            print(str_to_print)\n",
    "        step_rcd['step'] = step\n",
    "\n",
    "        records.append(step_rcd)\n",
    "\n",
    "        finished = (step + 1 >= args.last_step)\n",
    "        if finished:\n",
    "            break\n",
    "        \n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02040570-05d3-42aa-a701-2bd58dcb4649",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1024\n",
    "M = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa8b87-ca23-4c03-8f24-2d0e7a8591a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"data_dim\": data_dim,\n",
    "    \"lamb\": lamb,\n",
    "    \"batchsize\": k,\n",
    "    \"num_Ck_samples\": M,\n",
    "\n",
    "    # for the global optimization inner loop\n",
    "    \"y_init\": \"quick\",\n",
    "    \"y_quick_topn\": 3,\n",
    "    \"y_steps\": 1000,\n",
    "    \"y_tol\": 1e-6,\n",
    "    \"y_lr\": 0.01,\n",
    "    \"chunksize\": None,\n",
    "\n",
    "    \"lr\": 5e-4,\n",
    "    \"last_step\": 400,\n",
    "    \"beta\": 0.2,\n",
    "}\n",
    "\n",
    "args = utils.get_args_as_obj(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e309b-1ea4-4014-aa9a-a4490e12354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b294a7e-d8b8-40d5-9d51-f54fb23161c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensors([])\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.map(lambda _: source.sample(args.batchsize))\n",
    "\n",
    "lb_records = train_lb(lb_model, nn_optimizer, iter(dataset), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392321bf-bfbd-41b0-bc4d-0e101bab7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run full global optimization procedure to obtain final estimate of the lower bound intercept\n",
    "# (takes a while; can be parallelized)\n",
    "from rdlb import est_R_\n",
    "from copy import copy\n",
    "test_args = copy(args)\n",
    "test_args.y_init = 'exhaustive'\n",
    "test_args.verbose = False\n",
    "lb_result = est_R_(lb_model, lamb, iter(dataset), test_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ae83d-5943-4b30-bc4f-b868451e36f2",
   "metadata": {},
   "source": [
    "## Compare the results with the ground-truth R-D function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71672925-d2b1-44dd-9d67-bc24b06ac09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (6, 4)\n",
    "fig = plt.figure(figsize=figsize)\n",
    "\n",
    "gDs, gRs = utils.diag_gaussian_rdf(var, num_points=1000)\n",
    "plt.plot(gDs, gRs, label='true $R(D)$')\n",
    "\n",
    "results = {}\n",
    "results['proposed $\\\\hat R_U(D)$'] = {'R': ub_records['rate'][-1], 'D': ub_records['mse'][-1]}\n",
    "results['Blahut-Arimoto $\\\\hat R(D)$'] = {'R': ba_records[-1]['R'], 'D': ba_records[-1]['D']}\n",
    "results['proposed $\\\\hat R_L(D)$'] = {'lamb': lamb, 'R_': float(lb_result['R_'])}\n",
    "\n",
    "# plot point on the upper bound\n",
    "for key, res in results.items():\n",
    "    if key == 'proposed $\\\\hat R_L(D)$':\n",
    "        xs = np.linspace(0, 0.8, 100)\n",
    "        y = -res['lamb'] * xs + res['R_']  # nats\n",
    "        plt.plot(xs, y, label=key, color='red')\n",
    "        \n",
    "    else:\n",
    "        plt.scatter(res['D'], res['R'], label=key, s=30, marker='x')\n",
    "\n",
    "# plt.xlim(0, 0.6)\n",
    "plt.ylim(-0.1, 5)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Distortion (mean squared error)')\n",
    "plt.ylabel('Rate (nats per sample)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e448736-0ae6-4856-a604-0946b6d0e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fe2b7b-ffae-4701-9457-a61d926fe945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The points from the proposed upper bound and BA are visually indistinguishable on the plot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
