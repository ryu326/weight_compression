[Stage: Convert to HF format] K=2
W0413 07:57:06.991627 820190 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0413 07:57:07.019509 820190 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0413 07:57:07.468517 820190 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0413 07:57:07.468638 820190 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0413 07:57:07.468683 820190 utils.py:162] NumExpr defaulting to 16 threads.
I0413 07:57:07.642205 820190 config.py:58] PyTorch version 2.4.0 available.
W0413 07:57:08.127852 820190 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0413 07:57:29.223364 820190 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

W0413 07:57:29.224899 820190 warnings.py:110] /opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.33it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.54it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.69it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.43it/s]
Some weights of the model checkpoint at meta-llama/Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.Qidxs', 'model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.Wscale', 'model.layers.0.mlp.down_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.Qidxs', 'model.layers.0.mlp.upgate_proj.SU', 'model.layers.0.mlp.upgate_proj.SV', 'model.layers.0.mlp.upgate_proj.Wscale', 'model.layers.0.mlp.upgate_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.fuse_scales', 'model.layers.0.self_attn.o_proj.Qidxs', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.Wscale', 'model.layers.0.self_attn.o_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.Qidxs', 'model.layers.0.self_attn.qkv_proj.SU', 'model.layers.0.self_attn.qkv_proj.SV', 'model.layers.0.self_attn.qkv_proj.Wscale', 'model.layers.0.self_attn.qkv_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.fuse_scales', 'model.layers.1.mlp.down_proj.Qidxs', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.Wscale', 'model.layers.1.mlp.down_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.Qidxs', 'model.layers.1.mlp.upgate_proj.SU', 'model.layers.1.mlp.upgate_proj.SV', 'model.layers.1.mlp.upgate_proj.Wscale', 'model.layers.1.mlp.upgate_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.fuse_scales', 'model.layers.1.self_attn.o_proj.Qidxs', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.Wscale', 'model.layers.1.self_attn.o_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.Qidxs', 'model.layers.1.self_attn.qkv_proj.SU', 'model.layers.1.self_attn.qkv_proj.SV', 'model.layers.1.self_attn.qkv_proj.Wscale', 'model.layers.1.self_attn.qkv_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.fuse_scales', 'model.layers.10.mlp.down_proj.Qidxs', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.Wscale', 'model.layers.10.mlp.down_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.Qidxs', 'model.layers.10.mlp.upgate_proj.SU', 'model.layers.10.mlp.upgate_proj.SV', 'model.layers.10.mlp.upgate_proj.Wscale', 'model.layers.10.mlp.upgate_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.fuse_scales', 'model.layers.10.self_attn.o_proj.Qidxs', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.Wscale', 'model.layers.10.self_attn.o_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.Qidxs', 'model.layers.10.self_attn.qkv_proj.SU', 'model.layers.10.self_attn.qkv_proj.SV', 'model.layers.10.self_attn.qkv_proj.Wscale', 'model.layers.10.self_attn.qkv_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.fuse_scales', 'model.layers.11.mlp.down_proj.Qidxs', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.Wscale', 'model.layers.11.mlp.down_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.Qidxs', 'model.layers.11.mlp.upgate_proj.SU', 'model.layers.11.mlp.upgate_proj.SV', 'model.layers.11.mlp.upgate_proj.Wscale', 'model.layers.11.mlp.upgate_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.fuse_scales', 'model.layers.11.self_attn.o_proj.Qidxs', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.Wscale', 'model.layers.11.self_attn.o_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.Qidxs', 'model.layers.11.self_attn.qkv_proj.SU', 'model.layers.11.self_attn.qkv_proj.SV', 'model.layers.11.self_attn.qkv_proj.Wscale', 'model.layers.11.self_attn.qkv_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.fuse_scales', 'model.layers.12.mlp.down_proj.Qidxs', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.Wscale', 'model.layers.12.mlp.down_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.Qidxs', 'model.layers.12.mlp.upgate_proj.SU', 'model.layers.12.mlp.upgate_proj.SV', 'model.layers.12.mlp.upgate_proj.Wscale', 'model.layers.12.mlp.upgate_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.fuse_scales', 'model.layers.12.self_attn.o_proj.Qidxs', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.Wscale', 'model.layers.12.self_attn.o_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.Qidxs', 'model.layers.12.self_attn.qkv_proj.SU', 'model.layers.12.self_attn.qkv_proj.SV', 'model.layers.12.self_attn.qkv_proj.Wscale', 'model.layers.12.self_attn.qkv_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.fuse_scales', 'model.layers.13.mlp.down_proj.Qidxs', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.Wscale', 'model.layers.13.mlp.down_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.Qidxs', 'model.layers.13.mlp.upgate_proj.SU', 'model.layers.13.mlp.upgate_proj.SV', 'model.layers.13.mlp.upgate_proj.Wscale', 'model.layers.13.mlp.upgate_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.fuse_scales', 'model.layers.13.self_attn.o_proj.Qidxs', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.Wscale', 'model.layers.13.self_attn.o_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.Qidxs', 'model.layers.13.self_attn.qkv_proj.SU', 'model.layers.13.self_attn.qkv_proj.SV', 'model.layers.13.self_attn.qkv_proj.Wscale', 'model.layers.13.self_attn.qkv_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.fuse_scales', 'model.layers.14.mlp.down_proj.Qidxs', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.Wscale', 'model.layers.14.mlp.down_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.Qidxs', 'model.layers.14.mlp.upgate_proj.SU', 'model.layers.14.mlp.upgate_proj.SV', 'model.layers.14.mlp.upgate_proj.Wscale', 'model.layers.14.mlp.upgate_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.fuse_scales', 'model.layers.14.self_attn.o_proj.Qidxs', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.Wscale', 'model.layers.14.self_attn.o_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.Qidxs', 'model.layers.14.self_attn.qkv_proj.SU', 'model.layers.14.self_attn.qkv_proj.SV', 'model.layers.14.self_attn.qkv_proj.Wscale', 'model.layers.14.self_attn.qkv_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.fuse_scales', 'model.layers.15.mlp.down_proj.Qidxs', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.Wscale', 'model.layers.15.mlp.down_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.Qidxs', 'model.layers.15.mlp.upgate_proj.SU', 'model.layers.15.mlp.upgate_proj.SV', 'model.layers.15.mlp.upgate_proj.Wscale', 'model.layers.15.mlp.upgate_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.fuse_scales', 'model.layers.15.self_attn.o_proj.Qidxs', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.Wscale', 'model.layers.15.self_attn.o_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.Qidxs', 'model.layers.15.self_attn.qkv_proj.SU', 'model.layers.15.self_attn.qkv_proj.SV', 'model.layers.15.self_attn.qkv_proj.Wscale', 'model.layers.15.self_attn.qkv_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.fuse_scales', 'model.layers.16.mlp.down_proj.Qidxs', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.Wscale', 'model.layers.16.mlp.down_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.Qidxs', 'model.layers.16.mlp.upgate_proj.SU', 'model.layers.16.mlp.upgate_proj.SV', 'model.layers.16.mlp.upgate_proj.Wscale', 'model.layers.16.mlp.upgate_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.fuse_scales', 'model.layers.16.self_attn.o_proj.Qidxs', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.Wscale', 'model.layers.16.self_attn.o_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.Qidxs', 'model.layers.16.self_attn.qkv_proj.SU', 'model.layers.16.self_attn.qkv_proj.SV', 'model.layers.16.self_attn.qkv_proj.Wscale', 'model.layers.16.self_attn.qkv_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.fuse_scales', 'model.layers.17.mlp.down_proj.Qidxs', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.Wscale', 'model.layers.17.mlp.down_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.Qidxs', 'model.layers.17.mlp.upgate_proj.SU', 'model.layers.17.mlp.upgate_proj.SV', 'model.layers.17.mlp.upgate_proj.Wscale', 'model.layers.17.mlp.upgate_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.fuse_scales', 'model.layers.17.self_attn.o_proj.Qidxs', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.Wscale', 'model.layers.17.self_attn.o_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.Qidxs', 'model.layers.17.self_attn.qkv_proj.SU', 'model.layers.17.self_attn.qkv_proj.SV', 'model.layers.17.self_attn.qkv_proj.Wscale', 'model.layers.17.self_attn.qkv_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.fuse_scales', 'model.layers.18.mlp.down_proj.Qidxs', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.Wscale', 'model.layers.18.mlp.down_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.Qidxs', 'model.layers.18.mlp.upgate_proj.SU', 'model.layers.18.mlp.upgate_proj.SV', 'model.layers.18.mlp.upgate_proj.Wscale', 'model.layers.18.mlp.upgate_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.fuse_scales', 'model.layers.18.self_attn.o_proj.Qidxs', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.Wscale', 'model.layers.18.self_attn.o_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.Qidxs', 'model.layers.18.self_attn.qkv_proj.SU', 'model.layers.18.self_attn.qkv_proj.SV', 'model.layers.18.self_attn.qkv_proj.Wscale', 'model.layers.18.self_attn.qkv_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.fuse_scales', 'model.layers.19.mlp.down_proj.Qidxs', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.Wscale', 'model.layers.19.mlp.down_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.Qidxs', 'model.layers.19.mlp.upgate_proj.SU', 'model.layers.19.mlp.upgate_proj.SV', 'model.layers.19.mlp.upgate_proj.Wscale', 'model.layers.19.mlp.upgate_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.fuse_scales', 'model.layers.19.self_attn.o_proj.Qidxs', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.Wscale', 'model.layers.19.self_attn.o_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.Qidxs', 'model.layers.19.self_attn.qkv_proj.SU', 'model.layers.19.self_attn.qkv_proj.SV', 'model.layers.19.self_attn.qkv_proj.Wscale', 'model.layers.19.self_attn.qkv_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.fuse_scales', 'model.layers.2.mlp.down_proj.Qidxs', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.Wscale', 'model.layers.2.mlp.down_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.Qidxs', 'model.layers.2.mlp.upgate_proj.SU', 'model.layers.2.mlp.upgate_proj.SV', 'model.layers.2.mlp.upgate_proj.Wscale', 'model.layers.2.mlp.upgate_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.fuse_scales', 'model.layers.2.self_attn.o_proj.Qidxs', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.Wscale', 'model.layers.2.self_attn.o_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.Qidxs', 'model.layers.2.self_attn.qkv_proj.SU', 'model.layers.2.self_attn.qkv_proj.SV', 'model.layers.2.self_attn.qkv_proj.Wscale', 'model.layers.2.self_attn.qkv_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.fuse_scales', 'model.layers.20.mlp.down_proj.Qidxs', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.Wscale', 'model.layers.20.mlp.down_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.Qidxs', 'model.layers.20.mlp.upgate_proj.SU', 'model.layers.20.mlp.upgate_proj.SV', 'model.layers.20.mlp.upgate_proj.Wscale', 'model.layers.20.mlp.upgate_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.fuse_scales', 'model.layers.20.self_attn.o_proj.Qidxs', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.Wscale', 'model.layers.20.self_attn.o_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.Qidxs', 'model.layers.20.self_attn.qkv_proj.SU', 'model.layers.20.self_attn.qkv_proj.SV', 'model.layers.20.self_attn.qkv_proj.Wscale', 'model.layers.20.self_attn.qkv_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.fuse_scales', 'model.layers.21.mlp.down_proj.Qidxs', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.Wscale', 'model.layers.21.mlp.down_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.Qidxs', 'model.layers.21.mlp.upgate_proj.SU', 'model.layers.21.mlp.upgate_proj.SV', 'model.layers.21.mlp.upgate_proj.Wscale', 'model.layers.21.mlp.upgate_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.fuse_scales', 'model.layers.21.self_attn.o_proj.Qidxs', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.Wscale', 'model.layers.21.self_attn.o_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.Qidxs', 'model.layers.21.self_attn.qkv_proj.SU', 'model.layers.21.self_attn.qkv_proj.SV', 'model.layers.21.self_attn.qkv_proj.Wscale', 'model.layers.21.self_attn.qkv_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.fuse_scales', 'model.layers.22.mlp.down_proj.Qidxs', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.Wscale', 'model.layers.22.mlp.down_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.Qidxs', 'model.layers.22.mlp.upgate_proj.SU', 'model.layers.22.mlp.upgate_proj.SV', 'model.layers.22.mlp.upgate_proj.Wscale', 'model.layers.22.mlp.upgate_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.fuse_scales', 'model.layers.22.self_attn.o_proj.Qidxs', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.Wscale', 'model.layers.22.self_attn.o_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.Qidxs', 'model.layers.22.self_attn.qkv_proj.SU', 'model.layers.22.self_attn.qkv_proj.SV', 'model.layers.22.self_attn.qkv_proj.Wscale', 'model.layers.22.self_attn.qkv_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.fuse_scales', 'model.layers.23.mlp.down_proj.Qidxs', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.Wscale', 'model.layers.23.mlp.down_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.Qidxs', 'model.layers.23.mlp.upgate_proj.SU', 'model.layers.23.mlp.upgate_proj.SV', 'model.layers.23.mlp.upgate_proj.Wscale', 'model.layers.23.mlp.upgate_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.fuse_scales', 'model.layers.23.self_attn.o_proj.Qidxs', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.Wscale', 'model.layers.23.self_attn.o_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.Qidxs', 'model.layers.23.self_attn.qkv_proj.SU', 'model.layers.23.self_attn.qkv_proj.SV', 'model.layers.23.self_attn.qkv_proj.Wscale', 'model.layers.23.self_attn.qkv_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.fuse_scales', 'model.layers.24.mlp.down_proj.Qidxs', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.Wscale', 'model.layers.24.mlp.down_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.Qidxs', 'model.layers.24.mlp.upgate_proj.SU', 'model.layers.24.mlp.upgate_proj.SV', 'model.layers.24.mlp.upgate_proj.Wscale', 'model.layers.24.mlp.upgate_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.fuse_scales', 'model.layers.24.self_attn.o_proj.Qidxs', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.Wscale', 'model.layers.24.self_attn.o_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.Qidxs', 'model.layers.24.self_attn.qkv_proj.SU', 'model.layers.24.self_attn.qkv_proj.SV', 'model.layers.24.self_attn.qkv_proj.Wscale', 'model.layers.24.self_attn.qkv_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.fuse_scales', 'model.layers.25.mlp.down_proj.Qidxs', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.Wscale', 'model.layers.25.mlp.down_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.Qidxs', 'model.layers.25.mlp.upgate_proj.SU', 'model.layers.25.mlp.upgate_proj.SV', 'model.layers.25.mlp.upgate_proj.Wscale', 'model.layers.25.mlp.upgate_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.fuse_scales', 'model.layers.25.self_attn.o_proj.Qidxs', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.Wscale', 'model.layers.25.self_attn.o_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.Qidxs', 'model.layers.25.self_attn.qkv_proj.SU', 'model.layers.25.self_attn.qkv_proj.SV', 'model.layers.25.self_attn.qkv_proj.Wscale', 'model.layers.25.self_attn.qkv_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.fuse_scales', 'model.layers.26.mlp.down_proj.Qidxs', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.Wscale', 'model.layers.26.mlp.down_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.Qidxs', 'model.layers.26.mlp.upgate_proj.SU', 'model.layers.26.mlp.upgate_proj.SV', 'model.layers.26.mlp.upgate_proj.Wscale', 'model.layers.26.mlp.upgate_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.fuse_scales', 'model.layers.26.self_attn.o_proj.Qidxs', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.Wscale', 'model.layers.26.self_attn.o_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.Qidxs', 'model.layers.26.self_attn.qkv_proj.SU', 'model.layers.26.self_attn.qkv_proj.SV', 'model.layers.26.self_attn.qkv_proj.Wscale', 'model.layers.26.self_attn.qkv_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.fuse_scales', 'model.layers.27.mlp.down_proj.Qidxs', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.Wscale', 'model.layers.27.mlp.down_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.Qidxs', 'model.layers.27.mlp.upgate_proj.SU', 'model.layers.27.mlp.upgate_proj.SV', 'model.layers.27.mlp.upgate_proj.Wscale', 'model.layers.27.mlp.upgate_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.fuse_scales', 'model.layers.27.self_attn.o_proj.Qidxs', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.Wscale', 'model.layers.27.self_attn.o_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.Qidxs', 'model.layers.27.self_attn.qkv_proj.SU', 'model.layers.27.self_attn.qkv_proj.SV', 'model.layers.27.self_attn.qkv_proj.Wscale', 'model.layers.27.self_attn.qkv_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.fuse_scales', 'model.layers.28.mlp.down_proj.Qidxs', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.Wscale', 'model.layers.28.mlp.down_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.Qidxs', 'model.layers.28.mlp.upgate_proj.SU', 'model.layers.28.mlp.upgate_proj.SV', 'model.layers.28.mlp.upgate_proj.Wscale', 'model.layers.28.mlp.upgate_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.fuse_scales', 'model.layers.28.self_attn.o_proj.Qidxs', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.Wscale', 'model.layers.28.self_attn.o_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.Qidxs', 'model.layers.28.self_attn.qkv_proj.SU', 'model.layers.28.self_attn.qkv_proj.SV', 'model.layers.28.self_attn.qkv_proj.Wscale', 'model.layers.28.self_attn.qkv_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.fuse_scales', 'model.layers.29.mlp.down_proj.Qidxs', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.Wscale', 'model.layers.29.mlp.down_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.Qidxs', 'model.layers.29.mlp.upgate_proj.SU', 'model.layers.29.mlp.upgate_proj.SV', 'model.layers.29.mlp.upgate_proj.Wscale', 'model.layers.29.mlp.upgate_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.fuse_scales', 'model.layers.29.self_attn.o_proj.Qidxs', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.Wscale', 'model.layers.29.self_attn.o_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.Qidxs', 'model.layers.29.self_attn.qkv_proj.SU', 'model.layers.29.self_attn.qkv_proj.SV', 'model.layers.29.self_attn.qkv_proj.Wscale', 'model.layers.29.self_attn.qkv_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.fuse_scales', 'model.layers.3.mlp.down_proj.Qidxs', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.Wscale', 'model.layers.3.mlp.down_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.Qidxs', 'model.layers.3.mlp.upgate_proj.SU', 'model.layers.3.mlp.upgate_proj.SV', 'model.layers.3.mlp.upgate_proj.Wscale', 'model.layers.3.mlp.upgate_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.fuse_scales', 'model.layers.3.self_attn.o_proj.Qidxs', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.Wscale', 'model.layers.3.self_attn.o_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.Qidxs', 'model.layers.3.self_attn.qkv_proj.SU', 'model.layers.3.self_attn.qkv_proj.SV', 'model.layers.3.self_attn.qkv_proj.Wscale', 'model.layers.3.self_attn.qkv_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.fuse_scales', 'model.layers.30.mlp.down_proj.Qidxs', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.Wscale', 'model.layers.30.mlp.down_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.Qidxs', 'model.layers.30.mlp.upgate_proj.SU', 'model.layers.30.mlp.upgate_proj.SV', 'model.layers.30.mlp.upgate_proj.Wscale', 'model.layers.30.mlp.upgate_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.fuse_scales', 'model.layers.30.self_attn.o_proj.Qidxs', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.Wscale', 'model.layers.30.self_attn.o_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.Qidxs', 'model.layers.30.self_attn.qkv_proj.SU', 'model.layers.30.self_attn.qkv_proj.SV', 'model.layers.30.self_attn.qkv_proj.Wscale', 'model.layers.30.self_attn.qkv_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.fuse_scales', 'model.layers.31.mlp.down_proj.Qidxs', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.Wscale', 'model.layers.31.mlp.down_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.Qidxs', 'model.layers.31.mlp.upgate_proj.SU', 'model.layers.31.mlp.upgate_proj.SV', 'model.layers.31.mlp.upgate_proj.Wscale', 'model.layers.31.mlp.upgate_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.fuse_scales', 'model.layers.31.self_attn.o_proj.Qidxs', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.Wscale', 'model.layers.31.self_attn.o_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.Qidxs', 'model.layers.31.self_attn.qkv_proj.SU', 'model.layers.31.self_attn.qkv_proj.SV', 'model.layers.31.self_attn.qkv_proj.Wscale', 'model.layers.31.self_attn.qkv_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.fuse_scales', 'model.layers.4.mlp.down_proj.Qidxs', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.Wscale', 'model.layers.4.mlp.down_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.Qidxs', 'model.layers.4.mlp.upgate_proj.SU', 'model.layers.4.mlp.upgate_proj.SV', 'model.layers.4.mlp.upgate_proj.Wscale', 'model.layers.4.mlp.upgate_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.fuse_scales', 'model.layers.4.self_attn.o_proj.Qidxs', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.Wscale', 'model.layers.4.self_attn.o_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.Qidxs', 'model.layers.4.self_attn.qkv_proj.SU', 'model.layers.4.self_attn.qkv_proj.SV', 'model.layers.4.self_attn.qkv_proj.Wscale', 'model.layers.4.self_attn.qkv_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.fuse_scales', 'model.layers.5.mlp.down_proj.Qidxs', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.Wscale', 'model.layers.5.mlp.down_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.Qidxs', 'model.layers.5.mlp.upgate_proj.SU', 'model.layers.5.mlp.upgate_proj.SV', 'model.layers.5.mlp.upgate_proj.Wscale', 'model.layers.5.mlp.upgate_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.fuse_scales', 'model.layers.5.self_attn.o_proj.Qidxs', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.Wscale', 'model.layers.5.self_attn.o_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.Qidxs', 'model.layers.5.self_attn.qkv_proj.SU', 'model.layers.5.self_attn.qkv_proj.SV', 'model.layers.5.self_attn.qkv_proj.Wscale', 'model.layers.5.self_attn.qkv_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.fuse_scales', 'model.layers.6.mlp.down_proj.Qidxs', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.Wscale', 'model.layers.6.mlp.down_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.Qidxs', 'model.layers.6.mlp.upgate_proj.SU', 'model.layers.6.mlp.upgate_proj.SV', 'model.layers.6.mlp.upgate_proj.Wscale', 'model.layers.6.mlp.upgate_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.fuse_scales', 'model.layers.6.self_attn.o_proj.Qidxs', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.Wscale', 'model.layers.6.self_attn.o_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.Qidxs', 'model.layers.6.self_attn.qkv_proj.SU', 'model.layers.6.self_attn.qkv_proj.SV', 'model.layers.6.self_attn.qkv_proj.Wscale', 'model.layers.6.self_attn.qkv_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.fuse_scales', 'model.layers.7.mlp.down_proj.Qidxs', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.Wscale', 'model.layers.7.mlp.down_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.Qidxs', 'model.layers.7.mlp.upgate_proj.SU', 'model.layers.7.mlp.upgate_proj.SV', 'model.layers.7.mlp.upgate_proj.Wscale', 'model.layers.7.mlp.upgate_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.fuse_scales', 'model.layers.7.self_attn.o_proj.Qidxs', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.Wscale', 'model.layers.7.self_attn.o_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.Qidxs', 'model.layers.7.self_attn.qkv_proj.SU', 'model.layers.7.self_attn.qkv_proj.SV', 'model.layers.7.self_attn.qkv_proj.Wscale', 'model.layers.7.self_attn.qkv_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.fuse_scales', 'model.layers.8.mlp.down_proj.Qidxs', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.Wscale', 'model.layers.8.mlp.down_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.Qidxs', 'model.layers.8.mlp.upgate_proj.SU', 'model.layers.8.mlp.upgate_proj.SV', 'model.layers.8.mlp.upgate_proj.Wscale', 'model.layers.8.mlp.upgate_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.fuse_scales', 'model.layers.8.self_attn.o_proj.Qidxs', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.Wscale', 'model.layers.8.self_attn.o_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.Qidxs', 'model.layers.8.self_attn.qkv_proj.SU', 'model.layers.8.self_attn.qkv_proj.SV', 'model.layers.8.self_attn.qkv_proj.Wscale', 'model.layers.8.self_attn.qkv_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.fuse_scales', 'model.layers.9.mlp.down_proj.Qidxs', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.Wscale', 'model.layers.9.mlp.down_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.Qidxs', 'model.layers.9.mlp.upgate_proj.SU', 'model.layers.9.mlp.upgate_proj.SV', 'model.layers.9.mlp.upgate_proj.Wscale', 'model.layers.9.mlp.upgate_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.fuse_scales', 'model.layers.9.self_attn.o_proj.Qidxs', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.Wscale', 'model.layers.9.self_attn.o_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.Qidxs', 'model.layers.9.self_attn.qkv_proj.SU', 'model.layers.9.self_attn.qkv_proj.SV', 'model.layers.9.self_attn.qkv_proj.Wscale', 'model.layers.9.self_attn.qkv_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.fuse_scales']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
W0413 07:57:32.198455 820190 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0413 07:57:32.200189 820190 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_qkv.pt',

W0413 07:57:32.212585 820190 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0413 07:57:32.222615 820190 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0413 07:57:32.251982 820190 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0413 07:57:32.269416 820190 hfize_llama.py:78] loaded layer 0 down
I0413 07:57:32.335940 820190 hfize_llama.py:78] loaded layer 1 down
I0413 07:57:32.403910 820190 hfize_llama.py:78] loaded layer 2 down
I0413 07:57:32.447958 820190 hfize_llama.py:78] loaded layer 3 down
I0413 07:57:32.488390 820190 hfize_llama.py:78] loaded layer 4 down
I0413 07:57:32.539526 820190 hfize_llama.py:78] loaded layer 5 down
I0413 07:57:32.573763 820190 hfize_llama.py:78] loaded layer 6 down
I0413 07:57:32.619355 820190 hfize_llama.py:78] loaded layer 7 down
I0413 07:57:32.677685 820190 hfize_llama.py:78] loaded layer 8 down
I0413 07:57:32.728773 820190 hfize_llama.py:78] loaded layer 9 down
I0413 07:57:32.762597 820190 hfize_llama.py:78] loaded layer 10 down
I0413 07:57:32.816993 820190 hfize_llama.py:78] loaded layer 11 down
I0413 07:57:32.856064 820190 hfize_llama.py:78] loaded layer 12 down
I0413 07:57:32.906373 820190 hfize_llama.py:78] loaded layer 13 down
I0413 07:57:32.966484 820190 hfize_llama.py:78] loaded layer 14 down
I0413 07:57:33.011111 820190 hfize_llama.py:78] loaded layer 15 down
I0413 07:57:33.061703 820190 hfize_llama.py:78] loaded layer 16 down
I0413 07:57:33.095933 820190 hfize_llama.py:78] loaded layer 17 down
I0413 07:57:33.139505 820190 hfize_llama.py:78] loaded layer 18 down
I0413 07:57:33.215919 820190 hfize_llama.py:78] loaded layer 19 down
I0413 07:57:33.275935 820190 hfize_llama.py:78] loaded layer 20 down
I0413 07:57:33.342961 820190 hfize_llama.py:78] loaded layer 21 down
I0413 07:57:33.378888 820190 hfize_llama.py:78] loaded layer 22 down
I0413 07:57:33.420400 820190 hfize_llama.py:78] loaded layer 23 down
I0413 07:57:33.471326 820190 hfize_llama.py:78] loaded layer 24 down
I0413 07:57:33.515392 820190 hfize_llama.py:78] loaded layer 25 down
I0413 07:57:33.558545 820190 hfize_llama.py:78] loaded layer 26 down
I0413 07:57:33.623898 820190 hfize_llama.py:78] loaded layer 27 down
I0413 07:57:33.686253 820190 hfize_llama.py:78] loaded layer 28 down
I0413 07:57:33.757589 820190 hfize_llama.py:78] loaded layer 29 down
I0413 07:57:33.823569 820190 hfize_llama.py:78] loaded layer 30 down
I0413 07:57:33.893138 820190 hfize_llama.py:78] loaded layer 31 down
I0413 07:57:33.893208 820190 hfize_llama.py:80] saving model...
I0413 07:57:43.527022 820190 hfize_llama.py:87] successfully loaded hfized model
I0413 07:57:43.527179 820190 hfize_llama.py:89] generating some text...
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
I0413 07:57:55.986623 820190 hfize_llama.py:100] <|begin_of_text|>It is a truth universally acknowledged that the best way to spend your time in the Philippines is to spend it on the beach. There are beaches in the Philippines that are simply breathtaking, but there are also beaches that are a bit less than breathtaking. We decided to take a look at the top 10 beaches in the Philippines and rank them according to their overall
I0413 07:57:55.986748 820190 hfize_llama.py:101] elapsed: 12.459502220153809
[Stage: End-to-End Finetuning] K=2
W0413 07:57:58.794237 821336 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0413 07:57:58.825667 821336 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0413 07:57:59.122021 821336 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0413 07:57:59.122145 821336 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0413 07:57:59.122189 821336 utils.py:162] NumExpr defaulting to 16 threads.
I0413 07:57:59.238891 821336 config.py:58] PyTorch version 2.4.0 available.
W0413 07:57:59.609754 821336 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0413 07:58:19.971216 821336 warnings.py:110] /opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.27s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.79s/it]
/opt/conda/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[Stage: Eval PPL] K=2
[Stage: Eval Zero-shot] K=2
