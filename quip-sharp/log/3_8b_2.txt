W0412 06:43:38.887942 10085 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 06:43:38.917834 10085 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 06:43:39.258202 10085 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 06:43:39.258325 10085 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 06:43:39.258370 10085 utils.py:162] NumExpr defaulting to 16 threads.
I0412 06:43:39.392144 10085 config.py:58] PyTorch version 2.4.0 available.
W0412 06:43:39.779964 10085 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0412 06:44:05.233104 10085 warnings.py:110] /opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  25%|██▌       | 1/4 [07:44<23:14, 464.71s/it]Downloading shards:  50%|█████     | 2/4 [16:04<16:10, 485.11s/it]Downloading shards:  75%|███████▌  | 3/4 [24:43<08:20, 500.99s/it]Downloading shards: 100%|██████████| 4/4 [26:53<00:00, 354.19s/it]Downloading shards: 100%|██████████| 4/4 [26:53<00:00, 403.28s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  4.93it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  5.46it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  6.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  7.17it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.52it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
I0412 07:11:06.254179 10085 quantize_finetune_llama.py:159] loaded model
I0412 07:11:26.329571 10085 quantize_finetune_llama.py:163] loaded dataset and devset
I0412 07:11:27.637782 10085 quantize_finetune_llama.py:182] layer 0 gpu 0
I0412 07:11:52.593361 10085 quantize_finetune_llama.py:214] computed original embedding for layer 0 in 24.73292636871338s, pre msv 1.8340995666221716e-05, post msv 7.992622704477981e-05
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0412 07:12:03.840653 10085 quantize_finetune_llama.py:182] layer 1 gpu 0
W0412 07:12:05.445035 12380 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 07:12:05.473207 12380 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 07:12:05.792662 12380 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 07:12:05.792780 12380 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 07:12:05.792825 12380 utils.py:162] NumExpr defaulting to 16 threads.
I0412 07:12:05.952219 12380 config.py:58] PyTorch version 2.4.0 available.
W0412 07:12:06.399357 12380 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0412 07:12:32.636508 12380 data_utils.py:205] using 256 training seqs, 128 validation seqs
W0412 07:12:32.662174 12380 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/algo/quip.py:490: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(hessian_path, map_location=torch.device('cpu'))

I0412 07:12:33.887330 12380 quip.py:388] mean square of W: 1.0
I0412 07:12:33.887800 12380 quip.py:389] mean square of Wr: 1.0
I0412 07:12:33.896136 12380 quip.py:390] difference between Hr and Hr.T: 4.76837158203125e-07
I0412 07:12:33.896633 12380 quip.py:391] max abs of Hr: 3.1711902618408203
I0412 07:12:33.905480 12380 quip.py:392] min diag of Lhr: 0.14404690265655518
I0412 07:12:46.307886 12380 misc.py:19] ./ckpt/3_8b_2bit/0_qkv.pt frob  error: 0.24301308393478394
I0412 07:12:46.308019 12380 misc.py:20] ./ckpt/3_8b_2bit/0_qkv.pt proxy error: 0.0003071917162742466
W0412 07:12:46.424678 12380 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/algo/finetune.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_linear = torch.load(save_path,

I0412 07:12:58.056189 12380 finetune.py:25] layer 0_qkv initial loss 2.0280078842915827e-06
W0412 07:12:58.056371 12380 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0412 07:12:58.287420 12380 warnings.py:110] /opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:768: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0412 07:13:24.973627 12380 finetune.py:49] layer 0_qkv @ epoch 0 new loss 7.849283747418667e-07 old loss 2.0280078842915827e-06 BETTER
I0412 07:13:51.941892 12380 finetune.py:49] layer 0_qkv @ epoch 1 new loss 7.232937377921189e-07 old loss 7.849283747418667e-07 BETTER
I0412 07:14:18.016871 12380 finetune.py:49] layer 0_qkv @ epoch 2 new loss 6.970076356083155e-07 old loss 7.232937377921189e-07 BETTER
I0412 07:14:44.401883 12380 finetune.py:49] layer 0_qkv @ epoch 3 new loss 6.953173397050705e-07 old loss 6.970076356083155e-07 BETTER
I0412 07:15:11.322344 12380 finetune.py:49] layer 0_qkv @ epoch 4 new loss 6.718178724440804e-07 old loss 6.953173397050705e-07 BETTER
I0412 07:15:12.523862 12380 quip.py:388] mean square of W: 0.9999999403953552
I0412 07:15:12.524169 12380 quip.py:389] mean square of Wr: 1.0
I0412 07:15:12.525043 12380 quip.py:390] difference between Hr and Hr.T: 2.980232238769531e-07
I0412 07:15:12.525521 12380 quip.py:391] max abs of Hr: 2.663370132446289
I0412 07:15:12.525650 12380 quip.py:392] min diag of Lhr: 0.18672870099544525
I0412 07:15:23.982882 12380 misc.py:19] ./ckpt/3_8b_2bit/0_o.pt frob  error: 0.21497195959091187
I0412 07:15:23.983010 12380 misc.py:20] ./ckpt/3_8b_2bit/0_o.pt proxy error: 0.005467573646456003
I0412 07:15:34.562705 12380 finetune.py:25] layer 0_o initial loss 2.290298880325281e-06
I0412 07:16:00.719881 12380 finetune.py:49] layer 0_o @ epoch 0 new loss 1.3983723192723119e-06 old loss 2.290298880325281e-06 BETTER
I0412 07:16:26.930723 12380 finetune.py:49] layer 0_o @ epoch 1 new loss 1.2783166312146932e-06 old loss 1.3983723192723119e-06 BETTER
I0412 07:16:53.159337 12380 finetune.py:49] layer 0_o @ epoch 2 new loss 1.2221496490383288e-06 old loss 1.2783166312146932e-06 BETTER
I0412 07:19:27.115753 12380 finetune.py:49] layer 0_o @ epoch 3 new loss 1.19239689411188e-06 old loss 1.2221496490383288e-06 BETTER
I0412 07:19:52.914974 12380 finetune.py:49] layer 0_o @ epoch 4 new loss 1.1681643172778422e-06 old loss 1.19239689411188e-06 BETTER
I0412 07:19:54.653602 12380 quip.py:388] mean square of W: 1.0
I0412 07:19:54.655552 12380 quip.py:389] mean square of Wr: 1.0000001192092896
I0412 07:19:54.656316 12380 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0412 07:19:54.656825 12380 quip.py:391] max abs of Hr: 1.333940029144287
I0412 07:19:54.656955 12380 quip.py:392] min diag of Lhr: 0.6206536293029785
I0412 07:20:20.269452 12380 misc.py:19] ./ckpt/3_8b_2bit/0_up.pt frob  error: 0.13994039595127106
I0412 07:20:20.269577 12380 misc.py:20] ./ckpt/3_8b_2bit/0_up.pt proxy error: 0.035391610115766525
I0412 07:20:27.528394 12380 finetune.py:25] layer 0_up initial loss 4.365336280898191e-06
I0412 07:20:53.934155 12380 finetune.py:49] layer 0_up @ epoch 0 new loss 4.2487477003305685e-06 old loss 4.365336280898191e-06 BETTER
I0412 07:21:20.304259 12380 finetune.py:49] layer 0_up @ epoch 1 new loss 4.204091965220869e-06 old loss 4.2487477003305685e-06 BETTER
I0412 07:21:46.687298 12380 finetune.py:49] layer 0_up @ epoch 2 new loss 4.175313279120019e-06 old loss 4.204091965220869e-06 BETTER
I0412 07:22:13.082097 12380 finetune.py:49] layer 0_up @ epoch 3 new loss 4.1480857362330426e-06 old loss 4.175313279120019e-06 BETTER
I0412 07:22:39.385425 12380 finetune.py:49] layer 0_up @ epoch 4 new loss 4.130914021516219e-06 old loss 4.1480857362330426e-06 BETTER
I0412 07:22:42.857387 12380 quip.py:388] mean square of W: 1.0000001192092896
I0412 07:22:42.858440 12380 quip.py:389] mean square of Wr: 1.0
I0412 07:22:42.870382 12380 quip.py:390] difference between Hr and Hr.T: 2.0116567611694336e-07
I0412 07:22:42.873619 12380 quip.py:391] max abs of Hr: 1.2871439456939697
I0412 07:22:42.873778 12380 quip.py:392] min diag of Lhr: 0.6428602337837219
I0412 07:23:28.626785 12380 misc.py:19] ./ckpt/3_8b_2bit/0_down.pt frob  error: 0.13026614487171173
I0412 07:23:28.627005 12380 misc.py:20] ./ckpt/3_8b_2bit/0_down.pt proxy error: 0.032666705548763275
W0412 07:23:28.964692 12380 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/finetune.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
I0412 07:23:56.686978 10085 quantize_finetune_llama.py:214] computed original embedding for layer 1 in 24.820196390151978s, pre msv 7.992622704477981e-05, post msv 0.008141468279063702
I0412 07:23:56.991869 10085 quantize_finetune_llama.py:182] layer 2 gpu 0
W0412 07:23:58.559669 16015 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 07:23:58.588452 16015 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 07:23:58.902903 16015 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 07:23:58.903007 16015 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 07:23:58.903054 16015 utils.py:162] NumExpr defaulting to 16 threads.
I0412 07:23:59.029950 16015 config.py:58] PyTorch version 2.4.0 available.
W0412 07:23:59.429133 16015 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0412 07:24:25.413619 16015 data_utils.py:205] using 256 training seqs, 128 validation seqs
W0412 07:24:25.428847 16015 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/algo/quip.py:490: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(hessian_path, map_location=torch.device('cpu'))

I0412 07:24:26.905035 16015 quip.py:388] mean square of W: 1.0
I0412 07:24:26.905493 16015 quip.py:389] mean square of Wr: 0.9999998807907104
I0412 07:24:26.912831 16015 quip.py:390] difference between Hr and Hr.T: 7.152557373046875e-07
I0412 07:24:26.913338 16015 quip.py:391] max abs of Hr: 5.203253746032715
I0412 07:24:26.920336 16015 quip.py:392] min diag of Lhr: 0.22772681713104248
I0412 07:24:39.017020 16015 misc.py:19] ./ckpt/3_8b_2bit/1_qkv.pt frob  error: 0.15830610692501068
I0412 07:24:39.017154 16015 misc.py:20] ./ckpt/3_8b_2bit/1_qkv.pt proxy error: 0.0005234717391431332
W0412 07:24:39.145210 16015 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/algo/finetune.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_linear = torch.load(save_path,

I0412 07:24:50.698507 16015 finetune.py:25] layer 1_qkv initial loss 9.18911064218264e-06
W0412 07:24:50.698803 16015 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0412 07:24:50.924848 16015 warnings.py:110] /opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:768: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0412 07:25:17.101371 16015 finetune.py:49] layer 1_qkv @ epoch 0 new loss 1.8501885961086373e-06 old loss 9.18911064218264e-06 BETTER
I0412 07:25:43.495942 16015 finetune.py:56] layer 1_qkv @ epoch 1 new loss 2.090729367409949e-06 old loss 1.8501885961086373e-06 WORSE
I0412 07:26:10.348016 16015 finetune.py:56] layer 1_qkv @ epoch 2 new loss 1.8784618305289769e-06 old loss 1.8501885961086373e-06 WORSE
I0412 07:26:37.130930 16015 finetune.py:49] layer 1_qkv @ epoch 3 new loss 1.5715829704276985e-06 old loss 1.8501885961086373e-06 BETTER
I0412 07:27:04.025427 16015 finetune.py:49] layer 1_qkv @ epoch 4 new loss 1.5103400983207393e-06 old loss 1.5715829704276985e-06 BETTER
I0412 07:27:05.367186 16015 quip.py:388] mean square of W: 0.9999998807907104
I0412 07:27:05.367487 16015 quip.py:389] mean square of Wr: 0.9999998807907104
I0412 07:27:05.368390 16015 quip.py:390] difference between Hr and Hr.T: 1.7881393432617188e-07
I0412 07:27:05.368886 16015 quip.py:391] max abs of Hr: 1.6173222064971924
I0412 07:27:05.369015 16015 quip.py:392] min diag of Lhr: 0.3373611867427826
I0412 07:27:17.085509 16015 misc.py:19] ./ckpt/3_8b_2bit/1_o.pt frob  error: 0.18569007515907288
I0412 07:27:17.085653 16015 misc.py:20] ./ckpt/3_8b_2bit/1_o.pt proxy error: 0.020327145233750343
I0412 07:27:27.843271 16015 finetune.py:25] layer 1_o initial loss 5.740128472098149e-06
I0412 07:27:54.356760 16015 finetune.py:49] layer 1_o @ epoch 0 new loss 3.924978500435827e-06 old loss 5.740128472098149e-06 BETTER
I0412 07:28:21.035768 16015 finetune.py:49] layer 1_o @ epoch 1 new loss 3.685912133732927e-06 old loss 3.924978500435827e-06 BETTER
I0412 07:28:47.796470 16015 finetune.py:49] layer 1_o @ epoch 2 new loss 3.5996206406707643e-06 old loss 3.685912133732927e-06 BETTER
I0412 07:29:14.598939 16015 finetune.py:49] layer 1_o @ epoch 3 new loss 3.5183302315999754e-06 old loss 3.5996206406707643e-06 BETTER
I0412 07:29:41.430088 16015 finetune.py:49] layer 1_o @ epoch 4 new loss 3.4712074921117164e-06 old loss 3.5183302315999754e-06 BETTER
I0412 07:29:43.236885 16015 quip.py:388] mean square of W: 1.0
I0412 07:29:43.238774 16015 quip.py:389] mean square of Wr: 1.000000238418579
I0412 07:29:43.239537 16015 quip.py:390] difference between Hr and Hr.T: 1.4901161193847656e-07
I0412 07:29:43.240066 16015 quip.py:391] max abs of Hr: 1.4364413022994995
I0412 07:29:43.240280 16015 quip.py:392] min diag of Lhr: 0.6278841495513916
I0412 07:30:08.725158 16015 misc.py:19] ./ckpt/3_8b_2bit/1_up.pt frob  error: 0.1420704424381256
I0412 07:30:08.725284 16015 misc.py:20] ./ckpt/3_8b_2bit/1_up.pt proxy error: 0.04243543744087219
I0412 07:30:16.073764 16015 finetune.py:25] layer 1_up initial loss 1.3023364772379864e-05
I0412 07:30:42.673531 16015 finetune.py:49] layer 1_up @ epoch 0 new loss 1.0663318789738696e-05 old loss 1.3023364772379864e-05 BETTER
I0412 07:31:09.443965 16015 finetune.py:49] layer 1_up @ epoch 1 new loss 1.0549949365667999e-05 old loss 1.0663318789738696e-05 BETTER
I0412 07:31:35.992575 16015 finetune.py:49] layer 1_up @ epoch 2 new loss 1.0484706763236318e-05 old loss 1.0549949365667999e-05 BETTER
I0412 07:32:02.579531 16015 finetune.py:49] layer 1_up @ epoch 3 new loss 1.0429373105580453e-05 old loss 1.0484706763236318e-05 BETTER
I0412 07:32:29.096392 16015 finetune.py:49] layer 1_up @ epoch 4 new loss 1.0386769645265304e-05 old loss 1.0429373105580453e-05 BETTER
I0412 07:32:32.616674 16015 quip.py:388] mean square of W: 1.0
I0412 07:32:32.617707 16015 quip.py:389] mean square of Wr: 1.0
I0412 07:32:32.629714 16015 quip.py:390] difference between Hr and Hr.T: 1.9073486328125e-06
I0412 07:32:32.632958 16015 quip.py:391] max abs of Hr: 1.298557162284851
I0412 07:32:32.633100 16015 quip.py:392] min diag of Lhr: 0.13680946826934814
I0412 07:33:17.927273 16015 misc.py:19] ./ckpt/3_8b_2bit/1_down.pt frob  error: 0.11871065199375153
I0412 07:33:17.927526 16015 misc.py:20] ./ckpt/3_8b_2bit/1_down.pt proxy error: 0.0010448142420500517
W0412 07:33:18.287242 16015 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/finetune.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
I0412 07:33:47.118117 10085 quantize_finetune_llama.py:214] computed original embedding for layer 2 in 25.164481163024902s, pre msv 0.008141468279063702, post msv 0.008250508457422256
I0412 07:33:47.413146 10085 quantize_finetune_llama.py:182] layer 3 gpu 0
W0412 07:33:49.038458 16360 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 07:33:49.074349 16360 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 07:33:49.437875 16360 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 07:33:49.438013 16360 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 07:33:49.438057 16360 utils.py:162] NumExpr defaulting to 16 threads.
I0412 07:33:49.567219 16360 config.py:58] PyTorch version 2.4.0 available.
W0412 07:33:49.961572 16360 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

I0412 07:34:16.846945 16360 data_utils.py:205] using 256 training seqs, 128 validation seqs
W0412 07:34:16.865452 16360 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/algo/quip.py:490: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(hessian_path, map_location=torch.device('cpu'))

I0412 07:34:18.053008 16360 quip.py:388] mean square of W: 1.0000001192092896
I0412 07:34:18.053520 16360 quip.py:389] mean square of Wr: 1.0
I0412 07:34:18.063191 16360 quip.py:390] difference between Hr and Hr.T: 2.384185791015625e-07
I0412 07:34:18.063689 16360 quip.py:391] max abs of Hr: 2.3645687103271484
I0412 07:34:18.071074 16360 quip.py:392] min diag of Lhr: 0.4372110068798065
I0412 07:34:30.305282 16360 misc.py:19] ./ckpt/3_8b_2bit/2_qkv.pt frob  error: 0.13855107128620148
I0412 07:34:30.305420 16360 misc.py:20] ./ckpt/3_8b_2bit/2_qkv.pt proxy error: 0.004506920929998159
W0412 07:34:30.421838 16360 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/algo/finetune.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_linear = torch.load(save_path,

I0412 07:34:41.779706 16360 finetune.py:25] layer 2_qkv initial loss 4.092059680260718e-05
W0412 07:34:41.780033 16360 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/algo/finetune.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

W0412 07:34:42.019438 16360 warnings.py:110] /opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:768: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

I0412 07:35:08.392556 16360 finetune.py:49] layer 2_qkv @ epoch 0 new loss 5.796668574475916e-06 old loss 4.092059680260718e-05 BETTER
I0412 07:35:34.847903 16360 finetune.py:49] layer 2_qkv @ epoch 1 new loss 5.471060831041541e-06 old loss 5.796668574475916e-06 BETTER
I0412 07:36:03.248810 16360 finetune.py:56] layer 2_qkv @ epoch 2 new loss 6.528437552333344e-06 old loss 5.471060831041541e-06 WORSE
I0412 07:36:30.188946 16360 finetune.py:56] layer 2_qkv @ epoch 3 new loss 5.617774604615988e-06 old loss 5.471060831041541e-06 WORSE
I0412 07:36:57.130259 16360 finetune.py:56] layer 2_qkv @ epoch 4 new loss 5.606473223451758e-06 old loss 5.471060831041541e-06 WORSE
I0412 07:36:58.428816 16360 quip.py:388] mean square of W: 1.0
I0412 07:36:58.429108 16360 quip.py:389] mean square of Wr: 1.0
I0412 07:36:58.429910 16360 quip.py:390] difference between Hr and Hr.T: 2.980232238769531e-07
I0412 07:36:58.430394 16360 quip.py:391] max abs of Hr: 2.305504322052002
I0412 07:36:58.430518 16360 quip.py:392] min diag of Lhr: 0.46615687012672424
I0412 07:37:10.428694 16360 misc.py:19] ./ckpt/3_8b_2bit/2_o.pt frob  error: 0.14899931848049164
I0412 07:37:10.428821 16360 misc.py:20] ./ckpt/3_8b_2bit/2_o.pt proxy error: 0.01979009620845318
I0412 07:37:25.244137 16360 finetune.py:25] layer 2_o initial loss 1.2950924428878352e-05
Process Process-3:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 115, in quantize_llama_layer
    finetune.quantize_finetune_decoder_layer(mixed_layer,
  File "/workspace/Weight_compression/quip-sharp/lib/algo/finetune.py", line 133, in quantize_finetune_decoder_layer
    finetune_decoder_layer(mixed_layer, f'{idx}_{name}', device,
  File "/workspace/Weight_compression/quip-sharp/lib/algo/finetune.py", line 37, in finetune_decoder_layer
    output = layer(source.to(device), position_ids=position_ids)[0]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/quip-sharp/model/llama.py", line 783, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/quip-sharp/model/llama.py", line 255, in forward
    up_proj, gate_proj = self.upgate_proj(x.to(torch.float32))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/quip-sharp/lib/linear/fused_linear.py", line 14, in forward
    fused_output = super(FusedLinear, self).forward(input)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 493.00 MiB is free. Process 669512 has 528.00 MiB memory in use. Process 754798 has 5.03 GiB memory in use. Process 760454 has 41.46 GiB memory in use. Of the allocated memory 4.41 GiB is allocated by PyTorch, and 77.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 245, in <module>
    main(args)
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 199, in main
    model.model.layers[i](
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 756, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 240, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 405, in forward
    return F.silu(input, inplace=self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py", line 2105, in silu
    return torch._C._nn.silu(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.75 GiB. GPU 0 has a total capacity of 47.51 GiB of which 1.35 GiB is free. Process 669512 has 4.68 GiB memory in use. Process 760454 has 41.46 GiB memory in use. Of the allocated memory 4.17 GiB is allocated by PyTorch, and 13.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[Stage: Convert to HF format] K=2
W0412 07:37:37.335226 16707 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 07:37:37.363812 16707 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 07:37:37.677103 16707 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 07:37:37.677227 16707 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 07:37:37.677275 16707 utils.py:162] NumExpr defaulting to 16 threads.
I0412 07:37:37.809205 16707 config.py:58] PyTorch version 2.4.0 available.
W0412 07:37:38.207915 16707 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0412 07:38:03.229831 16707 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

W0412 07:38:03.230751 16707 warnings.py:110] /opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.45it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00, 11.11it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 11.16it/s]
Some weights of the model checkpoint at meta-llama/Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.Qidxs', 'model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.Wscale', 'model.layers.0.mlp.down_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.Qidxs', 'model.layers.0.mlp.upgate_proj.SU', 'model.layers.0.mlp.upgate_proj.SV', 'model.layers.0.mlp.upgate_proj.Wscale', 'model.layers.0.mlp.upgate_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.fuse_scales', 'model.layers.0.self_attn.o_proj.Qidxs', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.Wscale', 'model.layers.0.self_attn.o_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.Qidxs', 'model.layers.0.self_attn.qkv_proj.SU', 'model.layers.0.self_attn.qkv_proj.SV', 'model.layers.0.self_attn.qkv_proj.Wscale', 'model.layers.0.self_attn.qkv_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.fuse_scales', 'model.layers.1.mlp.down_proj.Qidxs', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.Wscale', 'model.layers.1.mlp.down_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.Qidxs', 'model.layers.1.mlp.upgate_proj.SU', 'model.layers.1.mlp.upgate_proj.SV', 'model.layers.1.mlp.upgate_proj.Wscale', 'model.layers.1.mlp.upgate_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.fuse_scales', 'model.layers.1.self_attn.o_proj.Qidxs', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.Wscale', 'model.layers.1.self_attn.o_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.Qidxs', 'model.layers.1.self_attn.qkv_proj.SU', 'model.layers.1.self_attn.qkv_proj.SV', 'model.layers.1.self_attn.qkv_proj.Wscale', 'model.layers.1.self_attn.qkv_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.fuse_scales', 'model.layers.10.mlp.down_proj.Qidxs', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.Wscale', 'model.layers.10.mlp.down_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.Qidxs', 'model.layers.10.mlp.upgate_proj.SU', 'model.layers.10.mlp.upgate_proj.SV', 'model.layers.10.mlp.upgate_proj.Wscale', 'model.layers.10.mlp.upgate_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.fuse_scales', 'model.layers.10.self_attn.o_proj.Qidxs', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.Wscale', 'model.layers.10.self_attn.o_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.Qidxs', 'model.layers.10.self_attn.qkv_proj.SU', 'model.layers.10.self_attn.qkv_proj.SV', 'model.layers.10.self_attn.qkv_proj.Wscale', 'model.layers.10.self_attn.qkv_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.fuse_scales', 'model.layers.11.mlp.down_proj.Qidxs', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.Wscale', 'model.layers.11.mlp.down_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.Qidxs', 'model.layers.11.mlp.upgate_proj.SU', 'model.layers.11.mlp.upgate_proj.SV', 'model.layers.11.mlp.upgate_proj.Wscale', 'model.layers.11.mlp.upgate_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.fuse_scales', 'model.layers.11.self_attn.o_proj.Qidxs', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.Wscale', 'model.layers.11.self_attn.o_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.Qidxs', 'model.layers.11.self_attn.qkv_proj.SU', 'model.layers.11.self_attn.qkv_proj.SV', 'model.layers.11.self_attn.qkv_proj.Wscale', 'model.layers.11.self_attn.qkv_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.fuse_scales', 'model.layers.12.mlp.down_proj.Qidxs', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.Wscale', 'model.layers.12.mlp.down_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.Qidxs', 'model.layers.12.mlp.upgate_proj.SU', 'model.layers.12.mlp.upgate_proj.SV', 'model.layers.12.mlp.upgate_proj.Wscale', 'model.layers.12.mlp.upgate_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.fuse_scales', 'model.layers.12.self_attn.o_proj.Qidxs', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.Wscale', 'model.layers.12.self_attn.o_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.Qidxs', 'model.layers.12.self_attn.qkv_proj.SU', 'model.layers.12.self_attn.qkv_proj.SV', 'model.layers.12.self_attn.qkv_proj.Wscale', 'model.layers.12.self_attn.qkv_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.fuse_scales', 'model.layers.13.mlp.down_proj.Qidxs', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.Wscale', 'model.layers.13.mlp.down_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.Qidxs', 'model.layers.13.mlp.upgate_proj.SU', 'model.layers.13.mlp.upgate_proj.SV', 'model.layers.13.mlp.upgate_proj.Wscale', 'model.layers.13.mlp.upgate_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.fuse_scales', 'model.layers.13.self_attn.o_proj.Qidxs', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.Wscale', 'model.layers.13.self_attn.o_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.Qidxs', 'model.layers.13.self_attn.qkv_proj.SU', 'model.layers.13.self_attn.qkv_proj.SV', 'model.layers.13.self_attn.qkv_proj.Wscale', 'model.layers.13.self_attn.qkv_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.fuse_scales', 'model.layers.14.mlp.down_proj.Qidxs', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.Wscale', 'model.layers.14.mlp.down_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.Qidxs', 'model.layers.14.mlp.upgate_proj.SU', 'model.layers.14.mlp.upgate_proj.SV', 'model.layers.14.mlp.upgate_proj.Wscale', 'model.layers.14.mlp.upgate_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.fuse_scales', 'model.layers.14.self_attn.o_proj.Qidxs', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.Wscale', 'model.layers.14.self_attn.o_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.Qidxs', 'model.layers.14.self_attn.qkv_proj.SU', 'model.layers.14.self_attn.qkv_proj.SV', 'model.layers.14.self_attn.qkv_proj.Wscale', 'model.layers.14.self_attn.qkv_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.fuse_scales', 'model.layers.15.mlp.down_proj.Qidxs', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.Wscale', 'model.layers.15.mlp.down_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.Qidxs', 'model.layers.15.mlp.upgate_proj.SU', 'model.layers.15.mlp.upgate_proj.SV', 'model.layers.15.mlp.upgate_proj.Wscale', 'model.layers.15.mlp.upgate_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.fuse_scales', 'model.layers.15.self_attn.o_proj.Qidxs', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.Wscale', 'model.layers.15.self_attn.o_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.Qidxs', 'model.layers.15.self_attn.qkv_proj.SU', 'model.layers.15.self_attn.qkv_proj.SV', 'model.layers.15.self_attn.qkv_proj.Wscale', 'model.layers.15.self_attn.qkv_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.fuse_scales', 'model.layers.16.mlp.down_proj.Qidxs', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.Wscale', 'model.layers.16.mlp.down_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.Qidxs', 'model.layers.16.mlp.upgate_proj.SU', 'model.layers.16.mlp.upgate_proj.SV', 'model.layers.16.mlp.upgate_proj.Wscale', 'model.layers.16.mlp.upgate_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.fuse_scales', 'model.layers.16.self_attn.o_proj.Qidxs', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.Wscale', 'model.layers.16.self_attn.o_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.Qidxs', 'model.layers.16.self_attn.qkv_proj.SU', 'model.layers.16.self_attn.qkv_proj.SV', 'model.layers.16.self_attn.qkv_proj.Wscale', 'model.layers.16.self_attn.qkv_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.fuse_scales', 'model.layers.17.mlp.down_proj.Qidxs', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.Wscale', 'model.layers.17.mlp.down_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.Qidxs', 'model.layers.17.mlp.upgate_proj.SU', 'model.layers.17.mlp.upgate_proj.SV', 'model.layers.17.mlp.upgate_proj.Wscale', 'model.layers.17.mlp.upgate_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.fuse_scales', 'model.layers.17.self_attn.o_proj.Qidxs', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.Wscale', 'model.layers.17.self_attn.o_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.Qidxs', 'model.layers.17.self_attn.qkv_proj.SU', 'model.layers.17.self_attn.qkv_proj.SV', 'model.layers.17.self_attn.qkv_proj.Wscale', 'model.layers.17.self_attn.qkv_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.fuse_scales', 'model.layers.18.mlp.down_proj.Qidxs', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.Wscale', 'model.layers.18.mlp.down_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.Qidxs', 'model.layers.18.mlp.upgate_proj.SU', 'model.layers.18.mlp.upgate_proj.SV', 'model.layers.18.mlp.upgate_proj.Wscale', 'model.layers.18.mlp.upgate_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.fuse_scales', 'model.layers.18.self_attn.o_proj.Qidxs', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.Wscale', 'model.layers.18.self_attn.o_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.Qidxs', 'model.layers.18.self_attn.qkv_proj.SU', 'model.layers.18.self_attn.qkv_proj.SV', 'model.layers.18.self_attn.qkv_proj.Wscale', 'model.layers.18.self_attn.qkv_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.fuse_scales', 'model.layers.19.mlp.down_proj.Qidxs', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.Wscale', 'model.layers.19.mlp.down_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.Qidxs', 'model.layers.19.mlp.upgate_proj.SU', 'model.layers.19.mlp.upgate_proj.SV', 'model.layers.19.mlp.upgate_proj.Wscale', 'model.layers.19.mlp.upgate_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.fuse_scales', 'model.layers.19.self_attn.o_proj.Qidxs', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.Wscale', 'model.layers.19.self_attn.o_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.Qidxs', 'model.layers.19.self_attn.qkv_proj.SU', 'model.layers.19.self_attn.qkv_proj.SV', 'model.layers.19.self_attn.qkv_proj.Wscale', 'model.layers.19.self_attn.qkv_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.fuse_scales', 'model.layers.2.mlp.down_proj.Qidxs', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.Wscale', 'model.layers.2.mlp.down_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.Qidxs', 'model.layers.2.mlp.upgate_proj.SU', 'model.layers.2.mlp.upgate_proj.SV', 'model.layers.2.mlp.upgate_proj.Wscale', 'model.layers.2.mlp.upgate_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.fuse_scales', 'model.layers.2.self_attn.o_proj.Qidxs', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.Wscale', 'model.layers.2.self_attn.o_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.Qidxs', 'model.layers.2.self_attn.qkv_proj.SU', 'model.layers.2.self_attn.qkv_proj.SV', 'model.layers.2.self_attn.qkv_proj.Wscale', 'model.layers.2.self_attn.qkv_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.fuse_scales', 'model.layers.20.mlp.down_proj.Qidxs', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.Wscale', 'model.layers.20.mlp.down_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.Qidxs', 'model.layers.20.mlp.upgate_proj.SU', 'model.layers.20.mlp.upgate_proj.SV', 'model.layers.20.mlp.upgate_proj.Wscale', 'model.layers.20.mlp.upgate_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.fuse_scales', 'model.layers.20.self_attn.o_proj.Qidxs', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.Wscale', 'model.layers.20.self_attn.o_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.Qidxs', 'model.layers.20.self_attn.qkv_proj.SU', 'model.layers.20.self_attn.qkv_proj.SV', 'model.layers.20.self_attn.qkv_proj.Wscale', 'model.layers.20.self_attn.qkv_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.fuse_scales', 'model.layers.21.mlp.down_proj.Qidxs', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.Wscale', 'model.layers.21.mlp.down_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.Qidxs', 'model.layers.21.mlp.upgate_proj.SU', 'model.layers.21.mlp.upgate_proj.SV', 'model.layers.21.mlp.upgate_proj.Wscale', 'model.layers.21.mlp.upgate_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.fuse_scales', 'model.layers.21.self_attn.o_proj.Qidxs', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.Wscale', 'model.layers.21.self_attn.o_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.Qidxs', 'model.layers.21.self_attn.qkv_proj.SU', 'model.layers.21.self_attn.qkv_proj.SV', 'model.layers.21.self_attn.qkv_proj.Wscale', 'model.layers.21.self_attn.qkv_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.fuse_scales', 'model.layers.22.mlp.down_proj.Qidxs', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.Wscale', 'model.layers.22.mlp.down_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.Qidxs', 'model.layers.22.mlp.upgate_proj.SU', 'model.layers.22.mlp.upgate_proj.SV', 'model.layers.22.mlp.upgate_proj.Wscale', 'model.layers.22.mlp.upgate_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.fuse_scales', 'model.layers.22.self_attn.o_proj.Qidxs', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.Wscale', 'model.layers.22.self_attn.o_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.Qidxs', 'model.layers.22.self_attn.qkv_proj.SU', 'model.layers.22.self_attn.qkv_proj.SV', 'model.layers.22.self_attn.qkv_proj.Wscale', 'model.layers.22.self_attn.qkv_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.fuse_scales', 'model.layers.23.mlp.down_proj.Qidxs', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.Wscale', 'model.layers.23.mlp.down_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.Qidxs', 'model.layers.23.mlp.upgate_proj.SU', 'model.layers.23.mlp.upgate_proj.SV', 'model.layers.23.mlp.upgate_proj.Wscale', 'model.layers.23.mlp.upgate_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.fuse_scales', 'model.layers.23.self_attn.o_proj.Qidxs', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.Wscale', 'model.layers.23.self_attn.o_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.Qidxs', 'model.layers.23.self_attn.qkv_proj.SU', 'model.layers.23.self_attn.qkv_proj.SV', 'model.layers.23.self_attn.qkv_proj.Wscale', 'model.layers.23.self_attn.qkv_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.fuse_scales', 'model.layers.24.mlp.down_proj.Qidxs', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.Wscale', 'model.layers.24.mlp.down_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.Qidxs', 'model.layers.24.mlp.upgate_proj.SU', 'model.layers.24.mlp.upgate_proj.SV', 'model.layers.24.mlp.upgate_proj.Wscale', 'model.layers.24.mlp.upgate_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.fuse_scales', 'model.layers.24.self_attn.o_proj.Qidxs', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.Wscale', 'model.layers.24.self_attn.o_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.Qidxs', 'model.layers.24.self_attn.qkv_proj.SU', 'model.layers.24.self_attn.qkv_proj.SV', 'model.layers.24.self_attn.qkv_proj.Wscale', 'model.layers.24.self_attn.qkv_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.fuse_scales', 'model.layers.25.mlp.down_proj.Qidxs', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.Wscale', 'model.layers.25.mlp.down_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.Qidxs', 'model.layers.25.mlp.upgate_proj.SU', 'model.layers.25.mlp.upgate_proj.SV', 'model.layers.25.mlp.upgate_proj.Wscale', 'model.layers.25.mlp.upgate_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.fuse_scales', 'model.layers.25.self_attn.o_proj.Qidxs', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.Wscale', 'model.layers.25.self_attn.o_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.Qidxs', 'model.layers.25.self_attn.qkv_proj.SU', 'model.layers.25.self_attn.qkv_proj.SV', 'model.layers.25.self_attn.qkv_proj.Wscale', 'model.layers.25.self_attn.qkv_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.fuse_scales', 'model.layers.26.mlp.down_proj.Qidxs', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.Wscale', 'model.layers.26.mlp.down_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.Qidxs', 'model.layers.26.mlp.upgate_proj.SU', 'model.layers.26.mlp.upgate_proj.SV', 'model.layers.26.mlp.upgate_proj.Wscale', 'model.layers.26.mlp.upgate_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.fuse_scales', 'model.layers.26.self_attn.o_proj.Qidxs', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.Wscale', 'model.layers.26.self_attn.o_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.Qidxs', 'model.layers.26.self_attn.qkv_proj.SU', 'model.layers.26.self_attn.qkv_proj.SV', 'model.layers.26.self_attn.qkv_proj.Wscale', 'model.layers.26.self_attn.qkv_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.fuse_scales', 'model.layers.27.mlp.down_proj.Qidxs', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.Wscale', 'model.layers.27.mlp.down_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.Qidxs', 'model.layers.27.mlp.upgate_proj.SU', 'model.layers.27.mlp.upgate_proj.SV', 'model.layers.27.mlp.upgate_proj.Wscale', 'model.layers.27.mlp.upgate_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.fuse_scales', 'model.layers.27.self_attn.o_proj.Qidxs', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.Wscale', 'model.layers.27.self_attn.o_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.Qidxs', 'model.layers.27.self_attn.qkv_proj.SU', 'model.layers.27.self_attn.qkv_proj.SV', 'model.layers.27.self_attn.qkv_proj.Wscale', 'model.layers.27.self_attn.qkv_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.fuse_scales', 'model.layers.28.mlp.down_proj.Qidxs', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.Wscale', 'model.layers.28.mlp.down_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.Qidxs', 'model.layers.28.mlp.upgate_proj.SU', 'model.layers.28.mlp.upgate_proj.SV', 'model.layers.28.mlp.upgate_proj.Wscale', 'model.layers.28.mlp.upgate_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.fuse_scales', 'model.layers.28.self_attn.o_proj.Qidxs', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.Wscale', 'model.layers.28.self_attn.o_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.Qidxs', 'model.layers.28.self_attn.qkv_proj.SU', 'model.layers.28.self_attn.qkv_proj.SV', 'model.layers.28.self_attn.qkv_proj.Wscale', 'model.layers.28.self_attn.qkv_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.fuse_scales', 'model.layers.29.mlp.down_proj.Qidxs', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.Wscale', 'model.layers.29.mlp.down_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.Qidxs', 'model.layers.29.mlp.upgate_proj.SU', 'model.layers.29.mlp.upgate_proj.SV', 'model.layers.29.mlp.upgate_proj.Wscale', 'model.layers.29.mlp.upgate_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.fuse_scales', 'model.layers.29.self_attn.o_proj.Qidxs', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.Wscale', 'model.layers.29.self_attn.o_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.Qidxs', 'model.layers.29.self_attn.qkv_proj.SU', 'model.layers.29.self_attn.qkv_proj.SV', 'model.layers.29.self_attn.qkv_proj.Wscale', 'model.layers.29.self_attn.qkv_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.fuse_scales', 'model.layers.3.mlp.down_proj.Qidxs', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.Wscale', 'model.layers.3.mlp.down_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.Qidxs', 'model.layers.3.mlp.upgate_proj.SU', 'model.layers.3.mlp.upgate_proj.SV', 'model.layers.3.mlp.upgate_proj.Wscale', 'model.layers.3.mlp.upgate_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.fuse_scales', 'model.layers.3.self_attn.o_proj.Qidxs', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.Wscale', 'model.layers.3.self_attn.o_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.Qidxs', 'model.layers.3.self_attn.qkv_proj.SU', 'model.layers.3.self_attn.qkv_proj.SV', 'model.layers.3.self_attn.qkv_proj.Wscale', 'model.layers.3.self_attn.qkv_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.fuse_scales', 'model.layers.30.mlp.down_proj.Qidxs', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.Wscale', 'model.layers.30.mlp.down_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.Qidxs', 'model.layers.30.mlp.upgate_proj.SU', 'model.layers.30.mlp.upgate_proj.SV', 'model.layers.30.mlp.upgate_proj.Wscale', 'model.layers.30.mlp.upgate_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.fuse_scales', 'model.layers.30.self_attn.o_proj.Qidxs', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.Wscale', 'model.layers.30.self_attn.o_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.Qidxs', 'model.layers.30.self_attn.qkv_proj.SU', 'model.layers.30.self_attn.qkv_proj.SV', 'model.layers.30.self_attn.qkv_proj.Wscale', 'model.layers.30.self_attn.qkv_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.fuse_scales', 'model.layers.31.mlp.down_proj.Qidxs', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.Wscale', 'model.layers.31.mlp.down_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.Qidxs', 'model.layers.31.mlp.upgate_proj.SU', 'model.layers.31.mlp.upgate_proj.SV', 'model.layers.31.mlp.upgate_proj.Wscale', 'model.layers.31.mlp.upgate_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.fuse_scales', 'model.layers.31.self_attn.o_proj.Qidxs', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.Wscale', 'model.layers.31.self_attn.o_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.Qidxs', 'model.layers.31.self_attn.qkv_proj.SU', 'model.layers.31.self_attn.qkv_proj.SV', 'model.layers.31.self_attn.qkv_proj.Wscale', 'model.layers.31.self_attn.qkv_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.fuse_scales', 'model.layers.4.mlp.down_proj.Qidxs', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.Wscale', 'model.layers.4.mlp.down_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.Qidxs', 'model.layers.4.mlp.upgate_proj.SU', 'model.layers.4.mlp.upgate_proj.SV', 'model.layers.4.mlp.upgate_proj.Wscale', 'model.layers.4.mlp.upgate_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.fuse_scales', 'model.layers.4.self_attn.o_proj.Qidxs', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.Wscale', 'model.layers.4.self_attn.o_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.Qidxs', 'model.layers.4.self_attn.qkv_proj.SU', 'model.layers.4.self_attn.qkv_proj.SV', 'model.layers.4.self_attn.qkv_proj.Wscale', 'model.layers.4.self_attn.qkv_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.fuse_scales', 'model.layers.5.mlp.down_proj.Qidxs', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.Wscale', 'model.layers.5.mlp.down_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.Qidxs', 'model.layers.5.mlp.upgate_proj.SU', 'model.layers.5.mlp.upgate_proj.SV', 'model.layers.5.mlp.upgate_proj.Wscale', 'model.layers.5.mlp.upgate_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.fuse_scales', 'model.layers.5.self_attn.o_proj.Qidxs', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.Wscale', 'model.layers.5.self_attn.o_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.Qidxs', 'model.layers.5.self_attn.qkv_proj.SU', 'model.layers.5.self_attn.qkv_proj.SV', 'model.layers.5.self_attn.qkv_proj.Wscale', 'model.layers.5.self_attn.qkv_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.fuse_scales', 'model.layers.6.mlp.down_proj.Qidxs', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.Wscale', 'model.layers.6.mlp.down_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.Qidxs', 'model.layers.6.mlp.upgate_proj.SU', 'model.layers.6.mlp.upgate_proj.SV', 'model.layers.6.mlp.upgate_proj.Wscale', 'model.layers.6.mlp.upgate_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.fuse_scales', 'model.layers.6.self_attn.o_proj.Qidxs', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.Wscale', 'model.layers.6.self_attn.o_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.Qidxs', 'model.layers.6.self_attn.qkv_proj.SU', 'model.layers.6.self_attn.qkv_proj.SV', 'model.layers.6.self_attn.qkv_proj.Wscale', 'model.layers.6.self_attn.qkv_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.fuse_scales', 'model.layers.7.mlp.down_proj.Qidxs', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.Wscale', 'model.layers.7.mlp.down_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.Qidxs', 'model.layers.7.mlp.upgate_proj.SU', 'model.layers.7.mlp.upgate_proj.SV', 'model.layers.7.mlp.upgate_proj.Wscale', 'model.layers.7.mlp.upgate_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.fuse_scales', 'model.layers.7.self_attn.o_proj.Qidxs', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.Wscale', 'model.layers.7.self_attn.o_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.Qidxs', 'model.layers.7.self_attn.qkv_proj.SU', 'model.layers.7.self_attn.qkv_proj.SV', 'model.layers.7.self_attn.qkv_proj.Wscale', 'model.layers.7.self_attn.qkv_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.fuse_scales', 'model.layers.8.mlp.down_proj.Qidxs', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.Wscale', 'model.layers.8.mlp.down_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.Qidxs', 'model.layers.8.mlp.upgate_proj.SU', 'model.layers.8.mlp.upgate_proj.SV', 'model.layers.8.mlp.upgate_proj.Wscale', 'model.layers.8.mlp.upgate_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.fuse_scales', 'model.layers.8.self_attn.o_proj.Qidxs', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.Wscale', 'model.layers.8.self_attn.o_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.Qidxs', 'model.layers.8.self_attn.qkv_proj.SU', 'model.layers.8.self_attn.qkv_proj.SV', 'model.layers.8.self_attn.qkv_proj.Wscale', 'model.layers.8.self_attn.qkv_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.fuse_scales', 'model.layers.9.mlp.down_proj.Qidxs', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.Wscale', 'model.layers.9.mlp.down_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.Qidxs', 'model.layers.9.mlp.upgate_proj.SU', 'model.layers.9.mlp.upgate_proj.SV', 'model.layers.9.mlp.upgate_proj.Wscale', 'model.layers.9.mlp.upgate_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.fuse_scales', 'model.layers.9.self_attn.o_proj.Qidxs', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.Wscale', 'model.layers.9.self_attn.o_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.Qidxs', 'model.layers.9.self_attn.qkv_proj.SU', 'model.layers.9.self_attn.qkv_proj.SV', 'model.layers.9.self_attn.qkv_proj.Wscale', 'model.layers.9.self_attn.qkv_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.fuse_scales']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
W0412 07:38:10.118681 16707 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0412 07:38:10.119734 16707 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_qkv.pt',

W0412 07:38:10.137062 16707 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0412 07:38:10.151624 16707 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0412 07:38:10.187469 16707 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0412 07:38:10.204732 16707 hfize_llama.py:78] loaded layer 0 down
I0412 07:38:10.280705 16707 hfize_llama.py:78] loaded layer 1 down
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py", line 66, in main
    saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './ckpt/3_8b_2bit/2_up.pt'
[Stage: End-to-End Finetuning] K=2
W0412 07:38:12.845691 16984 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 07:38:12.874488 16984 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 07:38:13.191836 16984 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 07:38:13.191943 16984 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 07:38:13.191987 16984 utils.py:162] NumExpr defaulting to 16 threads.
I0412 07:38:13.317120 16984 config.py:58] PyTorch version 2.4.0 available.
W0412 07:38:13.702232 16984 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0412 07:38:37.705575 16984 warnings.py:110] /opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
I0412 07:39:01.447229 16984 modeling.py:1086] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.03it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  3.19it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.32it/s]
W0412 07:39:03.088834 16984 big_modeling.py:436] Some parameters are on the meta device because they were offloaded to the cpu.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 149, in <module>
    main(args)
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 101, in main
    orig_logits = utils.calculate_logits(orig_model, devset, args.batch_size)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/quip-sharp/lib/utils/data_utils.py", line 223, in calculate_logits
    model(devset[i * batch_size:(i + 1) *
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1208, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1018, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 738, in forward
    hidden_states = self.input_layernorm(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 88, in forward
    variance = hidden_states.pow(2).mean(-1, keepdim=True)
               ^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 47.51 GiB of which 100.75 MiB is free. Process 760454 has 41.46 GiB memory in use. Process 763472 has 5.93 GiB memory in use. Of the allocated memory 5.45 GiB is allocated by PyTorch, and 1.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[Stage: Re-convert to HF (post-e2e)] K=2
W0412 07:39:06.241177 17403 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 07:39:06.271048 17403 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 07:39:06.591125 17403 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 07:39:06.591240 17403 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 07:39:06.591285 17403 utils.py:162] NumExpr defaulting to 16 threads.
I0412 07:39:06.720533 17403 config.py:58] PyTorch version 2.4.0 available.
W0412 07:39:07.111177 17403 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0412 07:39:31.603279 17403 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

W0412 07:39:31.604368 17403 warnings.py:110] /opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.33it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00, 11.43it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 11.77it/s]
Some weights of the model checkpoint at meta-llama/Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.Qidxs', 'model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.Wscale', 'model.layers.0.mlp.down_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.Qidxs', 'model.layers.0.mlp.upgate_proj.SU', 'model.layers.0.mlp.upgate_proj.SV', 'model.layers.0.mlp.upgate_proj.Wscale', 'model.layers.0.mlp.upgate_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.fuse_scales', 'model.layers.0.self_attn.o_proj.Qidxs', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.Wscale', 'model.layers.0.self_attn.o_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.Qidxs', 'model.layers.0.self_attn.qkv_proj.SU', 'model.layers.0.self_attn.qkv_proj.SV', 'model.layers.0.self_attn.qkv_proj.Wscale', 'model.layers.0.self_attn.qkv_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.fuse_scales', 'model.layers.1.mlp.down_proj.Qidxs', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.Wscale', 'model.layers.1.mlp.down_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.Qidxs', 'model.layers.1.mlp.upgate_proj.SU', 'model.layers.1.mlp.upgate_proj.SV', 'model.layers.1.mlp.upgate_proj.Wscale', 'model.layers.1.mlp.upgate_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.fuse_scales', 'model.layers.1.self_attn.o_proj.Qidxs', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.Wscale', 'model.layers.1.self_attn.o_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.Qidxs', 'model.layers.1.self_attn.qkv_proj.SU', 'model.layers.1.self_attn.qkv_proj.SV', 'model.layers.1.self_attn.qkv_proj.Wscale', 'model.layers.1.self_attn.qkv_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.fuse_scales', 'model.layers.10.mlp.down_proj.Qidxs', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.Wscale', 'model.layers.10.mlp.down_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.Qidxs', 'model.layers.10.mlp.upgate_proj.SU', 'model.layers.10.mlp.upgate_proj.SV', 'model.layers.10.mlp.upgate_proj.Wscale', 'model.layers.10.mlp.upgate_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.fuse_scales', 'model.layers.10.self_attn.o_proj.Qidxs', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.Wscale', 'model.layers.10.self_attn.o_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.Qidxs', 'model.layers.10.self_attn.qkv_proj.SU', 'model.layers.10.self_attn.qkv_proj.SV', 'model.layers.10.self_attn.qkv_proj.Wscale', 'model.layers.10.self_attn.qkv_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.fuse_scales', 'model.layers.11.mlp.down_proj.Qidxs', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.Wscale', 'model.layers.11.mlp.down_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.Qidxs', 'model.layers.11.mlp.upgate_proj.SU', 'model.layers.11.mlp.upgate_proj.SV', 'model.layers.11.mlp.upgate_proj.Wscale', 'model.layers.11.mlp.upgate_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.fuse_scales', 'model.layers.11.self_attn.o_proj.Qidxs', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.Wscale', 'model.layers.11.self_attn.o_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.Qidxs', 'model.layers.11.self_attn.qkv_proj.SU', 'model.layers.11.self_attn.qkv_proj.SV', 'model.layers.11.self_attn.qkv_proj.Wscale', 'model.layers.11.self_attn.qkv_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.fuse_scales', 'model.layers.12.mlp.down_proj.Qidxs', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.Wscale', 'model.layers.12.mlp.down_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.Qidxs', 'model.layers.12.mlp.upgate_proj.SU', 'model.layers.12.mlp.upgate_proj.SV', 'model.layers.12.mlp.upgate_proj.Wscale', 'model.layers.12.mlp.upgate_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.fuse_scales', 'model.layers.12.self_attn.o_proj.Qidxs', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.Wscale', 'model.layers.12.self_attn.o_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.Qidxs', 'model.layers.12.self_attn.qkv_proj.SU', 'model.layers.12.self_attn.qkv_proj.SV', 'model.layers.12.self_attn.qkv_proj.Wscale', 'model.layers.12.self_attn.qkv_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.fuse_scales', 'model.layers.13.mlp.down_proj.Qidxs', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.Wscale', 'model.layers.13.mlp.down_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.Qidxs', 'model.layers.13.mlp.upgate_proj.SU', 'model.layers.13.mlp.upgate_proj.SV', 'model.layers.13.mlp.upgate_proj.Wscale', 'model.layers.13.mlp.upgate_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.fuse_scales', 'model.layers.13.self_attn.o_proj.Qidxs', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.Wscale', 'model.layers.13.self_attn.o_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.Qidxs', 'model.layers.13.self_attn.qkv_proj.SU', 'model.layers.13.self_attn.qkv_proj.SV', 'model.layers.13.self_attn.qkv_proj.Wscale', 'model.layers.13.self_attn.qkv_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.fuse_scales', 'model.layers.14.mlp.down_proj.Qidxs', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.Wscale', 'model.layers.14.mlp.down_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.Qidxs', 'model.layers.14.mlp.upgate_proj.SU', 'model.layers.14.mlp.upgate_proj.SV', 'model.layers.14.mlp.upgate_proj.Wscale', 'model.layers.14.mlp.upgate_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.fuse_scales', 'model.layers.14.self_attn.o_proj.Qidxs', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.Wscale', 'model.layers.14.self_attn.o_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.Qidxs', 'model.layers.14.self_attn.qkv_proj.SU', 'model.layers.14.self_attn.qkv_proj.SV', 'model.layers.14.self_attn.qkv_proj.Wscale', 'model.layers.14.self_attn.qkv_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.fuse_scales', 'model.layers.15.mlp.down_proj.Qidxs', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.Wscale', 'model.layers.15.mlp.down_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.Qidxs', 'model.layers.15.mlp.upgate_proj.SU', 'model.layers.15.mlp.upgate_proj.SV', 'model.layers.15.mlp.upgate_proj.Wscale', 'model.layers.15.mlp.upgate_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.fuse_scales', 'model.layers.15.self_attn.o_proj.Qidxs', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.Wscale', 'model.layers.15.self_attn.o_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.Qidxs', 'model.layers.15.self_attn.qkv_proj.SU', 'model.layers.15.self_attn.qkv_proj.SV', 'model.layers.15.self_attn.qkv_proj.Wscale', 'model.layers.15.self_attn.qkv_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.fuse_scales', 'model.layers.16.mlp.down_proj.Qidxs', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.Wscale', 'model.layers.16.mlp.down_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.Qidxs', 'model.layers.16.mlp.upgate_proj.SU', 'model.layers.16.mlp.upgate_proj.SV', 'model.layers.16.mlp.upgate_proj.Wscale', 'model.layers.16.mlp.upgate_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.fuse_scales', 'model.layers.16.self_attn.o_proj.Qidxs', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.Wscale', 'model.layers.16.self_attn.o_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.Qidxs', 'model.layers.16.self_attn.qkv_proj.SU', 'model.layers.16.self_attn.qkv_proj.SV', 'model.layers.16.self_attn.qkv_proj.Wscale', 'model.layers.16.self_attn.qkv_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.fuse_scales', 'model.layers.17.mlp.down_proj.Qidxs', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.Wscale', 'model.layers.17.mlp.down_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.Qidxs', 'model.layers.17.mlp.upgate_proj.SU', 'model.layers.17.mlp.upgate_proj.SV', 'model.layers.17.mlp.upgate_proj.Wscale', 'model.layers.17.mlp.upgate_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.fuse_scales', 'model.layers.17.self_attn.o_proj.Qidxs', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.Wscale', 'model.layers.17.self_attn.o_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.Qidxs', 'model.layers.17.self_attn.qkv_proj.SU', 'model.layers.17.self_attn.qkv_proj.SV', 'model.layers.17.self_attn.qkv_proj.Wscale', 'model.layers.17.self_attn.qkv_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.fuse_scales', 'model.layers.18.mlp.down_proj.Qidxs', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.Wscale', 'model.layers.18.mlp.down_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.Qidxs', 'model.layers.18.mlp.upgate_proj.SU', 'model.layers.18.mlp.upgate_proj.SV', 'model.layers.18.mlp.upgate_proj.Wscale', 'model.layers.18.mlp.upgate_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.fuse_scales', 'model.layers.18.self_attn.o_proj.Qidxs', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.Wscale', 'model.layers.18.self_attn.o_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.Qidxs', 'model.layers.18.self_attn.qkv_proj.SU', 'model.layers.18.self_attn.qkv_proj.SV', 'model.layers.18.self_attn.qkv_proj.Wscale', 'model.layers.18.self_attn.qkv_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.fuse_scales', 'model.layers.19.mlp.down_proj.Qidxs', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.Wscale', 'model.layers.19.mlp.down_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.Qidxs', 'model.layers.19.mlp.upgate_proj.SU', 'model.layers.19.mlp.upgate_proj.SV', 'model.layers.19.mlp.upgate_proj.Wscale', 'model.layers.19.mlp.upgate_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.fuse_scales', 'model.layers.19.self_attn.o_proj.Qidxs', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.Wscale', 'model.layers.19.self_attn.o_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.Qidxs', 'model.layers.19.self_attn.qkv_proj.SU', 'model.layers.19.self_attn.qkv_proj.SV', 'model.layers.19.self_attn.qkv_proj.Wscale', 'model.layers.19.self_attn.qkv_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.fuse_scales', 'model.layers.2.mlp.down_proj.Qidxs', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.Wscale', 'model.layers.2.mlp.down_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.Qidxs', 'model.layers.2.mlp.upgate_proj.SU', 'model.layers.2.mlp.upgate_proj.SV', 'model.layers.2.mlp.upgate_proj.Wscale', 'model.layers.2.mlp.upgate_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.fuse_scales', 'model.layers.2.self_attn.o_proj.Qidxs', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.Wscale', 'model.layers.2.self_attn.o_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.Qidxs', 'model.layers.2.self_attn.qkv_proj.SU', 'model.layers.2.self_attn.qkv_proj.SV', 'model.layers.2.self_attn.qkv_proj.Wscale', 'model.layers.2.self_attn.qkv_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.fuse_scales', 'model.layers.20.mlp.down_proj.Qidxs', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.Wscale', 'model.layers.20.mlp.down_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.Qidxs', 'model.layers.20.mlp.upgate_proj.SU', 'model.layers.20.mlp.upgate_proj.SV', 'model.layers.20.mlp.upgate_proj.Wscale', 'model.layers.20.mlp.upgate_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.fuse_scales', 'model.layers.20.self_attn.o_proj.Qidxs', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.Wscale', 'model.layers.20.self_attn.o_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.Qidxs', 'model.layers.20.self_attn.qkv_proj.SU', 'model.layers.20.self_attn.qkv_proj.SV', 'model.layers.20.self_attn.qkv_proj.Wscale', 'model.layers.20.self_attn.qkv_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.fuse_scales', 'model.layers.21.mlp.down_proj.Qidxs', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.Wscale', 'model.layers.21.mlp.down_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.Qidxs', 'model.layers.21.mlp.upgate_proj.SU', 'model.layers.21.mlp.upgate_proj.SV', 'model.layers.21.mlp.upgate_proj.Wscale', 'model.layers.21.mlp.upgate_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.fuse_scales', 'model.layers.21.self_attn.o_proj.Qidxs', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.Wscale', 'model.layers.21.self_attn.o_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.Qidxs', 'model.layers.21.self_attn.qkv_proj.SU', 'model.layers.21.self_attn.qkv_proj.SV', 'model.layers.21.self_attn.qkv_proj.Wscale', 'model.layers.21.self_attn.qkv_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.fuse_scales', 'model.layers.22.mlp.down_proj.Qidxs', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.Wscale', 'model.layers.22.mlp.down_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.Qidxs', 'model.layers.22.mlp.upgate_proj.SU', 'model.layers.22.mlp.upgate_proj.SV', 'model.layers.22.mlp.upgate_proj.Wscale', 'model.layers.22.mlp.upgate_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.fuse_scales', 'model.layers.22.self_attn.o_proj.Qidxs', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.Wscale', 'model.layers.22.self_attn.o_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.Qidxs', 'model.layers.22.self_attn.qkv_proj.SU', 'model.layers.22.self_attn.qkv_proj.SV', 'model.layers.22.self_attn.qkv_proj.Wscale', 'model.layers.22.self_attn.qkv_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.fuse_scales', 'model.layers.23.mlp.down_proj.Qidxs', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.Wscale', 'model.layers.23.mlp.down_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.Qidxs', 'model.layers.23.mlp.upgate_proj.SU', 'model.layers.23.mlp.upgate_proj.SV', 'model.layers.23.mlp.upgate_proj.Wscale', 'model.layers.23.mlp.upgate_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.fuse_scales', 'model.layers.23.self_attn.o_proj.Qidxs', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.Wscale', 'model.layers.23.self_attn.o_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.Qidxs', 'model.layers.23.self_attn.qkv_proj.SU', 'model.layers.23.self_attn.qkv_proj.SV', 'model.layers.23.self_attn.qkv_proj.Wscale', 'model.layers.23.self_attn.qkv_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.fuse_scales', 'model.layers.24.mlp.down_proj.Qidxs', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.Wscale', 'model.layers.24.mlp.down_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.Qidxs', 'model.layers.24.mlp.upgate_proj.SU', 'model.layers.24.mlp.upgate_proj.SV', 'model.layers.24.mlp.upgate_proj.Wscale', 'model.layers.24.mlp.upgate_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.fuse_scales', 'model.layers.24.self_attn.o_proj.Qidxs', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.Wscale', 'model.layers.24.self_attn.o_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.Qidxs', 'model.layers.24.self_attn.qkv_proj.SU', 'model.layers.24.self_attn.qkv_proj.SV', 'model.layers.24.self_attn.qkv_proj.Wscale', 'model.layers.24.self_attn.qkv_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.fuse_scales', 'model.layers.25.mlp.down_proj.Qidxs', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.Wscale', 'model.layers.25.mlp.down_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.Qidxs', 'model.layers.25.mlp.upgate_proj.SU', 'model.layers.25.mlp.upgate_proj.SV', 'model.layers.25.mlp.upgate_proj.Wscale', 'model.layers.25.mlp.upgate_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.fuse_scales', 'model.layers.25.self_attn.o_proj.Qidxs', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.Wscale', 'model.layers.25.self_attn.o_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.Qidxs', 'model.layers.25.self_attn.qkv_proj.SU', 'model.layers.25.self_attn.qkv_proj.SV', 'model.layers.25.self_attn.qkv_proj.Wscale', 'model.layers.25.self_attn.qkv_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.fuse_scales', 'model.layers.26.mlp.down_proj.Qidxs', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.Wscale', 'model.layers.26.mlp.down_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.Qidxs', 'model.layers.26.mlp.upgate_proj.SU', 'model.layers.26.mlp.upgate_proj.SV', 'model.layers.26.mlp.upgate_proj.Wscale', 'model.layers.26.mlp.upgate_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.fuse_scales', 'model.layers.26.self_attn.o_proj.Qidxs', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.Wscale', 'model.layers.26.self_attn.o_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.Qidxs', 'model.layers.26.self_attn.qkv_proj.SU', 'model.layers.26.self_attn.qkv_proj.SV', 'model.layers.26.self_attn.qkv_proj.Wscale', 'model.layers.26.self_attn.qkv_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.fuse_scales', 'model.layers.27.mlp.down_proj.Qidxs', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.Wscale', 'model.layers.27.mlp.down_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.Qidxs', 'model.layers.27.mlp.upgate_proj.SU', 'model.layers.27.mlp.upgate_proj.SV', 'model.layers.27.mlp.upgate_proj.Wscale', 'model.layers.27.mlp.upgate_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.fuse_scales', 'model.layers.27.self_attn.o_proj.Qidxs', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.Wscale', 'model.layers.27.self_attn.o_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.Qidxs', 'model.layers.27.self_attn.qkv_proj.SU', 'model.layers.27.self_attn.qkv_proj.SV', 'model.layers.27.self_attn.qkv_proj.Wscale', 'model.layers.27.self_attn.qkv_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.fuse_scales', 'model.layers.28.mlp.down_proj.Qidxs', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.Wscale', 'model.layers.28.mlp.down_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.Qidxs', 'model.layers.28.mlp.upgate_proj.SU', 'model.layers.28.mlp.upgate_proj.SV', 'model.layers.28.mlp.upgate_proj.Wscale', 'model.layers.28.mlp.upgate_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.fuse_scales', 'model.layers.28.self_attn.o_proj.Qidxs', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.Wscale', 'model.layers.28.self_attn.o_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.Qidxs', 'model.layers.28.self_attn.qkv_proj.SU', 'model.layers.28.self_attn.qkv_proj.SV', 'model.layers.28.self_attn.qkv_proj.Wscale', 'model.layers.28.self_attn.qkv_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.fuse_scales', 'model.layers.29.mlp.down_proj.Qidxs', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.Wscale', 'model.layers.29.mlp.down_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.Qidxs', 'model.layers.29.mlp.upgate_proj.SU', 'model.layers.29.mlp.upgate_proj.SV', 'model.layers.29.mlp.upgate_proj.Wscale', 'model.layers.29.mlp.upgate_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.fuse_scales', 'model.layers.29.self_attn.o_proj.Qidxs', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.Wscale', 'model.layers.29.self_attn.o_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.Qidxs', 'model.layers.29.self_attn.qkv_proj.SU', 'model.layers.29.self_attn.qkv_proj.SV', 'model.layers.29.self_attn.qkv_proj.Wscale', 'model.layers.29.self_attn.qkv_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.fuse_scales', 'model.layers.3.mlp.down_proj.Qidxs', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.Wscale', 'model.layers.3.mlp.down_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.Qidxs', 'model.layers.3.mlp.upgate_proj.SU', 'model.layers.3.mlp.upgate_proj.SV', 'model.layers.3.mlp.upgate_proj.Wscale', 'model.layers.3.mlp.upgate_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.fuse_scales', 'model.layers.3.self_attn.o_proj.Qidxs', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.Wscale', 'model.layers.3.self_attn.o_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.Qidxs', 'model.layers.3.self_attn.qkv_proj.SU', 'model.layers.3.self_attn.qkv_proj.SV', 'model.layers.3.self_attn.qkv_proj.Wscale', 'model.layers.3.self_attn.qkv_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.fuse_scales', 'model.layers.30.mlp.down_proj.Qidxs', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.Wscale', 'model.layers.30.mlp.down_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.Qidxs', 'model.layers.30.mlp.upgate_proj.SU', 'model.layers.30.mlp.upgate_proj.SV', 'model.layers.30.mlp.upgate_proj.Wscale', 'model.layers.30.mlp.upgate_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.fuse_scales', 'model.layers.30.self_attn.o_proj.Qidxs', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.Wscale', 'model.layers.30.self_attn.o_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.Qidxs', 'model.layers.30.self_attn.qkv_proj.SU', 'model.layers.30.self_attn.qkv_proj.SV', 'model.layers.30.self_attn.qkv_proj.Wscale', 'model.layers.30.self_attn.qkv_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.fuse_scales', 'model.layers.31.mlp.down_proj.Qidxs', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.Wscale', 'model.layers.31.mlp.down_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.Qidxs', 'model.layers.31.mlp.upgate_proj.SU', 'model.layers.31.mlp.upgate_proj.SV', 'model.layers.31.mlp.upgate_proj.Wscale', 'model.layers.31.mlp.upgate_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.fuse_scales', 'model.layers.31.self_attn.o_proj.Qidxs', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.Wscale', 'model.layers.31.self_attn.o_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.Qidxs', 'model.layers.31.self_attn.qkv_proj.SU', 'model.layers.31.self_attn.qkv_proj.SV', 'model.layers.31.self_attn.qkv_proj.Wscale', 'model.layers.31.self_attn.qkv_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.fuse_scales', 'model.layers.4.mlp.down_proj.Qidxs', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.Wscale', 'model.layers.4.mlp.down_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.Qidxs', 'model.layers.4.mlp.upgate_proj.SU', 'model.layers.4.mlp.upgate_proj.SV', 'model.layers.4.mlp.upgate_proj.Wscale', 'model.layers.4.mlp.upgate_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.fuse_scales', 'model.layers.4.self_attn.o_proj.Qidxs', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.Wscale', 'model.layers.4.self_attn.o_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.Qidxs', 'model.layers.4.self_attn.qkv_proj.SU', 'model.layers.4.self_attn.qkv_proj.SV', 'model.layers.4.self_attn.qkv_proj.Wscale', 'model.layers.4.self_attn.qkv_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.fuse_scales', 'model.layers.5.mlp.down_proj.Qidxs', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.Wscale', 'model.layers.5.mlp.down_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.Qidxs', 'model.layers.5.mlp.upgate_proj.SU', 'model.layers.5.mlp.upgate_proj.SV', 'model.layers.5.mlp.upgate_proj.Wscale', 'model.layers.5.mlp.upgate_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.fuse_scales', 'model.layers.5.self_attn.o_proj.Qidxs', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.Wscale', 'model.layers.5.self_attn.o_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.Qidxs', 'model.layers.5.self_attn.qkv_proj.SU', 'model.layers.5.self_attn.qkv_proj.SV', 'model.layers.5.self_attn.qkv_proj.Wscale', 'model.layers.5.self_attn.qkv_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.fuse_scales', 'model.layers.6.mlp.down_proj.Qidxs', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.Wscale', 'model.layers.6.mlp.down_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.Qidxs', 'model.layers.6.mlp.upgate_proj.SU', 'model.layers.6.mlp.upgate_proj.SV', 'model.layers.6.mlp.upgate_proj.Wscale', 'model.layers.6.mlp.upgate_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.fuse_scales', 'model.layers.6.self_attn.o_proj.Qidxs', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.Wscale', 'model.layers.6.self_attn.o_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.Qidxs', 'model.layers.6.self_attn.qkv_proj.SU', 'model.layers.6.self_attn.qkv_proj.SV', 'model.layers.6.self_attn.qkv_proj.Wscale', 'model.layers.6.self_attn.qkv_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.fuse_scales', 'model.layers.7.mlp.down_proj.Qidxs', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.Wscale', 'model.layers.7.mlp.down_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.Qidxs', 'model.layers.7.mlp.upgate_proj.SU', 'model.layers.7.mlp.upgate_proj.SV', 'model.layers.7.mlp.upgate_proj.Wscale', 'model.layers.7.mlp.upgate_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.fuse_scales', 'model.layers.7.self_attn.o_proj.Qidxs', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.Wscale', 'model.layers.7.self_attn.o_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.Qidxs', 'model.layers.7.self_attn.qkv_proj.SU', 'model.layers.7.self_attn.qkv_proj.SV', 'model.layers.7.self_attn.qkv_proj.Wscale', 'model.layers.7.self_attn.qkv_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.fuse_scales', 'model.layers.8.mlp.down_proj.Qidxs', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.Wscale', 'model.layers.8.mlp.down_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.Qidxs', 'model.layers.8.mlp.upgate_proj.SU', 'model.layers.8.mlp.upgate_proj.SV', 'model.layers.8.mlp.upgate_proj.Wscale', 'model.layers.8.mlp.upgate_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.fuse_scales', 'model.layers.8.self_attn.o_proj.Qidxs', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.Wscale', 'model.layers.8.self_attn.o_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.Qidxs', 'model.layers.8.self_attn.qkv_proj.SU', 'model.layers.8.self_attn.qkv_proj.SV', 'model.layers.8.self_attn.qkv_proj.Wscale', 'model.layers.8.self_attn.qkv_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.fuse_scales', 'model.layers.9.mlp.down_proj.Qidxs', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.Wscale', 'model.layers.9.mlp.down_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.Qidxs', 'model.layers.9.mlp.upgate_proj.SU', 'model.layers.9.mlp.upgate_proj.SV', 'model.layers.9.mlp.upgate_proj.Wscale', 'model.layers.9.mlp.upgate_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.fuse_scales', 'model.layers.9.self_attn.o_proj.Qidxs', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.Wscale', 'model.layers.9.self_attn.o_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.Qidxs', 'model.layers.9.self_attn.qkv_proj.SU', 'model.layers.9.self_attn.qkv_proj.SV', 'model.layers.9.self_attn.qkv_proj.Wscale', 'model.layers.9.self_attn.qkv_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.fuse_scales']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
W0412 07:39:38.377270 17403 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0412 07:39:38.378494 17403 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_qkv.pt',

W0412 07:39:38.395805 17403 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0412 07:39:38.411124 17403 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0412 07:39:38.444409 17403 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0412 07:39:38.464728 17403 hfize_llama.py:78] loaded layer 0 down
I0412 07:39:38.545138 17403 hfize_llama.py:78] loaded layer 1 down
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py", line 108, in <module>
    main(args)
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py", line 66, in main
    saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './ckpt/3_8b_2bit/2_up.pt'
[Stage: Eval PPL] K=2
I0412 07:39:41.143170 17680 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 07:39:41.143375 17680 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 07:39:41.143427 17680 utils.py:162] NumExpr defaulting to 16 threads.
I0412 07:39:41.510769 17680 config.py:58] PyTorch version 2.4.0 available.
W0412 07:39:41.662642 17680 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 07:39:41.699729 17680 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0412 07:39:41.922984 17680 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 398, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/3_8b_2bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/eval/eval_ppl.py", line 85, in <module>
    main(args)
  File "/workspace/Weight_compression/quip-sharp/eval/eval_ppl.py", line 39, in main
    model, model_str = model_from_hf_path(
                       ^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/quip-sharp/lib/utils/unsafe_import.py", line 23, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 928, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 631, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 686, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 462, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/3_8b_2bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
[Stage: Eval Zero-shot] K=2
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/eval/eval_zeroshot.py", line 13, in <module>
    from lm_eval.models.huggingface import HFLM
ModuleNotFoundError: No module named 'lm_eval.models.huggingface'
